<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009807">
<title confidence="0.999051">
Competitive Grouping in Integrated Phrase Segmentation
and Alignment Model
</title>
<author confidence="0.999302">
Ying Zhang Stephan Vogel
</author>
<affiliation confidence="0.989458">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.752735">
Pittsburgh, PA 15213
</address>
<email confidence="0.999266">
{joy+,vogel+}@cs.cmu.edu
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913727272727">
This article describes the competitive
grouping algorithm at the core of our Inte-
grated Segmentation and Alignment (ISA)
model. ISA extracts phrase pairs from a
bilingual corpus without requiring the pre-
calculated word alignment as many other
phrase alignment models do. Experiments
conducted within the WPT-05 shared task
on statistical machine translation demon-
strate the simplicity and effectiveness of
this approach.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921736842105">
In recent years, various phrase translation ap-
proaches (Marcu and Wong, 2002; Och et al., 1999;
Koehn et al., 2003) have been shown to outper-
form word-to-word translation models (Brown et al.,
1993). Many of these phrase alignment strategies
rely on the pre-calculated word alignment and use
different heuristics to extract the phrase pairs from
the Viterbi word alignment path. The Integrated
Segmentation and Alignment (ISA) model (Zhang
et al., 2003) does not require such word alignment.
ISA segments the sentence into phrases and finds
their alignment simultaneously. ISA is simple and
fast. Translation experiments have shown compara-
ble performance to other phrase alignment strategies
which require complicated statistical model training.
In this paper, we describe the key idea behind this
model and connect it with the competitive linking al-
gorithm (Melamed, 1997) which was developed for
word-to-word alignment.
</bodyText>
<sectionHeader confidence="0.9267965" genericHeader="method">
2 Translation Likelihood as a Statistical
Test
</sectionHeader>
<bodyText confidence="0.999591615384615">
Given a bilingual corpus of language pair F (For-
eign, source language) and E (English, target lan-
guage), if we know the word alignment for each sen-
tence pair we can calculate the co-occurrence fre-
quency for each source/target word pair type C(f, e)
and the marginal frequency C(f) _ Ee C(f, e) and
C(e) _ E f C(f, e). We can apply various sta-
tistical tests (Manning and Sch¨utze, 1999) to mea-
sure how likely is the association between f and
e, in other words how likely they are mutual trans-
lations. In the following sections, we will use x2
statistics to measure the the mutual translation like-
lihood (Church and Hanks, 1990).
</bodyText>
<sectionHeader confidence="0.976637" genericHeader="method">
3 The Core of the Integrated Phrase
Segmentation and Alignment
</sectionHeader>
<bodyText confidence="0.989860533333333">
The competitive linking algorithm (CLA)
(Melamed, 1997) is a greedy word alignment
algorithm. It was designed to overcome the problem
of indirect associations using a simple heuristic:
whenever several word tokens fi in one half of the
bilingual corpus co-occur with a particular word to-
ken e in the other half of the corpus, the word that is
most likely to be e’s translation is the one for which
the likelihood L(f, e) of translational equivalence
is highest. The simplicity of this algorithm depends
on a one-to-one alignment assumption. Each word
translates to at most one other word. Thus when
one pair {f, e} is “linked”, neither f nor e can be
aligned with any other words. This assumption
renders CLA unusable in phrase level alignment.
</bodyText>
<page confidence="0.978811">
159
</page>
<note confidence="0.8346895">
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 159–162,
Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<bodyText confidence="0.999943">
We propose an extension, the competitive grouping,
as the core component in the ISA model.
</bodyText>
<subsectionHeader confidence="0.999715">
3.1 Competitive Grouping Algorithm (CGA)
</subsectionHeader>
<bodyText confidence="0.998817388888889">
The key modification to the competitive linking al-
gorithm is to make it less greedy. When a word pair
is found to be the winner of the competition, we al-
low it to invite its neighbors to join the “winner’s
club” and group them together as an aligned phrase
pair. The one-to-one assumption is thus discarded
in CGA. In addition, we introduce the locality as-
sumption for phrase alignment. Locality states that a
source phrase of adjacent words can only be aligned
to a target phrase composed of adjacent words. This
is not true of most language pairs in cases such as
the relative clause, passive tense, and prepositional
clause, etc.; however this assumption renders the
problem tractable. Here is a description of CGA:
For a sentence pair {f, e}, represent the word pair
statistics for each word pair {f, e} in a two dimen-
sional matrix LI,J, where L(i, j) = χ2(fi, ej) in
our implementation. 1
</bodyText>
<figureCaption confidence="0.998619">
Figure 1: Expanding the current phrase pair
</figureCaption>
<bodyText confidence="0.964059666666667">
Denote an aligned phrase pair { f, e} as
a tuple [istart, iend, jstart, jend] where f is
fistart, fistart+1, ... , fiend and similarly for E
</bodyText>
<listItem confidence="0.953492">
1. Find i* and j* such that L(i*, j*) is the highest.
Create aseedphrase pair [i*, i*, j*, j*] which is
simply the word pair {fi*, ej*} itself.
2. Expand the current phrase pair
[istart, iend, jstart, jend] to the neighboring
territory to include adjacent source and target
words in the phrase alignment group. There
</listItem>
<bodyText confidence="0.999139307692308">
1χ2 statistics were found to be more discriminative in our
experiments than other symmetric word association measures,
such as the averaged mutual information, 02 statistics and Dice-
coefficient.
are 8 ways to group new words into the phrase
pair. For example, one can expand to the
north by including an additional source word
fistart_1 to be aligned with all the target words
in the current group; or one can expand to the
northeast by including fistart_1 and ejend+1
(Figure 1).
Two criteria have to be satisfied for each expan-
sion:
</bodyText>
<listItem confidence="0.972390384615385">
(a) If a new source word fig is to be grouped,
maxjstart&lt;j&lt;jend L(i&apos;, j) should be no
smaller than max1&lt;j&lt;J L(i&apos;, j). Since
CGA is a greedy algorithm as described
below, this is to guarantee that fig will not
“regret” the decision of joining the phrase
pair because it does not have other “better”
target words to be aligned with. Similar
constraint is applied if a new target word
ejg is to be grouped.
(b) The highest value in the newly-expanded
area needs to be “similar” to the seed value
L(i*,j*).
</listItem>
<bodyText confidence="0.951392">
Expand the current phrase pair to the largest ex-
tend possible as long as both criteria are satis-
fied.
</bodyText>
<listItem confidence="0.996586571428572">
3. The locality assumption means that the aligned
phrase cannot be aligned again. Therefore, all
the source and target words in the phrase pair
are marked as “invalid” and will be skipped in
the following steps.
4. If there is another valid pair {fi, ej}, then re-
peat from Step 1.
</listItem>
<figureCaption confidence="0.9189775">
Figure 2 and Figure 3 show a simple example
of applying CGA on the sentence pair {je d´eclare
reprise la session/i declare resumed the session}.
Figure 2: Seed pair {je / i}, no expansion allowed
</figureCaption>
<page confidence="0.948351">
160
</page>
<figureCaption confidence="0.96867">
Figure 3: Seed pair {session/session}, expanded to
{la session/the session}
</figureCaption>
<subsectionHeader confidence="0.998504">
3.2 Exploring all possible groupings
</subsectionHeader>
<bodyText confidence="0.99995762962963">
The similarity criterion 2-(b) described previously
is used to control the granularity of phrase pairs.
In cases where the pairs {f1f2, e1e2}, {f1, e1} and
{f2, e2} are all valid translations pairs, similar-
ity is used to control whether we want to align
{f1f2, e1e2} as one phrase pair or two shorter ones.
The granularity of the phrase pairs is hard to op-
timize especially when the test data is unknown. On
the one hand, we prefer long phrases since inter-
action among the words in the phrase, for example
word sense, morphology and local reordering could
be encapsulated. On the other hand, long phrase
pairs are less likely to occur in the test data than the
shorter ones and may lead to low coverage. To have
both long and short phrases in the alignment, we ap-
ply a range of similarity thresholds for each of the
expansion operations. By applying a low similarity
threshold, the expanded phrase pairs tend to be large,
while a higher similarity threshold results in shorter
phrase pairs. As described above, CGA is a greedy
algorithm and the expansion of the seed pair restricts
the possible alignments for the rest of the sentence.
Figure 4 shows an example as we explore all the pos-
sible grouping choices in a depth-first search. In the
end, all unique phrase pairs along the path traveled
are output as phrase translation candidates for the
current sentence pair.
</bodyText>
<subsectionHeader confidence="0.998914">
3.3 Phrase translation probabilities
</subsectionHeader>
<bodyText confidence="0.885191875">
Each aligned phrase pair { �f, e} is assigned a likeli-
hood score L(f, e), defined as:
Ei maxj log L(fi, ej) + Ej maxi log L(fi, ej)
|�f |+ |�e|
where i ranges over all words in f and similarly j in
E
Given the collected phrase pairs and their likeli-
hood, we estimate the phrase translation probability
</bodyText>
<figureCaption confidence="0.992531">
Figure 4: Depth-first itinerary of all possible group-
ing choices.
</figureCaption>
<bodyText confidence="0.747551">
by their weighted frequency:
</bodyText>
<equation confidence="0.873162666666667">
f, E) · L(�f, e)
Ef˜ count(�f, e) · L(f, e)
No smoothing is applied to the probabilities.
</equation>
<sectionHeader confidence="0.803905" genericHeader="method">
4 Learning co-occurrence information
</sectionHeader>
<bodyText confidence="0.992872181818182">
In most cases, word alignment information is not
given and is treated as a hidden parameter in the
training process. We initialize a word pair co-
occurrence frequency by assuming uniform align-
ment for each sentence pair, i.e. for sentence pair
(f, e) where f has I words and e has J words, each
word pair {f, e} is considered to be aligned with fre-
quency 1
I�J . These co-occurrence frequencies will
be accumulated over the whole corpus to calculate
the initial L(f, e). Then we iterate the ISA model:
</bodyText>
<listItem confidence="0.926607111111111">
1. Apply the competitive grouping algorithm to
each sentence pair to find all possible phrase
pairs.
2. For each identified phrase pair { f, e}, increase
the co-occurrence counts for all word pairs in-
side { f, e} with weight1
� ˜f���˜e�.
3. Calculate L(f, e) again and goto Step 1 for sev-
eral iterations.
</listItem>
<sectionHeader confidence="0.997994" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9716055">
We participated the shared task in the WPT05 work-
shop2 and applied ISA to all four language pairs
</bodyText>
<footnote confidence="0.680974">
2http://www.statmt.org/wpt05/mt-shared-task/
</footnote>
<equation confidence="0.911002333333333">
P(
count
f |e) _
</equation>
<page confidence="0.988862">
161
</page>
<bodyText confidence="0.99683">
(French-English, Finnish-English, German-English
and Spanish-English). Table 1 shows the n-gram
coverage of the dev-test set. French and Spanish
data are better covered by the training data com-
pared to the German and Finnish sets. Since our
phrase alignment is constrained by the locality as-
sumption and we can only extract phrase pairs of
adjacent words, lower n-gram coverage will result in
lower translation scores. We used the training data
</bodyText>
<table confidence="0.998903222222222">
Dev-test DE ES FI FR
N=1 99.2 99.6 98.2 99.8
N=2 88.2 93.3 73.0 94.7
N=3 59.4 71.7 38.2 76.0
N=4 30.0 42.9 17.0 50.6
N=5 13.0 21.7 6.8 29.8
N=16 (8) (65) (1) (101)
N=19 (1) (23) (34)
N=23 (1) (1)
</table>
<tableCaption confidence="0.996851">
Table 1: Percentage of dev-test n-grams covered by
</tableCaption>
<bodyText confidence="0.9751819">
the training data. Numbers in parenthesis are the
actual counts of n-gram tokens in the dev-test data.
and the language model as provided and manually
tuned the parameters of the Pharaoh decoder3 to op-
timize BLEU scores. Table 2 shows the translation
results on the dev-test and the test set of WPT05.
The BLEU scores appear comparable to those of
other state-of-the-art phrase alignment systems, in
spite of the simplicity of the ISA model and ease of
training.
</bodyText>
<table confidence="0.997396666666667">
DE ES FI FR
Dev-test 18.63 26.20 12.88 26.20
Test 18.93 26.14 12.66 26.71
</table>
<tableCaption confidence="0.994866">
Table 2: BLEU scores of ISA in WPT05
</tableCaption>
<sectionHeader confidence="0.998203" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998345">
In this paper, we introduced the competitive group-
ing algorithm which is at the core of the ISA phrase
alignment model. As an extension to the competitive
linking algorithm which is used for word-to-word
alignment, CGA overcomes the assumption of one-
to-one mapping and makes it possible to align phrase
</bodyText>
<footnote confidence="0.788238">
3http://www.isi.edu/licensed-sw/pharaoh/
</footnote>
<bodyText confidence="0.999479">
pairs. Despite its simplicity, the ISA model has
achieved competitive translation results. We plan to
release ISA toolkit4 to the community in the near
future.
</bodyText>
<sectionHeader confidence="0.998472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999735942857143">
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263–311.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Comput. Linguist., 16(1):22–29.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguistics
Conference (HLT/NAACL), Edomonton, Canada, May
27-June 1.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. of the Conference on Empirical Meth-
ods in Natural Language Processing, Philadephia, PA,
July 6-7.
I. Dan Melamed. 1997. A word-to-word model of trans-
lational equivalence. In Proceedings of the 8-th con-
ference on EACL, pages 490–497, Morristown, NJ,
USA. Association for Computational Linguistics.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proc. of the Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 20–28, University of
Maryland, College Park, MD, June.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2003. In-
tegrated phrase segmentation and alignment algorithm
for statistical machine translation. In Proceedings of
NLP-KE’03, Beijing, China, October.
</reference>
<footnote confidence="0.839416">
4http://projectile.is.cs.cmu.edu/research/public/isa/index.htm
</footnote>
<page confidence="0.991295">
162
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.645651">
<title confidence="0.9025565">Competitive Grouping in Integrated Phrase and Alignment Model</title>
<author confidence="0.986914">Ying Zhang Stephan</author>
<affiliation confidence="0.954992">Language Technologies School of Computer Carnegie Mellon</affiliation>
<address confidence="0.962944">Pittsburgh, PA</address>
<abstract confidence="0.99668175">This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="875" citStr="Brown et al., 1993" startWordPosition="120" endWordPosition="123">scribes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach. 1 Introduction In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al., 1999; Koehn et al., 2003) have been shown to outperform word-to-word translation models (Brown et al., 1993). Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase alignment strategies which require complicated statistical model training. In this paper, we describe the key idea behind this </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Comput. Linguist.,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="2272" citStr="Church and Hanks, 1990" startWordPosition="347" endWordPosition="350">en a bilingual corpus of language pair F (Foreign, source language) and E (English, target language), if we know the word alignment for each sentence pair we can calculate the co-occurrence frequency for each source/target word pair type C(f, e) and the marginal frequency C(f) _ Ee C(f, e) and C(e) _ E f C(f, e). We can apply various statistical tests (Manning and Sch¨utze, 1999) to measure how likely is the association between f and e, in other words how likely they are mutual translations. In the following sections, we will use x2 statistics to measure the the mutual translation likelihood (Church and Hanks, 1990). 3 The Core of the Integrated Phrase Segmentation and Alignment The competitive linking algorithm (CLA) (Melamed, 1997) is a greedy word alignment algorithm. It was designed to overcome the problem of indirect associations using a simple heuristic: whenever several word tokens fi in one half of the bilingual corpus co-occur with a particular word token e in the other half of the corpus, the word that is most likely to be e’s translation is the one for which the likelihood L(f, e) of translational equivalence is highest. The simplicity of this algorithm depends on a one-to-one alignment assump</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguist., 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL),</booktitle>
<location>Edomonton, Canada,</location>
<contexts>
<context position="792" citStr="Koehn et al., 2003" startWordPosition="107" endWordPosition="110">n University Pittsburgh, PA 15213 {joy+,vogel+}@cs.cmu.edu Abstract This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach. 1 Introduction In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al., 1999; Koehn et al., 2003) have been shown to outperform word-to-word translation models (Brown et al., 1993). Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase alignment strategies which require complicat</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL), Edomonton, Canada, May 27-June 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of statistical natural language processing. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>6--7</pages>
<location>Philadephia, PA,</location>
<contexts>
<context position="753" citStr="Marcu and Wong, 2002" startWordPosition="99" endWordPosition="102">chool of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 {joy+,vogel+}@cs.cmu.edu Abstract This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach. 1 Introduction In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al., 1999; Koehn et al., 2003) have been shown to outperform word-to-word translation models (Brown et al., 1993). Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase align</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proc. of the Conference on Empirical Methods in Natural Language Processing, Philadephia, PA, July 6-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>A word-to-word model of translational equivalence.</title>
<date>1997</date>
<booktitle>In Proceedings of the 8-th conference on EACL,</booktitle>
<pages>490--497</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1550" citStr="Melamed, 1997" startWordPosition="222" endWordPosition="223">alculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase alignment strategies which require complicated statistical model training. In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment. 2 Translation Likelihood as a Statistical Test Given a bilingual corpus of language pair F (Foreign, source language) and E (English, target language), if we know the word alignment for each sentence pair we can calculate the co-occurrence frequency for each source/target word pair type C(f, e) and the marginal frequency C(f) _ Ee C(f, e) and C(e) _ E f C(f, e). We can apply various statistical tests (Manning and Sch¨utze, 1999) to measure how likely is the association between f and e, in other words how likely they are mutual translations. In t</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. Dan Melamed. 1997. A word-to-word model of translational equivalence. In Proceedings of the 8-th conference on EACL, pages 490–497, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<institution>University of Maryland,</institution>
<location>College Park, MD,</location>
<contexts>
<context position="771" citStr="Och et al., 1999" startWordPosition="103" endWordPosition="106">nce Carnegie Mellon University Pittsburgh, PA 15213 {joy+,vogel+}@cs.cmu.edu Abstract This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach. 1 Introduction In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al., 1999; Koehn et al., 2003) have been shown to outperform word-to-word translation models (Brown et al., 1993). Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase alignment strategies wh</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proc. of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28, University of Maryland, College Park, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Integrated phrase segmentation and alignment algorithm for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NLP-KE’03,</booktitle>
<location>Beijing, China,</location>
<contexts>
<context position="1130" citStr="Zhang et al., 2003" startWordPosition="158" endWordPosition="161"> Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach. 1 Introduction In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al., 1999; Koehn et al., 2003) have been shown to outperform word-to-word translation models (Brown et al., 1993). Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase alignment strategies which require complicated statistical model training. In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment. 2 Translation Likelihood as a Statistical Test Given a bilingual corpus of language pair F (Foreign, source language) and E (Englis</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2003</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2003. Integrated phrase segmentation and alignment algorithm for statistical machine translation. In Proceedings of NLP-KE’03, Beijing, China, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>