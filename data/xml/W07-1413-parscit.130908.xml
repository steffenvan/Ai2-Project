<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.9069035">
Combining Lexical-Syntactic Information with Machine Learning for
Recognizing Textual Entailment
</title>
<author confidence="0.943263">
Arturo Montejo-R´aez, Jose Manuel Perea, Fernando Martinez-Santiago,
Miguel ´Angel Garcia-Cumbreras, Maite Martin-Valdivia, Alfonso Ure˜na-L´opez
</author>
<affiliation confidence="0.752384">
Dpto. de Inform´atica, Universidad de Ja´en
Campus de las Lagunillas s/n, 23071 - Ja´en
</affiliation>
<email confidence="0.968426">
{amontejo, jmperea, dofer, magc, maite, laurena}@ujaen.es
</email>
<sectionHeader confidence="0.996384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934769230769">
This document contains the description of
the experiments carried out by SINAI group.
We have developed an approach based on
several lexical and syntactic measures inte-
grated by means of different machine learn-
ing models. More precisely, we have eval-
uated three features based on lexical sim-
ilarity and 11 features based on syntactic
tree comparison. In spite of the relatively
straightforward approach we have obtained
more than 60% for accuracy. Since this
is our first participation we think we have
reached a good result.
</bodyText>
<sectionHeader confidence="0.935031" genericHeader="categories and subject descriptors">
1 Approach description
</sectionHeader>
<bodyText confidence="0.999989083333333">
We fill face the textual entailment recognition us-
ing Machine Learning methods, i.e. identifying fea-
tures that characterize the relation between hypothe-
sis and associated text and generating a model using
existing entailment judgements that will allow us to
provide a new entailment judgement agains unseen
pairs text-hypothesis. This approach can be split into
the two processes shown in Figures 1 and 2.
In a more formal way, given a text t and an hy-
pothesis h we want to define a function e which takes
these two elements as arguments and returns and an-
swer to the entailment question:
</bodyText>
<equation confidence="0.794766666666667">
r YES if h is entailed by t
e(t, h) =Sl (1)
NO otherwise
</equation>
<bodyText confidence="0.658289">
Now the question is to find that ideal function
</bodyText>
<page confidence="0.892562">
78
</page>
<bodyText confidence="0.686723">
e(t, h). We will approximate this function using a
binary classifier:
</bodyText>
<equation confidence="0.927642">
6(t, h) = bc(f, m) (2)
</equation>
<bodyText confidence="0.998951777777778">
where
bc is a binary classifier
f is a set of features
m is the learned model for the classifier
Therefore, it only remains to select a binary clas-
sifier and a feature extraction method. We have per-
formed two experiments with different choices for
both decisions. These two experiments are detailed
below.
</bodyText>
<figureCaption confidence="0.9999965">
Figure 1: Training processes
Figure 2: Classification processes
</figureCaption>
<bodyText confidence="0.734778">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 78–82,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.999655">
1.1 Lexical similarity
</subsectionHeader>
<bodyText confidence="0.999643307692308">
This experiment approaches the textual entailment
task being based on the extraction of a set of lexical
measures that show the existing similarity between
the hypothesis-text pairs. Our approach is similar
to (Ferrandez et al., 2007) but we make matching
between similar words too while (Ferrandez et al.,
2007) apply exact matching (see below).
The first step previous to the calculation of the
different measures is to preprocess the pairs using
the English stopwords list. Next we have used the
GATE1 architecture to obtain the stems of tokens.
Once obtained stems, we have applied four different
measures or techniques:
</bodyText>
<listItem confidence="0.922792368421053">
• Simple Matching: this technique consists of
calculating the semantic distance between each
stem of the hypothesis and text. If this dis-
tance exceeds a threshold, both stems are con-
sidered similar and the similarity weight value
increases in one. The accumulated weight is
normalized dividing it by the number of ele-
ments of the hypothesis. In this experiment we
have considered the threshold 0.5. The values
of semantic distance measure range from 0 to
1. In order to calculate the semantic distance
between two tokens (stems), we have tried sev-
eral measures based on WordNet (Alexander
Budanitsky and Graeme Hirst, 2001). Lin’s
similarity measure (Lin, 1998) was shown to
be best overall measures. It uses the notion of
information content and the same elements as
Jiang and Conrath’s approach (Jiang and Con-
rath, 1997) but in a different fashion:
</listItem>
<equation confidence="0.876814">
P
iCH similarity(i)
SIMmatching = |H|
1http://gate.ac.uk/
</equation>
<bodyText confidence="0.9964065">
where H is the set that contains the elements of
the hypothesis and similarity(i) is defined like:
</bodyText>
<equation confidence="0.6985795">
n 2 1 if E1j E TsimL(i, j) &gt; 0.5
similarity(i) — 0 otherwise
</equation>
<listItem confidence="0.8984888">
• Binary Matching: this measure is the same
that the previous one but modifying the simi-
larity function:
1/2 similarity(i) 1 if ]j E T i = j
y() 0 otherwise
• Consecutive Subsequence Matching: this
technique relies on forming subsequences of
consecutive stems in the hypothesis and match-
ing them in the text. The minimal size of the
consecutive subsequences is two and the max-
</listItem>
<bodyText confidence="0.966268363636364">
imum is the maximum size of the hypothesis.
Every correct matching increases in one the fi-
nal weight. The sum of the obtained weights of
the matching between subsequences of a cer-
tain size or length is normalized by the number
of sets of consecutive subsequences of the hy-
pothesis created for this length. These weights
are accumulated and normalized by the size of
the hypothesis less one. The Consecutive Sub-
sequence Matching technique is defined in the
following equations:
</bodyText>
<equation confidence="0.9811015">
P�H�
i=2 f(SHi)
CSSmatching =
|H |− 1
</equation>
<bodyText confidence="0.996050666666667">
where SHi is the set that contains the subse-
quences of the hypothesis with i size or length
and f(SHi) is defined like:
</bodyText>
<equation confidence="0.993453333333333">
f (SHi) =
PjcSH, matching(j)
|H |− i + 1
</equation>
<bodyText confidence="0.805989">
where
</bodyText>
<equation confidence="0.888768">
1/2 1 if ]k E STi k = j
matching(i) =
0 otherwise
</equation>
<bodyText confidence="0.898247">
where STi represents the set that contains the
subsequences with i size from text.
</bodyText>
<listItem confidence="0.999023333333333">
• Trigrams: this technique relies on forming tri-
grams of words in the hypothesis and match-
ing them in the text. A trigram is a group of
</listItem>
<equation confidence="0.9993575">
simL(c1, c2) =
log p(c1) + log p(c2)
</equation>
<bodyText confidence="0.999498285714286">
where c1 and c2 are synsets, lso(c1,c2) is
the information content of their lowest super-
ordinate (most specific common subsumer) and
p(c) is the probability of encountering an in-
stance of a synset c in some specific corpus
(Resnik, 1995). The Simple Matching tech-
nique is defined in the following equation:
</bodyText>
<equation confidence="0.847428">
2 x log p(lso(c1,c2))
</equation>
<page confidence="0.975894">
79
</page>
<bodyText confidence="0.9998666">
three words. If a hypothesis trigram matches in
text, then the similarity weight value increases
in one. The accumulated weight is normalized
dividing it by the number of trigrams of the hy-
pothesis.
</bodyText>
<subsectionHeader confidence="0.993737">
1.2 Syntactic tree comparison
</subsectionHeader>
<bodyText confidence="0.99497575">
Some features have been extracted from pairs
hypothesis-text related to the syntactic information
that some parser can produce. The rationale be-
hind it consists in measuring the similarity between
the syntactic trees of both hypothesis and associated
text. To do that, terms appearing in both trees are
identified (we call this alignment) and then, graph
distances (number of nodes) between those terms in
both trees are compared, producing certain values as
result.
In our experiments, we have applied the
COLLINS (Collins, 1999) parser to generate the
syntactic tree of both pieces of text. In Figure 3 the
output of the syntactic parsing for a sample pair is
shown. This data is the result of the syntactical anal-
ysis performed by the mentioned parser. A graph
based view of the tree corresponding to the hypoth-
esis is drawn in Figure 4. This graph will help us to
understand how certain similarity measures are ob-
tained.
</bodyText>
<figureCaption confidence="0.8965265">
Figure 3: Syntactic trees of sample hypothesis and
its associated text
</figureCaption>
<equation confidence="0.639334">
&lt;t&gt;
(TOP (S (LST (LS 0302) (. .)) (NP (JJ Next) (NN year))
(VP (VBZ is) (NP (NP (DT the) (JJ 50th) (NN anniversary))
(PP (IN of) (NP (NP (DT the) (NNP Normandy) (NN invasion)
(, ,)) (NP (NP (DT an)(NN event)) (SBAR (IN that) (S (VP
(MD would) (RB n’t) (VP (VB have) (VP (VBN been) (ADJP
(JJ possible)) (PP (IN without) (NP (NP (DT the) (NNP
Liberty) (NN ships.)) (SBAR (S (NP (DT The) (NNS
volunteers)) (VP (VBP hope) (S (VP (TO to) (VP (VB raise)
(NP (JJ enough) (NN money)) (S (VP (TO to) (VP (VB sail)
(NP (DT the) (NNP O’Brien)) (PP (TO to) (NP (NNP France)))
(PP (IN for)(NP (DT the) (JJ big) (NNP D-Day) (NN celebration)
(. .))))))))))))))))))))))))))
&lt;/t&gt;
&lt;h&gt;
(TOP (S (NP (NP (CD 50th) (NNP Anniversary)) (PP (IN of)
(NP (NNP Normandy) (NNP Landings)))) (VP (VBZ lasts) (NP
(DT a) (NN year) (. .)))))
&lt;/h&gt;
</equation>
<bodyText confidence="0.999828333333333">
From the sample above, the terms normandy, year
and anniversary appear in both pieces of text. We
say that these terms are “aligned”. Therefore, for
the three possible pairs of aligned terms we can com-
pute the distance, in nodes, to go from one term to
the other at each tree. Then, the difference of these
</bodyText>
<figureCaption confidence="0.9996292">
Figure 4: Syntact tree of sample hypothesis
distances is computed and some statistics are gener-
ated. We can summarize the process of computing
this differences in the algorithm detailed in Figure 6.
Figure 5: Tree comparison process
</figureCaption>
<bodyText confidence="0.4519355">
For instance, in the tree represented in Figure 4
we can see that we have to perform 5 steps to go
from node Anniversary to node Normandy. Since
there are no more possible occurrences of these two
terms, then the minimal distance between them is
5. This value is also measured on the tree corre-
</bodyText>
<page confidence="0.995067">
80
</page>
<bodyText confidence="0.981747">
sponding to the text, and the absolute difference be-
tween these two minimal distances is stored in order
to compute final feature weights consisting in basic
statistical values. The algorithm to obtain the distri-
bution of distance differences is detailed in Figure 6.
</bodyText>
<figureCaption confidence="0.9675285">
Figure 6: Extraction of features based on syntactic
distance
</figureCaption>
<figure confidence="0.842726176470588">
Input:
a syntactic tree of the hypothesis Sh
a syntactic tree of the text St
Output :
the set of distance differences
Dd = {ddij : ti7 tj E T}
Pseudo code:
T +— aligned terms between Sh and St
Dd &lt;-- O
for i = 1..n do
for j = i + 1..n do
disth +— minimal distance between
nodes ti and tj in Sh
distt minimal distance between
nodes ti and tj in St
ddij |disth − distt|
Dd {ddij} U Dd
</figure>
<bodyText confidence="0.688924">
end-for
end-for
The statistics generated from the resulting list of
distances differences Dd are the following:
</bodyText>
<listItem confidence="0.995754833333333">
1. The number of aligned terms (3 in the given
example).
2. The number of matched POS values of aligned
terms, that is, if the term appears with the same
POS label in both texts (in the example An-
niversary differs in the POS label assigned).
3. The number of unmatched POS labels of
aligned terms.
4. The average distance in nodes through the syn-
tactic tree to go from one aligned term to an-
other.
5. The minimal distance difference found.
</listItem>
<tableCaption confidence="0.862525666666667">
Table 1: Results with TiMBL and BBR classifiers
(Exp5 is the only official result reported in this pa-
per).
</tableCaption>
<table confidence="0.999395">
Experiment Classifier Accuracy
Exp1 BBR 0.6475
Exp2 BBR 0.64625
Exp3 BBR 0.63875
Exp4 TiMBL 0.6062
Exp5 TiMBL 0.6037
Exp6 TiMBL 0.57
</table>
<listItem confidence="0.922887">
6. The maximal distance difference found.
7. The standard deviation of distance differences.
</listItem>
<bodyText confidence="0.880583">
In a similar way, differences in the depth level of
nodes for aligned terms are also calculated. From
the example exposed the following values were
computed:
</bodyText>
<table confidence="0.999044363636363">
* Aligned 3
* MatchedPOS 2
* UnmatchedPOS 1
* AvgDistDiff 0.0392156863
* MinDistDiff 0.0000000000
* MaxDistDiff 0.0588235294
* StdevDistDiff 0.0277296777
* AvgDepthDiff 2.0000000000
* MinDepthDiff 1.0000000000
* MaxDepthDiff 3.0000000000
* StdevDepthDiff 0.8164965809
</table>
<sectionHeader confidence="0.910259" genericHeader="evaluation">
2 Experiments and results
</sectionHeader>
<bodyText confidence="0.999854166666667">
The algorithms used as binary classifiers are two:
Bayesian Logistic Regression (BBR)2 and TiMBL
(Daelemans et al., 1998). Both algorithms have been
trained with the devel data provided by the organiza-
tion of the Pascal challange. As has been explained
in previous sections, a model is generated via the
supervised learning process. This model m is then
feed into the classification variant of the algorithm,
which will decide whether a new hypothesis sample
is entailed by the given text or not.
The experiments and results are shown in Table 1:
where:
</bodyText>
<listItem confidence="0.959722">
• Exp1 uses four features: three lexical similari-
ties (SIMmatching + CSSmatching + Trigrams)
and Syntactic tree comparison.
</listItem>
<footnote confidence="0.9952455">
2http://www.stat.rutgers.edu/˜madigan/BBR/ [available at
March 27, 2007]
</footnote>
<page confidence="0.997534">
81
</page>
<listItem confidence="0.987497142857143">
• Exp2 uses five features: four lexical similari-
ties (SIMmatching + CSSmatching + Trigrams
+ BINmatching) and Syntactic tree compari-
son.
• Exp3 uses only three lexical similarities
(SIMmatching + CSSmatching + Trigrams).
• Exp4 uses the four lexical similarities
(SIMmatching + CSSmatching + Trigrams +
BINmatching)
• Exp5 uses only three lexical similarities
(SIMmatching + CSSmatching + Trigrams).
• Exp6 uses four features: three lexical similari-
ties (SIMmatching + CSSmatching + Trigrams)
and Syntactic tree comparison.
</listItem>
<bodyText confidence="0.999966384615385">
As we expected, the best result we have obtained
is by means of the integration of the whole of the
features available. More surprising is the good result
obtained by using lexical features only, even better
than experiments based on syntactical features only.
On the other hand, we expected that the integration
of both sort of features improve significatively the
performance of the system, but the improvement re-
spect of lexical features is poor (less than 2%). .
Similar topics share similar vocabulary, but not sim-
ilar syntax at all. Thus, we think we should to inves-
tigate semantic features better than the syntactical
ones.
</bodyText>
<sectionHeader confidence="0.995403" genericHeader="conclusions">
3 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.9999866875">
In spite of the simplicity of the approach, we have
obtained remarkable results: each set of features has
reported to provide relevant information concerning
to the entailment judgement determination. On the
other hand, these two approaches can be merged into
one single system by using different features all to-
gether and feeding with them several binary classi-
fiers that could compose a voting system. We will
do that combining TiMBL, SVM and BBR.We ex-
pect to improve the performance of the entailment
recognizer by this integration.
Finally, we want to implement a hierarchical ar-
chitecture based on constraint satisfaction networks.
The constraints will be given by the set of avail-
able features and the maintenance of the integration
across the semantic interpretation process.
</bodyText>
<sectionHeader confidence="0.999014" genericHeader="acknowledgments">
4 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996946">
This work has been partially financed by the
TIMOM project (TIN2006-15265-C06-03) granted
by the Spanish Government Ministry of Science and
Technology and the RFC/PP2006/Id 514 granted by
the University of Ja´en.
</bodyText>
<sectionHeader confidence="0.998887" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99928636">
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in wordnet: An experimental, application-
oriented evaluation of five measures.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 1998. Timbl: Tilburg memory
based learner, version 1.0, reference guide.
Oscar Ferrandez, Daniel Micolo, Rafael Mu noz, and
Manuel Palomar. 2007. T´ecnicas l´exico-sint´acticas
para reconocimiento de inmplicaci´on textual.. Tec-
nologias de la Informac´on Multiling¨ue y Multimodal.
In press.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proceedings of International Conference on
Research in Computational Linguistics, Taiwan.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the 15th International
Conference on Machine Learning.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence,
Montreal.
</reference>
<page confidence="0.999083">
82
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000194">
<title confidence="0.99945">Combining Lexical-Syntactic Information with Machine Learning Recognizing Textual Entailment</title>
<author confidence="0.938411">Arturo Montejo-R´aez</author>
<author confidence="0.938411">Jose Manuel Perea</author>
<author confidence="0.938411">Fernando ´Angel Garcia-Cumbreras</author>
<author confidence="0.938411">Maite Martin-Valdivia</author>
<author confidence="0.938411">Alfonso</author>
<affiliation confidence="0.94164">Dpto. de Inform´atica, Universidad de</affiliation>
<address confidence="0.952291">Campus de las Lagunillas s/n, 23071 -</address>
<email confidence="0.973733">jmperea,dofer,magc,maite,</email>
<abstract confidence="0.989933362445415">This document contains the description of the experiments carried out by SINAI group. We have developed an approach based on several lexical and syntactic measures integrated by means of different machine learning models. More precisely, we have evaluated three features based on lexical similarity and 11 features based on syntactic tree comparison. In spite of the relatively straightforward approach we have obtained more than 60% for accuracy. Since this is our first participation we think we have reached a good result. 1 Approach description We fill face the textual entailment recognition using Machine Learning methods, i.e. identifying features that characterize the relation between hypothesis and associated text and generating a model using existing entailment judgements that will allow us to provide a new entailment judgement agains unseen pairs text-hypothesis. This approach can be split into the two processes shown in Figures 1 and 2. a more formal way, given a text an hywant to define a function takes these two elements as arguments and returns and answer to the entailment question: entailed by (1) Now the question is to find that ideal function 78 We will approximate this function using a binary classifier: = where a binary classifier a set of the learned model for the classifier Therefore, it only remains to select a binary classifier and a feature extraction method. We have performed two experiments with different choices for both decisions. These two experiments are detailed below. Figure 1: Training processes Figure 2: Classification processes of the Workshop on Textual Entailment and pages 78–82, June 2007. Association for Computational Linguistics 1.1 Lexical similarity This experiment approaches the textual entailment task being based on the extraction of a set of lexical measures that show the existing similarity between the hypothesis-text pairs. Our approach is similar to (Ferrandez et al., 2007) but we make matching between similar words too while (Ferrandez et al., 2007) apply exact matching (see below). The first step previous to the calculation of the different measures is to preprocess the pairs using English Next we have used the architecture to obtain the stems of tokens. Once obtained stems, we have applied four different measures or techniques: Simple this technique consists of calculating the semantic distance between each stem of the hypothesis and text. If this distance exceeds a threshold, both stems are considered similar and the similarity weight value increases in one. The accumulated weight is normalized dividing it by the number of elements of the hypothesis. In this experiment we have considered the threshold 0.5. The values of semantic distance measure range from 0 to 1. In order to calculate the semantic distance between two tokens (stems), we have tried several measures based on WordNet (Alexander and Graeme Hirst, 2001). measure 1998) was shown to be best overall measures. It uses the notion of information content and the same elements as Jiang and Conrath’s approach (Jiang and Conrath, 1997) but in a different fashion: P the set that contains the elements of hypothesis and defined like: — Binary this measure is the same the previous one but modifying the simi- 1/21 i 0 Consecutive Subsequence this technique relies on forming subsequences of consecutive stems in the hypothesis and matching them in the text. The minimal size of the consecutive subsequences is two and the maximum is the maximum size of the hypothesis. Every correct matching increases in one the final weight. The sum of the obtained weights of the matching between subsequences of a certain size or length is normalized by the number of sets of consecutive subsequences of the hypothesis created for this length. These weights are accumulated and normalized by the size of the hypothesis less one. The Consecutive Subsequence Matching technique is defined in the following equations: − the set that contains the subseof the hypothesis with or length defined like: = − 1 where = the set that contains the with from text. • this technique relies on forming trigrams of words in the hypothesis and matching them in the text. A trigram is a group of = + log synsets, the information content of their lowest superordinate (most specific common subsumer) and the probability of encountering an inof a synset some specific corpus (Resnik, 1995). The Simple Matching technique is defined in the following equation: 79 three words. If a hypothesis trigram matches in text, then the similarity weight value increases in one. The accumulated weight is normalized dividing it by the number of trigrams of the hypothesis. 1.2 Syntactic tree comparison Some features have been extracted from pairs hypothesis-text related to the syntactic information that some parser can produce. The rationale behind it consists in measuring the similarity between the syntactic trees of both hypothesis and associated text. To do that, terms appearing in both trees are (we call this and then, graph distances (number of nodes) between those terms in both trees are compared, producing certain values as result. In our experiments, we have applied the COLLINS (Collins, 1999) parser to generate the syntactic tree of both pieces of text. In Figure 3 the output of the syntactic parsing for a sample pair is shown. This data is the result of the syntactical analysis performed by the mentioned parser. A graph based view of the tree corresponding to the hypothesis is drawn in Figure 4. This graph will help us to understand how certain similarity measures are obtained. Figure 3: Syntactic trees of sample hypothesis and its associated text &lt;t&gt; (TOP (S (LST (LS 0302) (. .)) (NP (JJ Next) (NN year)) (VP (VBZ is) (NP (NP (DT the) (JJ 50th) (NN anniversary)) (PP (IN of) (NP (NP (DT the) (NNP Normandy) (NN invasion) (, ,)) (NP (NP (DT an)(NN event)) (SBAR (IN that) (S (VP (MD would) (RB n’t) (VP (VB have) (VP (VBN been) (ADJP (JJ possible)) (PP (IN without) (NP (NP (DT the) (NNP Liberty) (NN ships.)) (SBAR (S (NP (DT The) (NNS volunteers)) (VP (VBP hope) (S (VP (TO to) (VP (VB raise) (NP (JJ enough) (NN money)) (S (VP (TO to) (VP (VB sail) (NP (DT the) (NNP O’Brien)) (PP (TO to) (NP (NNP France))) (PP (IN for)(NP (DT the) (JJ big) (NNP D-Day) (NN celebration) (. .)))))))))))))))))))))))))) &lt;/t&gt; &lt;h&gt; (TOP (S (NP (NP (CD 50th) (NNP Anniversary)) (PP (IN of) (NP (NNP Normandy) (NNP Landings)))) (VP (VBZ lasts) (NP (DT a) (NN year) (. .))))) &lt;/h&gt; the sample above, the terms in both pieces of text. We say that these terms are “aligned”. Therefore, for the three possible pairs of aligned terms we can compute the distance, in nodes, to go from one term to the other at each tree. Then, the difference of these Figure 4: Syntact tree of sample hypothesis distances is computed and some statistics are generated. We can summarize the process of computing this differences in the algorithm detailed in Figure 6. Figure 5: Tree comparison process For instance, in the tree represented in Figure 4 we can see that we have to perform 5 steps to go node node Since there are no more possible occurrences of these two terms, then the minimal distance between them is This value is also measured on the tree corre- 80 sponding to the text, and the absolute difference between these two minimal distances is stored in order to compute final feature weights consisting in basic statistical values. The algorithm to obtain the distribution of distance differences is detailed in Figure 6. Figure 6: Extraction of features based on syntactic distance Input: syntactic tree of the hypothesis syntactic tree of the text Output : the set of distance differences Pseudo code: terms between O distance between distance between U end-for end-for The statistics generated from the resulting list of differences the following: 1. The number of aligned terms (3 in the given example). 2. The number of matched POS values of aligned terms, that is, if the term appears with the same label in both texts (in the example Anin the POS label assigned). 3. The number of unmatched POS labels of aligned terms. 4. The average distance in nodes through the syntactic tree to go from one aligned term to another. 5. The minimal distance difference found. Table 1: Results with TiMBL and BBR classifiers the only official result reported in this paper).</abstract>
<affiliation confidence="0.927153">Experiment Classifier Accuracy</affiliation>
<address confidence="0.948698833333333">Exp1 BBR 0.6475 Exp2 BBR 0.64625 Exp3 BBR 0.63875 Exp4 TiMBL 0.6062 Exp5 TiMBL 0.6037 Exp6 TiMBL 0.57</address>
<abstract confidence="0.9065495">6. The maximal distance difference found. 7. The standard deviation of distance differences. In a similar way, differences in the depth level of nodes for aligned terms are also calculated. From the example exposed the following values were computed:</abstract>
<note confidence="0.858375692307692">Aligned 3 * MatchedPOS 2 * UnmatchedPOS 1 * AvgDistDiff 0.0392156863 * MinDistDiff 0.0000000000 * MaxDistDiff 0.0588235294 * StdevDistDiff 0.0277296777 * AvgDepthDiff 2.0000000000 * MinDepthDiff 1.0000000000 * MaxDepthDiff 3.0000000000 * StdevDepthDiff 0.8164965809 2 Experiments and results The algorithms used as binary classifiers are two:</note>
<title confidence="0.513998">Logistic Regression and TiMBL</title>
<abstract confidence="0.98202595">(Daelemans et al., 1998). Both algorithms have been with the provided by the organization of the Pascal challange. As has been explained in previous sections, a model is generated via the learning process. This model then feed into the classification variant of the algorithm, which will decide whether a new hypothesis sample is entailed by the given text or not. The experiments and results are shown in Table 1: where: Exp1 four features: three lexical similari- + + Trigrams) and Syntactic tree comparison. [available at March 27, 2007] 81 Exp2 five features: four lexical similari- + + Trigrams and Syntactic tree comparison. Exp3 only three lexical similarities + Trigrams). Exp4 the four lexical similarities + Trigrams + Exp5 only three lexical similarities + Trigrams). Exp6 four features: three lexical similari- + + Trigrams) and Syntactic tree comparison. As we expected, the best result we have obtained is by means of the integration of the whole of the features available. More surprising is the good result obtained by using lexical features only, even better than experiments based on syntactical features only. On the other hand, we expected that the integration of both sort of features improve significatively the performance of the system, but the improvement respect of lexical features is poor (less than 2%). . Similar topics share similar vocabulary, but not similar syntax at all. Thus, we think we should to investigate semantic features better than the syntactical ones. 3 Conclusions and future work In spite of the simplicity of the approach, we have obtained remarkable results: each set of features has reported to provide relevant information concerning to the entailment judgement determination. On the other hand, these two approaches can be merged into one single system by using different features all together and feeding with them several binary classifiers that could compose a voting system. We will do that combining TiMBL, SVM and BBR.We expect to improve the performance of the entailment recognizer by this integration. Finally, we want to implement a hierarchical architecture based on constraint satisfaction networks. The constraints will be given by the set of available features and the maintenance of the integration across the semantic interpretation process. 4 Acknowledgements</abstract>
<note confidence="0.721078294117647">This work has been partially financed by the TIMOM project (TIN2006-15265-C06-03) granted by the Spanish Government Ministry of Science and Technology and the RFC/PP2006/Id 514 granted by the University of Ja´en. References Alexander Budanitsky and Graeme Hirst. 2001. Semantic distance in wordnet: An experimental, applicationoriented evaluation of five measures. Collins. 1999. Statistical Models Natural Language Ph.D. thesis, University of Pennsylvania. Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 1998. Timbl: Tilburg memory based learner, version 1.0, reference guide. Oscar Ferrandez, Daniel Micolo, Rafael Mu noz, and Manuel Palomar. 2007. T´ecnicas l´exico-sint´acticas</note>
<abstract confidence="0.591886214285714">reconocimiento de inmplicaci´on textual.. Tecnologias de la Informac´on Multiling¨ue y Multimodal. In press. Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxon- In of International Conference on in Computational Taiwan. Dekang Lin. 1998. An information-theoretic definition similarity. In of the 15th International on Machine Philip Resnik. 1995. Using information content to evalusemantic similarity. In of the 14th In- Joint Conference on Artificial Montreal.</abstract>
<intro confidence="0.471497">82</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Semantic distance in wordnet: An experimental, applicationoriented evaluation of five measures.</title>
<date>2001</date>
<marker>Budanitsky, Hirst, 2001</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2001. Semantic distance in wordnet: An experimental, applicationoriented evaluation of five measures.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="6428" citStr="Collins, 1999" startWordPosition="1057" endWordPosition="1058">iding it by the number of trigrams of the hypothesis. 1.2 Syntactic tree comparison Some features have been extracted from pairs hypothesis-text related to the syntactic information that some parser can produce. The rationale behind it consists in measuring the similarity between the syntactic trees of both hypothesis and associated text. To do that, terms appearing in both trees are identified (we call this alignment) and then, graph distances (number of nodes) between those terms in both trees are compared, producing certain values as result. In our experiments, we have applied the COLLINS (Collins, 1999) parser to generate the syntactic tree of both pieces of text. In Figure 3 the output of the syntactic parsing for a sample pair is shown. This data is the result of the syntactical analysis performed by the mentioned parser. A graph based view of the tree corresponding to the hypothesis is drawn in Figure 4. This graph will help us to understand how certain similarity measures are obtained. Figure 3: Syntactic trees of sample hypothesis and its associated text &lt;t&gt; (TOP (S (LST (LS 0302) (. .)) (NP (JJ Next) (NN year)) (VP (VBZ is) (NP (NP (DT the) (JJ 50th) (NN anniversary)) (PP (IN of) (NP (</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory based learner, version 1.0, reference guide.</title>
<date>1998</date>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 1998</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 1998. Timbl: Tilburg memory based learner, version 1.0, reference guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar Ferrandez</author>
<author>Daniel Micolo</author>
<author>Rafael Mu noz</author>
<author>Manuel Palomar</author>
</authors>
<title>T´ecnicas l´exico-sint´acticas para reconocimiento de inmplicaci´on textual.. Tecnologias de la Informac´on Multiling¨ue y Multimodal.</title>
<date>2007</date>
<note>In press.</note>
<contexts>
<context position="2513" citStr="Ferrandez et al., 2007" startWordPosition="389" endWordPosition="392"> and a feature extraction method. We have performed two experiments with different choices for both decisions. These two experiments are detailed below. Figure 1: Training processes Figure 2: Classification processes Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 78–82, Prague, June 2007. c�2007 Association for Computational Linguistics 1.1 Lexical similarity This experiment approaches the textual entailment task being based on the extraction of a set of lexical measures that show the existing similarity between the hypothesis-text pairs. Our approach is similar to (Ferrandez et al., 2007) but we make matching between similar words too while (Ferrandez et al., 2007) apply exact matching (see below). The first step previous to the calculation of the different measures is to preprocess the pairs using the English stopwords list. Next we have used the GATE1 architecture to obtain the stems of tokens. Once obtained stems, we have applied four different measures or techniques: • Simple Matching: this technique consists of calculating the semantic distance between each stem of the hypothesis and text. If this distance exceeds a threshold, both stems are considered similar and the sim</context>
</contexts>
<marker>Ferrandez, Micolo, noz, Palomar, 2007</marker>
<rawString>Oscar Ferrandez, Daniel Micolo, Rafael Mu noz, and Manuel Palomar. 2007. T´ecnicas l´exico-sint´acticas para reconocimiento de inmplicaci´on textual.. Tecnologias de la Informac´on Multiling¨ue y Multimodal. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="3726" citStr="Jiang and Conrath, 1997" startWordPosition="586" endWordPosition="590">and the similarity weight value increases in one. The accumulated weight is normalized dividing it by the number of elements of the hypothesis. In this experiment we have considered the threshold 0.5. The values of semantic distance measure range from 0 to 1. In order to calculate the semantic distance between two tokens (stems), we have tried several measures based on WordNet (Alexander Budanitsky and Graeme Hirst, 2001). Lin’s similarity measure (Lin, 1998) was shown to be best overall measures. It uses the notion of information content and the same elements as Jiang and Conrath’s approach (Jiang and Conrath, 1997) but in a different fashion: P iCH similarity(i) SIMmatching = |H| 1http://gate.ac.uk/ where H is the set that contains the elements of the hypothesis and similarity(i) is defined like: n 2 1 if E1j E TsimL(i, j) &gt; 0.5 similarity(i) — 0 otherwise • Binary Matching: this measure is the same that the previous one but modifying the similarity function: 1/2 similarity(i) 1 if ]j E T i = j y() 0 otherwise • Consecutive Subsequence Matching: this technique relies on forming subsequences of consecutive stems in the hypothesis and matching them in the text. The minimal size of the consecutive subseque</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="3565" citStr="Lin, 1998" startWordPosition="561" endWordPosition="562">lating the semantic distance between each stem of the hypothesis and text. If this distance exceeds a threshold, both stems are considered similar and the similarity weight value increases in one. The accumulated weight is normalized dividing it by the number of elements of the hypothesis. In this experiment we have considered the threshold 0.5. The values of semantic distance measure range from 0 to 1. In order to calculate the semantic distance between two tokens (stems), we have tried several measures based on WordNet (Alexander Budanitsky and Graeme Hirst, 2001). Lin’s similarity measure (Lin, 1998) was shown to be best overall measures. It uses the notion of information content and the same elements as Jiang and Conrath’s approach (Jiang and Conrath, 1997) but in a different fashion: P iCH similarity(i) SIMmatching = |H| 1http://gate.ac.uk/ where H is the set that contains the elements of the hypothesis and similarity(i) is defined like: n 2 1 if E1j E TsimL(i, j) &gt; 0.5 similarity(i) — 0 otherwise • Binary Matching: this measure is the same that the previous one but modifying the similarity function: 1/2 similarity(i) 1 if ]j E T i = j y() 0 otherwise • Consecutive Subsequence Matching:</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<location>Montreal.</location>
<contexts>
<context position="5574" citStr="Resnik, 1995" startWordPosition="922" endWordPosition="923">(SHi) is defined like: f (SHi) = PjcSH, matching(j) |H |− i + 1 where 1/2 1 if ]k E STi k = j matching(i) = 0 otherwise where STi represents the set that contains the subsequences with i size from text. • Trigrams: this technique relies on forming trigrams of words in the hypothesis and matching them in the text. A trigram is a group of simL(c1, c2) = log p(c1) + log p(c2) where c1 and c2 are synsets, lso(c1,c2) is the information content of their lowest superordinate (most specific common subsumer) and p(c) is the probability of encountering an instance of a synset c in some specific corpus (Resnik, 1995). The Simple Matching technique is defined in the following equation: 2 x log p(lso(c1,c2)) 79 three words. If a hypothesis trigram matches in text, then the similarity weight value increases in one. The accumulated weight is normalized dividing it by the number of trigrams of the hypothesis. 1.2 Syntactic tree comparison Some features have been extracted from pairs hypothesis-text related to the syntactic information that some parser can produce. The rationale behind it consists in measuring the similarity between the syntactic trees of both hypothesis and associated text. To do that, terms a</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, Montreal.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>