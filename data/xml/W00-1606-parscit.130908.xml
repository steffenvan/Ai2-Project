<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.978895">
Large Scale Parsing of Czech
</title>
<author confidence="0.974624">
Pavel Smri and Ale Horalc
</author>
<affiliation confidence="0.974378">
Faculty of Informatics, Masaryk University Brno
</affiliation>
<address confidence="0.712319">
Botanicka 68a, 602 00 Brno, Czech Republic
</address>
<email confidence="0.873966">
E-mail: Ismrz,halesl©fi.muni.cz
</email>
<sectionHeader confidence="0.992291" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999728357142857">
Syntactical analysis of free word order lan-
guages poses a big challenge for natural lan-
guage parsing. In this paper, we describe our
approach to feature agreement fulfilment that
uses an automatically expanded grammar. We
display the insides of the implemented system
with its three consecutively produced phases
the core meta-grammar, a generated grammar
and an expanded grammar. We present a com-
parison of parsing with those grammar forms in
terms of the parser running time and the num-
ber of resulting edges in the chart, and show the
need of a shared bank of testing grammars for
general parser evaluation.
</bodyText>
<sectionHeader confidence="0.993759" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969285714286">
Context-Free parsing techniques are well suited
to be incorporated into real-world NLP systems
for their time efficiency and low memory re-
quirements. However, it is a well-known fact
that some natural language phenomena can-
not be handled with the context-free grammar
(CFG) formalism. Researchers therefore often
use the CFG backbone as the core of their gram-
mar formalism and supplement it with context
sensitive feature structures (e.g., Pollard and
Sag (1994), Neidle (1994)). The mechanism for
the evaluation of feature agreement is usually
based on unification. The computation can be
either interleaved into the parsing process, or it
can be postponed until the resulting structure
which captures all the ambiguities in syntax has
been built (Lavie and Rose, 2000).
In our approach, we have explored the possi-
bility of shifting the task of feature agreement
fulfilment to the earliest phase of parsing pro-
cess — the CFG backbone. This technique can
lead to a combinatorial expansion of the num-
ber of rules, however, as we are going to show
in this paper, it does not need to cause serious
slow-down of the analysis.
In a certain sense, we investigate the inter-
face between phrasal and functional constraints
as described in Maxwell III and Kaplan (1991).
They compare four different strategies in-
terleaved pruning, non-interleaved pruning, fac-
tored pruning, and factored extraction and see
the fundamental asset in the factoring tech-
nique. On the other hand, we use a special
structure for constraint evaluation. This struc-
ture stores all the possible propagated informa-
tion in one place and allows to solve the func-
tional constraints efficiently at the time of the
chart edge closing. Therefore, factoring cannot
play such key role in our system.
Maxwell III and Kaplan (1991) further dis-
cussed the possibility of translating the func-
tional constraints to the context-free (CF)
phrasal constraints and vice versa and noted
that &amp;quot;many functional constraints can in prin-
ciple be converted to phrasal constraints, al-
though converting all such functional con-
straints is a bad idea, it can be quite advan-
tageous to convert some of them, namely, those
constraints that would enable the CF parser to
prune the space of constituents&amp;quot;. To date, the
correct choice of the functional constraints se-
lected for conversion has been explored mostly
for English. However, these results cannot sim-
ply be applied in morphologically rich languages
like Czech, because of the threat of massive ex-
pansion of the number of rules. Our preliminary
results in answering this question for Czech sug-
gest that converting the functional constraints
to CF rules can be valuable for noun phrases,
even if the number of rules generated from
one original rule can be up to 56 (see below).
An open question remains, how to incorporate
the process of expansion to other agreement
</bodyText>
<page confidence="0.99958">
43
</page>
<bodyText confidence="0.963539384615385">
test checking, especially the subject-predicate
agreement and verb sub categorization. Here,
the cause of problems are the free word order
and discontinuity of constituents omnipresent in
Czech. Moreover, ellipses (deletions) interfere
with the expansion of verb subcategorization
constraints and even of the subject-predicate
agreement tests (subject can be totally elided
in Czech).
2 Description of the System
We bring into play three successive grammar
forms. Human experts work with the meta-
grammar form, which encompasses high-level
generative constructs that reflect the meta-level
natural language phenomena like the word or-
der constraints, and enable to describe the lan-
guage with a maintainable number of rules. The
meta-grammar serves as a base for the second
grammar form which comes into existence by
expanding the constructs. This grammar con-
sists of context-free rules equipped with feature
agreement tests and other contextual actions.
The last phase of grammar induction lies in
the transformation of the tests into standard
rules of the expanded grammar with the actions
remaining to guarantee the contextual require-
ments.
Meta-grammar (G1) The meta-grammar
consists of global order constraints that safe-
guard the succession of given terminals, special
flags that impose particular restrictions to given
non-terminals and terminals on the right hand
side and of constructs used to generate combi-
nations of rule elements. The notation of the
flags can be illustrated by the following exam-
ples:
ss -&gt; conj clause
/* budu muset cist -
I will have to read */
</bodyText>
<equation confidence="0.678847">
futmod --&gt; VBU VOI VI
/* byl bych byval -
I would have had */
cpredcondgr ==&gt; VBL VBK VBLL
/* musim se ptat -
I must ask */
clause ===&gt; VO R VRI
</equation>
<bodyText confidence="0.999898736842105">
The thin short arrow (-&gt;) denotes an ordi-
nary CFG transcription. To allow discontinu-
ous constituents, as is needed in Czech syntactic
analysis, the long arrow (--&gt;) supplements the
right hand side with possible intersegments be-
tween each couple of listed elements. The thick
long arrow (==&gt;) adds (in addition to filling in
the intersegments) the checking of correct encli-
tics order. This flag is more useful in connection
with the order or rhs constructs discussed be-
low. The thick extra-long arrow (===&gt;) provides
the completion of the right hand side to form a
full clause. It allows the addition of interseg-
ments in the beginning and at the end of the
rule, and it also tries to supply the clause with
conjunctions, etc.
The global order constraints represent univer-
sal simple regulators, which are used to inhibit
some combinations of terminals in rules.
</bodyText>
<equation confidence="0.6408645">
/* jsem, bych, se -
am, would, self */
%enclitic = (VB12, VBK, R)
/* byl, cetl, ptal, musel -
</equation>
<bodyText confidence="0.8060246">
was, read, asked, had to */
%order VBL = {VL, VRL, VOL}
/* byval, cetl, ptal, musel -
had been, read, asked, had to */
%order VBLL = {VL, VRL, VOL}
In this example, the %enclitic specifies
which terminals should be regarded as enclitics
and determines their order in the sentence. The
%order constraints guarantee that the terminals
VBL and VBLL always go before any of the ter-
minals VL, VRL and VOL.
The main combining constructs in the meta-
grammar are order(), rhs() and first(),
which are used for generating variants of assort-
ments of given terminals and non-terminals.
</bodyText>
<equation confidence="0.855848714285714">
/* budu se ptat -
I will ask */
clause ===&gt; order(VBU,R,VRI)
/* ktery ... -
which ... */
relclause ===&gt; first(relprongr) \
rhs (clause)
</equation>
<bodyText confidence="0.9995616">
The order() construct generates all possible
permutations of its components. The first()
and rhs 0 constructs are employed to implant
content of all the right hand sides of specified
non-terminal to the rule. The rhs (N) construct
</bodyText>
<page confidence="0.995057">
44
</page>
<bodyText confidence="0.998814636363636">
generates the possible rewritings of the non-
terminal N. The resulting terms are then subject
to standard constraints and intersegment inser-
tion. In some cases, one needs to force a certain
constituent to be the first non-terminal on the
right hand side. The construct f irst (N) en-
sures that N is firmly tied to the beginning and
can neither be preceded by an intersegment nor
any other construct. In the above example, the
relclause is transformed to CF rules starting
with relprongr followed by the right hand sides
of the non-terminal clause with possible inter-
segments filled in.
In the current version, we have added two
generative constructs and the possibility to de-
fine rule templates to simplify the creation and
maintenance of the grammar. The first con-
struct is formed by a set of Volist_* expres-
sions, which automatically produce new rules
for a list of the given non-terminals either sim-
ply concatenated or separated by comma and
co-ordinative conjunctions:
</bodyText>
<figure confidence="0.9053309375">
/* (nesmim) zapomenout udelat -
to forget to do */
%list_nocoord
vi_list -&gt; VI
%list_nocoord_case_number_gender modif
/* velky cerveny -
big red */
modif -&gt; adjp
/* krute a drsne -
cruelly and roughly */
%list_coord adv_list
adv_list -&gt; ADV
%list_coord_case_number_gender np
/* krasny pes -
beautiful dog */
np -&gt; left_modif np
</figure>
<bodyText confidence="0.999625636363636">
The endings *_case, *_number_gender and
*_case _number _gender denote the kinds of
agreements between list constituents. The in-
corporation of this construct has decreased the
number of rules by approximately 15%.
A significant portion of the grammar is made
up by the verb group rules. Therefore we have
been seeking for an instrument that would catch
frequent repetitive constructions in verb groups.
The obtained addition is the %group keyword
illustrated by the following example:
</bodyText>
<equation confidence="0.967573">
%group verb={
V:head($1,intr)
add_verb($1),
VR R:head($1,intr)
add_verb($1)
set_R($2)
/* ctu - I am reading */
/* ptam se - I am asking */
clause ====&gt; order(group(verb),vi_list)
</equation>
<bodyText confidence="0.998119818181818">
Here, the group verb denotes two sets of
non-terminals with the corresponding actions
that are then substituted for the expression
group(verb) on the right hand side of the
clause non-terminal.
Apart from the common generative con-
structs, the metagrammar comprises feature
tagging actions that specify certain local aspects
of the denoted (non-)terminal. One of these ac-
tions is the specification of the head-dependent
relations in the rule the head() construct:
</bodyText>
<equation confidence="0.6104515">
/* prvni clanek - first article */
np -&gt; left_modif np
head($2,$1)
/* treba - perhaps */
part -&gt; PART
head(root,$1)
</equation>
<bodyText confidence="0.999948125">
In the first rule, head($2,$1) says that (the
head of) left_modif depends on (the head of)
np on the right hand side. In the second ex-
ample, head(root,$1) links the PART terminal
to the root of the resulting dependency tree.
More sophisticated constructs of this kind are
the set_local_root () and head_of 0, whose
usage is demonstrated in the following example:
</bodyText>
<equation confidence="0.9252944">
/* ktery ... -
which ... */
relclause ===&gt; first(relprongr) \
rhs(clause)
set _local_root (head_of ($2) )
</equation>
<bodyText confidence="0.9950435">
Here, the heads in rhs(clause) are as-
signed as specified in the derivation rules for
</bodyText>
<page confidence="0.996674">
45
</page>
<bodyText confidence="0.980071">
clause. This way we obtain one head of the
rhs (clause) part and can link all yet unlinked
terms to this head.
The Second Grammar Form (G2) As
we have mentioned earlier, several pre-defined
grammatical tests and procedures are used in
the description of context actions associated
with each grammatical rule of the system. We
use the following tests:
</bodyText>
<listItem confidence="0.9575305">
• grammatical case test for particular words
and noun groups
</listItem>
<equation confidence="0.842296">
noun-gen-group -&gt; noun-group \
noun-group
test_genitive($2)
propagate_all($1)
</equation>
<listItem confidence="0.812947">
• agreement test of case in prepositional con-
struction
</listItem>
<equation confidence="0.820474666666667">
prep-group -&gt; PREP \
noun-group
agree_case_and_propagate ($1 ,$2)
</equation>
<listItem confidence="0.8497065">
• agreement test of number and gender for
relative pronouns
</listItem>
<equation confidence="0.67361075">
ng-with-rel-pron -&gt; noun-group \
&apos;,&apos; rel-pron-group
agree_number_gender\
_and_propagate ($1 ,$3)
</equation>
<listItem confidence="0.918319125">
• agreement test of case, number and gender
for noun groups
adj -ng -&gt; adj -group noun-group
agree _ case _number_gender \
_and_propagate ($1 ,$2)
• test of agreement between subject and
predicate
• test of the verb valencies
</listItem>
<bodyText confidence="0.98684075">
clause -&gt; subj -part verb-part
agree_subj _pred ($1 ,$2)
test_valency_of ($2)
The contextual actions propagate_all and
agr e e _*_and_pr op agat e propagate all rele-
vant grammatical information from the non-
terminals on the right hand side to the one on
the left hand side of the rule.
Expanded Grammar Form (G3) The fea-
ture agreement tests can be transformed into
the context-free rules. For instance in Czech,
similar to other Slavic languages, we have 7
grammatical cases (nominative, genitive, da-
tive, accusative, vocative, locative and instru-
mental), two numbers (singular and plural) and
three genders (masculine, feminine and neuter),
in which masculine exists in two forms ani-
mate and inanimate. Thus, e.g., we get 56 pos-
sible variants for a full agreement between two
constituents.
</bodyText>
<subsectionHeader confidence="0.93672">
2.1 Parser
</subsectionHeader>
<bodyText confidence="0.999970315789474">
In our work, we have successively tried several
different techniques for syntactic analysis. We
have tested the top-down and bottom-up vari-
ants of the standard chart parser. For more
efficient natural language analysis, several re-
searchers have suggested the concept of head-
driven parsing (e.g., Kay (1989), van Noord
(1997)). Taking advantage of the fact that the
head-dependent relations are specified in ev-
ery rule of our grammar to enable the depen-
dency graph output, the head-driven approach
has been successfully adopted in our system.
Currently, we are testing the possibility of in-
corporating the Tomita&apos;s GLR parser (Tomita,
1986; Heemels et al., 1991) for the sake of com-
paring the efficiency of the parsers and the feasi-
bility of implanting a probabilistic control over
the parsing process to the parser.
Since the number of rules that we need to
work with is fairly big (tens of thousands), we
need efficient structures to store the parsing
process state. The standard chart parser im-
plementation used in our experiments employs
4 hash structures one for open edges, one
for closed edges, one hash table for the gram-
mar rules (needed in the prediction phase) and
one for all edges in the agenda or in the chart
(the hash key is made of all the attributes of an
edge the rule, the dot position and the sur-
face range). In the case of a head-driven chart
parser, we need two hashes for open edges and
also two hashes for closed edges.
The gain of this rather complex structure is
the linear dependency of the analysis speed on
the number of edges in the resulting chart. Each
edge is taken into consideration twice — when
it is inserted into the agenda and when it is
inserted into the chart. The overall complexity
</bodyText>
<page confidence="0.998535">
46
</page>
<bodyText confidence="0.995842166666667">
is therefore 2k, where k is the number of edges
in the resulting chart.
The number of chart edges that are involved
in the appropriate output derivation structure
is related to:
a) the number of words in the input sentence,
and
b) the ambiguity rate of the sentence.
The output of the chart parser is presented
in the form of a packed shared forest, which is
also a standard product of the generalized LR
parser. Thus, it enables the parser to run the
postprocessing actions on a uniform platform
for the different parsers involved.
During the process of design and implemen-
tation of our system, we started to distinguish
four kinds of contextual actions, tests or con-
straints:
</bodyText>
<listItem confidence="0.9978405">
1. rule-tied actions
2. agreement fulfilment constraints
3. post-processing actions
4. actions based on derivation tree
</listItem>
<bodyText confidence="0.9996977">
Rule-tied actions are quite rare and serve only
as special counters for rule-based probability es-
timation or as rule parameterization modifiers.
Agreement fulfilment constraints are used in
generating the G3 expanded grammar, in G2
they serve as chart pruning actions. In terms
of (Maxwell III and Kaplan, 1991), the agree-
ment fulfilment constraints represent the func-
tional constraints, whose processing can be in-
terleaved with that of phrasal constraints. The
post-processing actions are not triggered until
the chart is already completed. They are used,
for instance, in the packed dependency graph
generation. On the other hand, there are some
actions that do not need to work with the whole
chart structure, they are run after the best or
n most probable derivation trees are selected.
These actions do not prune anything, they may
be used, for example, for outputting the verb
valencies from the input sentence.
</bodyText>
<sectionHeader confidence="0.999839" genericHeader="introduction">
3 Results
</sectionHeader>
<bodyText confidence="0.997729666666667">
In our system, we work with a grammar of the
Czech language (Smr2. and Horak, 1999), which
is being developed in parallel with the parsing
mechanism. The grammar in the three forms,
as exemplified above, has the following numbers
of rules:
</bodyText>
<table confidence="0.5081048">
G1 meta-grammar — # rules 326
G2 generated grammar — # rules 2919
shift/reduce conflicts 48833
reduce/reduce conflicts 5067
G3 expanded grammar — # rules 10207
</table>
<bodyText confidence="0.999905487179487">
As a measure of the ambiguity rate of G2,
we display the number of shift/reduce and re-
duce/reduce conflicts as counted with a stan-
dard LR parser generator. These data, together
with the number of rules in the grammar, pro-
vide basic characteristics of the complexity of
analysis.
The comparison of parsing times when using
the grammars G3 and G2 without the actions
taken into account is summarized in Table 1.
We present the time taken for parsing a se-
lected subset of testing sentences only sen-
tences with more than 50 words were chosen.
The results show that in some cases, which
are not so rare in highly inflectional languages,
the expanded grammar achieves even less num-
ber of edges in the chart than the original gram-
mar. This effect significantly depends on the
ambiguity rate of the input text. A question re-
mains, how to exactly characterize the relation
between ambiguity in the grammar and in the
input.
The fully expanded grammar G3 is only mod-
erately larger than the G2 grammar (about
three times the size). The reason lies in the
fact that the full expansion takes place mainly
in the part of the grammar that describes noun
phrases. This part forms only a small amount
of the total number of G2 rules. Considering
this, it is not surprising that the parse times are
not much worse or even better. It also benefits
from early pruning by transforming the unifica-
tion constraints into the CFG. The agreement
tests between subject and predicate should also
be expanded. Nevertheless, we do not do this,
since the position of subject is free, it cannot
be described with CF rules without imposing a
huge amount of ambiguity to every input sen-
tence.
</bodyText>
<sectionHeader confidence="0.845767" genericHeader="method">
4 Packed Dependency Graph
</sectionHeader>
<bodyText confidence="0.949771">
Ambiguity is a fundamental property of natural
languages. Perhaps the most oppressive case of
</bodyText>
<page confidence="0.998654">
47
</page>
<table confidence="0.998109333333333">
Sent # of G2 G3 # edges
# words G3/G2
# edges time # edges time
1100 52 115093 1.95 143044 2.53 124%
1102 51 84960 1.56 107318 2.02 126%
1654 51 202678 3.23 361072 6.81 178%
1672 59 269458 4.08 434430 7.13 161%
1782 51 212695 3.36 168118 3.05 79%
2079 66 98363 1.83 223063 3.75 227%
2300 60 262157 4.03 443022 7.28 169%
2306 102 739351 12.94 715835 10.95 97%
2336 103 355749 5.21 565506 8.83 159%
</table>
<tableCaption confidence="0.999926">
Table 1: Running times for G2 and G3
</tableCaption>
<bodyText confidence="0.974294721311476">
ambiguity manifests itself on the syntactic level
of analysis. In order to face up to the high num-
ber of obtained derivation trees, we define a sort
order on the output trees that is specified by
probabilities computed from appropriate edges
in the chart structure. The statistics is also in-
volved in the process of sorting out the edges
from the agenda in the order that leads directly
to n most probable analyses.
A common approach to acquiring the statis-
tical data for the analysis of syntax employs
learning the values from a fully tagged tree-
bank training corpus. Building such corpora is
a tedious and expensive task and it requires a
team cooperation of linguists and computer sci-
entists. At present, the only source of Czech
tree-bank data is the Prague Dependency Tree-
Bank (PDTB) (Hajk, 1998), which contains de-
pendency analyses of about 20000 Czech sen-
tences.
The linguistic tradition of Czech syntactic
analysis is constituted by distinguishing the role
of head and dependent and describes the re-
lations between a head and its dependents in
terms of semantically motivated dependency re-
lations. In order to be able to exploit the data
from PDTB, we have supplemented our gram-
mar with the dependency specification for con-
stituents. Thus, the output of the analysis can
be presented in the form of a pure dependency
tree. At the same time, we unify classes of
derivation trees that correspond to one depen-
dency structure. We then define a canonical
form of the derivation to select one representa-
tive of the class which is used for assigning the
edge probabilities.
The dependency structures for all possible
analyses are stored in the form of a packed de-
pendency graph. Every &amp;quot;non-simple&amp;quot; rule (that
has more than one term on the right hand side)
is extended by a denotation of the head element
and its dependents. Thus, the dependency is of-
ten given as a relation between non-terminals,
which cover several input words. However, the
basic units of the dependency representation are
particular surface elements (words). To be able
to capture the standard dependency relations,
we propagate the information about a &amp;quot;local
head&amp;quot; from the surface level through all the pro-
cessed chart edges up to the top. A simplified
case that captures only one possible derivation
of sentence &apos;Mame k veefi kufe.&apos; (We have
a chicken for dinner.) can be described by the
following tree:
clause
head: 1
intr V intr
head: 0 head: 1 head: 2,4
. I 1 inter inter
inter Mame head: 2 head: 4
head: 0 1 1
</bodyText>
<table confidence="0.794117571428571">
1 pp up
c head: 2 head: 4
1
PREP N N
head: 2 head: 3 head: 4
1 1 1
k veeefi kufe
</table>
<page confidence="0.995033">
48
</page>
<figure confidence="0.996972962962963">
9999−ROOT
0−Vsak
1−Martin
2−s
3−konem
4−bydlel
5−u
6−stryce
7−se
9−prasetem
8−spinavym
9−prasetem
8−spinavym
9999−ROOT
4−bydlel
0−Vsak
1−Martin
2−s
7−se
5−u
3−konem 6−stryce 9−prasetem
8−spinavym
9999−ROOT
0−Vsak 4−bydlel
1−Martin 5−u
2−s 6−stryce
3−konem 7−se
</figure>
<bodyText confidence="0.9030091">
During the evaluation of post-processing ac-
tions, every head-dependent relation is then
recorded as an edge in the graph (without allow-
ing multi-edges). An example of the graph for
the sentence &apos;Vgak Martin s konem bydlel u
stryce se gpinayym prasetem.&apos; (literally:
However, Martin with a horse lived with his un-
cle with a dirty pig.) is depicted in Figure 1.
Two examples of unpacked derivation trees that
are generated from the graph are illustrated in
</bodyText>
<figureCaption confidence="0.707630111111111">
Figure 2.
The packed dependency graph enables us to
recover all the possible standard dependency
trees with some additional information gathered
during the analysis. The example graph repre-
sents two dependency trees only, however, in
the case of more complex sentences, especially
those with complicated noun phrases, the sav-
ing is much higher.
</figureCaption>
<sectionHeader confidence="0.998111" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999936411764706">
In this paper, we have shown that shifting all
possible feature agreement computations to the
CFG backbone is suitable for free word order
languages and it does not need to cause a seri-
ous increase in parsing time. We discuss three
consecutively produced forms of our grammar
and give a comparison of different parser run-
ning times on highly ambiguous input.
In the process of parsers evaluation, we lacked
the possibility to compare the parsing efficiency
on a large number of testing grammars. These
grammars cannot be automatically generated,
since they should reflect the situation in real-
world parsing systems. Future cooperation in
NL parsing could therefore lead to the creation
of a commonly shared bank of testing grammars
with precisely specified ambiguity measures.
</bodyText>
<sectionHeader confidence="0.998355" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999690090909091">
J. Hajie. 1998. Building a syntactically anno-
tated corpus: The Prague Dependency Tree-
bank. In Issues of Valency and Meaning,
pages 106-132, Prague. Karolinum.
R. Heemels, A. Nijholt, and K. Sikkel, edi-
tors. 1991. Tomitas Algorithm: Extensions
and Applications : Proceedings of the First
Twente Workshop on Language Technology,
Enschede. Universiteit Twente.
M. Kay. 1989. Head driven parsing. In Pro-
ceedings of Workshop on Parsing Technolo-
gies, Pittsburg.
A. Lavie and P. Rose, C. 2000. Optimal am-
biguity packing in context-free parsers with
interleaved unification. In Proceedings of
IWPT&apos;2000, Trento, Italy.
J. T. Maxwell III and R. M. Kaplan. 1991.
The interface between phrasal and functional
constraints. In M. Rosner, C. J. Rupp, and
R. Johnson, editors, Proceedings of the Work-
shop on Constraint Propagation, Linguistic
Description, and Computation, pages 105-
120. Instituto Dalle Molle IDSIA, Lugano.
Also in Computational Linguistics, Vol. 19,
No. 4, 571-590, 1994.
C. Neidle. 1994. Lexical-Functional Grammar
(LFG). In R. E. Asher, editor, Encyclopedia
of Language and Linguistics, volume 3, pages
2147-2153. Pergamon Press, Oxford.
G. Pollard and I. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of
Chicago Press, Chicago.
P. Smr2 and A. Horak. 1999. Implementation
of efficient and portable parser for Czech.
In Proceedings of TSD &apos;99, pages 105-108,
Berlin. Springer-Verlag. Lecture Notes in Ar-
tificial Intelligence 1692.
M. Tomita. 1986. Efficient Parsing for Natu-
ral Languages: A Fast Algorithm for Prac-
tical Systems. Kluwer Academic Publishers,
Boston, MA.
G. van Noord. 1997. An efficient implemen-
tation of the head-corner parser. Computa-
tional Linguistics, 23(3).
</reference>
<page confidence="0.997684">
50
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.820969">
<title confidence="0.987888">Large Scale Parsing of Czech</title>
<author confidence="0.870216">Smri</author>
<affiliation confidence="0.999781">Faculty of Informatics, Masaryk University</affiliation>
<address confidence="0.998395">Botanicka 68a, 602 00 Brno, Czech</address>
<email confidence="0.976799">E-mail:Ismrz,halesl©fi.muni.cz</email>
<abstract confidence="0.997883533333333">Syntactical analysis of free word order languages poses a big challenge for natural language parsing. In this paper, we describe our approach to feature agreement fulfilment that uses an automatically expanded grammar. We display the insides of the implemented system with its three consecutively produced phases the core meta-grammar, a generated grammar and an expanded grammar. We present a comparison of parsing with those grammar forms in terms of the parser running time and the number of resulting edges in the chart, and show the need of a shared bank of testing grammars for general parser evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Hajie</author>
</authors>
<title>Building a syntactically annotated corpus: The Prague Dependency Treebank.</title>
<date>1998</date>
<booktitle>In Issues of Valency and Meaning,</booktitle>
<pages>106--132</pages>
<location>Prague. Karolinum.</location>
<marker>Hajie, 1998</marker>
<rawString>J. Hajie. 1998. Building a syntactically annotated corpus: The Prague Dependency Treebank. In Issues of Valency and Meaning, pages 106-132, Prague. Karolinum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Heemels</author>
<author>A Nijholt</author>
<author>K Sikkel</author>
<author>editors</author>
</authors>
<date>1991</date>
<booktitle>Tomitas Algorithm: Extensions and Applications : Proceedings of the First Twente Workshop on Language Technology, Enschede. Universiteit Twente.</booktitle>
<contexts>
<context position="12838" citStr="Heemels et al., 1991" startWordPosition="2077" endWordPosition="2080">several different techniques for syntactic analysis. We have tested the top-down and bottom-up variants of the standard chart parser. For more efficient natural language analysis, several researchers have suggested the concept of headdriven parsing (e.g., Kay (1989), van Noord (1997)). Taking advantage of the fact that the head-dependent relations are specified in every rule of our grammar to enable the dependency graph output, the head-driven approach has been successfully adopted in our system. Currently, we are testing the possibility of incorporating the Tomita&apos;s GLR parser (Tomita, 1986; Heemels et al., 1991) for the sake of comparing the efficiency of the parsers and the feasibility of implanting a probabilistic control over the parsing process to the parser. Since the number of rules that we need to work with is fairly big (tens of thousands), we need efficient structures to store the parsing process state. The standard chart parser implementation used in our experiments employs 4 hash structures one for open edges, one for closed edges, one hash table for the grammar rules (needed in the prediction phase) and one for all edges in the agenda or in the chart (the hash key is made of all the attri</context>
</contexts>
<marker>Heemels, Nijholt, Sikkel, editors, 1991</marker>
<rawString>R. Heemels, A. Nijholt, and K. Sikkel, editors. 1991. Tomitas Algorithm: Extensions and Applications : Proceedings of the First Twente Workshop on Language Technology, Enschede. Universiteit Twente.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Head driven parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of Workshop on Parsing Technologies,</booktitle>
<location>Pittsburg.</location>
<contexts>
<context position="12483" citStr="Kay (1989)" startWordPosition="2022" endWordPosition="2023">dative, accusative, vocative, locative and instrumental), two numbers (singular and plural) and three genders (masculine, feminine and neuter), in which masculine exists in two forms animate and inanimate. Thus, e.g., we get 56 possible variants for a full agreement between two constituents. 2.1 Parser In our work, we have successively tried several different techniques for syntactic analysis. We have tested the top-down and bottom-up variants of the standard chart parser. For more efficient natural language analysis, several researchers have suggested the concept of headdriven parsing (e.g., Kay (1989), van Noord (1997)). Taking advantage of the fact that the head-dependent relations are specified in every rule of our grammar to enable the dependency graph output, the head-driven approach has been successfully adopted in our system. Currently, we are testing the possibility of incorporating the Tomita&apos;s GLR parser (Tomita, 1986; Heemels et al., 1991) for the sake of comparing the efficiency of the parsers and the feasibility of implanting a probabilistic control over the parsing process to the parser. Since the number of rules that we need to work with is fairly big (tens of thousands), we </context>
</contexts>
<marker>Kay, 1989</marker>
<rawString>M. Kay. 1989. Head driven parsing. In Proceedings of Workshop on Parsing Technologies, Pittsburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>P Rose</author>
<author>C</author>
</authors>
<title>Optimal ambiguity packing in context-free parsers with interleaved unification.</title>
<date>2000</date>
<booktitle>In Proceedings of IWPT&apos;2000,</booktitle>
<location>Trento, Italy.</location>
<marker>Lavie, Rose, C, 2000</marker>
<rawString>A. Lavie and P. Rose, C. 2000. Optimal ambiguity packing in context-free parsers with interleaved unification. In Proceedings of IWPT&apos;2000, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Maxwell</author>
<author>R M Kaplan</author>
</authors>
<title>The interface between phrasal and functional constraints.</title>
<date>1991</date>
<booktitle>Proceedings of the Workshop on Constraint Propagation, Linguistic Description, and Computation,</booktitle>
<volume>19</volume>
<pages>105--120</pages>
<editor>In M. Rosner, C. J. Rupp, and R. Johnson, editors,</editor>
<marker>Maxwell, Kaplan, 1991</marker>
<rawString>J. T. Maxwell III and R. M. Kaplan. 1991. The interface between phrasal and functional constraints. In M. Rosner, C. J. Rupp, and R. Johnson, editors, Proceedings of the Workshop on Constraint Propagation, Linguistic Description, and Computation, pages 105-120. Instituto Dalle Molle IDSIA, Lugano. Also in Computational Linguistics, Vol. 19, No. 4, 571-590, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Neidle</author>
</authors>
<title>Lexical-Functional Grammar (LFG).</title>
<date>1994</date>
<booktitle>Encyclopedia of Language and Linguistics,</booktitle>
<volume>3</volume>
<pages>2147--2153</pages>
<editor>In R. E. Asher, editor,</editor>
<publisher>Pergamon Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="1290" citStr="Neidle (1994)" startWordPosition="200" endWordPosition="201">r of resulting edges in the chart, and show the need of a shared bank of testing grammars for general parser evaluation. 1 Introduction Context-Free parsing techniques are well suited to be incorporated into real-world NLP systems for their time efficiency and low memory requirements. However, it is a well-known fact that some natural language phenomena cannot be handled with the context-free grammar (CFG) formalism. Researchers therefore often use the CFG backbone as the core of their grammar formalism and supplement it with context sensitive feature structures (e.g., Pollard and Sag (1994), Neidle (1994)). The mechanism for the evaluation of feature agreement is usually based on unification. The computation can be either interleaved into the parsing process, or it can be postponed until the resulting structure which captures all the ambiguities in syntax has been built (Lavie and Rose, 2000). In our approach, we have explored the possibility of shifting the task of feature agreement fulfilment to the earliest phase of parsing process — the CFG backbone. This technique can lead to a combinatorial expansion of the number of rules, however, as we are going to show in this paper, it does not need</context>
</contexts>
<marker>Neidle, 1994</marker>
<rawString>C. Neidle. 1994. Lexical-Functional Grammar (LFG). In R. E. Asher, editor, Encyclopedia of Language and Linguistics, volume 3, pages 2147-2153. Pergamon Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Pollard</author>
<author>I Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="1275" citStr="Pollard and Sag (1994)" startWordPosition="196" endWordPosition="199">nning time and the number of resulting edges in the chart, and show the need of a shared bank of testing grammars for general parser evaluation. 1 Introduction Context-Free parsing techniques are well suited to be incorporated into real-world NLP systems for their time efficiency and low memory requirements. However, it is a well-known fact that some natural language phenomena cannot be handled with the context-free grammar (CFG) formalism. Researchers therefore often use the CFG backbone as the core of their grammar formalism and supplement it with context sensitive feature structures (e.g., Pollard and Sag (1994), Neidle (1994)). The mechanism for the evaluation of feature agreement is usually based on unification. The computation can be either interleaved into the parsing process, or it can be postponed until the resulting structure which captures all the ambiguities in syntax has been built (Lavie and Rose, 2000). In our approach, we have explored the possibility of shifting the task of feature agreement fulfilment to the earliest phase of parsing process — the CFG backbone. This technique can lead to a combinatorial expansion of the number of rules, however, as we are going to show in this paper, i</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>G. Pollard and I. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Smr2</author>
<author>A Horak</author>
</authors>
<title>Implementation of efficient and portable parser for Czech.</title>
<date>1999</date>
<journal>Lecture Notes in Artificial Intelligence</journal>
<booktitle>In Proceedings of TSD &apos;99,</booktitle>
<pages>105--108</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin.</location>
<marker>Smr2, Horak, 1999</marker>
<rawString>P. Smr2 and A. Horak. 1999. Implementation of efficient and portable parser for Czech. In Proceedings of TSD &apos;99, pages 105-108, Berlin. Springer-Verlag. Lecture Notes in Artificial Intelligence 1692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Efficient Parsing for Natural Languages: A Fast Algorithm for Practical Systems.</title>
<date>1986</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="12815" citStr="Tomita, 1986" startWordPosition="2075" endWordPosition="2076">ssively tried several different techniques for syntactic analysis. We have tested the top-down and bottom-up variants of the standard chart parser. For more efficient natural language analysis, several researchers have suggested the concept of headdriven parsing (e.g., Kay (1989), van Noord (1997)). Taking advantage of the fact that the head-dependent relations are specified in every rule of our grammar to enable the dependency graph output, the head-driven approach has been successfully adopted in our system. Currently, we are testing the possibility of incorporating the Tomita&apos;s GLR parser (Tomita, 1986; Heemels et al., 1991) for the sake of comparing the efficiency of the parsers and the feasibility of implanting a probabilistic control over the parsing process to the parser. Since the number of rules that we need to work with is fairly big (tens of thousands), we need efficient structures to store the parsing process state. The standard chart parser implementation used in our experiments employs 4 hash structures one for open edges, one for closed edges, one hash table for the grammar rules (needed in the prediction phase) and one for all edges in the agenda or in the chart (the hash key i</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>M. Tomita. 1986. Efficient Parsing for Natural Languages: A Fast Algorithm for Practical Systems. Kluwer Academic Publishers, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
</authors>
<title>An efficient implementation of the head-corner parser.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>van Noord, 1997</marker>
<rawString>G. van Noord. 1997. An efficient implementation of the head-corner parser. Computational Linguistics, 23(3).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>