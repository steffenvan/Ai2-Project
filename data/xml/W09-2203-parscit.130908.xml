<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.81921">
Keepin’ It Real: Semi-Supervised Learning with Realistic Tuning
</title>
<author confidence="0.955096">
Andrew B. Goldberg
</author>
<affiliation confidence="0.825800333333333">
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706, USA
</affiliation>
<email confidence="0.997794">
goldberg@cs.wisc.edu
</email>
<author confidence="0.989426">
Xiaojin Zhu
</author>
<affiliation confidence="0.836737">
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706, USA
</affiliation>
<email confidence="0.998898">
jerryzhu@cs.wisc.edu
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979043478261">
We address two critical issues involved in ap-
plying semi-supervised learning (SSL) to a
real-world task: parameter tuning and choos-
ing which (if any) SSL algorithm is best suited
for the task at hand. To gain a better un-
derstanding of these issues, we carry out a
medium-scale empirical study comparing su-
pervised learning (SL) to two popular SSL al-
gorithms on eight natural language processing
tasks under three performance metrics. We
simulate how a practitioner would go about
tackling a new problem, including parameter
tuning using cross validation (CV). We show
that, under such realistic conditions, each of
the SSL algorithms can be worse than SL on
some datasets. However, we also show that
CV can select SL/SSL to achieve “agnostic
SSL,” whose performance is almost always no
worse than SL. While CV is often dismissed as
unreliable for SSL due to the small amount of
labeled data, we show that it is in fact effective
for accuracy even when the labeled dataset
size is as small as 10.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998755">
Imagine you are a real-world practitioner working
on a machine learning problem in natural language
processing. If you have unlabeled data, should you
use semi-supervised learning (SSL)? Which SSL al-
gorithm should you use? How should you set its pa-
rameters? Or could it actually hurt performance, in
which case you might be better off with supervised
learning (SL)?
A large number of SSL algorithms have been de-
veloped in recent years that allow one to improve
</bodyText>
<page confidence="0.98801">
19
</page>
<bodyText confidence="0.9995634">
performance with unlabeled data, in tasks such
as text classification, sequence labeling, and pars-
ing (Zhu, 2005; Chapelle et al., 2006; Brefeld and
Scheffer, 2006). However, many of them are tested
on “SSL-friendly” datasets, such as “two moons,”
USPS, and MNIST. Furthermore, the algorithms’
parameters are often chosen based on test set per-
formance or manually set based on heuristics and
researcher experience. These issues create practical
concerns for deploying SSL in the real world.
We note that (Chapelle et al., 2006)’s benchmark
chapter explores these issues to some extent by com-
paring several SSL methods on several real and ar-
tificial datasets. The authors reach the conclusions
that parameter tuning is difficult with little labeled
data and that no method is universally superior. We
reexamine these issues in the context of NLP tasks
and offer a simple attempt at overcoming these road-
blocks to practical application of SSL.
The contributions of this paper include:
</bodyText>
<listItem confidence="0.960786846153846">
• We present a medium-scale empirical study
comparing SL to two popular SSL algorithms
on eight less-familiar tasks using three per-
formance metrics. Importantly, we tune pa-
rameters realistically based on cross validation
(CV), as a practitioner would do in reality.
• We show that, under such realistic conditions,
each of the SSL algorithms can be worse than
SL on some datasets.
• However, this can be prevented. We show that
CV can be used to select SL/SSL to achieve
agnostic SSL, whose performance is almost al-
ways no worse than SL. Traditionally, CV is
</listItem>
<note confidence="0.980103">
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 19–27,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.918835875">
often dismissed as unreliable for SSL because
of the small labeled dataset size. But we show
that CV is effective when using accuracy as an
optimization criterion, even when the labeled
dataset size is as small as 10.
• We show the power of cloud computing: we
were able to complete roughly 3 months worth
of experiments in less than a week.
</bodyText>
<sectionHeader confidence="0.945802" genericHeader="introduction">
2 SSL with Realistic Tuning
</sectionHeader>
<bodyText confidence="0.9998542">
Given a particular labeled and unlabeled dataset,
how should you set parameters for a particular SSL
model? The most realistic approach for a practi-
tioner is to use CV to tune parameters on a grid. We
therefore argue that the model parameters obtained
in this way truly determine how SSL will perform
in practice. Algorithm 1 describes a particular in-
stance1 of CV in detail. We call it “RealSSL,” as
this is all a real user can hope to do when applying
SSL (and SL too) in a realistic problem scenario.
</bodyText>
<sectionHeader confidence="0.998058" genericHeader="method">
3 Experimental Procedure
</sectionHeader>
<bodyText confidence="0.999788571428571">
Given the RealSSL procedure in Algorithm 1, we
designed an experimental setup to simulate differ-
ent settings that a real-world practitioner might face
when given a new task and a set of algorithms to
choose from (some of which use unlabeled data).
This will allow us to compare algorithms across
datasets in a variety of situations. Algorithm 2
measures the performance of one algorithm on one
dataset for several different l and u combinations.
Specifically, we consider l ∈ {10,100} and u ∈
{100, 1000}. For each combination, we perform
multiple trials (T = 10 here) using different random
assignments of data to Dlabeled and Dunlabeled, to
obtain confidence intervals around our performance
measurements. All random selections of subsets of
data are the same across different algorithms’ runs,
to permit paired t-tests for evaluation. Note that,
when l =6 max(L) or u =6 max(U), a portion of
Diol is not used for training. Also, the RealSSL
procedure ensures that all parameters are tuned by
cross-validation without ever seeing the held-out
</bodyText>
<footnote confidence="0.99548175">
1The particular choice of 5-fold CV, the way to split labeled
and unlabeled data, and the parameter grid, is important too.
But we view them as secondary to the fact that we are tuning
SSL by CV.
</footnote>
<bodyText confidence="0.998957333333333">
test set Dtest. Lastly, we stress that the same grid
of algorithm-specific parameter values (discussed in
Section 5) is considered for all datasets.
</bodyText>
<sectionHeader confidence="0.998011" genericHeader="method">
4 Datasets
</sectionHeader>
<bodyText confidence="0.999741461538462">
Table 1 summarizes the datasets used for the com-
parisons. In this study we consider only binary clas-
sification tasks. Note that d is the number of dimen-
sions, P(y = 1) is the proportion of instances in
the full dataset belonging to class y = 1, and |Dtest|
refers to the size of the test set (the instances remain-
ing after max(L) + max(U) = 1100 have been set
aside for training trials).
[MacWin] is the Mac versus Windows text clas-
sification data from the 20-newsgroups dataset, pre-
processed by the authors of (Sindhwani et al., 2005).
[Interest] is a binary version of the word sense
disambiguation data from (Bruce and Wiebe, 1994).
The task is to distinguish the sense of “interest”
meaning “money paid for the use of money” from
the other five senses (e.g., “readiness to give atten-
tion,” “a share in a company or business”). The
data comes from a corpus of part-of-speech (POS)
tagged sentences containing the word “interest.”
Each instance is a bag-of-word/POS vector, exclud-
ing words containing the root “interest” and those
that appeared in less than three sentences overall.
Datasets [aut-avn] and [real-sim] are the
auto/aviation and real/simulated text classification
datasets from the SRAA corpus of UseNet articles.
The [ccat] and [gcat] datasets involve identifying
corporate and government articles, respectively, in
the RCV1 corpus. We use the versions of these
datasets prepared by the authors of (Sindhwani et
al., 2006).
Finally, the two WISH datasets come from (Gold-
berg et al., 2009) and involve discriminating be-
tween sentences that contain wishful expressions
and those that do not. The instances in [WISH-
politics] correspond to sentences taken from a po-
litical discussion board, while [WISH-products] is
based on sentences from Amazon product reviews.
The features are a combination of word and template
features as described in (Goldberg et al., 2009).
</bodyText>
<page confidence="0.973522">
20
</page>
<table confidence="0.67994275">
Input: dataset Dlabeled = {xi, yi}li=1, Dunlabeled = {xj}uj=1, algorithm, performance metric
Randomly partition Dlabeled into 5 equally-sized disjoint subsets {Dl1, Dl2, Dl3, Dl4, Dl5}.
Randomly partition Dunlabeled into 5 equally-sized disjoint subsets {Du1, Du2, Du3, Du4, Du5}.
Combine partitions: Let Dfold k = Dlk U Duk for all k = 1, ... , 5.
</table>
<tableCaption confidence="0.71235625">
foreach parameter configuration in grid do
foreach fold k do
Train model using algorithm on Ui�kDfold i.
Evaluate metric on Dfold k.
</tableCaption>
<figure confidence="0.491153">
end
Compute the average metric value across the 5 folds.
end
Choose parameter configuration that optimizes average metric.
Train model using algorithm and the chosen parameters on Dlabeled and Dunlabeled.
Output: Optimal model; Average metric value achieved by optimal parameters during tuning.
</figure>
<figureCaption confidence="0.5302615">
Algorithm 1: RealSSL procedure for running an SSL (or SL, simply ignore the unlabeled data) algorithm on a
specific labeled and unlabeled dataset using cross-validation to tune parameters.
</figureCaption>
<bodyText confidence="0.9799734">
Input: dataset D = {xi, yi}ni=1, algorithm, performance metric, set L, set U, trials T
Randomly divide D into Dpool (of size max(L) + max(U)) and Dtest (the rest).
foreach l in L do
foreach u in U do
foreach trial 1 up to T do
</bodyText>
<subsectionHeader confidence="0.336659">
Randomly select Dlabeled = {xj, yj}lj=l and Dunlabeled = {xk}uk=1 from Dpool.
</subsectionHeader>
<bodyText confidence="0.9236495">
Run RealSSL(Dlabeled, Dunlabeled, algorithm, metric) to obtain model and tuning
performance value (see Algorithm 1).
Use model to classify Dunlabeled and record transductive metric value.
Use model to classify Dtest and record test metric value.
</bodyText>
<listItem confidence="0.722773">
end
end
end
Output: Tuning, transductive, and test performance for T runs of algorithm using all l and u
combinations.
Algorithm 2: Experimental procedure used for all comparisons.
</listItem>
<page confidence="0.97421">
21
</page>
<table confidence="0.975823777777778">
Name d P(y = 1) |Dtest|
[MacWin] 7511 0.51 846
[Interest] 2687 0.53 1268
[aut-avn] 20707 0.65 70075
[real-sim] 20958 0.31 71209
[ccat] 47236 0.47 22019
[gcat] 47236 0.30 22019
[WISH-politics] 13610 0.34 4999
[WISH-products] 4823 0.12 129
</table>
<tableCaption confidence="0.9953895">
Table 1: Datasets used in benchmark comparison. See
text for details.
</tableCaption>
<sectionHeader confidence="0.992934" genericHeader="method">
5 Algorithms
</sectionHeader>
<bodyText confidence="0.9999318">
We consider only linear classifiers for this study,
since they tend to work well for text problems. In
future work, we plan to explore a range of kernels
and other non-linear classifiers.
As a baseline supervised learning method, we use
a support vector machine (SVM), as implemented
by SVMlight (Joachims, 1999). This baseline simply
ignores all the unlabeled data (xl+1, ... , xn). Recall
this solves the following regularized risk minimiza-
tion problem
</bodyText>
<equation confidence="0.753712">
max(0,1 − yif(xi)), (1)
</equation>
<bodyText confidence="0.999822224137931">
where f(x) = wTx + b, and C is a parame-
ter controlling the trade-off between training er-
rors and model complexity. Using the procedure
outlined above, we tune C over a grid of values
110−6,10−5,10−4,10−3,10−2,10−1,1,10,100}.
We consider two popular SSL algorithms, which
make different assumptions about the link between
the marginal data distribution Px and the conditional
label distribution Pylx. If the assumption does not
hold in a particular dataset, the SSL algorithm could
use the unlabeled data “incorrectly,” and perform
worse than SL.
The first SSL algorithm we use is a semi-
supervised support vector machine (S3VM), which
makes the cluster assumption: the classes are well-
separated clusters of data, such that the decision
boundary falls into a low density region in the fea-
ture space. While many implementations exist,
we chose the deterministic annealing (DA) algo-
rithm implemented in the SVMlin package (Sind-
hwani et al., 2006; Sindhwani and Keerthi, 2007).
This DA algorithm often achieved the best accu-
racy across several datasets in the empirical com-
parison in (Sindhwani and Keerthi, 2007), despite
being slower than the multi-switch algorithm pre-
sented in the same paper. Note that the transductive
SVM implemented in SVMlight would have been
prohibitively slow to carry out the range of exper-
iments conducted here. Recall that an S3VM seeks
an optimal classifier f* that cuts through a region of
low density between clusters of data. One way to
view this is that it tries to find the best possible la-
beling of the unlabeled data such the classifier maxi-
mizes the margin on both labeled and unlabeled data
points. This is achieved by solving the following
non-convex minimization problem
subject to a class-balance constraint. Note that
V is a loss function (typically the hinge loss
as in (1)), and the parameters A, A&apos; control the
relative importance of model complexity versus
locating a low-density region within the unlabeled
data. We tune both parameters in a grid of values
110−6,10−5,10−4,10−3,10−2,10−1,1,10,100}.
In past studies (Sindhwani et al., 2006), A was set to
1, and A&apos; was tuned over a grid containing a subset
of these values.
Finally, as an example of a graph-based
SSL method, we consider manifold regularization
(MR) (Belkin et al., 2006), using the implementa-
tion provided on the authors’ Web site.2 This algo-
rithm makes the manifold assumption: the labels are
“smooth” with respect to a graph connecting labeled
and unlabeled instances. In other words, if two in-
stances are connected by a strong edge (e.g., they
are highly similar to one another), their labels tend
to be the same. Manifold regularization represents a
family of methods; we specifically use the Laplacian
SVM, which extends the basic SVM optimization
</bodyText>
<footnote confidence="0.974459">
2http://manifold.cs.uchicago.edu/
manifold regularization/software.html
</footnote>
<figure confidence="0.994544916666666">
||
l �l A&apos; n V (y&apos;jf(xj)),
i=1 V (yif(xi) + u j=l+1
min
A f,Y&apos;E{−1,11u 2 ||f
2||f||2
1 2
+ C
min
f
�l
i=1
</figure>
<page confidence="0.93446">
22
</page>
<bodyText confidence="0.524066">
problem with a graph-based regularization term. rithm 1.3
</bodyText>
<equation confidence="0.6961122">
�l max(0,1 − yif(xi)) 2. Compare the best tuning performance (5-fold
minγA||f||22 + 1 CV average) achieved by the optimal parame-
fl ters for each of the algorithms.
i=1
n
i=1
+ γI
n
j=1
wij(f(xi) − f(xj))2,
</equation>
<bodyText confidence="0.999415294117647">
where γA and γI are parameters that trade off am-
bient and intrinsic smoothness, and wij is a graph
weight between instances xi and xj. In this pa-
per, we consider kNN graphs with k E {3,10, 20}.
Edge weights are formed using a heat kernel wij =
exp(−||Xi−Xj||2 2σ2), where Q is set to be the mean dis-
tance between nearest neighbors in the graph, as
in (Chapelle et al., 2006). The γ parameters are each
tuned over the grid {10−6,10−4,10−2,1,100}.
Of course, many other SSL algorithms exist, some
of which combine different assumptions (Chapelle
and Zien, 2005; Karlen et al., 2008), and others
which exploit multiple (real or artificial) views of
the data (Blum and Mitchell, 1998; Sindhwani and
Rosenberg, 2008). We plan to extend our study to
include many more diverse SSL algorithms in the
future.
</bodyText>
<sectionHeader confidence="0.966321" genericHeader="method">
6 Choosing an Algorithm for a Real-World
Task
</sectionHeader>
<bodyText confidence="0.9999274375">
Given the choice of several algorithms, how should
one choose the best one to apply to a particular learn-
ing setting? Traditionally, CV is used for model
selection in supervised learning settings. However,
with only a small amount of labeled data in semi-
supervised settings, model selection with CV is of-
ten viewed as unreliable. We explicitly tested this
hypothesis by using CV to not only choose the pa-
rameters of the model, but also choose the type of
model itself. The main goal is to automatically
choose between SVM, S3VM, and MR for a par-
ticular learning setting, in an attempt to ensure that
the final performance is never hurt by including un-
labeled data (which might be called agnostic SSL).
Given a set of algorithms (e.g., one SL, several
SSL), the procedure is the following:
</bodyText>
<listItem confidence="0.987469111111111">
1. Tune the parameters of each algorithm on the
labeled and unlabeled training set using Algo-
• If there are no ties, select the algorithm
with the highest tuning performance.
• If there is a tie, and SL is among the best,
select SL.
• If there is a tie between SSL algorithms,
select one of them at random.
3. Use the selected “Best Tuning” algorithm (and
</listItem>
<bodyText confidence="0.989441076923077">
the tuned parameters) to build a model on all
the training data; then apply it to the test data.
Note that the procedure is conservative in that it
prefers SL in the case of ties. In this paper, we use
this simple “best tuning performance” heuristic.
Finally, we stress the fact that, when applying
this procedure within the context of Algorithm 2, a
potentially different algorithm is chosen in each of
the 10 trials for a particular setting. This simulates
the real-world scenario where one only has a single
fixed training set of labeled and unlabeled data and
must choose a single algorithm to produce a model
for future predictions.
</bodyText>
<sectionHeader confidence="0.995306" genericHeader="method">
7 Performance Metrics
</sectionHeader>
<bodyText confidence="0.999981266666667">
We compare different algorithms’ performance us-
ing three metrics often used for evaluation in NLP
tasks: accuracy, maxF1, and AUROC. Accuracy
is simply the fraction of instances correctly classi-
fied. MaxF1 is the maximal F1 value (harmonic
mean of recall and precision) achieved over the en-
tire precision-recall curve (Cai and Hofmann, 2003).
AUROC is the area under the ROC curve (Fawcett,
2004). Throughout the paper, when we discuss a
result involving a particular metric, the algorithms
use this metric as the criterion for parameter tuning,
and we use it for the final evaluation. We are not
simply evaluating a single experiment using multi-
ple metrics—the experiments are fundamentally dif-
ferent and produce different learned models.
</bodyText>
<footnote confidence="0.9408205">
3We ensure each algorithm uses the same 5 partitions during
the tuning step.
</footnote>
<page confidence="0.999236">
23
</page>
<sectionHeader confidence="0.99971" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.9999926">
We now report the results of our empirical compar-
ison of SL and SSL on the eight NLP datasets. We
first consider each dataset separately and examine
how often each type of algorithm outperforms the
other. We then examine cross-dataset performance.
</bodyText>
<subsectionHeader confidence="0.992837">
8.1 Detailed Results
</subsectionHeader>
<bodyText confidence="0.974040096774193">
Table 2 contains all results for SVM, S3VM, and
MR for all datasets and all metrics.4 Note that within
each l,u cell for a particular dataset and evaluation
metric, we show the maximum value in each row
(tune, transductive, or test) in boldface. Results that
are not statistically significantly different using a
paired t-test are also shown in boldface.
Several things are immediately obvious from Ta-
ble 2. First, no algorithm is superior in all datasets
or settings. In several cases, all algorithms are statis-
tically indistinguishable. Most importantly, though,
each of the SSL algorithms can be worse than SL on
some datasets using some metric. We used paired
t-tests to compare transductive and test performance
of each SSL algorithm with SVM for a particular l,u
combination and dataset (32 settings total per evalu-
ation metric). In terms of accuracy, MR transductive
performance is significantly worse than SVM in 5
settings, while MR test performance is significantly
worse in 7 settings. MR is also significantly worse in
4 settings based on transductive maxF1, in 3 settings
based on transductive AUROC, and 1 setting based
on test AUROC. S3VM is significantly worse than
SVM in 2 settings based on transductive maxF1, 2
settings based on transductive AUROC, and in 1 set-
ting based on test AUROC. While these numbers
may seem relatively low, it is important to realize
that each algorithm may be worse than SSL many
times on a trial-by-trial basis, which is the more real-
istic scenario: a practitioner has only a single dataset
to work with. Results based on individual trials are
discussed below shortly.
4Note that the results here for a particular dataset and algo-
rithm combination may be qualitatively and quantitatively dif-
ferent than in previous published work, due to differences in
parameter tuning, choices of parameter grids, l and u sizes, and
randomization. We are not trying to replicate or raise doubt
about past results: we simply intend to compare algorithms on
a wide array of datasets using the standardized procedures out-
lined above.
We also applied our “Best Tuning” model selec-
tion procedure to automatically choose a single al-
gorithm for each trial in each setting. We compare
average SL test performance versus the average test
performance of the Best Tuning selections across the
10 trials (not shown in Table 2). Comparisons based
on transductive performance are similar. When the
performance metric is test accuracy, the Best Tuning
algorithm performs statistically significantly better
than SL in 24 settings and worse in only 6 settings.
In the remaining 2 settings, Best Tuning chose SL
in all 10 trials, so they are equivalent. These results
suggest that accuracy-based tuning is a valid method
for choosing a SSL algorithm to improve accuracy
on test data. To some extent, this holds for maxF1,
too: the Best Tuning selections perform better than
SL (on average) in 18 settings and worse in 14 set-
tings when tuning and test evaluation is based on
maxF1. However, when using AUROC as the per-
formance metric, cross validation seems to be unre-
liable: Best Tuning produces a better result in only
11 out of the 32 settings.
</bodyText>
<subsectionHeader confidence="0.881255">
8.2 Results Aggregated Across Datasets
</subsectionHeader>
<bodyText confidence="0.999969458333333">
We now aggregate the detailed results to better un-
derstand the relative performance of the different
methods across all datasets. We perform this sum-
mary evaluation in two ways, based on test set
performance (transductive performance is similar).
First, we compare the SSL algorithms across all
datasets based on the numbers of times each is worse
than, the same as, or better than SL. For each of
the 80 trials of a particular l,u,metric combination,
we compare the performance of S3VM, MR, and
Best Tuning to SVM. Note that each of these com-
parisons is akin to a real-world scenario where a
practitioner would have to choose an algorithm to
use. Table 3 lists tuples of the form “(#trials worse
than SVM, #trials equal to SVM, #trials better than
SVM).” Note that the numbers in each tuple sum to
80. The perfect SSL algorithm would have a tuple of
“(0, 0, 80),” meaning that it always outperforms SL.
In terms of accuracy (Table 3, top) and maxF1 (Ta-
ble 3, middle), the Best Tuning method turns out to
do worse than SVM less often than either S3VM or
MR does (i.e., the first number in the tuples for Best
Tuning is lower than the corresponding numbers for
the other algorithms). At the same time, Best Tuning
</bodyText>
<page confidence="0.996826">
24
</page>
<table confidence="0.987575">
accuracy maxF1 AUROC
u = 100 u = 1000 u = 100 u = 1000 u = 100 u = 1000
Dataset l SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR SVM S3VM MR
[MacWin] 10 0.60 0.72 0.83 0.60 0.72 0.86 0.66 0.67 0.67 0.66 0.67 0.67 0.63 0.69 0.67 0.63 0.69 0.69
0.51 0.51 0.70 0.51 0.50 0.69 0.74 0.77 0.80 0.74 0.74 0.75 0.72 0.75 0.82 0.72 0.71 0.80
0.53 0.50 0.71 0.53 0.50 0.68 0.74 0.75 0.79 0.74 0.75 0.74 0.73 0.72 0.83 0.73 0.71 0.76
100 0.87 0.87 0.91 0.87 0.87 0.90 0.94 0.95 0.95 0.94 0.95 0.95 0.96 0.97 0.97 0.96 0.96 0.96
0.89 0.89 0.89 0.89 0.89 0.89 0.91 0.93 0.92 0.91 0.90 0.90 0.97 0.97 0.96 0.97 0.97 0.96
0.89 0.89 0.91 0.89 0.89 0.90 0.92 0.92 0.92 0.92 0.91 0.91 0.97 0.97 0.97 0.97 0.97 0.97
[Interest] 10 0.68 0.75 0.78 0.68 0.75 0.79 0.73 0.77 0.77 0.73 0.78 0.77 0.52 0.66 0.66 0.52 0.68 0.64
0.52 0.56 0.56 0.52 0.56 0.56 0.72 0.72 0.72 0.72 0.71 0.71 0.55 0.54 0.54 0.55 0.56 0.61
0.52 0.57 0.57 0.52 0.57 0.58 0.68 0.69 0.69 0.68 0.69 0.69 0.58 0.56 0.61 0.58 0.58 0.62
100 0.77 0.78 0.76 0.77 0.78 0.77 0.84 0.85 0.85 0.84 0.85 0.84 0.89 0.90 0.89 0.89 0.85 0.84
0.79 0.79 0.71 0.79 0.79 0.77 0.84 0.83 0.82 0.84 0.81 0.81 0.91 0.91 0.89 0.91 0.79 0.87
0.81 0.80 0.78 0.81 0.80 0.79 0.82 0.81 0.81 0.82 0.81 0.81 0.90 0.91 0.89 0.90 0.81 0.88
[aut-avn] 10 0.72 0.76 0.82 0.72 0.76 0.79 0.89 0.92 0.91 0.89 0.92 0.91 0.58 0.67 0.65 0.58 0.67 0.65
0.65 0.63 0.67 0.65 0.61 0.69 0.83 0.83 0.84 0.83 0.81 0.82 0.71 0.67 0.73 0.71 0.65 0.72
0.62 0.61 0.67 0.62 0.61 0.67 0.80 0.81 0.82 0.80 0.81 0.81 0.71 0.70 0.73 0.71 0.65 0.69
100 0.75 0.82 0.87 0.75 0.82 0.86 0.94 0.94 0.95 0.94 0.94 0.94 0.93 0.94 0.94 0.93 0.94 0.93
0.77 0.79 0.88 0.77 0.83 0.87 0.92 0.92 0.91 0.92 0.91 0.90 0.93 0.93 0.91 0.93 0.94 0.93
0.77 0.82 0.89 0.77 0.83 0.87 0.91 0.91 0.91 0.91 0.91 0.91 0.95 0.94 0.95 0.95 0.95 0.95
[real-sim] 10 0.53 0.63 0.82 0.53 0.63 0.78 0.65 0.66 0.66 0.65 0.66 0.65 0.77 0.81 0.81 0.77 0.81 0.77
0.64 0.63 0.72 0.64 0.64 0.70 0.57 0.66 0.70 0.57 0.62 0.56 0.65 0.75 0.79 0.65 0.74 0.67
0.65 0.66 0.74 0.65 0.66 0.68 0.53 0.58 0.63 0.53 0.59 0.53 0.64 0.73 0.80 0.64 0.74 0.66
100 0.74 0.73 0.86 0.74 0.73 0.84 0.88 0.90 0.90 0.88 0.91 0.89 0.93 0.94 0.94 0.93 0.94 0.93
0.78 0.76 0.84 0.78 0.78 0.85 0.81 0.83 0.79 0.81 0.81 0.81 0.94 0.93 0.91 0.94 0.94 0.94
0.79 0.78 0.85 0.79 0.78 0.85 0.78 0.79 0.78 0.78 0.79 0.79 0.93 0.93 0.93 0.93 0.94 0.93
[ccat] 10 0.54 0.60 0.82 0.54 0.60 0.81 0.84 0.85 0.85 0.84 0.85 0.84 0.74 0.78 0.78 0.74 0.78 0.74
0.50 0.49 0.65 0.50 0.51 0.67 0.69 0.69 0.73 0.69 0.67 0.69 0.60 0.61 0.71 0.60 0.59 0.72
0.49 0.52 0.64 0.49 0.52 0.66 0.66 0.66 0.69 0.66 0.67 0.67 0.61 0.63 0.72 0.61 0.59 0.71
100 0.80 0.80 0.84 0.80 0.80 0.84 0.89 0.89 0.90 0.89 0.89 0.89 0.91 0.92 0.92 0.91 0.92 0.91
0.80 0.79 0.80 0.80 0.81 0.83 0.83 0.85 0.84 0.83 0.82 0.82 0.91 0.91 0.89 0.91 0.90 0.91
0.81 0.80 0.81 0.81 0.80 0.82 0.80 0.81 0.81 0.80 0.81 0.81 0.90 0.90 0.90 0.90 0.90 0.90
[gcat] 10 0.74 0.83 0.82 0.74 0.79 0.81 0.44 0.47 0.46 0.44 0.47 0.46 0.69 0.79 0.75 0.69 0.79 0.75
0.69 0.68 0.75 0.69 0.72 0.76 0.60 0.62 0.69 0.60 0.59 0.62 0.71 0.73 0.82 0.71 0.69 0.76
0.66 0.67 0.73 0.66 0.71 0.74 0.58 0.61 0.66 0.58 0.60 0.59 0.69 0.69 0.81 0.69 0.69 0.75
100 0.77 0.77 0.90 0.77 0.77 0.91 0.92 0.92 0.93 0.92 0.92 0.92 0.97 0.96 0.97 0.97 0.96 0.96
0.81 0.80 0.89 0.81 0.81 0.90 0.88 0.88 0.84 0.88 0.86 0.85 0.96 0.97 0.95 0.96 0.96 0.96
0.80 0.80 0.89 0.80 0.80 0.90 0.86 0.86 0.85 0.86 0.86 0.86 0.96 0.96 0.96 0.96 0.96 0.96
[WISH-politics] 10 0.70 0.77 0.79 0.70 0.77 0.82 0.61 0.62 0.61 0.61 0.62 0.61 0.74 0.78 0.74 0.74 0.78 0.76
0.50 0.56 0.63 0.50 0.62 0.56 0.58 0.58 0.61 0.58 0.55 0.53 0.62 0.62 0.69 0.62 0.62 0.61
0.52 0.56 0.60 0.52 0.62 0.53 0.52 0.53 0.53 0.52 0.54 0.52 0.57 0.58 0.61 0.57 0.62 0.60
100 0.75 0.75 0.75 0.75 0.75 0.74 0.74 0.75 0.76 0.74 0.75 0.75 0.79 0.80 0.80 0.79 0.80 0.80
0.73 0.73 0.71 0.73 0.73 0.70 0.65 0.66 0.67 0.65 0.64 0.64 0.76 0.74 0.75 0.76 0.75 0.76
0.75 0.75 0.72 0.75 0.75 0.71 0.64 0.63 0.63 0.64 0.63 0.64 0.78 0.76 0.77 0.78 0.76 0.77
[WISH-products] 10 0.89 0.89 0.67 0.89 0.89 0.67 0.19 0.22 0.16 0.19 0.22 0.16 0.76 0.80 0.74 0.76 0.80 0.74
0.87 0.87 0.66 0.87 0.87 0.61 0.31 0.29 0.32 0.31 0.24 0.25 0.56 0.52 0.58 0.56 0.54 0.56
0.90 0.90 0.67 0.90 0.90 0.61 0.22 0.23 0.30 0.22 0.24 0.27 0.50 0.53 0.62 0.50 0.54 0.59
100 0.90 0.90 0.82 0.90 0.90 0.81 0.49 0.50 0.54 0.49 0.52 0.52 0.73 0.73 0.77 0.73 0.78 0.75
0.88 0.88 0.81 0.88 0.88 0.80 0.34 0.28 0.37 0.34 0.27 0.30 0.60 0.55 0.57 0.60 0.57 0.61
0.90 0.90 0.79 0.90 0.91 0.76 0.33 0.28 0.33 0.33 0.32 0.38 0.59 0.56 0.60 0.59 0.56 0.60
</table>
<tableCaption confidence="0.857477">
Table 2: Benchmark comparison results. All numbers are averages over 10 trials. Within each cell of nine numbers,
the boldface indicates the maximum value in each row, as well as others in the row that are not statistically significantly
different based on a paired t-test.
</tableCaption>
<table confidence="0.989424454545454">
u = 100 u = 1000
Metric l S3VM MR Best Tuning S3VM MR Best Tuning
10 (14, 27, 39) (27, 0, 53) (8, 31, 41) (14, 25, 41) (27, 0, 53) (8, 29, 43)
accuracy
100 (27, 7, 46) (38, 0, 42) (20, 16, 44) (27, 6, 47) (37, 0, 43) (16, 19, 45)
Metric l S3VM MR Best Tuning S3VM MR Best Tuning
maxF1 10 (29, 2, 49) (16, 1, 63) (14, 55, 11) (27, 0, 53) (24, 0, 56) (13, 53, 14)
100 (39, 0, 41) (34, 4, 42) (31, 15, 34) (39, 1, 40) (44, 4, 32) (26, 21, 33)
Metric l S3VM MR Best Tuning S3VM MR Best Tuning
AUROC 10 (26, 0, 54) (11, 0, 69) (12, 57, 11) (25, 0, 55) (25, 0, 55) (11, 56, 13)
100 (43, 0, 37) (37, 0, 43) (38, 8, 34) (38, 0, 42) (46, 0, 34) (28, 24, 28)
</table>
<tableCaption confidence="0.9905465">
Table 3: Aggregate test performance comparisons versus SVM in 80 trials per setting. Each cell contains a tuple of
the form “(#trials worse than SVM, #trials equal to SVM, #trials better than SVM).”
</tableCaption>
<figure confidence="0.999097314814815">
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Tune
Trans
Test
Test
Test
Test
Test
Test
Test
</figure>
<page confidence="0.706109">
25
</page>
<table confidence="0.9513667">
u = 100 u = 1000
Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning
accuracy 10 0.61 0.62 0.67 0.68 0.61 0.63 0.64 0.67
100 0.81 0.82 0.83 0.85 0.81 0.82 0.83 0.85
Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning
maxF1 10 0.59 0.61 0.64 0.59 0.59 0.61 0.61 0.59
100 0.76 0.75 0.76 0.75 0.76 0.76 0.76 0.76
Metric l SVM S3VM MR Best Tuning SVM S3VM MR Best Tuning
AUROC 10 0.63 0.64 0.72 0.61 0.63 0.64 0.67 0.61
100 0.87 0.87 0.87 0.87 0.87 0.86 0.87 0.86
</table>
<tableCaption confidence="0.999969">
Table 4: Aggregate test results averaged over the 80 trials (8 datasets, 10 trials each) in a particular setting.
</tableCaption>
<figure confidence="0.945802">
Test
Test
Test
Test
Test
Test
</figure>
<bodyText confidence="0.997452076923077">
outperforms SVM in fewer trials than the other algo-
rithms in some settings for these two metrics. This
is because Best Tuning conservatively selects SVM
in many trials. The take home message is that tuning
using CV based on accuracy (and to a lesser extent
maxF1) appears to mitigate some risk involved in
applying SSL. AUROC, on the other hand, does not
appear as effective for this purpose. Table 3 (bottom)
shows that, for u = 1000, Best Tuning is worse than
SVM fewer times, but for u = 100, MR achieves
better performance overall.
We also compare overall average test performance
(across datasets) for each metric and l,u combina-
tion. Table 4 reports these results for accuracy,
maxF1, and AUROC. In terms of accuracy, we see
that the Best Tuning approach leads to better per-
formance than SVM, S3VM, or MR in all settings
when averaged over datasets. We appear to achieve
some synergy in dynamically choosing a different
algorithm in each trial. In terms of maxF1, Best
Tuning, S3VM, and MR are all at least as good as
SL in three of the four l,u settings, and nearly as
good in the fourth. Based on AUROC, though, the
results are mixed depending on the specific setting.
Notably, though, Best Tuning consistently leads to
worse performance than SL when using this metric.
</bodyText>
<subsectionHeader confidence="0.984919">
8.3 A Note on Cloud Computing
</subsectionHeader>
<bodyText confidence="0.986063">
The experiments were carried out using the Condor
High-Throughput Computing platform (Thain et al.,
2005). We ran many trials per algorithm (using dif-
ferent datasets, l, u, and metrics). Each trial in-
volved training hundreds of models using different
parameter configurations repeated across five folds,
and then training once more using the selected pa-
rameters. In the end, we trained a grand total of
794,880 individual models to produce the results in
Table 2. Through distributed computing on approxi-
mately 50 machines in parallel, we were able to run
all these experiments in less than a week, while us-
ing roughly three months worth of CPU time.
</bodyText>
<sectionHeader confidence="0.997577" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999990681818182">
We have explored “realistic SSL,” where all parame-
ters are tuned via 5-fold cross validation, to simulate
a real-world experience of trying to use unlabeled
data in a novel NLP task. Our medium-scale empir-
ical study of SVM, S3VM, and MR revealed that no
algorithm is always superior, and furthermore that
there are cases in which each SSL algorithm we ex-
amined can perform worse than SVM (in some cases
significantly worse across 10 trials). To mitigate
such risks, we proposed a simple meta-level proce-
dure that selects one of the three models based on
tuning performance. While cross validation is often
dismissed for model selection in SSL due to a lack
of labeled data, this Best Tuning approach proves ef-
fective in helping to ensure that incorporating unla-
beled data does not hurt performance. Interestingly,
this works well only when optimizing accuracy dur-
ing tuning. For future work, we plan to extend this
study to include additional datasets, algorithms, and
tuning criteria. We also plan to develop more so-
phisticated techniques for choosing which SL/SSL
algorithm to use in practice.
</bodyText>
<sectionHeader confidence="0.999777" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.942156">
A. Goldberg is supported in part by a Yahoo! Key
Technical Challenges Grant.
</reference>
<page confidence="0.998988">
26
</page>
<sectionHeader confidence="0.995839" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999310833333333">
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled exam-
ples. Journal ofMachine Learning Research, 7:2399–
2434, November.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT: Proceedings of the Workshop on Computa-
tional Learning Theory.
Ulf Brefeld and Tobias Scheffer. 2006. Semi-supervised
learning for structured output variables. In ICML06,
23rd International Conference on Machine Learning,
Pittsburgh, USA.
R. Bruce and J. Wiebe. 1994. Word-sense disambigua-
tion using decomposable models. In Proceedings of
the 32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 139–146.
Lijuan Cai and Thomas Hofmann. 2003. Text catego-
rization by boosting automatically extracted concepts.
In SIGIR ’03: Proceedings of the 26th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in informaion retrieval.
Olivier Chapelle and Alexander Zien. 2005. Semi-
supervised classification by low density separation. In
Proceedings of the Tenth International Workshop on
Artificial Intelligence and Statistics (AISTAT 2005).
Olivier Chapelle, Alexander Zien, and Bernhard
Sch¨olkopf, editors. 2006. Semi-supervised learning.
MIT Press.
Tom Fawcett. 2004. ROC graphs: Notes and practical
considerations for researchers. Technical Report HPL-
2003-4.
Andrew B. Goldberg, Nathanael Fillmore, David Andrze-
jewski, Zhiting Xu, Bryan Gibson, and Xiaojin Zhu.
2009. May all your wishes come true: A study of
wishes and how to recognize them. In Proceedings
ofNAACL HLT.
Thorsten Joachims. 1999. Making large-scale svm
learning practical. In B. Sch¨olkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.
M. Karlen, J. Weston, A. Erkan, and R. Collobert. 2008.
Large scale manifold transduction. In Andrew McCal-
lum and Sam Roweis, editors, Proceedings of the 25th
Annual International Conference on Machine Learn-
ing (ICML 2008), pages 448–455. Omnipress.
Vikas Sindhwani and S. Sathiya Keerthi. 2007. New-
ton methods for fast solution of semi-supervised linear
SVMs. In Leon Bottou, Olivier Chapelle, Dennis De-
Coste, and Jason Weston, editors, Large-Scale Kernel
Machines. MIT Press.
V. Sindhwani and D. Rosenberg. 2008. An rkhs for
multi-view learning and manifold co-regularization.
In Andrew McCallum and Sam Roweis, editors, Pro-
ceedings of the 25th Annual International Conference
on Machine Learning (ICML 2008), pages 976–983.
Omnipress.
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.
2005. Beyond the point cloud: from transductive to
semi-supervised learning. In ICML05, 22nd Interna-
tional Conference on Machine Learning.
Vikas Sindhwani, Sathiya Keerthi, and Olivier Chapelle.
2006. Deterministic annealing for semi-supervised
kernel machines. In ICML06, 23rd International Con-
ference on Machine Learning, Pittsburgh, USA.
Douglas Thain, Todd Tannenbaum, and Miron Livny.
2005. Distributed computing in practice: the condor
experience. Concurrency - Practice and Experience,
17(2-4):323–356.
Xiaojin Zhu. 2005. Semi-supervised learning literature
survey. Technical Report 1530, Department of Com-
puter Sciences, University of Wisconsin, Madison.
</reference>
<page confidence="0.998838">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.498568">
<title confidence="0.999635">Keepin’ It Real: Semi-Supervised Learning with Realistic Tuning</title>
<author confidence="0.998047">B Andrew</author>
<affiliation confidence="0.999446">Computer Sciences University of</affiliation>
<address confidence="0.997555">Madison, WI 53706,</address>
<email confidence="0.999844">goldberg@cs.wisc.edu</email>
<author confidence="0.54387">Xiaojin</author>
<affiliation confidence="0.99879">Computer Sciences University of</affiliation>
<address confidence="0.998173">Madison, WI 53706,</address>
<email confidence="0.999895">jerryzhu@cs.wisc.edu</email>
<abstract confidence="0.996731166666667">We address two critical issues involved in applying semi-supervised learning (SSL) to a real-world task: parameter tuning and choosing which (if any) SSL algorithm is best suited for the task at hand. To gain a better understanding of these issues, we carry out a medium-scale empirical study comparing supervised learning (SL) to two popular SSL algorithms on eight natural language processing tasks under three performance metrics. We simulate how a practitioner would go about tackling a new problem, including parameter tuning using cross validation (CV). We show that, under such realistic conditions, each of the SSL algorithms can be worse than SL on some datasets. However, we also show that CV can select SL/SSL to achieve “agnostic SSL,” whose performance is almost always no worse than SL. While CV is often dismissed as unreliable for SSL due to the small amount of labeled data, we show that it is in fact effective for accuracy even when the labeled dataset size is as small as 10.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>A</author>
</authors>
<title>Goldberg is supported in part by a Yahoo! Key Technical Challenges Grant.</title>
<marker>A, </marker>
<rawString>A. Goldberg is supported in part by a Yahoo! Key Technical Challenges Grant.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
<author>Vikas Sindhwani</author>
</authors>
<title>Manifold regularization: A geometric framework for learning from labeled and unlabeled examples.</title>
<date>2006</date>
<journal>Journal ofMachine Learning Research,</journal>
<volume>7</volume>
<pages>2434</pages>
<contexts>
<context position="12486" citStr="Belkin et al., 2006" startWordPosition="2024" endWordPosition="2027"> the following non-convex minimization problem subject to a class-balance constraint. Note that V is a loss function (typically the hinge loss as in (1)), and the parameters A, A&apos; control the relative importance of model complexity versus locating a low-density region within the unlabeled data. We tune both parameters in a grid of values 110−6,10−5,10−4,10−3,10−2,10−1,1,10,100}. In past studies (Sindhwani et al., 2006), A was set to 1, and A&apos; was tuned over a grid containing a subset of these values. Finally, as an example of a graph-based SSL method, we consider manifold regularization (MR) (Belkin et al., 2006), using the implementation provided on the authors’ Web site.2 This algorithm makes the manifold assumption: the labels are “smooth” with respect to a graph connecting labeled and unlabeled instances. In other words, if two instances are connected by a strong edge (e.g., they are highly similar to one another), their labels tend to be the same. Manifold regularization represents a family of methods; we specifically use the Laplacian SVM, which extends the basic SVM optimization 2http://manifold.cs.uchicago.edu/ manifold regularization/software.html || l �l A&apos; n V (y&apos;jf(xj)), i=1 V (yif(xi) + u</context>
</contexts>
<marker>Belkin, Niyogi, Sindhwani, 2006</marker>
<rawString>Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. 2006. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal ofMachine Learning Research, 7:2399– 2434, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT: Proceedings of the Workshop on Computational Learning Theory.</booktitle>
<contexts>
<context position="14093" citStr="Blum and Mitchell, 1998" startWordPosition="2298" endWordPosition="2301">d intrinsic smoothness, and wij is a graph weight between instances xi and xj. In this paper, we consider kNN graphs with k E {3,10, 20}. Edge weights are formed using a heat kernel wij = exp(−||Xi−Xj||2 2σ2), where Q is set to be the mean distance between nearest neighbors in the graph, as in (Chapelle et al., 2006). The γ parameters are each tuned over the grid {10−6,10−4,10−2,1,100}. Of course, many other SSL algorithms exist, some of which combine different assumptions (Chapelle and Zien, 2005; Karlen et al., 2008), and others which exploit multiple (real or artificial) views of the data (Blum and Mitchell, 1998; Sindhwani and Rosenberg, 2008). We plan to extend our study to include many more diverse SSL algorithms in the future. 6 Choosing an Algorithm for a Real-World Task Given the choice of several algorithms, how should one choose the best one to apply to a particular learning setting? Traditionally, CV is used for model selection in supervised learning settings. However, with only a small amount of labeled data in semisupervised settings, model selection with CV is often viewed as unreliable. We explicitly tested this hypothesis by using CV to not only choose the parameters of the model, but al</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT: Proceedings of the Workshop on Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Brefeld</author>
<author>Tobias Scheffer</author>
</authors>
<title>Semi-supervised learning for structured output variables.</title>
<date>2006</date>
<booktitle>In ICML06, 23rd International Conference on Machine Learning,</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="1953" citStr="Brefeld and Scheffer, 2006" startWordPosition="307" endWordPosition="310">magine you are a real-world practitioner working on a machine learning problem in natural language processing. If you have unlabeled data, should you use semi-supervised learning (SSL)? Which SSL algorithm should you use? How should you set its parameters? Or could it actually hurt performance, in which case you might be better off with supervised learning (SL)? A large number of SSL algorithms have been developed in recent years that allow one to improve 19 performance with unlabeled data, in tasks such as text classification, sequence labeling, and parsing (Zhu, 2005; Chapelle et al., 2006; Brefeld and Scheffer, 2006). However, many of them are tested on “SSL-friendly” datasets, such as “two moons,” USPS, and MNIST. Furthermore, the algorithms’ parameters are often chosen based on test set performance or manually set based on heuristics and researcher experience. These issues create practical concerns for deploying SSL in the real world. We note that (Chapelle et al., 2006)’s benchmark chapter explores these issues to some extent by comparing several SSL methods on several real and artificial datasets. The authors reach the conclusions that parameter tuning is difficult with little labeled data and that no</context>
</contexts>
<marker>Brefeld, Scheffer, 2006</marker>
<rawString>Ulf Brefeld and Tobias Scheffer. 2006. Semi-supervised learning for structured output variables. In ICML06, 23rd International Conference on Machine Learning, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>Word-sense disambiguation using decomposable models.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="6456" citStr="Bruce and Wiebe, 1994" startWordPosition="1063" endWordPosition="1066">arizes the datasets used for the comparisons. In this study we consider only binary classification tasks. Note that d is the number of dimensions, P(y = 1) is the proportion of instances in the full dataset belonging to class y = 1, and |Dtest| refers to the size of the test set (the instances remaining after max(L) + max(U) = 1100 have been set aside for training trials). [MacWin] is the Mac versus Windows text classification data from the 20-newsgroups dataset, preprocessed by the authors of (Sindhwani et al., 2005). [Interest] is a binary version of the word sense disambiguation data from (Bruce and Wiebe, 1994). The task is to distinguish the sense of “interest” meaning “money paid for the use of money” from the other five senses (e.g., “readiness to give attention,” “a share in a company or business”). The data comes from a corpus of part-of-speech (POS) tagged sentences containing the word “interest.” Each instance is a bag-of-word/POS vector, excluding words containing the root “interest” and those that appeared in less than three sentences overall. Datasets [aut-avn] and [real-sim] are the auto/aviation and real/simulated text classification datasets from the SRAA corpus of UseNet articles. The </context>
</contexts>
<marker>Bruce, Wiebe, 1994</marker>
<rawString>R. Bruce and J. Wiebe. 1994. Word-sense disambiguation using decomposable models. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 139–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijuan Cai</author>
<author>Thomas Hofmann</author>
</authors>
<title>Text categorization by boosting automatically extracted concepts.</title>
<date>2003</date>
<booktitle>In SIGIR ’03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval.</booktitle>
<contexts>
<context position="16406" citStr="Cai and Hofmann, 2003" startWordPosition="2696" endWordPosition="2699">thm is chosen in each of the 10 trials for a particular setting. This simulates the real-world scenario where one only has a single fixed training set of labeled and unlabeled data and must choose a single algorithm to produce a model for future predictions. 7 Performance Metrics We compare different algorithms’ performance using three metrics often used for evaluation in NLP tasks: accuracy, maxF1, and AUROC. Accuracy is simply the fraction of instances correctly classified. MaxF1 is the maximal F1 value (harmonic mean of recall and precision) achieved over the entire precision-recall curve (Cai and Hofmann, 2003). AUROC is the area under the ROC curve (Fawcett, 2004). Throughout the paper, when we discuss a result involving a particular metric, the algorithms use this metric as the criterion for parameter tuning, and we use it for the final evaluation. We are not simply evaluating a single experiment using multiple metrics—the experiments are fundamentally different and produce different learned models. 3We ensure each algorithm uses the same 5 partitions during the tuning step. 23 8 Results We now report the results of our empirical comparison of SL and SSL on the eight NLP datasets. We first conside</context>
</contexts>
<marker>Cai, Hofmann, 2003</marker>
<rawString>Lijuan Cai and Thomas Hofmann. 2003. Text categorization by boosting automatically extracted concepts. In SIGIR ’03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
<author>Alexander Zien</author>
</authors>
<title>Semisupervised classification by low density separation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics (AISTAT</booktitle>
<contexts>
<context position="13972" citStr="Chapelle and Zien, 2005" startWordPosition="2278" endWordPosition="2281">ach of the algorithms. i=1 n i=1 + γI n j=1 wij(f(xi) − f(xj))2, where γA and γI are parameters that trade off ambient and intrinsic smoothness, and wij is a graph weight between instances xi and xj. In this paper, we consider kNN graphs with k E {3,10, 20}. Edge weights are formed using a heat kernel wij = exp(−||Xi−Xj||2 2σ2), where Q is set to be the mean distance between nearest neighbors in the graph, as in (Chapelle et al., 2006). The γ parameters are each tuned over the grid {10−6,10−4,10−2,1,100}. Of course, many other SSL algorithms exist, some of which combine different assumptions (Chapelle and Zien, 2005; Karlen et al., 2008), and others which exploit multiple (real or artificial) views of the data (Blum and Mitchell, 1998; Sindhwani and Rosenberg, 2008). We plan to extend our study to include many more diverse SSL algorithms in the future. 6 Choosing an Algorithm for a Real-World Task Given the choice of several algorithms, how should one choose the best one to apply to a particular learning setting? Traditionally, CV is used for model selection in supervised learning settings. However, with only a small amount of labeled data in semisupervised settings, model selection with CV is often view</context>
</contexts>
<marker>Chapelle, Zien, 2005</marker>
<rawString>Olivier Chapelle and Alexander Zien. 2005. Semisupervised classification by low density separation. In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics (AISTAT 2005).</rawString>
</citation>
<citation valid="true">
<title>Semi-supervised learning.</title>
<date>2006</date>
<editor>Olivier Chapelle, Alexander Zien, and Bernhard Sch¨olkopf, editors.</editor>
<publisher>MIT Press.</publisher>
<marker>2006</marker>
<rawString>Olivier Chapelle, Alexander Zien, and Bernhard Sch¨olkopf, editors. 2006. Semi-supervised learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Fawcett</author>
</authors>
<title>ROC graphs: Notes and practical considerations for researchers.</title>
<date>2004</date>
<tech>Technical Report HPL2003-4.</tech>
<contexts>
<context position="16461" citStr="Fawcett, 2004" startWordPosition="2708" endWordPosition="2709">. This simulates the real-world scenario where one only has a single fixed training set of labeled and unlabeled data and must choose a single algorithm to produce a model for future predictions. 7 Performance Metrics We compare different algorithms’ performance using three metrics often used for evaluation in NLP tasks: accuracy, maxF1, and AUROC. Accuracy is simply the fraction of instances correctly classified. MaxF1 is the maximal F1 value (harmonic mean of recall and precision) achieved over the entire precision-recall curve (Cai and Hofmann, 2003). AUROC is the area under the ROC curve (Fawcett, 2004). Throughout the paper, when we discuss a result involving a particular metric, the algorithms use this metric as the criterion for parameter tuning, and we use it for the final evaluation. We are not simply evaluating a single experiment using multiple metrics—the experiments are fundamentally different and produce different learned models. 3We ensure each algorithm uses the same 5 partitions during the tuning step. 23 8 Results We now report the results of our empirical comparison of SL and SSL on the eight NLP datasets. We first consider each dataset separately and examine how often each ty</context>
</contexts>
<marker>Fawcett, 2004</marker>
<rawString>Tom Fawcett. 2004. ROC graphs: Notes and practical considerations for researchers. Technical Report HPL2003-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew B Goldberg</author>
<author>Nathanael Fillmore</author>
<author>David Andrzejewski</author>
<author>Zhiting Xu</author>
<author>Bryan Gibson</author>
<author>Xiaojin Zhu</author>
</authors>
<title>all your wishes come true: A study of wishes and how to recognize them.</title>
<date>2009</date>
<booktitle>In Proceedings ofNAACL HLT.</booktitle>
<contexts>
<context position="7327" citStr="Goldberg et al., 2009" startWordPosition="1198" endWordPosition="1202"> tagged sentences containing the word “interest.” Each instance is a bag-of-word/POS vector, excluding words containing the root “interest” and those that appeared in less than three sentences overall. Datasets [aut-avn] and [real-sim] are the auto/aviation and real/simulated text classification datasets from the SRAA corpus of UseNet articles. The [ccat] and [gcat] datasets involve identifying corporate and government articles, respectively, in the RCV1 corpus. We use the versions of these datasets prepared by the authors of (Sindhwani et al., 2006). Finally, the two WISH datasets come from (Goldberg et al., 2009) and involve discriminating between sentences that contain wishful expressions and those that do not. The instances in [WISHpolitics] correspond to sentences taken from a political discussion board, while [WISH-products] is based on sentences from Amazon product reviews. The features are a combination of word and template features as described in (Goldberg et al., 2009). 20 Input: dataset Dlabeled = {xi, yi}li=1, Dunlabeled = {xj}uj=1, algorithm, performance metric Randomly partition Dlabeled into 5 equally-sized disjoint subsets {Dl1, Dl2, Dl3, Dl4, Dl5}. Randomly partition Dunlabeled into 5 </context>
</contexts>
<marker>Goldberg, Fillmore, Andrzejewski, Xu, Gibson, Zhu, 2009</marker>
<rawString>Andrew B. Goldberg, Nathanael Fillmore, David Andrzejewski, Zhiting Xu, Bryan Gibson, and Xiaojin Zhu. 2009. May all your wishes come true: A study of wishes and how to recognize them. In Proceedings ofNAACL HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale svm learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10038" citStr="Joachims, 1999" startWordPosition="1630" endWordPosition="1631">test| [MacWin] 7511 0.51 846 [Interest] 2687 0.53 1268 [aut-avn] 20707 0.65 70075 [real-sim] 20958 0.31 71209 [ccat] 47236 0.47 22019 [gcat] 47236 0.30 22019 [WISH-politics] 13610 0.34 4999 [WISH-products] 4823 0.12 129 Table 1: Datasets used in benchmark comparison. See text for details. 5 Algorithms We consider only linear classifiers for this study, since they tend to work well for text problems. In future work, we plan to explore a range of kernels and other non-linear classifiers. As a baseline supervised learning method, we use a support vector machine (SVM), as implemented by SVMlight (Joachims, 1999). This baseline simply ignores all the unlabeled data (xl+1, ... , xn). Recall this solves the following regularized risk minimization problem max(0,1 − yif(xi)), (1) where f(x) = wTx + b, and C is a parameter controlling the trade-off between training errors and model complexity. Using the procedure outlined above, we tune C over a grid of values 110−6,10−5,10−4,10−3,10−2,10−1,1,10,100}. We consider two popular SSL algorithms, which make different assumptions about the link between the marginal data distribution Px and the conditional label distribution Pylx. If the assumption does not hold i</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale svm learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Karlen</author>
<author>J Weston</author>
<author>A Erkan</author>
<author>R Collobert</author>
</authors>
<title>Large scale manifold transduction.</title>
<date>2008</date>
<booktitle>In Andrew McCallum and Sam Roweis, editors, Proceedings of the 25th Annual International Conference on Machine Learning (ICML 2008),</booktitle>
<pages>448--455</pages>
<publisher>Omnipress.</publisher>
<contexts>
<context position="13994" citStr="Karlen et al., 2008" startWordPosition="2282" endWordPosition="2285">1 n i=1 + γI n j=1 wij(f(xi) − f(xj))2, where γA and γI are parameters that trade off ambient and intrinsic smoothness, and wij is a graph weight between instances xi and xj. In this paper, we consider kNN graphs with k E {3,10, 20}. Edge weights are formed using a heat kernel wij = exp(−||Xi−Xj||2 2σ2), where Q is set to be the mean distance between nearest neighbors in the graph, as in (Chapelle et al., 2006). The γ parameters are each tuned over the grid {10−6,10−4,10−2,1,100}. Of course, many other SSL algorithms exist, some of which combine different assumptions (Chapelle and Zien, 2005; Karlen et al., 2008), and others which exploit multiple (real or artificial) views of the data (Blum and Mitchell, 1998; Sindhwani and Rosenberg, 2008). We plan to extend our study to include many more diverse SSL algorithms in the future. 6 Choosing an Algorithm for a Real-World Task Given the choice of several algorithms, how should one choose the best one to apply to a particular learning setting? Traditionally, CV is used for model selection in supervised learning settings. However, with only a small amount of labeled data in semisupervised settings, model selection with CV is often viewed as unreliable. We e</context>
</contexts>
<marker>Karlen, Weston, Erkan, Collobert, 2008</marker>
<rawString>M. Karlen, J. Weston, A. Erkan, and R. Collobert. 2008. Large scale manifold transduction. In Andrew McCallum and Sam Roweis, editors, Proceedings of the 25th Annual International Conference on Machine Learning (ICML 2008), pages 448–455. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Sindhwani</author>
<author>S Sathiya Keerthi</author>
</authors>
<title>Newton methods for fast solution of semi-supervised linear SVMs. In</title>
<date>2007</date>
<editor>Leon Bottou, Olivier Chapelle, Dennis DeCoste, and Jason Weston, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11176" citStr="Sindhwani and Keerthi, 2007" startWordPosition="1808" endWordPosition="1811">ribution Px and the conditional label distribution Pylx. If the assumption does not hold in a particular dataset, the SSL algorithm could use the unlabeled data “incorrectly,” and perform worse than SL. The first SSL algorithm we use is a semisupervised support vector machine (S3VM), which makes the cluster assumption: the classes are wellseparated clusters of data, such that the decision boundary falls into a low density region in the feature space. While many implementations exist, we chose the deterministic annealing (DA) algorithm implemented in the SVMlin package (Sindhwani et al., 2006; Sindhwani and Keerthi, 2007). This DA algorithm often achieved the best accuracy across several datasets in the empirical comparison in (Sindhwani and Keerthi, 2007), despite being slower than the multi-switch algorithm presented in the same paper. Note that the transductive SVM implemented in SVMlight would have been prohibitively slow to carry out the range of experiments conducted here. Recall that an S3VM seeks an optimal classifier f* that cuts through a region of low density between clusters of data. One way to view this is that it tries to find the best possible labeling of the unlabeled data such the classifier m</context>
</contexts>
<marker>Sindhwani, Keerthi, 2007</marker>
<rawString>Vikas Sindhwani and S. Sathiya Keerthi. 2007. Newton methods for fast solution of semi-supervised linear SVMs. In Leon Bottou, Olivier Chapelle, Dennis DeCoste, and Jason Weston, editors, Large-Scale Kernel Machines. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sindhwani</author>
<author>D Rosenberg</author>
</authors>
<title>An rkhs for multi-view learning and manifold co-regularization.</title>
<date>2008</date>
<booktitle>Proceedings of the 25th Annual International Conference on Machine Learning (ICML 2008),</booktitle>
<pages>976--983</pages>
<editor>In Andrew McCallum and Sam Roweis, editors,</editor>
<publisher>Omnipress.</publisher>
<contexts>
<context position="14125" citStr="Sindhwani and Rosenberg, 2008" startWordPosition="2302" endWordPosition="2305">nd wij is a graph weight between instances xi and xj. In this paper, we consider kNN graphs with k E {3,10, 20}. Edge weights are formed using a heat kernel wij = exp(−||Xi−Xj||2 2σ2), where Q is set to be the mean distance between nearest neighbors in the graph, as in (Chapelle et al., 2006). The γ parameters are each tuned over the grid {10−6,10−4,10−2,1,100}. Of course, many other SSL algorithms exist, some of which combine different assumptions (Chapelle and Zien, 2005; Karlen et al., 2008), and others which exploit multiple (real or artificial) views of the data (Blum and Mitchell, 1998; Sindhwani and Rosenberg, 2008). We plan to extend our study to include many more diverse SSL algorithms in the future. 6 Choosing an Algorithm for a Real-World Task Given the choice of several algorithms, how should one choose the best one to apply to a particular learning setting? Traditionally, CV is used for model selection in supervised learning settings. However, with only a small amount of labeled data in semisupervised settings, model selection with CV is often viewed as unreliable. We explicitly tested this hypothesis by using CV to not only choose the parameters of the model, but also choose the type of model itse</context>
</contexts>
<marker>Sindhwani, Rosenberg, 2008</marker>
<rawString>V. Sindhwani and D. Rosenberg. 2008. An rkhs for multi-view learning and manifold co-regularization. In Andrew McCallum and Sam Roweis, editors, Proceedings of the 25th Annual International Conference on Machine Learning (ICML 2008), pages 976–983. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Sindhwani</author>
<author>Partha Niyogi</author>
<author>Mikhail Belkin</author>
</authors>
<title>Beyond the point cloud: from transductive to semi-supervised learning.</title>
<date>2005</date>
<booktitle>In ICML05, 22nd International Conference on Machine Learning.</booktitle>
<contexts>
<context position="6357" citStr="Sindhwani et al., 2005" startWordPosition="1047" endWordPosition="1050">ic parameter values (discussed in Section 5) is considered for all datasets. 4 Datasets Table 1 summarizes the datasets used for the comparisons. In this study we consider only binary classification tasks. Note that d is the number of dimensions, P(y = 1) is the proportion of instances in the full dataset belonging to class y = 1, and |Dtest| refers to the size of the test set (the instances remaining after max(L) + max(U) = 1100 have been set aside for training trials). [MacWin] is the Mac versus Windows text classification data from the 20-newsgroups dataset, preprocessed by the authors of (Sindhwani et al., 2005). [Interest] is a binary version of the word sense disambiguation data from (Bruce and Wiebe, 1994). The task is to distinguish the sense of “interest” meaning “money paid for the use of money” from the other five senses (e.g., “readiness to give attention,” “a share in a company or business”). The data comes from a corpus of part-of-speech (POS) tagged sentences containing the word “interest.” Each instance is a bag-of-word/POS vector, excluding words containing the root “interest” and those that appeared in less than three sentences overall. Datasets [aut-avn] and [real-sim] are the auto/avi</context>
</contexts>
<marker>Sindhwani, Niyogi, Belkin, 2005</marker>
<rawString>Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin. 2005. Beyond the point cloud: from transductive to semi-supervised learning. In ICML05, 22nd International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Sindhwani</author>
<author>Sathiya Keerthi</author>
<author>Olivier Chapelle</author>
</authors>
<title>Deterministic annealing for semi-supervised kernel machines.</title>
<date>2006</date>
<booktitle>In ICML06, 23rd International Conference on Machine Learning,</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="7261" citStr="Sindhwani et al., 2006" startWordPosition="1187" endWordPosition="1190">or business”). The data comes from a corpus of part-of-speech (POS) tagged sentences containing the word “interest.” Each instance is a bag-of-word/POS vector, excluding words containing the root “interest” and those that appeared in less than three sentences overall. Datasets [aut-avn] and [real-sim] are the auto/aviation and real/simulated text classification datasets from the SRAA corpus of UseNet articles. The [ccat] and [gcat] datasets involve identifying corporate and government articles, respectively, in the RCV1 corpus. We use the versions of these datasets prepared by the authors of (Sindhwani et al., 2006). Finally, the two WISH datasets come from (Goldberg et al., 2009) and involve discriminating between sentences that contain wishful expressions and those that do not. The instances in [WISHpolitics] correspond to sentences taken from a political discussion board, while [WISH-products] is based on sentences from Amazon product reviews. The features are a combination of word and template features as described in (Goldberg et al., 2009). 20 Input: dataset Dlabeled = {xi, yi}li=1, Dunlabeled = {xj}uj=1, algorithm, performance metric Randomly partition Dlabeled into 5 equally-sized disjoint subset</context>
<context position="11146" citStr="Sindhwani et al., 2006" startWordPosition="1803" endWordPosition="1807">n the marginal data distribution Px and the conditional label distribution Pylx. If the assumption does not hold in a particular dataset, the SSL algorithm could use the unlabeled data “incorrectly,” and perform worse than SL. The first SSL algorithm we use is a semisupervised support vector machine (S3VM), which makes the cluster assumption: the classes are wellseparated clusters of data, such that the decision boundary falls into a low density region in the feature space. While many implementations exist, we chose the deterministic annealing (DA) algorithm implemented in the SVMlin package (Sindhwani et al., 2006; Sindhwani and Keerthi, 2007). This DA algorithm often achieved the best accuracy across several datasets in the empirical comparison in (Sindhwani and Keerthi, 2007), despite being slower than the multi-switch algorithm presented in the same paper. Note that the transductive SVM implemented in SVMlight would have been prohibitively slow to carry out the range of experiments conducted here. Recall that an S3VM seeks an optimal classifier f* that cuts through a region of low density between clusters of data. One way to view this is that it tries to find the best possible labeling of the unlabe</context>
</contexts>
<marker>Sindhwani, Keerthi, Chapelle, 2006</marker>
<rawString>Vikas Sindhwani, Sathiya Keerthi, and Olivier Chapelle. 2006. Deterministic annealing for semi-supervised kernel machines. In ICML06, 23rd International Conference on Machine Learning, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Thain</author>
<author>Todd Tannenbaum</author>
<author>Miron Livny</author>
</authors>
<title>Distributed computing in practice: the condor experience. Concurrency - Practice and Experience,</title>
<date>2005</date>
<pages>17--2</pages>
<contexts>
<context position="29635" citStr="Thain et al., 2005" startWordPosition="5148" endWordPosition="5151">SVM, S3VM, or MR in all settings when averaged over datasets. We appear to achieve some synergy in dynamically choosing a different algorithm in each trial. In terms of maxF1, Best Tuning, S3VM, and MR are all at least as good as SL in three of the four l,u settings, and nearly as good in the fourth. Based on AUROC, though, the results are mixed depending on the specific setting. Notably, though, Best Tuning consistently leads to worse performance than SL when using this metric. 8.3 A Note on Cloud Computing The experiments were carried out using the Condor High-Throughput Computing platform (Thain et al., 2005). We ran many trials per algorithm (using different datasets, l, u, and metrics). Each trial involved training hundreds of models using different parameter configurations repeated across five folds, and then training once more using the selected parameters. In the end, we trained a grand total of 794,880 individual models to produce the results in Table 2. Through distributed computing on approximately 50 machines in parallel, we were able to run all these experiments in less than a week, while using roughly three months worth of CPU time. 9 Conclusions We have explored “realistic SSL,” where </context>
</contexts>
<marker>Thain, Tannenbaum, Livny, 2005</marker>
<rawString>Douglas Thain, Todd Tannenbaum, and Miron Livny. 2005. Distributed computing in practice: the condor experience. Concurrency - Practice and Experience, 17(2-4):323–356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
</authors>
<title>Semi-supervised learning literature survey.</title>
<date>2005</date>
<tech>Technical Report 1530,</tech>
<institution>Department of Computer Sciences, University of Wisconsin, Madison.</institution>
<contexts>
<context position="1901" citStr="Zhu, 2005" startWordPosition="301" endWordPosition="302">s as small as 10. 1 Introduction Imagine you are a real-world practitioner working on a machine learning problem in natural language processing. If you have unlabeled data, should you use semi-supervised learning (SSL)? Which SSL algorithm should you use? How should you set its parameters? Or could it actually hurt performance, in which case you might be better off with supervised learning (SL)? A large number of SSL algorithms have been developed in recent years that allow one to improve 19 performance with unlabeled data, in tasks such as text classification, sequence labeling, and parsing (Zhu, 2005; Chapelle et al., 2006; Brefeld and Scheffer, 2006). However, many of them are tested on “SSL-friendly” datasets, such as “two moons,” USPS, and MNIST. Furthermore, the algorithms’ parameters are often chosen based on test set performance or manually set based on heuristics and researcher experience. These issues create practical concerns for deploying SSL in the real world. We note that (Chapelle et al., 2006)’s benchmark chapter explores these issues to some extent by comparing several SSL methods on several real and artificial datasets. The authors reach the conclusions that parameter tuni</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>Xiaojin Zhu. 2005. Semi-supervised learning literature survey. Technical Report 1530, Department of Computer Sciences, University of Wisconsin, Madison.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>