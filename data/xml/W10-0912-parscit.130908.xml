<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001188">
<title confidence="0.986492">
Analogical Dialogue Acts: Supporting Learning by Reading Analogies
</title>
<author confidence="0.996132">
David Barbella
</author>
<affiliation confidence="0.9906535">
Qualitative Reasoning Group
Northwestern University
</affiliation>
<address confidence="0.862232">
2133 Sheridan Road, Evanston, IL, USA
</address>
<email confidence="0.997498">
barbella@u.northwestern.edu
</email>
<author confidence="0.980452">
Kenneth D. Forbus
</author>
<affiliation confidence="0.9845835">
Qualitative Reasoning Group
Northwestern University
</affiliation>
<address confidence="0.862529">
2133 Sheridan Road, Evanston, IL, 60201, US
</address>
<email confidence="0.997421">
forbus@northwestern.edu
</email>
<sectionHeader confidence="0.99561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996848368421053">
Analogy is heavily used in written explana-
tions, particularly in instructional texts. We
introduce the concept of analogical dialogue
acts (ADAs) which represent the roles utter-
ances play in instructional analogies. We de-
scribe a catalog of such acts, based on ideas
from structure-mapping theory. We focus on
the operations that these acts lead to while un-
derstanding instructional texts, using the
Structure-Mapping Engine (SME) and dynam-
ic case construction in a computational model.
We test this model on a small corpus of in-
structional analogies, expressed in simplified
English, which were understood via a semi-
automatic natural language system using ana-
logical dialogue acts. The model enabled a
system to answer questions after understand-
ing the analogies that it was not able to answer
without them.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999821">
People use analogy heavily in written explanations.
Instructional texts, for example, use analogy to
convey new concepts and systems of related ideas
to learners. Any learning by reading system must
ultimately include the capability of understanding
such analogies. Here we combine Gentner&apos;s
(1983) structure-mapping theory with ideas from
dialogue act theory (Traum, 2000) to describe a
catalog of analogical dialogue acts (ADAs) which
capture the functional roles that discourse elements
play in instructional analogies. We outline criteria
for identifying ADAs in text and describe what
</bodyText>
<page confidence="0.874984">
96
</page>
<bodyText confidence="0.980413382352941">
operations they suggest for discourse processing.
We provide evidence that this model captures im-
portant aspects of understanding instructional
analogies via a simulation that uses knowledge
gleaned from reading instructional analogies to
answer questions.
We start by reviewing the relevant aspects of
structure-mapping theory and dialogue act theory.
Then we describe our catalog of analogical dialo-
gue acts, based on a theoretical analysis of the
roles structure-mapping operations can play in lan-
guage understanding. A prototype implementation
of these ideas is described next, followed by an
experiment illustrating that these ideas can be used
to understand analogies in text, based on answering
questions. We close with a discussion of related
and future work.
Background
Dialogue act theories (also called speech acts
(Allen &amp; Perrault, 1980)) are concerned with the
roles utterances play in discourse and the effects
they have on the world or on understanding. An
utterance identified as a Requesting Information,
for example, might take the syntactic form of a
question that makes the information requested ex-
plicit, e.g. &amp;quot;What time is it?&amp;quot; The surface manife-
station might instead be a statement, or an indirect
question, e.g. &amp;quot;Do you have the time?&amp;quot; In other
words, its classification is based on its function in
the dialogue and the set of operations it suggests
for the recipient to undertake. We claim that there
exists a set of analogical dialogue acts that are used
in communicating analogies. Like other dialogue
acts, they have criteria by which they can be rec-
</bodyText>
<note confidence="0.984518">
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 96–104,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998029797979798">
ognized, and a set of implied commitments and
obligations for the dialogue participants. This pa-
per focuses on instructional analogies in texts, both
because they are an important phenomenon and
because it allows us to factor out follow-up ques-
tions, making it a useful starting point.
There are a wide variety of dialogue act models,
but all of them include some variation of acts like
Inform (Traum, 2000), which indicate the intent to
describe the state of the world. The analogical di-
alogue acts we discuss here can be viewed as spe-
cializations of Inform.
The organization of analogical dialogue acts fol-
lows directly from the concepts of structure-
mapping theory. In structure-mapping, analogical
matching takes as input two structured, relational
representations, the base and target, and produces
as output one or more mappings. Each mapping
consists of a set of correspondences, identifying
how entities and statements in the base align with
entities and statements in the target. Mappings
include a structural evaluation score providing an
estimate of their overall quality. This estimate is
based on systematicity, i.e., the amount of nested
relational structure in the mapping, especially
higher-order relations that serve as inferential con-
nections between other statements. Causal, logi-
cal, and mathematical statements are all examples
of higher-order relations. Systematicity thus
serves as a local heuristic measure of the explana-
tory promise of a mapping.
Mappings can also contain candidate inferences,
statements in the base that are projected onto the
target, using the correspondences of the mapping.
The candidate inferences represent conjectures
about the target, and constitute a source of analo-
gy&apos;s generative power. Whether or not the candi-
date inferences are in fact correct is evaluated
outside of the matching process. In discourse,
candidate inferences are often used to convey new
information about the target to the learner. Candi-
date inferences can be forward, from base to target,
or reverse, from target to base. Candidate infe-
rences also represent differences between two re-
presentations, when they cannot be consistently
projected from one description to the other.
The Structure-Mapping Engine (SME, Falken-
hainer et al 1989) provides a simulation of analogi-
cal matching. SME typically produces only one
mapping, but can produce a second or third map-
ping if they are sufficiently close to the best map-
ping. SME can accept input about the base and
target incrementally, updating its mappings as new
information becomes available (Forbus et al 1994),
which can be important for modeling the incre-
mental nature of discourse. One cost of SME&apos;s
greedy match algorithm and incremental operation
is that matches can go awry. Consequently, SME
also supports a small set of constraints, optionally
specified as part of the matcher&apos;s input, which
guide it based on task constraints. Here the rele-
vant constraints are those concerning correspon-
dences. That is, given a base item bi and target
item t;, either entities or statements, the following
constraints are defined: required(bi t) means that bi
must correspond to t; in every mapping, and ex-
cluded(bi t) means that bi cannot correspond to t; in
any mapping. The following open constraints are
also defined: requiredBase(bi�, means that some-
thing in every mapping must correspond to bi, with
requiredTarget(t) defined similarly. excluded-
Base(bi� means that bi cannot participate in any
correspondence, with excludedTarget(t) defined
similarly.
An important problem in understanding analogy
in discourse concerns how the representations pro-
vided to SME are constructed. As described be-
low, the representations that constitute an
understanding of the text are produced in our mod-
el via a semi-automatic natural language under-
standing system, which reduces tailorability. In
understanding instructional analogies, a learner is
expected to draw upon their existing world know-
ledge. In some situations, whole cases
representing a prior experience are retrieved from
memory. In other situations, cases seem to be con-
structed dynamically from one&apos;s general know-
ledge of the world. We use dynamic case
construction methods (Mostek et al 2000) to model
this process. In dynamic case construction, a seed
entity or concept is provided as a starting point,
and facts which mention it are gathered, perhaps
filtering by some criterion. For example, &amp;quot;The
economy of India&amp;quot; might have India as its seed,
and facts filtered based on their judged relevance
to economic matters. When a reader is processing
an instructional analogy, we believe that something
like this process is used to create representations to
be used in their understanding of the analogy.
</bodyText>
<page confidence="0.999738">
97
</page>
<sectionHeader confidence="0.992557" genericHeader="method">
3 Analogical Dialogue Acts
</sectionHeader>
<bodyText confidence="0.999772369565218">
Our model of analogical dialog acts is based on an
analysis of how the functional constraints on per-
forming analogical mapping and case construction
interact with the properties of discourse. To carry
out an analogy, a reader must be able to infer that
an analogy is required. They must understand
what goes into the base and what goes into the tar-
get, which can be complex because what is stated
in the text typically needs to be combined with the
reader&apos;s own knowledge. Since readers often
know quite a lot to begin with, figuring out which
subset of what they know is relevant to the analogy
can be complicated. Finally, they have to under-
stand how the author intends the mapping to go,
since there can be multiple mappings between the
same domains. Analogical dialogue acts, we ar-
gue, provide readers with information that they
need to perform these tasks.
Let us examine this process in more detail. To car-
ry out an analogy, the contents of the base and tar-
get representations must be identified. A
fundamental problem is that the reader must figure
out an appropriate construal of the base and target,
i.e., what subset of their knowledge should be
brought to bear in the current comparison? A
reader&apos;s starting knowledge may or may not be
sufficient to guide the mapping process correctly,
in order to reconstruct the mapping that the author
intended. This is especially true in instructional
analogies, of course. We believe that this is why
one commonly finds explicit information about
intended correspondences provided as part of in-
structional analogies. Such information provides a
source of constraints that can be used to guide case
construction and mapping. Similarly, and we be-
lieve for similar reasons, the desired inferences to
be drawn from the analogy are often highlighted.
Since there can be multiple construals (i.e., specific
sets of facts retrieved) for the given base and tar-
get, mentioning candidate inferences explicitly
provides clues to the reader about how to construe
the base and target (i.e., the given candidate infe-
rence should be derivable) as well as information
about its validity.
Next we describe our proposed analogy dialogue
acts. For each act, we give an example, some cri-
teria for identifying them, and describe what opera-
tions a reader might do when they detect such an
act has occurred. At this point our focus has been
on developing the basic set and the operations they
entail, rather than on developing a comprehensive
set of identification criteria. The first three acts are
concerned with introducing the representations to
be compared, and the rest are concerned with cor-
respondences and candidate inferences. We use a
greenhouse/atmosphere analogy as a source of ex-
amples.
Introduce Comparison: Introduces a compari-
son by providing both base and target. For exam-
ple, in &amp;quot;We can understand the greenhouse effect
by comparing it to what goes on in an actual
greenhouse.&amp;quot; the base is a greenhouse, and the tar-
get is the Earth&apos;s atmosphere. Recognizing an In-
troduce Comparison can require combining
information across multiple sentences. In Figure 1,
for example, the target is described in the para-
graph above the point where the comparison is in-
troduced. Sometimes this intent must be inferred
from parallel sentence structure in subsequent sen-
Heat flows from one place to another because the
temperature of the two places is different. A hot
brick loses heat to a cool room. The temperature
difference - the brick&apos;s temperature minus the
room&apos;s temperature — drives the heat from the
brick. Heat leaks from the brick until the tempera-
ture difference is gone. No more heat flows from
the brick when it becomes as cool as the room it is
in.
Similarly, a full can of water will leak volume
from a hole in the side of the can. The depth of the
water is higher than the depth of the hole, so the
depth difference drives volume out through the
hole.
Eventually, all the volume that can leak out does
so. When this happens, the water depth has fallen
so that it is the same as that of the hole. There is
no more depth difference, so no more volume
flows out through the hole. Just as a difference in
temperature causes heat to flow, so a difference in
depth causes volume to flow. When there is no
temperature difference, heat flow ceases; when
there is no depth difference, volume flow ceases.
</bodyText>
<figure confidence="0.979554">
Extend Target
Extend Base
</figure>
<figureCaption confidence="0.990527">
Figure 1: An analogy from our test corpus,
hand-annotated with analogical dialogue acts.
</figureCaption>
<figure confidence="0.9947075">
Introduce Comparison
Candidate Inference
</figure>
<page confidence="0.9986">
98
</page>
<bodyText confidence="0.999790490196079">
tences and other sophisticated rhetorical devices,
while in other cases, like this example, the compar-
ison is introduced explicitly.
What is the base and what is the target requires a
non-local assessment about what the containing
text is about. (This particular example is drawn
from a book on solar energy, and the rest of the
chapter makes clear that heat is the domain being
taught.) Since we assume that candidate inferences
can be constructed bidirectionally, an incorrect
assessment is not catastrophic.
Processing an Introduce Comparison act re-
quires finding appropriate construals of the base
and target. The target, as in this case, is con-
strained by what has already been introduced in the
text. The base, unless it has been used before in
the same text and is being used in a consistent
manner, must be constructed from the reader&apos;s
knowledge. Whether this is done aggressively or
lazily is, we suspect, a strategy that is subject to
individual variation. Ambiguity in linguistic cues
can lead to the need to explore multiple construals,
to find combinations with significant overlap.
Extend Base, Extend Target: These acts add
information to the base or target of a comparison,
respectively. Such acts are identified by relation-
ships and/or entities being mentioned in the same
statement as an entity in the base or target, but
which is not a statement about correspondences or
candidate inferences. For example, &amp;quot;The glass of a
greenhouse lets the short solar rays through.&amp;quot; is
extending the base, and &amp;quot;The earth&apos;s atmosphere
admits most of the solar radiation.&amp;quot; is an example
of extending the target. Entities that are mentioned
in these acts are added to the construal of the case,
if not there already, by retrieving additional know-
ledge about them, focusing on statements involv-
ing other entities in the current construal. If the
specific facts mentioned are not already known to
the reader, they are provisionally accepted as being
true about the base or target, as appropriate.
Introduce Correspondence: These acts provide
clues as to the author&apos;s intended mapping. For
example, &amp;quot;The Earth&apos;s atmosphere is like the glass
in the greenhouse.&amp;quot; indicates that &amp;quot;Earth&apos;s atmos-
phere&amp;quot; corresponds to &amp;quot;glass in greenhouse&amp;quot;. Dis-
tinguishing these acts from introducing a
comparison can be tricky, since &amp;quot;is like&amp;quot; is a syn-
tactic pattern common to both. The first occur-
rence of &amp;quot;is like&amp;quot; in such cases is typically the
introduction of the base and target, with subse-
quent statements introducing correspondences.
Sometimes Introduce Correspondence acts are ex-
pressed as identity statements, e.g. &amp;quot;The glass is
the atmosphere.&amp;quot; Sometimes these acts are sig-
naled by pairs of sentences, one expressing a fact
about the base followed immediately by one about
the target, with identical syntax.
When an Introduce Correspondence act is de-
tected, the base and target are checked to see if
they already contain the entities or relationships
mentioned. If they do not, then the descriptions
are extended to include them. The final step is in-
troducing a required constraint between them as
part of the input to SME. If mappings have al-
ready been generated that are not consistent with
this constraint, they are discarded and new map-
pings are generated.
Block Correspondence: These acts are pro-
vided by the author to block a correspondence that
a reader might otherwise find tempting. An exam-
ple is &amp;quot;The greenhouse door is not like the hole in
the ozone layer.&amp;quot; We believe that these acts are
relatively rare, and especially in written text com-
pared with spoken dialogue, where there are oppor-
tunities for feedback, a matter discussed later.
When both a base and target item are men-
tioned, an exclude constraint is introduced between
them. When only one of them is mentioned, the
minimal operation is to add an open exclusion con-
straint (e.g. excludedBase or excludedTarget). The
reader may decide to simply remove the excluded
item from the construal, along with all of the facts
that mention it. This would prevent it from being
mapped, but it would also prevent it from appear-
ing in any candidate inferences, and hence is more
extreme.
Introduce Candidate Inference: These acts
alert the reader to information that the author in-
tended to convey via the analogy. An example is
&amp;quot;Just as heat is trapped by the greenhouse roof,
heat is trapped by the Earth&apos;s atmosphere.&amp;quot; Phras-
es such as &amp;quot;just as&amp;quot; and &amp;quot;just like&amp;quot;, or even &amp;quot;Like
&lt;base statement to be projected&gt;, &lt;resulting can-
didate inference&gt;.&amp;quot; are clues for identifying such
acts. If the candidate inference can be found in the
mapping that the reader has built up so far, then
that surmise should be given additional weight as
being true. (If it is already known by the reader, it
may already be part of a mapping. This does not
indicate failure, only that it is uninformative for
that reader.) If the candidate inference cannot be
</bodyText>
<page confidence="0.994996">
99
</page>
<figureCaption confidence="0.994783">
Figure 2: Architecture of the experimental prototype. Processes performed by hand are marked with an asterisk.
</figureCaption>
<figure confidence="0.998807878787879">
Recognition
Rules
Source Text
Semantic
Representation
Discourse
Interpretation
Translation*
Build Required
Correspondences
QRG-CE Text
ADA
Hypotheses
EA NLU
Comprehension
Questions
Build Base and
Target
Required
Correspondences
Facts from
Memory
Dynamic Case
Construction
Cases
Translation*
SME
Candidate
Inferences
Question
Answering
Queries
Answers
</figure>
<bodyText confidence="0.999638886363637">
found, then there are several possibilities that a
reader should explore: Their construal of the base
and/or target might be too different from what the
author expects, or they should generate a different
mapping.
It is important to note that whether a statement
combining information from the base and target is
considered an intended correspondence versus an
intended candidate inference depends to some de-
gree on the reader&apos;s state of knowledge. If the tar-
get information is unknown, then for that reader, a
candidate inference is being introduced. A very
active reader may ponder whether it would be a
correspondence for a more informed reader, and
conversely, whether something an active and well-
informed reader views as a correspondence might
have been intended as a candidate inference. In
both cases, considering the alternate classification
would affect the reader&apos;s judgment of informative-
ness, so the distinction between these two types of
acts is useful to make. Candidate inferences
represent the point of the analogy, what it was set
up to convey, and hence distinguishing them seems
important.
Block Candidate Inference: These acts alert
the reader that an inference that they are likely to
make is not in fact correct. For example, &amp;quot;Unlike
solar radiation, radiation heat flow reacts in the
same way to different colors.&amp;quot; If the candidate
inference is part of the reader&apos;s mapping, then
these acts indicate that the reader should mark
them as incorrect. A reader with an aggressive
processing style who did not generate this infe-
rence might explore modifications of their base
and/or target to see if they can generate that infe-
rence, thereby ensuring they are more in sync with
the author&apos;s intentions and thus better able to
process subsequent statements. These acts are
sometimes identifiable by terms such as &amp;quot;unlike,&amp;quot;
&amp;quot;however,&amp;quot; or &amp;quot;you might expect... but&amp;quot; which
include one clause expressing information about
the base and one clause expressing information
about the target. We believe that, like Block Cor-
respondence, these occur relatively infrequently.
</bodyText>
<sectionHeader confidence="0.985801" genericHeader="method">
4 A prototype implementation
</sectionHeader>
<bodyText confidence="0.999698838709677">
To explore the utility of our analogical dialogue
acts theory, we implemented a simple computa-
tional model which uses ADAs to learn from in-
structional texts and answer questions based on
what it has learned, synthesized with what it al-
ready knows (Figure 1). Our model uses the FIRE
reasoning engine, which incorporates SME. The
knowledge base contents are extracted from Re-
searchCyc1 and extended with other knowledge,
including an analogy ontology that lets analogy
operations and other forms of reasoning be freely
mixed (Forbus et al 2002). In addition to the natu-
ral language lexical information built into Re-
searchCyc, we also use the COMLEX lexicon
(Macleod et al 1998) for part of speech and subcat
information. For natural language understanding,
we use EA NLU (Tomai &amp; Forbus, 2009), which
also uses FIRE and the same knowledge base. EA
NLU uses Allen&apos;s (1994) parser for syntactic
processing and construction of initial semantic re-
presentations. It uses Discourse Representation
Theory (Kamp &amp; Reyle, 1993) for dealing with
tense, quotation, logical and numerical quantifica-
tion, and counterfactuals.
EA NLU is useful for this type of learning by
reading experiment because it focuses on generat-
ing rich semantic representations. It does so at the
expense of syntactic coverage: We restrict inputs
syntactically, using QRG-CE (Kuehne &amp; Forbus,
2004), a form of simplified English much like CPL
(Clark et al 2005). For example, complex sen-
</bodyText>
<footnote confidence="0.972036">
1 http://research.cyc.com
</footnote>
<page confidence="0.990989">
100
</page>
<bodyText confidence="0.989035228070176">
tences are broken up into a number of shorter,
simpler sentences. Explicit object references (e.g.
&amp;quot;the greenhouse greenhouse12&amp;quot; every time the
same greenhouse is mentioned) are used to factor
out the difficulty of anaphora resolution. EA NLU
provides facilities for semi-automatic processing;
In this mode, the ambiguities it cannot resolve on
its own are presented as choices to the experimen-
ter. This keeps tailorability low, while allowing
the system to process more complex texts.
As noted above, we do not yet have a robust
model of identification criteria for analogical di-
alogue acts, so we extended EA NLU&apos;s grammar
to have at least one naturally occurring pattern for
every ADA. As part of the translation to QRG-CE,
texts are rewritten to use those patterns when we
view an analogical dialogue act as being present.
This allows the system to automatically classify
ADAs during processing. Here our goal is to mod-
el the processing that must take place once such
acts are recognized, since identifying such acts is
irrelevant if they are not useful for reasoning. EA
NLU&apos;s parsing system produces semantic repre-
sentations used in its discourse interpretation
processing. The ADA recognition rules are used
along with EA NLU&apos;s standard discourse interpre-
tation rules to generate ADA hypotheses as part of
its discourse representations (Figure 1).
We believe that there are significant individual
differences in processing strategies for these acts.
For example, some people seem to be quite aggres-
sive about building up mappings, whereas others
appear to do minimal work. Consequently, we
have started with the simplest possible approach.
Here is what our simulation currently does for each
of the types of acts:
Introduce Comparison: Builds initial con-
struals of the base and the target by retrieving rele-
vant facts from the knowledge base2.
Extend Base/Extend Target: The understand-
ing of the sentence is added to the base or target, as
appropriate. This decision is made by keeping
track of the concepts that are mentioned by state-
ments in each domain, starting with the Introduce
Comparison act.
Introduce Correspondence: A required corres-
pondence constraint is introduced for the entities
2 We use a case constructor similar to CaseFn from Mostek
et al 2000, but including automatic expansion of rule macro
predicates and using microtheory information for filtering.
involved, to be used when SME is run for this
analogy.
Introduce Candidate Inference: The informa-
tion in these statements is simply treated as a fact
about the target domain. We do not currently
change the mapping if a candidate inference in text
is not part of the mapping computed.
</bodyText>
<subsectionHeader confidence="0.523131">
Block Correspondence/Candidate Inference:
</subsectionHeader>
<bodyText confidence="0.99953956">
Not implemented currently, because examples of
these did not show up in our initial corpus.
Analogical dialogue acts are identified via infe-
rence rules that are run over the discourse-level
interpretation that EA NLU produces. Analogical
mapping occurs only at the end of processing a
text, rather than incrementally. Statements about
the base and target are accepted uncritically, rather
than being tested for inconsistencies against back-
ground knowledge. These simplifications
represent one point in the possible space of strate-
gies that people seem likely to use; plans to ex-
plore other strategies are discussed below.
Once the ADA hypotheses are used to construct
the base and target domain and the required cor-
respondences between them, this information is
used by SME to generate candidate inferences -
statements that might be true on the basis of the
analogy constructed. The base and target case are
expanded using dynamic case construction, which
adds knowledge from the KB to fill in information
that the text leaves out. For example, a text may
not explicitly mention that rain falls from the sky
to the earth, taking it as a given that the reader is
aware of this.
</bodyText>
<sectionHeader confidence="0.997058" genericHeader="method">
5 Experiment
</sectionHeader>
<bodyText confidence="0.999746666666666">
An essential test for a theory of analogy dialogue
acts is whether or not it can be used to construct
new knowledge from instructional analogies in
text. To test this, we extracted a small corpus of 6
instructional analogies from a book on solar energy
(Buckley, 1979) and a book on weather (Lehr et al
</bodyText>
<table confidence="0.999167875">
Example #O #A
Gold mining/Collecting solar energy 8 11
Water flow/heat flow 11 12
depth of water in bucket/temperature of house 8 16
Bucket with hole/house leaking heat 4 10
Bucket/Solar collector 5 8
Earth&apos;s atmosphere/greenhouse 7 14
Mean 7.2 11.8
</table>
<tableCaption confidence="0.974803">
Table 1: Corpus Information. #O/#A = # sen-
tences before/after translation to QRG-CE
</tableCaption>
<page confidence="0.997941">
101
</page>
<bodyText confidence="0.999919714285714">
1987). We simplified the syntax of the original
texts into QRG-CE, using the appropriate surface
forms for the analogy dialogue acts that we per-
ceived in the text. One of the analogies is illu-
strated in Figure 1, with part of its translation is
shown in Figure 3. Table 1 summarizes properties
of the original texts and the simplification process.
</bodyText>
<figureCaption confidence="0.788356583333333">
Original: Similarly, a full can of water will leak
volume from a hole in the side of the can.
QRG-CE: A hot brick brick005 is like a can
can001 of water water001. There is a hole hole001
in can can001. The water water001 exits can
can001 through hole hole001.
Figure 3: Example of translation to QRG-CE.
The specific individuals are added to factor out
anaphora processing. Cues to analogical dialo-
gue acts spread across multiple sentences in the
original text are combined into single sentences
during the translation process.
</figureCaption>
<bodyText confidence="0.999938666666667">
To test the effectiveness of knowledge capture,
12 comprehension questions similar to those found
in middle-school science texts were generated by
independent readers of the texts (see Figure 4 for
an example). All questions were designed to re-
quire understanding the analogy in order to answer
them. Moreover, some of the questions require
combining information from the knowledge base
with knowledge gleaned from the text.
</bodyText>
<figure confidence="0.588862444444444">
Question: What disappears as the heat leaks from the
brick?
Predicate calculus version:
(and
(inputsDestroyed ?d ?ourAnswer)
(after-Underspecified ?d ?leaving)
(objectMoving ?leaving heat005)
(isa ?heat ThermalEnergy)
(isa ?leaving LeavingAPlace)
</figure>
<figureCaption confidence="0.90668025">
(fromLocation ?leaving brick005))
Figure 4: A question for the analogy of Figure
1, in English and the hand-generated predicate
calculus generated from it.
</figureCaption>
<bodyText confidence="0.9995399">
Four experimental conditions were run, based
on a 2x2 design here the factors were whether or
not analogy was used (+A) or not used (-A), and
whether what was learned from the text was aug-
mented with information from the knowledge base
(+K) or not (-K).
Table 2 shows the results. The system was able
to answer all twelve questions when it understood
the analogy and combined what it learned by read-
ing with information from the knowledge base.
</bodyText>
<table confidence="0.969291125">
Condition # correct %
-A, -K 0 0
+A, -K 7 58
-A, +K 0 0
+A, +K 1 2 100
Table 2: Results for Q/A. +/- means
with/without, A means analogy, K
means facts retrieved from KB
</table>
<bodyText confidence="0.998673777777778">
That this was due to understanding the analogy can
be seen from the other conditions. The informa-
tion from the text alone is insufficient to answer
any of the questions (-A, -K), as is the information
from the KB alone (-A, +K). Analogy by itself
over what was learned by reading the passages can
handle over half the questions (+A, -K), but the
rest require combining facts learned by reading
with facts from the KB (+A, +K).
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999954333333333">
There has been very little work on modeling anal-
ogies in dialogue. One of the few efforts has been
Lulis &amp; Evans (2003), who examined the use of
analogies by human tutors for potential extensions
to their intelligent tutoring system for cardiac func-
tion. Recently they have begun incorporating
analogies into their tutor (Lulis, Evans, &amp; Michael,
2004), but they have not focused on understanding
novel analogies presented via language.
Because EA NLU is designed to explore issues
of understanding, it is focused more on semantic
coverage than on syntactic coverage. The most
similar system is Boeing&apos;s BLUE (Clark &amp; Harri-
son, 2008), which also uses simplified syntax and
focuses on integrating language with a knowledge
base and reasoning.
Aside from SME, we suspect that the only other
current widely tested model of analogy that might
be able to handle this task is IAM (Keane &amp; Bray-
shaw 1988). CAB (Larkey &amp; Love 2003) does not
model inference, and hence could not model this
task. Although LISA (Hummel &amp; Holyoak, 2003)
can model some analogical inferences, the number
of relations (see Table 3) in these analogies is
beyond the number of relationships it can currently
handle (2 or 3).
The first simulation of analogy to use natural
language input was Winston&apos;s (1982, 1986), which
used a simple domain-specific parser in modeling
the learning of if-then rules and censors. EA NLU
</bodyText>
<page confidence="0.996452">
102
</page>
<bodyText confidence="0.999818333333333">
benefits from subsequent progress in natural lan-
guage research, enabling it to handle a wider range
of phenomena.
</bodyText>
<sectionHeader confidence="0.997227" genericHeader="discussions">
7 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999979675675676">
Modeling the roles that analogy plays in under-
standing language is an important problem in
learning by reading. This paper is an initial explo-
ration of how analogy can be integrated into dialo-
gue act theories, focusing on instructional
analogies in text. We presented a catalog of ana-
logical dialogue acts, based on an analysis of how
the functional constraints of analogical mapping
and case construction interact with the properties
of discourse. We showed that a simulation using
these ideas, combined with a natural language un-
derstanding system to semi-automatically produce
input representations, can indeed learn information
from simplified English analogies, which is en-
couraging evidence for these ideas.
The next step is to expand the corpus substan-
tially, including more examples of all the ADAs, to
better test our model. We also need to implement
the rest of the ADAs, and experiment with a wider
range of processing strategies.
To better model how ADAs can be identified in
natural texts, we plan to use a large-scale web-
based corpus analysis. We have focused on text
here, but we believe that these ideas apply to spo-
ken dialogue as well. We predict more opportuni-
ties for blocking in spoken dialogue, due to
opportunities for feedback.
Our goal is to incorporate these ideas into a 2nd
generation learning by reading system (e.g., Forbus
et al 2007; Forbus et al 2009a), along with other
dialogue processing, to better interpret larger-scale
texts (e.g., Lockwood &amp; Forbus, 2009). This will
be built using the Companions cognitive architec-
ture (Forbus et al 2009b), to more easily model a
wider range of processing strategies, and so that
the system can learn to improve its interpretation
processes.
</bodyText>
<sectionHeader confidence="0.996332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<copyright confidence="0.512015333333333">
This research was supported by the Intelligent and
Autonomous Systems Program of the Office of
Naval Research.
</copyright>
<sectionHeader confidence="0.998419" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9959025">
Allen, J.F. (1994). Natural Language Understanding.
(2nd Ed.) Redwood City, CA: Benjamin/Cummings.
Allen, J. F. &amp; C. R. Perrault (1980). Analyzing Intention
in Utterances. Artificial Intelligence 15(3).
Buckley, S. (1979). From Sun Up to Sun Down. New
York: McGraw-Hill.
Clark, P. &amp; Harrison, P. (2008). Boeing&apos;s NLP system
and the challenges of semantic representation
Clark, P., Harrison, P., Jenkins, T., Thompson, J. &amp;
Wojcik, R. (2005). Acquiring and using world know-
ledge using a restricted subset of English. 18th In-
ternational FLAIRS Conference.
Falkenhainer, B., Forbus, K. &amp; Gentner, D. (1989). The
Structure-Mapping Engine: Algorithms and Exam-
ples. Artificial Intelligence, 41, 1-63.
Forbus, K., Ferguson, R. &amp; Gentner, D. (1994) Incre-
mental structure-mapping. Proceedings of CogSci94.
Forbus, K., Lockwood, K. &amp; Sharma, A. (2009). Steps
towards a 2nd generation learning by reading system.
AAAI Spring Symposium on Learning by Reading,
Spring 2009.
Forbus, K., Klenk, M., &amp; Hinrichs, T. , (2009). Compa-
nion Cognitive Systems: Design Goals and Lessons
Learned So Far. IEEE Intelligent Systems, vol. 24,
no. 4, pp. 36-46, July/August.
Forbus, K., Mostek, T. &amp; Ferguson, R. (2002). An anal-
ogy ontology for integrating analogical processing
and first-principles reasoning. Proceedings of IAAI-
02, July.
Forbus, K. Riesbeck, C., Birnbaum, L., Livingston, K.,
Sharma, A., &amp; Ureel, L. (2007). Integrating natural
language, knowledge representation and reasoning,
and analogical processing to learn by reading. Pro-
ceedings of AAAI-07 Vancouver, BC.
</reference>
<figure confidence="0.967712583333333">
Example #S #BA #BR #TA #TR
Gold mining/Collecting 8 26 32 4 4
solar energy
Water flow/heat flow 11 14 21 13 16
depth of water in buck- 8 12 19 9 12
et/temperature of house
Bucket with hole/house 4 14 20 8 6
leaking heat
Bucket/Solar collector 5 13 15 4 4
Earth&apos;s atmos- 7 12 19 11 14
phere/greenhouse
Mean 7.2 15.2 21 8.2 9.3
</figure>
<figureCaption confidence="0.64657225">
Table 3: Statistics of base and target domains
produced by EA NLU. #S = number of sen-
tences, B/T = Base, Target; A/T =
Attributes/Relations
</figureCaption>
<page confidence="0.997415">
103
</page>
<reference confidence="0.999875462962963">
Gentner, D. (1983). Structure-Mapping: A Theoretical
Framework for Analogy. Cognitive Science, 7: 155-
170.
Gentner, D., Bowdle, B., Wolff, P., &amp; Boronat, C.
(2001). Metaphor is like analogy. In Gentner, D.,
Holyoak, K., and Kokinov, B. (Eds.) The analogical
mind: Perspective from cognitive science. pp. 199-
253, Cambridge, M A: MIT Press.
Hummel, J. E., &amp; Holyoak, K. J. (2003). A symbolic-
connectionist theory of relational inference and gene-
ralization. Psychological Review, 110, 220-264.
Kamp, H. &amp; Reyle, U. (1993). From Discourse to Log-
ic: Introduction to Model-theoretic Semantics of
Natural Language. Kluwer Academic Dordrecht:
Boston.
Keane, M., and Brayshaw, M. (1988). The Incremental
Analogy machine: A computational model of analo-
gy. European Working Session on Learning.
Larkey, L. &amp; Love, B. (2003). CAB: Connectionist
Analogy Builder. Cognitive Science 27,781-794.
Lehr, P. E., Burnett, R. W., &amp; Zim, H. S. (1987).
Weather. New York, NY: Golden Books Publishing
Company, Inc.
Lockwood, K. &amp; Forbus, K. 2009. Multimodal know-
ledge capture from text and diagrams. Proceedings
of KCAP-2009.
Lulis, E. &amp; Evans, M. (2003). The use of analogies in
human tutoring dialogues. AAAI Technical Report
SS-03-06.
Lulis, E., Evans, M. &amp; Michael, J. (2004). Implement-
ing analogies in an electronic tutoring system. In
Lecture Notes in Computer Science, Vol 3220, pp.
228-231, Springer Berlin/Heidelberg.
Macleod, C., Grisham, R., &amp; Meyers, A. (1998).
COMLEX Syntax Reference Manual, Version 3.0.
Linguistic Data Consortium, University of Pennsyl-
vania: Philadelphia, P A.
Mostek, T., Forbus, K, &amp; Meverden, C. (2000). Dynam-
ic case creation and expansion for analogical reason-
ing. Proceedings of AAAI-2000. Austin, TX.
Tomai, E. &amp; Forbus, K. (2009). EA NLU: Practical
Language Understanding for Cognitive Modeling.
Proceedings of the 22nd International Florida Artifi-
cial Intelligence Research Society Conference. Sani-
bel Island, Florida.
Traum, David R. (2000). 20 Questions on Dialogue Act
Taxonomies. Journal of Semantics, 17, 7-30.
Winston, P.H. 1982. Learning new principles from pre-
cedents and exercises. Artificial Intelligence 23(12).
Winston, P. 1986. Learning by augmenting rules and
accumulating censors. In Michalski, R., Carbonell, J.
and Mitchell, T. (Eds.) Machine Learning: An Artifi-
cial Intelligence Approach, Volume 2. Pp. 45-62.
Morgan-Kaufman.
</reference>
<page confidence="0.998778">
104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.074324">
<title confidence="0.999271">Analogical Dialogue Acts: Supporting Learning by Reading Analogies</title>
<author confidence="0.642733">David</author>
<affiliation confidence="0.2880035">Qualitative Reasoning Northwestern</affiliation>
<address confidence="0.828089">2133 Sheridan Road, Evanston, IL,</address>
<email confidence="0.999343">barbella@u.northwestern.edu</email>
<author confidence="0.910078">D Kenneth</author>
<title confidence="0.59086">Qualitative Reasoning Northwestern</title>
<address confidence="0.930025">2133 Sheridan Road, Evanston, IL, 60201,</address>
<email confidence="0.999897">forbus@northwestern.edu</email>
<abstract confidence="0.9994417">Analogy is heavily used in written explanations, particularly in instructional texts. We the concept of dialogue which represent the roles utterances play in instructional analogies. We describe a catalog of such acts, based on ideas from structure-mapping theory. We focus on the operations that these acts lead to while understanding instructional texts, using the Structure-Mapping Engine (SME) and dynamic case construction in a computational model. We test this model on a small corpus of instructional analogies, expressed in simplified English, which were understood via a semiautomatic natural language system using analogical dialogue acts. The model enabled a system to answer questions after understanding the analogies that it was not able to answer without them.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
</authors>
<title>Natural Language Understanding. (2nd Ed.)</title>
<date>1994</date>
<publisher>Benjamin/Cummings.</publisher>
<location>Redwood City, CA:</location>
<marker>Allen, 1994</marker>
<rawString>Allen, J.F. (1994). Natural Language Understanding. (2nd Ed.) Redwood City, CA: Benjamin/Cummings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>C R Perrault</author>
</authors>
<title>Analyzing Intention in Utterances.</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="2614" citStr="Allen &amp; Perrault, 1980" startWordPosition="375" endWordPosition="378">nalogies to answer questions. We start by reviewing the relevant aspects of structure-mapping theory and dialogue act theory. Then we describe our catalog of analogical dialogue acts, based on a theoretical analysis of the roles structure-mapping operations can play in language understanding. A prototype implementation of these ideas is described next, followed by an experiment illustrating that these ideas can be used to understand analogies in text, based on answering questions. We close with a discussion of related and future work. Background Dialogue act theories (also called speech acts (Allen &amp; Perrault, 1980)) are concerned with the roles utterances play in discourse and the effects they have on the world or on understanding. An utterance identified as a Requesting Information, for example, might take the syntactic form of a question that makes the information requested explicit, e.g. &amp;quot;What time is it?&amp;quot; The surface manifestation might instead be a statement, or an indirect question, e.g. &amp;quot;Do you have the time?&amp;quot; In other words, its classification is based on its function in the dialogue and the set of operations it suggests for the recipient to undertake. We claim that there exists a set of analogi</context>
</contexts>
<marker>Allen, Perrault, 1980</marker>
<rawString>Allen, J. F. &amp; C. R. Perrault (1980). Analyzing Intention in Utterances. Artificial Intelligence 15(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buckley</author>
</authors>
<title>From Sun Up to Sun Down.</title>
<date>1979</date>
<publisher>McGraw-Hill.</publisher>
<location>New York:</location>
<contexts>
<context position="25912" citStr="Buckley, 1979" startWordPosition="4143" endWordPosition="4144"> basis of the analogy constructed. The base and target case are expanded using dynamic case construction, which adds knowledge from the KB to fill in information that the text leaves out. For example, a text may not explicitly mention that rain falls from the sky to the earth, taking it as a given that the reader is aware of this. 5 Experiment An essential test for a theory of analogy dialogue acts is whether or not it can be used to construct new knowledge from instructional analogies in text. To test this, we extracted a small corpus of 6 instructional analogies from a book on solar energy (Buckley, 1979) and a book on weather (Lehr et al Example #O #A Gold mining/Collecting solar energy 8 11 Water flow/heat flow 11 12 depth of water in bucket/temperature of house 8 16 Bucket with hole/house leaking heat 4 10 Bucket/Solar collector 5 8 Earth&apos;s atmosphere/greenhouse 7 14 Mean 7.2 11.8 Table 1: Corpus Information. #O/#A = # sentences before/after translation to QRG-CE 101 1987). We simplified the syntax of the original texts into QRG-CE, using the appropriate surface forms for the analogy dialogue acts that we perceived in the text. One of the analogies is illustrated in Figure 1, with part of i</context>
</contexts>
<marker>Buckley, 1979</marker>
<rawString>Buckley, S. (1979). From Sun Up to Sun Down. New York: McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clark</author>
<author>P Harrison</author>
</authors>
<title>Boeing&apos;s NLP system and the challenges of semantic representation</title>
<date>2008</date>
<contexts>
<context position="29679" citStr="Clark &amp; Harrison, 2008" startWordPosition="4769" endWordPosition="4773">been very little work on modeling analogies in dialogue. One of the few efforts has been Lulis &amp; Evans (2003), who examined the use of analogies by human tutors for potential extensions to their intelligent tutoring system for cardiac function. Recently they have begun incorporating analogies into their tutor (Lulis, Evans, &amp; Michael, 2004), but they have not focused on understanding novel analogies presented via language. Because EA NLU is designed to explore issues of understanding, it is focused more on semantic coverage than on syntactic coverage. The most similar system is Boeing&apos;s BLUE (Clark &amp; Harrison, 2008), which also uses simplified syntax and focuses on integrating language with a knowledge base and reasoning. Aside from SME, we suspect that the only other current widely tested model of analogy that might be able to handle this task is IAM (Keane &amp; Brayshaw 1988). CAB (Larkey &amp; Love 2003) does not model inference, and hence could not model this task. Although LISA (Hummel &amp; Holyoak, 2003) can model some analogical inferences, the number of relations (see Table 3) in these analogies is beyond the number of relationships it can currently handle (2 or 3). The first simulation of analogy to use n</context>
</contexts>
<marker>Clark, Harrison, 2008</marker>
<rawString>Clark, P. &amp; Harrison, P. (2008). Boeing&apos;s NLP system and the challenges of semantic representation</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clark</author>
<author>P Harrison</author>
<author>T Jenkins</author>
<author>J Thompson</author>
<author>R Wojcik</author>
</authors>
<title>Acquiring and using world knowledge using a restricted subset of English.</title>
<date>2005</date>
<booktitle>18th International FLAIRS Conference.</booktitle>
<contexts>
<context position="21691" citStr="Clark et al 2005" startWordPosition="3461" endWordPosition="3464">ses FIRE and the same knowledge base. EA NLU uses Allen&apos;s (1994) parser for syntactic processing and construction of initial semantic representations. It uses Discourse Representation Theory (Kamp &amp; Reyle, 1993) for dealing with tense, quotation, logical and numerical quantification, and counterfactuals. EA NLU is useful for this type of learning by reading experiment because it focuses on generating rich semantic representations. It does so at the expense of syntactic coverage: We restrict inputs syntactically, using QRG-CE (Kuehne &amp; Forbus, 2004), a form of simplified English much like CPL (Clark et al 2005). For example, complex sen1 http://research.cyc.com 100 tences are broken up into a number of shorter, simpler sentences. Explicit object references (e.g. &amp;quot;the greenhouse greenhouse12&amp;quot; every time the same greenhouse is mentioned) are used to factor out the difficulty of anaphora resolution. EA NLU provides facilities for semi-automatic processing; In this mode, the ambiguities it cannot resolve on its own are presented as choices to the experimenter. This keeps tailorability low, while allowing the system to process more complex texts. As noted above, we do not yet have a robust model of ident</context>
</contexts>
<marker>Clark, Harrison, Jenkins, Thompson, Wojcik, 2005</marker>
<rawString>Clark, P., Harrison, P., Jenkins, T., Thompson, J. &amp; Wojcik, R. (2005). Acquiring and using world knowledge using a restricted subset of English. 18th International FLAIRS Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Falkenhainer</author>
<author>K Forbus</author>
<author>D Gentner</author>
</authors>
<title>The Structure-Mapping Engine: Algorithms and Examples.</title>
<date>1989</date>
<journal>Artificial Intelligence,</journal>
<volume>41</volume>
<pages>1--63</pages>
<contexts>
<context position="5820" citStr="Falkenhainer et al 1989" startWordPosition="874" endWordPosition="878">ferences represent conjectures about the target, and constitute a source of analogy&apos;s generative power. Whether or not the candidate inferences are in fact correct is evaluated outside of the matching process. In discourse, candidate inferences are often used to convey new information about the target to the learner. Candidate inferences can be forward, from base to target, or reverse, from target to base. Candidate inferences also represent differences between two representations, when they cannot be consistently projected from one description to the other. The Structure-Mapping Engine (SME, Falkenhainer et al 1989) provides a simulation of analogical matching. SME typically produces only one mapping, but can produce a second or third mapping if they are sufficiently close to the best mapping. SME can accept input about the base and target incrementally, updating its mappings as new information becomes available (Forbus et al 1994), which can be important for modeling the incremental nature of discourse. One cost of SME&apos;s greedy match algorithm and incremental operation is that matches can go awry. Consequently, SME also supports a small set of constraints, optionally specified as part of the matcher&apos;s i</context>
</contexts>
<marker>Falkenhainer, Forbus, Gentner, 1989</marker>
<rawString>Falkenhainer, B., Forbus, K. &amp; Gentner, D. (1989). The Structure-Mapping Engine: Algorithms and Examples. Artificial Intelligence, 41, 1-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbus</author>
<author>R Ferguson</author>
<author>D Gentner</author>
</authors>
<title>Incremental structure-mapping.</title>
<date>1994</date>
<booktitle>Proceedings of CogSci94.</booktitle>
<contexts>
<context position="6142" citStr="Forbus et al 1994" startWordPosition="929" endWordPosition="932"> inferences can be forward, from base to target, or reverse, from target to base. Candidate inferences also represent differences between two representations, when they cannot be consistently projected from one description to the other. The Structure-Mapping Engine (SME, Falkenhainer et al 1989) provides a simulation of analogical matching. SME typically produces only one mapping, but can produce a second or third mapping if they are sufficiently close to the best mapping. SME can accept input about the base and target incrementally, updating its mappings as new information becomes available (Forbus et al 1994), which can be important for modeling the incremental nature of discourse. One cost of SME&apos;s greedy match algorithm and incremental operation is that matches can go awry. Consequently, SME also supports a small set of constraints, optionally specified as part of the matcher&apos;s input, which guide it based on task constraints. Here the relevant constraints are those concerning correspondences. That is, given a base item bi and target item t;, either entities or statements, the following constraints are defined: required(bi t) means that bi must correspond to t; in every mapping, and excluded(bi t</context>
</contexts>
<marker>Forbus, Ferguson, Gentner, 1994</marker>
<rawString>Forbus, K., Ferguson, R. &amp; Gentner, D. (1994) Incremental structure-mapping. Proceedings of CogSci94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbus</author>
<author>K Lockwood</author>
<author>A Sharma</author>
</authors>
<title>Steps towards a 2nd generation learning by reading system. AAAI Spring Symposium on Learning by Reading,</title>
<date>2009</date>
<location>Spring</location>
<marker>Forbus, Lockwood, Sharma, 2009</marker>
<rawString>Forbus, K., Lockwood, K. &amp; Sharma, A. (2009). Steps towards a 2nd generation learning by reading system. AAAI Spring Symposium on Learning by Reading, Spring 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbus</author>
<author>M Klenk</author>
<author>T Hinrichs</author>
</authors>
<title>Companion Cognitive Systems: Design Goals and Lessons Learned So Far.</title>
<date>2009</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>24</volume>
<pages>36--46</pages>
<marker>Forbus, Klenk, Hinrichs, 2009</marker>
<rawString>Forbus, K., Klenk, M., &amp; Hinrichs, T. , (2009). Companion Cognitive Systems: Design Goals and Lessons Learned So Far. IEEE Intelligent Systems, vol. 24, no. 4, pp. 36-46, July/August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbus</author>
<author>T Mostek</author>
<author>R Ferguson</author>
</authors>
<title>An analogy ontology for integrating analogical processing and first-principles reasoning.</title>
<date>2002</date>
<booktitle>Proceedings of IAAI02,</booktitle>
<contexts>
<context position="20811" citStr="Forbus et al 2002" startWordPosition="3321" endWordPosition="3324">ondence, these occur relatively infrequently. 4 A prototype implementation To explore the utility of our analogical dialogue acts theory, we implemented a simple computational model which uses ADAs to learn from instructional texts and answer questions based on what it has learned, synthesized with what it already knows (Figure 1). Our model uses the FIRE reasoning engine, which incorporates SME. The knowledge base contents are extracted from ResearchCyc1 and extended with other knowledge, including an analogy ontology that lets analogy operations and other forms of reasoning be freely mixed (Forbus et al 2002). In addition to the natural language lexical information built into ResearchCyc, we also use the COMLEX lexicon (Macleod et al 1998) for part of speech and subcat information. For natural language understanding, we use EA NLU (Tomai &amp; Forbus, 2009), which also uses FIRE and the same knowledge base. EA NLU uses Allen&apos;s (1994) parser for syntactic processing and construction of initial semantic representations. It uses Discourse Representation Theory (Kamp &amp; Reyle, 1993) for dealing with tense, quotation, logical and numerical quantification, and counterfactuals. EA NLU is useful for this type </context>
</contexts>
<marker>Forbus, Mostek, Ferguson, 2002</marker>
<rawString>Forbus, K., Mostek, T. &amp; Ferguson, R. (2002). An analogy ontology for integrating analogical processing and first-principles reasoning. Proceedings of IAAI02, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Riesbeck Forbus</author>
<author>C Birnbaum</author>
<author>L Livingston</author>
<author>K Sharma</author>
<author>A</author>
<author>L Ureel</author>
</authors>
<title>Integrating natural language, knowledge representation and reasoning, and analogical processing to learn by reading.</title>
<date>2007</date>
<booktitle>Proceedings of AAAI-07</booktitle>
<location>Vancouver, BC.</location>
<marker>Forbus, Birnbaum, Livingston, Sharma, A, Ureel, 2007</marker>
<rawString>Forbus, K. Riesbeck, C., Birnbaum, L., Livingston, K., Sharma, A., &amp; Ureel, L. (2007). Integrating natural language, knowledge representation and reasoning, and analogical processing to learn by reading. Proceedings of AAAI-07 Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gentner</author>
</authors>
<title>Structure-Mapping: A Theoretical Framework for Analogy.</title>
<date>1983</date>
<journal>Cognitive Science,</journal>
<volume>7</volume>
<pages>155--170</pages>
<marker>Gentner, 1983</marker>
<rawString>Gentner, D. (1983). Structure-Mapping: A Theoretical Framework for Analogy. Cognitive Science, 7: 155-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gentner</author>
<author>B Bowdle</author>
<author>P Wolff</author>
<author>C Boronat</author>
</authors>
<title>Metaphor is like analogy. In</title>
<date>2001</date>
<pages>199--253</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, M A:</location>
<marker>Gentner, Bowdle, Wolff, Boronat, 2001</marker>
<rawString>Gentner, D., Bowdle, B., Wolff, P., &amp; Boronat, C. (2001). Metaphor is like analogy. In Gentner, D., Holyoak, K., and Kokinov, B. (Eds.) The analogical mind: Perspective from cognitive science. pp. 199-253, Cambridge, M A: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hummel</author>
<author>K J Holyoak</author>
</authors>
<title>A symbolicconnectionist theory of relational inference and generalization.</title>
<date>2003</date>
<journal>Psychological Review,</journal>
<volume>110</volume>
<pages>220--264</pages>
<contexts>
<context position="30071" citStr="Hummel &amp; Holyoak, 2003" startWordPosition="4838" endWordPosition="4841"> analogies presented via language. Because EA NLU is designed to explore issues of understanding, it is focused more on semantic coverage than on syntactic coverage. The most similar system is Boeing&apos;s BLUE (Clark &amp; Harrison, 2008), which also uses simplified syntax and focuses on integrating language with a knowledge base and reasoning. Aside from SME, we suspect that the only other current widely tested model of analogy that might be able to handle this task is IAM (Keane &amp; Brayshaw 1988). CAB (Larkey &amp; Love 2003) does not model inference, and hence could not model this task. Although LISA (Hummel &amp; Holyoak, 2003) can model some analogical inferences, the number of relations (see Table 3) in these analogies is beyond the number of relationships it can currently handle (2 or 3). The first simulation of analogy to use natural language input was Winston&apos;s (1982, 1986), which used a simple domain-specific parser in modeling the learning of if-then rules and censors. EA NLU 102 benefits from subsequent progress in natural language research, enabling it to handle a wider range of phenomena. 7 Discussion and Future Work Modeling the roles that analogy plays in understanding language is an important problem in</context>
</contexts>
<marker>Hummel, Holyoak, 2003</marker>
<rawString>Hummel, J. E., &amp; Holyoak, K. J. (2003). A symbolicconnectionist theory of relational inference and generalization. Psychological Review, 110, 220-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
<author>U Reyle</author>
</authors>
<title>From Discourse to Logic: Introduction to Model-theoretic Semantics of Natural Language.</title>
<date>1993</date>
<publisher>Kluwer Academic</publisher>
<location>Dordrecht: Boston.</location>
<contexts>
<context position="21285" citStr="Kamp &amp; Reyle, 1993" startWordPosition="3397" endWordPosition="3400">with other knowledge, including an analogy ontology that lets analogy operations and other forms of reasoning be freely mixed (Forbus et al 2002). In addition to the natural language lexical information built into ResearchCyc, we also use the COMLEX lexicon (Macleod et al 1998) for part of speech and subcat information. For natural language understanding, we use EA NLU (Tomai &amp; Forbus, 2009), which also uses FIRE and the same knowledge base. EA NLU uses Allen&apos;s (1994) parser for syntactic processing and construction of initial semantic representations. It uses Discourse Representation Theory (Kamp &amp; Reyle, 1993) for dealing with tense, quotation, logical and numerical quantification, and counterfactuals. EA NLU is useful for this type of learning by reading experiment because it focuses on generating rich semantic representations. It does so at the expense of syntactic coverage: We restrict inputs syntactically, using QRG-CE (Kuehne &amp; Forbus, 2004), a form of simplified English much like CPL (Clark et al 2005). For example, complex sen1 http://research.cyc.com 100 tences are broken up into a number of shorter, simpler sentences. Explicit object references (e.g. &amp;quot;the greenhouse greenhouse12&amp;quot; every tim</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Kamp, H. &amp; Reyle, U. (1993). From Discourse to Logic: Introduction to Model-theoretic Semantics of Natural Language. Kluwer Academic Dordrecht: Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Keane</author>
<author>M Brayshaw</author>
</authors>
<title>The Incremental Analogy machine: A computational model of analogy. European Working Session on Learning.</title>
<date>1988</date>
<contexts>
<context position="29943" citStr="Keane &amp; Brayshaw 1988" startWordPosition="4815" endWordPosition="4819">egun incorporating analogies into their tutor (Lulis, Evans, &amp; Michael, 2004), but they have not focused on understanding novel analogies presented via language. Because EA NLU is designed to explore issues of understanding, it is focused more on semantic coverage than on syntactic coverage. The most similar system is Boeing&apos;s BLUE (Clark &amp; Harrison, 2008), which also uses simplified syntax and focuses on integrating language with a knowledge base and reasoning. Aside from SME, we suspect that the only other current widely tested model of analogy that might be able to handle this task is IAM (Keane &amp; Brayshaw 1988). CAB (Larkey &amp; Love 2003) does not model inference, and hence could not model this task. Although LISA (Hummel &amp; Holyoak, 2003) can model some analogical inferences, the number of relations (see Table 3) in these analogies is beyond the number of relationships it can currently handle (2 or 3). The first simulation of analogy to use natural language input was Winston&apos;s (1982, 1986), which used a simple domain-specific parser in modeling the learning of if-then rules and censors. EA NLU 102 benefits from subsequent progress in natural language research, enabling it to handle a wider range of ph</context>
</contexts>
<marker>Keane, Brayshaw, 1988</marker>
<rawString>Keane, M., and Brayshaw, M. (1988). The Incremental Analogy machine: A computational model of analogy. European Working Session on Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Larkey</author>
<author>B Love</author>
</authors>
<date>2003</date>
<journal>CAB: Connectionist Analogy Builder. Cognitive Science</journal>
<pages>27--781</pages>
<contexts>
<context position="29969" citStr="Larkey &amp; Love 2003" startWordPosition="4821" endWordPosition="4824">into their tutor (Lulis, Evans, &amp; Michael, 2004), but they have not focused on understanding novel analogies presented via language. Because EA NLU is designed to explore issues of understanding, it is focused more on semantic coverage than on syntactic coverage. The most similar system is Boeing&apos;s BLUE (Clark &amp; Harrison, 2008), which also uses simplified syntax and focuses on integrating language with a knowledge base and reasoning. Aside from SME, we suspect that the only other current widely tested model of analogy that might be able to handle this task is IAM (Keane &amp; Brayshaw 1988). CAB (Larkey &amp; Love 2003) does not model inference, and hence could not model this task. Although LISA (Hummel &amp; Holyoak, 2003) can model some analogical inferences, the number of relations (see Table 3) in these analogies is beyond the number of relationships it can currently handle (2 or 3). The first simulation of analogy to use natural language input was Winston&apos;s (1982, 1986), which used a simple domain-specific parser in modeling the learning of if-then rules and censors. EA NLU 102 benefits from subsequent progress in natural language research, enabling it to handle a wider range of phenomena. 7 Discussion and </context>
</contexts>
<marker>Larkey, Love, 2003</marker>
<rawString>Larkey, L. &amp; Love, B. (2003). CAB: Connectionist Analogy Builder. Cognitive Science 27,781-794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P E Lehr</author>
<author>R W Burnett</author>
<author>H S Zim</author>
</authors>
<date>1987</date>
<publisher>Golden Books Publishing Company, Inc.</publisher>
<location>Weather. New York, NY:</location>
<marker>Lehr, Burnett, Zim, 1987</marker>
<rawString>Lehr, P. E., Burnett, R. W., &amp; Zim, H. S. (1987). Weather. New York, NY: Golden Books Publishing Company, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lockwood</author>
<author>K Forbus</author>
</authors>
<title>Multimodal knowledge capture from text and diagrams.</title>
<date>2009</date>
<booktitle>Proceedings of KCAP-2009.</booktitle>
<marker>Lockwood, Forbus, 2009</marker>
<rawString>Lockwood, K. &amp; Forbus, K. 2009. Multimodal knowledge capture from text and diagrams. Proceedings of KCAP-2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Lulis</author>
<author>M Evans</author>
</authors>
<title>The use of analogies in human tutoring dialogues. AAAI</title>
<date>2003</date>
<tech>Technical Report SS-03-06.</tech>
<contexts>
<context position="29165" citStr="Lulis &amp; Evans (2003)" startWordPosition="4690" endWordPosition="4693">/without, A means analogy, K means facts retrieved from KB That this was due to understanding the analogy can be seen from the other conditions. The information from the text alone is insufficient to answer any of the questions (-A, -K), as is the information from the KB alone (-A, +K). Analogy by itself over what was learned by reading the passages can handle over half the questions (+A, -K), but the rest require combining facts learned by reading with facts from the KB (+A, +K). 6 Related Work There has been very little work on modeling analogies in dialogue. One of the few efforts has been Lulis &amp; Evans (2003), who examined the use of analogies by human tutors for potential extensions to their intelligent tutoring system for cardiac function. Recently they have begun incorporating analogies into their tutor (Lulis, Evans, &amp; Michael, 2004), but they have not focused on understanding novel analogies presented via language. Because EA NLU is designed to explore issues of understanding, it is focused more on semantic coverage than on syntactic coverage. The most similar system is Boeing&apos;s BLUE (Clark &amp; Harrison, 2008), which also uses simplified syntax and focuses on integrating language with a knowled</context>
</contexts>
<marker>Lulis, Evans, 2003</marker>
<rawString>Lulis, E. &amp; Evans, M. (2003). The use of analogies in human tutoring dialogues. AAAI Technical Report SS-03-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Lulis</author>
<author>M Evans</author>
<author>J Michael</author>
</authors>
<title>Implementing analogies in an electronic tutoring system.</title>
<date>2004</date>
<booktitle>In Lecture Notes in Computer Science,</booktitle>
<volume>3220</volume>
<pages>228--231</pages>
<publisher>Springer Berlin/Heidelberg.</publisher>
<contexts>
<context position="29397" citStr="Lulis, Evans, &amp; Michael, 2004" startWordPosition="4724" endWordPosition="4728">ions (-A, -K), as is the information from the KB alone (-A, +K). Analogy by itself over what was learned by reading the passages can handle over half the questions (+A, -K), but the rest require combining facts learned by reading with facts from the KB (+A, +K). 6 Related Work There has been very little work on modeling analogies in dialogue. One of the few efforts has been Lulis &amp; Evans (2003), who examined the use of analogies by human tutors for potential extensions to their intelligent tutoring system for cardiac function. Recently they have begun incorporating analogies into their tutor (Lulis, Evans, &amp; Michael, 2004), but they have not focused on understanding novel analogies presented via language. Because EA NLU is designed to explore issues of understanding, it is focused more on semantic coverage than on syntactic coverage. The most similar system is Boeing&apos;s BLUE (Clark &amp; Harrison, 2008), which also uses simplified syntax and focuses on integrating language with a knowledge base and reasoning. Aside from SME, we suspect that the only other current widely tested model of analogy that might be able to handle this task is IAM (Keane &amp; Brayshaw 1988). CAB (Larkey &amp; Love 2003) does not model inference, a</context>
</contexts>
<marker>Lulis, Evans, Michael, 2004</marker>
<rawString>Lulis, E., Evans, M. &amp; Michael, J. (2004). Implementing analogies in an electronic tutoring system. In Lecture Notes in Computer Science, Vol 3220, pp. 228-231, Springer Berlin/Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macleod</author>
<author>R Grisham</author>
<author>A Meyers</author>
</authors>
<title>COMLEX Syntax Reference Manual, Version 3.0. Linguistic Data Consortium,</title>
<date>1998</date>
<publisher>A.</publisher>
<location>University of Pennsylvania: Philadelphia, P</location>
<contexts>
<context position="20944" citStr="Macleod et al 1998" startWordPosition="3344" endWordPosition="3347">eory, we implemented a simple computational model which uses ADAs to learn from instructional texts and answer questions based on what it has learned, synthesized with what it already knows (Figure 1). Our model uses the FIRE reasoning engine, which incorporates SME. The knowledge base contents are extracted from ResearchCyc1 and extended with other knowledge, including an analogy ontology that lets analogy operations and other forms of reasoning be freely mixed (Forbus et al 2002). In addition to the natural language lexical information built into ResearchCyc, we also use the COMLEX lexicon (Macleod et al 1998) for part of speech and subcat information. For natural language understanding, we use EA NLU (Tomai &amp; Forbus, 2009), which also uses FIRE and the same knowledge base. EA NLU uses Allen&apos;s (1994) parser for syntactic processing and construction of initial semantic representations. It uses Discourse Representation Theory (Kamp &amp; Reyle, 1993) for dealing with tense, quotation, logical and numerical quantification, and counterfactuals. EA NLU is useful for this type of learning by reading experiment because it focuses on generating rich semantic representations. It does so at the expense of syntac</context>
</contexts>
<marker>Macleod, Grisham, Meyers, 1998</marker>
<rawString>Macleod, C., Grisham, R., &amp; Meyers, A. (1998). COMLEX Syntax Reference Manual, Version 3.0. Linguistic Data Consortium, University of Pennsylvania: Philadelphia, P A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mostek</author>
<author>K Forbus</author>
<author>C Meverden</author>
</authors>
<title>Dynamic case creation and expansion for analogical reasoning.</title>
<date>2000</date>
<booktitle>Proceedings of AAAI-2000.</booktitle>
<location>Austin, TX.</location>
<contexts>
<context position="7773" citStr="Mostek et al 2000" startWordPosition="1179" endWordPosition="1182">the representations provided to SME are constructed. As described below, the representations that constitute an understanding of the text are produced in our model via a semi-automatic natural language understanding system, which reduces tailorability. In understanding instructional analogies, a learner is expected to draw upon their existing world knowledge. In some situations, whole cases representing a prior experience are retrieved from memory. In other situations, cases seem to be constructed dynamically from one&apos;s general knowledge of the world. We use dynamic case construction methods (Mostek et al 2000) to model this process. In dynamic case construction, a seed entity or concept is provided as a starting point, and facts which mention it are gathered, perhaps filtering by some criterion. For example, &amp;quot;The economy of India&amp;quot; might have India as its seed, and facts filtered based on their judged relevance to economic matters. When a reader is processing an instructional analogy, we believe that something like this process is used to create representations to be used in their understanding of the analogy. 97 3 Analogical Dialogue Acts Our model of analogical dialog acts is based on an analysis </context>
<context position="24017" citStr="Mostek et al 2000" startWordPosition="3832" endWordPosition="3835">s what our simulation currently does for each of the types of acts: Introduce Comparison: Builds initial construals of the base and the target by retrieving relevant facts from the knowledge base2. Extend Base/Extend Target: The understanding of the sentence is added to the base or target, as appropriate. This decision is made by keeping track of the concepts that are mentioned by statements in each domain, starting with the Introduce Comparison act. Introduce Correspondence: A required correspondence constraint is introduced for the entities 2 We use a case constructor similar to CaseFn from Mostek et al 2000, but including automatic expansion of rule macro predicates and using microtheory information for filtering. involved, to be used when SME is run for this analogy. Introduce Candidate Inference: The information in these statements is simply treated as a fact about the target domain. We do not currently change the mapping if a candidate inference in text is not part of the mapping computed. Block Correspondence/Candidate Inference: Not implemented currently, because examples of these did not show up in our initial corpus. Analogical dialogue acts are identified via inference rules that are run</context>
</contexts>
<marker>Mostek, Forbus, Meverden, 2000</marker>
<rawString>Mostek, T., Forbus, K, &amp; Meverden, C. (2000). Dynamic case creation and expansion for analogical reasoning. Proceedings of AAAI-2000. Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tomai</author>
<author>K Forbus</author>
</authors>
<title>EA NLU: Practical Language Understanding for Cognitive Modeling.</title>
<date>2009</date>
<booktitle>Proceedings of the 22nd International Florida Artificial Intelligence Research Society Conference.</booktitle>
<location>Sanibel Island, Florida.</location>
<contexts>
<context position="21060" citStr="Tomai &amp; Forbus, 2009" startWordPosition="3363" endWordPosition="3366">stions based on what it has learned, synthesized with what it already knows (Figure 1). Our model uses the FIRE reasoning engine, which incorporates SME. The knowledge base contents are extracted from ResearchCyc1 and extended with other knowledge, including an analogy ontology that lets analogy operations and other forms of reasoning be freely mixed (Forbus et al 2002). In addition to the natural language lexical information built into ResearchCyc, we also use the COMLEX lexicon (Macleod et al 1998) for part of speech and subcat information. For natural language understanding, we use EA NLU (Tomai &amp; Forbus, 2009), which also uses FIRE and the same knowledge base. EA NLU uses Allen&apos;s (1994) parser for syntactic processing and construction of initial semantic representations. It uses Discourse Representation Theory (Kamp &amp; Reyle, 1993) for dealing with tense, quotation, logical and numerical quantification, and counterfactuals. EA NLU is useful for this type of learning by reading experiment because it focuses on generating rich semantic representations. It does so at the expense of syntactic coverage: We restrict inputs syntactically, using QRG-CE (Kuehne &amp; Forbus, 2004), a form of simplified English m</context>
</contexts>
<marker>Tomai, Forbus, 2009</marker>
<rawString>Tomai, E. &amp; Forbus, K. (2009). EA NLU: Practical Language Understanding for Cognitive Modeling. Proceedings of the 22nd International Florida Artificial Intelligence Research Society Conference. Sanibel Island, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
</authors>
<title>20 Questions on Dialogue Act Taxonomies.</title>
<date>2000</date>
<journal>Journal of Semantics,</journal>
<volume>17</volume>
<pages>7--30</pages>
<contexts>
<context position="1544" citStr="Traum, 2000" startWordPosition="219" endWordPosition="220">which were understood via a semiautomatic natural language system using analogical dialogue acts. The model enabled a system to answer questions after understanding the analogies that it was not able to answer without them. 1 Introduction People use analogy heavily in written explanations. Instructional texts, for example, use analogy to convey new concepts and systems of related ideas to learners. Any learning by reading system must ultimately include the capability of understanding such analogies. Here we combine Gentner&apos;s (1983) structure-mapping theory with ideas from dialogue act theory (Traum, 2000) to describe a catalog of analogical dialogue acts (ADAs) which capture the functional roles that discourse elements play in instructional analogies. We outline criteria for identifying ADAs in text and describe what 96 operations they suggest for discourse processing. We provide evidence that this model captures important aspects of understanding instructional analogies via a simulation that uses knowledge gleaned from reading instructional analogies to answer questions. We start by reviewing the relevant aspects of structure-mapping theory and dialogue act theory. Then we describe our catalo</context>
<context position="3967" citStr="Traum, 2000" startWordPosition="597" endWordPosition="598">of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 96–104, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics ognized, and a set of implied commitments and obligations for the dialogue participants. This paper focuses on instructional analogies in texts, both because they are an important phenomenon and because it allows us to factor out follow-up questions, making it a useful starting point. There are a wide variety of dialogue act models, but all of them include some variation of acts like Inform (Traum, 2000), which indicate the intent to describe the state of the world. The analogical dialogue acts we discuss here can be viewed as specializations of Inform. The organization of analogical dialogue acts follows directly from the concepts of structuremapping theory. In structure-mapping, analogical matching takes as input two structured, relational representations, the base and target, and produces as output one or more mappings. Each mapping consists of a set of correspondences, identifying how entities and statements in the base align with entities and statements in the target. Mappings include a </context>
</contexts>
<marker>Traum, 2000</marker>
<rawString>Traum, David R. (2000). 20 Questions on Dialogue Act Taxonomies. Journal of Semantics, 17, 7-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P H Winston</author>
</authors>
<title>Learning new principles from precedents and exercises.</title>
<date>1982</date>
<journal>Artificial Intelligence</journal>
<volume>23</volume>
<issue>12</issue>
<marker>Winston, 1982</marker>
<rawString>Winston, P.H. 1982. Learning new principles from precedents and exercises. Artificial Intelligence 23(12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Winston</author>
</authors>
<title>Learning by augmenting rules and accumulating censors. In</title>
<date>1986</date>
<journal>(Eds.) Machine Learning: An Artificial Intelligence Approach,</journal>
<volume>2</volume>
<pages>45--62</pages>
<publisher>Morgan-Kaufman.</publisher>
<marker>Winston, 1986</marker>
<rawString>Winston, P. 1986. Learning by augmenting rules and accumulating censors. In Michalski, R., Carbonell, J. and Mitchell, T. (Eds.) Machine Learning: An Artificial Intelligence Approach, Volume 2. Pp. 45-62. Morgan-Kaufman.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>