<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.836256">
Machine Translation with a Stochastic Grammatical Channel
</title>
<author confidence="0.713489">
Dekai Wu and Hongsing WONG
</author>
<affiliation confidence="0.8066336">
HKUST
Human Language Technology Center
Department of Computer Science
University of Science and Technology
Clear Water Bay, Hong Kong
</affiliation>
<email confidence="0.996272">
fdekai,wongl@cs.ust.hk
</email>
<sectionHeader confidence="0.994744" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999864045454546">
We introduce a stochastic grammatical channel
model for machine translation, that synthesizes sev-
eral desirable characteristics of both statistical and
grammatical machine translation. As with the
pure statistical translation model described by Wu
(1996) (in which a bracketing transduction gram-
mar models the channel), alternative hypotheses
compete probabilistically, exhaustive search of the
translation hypothesis space can be performed in
polynomial time, and robustness heuristics arise
naturally from a language-independent inversion-
transduction model. However, unlike pure statisti-
cal translation models, the generated output string
is guaranteed to conform to a given target gram-
mar. The model employs only (1) a translation
lexicon, (2) a context-free grammar for the target
language, and (3) a bigram language model. The
fact that no explicit bilingual translation rules are
used makes the model easily portable to a variety of
source languages. Initial experiments show that it
also achieves significant speed gains over our ear-
lier model.
</bodyText>
<sectionHeader confidence="0.984612" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.92464019047619">
Speed of statistical machine translation methods
has long been an issue. A step was taken by
Wu (Wu, 1996) who introduced a polynomial-time
algorithm for the runtime search for an optimal
translation. To achieve this, Wu&apos;s method substi-
tuted a language-independent stochastic bracketing
transduction grammar (SBTG) in place of the sim-
pler word-alignment channel models reviewed in
Section 2. The SBTG channel made exhaustive
search possible through dynamic programming, in-
stead of previous &amp;quot;stack search&amp;quot; heuristics. Trans-
lation accuracy was not compromised, because the
SBTG is apparently flexible enough to model word-
order variation (between English and Chinese) even
though it eliminates large portions of the space of
word alignments. The SBTG can be regarded as
a model of the language-universal hypothesis that
closely related arguments tend to stay together (Wu,
1995a; Wu, 1995b).
In this paper we introduce a generalization of
Wu&apos;s method with the objectives of
</bodyText>
<listItem confidence="0.919019285714286">
1. increasing translation speed further,
2. improving meaning-preservation accuracy,
3. improving grammaticality of the output, and
4. seeding a natural transition toward transduc-
tion rule models,
under the constraint of
• employing no additional knowledge resources
except a grammar for the target language.
To achieve these objectives, we:
• replace Wu&apos;s SBTG channel with a full
stochastic inversion transduction grammar or
SITG channel, discussed in Section 3, and
• (mis-)use the target language grammar as a
SITG, discussed in Section 4.
</listItem>
<bodyText confidence="0.998758263157895">
In Wu&apos;s SBTG method, the burden of generating
grammatical output rests mostly on the bigram lan-
guage model; explicit grammatical knowledge can-
not be used. As a result, output grammaticality can-
not be guaranteed. The advantage is that language-
dependent syntactic knowledge resources are not
needed.
We relax those constraints here by assuming a
good (monolingual) context-free grammar for the
target language. Compared to other knowledge
resources (such as transfer rules or semantic on-
tologies), monolingual syntactic grammars are rel-
atively easy to acquire or construct. We use the
grammar in the SITG channel, while retaining the
bigram language model. The new model facilitates
explicit coding of grammatical knowledge and finer
control over channel probabilities. Like Wu&apos;s SBTG
model, the translation hypothesis space can be ex-
haustively searched in polynomial time, as shown in
</bodyText>
<page confidence="0.9881">
1408
</page>
<bodyText confidence="0.9995115">
Section 5. The experiments discussed in Section 6
show promising results for these directions.
</bodyText>
<sectionHeader confidence="0.963993" genericHeader="introduction">
2 Review: Noisy Channel Model
</sectionHeader>
<bodyText confidence="0.997651277777778">
The statistical translation model introduced by IBM
(Brown et al., 1990) views translation as a noisy
channel process. The underlying generative model
contains a stochastic Chinese (input) sentence gen-
erator whose output is &amp;quot;corrupted&amp;quot; by the transla-
tion channel to produce English (output) sentences.
Assume, as we do throughout this paper, that the
input language is English and the task is to trans-
late into Chinese. In the IBM system, the language
model employs simple n-grams, while the transla-
tion model employs several sets of parameters as
discussed below. Estimation of the parameters has
been described elsewhere (Brown et al., 1993).
Translation is performed in the reverse direction
from generation, as usual for recognition under gen-
erative models. For each English sentence e to be
translated, the system attempts to find the Chinese
sentence c* such that:
</bodyText>
<equation confidence="0.864885">
c* = argmax Pr(cle) = argmaxPr(elc) Pr(c) (1)
</equation>
<bodyText confidence="0.999462416666667">
In the IBM model, the search for the optimal c* is
performed using a best-first heuristic &amp;quot;stack search&amp;quot;
similar to A* methods.
One of the primary obstacles to making the statis-
tical translation approach practical is slow speed of
translation, as performed in A* fashion. This price
is paid for the robustness that is obtained by using
very flexible language and translation models. The
language model allows sentences of arbitrary or-
der and the translation model allows arbitrary word-
order permutation. No structural constraints and
explicit linguistic grammars are imposed by this
model.
The translation channel is characterized by two
sets of parameters: translation and alignment prob-
abilities.1 The translation probabilities describe lex-
ical substitution, while alignment probabilities de-
scribe word-order permutation. The key problem
is that the formulation of alignment probabilities
a(ijj,17,T) permits the English word in position j
of a length-T sentence to map to any position i of a
length-V Chinese sentence. So VT alignments are
possible, yielding an exponential space with corre-
spondingly slow search times.
</bodyText>
<footnote confidence="0.6715515">
&apos;Various models have been constructed by the IBM team
(Brown et al., 1993). This description corresponds to one of the
simplest ones, &amp;quot;Model 2&amp;quot;; search costs for the more complex
models are correspondingly higher.
</footnote>
<sectionHeader confidence="0.987183" genericHeader="method">
3 A SITG Channel Model
</sectionHeader>
<bodyText confidence="0.979293">
The translation channel we propose is based on
the recently introduced bilingual language model-
ing approach. The model employs a stochastic ver-
sion of an inversion transduction grammar or ITG
(Wu, 1995c; Wu, 1995d; Wu, 1997). This formal-
ism was originally developed for the purpose of par-
allel corpus annotation, with applications for brack-
eting, alignment, and segmentation. Subsequently,
a method was developed to use a special case of the
ITG—the aforementioned BTG—for the translation
task itself (Wu, 1996). The next few paragraphs
briefly review the main properties of ITGs, before
we describe the SITG channel.
An ITG consists of context-free productions
where terminal symbols come in couples, for ex-
ample xly, where x is a English word and y is an
Chinese translation of x, with singletons of the form
x/f or El y representing function words that are used
in only one of the languages. Any parse tree thus
generates both English and Chinese strings simulta-
neously. Thus, the tree:
</bodyText>
<listItem confidence="0.989410727272727">
(1) [IJ N [Rook/*7 [a/— f/* book/6 ]Np 1vp
[for/8,i you/4]pp NI, Is
produces, for example, the mutual translations:
(2) a. [fl [[*7 [—*]NP 1vP [tiVZ]pp lvP Is
b. [I [[took [a book]Np ]vp [for you] pp bip Is
An additional mechanism accommodates a con-
servative degree of word-order variation between
the two languages. With each production of the
grammar is associated either a straight orientation
or an inverted orientation, respectively denoted as
follows: VP —&gt; [VP PP]
</listItem>
<equation confidence="0.629526">
VP -4 (VP PP)
</equation>
<bodyText confidence="0.999928333333333">
In the case of a production with straight orien-
tation, the right-hand-side symbols are visited left-
to-right for both the English and Chinese streams.
But for a production with inverted orientation, the
right-hand-side symbols are visited left-to-right for
English and right-to-left for Chinese. Thus, the tree:
</bodyText>
<listItem confidence="0.642908636363636">
(3) WTI (Rook/WT [a/— €/* book/]NP IvP
[for/M you/4]pp)VP Is
produces translations with different word order:
(4) a. [I [[took [a book]Np hip [for you]pp Iv p h
b. [ft [[16g4}pp [*7 [—*INP IVP NP Is
The surprising ability of ITGs to accommodate
nearly all word-order variation between fixed-word-
order languages2 (English and Chinese in particu-
lar), has been analyzed mathematically, linguisti-
2With the exception of higher-order phenomena such as
neg-raising and wh-movement.
</listItem>
<page confidence="0.990419">
1409
</page>
<bodyText confidence="0.999450916666667">
cally, and experimentally (Wu, 1995b; Wu, 1997).
Any ITG can be transformed to an equivalent
binary-branching normal form.
A stochastic ITG associates a probability with
each production. It follows that a SITG assigns
a probability Pr(e, c, q) to all generable trees q
and sentence-pairs. In principle it can be used as
the translation channel model by normalizing with
Pr(c) and integrating out Pr(q) to give Pr(eIc) in
Equation (1). In practice, a strong language model
makes this unnecessary, so we can instead optimize
the simpler Viterbi approximation
</bodyText>
<equation confidence="0.906151">
c* = argmax Pr(e, c, q) Pr(c) (2)
</equation>
<bodyText confidence="0.831332733333333">
To complete the picture we add a bigram model
= g(cj I c3_1) for the Chinese language
model Pr(c).
This approach was used for the SBTG chan-
nel (Wu, 1996), using the language-independent
bracketing degenerate case of the SITG:3
A Aby) [AA] Vx,y lexical translations
A a[] (AA) Vx language 1 vocabulary
A a() xly Vy language 2 vocabulary
A -4 x/E
b(x,€) EiY
NE,Y)
—&gt;
In the proposed model, a structured language-
dependent ITG is used instead.
</bodyText>
<sectionHeader confidence="0.981071" genericHeader="method">
4 A Grammatical Channel Model
</sectionHeader>
<bodyText confidence="0.955917">
Stated radically, our novel modeling thesis is that
a mirrored version of the target language grammar
can parse sentences of the source language.
Ideally, an ITG would be tailored for the desired
source and target languages, enumerating the trans-
duction patterns specific to that language pair. Con-
structing such an ITG, however, requires massive
manual labor effort for each language pair. Instead,
our approach is to take a more readily acquired
monolingual context-free grammar for the target
language, and use (or perhaps misuse) it in the SITG
channel, by employing the three tactics described
below: production mirroring, part-of-speech map-
ping, and word skipping.
In the following, keep in mind our convention
that language 1 is the source (English), while lan-
guage 2 is the target (Chinese).
3Wu (Wu, 1996) experimented with Chinese-English trans-
lation, while this paper experiments with English-Chinese
translation.
</bodyText>
<equation confidence="0.588479833333333">
S NP VP Punc
VP V NP
NP -+ N Mod N I Prn
S [NP VP Punc] I (Punc VP NP)
VP --+ [V NP] I (NP V)
NP [N Mod N] I (N Mod N) I [Prn]
</equation>
<figureCaption confidence="0.996551">
Figure 1: An input CFG and its mirrored ITG.
</figureCaption>
<subsectionHeader confidence="0.998455">
4.1 Production Mirroring
</subsectionHeader>
<bodyText confidence="0.971099">
The first step is to convert the monolingual Chi-
nese CFG to a bilingual ITG. The production mir-
roring tactic simply doubles the number of pro-
ductions, transforming every monolingual produc-
tion into two bilingual productions,4 one straight
and one inverted, as for example in Figure 1 where
the upper Chinese CFG becomes the lower ITG.
The intent of the mirroring is to add enough flex-
ibility to allow parsing of English sentences using
the language 1 side of the ITG. The extra produc-
tions accommodate reversed subconstituent order in
the source language&apos;s constituents, at the same time
restricting the language 2 output sentence to con-
form the given target grammar whether straight or
inverted productions are used.
The following example illustrates how produc-
tion mirroring works. Consider the input sentence
He is the son of Stephen, which can be parsed by
the ITG of Figure 1 to yield the corresponding out-
put sentenceft &apos;115-4-31:Erg927-, with the following
parse tree:
(5) [[[He/ft Jp„,]Np [[is/ Iv [the/E]NoisE
([son/92 IN [of/Ergimod [Stephen/54 3:1- IN
)NP Iv [Jo 1Punc Is
Production mirroring produced the inverted NP
constituent which was necessary to parse son of
Stephen, i.e., (son/ 92,T °fin Stephen/sU631 )Np.
If the target CFG is purely binary branching,
then the previous theoretical and linguistic analy-
ses (Wu, 1997) suggest that much of the requisite
constituent and word order transposition may be ac-
commodated without change to the mirrored ITG.
On the other hand, if the target CFG contains pro-
ductions with long right-hand-sides, then merely in-
verting the subconstituent order will probably be in-
sufficient. In such cases, a more complex transfor-
mation heuristic would be needed.
Objective 3 (improving grammaticality of the
output) can be directly tackled by using a tight tar-
</bodyText>
<footnote confidence="0.8272375">
4Except for unary productions, which yield only one bilin-
gual production.
</footnote>
<page confidence="0.982344">
1410
</page>
<bodyText confidence="0.999877318181818">
get grammar. To see this, consider using a mir-
rored Chinese CFG to parse English sentences with
the language 1 side of the ITG. Any resulting parse
tree must be consistent with the original Chinese
grammar. This follows from the fact that both the
straight and inverted versions of a production have
language 2 (Chinese) sides identical to the original
monolingual production: inverting production ori-
entation cancels out the mirroring of the right-hand-
side symbols. Thus, the output grammaticality de-
pends directly on the tightness of the original Chi-
nese grammar.
In principle, with this approach a single tar-
get grammar could be used for translation from
any number of other (fixed word-order) source lan-
guages, so long as a translation lexicon is available
for each source language.
Probabilities on the mirrored ITG cannot be re-
liably estimated from bilingual data without a very
large parallel corpus. A straightforward approxima-
tion is to employ EM or Viterbi training on just a
monolingual target language (Chinese) corpus.
</bodyText>
<subsectionHeader confidence="0.995401">
4.2 Part-of-Speech Mapping
</subsectionHeader>
<bodyText confidence="0.999978521739131">
The second problem is that the part-of-speech (PoS)
categories used by the target (Chinese) grammar do
not correspond to the source (English) words when
the source sentence is parsed. It is unlikely that any
English lexicon will list Chinese parts-of-speech.
We employ a simple part-of-speech mapping
technique that allows the PoS tag of any corre-
sponding word in the target language (as found in
the translation lexicon) to serve as a proxy for the
source word&apos;s PoS. The word view, for example,
may be tagged with the Chinese tags nc and vn,
since the translation lexicon holds both viewNN/S,
Eric and viewviiitita vn •
Unknown English words must be handled differ-
ently since they cannot be looked up in the transla-
tion lexicon. The English PoS tag is first found by
tagging the English sentence. A set of possible cor-
responding Chinese PoS tags is then found by table
lookup (using a small hand-constructed mapping ta-
ble). For example, NN may map to nc, loc and pref,
while VB may map to vi, vn, vp, vv, vs, etc. This
method generates many hypotheses and should only
be used as a last resort.
</bodyText>
<subsectionHeader confidence="0.999214">
4.3 Word Skipping
</subsectionHeader>
<bodyText confidence="0.99997445">
Regardless of how constituent-order transposition is
handled, some function words simply do not oc-
cur in both languages, for example Chinese aspect
markers. This is the rationale for the singletons
mentioned in Section 3.
If we create an explicit singleton hypothesis for
every possible input word, the resulting search
space will be too large. To recognize singletons, we
instead borrow the word-skipping technique from
speech recognition and robust parsing. As formal-
ized in the next section, we can do this by modifying
the item extension step in our chart-parser-like algo-
rithm. When the dot of an item is on the rightmost
position, we can use such constituent, a subtree, to
extend other items. In chart parsing, the valid sub-
trees that can be used to extend an item are those
that are located on the adjacent right of the dot po-
sition of the item and the anticipated category of the
item should also be equal to that of the subtrees.
If word-skipping is to be used, the valid subtrees
can be located a few positions right (or, left for the
item corresponding to inverted production) to the
dot position of the item. In other words, words be-
tween the dot position and the start of the subtee are
skipped, and considered to be singletons.
Consider Sentence 5 again. Word-skipping han-
dled the the which has no Chinese counterpart. At a
certain point during translation, we have the follow-
ing item: VP —4is/Cv •NP. With word-skipping,
it can be extended to VP -÷[is/]vNP. by the sub-
tree (son/ 52,T of/n Stephen/Z*4 )Np, even the
subtree is not adjacent (but within a certain distance,
see Section 5) to the dot position of the item. The
the located on the adjacent to the dot position of the
item is skipped.
Word-skipping provides us the flexibility to parse
the source input by skipping possible singleton(s),
if when we doing so, the source input can be parsed
with the highest likelihood, and grammatical output
can be produced.
</bodyText>
<sectionHeader confidence="0.989357" genericHeader="method">
5 Translation Algorithm
</sectionHeader>
<bodyText confidence="0.99984725">
The translation search algorithm differs from that of
Wu&apos;s SBTG model in that it handles arbitrary gram-
mars rather than binary bracketing grammars. As
such it is more similar to active chart parsing (Ear-
ley, 1970) rather than CYK parsing (Kasami, 1965;
Younger, 1967). We take the standard notion of
items (Aho and Ullman, 1972), and use the term an-
ticipation to mean an item which still has symbols
right of its dot. Items that don&apos;t have any symbols
right of the dot are called subtree.
As with Wu&apos;s SBTG model, the algorithm max-
imizes a probabilistic objective function, Equa-
</bodyText>
<page confidence="0.98167">
1411
</page>
<bodyText confidence="0.999165777777778">
tion (2), using dynamic programming similar to that
for HMM recognition (Viterbi, 1967). The presence
of the bigram model in the objective function ne-
cessitates indexes in the recurrence not only on sub-
trees over the source English string, but also on the
delimiting words of the target Chinese substrings.
The dynamic programming exploits a recursive
formulation of the objective function as follows.
Some notation remarks: e,..t denotes the subse-
</bodyText>
<figureCaption confidence="0.948941">
quence of English tokens es+i , ,et. We
use C(s..t) to denote the set of Chinese words that
are translations of the English word created by tak-
ing all tokens in es..t together. C(s, t) denotes the
set of Chinese words that are translations of any of
the English words anywhere within es..t. K is the
maximium number of consecutive English words
that can be skipped.5 Finally, the argmax operator is
generalized to vector notation to accommodate mul-
tiple indices.
</figureCaption>
<figure confidence="0.770292">
1. Initialization
0&lt;s&lt;t&lt;T
estyy = bi(es..t IY), YE C(s..t)
r is Y&apos;s PoS
2. Recursion
For all r,s,t, u, v such that
{
</figure>
<figureCaption confidence="0.178068">
r is the category of a constituent spanning s to t
</figureCaption>
<equation confidence="0.987445078947368">
0&lt;s&lt;t&lt;T .
u, v are the leftmost/rightmost words of the constituent
maxi6rfistuv 1 879.stuv, 8rostud
{{] if jr[j tuv &gt; MaX[8r()tut 0 8r0stuv]
0 if 6.795stuv &gt; MaS{6r[fstuv,8rOstud
0 otherwise
where6
6Pstuv = max a1(r) JJ 8r,s,t,u,v gu.ti.+1
i.0
[l
rstov
A.qo =-- argmax a( r) 116r,s,t,u,v,gv,u,41.
I) i=0
Xq
5In our experiments, K was set to 4
6s0 = s, sr, = t, 210 = u,--= v, gonu„1.1 = gvn+iun =
1, q = (r,sit,uivi)
6.o ,, tut, = max a(r) fl 6rstu,v g.,+,u,
5,&lt;t,&lt;s■+1 i=0
- &apos;rstuv
- Xqr,
X90
0
0
r-*(r0.-r0
argmax a(r) JJ Oro,t,u,v,9,.+0.4,
i=o
3. Reconstruction
Let go = (S, 0, T, u, v) be the optimal root. where
(u, v) = maxu,vEc(0,T) 8s stu v For any child of
q = (r, s, t, u, v) is given by:
CHILD (q, r) = ITU ,[i
NIL
&apos;9 Ar,s,t,u,v,,
Aris,t,u,v,1
otherwise
if yq == 0
if&amp;quot;6 == []
</equation>
<bodyText confidence="0.9943904375">
Assuming the number of translation per word is
bounded by some constant, then the maximum size
of C&apos;(8, t) is proportional to t — The asymptotic
time complexity for our algorithm is thus bounded
by 0(P). However, note that in theory the com-
plexity upper bound rises exponentially rather than
polynomially with the size of the grammar, just
as for context-free parsing (Barton et al., 1987),
whereas this is not a problem for Wu&apos;s SBTG algo-
rithm. In practice, natural language grammars are
usually sufficiently constrained so that speed is ac-
tually improved over the SBTG algorithm, as dis-
cussed later.
The dynamic programming is efficiently im-
plemented by an active-chart-parser-style agenda-
based algorithm, sketched as follows:
</bodyText>
<listItem confidence="0.517355727272727">
1. Initialization For each word in the input sentence, put a
subtree with category equal to the PoS of its translation
into the agenda.
2. Recursion Loop while agenda is not empty:
(a) If the current item is a subtree of category X, ex-
tend existing anticipations by calling ANTICIPA-
TIONEXTENSION. For each rule in the grammar
of Z -4 XW . . Y, add an initial anticipation of
the form Z X • W Y and put it into the
agenda. Add subtree X to the chart.
(b) If the current item is an anticipation of the form
</listItem>
<bodyText confidence="0.6795695">
Z W ....X Y from s to to, find all subtrees
in the chart with category X that start at position t1
and use each subtree to extend this anticipation by
calling ANTICIPATIONEXTENSION.
</bodyText>
<footnote confidence="0.889676">
ANTICIPATIONEXTENS1ON : Assuming the subtree we
found is of category X from position si to t, for any
</footnote>
<bodyText confidence="0.945077">
anticipation of the form Z W ... • X... Y from so
to [si — K, s j, extend it to Z TV... X • ... Y with
span from so to t and add it to the agenda.
</bodyText>
<equation confidence="0.671984">
Srstuv =
&apos;Yr shy)
</equation>
<page confidence="0.976069">
1412
</page>
<listItem confidence="0.391498">
3. Reconstruction The output string is recursively recon-
structed from the highest likelihood subtree, with cate-
gory S. that span the whole input sentence.
</listItem>
<sectionHeader confidence="0.998672" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999978531914893">
The grammatical channel was tested in the SILC
translation system. The translation lexicon was
partly constructed by training on government tran-
scripts from the HKUST English-Chinese Paral-
lel Bilingual Corpus, and partly entered by hand.
The corpus was sentence-aligned statistically (Wu,
1994); Chinese words and collocations were ex-
tracted (Fung and Wu, 1994; Wu and Fung, 1994);
then translation pairs were learned via an EM pro-
cedure (Wu and Xia, 1995). Together with hand-
constructed entries, the resulting English vocabu-
lary is approximately 9,500 words and the Chinese
vocabulary is approximately 14,500 words, with a
many-to-many translation mapping averaging 2.56
Chinese translations per English word. Since the
lexicon&apos;s content is mixed, we approximate transla-
tion probabilities by using the unigram distribution
of the target vocabulary from a small monolingual
corpus. Noise still exists in the lexicon.
The Chinese grammar we used is not tight—
it was written for robust parsing purposes, and as
such it over-generates. Because of this we have not
yet been able to conduct a fair quantitative assess-
ment of objective 3. Our productions were con-
structed with reference to a standard grammar (Bei-
jing Language and Culture Univ., 1996) and totalled
316 productions. Not all the original productions
are mirrored, since some (128) are unary produc-
tions, and others are Chinese-specific lexical con-
structions like S S NP S, which are
obviously unnecessary to handle English. About
27.7% of the non-unary Chinese productions were
mirrored and the total number of productions in the
final ITG is 368.
For the experiment, 222 English sentences with
a maximum length of 20 words from the parallel
corpus were randomly selected. Some examples of
the output are shown in Figure 2. No morphological
processing has been used to correct the output, and
up to now we have only been testing with a bigram
model trained on extremely small corpus.
With respect to objective 1 (increasing translation
speed), the new model is very encouraging. Ta-
ble 1 shows that over 90% of the samples can be
processed within one minute by the grammatical
channel model, whereas that for the SBTG channel
model is about 50%. This demonstrates the stronger
</bodyText>
<table confidence="0.999322">
Time SBTG Grammatical
(x) Channel Channel
x &lt; 30 secs. 15.6% 83.3%
30 secs. &lt; x &lt; 1 mm. 34.9% 7.6%
x &gt; 1 min. 49.5% 9.1%
</table>
<tableCaption confidence="0.999112">
Table 1: Translation speed.
</tableCaption>
<table confidence="0.9996725">
Sentence meaning SBTG Grammatical
preservation Channel Channel
Correct 25.9% 32.3%
Incorrect - 74.1% 67.7%
</table>
<tableCaption confidence="0.999669">
Table 2: Translation accuracy.
</tableCaption>
<bodyText confidence="0.998302210526316">
constraints on the search space given by the SITG.
The natural trade-off is that constraining the
structure of the input decreases robustness some-
what. Approximately 13% of the test corpus could
not be parsed in the grammatical channel model.
As mentioned earlier, this figure is likely to vary
widely depending on the characteristics of the tar-
get grammar. Of course, one can simply back off
to the SBTG model when the grammatical channel
rejects an input sentence.
With respect to objective 2 (improving meaning-
preservation accuracy), the new model is also
promising. Table 2 shows that the percentage of
meaningfully translated sentences rises from 26% to
32% (ignoring the rejected cases).7 We have judged
only whether the correct meaning is conveyed by the
translation, paying particular attention to word order
and grammaticality, but otherwise ignoring morpho-
logical and function word choices.
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9998889">
Currently we are designing a tight generation-
oriented Chinese grammar to replace our robust
parsing-oriented grammar. We will use the new
grammar to quantitatively evaluate objective 3. We
are also studying complementary approaches to
the English word deletion performed by word-
skipping—i.e., extensions that insert Chinese words
suggested by the target grammar into the output.
The framework seeds a natural transition toward
pattern-based translation models (objective 4). One
</bodyText>
<footnote confidence="0.787284666666667">
7These accuracy rates are relatively low because these ex-
periments are being conducted with new lexicons and grammar
on a new translation direction (English-Chinese).
</footnote>
<page confidence="0.976108">
1413
</page>
<bodyText confidence="0.999993965517242">
can post-edit the productions of a mirrored SITG
more carefully and extensively than we have done
in our cursory pruning, gradually transforming the
original monolingual productions into a set of true
transduction rule patterns. This provides a smooth
evolution from a purely statistical model toward a
hybrid model, as more linguistic resources become
available.
We have described a new stochastic grammati-
cal channel model for statistical machine translation
that exhibits several nice properties in comparison
with Wu&apos;s SBTG model and IBM&apos;s word alignment
model. The SITG-based channel increases trans-
lation speed, improves meaning-preservation accu-
racy, permits tight target CFGs to be incorporated
for improving output grammaticality, and suggests
a natural evolution toward transduction rule mod-
els. The input CFG is adapted for use via produc-
tion mirroring, part-of-speech mapping, and word-
skipping. We gave a polynomial-time translation
algorithm that requires only a translation lexicon,
plus a CFG and bigram language model for the tar-
get language. More linguistic knowledge about the
target language is employed than in pure statisti-
cal translation models, but Wu&apos;s SBTG polynomial-
time bound on search cost is retained and in fact the
search space can be significantly reduced by using
a good grammar. Output always conforms to the
given target grammar.
</bodyText>
<sectionHeader confidence="0.997982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.4937705">
Thanks to the SILC group members: Xuanyin Xia, Daniel
Chan, Aboy Wong, Vincent Chow &amp; James Pang.
</bodyText>
<sectionHeader confidence="0.989405" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995234430379747">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing.
Translation, and Compiling. Prentice Hall, Englewood Cliffs, NJ.
G. Edward Barton, Robert C. Berwick, and Eric. S Ristad. 1987. Com-
putational Complexity and Natural Language. MIT Press, Cam-
bridge, MA.
Beijing Language and Culture Univ.. 1996. Sucheng Hanyu Chuji
Jiaocheng (A Short Intensive Elementary Chinese Course), volume
1-4. Beijing Language And Culture Univ. Press,
Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vincent J. Del-
laPietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and
Paul S. Roossin. 1990. A statistical approach to machine transla-
tion. Computational Linguistics, 16(2):29-85.
Peter F. Brown, Stephen A. DellaPietra, Vincent J. DellaPietra, and
Robert L. Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computational Linguis-
tics, 19(2):263-311.
Jay Earley. 1970. An efficient context-free parsing algorithm. Com-
munications of the Assoc. for Computing Machinery, I 3(2):94—IO2.
Pascale Fung and Dekai Wu. 1994. Statistical augmentation of a Chi-
nese machine-readable dictionary. In Proc. of the 2nd Annual Work-
shop on Very Large Corpora, pg 69-85, Kyoto, Aug.
Input : I entirely agree with this point of view.
Output: ItttlitaffiraMo
Corpus: tlfsEMITSMOSA.
Input : This would create a tremendous financial
burden to taxpayers in Hong Kong.
Output: ?ICA 14glitt AABBO
Corpus: iftl4gititErAttAiIi32,Ingtgiftitlo
Input : The Government wants, and will work for, the
best education for all the children of Hong Kong.
Output: canoe r-vtigiarzsifoguiraTnA
itA illS2,20
Corpus, OZA:a5tV*itfifi-92,Itl3figtg /PT Ws]
CM, &amp;€1.*OtINVIIVItOo
Input : Let me repeat one simple point yet again.
Output: UtlIR ri tg 0
Corpus: 21101NEIff-11448117frgWM0
Input : We are very disappointed.
Output: Rill IMW
Corpus: #111#111t1T%#,
Figure 2: Example translation outputs from the
grammatical channel model.
T. Kasami. 1965. An efficient recognition and syntax analysis al-
gorithm for context-free languages. Technical Report AFCRL-65-
758, Air Force Cambridge Research Lab., Bedford, MA.
Andrew J. Viterbi. 1967. Error bounds for convolutional codes and an
asymptotically optimal decoding algorithm. IEEE Transactions on
Information Theory, 13:260-269.
Dekai Wu and Pascale Fung. 1994. Improving Chinese tokenization
with linguistic filters on statistical lexical acquisition. In Proc. of
4th Conf on ANLP, pg 180-181, Stuttgart, Oct.
Dekai Wu and Xuanyin Xia. 1995. Large-scale automatic extraction
of an English-Chinese lexicon. Machine Translation, 9(3-4):285-
313.
Dekai Wu. 1994. Aligning a parallel English-Chinese corpus statisti-
cally with lexical criteria. In Proc. of 32nd Annual Conf of Assoc.
for Computational Linguistics, pg 80-87, Las Cruces, Jun.
Dekai Wu. 1995a. An algorithm for simultaneously bracketing parallel
texts by aligning words. In Proc. of 33rd Annual Conf. of Assoc. for
Computational Linguistics, pg 244-251, Cambridge, MA, Jun.
Dekai Wu. 1995b. Grammarless extraction of phrasal translation ex-
amples from parallel texts. In TMI-95, Proc. of the 6th bun Conf
on Theoretical and Methodological Issues in Machine Translation,
volume 2, pg 354-372, Leuven, Belgium, Jul.
Dekai Wu. 1995c. Stochastic inversion transduction grammars, with
application to segmentation, bracketing, and alignment of parallel
corpora. In Proc. of IJCAI-95, 14th Intri Joint Conf on Artificial
Intelligence, pg 1328-1334, Montreal, Aug.
Dekai Wu. 1995d. Trainable coarse bilingual grammars for parallel
text bracketing. In Proc. of the 3rd Annual Workshop on Very Large
Corpora, pg 69-81, Cambridge, MA, Jun.
Dekai Wu. 1996. A polynomial-time algorithm for statistical machine
translation. In Proc. of the 34th Annual Conf of the Assoc. for Com-
putational Linguistics, pg 152-158, Santa Cruz, CA, Jun.
Dekai Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Linguistics,
23(3):377-404, Sept.
David H. Younger. 1967. Recognition and parsing of context-free lan-
guages in time n3. Information and Control, I0(2):189-208.
</reference>
<page confidence="0.991395">
1414
</page>
<figure confidence="0.9524355">
Machine Translation with a Stochastic Grammatical Channel
( -X&apos;-A-f-icMinrit.11311:-)4011 rIT)
Dekai WU (Rag.) and Hongsing WONG (*Aga
( AM:4 70-1-4A
(dekai,wong)@cs.ust.hk
IttM
RAPITMItitEz.*.f3CMIntt-gS-141M-AffIlM, ffi -11WILVIJ AlitnitEMIlt*f
IMI3111411.1. it Wu (1996) PfiliMMAPititfilgiUMM (ft, 1%-fIghutavA3ca
MIR 241(iig AZ) VIVI-00J :
1=VASAIRAI±M1FIA;
(-7-) T114114ril )141i3IMERIT3E-11*;
(E)r.fil Iii:EITRA. AtMillii=1=13.1443W*AtftlititaRM.
Pg. 4MT RI, At,Fla RIM fr011110 ffi * fCirl T=1&amp;quot; 3CM rrg V.
P.
PlatiMn±.-Ft „RUM; a
(E) b i gram
lia1r9TX-54110iMIMITIPMJ, EnliSAVfillIA4gitiMiltMCiftZ_L0
tAV LLIVP9-rligrAt&apos;fitV_FAIttliVAINTAA.
</figure>
<page confidence="0.94562">
1415
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.776730333333333">Machine Translation with a Stochastic Grammatical Channel and WONG HKUST</title>
<author confidence="0.603409">Human Language Technology Center</author>
<affiliation confidence="0.999896">Department of Computer Science University of Science and Technology</affiliation>
<address confidence="0.998551">Clear Water Bay, Hong Kong</address>
<email confidence="0.997764">fdekai,wongl@cs.ust.hk</email>
<abstract confidence="0.996570602620087">introduce a grammatical channel model for machine translation, that synthesizes several desirable characteristics of both statistical and grammatical machine translation. As with the pure statistical translation model described by Wu (1996) (in which a bracketing transduction grammar models the channel), alternative hypotheses compete probabilistically, exhaustive search of the translation hypothesis space can be performed in polynomial time, and robustness heuristics arise naturally from a language-independent inversiontransduction model. However, unlike pure statistical translation models, the generated output string is guaranteed to conform to a given target grammar. The model employs only (1) a translation lexicon, (2) a context-free grammar for the target language, and (3) a bigram language model. The fact that no explicit bilingual translation rules are used makes the model easily portable to a variety of source languages. Initial experiments show that it also achieves significant speed gains over our earlier model. 1 Motivation Speed of statistical machine translation methods has long been an issue. A step was taken by Wu (Wu, 1996) who introduced a polynomial-time algorithm for the runtime search for an optimal translation. To achieve this, Wu&apos;s method substituted a language-independent stochastic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &amp;quot;stack search&amp;quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions of the space of word alignments. The SBTG can be regarded as a model of the language-universal hypothesis that closely related arguments tend to stay together (Wu, 1995a; Wu, 1995b). In this paper we introduce a generalization of Wu&apos;s method with the objectives of 1. increasing translation speed further, 2. improving meaning-preservation accuracy, 3. improving grammaticality of the output, and 4. seeding a natural transition toward transduction rule models, under the constraint of • employing no additional knowledge resources except a grammar for the target language. To achieve these objectives, we: • replace Wu&apos;s SBTG channel with a full stochastic inversion transduction grammar or SITG channel, discussed in Section 3, and • (mis-)use the target language grammar as a SITG, discussed in Section 4. In Wu&apos;s SBTG method, the burden of generating grammatical output rests mostly on the bigram language model; explicit grammatical knowledge cannot be used. As a result, output grammaticality cannot be guaranteed. The advantage is that languagedependent syntactic knowledge resources are not needed. We relax those constraints here by assuming a good (monolingual) context-free grammar for the target language. Compared to other knowledge resources (such as transfer rules or semantic ontologies), monolingual syntactic grammars are relatively easy to acquire or construct. We use the grammar in the SITG channel, while retaining the bigram language model. The new model facilitates explicit coding of grammatical knowledge and finer control over channel probabilities. Like Wu&apos;s SBTG model, the translation hypothesis space can be exhaustively searched in polynomial time, as shown in 1408 Section 5. The experiments discussed in Section 6 show promising results for these directions. 2 Review: Noisy Channel Model The statistical translation model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process. The underlying generative model contains a stochastic Chinese (input) sentence generator whose output is &amp;quot;corrupted&amp;quot; by the translation channel to produce English (output) sentences. Assume, as we do throughout this paper, that the input language is English and the task is to translate into Chinese. In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed in the reverse direction from generation, as usual for recognition under genmodels. For each English sentence be translated, the system attempts to find the Chinese sentence c* such that: = argmax Pr(cle) = Pr(c) (1) the IBM model, the search for the optimal performed using a best-first heuristic &amp;quot;stack search&amp;quot; similar to A* methods. One of the primary obstacles to making the statistical translation approach practical is slow speed of translation, as performed in A* fashion. This price is paid for the robustness that is obtained by using very flexible language and translation models. The language model allows sentences of arbitrary order and the translation model allows arbitrary wordorder permutation. No structural constraints and explicit linguistic grammars are imposed by this model. The translation channel is characterized by two sets of parameters: translation and alignment prob- The translation probabilities describe lexical substitution, while alignment probabilities describe word-order permutation. The key problem is that the formulation of alignment probabilities the English word in position a length-T sentence to map to any position a Chinese sentence. So alignments are possible, yielding an exponential space with correspondingly slow search times. models have been constructed by the (Brown et al., 1993). This description corresponds to one of the simplest ones, &amp;quot;Model 2&amp;quot;; search costs for the more complex models are correspondingly higher. 3 A SITG Channel Model The translation channel we propose is based on recently introduced language model- The model employs a stochastic verof an transduction grammar ITG (Wu, 1995c; Wu, 1995d; Wu, 1997). This formalism was originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. Subsequently, a method was developed to use a special case of the ITG—the aforementioned BTG—for the translation task itself (Wu, 1996). The next few paragraphs briefly review the main properties of ITGs, before we describe the SITG channel. An ITG consists of context-free productions terminal symbols come in exa English word and y is an translation of the form y function words that are used in only one of the languages. Any parse tree thus generates both English and Chinese strings simultaneously. Thus, the tree: (1) [IJ N [Rook/*7 [a/— f/* book/6 ]Np 1vp you/4]pp produces, for example, the mutual translations: a. [fl 1vP [tiVZ]pp lvP Is [I [[took [a [for pp An additional mechanism accommodates a conservative degree of word-order variation between the two languages. With each production of the is associated either a an respectively denoted as VP PP] (VP In the case of a production with straight orientation, the right-hand-side symbols are visited leftto-right for both the English and Chinese streams. But for a production with inverted orientation, the right-hand-side symbols are visited left-to-right for English and right-to-left for Chinese. Thus, the tree: (3) WTI (Rook/WT [a/— €/* book/]NP IvP Is produces translations with different word order: a. [I [[took [a book]Np hip [for you]pp Ivp h [ft [[16g4}pp NP The surprising ability of ITGs to accommodate nearly all word-order variation between fixed-word- (English and Chinese in particuhas been analyzed mathematically, linguistithe exception of higher-order phenomena such as neg-raising and wh-movement. 1409 cally, and experimentally (Wu, 1995b; Wu, 1997). Any ITG can be transformed to an equivalent binary-branching normal form. A stochastic ITG associates a probability with each production. It follows that a SITG assigns probability Pr(e, q) all generable trees q and sentence-pairs. In principle it can be used as the translation channel model by normalizing with Pr(c) and integrating out Pr(q) to give Pr(eIc) in Equation (1). In practice, a strong language model makes this unnecessary, so we can instead optimize the simpler Viterbi approximation = Pr(e, q) (2) To complete the picture we add a bigram model g(cj I the Chinese language model Pr(c). This approach was used for the SBTG channel (Wu, 1996), using the language-independent case of the A a[] [AA] (AA) xly x/E translations A a() EiY Vx language 1 vocabulary A -4 Vy language 2 vocabulary A b(x,€) NE,Y) —&gt; In the proposed model, a structured languagedependent ITG is used instead. 4 A Grammatical Channel Model Stated radically, our novel modeling thesis is that of the grammar parse sentences of the Ideally, an ITG would be tailored for the desired source and target languages, enumerating the transduction patterns specific to that language pair. Constructing such an ITG, however, requires massive manual labor effort for each language pair. Instead, our approach is to take a more readily acquired monolingual context-free grammar for the target language, and use (or perhaps misuse) it in the SITG channel, by employing the three tactics described mirroring, mapskipping. In the following, keep in mind our convention that language 1 is the source (English), while language 2 is the target (Chinese). (Wu, 1996) experimented with Chinese-English translation, while this paper experiments with English-Chinese translation.</abstract>
<note confidence="0.4674468">S NP VP Punc VP V NP -+ N Mod N [NP VP Punc] VP NP) --+ [V NP] V) [N Mod N] Mod N) Figure 1: An input CFG and its mirrored ITG. 4.1 Production Mirroring The first step is to convert the monolingual Chi- CFG to a bilingual ITG. The mir-</note>
<abstract confidence="0.993978297752809">simply doubles the number of productions, transforming every monolingual producinto two bilingual one straight and one inverted, as for example in Figure 1 where the upper Chinese CFG becomes the lower ITG. The intent of the mirroring is to add enough flexibility to allow parsing of English sentences using the language 1 side of the ITG. The extra productions accommodate reversed subconstituent order in the source language&apos;s constituents, at the same time restricting the language 2 output sentence to conform the given target grammar whether straight or inverted productions are used. The following example illustrates how production mirroring works. Consider the input sentence is the son can be parsed by the ITG of Figure 1 to yield the corresponding outsentenceft with the following parse tree: [[[He/ft Jp„,]Np [[is/ [Stephen/54 IN [Jo Production mirroring produced the inverted NP which was necessary to parse (son/ 92,T °fin Stephen/sU631 If the target CFG is purely binary branching, then the previous theoretical and linguistic analyses (Wu, 1997) suggest that much of the requisite constituent and word order transposition may be accommodated without change to the mirrored ITG. On the other hand, if the target CFG contains productions with long right-hand-sides, then merely inverting the subconstituent order will probably be insufficient. In such cases, a more complex transformation heuristic would be needed. Objective 3 (improving grammaticality of the can be directly tackled by using a tight tarfor unary productions, which yield only one bilingual production. 1410 get grammar. To see this, consider using a mirrored Chinese CFG to parse English sentences with language 1 side of the ITG. resulting parse tree must be consistent with the original Chinese follows from the fact that both the straight and inverted versions of a production have language 2 (Chinese) sides identical to the original monolingual production: inverting production orientation cancels out the mirroring of the right-handside symbols. Thus, the output grammaticality depends directly on the tightness of the original Chinese grammar. In principle, with this approach a single target grammar could be used for translation from any number of other (fixed word-order) source languages, so long as a translation lexicon is available for each source language. Probabilities on the mirrored ITG cannot be reliably estimated from bilingual data without a very large parallel corpus. A straightforward approximation is to employ EM or Viterbi training on just a monolingual target language (Chinese) corpus. 4.2 Part-of-Speech Mapping The second problem is that the part-of-speech (PoS) categories used by the target (Chinese) grammar do not correspond to the source (English) words when the source sentence is parsed. It is unlikely that any English lexicon will list Chinese parts-of-speech. employ a simple mapping technique that allows the PoS tag of any corresponding word in the target language (as found in the translation lexicon) to serve as a proxy for the word&apos;s PoS. The word example, may be tagged with the Chinese tags nc and vn, since the translation lexicon holds both viewNN/S, viewviiitita vn • Unknown English words must be handled differently since they cannot be looked up in the translation lexicon. The English PoS tag is first found by tagging the English sentence. A set of possible corresponding Chinese PoS tags is then found by table lookup (using a small hand-constructed mapping table). For example, NN may map to nc, loc and pref, while VB may map to vi, vn, vp, vv, vs, etc. This method generates many hypotheses and should only be used as a last resort. 4.3 Word Skipping Regardless of how constituent-order transposition is handled, some function words simply do not occur in both languages, for example Chinese aspect This is the rationale for the mentioned in Section 3. If we create an explicit singleton hypothesis for every possible input word, the resulting search space will be too large. To recognize singletons, we borrow the from speech recognition and robust parsing. As formalized in the next section, we can do this by modifying the item extension step in our chart-parser-like algorithm. When the dot of an item is on the rightmost we can use such constituent, a extend other items. In chart parsing, the valid subtrees that can be used to extend an item are those that are located on the adjacent right of the dot position of the item and the anticipated category of the item should also be equal to that of the subtrees. If word-skipping is to be used, the valid subtrees can be located a few positions right (or, left for the item corresponding to inverted production) to the dot position of the item. In other words, words between the dot position and the start of the subtee are skipped, and considered to be singletons. Consider Sentence 5 again. Word-skipping hanthe has no Chinese counterpart. At a certain point during translation, we have the followitem: VP •NP. With word-skipping, it can be extended to VP -÷[is/]vNP. by the subtree (son/ 52,T of/n Stephen/Z*4 )Np, even the subtree is not adjacent (but within a certain distance, see Section 5) to the dot position of the item. The on the adjacent to the dot position of the item is skipped. Word-skipping provides us the flexibility to parse the source input by skipping possible singleton(s), if when we doing so, the source input can be parsed with the highest likelihood, and grammatical output can be produced. 5 Translation Algorithm The translation search algorithm differs from that of Wu&apos;s SBTG model in that it handles arbitrary grammars rather than binary bracketing grammars. As such it is more similar to active chart parsing (Earley, 1970) rather than CYK parsing (Kasami, 1965; Younger, 1967). We take the standard notion of and Ullman, 1972), and use the term anmean an item which still has symbols right of its dot. Items that don&apos;t have any symbols of the dot are called As with Wu&apos;s SBTG model, the algorithm maxa probabilistic objective function, Equa- 1411 tion (2), using dynamic programming similar to that for HMM recognition (Viterbi, 1967). The presence of the bigram model in the objective function necessitates indexes in the recurrence not only on subtrees over the source English string, but also on the delimiting words of the target Chinese substrings. The dynamic programming exploits a recursive formulation of the objective function as follows. notation remarks: denotes the subseof English tokens , We denote the set of Chinese words that are translations of the English word created by takall tokens in together. t) the set of Chinese words that are translations of any of English words anywhere within K the maximium number of consecutive English words can be Finally, the argmax operator is generalized to vector notation to accommodate multiple indices. 1. Initialization 0&lt;s&lt;t&lt;T = IY), Y&apos;s PoS 2. Recursion all such that { the category of a constituent spanning s to 0&lt;s&lt;t&lt;T . v the leftmost/rightmost words of the constituent if tuv &gt; if 0 otherwise = JJ i.0 [l rstov A.qo =-argmax r)</abstract>
<note confidence="0.649795666666667">I) i=0 Xq our experiments, set to 4 = = 210 = v, = = 1, q = (r,sit,uivi) tut, = 5,&lt;t,&lt;s■+1 i=0 - Xqr, X90 0 0 argmax a(r) JJ Oro,t,u,v,9,.+0.4,</note>
<email confidence="0.692144">i=o</email>
<abstract confidence="0.99840663372093">(S, u, be the optimal root. where v) = maxu,vEc(0,T) stu v any child of = (r, t, u, v) given by: r) = ,[i NIL &apos;9 otherwise 0 if&amp;quot;6 == [] Assuming the number of translation per word is bounded by some constant, then the maximum size of C&apos;(8, proportional to The asymptotic time complexity for our algorithm is thus bounded note that in theory the complexity upper bound rises exponentially rather than polynomially with the size of the grammar, just as for context-free parsing (Barton et al., 1987), whereas this is not a problem for Wu&apos;s SBTG algorithm. In practice, natural language grammars are usually sufficiently constrained so that speed is actually improved over the SBTG algorithm, as discussed later. The dynamic programming is efficiently implemented by an active-chart-parser-style agendabased algorithm, sketched as follows: Initialization each word in the input sentence, put a subtree with category equal to the PoS of its translation into the agenda. while agenda is not empty: (a) If the current item is a subtree of category X, extend existing anticipations by calling ANTICIPA- TIONEXTENSION. For each rule in the grammar -4 XW . . add an initial anticipation of form • W Y and put it into the agenda. Add subtree X to the chart. (b) If the current item is an anticipation of the form W ....X from to, find all subtrees in the chart with category X that start at position t1 and use each subtree to extend this anticipation by calling ANTICIPATIONEXTENSION. the subtree we is of category X from position any of the form ... • X... Y from so [si — s extend it to TV... X • ... with from so to add it to the agenda. &apos;Yr shy) 1412 output string is recursively reconstructed from the highest likelihood subtree, with catespan the whole input sentence. 6 Results The grammatical channel was tested in the SILC translation system. The translation lexicon was partly constructed by training on government transcripts from the HKUST English-Chinese Parallel Bilingual Corpus, and partly entered by hand. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995). Together with handconstructed entries, the resulting English vocabulary is approximately 9,500 words and the Chinese vocabulary is approximately 14,500 words, with a many-to-many translation mapping averaging 2.56 Chinese translations per English word. Since the lexicon&apos;s content is mixed, we approximate translation probabilities by using the unigram distribution of the target vocabulary from a small monolingual corpus. Noise still exists in the lexicon. The Chinese grammar we used is not tight— it was written for robust parsing purposes, and as such it over-generates. Because of this we have not yet been able to conduct a fair quantitative assessment of objective 3. Our productions were constructed with reference to a standard grammar (Beijing Language and Culture Univ., 1996) and totalled 316 productions. Not all the original productions are mirrored, since some (128) are unary producand others are Chinese-specific lexical constructions like S S NP S, which are obviously unnecessary to handle English. About 27.7% of the non-unary Chinese productions were mirrored and the total number of productions in the final ITG is 368. For the experiment, 222 English sentences with a maximum length of 20 words from the parallel corpus were randomly selected. Some examples of the output are shown in Figure 2. No morphological processing has been used to correct the output, and up to now we have only been testing with a bigram model trained on extremely small corpus. With respect to objective 1 (increasing translation speed), the new model is very encouraging. Table 1 shows that over 90% of the samples can be processed within one minute by the grammatical channel model, whereas that for the SBTG channel model is about 50%. This demonstrates the stronger Time SBTG Grammatical Channel (x) Channel secs. 15.6% 83.3% &lt; 1 mm. 34.9% 7.6% 1 min. 49.5% 9.1% Table 1: Translation speed. Sentence meaning SBTG Grammatical Channel preservation Channel Correct 25.9% 32.3% Incorrect - 74.1% 67.7% Table 2: Translation accuracy. constraints on the search space given by the SITG. The natural trade-off is that constraining the structure of the input decreases robustness somewhat. Approximately 13% of the test corpus could not be parsed in the grammatical channel model. As mentioned earlier, this figure is likely to vary widely depending on the characteristics of the target grammar. Of course, one can simply back off to the SBTG model when the grammatical channel rejects an input sentence. With respect to objective 2 (improving meaningpreservation accuracy), the new model is also promising. Table 2 shows that the percentage of meaningfully translated sentences rises from 26% to (ignoring the rejected We have judged only whether the correct meaning is conveyed by the translation, paying particular attention to word order and grammaticality, but otherwise ignoring morphological and function word choices. 7 Conclusion Currently we are designing a tight generationoriented Chinese grammar to replace our robust parsing-oriented grammar. We will use the new grammar to quantitatively evaluate objective 3. We are also studying complementary approaches to the English word deletion performed by wordskipping—i.e., extensions that insert Chinese words suggested by the target grammar into the output. The framework seeds a natural transition toward pattern-based translation models (objective 4). One accuracy rates are relatively low because these experiments are being conducted with new lexicons and grammar on a new translation direction (English-Chinese). 1413 can post-edit the productions of a mirrored SITG more carefully and extensively than we have done in our cursory pruning, gradually transforming the original monolingual productions into a set of true transduction rule patterns. This provides a smooth evolution from a purely statistical model toward a hybrid model, as more linguistic resources become available. have described a new grammatichannel for statistical machine translation that exhibits several nice properties in comparison with Wu&apos;s SBTG model and IBM&apos;s word alignment model. The SITG-based channel increases translation speed, improves meaning-preservation accuracy, permits tight target CFGs to be incorporated for improving output grammaticality, and suggests a natural evolution toward transduction rule models. The input CFG is adapted for use via production mirroring, part-of-speech mapping, and wordskipping. We gave a polynomial-time translation algorithm that requires only a translation lexicon, plus a CFG and bigram language model for the target language. More linguistic knowledge about the target language is employed than in pure statistical translation models, but Wu&apos;s SBTG polynomialtime bound on search cost is retained and in fact the search space can be significantly reduced by using a good grammar. Output always conforms to the given target grammar.</abstract>
<note confidence="0.936662083333333">Acknowledgments Thanks to the SILC group members: Xuanyin Xia, Daniel Chan, Aboy Wong, Vincent Chow &amp; James Pang. References V. Aho and Jeffrey D. Ullman. 1972. Theory of Parsing. and Compiling. Hall, Englewood Cliffs, NJ. Edward Barton, Robert C. Berwick, and Eric. S Ristad. 1987. Com- Complexity and Natural Language. Press, Cambridge, MA. Language and Culture Univ.. 1996. Hanyu Chuji (A Short Intensive Elementary Chinese Course), 1-4. Beijing Language And Culture Univ. Press,</note>
<author confidence="0.608595333333333">A statistical approach to machine transla-</author>
<email confidence="0.275131">Linguistics,</email>
<author confidence="0.7870535">The mathematics of statistical ma-</author>
<abstract confidence="0.579360631578947">translation: Parameter estimation. Linguis- Earley. 1970. An efficient context-free parsing algorithm. Comof the Assoc. for Computing Machinery, 3(2):94—IO2. Pascale Fung and Dekai Wu. 1994. Statistical augmentation of a Chimachine-readable dictionary. In of the 2nd Annual Workon Very Large Corpora, 69-85, Kyoto, Aug. : entirely agree with this point of view. Output: ItttlitaffiraMo Corpus: tlfsEMITSMOSA. : would create a tremendous financial burden to taxpayers in Hong Kong. ?ICA Input : The Government wants, and will work for, the best education for all the children of Hong Kong. /PT Ws] CM, &amp;€1.*OtINVIIVItOo Input : Let me repeat one simple point yet again. UtlIR tg : are very disappointed.</abstract>
<note confidence="0.932815108108108">IMW Corpus: #111#111t1T%#, Figure 2: Example translation outputs from the grammatical channel model. T. Kasami. 1965. An efficient recognition and syntax analysis algorithm for context-free languages. Technical Report AFCRL-65- 758, Air Force Cambridge Research Lab., Bedford, MA. Andrew J. Viterbi. 1967. Error bounds for convolutional codes and an optimal decoding algorithm. Transactions on Theory, Dekai Wu and Pascale Fung. 1994. Improving Chinese tokenization linguistic filters on statistical lexical acquisition. In of Conf on ANLP, 180-181, Stuttgart, Oct. Dekai Wu and Xuanyin Xia. 1995. Large-scale automatic extraction an English-Chinese lexicon. Translation, 9(3-4):285- 313. Dekai Wu. 1994. Aligning a parallel English-Chinese corpus statistiwith lexical criteria. In of 32nd Annual Conf of Assoc. Computational Linguistics, 80-87, Las Cruces, Jun. Dekai Wu. 1995a. An algorithm for simultaneously bracketing parallel by aligning words. In of 33rd Annual Conf. of Assoc. for Linguistics, 244-251, Cambridge, MA, Jun. Dekai Wu. 1995b. Grammarless extraction of phrasal translation exfrom parallel texts. In Proc. of the 6th bun Conf on Theoretical and Methodological Issues in Machine Translation, volume 2, pg 354-372, Leuven, Belgium, Jul. Dekai Wu. 1995c. Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel In of IJCAI-95, 14th Intri Joint Conf on Artificial 1328-1334, Montreal, Aug. Dekai Wu. 1995d. Trainable coarse bilingual grammars for parallel bracketing. In of the 3rd Annual Workshop on Very Large 69-81, Cambridge, MA, Jun. Dekai Wu. 1996. A polynomial-time algorithm for statistical machine In of the 34th Annual Conf of the Assoc. for Com- Linguistics, 152-158, Santa Cruz, CA, Jun. Dekai Wu. 1997. Stochastic inversion transduction grammars and</note>
<affiliation confidence="0.748659">parsing of parallel corpora. Linguistics,</affiliation>
<address confidence="0.905966">23(3):377-404, Sept.</address>
<author confidence="0.844448">Recognition</author>
<author confidence="0.844448">parsing of context-free lan-</author>
<affiliation confidence="0.836142">in time and Control,</affiliation>
<address confidence="0.872001">1414</address>
<author confidence="0.507677">Machine Translation with a Stochastic Grammatical Channel</author>
<email confidence="0.756683">rIT)</email>
<affiliation confidence="0.771257">WU (Rag.) and Hongsing (*Aga</affiliation>
<address confidence="0.769576">AM:4</address>
<email confidence="0.932632">(dekai,wong)@cs.ust.hk</email>
<note confidence="0.897027166666667">IttM AlitnitEMIlt*f Wu (ft, 241(iig AZ) : 1=VASAIRAI±M1FIA; T114114ril Iii:EITRA. 4MT At,Fla RIM fr011110 T=1&amp;quot; 3CM rrg V. „RUM; i gram lia1r9TX-54110iMIMITIPMJ, EnliSAVfillIA4gitiMiltMCiftZ_L0 tAVLLIVP9-rligrAt&apos;fitV_FAIttliVAINTAA.</note>
<intro confidence="0.539164">1415</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing. Translation, and Compiling.</booktitle>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="16914" citStr="Aho and Ullman, 1972" startWordPosition="2712" endWordPosition="2715"> to the dot position of the item is skipped. Word-skipping provides us the flexibility to parse the source input by skipping possible singleton(s), if when we doing so, the source input can be parsed with the highest likelihood, and grammatical output can be produced. 5 Translation Algorithm The translation search algorithm differs from that of Wu&apos;s SBTG model in that it handles arbitrary grammars rather than binary bracketing grammars. As such it is more similar to active chart parsing (Earley, 1970) rather than CYK parsing (Kasami, 1965; Younger, 1967). We take the standard notion of items (Aho and Ullman, 1972), and use the term anticipation to mean an item which still has symbols right of its dot. Items that don&apos;t have any symbols right of the dot are called subtree. As with Wu&apos;s SBTG model, the algorithm maximizes a probabilistic objective function, Equa1411 tion (2), using dynamic programming similar to that for HMM recognition (Viterbi, 1967). The presence of the bigram model in the objective function necessitates indexes in the recurrence not only on subtrees over the source English string, but also on the delimiting words of the target Chinese substrings. The dynamic programming exploits a rec</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing. Translation, and Compiling. Prentice Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ristad</author>
</authors>
<date>1987</date>
<booktitle>Computational Complexity and Natural Language.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Ristad, 1987</marker>
<rawString>G. Edward Barton, Robert C. Berwick, and Eric. S Ristad. 1987. Computational Complexity and Natural Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beijing Language</author>
<author>Culture Univ</author>
</authors>
<title>Sucheng Hanyu Chuji Jiaocheng (A Short Intensive Elementary Chinese Course), volume 1-4. Beijing Language And Culture</title>
<date>1996</date>
<publisher>Univ. Press,</publisher>
<marker>Language, Univ, 1996</marker>
<rawString>Beijing Language and Culture Univ.. 1996. Sucheng Hanyu Chuji Jiaocheng (A Short Intensive Elementary Chinese Course), volume 1-4. Beijing Language And Culture Univ. Press,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<contexts>
<context position="3917" citStr="Brown et al., 1990" startWordPosition="576" endWordPosition="579">les or semantic ontologies), monolingual syntactic grammars are relatively easy to acquire or construct. We use the grammar in the SITG channel, while retaining the bigram language model. The new model facilitates explicit coding of grammatical knowledge and finer control over channel probabilities. Like Wu&apos;s SBTG model, the translation hypothesis space can be exhaustively searched in polynomial time, as shown in 1408 Section 5. The experiments discussed in Section 6 show promising results for these directions. 2 Review: Noisy Channel Model The statistical translation model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process. The underlying generative model contains a stochastic Chinese (input) sentence generator whose output is &amp;quot;corrupted&amp;quot; by the translation channel to produce English (output) sentences. Assume, as we do throughout this paper, that the input language is English and the task is to translate into Chinese. In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed in</context>
</contexts>
<marker>Brown, Cocke, DellaPietra, DellaPietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vincent J. DellaPietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):29-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="4488" citStr="Brown et al., 1993" startWordPosition="666" endWordPosition="669">ion model introduced by IBM (Brown et al., 1990) views translation as a noisy channel process. The underlying generative model contains a stochastic Chinese (input) sentence generator whose output is &amp;quot;corrupted&amp;quot; by the translation channel to produce English (output) sentences. Assume, as we do throughout this paper, that the input language is English and the task is to translate into Chinese. In the IBM system, the language model employs simple n-grams, while the translation model employs several sets of parameters as discussed below. Estimation of the parameters has been described elsewhere (Brown et al., 1993). Translation is performed in the reverse direction from generation, as usual for recognition under generative models. For each English sentence e to be translated, the system attempts to find the Chinese sentence c* such that: c* = argmax Pr(cle) = argmaxPr(elc) Pr(c) (1) In the IBM model, the search for the optimal c* is performed using a best-first heuristic &amp;quot;stack search&amp;quot; similar to A* methods. One of the primary obstacles to making the statistical translation approach practical is slow speed of translation, as performed in A* fashion. This price is paid for the robustness that is obtained</context>
<context position="5959" citStr="Brown et al., 1993" startWordPosition="893" endWordPosition="896">this model. The translation channel is characterized by two sets of parameters: translation and alignment probabilities.1 The translation probabilities describe lexical substitution, while alignment probabilities describe word-order permutation. The key problem is that the formulation of alignment probabilities a(ijj,17,T) permits the English word in position j of a length-T sentence to map to any position i of a length-V Chinese sentence. So VT alignments are possible, yielding an exponential space with correspondingly slow search times. &apos;Various models have been constructed by the IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &amp;quot;Model 2&amp;quot;; search costs for the more complex models are correspondingly higher. 3 A SITG Channel Model The translation channel we propose is based on the recently introduced bilingual language modeling approach. The model employs a stochastic version of an inversion transduction grammar or ITG (Wu, 1995c; Wu, 1995d; Wu, 1997). This formalism was originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. Subsequently, a method was developed to use a special case of</context>
</contexts>
<marker>Brown, DellaPietra, DellaPietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. DellaPietra, Vincent J. DellaPietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Assoc. for Computing Machinery, I</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="16799" citStr="Earley, 1970" startWordPosition="2694" endWordPosition="2696"> within a certain distance, see Section 5) to the dot position of the item. The the located on the adjacent to the dot position of the item is skipped. Word-skipping provides us the flexibility to parse the source input by skipping possible singleton(s), if when we doing so, the source input can be parsed with the highest likelihood, and grammatical output can be produced. 5 Translation Algorithm The translation search algorithm differs from that of Wu&apos;s SBTG model in that it handles arbitrary grammars rather than binary bracketing grammars. As such it is more similar to active chart parsing (Earley, 1970) rather than CYK parsing (Kasami, 1965; Younger, 1967). We take the standard notion of items (Aho and Ullman, 1972), and use the term anticipation to mean an item which still has symbols right of its dot. Items that don&apos;t have any symbols right of the dot are called subtree. As with Wu&apos;s SBTG model, the algorithm maximizes a probabilistic objective function, Equa1411 tion (2), using dynamic programming similar to that for HMM recognition (Viterbi, 1967). The presence of the bigram model in the objective function necessitates indexes in the recurrence not only on subtrees over the source Englis</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the Assoc. for Computing Machinery, I 3(2):94—IO2.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Pascale Fung</author>
<author>Dekai Wu</author>
</authors>
<title>Statistical augmentation of a Chinese machine-readable dictionary.</title>
<date>1994</date>
<booktitle>In Proc. of the 2nd Annual Workshop on Very Large Corpora, pg 69-85,</booktitle>
<location>Kyoto,</location>
<contexts>
<context position="21248" citStr="Fung and Wu, 1994" startWordPosition="3486" endWordPosition="3489">o Z TV... X • ... Y with span from so to t and add it to the agenda. Srstuv = &apos;Yr shy) 1412 3. Reconstruction The output string is recursively reconstructed from the highest likelihood subtree, with category S. that span the whole input sentence. 6 Results The grammatical channel was tested in the SILC translation system. The translation lexicon was partly constructed by training on government transcripts from the HKUST English-Chinese Parallel Bilingual Corpus, and partly entered by hand. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995). Together with handconstructed entries, the resulting English vocabulary is approximately 9,500 words and the Chinese vocabulary is approximately 14,500 words, with a many-to-many translation mapping averaging 2.56 Chinese translations per English word. Since the lexicon&apos;s content is mixed, we approximate translation probabilities by using the unigram distribution of the target vocabulary from a small monolingual corpus. Noise still exists in the lexicon. The Chinese grammar we used is not tight— i</context>
</contexts>
<marker>Fung, Wu, 1994</marker>
<rawString>Pascale Fung and Dekai Wu. 1994. Statistical augmentation of a Chinese machine-readable dictionary. In Proc. of the 2nd Annual Workshop on Very Large Corpora, pg 69-85, Kyoto, Aug. Input : I entirely agree with this point of view. Output: ItttlitaffiraMo Corpus: tlfsEMITSMOSA. Input : This would create a tremendous financial burden to taxpayers in Hong Kong. Output: ?ICA 14glitt AABBO Corpus: iftl4gititErAttAiIi32,Ingtgiftitlo Input : The Government wants, and will work for, the best education for all the children of Hong Kong.</rawString>
</citation>
<citation valid="false">
<note>Output: canoe r-vtigiarzsifoguiraTnA itA illS2,20</note>
<marker></marker>
<rawString>Output: canoe r-vtigiarzsifoguiraTnA itA illS2,20</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corpus</author>
</authors>
<title>PT Ws] CM, &amp;€1.*OtINVIIVItOo Input : Let me repeat one simple point yet again. Output: UtlIR ri tg 0 Corpus: 21101NEIff-11448117frgWM0 Input : We are very disappointed.</title>
<date></date>
<marker>Corpus, </marker>
<rawString>Corpus, OZA:a5tV*itfifi-92,Itl3figtg /PT Ws] CM, &amp;€1.*OtINVIIVItOo Input : Let me repeat one simple point yet again. Output: UtlIR ri tg 0 Corpus: 21101NEIff-11448117frgWM0 Input : We are very disappointed.</rawString>
</citation>
<citation valid="false">
<title>Output: Rill IMW Corpus: #111#111t1T%#, Figure 2: Example translation outputs from the grammatical channel model.</title>
<marker></marker>
<rawString>Output: Rill IMW Corpus: #111#111t1T%#, Figure 2: Example translation outputs from the grammatical channel model.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An efficient recognition and syntax analysis algorithm for context-free languages.</title>
<date>1965</date>
<tech>Technical Report AFCRL-65-758,</tech>
<institution>Air Force Cambridge Research Lab.,</institution>
<location>Bedford, MA.</location>
<contexts>
<context position="16837" citStr="Kasami, 1965" startWordPosition="2701" endWordPosition="2702"> 5) to the dot position of the item. The the located on the adjacent to the dot position of the item is skipped. Word-skipping provides us the flexibility to parse the source input by skipping possible singleton(s), if when we doing so, the source input can be parsed with the highest likelihood, and grammatical output can be produced. 5 Translation Algorithm The translation search algorithm differs from that of Wu&apos;s SBTG model in that it handles arbitrary grammars rather than binary bracketing grammars. As such it is more similar to active chart parsing (Earley, 1970) rather than CYK parsing (Kasami, 1965; Younger, 1967). We take the standard notion of items (Aho and Ullman, 1972), and use the term anticipation to mean an item which still has symbols right of its dot. Items that don&apos;t have any symbols right of the dot are called subtree. As with Wu&apos;s SBTG model, the algorithm maximizes a probabilistic objective function, Equa1411 tion (2), using dynamic programming similar to that for HMM recognition (Viterbi, 1967). The presence of the bigram model in the objective function necessitates indexes in the recurrence not only on subtrees over the source English string, but also on the delimiting w</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>T. Kasami. 1965. An efficient recognition and syntax analysis algorithm for context-free languages. Technical Report AFCRL-65-758, Air Force Cambridge Research Lab., Bedford, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimal decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>13--260</pages>
<contexts>
<context position="17256" citStr="Viterbi, 1967" startWordPosition="2773" endWordPosition="2774">&apos;s SBTG model in that it handles arbitrary grammars rather than binary bracketing grammars. As such it is more similar to active chart parsing (Earley, 1970) rather than CYK parsing (Kasami, 1965; Younger, 1967). We take the standard notion of items (Aho and Ullman, 1972), and use the term anticipation to mean an item which still has symbols right of its dot. Items that don&apos;t have any symbols right of the dot are called subtree. As with Wu&apos;s SBTG model, the algorithm maximizes a probabilistic objective function, Equa1411 tion (2), using dynamic programming similar to that for HMM recognition (Viterbi, 1967). The presence of the bigram model in the objective function necessitates indexes in the recurrence not only on subtrees over the source English string, but also on the delimiting words of the target Chinese substrings. The dynamic programming exploits a recursive formulation of the objective function as follows. Some notation remarks: e,..t denotes the subsequence of English tokens es+i , ,et. We use C(s..t) to denote the set of Chinese words that are translations of the English word created by taking all tokens in es..t together. C(s, t) denotes the set of Chinese words that are translations</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Andrew J. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, 13:260-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Improving Chinese tokenization with linguistic filters on statistical lexical acquisition.</title>
<date>1994</date>
<booktitle>In Proc. of 4th Conf on ANLP, pg 180-181,</booktitle>
<location>Stuttgart,</location>
<contexts>
<context position="21268" citStr="Wu and Fung, 1994" startWordPosition="3490" endWordPosition="3493"> with span from so to t and add it to the agenda. Srstuv = &apos;Yr shy) 1412 3. Reconstruction The output string is recursively reconstructed from the highest likelihood subtree, with category S. that span the whole input sentence. 6 Results The grammatical channel was tested in the SILC translation system. The translation lexicon was partly constructed by training on government transcripts from the HKUST English-Chinese Parallel Bilingual Corpus, and partly entered by hand. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995). Together with handconstructed entries, the resulting English vocabulary is approximately 9,500 words and the Chinese vocabulary is approximately 14,500 words, with a many-to-many translation mapping averaging 2.56 Chinese translations per English word. Since the lexicon&apos;s content is mixed, we approximate translation probabilities by using the unigram distribution of the target vocabulary from a small monolingual corpus. Noise still exists in the lexicon. The Chinese grammar we used is not tight— it was written for ro</context>
</contexts>
<marker>Wu, Fung, 1994</marker>
<rawString>Dekai Wu and Pascale Fung. 1994. Improving Chinese tokenization with linguistic filters on statistical lexical acquisition. In Proc. of 4th Conf on ANLP, pg 180-181, Stuttgart, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Xuanyin Xia</author>
</authors>
<title>Large-scale automatic extraction of an English-Chinese lexicon.</title>
<date>1995</date>
<journal>Machine Translation,</journal>
<pages>9--3</pages>
<contexts>
<context position="21344" citStr="Wu and Xia, 1995" startWordPosition="3504" endWordPosition="3507">econstruction The output string is recursively reconstructed from the highest likelihood subtree, with category S. that span the whole input sentence. 6 Results The grammatical channel was tested in the SILC translation system. The translation lexicon was partly constructed by training on government transcripts from the HKUST English-Chinese Parallel Bilingual Corpus, and partly entered by hand. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995). Together with handconstructed entries, the resulting English vocabulary is approximately 9,500 words and the Chinese vocabulary is approximately 14,500 words, with a many-to-many translation mapping averaging 2.56 Chinese translations per English word. Since the lexicon&apos;s content is mixed, we approximate translation probabilities by using the unigram distribution of the target vocabulary from a small monolingual corpus. Noise still exists in the lexicon. The Chinese grammar we used is not tight— it was written for robust parsing purposes, and as such it over-generates. Because of this we hav</context>
</contexts>
<marker>Wu, Xia, 1995</marker>
<rawString>Dekai Wu and Xuanyin Xia. 1995. Large-scale automatic extraction of an English-Chinese lexicon. Machine Translation, 9(3-4):285-313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Aligning a parallel English-Chinese corpus statistically with lexical criteria.</title>
<date>1994</date>
<booktitle>In Proc. of 32nd Annual Conf of Assoc. for Computational Linguistics, pg 80-87,</booktitle>
<location>Las Cruces,</location>
<contexts>
<context position="21182" citStr="Wu, 1994" startWordPosition="3477" endWordPosition="3478">form Z W ... • X... Y from so to [si — K, s j, extend it to Z TV... X • ... Y with span from so to t and add it to the agenda. Srstuv = &apos;Yr shy) 1412 3. Reconstruction The output string is recursively reconstructed from the highest likelihood subtree, with category S. that span the whole input sentence. 6 Results The grammatical channel was tested in the SILC translation system. The translation lexicon was partly constructed by training on government transcripts from the HKUST English-Chinese Parallel Bilingual Corpus, and partly entered by hand. The corpus was sentence-aligned statistically (Wu, 1994); Chinese words and collocations were extracted (Fung and Wu, 1994; Wu and Fung, 1994); then translation pairs were learned via an EM procedure (Wu and Xia, 1995). Together with handconstructed entries, the resulting English vocabulary is approximately 9,500 words and the Chinese vocabulary is approximately 14,500 words, with a many-to-many translation mapping averaging 2.56 Chinese translations per English word. Since the lexicon&apos;s content is mixed, we approximate translation probabilities by using the unigram distribution of the target vocabulary from a small monolingual corpus. Noise still </context>
</contexts>
<marker>Wu, 1994</marker>
<rawString>Dekai Wu. 1994. Aligning a parallel English-Chinese corpus statistically with lexical criteria. In Proc. of 32nd Annual Conf of Assoc. for Computational Linguistics, pg 80-87, Las Cruces, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>An algorithm for simultaneously bracketing parallel texts by aligning words.</title>
<date>1995</date>
<booktitle>In Proc. of 33rd Annual Conf. of Assoc. for Computational Linguistics, pg 244-251,</booktitle>
<location>Cambridge, MA,</location>
<contexts>
<context position="2190" citStr="Wu, 1995" startWordPosition="316" endWordPosition="317">tochastic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &amp;quot;stack search&amp;quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions of the space of word alignments. The SBTG can be regarded as a model of the language-universal hypothesis that closely related arguments tend to stay together (Wu, 1995a; Wu, 1995b). In this paper we introduce a generalization of Wu&apos;s method with the objectives of 1. increasing translation speed further, 2. improving meaning-preservation accuracy, 3. improving grammaticality of the output, and 4. seeding a natural transition toward transduction rule models, under the constraint of • employing no additional knowledge resources except a grammar for the target language. To achieve these objectives, we: • replace Wu&apos;s SBTG channel with a full stochastic inversion transduction grammar or SITG channel, discussed in Section 3, and • (mis-)use the target language gr</context>
<context position="6323" citStr="Wu, 1995" startWordPosition="953" endWordPosition="954">-T sentence to map to any position i of a length-V Chinese sentence. So VT alignments are possible, yielding an exponential space with correspondingly slow search times. &apos;Various models have been constructed by the IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &amp;quot;Model 2&amp;quot;; search costs for the more complex models are correspondingly higher. 3 A SITG Channel Model The translation channel we propose is based on the recently introduced bilingual language modeling approach. The model employs a stochastic version of an inversion transduction grammar or ITG (Wu, 1995c; Wu, 1995d; Wu, 1997). This formalism was originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. Subsequently, a method was developed to use a special case of the ITG—the aforementioned BTG—for the translation task itself (Wu, 1996). The next few paragraphs briefly review the main properties of ITGs, before we describe the SITG channel. An ITG consists of context-free productions where terminal symbols come in couples, for example xly, where x is a English word and y is an Chinese translation of x, with singletons of</context>
<context position="8424" citStr="Wu, 1995" startWordPosition="1290" endWordPosition="1291">symbols are visited left-to-right for English and right-to-left for Chinese. Thus, the tree: (3) WTI (Rook/WT [a/— €/* book/]NP IvP [for/M you/4]pp)VP Is produces translations with different word order: (4) a. [I [[took [a book]Np hip [for you]pp Iv p h b. [ft [[16g4}pp [*7 [—*INP IVP NP Is The surprising ability of ITGs to accommodate nearly all word-order variation between fixed-wordorder languages2 (English and Chinese in particular), has been analyzed mathematically, linguisti2With the exception of higher-order phenomena such as neg-raising and wh-movement. 1409 cally, and experimentally (Wu, 1995b; Wu, 1997). Any ITG can be transformed to an equivalent binary-branching normal form. A stochastic ITG associates a probability with each production. It follows that a SITG assigns a probability Pr(e, c, q) to all generable trees q and sentence-pairs. In principle it can be used as the translation channel model by normalizing with Pr(c) and integrating out Pr(q) to give Pr(eIc) in Equation (1). In practice, a strong language model makes this unnecessary, so we can instead optimize the simpler Viterbi approximation c* = argmax Pr(e, c, q) Pr(c) (2) To complete the picture we add a bigram mode</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. 1995a. An algorithm for simultaneously bracketing parallel texts by aligning words. In Proc. of 33rd Annual Conf. of Assoc. for Computational Linguistics, pg 244-251, Cambridge, MA, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Grammarless extraction of phrasal translation examples from parallel texts.</title>
<date>1995</date>
<booktitle>In TMI-95, Proc. of the 6th bun Conf on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<volume>2</volume>
<pages>354--372</pages>
<location>Leuven, Belgium,</location>
<contexts>
<context position="2190" citStr="Wu, 1995" startWordPosition="316" endWordPosition="317">tochastic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &amp;quot;stack search&amp;quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions of the space of word alignments. The SBTG can be regarded as a model of the language-universal hypothesis that closely related arguments tend to stay together (Wu, 1995a; Wu, 1995b). In this paper we introduce a generalization of Wu&apos;s method with the objectives of 1. increasing translation speed further, 2. improving meaning-preservation accuracy, 3. improving grammaticality of the output, and 4. seeding a natural transition toward transduction rule models, under the constraint of • employing no additional knowledge resources except a grammar for the target language. To achieve these objectives, we: • replace Wu&apos;s SBTG channel with a full stochastic inversion transduction grammar or SITG channel, discussed in Section 3, and • (mis-)use the target language gr</context>
<context position="6323" citStr="Wu, 1995" startWordPosition="953" endWordPosition="954">-T sentence to map to any position i of a length-V Chinese sentence. So VT alignments are possible, yielding an exponential space with correspondingly slow search times. &apos;Various models have been constructed by the IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &amp;quot;Model 2&amp;quot;; search costs for the more complex models are correspondingly higher. 3 A SITG Channel Model The translation channel we propose is based on the recently introduced bilingual language modeling approach. The model employs a stochastic version of an inversion transduction grammar or ITG (Wu, 1995c; Wu, 1995d; Wu, 1997). This formalism was originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. Subsequently, a method was developed to use a special case of the ITG—the aforementioned BTG—for the translation task itself (Wu, 1996). The next few paragraphs briefly review the main properties of ITGs, before we describe the SITG channel. An ITG consists of context-free productions where terminal symbols come in couples, for example xly, where x is a English word and y is an Chinese translation of x, with singletons of</context>
<context position="8424" citStr="Wu, 1995" startWordPosition="1290" endWordPosition="1291">symbols are visited left-to-right for English and right-to-left for Chinese. Thus, the tree: (3) WTI (Rook/WT [a/— €/* book/]NP IvP [for/M you/4]pp)VP Is produces translations with different word order: (4) a. [I [[took [a book]Np hip [for you]pp Iv p h b. [ft [[16g4}pp [*7 [—*INP IVP NP Is The surprising ability of ITGs to accommodate nearly all word-order variation between fixed-wordorder languages2 (English and Chinese in particular), has been analyzed mathematically, linguisti2With the exception of higher-order phenomena such as neg-raising and wh-movement. 1409 cally, and experimentally (Wu, 1995b; Wu, 1997). Any ITG can be transformed to an equivalent binary-branching normal form. A stochastic ITG associates a probability with each production. It follows that a SITG assigns a probability Pr(e, c, q) to all generable trees q and sentence-pairs. In principle it can be used as the translation channel model by normalizing with Pr(c) and integrating out Pr(q) to give Pr(eIc) in Equation (1). In practice, a strong language model makes this unnecessary, so we can instead optimize the simpler Viterbi approximation c* = argmax Pr(e, c, q) Pr(c) (2) To complete the picture we add a bigram mode</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. 1995b. Grammarless extraction of phrasal translation examples from parallel texts. In TMI-95, Proc. of the 6th bun Conf on Theoretical and Methodological Issues in Machine Translation, volume 2, pg 354-372, Leuven, Belgium, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora.</title>
<date>1995</date>
<booktitle>In Proc. of IJCAI-95, 14th Intri Joint Conf on Artificial Intelligence,</booktitle>
<pages>1328--1334</pages>
<location>Montreal,</location>
<contexts>
<context position="2190" citStr="Wu, 1995" startWordPosition="316" endWordPosition="317">tochastic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &amp;quot;stack search&amp;quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions of the space of word alignments. The SBTG can be regarded as a model of the language-universal hypothesis that closely related arguments tend to stay together (Wu, 1995a; Wu, 1995b). In this paper we introduce a generalization of Wu&apos;s method with the objectives of 1. increasing translation speed further, 2. improving meaning-preservation accuracy, 3. improving grammaticality of the output, and 4. seeding a natural transition toward transduction rule models, under the constraint of • employing no additional knowledge resources except a grammar for the target language. To achieve these objectives, we: • replace Wu&apos;s SBTG channel with a full stochastic inversion transduction grammar or SITG channel, discussed in Section 3, and • (mis-)use the target language gr</context>
<context position="6323" citStr="Wu, 1995" startWordPosition="953" endWordPosition="954">-T sentence to map to any position i of a length-V Chinese sentence. So VT alignments are possible, yielding an exponential space with correspondingly slow search times. &apos;Various models have been constructed by the IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &amp;quot;Model 2&amp;quot;; search costs for the more complex models are correspondingly higher. 3 A SITG Channel Model The translation channel we propose is based on the recently introduced bilingual language modeling approach. The model employs a stochastic version of an inversion transduction grammar or ITG (Wu, 1995c; Wu, 1995d; Wu, 1997). This formalism was originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. Subsequently, a method was developed to use a special case of the ITG—the aforementioned BTG—for the translation task itself (Wu, 1996). The next few paragraphs briefly review the main properties of ITGs, before we describe the SITG channel. An ITG consists of context-free productions where terminal symbols come in couples, for example xly, where x is a English word and y is an Chinese translation of x, with singletons of</context>
<context position="8424" citStr="Wu, 1995" startWordPosition="1290" endWordPosition="1291">symbols are visited left-to-right for English and right-to-left for Chinese. Thus, the tree: (3) WTI (Rook/WT [a/— €/* book/]NP IvP [for/M you/4]pp)VP Is produces translations with different word order: (4) a. [I [[took [a book]Np hip [for you]pp Iv p h b. [ft [[16g4}pp [*7 [—*INP IVP NP Is The surprising ability of ITGs to accommodate nearly all word-order variation between fixed-wordorder languages2 (English and Chinese in particular), has been analyzed mathematically, linguisti2With the exception of higher-order phenomena such as neg-raising and wh-movement. 1409 cally, and experimentally (Wu, 1995b; Wu, 1997). Any ITG can be transformed to an equivalent binary-branching normal form. A stochastic ITG associates a probability with each production. It follows that a SITG assigns a probability Pr(e, c, q) to all generable trees q and sentence-pairs. In principle it can be used as the translation channel model by normalizing with Pr(c) and integrating out Pr(q) to give Pr(eIc) in Equation (1). In practice, a strong language model makes this unnecessary, so we can instead optimize the simpler Viterbi approximation c* = argmax Pr(e, c, q) Pr(c) (2) To complete the picture we add a bigram mode</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. 1995c. Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora. In Proc. of IJCAI-95, 14th Intri Joint Conf on Artificial Intelligence, pg 1328-1334, Montreal, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Trainable coarse bilingual grammars for parallel text bracketing.</title>
<date>1995</date>
<booktitle>In Proc. of the 3rd Annual Workshop on Very Large Corpora, pg 69-81,</booktitle>
<location>Cambridge, MA,</location>
<contexts>
<context position="2190" citStr="Wu, 1995" startWordPosition="316" endWordPosition="317">tochastic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &amp;quot;stack search&amp;quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions of the space of word alignments. The SBTG can be regarded as a model of the language-universal hypothesis that closely related arguments tend to stay together (Wu, 1995a; Wu, 1995b). In this paper we introduce a generalization of Wu&apos;s method with the objectives of 1. increasing translation speed further, 2. improving meaning-preservation accuracy, 3. improving grammaticality of the output, and 4. seeding a natural transition toward transduction rule models, under the constraint of • employing no additional knowledge resources except a grammar for the target language. To achieve these objectives, we: • replace Wu&apos;s SBTG channel with a full stochastic inversion transduction grammar or SITG channel, discussed in Section 3, and • (mis-)use the target language gr</context>
<context position="6323" citStr="Wu, 1995" startWordPosition="953" endWordPosition="954">-T sentence to map to any position i of a length-V Chinese sentence. So VT alignments are possible, yielding an exponential space with correspondingly slow search times. &apos;Various models have been constructed by the IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &amp;quot;Model 2&amp;quot;; search costs for the more complex models are correspondingly higher. 3 A SITG Channel Model The translation channel we propose is based on the recently introduced bilingual language modeling approach. The model employs a stochastic version of an inversion transduction grammar or ITG (Wu, 1995c; Wu, 1995d; Wu, 1997). This formalism was originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. Subsequently, a method was developed to use a special case of the ITG—the aforementioned BTG—for the translation task itself (Wu, 1996). The next few paragraphs briefly review the main properties of ITGs, before we describe the SITG channel. An ITG consists of context-free productions where terminal symbols come in couples, for example xly, where x is a English word and y is an Chinese translation of x, with singletons of</context>
<context position="8424" citStr="Wu, 1995" startWordPosition="1290" endWordPosition="1291">symbols are visited left-to-right for English and right-to-left for Chinese. Thus, the tree: (3) WTI (Rook/WT [a/— €/* book/]NP IvP [for/M you/4]pp)VP Is produces translations with different word order: (4) a. [I [[took [a book]Np hip [for you]pp Iv p h b. [ft [[16g4}pp [*7 [—*INP IVP NP Is The surprising ability of ITGs to accommodate nearly all word-order variation between fixed-wordorder languages2 (English and Chinese in particular), has been analyzed mathematically, linguisti2With the exception of higher-order phenomena such as neg-raising and wh-movement. 1409 cally, and experimentally (Wu, 1995b; Wu, 1997). Any ITG can be transformed to an equivalent binary-branching normal form. A stochastic ITG associates a probability with each production. It follows that a SITG assigns a probability Pr(e, c, q) to all generable trees q and sentence-pairs. In principle it can be used as the translation channel model by normalizing with Pr(c) and integrating out Pr(q) to give Pr(eIc) in Equation (1). In practice, a strong language model makes this unnecessary, so we can instead optimize the simpler Viterbi approximation c* = argmax Pr(e, c, q) Pr(c) (2) To complete the picture we add a bigram mode</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. 1995d. Trainable coarse bilingual grammars for parallel text bracketing. In Proc. of the 3rd Annual Workshop on Very Large Corpora, pg 69-81, Cambridge, MA, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Conf of the Assoc. for Computational Linguistics, pg 152-158,</booktitle>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="1422" citStr="Wu, 1996" startWordPosition="203" endWordPosition="204">wever, unlike pure statistical translation models, the generated output string is guaranteed to conform to a given target grammar. The model employs only (1) a translation lexicon, (2) a context-free grammar for the target language, and (3) a bigram language model. The fact that no explicit bilingual translation rules are used makes the model easily portable to a variety of source languages. Initial experiments show that it also achieves significant speed gains over our earlier model. 1 Motivation Speed of statistical machine translation methods has long been an issue. A step was taken by Wu (Wu, 1996) who introduced a polynomial-time algorithm for the runtime search for an optimal translation. To achieve this, Wu&apos;s method substituted a language-independent stochastic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2. The SBTG channel made exhaustive search possible through dynamic programming, instead of previous &amp;quot;stack search&amp;quot; heuristics. Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model wordorder variation (between English and Chinese) even though it eliminates large portions </context>
<context position="6633" citStr="Wu, 1996" startWordPosition="1000" endWordPosition="1001">l 2&amp;quot;; search costs for the more complex models are correspondingly higher. 3 A SITG Channel Model The translation channel we propose is based on the recently introduced bilingual language modeling approach. The model employs a stochastic version of an inversion transduction grammar or ITG (Wu, 1995c; Wu, 1995d; Wu, 1997). This formalism was originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. Subsequently, a method was developed to use a special case of the ITG—the aforementioned BTG—for the translation task itself (Wu, 1996). The next few paragraphs briefly review the main properties of ITGs, before we describe the SITG channel. An ITG consists of context-free productions where terminal symbols come in couples, for example xly, where x is a English word and y is an Chinese translation of x, with singletons of the form x/f or El y representing function words that are used in only one of the languages. Any parse tree thus generates both English and Chinese strings simultaneously. Thus, the tree: (1) [IJ N [Rook/*7 [a/— f/* book/6 ]Np 1vp [for/8,i you/4]pp NI, Is produces, for example, the mutual translations: (2) a</context>
<context position="9133" citStr="Wu, 1996" startWordPosition="1411" endWordPosition="1412"> ITG associates a probability with each production. It follows that a SITG assigns a probability Pr(e, c, q) to all generable trees q and sentence-pairs. In principle it can be used as the translation channel model by normalizing with Pr(c) and integrating out Pr(q) to give Pr(eIc) in Equation (1). In practice, a strong language model makes this unnecessary, so we can instead optimize the simpler Viterbi approximation c* = argmax Pr(e, c, q) Pr(c) (2) To complete the picture we add a bigram model = g(cj I c3_1) for the Chinese language model Pr(c). This approach was used for the SBTG channel (Wu, 1996), using the language-independent bracketing degenerate case of the SITG:3 A Aby) [AA] Vx,y lexical translations A a[] (AA) Vx language 1 vocabulary A a() xly Vy language 2 vocabulary A -4 x/E b(x,€) EiY NE,Y) —&gt; In the proposed model, a structured languagedependent ITG is used instead. 4 A Grammatical Channel Model Stated radically, our novel modeling thesis is that a mirrored version of the target language grammar can parse sentences of the source language. Ideally, an ITG would be tailored for the desired source and target languages, enumerating the transduction patterns specific to that lan</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Dekai Wu. 1996. A polynomial-time algorithm for statistical machine translation. In Proc. of the 34th Annual Conf of the Assoc. for Computational Linguistics, pg 152-158, Santa Cruz, CA, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="6346" citStr="Wu, 1997" startWordPosition="957" endWordPosition="958">any position i of a length-V Chinese sentence. So VT alignments are possible, yielding an exponential space with correspondingly slow search times. &apos;Various models have been constructed by the IBM team (Brown et al., 1993). This description corresponds to one of the simplest ones, &amp;quot;Model 2&amp;quot;; search costs for the more complex models are correspondingly higher. 3 A SITG Channel Model The translation channel we propose is based on the recently introduced bilingual language modeling approach. The model employs a stochastic version of an inversion transduction grammar or ITG (Wu, 1995c; Wu, 1995d; Wu, 1997). This formalism was originally developed for the purpose of parallel corpus annotation, with applications for bracketing, alignment, and segmentation. Subsequently, a method was developed to use a special case of the ITG—the aforementioned BTG—for the translation task itself (Wu, 1996). The next few paragraphs briefly review the main properties of ITGs, before we describe the SITG channel. An ITG consists of context-free productions where terminal symbols come in couples, for example xly, where x is a English word and y is an Chinese translation of x, with singletons of the form x/f or El y r</context>
<context position="8436" citStr="Wu, 1997" startWordPosition="1292" endWordPosition="1293"> visited left-to-right for English and right-to-left for Chinese. Thus, the tree: (3) WTI (Rook/WT [a/— €/* book/]NP IvP [for/M you/4]pp)VP Is produces translations with different word order: (4) a. [I [[took [a book]Np hip [for you]pp Iv p h b. [ft [[16g4}pp [*7 [—*INP IVP NP Is The surprising ability of ITGs to accommodate nearly all word-order variation between fixed-wordorder languages2 (English and Chinese in particular), has been analyzed mathematically, linguisti2With the exception of higher-order phenomena such as neg-raising and wh-movement. 1409 cally, and experimentally (Wu, 1995b; Wu, 1997). Any ITG can be transformed to an equivalent binary-branching normal form. A stochastic ITG associates a probability with each production. It follows that a SITG assigns a probability Pr(e, c, q) to all generable trees q and sentence-pairs. In principle it can be used as the translation channel model by normalizing with Pr(c) and integrating out Pr(q) to give Pr(eIc) in Equation (1). In practice, a strong language model makes this unnecessary, so we can instead optimize the simpler Viterbi approximation c* = argmax Pr(e, c, q) Pr(c) (2) To complete the picture we add a bigram model = g(cj I c</context>
<context position="11909" citStr="Wu, 1997" startWordPosition="1872" endWordPosition="1873">ing example illustrates how production mirroring works. Consider the input sentence He is the son of Stephen, which can be parsed by the ITG of Figure 1 to yield the corresponding output sentenceft &apos;115-4-31:Erg927-, with the following parse tree: (5) [[[He/ft Jp„,]Np [[is/ Iv [the/E]NoisE ([son/92 IN [of/Ergimod [Stephen/54 3:1- IN )NP Iv [Jo 1Punc Is Production mirroring produced the inverted NP constituent which was necessary to parse son of Stephen, i.e., (son/ 92,T °fin Stephen/sU631 )Np. If the target CFG is purely binary branching, then the previous theoretical and linguistic analyses (Wu, 1997) suggest that much of the requisite constituent and word order transposition may be accommodated without change to the mirrored ITG. On the other hand, if the target CFG contains productions with long right-hand-sides, then merely inverting the subconstituent order will probably be insufficient. In such cases, a more complex transformation heuristic would be needed. Objective 3 (improving grammaticality of the output) can be directly tackled by using a tight tar4Except for unary productions, which yield only one bilingual production. 1410 get grammar. To see this, consider using a mirrored Chi</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377-404, Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages</title>
<date>1967</date>
<booktitle>in time n3. Information and Control,</booktitle>
<pages>0--2</pages>
<contexts>
<context position="16853" citStr="Younger, 1967" startWordPosition="2703" endWordPosition="2704"> position of the item. The the located on the adjacent to the dot position of the item is skipped. Word-skipping provides us the flexibility to parse the source input by skipping possible singleton(s), if when we doing so, the source input can be parsed with the highest likelihood, and grammatical output can be produced. 5 Translation Algorithm The translation search algorithm differs from that of Wu&apos;s SBTG model in that it handles arbitrary grammars rather than binary bracketing grammars. As such it is more similar to active chart parsing (Earley, 1970) rather than CYK parsing (Kasami, 1965; Younger, 1967). We take the standard notion of items (Aho and Ullman, 1972), and use the term anticipation to mean an item which still has symbols right of its dot. Items that don&apos;t have any symbols right of the dot are called subtree. As with Wu&apos;s SBTG model, the algorithm maximizes a probabilistic objective function, Equa1411 tion (2), using dynamic programming similar to that for HMM recognition (Viterbi, 1967). The presence of the bigram model in the objective function necessitates indexes in the recurrence not only on subtrees over the source English string, but also on the delimiting words of the targ</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>David H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, I0(2):189-208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>