<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016643">
<title confidence="0.9946005">
Edinburgh’s Submission to all Tracks of the WMT2009 Shared Task
with Reordering and Speed Improvements to Moses
</title>
<author confidence="0.994504">
Philipp Koehn and Barry Haddow
</author>
<affiliation confidence="0.9985385">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.991233">
pkoehn@inf.ed.ac.uk bhaddow@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999784">
Edinburgh University participated in the
WMT 2009 shared task using the Moses
phrase-based statistical machine transla-
tion decoder, building systems for all lan-
guage pairs. The system configuration was
identical for all language pairs (with a few
additional components for the German-
English language pairs). This paper de-
scribes the configuration of the systems,
plus novel contributions to Moses includ-
ing truecasing, more efficient decoding
methods, and a framework to specify re-
ordering constraints.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999764692307692">
The commitment of the University of Edinburgh to
the WMT shared tasks is to provide a strong sta-
tistical machine translation baseline with our open
source tools for all language pairs. We are again
the only institution that participated in all tracks.
The shared task is also an opportunity to incor-
porate novel contributions and test them against
the best machine translation systems for these lan-
guage pairs. In this paper we describe the speed
improvements to the Moses decoder (Koehn et al.,
2007), as well as a novel framework to specify re-
ordering constraints with XML markup, which we
tested with punctuation-based constraints.
</bodyText>
<sectionHeader confidence="0.992283" genericHeader="method">
2 System Configuration
</sectionHeader>
<bodyText confidence="0.9983265">
We trained a default Moses system with the fol-
lowing non-default settings:
</bodyText>
<listItem confidence="0.999819833333333">
• maximum sentence length 80
• grow-diag-final-and symmetrization of
GIZA++ alignments
• interpolated Kneser-Ney discounted 5-gram
language model
• msd-bidrectional-fe lexicalized reordering
</listItem>
<table confidence="0.999025428571429">
Language ep nc news intpl.
English 449 486 216 192
French 264 311 147 131
German 785 821 449 402
Spanish 341 392 219 190
Czech *:1475 1615 752 690
Hungarian hung:2148 815 786
</table>
<tableCaption confidence="0.999227">
Table 1: Perplexity (ppl) of the domain-trained (ep
</tableCaption>
<bodyText confidence="0.905949333333333">
= Europarl (CzEng for Czech), nc = News Com-
mentary, news = News) and interpolated language
models.
</bodyText>
<subsectionHeader confidence="0.989185">
2.1 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999991642857143">
In contrast to last year’s task, where news transla-
tion was presented as a true out-of-domain prob-
lem, this year large monolingual news corpora
and a tuning set (last year’s test set) were pro-
vided. While still no in-domain news parallel cor-
pora were made available, the monolingual cor-
pora could be exploited for domain adaption.
For all language pairs, we built a 5-gram lan-
guage model, by first training separate language
models for the different training corpora (the par-
allel Europarl and News Commentary and new
monolingual news), and then interpolated them by
optimizing perplexity on the provided tuning set.
Perplexity numbers are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.998636">
2.2 Truecasing
</subsectionHeader>
<bodyText confidence="0.903086">
Our traditional method to handle case is to low-
ercase all training data, and then have a separate
recasing (or recapitalization) step. Last year, we
used truecasing: all words are normalized to their
natural case, e.g. the, John, eBay, meaning that
only sentence-leading words may be changed to
their most frequent form.
To refine last year’s approach, we record the
seen truecased instances and truecase words in test
sentences (even in the middle of sentences) to seen
forms, if possible.
Truecasing leads to small degradation in case-
Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 160–164,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.988214">
160
</page>
<table confidence="0.999528">
language pair baseline w/ news mbr/mp truecased big beam ued’08 best’08
French-English uncased 21.2 23.1 23.3 22.7 22.9 19.2 21.9
cased 21.7 21.6 21.8
English-French uncased 17.8 19.4 19.6 19.6 19.7 18.2 21.4
cased 18.1 18.7 18.8
Spanish-English uncased 22.5 24.4 24.7 24.5 24.7 20.1 22.9
cased 23.0 23.3 23.4
English-Spanish uncased 22.4 23.9 24.2 23.8 24.4 20.7 22.7
cased 22.1 22.8 23.1
Czech-English uncased 16.9 18.9 18.9 18.6 18.6 14.5 14.7
cased 17.3 17.4 17.4
English-Czech uncased 11.4 13.5 13.6 13.6 13.8 9.6 11.9
cased 12.2 13.0 13.2
Hungarian-English uncased - 11.3 11.4 10.9 11.0 8.8
cased 8.3 10.1 10.2
English-Hungarian uncased - 9.0 9.3 9.2 9.5 6.5
cased 8.1 8.4 8.7
</table>
<tableCaption confidence="0.996951">
Table 2: Results overview for news-dev2009b sets: We see significant BLEU score increases with the
</tableCaption>
<bodyText confidence="0.912854333333333">
addition of news data to the language model and using truecasing. As a comparison our results and the
best systems from last year on the full news-dev2009 set are shown.
insensitive BLEU, but to a significant gain in case-
sensitive BLEU. Note that we still do not properly
address all-caps portions or headlines with our ap-
proach.
</bodyText>
<subsectionHeader confidence="0.889236">
2.3 Results
</subsectionHeader>
<bodyText confidence="0.99993175">
Results on the development sets are summarized
in Table 2. We see significant gains with the addi-
tion of news data to the language model (about 2
BLEU points) and using truecasing (about 0.5–1.0
BLEU points), and minor if any gains using min-
imum Bayes risk decoding (mbr), the monotone-
at-punctuation reordering constraint (mp, see Sec-
tion 3.2), and bigger beam sizes.
</bodyText>
<subsectionHeader confidence="0.998308">
2.4 German–English
</subsectionHeader>
<bodyText confidence="0.998211117647059">
For German–English, we additionally incorpo-
rated
rule-based reordering — We parse the input us-
ing the Collins parser (Collins, 1997) and ap-
ply a set of reordering rules to re-arrange the
German sentence so that it corresponds more
closely English word order (Collins et al.,
2005).
compound splitting — We split German com-
pound words (mostly nouns), based on the
frequency of the words in the potential de-
compositions (Koehn and Knight, 2003a).
part-of-speech language model — We use fac-
tored translation models (Koehn and Hoang,
2007) to also output part-of-speech tags with
each word in a single phrase mapping and run
a second n-gram model over them. The En-
</bodyText>
<table confidence="0.998514909090909">
German–English BLEU
(ued’08: 17.1, best’08: 19.7) (uncased)
baseline 16.6
+ interpolated news LM 20.6
+ minimum Bayes risk decoding 20.6
+ monotone at punctuation 20.9
+ truecasing 20.9
+ rule-based reordering 21.7
+ compound splitting 22.0
+ part-of-speech LM 22.1
+ big beam 22.3
</table>
<tableCaption confidence="0.9926">
Table 3: Results for German–English with the in-
cremental addition of methods beyond a baseline
trained on the parallel corpus
</tableCaption>
<table confidence="0.999808333333333">
English–German BLEU
(ued’08: 12.1, best’08: 14.2) (uncased)
baseline 13.5
+ interpolated news LM 15.2
+ minimum Bayes risk decoding 15.2
+ monotone at punctuation 15.2
+ truecasing 15.2
+ morphological LM 15.2
+ big beam 15.7
</table>
<tableCaption confidence="0.981974">
Table 4: Results for English–German with the in-
</tableCaption>
<bodyText confidence="0.52451975">
cremental addition of methods beyiond a baseline
trained on the parallel corpus
glish part-of-speech tags are obtained using
MXPOST (Ratnaparkhi, 1996).
</bodyText>
<subsectionHeader confidence="0.979749">
2.5 English-German
</subsectionHeader>
<bodyText confidence="0.999950833333333">
For English–German, we additionally incorpo-
rated a morphological language model the same
way we incorporated a part-of-speech language
model in the other translation direction. The
morphological tags were obtained using LoPar
(Schmidt and Schulte im Walde, 2000).
</bodyText>
<page confidence="0.97368">
161
</page>
<figure confidence="0.9825475">
&amp;
German-English &amp; French-English
</figure>
<figureCaption confidence="0.999744">
Figure 1: Early discarding results in speedier but still accurate search, compared to reducing stack size.
</figureCaption>
<sectionHeader confidence="0.993153" genericHeader="method">
3 Recent Improvements
</sectionHeader>
<bodyText confidence="0.999382">
In this section, we describe recent improvements
to the Moses decoder for the WMT 2009 shared
task.
</bodyText>
<subsectionHeader confidence="0.996217">
3.1 Early Discarding
</subsectionHeader>
<bodyText confidence="0.999146125">
We implemented in Moses a more efficient beam
search, following suggestions by Moore and Quirk
(2007). In short, the guiding principle of this work
is not to build a hypothesis and not to compute its
language model scores, if it is likely to be too bad
anyway.
Before a hypothesis is generated, the following
checks are employed:
</bodyText>
<listItem confidence="0.998980375">
1. the minimum allowed score for a hypothesis
is the worst score on the stack (if full) or the
threshold for the stack (if higher or stack not
full) plus an early discarding threshold cush-
ion
2. if (a) new hypothesis future score, (b) the cur-
rent hypothesis actual score, and (c) the fu-
ture cost of the translation option are worse
than the allowed score, do not generate the
hypothesis
3. if adding all real costs except for the language
model costs (i.e., reordering costs) makes the
score worse than the allowed score, do not
generate the hypothesis.
4. complete generation of the hypothesis and
add it to the stack
</listItem>
<bodyText confidence="0.999961323529412">
Note that check 1 and 2 mostly consists of
adding and comparing already computed values.
In our implementation, step 3 implies the some-
what costly construction of the hypothesis data
structure, while step 4 performs the expensive
language model calculation. Without these opti-
mizations, the decoder spends about 60-70% of
the search time computing language model scores.
With these optimization, the vast majority of po-
tential hypotheses are not built.
See Figure 1 for the time/search-accuracy trade-
offs using this early discarding strategy. Given
a stack size, we can vary the threshold cushion
mentioned in step 1 above. A tighter threshold
(the factor 1.0 implies no cushion at all), results
in speedier but worse search. Note, however, that
the degradation in quality for a given time point
is less severe than the alternative — reducing the
stack size (and also tightening the beam thresh-
old, not shown in the figure). To mention just two
data points in the German-English setting: Stack
size of 500 and early discarding threshold of 1.0
results in faster search (150ms/word) and better
quality (73.5% search accuracy) than the default
search setting of a stack size 200 and no early dis-
carding (252ms/word for 62.5% seach accuracy).
Accuracy is measured against the best translations
found under any setting.
Note that this early discarding is related to ideas
behind cube pruning (Huang and Chiang, 2007),
which generates the top n most promising hy-
potheses, but in our method the decision not to
generate hypotheses is guided by the quality of hy-
potheses on the result stack.
</bodyText>
<subsectionHeader confidence="0.98329">
3.2 Framework to Specify Reordering
Constraints
</subsectionHeader>
<bodyText confidence="0.999911333333333">
Commonly in statistical machine translation,
punctuation tokens are treated just like words. For
tokens such as commas, many possible transla-
tions are collected and they may be translated into
any of these choices or reordered if the language
model sees gains. In fact, since the comma is one
</bodyText>
<page confidence="0.993892">
162
</page>
<table confidence="0.9963449">
�Requiring the translation of quoted material as a block: �
He said &lt;zone&gt; &amp;quot; yes &amp;quot; &lt;/zone&gt; .
Hard reordering constraint:
Number 1 : &lt;wall/&gt; the beginning .
Local hard reordering constraint within zone:
A new idea &lt;zone&gt; ( &lt;wall/&gt; maybe not new &lt;wall/&gt; ) &lt;/zone&gt; has come forward .
Nesting:
�
The &lt;zone&gt; &amp;quot; new &lt;zone&gt; ( old ) &lt;/zone&gt; &amp;quot; &lt;/zone&gt; proposal .
�
</table>
<figureCaption confidence="0.978267333333333">
Figure 2: Framework to specify reordering constraints with zones and walls. Words within zones have
to be translated without reordering with outside material. Walls form hard reordering constraints, over
which words may not be reordered (limited to zones, if defined within them).
</figureCaption>
<bodyText confidence="0.992869555555555">
the most frequent tokens in a corpus and not very
consistently translated across languages, it has a
very noisy translation table, often with 10,000s if
not 100,000s of translations.
Punctuation has a meaningful role in structur-
ing a sentence, and we see some gains exploiting
this in the systems we built last year. By dis-
allowing reordering over commas and sentence-
ending punctuation, we avoid mixing words from
different clauses, and typically see gains of 0.1–
0.2 BLEU.
But also other punctuation tokens imply re-
ordering constraints. Parentheses, brackets, and
quotation marks typically define units that should
be translated as blocks, meaning that words should
not be moved in or out of sequences in quotes and
alike.
To handle such reordering constraints, we intro-
duced a framework that uses what we call zones
and walls. A zone is a sequence of words that
should be translated as block. This does not mean
that the sequence cannot be reordered as a whole,
but that once we start to translate words in a zone,
we have to finish all its words before moving out-
side again. To put it another way: words may not
reordered into or out of zones.
A wall is a hard reordering constraint that re-
quires that all words preceeding it have to be trans-
lated before words after may be translated. If we
specify walls within zones, then we consider them
local walls where the before-mentioned constraint
only applies within the zone.
Walls and zones may be specified with XML
markup to the Moses decoder. See Figure 2 for a
few examples. We use the extended XML frame-
work to
</bodyText>
<listItem confidence="0.959059125">
1. limit reordering of clause-ending punctuation
(walls)
2. define zones for quoted and parenthetical
word sequences
3. limit reordering of quotes and parentheses
(local walls within zones)
4. specify translations for punctuation (not
comma).
</listItem>
<bodyText confidence="0.999346111111111">
Only (1) leads to any noticable change in BLEU in
the WMT 2009 shared task, a slight gain 0.1–0.2.
Note that this framework may be used in other
ways. For instance, we may want to revisit
our work on noun phrase translation (Koehn and
Knight, 2003b), and check if enforcing the trans-
lation of noun phrases as blocks is beneficial
or harmful to overall machine translation perfor-
mance.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999892">
This work was supported by the EuroMatrix
project funded by the European Commission (6th
Framework Programme) and made use of the re-
sources provided by the Edinburgh Compute and
Data Facility (http://www.ecdf.ed.ac.uk/).
The ECDF is partially supported by the eDIKT
initiative (http://www.edikt.org.uk/).
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983315714285714">
Collins, M. (1997). Three generative, lexicalized
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association of
Computational Linguistics (ACL).
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
</reference>
<page confidence="0.999186">
163
</page>
<bodyText confidence="0.808563833333333">
Meeting of the Association for Computational
Linguistics (ACL’05), pages 531–540, Ann Ar-
bor, Michigan. Association for Computational
Linguistics.
Huang, L. and Chiang, D. (2007). Forest rescor-
ing: Faster decoding with integrated language
</bodyText>
<reference confidence="0.995836636363636">
models. In Proceedings of the 45th Annual
Meeting of the Association of Computational
Linguistics, pages 144–151, Prague, Czech Re-
public. Association for Computational Linguis-
tics.
Koehn, P. and Hoang, H. (2007). Factored trans-
lation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 868–876.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B., Shen,
W., Moran, C., Zens, R., Dyer, C. J., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions,
pages 177–180, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
Koehn, P. and Knight, K. (2003a). Empirical
methods for compound splitting. In Proceed-
ings of Meeting of the European Chapter of
the Association of Computational Linguistics
(EACL).
Koehn, P. and Knight, K. (2003b). Feature-rich
translation of noun phrases. In 41st Annual
Meeting of the Association of Computational
Linguistics (ACL).
Moore, R. C. and Quirk, C. (2007). Faster beam-
search decoding for phrasal statistical machine
translation. In Proceedings of the MT Summit
XI.
Ratnaparkhi, A. (1996). A maximum entropy part-
of-speech tagger. In Proceedings of the Empir-
ical Methods in Natural Language Processing
Conference.
Schmidt, H. and Schulte im Walde, S. (2000). Ro-
bust German noun chunking with a probabilistic
context-free grammar. In Proceedings of the In-
ternational Conference on Computational Lin-
guistics (COLING).
</reference>
<page confidence="0.99849">
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.754048">
<title confidence="0.972946">Edinburgh’s Submission to all Tracks of the WMT2009 Shared with Reordering and Speed Improvements to Moses</title>
<author confidence="0.998768">Philipp Koehn</author>
<author confidence="0.998768">Barry</author>
<affiliation confidence="0.9963625">School of University of</affiliation>
<email confidence="0.841222">pkoehn@inf.ed.ac.ukbhaddow@inf.ed.ac.uk</email>
<abstract confidence="0.9964795">Edinburgh University participated in the WMT 2009 shared task using the Moses phrase-based statistical machine translation decoder, building systems for all language pairs. The system configuration was identical for all language pairs (with a few additional components for the German- English language pairs). This paper describes the configuration of the systems, plus novel contributions to Moses including truecasing, more efficient decoding methods, and a framework to specify reordering constraints.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5062" citStr="Collins, 1997" startWordPosition="801" endWordPosition="802">we still do not properly address all-caps portions or headlines with our approach. 2.3 Results Results on the development sets are summarized in Table 2. We see significant gains with the addition of news data to the language model (about 2 BLEU points) and using truecasing (about 0.5–1.0 BLEU points), and minor if any gains using minimum Bayes risk decoding (mbr), the monotoneat-punctuation reordering constraint (mp, see Section 3.2), and bigger beam sizes. 2.4 German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnGerman–English BLEU (ued’08: 17.1, best’08: 19.7) (uncased) baseline 16.</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, M. (1997). Three generative, lexicalized models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>144--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5210" citStr="Collins et al., 2005" startWordPosition="825" endWordPosition="828"> in Table 2. We see significant gains with the addition of news data to the language model (about 2 BLEU points) and using truecasing (about 0.5–1.0 BLEU points), and minor if any gains using minimum Bayes risk decoding (mbr), the monotoneat-punctuation reordering constraint (mp, see Section 3.2), and bigger beam sizes. 2.4 German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnGerman–English BLEU (ued’08: 17.1, best’08: 19.7) (uncased) baseline 16.6 + interpolated news LM 20.6 + minimum Bayes risk decoding 20.6 + monotone at punctuation 20.9 + truecasing 20.9 + rule-based reordering 21.7 + com</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Collins, M., Koehn, P., and Kucerova, I. (2005). Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>868--876</pages>
<contexts>
<context position="5465" citStr="Koehn and Hoang, 2007" startWordPosition="866" endWordPosition="869">reordering constraint (mp, see Section 3.2), and bigger beam sizes. 2.4 German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnGerman–English BLEU (ued’08: 17.1, best’08: 19.7) (uncased) baseline 16.6 + interpolated news LM 20.6 + minimum Bayes risk decoding 20.6 + monotone at punctuation 20.9 + truecasing 20.9 + rule-based reordering 21.7 + compound splitting 22.0 + part-of-speech LM 22.1 + big beam 22.3 Table 3: Results for German–English with the incremental addition of methods beyond a baseline trained on the parallel corpus English–German BLEU (ued’08: 12.1, best’08: 14.2) (uncased) baselin</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Koehn, P. and Hoang, H. (2007). Factored translation models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 868–876.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C J Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1258" citStr="Koehn et al., 2007" startWordPosition="187" endWordPosition="190">luding truecasing, more efficient decoding methods, and a framework to specify reordering constraints. 1 Introduction The commitment of the University of Edinburgh to the WMT shared tasks is to provide a strong statistical machine translation baseline with our open source tools for all language pairs. We are again the only institution that participated in all tracks. The shared task is also an opportunity to incorporate novel contributions and test them against the best machine translation systems for these language pairs. In this paper we describe the speed improvements to the Moses decoder (Koehn et al., 2007), as well as a novel framework to specify reordering constraints with XML markup, which we tested with punctuation-based constraints. 2 System Configuration We trained a default Moses system with the following non-default settings: • maximum sentence length 80 • grow-diag-final-and symmetrization of GIZA++ alignments • interpolated Kneser-Ney discounted 5-gram language model • msd-bidrectional-fe lexicalized reordering Language ep nc news intpl. English 449 486 216 192 French 264 311 147 131 German 785 821 449 402 Spanish 341 392 219 190 Czech *:1475 1615 752 690 Hungarian hung:2148 815 786 Ta</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C. J., Bojar, O., Constantin, A., and Herbst, E. (2007). Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of Meeting of the European Chapter of the Association of Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="5371" citStr="Koehn and Knight, 2003" startWordPosition="852" endWordPosition="855">), and minor if any gains using minimum Bayes risk decoding (mbr), the monotoneat-punctuation reordering constraint (mp, see Section 3.2), and bigger beam sizes. 2.4 German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnGerman–English BLEU (ued’08: 17.1, best’08: 19.7) (uncased) baseline 16.6 + interpolated news LM 20.6 + minimum Bayes risk decoding 20.6 + monotone at punctuation 20.9 + truecasing 20.9 + rule-based reordering 21.7 + compound splitting 22.0 + part-of-speech LM 22.1 + big beam 22.3 Table 3: Results for German–English with the incremental addition of methods beyond a baseline trai</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Koehn, P. and Knight, K. (2003a). Empirical methods for compound splitting. In Proceedings of Meeting of the European Chapter of the Association of Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Feature-rich translation of noun phrases.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5371" citStr="Koehn and Knight, 2003" startWordPosition="852" endWordPosition="855">), and minor if any gains using minimum Bayes risk decoding (mbr), the monotoneat-punctuation reordering constraint (mp, see Section 3.2), and bigger beam sizes. 2.4 German–English For German–English, we additionally incorporated rule-based reordering — We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al., 2005). compound splitting — We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). part-of-speech language model — We use factored translation models (Koehn and Hoang, 2007) to also output part-of-speech tags with each word in a single phrase mapping and run a second n-gram model over them. The EnGerman–English BLEU (ued’08: 17.1, best’08: 19.7) (uncased) baseline 16.6 + interpolated news LM 20.6 + minimum Bayes risk decoding 20.6 + monotone at punctuation 20.9 + truecasing 20.9 + rule-based reordering 21.7 + compound splitting 22.0 + part-of-speech LM 22.1 + big beam 22.3 Table 3: Results for German–English with the incremental addition of methods beyond a baseline trai</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Koehn, P. and Knight, K. (2003b). Feature-rich translation of noun phrases. In 41st Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>C Quirk</author>
</authors>
<title>Faster beamsearch decoding for phrasal statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the MT</booktitle>
<location>Summit XI.</location>
<contexts>
<context position="7096" citStr="Moore and Quirk (2007)" startWordPosition="1117" endWordPosition="1120">ally incorporated a morphological language model the same way we incorporated a part-of-speech language model in the other translation direction. The morphological tags were obtained using LoPar (Schmidt and Schulte im Walde, 2000). 161 &amp; German-English &amp; French-English Figure 1: Early discarding results in speedier but still accurate search, compared to reducing stack size. 3 Recent Improvements In this section, we describe recent improvements to the Moses decoder for the WMT 2009 shared task. 3.1 Early Discarding We implemented in Moses a more efficient beam search, following suggestions by Moore and Quirk (2007). In short, the guiding principle of this work is not to build a hypothesis and not to compute its language model scores, if it is likely to be too bad anyway. Before a hypothesis is generated, the following checks are employed: 1. the minimum allowed score for a hypothesis is the worst score on the stack (if full) or the threshold for the stack (if higher or stack not full) plus an early discarding threshold cushion 2. if (a) new hypothesis future score, (b) the current hypothesis actual score, and (c) the future cost of the translation option are worse than the allowed score, do not generate</context>
</contexts>
<marker>Moore, Quirk, 2007</marker>
<rawString>Moore, R. C. and Quirk, C. (2007). Faster beamsearch decoding for phrasal statistical machine translation. In Proceedings of the MT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference.</booktitle>
<contexts>
<context position="6422" citStr="Ratnaparkhi, 1996" startWordPosition="1020" endWordPosition="1021"> + compound splitting 22.0 + part-of-speech LM 22.1 + big beam 22.3 Table 3: Results for German–English with the incremental addition of methods beyond a baseline trained on the parallel corpus English–German BLEU (ued’08: 12.1, best’08: 14.2) (uncased) baseline 13.5 + interpolated news LM 15.2 + minimum Bayes risk decoding 15.2 + monotone at punctuation 15.2 + truecasing 15.2 + morphological LM 15.2 + big beam 15.7 Table 4: Results for English–German with the incremental addition of methods beyiond a baseline trained on the parallel corpus glish part-of-speech tags are obtained using MXPOST (Ratnaparkhi, 1996). 2.5 English-German For English–German, we additionally incorporated a morphological language model the same way we incorporated a part-of-speech language model in the other translation direction. The morphological tags were obtained using LoPar (Schmidt and Schulte im Walde, 2000). 161 &amp; German-English &amp; French-English Figure 1: Early discarding results in speedier but still accurate search, compared to reducing stack size. 3 Recent Improvements In this section, we describe recent improvements to the Moses decoder for the WMT 2009 shared task. 3.1 Early Discarding We implemented in Moses a m</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, A. (1996). A maximum entropy partof-speech tagger. In Proceedings of the Empirical Methods in Natural Language Processing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmidt</author>
<author>Schulte im Walde</author>
<author>S</author>
</authors>
<title>Robust German noun chunking with a probabilistic context-free grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING).</booktitle>
<marker>Schmidt, Walde, S, 2000</marker>
<rawString>Schmidt, H. and Schulte im Walde, S. (2000). Robust German noun chunking with a probabilistic context-free grammar. In Proceedings of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>