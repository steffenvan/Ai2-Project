<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002427">
<title confidence="0.771811">
Statistical Named Entity Recognizer Adaptation
</title>
<note confidence="0.62655">
John D. Burger and John C. Henderson and William T. Morgan
The MITRE Corporation
MS K309
</note>
<address confidence="0.907781">
202 Burlington Road
Bedford; MA 01730-1420
</address>
<email confidence="0.997608">
fjohn,jhndrsn,wmorganl@mitre.org
</email>
<sectionHeader confidence="0.999371" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941875">
Named entity recognition (NER) is a subtask
of widely-recognized utility of information ex-
traction (IE). NER has been explored in depth
to provide rapid characterization of newswire
data (Sundheim, 1995; Palmer and Day, 1997).
The NER task involves both identification of
spans of text referring to named entities, and
categorization of these entities into classes based
on the role they fill in context. The sentence
&amp;quot;Washington announced that Washington ate
seven hotdogs in Washington&amp;quot; provides an ex-
ample in which a single name can arguably re-
fer to three different entities: an organization,
a person, and a location.
Following the paradigm introduced by
Ramshaw and Marcus (1999), many researchers
reduce the NER problem to a word-tagging
problem, and address it with techniques similar
to those used for part of speech tagging (Meteer
et al., 1991; Brill, 1995). Borthwick explores
the maximum entropy approach in his dis-
sertation (1999). Collins and Singer (1999)
investigate semi-unsupervised methods for
named entity categorization. Cucerzan and
Yarowsky (1999) produce a unified technique
for producing NER systems for several lan-
guages, utilizing extensive bootstrapping from
small amounts of supervised data with an EM-
style algorithm. Miller et al. (2000) produce
a statistical Hidden Markov Model (1-1MM)
for NER which is similar to the one used by
Palmer et al. (1999); the latter system, named
phrag, is the NER engine utilized in the work
described in this paper.
The experiments described herein explore un-
supervised approaches to NER, with an eye to-
ward using unannotated corpora consisting of
a few hundred million words. Recent word
sense disambiguation results suggest that some
simple techniques can scale well with increased
data sizes (Banko and Brill, 2001). This pa-
per presents several experiments in adapting a
HMM-based named entity recognizer to a target
data set. Our core learning engine is a word-
based HMM, and we show two techniques, in-
formed smoothing and iterative adaptation, for
incorporating unsupervised data into the model,
which provide overall gains in performance.
</bodyText>
<subsectionHeader confidence="0.99353">
1.1 phrag
</subsectionHeader>
<bodyText confidence="0.999976185185185">
phrag&apos; is a trainable phrase tagger based on
HMMs. phrag uses bigram language models,
i.e., state emissions are conditioned on the pre-
vious word only. State transitions are similarly
conditioned, which allows the model to cap-
ture context words, such as &amp;quot;Mr&amp;quot;. All models
are smoothed with type c Witten-Bell discount-
ing (Bell et al., 1990). For named entity recogni-
tion, the typical HMM topology has two states
per phrase type. The first word of each phrase
is generated by the first state, and any subse-
quent words are generated by the second state.
This is essentially the BII scheme employed by
many chunkers, e.g. (Tjong Kim Sang, 2002).
Figure 1 presents a sketch of this topology.
The lexicon is constructed from the train-
ing corpus, excluding the least frequent
words. These words are pooled to form un-
known word models, e.g., unknown-number,
unknown-punctuation, unknown-alphabetic.
phrag can also use auxiliary resources such as
word lists to form additional equivalence classes.
These are consulted, in order of preference,
when a word is not in the main lexicon. If
the word is not in any word list, it is relegated
to the appropriate &amp;quot;truly unknown&amp;quot; class de-
scribed above. Language model statistics for
</bodyText>
<figure confidence="0.567195">
lhttp://www.openchannelsoftware.org/projects/
Qanda/.
Location1 Location2
Start
End
Other1
Other2
</figure>
<table confidence="0.967183666666667">
S D Size Description
,N/ ,N/ 120146 English location words from the TIPSTER gazetteer
88799 English surnames (U.S. Census Bureau, 1995)
,N/ 17576 All three-letter acronyms, AAA through ZZZ
&apos;N/ 5163 English given names (U.S. Census Bureau, 1995)
&apos;N/ 1410 Dutch surnames (Dupon, 2000)
&apos;N/ 1162 Dutch given names (Dupon, 2000)
&apos;N/ 1410 Dutch province and city names (Kuyper, 1865)
639 Spanish surnames (Word and Perkins, 1996)
362 Spanish names of capitals (prominent global cities)
203 Spanish geographic adjectives, e.g. &amp;quot;norteamericano&amp;quot;
138 Spanish country words e.g. &amp;quot;Estatos&amp;quot; and &amp;quot;Unidos&amp;quot;
</table>
<tableCaption confidence="0.999957">
Table 1: External word lists introduced to Spanish and Dutch wdlist systems.
</tableCaption>
<bodyText confidence="0.9989835">
words of Spanish Newswire, both distributed by
the LDC,4 and 2.3 million words of Dutch text,
harvested from the Dutch news site Planet In-
ternet.5 For the experiments below, a randomly
drawn subsa,mple of 200, 000 words of each lan-
guage was used.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="acknowledgments">
3 Results
</sectionHeader>
<bodyText confidence="0.999161428571429">
Table 2 presents the results of our experiments.
The left column gives an experiment label, and
the subsequent columns indicate the overall F-
measure as given by the CoNLL scoring software
for the Spanish and Dutch development and test
sets. The rows are sorted by performance on the
Spanish development set.
</bodyText>
<table confidence="0.950579777777778">
System
phrag* 69.13 74.01 66.60 71.23
rand 68.39 73.08 63.75 67.45
rviterbi 69.43 73.61 67.57 70.53
wdlst 70.49 74.37 70.20 72.60
adapt* 70.92 75.13 66.68 71.73
bridge* 71.69 75.51 70.25 73.51
pbridge* 72.00 75.77 70.61 72.57
tpbridge* 72.25 75.78 69.63 72.86
</table>
<tableCaption confidence="0.993168">
Table 2: Summary of experiment results.
</tableCaption>
<bodyText confidence="0.77640775">
(*indicates a system built using only develop-
ment data, i.e. excluding external resources.)
The first line, labeled phrag, gives the per-
formance of the baseline system using standard
</bodyText>
<footnote confidence="0.991492333333333">
4LDC catalog numbers LDC2000T51, LDC95T9, and
LDC99T41.
5http://www.planet.n1/.
</footnote>
<bodyText confidence="0.998056714285714">
maximum likelihood training.
In subsequent lines, we see how several types
of additional data and adaptation techniques
improve system performance. The alterations
from the base system are not cumulative, un-
less where obvious or indicated. Choosing the
best combination of experimental systems is left
as a mechanical exercise.
rand corresponds to adding words drawn
from a baseline-tagged randomly-selected 1-
million word subset of the large corpus to the
adaptation word lists. While this is a negative
result, performing one iteration of Viterbi train-
ing on that randomly drawn set improved over
the baseline for the two development sets (as
shown in the line labeled rviterbi).
qvdlist corresponds to incorporating the full
set of word lists described in Table 1, and the ex-
periment labeled adapt corresponds to produc-
ing word lists (smoothing classes as described in
section 2.2) from phrases in the test data that
were recognized by the baseline system.
In bridge, smoothing word lists were created
from both the training and baseline-tagged test
data. In the pair bridge system, pbrid,ge, prior-
ity was given to lists of words that were agreed
upon by both the bigram spelling model and the
baseline system. The type pair bridge system,
tpbridge, was the same as the pair bridge, except
the spelling model was built on the word types,
disregarding the distribution of the words in the
data.
Table 3 gives a breakdown of best system per-
formance (those entries bolded in Table 2) by
named-entity type.
</bodyText>
<figure confidence="0.8730098">
Spanish
Fdev Ftest
Dutch
Fdev Ftest
4 Concluding Remarks
</figure>
<bodyText confidence="0.98627625">
These results show that the simple EllVIM adap-
tation technique bridging can give more gain
than incorporating found word lists or perform-
ing Viterbi training on the test set.
</bodyText>
<sectionHeader confidence="0.925687" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.956214326530612">
M. Banko and E. Brill. 2001. Scaling to very very
large corpora for natural language disambigua-
tion. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics, pages
26-33. Toulouse. France.
T. C. Bell. J. G. Cleary. and I. H. Witten. 1990.
Text Compression. Prentice Bally.
A. Borthwick. 1999. A Maximum Entropy Approach
to Named Entity Recognition. Ph.D. thesis. New
York University.
E. Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational
Linguistics, 21(4):543-565.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proceed-
ings of the 1999 Joint SIGDAT Conference on
EMNLP and VLC. University of Maryland. MD.
S. Cucerzan and D. Yarowsky. 1999. Language in-
dependent named entity recognition combining
morphological and contextual evidence. In Pro-
ceedings of 1999 Joint SIGDAT Conference on
EMNLP and VLC
J. Dupon. 2000. Dutch genealogy. http: //www.
geocities . com/jwdupon/Dutch.html.
J. Kuyper. 1865. Gemeente Atlas van Neder-
land. Hugo Suringar Leeuwarden. http : //
dutchgenealogy. . com/Dutch_Maps/index. html.
M. Meteer. R. Schwartz. and R. Weischedel. 1991.
Empirical studies in part of speech labelling. In
Proceedings of the DARPA Speech and Natural
Language Workshop. Morgan Kaufmann.
D. Miller. S. Boisen. R. Schwartz. R. Stone. and
R. Weischedel. 2000. Named entity extraction
from noisy input: speech and OCR. In Proceed-
ings of Conference for Applied Natural Language
Processing, pages 316-324. Seattle. WA.
D. D. Palmer and D. S. Day. 1997. A statistical pro-
file of the named entity task. In Proceedings of
Fifth ACL Conference for Applied Natural Lan-
guage Processing, Washington D.C.
D. D. Palmer. J. D. Burger. and M. Ostendorf.
1999. Information extraction from broadcast
news speech data. In Proceedings of the DARPA
Broadcast News Workshop, pages 41-46.
L. Ramshaw and M. Marcus. 1999. Natural Lan-
guage Processing Using Very Large Corpora,
chapter Text Chunking Using Transformation-
based Learning. Kluwer.
</reference>
<table confidence="0.999653708333333">
Spanish dev. precision recall Fo=1
LOC 62.46% 81.42% 70.69
MISC 48.07% 47.64% 47.86
ORG 75.88% 72.53% 74.17
PER 86.75% 75.04% 80.47
overall 71.79% 72.70% 72.25
Spanish test precision recall Fo=1
LOC 75.52% 77.40% 76.45
MISC 50.74% 50.29% 50.52
ORG 74.63% 79.43% 76.96
PER 81.60% 86.26% 83.86
overall 74.19% 77.44% 75.78
Dutch dev. precision recall F0=1
LOC 77.73% 77.73% 77.73
MISC 68.30% 63.54% 65.83
ORG 78.29% 62.15% 69.29
PER 66.83% 77.40% 71.73
overall 71.73% 69.53% 70.61
Dutch test precision recall F0=1
LOC 81.08% 76.13% 78.53
MISC 67.87% 63.35% 65.53
ORG 73.27% 66.25% 69.58
PER 71.79% 84.60% 77.67
overall 72.69% 72.45% 72.57
</table>
<tableCaption confidence="0.899232333333333">
Table 3: Best results obtained for the develop-
ment and the test data sets for the two lan-
guages used in this shared task.
</tableCaption>
<reference confidence="0.976252526315789">
B. Sundheim. 1995. Overview of results of the
MUC-6 evaluation. In Proceedings of the Sixth
Message Understand Conference, November.
E. F. Tjong Kim Sang. 2002. Memory-based shallow
parsing. Journal of Machine Learning Research,
2(559-594). March.
U.S. Census Bureau. 1995. Frequently occurring
first names and surnames from the 1990 cen-
sus. http : //www. census . gov/genealogy/www/
freqnames . html.
A. Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimal decoding
algorithm IEEE Transactions on Information
Theory, IT-63.
D. L. Word and R. C. Perkins. Jr. 1996. Build-
ing a spanish surname list for the 1990&apos;s a new
approach to an old problem. Technical Work-
ing Paper 13, U.S. Census Bureau. http: //www .
census . gov/genealogy/www/spanname . html.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.286022">
<title confidence="0.999607">Statistical Named Entity Recognizer Adaptation</title>
<author confidence="0.762199">John D Burger</author>
<author confidence="0.762199">John C Henderson</author>
<author confidence="0.762199">William T The MITRE</author>
<affiliation confidence="0.84939">MS</affiliation>
<address confidence="0.8173785">202 Burlington MA</address>
<email confidence="0.820687">fjohn,jhndrsn,wmorganl@mitre.org</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<location>Toulouse. France.</location>
<contexts>
<context position="2010" citStr="Banko and Brill, 2001" startWordPosition="303" endWordPosition="306">tensive bootstrapping from small amounts of supervised data with an EMstyle algorithm. Miller et al. (2000) produce a statistical Hidden Markov Model (1-1MM) for NER which is similar to the one used by Palmer et al. (1999); the latter system, named phrag, is the NER engine utilized in the work described in this paper. The experiments described herein explore unsupervised approaches to NER, with an eye toward using unannotated corpora consisting of a few hundred million words. Recent word sense disambiguation results suggest that some simple techniques can scale well with increased data sizes (Banko and Brill, 2001). This paper presents several experiments in adapting a HMM-based named entity recognizer to a target data set. Our core learning engine is a wordbased HMM, and we show two techniques, informed smoothing and iterative adaptation, for incorporating unsupervised data into the model, which provide overall gains in performance. 1.1 phrag phrag&apos; is a trainable phrase tagger based on HMMs. phrag uses bigram language models, i.e., state emissions are conditioned on the previous word only. State transitions are similarly conditioned, which allows the model to capture context words, such as &amp;quot;Mr&amp;quot;. All m</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>M. Banko and E. Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 26-33. Toulouse. France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Cleary</author>
<author>I H Witten</author>
</authors>
<title>Text Compression.</title>
<date>1990</date>
<publisher>Prentice</publisher>
<location>Bally.</location>
<marker>Cleary, Witten, 1990</marker>
<rawString>T. C. Bell. J. G. Cleary. and I. H. Witten. 1990. Text Compression. Prentice Bally.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis.</tech>
<institution>New York University.</institution>
<marker>Borthwick, 1999</marker>
<rawString>A. Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. thesis. New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="1095" citStr="Brill, 1995" startWordPosition="165" endWordPosition="166">entification of spans of text referring to named entities, and categorization of these entities into classes based on the role they fill in context. The sentence &amp;quot;Washington announced that Washington ate seven hotdogs in Washington&amp;quot; provides an example in which a single name can arguably refer to three different entities: an organization, a person, and a location. Following the paradigm introduced by Ramshaw and Marcus (1999), many researchers reduce the NER problem to a word-tagging problem, and address it with techniques similar to those used for part of speech tagging (Meteer et al., 1991; Brill, 1995). Borthwick explores the maximum entropy approach in his dissertation (1999). Collins and Singer (1999) investigate semi-unsupervised methods for named entity categorization. Cucerzan and Yarowsky (1999) produce a unified technique for producing NER systems for several languages, utilizing extensive bootstrapping from small amounts of supervised data with an EMstyle algorithm. Miller et al. (2000) produce a statistical Hidden Markov Model (1-1MM) for NER which is similar to the one used by Palmer et al. (1999); the latter system, named phrag, is the NER engine utilized in the work described in</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC. University of Maryland. MD.</booktitle>
<contexts>
<context position="1198" citStr="Collins and Singer (1999)" startWordPosition="178" endWordPosition="181">ities into classes based on the role they fill in context. The sentence &amp;quot;Washington announced that Washington ate seven hotdogs in Washington&amp;quot; provides an example in which a single name can arguably refer to three different entities: an organization, a person, and a location. Following the paradigm introduced by Ramshaw and Marcus (1999), many researchers reduce the NER problem to a word-tagging problem, and address it with techniques similar to those used for part of speech tagging (Meteer et al., 1991; Brill, 1995). Borthwick explores the maximum entropy approach in his dissertation (1999). Collins and Singer (1999) investigate semi-unsupervised methods for named entity categorization. Cucerzan and Yarowsky (1999) produce a unified technique for producing NER systems for several languages, utilizing extensive bootstrapping from small amounts of supervised data with an EMstyle algorithm. Miller et al. (2000) produce a statistical Hidden Markov Model (1-1MM) for NER which is similar to the one used by Palmer et al. (1999); the latter system, named phrag, is the NER engine utilized in the work described in this paper. The experiments described herein explore unsupervised approaches to NER, with an eye towar</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC. University of Maryland. MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Language independent named entity recognition combining morphological and contextual evidence.</title>
<date>1999</date>
<booktitle>In Proceedings of 1999 Joint SIGDAT Conference on EMNLP and VLC</booktitle>
<contexts>
<context position="1298" citStr="Cucerzan and Yarowsky (1999)" startWordPosition="189" endWordPosition="192">t Washington ate seven hotdogs in Washington&amp;quot; provides an example in which a single name can arguably refer to three different entities: an organization, a person, and a location. Following the paradigm introduced by Ramshaw and Marcus (1999), many researchers reduce the NER problem to a word-tagging problem, and address it with techniques similar to those used for part of speech tagging (Meteer et al., 1991; Brill, 1995). Borthwick explores the maximum entropy approach in his dissertation (1999). Collins and Singer (1999) investigate semi-unsupervised methods for named entity categorization. Cucerzan and Yarowsky (1999) produce a unified technique for producing NER systems for several languages, utilizing extensive bootstrapping from small amounts of supervised data with an EMstyle algorithm. Miller et al. (2000) produce a statistical Hidden Markov Model (1-1MM) for NER which is similar to the one used by Palmer et al. (1999); the latter system, named phrag, is the NER engine utilized in the work described in this paper. The experiments described herein explore unsupervised approaches to NER, with an eye toward using unannotated corpora consisting of a few hundred million words. Recent word sense disambiguat</context>
</contexts>
<marker>Cucerzan, Yarowsky, 1999</marker>
<rawString>S. Cucerzan and D. Yarowsky. 1999. Language independent named entity recognition combining morphological and contextual evidence. In Proceedings of 1999 Joint SIGDAT Conference on EMNLP and VLC</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dupon</author>
</authors>
<date>2000</date>
<note>Dutch genealogy. http: //www. geocities . com/jwdupon/Dutch.html.</note>
<contexts>
<context position="3940" citStr="Dupon, 2000" startWordPosition="611" endWordPosition="612">sses. These are consulted, in order of preference, when a word is not in the main lexicon. If the word is not in any word list, it is relegated to the appropriate &amp;quot;truly unknown&amp;quot; class described above. Language model statistics for lhttp://www.openchannelsoftware.org/projects/ Qanda/. Location1 Location2 Start End Other1 Other2 S D Size Description ,N/ ,N/ 120146 English location words from the TIPSTER gazetteer 88799 English surnames (U.S. Census Bureau, 1995) ,N/ 17576 All three-letter acronyms, AAA through ZZZ &apos;N/ 5163 English given names (U.S. Census Bureau, 1995) &apos;N/ 1410 Dutch surnames (Dupon, 2000) &apos;N/ 1162 Dutch given names (Dupon, 2000) &apos;N/ 1410 Dutch province and city names (Kuyper, 1865) 639 Spanish surnames (Word and Perkins, 1996) 362 Spanish names of capitals (prominent global cities) 203 Spanish geographic adjectives, e.g. &amp;quot;norteamericano&amp;quot; 138 Spanish country words e.g. &amp;quot;Estatos&amp;quot; and &amp;quot;Unidos&amp;quot; Table 1: External word lists introduced to Spanish and Dutch wdlist systems. words of Spanish Newswire, both distributed by the LDC,4 and 2.3 million words of Dutch text, harvested from the Dutch news site Planet Internet.5 For the experiments below, a randomly drawn subsa,mple of 200, 000 </context>
</contexts>
<marker>Dupon, 2000</marker>
<rawString>J. Dupon. 2000. Dutch genealogy. http: //www. geocities . com/jwdupon/Dutch.html.</rawString>
</citation>
<citation valid="false">
<title>Gemeente Atlas van Nederland. Hugo Suringar Leeuwarden.</title>
<note>http : // dutchgenealogy. . com/Dutch_Maps/index. html.</note>
<marker></marker>
<rawString>J. Kuyper. 1865. Gemeente Atlas van Nederland. Hugo Suringar Leeuwarden. http : // dutchgenealogy. . com/Dutch_Maps/index. html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Empirical studies in part of speech labelling.</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<marker>Schwartz, Weischedel, 1991</marker>
<rawString>M. Meteer. R. Schwartz. and R. Weischedel. 1991. Empirical studies in part of speech labelling. In Proceedings of the DARPA Speech and Natural Language Workshop. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Boisen R Schwartz R Stone</author>
<author>R Weischedel</author>
</authors>
<title>Named entity extraction from noisy input: speech and OCR.</title>
<date>2000</date>
<booktitle>In Proceedings of Conference for Applied Natural Language Processing,</booktitle>
<pages>316--324</pages>
<location>Seattle. WA.</location>
<marker>Stone, Weischedel, 2000</marker>
<rawString>D. Miller. S. Boisen. R. Schwartz. R. Stone. and R. Weischedel. 2000. Named entity extraction from noisy input: speech and OCR. In Proceedings of Conference for Applied Natural Language Processing, pages 316-324. Seattle. WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Palmer</author>
<author>D S Day</author>
</authors>
<title>A statistical profile of the named entity task.</title>
<date>1997</date>
<booktitle>In Proceedings of Fifth ACL Conference for Applied Natural Language Processing,</booktitle>
<location>Washington D.C.</location>
<marker>Palmer, Day, 1997</marker>
<rawString>D. D. Palmer and D. S. Day. 1997. A statistical profile of the named entity task. In Proceedings of Fifth ACL Conference for Applied Natural Language Processing, Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Burger</author>
<author>M Ostendorf</author>
</authors>
<title>Information extraction from broadcast news speech data.</title>
<date>1999</date>
<booktitle>In Proceedings of the DARPA Broadcast News Workshop,</booktitle>
<pages>41--46</pages>
<marker>Burger, Ostendorf, 1999</marker>
<rawString>D. D. Palmer. J. D. Burger. and M. Ostendorf. 1999. Information extraction from broadcast news speech data. In Proceedings of the DARPA Broadcast News Workshop, pages 41-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Natural Language Processing Using Very Large Corpora, chapter Text Chunking Using Transformationbased Learning.</title>
<date>1999</date>
<publisher>Kluwer.</publisher>
<contexts>
<context position="912" citStr="Ramshaw and Marcus (1999)" startWordPosition="133" endWordPosition="136">ized utility of information extraction (IE). NER has been explored in depth to provide rapid characterization of newswire data (Sundheim, 1995; Palmer and Day, 1997). The NER task involves both identification of spans of text referring to named entities, and categorization of these entities into classes based on the role they fill in context. The sentence &amp;quot;Washington announced that Washington ate seven hotdogs in Washington&amp;quot; provides an example in which a single name can arguably refer to three different entities: an organization, a person, and a location. Following the paradigm introduced by Ramshaw and Marcus (1999), many researchers reduce the NER problem to a word-tagging problem, and address it with techniques similar to those used for part of speech tagging (Meteer et al., 1991; Brill, 1995). Borthwick explores the maximum entropy approach in his dissertation (1999). Collins and Singer (1999) investigate semi-unsupervised methods for named entity categorization. Cucerzan and Yarowsky (1999) produce a unified technique for producing NER systems for several languages, utilizing extensive bootstrapping from small amounts of supervised data with an EMstyle algorithm. Miller et al. (2000) produce a statis</context>
</contexts>
<marker>Ramshaw, Marcus, 1999</marker>
<rawString>L. Ramshaw and M. Marcus. 1999. Natural Language Processing Using Very Large Corpora, chapter Text Chunking Using Transformationbased Learning. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sundheim</author>
</authors>
<title>Overview of results of the MUC-6 evaluation.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understand Conference,</booktitle>
<marker>Sundheim, 1995</marker>
<rawString>B. Sundheim. 1995. Overview of results of the MUC-6 evaluation. In Proceedings of the Sixth Message Understand Conference, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
</authors>
<title>Memory-based shallow parsing.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--559</pages>
<contexts>
<context position="2985" citStr="Sang, 2002" startWordPosition="467" endWordPosition="468"> tagger based on HMMs. phrag uses bigram language models, i.e., state emissions are conditioned on the previous word only. State transitions are similarly conditioned, which allows the model to capture context words, such as &amp;quot;Mr&amp;quot;. All models are smoothed with type c Witten-Bell discounting (Bell et al., 1990). For named entity recognition, the typical HMM topology has two states per phrase type. The first word of each phrase is generated by the first state, and any subsequent words are generated by the second state. This is essentially the BII scheme employed by many chunkers, e.g. (Tjong Kim Sang, 2002). Figure 1 presents a sketch of this topology. The lexicon is constructed from the training corpus, excluding the least frequent words. These words are pooled to form unknown word models, e.g., unknown-number, unknown-punctuation, unknown-alphabetic. phrag can also use auxiliary resources such as word lists to form additional equivalence classes. These are consulted, in order of preference, when a word is not in the main lexicon. If the word is not in any word list, it is relegated to the appropriate &amp;quot;truly unknown&amp;quot; class described above. Language model statistics for lhttp://www.openchannelso</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>E. F. Tjong Kim Sang. 2002. Memory-based shallow parsing. Journal of Machine Learning Research, 2(559-594). March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U S Census Bureau</author>
</authors>
<title>Frequently occurring first names and surnames from the</title>
<date>1995</date>
<note>html.</note>
<contexts>
<context position="3793" citStr="Bureau, 1995" startWordPosition="588" endWordPosition="589">nknown-number, unknown-punctuation, unknown-alphabetic. phrag can also use auxiliary resources such as word lists to form additional equivalence classes. These are consulted, in order of preference, when a word is not in the main lexicon. If the word is not in any word list, it is relegated to the appropriate &amp;quot;truly unknown&amp;quot; class described above. Language model statistics for lhttp://www.openchannelsoftware.org/projects/ Qanda/. Location1 Location2 Start End Other1 Other2 S D Size Description ,N/ ,N/ 120146 English location words from the TIPSTER gazetteer 88799 English surnames (U.S. Census Bureau, 1995) ,N/ 17576 All three-letter acronyms, AAA through ZZZ &apos;N/ 5163 English given names (U.S. Census Bureau, 1995) &apos;N/ 1410 Dutch surnames (Dupon, 2000) &apos;N/ 1162 Dutch given names (Dupon, 2000) &apos;N/ 1410 Dutch province and city names (Kuyper, 1865) 639 Spanish surnames (Word and Perkins, 1996) 362 Spanish names of capitals (prominent global cities) 203 Spanish geographic adjectives, e.g. &amp;quot;norteamericano&amp;quot; 138 Spanish country words e.g. &amp;quot;Estatos&amp;quot; and &amp;quot;Unidos&amp;quot; Table 1: External word lists introduced to Spanish and Dutch wdlist systems. words of Spanish Newswire, both distributed by the LDC,4 and 2.3 mi</context>
</contexts>
<marker>Bureau, 1995</marker>
<rawString>U.S. Census Bureau. 1995. Frequently occurring first names and surnames from the 1990 census. http : //www. census . gov/genealogy/www/ freqnames . html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimal decoding algorithm</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>63</pages>
<marker>Viterbi, 1967</marker>
<rawString>A. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm IEEE Transactions on Information Theory, IT-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jr</author>
</authors>
<title>Building a spanish surname list for the 1990&apos;s a new approach to an old problem. Technical Working Paper 13, U.S. Census Bureau. http: //www . census . gov/genealogy/www/spanname .</title>
<date>1996</date>
<note>html.</note>
<marker>Jr, 1996</marker>
<rawString>D. L. Word and R. C. Perkins. Jr. 1996. Building a spanish surname list for the 1990&apos;s a new approach to an old problem. Technical Working Paper 13, U.S. Census Bureau. http: //www . census . gov/genealogy/www/spanname . html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>