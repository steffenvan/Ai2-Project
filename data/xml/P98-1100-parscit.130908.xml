<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.053673">
<title confidence="0.928225">
Text Segmentation Using Reiteration and Collocation
</title>
<author confidence="0.883697">
Amanda C. Jobbins
</author>
<affiliation confidence="0.8089445">
Department of Computing
Nottingham Trent University
</affiliation>
<address confidence="0.974863">
Nottingham NG1 4BU, UK
</address>
<email confidence="0.970778">
ajobbins @resumix.com
</email>
<author confidence="0.778695">
Lindsay J. Evett
</author>
<affiliation confidence="0.804656">
Department of Computing
Nottingham Trent University
</affiliation>
<address confidence="0.973441">
Nottingham NG1 4BU, UK
</address>
<email confidence="0.99649">
lje@doc.ntu.ac.uk
</email>
<sectionHeader confidence="0.993809" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999486692307692">
A method is presented for segmenting text into
subtopic areas. The proportion of related
pairwise words is calculated between adjacent
windows of text to determine their lexical
similarity. The lexical cohesion relations of
reiteration and collocation are used to identify
related words. These relations are automatically
located using a combination of three linguistic
features: word repetition, collocation and
relation weights. This method is shown to
successfully detect known subject changes in
text and corresponds well to the segmentations
placed by test subjects.
</bodyText>
<sectionHeader confidence="0.961608" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999407137931035">
Many examples of heterogeneous data can be
found in daily life. The Wall Street Journal
archives, for example, consist of a series of articles
about different subject areas. Segmenting such data
into distinct topics is useful for information
retrieval, where only those segments relevant to a
user&apos;s query can be retrieved. Text segmentation
could also be used as a pre-processing step in
automatic summarisation. Each segment could be
summarised individually and then combined to
provide an abstract for a document.
Previous work on text segmentation has used term
matching to identify clusters of related text. Salton
and Buckley (1992) and later, Hearst (1994)
extracted related text portions by matching high
frequency terms. Yaari (1997) segmented text into
a hierarchical structure, identifying sub-segments
of larger segments. Ponte and Croft (1997) used
word co-occurrences to expand the number of
terms for matching. Reynar (1994) compared all
words across a text rather than the more usual
nearest neighbours. A problem with using word
repetition is that inappropriate matches can be
made because of the lack of contextual information
(Salton et al., 1994). Another approach to text
segmentation is the detection of semantically
related words.
Hearst (1993) incorporated semantic information
derived from WordNet but in later work reported
that this information actually degraded word
repetition results (Hearst, 1994). Related words
have been located using spreading activation on a
semantic network (Kozima, 1993), although only
one text was segmented. Another approach
extracted semantic information from Roget&apos;s
Thesaurus (RT). Lexical cohesion relations
(Halliday and Hasan, 1976) between words were
identified in RT and used to construct lexical chains
of related words in five texts (Morris and Hirst,
1991). It was reported that the lexical chains
closely correlated to the intentional structure
(Grosz and Sidner, 1986) of the texts, where the
start and end of chains coincided with the intention
ranges. However, RT does not capture all types of
lexical cohesion relations. In previous work, it was
found that collocation (a lexical cohesion relation)
was under-represented in the thesaurus.
Furthermore, this process was not automated and
relied on subjective decision making.
Following Morris and Hirst&apos;s work, a segmentation
algorithm was developed based on identifying
lexical cohesion relations across a text. The
proposed algorithm is fully automated, and a
quantitative measure of the association between
words is calculated. This algorithm utilises
linguistic features additional to those captured in
the thesaurus to identify the other types of lexical
cohesion relations that can exist in text.
</bodyText>
<page confidence="0.998898">
614
</page>
<sectionHeader confidence="0.903621" genericHeader="method">
1 Background Theory: Lexical Cohesion
</sectionHeader>
<bodyText confidence="0.999884571428571">
Cohesion concerns how words in a text are related.
The major work on cohesion in English was
conducted by Halliday and Hasan (1976). An
instance of cohesion between a pair of elements is
referred to as a tie. Ties can be anaphoric or
cataphoric, and located at both the sentential and
supra-sentential level. Halliday and Hasan
classified cohesion under two types: grammatical
and lexical. Grammatical cohesion is expressed
through the grammatical relations in text such as
ellipsis and conjunction. Lexical cohesion is
expressed through the vocabulary used in text and
the semantic relations between those words.
Identifying semantic relations in a text can be a
useful indicator of its conceptual structure.
Lexical cohesion is divided into three classes:
general noun, reiteration and collocation. General
noun&apos;s cohesive function is both grammatical and
lexical, although Halliday and Hasan&apos;s analysis
showed that this class plays a minor cohesive role.
Consequently, it was not further considered.
Reiteration is subdivided into four cohesive
effects: word repetition (e.g. ascent and ascent),
synonym (e.g. ascent and climb) which includes
near-synonym and hyponym, superordinate (e.g.
ascent and task) and general word (e.g. ascent and
thing). The effect of general word is difficult to
automatically identify because no common
referent exists between the general word and the
word to which it refers. A collocation is a
predisposed combination of words, typically
pairwise words, that tend to regularly co-occur
(e.g. orange and peel). All semantic relations not
classified under the class of reiteration are
attributed to the class of collocation.
</bodyText>
<sectionHeader confidence="0.931485" genericHeader="method">
2 Identifying Lexical Cohesion
</sectionHeader>
<bodyText confidence="0.982028366666667">
To automatically detect lexical cohesion ties
between pairwise words, three linguistic features
were considered: word repetition, collocation and
relation weights. The first two methods represent
lexical cohesion relations. Word repetition is a
component of the lexical cohesion class of
reiteration, and collocation is a lexical cohesion
class in its entirety. The remaining types of lexical
cohesion considered, include synonym and
superordinate (the cohesive effect of general word
was not included). These types can be identified
using relation weights (Jobbins and Evett, 1998).
Word repetition: Word repetition ties in lexical
cohesion are identified by same word matches and
matches on inflections derived from the same stem.
An inflected word was reduced to its stem by look-
up in a lexicon (Keenan and Evett, 1989)
comprising inflection and stem word pair records
(e.g. &amp;quot;orange oranges&amp;quot;).
Collocation: Collocations were extracted from a
seven million word sample of the Longman
English Language Corpus using the association
ratio (Church and Hanks, 1990) and outputted to a
lexicon. Collocations were automatically located in
a text by looking up pairwise words in this lexicon.
Figure 1 shows the record for the headword orange
followed by its collocates. For example, the
pairwise words orange and peel form a collocation.
orange free green lemon peel red
state yellow
</bodyText>
<figureCaption confidence="0.990823">
Figure 1. Excerpt from the collocation lexicon.
</figureCaption>
<bodyText confidence="0.997770583333333">
Relation Weights: Relation weights quantify the
amount of semantic relation between words based
on the lexical organisation of RT (Jobbins and
Evett, 1995). A thesaurus is a collection of
synonym groups, indicating that synonym relations
are captured, and the hierarchical structure of RT
implies that superordinate relations are also
captured. An alphabetically-ordered index of RT
was generated, referred to as the Thesaurus
Lexicon (TLex). Relation weights for pairwise
words are calculated based on the satisfaction of
one or more of four possible connections in TLex.
</bodyText>
<sectionHeader confidence="0.991838" genericHeader="method">
3 Proposed Segmentation Algorithm
</sectionHeader>
<bodyText confidence="0.999863">
The proposed segmentation algorithm compares
adjacent windows of sentences and determines
their lexical similarity. A window size of three
sentences was found to produce the best results.
Multiple sentences were compared because
</bodyText>
<page confidence="0.995867">
615
</page>
<bodyText confidence="0.999639736842105">
calculating lexical similarity between words is too
fine (Rotondo, 1984) and between individual
sentences is unreliable (Salton and Buckley, 1991).
Lexical similarity is calculated for each window
comparison based on the proportion of related
words, and is given as a normalised score. Word
repetitions are identified between identical words
and words derived from the same stem.
Collocations are located by looking up word pairs
in the collocation lexicon. Relation weights are
calculated between pairwise words according to
their location in RT. The lexical similarity score
indicates the amount of lexical cohesion
demonstrated by two windows. Scores plotted on a
graph show a series of peaks (high scores) and
troughs (low scores). Low scores indicate a weak
level of cohesion. Hence, a trough signals a
potential subject change and texts can be
segmented at these points.
</bodyText>
<sectionHeader confidence="0.991542" genericHeader="method">
4 Experiment 1: Locating Subject Change
</sectionHeader>
<bodyText confidence="0.999409608695652">
An investigation was conducted to determine
whether the segmentation algorithm could reliably
locate subject change in text.
Method: Seven topical articles of between 250 to
450 words in length were extracted from the World
Wide Web. A total of 42 texts for test data were
generated by concatenating pairs of these articles.
Hence, each generated text consisted of two
articles. The transition from the first article to the
second represented a known subject change point.
Previous work has identified the breaks between
concatenated texts to evaluate the performance of
text segmentation algorithms (Reynar, 1994;
Stairmand, 1997). For each text, the troughs placed
by the segmentation algorithm were compared to
the location of the known subject change point in
that text. An error margin of one sentence either
side of this point, determined by empirical
analysis, was allowed.
Results: Table 1 gives the results for the
comparison of the troughs placed by the
segmentation algorithm to the known subject
change points.
</bodyText>
<table confidence="0.999325052631579">
linguistic feature troughs placed subject change
points located
(out of 42 poss.)
average std. dev.
word repetition 7.1 3.16 41
collocation (97.6%)
word repetition 7.3 5.22 41
relation weights (97.6%)
word repetition 8.5 3.62 41
(97.6%)
collocation 5.8 3.70 (95.2%)
relation weights 40
word repetition 6.4 4.72 40
collocation (95.2%)
relation weights
relation weights 7 4.23 39
(92.9%)
collocation 6.3 3.83 35
(83.3%)
</table>
<tableCaption confidence="0.975746">
Table 1. Comparison of segmentation algorithm
using different linguistic features.
</tableCaption>
<bodyText confidence="0.99945705882353">
Discussion: The segmentation algorithm using the
linguistic features word repetition and collocation
in combination achieved the best result. A total of
41 out of a possible 42 known subject change
points were identified from the least number of
troughs placed per text (7.1). For the text where the
known subject change point went undetected, a
total of three troughs were placed at sentences 6, 11
and 18. The subject change point occurred at
sentence 13, just two sentences after a predicted
subject change at sentence 11.
In this investigation, word repetition alone
achieved better results than using either collocation
or relation weights individually. The combination
of word repetition with another linguistic feature
improved on its individual result, where less
troughs were placed per text.
</bodyText>
<sectionHeader confidence="0.990402" genericHeader="method">
5 Experiment 2: Test Subject Evaluation
</sectionHeader>
<bodyText confidence="0.982553">
The objective of the current investigation was to
determine whether all troughs coincide with a
subject change. The troughs placed by the
</bodyText>
<page confidence="0.998003">
616
</page>
<bodyText confidence="0.997822">
algorithm were compared to the segmentations
identified by test subjects for the same texts.
Method: Twenty texts were randomly selected for
test data each consisting of approximately 500
words. These texts were presented to seven test
subjects who were instructed to identify the
sentences at which a new subject area commenced.
No restriction was placed on the number of subject
changes that could be identified. Segmentation
points, indicating a change of subject, were
determined by the agreement of three or more test
subjects (Litman and Passonneau, 1996). Adjacent
segmentation points were treated as one point
because it is likely that they refer to the same
subject change.
The troughs placed by the segmentation algorithm
were compared to the segmentation points
identified by the test subjects. In Experiment 1, the
top five approaches investigated identified at least
40 out of 42 known subject change points. Due to
that success, these five approaches were applied in
this experiment. To evaluate the results, the
information retrieval metrics precision and recall
were used. These metrics have tended to be
adopted for the assessment of text segmentation
algorithms, but they do not provide a scale of
correctness (Beeferman et al., 1997). The degree to
which a segmentation point was &apos;missed&apos; by a
trough, for instance, is not considered. Allowing an
error margin provides some degree of flexibility.
An error margin of two sentences either side of a
segmentation point was used by Hearst (1993) and
Reynar (1994) allowed three sentences. In this
investigation, an error margin of two sentences was
considered.
Results: Table 2 gives the mean values for the
comparison of troughs placed by the segmentation
algorithm to the segmentation points identified by
the test subjects for all the texts.
Discussion: The segmentation algorithm using
word repetition and relation weights in
combination achieved mean precision and recall
rates of 0.80 and 0.69, respectively. For 9 out of the
20 texts segmented, all troughs were relevant.
Therefore, many of the troughs placed by the
segmentation algorithm represented valid subject
</bodyText>
<table confidence="0.997713285714286">
linguistic mean values for all texts
feature
relevant relevant nonrel. prec. rec.
found found
word repetition 4.50 3.10 1.00 0.80 0.69
relation weights
word repetition 4.50 2.80 0.85 0.80 0.62
collocation
word repetition 4.50 2.80 0.85 0.80 0.62
collocation
relation weights
collocation 4.50 2.75 0.90 0.80 0.60
relation weights
word repetition 4.50 2.50 0.95 0.78 0.56
</table>
<tableCaption confidence="0.998772">
Table 2. Comparison of troughs to segmentation
</tableCaption>
<bodyText confidence="0.9753445">
points placed by the test subjects.
changes. Both word repetition in combination with
collocation and all three features in combination
also achieved a precision rate of 0.80 but attained a
lower recall rate of 0.62. These results demonstrate
that supplementing word repetition with other
linguistic features can improve text segmentation.
As an example, a text segmentation algorithm
developed by Hearst (1994) based on word
repetition alone attained inferior precision and
recall rates of 0.66 and 0.61.
In this investigation, recall rates tended to be lower
than precision rates because the algorithm
identified fewer segments (4.1 per text) than the
test subjects (4.5). Each text was only 500 words in
length and was related to a specific subject area.
These factors limited the degree of subject change
that occurred. Consequently, the test subjects
tended to identify subject changes that were more
subtle than the algorithm could detect.
</bodyText>
<sectionHeader confidence="0.964476" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999340166666667">
The text segmentation algorithm developed used
three linguistic features to automatically detect
lexical cohesion relations across windows. The
combination of features word repetition and
relation weights produced the best precision and
recall rates of 0.80 and 0.69. When used in
</bodyText>
<page confidence="0.993185">
617
</page>
<bodyText confidence="0.999940875">
isolation, the performance of each feature was
inferior to a combined approach. This fact provides
evidence that different lexical relations are
detected by each linguistic feature considered.
Areas for improving the segmentation algorithm
include incorporation of a threshold for troughs.
Currently, all troughs indicate a subject change,
however, minor fluctuations in scores may be
discounted. Future work with this algorithm should
include application to longer documents. With
trough thresholding the segments identified in
longer documents could detect significant subject
changes. Having located the related segments in
text, a method of determining the subject of each
segment could be developed, for example, for
information retrieval purposes.
</bodyText>
<sectionHeader confidence="0.998912" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998959273972603">
Beeferman D., Berger A. and Lafferty J. (1997) Text
segmentation using exponential models, Proceedings
of the 2nd Conference on Empirical Methods in
Natural Language Processing
Church K. W. and Hanks P. (1990) Word association
norms, mutual information and lexicography&apos;,
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics, pp. 76-83
Grosz, B. J. and Sidner, C. L. (1986) Attention,
intentions and the structure of discourse,
Computational Linguistics, 12(3), pp. 175-204
Halliday M. A. K. and Hasan R. (1976) Cohesion in
English, Longman Group
Hearst M. A. (1993) TextTiling: A quantitative approach
to discourse segmentation, Technical Report 93/24,
Sequoia 2000, University of California, Berkeley
Hearst M. A. (1994) Multi-paragraph segmentation of
expository texts, Report No. UCB/CSD 94/790,
University of California, Berkeley
Jobbins A. C and Evett L. J. (1995) Automatic
identification of cohesion in texts: Exploiting the
lexical organisation of Roget&apos;s Thesaurus,
Proceedings of ROCLING VIII, Taipei, Taiwan
Jobbins A. C. and Evett L. J. (1998) Semantic
Information from Roget&apos;s Thesaurus: Applied to the
Correction of Cursive Script Recognition Output,
Proceedings of the International Conference on
Computational Linguistics, Speech and Document
Processing, India, pp. 65-70
Keenan F. G and Evett L. J. (1989) Lexical structure for
natural language processing, Proceedings of the 1st
International Lexical Acquisition Workshop at IJCAI
Kozima H. (1993) Text segmentation based on similarity
between words, Proceedings of the 31st Annual
Meeting on the Association for Computational
Linguistics, pp. 286-288
Litman D. J. and Passonneau R. J. (1996) Combining
knowledge sources for discourse segmentation,
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics
Morris J. and Hirst G. (1991) Lexical cohesion
computed by thesaural relations as an indicator of the
structure of text, Computational Linguistics, 17(1),
pp. 21-48
Ponte J. M. and Croft W. B. (1997) Text Segmentation by
Topic, 1st European Conference on Research and
Advanced Technology for Digital Libraries
(ECDL&apos;97), pp. 113-125
Reynar J. C. (1994) An automatic method of finding
topic boundaries, Proceedings of the 32nd Annual
Meeting of the Association for Computational
Linguistics (Student Session), pp. 331-333
Rotondo J. A. (1984) Clustering analysis of subjective
partitions of text, Discourse Processes, 7, pp. 69-88
Salton G. and Buckley C. (1991) Global text matching
for information retrieval, Science, 253, pp. 1012-1015
Salton G. and Buckley C. (1992) Automatic text
structuring experiments in &amp;quot;Text-Based Intelligent
Systems: Current Research and Practice in
Information Extraction and Retrieval,&amp;quot; P. S. Jacobs,
ed, Lawrence Earlbaum Associates, New Jersey, pp.
199-210
Salton G., Allen J. and Buckley C. (1994) Automatic
structuring and retrieval of large text files,
Communications of the Association for Computing
Machinery, 37(2), pp. 97-108
Stairmand M. A. (1997) Textual context analysis for
information retrieval, Proceedings of the ACM SIGIR
Conference on Research and Development in
Information Retrieval, Philadelphia, pp. 140-147
Yaari Y. (1997) Segmentation of expository texts by
hierarchical agglomerative clustering, RANLP&apos;97,
Bulgaria
</reference>
<page confidence="0.993946">
618
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.856002">
<title confidence="0.999809">Text Segmentation Using Reiteration and Collocation</title>
<author confidence="0.99989">Amanda C Jobbins</author>
<affiliation confidence="0.9812385">Department of Computing Nottingham Trent University</affiliation>
<address confidence="0.999682">Nottingham NG1 4BU, UK</address>
<email confidence="0.943955">ajobbins@resumix.com</email>
<author confidence="0.999998">Lindsay J Evett</author>
<affiliation confidence="0.9812625">Department of Computing Nottingham Trent University</affiliation>
<address confidence="0.999765">Nottingham NG1 4BU, UK</address>
<email confidence="0.998472">lje@doc.ntu.ac.uk</email>
<abstract confidence="0.998603357142857">A method is presented for segmenting text into subtopic areas. The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity. The lexical cohesion relations of reiteration and collocation are used to identify related words. These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights. This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Text segmentation using exponential models,</title>
<date>1997</date>
<booktitle>Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing</booktitle>
<contexts>
<context position="12270" citStr="Beeferman et al., 1997" startWordPosition="1842" endWordPosition="1845">is likely that they refer to the same subject change. The troughs placed by the segmentation algorithm were compared to the segmentation points identified by the test subjects. In Experiment 1, the top five approaches investigated identified at least 40 out of 42 known subject change points. Due to that success, these five approaches were applied in this experiment. To evaluate the results, the information retrieval metrics precision and recall were used. These metrics have tended to be adopted for the assessment of text segmentation algorithms, but they do not provide a scale of correctness (Beeferman et al., 1997). The degree to which a segmentation point was &apos;missed&apos; by a trough, for instance, is not considered. Allowing an error margin provides some degree of flexibility. An error margin of two sentences either side of a segmentation point was used by Hearst (1993) and Reynar (1994) allowed three sentences. In this investigation, an error margin of two sentences was considered. Results: Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts. Discussion: The segmentation algorithm usi</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1997</marker>
<rawString>Beeferman D., Berger A. and Lafferty J. (1997) Text segmentation using exponential models, Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography&apos;,</title>
<date>1990</date>
<booktitle>Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="6388" citStr="Church and Hanks, 1990" startWordPosition="938" endWordPosition="941">ordinate (the cohesive effect of general word was not included). These types can be identified using relation weights (Jobbins and Evett, 1998). Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem. An inflected word was reduced to its stem by lookup in a lexicon (Keenan and Evett, 1989) comprising inflection and stem word pair records (e.g. &amp;quot;orange oranges&amp;quot;). Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon. Collocations were automatically located in a text by looking up pairwise words in this lexicon. Figure 1 shows the record for the headword orange followed by its collocates. For example, the pairwise words orange and peel form a collocation. orange free green lemon peel red state yellow Figure 1. Excerpt from the collocation lexicon. Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995). A thesaurus is a collection of synonym groups, indicating that synonym relatio</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Church K. W. and Hanks P. (1990) Word association norms, mutual information and lexicography&apos;, Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pp. 76-83</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Attention, intentions and the structure of discourse,</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<pages>175--204</pages>
<contexts>
<context position="2809" citStr="Grosz and Sidner, 1986" startWordPosition="404" endWordPosition="407">WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994). Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. Another approach extracted semantic information from Roget&apos;s Thesaurus (RT). Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991). It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges. However, RT does not capture all types of lexical cohesion relations. In previous work, it was found that collocation (a lexical cohesion relation) was under-represented in the thesaurus. Furthermore, this process was not automated and relied on subjective decision making. Following Morris and Hirst&apos;s work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text. The proposed algorithm is fully automated, and a quantitative measure of the association between words </context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, B. J. and Sidner, C. L. (1986) Attention, intentions and the structure of discourse, Computational Linguistics, 12(3), pp. 175-204</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>R Hasan</author>
</authors>
<title>Cohesion in English,</title>
<date>1976</date>
<publisher>Longman Group</publisher>
<contexts>
<context position="2566" citStr="Halliday and Hasan, 1976" startWordPosition="365" endWordPosition="368">ropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994). Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. Another approach extracted semantic information from Roget&apos;s Thesaurus (RT). Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991). It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges. However, RT does not capture all types of lexical cohesion relations. In previous work, it was found that collocation (a lexical cohesion relation) was under-represented in the thesaurus. Furthermore, this process was not automated and relied on subjective decision makin</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday M. A. K. and Hasan R. (1976) Cohesion in English, Longman Group</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>TextTiling: A quantitative approach to discourse segmentation,</title>
<date>1993</date>
<tech>Technical Report 93/24, Sequoia</tech>
<institution>University of California, Berkeley</institution>
<contexts>
<context position="2138" citStr="Hearst (1993)" startWordPosition="310" endWordPosition="311">st (1994) extracted related text portions by matching high frequency terms. Yaari (1997) segmented text into a hierarchical structure, identifying sub-segments of larger segments. Ponte and Croft (1997) used word co-occurrences to expand the number of terms for matching. Reynar (1994) compared all words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994). Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. Another approach extracted semantic information from Roget&apos;s Thesaurus (RT). Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991). It was reported that the lexical chains c</context>
<context position="12528" citStr="Hearst (1993)" startWordPosition="1887" endWordPosition="1888">n subject change points. Due to that success, these five approaches were applied in this experiment. To evaluate the results, the information retrieval metrics precision and recall were used. These metrics have tended to be adopted for the assessment of text segmentation algorithms, but they do not provide a scale of correctness (Beeferman et al., 1997). The degree to which a segmentation point was &apos;missed&apos; by a trough, for instance, is not considered. Allowing an error margin provides some degree of flexibility. An error margin of two sentences either side of a segmentation point was used by Hearst (1993) and Reynar (1994) allowed three sentences. In this investigation, an error margin of two sentences was considered. Results: Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts. Discussion: The segmentation algorithm using word repetition and relation weights in combination achieved mean precision and recall rates of 0.80 and 0.69, respectively. For 9 out of the 20 texts segmented, all troughs were relevant. Therefore, many of the troughs placed by the segmentation algorith</context>
</contexts>
<marker>Hearst, 1993</marker>
<rawString>Hearst M. A. (1993) TextTiling: A quantitative approach to discourse segmentation, Technical Report 93/24, Sequoia 2000, University of California, Berkeley</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository texts,</title>
<date>1994</date>
<tech>Report No. UCB/CSD 94/790,</tech>
<institution>University of California, Berkeley</institution>
<contexts>
<context position="1534" citStr="Hearst (1994)" startWordPosition="220" endWordPosition="221">n daily life. The Wall Street Journal archives, for example, consist of a series of articles about different subject areas. Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user&apos;s query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document. Previous work on text segmentation has used term matching to identify clusters of related text. Salton and Buckley (1992) and later, Hearst (1994) extracted related text portions by matching high frequency terms. Yaari (1997) segmented text into a hierarchical structure, identifying sub-segments of larger segments. Ponte and Croft (1997) used word co-occurrences to expand the number of terms for matching. Reynar (1994) compared all words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1</context>
<context position="13984" citStr="Hearst (1994)" startWordPosition="2106" endWordPosition="2107">ition 4.50 2.80 0.85 0.80 0.62 collocation relation weights collocation 4.50 2.75 0.90 0.80 0.60 relation weights word repetition 4.50 2.50 0.95 0.78 0.56 Table 2. Comparison of troughs to segmentation points placed by the test subjects. changes. Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62. These results demonstrate that supplementing word repetition with other linguistic features can improve text segmentation. As an example, a text segmentation algorithm developed by Hearst (1994) based on word repetition alone attained inferior precision and recall rates of 0.66 and 0.61. In this investigation, recall rates tended to be lower than precision rates because the algorithm identified fewer segments (4.1 per text) than the test subjects (4.5). Each text was only 500 words in length and was related to a specific subject area. These factors limited the degree of subject change that occurred. Consequently, the test subjects tended to identify subject changes that were more subtle than the algorithm could detect. Conclusion The text segmentation algorithm developed used three l</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Hearst M. A. (1994) Multi-paragraph segmentation of expository texts, Report No. UCB/CSD 94/790, University of California, Berkeley</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Jobbins</author>
<author>L J Evett</author>
</authors>
<title>Automatic identification of cohesion in texts: Exploiting the lexical organisation of Roget&apos;s Thesaurus,</title>
<date>1995</date>
<booktitle>Proceedings of ROCLING VIII,</booktitle>
<location>Taipei, Taiwan</location>
<contexts>
<context position="6908" citStr="Jobbins and Evett, 1995" startWordPosition="1020" endWordPosition="1023"> word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon. Collocations were automatically located in a text by looking up pairwise words in this lexicon. Figure 1 shows the record for the headword orange followed by its collocates. For example, the pairwise words orange and peel form a collocation. orange free green lemon peel red state yellow Figure 1. Excerpt from the collocation lexicon. Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995). A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured. An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex). Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex. 3 Proposed Segmentation Algorithm The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity. A window size of three sentences</context>
</contexts>
<marker>Jobbins, Evett, 1995</marker>
<rawString>Jobbins A. C and Evett L. J. (1995) Automatic identification of cohesion in texts: Exploiting the lexical organisation of Roget&apos;s Thesaurus, Proceedings of ROCLING VIII, Taipei, Taiwan</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Jobbins</author>
<author>L J Evett</author>
</authors>
<title>Semantic Information from Roget&apos;s Thesaurus: Applied to the Correction of Cursive Script Recognition Output,</title>
<date>1998</date>
<booktitle>Proceedings of the International Conference on Computational Linguistics, Speech and Document Processing, India,</booktitle>
<pages>65--70</pages>
<contexts>
<context position="5908" citStr="Jobbins and Evett, 1998" startWordPosition="863" endWordPosition="866">llocation. 2 Identifying Lexical Cohesion To automatically detect lexical cohesion ties between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety. The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights (Jobbins and Evett, 1998). Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem. An inflected word was reduced to its stem by lookup in a lexicon (Keenan and Evett, 1989) comprising inflection and stem word pair records (e.g. &amp;quot;orange oranges&amp;quot;). Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon. Collocations were automatically located in a text by looking up pairwise words in this lexi</context>
</contexts>
<marker>Jobbins, Evett, 1998</marker>
<rawString>Jobbins A. C. and Evett L. J. (1998) Semantic Information from Roget&apos;s Thesaurus: Applied to the Correction of Cursive Script Recognition Output, Proceedings of the International Conference on Computational Linguistics, Speech and Document Processing, India, pp. 65-70</rawString>
</citation>
<citation valid="true">
<authors>
<author>F G Keenan</author>
<author>L J Evett</author>
</authors>
<title>Lexical structure for natural language processing,</title>
<date>1989</date>
<booktitle>Proceedings of the 1st International Lexical Acquisition Workshop at IJCAI</booktitle>
<contexts>
<context position="6148" citStr="Keenan and Evett, 1989" startWordPosition="904" endWordPosition="907">ical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety. The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights (Jobbins and Evett, 1998). Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem. An inflected word was reduced to its stem by lookup in a lexicon (Keenan and Evett, 1989) comprising inflection and stem word pair records (e.g. &amp;quot;orange oranges&amp;quot;). Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon. Collocations were automatically located in a text by looking up pairwise words in this lexicon. Figure 1 shows the record for the headword orange followed by its collocates. For example, the pairwise words orange and peel form a collocation. orange free green lemon peel red state yellow Figure 1. Excerpt from the collocation lexi</context>
</contexts>
<marker>Keenan, Evett, 1989</marker>
<rawString>Keenan F. G and Evett L. J. (1989) Lexical structure for natural language processing, Proceedings of the 1st International Lexical Acquisition Workshop at IJCAI</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kozima</author>
</authors>
<title>Text segmentation based on similarity between words,</title>
<date>1993</date>
<booktitle>Proceedings of the 31st Annual Meeting on the Association for Computational Linguistics,</booktitle>
<pages>286--288</pages>
<contexts>
<context position="2396" citStr="Kozima, 1993" startWordPosition="345" endWordPosition="346">for matching. Reynar (1994) compared all words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994). Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. Another approach extracted semantic information from Roget&apos;s Thesaurus (RT). Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991). It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges. However, RT does not capture all types of lexical cohesion relations. In previous work, it was found </context>
</contexts>
<marker>Kozima, 1993</marker>
<rawString>Kozima H. (1993) Text segmentation based on similarity between words, Proceedings of the 31st Annual Meeting on the Association for Computational Linguistics, pp. 286-288</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>R J Passonneau</author>
</authors>
<title>Combining knowledge sources for discourse segmentation,</title>
<date>1996</date>
<booktitle>Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics</booktitle>
<contexts>
<context position="11579" citStr="Litman and Passonneau, 1996" startWordPosition="1733" endWordPosition="1736">ughs coincide with a subject change. The troughs placed by the 616 algorithm were compared to the segmentations identified by test subjects for the same texts. Method: Twenty texts were randomly selected for test data each consisting of approximately 500 words. These texts were presented to seven test subjects who were instructed to identify the sentences at which a new subject area commenced. No restriction was placed on the number of subject changes that could be identified. Segmentation points, indicating a change of subject, were determined by the agreement of three or more test subjects (Litman and Passonneau, 1996). Adjacent segmentation points were treated as one point because it is likely that they refer to the same subject change. The troughs placed by the segmentation algorithm were compared to the segmentation points identified by the test subjects. In Experiment 1, the top five approaches investigated identified at least 40 out of 42 known subject change points. Due to that success, these five approaches were applied in this experiment. To evaluate the results, the information retrieval metrics precision and recall were used. These metrics have tended to be adopted for the assessment of text segme</context>
</contexts>
<marker>Litman, Passonneau, 1996</marker>
<rawString>Litman D. J. and Passonneau R. J. (1996) Combining knowledge sources for discourse segmentation, Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text,</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<pages>21--48</pages>
<contexts>
<context position="2695" citStr="Morris and Hirst, 1991" startWordPosition="387" endWordPosition="390">tion is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994). Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. Another approach extracted semantic information from Roget&apos;s Thesaurus (RT). Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991). It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges. However, RT does not capture all types of lexical cohesion relations. In previous work, it was found that collocation (a lexical cohesion relation) was under-represented in the thesaurus. Furthermore, this process was not automated and relied on subjective decision making. Following Morris and Hirst&apos;s work, a segmentation algorithm was developed based on identifying lexical cohesion relations acro</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris J. and Hirst G. (1991) Lexical cohesion computed by thesaural relations as an indicator of the structure of text, Computational Linguistics, 17(1), pp. 21-48</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
<author>W B Croft</author>
</authors>
<title>Text Segmentation by Topic,</title>
<date>1997</date>
<booktitle>1st European Conference on Research and Advanced Technology for Digital Libraries (ECDL&apos;97),</booktitle>
<pages>113--125</pages>
<contexts>
<context position="1727" citStr="Ponte and Croft (1997)" startWordPosition="244" endWordPosition="247">rmation retrieval, where only those segments relevant to a user&apos;s query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document. Previous work on text segmentation has used term matching to identify clusters of related text. Salton and Buckley (1992) and later, Hearst (1994) extracted related text portions by matching high frequency terms. Yaari (1997) segmented text into a hierarchical structure, identifying sub-segments of larger segments. Ponte and Croft (1997) used word co-occurrences to expand the number of terms for matching. Reynar (1994) compared all words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994). Related words have been lo</context>
</contexts>
<marker>Ponte, Croft, 1997</marker>
<rawString>Ponte J. M. and Croft W. B. (1997) Text Segmentation by Topic, 1st European Conference on Research and Advanced Technology for Digital Libraries (ECDL&apos;97), pp. 113-125</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
</authors>
<title>An automatic method of finding topic boundaries,</title>
<date>1994</date>
<booktitle>Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (Student Session),</booktitle>
<pages>331--333</pages>
<contexts>
<context position="1810" citStr="Reynar (1994)" startWordPosition="259" endWordPosition="260">xt segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document. Previous work on text segmentation has used term matching to identify clusters of related text. Salton and Buckley (1992) and later, Hearst (1994) extracted related text portions by matching high frequency terms. Yaari (1997) segmented text into a hierarchical structure, identifying sub-segments of larger segments. Ponte and Croft (1997) used word co-occurrences to expand the number of terms for matching. Reynar (1994) compared all words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994). Related words have been located using spreading activation on a semantic network (Kozima, 1993), although onl</context>
<context position="9123" citStr="Reynar, 1994" startWordPosition="1356" endWordPosition="1357">n investigation was conducted to determine whether the segmentation algorithm could reliably locate subject change in text. Method: Seven topical articles of between 250 to 450 words in length were extracted from the World Wide Web. A total of 42 texts for test data were generated by concatenating pairs of these articles. Hence, each generated text consisted of two articles. The transition from the first article to the second represented a known subject change point. Previous work has identified the breaks between concatenated texts to evaluate the performance of text segmentation algorithms (Reynar, 1994; Stairmand, 1997). For each text, the troughs placed by the segmentation algorithm were compared to the location of the known subject change point in that text. An error margin of one sentence either side of this point, determined by empirical analysis, was allowed. Results: Table 1 gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points. linguistic feature troughs placed subject change points located (out of 42 poss.) average std. dev. word repetition 7.1 3.16 41 collocation (97.6%) word repetition 7.3 5.22 41 relation weigh</context>
<context position="12546" citStr="Reynar (1994)" startWordPosition="1890" endWordPosition="1891">oints. Due to that success, these five approaches were applied in this experiment. To evaluate the results, the information retrieval metrics precision and recall were used. These metrics have tended to be adopted for the assessment of text segmentation algorithms, but they do not provide a scale of correctness (Beeferman et al., 1997). The degree to which a segmentation point was &apos;missed&apos; by a trough, for instance, is not considered. Allowing an error margin provides some degree of flexibility. An error margin of two sentences either side of a segmentation point was used by Hearst (1993) and Reynar (1994) allowed three sentences. In this investigation, an error margin of two sentences was considered. Results: Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts. Discussion: The segmentation algorithm using word repetition and relation weights in combination achieved mean precision and recall rates of 0.80 and 0.69, respectively. For 9 out of the 20 texts segmented, all troughs were relevant. Therefore, many of the troughs placed by the segmentation algorithm represented vali</context>
</contexts>
<marker>Reynar, 1994</marker>
<rawString>Reynar J. C. (1994) An automatic method of finding topic boundaries, Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (Student Session), pp. 331-333</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Rotondo</author>
</authors>
<title>Clustering analysis of subjective partitions of text,</title>
<date>1984</date>
<booktitle>Discourse Processes,</booktitle>
<volume>7</volume>
<pages>69--88</pages>
<contexts>
<context position="7665" citStr="Rotondo, 1984" startWordPosition="1131" endWordPosition="1132">hat superordinate relations are also captured. An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex). Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex. 3 Proposed Segmentation Algorithm The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity. A window size of three sentences was found to produce the best results. Multiple sentences were compared because 615 calculating lexical similarity between words is too fine (Rotondo, 1984) and between individual sentences is unreliable (Salton and Buckley, 1991). Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score. Word repetitions are identified between identical words and words derived from the same stem. Collocations are located by looking up word pairs in the collocation lexicon. Relation weights are calculated between pairwise words according to their location in RT. The lexical similarity score indicates the amount of lexical cohesion demonstrated by two windows. Scores plotted on a graph</context>
</contexts>
<marker>Rotondo, 1984</marker>
<rawString>Rotondo J. A. (1984) Clustering analysis of subjective partitions of text, Discourse Processes, 7, pp. 69-88</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Global text matching for information retrieval,</title>
<date>1991</date>
<journal>Science,</journal>
<volume>253</volume>
<pages>1012--1015</pages>
<contexts>
<context position="7739" citStr="Salton and Buckley, 1991" startWordPosition="1139" endWordPosition="1142">y-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex). Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex. 3 Proposed Segmentation Algorithm The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity. A window size of three sentences was found to produce the best results. Multiple sentences were compared because 615 calculating lexical similarity between words is too fine (Rotondo, 1984) and between individual sentences is unreliable (Salton and Buckley, 1991). Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score. Word repetitions are identified between identical words and words derived from the same stem. Collocations are located by looking up word pairs in the collocation lexicon. Relation weights are calculated between pairwise words according to their location in RT. The lexical similarity score indicates the amount of lexical cohesion demonstrated by two windows. Scores plotted on a graph show a series of peaks (high scores) and troughs (low scores). Low scores</context>
</contexts>
<marker>Salton, Buckley, 1991</marker>
<rawString>Salton G. and Buckley C. (1991) Global text matching for information retrieval, Science, 253, pp. 1012-1015</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Automatic text structuring experiments in &amp;quot;Text-Based Intelligent Systems: Current Research and Practice in Information Extraction and Retrieval,&amp;quot;</title>
<date>1992</date>
<pages>199--210</pages>
<location>New Jersey,</location>
<contexts>
<context position="1509" citStr="Salton and Buckley (1992)" startWordPosition="214" endWordPosition="217"> of heterogeneous data can be found in daily life. The Wall Street Journal archives, for example, consist of a series of articles about different subject areas. Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user&apos;s query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document. Previous work on text segmentation has used term matching to identify clusters of related text. Salton and Buckley (1992) and later, Hearst (1994) extracted related text portions by matching high frequency terms. Yaari (1997) segmented text into a hierarchical structure, identifying sub-segments of larger segments. Ponte and Croft (1997) used word co-occurrences to expand the number of terms for matching. Reynar (1994) compared all words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically</context>
</contexts>
<marker>Salton, Buckley, 1992</marker>
<rawString>Salton G. and Buckley C. (1992) Automatic text structuring experiments in &amp;quot;Text-Based Intelligent Systems: Current Research and Practice in Information Extraction and Retrieval,&amp;quot; P. S. Jacobs, ed, Lawrence Earlbaum Associates, New Jersey, pp. 199-210</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>J Allen</author>
<author>C Buckley</author>
</authors>
<title>Automatic structuring and retrieval of large text files,</title>
<date>1994</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<volume>37</volume>
<issue>2</issue>
<pages>97--108</pages>
<contexts>
<context position="2037" citStr="Salton et al., 1994" startWordPosition="294" endWordPosition="297">ation has used term matching to identify clusters of related text. Salton and Buckley (1992) and later, Hearst (1994) extracted related text portions by matching high frequency terms. Yaari (1997) segmented text into a hierarchical structure, identifying sub-segments of larger segments. Ponte and Croft (1997) used word co-occurrences to expand the number of terms for matching. Reynar (1994) compared all words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994). Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. Another approach extracted semantic information from Roget&apos;s Thesaurus (RT). Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chai</context>
</contexts>
<marker>Salton, Allen, Buckley, 1994</marker>
<rawString>Salton G., Allen J. and Buckley C. (1994) Automatic structuring and retrieval of large text files, Communications of the Association for Computing Machinery, 37(2), pp. 97-108</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Stairmand</author>
</authors>
<title>Textual context analysis for information retrieval,</title>
<date>1997</date>
<booktitle>Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>140--147</pages>
<location>Philadelphia,</location>
<contexts>
<context position="9141" citStr="Stairmand, 1997" startWordPosition="1358" endWordPosition="1359">n was conducted to determine whether the segmentation algorithm could reliably locate subject change in text. Method: Seven topical articles of between 250 to 450 words in length were extracted from the World Wide Web. A total of 42 texts for test data were generated by concatenating pairs of these articles. Hence, each generated text consisted of two articles. The transition from the first article to the second represented a known subject change point. Previous work has identified the breaks between concatenated texts to evaluate the performance of text segmentation algorithms (Reynar, 1994; Stairmand, 1997). For each text, the troughs placed by the segmentation algorithm were compared to the location of the known subject change point in that text. An error margin of one sentence either side of this point, determined by empirical analysis, was allowed. Results: Table 1 gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points. linguistic feature troughs placed subject change points located (out of 42 poss.) average std. dev. word repetition 7.1 3.16 41 collocation (97.6%) word repetition 7.3 5.22 41 relation weights (97.6%) word re</context>
</contexts>
<marker>Stairmand, 1997</marker>
<rawString>Stairmand M. A. (1997) Textual context analysis for information retrieval, Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, Philadelphia, pp. 140-147</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yaari</author>
</authors>
<title>Segmentation of expository texts by hierarchical agglomerative clustering,</title>
<date>1997</date>
<location>RANLP&apos;97, Bulgaria</location>
<contexts>
<context position="1613" citStr="Yaari (1997)" startWordPosition="231" endWordPosition="232"> of articles about different subject areas. Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user&apos;s query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document. Previous work on text segmentation has used term matching to identify clusters of related text. Salton and Buckley (1992) and later, Hearst (1994) extracted related text portions by matching high frequency terms. Yaari (1997) segmented text into a hierarchical structure, identifying sub-segments of larger segments. Ponte and Croft (1997) used word co-occurrences to expand the number of terms for matching. Reynar (1994) compared all words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work r</context>
</contexts>
<marker>Yaari, 1997</marker>
<rawString>Yaari Y. (1997) Segmentation of expository texts by hierarchical agglomerative clustering, RANLP&apos;97, Bulgaria</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>