<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000126">
<title confidence="0.999345">
A Dependency Treelet String Correspondence
Model for Statistical Machine Translation
</title>
<author confidence="0.991214">
Deyi Xiong, Qun Liu and Shouxun Lin
</author>
<affiliation confidence="0.984755">
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.791273">
Beijing, China, 100080
</address>
<email confidence="0.98905">
{dyxiong, liuqun, sxlin}@ict.ac.cn
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912965517241">
This paper describes a novel model using
dependency structures on the source side
for syntax-based statistical machine transla-
tion: Dependency Treelet String Correspon-
dence Model (DTSC). The DTSC model
maps source dependency structures to tar-
get strings. In this model translation pairs of
source treelets and target strings with their
word alignments are learned automatically
from the parsed and aligned corpus. The
DTSC model allows source treelets and tar-
get strings with variables so that the model
can generalize to handle dependency struc-
tures with the same head word but with dif-
ferent modifiers and arguments. Addition-
ally, target strings can be also discontinuous
by using gaps which are corresponding to
the uncovered nodes which are not included
in the source treelets. A chart-style decod-
ing algorithm with two basic operations–
substituting and attaching–is designed for
the DTSC model. We argue that the DTSC
model proposed here is capable of lexical-
ization, generalization, and handling discon-
tinuous phrases which are very desirable for
machine translation. We finally evaluate our
current implementation of a simplified ver-
sion of DTSC for statistical machine trans-
lation.
</bodyText>
<sectionHeader confidence="0.998894" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993025">
Over the last several years, various statistical syntax-
based models were proposed to extend traditional
word/phrase based models in statistical machine
translation (SMT) (Lin, 2004; Chiang, 2005; Ding
et al., 2005; Quirk et al., 2005; Marcu et al., 2006;
Liu et al., 2006). It is believed that these models
can improve the quality of SMT significantly. Com-
pared with phrase-based models, syntax-based mod-
els lead to better reordering and higher flexibility
by introducing hierarchical structures and variables
which make syntax-based models capable of hierar-
chical reordering and generalization. Due to these
advantages, syntax-based approaches are becoming
an active area of research in machine translation.
In this paper, we propose a novel model based on
dependency structures: Dependency Treelet String
Correspondence Model (DTSC). The DTSC model
maps source dependency structures to target strings.
It just needs a source language parser. In contrast to
the work by Lin (2004) and by Quirk et al. (2005),
the DTSC model does not need to generate target
language dependency structures using source struc-
tures and word alignments. On the source side, we
extract treelets which are any connected subgraphs
and consistent with word alignments. While on the
target side, we allow the aligned target sequences
to be generalized and discontinuous by introducing
variables and gaps. The variables on the target side
are aligned to the corresponding variables of treelets,
while gaps between words or variables are corre-
sponding to the uncovered nodes which are not in-
cluded by treelets. To complete the translation pro-
cess, we design two basic operations for the decod-
ing: substituting and attaching. Substituting is used
to replace variable nodes which have been already
translated, while attaching is used to attach uncov-
</bodyText>
<page confidence="0.983921">
40
</page>
<note confidence="0.893984">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 40–47,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.997530461538462">
ered nodes to treelets.
In the remainder of the paper, we first define de-
pendency treelet string correspondence in section
2 and describe an algorithm for extracting DTSCs
from the parsed and word-aligned corpus in section
3. Then we build our model based on DTSC in sec-
tion 4. The decoding algorithm and related pruning
strategies are introduced in section 5. We also spec-
ify the strategy to integrate phrases into our model
in section 6. In section 7 we evaluate our current
implementation of a simplified version of DTSC for
statistical machine translation. And finally, we dis-
cuss related work and conclude.
</bodyText>
<sectionHeader confidence="0.994357" genericHeader="introduction">
2 Dependency Treelet String
Correspondence
</sectionHeader>
<bodyText confidence="0.985170903225806">
A dependency treelet string correspondence 7r is a
triple &lt; D, 5, A &gt; which describes a translation
pair &lt; D, 5 &gt; and their alignment A, where D is
the dependency treelet on the source side and 5 is
the translation string on the target side. &lt; D, 5 &gt;
must be consistent with the word alignment M of
the corresponding sentence pair
b(i,j)EM,iEDHjE5
A treelet is defined to be any connected subgraph,
which is similar to the definition in (Quirk et al.,
2005). Treelet is more representatively flexible than
subtree which is widely used in models based on
phrase structures (Marcu et al., 2006; Liu et al.,
2006). The most important distinction between the
treelet in (Quirk et al., 2005) and ours is that we al-
low variables at positions of subnodes. In our defini-
tion, the root node must be lexicalized but the subn-
odes can be replaced with a wild card. The target
counterpart of a wildcard node in 5 is also replaced
with a wild card. The wildcards introduced in this
way generalize DTSC to match dependency struc-
tures with the same head word but with different
modifiers or arguments.
Another unique feature of our DTSC is that we al-
low target strings with gaps between words or wild-
cards. Since source treelets may not cover all subn-
odes, the uncovered subnodes will generate a gap as
its counterpart on the target side. A sequence of con-
tinuous gaps will be merged to be one gap and gaps
at the beginning and the end of 5 will be removed
automatically.
</bodyText>
<figure confidence="0.5878715">
16�
111:7
Â
Â
Â
the conference cooperation of the ∗
</figure>
<figureCaption confidence="0.9423385">
Figure 1: DTSC examples. Note that * represents
variable and G represents gap.
</figureCaption>
<bodyText confidence="0.996608703703704">
Gap can be considered as a special kind of vari-
able whose counterpart on the source side is not
present. This makes the model more flexible to
match more partial dependency structures on the
source side. If only variables can be used, the model
has to match subtrees rather than treelets on the
source side. Furthermore, the positions of variables
on the target side are fixed so that some reorderings
related with them can be recorded in DTSC. The po-
sitions of gaps on the target side, however, are not
fixed until decoding. The presence of one gap and
its position can not be finalized until attaching op-
eration is performed. The introduction of gaps and
the related attaching operation in decoding is the
most important distinction between our model and
the previous syntax-based models.
Figure 1 shows several different DTSCs automat-
ically extracted from our training corpus. The top
left DTSC is totally lexicalized, while the top right
DTSC has one variable and the bottom has two vari-
ables and one gap. In the bottom DTSC, note that
the node Q which is aligned to the gap G of the
target string is an uncovered node and therefore not
included in the treelet actually. Here we just want
to show there is an uncovered node aligned with the
gap G.
Each node at the source treelet has three attributes
</bodyText>
<listItem confidence="0.9796126">
1. The headword
2. The category, i.e. the part of speech of the head
word
3. The node order which specifies the local order
of the current node relative to its parent node.
</listItem>
<figure confidence="0.980265354166667">
bbbbbbbbbbbbb
eeeeeee ��
s s
s
s s
°
]
S
w
w
5
] ]
] ]
] ]
∗1 keep a G with the ∗2
S YYYYYYY
s S S
s s
∗2S
] S ]
S ]
∗1
w
w
I
Â
Â
A
s
s
s
eeeeeee Â
s
]
] s ]s Â
∗
s
]
]Â
] ]
]
] ]
41
N#1h/VV
\
X
go on providingfinancial aid to Palestine
1 2 3 4 5 6 7
</figure>
<figureCaption confidence="0.949359">
Figure 2: An example dependency tree and its align-
ments
</figureCaption>
<bodyText confidence="0.999964857142857">
Note that the node order is defined at the context of
the extracted treelets but not the context of the orig-
inal tree. For example, the attributes for the node 与
in the bottom DTSC of Figure 1 are {—Ej, P, -11. For
two treelets, if and only if their structures are iden-
tical and each corresponding nodes share the same
attributes, we say they are matched.
</bodyText>
<sectionHeader confidence="0.983708" genericHeader="method">
3 Extracting DTSCs
</sectionHeader>
<bodyText confidence="0.9998689375">
To extract DTSCs from the training corpus, firstly
the corpus must be parsed on the source side and
aligned at the word level. The source structures pro-
duced by the parser are unlabelled, ordered depen-
dency trees with each word annotated with a part-of-
speech. Figure 2 shows an example of dependency
tree really used in our extractor.
When the source language dependency trees and
word alignments between source and target lan-
guages are obtained, the DTSC extraction algorithm
runs in two phases along the dependency trees and
alignments. In the first step, the extractor annotates
each node with specific attributes defined in section
3.1. These attributes are used in the second step
which extracts all possible DTSCs rooted at each
node recursively.
</bodyText>
<subsectionHeader confidence="0.999167">
3.1 Node annotation
</subsectionHeader>
<bodyText confidence="0.988848380952381">
For each source dependency node n, we define three
attributes: word span, node span and crossed.
Word span is defined to be the target word sequence
aligned with the head word of n, while node span is
defined to be the closure of the union of node spans
of all subnodes of n and its word span. These two at-
tributes are similar to those introduced by Lin (Lin,
2004). The third attribute crossed is an indicator that
has binary values. If the node span of n overlaps
the word span of its parent node or the node span
of its siblings, the crossed indicator of n is 1 and
n is therefore a crossed node, otherwise the crossed
indicator is 0 and n is a non-crossed node. Only
non-crossed nodes can generate DTSCs because the
target word sequence aligned with the whole subtree
rooted at it does not overlap any other sequences and
therefore can be extracted independently.
For the dependency tree and its alignments shown
in Figure 2, only the node 财政 is a crossed node
since its node span ([4,5]) overlaps the word span
([5,5]) of its parent node 援助 .
</bodyText>
<subsectionHeader confidence="0.999304">
3.2 DTSCs extraction
</subsectionHeader>
<bodyText confidence="0.999500878787879">
The DTSC extraction algorithm (shown in Figure 3)
runs recursively. For each non-crossed node, the al-
gorithm generates all possible DTSCs rooted at it by
combining DTSCs from some subsets of its direct
subnodes. If one subnode n selected in the com-
bination is a crossed node, all other nodes whose
word/node spans overlap the node span of n must be
also selected in this combination. This kind of com-
bination is defined to be consistent with the word
alignment because the DTSC generated by this com-
bination is consistent with the word alignment. All
DTSCs generated in this way will be returned to the
last call and outputted. For each crossed node, the
algorithm generates pseudo DTSCs1 using DTSCs
from all of its subnodes. These pseudo DTSCs will
be returned to the last call but not outputted.
During the combination of DTSCs from subnodes
into larger DTSCs, there are two major tasks. One
task is to generate the treelet using treelets from
subnodes and the current node. This is a basic tree
generation operation. It is worth mentioning that
some non-crossed nodes are to be replaced with a
wild card so the algorithm can learn generalized
DTSCs described in section 2. Currently, we re-
place any non-crossed node alone or together with
their sibling non-crossed nodes. The second task
is to combine target strings. The word sequences
aligned with uncovered nodes will be replaced with
a gap. The word sequences aligned with wildcard
nodes will be replaced with a wild card.
If a non-crossed node n has m direct subnodes,
all 2&amp;quot;&amp;quot; � combinations will be considered. This will
generate a very large number of DTSCs, which is
</bodyText>
<footnote confidence="0.9167345">
1Some words in the target string are aligned with nodes
which are not included in the source treelet.
</footnote>
<figure confidence="0.924721102040816">
b e Â
44/VV Ip]/P
X X X Xffi-/lvRX
D
eeeeeee
M/NN
z
z
X
\
ÂD
D
Â D
\
X�/NNz
X
\jX\ z
Â
D
Â j j z Â X \ X \
\ \ \
42
DTSCExtractor(Dnode n)
&lt; := ∅ (DTSC container of n)
for each subnode k of n do
R := DTSCExtractor(k)
L:=LUR
end for
if n.crossed! = 1 and there are no subnodes whose span
overlaps the word span of n then
Create a DTSC Ir =&lt; D, S, A &gt; where the dependency
treelet D only contains the node n (not including any chil-
dren of it)
output Ir
for each combination c of n’s subnodes do
if c is consistent with the word alignment then
Generate all DTSCs R by combining DTSCs (L)
from the selected subnodes with the current node n
&lt; := &lt;UR
end if
end for
output &lt;
return &lt;
else if n.crossed == 1 then
Create pseudo DTSCs P by combining all DTSCs from
n’s all subnodes.
&lt; := &lt;UP
return &lt;
end if
</figure>
<figureCaption confidence="0.999817">
Figure 3: DTSC Extraction Algorithm.
</figureCaption>
<bodyText confidence="0.9962315">
undesirable for training and decoding. Therefore we
filter DTSCs according to the following restrictions
</bodyText>
<listItem confidence="0.957493705882353">
1. If the number of direct subnodes of node n is
larger than 6, we only consider combining one
single subnode with n each time because in this
case reorderings of subnodes are always mono-
tone.
2. On the source side, the number of direct subn-
odes of each node is limited to be no greater
than ary-limit; the height of treelet D is limited
to be no greater than depth-limit.
3. On the target side, the length of S (including
gaps and variables) is limited to be no greater
than len-limit; the number of gaps in S is lim-
ited to be no greater than gap-limit.
4. During DTSC combination, the DTSCs from
each subnode are sorted by size (in descending
order). Only the top comb-limit DTSCs will be
selected to generate larger DTSCs.
</listItem>
<bodyText confidence="0.989665">
As an example, for the dependency tree and its
alignments in Figure 2, all DTSCs extracted by the
</bodyText>
<table confidence="0.996888">
Treelet String
(继续/VV/0) go on
(巴勒斯坦/NR/0) Palestine
(向/P/0) to
(向/P/0 (巴勒斯坦/NR/1)) to Palestine
(向/P/0 (∗/1)) to ∗
(援助/NN/0 (财政/NN/-1)) financial aid
(提供/VV/0) providing
(提供/VV/0 (∗/1)) providing ∗
(提供/VV/0 (∗/-1)) providing G ∗
(提供/VV/0 (继续/VV/-1)) go on providing
(提供/VV/0 (∗/-1)) ∗ providing
(提供/VV/0 (∗1/-1) (∗2/1)) providing ∗2 ∗1
(提供/VV/0 (∗1/-1 ) (∗2/1)) ∗1 providing ∗2
</table>
<tableCaption confidence="0.9726235">
Table 1: Examples of DTSCs extracted from Figure
2. Alignments are not shown here because they are
</tableCaption>
<equation confidence="0.92757425">
self-evident.
algorithm with parameters { ary-limit = 2, depth-
limit = 2, len-limit = 3, gap-limit = 1, comb-limit
= 20 } are shown in the table 1.
</equation>
<sectionHeader confidence="0.998999" genericHeader="method">
4 The Model
</sectionHeader>
<bodyText confidence="0.999892625">
Given an input dependency tree, the decoder gen-
erates translations for each dependency node in
bottom-up order. For each node, our algorithm will
search all matched DTSCs automatically learned
from the training corpus by the way mentioned in
section 3. When the root node is traversed, the trans-
lating is finished. This complicated procedure in-
volves a large number of sequences of applications
of DTSC rules. Each sequence of applications of
DTSC rules can derive a translation.
We define a derivation S as a sequence of appli-
cations of DTSC rules, and let c(S) and e(S) be the
source dependency tree and the target yield of S, re-
spectively. The score of S is defined to be the prod-
uct of the score of the DTSC rules used in the trans-
lation, and timed by other feature functions:
</bodyText>
<equation confidence="0.999284">
§(�) = II §(i) · pl,,t(e)i · exp(−AapA(s)) (1)
Z
</equation>
<bodyText confidence="0.999830714285714">
where §(i) is the score of the ith application of
DTSC rules, pl,,t(e) is the language model score,
and exp(−AapA(S)) is the attachment penalty,
where A(S) calculates the total number of attach-
ments occurring in the derivation S. The attach-
ment penalty gives some control over the selection
of DTSC rules which makes the model prefer rules
</bodyText>
<page confidence="0.999508">
43
</page>
<bodyText confidence="0.9985265">
with more nodes covered and therefore less attach-
ing operations involved.
For the score of DTSC rule 7r, we define it as fol-
lows:
</bodyText>
<equation confidence="0.984582">
�§(7r) = fj(7r)λj (2)
j
</equation>
<bodyText confidence="0.99973">
where the fj are feature functions defined on DTSC
rules. Currently, we used features proved to be ef-
fective in phrase-based SMT, which are:
</bodyText>
<listItem confidence="0.998513692307692">
1. The translation probability p(D|5).
2. The inverse translation probability p(5|D).
3. The lexical translation probability plex(D|5)
which is computed over the words that occur
on the source and target sides of a DTSC rule
by the IBM model 1.
4. The inverse lexical translation probability
plex(5|D) which is computed over the words
that occur on the source and target sides of a
DTSC rule by the IBM model 1.
5. The word penalty wp.
6. The DTSC penalty dp which allows the model
to favor longer or shorter derivations.
</listItem>
<bodyText confidence="0.999734571428571">
It is worth mentioning how to integrate the N-
gram language mode into our DTSC model. During
decoding, we have to encounter many partial transla-
tions with gaps and variables. For these translations,
firstly we only calculate the language model scores
for word sequences in the translations. Later we up-
date the scores when gaps are removed or specified
by attachments or variables are substituted. Each up-
dating involves merging two neighbor substrings sl
(left) and sr (right) into one bigger string s. Let the
sequence of n − 1 (n is the order of N-gram lan-
guage model used) rightmost words of sl be srl and
the sequence of n−1 leftmost words of sr be slr. we
have:
</bodyText>
<equation confidence="0.998366">
LM(s) = LM(sl) + LM(sr) + LM(srl slr)
−LM(srl ) − LM(slr) (3)
</equation>
<bodyText confidence="0.999545333333333">
where LM is the logarithm of the language model
probability. We only need to compute the increment
of the language model score:
</bodyText>
<equation confidence="0.961035333333333">
ALM = LM(srl slr) − LM(srl )
− LM(slr) (4)
for each node n of the input tree T, in bottom-up order do
</equation>
<table confidence="0.528350222222222">
Get all matched DTSCs rooted at n
for each matched DTSC it do
for each wildcard node n* in it do
Substitute the corresponding wildcard on the target
side with translations from the stack of n*
end for
for each uncovered node no by it do
Attach the translations from the stack of no to the
target side at the attaching point
</table>
<tableCaption confidence="0.911555">
end for
end for
end for
</tableCaption>
<figureCaption confidence="0.9987305">
Figure 4: Chart-style Decoding Algorithm for the
DTSC Model.
</figureCaption>
<bodyText confidence="0.65555">
Melamed (2004) also used a similar way to integrate
the language model.
</bodyText>
<sectionHeader confidence="0.996548" genericHeader="method">
5 Decoding
</sectionHeader>
<bodyText confidence="0.999962148148148">
Our decoding algorithm is similar to the bottom-up
chart parsing. The distinction is that the input is a
tree rather than a string and therefore the chart is in-
dexed by nodes of the tree rather than spans of the
string. Also, several other tree-based decoding al-
gorithms introduced by Eisner (2003), Quirk et al.
(2005) and Liu et al. (2006) can be classified as the
chart-style parsing algorithm too.
Our decoding algorithm is shown in Figure 4.
Given an input dependency tree, firstly we generate
the bottom-up order by postorder transversal. This
order guarantees that any subnodes of node n have
been translated before node n is done. For each
node n in the bottom-up order, all matched DTSCs
rooted at n are found, and a stack is also built for it to
store the candidate translations. A DTSC 7r is said to
match the input dependency subtree T rooted at n if
and only if there is a treelet rooted at n that matches
2 the treelet of 7r on the source side.
For each matched DTSC 7r, two operations will
be performed on it. The first one is substituting
which replaces a wildcard node with the correspond-
ing translated node. The second one is attaching
which attaches an uncovered node to 7r. The two op-
erations are shown in Figure 5. For each wildcard
node n*, translations from the stack of it will be se-
lected to replace the corresponding wildcard on the
</bodyText>
<footnote confidence="0.691488666666667">
2The words, categories and orders of each corresponding
nodes are matched. Please refer to the definition of matched
in section 2.
</footnote>
<page confidence="0.998066">
44
</page>
<figure confidence="0.998623846153846">
(a) ������� �������
A
B
*e Ae Be Ce
Substitute 4
De Ae Be Ce
Attach 4
(c) ������� �������
A
B D
������� �������
C E
De Ae Be Ee Ce
</figure>
<figureCaption confidence="0.998142">
Figure 5: Substituting and attaching operations for
</figureCaption>
<bodyText confidence="0.963015966666667">
decoding. Xe is the translation of X. Node that * is
a wildcard node to be substituted and node Q is an
uncovered node to be attached.
target side and the scores of new translations will be
calculated according to our model. For each uncov-
ered node n@, firstly we determine where transla-
tions from the stack of n@ should be attached on the
target side. There are several different mechanisms
for choosing attaching points. Currently, we imple-
ment a heuristic way: on the source side, we find the
node n@� which is the nearest neighbor of n@ from
its parent and sibling nodes, then the attaching point
is the left/right of the counterpart of n@� on the target
side according to their relative order. As an example,
see the uncovered node Q in Figure 5. The nearest
node to it is node B. Since node Q is at the right
of node B, the attaching point is the right of Be.
One can search all possible points using an ordering
model. And this ordering model can also use infor-
mation from gaps on the target side. We believe this
ordering model can improve the performance and let
it be one of directions for our future research.
Note that the gaps on the target side are not neces-
sarily attaching points in our current attaching mech-
anism. If they are not attaching point, they will be
removed automatically.
The search space of the decoding algorithm is
very large, therefore some pruning techniques have
to be used. To speed up the decoder, the following
pruning strategies are adopted.
</bodyText>
<listItem confidence="0.760645125">
1. Stack pruning. We use three pruning ways.
The first one is recombination which converts
the search to dynamic programming. When
two translations in the same stack have the
same w leftmost/rightmost words, where w de-
pends on the order of the language model, they
will be recombined by discarding the transla-
tion with lower score. The second one is the
threshold pruning which discards translations
that have a score worse than stack-threshold
times the best score in the same stack. The
last one is the histogram pruning which only
keeps the top stack-limit best translations for
each stack.
2. Node pruning. For each node, we only keep
the top node-limit matched DTSCs rooted at
that node, as ranked by the size of source
treelets.
3. Operation pruning. For each operation, sub-
stituting and attaching, the decoding will gen-
erate a large number of partial translations3
for the current node. We only keep the top
operation-limit partial translations each time
according to their scores.
</listItem>
<sectionHeader confidence="0.98509" genericHeader="method">
6 Integrating Phrases
</sectionHeader>
<bodyText confidence="0.999875866666667">
Although syntax-based models are good at dealing
with hierarchical reordering, but at the local level,
translating idioms and similar complicated expres-
sions can be a problem. However, phrase-based
models are good at dealing with these translations.
Therefore, integrating phrases into the syntax-based
models can improve the performance (Marcu et al.,
2006; Liu et al., 2006). Since our DTSC model is
based on dependency structures and lexicalized nat-
urally, DTSCs are more similar to phrases than other
translation units based on phrase structures. This
means that phrases will be easier to be integrated
into our model.
The way to integrate phrases is quite straightfor-
ward: if there is a treelet rooted at the current node,
</bodyText>
<footnote confidence="0.933681">
3There are wildcard nodes or uncovered nodes to be han-
dled.
</footnote>
<figure confidence="0.996226">
�������
C De
* + D
(b) A
������� �������
�������B
C Ee
D + E
</figure>
<page confidence="0.997542">
45
</page>
<bodyText confidence="0.9999925">
of which the word sequence is continuous and iden-
tical to the source of some phrase, then a phrase-
style DTSC will be generated which uses the target
string of the phrase as its own target. The procedure
is finished during decoding. In our experiments, in-
tegrating phrases improves the performance greatly.
</bodyText>
<sectionHeader confidence="0.992537" genericHeader="method">
7 Current Implementation
</sectionHeader>
<bodyText confidence="0.999912974358974">
To test our idea, we implemented the dependency
treelet string correspondence model in a Chinese-
English machine translation system. The current im-
plementation in this system is actually a simplified
version of the DTSC model introduced above. In
this version, we used a simple heuristic way for the
operation of attaching rather than a sophisticated sta-
tistical model which can learn ordering information
from the training corpus. Since dependency struc-
tures are more “flattened” compared with phrasal
structures, there are many subnodes which will not
be covered even by generalized matched DTSCs.
This means the attaching operation is very common
during decoding. Therefore better attaching model
which calculates the best point for attaching , we be-
lieve, will improve the performance greatly and is a
major goal for our future research.
To obtain the dependency structures of the source
side, one can parse the source sentences with a de-
pendency parser or parse them with a phrasal struc-
ture parser and then convert the phrasal structures
into dependency structures. In our experiments we
used a Chinese parser implemented by Xiong et
al. (2005) which generates phrasal structures. The
parser was trained on articles 1-270 of Penn Chinese
Treebank version 1.0 and achieved 79.4% (F1 mea-
sure). We then converted the phrasal structure trees
into dependency trees using the way introduced by
Xia (1999).
To obtain the word alignments, we use the way
of Koehn et al. (2005). After running GIZA++
(Och and Ney, 2000) in both directions, we apply
the “grow-diag-final” refinement rule on the in-
tersection alignments for each sentence pair.
The training corpus consists of 31, 149 sentence
pairs with 823K Chinese words and 927K English
words. For the language model, we used SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
trigram model with modified Kneser-Ney smooth-
</bodyText>
<table confidence="0.9961495">
Systems BLEU-4
PB 20.88 f 0.87
DTSC 20.20 f 0.81
DTSC + phrases 21.46 f 0.83
</table>
<tableCaption confidence="0.9517005">
Table 2: BLEU-4 scores for our system and a
phrase-based system.
</tableCaption>
<bodyText confidence="0.999796566666667">
ing on the 31, 149 English sentences. We selected
580 short sentences of length at most 50 characters
from the 2002 NIST MT Evaluation test set as our
development corpus and used it to tune As by max-
imizing the BLEU score (Och, 2003), and used the
2005 NIST MT Evaluation test set as our test corpus.
From the training corpus, we learned 2, 729,
964 distinct DTSCs with the configuration { ary-
limit = 4, depth-limit = 4, len-limit = 15, gap-limit
= 2, comb-limit = 20 }. Among them, 160,694
DTSCs are used for the test set. To run our de-
coder on the development and test set, we set stack-
thrshold = 0.0001, stack-limit = 100, node-limit =
100, operation-limit = 20.
We also ran a phrase-based system (PB) with a
distortion reordering model (Xiong et al., 2006) on
the same corpus. The results are shown in table 2.
For all BLEU scores, we also show the 95% confi-
dence intervals computed using Zhang’s significant
tester (Zhang et al., 2004) which was modified to
conform to NIST’s definition of the BLEU brevity
penalty. The BLEU score of our current system with
the DTSC model is lower than that of the phrase-
based system. However, with phrases integrated, the
performance is improved greatly, and the new BLEU
score is higher than that of the phrase-based SMT.
This difference is significant according to Zhang’s
tester. This result can be improved further using a
better parser (Quirk et al., 2006) or using a statisti-
cal attaching model.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999612714285714">
The DTSC model is different from previous work
based on dependency grammars by Eisner (2003),
Lin (2004), Quirk et al. (2005), Ding et al. (2005)
since they all deduce dependency structures on the
target side. Among them, the most similar work is
(Quirk et al., 2005). But there are still several major
differences beyond the one mentioned above. Our
</bodyText>
<page confidence="0.997193">
46
</page>
<bodyText confidence="0.9999076">
treelets allow variables at any non-crossed nodes and
target strings allow gaps, which are not available in
(Quirk et al., 2005). Our language model is calcu-
lated during decoding while Quirk’s language model
is computed after decoding because of the complex-
ity of their decoding.
The DTSC model is also quite distinct from pre-
vious tree-string models by Marcu et al. (2006)
and Liu et al. (2006). Firstly, their models are
based on phrase structure grammars. Secondly, sub-
trees instead of treelets are extracted in their mod-
els. Thirdly, it seems to be more difficult to integrate
phrases into their models. And finally, our model al-
low gaps on the target side, which is an advantage
shared by (Melamed, 2004) and (Simard, 2005).
</bodyText>
<sectionHeader confidence="0.996345" genericHeader="conclusions">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999882">
We presented a novel syntax-based model using
dependency trees on the source side–dependency
treelet string correspondence model–for statistical
machine translation. We described an algorithm to
learn DTSCs automatically from the training corpus
and a chart-style algorithm for decoding.
Currently, we implemented a simple version of
the DTSC model. We believe that our performance
can be improved greatly using a more sophisticated
mechanism for determining attaching points. There-
fore the most important future work should be to de-
sign a better attaching model. Furthermore, we plan
to use larger corpora for training and n-best depen-
dency trees for decoding, which both are helpful for
the improvement of translation quality.
</bodyText>
<sectionHeader confidence="0.99655" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999451333333333">
This work was supported by National Natural Sci-
ence Foundation of China, Contract No. 60603095
and 60573188.
</bodyText>
<sectionHeader confidence="0.99946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999808642857143">
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings ofACL.
Yuan Ding and Martha Palmer. 2005. Machine Translation Us-
ing Probabilistic Synchronous Dependency Insertion Gram-
mars. In Proceedings ofACL.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proceedings ofACL.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris
Callison-Burch, Miles Osborne and David Talbot. 2005.
Edinburgh System Description for the 2005 IWSLT Speech
Translation Evaluation. In International Workshop on Spo-
ken Language Translation.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical Machine Translation with
Syntactified Target Language Phraases. In Proceedings of
EMNLP.
I. Dan Melamed. 2004. Algorithms for Syntax-Aware Statisti-
cal Machine Translation. In Proceedings of the Conference
on Theoretical and Methodological Issues in Machine Trans-
lation (TMI), Baltimore, MD.
Dekang Lin. 2004. A path-based transfer model for machine
translation. In Proceedings of COLING.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Translation. In
Proceedings ofACL.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings ofACL.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proceedings ofACL.
Chris Quirk, Arul Menezes and Colin Cherry. 2005. Depen-
dency Treelet Translation: Syntactically Informed Phrasal
SMT. In Proceedings ofACL.
Chris Quirk and Simon Corston-Oliver. 2006. The impact of
parse quality on syntactically-informed statistical machine
translation. In Proceedings of EMNLP, Sydney, Australia.
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc
Dymetman, Eric Gaussier, Cyril Goutte, Kenji Yamada.
2005. Translating with non-contiguous phrases. In Proceed-
ings of HLT-EMNLP.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings ofInternational Conference on
Spoken Language Processing, volume 2, pages 901-904.
Fei Xia. 1999. Automatic Grammar Generation from Two Dif-
ferent Perspectives. PhD thesis, University of Pennsylvania.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum
Entropy Based Phrase Reordering Model for Statistical Ma-
chine Translation. In Proceedings of COLING-ACL, Sydney,
Australia.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang
Qian. 2005. Parsing the Penn Chinese Treebank with Se-
mantic Knowledge. In Proceedings ofIJCNLP, Jeju Island,
Korea.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do we
need to have a better system? In Proceedings of LREC,
pages 2051– 2054.
</reference>
<page confidence="0.99949">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.257957">
<title confidence="0.999518">A Dependency Treelet String Model for Statistical Machine Translation</title>
<author confidence="0.808995">Deyi Xiong</author>
<author confidence="0.808995">Qun Liu</author>
<author confidence="0.808995">Shouxun</author>
<affiliation confidence="0.892376">Key Laboratory of Intelligent Information Institute of Computing</affiliation>
<address confidence="0.638421">Chinese Academy of Beijing, China,</address>
<email confidence="0.996085">liuqun,</email>
<abstract confidence="0.993196566666666">This paper describes a novel model using dependency structures on the source side for syntax-based statistical machine translation: Dependency Treelet String Correspondence Model (DTSC). The DTSC model maps source dependency structures to target strings. In this model translation pairs of source treelets and target strings with their word alignments are learned automatically from the parsed and aligned corpus. The DTSC model allows source treelets and target strings with variables so that the model can generalize to handle dependency structures with the same head word but with different modifiers and arguments. Additionally, target strings can be also discontinuous by using gaps which are corresponding to the uncovered nodes which are not included in the source treelets. A chart-style decoding algorithm with two basic operations– substituting and attaching–is designed for the DTSC model. We argue that the DTSC model proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation. We finally evaluate our current implementation of a simplified version of DTSC for statistical machine translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1703" citStr="Chiang, 2005" startWordPosition="248" endWordPosition="249">t-style decoding algorithm with two basic operations– substituting and attaching–is designed for the DTSC model. We argue that the DTSC model proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation. We finally evaluate our current implementation of a simplified version of DTSC for statistical machine translation. 1 Introduction Over the last several years, various statistical syntaxbased models were proposed to extend traditional word/phrase based models in statistical machine translation (SMT) (Lin, 2004; Chiang, 2005; Ding et al., 2005; Quirk et al., 2005; Marcu et al., 2006; Liu et al., 2006). It is believed that these models can improve the quality of SMT significantly. Compared with phrase-based models, syntax-based models lead to better reordering and higher flexibility by introducing hierarchical structures and variables which make syntax-based models capable of hierarchical reordering and generalization. Due to these advantages, syntax-based approaches are becoming an active area of research in machine translation. In this paper, we propose a novel model based on dependency structures: Dependency Tr</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="17600" citStr="Eisner (2003)" startWordPosition="3087" endWordPosition="3088">the stack of n* end for for each uncovered node no by it do Attach the translations from the stack of no to the target side at the attaching point end for end for end for Figure 4: Chart-style Decoding Algorithm for the DTSC Model. Melamed (2004) also used a similar way to integrate the language model. 5 Decoding Our decoding algorithm is similar to the bottom-up chart parsing. The distinction is that the input is a tree rather than a string and therefore the chart is indexed by nodes of the tree rather than spans of the string. Also, several other tree-based decoding algorithms introduced by Eisner (2003), Quirk et al. (2005) and Liu et al. (2006) can be classified as the chart-style parsing algorithm too. Our decoding algorithm is shown in Figure 4. Given an input dependency tree, firstly we generate the bottom-up order by postorder transversal. This order guarantees that any subnodes of node n have been translated before node n is done. For each node n in the bottom-up order, all matched DTSCs rooted at n are found, and a stack is also built for it to store the candidate translations. A DTSC 7r is said to match the input dependency subtree T rooted at n if and only if there is a treelet root</context>
<context position="26219" citStr="Eisner (2003)" startWordPosition="4576" endWordPosition="4577">004) which was modified to conform to NIST’s definition of the BLEU brevity penalty. The BLEU score of our current system with the DTSC model is lower than that of the phrasebased system. However, with phrases integrated, the performance is improved greatly, and the new BLEU score is higher than that of the phrase-based SMT. This difference is significant according to Zhang’s tester. This result can be improved further using a better parser (Quirk et al., 2006) or using a statistical attaching model. 8 Related Work The DTSC model is different from previous work based on dependency grammars by Eisner (2003), Lin (2004), Quirk et al. (2005), Ding et al. (2005) since they all deduce dependency structures on the target side. Among them, the most similar work is (Quirk et al., 2005). But there are still several major differences beyond the one mentioned above. Our 46 treelets allow variables at any non-crossed nodes and target strings allow gaps, which are not available in (Quirk et al., 2005). Our language model is calculated during decoding while Quirk’s language model is computed after decoding because of the complexity of their decoding. The DTSC model is also quite distinct from previous tree-s</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>Edinburgh System Description for the</booktitle>
<contexts>
<context position="24126" citStr="Koehn et al. (2005)" startWordPosition="4208" endWordPosition="4211"> dependency structures of the source side, one can parse the source sentences with a dependency parser or parse them with a phrasal structure parser and then convert the phrasal structures into dependency structures. In our experiments we used a Chinese parser implemented by Xiong et al. (2005) which generates phrasal structures. The parser was trained on articles 1-270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure). We then converted the phrasal structure trees into dependency trees using the way introduced by Xia (1999). To obtain the word alignments, we use the way of Koehn et al. (2005). After running GIZA++ (Och and Ney, 2000) in both directions, we apply the “grow-diag-final” refinement rule on the intersection alignments for each sentence pair. The training corpus consists of 31, 149 sentence pairs with 823K Chinese words and 927K English words. For the language model, we used SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothSystems BLEU-4 PB 20.88 f 0.87 DTSC 20.20 f 0.81 DTSC + phrases 21.46 f 0.83 Table 2: BLEU-4 scores for our system and a phrase-based system. ing on the 31, 149 English sentences. We selected 580 sh</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical Machine Translation with Syntactified Target Language Phraases.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1762" citStr="Marcu et al., 2006" startWordPosition="258" endWordPosition="261"> substituting and attaching–is designed for the DTSC model. We argue that the DTSC model proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation. We finally evaluate our current implementation of a simplified version of DTSC for statistical machine translation. 1 Introduction Over the last several years, various statistical syntaxbased models were proposed to extend traditional word/phrase based models in statistical machine translation (SMT) (Lin, 2004; Chiang, 2005; Ding et al., 2005; Quirk et al., 2005; Marcu et al., 2006; Liu et al., 2006). It is believed that these models can improve the quality of SMT significantly. Compared with phrase-based models, syntax-based models lead to better reordering and higher flexibility by introducing hierarchical structures and variables which make syntax-based models capable of hierarchical reordering and generalization. Due to these advantages, syntax-based approaches are becoming an active area of research in machine translation. In this paper, we propose a novel model based on dependency structures: Dependency Treelet String Correspondence Model (DTSC). The DTSC model ma</context>
<context position="4730" citStr="Marcu et al., 2006" startWordPosition="734" endWordPosition="737">elet String Correspondence A dependency treelet string correspondence 7r is a triple &lt; D, 5, A &gt; which describes a translation pair &lt; D, 5 &gt; and their alignment A, where D is the dependency treelet on the source side and 5 is the translation string on the target side. &lt; D, 5 &gt; must be consistent with the word alignment M of the corresponding sentence pair b(i,j)EM,iEDHjE5 A treelet is defined to be any connected subgraph, which is similar to the definition in (Quirk et al., 2005). Treelet is more representatively flexible than subtree which is widely used in models based on phrase structures (Marcu et al., 2006; Liu et al., 2006). The most important distinction between the treelet in (Quirk et al., 2005) and ours is that we allow variables at positions of subnodes. In our definition, the root node must be lexicalized but the subnodes can be replaced with a wild card. The target counterpart of a wildcard node in 5 is also replaced with a wild card. The wildcards introduced in this way generalize DTSC to match dependency structures with the same head word but with different modifiers or arguments. Another unique feature of our DTSC is that we allow target strings with gaps between words or wildcards. </context>
<context position="21827" citStr="Marcu et al., 2006" startWordPosition="3825" endWordPosition="3828">uning. For each operation, substituting and attaching, the decoding will generate a large number of partial translations3 for the current node. We only keep the top operation-limit partial translations each time according to their scores. 6 Integrating Phrases Although syntax-based models are good at dealing with hierarchical reordering, but at the local level, translating idioms and similar complicated expressions can be a problem. However, phrase-based models are good at dealing with these translations. Therefore, integrating phrases into the syntax-based models can improve the performance (Marcu et al., 2006; Liu et al., 2006). Since our DTSC model is based on dependency structures and lexicalized naturally, DTSCs are more similar to phrases than other translation units based on phrase structures. This means that phrases will be easier to be integrated into our model. The way to integrate phrases is quite straightforward: if there is a treelet rooted at the current node, 3There are wildcard nodes or uncovered nodes to be handled. ������� C De * + D (b) A ������� ������� �������B C Ee D + E 45 of which the word sequence is continuous and identical to the source of some phrase, then a phrasestyle D</context>
<context position="26854" citStr="Marcu et al. (2006)" startWordPosition="4681" endWordPosition="4684">irk et al. (2005), Ding et al. (2005) since they all deduce dependency structures on the target side. Among them, the most similar work is (Quirk et al., 2005). But there are still several major differences beyond the one mentioned above. Our 46 treelets allow variables at any non-crossed nodes and target strings allow gaps, which are not available in (Quirk et al., 2005). Our language model is calculated during decoding while Quirk’s language model is computed after decoding because of the complexity of their decoding. The DTSC model is also quite distinct from previous tree-string models by Marcu et al. (2006) and Liu et al. (2006). Firstly, their models are based on phrase structure grammars. Secondly, subtrees instead of treelets are extracted in their models. Thirdly, it seems to be more difficult to integrate phrases into their models. And finally, our model allow gaps on the target side, which is an advantage shared by (Melamed, 2004) and (Simard, 2005). 9 Conclusions and Future Work We presented a novel syntax-based model using dependency trees on the source side–dependency treelet string correspondence model–for statistical machine translation. We described an algorithm to learn DTSCs automa</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical Machine Translation with Syntactified Target Language Phraases. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Algorithms for Syntax-Aware Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Theoretical and Methodological Issues in Machine Translation (TMI),</booktitle>
<location>Baltimore, MD.</location>
<contexts>
<context position="17233" citStr="Melamed (2004)" startWordPosition="3023" endWordPosition="3024">ge model probability. We only need to compute the increment of the language model score: ALM = LM(srl slr) − LM(srl ) − LM(slr) (4) for each node n of the input tree T, in bottom-up order do Get all matched DTSCs rooted at n for each matched DTSC it do for each wildcard node n* in it do Substitute the corresponding wildcard on the target side with translations from the stack of n* end for for each uncovered node no by it do Attach the translations from the stack of no to the target side at the attaching point end for end for end for Figure 4: Chart-style Decoding Algorithm for the DTSC Model. Melamed (2004) also used a similar way to integrate the language model. 5 Decoding Our decoding algorithm is similar to the bottom-up chart parsing. The distinction is that the input is a tree rather than a string and therefore the chart is indexed by nodes of the tree rather than spans of the string. Also, several other tree-based decoding algorithms introduced by Eisner (2003), Quirk et al. (2005) and Liu et al. (2006) can be classified as the chart-style parsing algorithm too. Our decoding algorithm is shown in Figure 4. Given an input dependency tree, firstly we generate the bottom-up order by postorder</context>
<context position="27190" citStr="Melamed, 2004" startWordPosition="4741" endWordPosition="4742">available in (Quirk et al., 2005). Our language model is calculated during decoding while Quirk’s language model is computed after decoding because of the complexity of their decoding. The DTSC model is also quite distinct from previous tree-string models by Marcu et al. (2006) and Liu et al. (2006). Firstly, their models are based on phrase structure grammars. Secondly, subtrees instead of treelets are extracted in their models. Thirdly, it seems to be more difficult to integrate phrases into their models. And finally, our model allow gaps on the target side, which is an advantage shared by (Melamed, 2004) and (Simard, 2005). 9 Conclusions and Future Work We presented a novel syntax-based model using dependency trees on the source side–dependency treelet string correspondence model–for statistical machine translation. We described an algorithm to learn DTSCs automatically from the training corpus and a chart-style algorithm for decoding. Currently, we implemented a simple version of the DTSC model. We believe that our performance can be improved greatly using a more sophisticated mechanism for determining attaching points. Therefore the most important future work should be to design a better at</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>I. Dan Melamed. 2004. Algorithms for Syntax-Aware Statistical Machine Translation. In Proceedings of the Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A path-based transfer model for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1689" citStr="Lin, 2004" startWordPosition="246" endWordPosition="247">ets. A chart-style decoding algorithm with two basic operations– substituting and attaching–is designed for the DTSC model. We argue that the DTSC model proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation. We finally evaluate our current implementation of a simplified version of DTSC for statistical machine translation. 1 Introduction Over the last several years, various statistical syntaxbased models were proposed to extend traditional word/phrase based models in statistical machine translation (SMT) (Lin, 2004; Chiang, 2005; Ding et al., 2005; Quirk et al., 2005; Marcu et al., 2006; Liu et al., 2006). It is believed that these models can improve the quality of SMT significantly. Compared with phrase-based models, syntax-based models lead to better reordering and higher flexibility by introducing hierarchical structures and variables which make syntax-based models capable of hierarchical reordering and generalization. Due to these advantages, syntax-based approaches are becoming an active area of research in machine translation. In this paper, we propose a novel model based on dependency structures:</context>
<context position="9024" citStr="Lin, 2004" startWordPosition="1542" endWordPosition="1543">lignments. In the first step, the extractor annotates each node with specific attributes defined in section 3.1. These attributes are used in the second step which extracts all possible DTSCs rooted at each node recursively. 3.1 Node annotation For each source dependency node n, we define three attributes: word span, node span and crossed. Word span is defined to be the target word sequence aligned with the head word of n, while node span is defined to be the closure of the union of node spans of all subnodes of n and its word span. These two attributes are similar to those introduced by Lin (Lin, 2004). The third attribute crossed is an indicator that has binary values. If the node span of n overlaps the word span of its parent node or the node span of its siblings, the crossed indicator of n is 1 and n is therefore a crossed node, otherwise the crossed indicator is 0 and n is a non-crossed node. Only non-crossed nodes can generate DTSCs because the target word sequence aligned with the whole subtree rooted at it does not overlap any other sequences and therefore can be extracted independently. For the dependency tree and its alignments shown in Figure 2, only the node 财政 is a crossed node </context>
<context position="26231" citStr="Lin (2004)" startWordPosition="4578" endWordPosition="4579">modified to conform to NIST’s definition of the BLEU brevity penalty. The BLEU score of our current system with the DTSC model is lower than that of the phrasebased system. However, with phrases integrated, the performance is improved greatly, and the new BLEU score is higher than that of the phrase-based SMT. This difference is significant according to Zhang’s tester. This result can be improved further using a better parser (Quirk et al., 2006) or using a statistical attaching model. 8 Related Work The DTSC model is different from previous work based on dependency grammars by Eisner (2003), Lin (2004), Quirk et al. (2005), Ding et al. (2005) since they all deduce dependency structures on the target side. Among them, the most similar work is (Quirk et al., 2005). But there are still several major differences beyond the one mentioned above. Our 46 treelets allow variables at any non-crossed nodes and target strings allow gaps, which are not available in (Quirk et al., 2005). Our language model is calculated during decoding while Quirk’s language model is computed after decoding because of the complexity of their decoding. The DTSC model is also quite distinct from previous tree-string models</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-to-String Alignment Template for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1781" citStr="Liu et al., 2006" startWordPosition="262" endWordPosition="265">taching–is designed for the DTSC model. We argue that the DTSC model proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation. We finally evaluate our current implementation of a simplified version of DTSC for statistical machine translation. 1 Introduction Over the last several years, various statistical syntaxbased models were proposed to extend traditional word/phrase based models in statistical machine translation (SMT) (Lin, 2004; Chiang, 2005; Ding et al., 2005; Quirk et al., 2005; Marcu et al., 2006; Liu et al., 2006). It is believed that these models can improve the quality of SMT significantly. Compared with phrase-based models, syntax-based models lead to better reordering and higher flexibility by introducing hierarchical structures and variables which make syntax-based models capable of hierarchical reordering and generalization. Due to these advantages, syntax-based approaches are becoming an active area of research in machine translation. In this paper, we propose a novel model based on dependency structures: Dependency Treelet String Correspondence Model (DTSC). The DTSC model maps source dependenc</context>
<context position="4749" citStr="Liu et al., 2006" startWordPosition="738" endWordPosition="741">ndence A dependency treelet string correspondence 7r is a triple &lt; D, 5, A &gt; which describes a translation pair &lt; D, 5 &gt; and their alignment A, where D is the dependency treelet on the source side and 5 is the translation string on the target side. &lt; D, 5 &gt; must be consistent with the word alignment M of the corresponding sentence pair b(i,j)EM,iEDHjE5 A treelet is defined to be any connected subgraph, which is similar to the definition in (Quirk et al., 2005). Treelet is more representatively flexible than subtree which is widely used in models based on phrase structures (Marcu et al., 2006; Liu et al., 2006). The most important distinction between the treelet in (Quirk et al., 2005) and ours is that we allow variables at positions of subnodes. In our definition, the root node must be lexicalized but the subnodes can be replaced with a wild card. The target counterpart of a wildcard node in 5 is also replaced with a wild card. The wildcards introduced in this way generalize DTSC to match dependency structures with the same head word but with different modifiers or arguments. Another unique feature of our DTSC is that we allow target strings with gaps between words or wildcards. Since source treele</context>
<context position="17643" citStr="Liu et al. (2006)" startWordPosition="3094" endWordPosition="3097">red node no by it do Attach the translations from the stack of no to the target side at the attaching point end for end for end for Figure 4: Chart-style Decoding Algorithm for the DTSC Model. Melamed (2004) also used a similar way to integrate the language model. 5 Decoding Our decoding algorithm is similar to the bottom-up chart parsing. The distinction is that the input is a tree rather than a string and therefore the chart is indexed by nodes of the tree rather than spans of the string. Also, several other tree-based decoding algorithms introduced by Eisner (2003), Quirk et al. (2005) and Liu et al. (2006) can be classified as the chart-style parsing algorithm too. Our decoding algorithm is shown in Figure 4. Given an input dependency tree, firstly we generate the bottom-up order by postorder transversal. This order guarantees that any subnodes of node n have been translated before node n is done. For each node n in the bottom-up order, all matched DTSCs rooted at n are found, and a stack is also built for it to store the candidate translations. A DTSC 7r is said to match the input dependency subtree T rooted at n if and only if there is a treelet rooted at n that matches 2 the treelet of 7r on</context>
<context position="21846" citStr="Liu et al., 2006" startWordPosition="3829" endWordPosition="3832">ation, substituting and attaching, the decoding will generate a large number of partial translations3 for the current node. We only keep the top operation-limit partial translations each time according to their scores. 6 Integrating Phrases Although syntax-based models are good at dealing with hierarchical reordering, but at the local level, translating idioms and similar complicated expressions can be a problem. However, phrase-based models are good at dealing with these translations. Therefore, integrating phrases into the syntax-based models can improve the performance (Marcu et al., 2006; Liu et al., 2006). Since our DTSC model is based on dependency structures and lexicalized naturally, DTSCs are more similar to phrases than other translation units based on phrase structures. This means that phrases will be easier to be integrated into our model. The way to integrate phrases is quite straightforward: if there is a treelet rooted at the current node, 3There are wildcard nodes or uncovered nodes to be handled. ������� C De * + D (b) A ������� ������� �������B C Ee D + E 45 of which the word sequence is continuous and identical to the source of some phrase, then a phrasestyle DTSC will be generat</context>
<context position="26876" citStr="Liu et al. (2006)" startWordPosition="4686" endWordPosition="4689">et al. (2005) since they all deduce dependency structures on the target side. Among them, the most similar work is (Quirk et al., 2005). But there are still several major differences beyond the one mentioned above. Our 46 treelets allow variables at any non-crossed nodes and target strings allow gaps, which are not available in (Quirk et al., 2005). Our language model is calculated during decoding while Quirk’s language model is computed after decoding because of the complexity of their decoding. The DTSC model is also quite distinct from previous tree-string models by Marcu et al. (2006) and Liu et al. (2006). Firstly, their models are based on phrase structure grammars. Secondly, subtrees instead of treelets are extracted in their models. Thirdly, it seems to be more difficult to integrate phrases into their models. And finally, our model allow gaps on the target side, which is an advantage shared by (Melamed, 2004) and (Simard, 2005). 9 Conclusions and Future Work We presented a novel syntax-based model using dependency trees on the source side–dependency treelet string correspondence model–for statistical machine translation. We described an algorithm to learn DTSCs automatically from the train</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-String Alignment Template for Statistical Machine Translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="24903" citStr="Och, 2003" startWordPosition="4345" endWordPosition="4346">e training corpus consists of 31, 149 sentence pairs with 823K Chinese words and 927K English words. For the language model, we used SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothSystems BLEU-4 PB 20.88 f 0.87 DTSC 20.20 f 0.81 DTSC + phrases 21.46 f 0.83 Table 2: BLEU-4 scores for our system and a phrase-based system. ing on the 31, 149 English sentences. We selected 580 short sentences of length at most 50 characters from the 2002 NIST MT Evaluation test set as our development corpus and used it to tune As by maximizing the BLEU score (Och, 2003), and used the 2005 NIST MT Evaluation test set as our test corpus. From the training corpus, we learned 2, 729, 964 distinct DTSCs with the configuration { arylimit = 4, depth-limit = 4, len-limit = 15, gap-limit = 2, comb-limit = 20 }. Among them, 160,694 DTSCs are used for the test set. To run our decoder on the development and test set, we set stackthrshold = 0.0001, stack-limit = 100, node-limit = 100, operation-limit = 20. We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al., 2006) on the same corpus. The results are shown in table 2. For all BLEU score</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="24168" citStr="Och and Ney, 2000" startWordPosition="4215" endWordPosition="4218">one can parse the source sentences with a dependency parser or parse them with a phrasal structure parser and then convert the phrasal structures into dependency structures. In our experiments we used a Chinese parser implemented by Xiong et al. (2005) which generates phrasal structures. The parser was trained on articles 1-270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure). We then converted the phrasal structure trees into dependency trees using the way introduced by Xia (1999). To obtain the word alignments, we use the way of Koehn et al. (2005). After running GIZA++ (Och and Ney, 2000) in both directions, we apply the “grow-diag-final” refinement rule on the intersection alignments for each sentence pair. The training corpus consists of 31, 149 sentence pairs with 823K Chinese words and 927K English words. For the language model, we used SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothSystems BLEU-4 PB 20.88 f 0.87 DTSC 20.20 f 0.81 DTSC + phrases 21.46 f 0.83 Table 2: BLEU-4 scores for our system and a phrase-based system. ing on the 31, 149 English sentences. We selected 580 short sentences of length at most 50 charact</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1742" citStr="Quirk et al., 2005" startWordPosition="254" endWordPosition="257">wo basic operations– substituting and attaching–is designed for the DTSC model. We argue that the DTSC model proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation. We finally evaluate our current implementation of a simplified version of DTSC for statistical machine translation. 1 Introduction Over the last several years, various statistical syntaxbased models were proposed to extend traditional word/phrase based models in statistical machine translation (SMT) (Lin, 2004; Chiang, 2005; Ding et al., 2005; Quirk et al., 2005; Marcu et al., 2006; Liu et al., 2006). It is believed that these models can improve the quality of SMT significantly. Compared with phrase-based models, syntax-based models lead to better reordering and higher flexibility by introducing hierarchical structures and variables which make syntax-based models capable of hierarchical reordering and generalization. Due to these advantages, syntax-based approaches are becoming an active area of research in machine translation. In this paper, we propose a novel model based on dependency structures: Dependency Treelet String Correspondence Model (DTSC</context>
<context position="4596" citStr="Quirk et al., 2005" startWordPosition="713" endWordPosition="716">f a simplified version of DTSC for statistical machine translation. And finally, we discuss related work and conclude. 2 Dependency Treelet String Correspondence A dependency treelet string correspondence 7r is a triple &lt; D, 5, A &gt; which describes a translation pair &lt; D, 5 &gt; and their alignment A, where D is the dependency treelet on the source side and 5 is the translation string on the target side. &lt; D, 5 &gt; must be consistent with the word alignment M of the corresponding sentence pair b(i,j)EM,iEDHjE5 A treelet is defined to be any connected subgraph, which is similar to the definition in (Quirk et al., 2005). Treelet is more representatively flexible than subtree which is widely used in models based on phrase structures (Marcu et al., 2006; Liu et al., 2006). The most important distinction between the treelet in (Quirk et al., 2005) and ours is that we allow variables at positions of subnodes. In our definition, the root node must be lexicalized but the subnodes can be replaced with a wild card. The target counterpart of a wildcard node in 5 is also replaced with a wild card. The wildcards introduced in this way generalize DTSC to match dependency structures with the same head word but with diffe</context>
<context position="17621" citStr="Quirk et al. (2005)" startWordPosition="3089" endWordPosition="3092"> end for for each uncovered node no by it do Attach the translations from the stack of no to the target side at the attaching point end for end for end for Figure 4: Chart-style Decoding Algorithm for the DTSC Model. Melamed (2004) also used a similar way to integrate the language model. 5 Decoding Our decoding algorithm is similar to the bottom-up chart parsing. The distinction is that the input is a tree rather than a string and therefore the chart is indexed by nodes of the tree rather than spans of the string. Also, several other tree-based decoding algorithms introduced by Eisner (2003), Quirk et al. (2005) and Liu et al. (2006) can be classified as the chart-style parsing algorithm too. Our decoding algorithm is shown in Figure 4. Given an input dependency tree, firstly we generate the bottom-up order by postorder transversal. This order guarantees that any subnodes of node n have been translated before node n is done. For each node n in the bottom-up order, all matched DTSCs rooted at n are found, and a stack is also built for it to store the candidate translations. A DTSC 7r is said to match the input dependency subtree T rooted at n if and only if there is a treelet rooted at n that matches </context>
<context position="26252" citStr="Quirk et al. (2005)" startWordPosition="4580" endWordPosition="4583">conform to NIST’s definition of the BLEU brevity penalty. The BLEU score of our current system with the DTSC model is lower than that of the phrasebased system. However, with phrases integrated, the performance is improved greatly, and the new BLEU score is higher than that of the phrase-based SMT. This difference is significant according to Zhang’s tester. This result can be improved further using a better parser (Quirk et al., 2006) or using a statistical attaching model. 8 Related Work The DTSC model is different from previous work based on dependency grammars by Eisner (2003), Lin (2004), Quirk et al. (2005), Ding et al. (2005) since they all deduce dependency structures on the target side. Among them, the most similar work is (Quirk et al., 2005). But there are still several major differences beyond the one mentioned above. Our 46 treelets allow variables at any non-crossed nodes and target strings allow gaps, which are not available in (Quirk et al., 2005). Our language model is calculated during decoding while Quirk’s language model is computed after decoding because of the complexity of their decoding. The DTSC model is also quite distinct from previous tree-string models by Marcu et al. (200</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Simon Corston-Oliver</author>
</authors>
<title>The impact of parse quality on syntactically-informed statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Sydney, Australia.</location>
<marker>Quirk, Corston-Oliver, 2006</marker>
<rawString>Chris Quirk and Simon Corston-Oliver. 2006. The impact of parse quality on syntactically-informed statistical machine translation. In Proceedings of EMNLP, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Nicola Cancedda</author>
<author>Bruno Cavestro</author>
<author>Marc Dymetman</author>
</authors>
<title>Eric Gaussier, Cyril Goutte,</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<location>Kenji Yamada.</location>
<marker>Simard, Cancedda, Cavestro, Dymetman, 2005</marker>
<rawString>Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc Dymetman, Eric Gaussier, Cyril Goutte, Kenji Yamada. 2005. Translating with non-contiguous phrases. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings ofInternational Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="24471" citStr="Stolcke, 2002" startWordPosition="4265" endWordPosition="4266">ined on articles 1-270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure). We then converted the phrasal structure trees into dependency trees using the way introduced by Xia (1999). To obtain the word alignments, we use the way of Koehn et al. (2005). After running GIZA++ (Och and Ney, 2000) in both directions, we apply the “grow-diag-final” refinement rule on the intersection alignments for each sentence pair. The training corpus consists of 31, 149 sentence pairs with 823K Chinese words and 927K English words. For the language model, we used SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothSystems BLEU-4 PB 20.88 f 0.87 DTSC 20.20 f 0.81 DTSC + phrases 21.46 f 0.83 Table 2: BLEU-4 scores for our system and a phrase-based system. ing on the 31, 149 English sentences. We selected 580 short sentences of length at most 50 characters from the 2002 NIST MT Evaluation test set as our development corpus and used it to tune As by maximizing the BLEU score (Och, 2003), and used the 2005 NIST MT Evaluation test set as our test corpus. From the training corpus, we learned 2, 729, 964 distinct DTSCs with the configuration { arylimit = </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings ofInternational Conference on Spoken Language Processing, volume 2, pages 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Automatic Grammar Generation from Two Different Perspectives.</title>
<date>1999</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="24056" citStr="Xia (1999)" startWordPosition="4196" endWordPosition="4197">ly and is a major goal for our future research. To obtain the dependency structures of the source side, one can parse the source sentences with a dependency parser or parse them with a phrasal structure parser and then convert the phrasal structures into dependency structures. In our experiments we used a Chinese parser implemented by Xiong et al. (2005) which generates phrasal structures. The parser was trained on articles 1-270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure). We then converted the phrasal structure trees into dependency trees using the way introduced by Xia (1999). To obtain the word alignments, we use the way of Koehn et al. (2005). After running GIZA++ (Och and Ney, 2000) in both directions, we apply the “grow-diag-final” refinement rule on the intersection alignments for each sentence pair. The training corpus consists of 31, 149 sentence pairs with 823K Chinese words and 927K English words. For the language model, we used SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothSystems BLEU-4 PB 20.88 f 0.87 DTSC 20.20 f 0.81 DTSC + phrases 21.46 f 0.83 Table 2: BLEU-4 scores for our system and a phrase-</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fei Xia. 1999. Automatic Grammar Generation from Two Different Perspectives. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="25430" citStr="Xiong et al., 2006" startWordPosition="4440" endWordPosition="4443">et as our development corpus and used it to tune As by maximizing the BLEU score (Och, 2003), and used the 2005 NIST MT Evaluation test set as our test corpus. From the training corpus, we learned 2, 729, 964 distinct DTSCs with the configuration { arylimit = 4, depth-limit = 4, len-limit = 15, gap-limit = 2, comb-limit = 20 }. Among them, 160,694 DTSCs are used for the test set. To run our decoder on the development and test set, we set stackthrshold = 0.0001, stack-limit = 100, node-limit = 100, operation-limit = 20. We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al., 2006) on the same corpus. The results are shown in table 2. For all BLEU scores, we also show the 95% confidence intervals computed using Zhang’s significant tester (Zhang et al., 2004) which was modified to conform to NIST’s definition of the BLEU brevity penalty. The BLEU score of our current system with the DTSC model is lower than that of the phrasebased system. However, with phrases integrated, the performance is improved greatly, and the new BLEU score is higher than that of the phrase-based SMT. This difference is significant according to Zhang’s tester. This result can be improved further u</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In Proceedings of COLING-ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
<author>Yueliang Qian</author>
</authors>
<title>Parsing the Penn Chinese Treebank with Semantic Knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings ofIJCNLP, Jeju Island,</booktitle>
<contexts>
<context position="23802" citStr="Xiong et al. (2005)" startWordPosition="4154" endWordPosition="4157">ubnodes which will not be covered even by generalized matched DTSCs. This means the attaching operation is very common during decoding. Therefore better attaching model which calculates the best point for attaching , we believe, will improve the performance greatly and is a major goal for our future research. To obtain the dependency structures of the source side, one can parse the source sentences with a dependency parser or parse them with a phrasal structure parser and then convert the phrasal structures into dependency structures. In our experiments we used a Chinese parser implemented by Xiong et al. (2005) which generates phrasal structures. The parser was trained on articles 1-270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure). We then converted the phrasal structure trees into dependency trees using the way introduced by Xia (1999). To obtain the word alignments, we use the way of Koehn et al. (2005). After running GIZA++ (Och and Ney, 2000) in both directions, we apply the “grow-diag-final” refinement rule on the intersection alignments for each sentence pair. The training corpus consists of 31, 149 sentence pairs with 823K Chinese words and 927K English words. For the </context>
</contexts>
<marker>Xiong, Li, Liu, Lin, Qian, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang Qian. 2005. Parsing the Penn Chinese Treebank with Semantic Knowledge. In Proceedings ofIJCNLP, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>pages</pages>
<contexts>
<context position="25610" citStr="Zhang et al., 2004" startWordPosition="4472" endWordPosition="4475">rpus, we learned 2, 729, 964 distinct DTSCs with the configuration { arylimit = 4, depth-limit = 4, len-limit = 15, gap-limit = 2, comb-limit = 20 }. Among them, 160,694 DTSCs are used for the test set. To run our decoder on the development and test set, we set stackthrshold = 0.0001, stack-limit = 100, node-limit = 100, operation-limit = 20. We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al., 2006) on the same corpus. The results are shown in table 2. For all BLEU scores, we also show the 95% confidence intervals computed using Zhang’s significant tester (Zhang et al., 2004) which was modified to conform to NIST’s definition of the BLEU brevity penalty. The BLEU score of our current system with the DTSC model is lower than that of the phrasebased system. However, with phrases integrated, the performance is improved greatly, and the new BLEU score is higher than that of the phrase-based SMT. This difference is significant according to Zhang’s tester. This result can be improved further using a better parser (Quirk et al., 2006) or using a statistical attaching model. 8 Related Work The DTSC model is different from previous work based on dependency grammars by Eisn</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proceedings of LREC, pages 2051– 2054.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>