<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000105">
<title confidence="0.993447">
Association-based Natural Language Processing
with Neural Networks
</title>
<author confidence="0.777984">
KIMURA Kazuhiro SUZUOK A Takashi
</author>
<affiliation confidence="0.661225666666667">
AMANO Sin-ya
Information Systems Laboratory
Research and Development Center
</affiliation>
<address confidence="0.6656705">
TOSHIBA Corp.
1 Komukai-Tosiba-ty6, Saiwai-ku, Kawasaki 210 Japan
</address>
<email confidence="0.765597">
kimOisl.rdc.toshiba.co.jp
</email>
<sectionHeader confidence="0.992018" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999820777777778">
This paper describes a natural language pro-
cessing system reinforced by the use of associ-
ation of words and concepts, implemented as a
neural network. Combining an associative net-
work with a conventional system contributes
to semantic disambiguation in the process of
interpretation. The model is employed within
a kana-kanji conversion system and the advan-
tages over conventional ones are shown.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967052631579">
Currently, most practical applications in nat-
ural language processing (NLP) have been
realized via symbolic manipulation engines,
such as grammar parsers. However, the cur-
rent trend (and focus of research) is shift-
ing to consider aspects of semantics and dis-
course as part of NLP. This can be seen in
the emergence of new theories of language,
such as Situation Theory [Barwise 83] and
Discourse Representation Theory [Kamp 84].
While these theories provide an excellent the-
oretical framework for natural language un-
derstanding, the practical treatment of con-
text dependency within the language can also
be improved by enhancing underlying compo-
nent technologies, such as knowledge based
systems. In particular, alternate approaches
to symbolic manipulation provided by connec-
tionist models [Rumelhart 86] have emerged.
Connectionist approaches enable the extrac-
tion of processing knowledge from examples,
instead of building knowledge bases manually.
The model described here represents the
unification of the connectionist approach and
conventional symbolic manipulation; its most
valuable feature is the use of word as-
sociations using neural network technology.
Word and concept associations appear to
be central in human cognition [Minsky 881.
Therefore, simulating word associations con-
tributes to semantic disambiguation in the
computational process of interpreting sen-
tences by putting a strong preference to ex-
pected words(meanings).
The paper describes NLP reinforced by as-
sociation of concepts and words via a con-
nectionist network. The model is employed
within a NLP application system for kana-
</bodyText>
<page confidence="0.996458">
224
</page>
<bodyText confidence="0.997819">
kanji conversion&apos;. Finally, an evaluation of
the system and advantages over conventional
systems are presented.
</bodyText>
<sectionHeader confidence="0.9931325" genericHeader="introduction">
2 A brief overview of
kana-kanji conversion
</sectionHeader>
<bodyText confidence="0.990396941176471">
Japanese has a several interesting feature in
its variety of letters. Especially the ex-
istence of several thousand of kanji (based
on Chinese characters; it, t11,..) made typing
task hard before the invention of kana-kanji
conversion[Amano 79] . Now it has become
a standard method in inputting Japanese to
computers. It is also used in word processors
and is familiar to those who are not computer
experts. It comes from the simpleness of op-
erations. By only typing sentences by pho-
netic expressions of Japanese (kana), the kana-
kanji converter automatically converts kana
into meaningful expressions(kanji). The sim-
plified mechanism of kana-kanji conversion can
be described as two stages of processing: mor-
phological analysis and homonym selection.
</bodyText>
<listItem confidence="0.914137">
• Morphological Analysis
</listItem>
<bodyText confidence="0.9937315">
Kana-inputted (fragment of) sentences
are morphologically analized through dic-
tionary look up, both lexicons and gram-
mars. There are many ambiguities in
word division due to the agglutinative na-
ture of Japanese (Japanese has no spaces
in text). Each partitioning of the kana
is then further open to being a possible
interpretation of several alternate kanji.
The spoken word douki, for example, can
mean motivation, pulsation, synchroniza-
tion, or copperware. All of them are spelt
identically in kana( , but have dif-
ferent kanji characters(aa, ES, FM, lig
</bodyText>
<footnote confidence="0.557906">
1Many commercial products use kana-kanji conver-
sion technology in Japan, including the TOSHIBA
</footnote>
<subsectionHeader confidence="0.50063">
Tosword-series of Japanese word processors.
</subsectionHeader>
<bodyText confidence="0.8200942">
.,respectively). Some kana words have
10 or more possible meanings. Therefore
the stage of Homonym Selection is indis-
pensable to kana-kanji conversion for the
reduction of homonyms.
</bodyText>
<listItem confidence="0.993631">
• Homonym Selection
</listItem>
<bodyText confidence="0.997127714285714">
Preferable semantic homonyms are se-
lected according to the co-occurrence
restrictions and selectional restrictions.
The frequency of use of each word is also
taken into account. Usually, the selection
is also reinforced by a simple context hold-
ing mechanism; when homonyms appear
in previous discourse and one of them is
chosen by a user, the word is automat-
ically memorized in the system as in a
cache technology. Then, when the same
homonyms appear the memorized word is
selected as the most preferred candidate
and is shown to the user.
</bodyText>
<sectionHeader confidence="0.942546" genericHeader="method">
3 Association-based kana-
kanji conversion
</sectionHeader>
<bodyText confidence="0.999989333333333">
The above mechanisms are simple and effec-
tive in regarding kana-kanji converter as a typ-
ing aid. However, the abundance of homonyms
in Japanese contributes to many of the am-
biguities and a user is forced to choose the
desired kanji from many candidates. To re-
duce homonym ambiguities a variety of tech-
niques are available; however, these tend to
be limited from a semantic disambiguation
perspective. In using word co-occurrence re-
strictions, it is necessary to collect a large
amount of co-occurrence phenomena, a prac-
tically impossible task. In the case of the
use of selectional restrictions, an appropri-
ate thesaurus is necessary but it is known
that defining the conceptual hierarchy is dif-
ficult work [Lenat 89][EDR 90]. Techniques
for storing previous kanji selections (cache)
</bodyText>
<page confidence="0.991954">
225
</page>
<figure confidence="0.707351">
Textual Input
</figure>
<figureCaption confidence="0.996947">
Figure 1: Kana-Kanji Conversion with a Neural Network
</figureCaption>
<figure confidence="0.998889545454546">
e doeki
ei 8
%N.
••■,.
shortness of breath
1:17 )sr&amp; .bril Itto.-czolgett1cit7411:111(1)i2lftb.
At dr rising edge of Us dock. the signal is hacked by de processor.
zoii‘zei6te . . ...
Like this, dra doubt . • . ..
•
N.
</figure>
<bodyText confidence="0.99993953125">
are too simple to disambiguate between possi-
ble previous selections for the same homonym
with respect to the context or between context
switches.
To avoid these problems without increasing
computational costs, we propose the use of the
associative functionality of neural networks.
The use of association is a natural extension to
the conventional context holding mechanism.
The idea is summarized as follows. There are
two stages of processing: network generation
and kana-kanji conversion.
A network representing the strength of word
association is automatically generated from
real documents. Real documents can be con-
sidered as training data because they are made
of correctly converted kanji. Each node in
the network uniquely correspond to a word
entry in the dictionary of kana-kanji conver-
sion. Each node has an activation level.
The link between nodes is a weighted link
and represents the strength of association be-
tween words. The network is a Hopfield-type
network[Hopfield 84]; links are bidirectional
and a network is one layered.
When the user chooses a word from
homonym candidates, a certain value is in-
putted to the node corresponding to the cho-
sen word and the node will be activated. The
activation level of nodes connected to the ac-
tivated node will be then activated. In this
manner, the activation spreads over the net-
</bodyText>
<page confidence="0.990198">
226
</page>
<bodyText confidence="0.998880454545455">
work through the links and the active part of
the network can be considered as the associa-
tive words in that context. In kana-kanji con-
version, the converter decides the preference
of word order for homonyms in the given con-
text by comparing the node activation level of
each node of homonyms. An example of the
method is shown in Figure 1.
Assume the network is already built from
certain documents. A user is inputting a text
whose topic is related to Computer hardware.
In the example, words like clock Er 7
and signal (fg-g-) already appear in the previ-
ous context, so their activation levels are rela-
tively high. When the word DOUKI e5-
is inputted in kana and the conversion starts,
the activation level of synchronization (PWA)
is higher than that of other candidates due to
its relationship to clock or signal. The input
douki is then correctly converted into synchro-
nization (POW).
The advantages of our method are:
</bodyText>
<listItem confidence="0.993337">
• The method enables kanji to be given
based on a preference related to the cur-
rent context. Alternative kanji selections
are not discarded but are just given a
lower context weighing. Should the con-
text switch, the other possible selections
will obtain a stronger context preference;
this strategy allows the system to capably
handle context change.
• Word preferences of a user are reflected in
the network.
• The correctness of the conversion is im-
proved without high-cost computation
such as semantic/discourse analyses.
</listItem>
<sectionHeader confidence="0.996712" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.99990212">
The system was built on Toshiba AS-4000
workstation (Sun4 compatible machine) using
C. The system configuration is shown in Fig-
ure 2.
The left-hand side of the dashed line repre-
sents an off-line network building process. The
right-hand side represents a kana-kanji con-
version process reinforced with a neural net-
work handler. The network is used by the
neural network handler and word associations
are done in parallel with kana-kanji conver-
sion. The kana-kanji converter receives kana-
sequences from a user. It searches the dictio-
nary for lexical and grammatical information
and finally creates a list of possible homonym
candidates. Then the neural network handler
is requested for activation levels of homonyms.
After the selection of preferred homonyms, it
shows the candidates in kanji to a user. When
the user chooses the desired one, the chosen
word information is sent to the neural network
handler through a homonym choice interface
and the corresponding node is activated.
The roles and the functions of main compo-
nents are described as follows.
</bodyText>
<listItem confidence="0.936865">
• Neural Network Generator
</listItem>
<bodyText confidence="0.999720111111111">
Several real documents are analyzed and
the network nodes and the weights of links
are automatically decided. The docu-
ments consist of the mixture of kana and
kanji; homonyms for the kanji within the
given context are also provided. The doc-
uments, therefore, can be seen as training
data for the neural network. The analysis
proceeds through the following steps.
</bodyText>
<listItem confidence="0.998531">
1. Analyze the documents morpholog-
ically and convert into a sequence
of words. Note that particles and
demonstratives are ignored because
they have no characteristics in word
association.
2. Count up the frequency of the all
combination of co-appeared word-
pair in a paragraph and memorize
</listItem>
<page confidence="0.978013">
227
</page>
<figure confidence="0.999872464285714">
associative
network
Neural Network
Generator
homonym
candidates
(in kanji)
Homonym Choice
intrekue
activation levels
of neurons
Lexicons S
Grammars
hire gene
sequences
/Cana Input
interface
•
Neural Network
Handler
Kano-Kenji
Cotwerter
•
1.
Dictionary
activating chosen neurons
• Dacumente
Alec
</figure>
<figureCaption confidence="0.999897">
Figure 2: System Configuration
</figureCaption>
<bodyText confidence="0.999596333333333">
them as the strength of connection.
A paragraph is recognized only by a
format information of documents.
</bodyText>
<listItem confidence="0.98236">
3. Sum up the strength of connection
for each word-pair.
4. Regularize the training data; this
involves removing low occurrences
(noise) and partitioning the fre-
quency range in order to obtain
a monotonically decreasing (in fre-
quency) training set.
</listItem>
<bodyText confidence="0.99587965">
Although the network data have
only positive links and not all nodes
are connected, non-connected nodes
are assumed to be connected by neg-
ative weights so that the Hopfield
conditions [Hopfield 84] are satisfied.
As described above, the technique used
here is a morphological and statistical
analysis. Actually this module is a pat-
tern learning of co-appearing words in a
paragraph.
The idea behind of this approach is that
words that appear together in a para-
graph have some sort of associative con-
nection. By accumulating them, pairs
without such relationships will be statis-
tically rejected.
From a practical point of view, automated
network generation is inevitable. Since
human word association differ by individ-
</bodyText>
<page confidence="0.995483">
228
</page>
<bodyText confidence="0.999467125">
ual, creation of a general purpose asso-
ciative network is not realistic. Because
the training data for the network is sup-
posed to be supplied by users&apos; documents
in our system, automatic network genera-
tion mechanism is necessary even if the
generated network is somewhat inaccu-
rate.
</bodyText>
<listItem confidence="0.964566">
• Neural Network Handler
</listItem>
<bodyText confidence="0.999174833333333">
The role of the module is to recall the
total patterns of co-appearing words in a
paragraph from the partial patterns of the
current paragraph given by a user.
The output value Oi for each node j is
calculated by following equations.
</bodyText>
<equation confidence="0.999562">
Goi = 1(111)
n. = (1 — b)ni + .5(EwijOi + .1i)
</equation>
<bodyText confidence="0.9755582">
where
f : a sigmoidal function
: a real number representing the inertia
of the network(0 &lt; S &lt; 1).
nj : input value to node j.
: external input value to node j.
: weight of a link from node i to node
j; wji = w3, = O.
The external input value Ij takes a cer-
tain positive value when the word corre-
sponding to node j is chosen by a user.
Otherwise zero.
Although the module is software imple-
mented, it is fast enough to follow the
typing speed of a user. 2
</bodyText>
<listItem confidence="0.843858">
• Kana-Kanji Converter
</listItem>
<bodyText confidence="0.9998557">
The basic algorithm is almost same as
the conventional one. The difference is
that homonym candidates are sorted by
the activation levels of the correspond-
ing nodes in the network, except when lo-
cal constraints such as word co-occurrence
restrictions are applicable to the candi-
dates. The associative information also
affects the preference decision of gram-
matical ambiguities.
</bodyText>
<sectionHeader confidence="0.996215" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999957533333333">
To evaluate the method, we tested the im-
plemented sytem by doing kana-kanji conver-
sion for real documents. The training data
and tested data were taken from four types
of documents: business letters, personal let-
ters, news articles, and technical articles. The
amount of training data and tested data was
over 100,000 phrases and 10,000 phrases re-
spectively, for each type of document. The
measure for accuracy of conversion was a re-
duction ratio(RR) of the homonym choice
operations of a user. For comparison, we
also evaluated the reduction ratio(RR&apos;) of the
kana-kanji conversion with a conventional con-
text holding mechanism.
</bodyText>
<equation confidence="0.503424">
RR= (A — B)IA
RR&apos; = (A — C)I A
</equation>
<bodyText confidence="0.9205637">
where
A: number of choice operations required when
an untrained kana-kanji converter was used.
B : number of choice operations required when
a NN-trained kana-kanji converter was used.
C : number of choice operations required
when a. kana-kanji converter with a conven-
tional context holding mechanism was used.
2A certain optimization technique is used respect- The result is shown in Table 1. The ad-
ing for the sparseness of the network, vantages of our method is clear for each type
</bodyText>
<page confidence="0.998976">
229
</page>
<tableCaption confidence="0.999795">
Table 1: Result of the Evaluation
</tableCaption>
<table confidence="0.9938738">
document-type RR(%) RR&apos; (%)
business letters 41.8 32.6
personal letters 20.7 12.7
news articles 23.4 12.2
technical articles 45.6 40.7
</table>
<bodyText confidence="0.9936785">
of documents. Especially, it is notable that
the advantages in business letter field is promi-
nent, because more than 80% of word proces-
sor users write business letters.
</bodyText>
<sectionHeader confidence="0.999856" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9984384">
Although the result of conversion test is sat-
isfactory, word associations by neural network
are not human-like ones yet. Following is a list
of improvements that many further enhance
the system:
</bodyText>
<listItem confidence="0.799487">
• Improvements for generating a network
</listItem>
<bodyText confidence="0.986498094339623">
The quality of the network depends on
how to reduce noisy word occurrence in
the network from the point of view of as-
sociation. The existence of noisy words
is inevitable in automatic generation but
plays a role to make unwanted associa-
tions. One approach to reducing noisy
words is to identify those words which
are context independent and remove them
from the network generation stage. The
identification can be based on word cat-
egories and meanings. In most cases,
words representing very abstract concepts
are noisy because they force unwanted ac-
tivations in unrelated contexts. There-
fore they should be detected through ex-
periments. Another problem arises be-
cause of the ambiguity of morphological
analysis. Word extraction from real doc-
uments is not always correct because of
the agglutinative nature of the Japanese
language. Other possibility for network
improvement is to consider a syntactic
relationship or co-occurrence relationship
while deciding link weights. In addition,
there are keywords in a document in gen-
eral which play a central role in associa-
tion. They will be reflected in a network
more in consideration of technical terms .
• Preference decision in kana-kanji conver-
sion
The reinforcement of associative informa-
tion complicates the decision of homonym
preference in kana-kanji conversion. We
already have several means of seman-
tic disambiguation of homonyms: co-
occurrence restrictions and selectional re-
strictions. As building a complete the-
saurus is very difficult, our thesaurus
is still not enough to select the cor-
rect meaning(kanji-conversion) of kana-
written word. So selectional restrictions
should be weak constraints in homonym
selection. In the same vein, associative
information should be considered a weak
constraint because associations by neural
networks are not always reliable. Pos-
sible conflict between selectional restric-
tions and associative information, added
to the grammatical ambiguities remaining
in the stage of homonym selection, make
kanji selection very complex. The prob-
lem of multiply and weakly constrained
</bodyText>
<page confidence="0.972606">
230
</page>
<bodyText confidence="0.9998655">
homonyms is one to which we have not
yet found the best solution.
</bodyText>
<sectionHeader confidence="0.999067" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999995454545455">
This paper described an association based nat-
ural language processing and its application
to kana-kanji conversion. We showed advan-
tages of the method over the conventional one
through the experiments. After the improve-
ments discussed above, we are planning to de-
velop a neuro-word processor available in com-
mercial use. We are also planning the applica-
tion of the method to other fields including
machine translations and discourse analyses
for natural language interface to computers.
</bodyText>
<sectionHeader confidence="0.999669" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995797611111111">
[Amano 79] Kawada, T. and Amano, S.,
&amp;quot;Japanese Word Processor,&amp;quot;
Proc. IJCAI-79, pp. 466-468,
1979.
[Barwise 83] Barwise, J. and Perry, J., &amp;quot;Sit-
uations and Attitudes,&amp;quot; MIT
Press, 1983.
[EDR 90] Japan Electronic Dictionary
Research Institute,
&amp;quot;Concept Dictionary,&amp;quot; Tech.
Rep. No.027, 1990.
[Hopfield 84] Hopfield, J., &amp;quot;Neurons with
Graded Response Have Col-
lective Computational Proper-
ties Like Those of Two-State
Neurons,&amp;quot; Proc. Natl. Acad.
Sci. USA 81, pp. 3088-3092,
1984.
</reference>
<figure confidence="0.747092695652174">
al(eds.) &amp;quot;Truth, Interpreta-
tion and Information&amp;quot;, 1984.
[Lenat 89] Lenat, D. and Guha, R.,
&amp;quot;Building Large Knowledge-
Based Systems: Represen-
tation and Inference in
the Cyc Project,&amp;quot; Addison-
Wesley, 1989.
[Minsky 88] Minsky, M., &amp;quot;The Society Of
Mind,&amp;quot;, Simon &amp; Schuster
Inc., 1988.
[Rumelhart 86] Rumelhart, D., McClelland,
J., and the PDP Research
Group, &amp;quot;Parallel Distributed
Processing: Explorations in
the Microstructure of Cogni-
tion,&amp;quot; MIT Press, 1986.
[Waltz 85] Waltz, D. and Pollack, J.,
&amp;quot;Massively, Parallel Parsing:
A Strongly Interactive Model
of Natural Language Interpre-
tation,&amp;quot; Cognitive Science, pp.
51-74, 1985.
</figure>
<reference confidence="0.328659666666667">
[Kamp 84] Kamp, H., &amp;quot;A Theory of
Truth and Semantic Repre-
sentation,&amp;quot; in Groenendijk et
</reference>
<page confidence="0.995216">
231
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.484295">
<title confidence="0.999307">Association-based Natural Language Processing with Neural Networks</title>
<author confidence="0.8981845">KIMURA Kazuhiro SUZUOK A Takashi AMANO Sin-ya</author>
<affiliation confidence="0.941953333333333">Information Systems Laboratory Research and Development Center TOSHIBA Corp.</affiliation>
<address confidence="0.73736">1 Komukai-Tosiba-ty6, Saiwai-ku, Kawasaki 210 Japan</address>
<email confidence="0.972893">kimOisl.rdc.toshiba.co.jp</email>
<abstract confidence="0.9975656">This paper describes a natural language processing system reinforced by the use of association of words and concepts, implemented as a neural network. Combining an associative network with a conventional system contributes to semantic disambiguation in the process of interpretation. The model is employed within conversion system the advantages over conventional ones are shown.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Kawada</author>
<author>S Amano</author>
</authors>
<title>Japanese Word Processor,&amp;quot;</title>
<date>1979</date>
<booktitle>Proc. IJCAI-79,</booktitle>
<pages>466--468</pages>
<marker>[Amano 79]</marker>
<rawString>Kawada, T. and Amano, S., &amp;quot;Japanese Word Processor,&amp;quot; Proc. IJCAI-79, pp. 466-468, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Barwise</author>
<author>J Perry</author>
</authors>
<title>Situations and Attitudes,&amp;quot;</title>
<date>1983</date>
<publisher>MIT Press,</publisher>
<marker>[Barwise 83]</marker>
<rawString>Barwise, J. and Perry, J., &amp;quot;Situations and Attitudes,&amp;quot; MIT Press, 1983.</rawString>
</citation>
<citation valid="false">
<date>1990</date>
<tech>Tech. Rep. No.027,</tech>
<institution>Japan Electronic Dictionary Research Institute, &amp;quot;Concept Dictionary,&amp;quot;</institution>
<marker>[EDR 90]</marker>
<rawString>Japan Electronic Dictionary Research Institute, &amp;quot;Concept Dictionary,&amp;quot; Tech. Rep. No.027, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hopfield</author>
</authors>
<title>Neurons with Graded Response Have Collective Computational Properties Like Those of Two-State Neurons,&amp;quot;</title>
<date>1984</date>
<journal>Proc. Natl. Acad. Sci. USA</journal>
<volume>81</volume>
<pages>3088--3092</pages>
<marker>[Hopfield 84]</marker>
<rawString>Hopfield, J., &amp;quot;Neurons with Graded Response Have Collective Computational Properties Like Those of Two-State Neurons,&amp;quot; Proc. Natl. Acad. Sci. USA 81, pp. 3088-3092, 1984.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Kamp</author>
</authors>
<title>A Theory of Truth and Semantic Representation,&amp;quot;</title>
<note>in Groenendijk et</note>
<marker>[Kamp 84]</marker>
<rawString>Kamp, H., &amp;quot;A Theory of Truth and Semantic Representation,&amp;quot; in Groenendijk et</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>