<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<note confidence="0.5914546">
Prefix Probabilities from Stochastic Tree Adjoining Grammars*
Mark-Jan Nederhof
DFKI
Stuhlsatzenhausweg 3,
D-66123 Saarbracken,
</note>
<address confidence="0.672413">
Germany
</address>
<email confidence="0.988435">
nederhof@dfki.de
</email>
<author confidence="0.926163">
Anoop Sarkar
</author>
<affiliation confidence="0.9498205">
Dept. of Computer and Info. Sc.
Univ of Pennsylvania
</affiliation>
<address confidence="0.9361165">
200 South 33rd Street,
Philadelphia, PA 19104 USA
</address>
<email confidence="0.960979">
anoopOlinc.cis.upenn.edu
</email>
<note confidence="0.765036666666667">
Giorgio Satta
Dip. di Elettr. e Inf.
Univ. di Padova
via Gradenigo 6/A,
35131 Padova, Italy
sattadei .unipd. it
</note>
<sectionHeader confidence="0.980902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998894">
Language models for speech recognition typ-
ically use a probability model of the form
Pr (an I al , az , . . . , an_i) . Stochastic grammars,
on the other hand, are typically used to as-
sign structure to utterances. A language model
of the above form is constructed from such
grammars by computing the prefix probabil-
ity EwEE. Pr(ai • • anw), where w represents
all possible terminations of the prefix al • • an.
The main result in this paper is an algorithm
to compute such prefix probabilities given a
stochastic Tree Adjoining Grammar (TAG).
The algorithm achieves the required computa-
tion in 0(n6) time. The probability of sub-
derivations that do not derive any words in the
prefix, but contribute structurally to its deriva-
tion, are precomputed to achieve termination.
This algorithm enables existing corpus-based es-
timation techniques for stochastic TAGs to be
used for language modelling.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994034714285714">
Given some word sequence al • • • an_i, speech
recognition language models are used to hy-
pothesize the next word an, which could be
any word from the vocabulary E. This
is typically done using a probability model
Pr(anlai, , an_i). Based on the assumption
that modelling the hidden structure of nat-
</bodyText>
<listItem confidence="0.973392153846154">
• Part of this research was done while the first and the
third authors were visiting the Institute for Research
in Cognitive Science, University of Pennsylvania. The
first author was supported by the German Federal Min-
istry of Education, Science, Research and Technology
(BMBF) in the framework of the VERBMOBIL Project un-
der Grant 01 IV 701 VO, and by the Priority Programme
Language and Speech Technology, which is sponsored by
NWO (Dutch Organization for Scientific Research). The
second and third authors were partially supported by
NSF grant SBR8920230 and ARO grant DAAH0404-94-
G-0426. The authors wish to thank Aravind Joshi for
his support in this research.
</listItem>
<bodyText confidence="0.999905071428572">
ural language would improve performance of
such language models, some researchers tried to
use stochastic context-free grammars (CFGs) to
produce language models (Wright and Wrigley,
1989; Jelinek and Lafferty, 1991; Stolcke, 1995).
The probability model used for a stochas-
tic grammar was E„EE.. Pr(ai • • anw). How-
ever, language models that are based on tri-
gram probability models out-perform stochastic
CFGs. The common wisdom about this failure
of CFGs is that trigram models are lexicalized
models while CFGs are not.
Tree Adjoining Grammars (TAGs) are impor-
tant in this respect since they are easily lexical-
ized while capturing the constituent structure
of language. More importantly, TAGs allow
greater linguistic expressiveness. The trees as-
sociated with words can be used to encode argu-
ment and adjunct relations in various syntactic
environments. This paper assumes some famil-
iarity with the TAG formalism. (Joshi, 1988)
and (Joshi and Schabes, 1992) are good intro-
ductions to the formalism and its linguistic rele-
vance. TAGs have been shown to have relations
with both phrase-structure grammars and de-
pendency grammars (Rambow and Joshi, 1995),
which is relevant because recent work on struc-
tured language models (Chelba et al., 1997) have
used dependency grammars to exploit their lex-
icalization. We use stochastic TAGs as such a
structured language model in contrast with ear-
lier work where TAGs have been exploited in
a class-based n-gram language model (Srinivas,
1996).
This paper derives an algorithm to compute
prefix probabilities E„,,E. Pr(ai • • • anw). The
algorithm assumes as input a stochastic TAG G
and a string which is a prefix of some string in
L(G), the language generated by G. This algo-
rithm enables existing corpus-based estimation
techniques (Schabes, 1992) in stochastic TAGs
to be used for language modelling.
</bodyText>
<page confidence="0.999314">
953
</page>
<sectionHeader confidence="0.978784" genericHeader="introduction">
2 Notation
</sectionHeader>
<bodyText confidence="0.997688785714286">
A stochastic Tree Adjoining Grammar (STAG)
is represented by a tuple (NT, E, I, A, 0) where
NT is a set of nonterminal symbols, E is a set
of terminal symbols, I is a set of initial trees
and A is a set of auxiliary trees. Trees in IU A
are also called elementary trees.
We refer to the root of an elementary tree t as
R. Each auxiliary tree has exactly one distin-
guished leaf, which is called the foot. We refer
to the foot of an auxiliary tree t as F. We let
V denote the set of all nodes in the elementary
trees.
For each leaf N in an elementary tree, except
when it is a foot, we define label(N) to be the
label of the node, which is either a terminal from
E or the empty string E. For each other node
N, label(N) is an element from NT.
At a node N in a tree such that label(N) E
NT an operation called adjunction can be ap-
plied, which excises the tree at N and inserts
an auxiliary tree.
Function 0 assigns a probability to each ad-
junction. The probability of adjunction oft E A
at node N is denoted by 0(t, N). The probabil-
ity that at N no adjunction is applied is denoted
by 0(nil, N). We assume that each STAG G
that we consider is proper. That is, for each
N such that label(N) E NT,
</bodyText>
<equation confidence="0.99269">
E 0(t,N) = 1.
tEAU{nil}
</equation>
<bodyText confidence="0.965609428571429">
For each non-leaf node N we construct the
string cdn(N) = N1 • - • Nni from the (ordered)
list of children nodes N1,... , N7,2 by defining,
for each d such that 1 &lt;d &lt; m, Nd = label(Nd)
in case label(Nd) E E U {e}, and Nd = Nd oth-
erwise. In other words, children nodes are re-
placed by their labels unless the labels are non-
terminal symbols.
To simplify the exposition, we assume an ad-
ditional node for each auxiliary tree t, which
we denote by I. This is the unique child of the
actual foot node F. That is, we change the def-
inition of cdn such that cdn(Ft) = _I_ for each
auxiliary tree t. We set
VI = {N E V I label(N) E NT} U E U {1}.
We use symbols a, b, c,... to range over E,
symbols v, w, x,... to range over E*, sym-
bols N, M,... to range over V-I-, and symbols
a, 13, 7, . . . to range over (V-1-)*. We use t, ti, ...
to denote trees in I U A or subtrees thereof.
We define the predicate dft on elements from
V-1- as dft(N) if and only if (i) N E V and N
dominates I, or (ii) N = I. We extend dft
to strings of the form N1 ... Nrn E (V±)* by
defining dft(Ni ... N,n) if and only if there is a
d (1 &lt; d &lt; m) such that dft(Nd).
For some logical expression p, we define
8(p) = 1 if p is true, 6(p) = 0 otherwise.
</bodyText>
<sectionHeader confidence="0.995366" genericHeader="method">
3 Overview
</sectionHeader>
<bodyText confidence="0.999978857142857">
The approach we adopt in the next section to
derive a method for the computation of prefix
probabilities for TAGs is based on transforma-
tions of equations. Here we informally discuss
the general ideas underlying equation transfor-
mations.
Let w = ala2 • • • an E E* be a string and let
N E V-L. We use the following representation
which is standard in tabular methods for TAG
parsing. An item is a tuple [N, i, j, Ii, 12] rep-
resenting the set of all trees t such that (i) t is a
subtree rooted at N of some derived elementary
tree; and (ii) t&apos;s root spans from position i to
position j in w, t&apos;s foot node spans from posi-
tion Ii to position 12 in w. In case N does not
dominate the foot, we set Ii = 12 = —. We gen-
eralize in the obvious way to items [t, i, j, Ii, 12],
where t is an elementary tree, and [a, i, j, fi, f2],
where cdn(N) = ctf3 for some N and O.
To introduce our approach, let us start with
some considerations concerning the TAG pars-
ing problem. When parsing w with a TAG G,
one usually composes items in order to con-
struct new items spanning a larger portion of
the input string. Assume there are instances of
auxiliary trees t and t&apos; in G, where the yield of
t&apos;, apart from its foot, is the empty string. If
0(t, N) &gt; 0 for some node N on the spine of t&apos;,
and we have recognized an item [Re, i, i, 11, 12],
then we may adjoin t at N and hence deduce
the existence of an item [Re, i, j, fi,12] (see
Fig. 1(a)). Similarly, if t can be adjoined at
a node N to the left of the spine of t&apos; and
Ii = 12, we may deduce the existence of an item
[Rt,, i, j, j, A (see Fig. 1(b)). Importantly, one
or more other auxiliary trees with empty yield
could wrap the tree t&apos; before t adjoins. Adjunc-
tions in this situation are potentially nontermi-
nating.
One may argue that situations where auxil-
iary trees have empty yield do not occur in prac-
tice, and are even by definition excluded in the
</bodyText>
<page confidence="0.99771">
954
</page>
<figureCaption confidence="0.9744">
Figure 1: Wrapping in auxiliary trees with
empty yield
</figureCaption>
<bodyText confidence="0.984874826923077">
case of lexicalized TAGs. However, in the com-
putation of the prefix probability we must take
into account trees with non-empty yield which
behave like trees with empty yield because their
lexical nodes fall to the right of the right bound-
ary of the prefix string. For example, the two
cases previously considered in Fig. 1 now gen-
eralize to those in Fig. 2.
Figure 2: Wrapping of auxiliary trees when
computing the prefix probability
To derive a method for the computation of
prefix probabilities, we give some simple recur-
sive equations. Each equation decomposes an
item into other items in all possible ways, in
the sense that it expresses the probability of
that item as a function of the probabilities of
items associated with equal or smaller portions
of the input.
In specifying the equations, we exploit tech-
niques used in the parsing of incomplete in-
put (Lang, 1988). This allows us to compute
the prefix probability as a by-product of com-
puting the inside probability.
In order to avoid the problem of nontermi-
nation outlined above, we transform our equa-
tions to remove infinite recursion, while preserv-
ing the correctness of the probability computa-
tion. The transformation of the equations is
explained as follows. For an item I, the span
of I, written a(/), is the 4-tuple representing
the 4 input positions in I. We will define an
equivalence relation on spans that relates to the
portion of the input that is covered. The trans-
formations that we apply to our equations pro-
duce two new sets of equations. The first set
of equations are concerned with all possible de-
compositions of a given item I into set of items
of which one has a span equivalent to that of I
and the others have an empty span. Equations
in this set represent endless recursion. The sys-
tem of all such equations can be solved indepen-
dently of the actual input w. This is done once
for a given grammar.
The second set of equations have the property
that, when evaluated, recursion always termi-
nates. The evaluation of these equations com-
putes the probability of the input string modulo
the computation of some parts of the derivation
that do not contribute to the input itself. Com-
bination of the second set of equations with the
solutions obtained from the first set allows the
effective computation of the prefix probability.
</bodyText>
<sectionHeader confidence="0.975626" genericHeader="method">
4 Computing Prefix Probabilities
</sectionHeader>
<bodyText confidence="0.999921">
This section develops an algorithm for the com-
putation of prefix probabilities for stochastic
TAGs.
</bodyText>
<subsectionHeader confidence="0.990051">
4.1 General equations
</subsectionHeader>
<bodyText confidence="0.954206">
The prefix probability is given by:
</bodyText>
<equation confidence="0.9911385">
EPr(ai • • • anw)
weE* tEl
</equation>
<bodyText confidence="0.869178">
where P is a function over items recursively de-
fined as follows:
</bodyText>
<equation confidence="0.9925825">
fi, f2]) P(ERt,i, fi, f21); (1)
PaceN,i, j, = (2)
E P([a,i,k,—,—]) • PaN,k,j,—,-1),
k(i&lt;k&lt;j)
</equation>
<construct confidence="0.4013996">
if a e A --idft(aN);
fi, f21) = (3)
E P (ia, i, k, — —i) PaN,k, j, fi, 121),
k(i &lt; k &lt; fi)
if a € A dft(N);
</construct>
<figure confidence="0.940176">
(a) Rtt
spine
</figure>
<page confidence="0.925969">
955
</page>
<equation confidence="0.989281863636364">
P([aN,i,i, 11,M) = (4)
E P([a, i, k,h,12]) • PUN, k,
k(12 &lt;k j)
if a e A dft(a);
P({N, i, :7, 11, 12]) = (5)
0(ni1, N) • P({cdn(N),i, j, fi,12]) +
E Pc[cdn(N), fi,12]) •
E o(t, N) • Pat, i,
tEA
if NEVA dft(N);
— —I) = (6)
N) P([cdn(N),i, j, —j) +
E P([cdn(N), —, —]) •
fl,.1*(2 5_ fi .f; i)
E ot, N) • P([t, i, f ffl),
tEA
if NE VA --idft(N);
P([a, —]) = (7)
+ 1 = j A ai =a)+5(i =j =n);
i, 11, 12]) = = fi Aj = f2); (8)
i, —]) = (5(i = j). (9)
Term P j, f, 1211 gives the inside probabil-
</equation>
<bodyText confidence="0.999912642857143">
ity of all possible trees derived from elementary
tree t, having the indicated span over the input.
This is decomposed into the contribution of each
single node of t in equations (1) through (6).
In equations (5) and (6) the contribution of a
node N is determined by the combination of
the inside probabilities of N&apos;s children and by
all possible adjunctions at N. In (7) we rec-
ognize some terminal symbol if it occurs in the
prefix, or ignore its contribution to the span if it
occurs after the last symbol of the prefix. Cru-
cially, this step allows us to reduce the compu-
tation of prefix probabilities to the computation
of inside probabilities.
</bodyText>
<subsectionHeader confidence="0.994849">
4.2 Terminating equations
</subsectionHeader>
<bodyText confidence="0.949172888888889">
In general, the recursive equations (1) to (9)
are not directly computable. This is because
the value of P([A, i, j, f, f9) might indirectly de-
pend on itself, giving rise to nontermination.
We therefore rewrite the equations.
We define an equivalence relation over spans,
that expresses when two items are associated
with equivalent portions of the input:
(i&apos;, j&apos;, , (i, j, 12) if and only if
</bodyText>
<equation confidence="0.99288275">
= (i,i))A
12) = (11,12)V
(( = = = f =ff V =j V l —)A
(11 = 12 = iVf1 = 12 jVfi = 12 = —)))
</equation>
<bodyText confidence="0.999799866666667">
We introduce two new functions Pow and
P. When evaluated on some item I, Piow re-
cursively calls itself as long as some other item
I&apos; with a given elementary tree as its first com-
ponent can be reached, such that cr(/)
Pow returns 0 if the actual branch of recursion
cannot eventually reach such an item .11, thus
removing the contribution to the prefix proba-
bility of that branch. If item I&apos; is reached, then
Pow switches to Papht. Complementary to Pio.,
function Pspi,, tries to decompose an argument
item I into items I&apos; such that a(I) a-(I&apos;). If
this is not possible through the actual branch
of recursion, P3 returns 0. If decomposition
is indeed possible, then we start again with Pi.
at items produced by the decomposition. The
effect of this intermixing of function calls is the
simulation of the original function P, with Plow
being called only on potentially nonterminating
parts of the computation, and P8,12, being called
on parts that are guaranteed to terminate.
Consider some derivation tree spanning some
portion of the input string, and the associated
derivation tree r. There must be a unique ele-
mentary tree which is represented by a node in
T that is the &amp;quot;lowest&amp;quot; one that entirely spans
the portion of the input of interest. (This node
might be the root of T itself.) Then, for each
t E A and for each i, j, it, 12 such that i &lt; j
and i &lt; Ii &lt;12 &lt; j, we must have:
</bodyText>
<equation confidence="0.9549415">
P([t, i, fi, f2]) = (10)
E 1&apos;1,12], [t&apos;, f I, A]).
t&apos;E A, .f;, fi, f2))
Similarly, for each t E I and for each i, j such
that i &lt;j, we must have:
P ([t, —]) = (11)
EPlow([t, i, —,—], [t&apos;, 1,1]).
t&apos;E {t} U A, f E {-,i,3}
</equation>
<bodyText confidence="0.9452915">
The reason why Piow keeps a record of indices
fl and f, i.e., the spanning of the foot node
of the lowest tree (in the above sense) on which
Pow is called, will become clear later, when we
introduce equations (29) and (30).
We define Piou,([t,i, j,fi, 12], [t&apos;, ff, AD and
P10([a,i,j,fi,f2],[t&apos;,f,f])for i &lt; j and
(i,j, 11,12) (i, j, f), as follows.
</bodyText>
<page confidence="0.806063">
956
</page>
<equation confidence="0.992533454545455">
(12) Plow ([1, , j, f 1, 12], ft, f = 0; (19)
Plow*, i; -; -1; [t; fl, = 0. (20)
f21, =
PlowgRt, 2,3, f1 121, [ti f2]) +
6((t, f2) = (ti , 11, A)) •
P.„,it([Rt, i, j, fi, 12]);
Pio.([aN, j, -], [t,fl, = (13)
j, ft, 11, .0 •
P([V; i; i; -; -]) +
P([a, -1) •
Piow([N, -], ft, fi,
if a 0 E A -4ft(aN);
Pio.([aN, j, , f2], [t, 11, = (14)
6(.fi =j) • Pio.([0‘;i; i; g) •
P([N, j, 11,121) +
Pqa, -]) •
Pio.([N, j, f2], ft, 11,
ifa0cAdft(N);
Pio.([a]V,i, j, f2], ft, ft = (15)
([a; , f 1, 12], [t f 1, g) •
P([N, j, j, -1) +
a(i = f2) P([a, f2i) •
ft, 11, g),
if a 0 A dft(a);
([N, ; [t; fl, = (16)
N)
Plow([cdn(N),i, j, f , 12], ft , 11, +
Plow([cdn(N),i, j, f , 12], [t, 11, g) •
Et, EA O(t&apos;, N) • P({t&apos; , i,j, j]) +
Paccin(N), 11, 12, 11, 121) •
E o(ti, N) • Pio. qt1, f21, [t, .0,
e EA
if NE VA dft(N);
[t, fi, = (17)
N) •
Ph,w([cdn(N),i, j, [t, ffl) +
j, -1, [t,
Et,EA O(t, N) • Pat&apos; , j, +
E P([cdn(N), f, g,-,-]) •
:11 .11 =g=ivn=g= .1)
E gl, fl,
t&apos; EA
if NE VA -icift(N);
Plow([a,i, -,-], [t, fi, = 0;
</equation>
<bodyText confidence="0.999904">
The definition of Pio,„ parallels the one of P
given in §4.1. In (12), the second term in the
right-hand side accounts for the case in which
the tree we are visiting is the &amp;quot;lowest&amp;quot; one on
which Pow should be called. Note how in the
above equations Pow must be called also on
nodes that do not dominate the footnode of the
elementary tree they belong to (cf. the definition
of r•:::). Since no call to Piplit is possible through
the terms in (18), (19) and (20), we must set
the right-hand side of these equations to 0.
The specification of Pspht([ce,i, fi,12]) is
given below. Again, the definition parallels the
one of P given in §4.1.
</bodyText>
<equation confidence="0.983476652173913">
Psplit ([aN, i, j, - -i) = (21)
E ([ce, i, k, -1) P([N, k, j, -]) +
k(i &lt; k &lt;j)
Papist (fa, j, j, -; -1) P([1 V -1) ±
P([a, -I) • PapIttaN,i -1),
if a 0 E A -idft(aN);
Pspi,t([ceN, i; j, 11,121) = (22)
E P([a, i, k, -,-1) • PQN,k, j, fi, 121) +
k(i&lt;k&lt;fink &lt;
(5(.fl = ..7) • Paplitaa,i,..), -1) •
fl, f21)
-I) • PsptitaN,i, j, .f1,121),
ifa0eAdft(N);
Psptit([aN,i, j, fi, 12]) = (23)
E f2]) • P([N,k,j,-,-]) +
k(i&lt;kAf2&lt;k&lt;i)
Paptitqa,i, j, 11,121) • P([N, j, j, -, -1) +
(5(i = 12) • P([a, f2i) •
Paput([N,i, -,
ifa0eAdft(a);
f21) = (24)
0(nil, N) • P812([cdn(N), i,j, Ii, 12]) +
E Pc[cdn(N), f2]) •
.f; (i&lt;fi&lt;finf2.5_f;&lt;in
fD 0 (i,j) A (f, f) (11,12))
E (a(t, N) • P([t, i, ffl)
tE.A
tEA
P31([cdn(N), i,j, 11,12]) •
(18) E o(t, N) Pgt, i, j,
957
ifNEVAdft(N);
p1( [JV, .17 = (25)
N) • P„„iit([cdn(N),i, —]) +
E P ([cdn(N), —]) •
(i 5 fi &lt; A (fl .fZ) 0 (0) A
=1 =ivf=f=j))
E 0(t, N) • P fft, j, 1, +
tEA
P„Iitacdn(N),i, j, —]) •
E q5(t, N) • Pat, j, jll,
tEA
if NEVA --,dft(N);
Pwit(ra, =-- 5(i + 1 = j A = a);
Poptit ([I, fi, f21) = 0;
Paplit (1E, i7 j, —, —1) 0.
</equation>
<bodyText confidence="0.999672181818182">
We can now separate those branches of re-
cursion that terminate on the given input from
the cases of endless recursion. We assume be-
low that PoptitaRt, AD &gt; 0. Even if this
is not always valid, for the purpose of deriving
the equations below, this assumption does not
lead to invalid results. We define a new function
Pouter, which accounts for probabilities of sub-
derivations that do not derive any words in the
prefix, but contribute structurally to its deriva-
tion:
</bodyText>
<equation confidence="0.972149714285714">
Pouter([t,i, fi,12], [ti =
j, f2], [t&apos; ,
Pspii,([11t, , j, 1;,
Pouteraa,i, j, 11,12], ; fIAD =
j, fi, f21, [t&apos;, 11, AD
ti tt
P.plit(tRe , 1,3, Ii&apos; J2
</equation>
<bodyText confidence="0.997887">
We can now eliminate the infinite recur-
sion that arises in (10) and (11) by rewriting
</bodyText>
<equation confidence="0.996062222222222">
P(Et, i, j, 11,12D in terms of Pouter:
Pat, i, fi, 12D = (31)
EPoutext,i,i,h,f2], [ti , AD •
ti e A, fi, ft, f2))
Paplit(rRe fi, AD;
P(ft,i, j,—,—D = (32)
E Pouter at1 —1 —1, re 71,1D •
t&apos; E {t} U A, f E {—,i,j}
Papt.t([Rt, 1]) •
</equation>
<bodyText confidence="0.999259857142857">
Equations for Pout, will be derived in the next
subsection.
In summary, terminating computation of pre-
fix probabilities should be based on equa-
tions (31) and (32), which replace (1), along
with equations (2) to (9) and all the equations
for
</bodyText>
<subsectionHeader confidence="0.920415">
4.3 Off-line Equations
</subsectionHeader>
<bodyText confidence="0.995767">
In this section we derive equations for function
Pouter introduced in §4.2 and deal with all re-
maining cases of equations that cause infinite
recursion.
In some cases, function P can be computed
independently of the actual input. For any
i &lt; n we can consistently define the following
quantities, where t E IUA and a E V1 or
cdn(N) = afi for some N and 0:
</bodyText>
<equation confidence="0.9930975">
lit = Pqt, f , fp;
Ha =
</equation>
<bodyText confidence="0.999906352941177">
where f = i if t E A, f = — otherwise, and f =
i if dft(a), f = — otherwise. Thus, Ht is the
probability of all derived trees obtained from t,
with no lexical node at their yields. Quantities
lit and Hc, can be computed by means of a sys-
tem of equations which can be directly obtained
from equations (1) to (9). Similar quantities as
above must be introduced for the case i n.
For instance, we can set 14 = P([t, n, n, f]),
f specified as above, which gives the probabil-
ity of all derived trees obtained from t (with no
restriction at their yields).
FunctionPouter is also independent of the
actual input. Let us focus here on the case
11,12 0 {i,j,—} (this enforces (fi, f2) =
below). For any i, j, f, 12 &lt; n, we can consis-
tently define the following quantities.
</bodyText>
<equation confidence="0.968616">
Lt,t, = Pouter atl f , 12], [ti , 1 g );
La,t&apos; = Pouter qa, fi, f2], , AD.
</equation>
<bodyText confidence="0.9817571875">
In the case at hand, Lt,t, is the probability of all
derived trees obtained from t such that (i) no
lexical node is found at their yields; and (ii) at
some &apos;unfinished&apos; node dominating the foot of
t, the probability of the adjunction of t&apos; has al-
ready been accounted for, but t&apos; itself has not
been adjoined.
It is straightforward to establish a system of
equations for the computation of Lt,t, and La,e,
by rewriting equations (12) to (20) according
to (29) and (30). For instance, combining (12)
and (29) gives (using the above assumptions on
fi and f2):
Lt,t, = LR,tF 6(t t&apos;).
Also, if a 0 E and dft(N), combining (14)
and (30) gives (again, using previous assump-
</bodyText>
<page confidence="0.993667">
958
</page>
<bodyText confidence="0.888196454545454">
tions on Ii and 12; note that the IL&apos;s are known
terms here):
LaN,t&apos;
For any i, f2 &lt;n
define:
rI
Va,t,
Here Vt,t, is the probability of all derived trees
obtained from t with a node dominating the
foot node of t, that is an adjunction site for t1
and is &apos;unfinished&apos; in the same sense as above,
and with lexical nodes only in the portion of
the tree to the right of that node. When we
drop our assumption on h. and 12, we must
(pre)compute in addition terms of the form
Pouter at i) j7ilill [ti ,i,i]) and Pouter ([t1
{ti for i &lt; j &lt; n,
[ti, g) for i &lt; &lt; n, Pouter at n,
Pot n, n,
[t&apos;,,f1,1D for &lt; n, and similar.Again,these
are independent of the choice of i, j and fi. Full
treatment is omitted due to length restrictions.
</bodyText>
<sectionHeader confidence="0.9895465" genericHeader="evaluation">
5 Complexity and concluding
remarks
</sectionHeader>
<bodyText confidence="0.999995078431372">
We have presented a method for the computa-
tion of the prefix probability when the underly-
ing model is a Tree Adjoining Grammar. Func-
tion P3 is the core of the method. Its equa-
tions can be directly translated into an effective
algorithm, using standard functional memoiza-
tion or other tabular techniques. It is easy to
see that such an algorithm can be made to run
in time 0(n6), where n is the length of the input
prefix.
All the quantities introduced in §4.3 (He,
Lex , etc.) are independent of the input and
should be computed off-line, using the system of
equations that can be derived as indicated. For
quantities He we have a non-linear system, since
equations (2) to (6) contain quadratic terms.
Solutions can then be approximated to any de-
gree of precision using standard iterative meth-
ods, as for instance those exploited in (Stolcke,
1995). Under the hypothesis that the grammar
is consistent, that is Pr(L(G)) = 1, all quanti-
ties fi; and 14, evaluate to one. For quantities
Lex and the like, §4.3 provides linear systems
whose solutions can easily be obtained using
standard methods. Note also that quantities
are only used in the off-line computation
of quantities Lt,t, , they do not need to be stored
for the computation of prefix probabilities (com-
pare equations for Le,t, with (31) and (32)).
We can easily develop implementations of our
method that can compute prefix probabilities
incrementally. That is, after we have computed
the prefix probability for a prefix al • • • an, on in-
put an+i we can extend the calculation to prefix
al • • anan+1 without having to recompute all
intermediate steps that do not depend on
This step takes time 0(716).
In this paper we have assumed that the pa-
rameters of the stochastic TAG have been pre-
viously estimated. In practice, smoothing to
avoid sparse data problems plays an important
role. Smoothing can be handled for prefix prob-
ability computation in the following ways. Dis-
counting methods for smoothing simply pro-
duce a modified STAG model which is then
treated as input to the prefix probability com-
putation. Smoothing using methods such as
deleted interpolation which combine class-based
models with word-based models to avoid sparse
data problems have to be handled by a cognate
interpolation of prefix probability models.
</bodyText>
<sectionHeader confidence="0.998753" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999589111111111">
C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khu-
danpur, L. Mangu, H. Printz, E. Ristad, A. Stolcke,
R. Rosenfeld, and D. Wu. 1997. Structure and per-
formance of a dependency language model. In Proc.
of Eurospeech 97, volume 5, pages 2775-2778.
F. Jelinek and J. Lafferty. 1991. Computation of the
probability of initial substring generation by stochas-
tic context-free grammars. Computational Linguis-
tics, 17(3):315-323.
A. K. Joshi and Y. Schabes. 1992. Tree-adjoining gram-
mars and lexicalized grammars. In M. Nivat and
A. Podelski, editors, 7&apos;ree automata and languages,
pages 409-431. Elsevier Science.
A. K. Joshi. 1988. An introduction to tree adjoining
grammars. In A. Manaster-Ramer, editor, Mathemat-
ics of Language. John Benjamins, Amsterdam.
B. Lang. 1988. Parsing incomplete sentences. In Proc. of
the 12th International Conference on Computational
Linguistics, volume 1, pages 365-371, Budapest.
0. Rainbow and A. Joshi. 1995. A formal look at de-
pendency grammars and phrase-structure grammars,
with special consideration of word-order phenomena.
In Leo Wanner, editor, Current Issues in Meaning-
Text Theory. Pinter, London.
Y. Schabes. 1992. Stochastic lexicalized tree-adjoining
grammars. In Proc. of COLING &apos;92, volume 2, pages
426-432, Nantes, France.
B. Srinivas. 1996. &amp;quot;Almost Parsing&amp;quot; technique for lan-
guage modeling. In Proc. ICSLP &apos;96, volume 3, pages
1173-1176, Philadelphia, PA, Oct 3-6.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):165-201.
J. H. Wright and E. N. Wrigley. 1989. Probabilistic LR
parsing for speech recognition. In IWPT &apos;89, pages
105-114.
</reference>
<bodyText confidence="0.88320275">
Ho • LN,t1.
and j = n, we also need to
= Pouter at, i, fi, f21, [ti fi, g);
= Pouter ff,
</bodyText>
<page confidence="0.990517">
959
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.144694">
<title confidence="0.999538">Prefix Probabilities from Stochastic Tree Adjoining Grammars*</title>
<author confidence="0.863358">Mark-Jan Nederhof</author>
<affiliation confidence="0.754536">DFKI</affiliation>
<address confidence="0.693537666666667">Stuhlsatzenhausweg 3, D-66123 Saarbracken, Germany</address>
<email confidence="0.997493">nederhof@dfki.de</email>
<author confidence="0.987171">Anoop Sarkar</author>
<affiliation confidence="0.999748">Dept. of Computer and Info. Sc. Univ of Pennsylvania</affiliation>
<address confidence="0.997988">200 South 33rd Street, Philadelphia, PA 19104 USA</address>
<email confidence="0.99937">anoopOlinc.cis.upenn.edu</email>
<author confidence="0.998785">Giorgio Satta</author>
<affiliation confidence="0.9986305">Dip. di Elettr. e Inf. Univ. di Padova</affiliation>
<address confidence="0.747643">via Gradenigo 6/A, 35131 Padova, Italy</address>
<email confidence="0.960302">sattadei.unipd.it</email>
<abstract confidence="0.998795285714286">Language models for speech recognition typically use a probability model of the form I al , az , . . . , . Stochastic grammars, on the other hand, are typically used to assign structure to utterances. A language model of the above form is constructed from such grammars by computing the prefix probabil- Pr(ai • • w represents possible terminations of the prefix al • • The main result in this paper is an algorithm to compute such prefix probabilities given a stochastic Tree Adjoining Grammar (TAG). The algorithm achieves the required computain time. The probability of subderivations that do not derive any words in the prefix, but contribute structurally to its derivation, are precomputed to achieve termination. This algorithm enables existing corpus-based estimation techniques for stochastic TAGs to be used for language modelling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>D Engle</author>
<author>F Jelinek</author>
<author>V Jimenez</author>
<author>S Khudanpur</author>
<author>L Mangu</author>
<author>H Printz</author>
<author>E Ristad</author>
<author>A Stolcke</author>
<author>R Rosenfeld</author>
<author>D Wu</author>
</authors>
<title>Structure and performance of a dependency language model.</title>
<date>1997</date>
<booktitle>In Proc. of Eurospeech 97,</booktitle>
<volume>5</volume>
<pages>2775--2778</pages>
<contexts>
<context position="3532" citStr="Chelba et al., 1997" startWordPosition="560" endWordPosition="563">apturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have been shown to have relations with both phrase-structure grammars and dependency grammars (Rambow and Joshi, 1995), which is relevant because recent work on structured language models (Chelba et al., 1997) have used dependency grammars to exploit their lexicalization. We use stochastic TAGs as such a structured language model in contrast with earlier work where TAGs have been exploited in a class-based n-gram language model (Srinivas, 1996). This paper derives an algorithm to compute prefix probabilities E„,,E. Pr(ai • • • anw). The algorithm assumes as input a stochastic TAG G and a string which is a prefix of some string in L(G), the language generated by G. This algorithm enables existing corpus-based estimation techniques (Schabes, 1992) in stochastic TAGs to be used for language modelling.</context>
</contexts>
<marker>Chelba, Engle, Jelinek, Jimenez, Khudanpur, Mangu, Printz, Ristad, Stolcke, Rosenfeld, Wu, 1997</marker>
<rawString>C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khudanpur, L. Mangu, H. Printz, E. Ristad, A. Stolcke, R. Rosenfeld, and D. Wu. 1997. Structure and performance of a dependency language model. In Proc. of Eurospeech 97, volume 5, pages 2775-2778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J Lafferty</author>
</authors>
<title>Computation of the probability of initial substring generation by stochastic context-free grammars.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--3</pages>
<contexts>
<context position="2503" citStr="Jelinek and Lafferty, 1991" startWordPosition="396" endWordPosition="399">and Technology (BMBF) in the framework of the VERBMOBIL Project under Grant 01 IV 701 VO, and by the Priority Programme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94- G-0426. The authors wish to thank Aravind Joshi for his support in this research. ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was E„EE.. Pr(ai • • anw). However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct rela</context>
</contexts>
<marker>Jelinek, Lafferty, 1991</marker>
<rawString>F. Jelinek and J. Lafferty. 1991. Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics, 17(3):315-323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-adjoining grammars and lexicalized grammars.</title>
<date>1992</date>
<booktitle>7&apos;ree automata and languages,</booktitle>
<pages>409--431</pages>
<editor>In M. Nivat and A. Podelski, editors,</editor>
<publisher>Elsevier Science.</publisher>
<contexts>
<context position="3247" citStr="Joshi and Schabes, 1992" startWordPosition="514" endWordPosition="517">s that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have been shown to have relations with both phrase-structure grammars and dependency grammars (Rambow and Joshi, 1995), which is relevant because recent work on structured language models (Chelba et al., 1997) have used dependency grammars to exploit their lexicalization. We use stochastic TAGs as such a structured language model in contrast with earlier work where TAGs have been exploited in a class-based n-gram language model (Srinivas, 1996). This paper derives an algorithm to compute prefix probabilities E„,,E. Pr(</context>
</contexts>
<marker>Joshi, Schabes, 1992</marker>
<rawString>A. K. Joshi and Y. Schabes. 1992. Tree-adjoining grammars and lexicalized grammars. In M. Nivat and A. Podelski, editors, 7&apos;ree automata and languages, pages 409-431. Elsevier Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
</authors>
<title>An introduction to tree adjoining grammars.</title>
<date>1988</date>
<booktitle>Mathematics of Language. John Benjamins,</booktitle>
<editor>In A. Manaster-Ramer, editor,</editor>
<location>Amsterdam.</location>
<contexts>
<context position="3217" citStr="Joshi, 1988" startWordPosition="511" endWordPosition="512">er, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have been shown to have relations with both phrase-structure grammars and dependency grammars (Rambow and Joshi, 1995), which is relevant because recent work on structured language models (Chelba et al., 1997) have used dependency grammars to exploit their lexicalization. We use stochastic TAGs as such a structured language model in contrast with earlier work where TAGs have been exploited in a class-based n-gram language model (Srinivas, 1996). This paper derives an algorithm to compute p</context>
</contexts>
<marker>Joshi, 1988</marker>
<rawString>A. K. Joshi. 1988. An introduction to tree adjoining grammars. In A. Manaster-Ramer, editor, Mathematics of Language. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Parsing incomplete sentences.</title>
<date>1988</date>
<booktitle>In Proc. of the 12th International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>365--371</pages>
<location>Budapest.</location>
<contexts>
<context position="9420" citStr="Lang, 1988" startWordPosition="1727" endWordPosition="1728"> For example, the two cases previously considered in Fig. 1 now generalize to those in Fig. 2. Figure 2: Wrapping of auxiliary trees when computing the prefix probability To derive a method for the computation of prefix probabilities, we give some simple recursive equations. Each equation decomposes an item into other items in all possible ways, in the sense that it expresses the probability of that item as a function of the probabilities of items associated with equal or smaller portions of the input. In specifying the equations, we exploit techniques used in the parsing of incomplete input (Lang, 1988). This allows us to compute the prefix probability as a by-product of computing the inside probability. In order to avoid the problem of nontermination outlined above, we transform our equations to remove infinite recursion, while preserving the correctness of the probability computation. The transformation of the equations is explained as follows. For an item I, the span of I, written a(/), is the 4-tuple representing the 4 input positions in I. We will define an equivalence relation on spans that relates to the portion of the input that is covered. The transformations that we apply to our eq</context>
</contexts>
<marker>Lang, 1988</marker>
<rawString>B. Lang. 1988. Parsing incomplete sentences. In Proc. of the 12th International Conference on Computational Linguistics, volume 1, pages 365-371, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rainbow</author>
<author>A Joshi</author>
</authors>
<title>A formal look at dependency grammars and phrase-structure grammars, with special consideration of word-order phenomena.</title>
<date>1995</date>
<booktitle>Current Issues in MeaningText Theory.</booktitle>
<editor>In Leo Wanner, editor,</editor>
<publisher>Pinter,</publisher>
<location>London.</location>
<marker>Rainbow, Joshi, 1995</marker>
<rawString>0. Rainbow and A. Joshi. 1995. A formal look at dependency grammars and phrase-structure grammars, with special consideration of word-order phenomena. In Leo Wanner, editor, Current Issues in MeaningText Theory. Pinter, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>Stochastic lexicalized tree-adjoining grammars.</title>
<date>1992</date>
<booktitle>In Proc. of COLING &apos;92,</booktitle>
<volume>2</volume>
<pages>426--432</pages>
<location>Nantes, France.</location>
<contexts>
<context position="3247" citStr="Schabes, 1992" startWordPosition="516" endWordPosition="517"> based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have been shown to have relations with both phrase-structure grammars and dependency grammars (Rambow and Joshi, 1995), which is relevant because recent work on structured language models (Chelba et al., 1997) have used dependency grammars to exploit their lexicalization. We use stochastic TAGs as such a structured language model in contrast with earlier work where TAGs have been exploited in a class-based n-gram language model (Srinivas, 1996). This paper derives an algorithm to compute prefix probabilities E„,,E. Pr(</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Y. Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Proc. of COLING &apos;92, volume 2, pages 426-432, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
</authors>
<title>Almost Parsing&amp;quot; technique for language modeling.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP &apos;96,</booktitle>
<volume>3</volume>
<pages>1173--1176</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="3771" citStr="Srinivas, 1996" startWordPosition="600" endWordPosition="601">ssumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have been shown to have relations with both phrase-structure grammars and dependency grammars (Rambow and Joshi, 1995), which is relevant because recent work on structured language models (Chelba et al., 1997) have used dependency grammars to exploit their lexicalization. We use stochastic TAGs as such a structured language model in contrast with earlier work where TAGs have been exploited in a class-based n-gram language model (Srinivas, 1996). This paper derives an algorithm to compute prefix probabilities E„,,E. Pr(ai • • • anw). The algorithm assumes as input a stochastic TAG G and a string which is a prefix of some string in L(G), the language generated by G. This algorithm enables existing corpus-based estimation techniques (Schabes, 1992) in stochastic TAGs to be used for language modelling. 953 2 Notation A stochastic Tree Adjoining Grammar (STAG) is represented by a tuple (NT, E, I, A, 0) where NT is a set of nonterminal symbols, E is a set of terminal symbols, I is a set of initial trees and A is a set of auxiliary trees. </context>
</contexts>
<marker>Srinivas, 1996</marker>
<rawString>B. Srinivas. 1996. &amp;quot;Almost Parsing&amp;quot; technique for language modeling. In Proc. ICSLP &apos;96, volume 3, pages 1173-1176, Philadelphia, PA, Oct 3-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--2</pages>
<contexts>
<context position="2519" citStr="Stolcke, 1995" startWordPosition="400" endWordPosition="401"> framework of the VERBMOBIL Project under Grant 01 IV 701 VO, and by the Priority Programme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94- G-0426. The authors wish to thank Aravind Joshi for his support in this research. ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was E„EE.. Pr(ai • • anw). However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations in various</context>
<context position="22360" citStr="Stolcke, 1995" startWordPosition="4258" endWordPosition="4259">m, using standard functional memoization or other tabular techniques. It is easy to see that such an algorithm can be made to run in time 0(n6), where n is the length of the input prefix. All the quantities introduced in §4.3 (He, Lex , etc.) are independent of the input and should be computed off-line, using the system of equations that can be derived as indicated. For quantities He we have a non-linear system, since equations (2) to (6) contain quadratic terms. Solutions can then be approximated to any degree of precision using standard iterative methods, as for instance those exploited in (Stolcke, 1995). Under the hypothesis that the grammar is consistent, that is Pr(L(G)) = 1, all quantities fi; and 14, evaluate to one. For quantities Lex and the like, §4.3 provides linear systems whose solutions can easily be obtained using standard methods. Note also that quantities are only used in the off-line computation of quantities Lt,t, , they do not need to be stored for the computation of prefix probabilities (compare equations for Le,t, with (31) and (32)). We can easily develop implementations of our method that can compute prefix probabilities incrementally. That is, after we have computed the</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>A. Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165-201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Wright</author>
<author>E N Wrigley</author>
</authors>
<title>Probabilistic LR parsing for speech recognition.</title>
<date>1989</date>
<booktitle>In IWPT &apos;89,</booktitle>
<pages>105--114</pages>
<contexts>
<context position="2475" citStr="Wright and Wrigley, 1989" startWordPosition="392" endWordPosition="395">cation, Science, Research and Technology (BMBF) in the framework of the VERBMOBIL Project under Grant 01 IV 701 VO, and by the Priority Programme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94- G-0426. The authors wish to thank Aravind Joshi for his support in this research. ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was E„EE.. Pr(ai • • anw). However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to enco</context>
</contexts>
<marker>Wright, Wrigley, 1989</marker>
<rawString>J. H. Wright and E. N. Wrigley. 1989. Probabilistic LR parsing for speech recognition. In IWPT &apos;89, pages 105-114.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>