<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.144880">
<title confidence="0.993804">
Generation under Uncertainty
</title>
<author confidence="0.996462">
Oliver Lemon Srini Janarthanam Verena Rieser
</author>
<affiliation confidence="0.981964">
Heriot-Watt University Edinburgh University Edinburgh University
Edinburgh, United Kingdom Edinburgh, United Kingdom Edinburgh, United Kingdom
</affiliation>
<email confidence="0.996121">
o.lemon@hw.ac.uk s.janarthanam@ed.ac.ukvrieser@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993823" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999372">
We invite the research community to con-
sider challenges for NLG which arise from
uncertainty. NLG systems should be able
to adapt to their audience and the genera-
tion environment in general, but often the
important features for adaptation are not
known precisely. We explore generation
challenges which could employ simulated
environments to study NLG which is adap-
tive under uncertainty, and suggest possi-
ble metrics for such tasks. It would be par-
ticularly interesting to explore how differ-
ent planning approaches to NLG perform
in challenges involving uncertainty in the
generation environment.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931875">
We would like to highlight the design of NLG sys-
tems for environments where there may be incom-
plete or faulty information, where actions may not
always have the same results, and where there may
be tradeoffs between the different possible out-
comes of actions and plans.
There are various sources of uncertainty in sys-
tems which employ NLG techniques, for example:
</bodyText>
<listItem confidence="0.997812777777778">
• the current state of the user / audience (e.g.
their knowledge, preferred vocabulary, goals,
preferences....),
• the likely user reaction to the generated out-
put,
• the behaviour of related components (e.g. a
surface realiser, or TTS module),
• noise in the environment (for spoken output),
• ambiguity of the generated output.
</listItem>
<bodyText confidence="0.999376818181818">
The problem here is to generate output that
takes these types of uncertainty into account ap-
propriately. For example, you may need to choose
a referring expression for a user, even though you
are not sure whether they are an expert or novice in
the domain. In addition, the next time you speak
to that user, you need to adapt to new informa-
tion you have gained about them (Janarthanam and
Lemon, 2010). The issue of uncertainty for refer-
ring expression generation has been discussed be-
fore by (Reiter, 1991; Horacek, 2005).
Another example is in planning an Information
Presentation for a user, when you cannot know
with certainty how they will respond to it (Rieser
and Lemon, 2009; Rieser et al., 2010). In the worst
case, you may even be uncertain about the user’s
goals or information needs (as in “POMDP” ap-
proaches to dialogue management (Young et al.,
2009; Henderson and Lemon, 2008a)), but you
still need to generate output for them in an appro-
priate way.
In particular, in interactive applications of NLG:
</bodyText>
<listItem confidence="0.997373666666667">
• each NLG action changes the environment
state or context,
• the effect of each NLG action is uncertain.
</listItem>
<bodyText confidence="0.9996921">
Several recent approaches describe NLG tasks
as different kinds of planning, e.g. (Koller and Pet-
rick, 2008; Rieser et al., 2010; Janarthanam and
Lemon, 2010), or as contextual decision making
according to a cost function (van Deemter, 2009).
It would be very interesting to explore how differ-
ent approaches perform in NLG problems where
different types of uncertainty are present in the
generation environment.
In the following we discuss possible genera-
tion challenges arising from such considerations,
which we hope will lead to work on an agreed
shared challenge in this research community. In
section 2 we briefly review recent work showing
that simulated environments can be used to evalu-
ate generation under uncertainty, and in section 3
we discuss some possible metrics for such tasks.
Section 4 concludes by considering how a useful
generation challenge could be constructed using
similar methods.
</bodyText>
<sectionHeader confidence="0.967057" genericHeader="method">
2 Generation in Uncertain Simulated
Environments
</sectionHeader>
<bodyText confidence="0.999946538461538">
Finding the best (or “optimal”) way to generate
under uncertainty requires exploring the possible
outcomes of actions in stochastic environments.
Therefore, related research on Dialogue Strategy
learning has used data-driven simulated environ-
ments as a cheap and efficient way to explore un-
certainty (Lemon and Pietquin, 2007). However,
building good simulated environments is a chal-
lenge in its own right, as we illustrate in the fol-
lowing using the examples of Information Presen-
tation and Referring Expression Generation. We
also point out the additional challenges these sim-
ulations have to face when being used for NLG.
</bodyText>
<subsectionHeader confidence="0.987681">
2.1 User Simulations for Information
Presentation
</subsectionHeader>
<bodyText confidence="0.999197571428571">
User Simulations can provide a model of proba-
ble, but uncertain, user reactions to NLG actions,
and we propose that they are a useful potential
direction for exploring and evaluate different ap-
proaches to handling uncertainty in generation.
User Simulations are commonly used to train
strategies for Dialogue Management, see for ex-
ample (Young et al., 2007). A user simulation for
Information Presentation is very similar, in that it
is a predictive model of the most likely next user
act. 1 However, this NLG predicted user act does
not actually change the overall dialogue state (e.g.
by filling slots) but it only changes the generator
state. In other words, this NLG user simulation
tells us what the user is most likely to do next, if
we were to stop generating now.
In addition to the challenges of building user
simulations for learning Dialogue policies, e.g.
modelling, evaluation, and available data sets
(Lemon and Pietquin, 2007), a crucial decision for
NLG is the level of detail needed to train sensible
</bodyText>
<footnote confidence="0.8184052">
1Similar to the internal user models applied in recent
work on POMDP (Partially Observable Markov Decision
Process) dialogue managers (Young et al., 2007; Henderson
and Lemon, 2008b; Gasic et al., 2008) for estimation of user
act probabilities.
</footnote>
<bodyText confidence="0.999612083333333">
policies. While high-level dialogue act descrip-
tions may be sufficient for dialogue policies, NLG
decisions may require a much finer level of detail.
The finer the required detail of user reactions, the
more data is needed to build data-driven simula-
tions.
For content selection in Information Presen-
tation tasks (choosing presentation strategy and
number of attributes), for example, the level of de-
scription can still be fairly abstract. We were most
interested in probability distributions over the fol-
lowing possible user reactions:
</bodyText>
<listItem confidence="0.972932555555555">
1. select: the user chooses one of the pre-
sented items, e.g. “Yes, I’ll take that one.”.
This reply type indicates that the informa-
tion presentation was sufficient for the user
to make a choice.
2. addInfo: The user provides more at-
tributes, e.g. “I want something cheap.”. This
reply type indicates that the user has more
specific requests, which s/he wants to specify
after being presented with the current infor-
mation.
3. requestMoreInfo: The user asks for
more information, e.g. “Can you recommend
me one?”, “What is the price range of the
last item?”. This reply type indicates that the
system failed to present the information the
user was looking for.
4. askRepeat: The user asks the system to
repeat the same message again, e.g. “Can you
repeat?”. This reply type indicates that the
utterance was either too long or confusing for
the user to remember, or the TTS quality was
not good enough, or both.
5. silence: The user does not say anything.
In this case it is up to the system to take ini-
tiative.
6. hangup: The user closes the interaction.
</listItem>
<bodyText confidence="0.994477296296296">
We have built user simulations using n-gram
models of system (s) and user (u) acts, as first
introduced by (Eckert et al., 1997). In order to
account for data sparsity, we apply different dis-
counting (“smoothing”) techniques including au-
tomatic back-off, using the CMU Statistical Lan-
guage Modelling toolkit (Clarkson and Rosenfeld,
1997). For example we have constructed a bi-
gram model2 for the users’ reactions to the sys-
tem’s IP structure decisions (P(au,t|IPs,t)), and
a tri-gram (i.e. IP structure + attribute choice)
model for predicting user reactions to the system’s
combined IP structure and attribute selection deci-
sions: P(au,t|IPs,t, attributess,t).
We have evaluated the performance of these
models by measuring dialogue similarity to the
original data, based on the Kullback-Leibler (KL)
divergence, as also used by e.g. (Cuay´ahuitl et al.,
2005; Jung et al., 2009; Janarthanam and Lemon,
2009). We compared the raw probabilities as ob-
served in the data with the probabilities generated
by our n-gram models using different discounting
techniques for each context. All the models have a
small divergence from the original data (especially
the bi-gram model), suggesting that they are rea-
sonable simulations for training and testing NLG
policies (Rieser et al., 2010).
</bodyText>
<subsectionHeader confidence="0.99519">
2.2 Other Simulated Components
</subsectionHeader>
<bodyText confidence="0.999932944444444">
In some systems, NLG decisions may also depend
on related components, such as the database, sub-
sequent generation steps, or the Text-to-Speech
module for spoken generation. Building simula-
tions for these components to capture their inher-
ent uncertainty, again, is an interesting challenge.
For example, one might want to adapt the gen-
erated output according to the predicted TTS qual-
ity. Therefore, one needs a model of the expected/
predicted TTS quality for a TTS engine (Boidin et
al., 2009).
Furthermore, NLG decisions might be inputs
to a stochastic sentence realiser, such as SPaRKy
(Stent et al., 2004). However, one might not have
a fully trained stochastic sentence realiser for this
domain (yet). In (Rieser et al., 2010) we therefore
modelled the variance as observed in the top rank-
ing SPaRKy examples.
</bodyText>
<subsectionHeader confidence="0.7518235">
2.3 Generating Referring Expressions under
uncertainty
</subsectionHeader>
<bodyText confidence="0.999389666666667">
In this section, we present an example user simu-
lation (US) model, that simulates the dialogue be-
haviour of users who react to referring expressions
depending on their domain knowledge. These ex-
ternal simulation models are different from inter-
nal user models used by dialogue systems. In
</bodyText>
<footnote confidence="0.99720875">
2Where au,t is the predicted next user action at time t,
IP3,t was the system’s Information Presentation action at t,
and attributes3,t is the set of attributes selected by the sys-
tem at t.
</footnote>
<bodyText confidence="0.9998045">
particular, such models must be sensitive to a
system’s choices of referring expressions. The
simulation has a statistical distribution of in-built
knowledge profiles that determines the dialogue
behaviour of the user being simulated. Uncer-
tainty arises because if the user does not know a
referring expression, then he is more likely to re-
quest clarification. If the user is able to interpret
the referring expressions and identify the refer-
ences then he is more likely to follow the system’s
instruction. This behaviour is simulated by the ac-
tion selection models described below.
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a user
action Au,t (u denoting user). This can either be a
clarification request (cr) or an instruction response
(ir). We used two kinds of action selection mod-
els: a corpus-driven statistical model and a hand-
coded rule-based model.
</bodyText>
<subsectionHeader confidence="0.998127">
2.4 Corpus-driven action selection model
</subsectionHeader>
<bodyText confidence="0.995696741935484">
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a user
action Au,t (u denoting user). This can either be a
clarification request (cr) or an instruction response
(ir). The US produces a clarification request cr
based on the class of the referent C(Ri), type of
the referring expression Ti, and the current domain
knowledge of the user for the referring expression
DKu,t(Ri, Ti). Domain entities whose jargon ex-
pressions raised clarification requests in the cor-
pus were listed and those that had more than the
mean number of clarification requests were clas-
sified as difficult and others as easy enti-
ties (for example, “power adaptor” is easy - all
users understood this expression, “broadband fil-
ter” is difficult). Clarification requests are
produced using the following model.
P(Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri,Ti))
where (Ri, Ti) ∈ RECs,t
One should note that the actual literal expres-
sion is not used in the transaction. Only the entity
that it is referring to (Ri) and its type (Ti) are used.
However, the above model simulates the process
of interpreting and resolving the expression and
identifying the domain entity of interest in the in-
struction. The user identification of the entity is
signified when there is no clarification request pro-
duced (i.e. Au,t = none). When no clarification
request is produced, the environment action EAu,t
is generated using the following model.
</bodyText>
<equation confidence="0.933879">
P (EAu,t|As,t) if Au,t! = cr(Ri,Ti)
</equation>
<bodyText confidence="0.9992564">
Finally, the user action is an instruction re-
sponse which is determined by the system ac-
tion As,t. Instruction responses can be ei-
ther provide info, acknowledgement or other
based on the system’s instruction.
</bodyText>
<equation confidence="0.973967">
P(Au,t = ir|EAu,t, As,t)
</equation>
<bodyText confidence="0.9998904">
All the above models were trained on our cor-
pus data using maximum likelihood estimation
and smoothed using a variant of Witten-Bell dis-
counting. According to the data, clarification re-
quests are much more likely when jargon expres-
sions are used to refer to the referents that be-
long to the difficult class and which the user
doesn’t know about. When the system uses ex-
pressions that the user knows, the user gener-
ally responds to the instruction given by the sys-
tem. These user simulation models have been
evaluated and found to produce behaviour that is
very similar to the original corpus data, using the
Kullback-Leibler divergence metric (Janarthanam
and Lemon, 2010).
</bodyText>
<sectionHeader confidence="0.994626" genericHeader="method">
3 Metrics
</sectionHeader>
<bodyText confidence="0.9999504">
Here we discuss some possible evaluation met-
rics that will allow different approaches to NLG
under uncertainty to be compared. We envisage
that other metrics should be explored, in particular
those measuring adaptivity of various types.
</bodyText>
<subsectionHeader confidence="0.998927">
3.1 Adaptive Information Presentation
</subsectionHeader>
<bodyText confidence="0.99983604">
Given a suitable corpus, a data-driven evaluation
function can be constructed, using a stepwise lin-
ear regression, following the PARADISE frame-
work (Walker et al., 2000).
For example, in (Rieser et al., 2010) we
build a model which selects the features which
significantly influenced the users’ ratings for
NLG strategies in a Wizard-of-Oz study. We
also assign a value to the user’s reactions
(valueUserReaction), similar to optimising task
success for DM (Young et al., 2007). This re-
flects the fact that good Information Presentation
strategies should help the user to select an item
(valueUserReaction = +100) or provide more
constraints addInfo (valueUserReaction =
±0), but the user should not do anything else
(valueUserReaction = −100). The regression
in equation 1 (R2 = .26) indicates that users’ rat-
ings are influenced by higher level and lower level
features: Users like to be focused on a small set
of database hits (where #DBhits ranges over [1-
100]), which will enable them to choose an item
(valueUserReaction), while keeping the IP ut-
terances short (where #sentence was in the range
[2-18]):
</bodyText>
<equation confidence="0.995412333333333">
Reward = (−1.2) × #DBhits (1)
+(.121) × valueUserReaction
−(1.43) × #sentence
</equation>
<subsectionHeader confidence="0.9962065">
3.2 Measuring Adaptivity of Referring
Expressions
</subsectionHeader>
<bodyText confidence="0.999925608695652">
We have also designed a metric for the goal of
adapting referring expressions to each user’s do-
main knowledge. We present the Adaptation Ac-
curacy score AA that calculates how accurately
the agent chose the expressions for each referent
r, with respect to the user’s knowledge. Appro-
priateness of an expression is based on the user’s
knowledge of the expression. So, when the user
knows the jargon expression for r, the appropri-
ate expression to use is jargon, and if s/he doesn’t
know the jargon, an descriptive expression is ap-
propriate. Although the user’s domain knowledge
is dynamically changing due to learning, we base
appropriateness on the initial state, because our
objective is to adapt to the initial state of the user
DKu,initial. However, in reality, designers might
want their system to account for user’s changing
knowledge as well. We calculate accuracy per ref-
erent RAr as the ratio of number of appropriate
expressions to the total number of instances of the
referent in the dialogue. We then calculate the
overall mean accuracy over all referents as shown
below.
</bodyText>
<equation confidence="0.8816585">
RAr = #(appropriate expressions(r))
#(instances(r))
AdaptationAccuracyAA = 1
#(r)ΣrRAr
</equation>
<sectionHeader confidence="0.999356" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999969">
We have invited the research community to con-
sider challenges for NLG which arise from uncer-
tainty. We argue that NLG systems, like dialogue
managers, should be able to adapt to their audi-
ence and the generation environment. However,
often the important features for adaptation are not
precisely known. We then summarised 2 potential
directions for such challenges – example genera-
tion tasks which employ simulated uncertain en-
vironments to study adaptive NLG, and discussed
some possible metrics for such tasks. We hope
that this will lead to discussions on a shared chal-
lenge allowing comparison of different approaches
to NLG with respect to how well they handle un-
certainty.
</bodyText>
<sectionHeader confidence="0.997384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999895">
The research leading to these results has received
funding from the European Community’s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 216594 (CLASSiC project
www.classic-project.org) and from the
EPSRC, project no. EP/G069840/1.
</bodyText>
<sectionHeader confidence="0.998921" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998916361445783">
Cedric Boidin, Verena Rieser, Lonneke van der Plas,
Oliver Lemon, and Jonathan Chevelu. 2009. Pre-
dicting how it sounds: Re-ranking alternative in-
puts to TTS using latent variables (forthcoming). In
Proc. of Interspeech/ICSLP, Special Session on Ma-
chine Learning for Adaptivity in Spoken Dialogue
Systems.
P.R. Clarkson and R. Rosenfeld. 1997. Statisti-
cal Language Modeling Using the CMU-Cambridge
Toolkit. In Proc. of ESCA Eurospeech.
Heriberto Cuay´ahuitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2005. Human-computer dia-
logue simulation using hidden markov models. In
Proc. of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU).
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
modeling for spoken dialogue system evaluation. In
Proc. of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU).
M. Gasic, S. Keizer, F. Mairesse, J. Schatzmann,
B. Thomson, and S. Young. 2008. Training and
Evaluation of the HIS POMDP Dialogue System in
Noise. In Proc. of SIGdial Workshop on Discourse
and Dialogue.
James Henderson and Oliver Lemon. 2008a. Mixture
Model POMDPs for Efficient Handling of Uncer-
tainty in Dialogue Management. In Proceedings of
ACL.
James Henderson and Oliver Lemon. 2008b. Mixture
Model POMDPs for Efficient Handling of Uncer-
tainty in Dialogue Management. In Proc. of ACL.
Helmut Horacek. 2005. Generating referential de-
scriptions under conditions of uncertainty. In ENLG.
Srinivasan Janarthanam and Oliver Lemon. 2009. A
Two-tier User Simulation Model for Reinforcement
Learning of Adaptive Referring Expression Genera-
tion Policies. In Proc. of SIGdial.
Srini Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: Referring expression
generation in spoken dialogue systems. In Proceed-
ings of ACL. (to appear).
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
woo Jeong, and Gary Geunbae Lee. 2009. Data-
driven user simulation for automated evaluation of
spoken dialog systems. Computer, Speech &amp; Lan-
guage, 23:479–509.
Alexander Koller and Ronald Petrick. 2008. Experi-
ences with planning for natural language generation.
In ICAPS.
Oliver Lemon and Olivier Pietquin. 2007. Machine
learning for spoken dialogue systems. In Inter-
speech.
E. Reiter. 1991. Generating Descriptions that Exploit a
User’s Domain Knowledge. In R. Dale, C. Mellish,
and M. Zock, editors, Current Research in Natural
Language Generation, pages 257–285. Academic
Press.
Verena Rieser and Oliver Lemon. 2009. Natural lan-
guage generation as planning under uncertainty for
spoken dialogue systems. In EACL.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In Proceedings of ACL. (to appear).
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Association for Computational Linguistics.
Kees van Deemter. 2009. What game theory can do
for NLG: the case of vague language. In 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG).
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards Developing General Mod-
els of Usability with PARADISE. Natural Lan-
guage Engineering, 6(3).
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to
Dialog Management. In ICASSP 2007.
S. Young, M. Gaˇsi´c, S. Keizer, F. Mairesse, B. Thom-
son, and K. Yu. 2009. The Hidden Information
State model: a practical framework for POMDP
based spoken dialogue management. Computer
Speech and Language. To appear.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.881344">
<title confidence="0.998531">Generation under Uncertainty</title>
<author confidence="0.999903">Oliver Lemon Srini Janarthanam Verena Rieser</author>
<affiliation confidence="0.999971">Heriot-Watt University Edinburgh University Edinburgh</affiliation>
<address confidence="0.920452">Edinburgh, United Kingdom Edinburgh, United Kingdom Edinburgh, United Kingdom</address>
<email confidence="0.973727">o.lemon@hw.ac.uks.janarthanam@ed.ac.ukvrieser@inf.ed.ac.uk</email>
<abstract confidence="0.998834625">We invite the research community to consider challenges for NLG which arise from uncertainty. NLG systems should be able to adapt to their audience and the generation environment in general, but often the important features for adaptation are not known precisely. We explore generation challenges which could employ simulated environments to study NLG which is adaptive under uncertainty, and suggest possible metrics for such tasks. It would be particularly interesting to explore how different planning approaches to NLG perform in challenges involving uncertainty in the generation environment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cedric Boidin</author>
<author>Verena Rieser</author>
<author>Lonneke van der Plas</author>
<author>Oliver Lemon</author>
<author>Jonathan Chevelu</author>
</authors>
<title>Predicting how it sounds: Re-ranking alternative inputs to TTS using latent variables (forthcoming).</title>
<date>2009</date>
<booktitle>In Proc. of Interspeech/ICSLP, Special Session on Machine Learning for Adaptivity in Spoken Dialogue Systems.</booktitle>
<marker>Boidin, Rieser, van der Plas, Lemon, Chevelu, 2009</marker>
<rawString>Cedric Boidin, Verena Rieser, Lonneke van der Plas, Oliver Lemon, and Jonathan Chevelu. 2009. Predicting how it sounds: Re-ranking alternative inputs to TTS using latent variables (forthcoming). In Proc. of Interspeech/ICSLP, Special Session on Machine Learning for Adaptivity in Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical Language Modeling Using the CMU-Cambridge Toolkit.</title>
<date>1997</date>
<booktitle>In Proc. of ESCA Eurospeech.</booktitle>
<contexts>
<context position="7537" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="1204" endWordPosition="1207">eply type indicates that the utterance was either too long or confusing for the user to remember, or the TTS quality was not good enough, or both. 5. silence: The user does not say anything. In this case it is up to the system to take initiative. 6. hangup: The user closes the interaction. We have built user simulations using n-gram models of system (s) and user (u) acts, as first introduced by (Eckert et al., 1997). In order to account for data sparsity, we apply different discounting (“smoothing”) techniques including automatic back-off, using the CMU Statistical Language Modelling toolkit (Clarkson and Rosenfeld, 1997). For example we have constructed a bigram model2 for the users’ reactions to the system’s IP structure decisions (P(au,t|IPs,t)), and a tri-gram (i.e. IP structure + attribute choice) model for predicting user reactions to the system’s combined IP structure and attribute selection decisions: P(au,t|IPs,t, attributess,t). We have evaluated the performance of these models by measuring dialogue similarity to the original data, based on the Kullback-Leibler (KL) divergence, as also used by e.g. (Cuay´ahuitl et al., 2005; Jung et al., 2009; Janarthanam and Lemon, 2009). We compared the raw probabi</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>P.R. Clarkson and R. Rosenfeld. 1997. Statistical Language Modeling Using the CMU-Cambridge Toolkit. In Proc. of ESCA Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heriberto Cuay´ahuitl</author>
<author>Steve Renals</author>
<author>Oliver Lemon</author>
<author>Hiroshi Shimodaira</author>
</authors>
<title>Human-computer dialogue simulation using hidden markov models.</title>
<date>2005</date>
<booktitle>In Proc. of the IEEE workshop on Automatic Speech Recognition and Understanding (ASRU).</booktitle>
<marker>Cuay´ahuitl, Renals, Lemon, Shimodaira, 2005</marker>
<rawString>Heriberto Cuay´ahuitl, Steve Renals, Oliver Lemon, and Hiroshi Shimodaira. 2005. Human-computer dialogue simulation using hidden markov models. In Proc. of the IEEE workshop on Automatic Speech Recognition and Understanding (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Eckert</author>
<author>E Levin</author>
<author>R Pieraccini</author>
</authors>
<title>User modeling for spoken dialogue system evaluation.</title>
<date>1997</date>
<booktitle>In Proc. of the IEEE workshop on Automatic Speech Recognition and Understanding (ASRU).</booktitle>
<contexts>
<context position="7327" citStr="Eckert et al., 1997" startWordPosition="1174" endWordPosition="1177"> reply type indicates that the system failed to present the information the user was looking for. 4. askRepeat: The user asks the system to repeat the same message again, e.g. “Can you repeat?”. This reply type indicates that the utterance was either too long or confusing for the user to remember, or the TTS quality was not good enough, or both. 5. silence: The user does not say anything. In this case it is up to the system to take initiative. 6. hangup: The user closes the interaction. We have built user simulations using n-gram models of system (s) and user (u) acts, as first introduced by (Eckert et al., 1997). In order to account for data sparsity, we apply different discounting (“smoothing”) techniques including automatic back-off, using the CMU Statistical Language Modelling toolkit (Clarkson and Rosenfeld, 1997). For example we have constructed a bigram model2 for the users’ reactions to the system’s IP structure decisions (P(au,t|IPs,t)), and a tri-gram (i.e. IP structure + attribute choice) model for predicting user reactions to the system’s combined IP structure and attribute selection decisions: P(au,t|IPs,t, attributess,t). We have evaluated the performance of these models by measuring dia</context>
</contexts>
<marker>Eckert, Levin, Pieraccini, 1997</marker>
<rawString>W. Eckert, E. Levin, and R. Pieraccini. 1997. User modeling for spoken dialogue system evaluation. In Proc. of the IEEE workshop on Automatic Speech Recognition and Understanding (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gasic</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>S Young</author>
</authors>
<date>2008</date>
<booktitle>Training and Evaluation of the HIS POMDP Dialogue System in Noise. In Proc. of SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="5566" citStr="Gasic et al., 2008" startWordPosition="878" endWordPosition="881">ts) but it only changes the generator state. In other words, this NLG user simulation tells us what the user is most likely to do next, if we were to stop generating now. In addition to the challenges of building user simulations for learning Dialogue policies, e.g. modelling, evaluation, and available data sets (Lemon and Pietquin, 2007), a crucial decision for NLG is the level of detail needed to train sensible 1Similar to the internal user models applied in recent work on POMDP (Partially Observable Markov Decision Process) dialogue managers (Young et al., 2007; Henderson and Lemon, 2008b; Gasic et al., 2008) for estimation of user act probabilities. policies. While high-level dialogue act descriptions may be sufficient for dialogue policies, NLG decisions may require a much finer level of detail. The finer the required detail of user reactions, the more data is needed to build data-driven simulations. For content selection in Information Presentation tasks (choosing presentation strategy and number of attributes), for example, the level of description can still be fairly abstract. We were most interested in probability distributions over the following possible user reactions: 1. select: the user </context>
</contexts>
<marker>Gasic, Keizer, Mairesse, Schatzmann, Thomson, Young, 2008</marker>
<rawString>M. Gasic, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, and S. Young. 2008. Training and Evaluation of the HIS POMDP Dialogue System in Noise. In Proc. of SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Oliver Lemon</author>
</authors>
<title>Mixture Model POMDPs for Efficient Handling of Uncertainty in Dialogue Management.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2483" citStr="Henderson and Lemon, 2008" startWordPosition="388" endWordPosition="391">ion, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain. Several recent approaches describe NLG tasks as different kinds of planning, e.g. (Koller and Petrick, 2008; Rieser et al., 2010; Janarthanam and Lemon, 2010), or as contextual decision making according to a cost function (van Deemter, 2009). It would be very interesting to explore how different approaches perform in NLG problems where different types of uncertainty</context>
<context position="5544" citStr="Henderson and Lemon, 2008" startWordPosition="874" endWordPosition="877">e state (e.g. by filling slots) but it only changes the generator state. In other words, this NLG user simulation tells us what the user is most likely to do next, if we were to stop generating now. In addition to the challenges of building user simulations for learning Dialogue policies, e.g. modelling, evaluation, and available data sets (Lemon and Pietquin, 2007), a crucial decision for NLG is the level of detail needed to train sensible 1Similar to the internal user models applied in recent work on POMDP (Partially Observable Markov Decision Process) dialogue managers (Young et al., 2007; Henderson and Lemon, 2008b; Gasic et al., 2008) for estimation of user act probabilities. policies. While high-level dialogue act descriptions may be sufficient for dialogue policies, NLG decisions may require a much finer level of detail. The finer the required detail of user reactions, the more data is needed to build data-driven simulations. For content selection in Information Presentation tasks (choosing presentation strategy and number of attributes), for example, the level of description can still be fairly abstract. We were most interested in probability distributions over the following possible user reactions</context>
</contexts>
<marker>Henderson, Lemon, 2008</marker>
<rawString>James Henderson and Oliver Lemon. 2008a. Mixture Model POMDPs for Efficient Handling of Uncertainty in Dialogue Management. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Oliver Lemon</author>
</authors>
<title>Mixture Model POMDPs for Efficient Handling of Uncertainty in Dialogue Management.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2483" citStr="Henderson and Lemon, 2008" startWordPosition="388" endWordPosition="391">ion, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain. Several recent approaches describe NLG tasks as different kinds of planning, e.g. (Koller and Petrick, 2008; Rieser et al., 2010; Janarthanam and Lemon, 2010), or as contextual decision making according to a cost function (van Deemter, 2009). It would be very interesting to explore how different approaches perform in NLG problems where different types of uncertainty</context>
<context position="5544" citStr="Henderson and Lemon, 2008" startWordPosition="874" endWordPosition="877">e state (e.g. by filling slots) but it only changes the generator state. In other words, this NLG user simulation tells us what the user is most likely to do next, if we were to stop generating now. In addition to the challenges of building user simulations for learning Dialogue policies, e.g. modelling, evaluation, and available data sets (Lemon and Pietquin, 2007), a crucial decision for NLG is the level of detail needed to train sensible 1Similar to the internal user models applied in recent work on POMDP (Partially Observable Markov Decision Process) dialogue managers (Young et al., 2007; Henderson and Lemon, 2008b; Gasic et al., 2008) for estimation of user act probabilities. policies. While high-level dialogue act descriptions may be sufficient for dialogue policies, NLG decisions may require a much finer level of detail. The finer the required detail of user reactions, the more data is needed to build data-driven simulations. For content selection in Information Presentation tasks (choosing presentation strategy and number of attributes), for example, the level of description can still be fairly abstract. We were most interested in probability distributions over the following possible user reactions</context>
</contexts>
<marker>Henderson, Lemon, 2008</marker>
<rawString>James Henderson and Oliver Lemon. 2008b. Mixture Model POMDPs for Efficient Handling of Uncertainty in Dialogue Management. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Horacek</author>
</authors>
<title>Generating referential descriptions under conditions of uncertainty.</title>
<date>2005</date>
<booktitle>In ENLG.</booktitle>
<contexts>
<context position="2115" citStr="Horacek, 2005" startWordPosition="327" endWordPosition="328">dule), • noise in the environment (for spoken output), • ambiguity of the generated output. The problem here is to generate output that takes these types of uncertainty into account appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain. </context>
</contexts>
<marker>Horacek, 2005</marker>
<rawString>Helmut Horacek. 2005. Generating referential descriptions under conditions of uncertainty. In ENLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivasan Janarthanam</author>
<author>Oliver Lemon</author>
</authors>
<title>A Two-tier User Simulation Model for Reinforcement Learning of Adaptive Referring Expression Generation Policies. In</title>
<date>2009</date>
<booktitle>Proc. of SIGdial.</booktitle>
<contexts>
<context position="8108" citStr="Janarthanam and Lemon, 2009" startWordPosition="1291" endWordPosition="1294">Language Modelling toolkit (Clarkson and Rosenfeld, 1997). For example we have constructed a bigram model2 for the users’ reactions to the system’s IP structure decisions (P(au,t|IPs,t)), and a tri-gram (i.e. IP structure + attribute choice) model for predicting user reactions to the system’s combined IP structure and attribute selection decisions: P(au,t|IPs,t, attributess,t). We have evaluated the performance of these models by measuring dialogue similarity to the original data, based on the Kullback-Leibler (KL) divergence, as also used by e.g. (Cuay´ahuitl et al., 2005; Jung et al., 2009; Janarthanam and Lemon, 2009). We compared the raw probabilities as observed in the data with the probabilities generated by our n-gram models using different discounting techniques for each context. All the models have a small divergence from the original data (especially the bi-gram model), suggesting that they are reasonable simulations for training and testing NLG policies (Rieser et al., 2010). 2.2 Other Simulated Components In some systems, NLG decisions may also depend on related components, such as the database, subsequent generation steps, or the Text-to-Speech module for spoken generation. Building simulations f</context>
</contexts>
<marker>Janarthanam, Lemon, 2009</marker>
<rawString>Srinivasan Janarthanam and Oliver Lemon. 2009. A Two-tier User Simulation Model for Reinforcement Learning of Adaptive Referring Expression Generation Policies. In Proc. of SIGdial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Janarthanam</author>
<author>Oliver Lemon</author>
</authors>
<title>Learning to adapt to unknown users: Referring expression generation in spoken dialogue systems.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<note>(to appear).</note>
<contexts>
<context position="1994" citStr="Janarthanam and Lemon, 2010" startWordPosition="306" endWordPosition="309">ces....), • the likely user reaction to the generated output, • the behaviour of related components (e.g. a surface realiser, or TTS module), • noise in the environment (for spoken output), • ambiguity of the generated output. The problem here is to generate output that takes these types of uncertainty into account appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applic</context>
<context position="13276" citStr="Janarthanam and Lemon, 2010" startWordPosition="2128" endWordPosition="2131">pus data using maximum likelihood estimation and smoothed using a variant of Witten-Bell discounting. According to the data, clarification requests are much more likely when jargon expressions are used to refer to the referents that belong to the difficult class and which the user doesn’t know about. When the system uses expressions that the user knows, the user generally responds to the instruction given by the system. These user simulation models have been evaluated and found to produce behaviour that is very similar to the original corpus data, using the Kullback-Leibler divergence metric (Janarthanam and Lemon, 2010). 3 Metrics Here we discuss some possible evaluation metrics that will allow different approaches to NLG under uncertainty to be compared. We envisage that other metrics should be explored, in particular those measuring adaptivity of various types. 3.1 Adaptive Information Presentation Given a suitable corpus, a data-driven evaluation function can be constructed, using a stepwise linear regression, following the PARADISE framework (Walker et al., 2000). For example, in (Rieser et al., 2010) we build a model which selects the features which significantly influenced the users’ ratings for NLG st</context>
</contexts>
<marker>Janarthanam, Lemon, 2010</marker>
<rawString>Srini Janarthanam and Oliver Lemon. 2010. Learning to adapt to unknown users: Referring expression generation in spoken dialogue systems. In Proceedings of ACL. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangkeun Jung</author>
<author>Cheongjae Lee</author>
<author>Kyungduk Kim</author>
<author>Minwoo Jeong</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Datadriven user simulation for automated evaluation of spoken dialog systems.</title>
<date>2009</date>
<journal>Computer, Speech &amp; Language,</journal>
<pages>23--479</pages>
<contexts>
<context position="8078" citStr="Jung et al., 2009" startWordPosition="1287" endWordPosition="1290">he CMU Statistical Language Modelling toolkit (Clarkson and Rosenfeld, 1997). For example we have constructed a bigram model2 for the users’ reactions to the system’s IP structure decisions (P(au,t|IPs,t)), and a tri-gram (i.e. IP structure + attribute choice) model for predicting user reactions to the system’s combined IP structure and attribute selection decisions: P(au,t|IPs,t, attributess,t). We have evaluated the performance of these models by measuring dialogue similarity to the original data, based on the Kullback-Leibler (KL) divergence, as also used by e.g. (Cuay´ahuitl et al., 2005; Jung et al., 2009; Janarthanam and Lemon, 2009). We compared the raw probabilities as observed in the data with the probabilities generated by our n-gram models using different discounting techniques for each context. All the models have a small divergence from the original data (especially the bi-gram model), suggesting that they are reasonable simulations for training and testing NLG policies (Rieser et al., 2010). 2.2 Other Simulated Components In some systems, NLG decisions may also depend on related components, such as the database, subsequent generation steps, or the Text-to-Speech module for spoken gene</context>
</contexts>
<marker>Jung, Lee, Kim, Jeong, Lee, 2009</marker>
<rawString>Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Minwoo Jeong, and Gary Geunbae Lee. 2009. Datadriven user simulation for automated evaluation of spoken dialog systems. Computer, Speech &amp; Language, 23:479–509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Ronald Petrick</author>
</authors>
<title>Experiences with planning for natural language generation.</title>
<date>2008</date>
<booktitle>In ICAPS.</booktitle>
<contexts>
<context position="2822" citStr="Koller and Petrick, 2008" startWordPosition="444" endWordPosition="448">annot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain. Several recent approaches describe NLG tasks as different kinds of planning, e.g. (Koller and Petrick, 2008; Rieser et al., 2010; Janarthanam and Lemon, 2010), or as contextual decision making according to a cost function (van Deemter, 2009). It would be very interesting to explore how different approaches perform in NLG problems where different types of uncertainty are present in the generation environment. In the following we discuss possible generation challenges arising from such considerations, which we hope will lead to work on an agreed shared challenge in this research community. In section 2 we briefly review recent work showing that simulated environments can be used to evaluate generatio</context>
</contexts>
<marker>Koller, Petrick, 2008</marker>
<rawString>Alexander Koller and Ronald Petrick. 2008. Experiences with planning for natural language generation. In ICAPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Olivier Pietquin</author>
</authors>
<title>Machine learning for spoken dialogue systems.</title>
<date>2007</date>
<journal>In Interspeech.</journal>
<contexts>
<context position="3997" citStr="Lemon and Pietquin, 2007" startWordPosition="623" endWordPosition="626">ted environments can be used to evaluate generation under uncertainty, and in section 3 we discuss some possible metrics for such tasks. Section 4 concludes by considering how a useful generation challenge could be constructed using similar methods. 2 Generation in Uncertain Simulated Environments Finding the best (or “optimal”) way to generate under uncertainty requires exploring the possible outcomes of actions in stochastic environments. Therefore, related research on Dialogue Strategy learning has used data-driven simulated environments as a cheap and efficient way to explore uncertainty (Lemon and Pietquin, 2007). However, building good simulated environments is a challenge in its own right, as we illustrate in the following using the examples of Information Presentation and Referring Expression Generation. We also point out the additional challenges these simulations have to face when being used for NLG. 2.1 User Simulations for Information Presentation User Simulations can provide a model of probable, but uncertain, user reactions to NLG actions, and we propose that they are a useful potential direction for exploring and evaluate different approaches to handling uncertainty in generation. User Simul</context>
<context position="5287" citStr="Lemon and Pietquin, 2007" startWordPosition="833" endWordPosition="836">ent, see for example (Young et al., 2007). A user simulation for Information Presentation is very similar, in that it is a predictive model of the most likely next user act. 1 However, this NLG predicted user act does not actually change the overall dialogue state (e.g. by filling slots) but it only changes the generator state. In other words, this NLG user simulation tells us what the user is most likely to do next, if we were to stop generating now. In addition to the challenges of building user simulations for learning Dialogue policies, e.g. modelling, evaluation, and available data sets (Lemon and Pietquin, 2007), a crucial decision for NLG is the level of detail needed to train sensible 1Similar to the internal user models applied in recent work on POMDP (Partially Observable Markov Decision Process) dialogue managers (Young et al., 2007; Henderson and Lemon, 2008b; Gasic et al., 2008) for estimation of user act probabilities. policies. While high-level dialogue act descriptions may be sufficient for dialogue policies, NLG decisions may require a much finer level of detail. The finer the required detail of user reactions, the more data is needed to build data-driven simulations. For content selection</context>
</contexts>
<marker>Lemon, Pietquin, 2007</marker>
<rawString>Oliver Lemon and Olivier Pietquin. 2007. Machine learning for spoken dialogue systems. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
</authors>
<title>Generating Descriptions that Exploit a User’s Domain Knowledge. In</title>
<date>1991</date>
<booktitle>Current Research in Natural Language Generation,</booktitle>
<pages>257--285</pages>
<editor>R. Dale, C. Mellish, and M. Zock, editors,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="2099" citStr="Reiter, 1991" startWordPosition="325" endWordPosition="326">ser, or TTS module), • noise in the environment (for spoken output), • ambiguity of the generated output. The problem here is to generate output that takes these types of uncertainty into account appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG actio</context>
</contexts>
<marker>Reiter, 1991</marker>
<rawString>E. Reiter. 1991. Generating Descriptions that Exploit a User’s Domain Knowledge. In R. Dale, C. Mellish, and M. Zock, editors, Current Research in Natural Language Generation, pages 257–285. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Natural language generation as planning under uncertainty for spoken dialogue systems.</title>
<date>2009</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="2275" citStr="Rieser and Lemon, 2009" startWordPosition="352" endWordPosition="355">s of uncertainty into account appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain. Several recent approaches describe NLG tasks as different kinds of planning, e.g. (Koller and Petrick, 2008; Rieser et al., 2010; Janarthanam and Lemon, 2010), </context>
</contexts>
<marker>Rieser, Lemon, 2009</marker>
<rawString>Verena Rieser and Oliver Lemon. 2009. Natural language generation as planning under uncertainty for spoken dialogue systems. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
<author>Xingkun Liu</author>
</authors>
<title>Optimising information presentation for spoken dialogue systems.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<note>(to appear).</note>
<contexts>
<context position="2297" citStr="Rieser et al., 2010" startWordPosition="356" endWordPosition="359">count appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain. Several recent approaches describe NLG tasks as different kinds of planning, e.g. (Koller and Petrick, 2008; Rieser et al., 2010; Janarthanam and Lemon, 2010), or as contextual decis</context>
<context position="8480" citStr="Rieser et al., 2010" startWordPosition="1349" endWordPosition="1352"> We have evaluated the performance of these models by measuring dialogue similarity to the original data, based on the Kullback-Leibler (KL) divergence, as also used by e.g. (Cuay´ahuitl et al., 2005; Jung et al., 2009; Janarthanam and Lemon, 2009). We compared the raw probabilities as observed in the data with the probabilities generated by our n-gram models using different discounting techniques for each context. All the models have a small divergence from the original data (especially the bi-gram model), suggesting that they are reasonable simulations for training and testing NLG policies (Rieser et al., 2010). 2.2 Other Simulated Components In some systems, NLG decisions may also depend on related components, such as the database, subsequent generation steps, or the Text-to-Speech module for spoken generation. Building simulations for these components to capture their inherent uncertainty, again, is an interesting challenge. For example, one might want to adapt the generated output according to the predicted TTS quality. Therefore, one needs a model of the expected/ predicted TTS quality for a TTS engine (Boidin et al., 2009). Furthermore, NLG decisions might be inputs to a stochastic sentence rea</context>
<context position="13771" citStr="Rieser et al., 2010" startWordPosition="2203" endWordPosition="2206"> that is very similar to the original corpus data, using the Kullback-Leibler divergence metric (Janarthanam and Lemon, 2010). 3 Metrics Here we discuss some possible evaluation metrics that will allow different approaches to NLG under uncertainty to be compared. We envisage that other metrics should be explored, in particular those measuring adaptivity of various types. 3.1 Adaptive Information Presentation Given a suitable corpus, a data-driven evaluation function can be constructed, using a stepwise linear regression, following the PARADISE framework (Walker et al., 2000). For example, in (Rieser et al., 2010) we build a model which selects the features which significantly influenced the users’ ratings for NLG strategies in a Wizard-of-Oz study. We also assign a value to the user’s reactions (valueUserReaction), similar to optimising task success for DM (Young et al., 2007). This reflects the fact that good Information Presentation strategies should help the user to select an item (valueUserReaction = +100) or provide more constraints addInfo (valueUserReaction = ±0), but the user should not do anything else (valueUserReaction = −100). The regression in equation 1 (R2 = .26) indicates that users’ r</context>
</contexts>
<marker>Rieser, Lemon, Liu, 2010</marker>
<rawString>Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010. Optimising information presentation for spoken dialogue systems. In Proceedings of ACL. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Stent</author>
<author>Rashmi Prasad</author>
<author>Marilyn Walker</author>
</authors>
<title>Trainable sentence planning for complex information presentation in spoken dialog systems.</title>
<date>2004</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9122" citStr="Stent et al., 2004" startWordPosition="1451" endWordPosition="1454">omponents In some systems, NLG decisions may also depend on related components, such as the database, subsequent generation steps, or the Text-to-Speech module for spoken generation. Building simulations for these components to capture their inherent uncertainty, again, is an interesting challenge. For example, one might want to adapt the generated output according to the predicted TTS quality. Therefore, one needs a model of the expected/ predicted TTS quality for a TTS engine (Boidin et al., 2009). Furthermore, NLG decisions might be inputs to a stochastic sentence realiser, such as SPaRKy (Stent et al., 2004). However, one might not have a fully trained stochastic sentence realiser for this domain (yet). In (Rieser et al., 2010) we therefore modelled the variance as observed in the top ranking SPaRKy examples. 2.3 Generating Referring Expressions under uncertainty In this section, we present an example user simulation (US) model, that simulates the dialogue behaviour of users who react to referring expressions depending on their domain knowledge. These external simulation models are different from internal user models used by dialogue systems. In 2Where au,t is the predicted next user action at ti</context>
</contexts>
<marker>Stent, Prasad, Walker, 2004</marker>
<rawString>Amanda Stent, Rashmi Prasad, and Marilyn Walker. 2004. Trainable sentence planning for complex information presentation in spoken dialog systems. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
</authors>
<title>What game theory can do for NLG: the case of vague language. In</title>
<date>2009</date>
<booktitle>12th European Workshop on Natural Language Generation (ENLG).</booktitle>
<marker>van Deemter, 2009</marker>
<rawString>Kees van Deemter. 2009. What game theory can do for NLG: the case of vague language. In 12th European Workshop on Natural Language Generation (ENLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Candace A Kamm</author>
<author>Diane J Litman</author>
</authors>
<title>Towards Developing General Models of Usability with PARADISE.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="13732" citStr="Walker et al., 2000" startWordPosition="2196" endWordPosition="2199">valuated and found to produce behaviour that is very similar to the original corpus data, using the Kullback-Leibler divergence metric (Janarthanam and Lemon, 2010). 3 Metrics Here we discuss some possible evaluation metrics that will allow different approaches to NLG under uncertainty to be compared. We envisage that other metrics should be explored, in particular those measuring adaptivity of various types. 3.1 Adaptive Information Presentation Given a suitable corpus, a data-driven evaluation function can be constructed, using a stepwise linear regression, following the PARADISE framework (Walker et al., 2000). For example, in (Rieser et al., 2010) we build a model which selects the features which significantly influenced the users’ ratings for NLG strategies in a Wizard-of-Oz study. We also assign a value to the user’s reactions (valueUserReaction), similar to optimising task success for DM (Young et al., 2007). This reflects the fact that good Information Presentation strategies should help the user to select an item (valueUserReaction = +100) or provide more constraints addInfo (valueUserReaction = ±0), but the user should not do anything else (valueUserReaction = −100). The regression in equati</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>Marilyn A. Walker, Candace A. Kamm, and Diane J. Litman. 2000. Towards Developing General Models of Usability with PARADISE. Natural Language Engineering, 6(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>SJ Young</author>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>H Ye</author>
</authors>
<title>The Hidden Information State Approach to Dialog Management.</title>
<date>2007</date>
<booktitle>In ICASSP</booktitle>
<contexts>
<context position="4703" citStr="Young et al., 2007" startWordPosition="735" endWordPosition="738">illustrate in the following using the examples of Information Presentation and Referring Expression Generation. We also point out the additional challenges these simulations have to face when being used for NLG. 2.1 User Simulations for Information Presentation User Simulations can provide a model of probable, but uncertain, user reactions to NLG actions, and we propose that they are a useful potential direction for exploring and evaluate different approaches to handling uncertainty in generation. User Simulations are commonly used to train strategies for Dialogue Management, see for example (Young et al., 2007). A user simulation for Information Presentation is very similar, in that it is a predictive model of the most likely next user act. 1 However, this NLG predicted user act does not actually change the overall dialogue state (e.g. by filling slots) but it only changes the generator state. In other words, this NLG user simulation tells us what the user is most likely to do next, if we were to stop generating now. In addition to the challenges of building user simulations for learning Dialogue policies, e.g. modelling, evaluation, and available data sets (Lemon and Pietquin, 2007), a crucial deci</context>
<context position="14040" citStr="Young et al., 2007" startWordPosition="2245" endWordPosition="2248">isage that other metrics should be explored, in particular those measuring adaptivity of various types. 3.1 Adaptive Information Presentation Given a suitable corpus, a data-driven evaluation function can be constructed, using a stepwise linear regression, following the PARADISE framework (Walker et al., 2000). For example, in (Rieser et al., 2010) we build a model which selects the features which significantly influenced the users’ ratings for NLG strategies in a Wizard-of-Oz study. We also assign a value to the user’s reactions (valueUserReaction), similar to optimising task success for DM (Young et al., 2007). This reflects the fact that good Information Presentation strategies should help the user to select an item (valueUserReaction = +100) or provide more constraints addInfo (valueUserReaction = ±0), but the user should not do anything else (valueUserReaction = −100). The regression in equation 1 (R2 = .26) indicates that users’ ratings are influenced by higher level and lower level features: Users like to be focused on a small set of database hits (where #DBhits ranges over [1- 100]), which will enable them to choose an item (valueUserReaction), while keeping the IP utterances short (where #se</context>
</contexts>
<marker>Young, Schatzmann, Weilhammer, Ye, 2007</marker>
<rawString>SJ Young, J Schatzmann, K Weilhammer, and H Ye. 2007. The Hidden Information State Approach to Dialog Management. In ICASSP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>M Gaˇsi´c</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>B Thomson</author>
<author>K Yu</author>
</authors>
<title>The Hidden Information State model: a practical framework for POMDP based spoken dialogue management. Computer Speech and Language.</title>
<date>2009</date>
<note>To appear.</note>
<marker>Young, Gaˇsi´c, Keizer, Mairesse, Thomson, Yu, 2009</marker>
<rawString>S. Young, M. Gaˇsi´c, S. Keizer, F. Mairesse, B. Thomson, and K. Yu. 2009. The Hidden Information State model: a practical framework for POMDP based spoken dialogue management. Computer Speech and Language. To appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>