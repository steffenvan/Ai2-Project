<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000751">
<title confidence="0.9978575">
Distributional Semantics Beyond Words:
Supervised Learning of Analogy and Paraphrase
</title>
<author confidence="0.99142">
Peter D. Turney
</author>
<affiliation confidence="0.8805">
National Research Council Canada
Information and Communications Technologies
Ottawa, Ontario, Canada, K1A 0R6
</affiliation>
<email confidence="0.697497">
peter.turney@nrc-cnrc.gc.ca
</email>
<sectionHeader confidence="0.99243" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999862136363636">
There have been several efforts to extend
distributional semantics beyond individual
words, to measure the similarity of word pairs,
phrases, and sentences (briefly, tuples; ordered
sets of words, contiguous or noncontiguous).
One way to extend beyond words is to com-
pare two tuples using a function that com-
bines pairwise similarities between the com-
ponent words in the tuples. A strength of
this approach is that it works with both rela-
tional similarity (analogy) and compositional
similarity (paraphrase). However, past work
required hand-coding the combination func-
tion for different tasks. The main contribution
of this paper is that combination functions are
generated by supervised learning. We achieve
state-of-the-art results in measuring relational
similarity between word pairs (SAT analo-
gies and SemEval 2012 Task 2) and measur-
ing compositional similarity between noun-
modifier phrases and unigrams (multiple-
choice paraphrase questions).
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999909857142857">
Harris (1954) and Firth (1957) hypothesized that
words that appear in similar contexts tend to have
similar meanings. This hypothesis is the founda-
tion for distributional semantics, in which words are
represented by context vectors. The similarity of
two words is calculated by comparing the two cor-
responding context vectors (Lund et al., 1995; Lan-
dauer and Dumais, 1997; Turney and Pantel, 2010).
Distributional semantics is highly effective for
measuring the semantic similarity between individ-
ual words. On a set of eighty multiple-choice syn-
onym questions from the test of English as a for-
eign language (TOEFL), a distributional approach
recently achieved 100% accuracy (Bullinaria and
Levy, 2012). However, it has been difficult to extend
distributional semantics beyond individual words, to
word pairs, phrases, and sentences.
Moving beyond individual words, there are vari-
ous types of semantic similarity to consider. Here
we focus on paraphrase and analogy. Paraphrase
is similarity in the meaning of two pieces of text
(Androutsopoulos and Malakasiotis, 2010). Anal-
ogy is similarity in the semantic relations of two sets
of words (Turney, 2008a).
It is common to study paraphrase at the sentence
level (Androutsopoulos and Malakasiotis, 2010), but
we prefer to concentrate on the simplest type of
paraphrase, where a bigram paraphrases a unigram.
For example, dog house is a paraphrase of kennel. In
our experiments, we concentrate on noun-modifier
bigrams and noun unigrams.
Analogies map terms in one domain to terms in
another domain (Gentner, 1983). The familiar anal-
ogy between the solar system and the Rutherford-
Bohr atomic model involves several terms from the
domain of the solar system and the domain of the
atomic model (Turney, 2008a).
The simplest type of analogy is proportional anal-
ogy, which involves two pairs of words (Turney,
2006b). For example, the pair (cook, raw) is anal-
ogous to the pair (decorate, plain). If we cook a
thing, it is no longer raw; if we decorate a thing, it
</bodyText>
<page confidence="0.995535">
353
</page>
<bodyText confidence="0.989706321428571">
Transactions of the Association for Computational Linguistics, 1 (2013) 353–366. Action Editor: Patrick Pantel.
Submitted 5/2013; Revised 7/2013; Published 10/2013. c�2013 Association for Computational Linguistics.
is no longer plain. The semantic relations between
cook and raw are similar to the semantic relations
between decorate and plain. In the following exper-
iments, we focus on proportional analogies.
Erk (2013) distinguished four approaches to
extend distributional semantics beyond words: In
the first, a single vector space representation for a
phrase or sentence is computed from the represen-
tations of the individual words (Mitchell and Lap-
ata, 2010; Baroni and Zamparelli, 2010). In the sec-
ond, two phrases or sentences are compared by com-
bining multiple pairwise similarity values (Socher et
al., 2011; Turney, 2012). Third, weighted inference
rules integrate distributional similarity and formal
logic (Garrette et al., 2011). Fourth, a single space
integrates formal logic and vectors (Clarke, 2012).
Taking the second approach, Turney (2012) intro-
duced a dual-space model, with one space for mea-
suring domain similarity (similarity of topic or field)
and another for function similarity (similarity of role
or usage). Similarities beyond individual words are
calculated by functions that combine domain and
function similarities of component words.
The dual-space model has been applied to mea-
suring compositional similarity (paraphrase recogni-
tion) and relational similarity (analogy recognition).
In experiments that tested for sensitivity to word
order, the dual-space model performed significantly
better than competing approaches (Turney, 2012).
A limitation of past work with the dual-space
model is that the combination functions were hand-
coded. Our main contribution is to show how hand-
coding can be eliminated with supervised learning.
For ease of reference, we will call our approach
SuperSim (supervised similarity). With no modifi-
cation of SuperSim for the specific task (relational
similarity or compositional similarity), we achieve
better results than previous hand-coded models.
Compositional similarity (paraphrase) compares
two contiguous phrases or sentences (n-grams),
whereas relational similarity (analogy) does not
require contiguity. We use tuple to refer to both con-
tiguous and noncontiguous word sequences.
We approach analogy as a problem of supervised
tuple classification. To measure the relational sim-
ilarity between two word pairs, we train SuperSim
with quadruples that are labeled as positive and neg-
ative examples of analogies. For example, the pro-
portional analogy hcook, raw, decorate, plaini is
labeled as a positive example.
A quadruple is represented by a feature vector,
composed of domain and function similarities from
the dual-space model and other features based on
corpus frequencies. SuperSim uses a support vector
machine (Platt, 1998) to learn the probability that a
quadruple ha, b, c, di consists of a word pair ha, bi
and an analogous word pair hc, di. The probability
can be interpreted as the degree of relational similar-
ity between the two given word pairs.
We also approach paraphrase as supervised tuple
classification. To measure the compositional simi-
larity beween an m-gram and an n-gram, we train
the learning algorithm with (m + n)-tuples that are
positive and negative examples of paraphrases.
SuperSim learns to estimate the probability that
a triple ha, b, ci consists of a compositional bigram
ab and a synonymous unigram c. For instance, the
phrase fish tank is synonymous with aquarium; that
is, fish tank and aquarium have high compositional
similarity. The triple hfish, tank, aquariumi is repre-
sented using the same features that we used for anal-
ogy. The probability of the triple can be interpreted
as the degree of compositional similarity between
the given bigram and unigram.
We review related work in Section 2. The gen-
eral feature space for learning relations and compo-
sitions is presented in Section 3. The experiments
with relational similarity are described in Section 4,
and Section 5 reports the results with compositional
similarity. Section 6 discusses the implications of
the results. We consider future work in Section 7
and conclude in Section 8.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999918636363636">
In SemEval 2012, Task 2 was concerned with mea-
suring the degree of relational similarity between
two word pairs (Jurgens et al., 2012) and Task 6
(Agirre et al., 2012) examined the degree of seman-
tic equivalence between two sentences. These two
areas of research have been mostly independent,
although Socher et al. (2012) and Turney (2012)
present unified perspectives on the two tasks. We
first discuss some work on relational similarity, then
some work on compositional similarity, and lastly
work that unifies the two types of similarity.
</bodyText>
<page confidence="0.998218">
354
</page>
<subsectionHeader confidence="0.959238">
2.1 Relational Similarity
</subsectionHeader>
<bodyText confidence="0.99978775">
LRA (latent relational analysis) measures rela-
tional similarity with a pair–pattern matrix (Turney,
2006b). Rows in the matrix correspond to word
pairs (a, b) and columns correspond to patterns that
connect the pairs (“a for the b”) in a large cor-
pus. This is a holistic (noncompositional) approach
to distributional similarity, since the word pairs are
opaque wholes; the component words have no sep-
arate representations. A compositional approach to
analogy has a representation for each word, and a
word pair is represented by composing the represen-
tations for each member of the pair. Given a vocabu-
lary of N words, a compositional approach requires
N representations to handle all possible word pairs,
but a holistic approach requires N2 representations.
Holistic approaches do not scale up (Turney, 2012).
LRA required nine days to run.
Bollegala et al. (2008) answered the SAT anal-
ogy questions with a support vector machine trained
on quadruples (proportional analogies), as we do
here. However, their feature vectors are holistic, and
hence there are scaling problems.
Herda˘gdelen and Baroni (2009) used a support
vector machine to learn relational similarity. Their
feature vectors contained a combination of holistic
and compositional features.
Measuring relational similarity is closely con-
nected to classifying word pairs according to their
semantic relations (Turney and Littman, 2005).
Semantic relation classification was the focus of
SemEval 2007 Task 4 (Girju et al., 2007) and
SemEval 2010 Task 8 (Hendrickx et al., 2010).
</bodyText>
<subsectionHeader confidence="0.999545">
2.2 Compositional Similarity
</subsectionHeader>
<bodyText confidence="0.999926804347826">
To extend distributional semantics beyond words,
many researchers take the first approach described
by Erk (2013), in which a single vector space is used
for individual words, phrases, and sentences (Lan-
dauer and Dumais, 1997; Mitchell and Lapata, 2008;
Mitchell and Lapata, 2010). In this approach, given
the words a and b with context vectors a and b, we
construct a vector for the bigram ab by applying vec-
tor operations to a and b.
Mitchell and Lapata (2010) experiment with
many different vector operations and find that
element-wise multiplication performs well. The
bigram ab is represented by c = a O b, where
ci = ai · bi. However, element-wise multiplica-
tion is commutative, so the bigrams ab and ba map
to the same vector c. In experiments that test for
order sensitivity, element-wise multiplication per-
forms poorly (Turney, 2012).
We can treat the bigram ab as a unit, as if it were
a single word, and construct a context vector for
ab from occurrences of ab in a large corpus. This
holistic approach to representing bigrams performs
well when a limited set of bigrams is specified in
advance (before building the word–context matrix),
but it does not scale up, because there are too many
possible bigrams (Turney, 2012).
Although the holistic approach does not scale up,
we can generate a few holistic bigram vectors and
use them to train a supervised regression model
(Guevara, 2010; Baroni and Zamparelli, 2010).
Given a new bigram cd, not observed in the corpus,
the regression model can predict a holistic vector for
cd, if c and d have been observed separately. We
show in Section 5 that this idea can be adapted to
train SuperSim without manually labeled data.
Socher et al. (2011) take the second approach
described by Erk (2013), in which two sentences are
compared by combining multiple pairwise similar-
ity values. They construct a variable-sized similar-
ity matrix X, in which the element xij is the sim-
ilarity between the i-th phrase of one sentence and
the j-th phrase of the other. Since supervised learn-
ing is simpler with fixed-sized feature vectors, the
variable-sized similarity matrix is then reduced to a
smaller fixed-sized matrix, to allow comparison of
pairs of sentences of varying lengths.
</bodyText>
<subsectionHeader confidence="0.999275">
2.3 Unified Perspectives on Similarity
</subsectionHeader>
<bodyText confidence="0.988094181818182">
Socher et al. (2012) represent words and phrases
with a pair, consisting of a vector and a matrix. The
vector captures the meaning of the word or phrase
and the matrix captures how a word or phrase mod-
ifies the meaning of another word or phrase when
they are combined. They apply this matrix–vector
representation to both compositions and relations.
Turney (2012) represents words with two vectors,
a vector from domain space and a vector from func-
tion space. The domain vector captures the topic or
field of the word and the function vector captures the
</bodyText>
<page confidence="0.998323">
355
</page>
<bodyText confidence="0.999980315789474">
functional role of the word. This dual-space model
is applied to both compositions and relations.
Here we extend the dual-space model of Tur-
ney (2012) in two ways: Hand-coding is replaced
with supervised learning and two new sets of fea-
tures augment domain and function space. Moving
to supervised learning instead of hand-coding makes
it easier to introduce new features.
In the dual-space model, parameterized similar-
ity measures provided the input values for hand-
crafted functions. Each task required a different set
of hand-crafted functions. The parameters of the
similarity measures were tuned using a customized
grid search algorithm. The grid search algorithm
was not suitable for integration with a supervised
learning algorithm. The insight behind SuperSim is
that, given appropriate features, a supervised learn-
ing algorithm can replace the grid search algorithm
and the hand-crafted functions.
</bodyText>
<sectionHeader confidence="0.999823" genericHeader="method">
3 Features for Tuple Classification
</sectionHeader>
<bodyText confidence="0.999963">
We represent a tuple with four types of features, all
based on frequencies in a large corpus. The first
type of feature is the logarithm of the frequency
of a word. The second type is the positive point-
wise mutual information (PPMI) between two words
(Church and Hanks, 1989; Bullinaria and Levy,
2007). Third and fourth are the similarities of two
words in domain and function space (Turney, 2012).
In the following experiments, we use the PPMI
matrix from Turney et al. (2011) and the domain and
function matrices from Turney (2012).1 The three
matrices and the word frequency data are based on
the same corpus, a collection of web pages gath-
ered from university web sites, containing 5 × 1010
words.2 All three matrices are word–context matri-
ces, in which the rows correspond to terms (words
and phrases) in WordNet.3 The columns correspond
to the contexts in which the terms appear; each
matrix involves a different kind of context.
</bodyText>
<footnote confidence="0.989673375">
1The three matrices and the word frequency data are avail-
able on request from the author. The matrix files range from
two to five gigabytes when packaged and compressed for distri-
bution.
2The corpus was collected by Charles Clarke at the Univer-
sity of Waterloo. It is about 280 gigabytes of plain text.
3See http://wordnet.princeton.edu/ for infor-
mation about WordNet.
</footnote>
<bodyText confidence="0.999919808510639">
Let (x1, x2, . . . , xn) be an n-tuple of words. The
number of features we use to represent this tuple
increases as a function of n.
The first set of features consists of log frequency
values for each word xi in the n-tuple. Let freq(xi)
be the frequency of xi in the corpus. We define
LF(xi) as log(freq(xi)+1). If xi is not in the corpus,
freq(xi) is zero, and thus LF(xi) is also zero. There
are n log frequency features, one LF(xi) feature for
each word in the n-tuple.
The second set of features consists of positive
pointwise mutual information values for each pair of
words in the n-tuple. We use the raw PPMI matrix
from Turney et al. (2011). Although they computed
the singular value decomposition (SVD) to project
the row vectors into a lower-dimensional space, we
need the original high-dimensional columns for our
features. The raw PPMI matrix has 114,501 rows
and 139,246 columns with a density of 1.2%. For
each term in WordNet, there is a corresponding row
in the raw PPMI matrix. For each unigram in Word-
Net, there are two corresponding columns in the raw
PPMI matrix, one marked left and the other right.
Suppose xi corresponds to the i-th row of the
PPMI matrix and xj corresponds the j-th column,
marked left. The value in the i-th row and j-th col-
umn of the PPMI matrix, PPMI(xi, xj, left), is the
positive pointwise mutual information of xi and xj
co-occurring in the corpus, where xj is the first word
to the left of xi, ignoring any intervening stop words
(that is, ignoring any words that are not in WordNet).
If xi (or xj) has no corresponding row (or column)
in the matrix, then the PPMI value is set to zero.
Turney et al. (2011) estimated PPMI(xi, xj, left)
by sampling the corpus for phrases containing xi and
then looking for xj to the left of xi in the sampled
phrases (and likewise for right). Due to this sam-
pling process, PPMI(xi, xj, left) does not necessar-
ily equal PPMI(xj, xi, right). For example, suppose
xi is a rare word and xj is a common word. With
PPMI(xi, xj, left), when we sample phrases contain-
ing xi, we are relatively likely to find xj in some of
these phrases. With PPMI(xj, xi, right), when we
sample phrases containing xj, we are less likely to
find any phrases containing xi. Although, in theory,
PPMI(xi, xj, left) should equal PPMI(xj, xi, right),
they are likely to be unequal given a limited sample.
</bodyText>
<page confidence="0.994648">
356
</page>
<bodyText confidence="0.999599306122449">
From the n-tuple, we select all of the n(n − 1)
pairs, (xi, xj), such that i =� j. We then gener-
ate two features for each pair, PPMI(xi, xj, left) and
PPMI(xi, xj, right). Thus there are 2n(n−1) PPMI
values in the second set of features.
The third set of features consists of domain space
similarity values for each pair of words in the
n-tuple. Domain space was designed to capture the
topic of a word. Turney (2012) first constructed a
frequency matrix, in which the rows correspond to
terms in WordNet and the columns correspond to
nearby nouns. Given a term xi, the corpus was sam-
pled for phrases containing xi and the phrases were
processed with a part-of-speech tagger, to identify
nouns. If the noun xj was the closest noun to the left
or right of xi, then the frequency count for the i-th
row and j-th column was incremented. The hypoth-
esis was that the nouns near a term characterize the
topics associated with the term.
The word–context frequency matrix for domain
space has 114,297 rows (terms) and 50,000 columns
(noun contexts, topics), with a density of 2.6%. The
frequency matrix was converted to a PPMI matrix
and then smoothed with SVD. The SVD yields three
matrices, U, E, and V.
A term in domain space is represented by a row
vector in UkEpk. The parameter k specifies the num-
ber of singular values in the truncated singular value
decomposition; that is, k is the number of latent
factors in the low-dimensional representation of the
term (Landauer and Dumais, 1997). We generate
Uk and Ek by deleting the columns in U and E
corresponding to the smallest singular values. The
parameter p raises the singular values in Ek to the
power p (Caron, 2001). As p goes from one to zero,
factors with smaller singular values are given more
weight. This has the effect of making the similarity
measure more discriminating (Turney, 2012).
The similarity of two words in domain space,
Dom(xi, xj, k, p), is computed by extracting the row
vectors in UkEpk that correspond to the words xi and
xj, and then calculating their cosine. Optimal per-
formance requires tuning the parameters k and p for
the task (Bullinaria and Levy, 2012; Turney, 2012).
In the following experiments, we avoid directly tun-
ing k and p by generating features with a variety of
values for k and p, allowing the supervised learning
algorithm to decide which features to use.
Feature set Size of set
</bodyText>
<equation confidence="0.991191">
LF(xi) n
PPMI(xi, xj, handedness) 2n(n − 1)
Dom(xi, xj, k, p) 2n(n − 1)nknp
1
Fun(xi, xj, k, p) 2n(n − 1)nknp
1
</equation>
<tableCaption confidence="0.996045">
Table 1: The four sets of features and their sizes.
</tableCaption>
<bodyText confidence="0.999965666666666">
From the n-tuple, we select all 12n(n − 1) pairs,
(xi, xj), such that i &lt; j. For each pair, we gen-
erate domain similarity features, Dom(xi, xj, k, p),
where k varies from 100 to 1000 in steps of 100 and
p varies from 0 to 1 in steps of 0.1. The number of k
values, nk, is 10 and the number of p values, np, is
11; therefore there are 110 features, nknp, for each
pair, (xi, xj). Thus there are 12n(n−1)nknp domain
space similarity values in the third set of features.
The fourth set of features consists of function
space similarity values for each pair of words in the
n-tuple. Function space was designed to capture the
functional role of a word. It is similar to domain
space, except the context is based on verbal patterns,
instead of nearby nouns. The hypothesis was that
the functional role of a word is characterized by the
patterns that relate the word to nearby verbs.
The word–context frequency matrix for function
space has 114,101 rows (terms) and 50,000 columns
(verb pattern contexts, functional roles), with a den-
sity of 1.2%. The frequency matrix was converted to
a PPMI matrix and smoothed with SVD.
From the n-tuple, we select all 12n(n − 1) pairs,
(xi, xj), such that i &lt; j. For each pair, we generate
function similarity features, Fun(xi, xj, k, p), where
k and p vary as they did with domain space. Thus
there are 12n(n − 1)nknp function space similarity
values in the fourth set of features.
Table 1 summarizes the four sets of features and
the size of each set as a function of n, the number of
words in the given tuple. The values of nk and np
(10 and 11) are considered to be constants. Table 2
shows the number of elements in the feature vector,
as n varies from 1 to 6. The total number of features
is O(n2). We believe that this is acceptable growth
and will scale up to comparing sentence pairs.
The four sets of features have a hierarchical rela-
tionship. The log frequency features are based on
counting isolated occurrences of each word in the
</bodyText>
<page confidence="0.960237">
357
</page>
<figure confidence="0.9403074">
n-tuple LF PPMI Dom Fun Total
1 1 0 0 0 1
2 2 4 110 110 226
3 3 12 330 330 675
4 4 24 660 660 1348
5 5 40 1100 1100 2245
6 6 60 1650 1650 3366
Table 2: Number of features for various tuple sizes.
Stem: word:language
Choices: (1) paint:portrait
(2) poetry:rhythm
(3) note:music
(4) tale:story
(5) week:year
Solution: (3) note:music
</figure>
<tableCaption confidence="0.993499">
Table 3: A five-choice SAT analogy question.
</tableCaption>
<bodyText confidence="0.999960368421053">
corpus. The PPMI features are based on direct co-
occurrences of two words; that is, PPMI is only
greater than zero if the two words actually occur
together in the corpus. Domain and function space
capture indirect or higher-order co-occurrence, due
to the truncated SVD (Lemaire and Denhi`ere,
2006); that is, the values of Dom(xi, xj, k, p) and
Fun(xi, xj, k, p) can be high even when xi and xj
do not actually co-occur in the corpus. We conjec-
ture that there are yet higher orders in this hierarchy
that would provide improved similarity measures.
SuperSim learns to classify tuples by representing
them with these features. SuperSim uses the sequen-
tial minimal optimization (SMO) support vector
machine (SVM) as implemented in Weka (Platt,
1998; Witten et al., 2011).4 The kernel is a normal-
ized third-order polynomial. Weka provides proba-
bility estimates for the classes by fitting the outputs
of the SVM with logistic regression models.
</bodyText>
<sectionHeader confidence="0.997999" genericHeader="method">
4 Relational Similarity
</sectionHeader>
<bodyText confidence="0.999944545454545">
This section presents experiments with learning rela-
tional similarity using SuperSim. The training
datasets consist of quadruples that are labeled as
positive and negative examples of analogies. Table 2
shows that the feature vectors have 1,348 elements.
We experiment with three datasets, a collection
of 374 five-choice questions from the SAT col-
lege entrance exam (Turney et al., 2003), a modi-
fied ten-choice variation of the SAT questions (Tur-
ney, 2012), and the relational similarity dataset from
SemEval 2012 Task 2 (Jurgens et al., 2012).5
</bodyText>
<footnote confidence="0.9968544">
4Weka is available at http://www.cs.waikato.ac.
nz/ml/weka/.
5The SAT questions are available on request from the author.
The SemEval 2012 Task 2 dataset is available at https://
sites.google.com/site/semeval2012task2/.
</footnote>
<subsectionHeader confidence="0.984799">
4.1 Five-choice SAT Questions
</subsectionHeader>
<bodyText confidence="0.999952193548387">
Table 3 is an example of a question from the 374
five-choice SAT questions. Each five-choice ques-
tion yields five labeled quadruples, by combining the
stem with each choice. The quadruple (word, lan-
guage, note, music) is labeled positive and the other
four quadruples are labeled negative.
Since learning works better with balanced train-
ing data (Japkowicz and Stephen, 2002), we use the
symmetries of proportional analogies to add more
positive examples (Lepage and Shin-ichi, 1996). For
each positive quadruple, (a, b, c, d), we add three
more positive quadruples, (b, a, d, c), (c, d, a, b),
and (d, c, b, a). Thus each five-choice question pro-
vides four positive and four negative quadruples.
We use ten-fold cross-validation to apply Super-
Sim to the SAT questions. The folds are constructed
so that the eight quadruples from each SAT question
are kept together in the same fold. To answer a ques-
tion in the testing fold, the learned model assigns a
probability to each of the five choices and guesses
the choice with the highest probability. SuperSim
achieves a score of 54.8% correct (205 out of 374).
Table 4 gives the rank of SuperSim in the list of
the top ten results with the SAT analogy questions.6
The scores ranging from 51.1% to 57.0% are not sig-
nificantly different from SuperSim’s score of 54.8%,
according to Fisher’s exact test at the 95% confi-
dence level. However, SuperSim answers the SAT
questions in a few minutes, whereas LRA requires
nine days, and SuperSim learns its models automat-
ically, unlike the hand-coding of Turney (2012).
</bodyText>
<footnote confidence="0.9929915">
6See the State of the Art page on the ACL Wiki at http:
//aclweb.org/aclwiki.
</footnote>
<page confidence="0.975337">
358
</page>
<table confidence="0.99765375">
Algorithm Reference Correct
Know-Best Veale (2004) 43.0
k-means Bic¸ici &amp; Yuret (2006) 44.0
BagPack Herda˘gdelen &amp; Baroni (2009) 44.1
VSM Turney &amp; Littman (2005) 47.1
Dual-Space Turney (2012) 51.1
BMI Bollegala et al. (2009) 51.1
PairClass Turney (2008b) 52.1
PERT Turney (2006a) 53.5
SuperSim — 54.8
LRA Turney (2006b) 56.1
Human Average college applicant 57.0
</table>
<tableCaption confidence="0.999375">
Table 4: The top ten results on five-choice SAT questions.
</tableCaption>
<table confidence="0.996607083333333">
Features
Algorithm LF PPMI Dom Fun Correct
Dual-Space 0 0 1 1 47.9
SuperSim 1 1 1 1 52.7
SuperSim 0 1 1 1 52.7
SuperSim 1 0 1 1 52.7
SuperSim 1 1 0 1 45.7
SuperSim 1 1 1 0 41.7
SuperSim 1 0 0 0 5.6
SuperSim 0 1 0 0 32.4
SuperSim 0 0 1 0 39.6
SuperSim 0 0 0 1 39.3
</table>
<tableCaption confidence="0.997211">
Table 5: Feature ablation with ten-choice SAT questions.
</tableCaption>
<subsectionHeader confidence="0.959896">
4.2 Ten-choice SAT Questions
</subsectionHeader>
<bodyText confidence="0.99977168627451">
In addition to symmetries, proportional analogies
have asymmetries. In general, if the quadruple
(a, b, c, d) is positive, (a, d, c, b) is negative. For
example, (word, language, note, music) is a good
analogy, but (word, music, note, language) is not.
Words are the basic units of language and notes are
the basic units of music, but words are not necessary
for music and notes are not necessary for language.
Turney (2012) used this asymmetry to convert
the 374 five-choice SAT questions into 374 ten-
choice SAT questions. Each choice (c, d) was
expanded with the stem (a, b), resulting in the
quadruple (a, b, c, d), and then the order was shuf-
fled to (a, d, c, b), so that each choice pair in a five-
choice question generated two choice quadruples in
a ten-choice question. Nine of the quadruples are
negative examples and the quadruple consisting of
the stem pair followed by the solution pair is the only
positive example. The purpose of the ten-choice
questions is to test the ability of measures of rela-
tional similarity to avoid the asymmetric distractors.
We use the ten-choice questions to compare the
hand-coded dual-space approach (Turney, 2012)
with SuperSim. We also use these questions to per-
form an ablation study of the four sets of features
in SuperSim. As with the five-choice questions,
we use the symmetries of proportional analogies to
add three more positive examples, so the training
dataset has nine negative examples and four posi-
tive examples per question. We apply ten-fold cross-
validation to the 374 ten-choice questions.
On the ten-choice questions, SuperSim’s score
is 52.7% (Table 5), compared to 54.8% on the
five-choice questions (Table 4), a drop of 2.1%.
The hand-coded dual-space model scores 47.9%
(Table 5), compared to 51.1% on the five-choice
questions (Table 4), a drop of 3.2%. The dif-
ference between SuperSim (52.7%) and the hand-
coded dual-space model (47.9%) is not significant
according to Fisher’s exact test at the 95% confi-
dence level. The advantage of SuperSim is that it
does not need hand-coding. The results show that
SuperSim can avoid the asymmetric distractors.
Table 5 shows the impact of different subsets of
features on the percentage of correct answers to the
ten-choice SAT questions. Included features are
marked 1 and ablated features are marked 0. The
results show that the log frequency (LF) and PPMI
features are not helpful (but also not harmful) for
relational similarity. We also see that domain space
and function space are both needed for good results.
</bodyText>
<subsectionHeader confidence="0.999626">
4.3 SemEval 2012 Task 2
</subsectionHeader>
<bodyText confidence="0.999968833333333">
The SemEval 2012 Task 2 dataset is based on the
semantic relation classification scheme of Bejar et
al. (1991), consisting of ten high-level categories
of relations and seventy-nine subcategories, with
paradigmatic examples of each subcategory. For
instance, the subcategory taxonomic in the cate-
gory class inclusion has three paradigmatic exam-
ples, flower:tulip, emotion:rage, and poem:sonnet.
Jurgens et al. (2012) used Amazon’s Mechanical
Turk to create the SemEval 2012 Task 2 dataset in
two phases. In the first phase, Turkers expanded the
paradigmatic examples for each subcategory to an
</bodyText>
<page confidence="0.998224">
359
</page>
<table confidence="0.981311272727273">
Algorithm Reference Spearman
BUAP Tovar et al. (2012) 0.014
Duluth-V2 Pedersen (2012) 0.038
Duluth-V1 Pedersen (2012) 0.039
Duluth-V0 Pedersen (2012) 0.050
UTD-SVM Rink &amp; Harabagiu (2012) 0.116
UTD-NB Rink &amp; Harabagiu (2012) 0.229
RNN-1600 Mikolov et al. (2013) 0.275
UTD-LDA Rink &amp; Harabagiu (2013) 0.334
Com Zhila et al. (2013) 0.353
SuperSim — 0.408
</table>
<tableCaption confidence="0.998003">
Table 6: Spearman correlations for SemEval 2012 Task 2.
</tableCaption>
<bodyText confidence="0.99992315">
average of forty-one word pairs per subcategory, a
total of 3,218 pairs. In the second phase, each word
pair from the first phase was assigned a prototypical-
ity score, indicating its similarity to the paradigmatic
examples. The challenge of SemEval 2012 Task 2
was to guess the prototypicality scores.
SuperSim was trained on the five-choice SAT
questions and evaluated on the SemEval 2012 Task 2
test dataset. For a given a word pair, we created
quadruples, combining the word pair with each of
the paradigmatic examples for its subcategory. We
then used SuperSim to compute the probabilities for
each quadruple. Our guess for the prototypicality
score of the given word pair was the average of
the probabilities. Spearman’s rank correlation coef-
ficient between the Turkers’ prototypicality scores
and SuperSim’s scores was 0.408, averaged over the
sixty-nine subcategories in the testing set. Super-
Sim has the highest Spearman correlation achieved
to date on SemEval 2012 Task 2 (see Table 6).
</bodyText>
<sectionHeader confidence="0.984346" genericHeader="method">
5 Compositional Similarity
</sectionHeader>
<bodyText confidence="0.999718444444444">
This section presents experiments using SuperSim
to learn compositional similarity. The datasets con-
sist of triples, (a, b, c), such that ab is a noun-
modifier bigram and c is a noun unigram. The
triples are labeled as positive and negative exam-
ples of paraphrases. Table 2 shows that the fea-
ture vectors have 675 elements. We experiment
with two datasets, seven-choice and fourteen-choice
noun-modifier questions (Turney, 2012).7
</bodyText>
<footnote confidence="0.991641666666667">
7The seven-choice dataset is available at http://jair.
org/papers/paper3640.html. The fourteen-choice
dataset can be generated from the seven-choice dataset.
</footnote>
<figure confidence="0.287258222222222">
Stem: fantasy world
Choices: (1) fairyland
(2) fantasy
(3) world
(4) phantasy
(5) universe
(6) ranter
(7) souring
Solution: (1) fairyland
</figure>
<tableCaption confidence="0.987024">
Table 7: A noun-modifier question based on WordNet.
</tableCaption>
<subsectionHeader confidence="0.869059">
5.1 Noun-Modifier Questions
</subsectionHeader>
<bodyText confidence="0.999968147058824">
The first dataset is a seven-choice noun-modifier
question dataset, constructed from WordNet (Tur-
ney, 2012). The dataset contains 680 questions for
training and 1,500 for testing, a total of 2,180 ques-
tions. Table 7 shows one of the questions.
The stem is a bigram and the choices are uni-
grams. The bigram is composed of a head noun
(world), modified by an adjective or noun (fantasy).
The solution is the unigram (fairyland) that belongs
to the same WordNet synset as the stem.
The distractors are designed to be difficult for cur-
rent approaches to composition. For example, iffan-
tasy world is represented by element-wise multipli-
cation of the context vectors for fantasy and world
(Mitchell and Lapata, 2010), the most likely guess
is fantasy or world, not fairyland (Turney, 2012).
Each seven-choice question yields seven labeled
triples, by combining the stem with each choice.
The triple (fantasy, world, fairyland) is labeled pos-
itive and the other six triples are labeled negative.
In general, if (a, b, c) is a positive example, then
(b, a, c) is negative. For example, world fantasy is
not a paraphrase of fairyland. The second dataset
is constructed by applying this shuffling transfor-
mation to convert the 2,180 seven-choice questions
into 2,180 fourteen-choice questions (Turney, 2012).
The second dataset is designed to be difficult for
approaches that are not sensitive to word order.
Table 8 shows the percentage of the testing
questions that are answered correctly for the two
datasets. Because vector addition and element-wise
multiplication are not sensitive to word order, they
perform poorly on the fourteen-choice questions.
For both datasets, SuperSim performs significantly
</bodyText>
<page confidence="0.995015">
360
</page>
<table confidence="0.878267857142857">
Correct
Algorithm 7-choices 14-choices
Vector addition 50.1 22.5
Element-wise multiplication 57.5 27.4
Dual-Space model 58.3 41.5
SuperSim 75.9 68.0
Holistic model 81.6 —
</table>
<tableCaption confidence="0.995916">
Table 8: Results for the two noun-modifier datasets.
</tableCaption>
<bodyText confidence="0.999792285714286">
better than all other approaches, except for the holis-
tic approach, according to Fisher’s exact test at the
95% confidence level.8
The holistic approach is noncompositional. The
stem bigram is represented by a single context vec-
tor, generated by treating the bigram as if it were
a unigram. A noncompositional approach cannot
scale up to realistic applications (Turney, 2012). The
holistic approach cannot be applied to the fourteen-
choice questions, because the bigrams in these ques-
tions do not correspond to terms in WordNet, and
hence they do not correspond to row vectors in the
matrices we use (see Section 3).
Turney (2012) found it necessary to hand-code a
soundness check into all of the algorithms (vector
addition, element-wise multiplication, dual-space,
and holistic). Given a stem ab and a choice c, the
hand-coded check assigns a minimal score to the
choice if c = a or c = b. We do not need to hand-
code any checking into SuperSim. It learns automat-
ically from the training data to avoid such choices.
</bodyText>
<subsectionHeader confidence="0.998944">
5.2 Ablation Experiments
</subsectionHeader>
<bodyText confidence="0.999971769230769">
Table 9 shows the effects of ablating sets of fea-
tures on the performance of SuperSim with the
fourteen-choice questions. PPMI features are the
most important; by themselves, they achieve 59.7%
correct, although the other features are needed to
reach 68.0%. Domain space features reach the sec-
ond highest performance when used alone (34.6%),
but they reduce performance (from 69.3% to 68.0%)
when combined with other features; however, the
drop is not significant according to Fisher’s exact
test at the 95% significance level.
Since the PPMI features play an important role in
answering the noun-modifier questions, let us take
</bodyText>
<tableCaption confidence="0.8082255">
8The results for SuperSim are new but the other results in
Table 8 are from Turney (2012).
</tableCaption>
<table confidence="0.993621583333333">
Features
Algorithm LF PPMI Dom Fun Correct
Dual-Space 0 0 1 1 41.5
SuperSim 1 1 1 1 68.0
SuperSim 0 1 1 1 66.6
SuperSim 1 0 1 1 52.3
SuperSim 1 1 0 1 69.3
SuperSim 1 1 1 0 65.9
SuperSim 1 0 0 0 14.1
SuperSim 0 1 0 0 59.7
SuperSim 0 0 1 0 34.6
SuperSim 0 0 0 1 32.9
</table>
<tableCaption confidence="0.994597">
Table 9: Ablation with fourteen-choice questions.
</tableCaption>
<table confidence="0.9918964">
PPMI feature subsets
(a, b) (a, c) (b, c) Correct
1 1 1 68.0
0 1 1 59.9
1 0 1 65.4
1 1 0 67.5
1 0 0 62.6
0 1 0 58.1
0 0 1 55.6
0 0 0 52.3
</table>
<tableCaption confidence="0.999294">
Table 10: PPMI subset ablation with fourteen-choices.
</tableCaption>
<bodyText confidence="0.999736823529412">
a closer look at them. From Table 2, we see that
there are twelve PPMI features for the triple (a, b, c),
where ab is a noun-modifier bigram and c is a noun
unigram. We can split the twelve features into three
subsets, one subset for each pair of words, (a, b),
(a, c), and (b, c). For example, the subset for (a, b)
is the four features PPMI(a, b, left), PPMI(b, a, left),
PPMI(a, b, right), and PPMI(b, a, right). Table 10
shows the effects of ablating these subsets.
The results in Table 10 indicate that all three
PPMI subsets contribute to the performance of
SuperSim, but the (a, b) subset contributes more
than the other two subsets. The (a, b) features help
to increase the sensitivity of SuperSim to the order
of the words in the noun-modifier bigram; for exam-
ple, they make it easier to distinguish fantasy world
from world fantasy.
</bodyText>
<subsectionHeader confidence="0.998865">
5.3 Holistic Training
</subsectionHeader>
<bodyText confidence="0.989142666666667">
SuperSim uses 680 training questions to learn to rec-
ognize when a bigram is a paraphrase of a unigram;
it learns from expert knowledge implicit in WordNet
</bodyText>
<page confidence="0.994373">
361
</page>
<figure confidence="0.740311125">
Stem: search engine
Choices: (1) search engine
(2) search
(3) engine
(4) search language
(5) search warrant
(6) diesel engine
(7) steam engine
</figure>
<tableCaption confidence="0.942817">
Solution: (1) search engine
Table 11: A question based on holistic vectors.
</tableCaption>
<bodyText confidence="0.999084638888889">
synsets. It would be advantageous to be able to train
SuperSim with less reliance on expert knowledge.
Past work with adjective-noun bigrams has shown
that we can use holistic bigram vectors to train a
supervised regression model (Guevara, 2010; Baroni
and Zamparelli, 2010). The output of the regression
model is a vector representation for a bigram that
approximates the holistic vector for the bigram; that
is, it approximates the vector we would get by treat-
ing the bigram as if it were a unigram.
SuperSim does not generate vectors as output, but
we can still use holistic bigram vectors for training.
Table 11 shows a seven-choice training question that
was generated without using WordNet synsets. The
choices of the form a b are bigrams, but we repre-
sent them with holistic bigram vectors; we pretend
they are unigrams. We call a b bigrams pseudo-
unigrams. As far as SuperSim is concerned, there
is no difference between these pseudo-unigrams and
true unigrams. The question in Table 11 is treated
the same as the question in Table 7.
We generate 680 holistic training questions by
randomly selecting 680 noun-modifier bigrams from
WordNet as stems for the questions (search engine),
avoiding any bigrams that appear as stems in the
testing questions. The solution (search engine)
is the pseudo-unigram that corresponds to the
stem bigram. In the matrices in Section 3, each
term in WordNet corresponds to a row vector.
These corresponding row vectors enable us to treat
bigrams from WordNet as if they were unigrams.
The distractors are the component unigrams in
the stem bigram (search and engine) and pseudo-
unigrams that share a component word with the stem
(search warrant, diesel engine). To construct the
holistic training questions, we used WordNet as a
</bodyText>
<table confidence="0.94265675">
Correct
Training 7-choices 14-choices
Holistic 61.8 54.4
Standard 75.9 68.0
</table>
<tableCaption confidence="0.979011">
Table 12: Results for SuperSim with holistic training.
</tableCaption>
<bodyText confidence="0.999351516129032">
source of bigrams, but we ignored the rich infor-
mation that WordNet provides about these bigrams,
such as their synonyms, hypernyms, hyponyms,
meronyms, and glosses.
Table 12 compares holistic training to standard
training (that is, training with questions like Table 11
versus training with questions like Table 7). The
testing set is the standard testing set in both cases.
There is a significant drop in performance with
holistic training, but the performance still surpasses
vector addition, element-wise multiplication, and
the hand-coded dual-space model (see Table 8).
Since holistic questions can be generated auto-
matically without human expertise, we experi-
mented with increasing the size of the holistic train-
ing dataset, growing it from 1,000 to 10,000 ques-
tions in increments of 1,000. The performance
on the fourteen-choice questions with holistic train-
ing and standard testing varied between 53.3% and
55.1% correct, with no clear trend up or down. This
is not significantly different from the performance
with 680 holistic training questions (54.4%).
It seems likely that the drop in performance with
holistic training instead of standard training is due to
a difference in the nature of the standard questions
(Table 7) and the holistic questions (Table 11). We
are currently investigating this issue. We expect to
be able to close the performance gap in future work,
by improving the holistic questions. However, it is
possible that there are fundamental limits to holistic
training.
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999885142857143">
SuperSim performs slightly better (not statistically
significant) than the hand-coded dual-space model
on relational similarity problems (Section 4), but it
performs much better on compositional similarity
problems (Section 5). The ablation studies suggest
this is due to the PPMI features, which have no effect
on ten-choice SAT performance (Table 5), but have a
</bodyText>
<page confidence="0.995769">
362
</page>
<bodyText confidence="0.99966075">
large effect on fourteen-choice noun-modifier para-
phrase performance (Table 9).
One advantage of supervised learning over hand-
coding is that it facilitates adding new features. It
is not clear how to modify the hand-coded equations
for the dual-space model of noun-modifier composi-
tion (Turney, 2012) to include PPMI information.
SuperSim is one of the few approaches to distri-
butional semantics beyond words that has attempted
to address both relational and compositional similar-
ity (see Section 2.3). It is a strength of this approach
that it works well with both kinds of similarity.
</bodyText>
<sectionHeader confidence="0.985442" genericHeader="method">
7 Future Work and Limitations
</sectionHeader>
<bodyText confidence="0.998114255319149">
Given the promising results with holistic training for
noun-modifier paraphrases, we plan to experiment
with holistic training for analogies. Consider the
proportional analogy hard is to hard time as good
is to good time, where hard time and good time are
pseudo-unigrams. To a human, this analogy is triv-
ial, but SuperSim has no access to the surface form
of a term. As far as SuperSim is concerned, this
analogy is much the same as the analogy hard is to
difficulty as good is to fun. This strategy automat-
ically converts simple, easily generated analogies
into more complex, challenging analogies, which
may be suited to training SuperSim.
This also suggests that noun-modifier paraphrases
may be used to solve analogies. Perhaps we
can evaluate the quality of a candidate analogy
(a, b, c, d) by searching for a term e such that
(b, e, a) and (d, e, c) are good paraphrases. For
example, consider the analogy mason is to stone as
carpenter is to wood. We can paraphrase mason as
stone worker and carpenter as wood worker. This
transforms the analogy to stone worker is to stone
as wood worker is to wood, which makes it easier to
recognize the relational similarity.
Another area for future work is extending Super-
Sim beyond noun-modifier paraphrases to measur-
ing the similarity of sentence pairs. We plan to adapt
ideas from Socher et al. (2011) for this task. They
use dynamic pooling to represent sentences of vary-
ing size with fixed-size feature vectors. Using fixed-
size feature vectors avoids the problem of quadratic
growth and it enables the supervised learner to gen-
eralize over sentences of varying length.
Some of the competing approaches discussed by
Erk (2013) incorporate formal logic. The work of
Baroni et al. (2012) suggests ways that SuperSim
could be developed to deal with logic.
We believe that SuperSim could benefit from
more features, with greater diversity. One place to
look for these features is higher levels in the hierar-
chy that we sketch in Section 3.
Our ablation experiments suggest that domain and
function spaces provide the most important features
for relational similarity, but PPMI values provide the
most important features for noun-modifier composi-
tional similarity. Explaining this is another topic for
future research.
</bodyText>
<sectionHeader confidence="0.998352" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999845833333333">
In this paper, we have presented SuperSim, a unified
approach to analogy (relational similarity) and para-
phrase (compositional similarity). SuperSim treats
them both as problems of supervised tuple classifi-
cation. The supervised learning algorithm is a stan-
dard support vector machine. The main contribution
of SuperSim is a set of four types of features for rep-
resenting tuples. The features work well with both
analogy and paraphrase, with no task-specific mod-
ifications. SuperSim matches the state of the art on
SAT analogy questions and substantially advances
the state of the art on the SemEval 2012 Task 2 chal-
lenge and the noun-modifier paraphrase questions.
SuperSim runs much faster than LRA (Turney,
2006b), answering the SAT questions in minutes
instead of days. Unlike the dual-space model (Tur-
ney, 2012), SuperSim requires no hand-coded simi-
larity composition functions. Since there is no hand-
coding, it is easy to add new features to SuperSim.
Much work remains to be done, such as incorporat-
ing logic and scaling up to sentence paraphrases, but
past work suggests that these problems are tractable.
In the four approaches described by Erk (2013),
SuperSim is an instance of the second approach
to extending distributional semantics beyond words,
comparing word pairs, phrases, or sentences (in gen-
eral, tuples) by combining multiple pairwise simi-
larity values. Perhaps the main significance of this
paper is that it provides some evidence in support of
this general approach.
</bodyText>
<page confidence="0.998832">
363
</page>
<sectionHeader confidence="0.989442" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999649433962264">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Compu-
tational Semantics (*SEM), pages 385–393, Montr´eal,
Canada.
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entailment
methods. Journal of Artificial Intelligence Research,
38:135–187.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2010), pages 1183–1193.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and
Chung-chieh Shan. 2012. Entailment above the word
level in distributional semantics. In Proceedings of the
13th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL 2012),
pages 23–32.
Isaac I. Bejar, Roger Chaffin, and Susan E. Embretson.
1991. Cognitive and Psychometric Analysis of Ana-
logical Problem Solving. Springer-Verlag.
Ergun Bic¸ici and Deniz Yuret. 2006. Clustering word
pairs to answer analogy questions. In Proceedings of
the Fifteenth Turkish Symposium on Artificial Intelli-
gence and Neural Networks (TAINN 2006), Akyaka,
Mugla, Turkey.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2008. WWW sits the SAT: Measuring rela-
tional similarity on the Web. In Proceedings of the
18th European Conference on Artificial Intelligence
(ECAI 2008), pages 333–337, Patras, Greece.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2009. Measuring the similarity between
implicit semantic relations from the Web. In Proceed-
ings of the 18th International Conference on World
Wide Web (WWW 2009), pages 651–660.
John Bullinaria and Joseph Levy. 2007. Extract-
ing semantic representations from word co-occurrence
statistics: A computational study. Behavior Research
Methods, 39(3):510–526.
John Bullinaria and Joseph Levy. 2012. Extract-
ing semantic representations from word co-occurrence
statistics: Stop-lists, stemming, and SVD. Behavior
Research Methods, 44(3):890–907.
John Caron. 2001. Experiments with LSA scor-
ing: Optimal rank and basis. In Proceedings of
the SIAM Computational Information Retrieval Work-
shop, pages 157–169, Raleigh, NC.
Kenneth Church and Patrick Hanks. 1989. Word asso-
ciation norms, mutual information, and lexicography.
In Proceedings of the 27th Annual Conference of the
Association of Computational Linguistics, pages 76–
83, Vancouver, British Columbia.
Daoud Clarke. 2012. A context-theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1):41–71.
Katrin Erk. 2013. Towards a semantics for distributional
representations. In Proceedings of the 10th Interna-
tional Conference on Computational Semantics (IWCS
2013), Potsdam, Germany.
John Rupert Firth. 1957. A synopsis of linguistic theory
1930–1955. In Studies in Linguistic Analysis, pages
1–32. Blackwell, Oxford.
Dan Garrette, Katrin Erk, and Ray Mooney. 2011. Inte-
grating logical representations with probabilistic infor-
mation using markov logic. In Proceedings of the 9th
International Conference on Computational Semantics
(IWCS 2011), pages 105–114.
Dedre Gentner. 1983. Structure-mapping: A theoretical
framework for analogy. Cognitive Science, 7(2):155–
170.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan
Szpakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic
relations between nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval 2007), pages 13–18, Prague, Czech
Republic.
Emiliano Guevara. 2010. A regression model
of adjective-noun compositionality in distributional
semantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Semantics
(GEMS 2010), pages 33–37.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. Semeval-2010 task 8: Multi-way classifica-
tion of semantic relations between pairs of nominals.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 33–38, Uppsala, Sweden.
Amac¸ Herda˘gdelen and Marco Baroni. 2009. Bagpack:
A general framework to represent semantic relations.
In Proceedings of the EACL 2009 Geometrical Models
for Natural Language Semantics (GEMS) Workshop,
pages 33–40.
Nathalie Japkowicz and Shaju Stephen. 2002. The class
imbalance problem: A systematic study. Intelligent
Data Analysis, 6(5):429–449.
David A. Jurgens, Saif M. Mohammad, Peter D. Tur-
ney, and Keith J. Holyoak. 2012. SemEval-2012
</reference>
<page confidence="0.98803">
364
</page>
<reference confidence="0.999496333333333">
Task 2: Measuring degrees of relational similarity.
In Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics (*SEM), pages 356–
364, Montr´eal, Canada.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to Plato’s problem: The latent seman-
tic analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Review,
104(2):211–240.
Benoit Lemaire and Guy Denhi`ere. 2006. Effects of
high-order co-occurrences on word semantic similar-
ity. Current Psychology Letters: Behaviour, Brain &amp;
Cognition, 18(1).
Yves Lepage and Ando Shin-ichi. 1996. Saussurian
analogy: A theoretical account and its application.
In Proceedings of the 16th International Conference
on Computational Linguistics (COLING 1996), pages
717–722.
Kevin Lund, Curt Burgess, and Ruth Ann Atchley. 1995.
Semantic and associative priming in high-dimensional
semantic space. In Proceedings of the 17th Annual
Conference of the Cognitive Science Society, pages
660–665.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space word
representations. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL 2013), Atlanta, Georgia.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236–244, Columbus, Ohio.
Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.
Ted Pedersen. 2012. Duluth: Measuring degrees of
relational similarity with the gloss vector measure of
semantic relatedness. In First Joint Conference on
Lexical and Computational Semantics (*SEM), pages
497–501, Montreal, Canada.
John C. Platt. 1998. Fast training of support vector
machines using sequential minimal optimization. In
Advances in Kernel Methods: Support Vector Learn-
ing, pages 185–208, Cambridge, MA. MIT Press.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
First Joint Conference on Lexical and Computational
Semantics (*SEM), pages 413–418, Montreal, Canada.
Bryan Rink and Sanda Harabagiu. 2013. The impact
of selectional preference agreement on semantic rela-
tional similarity. In Proceedings of the 10th Interna-
tional Conference on Computational Semantics (IWCS
2013), Potsdam, Germany.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Advances in
Neural Information Processing Systems (NIPS 2011),
pages 801–809.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL
2012), pages 1201–1211.
Mireya Tovar, J. Alejandro Reyes, Azucena Montes,
Darnes Vilari˜no, David Pinto, and Saul Le´on. 2012.
BUAP: A first approximation to relational similar-
ity measuring. In First Joint Conference on Lexi-
cal and Computational Semantics (*SEM), pages 502–
505, Montreal, Canada.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1–3):251–278.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141–
188.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and
Victor Shnayder. 2003. Combining independent mod-
ules to solve multiple-choice synonym and analogy
problems. In Proceedings of the International Con-
ference on Recent Advances in Natural Language Pro-
cessing (RANLP-03), pages 482–489, Borovets, Bul-
garia.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense identifi-
cation through concrete and abstract context. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 680–690.
Peter D. Turney. 2006a. Expressing implicit semantic
relations without supervision. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics (Coling/ACL-06), pages
313–320, Sydney, Australia.
Peter D. Turney. 2006b. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Peter D. Turney. 2008a. The latent relation mapping
engine: Algorithm and experiments. Journal of Artifi-
cial Intelligence Research, 33:615–655.
Peter D. Turney. 2008b. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 905–
912, Manchester, UK.
</reference>
<page confidence="0.986642">
365
</page>
<reference confidence="0.999590368421053">
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533–
585.
Tony Veale. 2004. WordNet sits the SAT: A knowledge-
based approach to lexical analogy. In Proceedings
of the 16th European Conference on Artificial Intel-
ligence (ECAI2004), pages 606–612, Valencia, Spain.
Ian H. Witten, Eibe Frank, and Mark A. Hall. 2011. Data
Mining: Practical Machine Learning Tools and Tech-
niques, Third Edition. Morgan Kaufmann, San Fran-
cisco.
Alisa Zhila, Wen-tau Yih, Christopher Meek, Geoffrey
Zweig, and Tomas Mikolov. 2013. Combining het-
erogeneous models for measuring relational similar-
ity. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (NAACL 2013), Atlanta, Georgia.
</reference>
<page confidence="0.999078">
366
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.876099">
<title confidence="0.9957975">Distributional Semantics Beyond Supervised Learning of Analogy and Paraphrase</title>
<author confidence="0.997726">D Peter</author>
<affiliation confidence="0.964234">National Research Council Information and Communications</affiliation>
<address confidence="0.973402">Ottawa, Ontario, Canada, K1A</address>
<email confidence="0.991206">peter.turney@nrc-cnrc.gc.ca</email>
<abstract confidence="0.997659">There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, and sentences (briefly, ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval 2012 Task 2) and measuring compositional similarity between nounmodifier phrases and unigrams (multiplechoice paraphrase questions).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 Task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>385--393</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="7659" citStr="Agirre et al., 2012" startWordPosition="1163" endWordPosition="1166">ional similarity between the given bigram and unigram. We review related work in Section 2. The general feature space for learning relations and compositions is presented in Section 3. The experiments with relational similarity are described in Section 4, and Section 5 reports the results with compositional similarity. Section 6 discusses the implications of the results. We consider future work in Section 7 and conclude in Section 8. 2 Related Work In SemEval 2012, Task 2 was concerned with measuring the degree of relational similarity between two word pairs (Jurgens et al., 2012) and Task 6 (Agirre et al., 2012) examined the degree of semantic equivalence between two sentences. These two areas of research have been mostly independent, although Socher et al. (2012) and Turney (2012) present unified perspectives on the two tasks. We first discuss some work on relational similarity, then some work on compositional similarity, and lastly work that unifies the two types of similarity. 354 2.1 Relational Similarity LRA (latent relational analysis) measures relational similarity with a pair–pattern matrix (Turney, 2006b). Rows in the matrix correspond to word pairs (a, b) and columns correspond to patterns </context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 Task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM), pages 385–393, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>Prodromos Malakasiotis</author>
</authors>
<title>A survey of paraphrasing and textual entailment methods.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>38--135</pages>
<contexts>
<context position="2281" citStr="Androutsopoulos and Malakasiotis, 2010" startWordPosition="327" endWordPosition="330">hly effective for measuring the semantic similarity between individual words. On a set of eighty multiple-choice synonym questions from the test of English as a foreign language (TOEFL), a distributional approach recently achieved 100% accuracy (Bullinaria and Levy, 2012). However, it has been difficult to extend distributional semantics beyond individual words, to word pairs, phrases, and sentences. Moving beyond individual words, there are various types of semantic similarity to consider. Here we focus on paraphrase and analogy. Paraphrase is similarity in the meaning of two pieces of text (Androutsopoulos and Malakasiotis, 2010). Analogy is similarity in the semantic relations of two sets of words (Turney, 2008a). It is common to study paraphrase at the sentence level (Androutsopoulos and Malakasiotis, 2010), but we prefer to concentrate on the simplest type of paraphrase, where a bigram paraphrases a unigram. For example, dog house is a paraphrase of kennel. In our experiments, we concentrate on noun-modifier bigrams and noun unigrams. Analogies map terms in one domain to terms in another domain (Gentner, 1983). The familiar analogy between the solar system and the RutherfordBohr atomic model involves several terms </context>
</contexts>
<marker>Androutsopoulos, Malakasiotis, 2010</marker>
<rawString>Ion Androutsopoulos and Prodromos Malakasiotis. 2010. A survey of paraphrasing and textual entailment methods. Journal of Artificial Intelligence Research, 38:135–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>1183--1193</pages>
<contexts>
<context position="3918" citStr="Baroni and Zamparelli, 2010" startWordPosition="586" endWordPosition="589">2013) 353–366. Action Editor: Patrick Pantel. Submitted 5/2013; Revised 7/2013; Published 10/2013. c�2013 Association for Computational Linguistics. is no longer plain. The semantic relations between cook and raw are similar to the semantic relations between decorate and plain. In the following experiments, we focus on proportional analogies. Erk (2013) distinguished four approaches to extend distributional semantics beyond words: In the first, a single vector space representation for a phrase or sentence is computed from the representations of the individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). In the second, two phrases or sentences are compared by combining multiple pairwise similarity values (Socher et al., 2011; Turney, 2012). Third, weighted inference rules integrate distributional similarity and formal logic (Garrette et al., 2011). Fourth, a single space integrates formal logic and vectors (Clarke, 2012). Taking the second approach, Turney (2012) introduced a dual-space model, with one space for measuring domain similarity (similarity of topic or field) and another for function similarity (similarity of role or usage). Similarities beyond individual words are calculated by f</context>
<context position="11063" citStr="Baroni and Zamparelli, 2010" startWordPosition="1707" endWordPosition="1710">ltiplication performs poorly (Turney, 2012). We can treat the bigram ab as a unit, as if it were a single word, and construct a context vector for ab from occurrences of ab in a large corpus. This holistic approach to representing bigrams performs well when a limited set of bigrams is specified in advance (before building the word–context matrix), but it does not scale up, because there are too many possible bigrams (Turney, 2012). Although the holistic approach does not scale up, we can generate a few holistic bigram vectors and use them to train a supervised regression model (Guevara, 2010; Baroni and Zamparelli, 2010). Given a new bigram cd, not observed in the corpus, the regression model can predict a holistic vector for cd, if c and d have been observed separately. We show in Section 5 that this idea can be adapted to train SuperSim without manually labeled data. Socher et al. (2011) take the second approach described by Erk (2013), in which two sentences are compared by combining multiple pairwise similarity values. They construct a variable-sized similarity matrix X, in which the element xij is the similarity between the i-th phrase of one sentence and the j-th phrase of the other. Since supervised le</context>
<context position="37231" citStr="Baroni and Zamparelli, 2010" startWordPosition="6161" endWordPosition="6164">ng questions to learn to recognize when a bigram is a paraphrase of a unigram; it learns from expert knowledge implicit in WordNet 361 Stem: search engine Choices: (1) search engine (2) search (3) engine (4) search language (5) search warrant (6) diesel engine (7) steam engine Solution: (1) search engine Table 11: A question based on holistic vectors. synsets. It would be advantageous to be able to train SuperSim with less reliance on expert knowledge. Past work with adjective-noun bigrams has shown that we can use holistic bigram vectors to train a supervised regression model (Guevara, 2010; Baroni and Zamparelli, 2010). The output of the regression model is a vector representation for a bigram that approximates the holistic vector for the bigram; that is, it approximates the vector we would get by treating the bigram as if it were a unigram. SuperSim does not generate vectors as output, but we can still use holistic bigram vectors for training. Table 11 shows a seven-choice training question that was generated without using WordNet synsets. The choices of the form a b are bigrams, but we represent them with holistic bigram vectors; we pretend they are unigrams. We call a b bigrams pseudounigrams. As far as </context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pages 1183–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Ngoc-Quynh Do</author>
<author>Chung-chieh Shan</author>
</authors>
<title>Entailment above the word level in distributional semantics.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<pages>23--32</pages>
<contexts>
<context position="43090" citStr="Baroni et al. (2012)" startWordPosition="7107" endWordPosition="7110">makes it easier to recognize the relational similarity. Another area for future work is extending SuperSim beyond noun-modifier paraphrases to measuring the similarity of sentence pairs. We plan to adapt ideas from Socher et al. (2011) for this task. They use dynamic pooling to represent sentences of varying size with fixed-size feature vectors. Using fixedsize feature vectors avoids the problem of quadratic growth and it enables the supervised learner to generalize over sentences of varying length. Some of the competing approaches discussed by Erk (2013) incorporate formal logic. The work of Baroni et al. (2012) suggests ways that SuperSim could be developed to deal with logic. We believe that SuperSim could benefit from more features, with greater diversity. One place to look for these features is higher levels in the hierarchy that we sketch in Section 3. Our ablation experiments suggest that domain and function spaces provide the most important features for relational similarity, but PPMI values provide the most important features for noun-modifier compositional similarity. Explaining this is another topic for future research. 8 Conclusion In this paper, we have presented SuperSim, a unified appro</context>
</contexts>
<marker>Baroni, Bernardi, Do, Shan, 2012</marker>
<rawString>Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012), pages 23–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac I Bejar</author>
<author>Roger Chaffin</author>
<author>Susan E Embretson</author>
</authors>
<title>Cognitive and Psychometric Analysis of Analogical Problem Solving.</title>
<date>1991</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="28795" citStr="Bejar et al. (1991)" startWordPosition="4771" endWordPosition="4774">d-coding. The results show that SuperSim can avoid the asymmetric distractors. Table 5 shows the impact of different subsets of features on the percentage of correct answers to the ten-choice SAT questions. Included features are marked 1 and ablated features are marked 0. The results show that the log frequency (LF) and PPMI features are not helpful (but also not harmful) for relational similarity. We also see that domain space and function space are both needed for good results. 4.3 SemEval 2012 Task 2 The SemEval 2012 Task 2 dataset is based on the semantic relation classification scheme of Bejar et al. (1991), consisting of ten high-level categories of relations and seventy-nine subcategories, with paradigmatic examples of each subcategory. For instance, the subcategory taxonomic in the category class inclusion has three paradigmatic examples, flower:tulip, emotion:rage, and poem:sonnet. Jurgens et al. (2012) used Amazon’s Mechanical Turk to create the SemEval 2012 Task 2 dataset in two phases. In the first phase, Turkers expanded the paradigmatic examples for each subcategory to an 359 Algorithm Reference Spearman BUAP Tovar et al. (2012) 0.014 Duluth-V2 Pedersen (2012) 0.038 Duluth-V1 Pedersen (</context>
</contexts>
<marker>Bejar, Chaffin, Embretson, 1991</marker>
<rawString>Isaac I. Bejar, Roger Chaffin, and Susan E. Embretson. 1991. Cognitive and Psychometric Analysis of Analogical Problem Solving. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>Clustering word pairs to answer analogy questions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifteenth Turkish Symposium on Artificial Intelligence and Neural Networks (TAINN</booktitle>
<location>Akyaka, Mugla, Turkey.</location>
<marker>Bic¸ici, Yuret, 2006</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2006. Clustering word pairs to answer analogy questions. In Proceedings of the Fifteenth Turkish Symposium on Artificial Intelligence and Neural Networks (TAINN 2006), Akyaka, Mugla, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>WWW sits the SAT: Measuring relational similarity on the Web.</title>
<date>2008</date>
<booktitle>In Proceedings of the 18th European Conference on Artificial Intelligence (ECAI</booktitle>
<pages>333--337</pages>
<location>Patras, Greece.</location>
<contexts>
<context position="8930" citStr="Bollegala et al. (2008)" startWordPosition="1363" endWordPosition="1366">e corpus. This is a holistic (noncompositional) approach to distributional similarity, since the word pairs are opaque wholes; the component words have no separate representations. A compositional approach to analogy has a representation for each word, and a word pair is represented by composing the representations for each member of the pair. Given a vocabulary of N words, a compositional approach requires N representations to handle all possible word pairs, but a holistic approach requires N2 representations. Holistic approaches do not scale up (Turney, 2012). LRA required nine days to run. Bollegala et al. (2008) answered the SAT analogy questions with a support vector machine trained on quadruples (proportional analogies), as we do here. However, their feature vectors are holistic, and hence there are scaling problems. Herda˘gdelen and Baroni (2009) used a support vector machine to learn relational similarity. Their feature vectors contained a combination of holistic and compositional features. Measuring relational similarity is closely connected to classifying word pairs according to their semantic relations (Turney and Littman, 2005). Semantic relation classification was the focus of SemEval 2007 T</context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2008</marker>
<rawString>Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2008. WWW sits the SAT: Measuring relational similarity on the Web. In Proceedings of the 18th European Conference on Artificial Intelligence (ECAI 2008), pages 333–337, Patras, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Measuring the similarity between implicit semantic relations from the Web.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th International Conference on World Wide Web (WWW</booktitle>
<pages>651--660</pages>
<contexts>
<context position="25591" citStr="Bollegala et al. (2009)" startWordPosition="4215" endWordPosition="4218"> to 57.0% are not significantly different from SuperSim’s score of 54.8%, according to Fisher’s exact test at the 95% confidence level. However, SuperSim answers the SAT questions in a few minutes, whereas LRA requires nine days, and SuperSim learns its models automatically, unlike the hand-coding of Turney (2012). 6See the State of the Art page on the ACL Wiki at http: //aclweb.org/aclwiki. 358 Algorithm Reference Correct Know-Best Veale (2004) 43.0 k-means Bic¸ici &amp; Yuret (2006) 44.0 BagPack Herda˘gdelen &amp; Baroni (2009) 44.1 VSM Turney &amp; Littman (2005) 47.1 Dual-Space Turney (2012) 51.1 BMI Bollegala et al. (2009) 51.1 PairClass Turney (2008b) 52.1 PERT Turney (2006a) 53.5 SuperSim — 54.8 LRA Turney (2006b) 56.1 Human Average college applicant 57.0 Table 4: The top ten results on five-choice SAT questions. Features Algorithm LF PPMI Dom Fun Correct Dual-Space 0 0 1 1 47.9 SuperSim 1 1 1 1 52.7 SuperSim 0 1 1 1 52.7 SuperSim 1 0 1 1 52.7 SuperSim 1 1 0 1 45.7 SuperSim 1 1 1 0 41.7 SuperSim 1 0 0 0 5.6 SuperSim 0 1 0 0 32.4 SuperSim 0 0 1 0 39.6 SuperSim 0 0 0 1 39.3 Table 5: Feature ablation with ten-choice SAT questions. 4.2 Ten-choice SAT Questions In addition to symmetries, proportional analogies hav</context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2009</marker>
<rawString>Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2009. Measuring the similarity between implicit semantic relations from the Web. In Proceedings of the 18th International Conference on World Wide Web (WWW 2009), pages 651–660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bullinaria</author>
<author>Joseph Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="13705" citStr="Bullinaria and Levy, 2007" startWordPosition="2138" endWordPosition="2141"> grid search algorithm. The grid search algorithm was not suitable for integration with a supervised learning algorithm. The insight behind SuperSim is that, given appropriate features, a supervised learning algorithm can replace the grid search algorithm and the hand-crafted functions. 3 Features for Tuple Classification We represent a tuple with four types of features, all based on frequencies in a large corpus. The first type of feature is the logarithm of the frequency of a word. The second type is the positive pointwise mutual information (PPMI) between two words (Church and Hanks, 1989; Bullinaria and Levy, 2007). Third and fourth are the similarities of two words in domain and function space (Turney, 2012). In the following experiments, we use the PPMI matrix from Turney et al. (2011) and the domain and function matrices from Turney (2012).1 The three matrices and the word frequency data are based on the same corpus, a collection of web pages gathered from university web sites, containing 5 × 1010 words.2 All three matrices are word–context matrices, in which the rows correspond to terms (words and phrases) in WordNet.3 The columns correspond to the contexts in which the terms appear; each matrix inv</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John Bullinaria and Joseph Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39(3):510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bullinaria</author>
<author>Joseph Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: Stop-lists, stemming, and SVD.</title>
<date>2012</date>
<journal>Behavior Research Methods,</journal>
<volume>44</volume>
<issue>3</issue>
<contexts>
<context position="1914" citStr="Bullinaria and Levy, 2012" startWordPosition="273" endWordPosition="276">r contexts tend to have similar meanings. This hypothesis is the foundation for distributional semantics, in which words are represented by context vectors. The similarity of two words is calculated by comparing the two corresponding context vectors (Lund et al., 1995; Landauer and Dumais, 1997; Turney and Pantel, 2010). Distributional semantics is highly effective for measuring the semantic similarity between individual words. On a set of eighty multiple-choice synonym questions from the test of English as a foreign language (TOEFL), a distributional approach recently achieved 100% accuracy (Bullinaria and Levy, 2012). However, it has been difficult to extend distributional semantics beyond individual words, to word pairs, phrases, and sentences. Moving beyond individual words, there are various types of semantic similarity to consider. Here we focus on paraphrase and analogy. Paraphrase is similarity in the meaning of two pieces of text (Androutsopoulos and Malakasiotis, 2010). Analogy is similarity in the semantic relations of two sets of words (Turney, 2008a). It is common to study paraphrase at the sentence level (Androutsopoulos and Malakasiotis, 2010), but we prefer to concentrate on the simplest typ</context>
<context position="19206" citStr="Bullinaria and Levy, 2012" startWordPosition="3112" endWordPosition="3115">y deleting the columns in U and E corresponding to the smallest singular values. The parameter p raises the singular values in Ek to the power p (Caron, 2001). As p goes from one to zero, factors with smaller singular values are given more weight. This has the effect of making the similarity measure more discriminating (Turney, 2012). The similarity of two words in domain space, Dom(xi, xj, k, p), is computed by extracting the row vectors in UkEpk that correspond to the words xi and xj, and then calculating their cosine. Optimal performance requires tuning the parameters k and p for the task (Bullinaria and Levy, 2012; Turney, 2012). In the following experiments, we avoid directly tuning k and p by generating features with a variety of values for k and p, allowing the supervised learning algorithm to decide which features to use. Feature set Size of set LF(xi) n PPMI(xi, xj, handedness) 2n(n − 1) Dom(xi, xj, k, p) 2n(n − 1)nknp 1 Fun(xi, xj, k, p) 2n(n − 1)nknp 1 Table 1: The four sets of features and their sizes. From the n-tuple, we select all 12n(n − 1) pairs, (xi, xj), such that i &lt; j. For each pair, we generate domain similarity features, Dom(xi, xj, k, p), where k varies from 100 to 1000 in steps of </context>
</contexts>
<marker>Bullinaria, Levy, 2012</marker>
<rawString>John Bullinaria and Joseph Levy. 2012. Extracting semantic representations from word co-occurrence statistics: Stop-lists, stemming, and SVD. Behavior Research Methods, 44(3):890–907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Caron</author>
</authors>
<title>Experiments with LSA scoring: Optimal rank and basis.</title>
<date>2001</date>
<booktitle>In Proceedings of the SIAM Computational Information Retrieval Workshop,</booktitle>
<pages>157--169</pages>
<location>Raleigh, NC.</location>
<contexts>
<context position="18739" citStr="Caron, 2001" startWordPosition="3034" endWordPosition="3035"> density of 2.6%. The frequency matrix was converted to a PPMI matrix and then smoothed with SVD. The SVD yields three matrices, U, E, and V. A term in domain space is represented by a row vector in UkEpk. The parameter k specifies the number of singular values in the truncated singular value decomposition; that is, k is the number of latent factors in the low-dimensional representation of the term (Landauer and Dumais, 1997). We generate Uk and Ek by deleting the columns in U and E corresponding to the smallest singular values. The parameter p raises the singular values in Ek to the power p (Caron, 2001). As p goes from one to zero, factors with smaller singular values are given more weight. This has the effect of making the similarity measure more discriminating (Turney, 2012). The similarity of two words in domain space, Dom(xi, xj, k, p), is computed by extracting the row vectors in UkEpk that correspond to the words xi and xj, and then calculating their cosine. Optimal performance requires tuning the parameters k and p for the task (Bullinaria and Levy, 2012; Turney, 2012). In the following experiments, we avoid directly tuning k and p by generating features with a variety of values for k</context>
</contexts>
<marker>Caron, 2001</marker>
<rawString>John Caron. 2001. Experiments with LSA scoring: Optimal rank and basis. In Proceedings of the SIAM Computational Information Retrieval Workshop, pages 157–169, Raleigh, NC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Conference of the Association of Computational Linguistics,</booktitle>
<pages>76--83</pages>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="13677" citStr="Church and Hanks, 1989" startWordPosition="2134" endWordPosition="2137">tuned using a customized grid search algorithm. The grid search algorithm was not suitable for integration with a supervised learning algorithm. The insight behind SuperSim is that, given appropriate features, a supervised learning algorithm can replace the grid search algorithm and the hand-crafted functions. 3 Features for Tuple Classification We represent a tuple with four types of features, all based on frequencies in a large corpus. The first type of feature is the logarithm of the frequency of a word. The second type is the positive pointwise mutual information (PPMI) between two words (Church and Hanks, 1989; Bullinaria and Levy, 2007). Third and fourth are the similarities of two words in domain and function space (Turney, 2012). In the following experiments, we use the PPMI matrix from Turney et al. (2011) and the domain and function matrices from Turney (2012).1 The three matrices and the word frequency data are based on the same corpus, a collection of web pages gathered from university web sites, containing 5 × 1010 words.2 All three matrices are word–context matrices, in which the rows correspond to terms (words and phrases) in WordNet.3 The columns correspond to the contexts in which the t</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Kenneth Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography. In Proceedings of the 27th Annual Conference of the Association of Computational Linguistics, pages 76– 83, Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>A context-theoretic framework for compositionality in distributional semantics.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="4242" citStr="Clarke, 2012" startWordPosition="636" endWordPosition="637">gies. Erk (2013) distinguished four approaches to extend distributional semantics beyond words: In the first, a single vector space representation for a phrase or sentence is computed from the representations of the individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). In the second, two phrases or sentences are compared by combining multiple pairwise similarity values (Socher et al., 2011; Turney, 2012). Third, weighted inference rules integrate distributional similarity and formal logic (Garrette et al., 2011). Fourth, a single space integrates formal logic and vectors (Clarke, 2012). Taking the second approach, Turney (2012) introduced a dual-space model, with one space for measuring domain similarity (similarity of topic or field) and another for function similarity (similarity of role or usage). Similarities beyond individual words are calculated by functions that combine domain and function similarities of component words. The dual-space model has been applied to measuring compositional similarity (paraphrase recognition) and relational similarity (analogy recognition). In experiments that tested for sensitivity to word order, the dual-space model performed significan</context>
</contexts>
<marker>Clarke, 2012</marker>
<rawString>Daoud Clarke. 2012. A context-theoretic framework for compositionality in distributional semantics. Computational Linguistics, 38(1):41–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Towards a semantics for distributional representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013),</booktitle>
<location>Potsdam, Germany.</location>
<contexts>
<context position="3645" citStr="Erk (2013)" startWordPosition="547" endWordPosition="548">es two pairs of words (Turney, 2006b). For example, the pair (cook, raw) is analogous to the pair (decorate, plain). If we cook a thing, it is no longer raw; if we decorate a thing, it 353 Transactions of the Association for Computational Linguistics, 1 (2013) 353–366. Action Editor: Patrick Pantel. Submitted 5/2013; Revised 7/2013; Published 10/2013. c�2013 Association for Computational Linguistics. is no longer plain. The semantic relations between cook and raw are similar to the semantic relations between decorate and plain. In the following experiments, we focus on proportional analogies. Erk (2013) distinguished four approaches to extend distributional semantics beyond words: In the first, a single vector space representation for a phrase or sentence is computed from the representations of the individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). In the second, two phrases or sentences are compared by combining multiple pairwise similarity values (Socher et al., 2011; Turney, 2012). Third, weighted inference rules integrate distributional similarity and formal logic (Garrette et al., 2011). Fourth, a single space integrates formal logic and vectors (Clarke, 2012). T</context>
<context position="9749" citStr="Erk (2013)" startWordPosition="1484" endWordPosition="1485">s. Herda˘gdelen and Baroni (2009) used a support vector machine to learn relational similarity. Their feature vectors contained a combination of holistic and compositional features. Measuring relational similarity is closely connected to classifying word pairs according to their semantic relations (Turney and Littman, 2005). Semantic relation classification was the focus of SemEval 2007 Task 4 (Girju et al., 2007) and SemEval 2010 Task 8 (Hendrickx et al., 2010). 2.2 Compositional Similarity To extend distributional semantics beyond words, many researchers take the first approach described by Erk (2013), in which a single vector space is used for individual words, phrases, and sentences (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). In this approach, given the words a and b with context vectors a and b, we construct a vector for the bigram ab by applying vector operations to a and b. Mitchell and Lapata (2010) experiment with many different vector operations and find that element-wise multiplication performs well. The bigram ab is represented by c = a O b, where ci = ai · bi. However, element-wise multiplication is commutative, so the bigrams ab and ba map</context>
<context position="11386" citStr="Erk (2013)" startWordPosition="1767" endWordPosition="1768">matrix), but it does not scale up, because there are too many possible bigrams (Turney, 2012). Although the holistic approach does not scale up, we can generate a few holistic bigram vectors and use them to train a supervised regression model (Guevara, 2010; Baroni and Zamparelli, 2010). Given a new bigram cd, not observed in the corpus, the regression model can predict a holistic vector for cd, if c and d have been observed separately. We show in Section 5 that this idea can be adapted to train SuperSim without manually labeled data. Socher et al. (2011) take the second approach described by Erk (2013), in which two sentences are compared by combining multiple pairwise similarity values. They construct a variable-sized similarity matrix X, in which the element xij is the similarity between the i-th phrase of one sentence and the j-th phrase of the other. Since supervised learning is simpler with fixed-sized feature vectors, the variable-sized similarity matrix is then reduced to a smaller fixed-sized matrix, to allow comparison of pairs of sentences of varying lengths. 2.3 Unified Perspectives on Similarity Socher et al. (2012) represent words and phrases with a pair, consisting of a vector</context>
<context position="43031" citStr="Erk (2013)" startWordPosition="7099" endWordPosition="7100">ker is to stone as wood worker is to wood, which makes it easier to recognize the relational similarity. Another area for future work is extending SuperSim beyond noun-modifier paraphrases to measuring the similarity of sentence pairs. We plan to adapt ideas from Socher et al. (2011) for this task. They use dynamic pooling to represent sentences of varying size with fixed-size feature vectors. Using fixedsize feature vectors avoids the problem of quadratic growth and it enables the supervised learner to generalize over sentences of varying length. Some of the competing approaches discussed by Erk (2013) incorporate formal logic. The work of Baroni et al. (2012) suggests ways that SuperSim could be developed to deal with logic. We believe that SuperSim could benefit from more features, with greater diversity. One place to look for these features is higher levels in the hierarchy that we sketch in Section 3. Our ablation experiments suggest that domain and function spaces provide the most important features for relational similarity, but PPMI values provide the most important features for noun-modifier compositional similarity. Explaining this is another topic for future research. 8 Conclusion</context>
</contexts>
<marker>Erk, 2013</marker>
<rawString>Katrin Erk. 2013. Towards a semantics for distributional representations. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013), Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Rupert Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930–1955.</title>
<date>1957</date>
<booktitle>In Studies in Linguistic Analysis,</booktitle>
<pages>1--32</pages>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="1242" citStr="Firth (1957)" startWordPosition="172" endWordPosition="173">trength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval 2012 Task 2) and measuring compositional similarity between nounmodifier phrases and unigrams (multiplechoice paraphrase questions). 1 Introduction Harris (1954) and Firth (1957) hypothesized that words that appear in similar contexts tend to have similar meanings. This hypothesis is the foundation for distributional semantics, in which words are represented by context vectors. The similarity of two words is calculated by comparing the two corresponding context vectors (Lund et al., 1995; Landauer and Dumais, 1997; Turney and Pantel, 2010). Distributional semantics is highly effective for measuring the semantic similarity between individual words. On a set of eighty multiple-choice synonym questions from the test of English as a foreign language (TOEFL), a distributio</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John Rupert Firth. 1957. A synopsis of linguistic theory 1930–1955. In Studies in Linguistic Analysis, pages 1–32. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Ray Mooney</author>
</authors>
<title>Integrating logical representations with probabilistic information using markov logic.</title>
<date>2011</date>
<booktitle>In Proceedings of the 9th International Conference on Computational Semantics (IWCS</booktitle>
<pages>105--114</pages>
<contexts>
<context position="4167" citStr="Garrette et al., 2011" startWordPosition="623" endWordPosition="626">een decorate and plain. In the following experiments, we focus on proportional analogies. Erk (2013) distinguished four approaches to extend distributional semantics beyond words: In the first, a single vector space representation for a phrase or sentence is computed from the representations of the individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). In the second, two phrases or sentences are compared by combining multiple pairwise similarity values (Socher et al., 2011; Turney, 2012). Third, weighted inference rules integrate distributional similarity and formal logic (Garrette et al., 2011). Fourth, a single space integrates formal logic and vectors (Clarke, 2012). Taking the second approach, Turney (2012) introduced a dual-space model, with one space for measuring domain similarity (similarity of topic or field) and another for function similarity (similarity of role or usage). Similarities beyond individual words are calculated by functions that combine domain and function similarities of component words. The dual-space model has been applied to measuring compositional similarity (paraphrase recognition) and relational similarity (analogy recognition). In experiments that test</context>
</contexts>
<marker>Garrette, Erk, Mooney, 2011</marker>
<rawString>Dan Garrette, Katrin Erk, and Ray Mooney. 2011. Integrating logical representations with probabilistic information using markov logic. In Proceedings of the 9th International Conference on Computational Semantics (IWCS 2011), pages 105–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dedre Gentner</author>
</authors>
<title>Structure-mapping: A theoretical framework for analogy.</title>
<date>1983</date>
<journal>Cognitive Science,</journal>
<volume>7</volume>
<issue>2</issue>
<pages>170</pages>
<contexts>
<context position="2774" citStr="Gentner, 1983" startWordPosition="408" endWordPosition="409">hrase and analogy. Paraphrase is similarity in the meaning of two pieces of text (Androutsopoulos and Malakasiotis, 2010). Analogy is similarity in the semantic relations of two sets of words (Turney, 2008a). It is common to study paraphrase at the sentence level (Androutsopoulos and Malakasiotis, 2010), but we prefer to concentrate on the simplest type of paraphrase, where a bigram paraphrases a unigram. For example, dog house is a paraphrase of kennel. In our experiments, we concentrate on noun-modifier bigrams and noun unigrams. Analogies map terms in one domain to terms in another domain (Gentner, 1983). The familiar analogy between the solar system and the RutherfordBohr atomic model involves several terms from the domain of the solar system and the domain of the atomic model (Turney, 2008a). The simplest type of analogy is proportional analogy, which involves two pairs of words (Turney, 2006b). For example, the pair (cook, raw) is analogous to the pair (decorate, plain). If we cook a thing, it is no longer raw; if we decorate a thing, it 353 Transactions of the Association for Computational Linguistics, 1 (2013) 353–366. Action Editor: Patrick Pantel. Submitted 5/2013; Revised 7/2013; Publ</context>
</contexts>
<marker>Gentner, 1983</marker>
<rawString>Dedre Gentner. 1983. Structure-mapping: A theoretical framework for analogy. Cognitive Science, 7(2):155– 170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>Semeval-2007 task 04: Classification of semantic relations between nominals.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval</booktitle>
<pages>13--18</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9556" citStr="Girju et al., 2007" startWordPosition="1454" endWordPosition="1457">ed the SAT analogy questions with a support vector machine trained on quadruples (proportional analogies), as we do here. However, their feature vectors are holistic, and hence there are scaling problems. Herda˘gdelen and Baroni (2009) used a support vector machine to learn relational similarity. Their feature vectors contained a combination of holistic and compositional features. Measuring relational similarity is closely connected to classifying word pairs according to their semantic relations (Turney and Littman, 2005). Semantic relation classification was the focus of SemEval 2007 Task 4 (Girju et al., 2007) and SemEval 2010 Task 8 (Hendrickx et al., 2010). 2.2 Compositional Similarity To extend distributional semantics beyond words, many researchers take the first approach described by Erk (2013), in which a single vector space is used for individual words, phrases, and sentences (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). In this approach, given the words a and b with context vectors a and b, we construct a vector for the bigram ab by applying vector operations to a and b. Mitchell and Lapata (2010) experiment with many different vector operations and find</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. Semeval-2007 task 04: Classification of semantic relations between nominals. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval 2007), pages 13–18, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics (GEMS</booktitle>
<pages>33--37</pages>
<contexts>
<context position="11033" citStr="Guevara, 2010" startWordPosition="1705" endWordPosition="1706">element-wise multiplication performs poorly (Turney, 2012). We can treat the bigram ab as a unit, as if it were a single word, and construct a context vector for ab from occurrences of ab in a large corpus. This holistic approach to representing bigrams performs well when a limited set of bigrams is specified in advance (before building the word–context matrix), but it does not scale up, because there are too many possible bigrams (Turney, 2012). Although the holistic approach does not scale up, we can generate a few holistic bigram vectors and use them to train a supervised regression model (Guevara, 2010; Baroni and Zamparelli, 2010). Given a new bigram cd, not observed in the corpus, the regression model can predict a holistic vector for cd, if c and d have been observed separately. We show in Section 5 that this idea can be adapted to train SuperSim without manually labeled data. Socher et al. (2011) take the second approach described by Erk (2013), in which two sentences are compared by combining multiple pairwise similarity values. They construct a variable-sized similarity matrix X, in which the element xij is the similarity between the i-th phrase of one sentence and the j-th phrase of </context>
<context position="37201" citStr="Guevara, 2010" startWordPosition="6159" endWordPosition="6160">uses 680 training questions to learn to recognize when a bigram is a paraphrase of a unigram; it learns from expert knowledge implicit in WordNet 361 Stem: search engine Choices: (1) search engine (2) search (3) engine (4) search language (5) search warrant (6) diesel engine (7) steam engine Solution: (1) search engine Table 11: A question based on holistic vectors. synsets. It would be advantageous to be able to train SuperSim with less reliance on expert knowledge. Past work with adjective-noun bigrams has shown that we can use holistic bigram vectors to train a supervised regression model (Guevara, 2010; Baroni and Zamparelli, 2010). The output of the regression model is a vector representation for a bigram that approximates the holistic vector for the bigram; that is, it approximates the vector we would get by treating the bigram as if it were a unigram. SuperSim does not generate vectors as output, but we can still use holistic bigram vectors for training. Table 11 shows a seven-choice training question that was generated without using WordNet synsets. The choices of the form a b are bigrams, but we represent them with holistic bigram vectors; we pretend they are unigrams. We call a b bigr</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics (GEMS 2010), pages 33–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="1225" citStr="Harris (1954)" startWordPosition="169" endWordPosition="170">in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval 2012 Task 2) and measuring compositional similarity between nounmodifier phrases and unigrams (multiplechoice paraphrase questions). 1 Introduction Harris (1954) and Firth (1957) hypothesized that words that appear in similar contexts tend to have similar meanings. This hypothesis is the foundation for distributional semantics, in which words are represented by context vectors. The similarity of two words is calculated by comparing the two corresponding context vectors (Lund et al., 1995; Landauer and Dumais, 1997; Turney and Pantel, 2010). Distributional semantics is highly effective for measuring the semantic similarity between individual words. On a set of eighty multiple-choice synonym questions from the test of English as a foreign language (TOEF</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>33--38</pages>
<location>Uppsala,</location>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2010</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 33–38, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amac¸ Herda˘gdelen</author>
<author>Marco Baroni</author>
</authors>
<title>Bagpack: A general framework to represent semantic relations.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL</booktitle>
<pages>33--40</pages>
<marker>Herda˘gdelen, Baroni, 2009</marker>
<rawString>Amac¸ Herda˘gdelen and Marco Baroni. 2009. Bagpack: A general framework to represent semantic relations. In Proceedings of the EACL 2009 Geometrical Models for Natural Language Semantics (GEMS) Workshop, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathalie Japkowicz</author>
<author>Shaju Stephen</author>
</authors>
<title>The class imbalance problem: A systematic study.</title>
<date>2002</date>
<journal>Intelligent Data Analysis,</journal>
<volume>6</volume>
<issue>5</issue>
<contexts>
<context position="24103" citStr="Japkowicz and Stephen, 2002" startWordPosition="3963" endWordPosition="3966">.5 4Weka is available at http://www.cs.waikato.ac. nz/ml/weka/. 5The SAT questions are available on request from the author. The SemEval 2012 Task 2 dataset is available at https:// sites.google.com/site/semeval2012task2/. 4.1 Five-choice SAT Questions Table 3 is an example of a question from the 374 five-choice SAT questions. Each five-choice question yields five labeled quadruples, by combining the stem with each choice. The quadruple (word, language, note, music) is labeled positive and the other four quadruples are labeled negative. Since learning works better with balanced training data (Japkowicz and Stephen, 2002), we use the symmetries of proportional analogies to add more positive examples (Lepage and Shin-ichi, 1996). For each positive quadruple, (a, b, c, d), we add three more positive quadruples, (b, a, d, c), (c, d, a, b), and (d, c, b, a). Thus each five-choice question provides four positive and four negative quadruples. We use ten-fold cross-validation to apply SuperSim to the SAT questions. The folds are constructed so that the eight quadruples from each SAT question are kept together in the same fold. To answer a question in the testing fold, the learned model assigns a probability to each o</context>
</contexts>
<marker>Japkowicz, Stephen, 2002</marker>
<rawString>Nathalie Japkowicz and Shaju Stephen. 2002. The class imbalance problem: A systematic study. Intelligent Data Analysis, 6(5):429–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Jurgens</author>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
<author>Keith J Holyoak</author>
</authors>
<title>SemEval-2012 Task 2: Measuring degrees of relational similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>356--364</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="7626" citStr="Jurgens et al., 2012" startWordPosition="1156" endWordPosition="1159">erpreted as the degree of compositional similarity between the given bigram and unigram. We review related work in Section 2. The general feature space for learning relations and compositions is presented in Section 3. The experiments with relational similarity are described in Section 4, and Section 5 reports the results with compositional similarity. Section 6 discusses the implications of the results. We consider future work in Section 7 and conclude in Section 8. 2 Related Work In SemEval 2012, Task 2 was concerned with measuring the degree of relational similarity between two word pairs (Jurgens et al., 2012) and Task 6 (Agirre et al., 2012) examined the degree of semantic equivalence between two sentences. These two areas of research have been mostly independent, although Socher et al. (2012) and Turney (2012) present unified perspectives on the two tasks. We first discuss some work on relational similarity, then some work on compositional similarity, and lastly work that unifies the two types of similarity. 354 2.1 Relational Similarity LRA (latent relational analysis) measures relational similarity with a pair–pattern matrix (Turney, 2006b). Rows in the matrix correspond to word pairs (a, b) an</context>
<context position="23475" citStr="Jurgens et al., 2012" startWordPosition="3872" endWordPosition="3875"> outputs of the SVM with logistic regression models. 4 Relational Similarity This section presents experiments with learning relational similarity using SuperSim. The training datasets consist of quadruples that are labeled as positive and negative examples of analogies. Table 2 shows that the feature vectors have 1,348 elements. We experiment with three datasets, a collection of 374 five-choice questions from the SAT college entrance exam (Turney et al., 2003), a modified ten-choice variation of the SAT questions (Turney, 2012), and the relational similarity dataset from SemEval 2012 Task 2 (Jurgens et al., 2012).5 4Weka is available at http://www.cs.waikato.ac. nz/ml/weka/. 5The SAT questions are available on request from the author. The SemEval 2012 Task 2 dataset is available at https:// sites.google.com/site/semeval2012task2/. 4.1 Five-choice SAT Questions Table 3 is an example of a question from the 374 five-choice SAT questions. Each five-choice question yields five labeled quadruples, by combining the stem with each choice. The quadruple (word, language, note, music) is labeled positive and the other four quadruples are labeled negative. Since learning works better with balanced training data (</context>
<context position="29101" citStr="Jurgens et al. (2012)" startWordPosition="4811" endWordPosition="4814">requency (LF) and PPMI features are not helpful (but also not harmful) for relational similarity. We also see that domain space and function space are both needed for good results. 4.3 SemEval 2012 Task 2 The SemEval 2012 Task 2 dataset is based on the semantic relation classification scheme of Bejar et al. (1991), consisting of ten high-level categories of relations and seventy-nine subcategories, with paradigmatic examples of each subcategory. For instance, the subcategory taxonomic in the category class inclusion has three paradigmatic examples, flower:tulip, emotion:rage, and poem:sonnet. Jurgens et al. (2012) used Amazon’s Mechanical Turk to create the SemEval 2012 Task 2 dataset in two phases. In the first phase, Turkers expanded the paradigmatic examples for each subcategory to an 359 Algorithm Reference Spearman BUAP Tovar et al. (2012) 0.014 Duluth-V2 Pedersen (2012) 0.038 Duluth-V1 Pedersen (2012) 0.039 Duluth-V0 Pedersen (2012) 0.050 UTD-SVM Rink &amp; Harabagiu (2012) 0.116 UTD-NB Rink &amp; Harabagiu (2012) 0.229 RNN-1600 Mikolov et al. (2013) 0.275 UTD-LDA Rink &amp; Harabagiu (2013) 0.334 Com Zhila et al. (2013) 0.353 SuperSim — 0.408 Table 6: Spearman correlations for SemEval 2012 Task 2. average o</context>
</contexts>
<marker>Jurgens, Mohammad, Turney, Holyoak, 2012</marker>
<rawString>David A. Jurgens, Saif M. Mohammad, Peter D. Turney, and Keith J. Holyoak. 2012. SemEval-2012 Task 2: Measuring degrees of relational similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM), pages 356– 364, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="1583" citStr="Landauer and Dumais, 1997" startWordPosition="223" endWordPosition="227">state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval 2012 Task 2) and measuring compositional similarity between nounmodifier phrases and unigrams (multiplechoice paraphrase questions). 1 Introduction Harris (1954) and Firth (1957) hypothesized that words that appear in similar contexts tend to have similar meanings. This hypothesis is the foundation for distributional semantics, in which words are represented by context vectors. The similarity of two words is calculated by comparing the two corresponding context vectors (Lund et al., 1995; Landauer and Dumais, 1997; Turney and Pantel, 2010). Distributional semantics is highly effective for measuring the semantic similarity between individual words. On a set of eighty multiple-choice synonym questions from the test of English as a foreign language (TOEFL), a distributional approach recently achieved 100% accuracy (Bullinaria and Levy, 2012). However, it has been difficult to extend distributional semantics beyond individual words, to word pairs, phrases, and sentences. Moving beyond individual words, there are various types of semantic similarity to consider. Here we focus on paraphrase and analogy. Para</context>
<context position="9861" citStr="Landauer and Dumais, 1997" startWordPosition="1500" endWordPosition="1504">heir feature vectors contained a combination of holistic and compositional features. Measuring relational similarity is closely connected to classifying word pairs according to their semantic relations (Turney and Littman, 2005). Semantic relation classification was the focus of SemEval 2007 Task 4 (Girju et al., 2007) and SemEval 2010 Task 8 (Hendrickx et al., 2010). 2.2 Compositional Similarity To extend distributional semantics beyond words, many researchers take the first approach described by Erk (2013), in which a single vector space is used for individual words, phrases, and sentences (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). In this approach, given the words a and b with context vectors a and b, we construct a vector for the bigram ab by applying vector operations to a and b. Mitchell and Lapata (2010) experiment with many different vector operations and find that element-wise multiplication performs well. The bigram ab is represented by c = a O b, where ci = ai · bi. However, element-wise multiplication is commutative, so the bigrams ab and ba map to the same vector c. In experiments that test for order sensitivity, element-wise multiplication performs poor</context>
<context position="18556" citStr="Landauer and Dumais, 1997" startWordPosition="2998" endWordPosition="3001">he nouns near a term characterize the topics associated with the term. The word–context frequency matrix for domain space has 114,297 rows (terms) and 50,000 columns (noun contexts, topics), with a density of 2.6%. The frequency matrix was converted to a PPMI matrix and then smoothed with SVD. The SVD yields three matrices, U, E, and V. A term in domain space is represented by a row vector in UkEpk. The parameter k specifies the number of singular values in the truncated singular value decomposition; that is, k is the number of latent factors in the low-dimensional representation of the term (Landauer and Dumais, 1997). We generate Uk and Ek by deleting the columns in U and E corresponding to the smallest singular values. The parameter p raises the singular values in Ek to the power p (Caron, 2001). As p goes from one to zero, factors with smaller singular values are given more weight. This has the effect of making the similarity measure more discriminating (Turney, 2012). The similarity of two words in domain space, Dom(xi, xj, k, p), is computed by extracting the row vectors in UkEpk that correspond to the words xi and xj, and then calculating their cosine. Optimal performance requires tuning the paramete</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Lemaire</author>
<author>Guy Denhi`ere</author>
</authors>
<title>Effects of high-order co-occurrences on word semantic similarity.</title>
<date>2006</date>
<journal>Current Psychology Letters: Behaviour, Brain &amp; Cognition,</journal>
<volume>18</volume>
<issue>1</issue>
<marker>Lemaire, Denhi`ere, 2006</marker>
<rawString>Benoit Lemaire and Guy Denhi`ere. 2006. Effects of high-order co-occurrences on word semantic similarity. Current Psychology Letters: Behaviour, Brain &amp; Cognition, 18(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lepage</author>
<author>Ando Shin-ichi</author>
</authors>
<title>Saussurian analogy: A theoretical account and its application.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING</booktitle>
<pages>717--722</pages>
<contexts>
<context position="24211" citStr="Lepage and Shin-ichi, 1996" startWordPosition="3979" endWordPosition="3982">rom the author. The SemEval 2012 Task 2 dataset is available at https:// sites.google.com/site/semeval2012task2/. 4.1 Five-choice SAT Questions Table 3 is an example of a question from the 374 five-choice SAT questions. Each five-choice question yields five labeled quadruples, by combining the stem with each choice. The quadruple (word, language, note, music) is labeled positive and the other four quadruples are labeled negative. Since learning works better with balanced training data (Japkowicz and Stephen, 2002), we use the symmetries of proportional analogies to add more positive examples (Lepage and Shin-ichi, 1996). For each positive quadruple, (a, b, c, d), we add three more positive quadruples, (b, a, d, c), (c, d, a, b), and (d, c, b, a). Thus each five-choice question provides four positive and four negative quadruples. We use ten-fold cross-validation to apply SuperSim to the SAT questions. The folds are constructed so that the eight quadruples from each SAT question are kept together in the same fold. To answer a question in the testing fold, the learned model assigns a probability to each of the five choices and guesses the choice with the highest probability. SuperSim achieves a score of 54.8% c</context>
</contexts>
<marker>Lepage, Shin-ichi, 1996</marker>
<rawString>Yves Lepage and Ando Shin-ichi. 1996. Saussurian analogy: A theoretical account and its application. In Proceedings of the 16th International Conference on Computational Linguistics (COLING 1996), pages 717–722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
<author>Ruth Ann Atchley</author>
</authors>
<title>Semantic and associative priming in high-dimensional semantic space.</title>
<date>1995</date>
<booktitle>In Proceedings of the 17th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>660--665</pages>
<contexts>
<context position="1556" citStr="Lund et al., 1995" startWordPosition="219" endWordPosition="222">arning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval 2012 Task 2) and measuring compositional similarity between nounmodifier phrases and unigrams (multiplechoice paraphrase questions). 1 Introduction Harris (1954) and Firth (1957) hypothesized that words that appear in similar contexts tend to have similar meanings. This hypothesis is the foundation for distributional semantics, in which words are represented by context vectors. The similarity of two words is calculated by comparing the two corresponding context vectors (Lund et al., 1995; Landauer and Dumais, 1997; Turney and Pantel, 2010). Distributional semantics is highly effective for measuring the semantic similarity between individual words. On a set of eighty multiple-choice synonym questions from the test of English as a foreign language (TOEFL), a distributional approach recently achieved 100% accuracy (Bullinaria and Levy, 2012). However, it has been difficult to extend distributional semantics beyond individual words, to word pairs, phrases, and sentences. Moving beyond individual words, there are various types of semantic similarity to consider. Here we focus on p</context>
</contexts>
<marker>Lund, Burgess, Atchley, 1995</marker>
<rawString>Kevin Lund, Curt Burgess, and Ruth Ann Atchley. 1995. Semantic and associative priming in high-dimensional semantic space. In Proceedings of the 17th Annual Conference of the Cognitive Science Society, pages 660–665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2013),</booktitle>
<location>Atlanta,</location>
<contexts>
<context position="29544" citStr="Mikolov et al. (2013)" startWordPosition="4879" endWordPosition="4882">egory. For instance, the subcategory taxonomic in the category class inclusion has three paradigmatic examples, flower:tulip, emotion:rage, and poem:sonnet. Jurgens et al. (2012) used Amazon’s Mechanical Turk to create the SemEval 2012 Task 2 dataset in two phases. In the first phase, Turkers expanded the paradigmatic examples for each subcategory to an 359 Algorithm Reference Spearman BUAP Tovar et al. (2012) 0.014 Duluth-V2 Pedersen (2012) 0.038 Duluth-V1 Pedersen (2012) 0.039 Duluth-V0 Pedersen (2012) 0.050 UTD-SVM Rink &amp; Harabagiu (2012) 0.116 UTD-NB Rink &amp; Harabagiu (2012) 0.229 RNN-1600 Mikolov et al. (2013) 0.275 UTD-LDA Rink &amp; Harabagiu (2013) 0.334 Com Zhila et al. (2013) 0.353 SuperSim — 0.408 Table 6: Spearman correlations for SemEval 2012 Task 2. average of forty-one word pairs per subcategory, a total of 3,218 pairs. In the second phase, each word pair from the first phase was assigned a prototypicality score, indicating its similarity to the paradigmatic examples. The challenge of SemEval 2012 Task 2 was to guess the prototypicality scores. SuperSim was trained on the five-choice SAT questions and evaluated on the SemEval 2012 Task 2 test dataset. For a given a word pair, we created quadr</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2013), Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="9888" citStr="Mitchell and Lapata, 2008" startWordPosition="1505" endWordPosition="1508">ned a combination of holistic and compositional features. Measuring relational similarity is closely connected to classifying word pairs according to their semantic relations (Turney and Littman, 2005). Semantic relation classification was the focus of SemEval 2007 Task 4 (Girju et al., 2007) and SemEval 2010 Task 8 (Hendrickx et al., 2010). 2.2 Compositional Similarity To extend distributional semantics beyond words, many researchers take the first approach described by Erk (2013), in which a single vector space is used for individual words, phrases, and sentences (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). In this approach, given the words a and b with context vectors a and b, we construct a vector for the bigram ab by applying vector operations to a and b. Mitchell and Lapata (2010) experiment with many different vector operations and find that element-wise multiplication performs well. The bigram ab is represented by c = a O b, where ci = ai · bi. However, element-wise multiplication is commutative, so the bigrams ab and ba map to the same vector c. In experiments that test for order sensitivity, element-wise multiplication performs poorly (Turney, 2012). We can t</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="3888" citStr="Mitchell and Lapata, 2010" startWordPosition="581" endWordPosition="585">putational Linguistics, 1 (2013) 353–366. Action Editor: Patrick Pantel. Submitted 5/2013; Revised 7/2013; Published 10/2013. c�2013 Association for Computational Linguistics. is no longer plain. The semantic relations between cook and raw are similar to the semantic relations between decorate and plain. In the following experiments, we focus on proportional analogies. Erk (2013) distinguished four approaches to extend distributional semantics beyond words: In the first, a single vector space representation for a phrase or sentence is computed from the representations of the individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). In the second, two phrases or sentences are compared by combining multiple pairwise similarity values (Socher et al., 2011; Turney, 2012). Third, weighted inference rules integrate distributional similarity and formal logic (Garrette et al., 2011). Fourth, a single space integrates formal logic and vectors (Clarke, 2012). Taking the second approach, Turney (2012) introduced a dual-space model, with one space for measuring domain similarity (similarity of topic or field) and another for function similarity (similarity of role or usage). Similarities beyond indivi</context>
<context position="9916" citStr="Mitchell and Lapata, 2010" startWordPosition="1509" endWordPosition="1512">ic and compositional features. Measuring relational similarity is closely connected to classifying word pairs according to their semantic relations (Turney and Littman, 2005). Semantic relation classification was the focus of SemEval 2007 Task 4 (Girju et al., 2007) and SemEval 2010 Task 8 (Hendrickx et al., 2010). 2.2 Compositional Similarity To extend distributional semantics beyond words, many researchers take the first approach described by Erk (2013), in which a single vector space is used for individual words, phrases, and sentences (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). In this approach, given the words a and b with context vectors a and b, we construct a vector for the bigram ab by applying vector operations to a and b. Mitchell and Lapata (2010) experiment with many different vector operations and find that element-wise multiplication performs well. The bigram ab is represented by c = a O b, where ci = ai · bi. However, element-wise multiplication is commutative, so the bigrams ab and ba map to the same vector c. In experiments that test for order sensitivity, element-wise multiplication performs poorly (Turney, 2012). We can treat the bigram ab as a unit</context>
<context position="32231" citStr="Mitchell and Lapata, 2010" startWordPosition="5302" endWordPosition="5305"> WordNet (Turney, 2012). The dataset contains 680 questions for training and 1,500 for testing, a total of 2,180 questions. Table 7 shows one of the questions. The stem is a bigram and the choices are unigrams. The bigram is composed of a head noun (world), modified by an adjective or noun (fantasy). The solution is the unigram (fairyland) that belongs to the same WordNet synset as the stem. The distractors are designed to be difficult for current approaches to composition. For example, iffantasy world is represented by element-wise multiplication of the context vectors for fantasy and world (Mitchell and Lapata, 2010), the most likely guess is fantasy or world, not fairyland (Turney, 2012). Each seven-choice question yields seven labeled triples, by combining the stem with each choice. The triple (fantasy, world, fairyland) is labeled positive and the other six triples are labeled negative. In general, if (a, b, c) is a positive example, then (b, a, c) is negative. For example, world fantasy is not a paraphrase of fairyland. The second dataset is constructed by applying this shuffling transformation to convert the 2,180 seven-choice questions into 2,180 fourteen-choice questions (Turney, 2012). The second </context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Duluth: Measuring degrees of relational similarity with the gloss vector measure of semantic relatedness.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>497--501</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="29368" citStr="Pedersen (2012)" startWordPosition="4855" endWordPosition="4856">assification scheme of Bejar et al. (1991), consisting of ten high-level categories of relations and seventy-nine subcategories, with paradigmatic examples of each subcategory. For instance, the subcategory taxonomic in the category class inclusion has three paradigmatic examples, flower:tulip, emotion:rage, and poem:sonnet. Jurgens et al. (2012) used Amazon’s Mechanical Turk to create the SemEval 2012 Task 2 dataset in two phases. In the first phase, Turkers expanded the paradigmatic examples for each subcategory to an 359 Algorithm Reference Spearman BUAP Tovar et al. (2012) 0.014 Duluth-V2 Pedersen (2012) 0.038 Duluth-V1 Pedersen (2012) 0.039 Duluth-V0 Pedersen (2012) 0.050 UTD-SVM Rink &amp; Harabagiu (2012) 0.116 UTD-NB Rink &amp; Harabagiu (2012) 0.229 RNN-1600 Mikolov et al. (2013) 0.275 UTD-LDA Rink &amp; Harabagiu (2013) 0.334 Com Zhila et al. (2013) 0.353 SuperSim — 0.408 Table 6: Spearman correlations for SemEval 2012 Task 2. average of forty-one word pairs per subcategory, a total of 3,218 pairs. In the second phase, each word pair from the first phase was assigned a prototypicality score, indicating its similarity to the paradigmatic examples. The challenge of SemEval 2012 Task 2 was to guess th</context>
</contexts>
<marker>Pedersen, 2012</marker>
<rawString>Ted Pedersen. 2012. Duluth: Measuring degrees of relational similarity with the gloss vector measure of semantic relatedness. In First Joint Conference on Lexical and Computational Semantics (*SEM), pages 497–501, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Fast training of support vector machines using sequential minimal optimization.</title>
<date>1998</date>
<booktitle>In Advances in Kernel Methods: Support Vector Learning,</booktitle>
<pages>185--208</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6126" citStr="Platt, 1998" startWordPosition="909" endWordPosition="910"> to refer to both contiguous and noncontiguous word sequences. We approach analogy as a problem of supervised tuple classification. To measure the relational similarity between two word pairs, we train SuperSim with quadruples that are labeled as positive and negative examples of analogies. For example, the proportional analogy hcook, raw, decorate, plaini is labeled as a positive example. A quadruple is represented by a feature vector, composed of domain and function similarities from the dual-space model and other features based on corpus frequencies. SuperSim uses a support vector machine (Platt, 1998) to learn the probability that a quadruple ha, b, c, di consists of a word pair ha, bi and an analogous word pair hc, di. The probability can be interpreted as the degree of relational similarity between the two given word pairs. We also approach paraphrase as supervised tuple classification. To measure the compositional similarity beween an m-gram and an n-gram, we train the learning algorithm with (m + n)-tuples that are positive and negative examples of paraphrases. SuperSim learns to estimate the probability that a triple ha, b, ci consists of a compositional bigram ab and a synonymous uni</context>
<context position="22712" citStr="Platt, 1998" startWordPosition="3755" endWordPosition="3756">ally occur together in the corpus. Domain and function space capture indirect or higher-order co-occurrence, due to the truncated SVD (Lemaire and Denhi`ere, 2006); that is, the values of Dom(xi, xj, k, p) and Fun(xi, xj, k, p) can be high even when xi and xj do not actually co-occur in the corpus. We conjecture that there are yet higher orders in this hierarchy that would provide improved similarity measures. SuperSim learns to classify tuples by representing them with these features. SuperSim uses the sequential minimal optimization (SMO) support vector machine (SVM) as implemented in Weka (Platt, 1998; Witten et al., 2011).4 The kernel is a normalized third-order polynomial. Weka provides probability estimates for the classes by fitting the outputs of the SVM with logistic regression models. 4 Relational Similarity This section presents experiments with learning relational similarity using SuperSim. The training datasets consist of quadruples that are labeled as positive and negative examples of analogies. Table 2 shows that the feature vectors have 1,348 elements. We experiment with three datasets, a collection of 374 five-choice questions from the SAT college entrance exam (Turney et al.</context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>John C. Platt. 1998. Fast training of support vector machines using sequential minimal optimization. In Advances in Kernel Methods: Support Vector Learning, pages 185–208, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Rink</author>
<author>Sanda Harabagiu</author>
</authors>
<title>UTD: Determining relational similarity using lexical patterns.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>413--418</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="29470" citStr="Rink &amp; Harabagiu (2012)" startWordPosition="4867" endWordPosition="4870">ns and seventy-nine subcategories, with paradigmatic examples of each subcategory. For instance, the subcategory taxonomic in the category class inclusion has three paradigmatic examples, flower:tulip, emotion:rage, and poem:sonnet. Jurgens et al. (2012) used Amazon’s Mechanical Turk to create the SemEval 2012 Task 2 dataset in two phases. In the first phase, Turkers expanded the paradigmatic examples for each subcategory to an 359 Algorithm Reference Spearman BUAP Tovar et al. (2012) 0.014 Duluth-V2 Pedersen (2012) 0.038 Duluth-V1 Pedersen (2012) 0.039 Duluth-V0 Pedersen (2012) 0.050 UTD-SVM Rink &amp; Harabagiu (2012) 0.116 UTD-NB Rink &amp; Harabagiu (2012) 0.229 RNN-1600 Mikolov et al. (2013) 0.275 UTD-LDA Rink &amp; Harabagiu (2013) 0.334 Com Zhila et al. (2013) 0.353 SuperSim — 0.408 Table 6: Spearman correlations for SemEval 2012 Task 2. average of forty-one word pairs per subcategory, a total of 3,218 pairs. In the second phase, each word pair from the first phase was assigned a prototypicality score, indicating its similarity to the paradigmatic examples. The challenge of SemEval 2012 Task 2 was to guess the prototypicality scores. SuperSim was trained on the five-choice SAT questions and evaluated on the S</context>
</contexts>
<marker>Rink, Harabagiu, 2012</marker>
<rawString>Bryan Rink and Sanda Harabagiu. 2012. UTD: Determining relational similarity using lexical patterns. In First Joint Conference on Lexical and Computational Semantics (*SEM), pages 413–418, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Rink</author>
<author>Sanda Harabagiu</author>
</authors>
<title>The impact of selectional preference agreement on semantic relational similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013),</booktitle>
<location>Potsdam, Germany.</location>
<contexts>
<context position="29582" citStr="Rink &amp; Harabagiu (2013)" startWordPosition="4885" endWordPosition="4888"> taxonomic in the category class inclusion has three paradigmatic examples, flower:tulip, emotion:rage, and poem:sonnet. Jurgens et al. (2012) used Amazon’s Mechanical Turk to create the SemEval 2012 Task 2 dataset in two phases. In the first phase, Turkers expanded the paradigmatic examples for each subcategory to an 359 Algorithm Reference Spearman BUAP Tovar et al. (2012) 0.014 Duluth-V2 Pedersen (2012) 0.038 Duluth-V1 Pedersen (2012) 0.039 Duluth-V0 Pedersen (2012) 0.050 UTD-SVM Rink &amp; Harabagiu (2012) 0.116 UTD-NB Rink &amp; Harabagiu (2012) 0.229 RNN-1600 Mikolov et al. (2013) 0.275 UTD-LDA Rink &amp; Harabagiu (2013) 0.334 Com Zhila et al. (2013) 0.353 SuperSim — 0.408 Table 6: Spearman correlations for SemEval 2012 Task 2. average of forty-one word pairs per subcategory, a total of 3,218 pairs. In the second phase, each word pair from the first phase was assigned a prototypicality score, indicating its similarity to the paradigmatic examples. The challenge of SemEval 2012 Task 2 was to guess the prototypicality scores. SuperSim was trained on the five-choice SAT questions and evaluated on the SemEval 2012 Task 2 test dataset. For a given a word pair, we created quadruples, combining the word pair with ea</context>
</contexts>
<marker>Rink, Harabagiu, 2013</marker>
<rawString>Bryan Rink and Sanda Harabagiu. 2013. The impact of selectional preference agreement on semantic relational similarity. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013), Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS</booktitle>
<pages>801--809</pages>
<contexts>
<context position="4042" citStr="Socher et al., 2011" startWordPosition="607" endWordPosition="610">al Linguistics. is no longer plain. The semantic relations between cook and raw are similar to the semantic relations between decorate and plain. In the following experiments, we focus on proportional analogies. Erk (2013) distinguished four approaches to extend distributional semantics beyond words: In the first, a single vector space representation for a phrase or sentence is computed from the representations of the individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). In the second, two phrases or sentences are compared by combining multiple pairwise similarity values (Socher et al., 2011; Turney, 2012). Third, weighted inference rules integrate distributional similarity and formal logic (Garrette et al., 2011). Fourth, a single space integrates formal logic and vectors (Clarke, 2012). Taking the second approach, Turney (2012) introduced a dual-space model, with one space for measuring domain similarity (similarity of topic or field) and another for function similarity (similarity of role or usage). Similarities beyond individual words are calculated by functions that combine domain and function similarities of component words. The dual-space model has been applied to measurin</context>
<context position="11337" citStr="Socher et al. (2011)" startWordPosition="1757" endWordPosition="1760"> is specified in advance (before building the word–context matrix), but it does not scale up, because there are too many possible bigrams (Turney, 2012). Although the holistic approach does not scale up, we can generate a few holistic bigram vectors and use them to train a supervised regression model (Guevara, 2010; Baroni and Zamparelli, 2010). Given a new bigram cd, not observed in the corpus, the regression model can predict a holistic vector for cd, if c and d have been observed separately. We show in Section 5 that this idea can be adapted to train SuperSim without manually labeled data. Socher et al. (2011) take the second approach described by Erk (2013), in which two sentences are compared by combining multiple pairwise similarity values. They construct a variable-sized similarity matrix X, in which the element xij is the similarity between the i-th phrase of one sentence and the j-th phrase of the other. Since supervised learning is simpler with fixed-sized feature vectors, the variable-sized similarity matrix is then reduced to a smaller fixed-sized matrix, to allow comparison of pairs of sentences of varying lengths. 2.3 Unified Perspectives on Similarity Socher et al. (2012) represent word</context>
<context position="42705" citStr="Socher et al. (2011)" startWordPosition="7045" endWordPosition="7048">ps we can evaluate the quality of a candidate analogy (a, b, c, d) by searching for a term e such that (b, e, a) and (d, e, c) are good paraphrases. For example, consider the analogy mason is to stone as carpenter is to wood. We can paraphrase mason as stone worker and carpenter as wood worker. This transforms the analogy to stone worker is to stone as wood worker is to wood, which makes it easier to recognize the relational similarity. Another area for future work is extending SuperSim beyond noun-modifier paraphrases to measuring the similarity of sentence pairs. We plan to adapt ideas from Socher et al. (2011) for this task. They use dynamic pooling to represent sentences of varying size with fixed-size feature vectors. Using fixedsize feature vectors avoids the problem of quadratic growth and it enables the supervised learner to generalize over sentences of varying length. Some of the competing approaches discussed by Erk (2013) incorporate formal logic. The work of Baroni et al. (2012) suggests ways that SuperSim could be developed to deal with logic. We believe that SuperSim could benefit from more features, with greater diversity. One place to look for these features is higher levels in the hie</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems (NIPS 2011), pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="7814" citStr="Socher et al. (2012)" startWordPosition="1187" endWordPosition="1190">ns is presented in Section 3. The experiments with relational similarity are described in Section 4, and Section 5 reports the results with compositional similarity. Section 6 discusses the implications of the results. We consider future work in Section 7 and conclude in Section 8. 2 Related Work In SemEval 2012, Task 2 was concerned with measuring the degree of relational similarity between two word pairs (Jurgens et al., 2012) and Task 6 (Agirre et al., 2012) examined the degree of semantic equivalence between two sentences. These two areas of research have been mostly independent, although Socher et al. (2012) and Turney (2012) present unified perspectives on the two tasks. We first discuss some work on relational similarity, then some work on compositional similarity, and lastly work that unifies the two types of similarity. 354 2.1 Relational Similarity LRA (latent relational analysis) measures relational similarity with a pair–pattern matrix (Turney, 2006b). Rows in the matrix correspond to word pairs (a, b) and columns correspond to patterns that connect the pairs (“a for the b”) in a large corpus. This is a holistic (noncompositional) approach to distributional similarity, since the word pairs</context>
<context position="11922" citStr="Socher et al. (2012)" startWordPosition="1850" endWordPosition="1853"> labeled data. Socher et al. (2011) take the second approach described by Erk (2013), in which two sentences are compared by combining multiple pairwise similarity values. They construct a variable-sized similarity matrix X, in which the element xij is the similarity between the i-th phrase of one sentence and the j-th phrase of the other. Since supervised learning is simpler with fixed-sized feature vectors, the variable-sized similarity matrix is then reduced to a smaller fixed-sized matrix, to allow comparison of pairs of sentences of varying lengths. 2.3 Unified Perspectives on Similarity Socher et al. (2012) represent words and phrases with a pair, consisting of a vector and a matrix. The vector captures the meaning of the word or phrase and the matrix captures how a word or phrase modifies the meaning of another word or phrase when they are combined. They apply this matrix–vector representation to both compositions and relations. Turney (2012) represents words with two vectors, a vector from domain space and a vector from function space. The domain vector captures the topic or field of the word and the function vector captures the 355 functional role of the word. This dual-space model is applied</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher Manning, and Andrew Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012), pages 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mireya Tovar</author>
<author>J Alejandro Reyes</author>
<author>Azucena Montes</author>
<author>Darnes Vilari˜no</author>
<author>David Pinto</author>
<author>Saul Le´on</author>
</authors>
<title>BUAP: A first approximation to relational similarity measuring.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>502--505</pages>
<location>Montreal, Canada.</location>
<marker>Tovar, Reyes, Montes, Vilari˜no, Pinto, Le´on, 2012</marker>
<rawString>Mireya Tovar, J. Alejandro Reyes, Azucena Montes, Darnes Vilari˜no, David Pinto, and Saul Le´on. 2012. BUAP: A first approximation to relational similarity measuring. In First Joint Conference on Lexical and Computational Semantics (*SEM), pages 502– 505, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Corpusbased learning of analogies and semantic relations.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="9464" citStr="Turney and Littman, 2005" startWordPosition="1439" endWordPosition="1442">ches do not scale up (Turney, 2012). LRA required nine days to run. Bollegala et al. (2008) answered the SAT analogy questions with a support vector machine trained on quadruples (proportional analogies), as we do here. However, their feature vectors are holistic, and hence there are scaling problems. Herda˘gdelen and Baroni (2009) used a support vector machine to learn relational similarity. Their feature vectors contained a combination of holistic and compositional features. Measuring relational similarity is closely connected to classifying word pairs according to their semantic relations (Turney and Littman, 2005). Semantic relation classification was the focus of SemEval 2007 Task 4 (Girju et al., 2007) and SemEval 2010 Task 8 (Hendrickx et al., 2010). 2.2 Compositional Similarity To extend distributional semantics beyond words, many researchers take the first approach described by Erk (2013), in which a single vector space is used for individual words, phrases, and sentences (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). In this approach, given the words a and b with context vectors a and b, we construct a vector for the bigram ab by applying vector operations to a</context>
<context position="25528" citStr="Turney &amp; Littman (2005)" startWordPosition="4205" endWordPosition="4208"> with the SAT analogy questions.6 The scores ranging from 51.1% to 57.0% are not significantly different from SuperSim’s score of 54.8%, according to Fisher’s exact test at the 95% confidence level. However, SuperSim answers the SAT questions in a few minutes, whereas LRA requires nine days, and SuperSim learns its models automatically, unlike the hand-coding of Turney (2012). 6See the State of the Art page on the ACL Wiki at http: //aclweb.org/aclwiki. 358 Algorithm Reference Correct Know-Best Veale (2004) 43.0 k-means Bic¸ici &amp; Yuret (2006) 44.0 BagPack Herda˘gdelen &amp; Baroni (2009) 44.1 VSM Turney &amp; Littman (2005) 47.1 Dual-Space Turney (2012) 51.1 BMI Bollegala et al. (2009) 51.1 PairClass Turney (2008b) 52.1 PERT Turney (2006a) 53.5 SuperSim — 54.8 LRA Turney (2006b) 56.1 Human Average college applicant 57.0 Table 4: The top ten results on five-choice SAT questions. Features Algorithm LF PPMI Dom Fun Correct Dual-Space 0 0 1 1 47.9 SuperSim 1 1 1 1 52.7 SuperSim 0 1 1 1 52.7 SuperSim 1 0 1 1 52.7 SuperSim 1 1 0 1 45.7 SuperSim 1 1 1 0 41.7 SuperSim 1 0 0 0 5.6 SuperSim 0 1 0 0 32.4 SuperSim 0 0 1 0 39.6 SuperSim 0 0 0 1 39.3 Table 5: Feature ablation with ten-choice SAT questions. 4.2 Ten-choice SAT </context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2005. Corpusbased learning of analogies and semantic relations. Machine Learning, 60(1–3):251–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<pages>188</pages>
<contexts>
<context position="1609" citStr="Turney and Pantel, 2010" startWordPosition="228" endWordPosition="231"> measuring relational similarity between word pairs (SAT analogies and SemEval 2012 Task 2) and measuring compositional similarity between nounmodifier phrases and unigrams (multiplechoice paraphrase questions). 1 Introduction Harris (1954) and Firth (1957) hypothesized that words that appear in similar contexts tend to have similar meanings. This hypothesis is the foundation for distributional semantics, in which words are represented by context vectors. The similarity of two words is calculated by comparing the two corresponding context vectors (Lund et al., 1995; Landauer and Dumais, 1997; Turney and Pantel, 2010). Distributional semantics is highly effective for measuring the semantic similarity between individual words. On a set of eighty multiple-choice synonym questions from the test of English as a foreign language (TOEFL), a distributional approach recently achieved 100% accuracy (Bullinaria and Levy, 2012). However, it has been difficult to extend distributional semantics beyond individual words, to word pairs, phrases, and sentences. Moving beyond individual words, there are various types of semantic similarity to consider. Here we focus on paraphrase and analogy. Paraphrase is similarity in th</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141– 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
<author>Jeffrey Bigham</author>
<author>Victor Shnayder</author>
</authors>
<title>Combining independent modules to solve multiple-choice synonym and analogy problems.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-03),</booktitle>
<pages>482--489</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="23319" citStr="Turney et al., 2003" startWordPosition="3846" endWordPosition="3849"> (Platt, 1998; Witten et al., 2011).4 The kernel is a normalized third-order polynomial. Weka provides probability estimates for the classes by fitting the outputs of the SVM with logistic regression models. 4 Relational Similarity This section presents experiments with learning relational similarity using SuperSim. The training datasets consist of quadruples that are labeled as positive and negative examples of analogies. Table 2 shows that the feature vectors have 1,348 elements. We experiment with three datasets, a collection of 374 five-choice questions from the SAT college entrance exam (Turney et al., 2003), a modified ten-choice variation of the SAT questions (Turney, 2012), and the relational similarity dataset from SemEval 2012 Task 2 (Jurgens et al., 2012).5 4Weka is available at http://www.cs.waikato.ac. nz/ml/weka/. 5The SAT questions are available on request from the author. The SemEval 2012 Task 2 dataset is available at https:// sites.google.com/site/semeval2012task2/. 4.1 Five-choice SAT Questions Table 3 is an example of a question from the 374 five-choice SAT questions. Each five-choice question yields five labeled quadruples, by combining the stem with each choice. The quadruple (wo</context>
</contexts>
<marker>Turney, Littman, Bigham, Shnayder, 2003</marker>
<rawString>Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. 2003. Combining independent modules to solve multiple-choice synonym and analogy problems. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-03), pages 482–489, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Yair Neuman</author>
<author>Dan Assaf</author>
<author>Yohai Cohen</author>
</authors>
<title>Literal and metaphorical sense identification through concrete and abstract context.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>680--690</pages>
<contexts>
<context position="13881" citStr="Turney et al. (2011)" startWordPosition="2168" endWordPosition="2171">es, a supervised learning algorithm can replace the grid search algorithm and the hand-crafted functions. 3 Features for Tuple Classification We represent a tuple with four types of features, all based on frequencies in a large corpus. The first type of feature is the logarithm of the frequency of a word. The second type is the positive pointwise mutual information (PPMI) between two words (Church and Hanks, 1989; Bullinaria and Levy, 2007). Third and fourth are the similarities of two words in domain and function space (Turney, 2012). In the following experiments, we use the PPMI matrix from Turney et al. (2011) and the domain and function matrices from Turney (2012).1 The three matrices and the word frequency data are based on the same corpus, a collection of web pages gathered from university web sites, containing 5 × 1010 words.2 All three matrices are word–context matrices, in which the rows correspond to terms (words and phrases) in WordNet.3 The columns correspond to the contexts in which the terms appear; each matrix involves a different kind of context. 1The three matrices and the word frequency data are available on request from the author. The matrix files range from two to five gigabytes w</context>
<context position="15358" citStr="Turney et al. (2011)" startWordPosition="2428" endWordPosition="2431">n n-tuple of words. The number of features we use to represent this tuple increases as a function of n. The first set of features consists of log frequency values for each word xi in the n-tuple. Let freq(xi) be the frequency of xi in the corpus. We define LF(xi) as log(freq(xi)+1). If xi is not in the corpus, freq(xi) is zero, and thus LF(xi) is also zero. There are n log frequency features, one LF(xi) feature for each word in the n-tuple. The second set of features consists of positive pointwise mutual information values for each pair of words in the n-tuple. We use the raw PPMI matrix from Turney et al. (2011). Although they computed the singular value decomposition (SVD) to project the row vectors into a lower-dimensional space, we need the original high-dimensional columns for our features. The raw PPMI matrix has 114,501 rows and 139,246 columns with a density of 1.2%. For each term in WordNet, there is a corresponding row in the raw PPMI matrix. For each unigram in WordNet, there are two corresponding columns in the raw PPMI matrix, one marked left and the other right. Suppose xi corresponds to the i-th row of the PPMI matrix and xj corresponds the j-th column, marked left. The value in the i-t</context>
</contexts>
<marker>Turney, Neuman, Assaf, Cohen, 2011</marker>
<rawString>Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identification through concrete and abstract context. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 680–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Expressing implicit semantic relations without supervision.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (Coling/ACL-06),</booktitle>
<pages>313--320</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3070" citStr="Turney, 2006" startWordPosition="458" endWordPosition="459"> 2010), but we prefer to concentrate on the simplest type of paraphrase, where a bigram paraphrases a unigram. For example, dog house is a paraphrase of kennel. In our experiments, we concentrate on noun-modifier bigrams and noun unigrams. Analogies map terms in one domain to terms in another domain (Gentner, 1983). The familiar analogy between the solar system and the RutherfordBohr atomic model involves several terms from the domain of the solar system and the domain of the atomic model (Turney, 2008a). The simplest type of analogy is proportional analogy, which involves two pairs of words (Turney, 2006b). For example, the pair (cook, raw) is analogous to the pair (decorate, plain). If we cook a thing, it is no longer raw; if we decorate a thing, it 353 Transactions of the Association for Computational Linguistics, 1 (2013) 353–366. Action Editor: Patrick Pantel. Submitted 5/2013; Revised 7/2013; Published 10/2013. c�2013 Association for Computational Linguistics. is no longer plain. The semantic relations between cook and raw are similar to the semantic relations between decorate and plain. In the following experiments, we focus on proportional analogies. Erk (2013) distinguished four appro</context>
<context position="8169" citStr="Turney, 2006" startWordPosition="1241" endWordPosition="1242">f relational similarity between two word pairs (Jurgens et al., 2012) and Task 6 (Agirre et al., 2012) examined the degree of semantic equivalence between two sentences. These two areas of research have been mostly independent, although Socher et al. (2012) and Turney (2012) present unified perspectives on the two tasks. We first discuss some work on relational similarity, then some work on compositional similarity, and lastly work that unifies the two types of similarity. 354 2.1 Relational Similarity LRA (latent relational analysis) measures relational similarity with a pair–pattern matrix (Turney, 2006b). Rows in the matrix correspond to word pairs (a, b) and columns correspond to patterns that connect the pairs (“a for the b”) in a large corpus. This is a holistic (noncompositional) approach to distributional similarity, since the word pairs are opaque wholes; the component words have no separate representations. A compositional approach to analogy has a representation for each word, and a word pair is represented by composing the representations for each member of the pair. Given a vocabulary of N words, a compositional approach requires N representations to handle all possible word pairs</context>
<context position="25644" citStr="Turney (2006" startWordPosition="4225" endWordPosition="4226">of 54.8%, according to Fisher’s exact test at the 95% confidence level. However, SuperSim answers the SAT questions in a few minutes, whereas LRA requires nine days, and SuperSim learns its models automatically, unlike the hand-coding of Turney (2012). 6See the State of the Art page on the ACL Wiki at http: //aclweb.org/aclwiki. 358 Algorithm Reference Correct Know-Best Veale (2004) 43.0 k-means Bic¸ici &amp; Yuret (2006) 44.0 BagPack Herda˘gdelen &amp; Baroni (2009) 44.1 VSM Turney &amp; Littman (2005) 47.1 Dual-Space Turney (2012) 51.1 BMI Bollegala et al. (2009) 51.1 PairClass Turney (2008b) 52.1 PERT Turney (2006a) 53.5 SuperSim — 54.8 LRA Turney (2006b) 56.1 Human Average college applicant 57.0 Table 4: The top ten results on five-choice SAT questions. Features Algorithm LF PPMI Dom Fun Correct Dual-Space 0 0 1 1 47.9 SuperSim 1 1 1 1 52.7 SuperSim 0 1 1 1 52.7 SuperSim 1 0 1 1 52.7 SuperSim 1 1 0 1 45.7 SuperSim 1 1 1 0 41.7 SuperSim 1 0 0 0 5.6 SuperSim 0 1 0 0 32.4 SuperSim 0 0 1 0 39.6 SuperSim 0 0 0 1 39.3 Table 5: Feature ablation with ten-choice SAT questions. 4.2 Ten-choice SAT Questions In addition to symmetries, proportional analogies have asymmetries. In general, if the quadruple (a, b, c,</context>
<context position="44346" citStr="Turney, 2006" startWordPosition="7304" endWordPosition="7305">araphrase (compositional similarity). SuperSim treats them both as problems of supervised tuple classification. The supervised learning algorithm is a standard support vector machine. The main contribution of SuperSim is a set of four types of features for representing tuples. The features work well with both analogy and paraphrase, with no task-specific modifications. SuperSim matches the state of the art on SAT analogy questions and substantially advances the state of the art on the SemEval 2012 Task 2 challenge and the noun-modifier paraphrase questions. SuperSim runs much faster than LRA (Turney, 2006b), answering the SAT questions in minutes instead of days. Unlike the dual-space model (Turney, 2012), SuperSim requires no hand-coded similarity composition functions. Since there is no handcoding, it is easy to add new features to SuperSim. Much work remains to be done, such as incorporating logic and scaling up to sentence paraphrases, but past work suggests that these problems are tractable. In the four approaches described by Erk (2013), SuperSim is an instance of the second approach to extending distributional semantics beyond words, comparing word pairs, phrases, or sentences (in gener</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006a. Expressing implicit semantic relations without supervision. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (Coling/ACL-06), pages 313–320, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="3070" citStr="Turney, 2006" startWordPosition="458" endWordPosition="459"> 2010), but we prefer to concentrate on the simplest type of paraphrase, where a bigram paraphrases a unigram. For example, dog house is a paraphrase of kennel. In our experiments, we concentrate on noun-modifier bigrams and noun unigrams. Analogies map terms in one domain to terms in another domain (Gentner, 1983). The familiar analogy between the solar system and the RutherfordBohr atomic model involves several terms from the domain of the solar system and the domain of the atomic model (Turney, 2008a). The simplest type of analogy is proportional analogy, which involves two pairs of words (Turney, 2006b). For example, the pair (cook, raw) is analogous to the pair (decorate, plain). If we cook a thing, it is no longer raw; if we decorate a thing, it 353 Transactions of the Association for Computational Linguistics, 1 (2013) 353–366. Action Editor: Patrick Pantel. Submitted 5/2013; Revised 7/2013; Published 10/2013. c�2013 Association for Computational Linguistics. is no longer plain. The semantic relations between cook and raw are similar to the semantic relations between decorate and plain. In the following experiments, we focus on proportional analogies. Erk (2013) distinguished four appro</context>
<context position="8169" citStr="Turney, 2006" startWordPosition="1241" endWordPosition="1242">f relational similarity between two word pairs (Jurgens et al., 2012) and Task 6 (Agirre et al., 2012) examined the degree of semantic equivalence between two sentences. These two areas of research have been mostly independent, although Socher et al. (2012) and Turney (2012) present unified perspectives on the two tasks. We first discuss some work on relational similarity, then some work on compositional similarity, and lastly work that unifies the two types of similarity. 354 2.1 Relational Similarity LRA (latent relational analysis) measures relational similarity with a pair–pattern matrix (Turney, 2006b). Rows in the matrix correspond to word pairs (a, b) and columns correspond to patterns that connect the pairs (“a for the b”) in a large corpus. This is a holistic (noncompositional) approach to distributional similarity, since the word pairs are opaque wholes; the component words have no separate representations. A compositional approach to analogy has a representation for each word, and a word pair is represented by composing the representations for each member of the pair. Given a vocabulary of N words, a compositional approach requires N representations to handle all possible word pairs</context>
<context position="25644" citStr="Turney (2006" startWordPosition="4225" endWordPosition="4226">of 54.8%, according to Fisher’s exact test at the 95% confidence level. However, SuperSim answers the SAT questions in a few minutes, whereas LRA requires nine days, and SuperSim learns its models automatically, unlike the hand-coding of Turney (2012). 6See the State of the Art page on the ACL Wiki at http: //aclweb.org/aclwiki. 358 Algorithm Reference Correct Know-Best Veale (2004) 43.0 k-means Bic¸ici &amp; Yuret (2006) 44.0 BagPack Herda˘gdelen &amp; Baroni (2009) 44.1 VSM Turney &amp; Littman (2005) 47.1 Dual-Space Turney (2012) 51.1 BMI Bollegala et al. (2009) 51.1 PairClass Turney (2008b) 52.1 PERT Turney (2006a) 53.5 SuperSim — 54.8 LRA Turney (2006b) 56.1 Human Average college applicant 57.0 Table 4: The top ten results on five-choice SAT questions. Features Algorithm LF PPMI Dom Fun Correct Dual-Space 0 0 1 1 47.9 SuperSim 1 1 1 1 52.7 SuperSim 0 1 1 1 52.7 SuperSim 1 0 1 1 52.7 SuperSim 1 1 0 1 45.7 SuperSim 1 1 1 0 41.7 SuperSim 1 0 0 0 5.6 SuperSim 0 1 0 0 32.4 SuperSim 0 0 1 0 39.6 SuperSim 0 0 0 1 39.3 Table 5: Feature ablation with ten-choice SAT questions. 4.2 Ten-choice SAT Questions In addition to symmetries, proportional analogies have asymmetries. In general, if the quadruple (a, b, c,</context>
<context position="44346" citStr="Turney, 2006" startWordPosition="7304" endWordPosition="7305">araphrase (compositional similarity). SuperSim treats them both as problems of supervised tuple classification. The supervised learning algorithm is a standard support vector machine. The main contribution of SuperSim is a set of four types of features for representing tuples. The features work well with both analogy and paraphrase, with no task-specific modifications. SuperSim matches the state of the art on SAT analogy questions and substantially advances the state of the art on the SemEval 2012 Task 2 challenge and the noun-modifier paraphrase questions. SuperSim runs much faster than LRA (Turney, 2006b), answering the SAT questions in minutes instead of days. Unlike the dual-space model (Turney, 2012), SuperSim requires no hand-coded similarity composition functions. Since there is no handcoding, it is easy to add new features to SuperSim. Much work remains to be done, such as incorporating logic and scaling up to sentence paraphrases, but past work suggests that these problems are tractable. In the four approaches described by Erk (2013), SuperSim is an instance of the second approach to extending distributional semantics beyond words, comparing word pairs, phrases, or sentences (in gener</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006b. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>The latent relation mapping engine: Algorithm and experiments.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>33--615</pages>
<contexts>
<context position="2365" citStr="Turney, 2008" startWordPosition="344" endWordPosition="345">e synonym questions from the test of English as a foreign language (TOEFL), a distributional approach recently achieved 100% accuracy (Bullinaria and Levy, 2012). However, it has been difficult to extend distributional semantics beyond individual words, to word pairs, phrases, and sentences. Moving beyond individual words, there are various types of semantic similarity to consider. Here we focus on paraphrase and analogy. Paraphrase is similarity in the meaning of two pieces of text (Androutsopoulos and Malakasiotis, 2010). Analogy is similarity in the semantic relations of two sets of words (Turney, 2008a). It is common to study paraphrase at the sentence level (Androutsopoulos and Malakasiotis, 2010), but we prefer to concentrate on the simplest type of paraphrase, where a bigram paraphrases a unigram. For example, dog house is a paraphrase of kennel. In our experiments, we concentrate on noun-modifier bigrams and noun unigrams. Analogies map terms in one domain to terms in another domain (Gentner, 1983). The familiar analogy between the solar system and the RutherfordBohr atomic model involves several terms from the domain of the solar system and the domain of the atomic model (Turney, 2008</context>
<context position="25619" citStr="Turney (2008" startWordPosition="4221" endWordPosition="4222">nt from SuperSim’s score of 54.8%, according to Fisher’s exact test at the 95% confidence level. However, SuperSim answers the SAT questions in a few minutes, whereas LRA requires nine days, and SuperSim learns its models automatically, unlike the hand-coding of Turney (2012). 6See the State of the Art page on the ACL Wiki at http: //aclweb.org/aclwiki. 358 Algorithm Reference Correct Know-Best Veale (2004) 43.0 k-means Bic¸ici &amp; Yuret (2006) 44.0 BagPack Herda˘gdelen &amp; Baroni (2009) 44.1 VSM Turney &amp; Littman (2005) 47.1 Dual-Space Turney (2012) 51.1 BMI Bollegala et al. (2009) 51.1 PairClass Turney (2008b) 52.1 PERT Turney (2006a) 53.5 SuperSim — 54.8 LRA Turney (2006b) 56.1 Human Average college applicant 57.0 Table 4: The top ten results on five-choice SAT questions. Features Algorithm LF PPMI Dom Fun Correct Dual-Space 0 0 1 1 47.9 SuperSim 1 1 1 1 52.7 SuperSim 0 1 1 1 52.7 SuperSim 1 0 1 1 52.7 SuperSim 1 1 0 1 45.7 SuperSim 1 1 1 0 41.7 SuperSim 1 0 0 0 5.6 SuperSim 0 1 0 0 32.4 SuperSim 0 0 1 0 39.6 SuperSim 0 0 0 1 39.3 Table 5: Feature ablation with ten-choice SAT questions. 4.2 Ten-choice SAT Questions In addition to symmetries, proportional analogies have asymmetries. In general, i</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008a. The latent relation mapping engine: Algorithm and experiments. Journal of Artificial Intelligence Research, 33:615–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>905--912</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="2365" citStr="Turney, 2008" startWordPosition="344" endWordPosition="345">e synonym questions from the test of English as a foreign language (TOEFL), a distributional approach recently achieved 100% accuracy (Bullinaria and Levy, 2012). However, it has been difficult to extend distributional semantics beyond individual words, to word pairs, phrases, and sentences. Moving beyond individual words, there are various types of semantic similarity to consider. Here we focus on paraphrase and analogy. Paraphrase is similarity in the meaning of two pieces of text (Androutsopoulos and Malakasiotis, 2010). Analogy is similarity in the semantic relations of two sets of words (Turney, 2008a). It is common to study paraphrase at the sentence level (Androutsopoulos and Malakasiotis, 2010), but we prefer to concentrate on the simplest type of paraphrase, where a bigram paraphrases a unigram. For example, dog house is a paraphrase of kennel. In our experiments, we concentrate on noun-modifier bigrams and noun unigrams. Analogies map terms in one domain to terms in another domain (Gentner, 1983). The familiar analogy between the solar system and the RutherfordBohr atomic model involves several terms from the domain of the solar system and the domain of the atomic model (Turney, 2008</context>
<context position="25619" citStr="Turney (2008" startWordPosition="4221" endWordPosition="4222">nt from SuperSim’s score of 54.8%, according to Fisher’s exact test at the 95% confidence level. However, SuperSim answers the SAT questions in a few minutes, whereas LRA requires nine days, and SuperSim learns its models automatically, unlike the hand-coding of Turney (2012). 6See the State of the Art page on the ACL Wiki at http: //aclweb.org/aclwiki. 358 Algorithm Reference Correct Know-Best Veale (2004) 43.0 k-means Bic¸ici &amp; Yuret (2006) 44.0 BagPack Herda˘gdelen &amp; Baroni (2009) 44.1 VSM Turney &amp; Littman (2005) 47.1 Dual-Space Turney (2012) 51.1 BMI Bollegala et al. (2009) 51.1 PairClass Turney (2008b) 52.1 PERT Turney (2006a) 53.5 SuperSim — 54.8 LRA Turney (2006b) 56.1 Human Average college applicant 57.0 Table 4: The top ten results on five-choice SAT questions. Features Algorithm LF PPMI Dom Fun Correct Dual-Space 0 0 1 1 47.9 SuperSim 1 1 1 1 52.7 SuperSim 0 1 1 1 52.7 SuperSim 1 0 1 1 52.7 SuperSim 1 1 0 1 45.7 SuperSim 1 1 1 0 41.7 SuperSim 1 0 0 0 5.6 SuperSim 0 1 0 0 32.4 SuperSim 0 0 1 0 39.6 SuperSim 0 0 0 1 39.3 Table 5: Feature ablation with ten-choice SAT questions. 4.2 Ten-choice SAT Questions In addition to symmetries, proportional analogies have asymmetries. In general, i</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008b. A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 905– 912, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Domain and function: A dualspace model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>44</volume>
<pages>585</pages>
<contexts>
<context position="4057" citStr="Turney, 2012" startWordPosition="611" endWordPosition="612"> longer plain. The semantic relations between cook and raw are similar to the semantic relations between decorate and plain. In the following experiments, we focus on proportional analogies. Erk (2013) distinguished four approaches to extend distributional semantics beyond words: In the first, a single vector space representation for a phrase or sentence is computed from the representations of the individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). In the second, two phrases or sentences are compared by combining multiple pairwise similarity values (Socher et al., 2011; Turney, 2012). Third, weighted inference rules integrate distributional similarity and formal logic (Garrette et al., 2011). Fourth, a single space integrates formal logic and vectors (Clarke, 2012). Taking the second approach, Turney (2012) introduced a dual-space model, with one space for measuring domain similarity (similarity of topic or field) and another for function similarity (similarity of role or usage). Similarities beyond individual words are calculated by functions that combine domain and function similarities of component words. The dual-space model has been applied to measuring compositional</context>
<context position="7832" citStr="Turney (2012)" startWordPosition="1192" endWordPosition="1193">n 3. The experiments with relational similarity are described in Section 4, and Section 5 reports the results with compositional similarity. Section 6 discusses the implications of the results. We consider future work in Section 7 and conclude in Section 8. 2 Related Work In SemEval 2012, Task 2 was concerned with measuring the degree of relational similarity between two word pairs (Jurgens et al., 2012) and Task 6 (Agirre et al., 2012) examined the degree of semantic equivalence between two sentences. These two areas of research have been mostly independent, although Socher et al. (2012) and Turney (2012) present unified perspectives on the two tasks. We first discuss some work on relational similarity, then some work on compositional similarity, and lastly work that unifies the two types of similarity. 354 2.1 Relational Similarity LRA (latent relational analysis) measures relational similarity with a pair–pattern matrix (Turney, 2006b). Rows in the matrix correspond to word pairs (a, b) and columns correspond to patterns that connect the pairs (“a for the b”) in a large corpus. This is a holistic (noncompositional) approach to distributional similarity, since the word pairs are opaque wholes</context>
<context position="10478" citStr="Turney, 2012" startWordPosition="1609" endWordPosition="1610">tchell and Lapata, 2008; Mitchell and Lapata, 2010). In this approach, given the words a and b with context vectors a and b, we construct a vector for the bigram ab by applying vector operations to a and b. Mitchell and Lapata (2010) experiment with many different vector operations and find that element-wise multiplication performs well. The bigram ab is represented by c = a O b, where ci = ai · bi. However, element-wise multiplication is commutative, so the bigrams ab and ba map to the same vector c. In experiments that test for order sensitivity, element-wise multiplication performs poorly (Turney, 2012). We can treat the bigram ab as a unit, as if it were a single word, and construct a context vector for ab from occurrences of ab in a large corpus. This holistic approach to representing bigrams performs well when a limited set of bigrams is specified in advance (before building the word–context matrix), but it does not scale up, because there are too many possible bigrams (Turney, 2012). Although the holistic approach does not scale up, we can generate a few holistic bigram vectors and use them to train a supervised regression model (Guevara, 2010; Baroni and Zamparelli, 2010). Given a new b</context>
<context position="12265" citStr="Turney (2012)" startWordPosition="1910" endWordPosition="1911">pervised learning is simpler with fixed-sized feature vectors, the variable-sized similarity matrix is then reduced to a smaller fixed-sized matrix, to allow comparison of pairs of sentences of varying lengths. 2.3 Unified Perspectives on Similarity Socher et al. (2012) represent words and phrases with a pair, consisting of a vector and a matrix. The vector captures the meaning of the word or phrase and the matrix captures how a word or phrase modifies the meaning of another word or phrase when they are combined. They apply this matrix–vector representation to both compositions and relations. Turney (2012) represents words with two vectors, a vector from domain space and a vector from function space. The domain vector captures the topic or field of the word and the function vector captures the 355 functional role of the word. This dual-space model is applied to both compositions and relations. Here we extend the dual-space model of Turney (2012) in two ways: Hand-coding is replaced with supervised learning and two new sets of features augment domain and function space. Moving to supervised learning instead of hand-coding makes it easier to introduce new features. In the dual-space model, parame</context>
<context position="13801" citStr="Turney, 2012" startWordPosition="2156" endWordPosition="2157"> algorithm. The insight behind SuperSim is that, given appropriate features, a supervised learning algorithm can replace the grid search algorithm and the hand-crafted functions. 3 Features for Tuple Classification We represent a tuple with four types of features, all based on frequencies in a large corpus. The first type of feature is the logarithm of the frequency of a word. The second type is the positive pointwise mutual information (PPMI) between two words (Church and Hanks, 1989; Bullinaria and Levy, 2007). Third and fourth are the similarities of two words in domain and function space (Turney, 2012). In the following experiments, we use the PPMI matrix from Turney et al. (2011) and the domain and function matrices from Turney (2012).1 The three matrices and the word frequency data are based on the same corpus, a collection of web pages gathered from university web sites, containing 5 × 1010 words.2 All three matrices are word–context matrices, in which the rows correspond to terms (words and phrases) in WordNet.3 The columns correspond to the contexts in which the terms appear; each matrix involves a different kind of context. 1The three matrices and the word frequency data are available</context>
<context position="17488" citStr="Turney (2012)" startWordPosition="2815" endWordPosition="2816">ining xj, we are less likely to find any phrases containing xi. Although, in theory, PPMI(xi, xj, left) should equal PPMI(xj, xi, right), they are likely to be unequal given a limited sample. 356 From the n-tuple, we select all of the n(n − 1) pairs, (xi, xj), such that i =� j. We then generate two features for each pair, PPMI(xi, xj, left) and PPMI(xi, xj, right). Thus there are 2n(n−1) PPMI values in the second set of features. The third set of features consists of domain space similarity values for each pair of words in the n-tuple. Domain space was designed to capture the topic of a word. Turney (2012) first constructed a frequency matrix, in which the rows correspond to terms in WordNet and the columns correspond to nearby nouns. Given a term xi, the corpus was sampled for phrases containing xi and the phrases were processed with a part-of-speech tagger, to identify nouns. If the noun xj was the closest noun to the left or right of xi, then the frequency count for the i-th row and j-th column was incremented. The hypothesis was that the nouns near a term characterize the topics associated with the term. The word–context frequency matrix for domain space has 114,297 rows (terms) and 50,000 </context>
<context position="18916" citStr="Turney, 2012" startWordPosition="3063" endWordPosition="3064">ted by a row vector in UkEpk. The parameter k specifies the number of singular values in the truncated singular value decomposition; that is, k is the number of latent factors in the low-dimensional representation of the term (Landauer and Dumais, 1997). We generate Uk and Ek by deleting the columns in U and E corresponding to the smallest singular values. The parameter p raises the singular values in Ek to the power p (Caron, 2001). As p goes from one to zero, factors with smaller singular values are given more weight. This has the effect of making the similarity measure more discriminating (Turney, 2012). The similarity of two words in domain space, Dom(xi, xj, k, p), is computed by extracting the row vectors in UkEpk that correspond to the words xi and xj, and then calculating their cosine. Optimal performance requires tuning the parameters k and p for the task (Bullinaria and Levy, 2012; Turney, 2012). In the following experiments, we avoid directly tuning k and p by generating features with a variety of values for k and p, allowing the supervised learning algorithm to decide which features to use. Feature set Size of set LF(xi) n PPMI(xi, xj, handedness) 2n(n − 1) Dom(xi, xj, k, p) 2n(n − </context>
<context position="23388" citStr="Turney, 2012" startWordPosition="3859" endWordPosition="3861"> polynomial. Weka provides probability estimates for the classes by fitting the outputs of the SVM with logistic regression models. 4 Relational Similarity This section presents experiments with learning relational similarity using SuperSim. The training datasets consist of quadruples that are labeled as positive and negative examples of analogies. Table 2 shows that the feature vectors have 1,348 elements. We experiment with three datasets, a collection of 374 five-choice questions from the SAT college entrance exam (Turney et al., 2003), a modified ten-choice variation of the SAT questions (Turney, 2012), and the relational similarity dataset from SemEval 2012 Task 2 (Jurgens et al., 2012).5 4Weka is available at http://www.cs.waikato.ac. nz/ml/weka/. 5The SAT questions are available on request from the author. The SemEval 2012 Task 2 dataset is available at https:// sites.google.com/site/semeval2012task2/. 4.1 Five-choice SAT Questions Table 3 is an example of a question from the 374 five-choice SAT questions. Each five-choice question yields five labeled quadruples, by combining the stem with each choice. The quadruple (word, language, note, music) is labeled positive and the other four qua</context>
<context position="25283" citStr="Turney (2012)" startWordPosition="4168" endWordPosition="4169">l assigns a probability to each of the five choices and guesses the choice with the highest probability. SuperSim achieves a score of 54.8% correct (205 out of 374). Table 4 gives the rank of SuperSim in the list of the top ten results with the SAT analogy questions.6 The scores ranging from 51.1% to 57.0% are not significantly different from SuperSim’s score of 54.8%, according to Fisher’s exact test at the 95% confidence level. However, SuperSim answers the SAT questions in a few minutes, whereas LRA requires nine days, and SuperSim learns its models automatically, unlike the hand-coding of Turney (2012). 6See the State of the Art page on the ACL Wiki at http: //aclweb.org/aclwiki. 358 Algorithm Reference Correct Know-Best Veale (2004) 43.0 k-means Bic¸ici &amp; Yuret (2006) 44.0 BagPack Herda˘gdelen &amp; Baroni (2009) 44.1 VSM Turney &amp; Littman (2005) 47.1 Dual-Space Turney (2012) 51.1 BMI Bollegala et al. (2009) 51.1 PairClass Turney (2008b) 52.1 PERT Turney (2006a) 53.5 SuperSim — 54.8 LRA Turney (2006b) 56.1 Human Average college applicant 57.0 Table 4: The top ten results on five-choice SAT questions. Features Algorithm LF PPMI Dom Fun Correct Dual-Space 0 0 1 1 47.9 SuperSim 1 1 1 1 52.7 SuperS</context>
<context position="26562" citStr="Turney (2012)" startWordPosition="4400" endWordPosition="4401">im 1 1 1 0 41.7 SuperSim 1 0 0 0 5.6 SuperSim 0 1 0 0 32.4 SuperSim 0 0 1 0 39.6 SuperSim 0 0 0 1 39.3 Table 5: Feature ablation with ten-choice SAT questions. 4.2 Ten-choice SAT Questions In addition to symmetries, proportional analogies have asymmetries. In general, if the quadruple (a, b, c, d) is positive, (a, d, c, b) is negative. For example, (word, language, note, music) is a good analogy, but (word, music, note, language) is not. Words are the basic units of language and notes are the basic units of music, but words are not necessary for music and notes are not necessary for language. Turney (2012) used this asymmetry to convert the 374 five-choice SAT questions into 374 tenchoice SAT questions. Each choice (c, d) was expanded with the stem (a, b), resulting in the quadruple (a, b, c, d), and then the order was shuffled to (a, d, c, b), so that each choice pair in a fivechoice question generated two choice quadruples in a ten-choice question. Nine of the quadruples are negative examples and the quadruple consisting of the stem pair followed by the solution pair is the only positive example. The purpose of the ten-choice questions is to test the ability of measures of relational similari</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter D. Turney. 2012. Domain and function: A dualspace model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533– 585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Veale</author>
</authors>
<title>WordNet sits the SAT: A knowledgebased approach to lexical analogy.</title>
<date>2004</date>
<booktitle>In Proceedings of the 16th European Conference on Artificial Intelligence (ECAI2004),</booktitle>
<pages>606--612</pages>
<location>Valencia,</location>
<marker>Veale, 2004</marker>
<rawString>Tony Veale. 2004. WordNet sits the SAT: A knowledgebased approach to lexical analogy. In Proceedings of the 16th European Conference on Artificial Intelligence (ECAI2004), pages 606–612, Valencia, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
<author>Mark A Hall</author>
</authors>
<date>2011</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques, Third Edition.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="22734" citStr="Witten et al., 2011" startWordPosition="3757" endWordPosition="3760">gether in the corpus. Domain and function space capture indirect or higher-order co-occurrence, due to the truncated SVD (Lemaire and Denhi`ere, 2006); that is, the values of Dom(xi, xj, k, p) and Fun(xi, xj, k, p) can be high even when xi and xj do not actually co-occur in the corpus. We conjecture that there are yet higher orders in this hierarchy that would provide improved similarity measures. SuperSim learns to classify tuples by representing them with these features. SuperSim uses the sequential minimal optimization (SMO) support vector machine (SVM) as implemented in Weka (Platt, 1998; Witten et al., 2011).4 The kernel is a normalized third-order polynomial. Weka provides probability estimates for the classes by fitting the outputs of the SVM with logistic regression models. 4 Relational Similarity This section presents experiments with learning relational similarity using SuperSim. The training datasets consist of quadruples that are labeled as positive and negative examples of analogies. Table 2 shows that the feature vectors have 1,348 elements. We experiment with three datasets, a collection of 374 five-choice questions from the SAT college entrance exam (Turney et al., 2003), a modified te</context>
</contexts>
<marker>Witten, Frank, Hall, 2011</marker>
<rawString>Ian H. Witten, Eibe Frank, and Mark A. Hall. 2011. Data Mining: Practical Machine Learning Tools and Techniques, Third Edition. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alisa Zhila</author>
<author>Wen-tau Yih</author>
<author>Christopher Meek</author>
<author>Geoffrey Zweig</author>
<author>Tomas Mikolov</author>
</authors>
<title>Combining heterogeneous models for measuring relational similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2013),</booktitle>
<location>Atlanta,</location>
<contexts>
<context position="29612" citStr="Zhila et al. (2013)" startWordPosition="4891" endWordPosition="4894">nclusion has three paradigmatic examples, flower:tulip, emotion:rage, and poem:sonnet. Jurgens et al. (2012) used Amazon’s Mechanical Turk to create the SemEval 2012 Task 2 dataset in two phases. In the first phase, Turkers expanded the paradigmatic examples for each subcategory to an 359 Algorithm Reference Spearman BUAP Tovar et al. (2012) 0.014 Duluth-V2 Pedersen (2012) 0.038 Duluth-V1 Pedersen (2012) 0.039 Duluth-V0 Pedersen (2012) 0.050 UTD-SVM Rink &amp; Harabagiu (2012) 0.116 UTD-NB Rink &amp; Harabagiu (2012) 0.229 RNN-1600 Mikolov et al. (2013) 0.275 UTD-LDA Rink &amp; Harabagiu (2013) 0.334 Com Zhila et al. (2013) 0.353 SuperSim — 0.408 Table 6: Spearman correlations for SemEval 2012 Task 2. average of forty-one word pairs per subcategory, a total of 3,218 pairs. In the second phase, each word pair from the first phase was assigned a prototypicality score, indicating its similarity to the paradigmatic examples. The challenge of SemEval 2012 Task 2 was to guess the prototypicality scores. SuperSim was trained on the five-choice SAT questions and evaluated on the SemEval 2012 Task 2 test dataset. For a given a word pair, we created quadruples, combining the word pair with each of the paradigmatic example</context>
</contexts>
<marker>Zhila, Yih, Meek, Zweig, Mikolov, 2013</marker>
<rawString>Alisa Zhila, Wen-tau Yih, Christopher Meek, Geoffrey Zweig, and Tomas Mikolov. 2013. Combining heterogeneous models for measuring relational similarity. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2013), Atlanta, Georgia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>