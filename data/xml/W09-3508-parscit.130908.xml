<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.084380">
<title confidence="0.996591">
Experiences with English-Hindi, English-Tamil and English-Kannada
Transliteration Tasks at NEWS 2009
</title>
<author confidence="0.931331">
Manoj Kumar Chinnakotla and Om P. Damani
</author>
<affiliation confidence="0.756596">
Department of Computer Science and Engineering,
IIT Bombay,
Mumbai, India
</affiliation>
<email confidence="0.997265">
{manoj,damani}@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.993866" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999870375">
We use a Phrase-Based Statistical Ma-
chine Translation approach to Translitera-
tion where the words are replaced by char-
acters and sentences by words. We employ
the standard SMT tools like GIZA++ for
learning alignments and Moses for learn-
ing the phrase tables and decoding. Be-
sides tuning the standard SMT parame-
ters, we focus on tuning the Character Se-
quence Model (CSM) related parameters
like order of the CSM, weight assigned to
CSM during decoding and corpus used for
CSM estimation. Our results show that
paying sufficient attention to CSM pays
off in terms of increased transliteration ac-
curacies.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999668588235294">
Transliteration of Named-Entities (NEs) is an im-
portant problem that affects the accuracy of many
NLP applications like Cross Lingual Search and
Machine Translation. Transliteration is defined
as the process of automatically mapping a given
grapheme sequence in the source language to a
grapheme sequence in the target language such
that it preserves the pronunciation of the origi-
nal source word. A Grapheme refers to the unit
of written language which expresses a phoneme
in the language. Multiple alphabets could be
used to express a grapheme. For example, sh
is considered a single grapheme expressing the
phoneme /SH/. For phonetic orthography like De-
vanagari, each grapheme corresponds to a unique
phoneme. However, for English, a grapheme like
c may map to multiple phonemes /S/,/K/. An ex-
ample of transliteration is mapping the Devana-
gari grapheme sequence FRV kft to its phoneti-
cally equivalent grapheme sequence Prince Harry
in English.
This paper discusses our transliteration ap-
proach taken for the NEWS 2009 Machine
Transliteration Shared Task [Li et al.2009b, Li et
al.2009a]. We model the transliteration problem
as a Phrased-Based Machine Translation prob-
lem. Later, using the development set, we tune
the various parameters of the system like order of
the Character Sequence Model (CSM), typically
called language model, weight assigned to CSM
during decoding and corpus used to estimate the
CSM. Our results show that paying sufficient at-
tention to the CSM pays off in terms of improved
accuracies.
</bodyText>
<sectionHeader confidence="0.969814" genericHeader="method">
2 Phrase-Based SMT Approach to
Transliteration
</sectionHeader>
<bodyText confidence="0.998915142857143">
In the Phrase-Based SMT Approach to Transliter-
ation [Sherif and Kondrak2007, Huang2005], the
words are replaced by characters and sentences are
replaced by words. The corresponding noisy chan-
nel model formulation where a given english word
e is to be transliterated into a foreign word h, is
given as:
</bodyText>
<equation confidence="0.9857925">
h* = argmax Pr(h|e)
h
= argmax Pr(e|h) · Pr(h) (1)
h
</equation>
<bodyText confidence="0.999836142857143">
In Equation 1, Pr(e|h) is known as the translation
model which gives the probability that the char-
acter sequence h could be transliterated to e and
Pr(h) is known as the character sequence model
typically called language model which gives the
probability that the character sequence h forms a
valid word in the target language.
</bodyText>
<page confidence="0.983423">
44
</page>
<note confidence="0.989014">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 44–47,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<table confidence="0.992820928571429">
Task Run Optimal Accuracy Mean F- MRR MAPref MAP10 MAPsys
Parameter Set in top-1 score
English-Hindi Standard LM Order: 5, 0.47 0.86 0.58 0.47 0.18 0.20
LM Weight:
0.6
English-Hindi Non- LM Order: 5, 0.52 0.87 0.62 0.52 0.19 0.21
standard LM Weight:
0.6
English-Tamil Standard LM Order: 5, 0.45 0.88 0.56 0.45 0.18 0.18
LM Weight:
0.3
English- Standard LM Order: 5, 0.44 0.87 0.55 0.44 0.17 0.18
Kannada LM Weight:
0.3
</table>
<figureCaption confidence="0.659547">
Figure 1: NEWS 2009 Development Set Results
</figureCaption>
<table confidence="0.997399833333333">
Task Run Accuracy in Mean F- MRR MAPref MAP10 MAPsys
top-1 score
English-Hindi Standard 0.42 0.86 0.54 0.42 0.18 0.20
English-Hindi Non-standard 0.49 0.87 0.59 0.48 0.20 0.23
English-Tamil Standard 0.41 0.89 0.54 0.40 0.18 0.18
English-Kannada Standard 0.36 0.86 0.48 0.35 0.16 0.16
</table>
<figureCaption confidence="0.988608">
Figure 2: NEWS 2009 Test Set Results
</figureCaption>
<bodyText confidence="0.999958227272727">
Given the parallel training data pairs, we pre-
processed the source (English) and target (Hindi,
Tamil and Kannada) strings into character se-
quences. We then ran the GIZA++ [Och and
Ney2003] aligner with default options to obtain
the character-level alignments. For alignment, ex-
cept for Hindi, we used single character-level units
without any segmentation. In case of Hindi, we
did a simple segmentation where we added the
halant character (U094D) to the previous Hindi
character. Moses Toolkit [Hoang et al.2007] was
then used to learn the phrase-tables for English-
Hindi, English-Tamil and English-Kannada. We
also learnt the character sequence models on the
target language training words using the SRILM
toolkit [Stolcke2002]. Given a new English word,
we split the word into sequence of characters and
run the Moses decoder with the phrase-table of tar-
get language obtained above to get the transliter-
ated word. We ran Moses with the DISTINCT op-
tion to obtain the top k distinct transliterated op-
tions.
</bodyText>
<subsectionHeader confidence="0.992162">
2.1 Moses Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.999872">
The Moses decoder computes the cost of each
translation as a product of probability costs of four
models: a) translation model b) language model
c) distortion model and d) word penalty as shown
in Equation 2. The distortion model controls the
</bodyText>
<table confidence="0.947926428571429">
Task Run Baseline Best Run %
Order N=3) Impr
English-Hindi Standard 0.4 0.42 5.00
English-Hindi Non-standard 0.37 0.49 32.43
English-Tamil Standard 0.39 0.45 15.38
Kannada Standard 0.36 0.36 0.00
Model (LM
</table>
<figureCaption confidence="0.9919225">
Figure 3: Improvements Obtained over Baseline
on Test Set due to Language Model Tuning
</figureCaption>
<bodyText confidence="0.941609285714286">
English-
cost of re-ordering phrases (transliteration units)
in a given sentence (word) and the word penalty
model controls the length of the final translation.
The parameters AT, ACSM, AD and AW control
the relative importance given to each of the above
models.
</bodyText>
<equation confidence="0.999429">
Pr(h|e) = PrT(e|h)AT · PrCSM(h)ACSm·
PrD(h, e)AD · Wlength(h)·AW (2)
</equation>
<bodyText confidence="0.998720714285714">
Since no re-ordering of phrases is required during
translation task, we assign a zero weight to AD.
Similarly, we varied the word penalty factor AW
between {−1, 0, +1} and found that it achieves
maximum accuracy at 0. All the above tuning was
done with a trigram CSM and default weight (0.5)
in Moses for AT.
</bodyText>
<page confidence="0.998232">
45
</page>
<subsectionHeader confidence="0.993876">
2.2 Improving CSM Performance
</subsectionHeader>
<bodyText confidence="0.999981823529412">
In addition to the above mentioned parameters,
we varied the order of the CSM and the mono-
lingual corpus used to estimate the CSM. For each
task, we started with a trigram CSM as mentioned
above and tuned both the order of the CSM and
ACSM on the development set. The optimal set
of parameters and the development set results are
shown in Figure 1. In addition, we use a mono-
lingual Hindi corpus of around 0.4 million doc-
uments called Guruji corpus. We extracted the
2.6 million unique words from the above corpus
and trained a CSM on that. This CSM which was
learnt on the monolingual Hindi corpus was used
for the non-standard Hindi run. We repeat the
above procedure of tuning the order of CSM and
ACSM and find the optimal set of parameters for
the non-standard run on the development set.
</bodyText>
<sectionHeader confidence="0.99915" genericHeader="evaluation">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9999785">
The details of the NEWS 2009 dataset for Hindi,
Kannada and Tamil are given in [Li et al.2009a,
Kumaran and Kellner2007]. The final results of
our system on the test set are shown in Figure 2.
Figure 3 shows the improvements obtained on test
set by tuning the CSM parameters. The trigram
CSM model used along with the optimal Moses
parameter set tuned on development set was taken
as baseline for the above experiments. The results
show that a major improvement (32.43%) was ob-
tained in the non-standard run where the monolin-
gual Hindi corpus was used to learn the CSM. Be-
cause of the use of monolingual Hindi corpus in
the non-standard run, the transliteration accuracy
improved by 22.5% when compared to the stan-
dard run. The improvements (15.38%) obtained in
Tamil are also significant. However, the improve-
ment in Hindi standard run was not significant. In
Kannada, there was no improvement due to tuning
of LM parameters. This needs further investiga-
tion.
The above results clearly highlight the impor-
tance of improving CSM accuracy since it helps
in improving the transliteration accuracy. More-
over, improving the CSM accuracy only requires
monolingual language resources which are easy
to obtain when compared to parallel transliteration
training data.
</bodyText>
<sectionHeader confidence="0.998664" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999936916666667">
We presented the transliteration system which we
used for our participation in the NEWS 2009 Ma-
chine Transliteration Shared Task on Translitera-
tion. We took a Phrase-Based SMT approach to
transliteration where words are replaced by char-
acters and sentences by words. In addition to the
standard SMT parameters, we tuned the CSM re-
lated parameters like order of the CSM, weight as-
signed to CSM and corpus used to estimate the
CSM. Our results show that improving the ac-
curacy of CSM pays off in terms of improved
transliteration accuracies.
</bodyText>
<sectionHeader confidence="0.996932" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9995858">
We would like to thank the Indian search-engine
company Guruji (http://www.guruji.com)
for providing us the Hindi web content which was
used to train the language model for our non-
standard Hindi runs.
</bodyText>
<sectionHeader confidence="0.998082" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999657766666667">
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and
Ondej Bojar. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In In Proceed-
ings of ACL, Demonstration Session, pages 177–
180.
Fei Huang. 2005. Cluster-specific Named Entity
Transliteration. In HLT ’05: Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 435–442, Morristown, NJ, USA. Association
for Computational Linguistics.
A. Kumaran and Tobias Kellner. 2007. A Generic
Framework for Machine Transliteration. In SIGIR
’07: Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 721–722, New
York, NY, USA. ACM.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009a. Report on NEWS 2009 Ma-
chine Transliteration Shared Task. In Proceed-
ings of ACL-IJCNLP 2009 Named Entities Work-
shop (NEWS 2009).
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009b. Whitepaper of NEWS 2009
Machine Transliteration Shared Task. In Proceed-
ings of ACL-IJCNLP 2009 Named Entities Work-
shop (NEWS 2009).
</reference>
<page confidence="0.989661">
46
</page>
<reference confidence="0.998936111111111">
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
Based Transliteration. In In Proceedings of ACL
2007. The Association for Computer Linguistics.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In In Proceedings of Intl.
Conf. on Spoken Language Processing.
</reference>
<page confidence="0.999491">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.412716">
<title confidence="0.8937565">Experiences with English-Hindi, English-Tamil and Transliteration Tasks at NEWS 2009</title>
<author confidence="0.898543">Kumar Chinnakotla P</author>
<affiliation confidence="0.985381">Department of Computer Science and IIT</affiliation>
<address confidence="0.610309">Mumbai,</address>
<abstract confidence="0.989599470588235">We use a Phrase-Based Statistical Machine Translation approach to Transliteration where the words are replaced by characters and sentences by words. We employ the standard SMT tools like GIZA++ for learning alignments and Moses for learning the phrase tables and decoding. Besides tuning the standard SMT parameters, we focus on tuning the Character Sequence Model (CSM) related parameters like order of the CSM, weight assigned to CSM during decoding and corpus used for CSM estimation. Our results show that paying sufficient attention to CSM pays off in terms of increased transliteration accuracies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-burch</author>
<author>Richard Zens</author>
<author>Rwth Aachen</author>
<author>Alexandra Constantin</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Chris Dyer</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Ondej Bojar</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. In</title>
<date>2007</date>
<booktitle>In Proceedings of ACL, Demonstration Session,</booktitle>
<pages>177--180</pages>
<marker>Hoang, Birch, Callison-burch, Zens, Aachen, Constantin, Federico, Bertoldi, Dyer, Cowan, Shen, Moran, Bojar, 2007</marker>
<rawString>Hieu Hoang, Alexandra Birch, Chris Callison-burch, Richard Zens, Rwth Aachen, Alexandra Constantin, Marcello Federico, Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade Shen, Christine Moran, and Ondej Bojar. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In In Proceedings of ACL, Demonstration Session, pages 177– 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
</authors>
<title>Cluster-specific Named Entity Transliteration. In</title>
<date>2005</date>
<booktitle>HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>435--442</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Huang, 2005</marker>
<rawString>Fei Huang. 2005. Cluster-specific Named Entity Transliteration. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 435–442, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A Generic Framework for Machine Transliteration. In</title>
<date>2007</date>
<booktitle>SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>721--722</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>A. Kumaran and Tobias Kellner. 2007. A Generic Framework for Machine Transliteration. In SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 721–722, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Vladimir Pervouchine</author>
<author>Min Zhang</author>
</authors>
<title>Machine Transliteration Shared Task.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<tech>2009a. Report on NEWS</tech>
<marker>Li, Kumaran, Pervouchine, Zhang, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Vladimir Pervouchine, and Min Zhang. 2009a. Report on NEWS 2009 Machine Transliteration Shared Task. In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Min Zhang</author>
<author>Vladimir Pervouchine</author>
</authors>
<title>Machine Transliteration Shared Task.</title>
<date>2009</date>
<journal>Whitepaper of NEWS</journal>
<booktitle>In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<marker>Li, Kumaran, Zhang, Pervouchine, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Min Zhang, and Vladimir Pervouchine. 2009b. Whitepaper of NEWS 2009 Machine Transliteration Shared Task. In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tarek Sherif</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>SubstringBased Transliteration. In</title>
<date>2007</date>
<booktitle>In Proceedings of ACL 2007. The Association</booktitle>
<institution>for Computer Linguistics.</institution>
<marker>Sherif, Kondrak, 2007</marker>
<rawString>Tarek Sherif and Grzegorz Kondrak. 2007. SubstringBased Transliteration. In In Proceedings of ACL 2007. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit. In</title>
<date>2002</date>
<booktitle>In Proceedings of Intl. Conf. on Spoken Language Processing.</booktitle>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In In Proceedings of Intl. Conf. on Spoken Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>