<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000425">
<title confidence="0.996047">
Using Domain Similarity for Performance Estimation
</title>
<author confidence="0.992139">
Vincent Van Asch
</author>
<affiliation confidence="0.799532">
CLiPS - University of Antwerp
Antwerp, Belgium
</affiliation>
<email confidence="0.95122">
Vincent.VanAsch@ua.ac.be
</email>
<author confidence="0.975467">
Walter Daelemans
</author>
<affiliation confidence="0.7873455">
CLiPS - University of Antwerp
Antwerp, Belgium
</affiliation>
<email confidence="0.973785">
Walter.Daelemans@ua.ac.be
</email>
<sectionHeader confidence="0.993253" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967952380952">
Many natural language processing (NLP)
tools exhibit a decrease in performance
when they are applied to data that is lin-
guistically different from the corpus used
during development. This makes it hard to
develop NLP tools for domains for which
annotated corpora are not available. This
paper explores a number of metrics that
attempt to predict the cross-domain per-
formance of an NLP tool through statis-
tical inference. We apply different sim-
ilarity metrics to compare different do-
mains and investigate the correlation be-
tween similarity and accuracy loss of NLP
tool. We find that the correlation between
the performance of the tool and the sim-
ilarity metric is linear and that the latter
can therefore be used to predict the perfor-
mance of an NLP tool on out-of-domain
data. The approach also provides a way to
quantify the difference between domains.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999868032258065">
Domain adaptation has recently turned into a
broad field of study (Bellegarda, 2004). Many re-
searchers note that the linguistic variation between
training and testing corpora is an important fac-
tor in assessing the performance of an NLP tool
across domains. For example, a tool that has been
developed to extract predicate-argument structures
from abstracts of biomedical research papers, will
exhibit a lower performance when applied to legal
texts.
However, the notion of domain is mostly arbi-
trarily used to refer to some kind of semantic area.
There is unfortunately no unambiguous measure
to assert a domain shift, except by observing the
performance loss of an NLP tool when applied
across different domains. This means that we typ-
ically need annotated data to reveal a domain shift.
In this paper we will show how unannotated data
can be used to get a clearer view on how datasets
differ. This unsupervised way of looking at data
will give us a method to measure the difference be-
tween data sets and allows us to predict the perfor-
mance of an NLP tool on unseen, out-of-domain
data.
In Section 2 we will explain our approach in
detail. In Section 3 we deal with a case study
involving basic part-of-speech taggers, applied to
different domains. An overview of related work
can be found in Section 4. Finally, Section 5 con-
cludes this paper and discusses options for further
research.
</bodyText>
<sectionHeader confidence="0.991828" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.99913768">
When developing an NLP tool using supervised
learning, annotated data with the same linguistic
properties as the data for which the tool is devel-
oped is needed, but not always available. In many
cases, this means that the developer needs to col-
lect and annotate data suited for the task. When
this is not possible, it would be useful to have a
method that can estimate the performance on cor-
pus B of an NLP tool trained on corpus A in an
unsupervised way, i.e., without the necessity to an-
notate a part of B.
In order to be able to predict in an unsupervised
way the performance of an NLP tool on different
corpora, we need a way to measure the differences
between the corpora. The metric at hand should
be independent from the annotation labels, so that
it can be easily applied on any given corpus. The
aim is to find a metric such that the correlation be-
tween the metric and the performance is statisti-
cally significant. In the scope of this article the
concept metric stands for any way of assigning a
sufficiently fine-grained label to a corpus, using
only unannotated data. This means that, in our
view, a metric can be an elaborate mixture of fre-
quency counts, rules, syntactic pattern matching or
</bodyText>
<page confidence="0.998842">
31
</page>
<note confidence="0.631828">
Proceedings of the 2010 Workshop on Domain Ada tation for Natural Language Processing, ACL 2010, pages 31–36,
Uppsala, Sweden, 15 July 2010. 82010 Association for Computational Linguistics
</note>
<bodyText confidence="0.995559">
R´enyi divergence between corpus P and corpus Q
the following formula is applied:
</bodyText>
<equation confidence="0.984193">
R6nyi (P; Q; α) = (α1 1) loge �
Ek pk αqαk
</equation>
<bodyText confidence="0.9990485">
even machine learner driven tools. However, in the
remainder of this paper we will only look at fre-
quency based similarity metrics since these met-
rics are easily applicable and the experiments con-
ducted using these metrics were already encourag-
ing.
</bodyText>
<sectionHeader confidence="0.989418" genericHeader="method">
3 Experimental design
</sectionHeader>
<subsectionHeader confidence="0.997449">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999916590909091">
We used data extracted from the British National
Corpus (BNC) (2001) and consisting of written
books and periodicals1. The BNC annotators pro-
vided 9 domain codes (i.e. wridom), making it
possible to divide the text from books and peri-
odicals into 9 subcorpora. These annotated se-
mantic domains are: imaginative (wridom1), nat-
ural &amp; pure science (wridom2), applied science
(wridom3), social science (wridom4), world af-
fairs (wridom5), commerce &amp; finance (wridom6),
arts (wridom7), belief &amp; thought (wridom8), and
leisure (wridom9).
The extracted corpus contains sentences in
which every token is tagged with a part-of-speech
tag as defined by the BNC. Since the BNC has
been tagged automatically, using the CLAWS4 au-
tomatic tagger (Leech et al., 1994) and the Tem-
plate Tagger (Pacey et al., 1997), the experiments
in this article are artificial in the sense that they do
not learn real part-of-speech tags but rather part-
of-speech tags as they are assigned by the auto-
matic taggers.
</bodyText>
<subsectionHeader confidence="0.999514">
3.2 Similarity metrics
</subsectionHeader>
<bodyText confidence="0.999880666666667">
To measure the difference between two corpora
we implemented six similarity metrics: R´enyi2
(R´enyi, 1961), Variational (L1) (Lee, 2001),
Euclidean (Lee, 2001), Cosine (Lee, 2001),
Kullback-Leibler (Kullback and Leibler, 1951)
and Bhattacharyya coefficient (Comaniciu et al.,
2003; Bhattacharyya, 1943). We selected these
measures because they are well-described and pro-
duce results for this task in an acceptable time
span.
The metrics are computed using the relative fre-
quencies of words. For example, to calculate the
</bodyText>
<footnote confidence="0.995153833333333">
1This is done by selecting texts with BNC category codes
for text type (i.e. alltyp3 (written books and periodicals)) and
for medium (i.e. wrimed1 (book), wrimed2 (periodical), and
wrimed3 (miscellaneous: published)).
2The R´enyi divergence has a parameter α and Kullback-
Leibler is a special case of the R´enyi divergence, viz. with
</footnote>
<equation confidence="0.41122">
α = 1.
</equation>
<bodyText confidence="0.99348">
pk is the relative frequency of a token k in the
first corpus P, and qk is the relative frequency of
token k in the second corpus Q. α is a free param-
eter and with α = 1 the R´enyi divergence becomes
equivalent to the Kullback-Leibler divergence.
</bodyText>
<figureCaption confidence="0.8072484">
Figure 1: A visual comparison of two similarity
metrics: R´enyi with α = 0.99 and Euclidean.
Figure 1 gives an impression of the difference
between two similarity metrics: R´enyi (α = 0.99)
and Euclidean. Only four domain combinations
</figureCaption>
<bodyText confidence="0.997814904761905">
are shown for the sake of clarity. From the graph
it can be observed that the social and imaginative
domains are the least similar in both cases. Be-
sides the different ordering, there is also a differ-
ence in symmetry. Contrary to the symmetric Eu-
clidean metric, the R´enyi scores differ, depending
on whether social constitutes the test set and art
the training set, or vice versa. The dashed line on
Figure 1 (left) is a reverse score, namely for art-
social. A divergence score may diverge a lot from
its reverse score.
In practice, the best metric to choose is the met-
ric that gives the best linear correlation between
the metric and the accuracy of an NLP tool applied
across domains. We tested 6 metrics: R´enyi, Vari-
ational (L1), Euclidean, Cosine, Kullback-Leibler,
and the Bhattacharyya coefficient. For R´enyi, we
tested four different α-values: 0.95, 0.99, 1.05,
and 1.1. Most metrics gave a linear correlation
but for our experiments with data-driven POS tag-
ging, the R´enyi metric with α = 0.99 was the best
</bodyText>
<figure confidence="0.996355785714286">
R6nyi 0.99 Euclidean
MORE SIMILAR
social-world
social-art
social-belief
MORE SIMILAR
social-belief
social-world
social-art
art-social
social-imaginative
social-imaginative
LESS SIMILAR
LESS SIMILAR
</figure>
<page confidence="0.994424">
32
</page>
<bodyText confidence="0.999809333333333">
according to the Pearson product-moment corre-
lation. For majority this correlation was 0.91, for
Mbt 0.93, and for SVMTool 0.93.
</bodyText>
<subsectionHeader confidence="0.999002">
3.3 Part-of-speech tagging
</subsectionHeader>
<bodyText confidence="0.99990005">
The experiments carried out in the scope of this
article are all part-of-speech (POS) tagging tasks.
There are 91 different POS labels in the BNC cor-
pus which are combinations of 57 basic labels. We
used three algorithms to assign part-of-speech la-
bels to the words from the test corpus:
Majority This algorithm assigns the POS label
that occurs most frequently in the training set for a
given word, to the word in the test set. If the word
did not occur in train, the overall most frequent tag
was used.
Memory based POS tagger (Daelemans and
van den Bosch, 2005) A machine learner that
stores examples in memory (Mbt) and uses the
kNN algorithm to assign POS labels. The default
settings were used.
SVMTool POS tagger (Gim´enez and M´arquez,
2004) Support vectors machines in a sequential
setup are used to assign the POS labels. The de-
fault settings were used.
</bodyText>
<subsectionHeader confidence="0.607746">
3.4 Results and analysis
</subsectionHeader>
<bodyText confidence="0.999662958333333">
Figure 2 shows the outcome of 72 cross-validation
experiments on the data from the British National
Corpus. The graph for the majority baseline is
shown in Figure 2a. The results for the memory
based tagger are shown in Figure 2b and the graph
for SVMTool is displayed in Figure 2c.
For every domain, the data is divided into five
parts. For all pairs of domains, each part from
the training domain is paired with each part from
the testing domain. This results in a 25 cross-
validation cross-domain experiment. A data point
in Figure 2 is the average outcome of such a 25
fold experiment. The abscissa of a data point
is the R´enyi similarity score between the train-
ing and testing component of an experiment. The
α parameter was set to 0.99. We propose that
the higher (less negative) the similarity score, the
more similar training and testing data are.
The ordinate is the accuracy of the POS tagging
experiment. The dotted lines are the 95% predic-
tion intervals for every data point. These bound-
aries are obtained by linear regression using all
other data points. The interpretation of the inter-
vals is that any point, given all other data points
</bodyText>
<figure confidence="0.998253">
(a) Majority POS tagger.
(b) Memory based POS tagger.
SVMTool accuracy prediction
95
94
93
92
91
90
95% prediction interval
(72 data points)
25 20 15 10 5
Rényi divergence score with alpha=0.99
(c) SVMTool POS tagger.
</figure>
<figureCaption confidence="0.964085">
Figure 2: The varying accuracy of three POS tag-
gers with varying distance between train and test
corpus of different domains.
</figureCaption>
<bodyText confidence="0.999804">
from the graph, can be predicted with 95% cer-
tainty, to lie between the upper and lower interval
boundary at the similarity score of that point. The
average difference between the lower and the up-
per interval boundary is 4.36% for majority, 1.92%
for Mbt and 1.59% for SVMTool. This means that,
</bodyText>
<figure confidence="0.970056558823529">
Majority accuracy prediction
95% prediction interval
(72 data points)
25 20 15 10 5
Rényi divergence score with alpha=0.99
Majority accuracy (%) 92
90
88
86
84
82
80
78
76
74
Mbt accuracy prediction
95% prediction interval
(72 data points)
25 20 15 10 5
Rényi divergence score with alpha=0.99
Mbt accuracy (%) 95
94
93
92
91
90
89
88
87
86
SVMTool accuracy (%)
96
89
88
</figure>
<page confidence="0.992068">
33
</page>
<table confidence="0.956684666666667">
Majority Mbt SVMTool
average accuracy 84.94 91.84 93.48
standard deviation 2.50 1.30 1.07
</table>
<tableCaption confidence="0.999786">
Table 1: Average accuracy and standard deviation on 72 cross-validation experiments.
</tableCaption>
<bodyText confidence="0.999962711111111">
when taking the middle of the interval as the ex-
pected accuracy, the maximum error is 0.8% for
SVMTool. Since the difference between the best
and worst accuracy score is 4.93%, using linear re-
gression means that one can predict the accuracy
three times better. For Mbt with a range of 5.84%
between best and worst accuracy and for majority
with 12.7%, a similar figure is obtained.
Table 1 shows the average accuracies of the al-
gorithms for all 72 experiments. For this article,
the absolute accuracy of the algorithms is not un-
der consideration. Therefore, no effort has been
made to improve on these accuracy scores. One
can see that the standard deviation for SVMTool
and Mbt is lower than for majority, suggesting that
these algorithms are less susceptible to domain
variation.
The good linear fit for the graphs of Figure 2
cannot be reproduced with every algorithm. For
algorithms that do not have a sufficiently strong re-
lation between training corpus and assigned class
label, the linear relation is lost. Clearly, it remains
feasible to compute an interval for the data points,
but as a consequence of the non-linearity, the pre-
dicted intervals would be similar or even bigger
than the difference between the lowest and highest
accuracy score.
In Figure 3 the experiments of Figure 2 are
reproduced using test and training sets from the
same domain. Since we used the same data sets
as for the out-of-domain experiments, we had to
carry out 20 fold cross-validation for these exper-
iments. Because of this different setup the results
are shown in a different figure. There is a data
point for every domain.
Although the average distance between test and
training set are smaller for in-domain experiments,
we still observe a linear relation for Mbt and SVM,
for majority there is still a visual hint of linearity.
For in-domain the biggest difference between test
and train set is for the leisure domain (R´enyi score:
-6.0) which is very close to the smallest out-of-
domain difference (-6.3 for social sciences–world
affairs). This could mean that the random varia-
tion between test and train can approach the varia-
</bodyText>
<figure confidence="0.9655535">
Majority accuracy prediction
6.5 6.0 5.5 5.0 4.5 4.0
Rényi divergence score with alpha=0.99
(a) Majority POS tagger.
(b) Memory based POS tagger.
(c) SVMTool POS tagger.
</figure>
<figureCaption confidence="0.995672">
Figure 3: The varying accuracy of three POS tag-
gers with varying distance between train and test
corpus of the same domain.
</figureCaption>
<figure confidence="0.933553454545455">
95% prediction interval
(9 data points)
Mbt accuracy prediction
95% prediction interval
(9 data points)
6.5 6.0 5.5 5.0 4.5 4.0
Rényi divergence score with alpha=0.99
accuracy (%) 95.5
95.0
94.5
94.0
93.5
93.0
92.5
SVMTool accuracy prediction
95% prediction interval
(9 data points)
6.5 6.0 5.5 5.0 4.5 4.0
Rényi divergence score with alpha=0.99
accuracy (%) 96.5
96.0
95.5
95.0
94.5
94.0
93.5
accuracy (%) 91
90
89
88
87
86
85
</figure>
<page confidence="0.997938">
34
</page>
<bodyText confidence="0.999987857142857">
tion between domains but this observation is made
in abstraction from the different data set sizes for
in and out of domain experiments. For majority
the average accuracy over all domains is 88.25%
(stdev: 0.87), for Mbt 94.07% (0.63), and for
SVMTool 95.06% (0.59). Which are, as expected,
higher scores than the figures in Table 1.
</bodyText>
<sectionHeader confidence="0.999888" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999976580645161">
In articles dealing with the influence of domain
shifts on the performance of an NLP tool, the
in-domain data and out-of-domain data are taken
from different corpora, e.g., sentences from movie
snippets, newspaper texts and personal weblogs
(Andreevskaia and Bergler, 2008). It can be ex-
pected that these corpora are indeed dissimilar
enough to consider them as separate domains, but
no objective measure has been used to define them
as such. The fact that the NLP tool produces
lower results for cross-domain experiments can be
taken as an indication of the presence of sepa-
rate domains. A nice overview paper on statisti-
cal domain adaptation can be found in Bellegarda
(2004).
A way to express the degree of relatedness,
apart from this well-known accuracy drop, can be
found in Daum´e and Marcu (2006). They propose
a domain adaptation framework containing a pa-
rameter 7r. Low values of 7r mean that in-domain
and out-of-domain data differ significantly. They
also used Kullback-Leibler divergence to compute
the similarity between unigram language models.
Blitzer et al. (2007) propose a supervised way
of measuring the similarity between the two do-
mains. They compute the Huber loss, as a proxy
of the A-distance (Kifer et al., 2004), for every
instance that they labeled with their tool. The re-
sulting measure correlates with the adaptation loss
they observe when applying a sentiment classifi-
cation tool on different domains.
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.988378134615385">
This paper showed that it is possible to narrow
down the prediction of the accuracy of an NLP
tool on an unannotated corpus by measuring the
similarity between this unannotated corpus and the
corpus the tagger was trained on in an unsuper-
vised way. A prerequisite to be able to make a reli-
able prediction, is to have sufficient annotated data
to measure the correlation between the accuracy
and a metric. We observed that, in order to make a
prediction interval that is narrower than the differ-
ence between the lowest and highest accuracy on
the annotated corpora, the algorithm used, should
capture sufficient information from training.
The observation that it is feasible to make re-
liable predictions using unannotated data, can be
of help when training a system for a task in a do-
main for which no annotated data is available. As
a first step, the metric resulting in the best linear
fit between the metric and the accuracy should be
searched. If a linear relation can be established,
one can take annotated training data from the do-
main that is closest to the unannotated corpus and
assume that this will give the best accuracy score.
In this article we implemented a way to mea-
sure the similarity between two corpora. One may
decide to use such a metric to categorize the avail-
able corpora for a given task into groups, depend-
ing on their similarity. It should be noted that in
order to do this, a symmetric metric should be
used. Indeed, an asymmetric metric like the R´enyi
divergence will give a different value depending
on whether the similarity between corpus P and
corpus Q is measured as Renyi(P; Q; α) or as
Renyi(Q; P; α).
Further research should explore the usability of
linear regression for other NLP tasks. Although
no specific adaptation to the POS tagging task was
made, it may not be straightforward to find a lin-
ear relation for more complicated tasks. For such
tasks, it may be useful to insert n-grams into the
metric. Or, if a parser was first applied to the data,
it is possible to insert syntactic features in the met-
ric. Of course, these adaptations may influence
the efficiency of the metric, but if a good linear
relation between the metric and the accuracy can
be found, the metric is useful. Another option to
make the use of the metric less task dependent is
by not using the distribution of the tokens but by
using distributions of the features used by the ma-
chine learner. Applying this more generic setup of
our experiments to other NLP tools may lead to the
discovery of a metric that is generally applicable.
</bodyText>
<sectionHeader confidence="0.998289" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.976952">
This research was made possible through finan-
cial support from the University of Antwerp
(BIOGRAPH GOA-project).
</bodyText>
<page confidence="0.997429">
35
</page>
<note confidence="0.758883">
Daniel Kifer, Shai Ben-David, and Johannes Gehrke.
2004. Detecting change in data streams. Proceed-
ings of the 30th Very Large Data Bases Conference
(VLDB’04), 180–191. VLDB Endowment. Toronto,
Canada.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation and Compiling, vol-
ume 1. Prentice-Hall, Englewood Cliffs, NJ.
</note>
<reference confidence="0.998674333333333">
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Over-
coming Domain Dependence in Sentiment Tagging.
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08:HLT), 290–298. As-
sociation for Computational Linguistics. Columbus,
Ohio, USA.
Jerome R. Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. Speech
Communication, 42:93–108.
Anil Bhattacharyya. 1943. On a measure of divergence
between two statistical populations defined by their
probability distributions. Bulletin of the Calcutta
Mathematical Society, 35:99–109.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes and
Blenders: Domain Adaptation for Sentiment Clas-
sification. Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
440–447. Association for Computational Linguis-
tics. Prague, Czech Republic.
British National Corpus Consortium. 2001. The
British National Corpus, version 2 (BNC World).
Distributed by Oxford University Computing
Services on behalf of the BNC Consortium.
http://www.natcorp.ox.ac.uk (Last accessed: April
2, 2010).
Dorin Comaniciu, Visvanathan Ramesh, and Peter
Meer. 2003. Kernel-Based Object Tracking. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 25(5):564–575.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing. Cambridge
University Press, Cambridge, UK.
Hal Daum´e III and Daniel Marcu. 2006. Domain
Adaptation for Statistical Classifiers. Journal of Ar-
tificial Intelligence Research, 26:101–126.
T. Mark Ellison and Simon Kirby. 2006. Measuring
Language Divergence by Intra-Lexical Comparison.
Proceedings of the 21&amp;quot; International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, 273–280. Association for Computa-
tional Linguistics. Sidney, Australia.
Jes´us Gim´enez and Llu´ıs M´arquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation (LREC’04), 43–46. European Language Re-
sources Association. Lisbon, Portugal.
Solomon Kullback and Richard. A. Leibler. 1951. On
Information and Sufficiency. The Annals of Mathe-
matical Statistics, 22(1):79–86.
Lillian Lee. 2001. On the Effectiveness
of the Skew Divergence for Statistical Lan-
guage Analysis. 8th International Workshop
on Artificial Intelligence and Statistics (AISTATS
2001), 65–72. Florida, USA. Online reposi-
tory http://www.gatsby.ucl.ac.uk/aistats/aistats2001
(Last accessed: April 2, 2010).
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. CLAWS4: The tagging of the British Na-
tional Corpus. Proceedings of the 15th International
Conference on Computational Linguistics (COLING
94), 622–628. Kyoto, Japan.
Michael Pacey, Steven Fligelstone, and Paul Rayson.
1997. How to generalize the task of annotation.
Corpus Annotation: Linguistic Information from
Computer Text Corpora, 122–136. London: Long-
man.
Alfr´ed R´enyi. 1961. On measures of information
and entropy. Proceedings of the 4th Berkeley Sym-
posium on Mathematics, Statistics and Probability,
1:547–561. University of California Press. Berke-
ley, California, USA.
</reference>
<page confidence="0.998943">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.053285">
<title confidence="0.999975">Using Domain Similarity for Performance Estimation</title>
<author confidence="0.999843">Vincent Van</author>
<affiliation confidence="0.560506">CLiPS - University of Antwerp,</affiliation>
<email confidence="0.510783">Vincent.VanAsch@ua.ac.be</email>
<author confidence="0.67288">Walter</author>
<affiliation confidence="0.5681175">CLiPS - University of Antwerp,</affiliation>
<email confidence="0.837962">Walter.Daelemans@ua.ac.be</email>
<abstract confidence="0.999840636363636">Many natural language processing (NLP) tools exhibit a decrease in performance when they are applied to data that is linguistically different from the corpus used during development. This makes it hard to develop NLP tools for domains for which annotated corpora are not available. This paper explores a number of metrics that attempt to predict the cross-domain performance of an NLP tool through statistical inference. We apply different similarity metrics to compare different domains and investigate the correlation between similarity and accuracy loss of NLP tool. We find that the correlation between the performance of the tool and the similarity metric is linear and that the latter can therefore be used to predict the performance of an NLP tool on out-of-domain data. The approach also provides a way to quantify the difference between domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alina Andreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging.</title>
<date>2008</date>
<booktitle>Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), 290–298. Association for Computational Linguistics.</booktitle>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="14735" citStr="Andreevskaia and Bergler, 2008" startWordPosition="2461" endWordPosition="2464">86 85 34 tion between domains but this observation is made in abstraction from the different data set sizes for in and out of domain experiments. For majority the average accuracy over all domains is 88.25% (stdev: 0.87), for Mbt 94.07% (0.63), and for SVMTool 95.06% (0.59). Which are, as expected, higher scores than the figures in Table 1. 4 Related Work In articles dealing with the influence of domain shifts on the performance of an NLP tool, the in-domain data and out-of-domain data are taken from different corpora, e.g., sentences from movie snippets, newspaper texts and personal weblogs (Andreevskaia and Bergler, 2008). It can be expected that these corpora are indeed dissimilar enough to consider them as separate domains, but no objective measure has been used to define them as such. The fact that the NLP tool produces lower results for cross-domain experiments can be taken as an indication of the presence of separate domains. A nice overview paper on statistical domain adaptation can be found in Bellegarda (2004). A way to express the degree of relatedness, apart from this well-known accuracy drop, can be found in Daum´e and Marcu (2006). They propose a domain adaptation framework containing a parameter 7</context>
</contexts>
<marker>Andreevskaia, Bergler, 2008</marker>
<rawString>Alina Andreevskaia and Sabine Bergler. 2008. When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging. Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), 290–298. Association for Computational Linguistics. Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Statistical language model adaptation: review and perspectives.</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<pages>42--93</pages>
<contexts>
<context position="1193" citStr="Bellegarda, 2004" startWordPosition="183" endWordPosition="184"> predict the cross-domain performance of an NLP tool through statistical inference. We apply different similarity metrics to compare different domains and investigate the correlation between similarity and accuracy loss of NLP tool. We find that the correlation between the performance of the tool and the similarity metric is linear and that the latter can therefore be used to predict the performance of an NLP tool on out-of-domain data. The approach also provides a way to quantify the difference between domains. 1 Introduction Domain adaptation has recently turned into a broad field of study (Bellegarda, 2004). Many researchers note that the linguistic variation between training and testing corpora is an important factor in assessing the performance of an NLP tool across domains. For example, a tool that has been developed to extract predicate-argument structures from abstracts of biomedical research papers, will exhibit a lower performance when applied to legal texts. However, the notion of domain is mostly arbitrarily used to refer to some kind of semantic area. There is unfortunately no unambiguous measure to assert a domain shift, except by observing the performance loss of an NLP tool when app</context>
<context position="15139" citStr="Bellegarda (2004)" startWordPosition="2533" endWordPosition="2534">he performance of an NLP tool, the in-domain data and out-of-domain data are taken from different corpora, e.g., sentences from movie snippets, newspaper texts and personal weblogs (Andreevskaia and Bergler, 2008). It can be expected that these corpora are indeed dissimilar enough to consider them as separate domains, but no objective measure has been used to define them as such. The fact that the NLP tool produces lower results for cross-domain experiments can be taken as an indication of the presence of separate domains. A nice overview paper on statistical domain adaptation can be found in Bellegarda (2004). A way to express the degree of relatedness, apart from this well-known accuracy drop, can be found in Daum´e and Marcu (2006). They propose a domain adaptation framework containing a parameter 7r. Low values of 7r mean that in-domain and out-of-domain data differ significantly. They also used Kullback-Leibler divergence to compute the similarity between unigram language models. Blitzer et al. (2007) propose a supervised way of measuring the similarity between the two domains. They compute the Huber loss, as a proxy of the A-distance (Kifer et al., 2004), for every instance that they labeled </context>
</contexts>
<marker>Bellegarda, 2004</marker>
<rawString>Jerome R. Bellegarda. 2004. Statistical language model adaptation: review and perspectives. Speech Communication, 42:93–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anil Bhattacharyya</author>
</authors>
<title>On a measure of divergence between two statistical populations defined by their probability distributions.</title>
<date>1943</date>
<journal>Bulletin of the Calcutta Mathematical Society,</journal>
<pages>35--99</pages>
<contexts>
<context position="5623" citStr="Bhattacharyya, 1943" startWordPosition="924" endWordPosition="925">ically, using the CLAWS4 automatic tagger (Leech et al., 1994) and the Template Tagger (Pacey et al., 1997), the experiments in this article are artificial in the sense that they do not learn real part-of-speech tags but rather partof-speech tags as they are assigned by the automatic taggers. 3.2 Similarity metrics To measure the difference between two corpora we implemented six similarity metrics: R´enyi2 (R´enyi, 1961), Variational (L1) (Lee, 2001), Euclidean (Lee, 2001), Cosine (Lee, 2001), Kullback-Leibler (Kullback and Leibler, 1951) and Bhattacharyya coefficient (Comaniciu et al., 2003; Bhattacharyya, 1943). We selected these measures because they are well-described and produce results for this task in an acceptable time span. The metrics are computed using the relative frequencies of words. For example, to calculate the 1This is done by selecting texts with BNC category codes for text type (i.e. alltyp3 (written books and periodicals)) and for medium (i.e. wrimed1 (book), wrimed2 (periodical), and wrimed3 (miscellaneous: published)). 2The R´enyi divergence has a parameter α and KullbackLeibler is a special case of the R´enyi divergence, viz. with α = 1. pk is the relative frequency of a token k</context>
</contexts>
<marker>Bhattacharyya, 1943</marker>
<rawString>Anil Bhattacharyya. 1943. On a measure of divergence between two statistical populations defined by their probability distributions. Bulletin of the Calcutta Mathematical Society, 35:99–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, 440–447. Association for Computational Linguistics.</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="15543" citStr="Blitzer et al. (2007)" startWordPosition="2593" endWordPosition="2596"> NLP tool produces lower results for cross-domain experiments can be taken as an indication of the presence of separate domains. A nice overview paper on statistical domain adaptation can be found in Bellegarda (2004). A way to express the degree of relatedness, apart from this well-known accuracy drop, can be found in Daum´e and Marcu (2006). They propose a domain adaptation framework containing a parameter 7r. Low values of 7r mean that in-domain and out-of-domain data differ significantly. They also used Kullback-Leibler divergence to compute the similarity between unigram language models. Blitzer et al. (2007) propose a supervised way of measuring the similarity between the two domains. They compute the Huber loss, as a proxy of the A-distance (Kifer et al., 2004), for every instance that they labeled with their tool. The resulting measure correlates with the adaptation loss they observe when applying a sentiment classification tool on different domains. 5 Conclusions and future work This paper showed that it is possible to narrow down the prediction of the accuracy of an NLP tool on an unannotated corpus by measuring the similarity between this unannotated corpus and the corpus the tagger was trai</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, 440–447. Association for Computational Linguistics. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>British National Corpus Consortium</author>
</authors>
<title>The British National Corpus, version 2 (BNC World). Distributed by</title>
<date>2001</date>
<booktitle>Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk (Last accessed:</booktitle>
<institution>Oxford University</institution>
<marker>Consortium, 2001</marker>
<rawString>British National Corpus Consortium. 2001. The British National Corpus, version 2 (BNC World). Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk (Last accessed: April 2, 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dorin Comaniciu</author>
<author>Visvanathan Ramesh</author>
<author>Peter Meer</author>
</authors>
<title>Kernel-Based Object Tracking.</title>
<date>2003</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>25</volume>
<issue>5</issue>
<contexts>
<context position="5601" citStr="Comaniciu et al., 2003" startWordPosition="920" endWordPosition="923"> has been tagged automatically, using the CLAWS4 automatic tagger (Leech et al., 1994) and the Template Tagger (Pacey et al., 1997), the experiments in this article are artificial in the sense that they do not learn real part-of-speech tags but rather partof-speech tags as they are assigned by the automatic taggers. 3.2 Similarity metrics To measure the difference between two corpora we implemented six similarity metrics: R´enyi2 (R´enyi, 1961), Variational (L1) (Lee, 2001), Euclidean (Lee, 2001), Cosine (Lee, 2001), Kullback-Leibler (Kullback and Leibler, 1951) and Bhattacharyya coefficient (Comaniciu et al., 2003; Bhattacharyya, 1943). We selected these measures because they are well-described and produce results for this task in an acceptable time span. The metrics are computed using the relative frequencies of words. For example, to calculate the 1This is done by selecting texts with BNC category codes for text type (i.e. alltyp3 (written books and periodicals)) and for medium (i.e. wrimed1 (book), wrimed2 (periodical), and wrimed3 (miscellaneous: published)). 2The R´enyi divergence has a parameter α and KullbackLeibler is a special case of the R´enyi divergence, viz. with α = 1. pk is the relative </context>
</contexts>
<marker>Comaniciu, Ramesh, Meer, 2003</marker>
<rawString>Dorin Comaniciu, Visvanathan Ramesh, and Peter Meer. 2003. Kernel-Based Object Tracking. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(5):564–575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
</authors>
<title>Memory-Based Language Processing.</title>
<date>2005</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<marker>Daelemans, van den Bosch, 2005</marker>
<rawString>Walter Daelemans and Antal van den Bosch. 2005. Memory-Based Language Processing. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain Adaptation for Statistical Classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>26--101</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain Adaptation for Statistical Classifiers. Journal of Artificial Intelligence Research, 26:101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mark Ellison</author>
<author>Simon Kirby</author>
</authors>
<title>Measuring Language Divergence by Intra-Lexical Comparison.</title>
<date>2006</date>
<booktitle>Proceedings of the 21&amp;quot; International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>273--280</pages>
<location>Sidney, Australia.</location>
<marker>Ellison, Kirby, 2006</marker>
<rawString>T. Mark Ellison and Simon Kirby. 2006. Measuring Language Divergence by Intra-Lexical Comparison. Proceedings of the 21&amp;quot; International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, 273–280. Association for Computational Linguistics. Sidney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M´arquez</author>
</authors>
<title>SVMTool: A general POS tagger generator based on Support Vector Machines.</title>
<date>2004</date>
<booktitle>Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04), 43–46. European Language Resources Association.</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Gim´enez, M´arquez, 2004</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M´arquez. 2004. SVMTool: A general POS tagger generator based on Support Vector Machines. Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04), 43–46. European Language Resources Association. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Leibler</author>
</authors>
<date>1951</date>
<booktitle>On Information and Sufficiency. The Annals of Mathematical Statistics,</booktitle>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="5547" citStr="Leibler, 1951" startWordPosition="915" endWordPosition="916">peech tag as defined by the BNC. Since the BNC has been tagged automatically, using the CLAWS4 automatic tagger (Leech et al., 1994) and the Template Tagger (Pacey et al., 1997), the experiments in this article are artificial in the sense that they do not learn real part-of-speech tags but rather partof-speech tags as they are assigned by the automatic taggers. 3.2 Similarity metrics To measure the difference between two corpora we implemented six similarity metrics: R´enyi2 (R´enyi, 1961), Variational (L1) (Lee, 2001), Euclidean (Lee, 2001), Cosine (Lee, 2001), Kullback-Leibler (Kullback and Leibler, 1951) and Bhattacharyya coefficient (Comaniciu et al., 2003; Bhattacharyya, 1943). We selected these measures because they are well-described and produce results for this task in an acceptable time span. The metrics are computed using the relative frequencies of words. For example, to calculate the 1This is done by selecting texts with BNC category codes for text type (i.e. alltyp3 (written books and periodicals)) and for medium (i.e. wrimed1 (book), wrimed2 (periodical), and wrimed3 (miscellaneous: published)). 2The R´enyi divergence has a parameter α and KullbackLeibler is a special case of the R</context>
</contexts>
<marker>Leibler, 1951</marker>
<rawString>Solomon Kullback and Richard. A. Leibler. 1951. On Information and Sufficiency. The Annals of Mathematical Statistics, 22(1):79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>On the Effectiveness of the Skew Divergence for Statistical Language Analysis.</title>
<date>2001</date>
<booktitle>8th International Workshop on Artificial Intelligence and Statistics (AISTATS 2001), 65–72. Florida, USA. Online repository http://www.gatsby.ucl.ac.uk/aistats/aistats2001 (Last accessed:</booktitle>
<contexts>
<context position="5457" citStr="Lee, 2001" startWordPosition="904" endWordPosition="905">he extracted corpus contains sentences in which every token is tagged with a part-of-speech tag as defined by the BNC. Since the BNC has been tagged automatically, using the CLAWS4 automatic tagger (Leech et al., 1994) and the Template Tagger (Pacey et al., 1997), the experiments in this article are artificial in the sense that they do not learn real part-of-speech tags but rather partof-speech tags as they are assigned by the automatic taggers. 3.2 Similarity metrics To measure the difference between two corpora we implemented six similarity metrics: R´enyi2 (R´enyi, 1961), Variational (L1) (Lee, 2001), Euclidean (Lee, 2001), Cosine (Lee, 2001), Kullback-Leibler (Kullback and Leibler, 1951) and Bhattacharyya coefficient (Comaniciu et al., 2003; Bhattacharyya, 1943). We selected these measures because they are well-described and produce results for this task in an acceptable time span. The metrics are computed using the relative frequencies of words. For example, to calculate the 1This is done by selecting texts with BNC category codes for text type (i.e. alltyp3 (written books and periodicals)) and for medium (i.e. wrimed1 (book), wrimed2 (periodical), and wrimed3 (miscellaneous: published)</context>
</contexts>
<marker>Lee, 2001</marker>
<rawString>Lillian Lee. 2001. On the Effectiveness of the Skew Divergence for Statistical Language Analysis. 8th International Workshop on Artificial Intelligence and Statistics (AISTATS 2001), 65–72. Florida, USA. Online repository http://www.gatsby.ucl.ac.uk/aistats/aistats2001 (Last accessed: April 2, 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Michael Bryant</author>
</authors>
<title>CLAWS4: The tagging of the British National Corpus.</title>
<date>1994</date>
<booktitle>Proceedings of the 15th International Conference on Computational Linguistics (COLING 94),</booktitle>
<pages>622--628</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="5065" citStr="Leech et al., 1994" startWordPosition="839" endWordPosition="842">tators provided 9 domain codes (i.e. wridom), making it possible to divide the text from books and periodicals into 9 subcorpora. These annotated semantic domains are: imaginative (wridom1), natural &amp; pure science (wridom2), applied science (wridom3), social science (wridom4), world affairs (wridom5), commerce &amp; finance (wridom6), arts (wridom7), belief &amp; thought (wridom8), and leisure (wridom9). The extracted corpus contains sentences in which every token is tagged with a part-of-speech tag as defined by the BNC. Since the BNC has been tagged automatically, using the CLAWS4 automatic tagger (Leech et al., 1994) and the Template Tagger (Pacey et al., 1997), the experiments in this article are artificial in the sense that they do not learn real part-of-speech tags but rather partof-speech tags as they are assigned by the automatic taggers. 3.2 Similarity metrics To measure the difference between two corpora we implemented six similarity metrics: R´enyi2 (R´enyi, 1961), Variational (L1) (Lee, 2001), Euclidean (Lee, 2001), Cosine (Lee, 2001), Kullback-Leibler (Kullback and Leibler, 1951) and Bhattacharyya coefficient (Comaniciu et al., 2003; Bhattacharyya, 1943). We selected these measures because they </context>
</contexts>
<marker>Leech, Garside, Bryant, 1994</marker>
<rawString>Geoffrey Leech, Roger Garside, and Michael Bryant. 1994. CLAWS4: The tagging of the British National Corpus. Proceedings of the 15th International Conference on Computational Linguistics (COLING 94), 622–628. Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Pacey</author>
<author>Steven Fligelstone</author>
<author>Paul Rayson</author>
</authors>
<title>How to generalize the task of annotation.</title>
<date>1997</date>
<booktitle>Corpus Annotation: Linguistic Information from Computer Text Corpora,</booktitle>
<pages>122--136</pages>
<publisher>Longman.</publisher>
<location>London:</location>
<contexts>
<context position="5110" citStr="Pacey et al., 1997" startWordPosition="848" endWordPosition="851"> making it possible to divide the text from books and periodicals into 9 subcorpora. These annotated semantic domains are: imaginative (wridom1), natural &amp; pure science (wridom2), applied science (wridom3), social science (wridom4), world affairs (wridom5), commerce &amp; finance (wridom6), arts (wridom7), belief &amp; thought (wridom8), and leisure (wridom9). The extracted corpus contains sentences in which every token is tagged with a part-of-speech tag as defined by the BNC. Since the BNC has been tagged automatically, using the CLAWS4 automatic tagger (Leech et al., 1994) and the Template Tagger (Pacey et al., 1997), the experiments in this article are artificial in the sense that they do not learn real part-of-speech tags but rather partof-speech tags as they are assigned by the automatic taggers. 3.2 Similarity metrics To measure the difference between two corpora we implemented six similarity metrics: R´enyi2 (R´enyi, 1961), Variational (L1) (Lee, 2001), Euclidean (Lee, 2001), Cosine (Lee, 2001), Kullback-Leibler (Kullback and Leibler, 1951) and Bhattacharyya coefficient (Comaniciu et al., 2003; Bhattacharyya, 1943). We selected these measures because they are well-described and produce results for th</context>
</contexts>
<marker>Pacey, Fligelstone, Rayson, 1997</marker>
<rawString>Michael Pacey, Steven Fligelstone, and Paul Rayson. 1997. How to generalize the task of annotation. Corpus Annotation: Linguistic Information from Computer Text Corpora, 122–136. London: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfr´ed R´enyi</author>
</authors>
<title>On measures of information and entropy.</title>
<date>1961</date>
<booktitle>Proceedings of the 4th Berkeley Symposium on Mathematics, Statistics and Probability,</booktitle>
<pages>1--547</pages>
<institution>University of California Press. Berkeley,</institution>
<location>California, USA.</location>
<marker>R´enyi, 1961</marker>
<rawString>Alfr´ed R´enyi. 1961. On measures of information and entropy. Proceedings of the 4th Berkeley Symposium on Mathematics, Statistics and Probability, 1:547–561. University of California Press. Berkeley, California, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>