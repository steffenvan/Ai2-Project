<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.995929">
AutoExtend: Extending Word Embeddings to Embeddings for Synsets
and Lexemes
</title>
<author confidence="0.993771">
Sascha Rothe and Hinrich Sch¨utze
</author>
<affiliation confidence="0.9973905">
Center for Information &amp; Language Processing
University of Munich
</affiliation>
<email confidence="0.99528">
sascha@cis.lmu.de
</email>
<sectionHeader confidence="0.997341" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997716">
We present AutoExtend, a system to learn
embeddings for synsets and lexemes. It is
flexible in that it can take any word embed-
dings as input and does not need an addi-
tional training corpus. The synset/lexeme
embeddings obtained live in the same vec-
tor space as the word embeddings. A
sparse tensor formalization guarantees ef-
ficiency and parallelizability. We use
WordNet as a lexical resource, but Auto-
Extend can be easily applied to other
resources like Freebase. AutoExtend
achieves state-of-the-art performance on
word similarity and word sense disam-
biguation tasks.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999862">
Unsupervised methods for word embeddings (also
called “distributed word representations”) have
become popular in natural language processing
(NLP). These methods only need very large cor-
pora as input to create sparse representations (e.g.,
based on local collocations) and project them into
a lower dimensional dense vector space. Examples
for word embeddings are SENNA (Collobert and
Weston, 2008), the hierarchical log-bilinear model
(Mnih and Hinton, 2009), word2vec (Mikolov et
al., 2013c) and GloVe (Pennington et al., 2014).
However, there are many other resources that are
undoubtedly useful in NLP, including lexical re-
sources like WordNet and Wiktionary and knowl-
edge bases like Wikipedia and Freebase. We will
simply call these resources in the rest of the pa-
per. Our goal is to enrich these valuable resources
with embeddings for those data types that are not
words; e.g., we want to enrich WordNet with em-
beddings for synsets and lexemes. A synset is a set
of synonyms that are interchangeable in some con-
text. A lexeme pairs a particular spelling or pro-
nunciation with a particular meaning, i.e., a lex-
eme is a conjunction of a word and a synset. Our
premise is that many NLP applications will bene-
fit if the non-word data types of resources – e.g.,
synsets in WordNet – are also available as embed-
dings. For example, in machine translation, en-
riching and improving translation dictionaries (cf.
Mikolov et al. (2013b)) would benefit from these
embeddings because they would enable us to cre-
ate an enriched dictionary for word senses. Gen-
erally, our premise is that the arguments for the
utility of embeddings for word forms should carry
over to the utility of embeddings for other data
types like synsets in WordNet.
The insight underlying the method we propose
is that the constraints of a resource can be formal-
ized as constraints on embeddings and then allow
us to extend word embeddings to embeddings of
other data types like synsets. For example, the hy-
ponymy relation in WordNet can be formalized as
such a constraint.
The advantage of our approach is that it de-
couples embedding learning from the extension of
embeddings to non-word data types in a resource.
If somebody comes up with a better way of learn-
ing embeddings, these embeddings become imme-
diately usable for resources. And we do not rely on
any specific properties of embeddings that make
them usable in some resources, but not in others.
An alternative to our approach is to train embed-
dings on annotated text, e.g., to train synset em-
beddings on corpora annotated with synsets. How-
ever, successful embedding learning generally re-
quires very large corpora and sense labeling is too
expensive to produce corpora of such a size.
Another alternative to our approach is to add up
all word embedding vectors related to a particular
node in a resource; e.g., to create the synset vector
of lawsuit in WordNet, we can add the word vec-
tors of the three words that are part of the synset
(lawsuit, suit, case). We will call this approach
</bodyText>
<page confidence="0.850855">
1793
</page>
<note confidence="0.973224">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1793–1803,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999079032258065">
naive and use it as a baseline (Snaive in Table 3).
We will focus on WordNet (Fellbaum, 1998) in
this paper, but our method – based on a formaliza-
tion that exploits the constraints of a resource for
extending embeddings from words to other data
types – is broadly applicable to other resources in-
cluding Wikipedia and Freebase.
A word in WordNet can be viewed as a compo-
sition of several lexemes. Lexemes from different
words together can form a synset. When a synset
is given, it can be decomposed into its lexemes.
And these lexemes then join to form words. These
observations are the basis for the formalization of
the constraints encoded in WordNet that will be
presented in the next section: we view words as
the sum of their lexemes and, analogously, synsets
as the sum of their lexemes.
Another motivation for our formalization stems
from the analogy calculus developed by Mikolov
et al. (2013a), which can be viewed as a group
theory formalization of word relations: we have
a set of elements (our vectors) and an operation
(addition) satisfying the properties of a mathemat-
ical group, in particular, associativity and invert-
ibility. For example, you can take the vector of
king, subtract the vector of man and add the vec-
tor of woman to get a vector near queen. In other
words, you remove the properties of man and add
the properties of woman. We can also see the vec-
tor of king as the sum of the vector of man and the
vector of a gender-neutral ruler. The next thing
to notice is that this does not only work for words
that combine several properties, but also for words
that combine several senses. The vector of suit can
be seen as the sum of a vector representing law-
suit and a vector representing business suit. Auto-
Extend is designed to take word vectors as input
and unravel the word vectors to the vectors of their
lexemes. The lexeme vectors will then give us the
synset vectors.
The main contributions of this paper are: (i)
We present AutoExtend, a flexible method that ex-
tends word embeddings to embeddings of synsets
and lexemes. AutoExtend is completely general in
that it can be used for any set of embeddings and
for any resource that imposes constraints of a cer-
tain type on the relationship between words and
other data types. (ii) We show that AutoExtend
achieves state-of-the-art word similarity and word
sense disambiguation (WSD) performance. (iii)
We publish the AutoExtend code for extending
word embeddings to other data types, the lexeme
and synset embeddings and the software to repli-
cate our WSD evaluation.
This paper is structured as follows. Section 2 in-
troduces the model, first as a general tensor formu-
lation then as a matrix formulation making addi-
tional assumptions. In Section 3, we describe data,
experiments and evaluation. We analyze Auto-
Extend in Section 4 and give a short summary on
how to extend our method to other resources in
Section 5. Section 6 discusses related work.
</bodyText>
<sectionHeader confidence="0.98174" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.999987666666667">
We are looking for a model that extends standard
embeddings for words to embeddings for the other
two data types in WordNet: synsets and lexemes.
We want all three data types – words, lexemes,
synsets – to live in the same embedding space.
The basic premise of our model is: (i) words are
sums of their lexemes and (ii) synsets are sums of
their lexemes. We refer to these two premises as
synset constraints. For example, the embedding
of the word bloom is a sum of the embeddings of
its two lexemes bloom(organ) and bloom(period);
and the embedding of the synset flower-bloom-
blossom(organ) is a sum of the embeddings of
its three lexemes flower(organ), bloom(organ) and
blossom(organ).
The synset constraints can be argued to be the
simplest possible relationship between the three
WordNet data types. They can also be motivated
by the way many embeddings are learned from
corpora – for example, the counts in vector space
models are additive, supporting the view of words
as the sum of their senses. The same assumption
is frequently made; for example, it underlies the
group theory formalization of analogy discussed
in Section 1.
We denote word vectors as w(i) ∈ Rn, synset
vectors as s(j) ∈ Rn, and lexeme vectors as l(i,j) ∈
Rn. l(i,j) is that lexeme of word w(i) that is a mem-
ber of synset s(j). We set lexeme vectors l(i,j) that
do not exist to zero. For example, the non-existing
lexeme flower(truck) is set to zero. We can then
formalize our premise that the two constraints (i)
and (ii) hold as follows:
</bodyText>
<equation confidence="0.959748571428572">
�w(i) _ l(i,j) (1)
j
s(j) _ � l(i,j) (2)
i
1794
not exist: Ei,d1 = 0 if l(i,j) = 0; i.e., l(i,j) =� 0
j,d2
</equation>
<bodyText confidence="0.99390975">
only if word i has a lexeme that is a member of
synset j. To summarize the sparsity:
These two equations are underspecified. We there-
fore introduce the matrix E(i,j) E Rnxn:
</bodyText>
<equation confidence="0.999826">
l(i,j) = E(i,j)w(i) (3)
</equation>
<bodyText confidence="0.999761666666667">
We make the assumption that the dimensions in
Eq. 3 are independent of each other, i.e., E(i,j)
is a diagonal matrix. Our motivation for this as-
sumption is: (i) This makes the computation tech-
nically feasible by significantly reducing the num-
ber of parameters and by supporting parallelism.
(ii) Treating word embeddings on a per-dimension
basis is a frequent design choice (e.g., Kalchbren-
ner et al. (2014)). Note that we allow E(i,j) &lt; 0
and in general the distribution weights for each di-
mension (diagonal entries of E(i,j)) will be differ-
ent. Our assumption can be interpreted as word
w(i) distributing its embedding activations to its
lexemes on each dimension separately. Therefore,
Eqs. 1-2 can be written as follows:
</bodyText>
<equation confidence="0.982009875">
�w(i) = E(i,j)w(i) (4)
j
s(j) = � E(i,j)w(i) (5)
i
Note that from Eq. 4 it directly follows that:
`
E(i,j) = In Vi (6)
j
</equation>
<bodyText confidence="0.9991285">
with In being the identity matrix.
Let W be a |W  |x n matrix where n is the di-
mensionality of the embedding space, |W |is the
number of words and each row w(i) is a word em-
bedding; and let S be a |S|xn matrix where |S |is
the number of synsets and each row s(j) is a synset
embedding. W and S can be interpreted as linear
maps and a mapping between them is given by the
rank 4 tensor E E R|S|xnx|W|xn. We can then
write Eq. 5 as a tensor product:
</bodyText>
<equation confidence="0.991929">
S = E O W (7)
while Eq. 6 states, that
Ei,d1 = 0 ,#= d1 =� d2 � l(i,j) = 0 (9)
j,d2
</equation>
<subsectionHeader confidence="0.98666">
2.1 Learning
</subsectionHeader>
<bodyText confidence="0.998338714285714">
We adopt an autoencoding framework to learn em-
beddings for lexemes and synsets. To this end, we
view the tensor equation S = E O W as the en-
coding part of the autoencoder: the synsets are the
encoding of the words. We define a corresponding
decoding part that decodes the synsets into words
as follows:
</bodyText>
<equation confidence="0.9494792">
s(j) = � l(i,j),w(i) = � l(i,j) (10)
i j
In analogy to E(i,j), we introduce the diagonal ma-
trix D(j,i):
l(i,j) = D(j,i)s(j) (11)
</equation>
<bodyText confidence="0.9846975">
In this case, it is the synset that distributes itself to
its lexemes. We can then rewrite Eq. 10 to:
</bodyText>
<equation confidence="0.974704">
s(j) = � D(j,i)s(j), w(i) = � D(j,i)s(j) (12)
i j
</equation>
<bodyText confidence="0.937208">
and we also get the equivalent of Eq. 6 for D(j,i):
</bodyText>
<equation confidence="0.8970265">
� D(j,i) = In Vj (13)
i
</equation>
<bodyText confidence="0.935147">
and in tensor notation:
</bodyText>
<equation confidence="0.635825">
W=DOS (14)
</equation>
<bodyText confidence="0.9994905">
Normalization and sparseness properties for the
decoding part are analogous to the encoding part:
</bodyText>
<equation confidence="0.9982282">
� Dj,d2
i,d1 = 1 Vj, d1, d2 (15)
i
Dj,d2
i,d1 = 0 ,#= d1 =� d2 V l(i,j) = 0 (16)
</equation>
<bodyText confidence="0.791226">
We can state the learning objective of the autoen-
coder as follows:
</bodyText>
<equation confidence="0.7738485">
�Ei,d1 = 1 Vi, d1, d2 (8) argmin 11D O E O W − W11 (17)
j,d2 E,D
j
Additionally, there is no interaction between dif-
ferent dimensions, so Ei,d1
j,d2 = 0 if d1 =� d2. In
</equation>
<bodyText confidence="0.999576125">
other words, we are creating the tensor by stacking
the diagonal matrices E(i,j) over i and j. Another
sparsity arises from the fact that many lexemes do
under the conditions Eq. 8, 9, 15 and 16.
Now we have an autoencoder where input and
output layers are the word embeddings and the
hidden layer represents the synset vectors. A sim-
plified version is shown in Figure 1. The tensors E
</bodyText>
<page confidence="0.941312">
1795
</page>
<bodyText confidence="0.99998775">
and D have to be learned. They are rank 4 tensors
of size ≈1015. However, we already discussed that
they are very sparse, for two reasons: (i) We make
the assumption that there is no interaction between
dimensions. (ii) There are only few interactions
between words and synsets (only when a lexeme
exists). In practice, there are only ≈107 elements
to learn, which is technically feasible.
</bodyText>
<subsectionHeader confidence="0.997552">
2.2 Matrix formalization
</subsectionHeader>
<bodyText confidence="0.999950888888889">
Based on the assumption that each dimension is
fully independent from other dimensions, a sepa-
rate autoencoder for each dimension can be cre-
ated and trained in parallel. Let W ∈ R|W |×n be
a matrix where each row is a word embedding and
w(d) = W·,d the d-th column of W, i.e., a vector
that holds the d-th dimension of each word vector.
In the same way, s(d) = S·,d holds the d-th di-
mension of each synset vector and E(d) = E·,d
</bodyText>
<equation confidence="0.984569428571428">
·,d ∈
R|S|×|W |. We can write S = E ⊗ W as:
s(d) = E(d)w(d) ∀d (18)
with E(d)
i,j = 0 if l(i,j) = 0. The decoding equation
W = D ⊗ S takes this form:
w(d) = D(d)s(d) ∀d (19)
</equation>
<bodyText confidence="0.698307">
where D(d) = D·,d
</bodyText>
<listItem confidence="0.857663">
·,d ∈ R|W|×|S |and D(d)
j,i = 0 if
l(i,j) = 0. So E and D are symmetric in terms
of non-zero elements. The learning objective be-
comes:
</listItem>
<equation confidence="0.8459525">
argmin kD(d)E(d)w(d) − w(d)k ∀d (20)
E(d),D(d)
</equation>
<subsectionHeader confidence="0.998567">
2.3 Lexeme embeddings
</subsectionHeader>
<bodyText confidence="0.953939666666667">
The hidden layer S of the autoencoder gives us
synset embeddings. The lexeme embeddings are
defined when transitioning from W to S, or more
explicitly by:
noun verb adj adv
hypernymy 84,505 13,256 0 0
antonymy 2,154 1,093 4,024 712
similarity 0 0 21,434 0
verb group 0 1,744 0 0
</bodyText>
<tableCaption confidence="0.995259">
Table 1: # of WN relations by part-of-speech
</tableCaption>
<bodyText confidence="0.867884">
This can also be expressed dimension-wise. The
matrix formulation is given by:
</bodyText>
<equation confidence="0.837616">
argmin E(d) diag(w(d)) − �D(d) diag(s(d))�T ∀d
E(d),D(d)
</equation>
<bodyText confidence="0.9655785">
(24)
with diag(x) being a square matrix having x
on the main diagonal and vector s(d) defined by
Eq. 18. While we try to align the embeddings,
there are still two different lexeme embeddings. In
all experiments reported in Section 4 we will use
the average of both embeddings and in Section 4
we will analyze the weighting in more detail.
</bodyText>
<subsectionHeader confidence="0.990663">
2.4 WN relations
</subsectionHeader>
<bodyText confidence="0.999546277777778">
Some WordNet synsets contain only a single word
(lexeme). The autoencoder learns based on the
synset constraints, i.e., lexemes being shared by
different synsets (and also words); thus, it is dif-
ficult to learn good embeddings for single-lexeme
synsets. To remedy this problem, we impose the
constraint that synsets related by WordNet (WN)
relations should have similar embeddings. Table 1
shows relations we used. WN relations are entered
in a new matrix R ∈ Rr×|S|, where r is the number
of WN relation tuples. For each relation tuple, i.e.,
row in R, we set the columns corresponding to the
first and second synset to 1 and −1, respectively.
The values of R are not updated during training.
We use a squared error function and 0 as target
value. This forces the system to find similar val-
ues for related synsets. Formally, the WN relation
constraints are:
</bodyText>
<equation confidence="0.9967785">
l(i,j) = E(i,j)w(i) (21) argmin kRE(d)w(d)k ∀d (25)
E(d)
</equation>
<bodyText confidence="0.9963115">
However, there is also a second lexeme embedding
in AutoExtend when transitioning form S to W:
</bodyText>
<equation confidence="0.998091">
l(i,j) = D(j,i)s(j) (22)
</equation>
<bodyText confidence="0.9999185">
Aligning these two representations seems natural,
so we impose the following lexeme constraints:
</bodyText>
<equation confidence="0.889981">
argmin E(i,j)w(i) − D(j,i)s(j) ∀i, j (23)
E(i,j),D(j,i)
</equation>
<subsectionHeader confidence="0.840077">
2.5 Implementation
</subsectionHeader>
<bodyText confidence="0.993448">
Our training objective is minimization of the sum
of synset constraints (Eq. 20), weighted by α, the
lexeme constraints (Eq. 24), weighted by β, and
the WN relation constraints (Eq. 25), weighted by
1 − α − β.
The training objective cannot be solved analyt-
ically because it is subject to constraints Eq. 8,
</bodyText>
<page confidence="0.956573">
1796
</page>
<figure confidence="0.847883">
L/suit (textil) S/suit-of-clothes L/suit (textil)
</figure>
<figureCaption confidence="0.996757666666667">
Figure 1: A small subgraph of WordNet. The circles are intended to show four different embedding dimensions. These
dimensions are treated as independent. The synset constraints align the input and the output layer. The lexeme constraints align
the second and fourth layers.
</figureCaption>
<table confidence="0.6360988">
W/lawsuit L/lawsuit L/lawsuit W/lawsuit
W/case L/case S/lawsuit L/case W/case
W/suit
L/suit (law) L/suit (law)
W/suit
</table>
<bodyText confidence="0.995454888888889">
Eq. 9, Eq. 15 and Eq. 16. We therefore use back-
propagation. We do not use regularization since
we found that all learned weights are in [−2, 2].
AutoExtend is implemented in MATLAB. We
run 1000 iterations of gradient descent. On an In-
tel Xeon CPU E7-8857 v2 3.00GHz, one iteration
on one dimension takes less than a minute because
the gradient computation ignores zero entries in
the matrix.
</bodyText>
<subsectionHeader confidence="0.989513">
2.6 Column normalization
</subsectionHeader>
<bodyText confidence="0.999991277777778">
Our model is based on the premise that a word is
the sum of its lexemes (Eq. 1). From the defini-
tion of E(z,j), we derived that E E R|S|×n×|W|×n
is normalized over the first dimension (Eq. 8). So
E(d) E R|S|×|W |is also normalized over the first
dimension. In other words, E(d) is a column nor-
malized matrix. Another premise of the model is
that a synset is the sum of its lexemes. Therefore,
D(d) is also column normalized. A simple way
to implement this is to start the computation with
column normalized matrices and normalize them
again after each iteration as long as the error func-
tion still decreases. When the error function starts
increasing, we stop normalizing the matrices and
continue with a normal gradient descent. This re-
spects that while E(d) and D(d) should be column
normalized in theory, there are a lot of practical
issues that prevent this, e.g., OOV words.
</bodyText>
<sectionHeader confidence="0.965734" genericHeader="method">
3 Data, experiments and evaluation
</sectionHeader>
<bodyText confidence="0.999990724137931">
We downloaded 300-dimensional embeddings for
3,000,000 words and phrases trained on Google
News, a corpus of ≈1011 tokens, using word2vec
CBOW (Mikolov et al., 2013c). Many words
in the word2vec vocabulary are not in WordNet,
e.g., inflected forms (cars) and proper nouns (Tony
Blair). Conversely, many WordNet lemmas are
not in the word2vec vocabulary, e.g., 42 (digits
were converted to 0). This results in a number of
empty synsets (see Table 2). Note however that
AutoExtend can produce embeddings for empty
synsets because we use WN relation constraints in
addition to synset and lexeme constraints.
We run AutoExtend on the word2vec vectors.
As we do not know anything about a suitable
weighting for the three different constraints, we
set α = Q = 0.33. Our main goal is to produce
compatible embeddings for lexemes and synsets.
Thus, we can compute nearest neighbors across all
three data types as shown in Figure 2.
We evaluate the embeddings on WSD and on
similarity performance. Our results depend di-
rectly on the quality of the underlying word em-
beddings, in our case word2vec embeddings. We
would expect even better evaluation results as
word representation learning methods improve.
Using a new and improved set of underlying em-
beddings is simple: it is a simple switch of the
input file that contains the word embeddings.
</bodyText>
<subsectionHeader confidence="0.998917">
3.1 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.997102666666667">
For WSD we use the shared tasks of Senseval-
2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et
al., 2004) and a system named IMS (Zhong and
</bodyText>
<table confidence="0.7229065">
WordNet n word2vec
words 147,478 54,570
synsets 117,791 73,844
lexemes 207,272 106,167
</table>
<tableCaption confidence="0.975538">
Table 2: # of items in WordNet and after intersection with
word2vec vectors
</tableCaption>
<page confidence="0.832295">
1797
</page>
<table confidence="0.971843066666667">
nearest neighbors of W/suit
S/suit (businessman), L/suit (businessman),
L/accomodate, S/suit (be acceptable), L/suit (be accept-
able), L/lawsuit, W/lawsuit, S/suit (playing card), L/suit
(playing card), S/suit (petition), S/lawsuit, W/countersuit,
W/complaint, W/counterclaim
nearest neighbors of W/lawsuit
L/lawsuit, S/lawsuit, S/countersuit, L/countersuit,
W/countersuit, W/suit, W/counterclaim, S/counterclaim
(n), L/counterclaim (n), S/counterclaim (v),
L/counterclaim (v), W/sue, S/sue (n), L/sue (n)
nearest neighbors of S/suit-of-clothes
L/suit-of-clothes, S/zoot-suit, L/zoot-suit, W/zoot-suit,
S/garment, L/garment, S/dress, S/trousers, L/pinstripe,
L/shirt, W/tuxedo, W/gabardine, W/tux, W/pinstripe
</table>
<figureCaption confidence="0.8755335">
Figure 2: Five nearest word (W/), lexeme (L/) and synset (S/)
neighbors for three items, ordered by cosine
</figureCaption>
<bodyText confidence="0.993856862068965">
Ng, 2010). Senseval-2 contains 139, Senseval-3
57 different words. They provide 8,611, respec-
tively 8,022 training instances and 4,328, respec-
tively 3,944 test instances. For the system, we
use the same setting as in the original paper. Pre-
processing consists of sentence splitting, tokeniza-
tion, POS tagging and lemmatization; the classi-
fier is a linear SVM. In our experiments (Table 3),
we run IMS with each feature set by itself to as-
sess the relative strengths of feature sets (lines 1–
7) and on feature set combinations to determine
which combination is best for WSD (lines 8, 12–
15).
IMS implements three standard WSD feature
sets: part of speech (POS), surrounding word and
local collocation (lines 1–3).
Let w be an ambiguous word with k senses. The
three feature sets on lines 5–7 are based on the
AutoExtend embeddings s(j), 1 &lt; j &lt; k, of the
synsets of w and the centroid c of the sentence in
which w occurs. The centroid is simply the sum of
all word2vec vectors of the words in the sentence,
excluding stop words.
The S-cosine feature set consists of the k
cosines of centroid and synset vectors:
&lt; cos(c, s(1)), cos(c, s(2)), ... , cos(c, s(k)) &gt;
The S-product feature set consists of the nk
element-wise products of centroid and synset vec-
tors:
</bodyText>
<equation confidence="0.94011">
&lt; c1s(1)
1 , ... , cns(1)
n , ... , c1s(k)
1 , ... , cns(k)
</equation>
<bodyText confidence="0.931247166666667">
n &gt;
where ci (resp. s(j)
i ) is element i of c (resp. s(j)).
The idea is that we let the SVM estimate how im-
portant each dimension is for WSD instead of giv-
ing all equal weight as in S-cosine.
</bodyText>
<equation confidence="0.922068833333333">
The S-raw feature set simply consists of the
n(k + 1) elements of centroid and synset vectors:
&lt; c1,...,cn,s(1)
1 ,...,s(1)
n ,...,s(k)
1 ,...,s(k) n&gt;
</equation>
<bodyText confidence="0.999969581395349">
Our main goal is to determine if AutoExtend
features improve WSD performance when added
to standard WSD features. To make sure that
improvements we get are not solely due to the
power of word2vec, we also investigate a sim-
ple word2vec baseline. For S-product, the Auto-
Extend feature set that performs best in the exper-
iment (cf. lines 6 and 14), we test the alternative
word2vec-based Snaive-product feature set. It has
the same definition as S-product except that we
replace the synset vectors s(j) with naive synset
vectors z(j), defined as the sum of the word2vec
vectors of the words that are members of synset j.
Lines 1–7 in Table 3 show the performance of
each feature set by itself. We see that the synset
feature sets (lines 5–7) have a comparable perfor-
mance to standard feature sets. S-product is the
strongest of them.
Lines 8–16 show the performance of different
feature set combinations. MFS (line 8) is the most
frequent sense baseline. Lines 9&amp;10 are the win-
ners of Senseval. The standard configuration of
IMS (line 11) uses the three feature sets on lines
1–3 (POS, surrounding word, local collocation)
and achieves an accuracy of 65.2% on the English
lexical sample task of Senseval-2 and 72.3% on
Senseval-3.1 Lines 12–16 add one additional fea-
ture set to the IMS system on line 11; e.g., the sys-
tem on line 14 uses POS, surrounding word, local
collocation and S-product feature sets. The system
on line 14 outperforms all previous systems, most
of them significantly. While S-raw performs quite
reasonably as a feature set alone, it hurts the per-
formance when used as an additional feature set.
As this is the feature set that contains the largest
number of features (n(k + 1)), overfitting is the
likely reason. Conversely, S-cosine only adds k
features and therefore may suffer from underfit-
ting.
We do a grid search (step size .1) for optimal
values of α and β, optimizing the average score of
Senseval-2 and Senseval-3. The best performing
feature set combination is Soptimized-product with
</bodyText>
<footnote confidence="0.4764916">
1Zhong and Ng (2010) report accuracies of 65.3% /
72.6% for this configuration.
†In Table 3 and Table 4, results significantly worse than
the best (bold) result in each column are marked † for α =
.05 and ‡ for α = .10 (one-tailed Z-test).
</footnote>
<page confidence="0.943096">
1798
</page>
<table confidence="0.999733352941177">
Senseval-2 Senseval-3
1 POS 53.6 58.0
2 surrounding word 57.6 65.3
3 local collocation 58.7 64.7
4 Snaive-product 56.5 62.2
5 S-cosine 55.5 60.5
6 S-product 58.3 64.3
7 S-raw 56.8 63.1
8 MFS 47.6† 55.2†
9 Rank 1 system 64.2† 72.9
10 Rank 2 system 63.8† 72.6
11 IMS 65.2‡ 72.3‡
12 IMS + Snaive-prod. 62.6† 69.4†
13 IMS + S-cosine 65.1‡ 72.4‡
14 IMS + S-product 66.5 73.6
15 IMS + S-raw 62.1† 66.8†
16 IMS + Soptimized-prod. 66.6 73.6
</table>
<tableCaption confidence="0.9929905">
Table 3: WSD accuracy for different feature sets and systems.
Best result (excluding line 16) in each column in bold.
</tableCaption>
<bodyText confidence="0.9997574">
α = 0.2 and Q = 0.5, with only a small improve-
ment (line 16).
The main result of this experiment is that we
achieve an improvement of more than 1% in WSD
performance when using AutoExtend.
</bodyText>
<subsectionHeader confidence="0.998857">
3.2 Synset and lexeme similarity
</subsectionHeader>
<bodyText confidence="0.993016888888889">
We use SCWS (Huang et al., 2012) for the similar-
ity evaluation. SCWS provides not only isolated
words and corresponding similarity scores, but
also a context for each word. SCWS is based on
WordNet, but the information as to which synset a
word in context came from is not available. How-
ever, the dataset is the closest we could find for
sense similarity. Synset and lexeme embeddings
are obtained by running AutoExtend. Based on
the results of the WSD task, we set α = 0.2 and
Q = 0.5. Lexeme embeddings are the natural
choice for this task as human subjects are provided
with two words and a context for each and then
have to assign a similarity score. But for complete-
ness, we also run experiments for synsets.
For each word, we compute a context vector
c by adding all word vectors of the context, ex-
cluding the test word itself. Following Reisinger
and Mooney (2010), we compute the lexeme (resp.
synset) vector l either as the simple average of
the lexeme (resp. synset) vectors l(ij) (method
AvgSim, no dependence on c in this case) or
as the average of the lexeme (resp. synset) vec-
tors weighted by cosine similarity to c (method
AvgSimC).
Table 4 shows that AutoExtend lexeme embed-
dings (line 7) perform better than previous work,
</bodyText>
<note confidence="0.8005475">
AvgSim AvgSimC
1 Huang et al. (2012) 62.8† 65.7†
2 Tian et al. (2014) – 65.4†
3 Neelakantan et al. (2014) 67.2 69.3
4 Chen et al. (2014) 66.2† 68.9
5 words (word2vec) 66.6‡ 66.6†
6 synsets 62.6† 63.7†
7 lexemes 68.9 69.8
</note>
<tableCaption confidence="0.9978645">
Table 4: Spearman correlation (ρ × 100) on SCWS. Best re-
sult per column in bold.
</tableCaption>
<bodyText confidence="0.999805952380952">
including (Huang et al., 2012) and (Tian et al.,
2014). Lexeme embeddings perform better than
synset embeddings (lines 7 vs. 6), presumably be-
cause using a representation that is specific to the
actual word being judged is more precise than us-
ing a representation that also includes synonyms.
A simple baseline is to use the underlying
word2vec embeddings directly (line 5). In this
case, there is only one embedding, so there is no
difference between AvgSim and AvgSimC. It is in-
teresting that even if we do not take the context
into account (method AvgSim) the lexeme embed-
dings outperform the original word embeddings.
As AvgSim simply adds up all lexemes of a word,
this is equivalent to the constraint we proposed in
the beginning of the paper (Eq. 1). Thus, replacing
a word’s embedding by the sum of the embeddings
of its senses could generally improve the quality of
embeddings (cf. Huang et al. (2012) for a similar
point). We will leave a deeper evaluation of this
topic for future work.
</bodyText>
<sectionHeader confidence="0.986274" genericHeader="method">
4 Analysis
</sectionHeader>
<bodyText confidence="0.997249277777778">
We first look at the impact of the parameters α, Q
(Section 2.5) that control the weighting of synset
constraints vs lexeme constraints vs WN relation
constraints. We investigate the impact for three
different tasks. WSD-alone: accuracy of IMS
(average of Senseval-2 and Senseval-3) if only S-
product is used as a feature set (line 6 in Table 3).
WSD-additional: accuracy of IMS (average of
Senseval-2 and Senseval-3) if S-product is used
together with the feature sets POS, surrounding
word and local collocation (line 14 in Table 3).
SCWS: Spearman correlation on SCWS (line 7 in
Table 4).
For WSD-alone (Figure 3, center), the best per-
forming weightings (red) all have high weights
for WN relations and are therefore at the top of
triangle. Thus, WN relations are very important
for WSD-alone and adding more weight to the
</bodyText>
<figure confidence="0.8189865">
IMS feature sets
system comparison
</figure>
<page confidence="0.994694">
1799
</page>
<bodyText confidence="0.999963194444445">
synset and lexeme constraints does not help. How-
ever, all three constraints are important in WSD-
additional: the red area is in the middle (corre-
sponding to nonzero weights for all three con-
straints) in the left panel of Figure 3. Apparently,
strongly weighted lexeme and synset constraints
enable learning of representations that in their in-
teraction with standard WSD feature sets like lo-
cal collocation increase WSD performance. For
SCWS (right panel), we should not put too much
weight on WN relations as they artificially bring
related, but not similar lexemes together. So the
maximum for this task is located in the lower part
of the triangle.
The main result of this analysis is that Auto-
Extend never achieves its maximum performance
when using only one set of constraints. All three
constraints are important – synset, lexeme and WN
relation constraints – with different weights for
different applications.
We also analyzed the impact of the four differ-
ent WN relations (see Table 1) on performance. In
Table 3 and Table 4, all four WN relations are used
together. We found that any combination of three
relation types performs worse than using all four
together. A comparison of different relations must
be done carefully as they differ in the POS they
affect and in quantity (see Table 1). In general, re-
lation types with more relations outperformed re-
lation types with fewer relations.
Finally, the relative weighting of l(z,j) and l(z,j)
when computing lexeme embeddings is also a pa-
rameter that can be tuned. We use simple aver-
aging (0 = 0.5) for all experiments reported in
this paper. We found only small changes in per-
formance for 0.2 &lt; 0 &lt; 0.8.
</bodyText>
<sectionHeader confidence="0.973992" genericHeader="method">
5 Resources other than WordNet
</sectionHeader>
<bodyText confidence="0.99997736">
AutoExtend is broadly applicable to lexical and
knowledge resources that have certain properties.
While we only run experiments with WordNet in
this paper, we will briefly address other resources.
For Freebase (Bollacker et al., 2008), we could re-
place the synsets with Freebase entities. Each en-
tity has several aliases, e.g. Barack Obama, Presi-
dent Obama, Obama. The role of words in Word-
Net would correspond to these aliases in Freebase.
This will give us the synset constraint, as well as
the lexeme constraint of the system. Relations are
given by Freebase types; e.g., we can add a con-
straint that entity embeddings of the type ”Presi-
dent of the US” should be similar.
To explorer multilingual word embeddings we
require the word embeddings of different lan-
guages to live in the same vector space, which
can easily be achieved by training a transforma-
tion matrix L between two languages using known
translations (Mikolov et al., 2013b). Let X be a
matrix where each row is a word embedding in
language 1 and Y a matrix where each row is a
word embedding in language 2. For each row the
words of X and Y are a translation of each other.
We then want to minimize the following objective:
</bodyText>
<equation confidence="0.6845695">
argmin I ILX − Y I I (26)
L
</equation>
<bodyText confidence="0.996291666666667">
We can use a gradient descent to solve this but a
matrix inversion will run faster. The matrix L is
given by:
</bodyText>
<equation confidence="0.998197">
L = (XT * X)−1(XT * Y ) (27)
</equation>
<bodyText confidence="0.999826125">
The matrix L can be used to transform unknown
embeddings into the new vector space, which en-
ables us to use a multilingual WordNet like Ba-
belNet (Navigli and Ponzetto, 2010) to compute
synset embeddings. We can add cross-linguistic
relationships to our model, e.g., aligning German
and English synset embeddings of the same con-
cept.
</bodyText>
<sectionHeader confidence="0.999987" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999850380952381">
Rumelhart et al. (1988) introduced distributed
word representations, usually called word embed-
dings today. There has been a resurgence of
work on them recently (e.g., Bengio et al. (2003)
Mnih and Hinton (2007), Collobert et al. (2011),
Mikolov et al. (2013a), Pennington et al. (2014)).
These models produce only a single embedding
for each word. All of them can be used as input
for AutoExtend.
There are several approaches to finding embed-
dings for senses, variously called meaning, sense
and multiple word embeddings. Sch¨utze (1998)
created sense representations by clustering context
representations derived from co-occurrence. The
representation of a sense is simply the centroid of
its cluster. Huang et al. (2012) improved this by
learning single-prototype embeddings before per-
forming word sense discrimination on them. Bor-
des et al. (2011) created similarity measures for
relations in WordNet and Freebase to learn en-
tity embeddings. An energy based model was
</bodyText>
<page confidence="0.927175">
1800
</page>
<figure confidence="0.9572455">
WSD-additional WSD-alone SCWS
WN relations
</figure>
<figureCaption confidence="0.9972805">
Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the
three tasks WSD-additional, WSD-alone and SCWS. “x” indicates the maximum; “o” indicates a local minimum.
</figureCaption>
<bodyText confidence="0.999829843137255">
proposed by Bordes et al. (2012) to create dis-
ambiguated meaning embeddings and Neelakan-
tan et al. (2014) and Tian et al. (2014) extended
the Skip-gram model (Mikolov et al., 2013a) to
learn multiple word embeddings. While these em-
beddings can correspond to different word senses,
there is no clear mapping between them and a lexi-
cal resource like WordNet. Chen et al. (2014) also
modified word2vec to learn sense embeddings,
each corresponding to a WordNet synset. They
use glosses to initialize sense embedding, which
in turn can be used for WSD. The sense disam-
biguated data can again be used to improve sense
embeddings.
This prior work needs a training step to learn
embeddings. In contrast, we can “AutoExtend”
any set of given word embeddings – without
(re)training them.
There is only little work on taking existing
word embeddings and producing embeddings in
the same space. Labutov and Lipson (2013) tuned
existing word embeddings in supervised training,
not to create new embeddings for senses or enti-
ties, but to get better predictive performance on a
task while not changing the space of embeddings.
Lexical resources have also been used to im-
prove word embeddings. In the Relation Con-
strained Model, Yu and Dredze (2014) use
word2vec to learn embeddings that are optimized
to predict a related word in the resource, with good
evaluation results. Bian et al. (2014) used not
only semantic, but also morphological and syn-
tactic knowledge to compute more effective word
embeddings.
Another interesting approach to create sense
specific word embeddings uses bilingual resources
(Guo et al., 2014). The downside of this approach
is that parallel data is needed.
We used the SCWS dataset for the word similar-
ity task, as it provides a context. Other frequently
used datasets are WordSim-353 (Finkelstein et al.,
2001) or MEN (Bruni et al., 2014).
And while we use cosine to compute similar-
ity between synsets, there are also a lot of simi-
larity measures that only rely on a given resource,
mostly WordNet. These measures are often func-
tions that depend on the provided information like
gloss or the topology like shortest-path. Examples
include (Wu and Palmer, 1994) and (Leacock and
Chodorow, 1998); Blanchard et al. (2005) give a
good overview.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999990142857143">
We presented AutoExtend, a flexible method to
learn synset and lexeme embeddings from word
embeddings. It is completely general and can be
used for any other set of embeddings and for any
other resource that imposes constraints of a cer-
tain type on the relationship between words and
other data types. Our experimental results show
that AutoExtend achieves state-of-the-art perfor-
mance on word similarity and word sense disam-
biguation. Along with this paper, we will pub-
lish AutoExtend for extending word embeddings
to other data types; the lexeme and synset em-
beddings used in the experiments; and the code
needed to replicate our WSD evaluation2.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997462">
This work was partially funded by Deutsche
Forschungsgemeinschaft (DFG SCHU 2246/2-2).
We are grateful to Christiane Fellbaum for discus-
sions leading up to this paper and to the anony-
mous reviewers for their comments.
</bodyText>
<footnote confidence="0.974217">
2http://cistern.cis.lmu.de/
</footnote>
<page confidence="0.993103">
1801
</page>
<sectionHeader confidence="0.996209" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999897421568627">
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137–1155.
Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014.
Knowledge-powered deep learning for word embed-
ding. In Proceedings of ECML PKDD.
Emmanuel Blanchard, Mounira Harzallah, Henri
Briand, and Pascale Kuntz. 2005. A typology of
ontology-based semantic measures. In Proceedings
of EMOI - INTEROP.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of ACM SIGMOD.
Antoine Bordes, Jason Weston, Ronan Collobert,
Yoshua Bengio, et al. 2011. Learning structured
embeddings of knowledge bases. In Proceedings of
AAAI.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text semantic
parsing. In Proceedings of AISTATS.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49(1):1–47.
Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In Proceedings of EMNLP.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of WWW.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-specific word embed-
dings by exploiting bilingual resources. In Proceed-
ings of Coling, Technical Papers.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of ACL.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.
Adam Kilgarriff. 2001. English lexical sample task
description. In Proceedings of SENSEVAL-2.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Proceedings of ACL.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The senseval-3 english lexical sample
task. In Proceedings of SENSEVAL-3.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013b. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013c. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS.
George A Miller and Walter G Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
Cognitive Processes, 6(1):1–28.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of ICML.
Andriy Mnih and Geoffrey E Hinton. 2009. A scalable
hierarchical distributed language model. In Pro-
ceedings of NIPS.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
Babelnet: Building a very large multilingual seman-
tic network. In Proceedings of ACL.
Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of EMNLP.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of EMNLP.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceedings of NAACL.
Herbert Rubenstein and John B Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.
</reference>
<page confidence="0.913144">
1802
</page>
<reference confidence="0.994761315789474">
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1988. Learning representations by back-
propagating errors. Cognitive Modeling, 5:213–220.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123.
Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In Proceedings of Coling, Technical Papers.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of ACL.
Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proceedings
of ACL.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation system
for free text. In Proceedings ofACL, System Demon-
strations.
</reference>
<page confidence="0.978026">
1803
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.916501">
<title confidence="0.9984235">AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes</title>
<author confidence="0.996471">Sascha Rothe</author>
<author confidence="0.996471">Hinrich</author>
<affiliation confidence="0.9990645">Center for Information &amp; Language University of</affiliation>
<email confidence="0.962585">sascha@cis.lmu.de</email>
<abstract confidence="0.997267625">present a system to learn embeddings for synsets and lexemes. It is flexible in that it can take any word embeddings as input and does not need an additional training corpus. The synset/lexeme embeddings obtained live in the same vector space as the word embeddings. A sparse tensor formalization guarantees efficiency and parallelizability. We use WordNet as a lexical resource, but Auto- Extend can be easily applied to other resources like Freebase. achieves state-of-the-art performance on word similarity and word sense disambiguation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rejean Ducharme</author>
<author>Pascal Vincent</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="31255" citStr="Bengio et al. (2003)" startWordPosition="5408" endWordPosition="5411">ix inversion will run faster. The matrix L is given by: L = (XT * X)−1(XT * Y ) (27) The matrix L can be used to transform unknown embeddings into the new vector space, which enables us to use a multilingual WordNet like BabelNet (Navigli and Ponzetto, 2010) to compute synset embeddings. We can add cross-linguistic relationships to our model, e.g., aligning German and English synset embeddings of the same concept. 6 Related Work Rumelhart et al. (1988) introduced distributed word representations, usually called word embeddings today. There has been a resurgence of work on them recently (e.g., Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch¨utze (1998) created sense representations by clustering context representations derived from co-occurrence. The representation of a sense is simply the centroid of its cluster. Huang et al. (2012) improved this by learning single-prototype embeddings before per</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2003</marker>
<rawString>Yoshua Bengio, Rejean Ducharme, and Pascal Vincent. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Knowledge-powered deep learning for word embedding.</title>
<date>2014</date>
<booktitle>In Proceedings of ECML PKDD.</booktitle>
<contexts>
<context position="33697" citStr="Bian et al. (2014)" startWordPosition="5794" endWordPosition="5797">(re)training them. There is only little work on taking existing word embeddings and producing embeddings in the same space. Labutov and Lipson (2013) tuned existing word embeddings in supervised training, not to create new embeddings for senses or entities, but to get better predictive performance on a task while not changing the space of embeddings. Lexical resources have also been used to improve word embeddings. In the Relation Constrained Model, Yu and Dredze (2014) use word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian et al. (2014) used not only semantic, but also morphological and syntactic knowledge to compute more effective word embeddings. Another interesting approach to create sense specific word embeddings uses bilingual resources (Guo et al., 2014). The downside of this approach is that parallel data is needed. We used the SCWS dataset for the word similarity task, as it provides a context. Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN (Bruni et al., 2014). And while we use cosine to compute similarity between synsets, there are also a lot of similarity measures that only rely o</context>
</contexts>
<marker>Bian, Gao, Liu, 2014</marker>
<rawString>Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014. Knowledge-powered deep learning for word embedding. In Proceedings of ECML PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Blanchard</author>
<author>Mounira Harzallah</author>
<author>Henri Briand</author>
<author>Pascale Kuntz</author>
</authors>
<title>A typology of ontology-based semantic measures.</title>
<date>2005</date>
<booktitle>In Proceedings of EMOI - INTEROP.</booktitle>
<contexts>
<context position="34551" citStr="Blanchard et al. (2005)" startWordPosition="5933" endWordPosition="5936">he downside of this approach is that parallel data is needed. We used the SCWS dataset for the word similarity task, as it provides a context. Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN (Bruni et al., 2014). And while we use cosine to compute similarity between synsets, there are also a lot of similarity measures that only rely on a given resource, mostly WordNet. These measures are often functions that depend on the provided information like gloss or the topology like shortest-path. Examples include (Wu and Palmer, 1994) and (Leacock and Chodorow, 1998); Blanchard et al. (2005) give a good overview. 7 Conclusion We presented AutoExtend, a flexible method to learn synset and lexeme embeddings from word embeddings. It is completely general and can be used for any other set of embeddings and for any other resource that imposes constraints of a certain type on the relationship between words and other data types. Our experimental results show that AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation. Along with this paper, we will publish AutoExtend for extending word embeddings to other data types; the lexeme and synset embed</context>
</contexts>
<marker>Blanchard, Harzallah, Briand, Kuntz, 2005</marker>
<rawString>Emmanuel Blanchard, Mounira Harzallah, Henri Briand, and Pascale Kuntz. 2005. A typology of ontology-based semantic measures. In Proceedings of EMOI - INTEROP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of ACM SIGMOD.</booktitle>
<contexts>
<context position="29595" citStr="Bollacker et al., 2008" startWordPosition="5107" endWordPosition="5110">eneral, relation types with more relations outperformed relation types with fewer relations. Finally, the relative weighting of l(z,j) and l(z,j) when computing lexeme embeddings is also a parameter that can be tuned. We use simple averaging (0 = 0.5) for all experiments reported in this paper. We found only small changes in performance for 0.2 &lt; 0 &lt; 0.8. 5 Resources other than WordNet AutoExtend is broadly applicable to lexical and knowledge resources that have certain properties. While we only run experiments with WordNet in this paper, we will briefly address other resources. For Freebase (Bollacker et al., 2008), we could replace the synsets with Freebase entities. Each entity has several aliases, e.g. Barack Obama, President Obama, Obama. The role of words in WordNet would correspond to these aliases in Freebase. This will give us the synset constraint, as well as the lexeme constraint of the system. Relations are given by Freebase types; e.g., we can add a constraint that entity embeddings of the type ”President of the US” should be similar. To explorer multilingual word embeddings we require the word embeddings of different languages to live in the same vector space, which can easily be achieved b</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of ACM SIGMOD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Jason Weston</author>
<author>Ronan Collobert</author>
<author>Yoshua Bengio</author>
</authors>
<title>Learning structured embeddings of knowledge bases.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="31918" citStr="Bordes et al. (2011)" startWordPosition="5508" endWordPosition="5512">2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch¨utze (1998) created sense representations by clustering context representations derived from co-occurrence. The representation of a sense is simply the centroid of its cluster. Huang et al. (2012) improved this by learning single-prototype embeddings before performing word sense discrimination on them. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings. An energy based model was 1800 WSD-additional WSD-alone SCWS WN relations Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the three tasks WSD-additional, WSD-alone and SCWS. “x” indicates the maximum; “o” indicates a local minimum. proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple</context>
</contexts>
<marker>Bordes, Weston, Collobert, Bengio, 2011</marker>
<rawString>Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, et al. 2011. Learning structured embeddings of knowledge bases. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Xavier Glorot</author>
<author>Jason Weston</author>
<author>Yoshua Bengio</author>
</authors>
<title>Joint learning of words and meaning representations for open-text semantic parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of AISTATS.</booktitle>
<contexts>
<context position="32351" citStr="Bordes et al. (2012)" startWordPosition="5571" endWordPosition="5574"> is simply the centroid of its cluster. Huang et al. (2012) improved this by learning single-prototype embeddings before performing word sense discrimination on them. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings. An energy based model was 1800 WSD-additional WSD-alone SCWS WN relations Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the three tasks WSD-additional, WSD-alone and SCWS. “x” indicates the maximum; “o” indicates a local minimum. proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple word embeddings. While these embeddings can correspond to different word senses, there is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They use glosses to initialize sense embedding, which in turn can be used for WSD. The sense disambiguated data can again be used to improve sense embeddings. This pr</context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2012</marker>
<rawString>Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2012. Joint learning of words and meaning representations for open-text semantic parsing. In Proceedings of AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>49</volume>
<issue>1</issue>
<contexts>
<context position="34172" citStr="Bruni et al., 2014" startWordPosition="5870" endWordPosition="5873"> use word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian et al. (2014) used not only semantic, but also morphological and syntactic knowledge to compute more effective word embeddings. Another interesting approach to create sense specific word embeddings uses bilingual resources (Guo et al., 2014). The downside of this approach is that parallel data is needed. We used the SCWS dataset for the word similarity task, as it provides a context. Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN (Bruni et al., 2014). And while we use cosine to compute similarity between synsets, there are also a lot of similarity measures that only rely on a given resource, mostly WordNet. These measures are often functions that depend on the provided information like gloss or the topology like shortest-path. Examples include (Wu and Palmer, 1994) and (Leacock and Chodorow, 1998); Blanchard et al. (2005) give a good overview. 7 Conclusion We presented AutoExtend, a flexible method to learn synset and lexeme embeddings from word embeddings. It is completely general and can be used for any other set of embeddings and for a</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinxiong Chen</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
</authors>
<title>A unified model for word sense representation and disambiguation.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="25628" citStr="Chen et al. (2014)" startWordPosition="4435" endWordPosition="4438"> by adding all word vectors of the context, excluding the test word itself. Following Reisinger and Mooney (2010), we compute the lexeme (resp. synset) vector l either as the simple average of the lexeme (resp. synset) vectors l(ij) (method AvgSim, no dependence on c in this case) or as the average of the lexeme (resp. synset) vectors weighted by cosine similarity to c (method AvgSimC). Table 4 shows that AutoExtend lexeme embeddings (line 7) perform better than previous work, AvgSim AvgSimC 1 Huang et al. (2012) 62.8† 65.7† 2 Tian et al. (2014) – 65.4† 3 Neelakantan et al. (2014) 67.2 69.3 4 Chen et al. (2014) 66.2† 68.9 5 words (word2vec) 66.6‡ 66.6† 6 synsets 62.6† 63.7† 7 lexemes 68.9 69.8 Table 4: Spearman correlation (ρ × 100) on SCWS. Best result per column in bold. including (Huang et al., 2012) and (Tian et al., 2014). Lexeme embeddings perform better than synset embeddings (lines 7 vs. 6), presumably because using a representation that is specific to the actual word being judged is more precise than using a representation that also includes synonyms. A simple baseline is to use the underlying word2vec embeddings directly (line 5). In this case, there is only one embedding, so there is no d</context>
<context position="32694" citStr="Chen et al. (2014)" startWordPosition="5629" endWordPosition="5632"> relations Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the three tasks WSD-additional, WSD-alone and SCWS. “x” indicates the maximum; “o” indicates a local minimum. proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple word embeddings. While these embeddings can correspond to different word senses, there is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They use glosses to initialize sense embedding, which in turn can be used for WSD. The sense disambiguated data can again be used to improve sense embeddings. This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of given word embeddings – without (re)training them. There is only little work on taking existing word embeddings and producing embeddings in the same space. Labutov and Lipson (2013) tuned existing word embeddings in supervised training, not to cre</context>
</contexts>
<marker>Chen, Liu, Sun, 2014</marker>
<rawString>Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014. A unified model for word sense representation and disambiguation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="1187" citStr="Collobert and Weston, 2008" startWordPosition="172" endWordPosition="175">We use WordNet as a lexical resource, but AutoExtend can be easily applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks. 1 Introduction Unsupervised methods for word embeddings (also called “distributed word representations”) have become popular in natural language processing (NLP). These methods only need very large corpora as input to create sparse representations (e.g., based on local collocations) and project them into a lower dimensional dense vector space. Examples for word embeddings are SENNA (Collobert and Weston, 2008), the hierarchical log-bilinear model (Mnih and Hinton, 2009), word2vec (Mikolov et al., 2013c) and GloVe (Pennington et al., 2014). However, there are many other resources that are undoubtedly useful in NLP, including lexical resources like WordNet and Wiktionary and knowledge bases like Wikipedia and Freebase. We will simply call these resources in the rest of the paper. Our goal is to enrich these valuable resources with embeddings for those data types that are not words; e.g., we want to enrich WordNet with embeddings for synsets and lexemes. A synset is a set of synonyms that are intercha</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="31303" citStr="Collobert et al. (2011)" startWordPosition="5416" endWordPosition="5419"> given by: L = (XT * X)−1(XT * Y ) (27) The matrix L can be used to transform unknown embeddings into the new vector space, which enables us to use a multilingual WordNet like BabelNet (Navigli and Ponzetto, 2010) to compute synset embeddings. We can add cross-linguistic relationships to our model, e.g., aligning German and English synset embeddings of the same concept. 6 Related Work Rumelhart et al. (1988) introduced distributed word representations, usually called word embeddings today. There has been a resurgence of work on them recently (e.g., Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch¨utze (1998) created sense representations by clustering context representations derived from co-occurrence. The representation of a sense is simply the centroid of its cluster. Huang et al. (2012) improved this by learning single-prototype embeddings before performing word sense discrimination on them. Borde</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="4162" citStr="Fellbaum, 1998" startWordPosition="677" endWordPosition="678"> all word embedding vectors related to a particular node in a resource; e.g., to create the synset vector of lawsuit in WordNet, we can add the word vectors of the three words that are part of the synset (lawsuit, suit, case). We will call this approach 1793 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1793–1803, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics naive and use it as a baseline (Snaive in Table 3). We will focus on WordNet (Fellbaum, 1998) in this paper, but our method – based on a formalization that exploits the constraints of a resource for extending embeddings from words to other data types – is broadly applicable to other resources including Wikipedia and Freebase. A word in WordNet can be viewed as a composition of several lexemes. Lexemes from different words together can form a synset. When a synset is given, it can be decomposed into its lexemes. And these lexemes then join to form words. These observations are the basis for the formalization of the constraints encoded in WordNet that will be presented in the next secti</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="34144" citStr="Finkelstein et al., 2001" startWordPosition="5864" endWordPosition="5867">rained Model, Yu and Dredze (2014) use word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian et al. (2014) used not only semantic, but also morphological and syntactic knowledge to compute more effective word embeddings. Another interesting approach to create sense specific word embeddings uses bilingual resources (Guo et al., 2014). The downside of this approach is that parallel data is needed. We used the SCWS dataset for the word similarity task, as it provides a context. Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN (Bruni et al., 2014). And while we use cosine to compute similarity between synsets, there are also a lot of similarity measures that only rely on a given resource, mostly WordNet. These measures are often functions that depend on the provided information like gloss or the topology like shortest-path. Examples include (Wu and Palmer, 1994) and (Leacock and Chodorow, 1998); Blanchard et al. (2005) give a good overview. 7 Conclusion We presented AutoExtend, a flexible method to learn synset and lexeme embeddings from word embeddings. It is completely general and can be used for any other</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Learning sense-specific word embeddings by exploiting bilingual resources.</title>
<date>2014</date>
<booktitle>In Proceedings of Coling, Technical Papers.</booktitle>
<contexts>
<context position="33925" citStr="Guo et al., 2014" startWordPosition="5827" endWordPosition="5830">ings for senses or entities, but to get better predictive performance on a task while not changing the space of embeddings. Lexical resources have also been used to improve word embeddings. In the Relation Constrained Model, Yu and Dredze (2014) use word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian et al. (2014) used not only semantic, but also morphological and syntactic knowledge to compute more effective word embeddings. Another interesting approach to create sense specific word embeddings uses bilingual resources (Guo et al., 2014). The downside of this approach is that parallel data is needed. We used the SCWS dataset for the word similarity task, as it provides a context. Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN (Bruni et al., 2014). And while we use cosine to compute similarity between synsets, there are also a lot of similarity measures that only rely on a given resource, mostly WordNet. These measures are often functions that depend on the provided information like gloss or the topology like shortest-path. Examples include (Wu and Palmer, 1994) and (Leacock and Chodorow, 1998</context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning sense-specific word embeddings by exploiting bilingual resources. In Proceedings of Coling, Technical Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="24284" citStr="Huang et al., 2012" startWordPosition="4194" endWordPosition="4197"> MFS 47.6† 55.2† 9 Rank 1 system 64.2† 72.9 10 Rank 2 system 63.8† 72.6 11 IMS 65.2‡ 72.3‡ 12 IMS + Snaive-prod. 62.6† 69.4† 13 IMS + S-cosine 65.1‡ 72.4‡ 14 IMS + S-product 66.5 73.6 15 IMS + S-raw 62.1† 66.8† 16 IMS + Soptimized-prod. 66.6 73.6 Table 3: WSD accuracy for different feature sets and systems. Best result (excluding line 16) in each column in bold. α = 0.2 and Q = 0.5, with only a small improvement (line 16). The main result of this experiment is that we achieve an improvement of more than 1% in WSD performance when using AutoExtend. 3.2 Synset and lexeme similarity We use SCWS (Huang et al., 2012) for the similarity evaluation. SCWS provides not only isolated words and corresponding similarity scores, but also a context for each word. SCWS is based on WordNet, but the information as to which synset a word in context came from is not available. However, the dataset is the closest we could find for sense similarity. Synset and lexeme embeddings are obtained by running AutoExtend. Based on the results of the WSD task, we set α = 0.2 and Q = 0.5. Lexeme embeddings are the natural choice for this task as human subjects are provided with two words and a context for each and then have to assi</context>
<context position="25528" citStr="Huang et al. (2012)" startWordPosition="4414" endWordPosition="4417">t for completeness, we also run experiments for synsets. For each word, we compute a context vector c by adding all word vectors of the context, excluding the test word itself. Following Reisinger and Mooney (2010), we compute the lexeme (resp. synset) vector l either as the simple average of the lexeme (resp. synset) vectors l(ij) (method AvgSim, no dependence on c in this case) or as the average of the lexeme (resp. synset) vectors weighted by cosine similarity to c (method AvgSimC). Table 4 shows that AutoExtend lexeme embeddings (line 7) perform better than previous work, AvgSim AvgSimC 1 Huang et al. (2012) 62.8† 65.7† 2 Tian et al. (2014) – 65.4† 3 Neelakantan et al. (2014) 67.2 69.3 4 Chen et al. (2014) 66.2† 68.9 5 words (word2vec) 66.6‡ 66.6† 6 synsets 62.6† 63.7† 7 lexemes 68.9 69.8 Table 4: Spearman correlation (ρ × 100) on SCWS. Best result per column in bold. including (Huang et al., 2012) and (Tian et al., 2014). Lexeme embeddings perform better than synset embeddings (lines 7 vs. 6), presumably because using a representation that is specific to the actual word being judged is more precise than using a representation that also includes synonyms. A simple baseline is to use the underlyin</context>
<context position="31790" citStr="Huang et al. (2012)" startWordPosition="5490" endWordPosition="5493">ay. There has been a resurgence of work on them recently (e.g., Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch¨utze (1998) created sense representations by clustering context representations derived from co-occurrence. The representation of a sense is simply the centroid of its cluster. Huang et al. (2012) improved this by learning single-prototype embeddings before performing word sense discrimination on them. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings. An energy based model was 1800 WSD-additional WSD-alone SCWS WN relations Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the three tasks WSD-additional, WSD-alone and SCWS. “x” indicates the maximum; “o” indicates a local minimum. proposed by Bordes et al. (2012) to create disambiguated meaning embedd</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="9231" citStr="Kalchbrenner et al. (2014)" startWordPosition="1561" endWordPosition="1565">e., l(i,j) =� 0 j,d2 only if word i has a lexeme that is a member of synset j. To summarize the sparsity: These two equations are underspecified. We therefore introduce the matrix E(i,j) E Rnxn: l(i,j) = E(i,j)w(i) (3) We make the assumption that the dimensions in Eq. 3 are independent of each other, i.e., E(i,j) is a diagonal matrix. Our motivation for this assumption is: (i) This makes the computation technically feasible by significantly reducing the number of parameters and by supporting parallelism. (ii) Treating word embeddings on a per-dimension basis is a frequent design choice (e.g., Kalchbrenner et al. (2014)). Note that we allow E(i,j) &lt; 0 and in general the distribution weights for each dimension (diagonal entries of E(i,j)) will be different. Our assumption can be interpreted as word w(i) distributing its embedding activations to its lexemes on each dimension separately. Therefore, Eqs. 1-2 can be written as follows: �w(i) = E(i,j)w(i) (4) j s(j) = � E(i,j)w(i) (5) i Note that from Eq. 4 it directly follows that: ` E(i,j) = In Vi (6) j with In being the identity matrix. Let W be a |W |x n matrix where n is the dimensionality of the embedding space, |W |is the number of words and each row w(i) i</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>English lexical sample task description.</title>
<date>2001</date>
<booktitle>In Proceedings of SENSEVAL-2.</booktitle>
<contexts>
<context position="18494" citStr="Kilgarriff, 2001" startWordPosition="3223" endWordPosition="3224">r lexemes and synsets. Thus, we can compute nearest neighbors across all three data types as shown in Figure 2. We evaluate the embeddings on WSD and on similarity performance. Our results depend directly on the quality of the underlying word embeddings, in our case word2vec embeddings. We would expect even better evaluation results as word representation learning methods improve. Using a new and improved set of underlying embeddings is simple: it is a simple switch of the input file that contains the word embeddings. 3.1 Word Sense Disambiguation For WSD we use the shared tasks of Senseval2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et al., 2004) and a system named IMS (Zhong and WordNet n word2vec words 147,478 54,570 synsets 117,791 73,844 lexemes 207,272 106,167 Table 2: # of items in WordNet and after intersection with word2vec vectors 1797 nearest neighbors of W/suit S/suit (businessman), L/suit (businessman), L/accomodate, S/suit (be acceptable), L/suit (be acceptable), L/lawsuit, W/lawsuit, S/suit (playing card), L/suit (playing card), S/suit (petition), S/lawsuit, W/countersuit, W/complaint, W/counterclaim nearest neighbors of W/lawsuit L/lawsuit, S/lawsuit, S/countersuit, L/countersuit, </context>
</contexts>
<marker>Kilgarriff, 2001</marker>
<rawString>Adam Kilgarriff. 2001. English lexical sample task description. In Proceedings of SENSEVAL-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Labutov</author>
<author>Hod Lipson</author>
</authors>
<title>Re-embedding words.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="33228" citStr="Labutov and Lipson (2013)" startWordPosition="5716" endWordPosition="5719">re is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They use glosses to initialize sense embedding, which in turn can be used for WSD. The sense disambiguated data can again be used to improve sense embeddings. This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of given word embeddings – without (re)training them. There is only little work on taking existing word embeddings and producing embeddings in the same space. Labutov and Lipson (2013) tuned existing word embeddings in supervised training, not to create new embeddings for senses or entities, but to get better predictive performance on a task while not changing the space of embeddings. Lexical resources have also been used to improve word embeddings. In the Relation Constrained Model, Yu and Dredze (2014) use word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian et al. (2014) used not only semantic, but also morphological and syntactic knowledge to compute more effective word embeddings. Another interest</context>
</contexts>
<marker>Labutov, Lipson, 2013</marker>
<rawString>Igor Labutov and Hod Lipson. 2013. Re-embedding words. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>49--2</pages>
<contexts>
<context position="34526" citStr="Leacock and Chodorow, 1998" startWordPosition="5929" endWordPosition="5932">sources (Guo et al., 2014). The downside of this approach is that parallel data is needed. We used the SCWS dataset for the word similarity task, as it provides a context. Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN (Bruni et al., 2014). And while we use cosine to compute similarity between synsets, there are also a lot of similarity measures that only rely on a given resource, mostly WordNet. These measures are often functions that depend on the provided information like gloss or the topology like shortest-path. Examples include (Wu and Palmer, 1994) and (Leacock and Chodorow, 1998); Blanchard et al. (2005) give a good overview. 7 Conclusion We presented AutoExtend, a flexible method to learn synset and lexeme embeddings from word embeddings. It is completely general and can be used for any other set of embeddings and for any other resource that imposes constraints of a certain type on the relationship between words and other data types. Our experimental results show that AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation. Along with this paper, we will publish AutoExtend for extending word embeddings to other data types; th</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Timothy Chklovski</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The senseval-3 english lexical sample task.</title>
<date>2004</date>
<booktitle>In Proceedings of SENSEVAL-3.</booktitle>
<contexts>
<context position="18533" citStr="Mihalcea et al., 2004" startWordPosition="3227" endWordPosition="3230">n compute nearest neighbors across all three data types as shown in Figure 2. We evaluate the embeddings on WSD and on similarity performance. Our results depend directly on the quality of the underlying word embeddings, in our case word2vec embeddings. We would expect even better evaluation results as word representation learning methods improve. Using a new and improved set of underlying embeddings is simple: it is a simple switch of the input file that contains the word embeddings. 3.1 Word Sense Disambiguation For WSD we use the shared tasks of Senseval2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et al., 2004) and a system named IMS (Zhong and WordNet n word2vec words 147,478 54,570 synsets 117,791 73,844 lexemes 207,272 106,167 Table 2: # of items in WordNet and after intersection with word2vec vectors 1797 nearest neighbors of W/suit S/suit (businessman), L/suit (businessman), L/accomodate, S/suit (be acceptable), L/suit (be acceptable), L/lawsuit, W/lawsuit, S/suit (playing card), L/suit (playing card), S/suit (petition), S/lawsuit, W/countersuit, W/complaint, W/counterclaim nearest neighbors of W/lawsuit L/lawsuit, S/lawsuit, S/countersuit, L/countersuit, W/countersuit, W/suit, W/counterclaim, </context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Rada Mihalcea, Timothy Chklovski, and Adam Kilgarriff. 2004. The senseval-3 english lexical sample task. In Proceedings of SENSEVAL-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="1280" citStr="Mikolov et al., 2013" startWordPosition="185" endWordPosition="188">reebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks. 1 Introduction Unsupervised methods for word embeddings (also called “distributed word representations”) have become popular in natural language processing (NLP). These methods only need very large corpora as input to create sparse representations (e.g., based on local collocations) and project them into a lower dimensional dense vector space. Examples for word embeddings are SENNA (Collobert and Weston, 2008), the hierarchical log-bilinear model (Mnih and Hinton, 2009), word2vec (Mikolov et al., 2013c) and GloVe (Pennington et al., 2014). However, there are many other resources that are undoubtedly useful in NLP, including lexical resources like WordNet and Wiktionary and knowledge bases like Wikipedia and Freebase. We will simply call these resources in the rest of the paper. Our goal is to enrich these valuable resources with embeddings for those data types that are not words; e.g., we want to enrich WordNet with embeddings for synsets and lexemes. A synset is a set of synonyms that are interchangeable in some context. A lexeme pairs a particular spelling or pronunciation with a particu</context>
<context position="4969" citStr="Mikolov et al. (2013" startWordPosition="814" endWordPosition="817">her resources including Wikipedia and Freebase. A word in WordNet can be viewed as a composition of several lexemes. Lexemes from different words together can form a synset. When a synset is given, it can be decomposed into its lexemes. And these lexemes then join to form words. These observations are the basis for the formalization of the constraints encoded in WordNet that will be presented in the next section: we view words as the sum of their lexemes and, analogously, synsets as the sum of their lexemes. Another motivation for our formalization stems from the analogy calculus developed by Mikolov et al. (2013a), which can be viewed as a group theory formalization of word relations: we have a set of elements (our vectors) and an operation (addition) satisfying the properties of a mathematical group, in particular, associativity and invertibility. For example, you can take the vector of king, subtract the vector of man and add the vector of woman to get a vector near queen. In other words, you remove the properties of man and add the properties of woman. We can also see the vector of king as the sum of the vector of man and the vector of a gender-neutral ruler. The next thing to notice is that this </context>
<context position="17229" citStr="Mikolov et al., 2013" startWordPosition="3010" endWordPosition="3013">to start the computation with column normalized matrices and normalize them again after each iteration as long as the error function still decreases. When the error function starts increasing, we stop normalizing the matrices and continue with a normal gradient descent. This respects that while E(d) and D(d) should be column normalized in theory, there are a lot of practical issues that prevent this, e.g., OOV words. 3 Data, experiments and evaluation We downloaded 300-dimensional embeddings for 3,000,000 words and phrases trained on Google News, a corpus of ≈1011 tokens, using word2vec CBOW (Mikolov et al., 2013c). Many words in the word2vec vocabulary are not in WordNet, e.g., inflected forms (cars) and proper nouns (Tony Blair). Conversely, many WordNet lemmas are not in the word2vec vocabulary, e.g., 42 (digits were converted to 0). This results in a number of empty synsets (see Table 2). Note however that AutoExtend can produce embeddings for empty synsets because we use WN relation constraints in addition to synset and lexeme constraints. We run AutoExtend on the word2vec vectors. As we do not know anything about a suitable weighting for the three different constraints, we set α = Q = 0.33. Our </context>
<context position="30300" citStr="Mikolov et al., 2013" startWordPosition="5229" endWordPosition="5232">s, e.g. Barack Obama, President Obama, Obama. The role of words in WordNet would correspond to these aliases in Freebase. This will give us the synset constraint, as well as the lexeme constraint of the system. Relations are given by Freebase types; e.g., we can add a constraint that entity embeddings of the type ”President of the US” should be similar. To explorer multilingual word embeddings we require the word embeddings of different languages to live in the same vector space, which can easily be achieved by training a transformation matrix L between two languages using known translations (Mikolov et al., 2013b). Let X be a matrix where each row is a word embedding in language 1 and Y a matrix where each row is a word embedding in language 2. For each row the words of X and Y are a translation of each other. We then want to minimize the following objective: argmin I ILX − Y I I (26) L We can use a gradient descent to solve this but a matrix inversion will run faster. The matrix L is given by: L = (XT * X)−1(XT * Y ) (27) The matrix L can be used to transform unknown embeddings into the new vector space, which enables us to use a multilingual WordNet like BabelNet (Navigli and Ponzetto, 2010) to com</context>
<context position="32498" citStr="Mikolov et al., 2013" startWordPosition="5596" endWordPosition="5599">rimination on them. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings. An energy based model was 1800 WSD-additional WSD-alone SCWS WN relations Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the three tasks WSD-additional, WSD-alone and SCWS. “x” indicates the maximum; “o” indicates a local minimum. proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple word embeddings. While these embeddings can correspond to different word senses, there is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They use glosses to initialize sense embedding, which in turn can be used for WSD. The sense disambiguated data can again be used to improve sense embeddings. This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of given word embeddings – without (re)training them. </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</title>
<date>2013</date>
<contexts>
<context position="1280" citStr="Mikolov et al., 2013" startWordPosition="185" endWordPosition="188">reebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks. 1 Introduction Unsupervised methods for word embeddings (also called “distributed word representations”) have become popular in natural language processing (NLP). These methods only need very large corpora as input to create sparse representations (e.g., based on local collocations) and project them into a lower dimensional dense vector space. Examples for word embeddings are SENNA (Collobert and Weston, 2008), the hierarchical log-bilinear model (Mnih and Hinton, 2009), word2vec (Mikolov et al., 2013c) and GloVe (Pennington et al., 2014). However, there are many other resources that are undoubtedly useful in NLP, including lexical resources like WordNet and Wiktionary and knowledge bases like Wikipedia and Freebase. We will simply call these resources in the rest of the paper. Our goal is to enrich these valuable resources with embeddings for those data types that are not words; e.g., we want to enrich WordNet with embeddings for synsets and lexemes. A synset is a set of synonyms that are interchangeable in some context. A lexeme pairs a particular spelling or pronunciation with a particu</context>
<context position="4969" citStr="Mikolov et al. (2013" startWordPosition="814" endWordPosition="817">her resources including Wikipedia and Freebase. A word in WordNet can be viewed as a composition of several lexemes. Lexemes from different words together can form a synset. When a synset is given, it can be decomposed into its lexemes. And these lexemes then join to form words. These observations are the basis for the formalization of the constraints encoded in WordNet that will be presented in the next section: we view words as the sum of their lexemes and, analogously, synsets as the sum of their lexemes. Another motivation for our formalization stems from the analogy calculus developed by Mikolov et al. (2013a), which can be viewed as a group theory formalization of word relations: we have a set of elements (our vectors) and an operation (addition) satisfying the properties of a mathematical group, in particular, associativity and invertibility. For example, you can take the vector of king, subtract the vector of man and add the vector of woman to get a vector near queen. In other words, you remove the properties of man and add the properties of woman. We can also see the vector of king as the sum of the vector of man and the vector of a gender-neutral ruler. The next thing to notice is that this </context>
<context position="17229" citStr="Mikolov et al., 2013" startWordPosition="3010" endWordPosition="3013">to start the computation with column normalized matrices and normalize them again after each iteration as long as the error function still decreases. When the error function starts increasing, we stop normalizing the matrices and continue with a normal gradient descent. This respects that while E(d) and D(d) should be column normalized in theory, there are a lot of practical issues that prevent this, e.g., OOV words. 3 Data, experiments and evaluation We downloaded 300-dimensional embeddings for 3,000,000 words and phrases trained on Google News, a corpus of ≈1011 tokens, using word2vec CBOW (Mikolov et al., 2013c). Many words in the word2vec vocabulary are not in WordNet, e.g., inflected forms (cars) and proper nouns (Tony Blair). Conversely, many WordNet lemmas are not in the word2vec vocabulary, e.g., 42 (digits were converted to 0). This results in a number of empty synsets (see Table 2). Note however that AutoExtend can produce embeddings for empty synsets because we use WN relation constraints in addition to synset and lexeme constraints. We run AutoExtend on the word2vec vectors. As we do not know anything about a suitable weighting for the three different constraints, we set α = Q = 0.33. Our </context>
<context position="30300" citStr="Mikolov et al., 2013" startWordPosition="5229" endWordPosition="5232">s, e.g. Barack Obama, President Obama, Obama. The role of words in WordNet would correspond to these aliases in Freebase. This will give us the synset constraint, as well as the lexeme constraint of the system. Relations are given by Freebase types; e.g., we can add a constraint that entity embeddings of the type ”President of the US” should be similar. To explorer multilingual word embeddings we require the word embeddings of different languages to live in the same vector space, which can easily be achieved by training a transformation matrix L between two languages using known translations (Mikolov et al., 2013b). Let X be a matrix where each row is a word embedding in language 1 and Y a matrix where each row is a word embedding in language 2. For each row the words of X and Y are a translation of each other. We then want to minimize the following objective: argmin I ILX − Y I I (26) L We can use a gradient descent to solve this but a matrix inversion will run faster. The matrix L is given by: L = (XT * X)−1(XT * Y ) (27) The matrix L can be used to transform unknown embeddings into the new vector space, which enables us to use a multilingual WordNet like BabelNet (Navigli and Ponzetto, 2010) to com</context>
<context position="32498" citStr="Mikolov et al., 2013" startWordPosition="5596" endWordPosition="5599">rimination on them. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings. An energy based model was 1800 WSD-additional WSD-alone SCWS WN relations Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the three tasks WSD-additional, WSD-alone and SCWS. “x” indicates the maximum; “o” indicates a local minimum. proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple word embeddings. While these embeddings can correspond to different word senses, there is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They use glosses to initialize sense embedding, which in turn can be used for WSD. The sense disambiguated data can again be used to improve sense embeddings. This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of given word embeddings – without (re)training them. </context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="1280" citStr="Mikolov et al., 2013" startWordPosition="185" endWordPosition="188">reebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks. 1 Introduction Unsupervised methods for word embeddings (also called “distributed word representations”) have become popular in natural language processing (NLP). These methods only need very large corpora as input to create sparse representations (e.g., based on local collocations) and project them into a lower dimensional dense vector space. Examples for word embeddings are SENNA (Collobert and Weston, 2008), the hierarchical log-bilinear model (Mnih and Hinton, 2009), word2vec (Mikolov et al., 2013c) and GloVe (Pennington et al., 2014). However, there are many other resources that are undoubtedly useful in NLP, including lexical resources like WordNet and Wiktionary and knowledge bases like Wikipedia and Freebase. We will simply call these resources in the rest of the paper. Our goal is to enrich these valuable resources with embeddings for those data types that are not words; e.g., we want to enrich WordNet with embeddings for synsets and lexemes. A synset is a set of synonyms that are interchangeable in some context. A lexeme pairs a particular spelling or pronunciation with a particu</context>
<context position="4969" citStr="Mikolov et al. (2013" startWordPosition="814" endWordPosition="817">her resources including Wikipedia and Freebase. A word in WordNet can be viewed as a composition of several lexemes. Lexemes from different words together can form a synset. When a synset is given, it can be decomposed into its lexemes. And these lexemes then join to form words. These observations are the basis for the formalization of the constraints encoded in WordNet that will be presented in the next section: we view words as the sum of their lexemes and, analogously, synsets as the sum of their lexemes. Another motivation for our formalization stems from the analogy calculus developed by Mikolov et al. (2013a), which can be viewed as a group theory formalization of word relations: we have a set of elements (our vectors) and an operation (addition) satisfying the properties of a mathematical group, in particular, associativity and invertibility. For example, you can take the vector of king, subtract the vector of man and add the vector of woman to get a vector near queen. In other words, you remove the properties of man and add the properties of woman. We can also see the vector of king as the sum of the vector of man and the vector of a gender-neutral ruler. The next thing to notice is that this </context>
<context position="17229" citStr="Mikolov et al., 2013" startWordPosition="3010" endWordPosition="3013">to start the computation with column normalized matrices and normalize them again after each iteration as long as the error function still decreases. When the error function starts increasing, we stop normalizing the matrices and continue with a normal gradient descent. This respects that while E(d) and D(d) should be column normalized in theory, there are a lot of practical issues that prevent this, e.g., OOV words. 3 Data, experiments and evaluation We downloaded 300-dimensional embeddings for 3,000,000 words and phrases trained on Google News, a corpus of ≈1011 tokens, using word2vec CBOW (Mikolov et al., 2013c). Many words in the word2vec vocabulary are not in WordNet, e.g., inflected forms (cars) and proper nouns (Tony Blair). Conversely, many WordNet lemmas are not in the word2vec vocabulary, e.g., 42 (digits were converted to 0). This results in a number of empty synsets (see Table 2). Note however that AutoExtend can produce embeddings for empty synsets because we use WN relation constraints in addition to synset and lexeme constraints. We run AutoExtend on the word2vec vectors. As we do not know anything about a suitable weighting for the three different constraints, we set α = Q = 0.33. Our </context>
<context position="30300" citStr="Mikolov et al., 2013" startWordPosition="5229" endWordPosition="5232">s, e.g. Barack Obama, President Obama, Obama. The role of words in WordNet would correspond to these aliases in Freebase. This will give us the synset constraint, as well as the lexeme constraint of the system. Relations are given by Freebase types; e.g., we can add a constraint that entity embeddings of the type ”President of the US” should be similar. To explorer multilingual word embeddings we require the word embeddings of different languages to live in the same vector space, which can easily be achieved by training a transformation matrix L between two languages using known translations (Mikolov et al., 2013b). Let X be a matrix where each row is a word embedding in language 1 and Y a matrix where each row is a word embedding in language 2. For each row the words of X and Y are a translation of each other. We then want to minimize the following objective: argmin I ILX − Y I I (26) L We can use a gradient descent to solve this but a matrix inversion will run faster. The matrix L is given by: L = (XT * X)−1(XT * Y ) (27) The matrix L can be used to transform unknown embeddings into the new vector space, which enables us to use a multilingual WordNet like BabelNet (Navigli and Ponzetto, 2010) to com</context>
<context position="32498" citStr="Mikolov et al., 2013" startWordPosition="5596" endWordPosition="5599">rimination on them. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings. An energy based model was 1800 WSD-additional WSD-alone SCWS WN relations Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the three tasks WSD-additional, WSD-alone and SCWS. “x” indicates the maximum; “o” indicates a local minimum. proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple word embeddings. While these embeddings can correspond to different word senses, there is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They use glosses to initialize sense embedding, which in turn can be used for WSD. The sense disambiguated data can again be used to improve sense embeddings. This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of given word embeddings – without (re)training them. </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013c. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<marker>Miller, Charles, 1991</marker>
<rawString>George A Miller and Walter G Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="31278" citStr="Mnih and Hinton (2007)" startWordPosition="5412" endWordPosition="5415"> faster. The matrix L is given by: L = (XT * X)−1(XT * Y ) (27) The matrix L can be used to transform unknown embeddings into the new vector space, which enables us to use a multilingual WordNet like BabelNet (Navigli and Ponzetto, 2010) to compute synset embeddings. We can add cross-linguistic relationships to our model, e.g., aligning German and English synset embeddings of the same concept. 6 Related Work Rumelhart et al. (1988) introduced distributed word representations, usually called word embeddings today. There has been a resurgence of work on them recently (e.g., Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch¨utze (1998) created sense representations by clustering context representations derived from co-occurrence. The representation of a sense is simply the centroid of its cluster. Huang et al. (2012) improved this by learning single-prototype embeddings before performing word sense disc</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="1248" citStr="Mnih and Hinton, 2009" startWordPosition="180" endWordPosition="183"> applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks. 1 Introduction Unsupervised methods for word embeddings (also called “distributed word representations”) have become popular in natural language processing (NLP). These methods only need very large corpora as input to create sparse representations (e.g., based on local collocations) and project them into a lower dimensional dense vector space. Examples for word embeddings are SENNA (Collobert and Weston, 2008), the hierarchical log-bilinear model (Mnih and Hinton, 2009), word2vec (Mikolov et al., 2013c) and GloVe (Pennington et al., 2014). However, there are many other resources that are undoubtedly useful in NLP, including lexical resources like WordNet and Wiktionary and knowledge bases like Wikipedia and Freebase. We will simply call these resources in the rest of the paper. Our goal is to enrich these valuable resources with embeddings for those data types that are not words; e.g., we want to enrich WordNet with embeddings for synsets and lexemes. A synset is a set of synonyms that are interchangeable in some context. A lexeme pairs a particular spelling</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2009. A scalable hierarchical distributed language model. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Babelnet: Building a very large multilingual semantic network.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="30893" citStr="Navigli and Ponzetto, 2010" startWordPosition="5352" endWordPosition="5355">translations (Mikolov et al., 2013b). Let X be a matrix where each row is a word embedding in language 1 and Y a matrix where each row is a word embedding in language 2. For each row the words of X and Y are a translation of each other. We then want to minimize the following objective: argmin I ILX − Y I I (26) L We can use a gradient descent to solve this but a matrix inversion will run faster. The matrix L is given by: L = (XT * X)−1(XT * Y ) (27) The matrix L can be used to transform unknown embeddings into the new vector space, which enables us to use a multilingual WordNet like BabelNet (Navigli and Ponzetto, 2010) to compute synset embeddings. We can add cross-linguistic relationships to our model, e.g., aligning German and English synset embeddings of the same concept. 6 Related Work Rumelhart et al. (1988) introduced distributed word representations, usually called word embeddings today. There has been a resurgence of work on them recently (e.g., Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches</context>
</contexts>
<marker>Navigli, Ponzetto, 2010</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2010. Babelnet: Building a very large multilingual semantic network. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Jeevan Shankar</author>
<author>Alexandre Passos</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient nonparametric estimation of multiple embeddings per word in vector space.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="25597" citStr="Neelakantan et al. (2014)" startWordPosition="4428" endWordPosition="4431">ch word, we compute a context vector c by adding all word vectors of the context, excluding the test word itself. Following Reisinger and Mooney (2010), we compute the lexeme (resp. synset) vector l either as the simple average of the lexeme (resp. synset) vectors l(ij) (method AvgSim, no dependence on c in this case) or as the average of the lexeme (resp. synset) vectors weighted by cosine similarity to c (method AvgSimC). Table 4 shows that AutoExtend lexeme embeddings (line 7) perform better than previous work, AvgSim AvgSimC 1 Huang et al. (2012) 62.8† 65.7† 2 Tian et al. (2014) – 65.4† 3 Neelakantan et al. (2014) 67.2 69.3 4 Chen et al. (2014) 66.2† 68.9 5 words (word2vec) 66.6‡ 66.6† 6 synsets 62.6† 63.7† 7 lexemes 68.9 69.8 Table 4: Spearman correlation (ρ × 100) on SCWS. Best result per column in bold. including (Huang et al., 2012) and (Tian et al., 2014). Lexeme embeddings perform better than synset embeddings (lines 7 vs. 6), presumably because using a representation that is specific to the actual word being judged is more precise than using a representation that also includes synonyms. A simple baseline is to use the underlying word2vec embeddings directly (line 5). In this case, there is only </context>
<context position="32424" citStr="Neelakantan et al. (2014)" startWordPosition="5582" endWordPosition="5586"> this by learning single-prototype embeddings before performing word sense discrimination on them. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings. An energy based model was 1800 WSD-additional WSD-alone SCWS WN relations Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the three tasks WSD-additional, WSD-alone and SCWS. “x” indicates the maximum; “o” indicates a local minimum. proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple word embeddings. While these embeddings can correspond to different word senses, there is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They use glosses to initialize sense embedding, which in turn can be used for WSD. The sense disambiguated data can again be used to improve sense embeddings. This prior work needs a training step to learn embeddings. In contrast, we can “</context>
</contexts>
<marker>Neelakantan, Shankar, Passos, McCallum, 2014</marker>
<rawString>Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient nonparametric estimation of multiple embeddings per word in vector space. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1318" citStr="Pennington et al., 2014" startWordPosition="191" endWordPosition="194">-of-the-art performance on word similarity and word sense disambiguation tasks. 1 Introduction Unsupervised methods for word embeddings (also called “distributed word representations”) have become popular in natural language processing (NLP). These methods only need very large corpora as input to create sparse representations (e.g., based on local collocations) and project them into a lower dimensional dense vector space. Examples for word embeddings are SENNA (Collobert and Weston, 2008), the hierarchical log-bilinear model (Mnih and Hinton, 2009), word2vec (Mikolov et al., 2013c) and GloVe (Pennington et al., 2014). However, there are many other resources that are undoubtedly useful in NLP, including lexical resources like WordNet and Wiktionary and knowledge bases like Wikipedia and Freebase. We will simply call these resources in the rest of the paper. Our goal is to enrich these valuable resources with embeddings for those data types that are not words; e.g., we want to enrich WordNet with embeddings for synsets and lexemes. A synset is a set of synonyms that are interchangeable in some context. A lexeme pairs a particular spelling or pronunciation with a particular meaning, i.e., a lexeme is a conju</context>
<context position="31353" citStr="Pennington et al. (2014)" startWordPosition="5424" endWordPosition="5427">x L can be used to transform unknown embeddings into the new vector space, which enables us to use a multilingual WordNet like BabelNet (Navigli and Ponzetto, 2010) to compute synset embeddings. We can add cross-linguistic relationships to our model, e.g., aligning German and English synset embeddings of the same concept. 6 Related Work Rumelhart et al. (1988) introduced distributed word representations, usually called word embeddings today. There has been a resurgence of work on them recently (e.g., Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch¨utze (1998) created sense representations by clustering context representations derived from co-occurrence. The representation of a sense is simply the centroid of its cluster. Huang et al. (2012) improved this by learning single-prototype embeddings before performing word sense discrimination on them. Bordes et al. (2011) created similarity measures for re</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="25123" citStr="Reisinger and Mooney (2010)" startWordPosition="4344" endWordPosition="4347"> in context came from is not available. However, the dataset is the closest we could find for sense similarity. Synset and lexeme embeddings are obtained by running AutoExtend. Based on the results of the WSD task, we set α = 0.2 and Q = 0.5. Lexeme embeddings are the natural choice for this task as human subjects are provided with two words and a context for each and then have to assign a similarity score. But for completeness, we also run experiments for synsets. For each word, we compute a context vector c by adding all word vectors of the context, excluding the test word itself. Following Reisinger and Mooney (2010), we compute the lexeme (resp. synset) vector l either as the simple average of the lexeme (resp. synset) vectors l(ij) (method AvgSim, no dependence on c in this case) or as the average of the lexeme (resp. synset) vectors weighted by cosine similarity to c (method AvgSimC). Table 4 shows that AutoExtend lexeme embeddings (line 7) perform better than previous work, AvgSim AvgSimC 1 Huang et al. (2012) 62.8† 65.7† 2 Tian et al. (2014) – 65.4† 3 Neelakantan et al. (2014) 67.2 69.3 4 Chen et al. (2014) 66.2† 68.9 5 words (word2vec) 66.6‡ 66.6† 6 synsets 62.6† 63.7† 7 lexemes 68.9 69.8 Table 4: S</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning representations by backpropagating errors. Cognitive Modeling,</title>
<date>1988</date>
<pages>5--213</pages>
<contexts>
<context position="31091" citStr="Rumelhart et al. (1988)" startWordPosition="5383" endWordPosition="5386">Y are a translation of each other. We then want to minimize the following objective: argmin I ILX − Y I I (26) L We can use a gradient descent to solve this but a matrix inversion will run faster. The matrix L is given by: L = (XT * X)−1(XT * Y ) (27) The matrix L can be used to transform unknown embeddings into the new vector space, which enables us to use a multilingual WordNet like BabelNet (Navigli and Ponzetto, 2010) to compute synset embeddings. We can add cross-linguistic relationships to our model, e.g., aligning German and English synset embeddings of the same concept. 6 Related Work Rumelhart et al. (1988) introduced distributed word representations, usually called word embeddings today. There has been a resurgence of work on them recently (e.g., Bengio et al. (2003) Mnih and Hinton (2007), Collobert et al. (2011), Mikolov et al. (2013a), Pennington et al. (2014)). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend. There are several approaches to finding embeddings for senses, variously called meaning, sense and multiple word embeddings. Sch¨utze (1998) created sense representations by clustering context representations derived from co-o</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1988</marker>
<rawString>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1988. Learning representations by backpropagating errors. Cognitive Modeling, 5:213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Tian</author>
<author>Hanjun Dai</author>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Rui Zhang</author>
<author>Enhong Chen</author>
<author>Tie-Yan Liu</author>
</authors>
<title>A probabilistic model for learning multi-prototype word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of Coling, Technical Papers.</booktitle>
<contexts>
<context position="25561" citStr="Tian et al. (2014)" startWordPosition="4421" endWordPosition="4424">periments for synsets. For each word, we compute a context vector c by adding all word vectors of the context, excluding the test word itself. Following Reisinger and Mooney (2010), we compute the lexeme (resp. synset) vector l either as the simple average of the lexeme (resp. synset) vectors l(ij) (method AvgSim, no dependence on c in this case) or as the average of the lexeme (resp. synset) vectors weighted by cosine similarity to c (method AvgSimC). Table 4 shows that AutoExtend lexeme embeddings (line 7) perform better than previous work, AvgSim AvgSimC 1 Huang et al. (2012) 62.8† 65.7† 2 Tian et al. (2014) – 65.4† 3 Neelakantan et al. (2014) 67.2 69.3 4 Chen et al. (2014) 66.2† 68.9 5 words (word2vec) 66.6‡ 66.6† 6 synsets 62.6† 63.7† 7 lexemes 68.9 69.8 Table 4: Spearman correlation (ρ × 100) on SCWS. Best result per column in bold. including (Huang et al., 2012) and (Tian et al., 2014). Lexeme embeddings perform better than synset embeddings (lines 7 vs. 6), presumably because using a representation that is specific to the actual word being judged is more precise than using a representation that also includes synonyms. A simple baseline is to use the underlying word2vec embeddings directly (l</context>
<context position="32447" citStr="Tian et al. (2014)" startWordPosition="5588" endWordPosition="5591">type embeddings before performing word sense discrimination on them. Bordes et al. (2011) created similarity measures for relations in WordNet and Freebase to learn entity embeddings. An energy based model was 1800 WSD-additional WSD-alone SCWS WN relations Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the three tasks WSD-additional, WSD-alone and SCWS. “x” indicates the maximum; “o” indicates a local minimum. proposed by Bordes et al. (2012) to create disambiguated meaning embeddings and Neelakantan et al. (2014) and Tian et al. (2014) extended the Skip-gram model (Mikolov et al., 2013a) to learn multiple word embeddings. While these embeddings can correspond to different word senses, there is no clear mapping between them and a lexical resource like WordNet. Chen et al. (2014) also modified word2vec to learn sense embeddings, each corresponding to a WordNet synset. They use glosses to initialize sense embedding, which in turn can be used for WSD. The sense disambiguated data can again be used to improve sense embeddings. This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of </context>
</contexts>
<marker>Tian, Dai, Bian, Gao, Zhang, Chen, Liu, 2014</marker>
<rawString>Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang, Enhong Chen, and Tie-Yan Liu. 2014. A probabilistic model for learning multi-prototype word embeddings. In Proceedings of Coling, Technical Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="34493" citStr="Wu and Palmer, 1994" startWordPosition="5924" endWordPosition="5927">beddings uses bilingual resources (Guo et al., 2014). The downside of this approach is that parallel data is needed. We used the SCWS dataset for the word similarity task, as it provides a context. Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN (Bruni et al., 2014). And while we use cosine to compute similarity between synsets, there are also a lot of similarity measures that only rely on a given resource, mostly WordNet. These measures are often functions that depend on the provided information like gloss or the topology like shortest-path. Examples include (Wu and Palmer, 1994) and (Leacock and Chodorow, 1998); Blanchard et al. (2005) give a good overview. 7 Conclusion We presented AutoExtend, a flexible method to learn synset and lexeme embeddings from word embeddings. It is completely general and can be used for any other set of embeddings and for any other resource that imposes constraints of a certain type on the relationship between words and other data types. Our experimental results show that AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation. Along with this paper, we will publish AutoExtend for extending word e</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Improving lexical embeddings with semantic knowledge.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="33553" citStr="Yu and Dredze (2014)" startWordPosition="5770" endWordPosition="5773">mbeddings. This prior work needs a training step to learn embeddings. In contrast, we can “AutoExtend” any set of given word embeddings – without (re)training them. There is only little work on taking existing word embeddings and producing embeddings in the same space. Labutov and Lipson (2013) tuned existing word embeddings in supervised training, not to create new embeddings for senses or entities, but to get better predictive performance on a task while not changing the space of embeddings. Lexical resources have also been used to improve word embeddings. In the Relation Constrained Model, Yu and Dredze (2014) use word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results. Bian et al. (2014) used not only semantic, but also morphological and syntactic knowledge to compute more effective word embeddings. Another interesting approach to create sense specific word embeddings uses bilingual resources (Guo et al., 2014). The downside of this approach is that parallel data is needed. We used the SCWS dataset for the word similarity task, as it provides a context. Other frequently used datasets are WordSim-353 (Finkelstein et al., 2001) or MEN (</context>
</contexts>
<marker>Yu, Dredze, 2014</marker>
<rawString>Mo Yu and Mark Dredze. 2014. Improving lexical embeddings with semantic knowledge. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>It makes sense: A wide-coverage word sense disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL, System Demonstrations.</booktitle>
<contexts>
<context position="23254" citStr="Zhong and Ng (2010)" startWordPosition="4001" endWordPosition="4004">n line 14 outperforms all previous systems, most of them significantly. While S-raw performs quite reasonably as a feature set alone, it hurts the performance when used as an additional feature set. As this is the feature set that contains the largest number of features (n(k + 1)), overfitting is the likely reason. Conversely, S-cosine only adds k features and therefore may suffer from underfitting. We do a grid search (step size .1) for optimal values of α and β, optimizing the average score of Senseval-2 and Senseval-3. The best performing feature set combination is Soptimized-product with 1Zhong and Ng (2010) report accuracies of 65.3% / 72.6% for this configuration. †In Table 3 and Table 4, results significantly worse than the best (bold) result in each column are marked † for α = .05 and ‡ for α = .10 (one-tailed Z-test). 1798 Senseval-2 Senseval-3 1 POS 53.6 58.0 2 surrounding word 57.6 65.3 3 local collocation 58.7 64.7 4 Snaive-product 56.5 62.2 5 S-cosine 55.5 60.5 6 S-product 58.3 64.3 7 S-raw 56.8 63.1 8 MFS 47.6† 55.2† 9 Rank 1 system 64.2† 72.9 10 Rank 2 system 63.8† 72.6 11 IMS 65.2‡ 72.3‡ 12 IMS + Snaive-prod. 62.6† 69.4† 13 IMS + S-cosine 65.1‡ 72.4‡ 14 IMS + S-product 66.5 73.6 15 IM</context>
</contexts>
<marker>Zhong, Ng, 2010</marker>
<rawString>Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: A wide-coverage word sense disambiguation system for free text. In Proceedings ofACL, System Demonstrations.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>