<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.972544">
Grammatical Error Correction Using Integer Linear Programming
</title>
<author confidence="0.991045">
Yuanbin Wu
</author>
<affiliation confidence="0.99979">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.967803">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.999061">
wuyb@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.995651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888285714286">
We propose a joint inference algorithm for
grammatical error correction. Different
from most previous work where different
error types are corrected independently,
our proposed inference process considers
all possible errors in a unified framework.
We use integer linear programming (ILP)
to model the inference process, which can
easily incorporate both the power of exist-
ing error classifiers and prior knowledge
on grammatical error correction. Exper-
imental results on the Helping Our Own
shared task show that our method is com-
petitive with state-of-the-art systems.
</bodyText>
<sectionHeader confidence="0.9979" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999302083333334">
Grammatical error correction is an important task
of natural language processing (NLP). It has many
potential applications and may help millions of
people who learn English as a second language
(ESL). As a research field, it faces the challenge of
processing ungrammatical language, which is dif-
ferent from other NLP tasks. The task has received
much attention in recent years, and was the focus
of two shared tasks on grammatical error correc-
tion in 2011 and 2012 (Dale and Kilgarriff, 2011;
Dale et al., 2012).
To detect and correct grammatical errors, two
different approaches are typically used — knowl-
edge engineering or machine learning. The first
relies on handcrafting a set of rules. For exam-
ple, the superlative adjective best is preceded by
the article the. In contrast, the machine learn-
ing approach formulates the task as a classification
problem based on learning from training data. For
example, an article classifier takes a noun phrase
(NP) as input and predicts its article using class
labels a/an, the, or ɛ (no article).
Both approaches have their advantages and dis-
advantages. One can readily handcraft a set of
</bodyText>
<affiliation confidence="0.782033666666667">
Hwee Tou Ng
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.9494775">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.991">
nght@comp.nus.edu.sg
</email>
<bodyText confidence="0.999832804878049">
rules to incorporate various prior knowledge from
grammar books and dictionaries, but rules often
have exceptions and it is difficult to build rules
for all grammatical errors. On the other hand, the
machine learning approach can learn from texts
written by ESL learners where grammatical errors
have been annotated. However, training data may
be noisy and classifiers may need prior knowledge
to guide their predictions.
Another consideration in grammatical error cor-
rection is how to deal with multiple errors in an
input sentence. Most previous work deals with
errors individually: different classifiers (or rules)
are developed for different types of errors (article
classifier, preposition classifier, etc). Classifiers
are then deployed independently. An example is
a pipeline system, where each classifier takes the
output of the previous classifier as its input and
proposes corrections of one error type.
One problem of this pipeline approach is that
the relations between errors are ignored. For ex-
ample, assume that an input sentence contains a
cats. An article classifier may propose to delete
a, while a noun number classifier may propose
to change cats to cat. A pipeline approach will
choose one of the two corrections based purely
on which error classifier is applied first. Another
problem is that when applying a classifier, the sur-
rounding words in the context are assumed to be
correct, which is not true if grammatical errors ap-
pear close to each other in a sentence.
In this paper, we formulate grammatical er-
ror correction as a task suited for joint inference.
Given an input sentence, different types of errors
are jointly corrected as follows. For every possi-
ble error correction, we assign a score which mea-
sures how grammatical the resulting sentence is if
the correction is accepted. We then choose a set
of corrections which will result in a corrected sen-
tence that is judged to be the most grammatical.
The inference problem is solved by integer lin-
</bodyText>
<page confidence="0.940289">
1456
</page>
<note confidence="0.9141735">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1456–1465,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99933152631579">
ear programming (ILP). Variables of ILP are indi-
cators of possible grammatical error corrections,
the objective function aims to select the best set of
corrections, and the constraints help to enforce a
valid and grammatical output. Furthermore, ILP
not only provides a method to solve the inference
problem, but also allows for a natural integration
of grammatical constraints into a machine learn-
ing approach. We will show that ILP fully utilizes
individual error classifiers, while prior knowledge
on grammatical error correction can be easily ex-
pressed using linear constraints. We evaluate our
proposed ILP approach on the test data from the
Helping Our Own (HOO) 2011 shared task (Dale
and Kilgarriff, 2011). Experimental results show
that the ILP formulation is competitive with state-
of-the-art grammatical error correction systems.
The remainder of this paper is organized as fol-
lows. Section 2 gives the related work. Section
</bodyText>
<listItem confidence="0.4924834">
3 introduces a basic ILP formulation. Sections
4 and 5 improve the basic ILP formulation with
more constraints and second order variables, re-
spectively. Section 6 presents the experimental re-
sults. Section 7 concludes the paper.
</listItem>
<sectionHeader confidence="0.997714" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999906857142857">
The knowledge engineering approach has been
used in early grammatical error correction systems
(Murata and Nagao, 1993; Bond et al., 1995; Bond
and Ikehara, 1996; Heine, 1998). However, as
noted by (Han et al., 2006), rules usually have ex-
ceptions, and it is hard to utilize corpus statistics
in handcrafted rules. As such, the machine learn-
ing approach has become the dominant approach
in grammatical error correction.
Previous work in the machine learning approach
typically formulates the task as a classification
problem. Article and preposition errors are the two
main research topics (Knight and Chander, 1994;
Han et al., 2006; Tetreault and Chodorow, 2008;
Dahlmeier and Ng, 2011). Features used in classi-
fication include surrounding words, part-of-speech
tags, language model scores (Gamon, 2010), and
parse tree structures (Tetreault et al., 2010). Learn-
ing algorithms used include maximum entropy
(Han et al., 2006; Tetreault and Chodorow, 2008),
averaged perceptron, naive Bayes (Rozovskaya
and Roth, 2011), etc. Besides article and prepo-
sition errors, verb form errors also attract some
attention recently (Liu et al., 2010; Tajiri et al.,
2012).
Several research efforts have started to deal with
correcting different errors in an integrated manner
(Gamon, 2011; Park and Levy, 2011; Dahlmeier
and Ng, 2012a). Gamon (2011) uses a high-order
sequential labeling model to detect various errors.
Park and Levy (2011) models grammatical error
correction using a noisy channel model, where a
predefined generative model produces correct sen-
tences and errors are added through a noise model.
The work of (Dahlmeier and Ng, 2012a) is proba-
bly the closest to our current work. It uses a beam-
search decoder, which iteratively corrects an in-
put sentence to arrive at the best corrected output.
The difference between their work and our ILP
approach is that the beam-search decoder returns
an approximate solution to the original inference
problem, while ILP returns an exact solution to an
approximate inference problem.
Integer linear programming has been success-
fully applied to many NLP tasks, such as depen-
dency parsing (Riedel and Clarke, 2006; Martins
et al., 2009), semantic role labeling (Punyakanok
et al., 2005), and event extraction (Riedel and Mc-
Callum, 2011).
</bodyText>
<sectionHeader confidence="0.988931" genericHeader="method">
3 Inference with First Order Variables
</sectionHeader>
<bodyText confidence="0.997618375">
The inference problem for grammatical error cor-
rection can be stated as follows: “Given an input
sentence, choose a set of corrections which results
in the best output sentence.” In this paper, this
problem will be expressed and solved by integer
linear programming (ILP).
To express an NLP task in the framework of ILP
requires the following steps:
</bodyText>
<listItem confidence="0.9928135">
1. Encode the output space of the NLP task us-
ing integer variables;
2. Express the inference objective as a linear
objective function; and
3. Introduce problem-specific constraints to re-
fine the feasible output space.
</listItem>
<bodyText confidence="0.992151875">
In the following sections, we follow the above
formulation. For the grammatical error correc-
tion task, the variables in ILP are indicators of the
corrections that a word needs, the objective func-
tion measures how grammatical the whole sen-
tence is if some corrections are accepted, and the
constraints guarantee that the corrections do not
conflict with each other.
</bodyText>
<page confidence="0.988461">
1457
</page>
<subsectionHeader confidence="0.998633">
3.1 First Order Variables
</subsectionHeader>
<bodyText confidence="0.994931285714286">
Given an input sentence, the main question that a
grammatical error correction system needs to an-
swer is: What corrections at which positions? For
example, is it reasonable to change the word cats
to cat in the sentence A cats sat on the mat? Given
the corrections at various positions in a sentence,
the system can readily come up with the corrected
sentence. Thus, a natural way to encode the output
space of grammatical error correction requires in-
formation about sentence position, error type (e.g.,
noun number error), and correction (e.g., cat).
Suppose s is an input sentence, and |s |is its
length (i.e., the number of words in s). Define first
order variables:
</bodyText>
<equation confidence="0.984431">
Zkl,p ∈ {0,1}, (1)
</equation>
<bodyText confidence="0.510111">
where
</bodyText>
<equation confidence="0.859928">
p∈ {1, 2, ... , |s|} is a position in a sentence,
l∈ L is an error type,
k∈ {1, 2, ... , C(l)} is a correction of type l.
L: the set of error types,
C(l): the number of corrections for error type l.
</equation>
<bodyText confidence="0.9997071">
If Zkl,p = 1, the word at position p should be cor-
rected to k that is of error type l. Otherwise, the
word at position p is not applicable for this correc-
tion. Deletion of a word is represented as k = ɛ.
For example, Z�Art,1 = 1 means that the article
(Art) at position 1 of the sentence should be a. If
ZArt,1 = 0, then the article should not be a. Ta-
ble 1 contains the error types handled in this work,
their possible corrections and applicable positions
in a sentence.
</bodyText>
<subsectionHeader confidence="0.999435">
3.2 The Objective Function
</subsectionHeader>
<bodyText confidence="0.998701307692308">
The objective of the inference problem is to find
the best output sentence. However, there are expo-
nentially many different combinations of correc-
tions, and it is not possible to consider all com-
binations. Therefore, instead of solving the orig-
inal inference problem, we will solve an approx-
imate inference problem by introducing the fol-
lowing decomposable assumption: Measuring the
output quality of multiple corrections can be de-
composed into measuring the quality of the indi-
vidual corrections.
Let s0 be the resulting sentence if the correction
Zkl,p is accepted for s, or for simplicity denoting
</bodyText>
<equation confidence="0.890712666666667">
Zk
it as s −−→ s0. Let wl,p,k ∈ R, measure how
l,p
grammatical s0 is. Define the objective function as
∑max wl,p,kZkl,p.
l,p,k
</equation>
<bodyText confidence="0.990370081081081">
This linear objective function aims to select a set
of Zkl,p, such that the sum of their weights is the
largest among all possible candidate corrections,
which in turn gives the most grammatical sentence
under the decomposable assumption.
Although the decomposable assumption is a
strong assumption, it performs well in practice,
and one can relax the assumption by using higher
order variables (see Section 5).
For an individual correction Zkl,p, we measure
the quality of s0 based on three factors:
1. The language model score h(s0, LM) of s0
based on a large web corpus;
2. The confidence scores f(s0, t) of classifiers,
where t ∈ E and E is the set of classifiers. For ex-
ample, an article classifier trained on well-written
documents will score every article in s0, and mea-
sure the quality of s0 from the perspective of an
article “expert”.
3. The disagreement scores g(s0, t) of classi-
fiers, where t ∈ E. A disagreement score mea-
sures how ungrammatical s0 is from the perspec-
tive of a classifier. Take the article classifier as an
example. For each article instance in s0, the clas-
sifier computes the difference between the maxi-
mum confidence score among all possible choices
of articles, and the confidence score of the ob-
served article. This difference represents the dis-
agreement on the observed article by the article
classifier or “expert”. Define the maximum differ-
ence over all article instances in s0 to be the article
classifier disagreement score of s0. In general, this
score is large if the sentence s0 is more ungram-
matical.
The weight wl,p,k is a combination of these
scores:
where νLM, λt, and µt are the coefficients.
</bodyText>
<subsectionHeader confidence="0.970813">
3.3 Constraints
</subsectionHeader>
<bodyText confidence="0.999556">
An observation on the objective function is that
it is possible, for example, to set Z�Art,p = 1 and
</bodyText>
<equation confidence="0.650254153846154">
∑wl,p,k = νLMh(s0, LM) + λtf(s0, t)
t∈E
∑ µtg(s0, t), (2)
+
t∈E
1458
Type l Correction k C(l) Applicable Variables
the ɛ
article a, the, ɛ 3 article or NP ZaArt,p, ZArt,p,ZArt,p
preposition on, at, in, ... |confusion set |preposition Zon
Prep,p, Zat
Prep,p, ZinPrep,p, .. .
Zsingular
</equation>
<bodyText confidence="0.670102">
noun number singular, plural 2 noun Noun,p , Zplural
Noun,p
punctuation punctuation symbols |candidates |determined by rules Zoriginal
</bodyText>
<equation confidence="0.703403333333333">
Punct,p, Zcand1
Punct,p, Zcand2
Punct,p,. . .
</equation>
<bodyText confidence="0.60693">
spelling correctly spelled |candidates |determined by a Zoriginal
Spell,p , Zcand1
Spell,p, Zcand2
Spell,p,...
words spell checker
</bodyText>
<tableCaption confidence="0.9910295">
Table 1: Error types and corrections. The Applicable column indicates which parts of a sentence are
applicable to an error type. In the first row, ɛ means deleting an article.
</tableCaption>
<equation confidence="0.547113">
Zthe
Art,p = 1, which means there are two corrections
</equation>
<bodyText confidence="0.999565333333333">
a and the for the same sentence position p, but ob-
viously only one article is allowed.
A simple constraint to avoid these conflicts is
</bodyText>
<equation confidence="0.7061555">
∑ Zkl,p = 1, ∀ applicable l,p
k
</equation>
<bodyText confidence="0.999811333333333">
It reads as follows: for each error type l, only one
output k is allowed at any applicable position p
(note that Zkl,p is a Boolean variable).
Putting the variables, objective function, and
constraints together, the ILP problem with respect
to first order variables is as follows:
</bodyText>
<equation confidence="0.998493">
wl,p,kZk (3)
l,p
Zkl,p = 1, ∀ applicable l, p (4)
Zkl,p ∈ {0,1} (5)
</equation>
<bodyText confidence="0.9994525">
The ILP problem is solved using lp solve1, an
integer linear programming solver based on the re-
vised simplex method and the branch-and-bound
method for integers.
</bodyText>
<subsectionHeader confidence="0.99542">
3.4 An Illustrating Example
</subsectionHeader>
<bodyText confidence="0.9863965">
To illustrate the ILP formulation, consider an ex-
ample input sentence s:
A cats sat on the mat . (6)
First, the constraint (4) at position 1 is:
</bodyText>
<equation confidence="0.912646">
Za Art,1 + Zthe
Art,1 + Zɛ Art,1 = 1,
</equation>
<bodyText confidence="0.4348885">
which means only one article in {a, the, ɛ} is se-
lected.
</bodyText>
<footnote confidence="0.687241">
1http://lpsolve.sourceforge.net/
</footnote>
<bodyText confidence="0.9963488">
Next, to compute wl,p,k, we collect language
model score and confidence scores from the arti-
cle (ART), preposition (PREP), and noun number
(NOUN) classifier, i.e., E = {ART, PREP, NOUN}.
The weight for Zsingular
</bodyText>
<equation confidence="0.999004333333333">
Noun,2 is:
wNoun,2,singulu = νLMh(s0, LM)+
λARTf(s0, ART) + λPREPf(s0, PREP) + λNOUNf(s0, NOUN)+
µARTg(s0, ART) + µPREPg(s0, PREP) + µNOUNg(s0, NOUN).
Zsingultu-
Noun,2
</equation>
<bodyText confidence="0.929174666666667">
where s −−−−→ s0 = A cat sat on the mat .
The confidence score f(s0, t) of classifier t is
the average of the confidence scores of t on the
applicable instances in s0.
For example, there are two article instances in
s0, located at position 1 and 5 respectively, hence,
</bodyText>
<equation confidence="0.95341925">
f(s0, ART)= 1(f(s0[1], 1, ART) + f(s0[5], 5, ART))
2
1 (f(a, 1, ART) + f(the, 5, ART)).
= 2
</equation>
<bodyText confidence="0.9973672">
Here, the symbol ft(s0[p], p, ART) refers to the
confidence score of the article classifier at position
p, and s0[p] is the word at position p of s0.
Similarly, the disagreement score g(s0, ART) of
the article classifier is
</bodyText>
<equation confidence="0.980956590909091">
g(s0, ART) = max(g1, g2)
g1=arg max f(k, 1, ART) − f(a, 1, ART)
k
f(k, 5, ART) − f(the, 5, ART)
g2= arg max
k
Putting them together, the weight for Zsingular
Noun,2 is:
wNoun,2,singulu = νLMh(s0, LM)
λART (f(a, 1, ART) + f(the, 5, ART))
+ 2
+ λPREPf(on, 4, PREP)
NOUN
+ (f(cat, 2, NOUN) + f(mat, 6, NOUN))
2
+ µARTg(s0, ART)
+ µPREPg(s0, PREP)
+ µNOUNg(s0, NOUN)
∑max
l,p,k
∑s.t.
k
</equation>
<page confidence="0.944494">
1459
</page>
<table confidence="0.985468923076923">
Input A cats sat on the mat
Corrections The, ɛ cat at, in a, ɛ mats
Za Art,1 Zsingular Zon
Prep,4 Za Art,5 Zsingular
Noun,2 Noun,6
Variables Zthe
Art,1 Zplural
Noun,2 Zat
Prep,4 Zthe
Art,5 Zplural
Noun,6
Zɛ Zin Prep,4 Zɛ
Art,1 Art,5
</table>
<tableCaption confidence="0.999669">
Table 2: The possible corrections on example (6).
</tableCaption>
<subsectionHeader confidence="0.765865">
3.5 Complexity
</subsectionHeader>
<bodyText confidence="0.999882166666667">
The time complexity of ILP is determined by
the number of variables and constraints. Assume
that for each sentence position, at most K classi-
fiers are applicable2. The number of variables is
O(K|s|C(l*)), where l* = argmaxlELC(l). The
number of constraints is O(K|s|).
</bodyText>
<sectionHeader confidence="0.998228" genericHeader="method">
4 Constraints for Prior Knowledge
</sectionHeader>
<subsectionHeader confidence="0.968624">
4.1 Modification Count Constraints
</subsectionHeader>
<bodyText confidence="0.878347583333333">
In practice, we usually have some rough gauge
of the quality of an input sentence. If an input
sentence is mostly grammatical, the system is ex-
pected to make few corrections. This require-
ment can be easily satisfied by adding modifica-
tion count constraints.
In this work, we constrain the number of modifi-
cations according to error types. For the error type
l, a parameter Nl controls the number of modifi-
cations allowed for type l. For example, the mod-
ification count constraint for article corrections is
∑ ZkArt,p ≤ NArt, where k =6 s[p]. (7)
p,k
The condition ensures that the correction k is dif-
ferent from the original word in the input sentence.
Hence, the summation only counts real modifica-
tions. There are similar constraints for preposi-
tion, noun number, and spelling corrections:
∑ Zk Prep,p≤ NPrep, where k =6 s[p], (8)
p,k
∑ ZkNoun,p≤ NNoun, where k =6 s[p], (9)
p,k
∑ ZkSpell,p≤ NSpell, where k =6 s[p]. (10)
p,k
</bodyText>
<footnote confidence="0.654059">
2Tn most cases, K = 1. An example of K &gt; 1 is a noun
that requires changing the word form (between singular and
plural) and inserting an article, for which K = 2.
</footnote>
<subsectionHeader confidence="0.982641">
4.2 Article-Noun Agreement Constraints
</subsectionHeader>
<bodyText confidence="0.992561181818182">
An advantage of the ILP formulation is that it
is relatively easy to incorporate prior linguistic
knowledge. We now take article-noun agreement
as an example to illustrate how to encode such
prior knowledge using linear constraints.
A noun in plural form cannot have a (or an)
as its article. That two Boolean variables Z1 and
Z2 are mutually exclusive can be handled using a
simple inequality Z1 + Z2 ≤ 1. Thus, the fol-
lowing inequality correctly enforces article-noun
agreement:
</bodyText>
<equation confidence="0.9849615">
a plural ( )
ZArt,p1 + ZNoun, - p2 1, 1 1
</equation>
<bodyText confidence="0.999457">
where the article at p1 modifies the noun at p2.
</bodyText>
<subsectionHeader confidence="0.982245">
4.3 Dependency Relation Constraints
</subsectionHeader>
<bodyText confidence="0.999935222222222">
Another set of constraints involves dependency
relations, including subject-verb relation and
determiner-noun relation. Specifically, for a noun
n at position p, we check the word w related to n
via a child-parent or parent-child relation. If w be-
longs to a set of verbs or determiners (are, were,
these, all) that takes a plural noun, then the noun
n is required to be in plural form by adding the
following constraint:
</bodyText>
<equation confidence="0.970459">
Zplural
N oun,p = 1. (12)
</equation>
<bodyText confidence="0.99084">
Similarly, if a noun n at position p is required to
be in singular form due to subject-verb relation
or determiner-noun relation, we add the following
constraint:
</bodyText>
<equation confidence="0.8106685">
Zsingular = 1. (13)
Noun,p
</equation>
<sectionHeader confidence="0.993721" genericHeader="method">
5 Inference with Second Order Variables
</sectionHeader>
<subsectionHeader confidence="0.981306">
5.1 Motivation and Definition
</subsectionHeader>
<bodyText confidence="0.998858">
To relax the decomposable assumption in Section
3.2, instead of treating each correction separately,
one can combine multiple corrections into a single
correction by introducing higher order variables.
</bodyText>
<page confidence="0.97863">
1460
</page>
<bodyText confidence="0.9964185">
Consider the sentence A cat sat on the mat.
When measuring the gain due to Zplural
</bodyText>
<equation confidence="0.969339">
Noun,2 = 1
</equation>
<bodyText confidence="0.97412945">
(change cat to cats), the weight wNoun,2,plural is
likely to be small since A cats will get a low lan-
guage model score, a low article classifier con-
fidence score, and a low noun number classifier
confidence score. Similarly, the weight wArt,1,ɛ of
ZɛArt,1 (delete article A) is also likely to be small
because of the missing article. Thus, if one con-
siders the two corrections separately, they are both
unlikely to appear in the final corrected output.
However, the correction from A cat sat on the
mat. to Cats sat on the mat. should be a reasonable
candidate, especially if the context indicates that
there are many cats (more than one) on the mat.
Due to treating corrections separately, it is difficult
to deal with multiple interacting corrections with
only first order variables.
In order to include the correction ɛ Cats, one
can use a new set of variables, second order vari-
ables. To keep symbols clear, let Z = {Zu|Zu =
Zkl,p, ∀l, p, k} be the set of first order variables, and
</bodyText>
<equation confidence="0.898426">
wu = wl,p,k be the weight of Zu = Zkl,p. Define a
second order variable Xu,v:
Xu,v = Zu ∧ Zv, (14)
</equation>
<bodyText confidence="0.962335">
where Zu and Zv are first order variables:
</bodyText>
<equation confidence="0.9758875">
A k1 o k2 ( )
Zu — Zl1,p1, Zv = Zl2,p2. 15
</equation>
<bodyText confidence="0.999635666666667">
The definition of Xu,v states that a second order
variable is set to 1 if and only if its two compo-
nent first order variables are both set to 1. Thus, it
combines two corrections into a single correction.
In the above example, a second order variable is
introduced:
</bodyText>
<equation confidence="0.98466925">
ɛ plural
Xu,v = ZArt,1 ∧ ZNoun,2,
Xu v /
s −−−→ s = Cats sat on the mat .
</equation>
<bodyText confidence="0.9998292">
Similar to first order variables, let wu,v be the
weight of Xu,v. Note that definition (2) only de-
pends on the output sentence s/, and the weight of
the second order variable wu,v can be defined in
the same way:
</bodyText>
<subsectionHeader confidence="0.988216">
5.2 ILP with Second Order Variables
</subsectionHeader>
<bodyText confidence="0.9992245">
A set of new constraints is needed to enforce con-
sistency between the first and second order vari-
ables. These constraints are the linearization of
definition (14) of Xu,v:
</bodyText>
<equation confidence="0.9986825">
Xu,v ≤ Zu
Xu,v = Zu ∧ Zv ⇔ Xu,v ≤ Zv
Xu,v ≥ Zu + Zv − 1
(17)
</equation>
<bodyText confidence="0.9991565">
A new objective function combines the weights
from both first and second order variables:
</bodyText>
<equation confidence="0.9943415">
∑max ∑wl,p,kZkl,p + wu,vXu,v. (18)
l,p,k u,v
</equation>
<bodyText confidence="0.99967175">
In our experiments, due to noisy data, some
weights of second order variables are small, even
if both of its first order variables have large
weights and satisfy all prior knowledge con-
straints. They will affect ILP proposing good cor-
rections. We find that the performance will be bet-
ter if we change the weights of second order vari-
ables to w/u,v, where
</bodyText>
<equation confidence="0.932843">
w/u,v °= max{wu,v, wu, wv}. (19)
</equation>
<bodyText confidence="0.99902075">
Putting them together, (20)-(25) is an ILP for-
mulation using second order variables, where X is
the set of all second order variables which will be
explained in the next subsection.
</bodyText>
<equation confidence="0.996691">
Xu,v ≤ Zu, (22)
Xu,v ≤ Zv, (23)
Xu,v ≥ Zu + Zv − 1, ∀Xu,v ∈ X (24)
Xu,v, Zkl,p ∈ {0,1} (25)
</equation>
<subsectionHeader confidence="0.957334">
5.3 Complexity and Variable Selection
</subsectionHeader>
<bodyText confidence="0.9950799">
Using the notation in section 3.5, the num-
ber of second order variables is O(|Z|2) =
O(K2|s|2C(l*)2) and the number of constraints is
O(K2|s|2C(l*)2). More generally, for variables
with higher order h ≥ 2, the number of variables
(and constraints) is O(Kh|s|hC(l*)h).
Note that both the number of variables and the
number of constraints increase exponentially with
increasing variable order. In practice, a small
subset of second order variables is sufficient to
</bodyText>
<equation confidence="0.577601">
∑wu,v = νLMh(s/, LM) + λtf(s/, t)
tEE
∑ µtg(s/, t). (16)
</equation>
<page confidence="0.410714">
+
tEE
</page>
<figure confidence="0.990145333333333">
∑max ∑wl,p,kZkl,p + /( )
l,p,k u,v w
u vXu v
20
∑s.t. Zkl,p = 1, ∀ applicable l, p (21)
k
</figure>
<page confidence="0.957643">
1461
</page>
<table confidence="0.984703">
Data set Sentences Words Edits
Dev set 939 22,808 1,264
Test set 722 18,790 1,057
</table>
<tableCaption confidence="0.816744666666667">
Table 3: Overview of the HOO 2011 data sets.
Corrections are called edits in the HOO 2011
shared task.
</tableCaption>
<bodyText confidence="0.959718230769231">
achieve good performance. For example, noun
number corrections are only coupled with nearby
article corrections, and have no connection with
distant or other types of corrections.
In this work, we only introduce second or-
der variables that combine article corrections and
noun number corrections. Furthermore, we re-
quire that the article and the noun be in the same
noun phrase. The set X of second order variables
in Equation (24) is defined as follows:
X ={Xu,v = Zu ∧ Zv|l1 = Art, l2 = Noun,
s[p1], s[p2] are in the same noun phrase},
where l1, l2,p1,p2 are taken from Equation (15).
</bodyText>
<sectionHeader confidence="0.997896" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999828">
Our experiments mainly focus on two aspects:
how our ILP approach performs compared to other
grammatical error correction systems; and how
the different constraints and the second order vari-
ables affect the ILP performance.
</bodyText>
<subsectionHeader confidence="0.999018">
6.1 Evaluation Corpus and Metric
</subsectionHeader>
<bodyText confidence="0.995809222222222">
We follow the evaluation setup in the HOO 2011
shared task on grammatical error correction (Dale
and Kilgarriff, 2011). The development set and
test set in the shared task consist of conference and
workshop papers taken from the Association for
Computational Linguistics (ACL). Table 3 gives
an overview of the data sets.
System performance is measured by precision,
recall, and F measure:
</bodyText>
<equation confidence="0.908903333333333">
_ # true edits = # true edits _ 2PR
P R # system edits, # gold edits, F P + R .
(26)
</equation>
<bodyText confidence="0.998642138888889">
The difficulty lies in how to generate the system
edits from the system output. In the HOO 2011
shared task, participants can submit system edits
directly or the corrected plain-text system output.
In the latter case, the official HOO scorer will ex-
tract system edits based on the original (ungram-
matical) input text and the corrected system output
text, using GNU Wdiff�.
Consider an input sentence The data is simi-
lar with test set. taken from (Dahlmeier and Ng,
2012a). The gold-standard edits are with → to and
c → the. That is, the grammatically correct sen-
tence should be The data is similar to the test set.
Suppose the corrected output of a system to be
evaluated is exactly this perfectly corrected sen-
tence The data is similar to the test set. However,
the official HOO scorer using GNU Wdiff will au-
tomatically extract only one system edit with → to
the for this system output. Since this single system
edit does not match any of the two gold-standard
edits, the HOO scorer returns an F measure of 0,
even though the system output is perfectly correct.
In order to overcome this problem, the Max-
Match (M2) scorer was proposed in (Dahlmeier
and Ng, 2012b). Given a set of gold-standard ed-
its, the original (ungrammatical) input text, and
the corrected system output text, the M2 scorer
searches for the system edits that have the largest
overlap with the gold-standard edits. For the above
example, the system edits automatically deter-
mined by the M2 scorer are identical to the gold-
standard edits, resulting in an F measure of 1 as we
would expect. We will use the M2 scorer in this
paper to determine the best system edits. Once the
system edits are found, P, R, and F are computed
using the standard definition (26).
</bodyText>
<subsectionHeader confidence="0.983322">
6.2 ILP Configuration
6.2.1 Variables
</subsectionHeader>
<bodyText confidence="0.9997436875">
The first order variables are given in Table 1. If
the indefinite article correction a is chosen, then
the final choice between a and an is decided by a
rule-based post-processing step. For each prepo-
sition error variable Zk ����,p, the correction k is re-
stricted to a pre-defined confusion set of prepo-
sitions which depends on the observed preposi-
tion at position p. For example, the confusion
set of on is { at, for, in, of }. The list of prepo-
sitions corrected by our system is about, among,
at, by, for, in, into, of, on, over, to, under, with,
and within. Only selected positions in a sentence
(determined by rules) undergo punctuation correc-
tion. The spelling correction candidates are given
by a spell checker. We used GNU Aspell4 in our
work.
</bodyText>
<footnote confidence="0.999933">
3http://www.gnu.org/software/wdiff/
4http://aspell.net
</footnote>
<page confidence="0.976911">
1462
</page>
<subsectionHeader confidence="0.383466">
6.2.2 Weights
</subsectionHeader>
<bodyText confidence="0.999956285714286">
As described in Section 3.2, the weight of each
variable is a linear combination of the language
model score, three classifier confidence scores,
and three classifier disagreement scores. We use
the Web 1T 5-gram corpus (Brants and Franz,
2006) to compute the language model score for
a sentence. Each of the three classifiers (article,
preposition, and noun number) is trained with the
multi-class confidence weighted algorithm (Cram-
mer et al., 2009). The training data consists of all
non-OCR papers in the ACL Anthology5, minus
the documents that overlap with the HOO 2011
data set. The features used for the classifiers fol-
low those in (Dahlmeier and Ng, 2012a), which
include lexical and part-of-speech n-grams, lexi-
cal head words, web-scale n-gram counts, depen-
dency heads and children, etc. Over 5 million
training examples are extracted from the ACL An-
thology for use as training data for the article and
noun number classifiers, and over 1 million train-
ing examples for the preposition classifier.
Finally, the language model score, classifier
confidence scores, and classifier disagreement
scores are normalized to take values in [0, 1],
based on the HOO 2011 development data. We use
the following values for the coefficients: vLM = 1
(language model); At = 1 (classifier confidence);
and µt = −1 (classifier disagreement).
</bodyText>
<subsubsectionHeader confidence="0.610509">
6.2.3 Constraints
</subsubsectionHeader>
<bodyText confidence="0.999706833333333">
In Section 4, three sets of constraints are in-
troduced: modification count (MC), article-noun
agreement (ANA), and dependency relation (DR)
constraints. The values for the modification count
parameters are set as follows: NA, = 3, NP,.ep =
2, NNoun = 2, and NSpell = 1.
</bodyText>
<subsectionHeader confidence="0.991816">
6.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999964636363636">
We compare our ILP approach with two other sys-
tems: the beam search decoder of (Dahlmeier and
Ng, 2012a) which achieves the best published per-
formance to date on the HOO 2011 data set, and
UI Run1 (Rozovskaya et al., 2011) which achieves
the best performance among all participating sys-
tems at the HOO 2011 shared task. The results are
given in Table 4.
The HOO 2011 shared task provides two sets of
gold-standard edits: the original gold-standard ed-
its produced by the annotator, and the official gold-
</bodyText>
<footnote confidence="0.965387">
5http://aclweb.org/anthology-new/
</footnote>
<table confidence="0.999493">
System Original F P Official F
P R R
UT Run1 40.86 11.21 17.59 54.61 14.57 23.00
Beam search 30.28 19.17 23.48 33.59 20.53 25.48
ILP 20.54 27.93 23.67 21.99 29.04 25.03
</table>
<tableCaption confidence="0.946856">
Table 4: Comparison of three grammatical error
correction systems.
</tableCaption>
<bodyText confidence="0.985323952380953">
standard edits which incorporated corrections pro-
posed by the HOO 2011 shared task participants.
All three systems listed in Table 4 use the M2
scorer to extract system edits. The results of the
beam search decoder and UI Run1 are taken from
Table 2 of (Dahlmeier and Ng, 2012a).
Overall, ILP inference outperforms UI Run1 on
both the original and official gold-standard edits,
and the improvements are statistically significant
at the level of significance 0.01. The performance
of ILP inference is also competitive with the beam
search decoder. The results indicate that a gram-
matical error correction system benefits from cor-
rections made at a whole sentence level, and that
joint correction of multiple error types achieves
state-of-the-art performance.
Table 5 provides the comparison of the beam
search decoder and ILP inference in detail. The
main difference between the two is that, except for
spelling errors, ILP inference gives higher recall
than the beam search decoder, while its precision
is lower. This indicates that ILP inference is more
aggressive in proposing corrections.
Next, we evaluate ILP inference in different
configurations. We only focus on article and noun
number error types. Table 6 shows the perfor-
mance of ILP in different configurations. From
the results, MC and DR constraints improve pre-
cision, indicating that the two constraints can help
to restrict the number of erroneous corrections. In-
cluding second order variables gives the best F
measure, which supports our motivation for intro-
ducing higher order variables.
Adding article-noun agreement constraints
(ANA) slightly decreases performance. By exam-
ining the output, we find that although the overall
performance worsens slightly, the agreement re-
quirement is satisfied. For example, for the input
We utilize search engine to ..., the output without
ANA is We utilize a search engines to ... but with
ANA is We utilize the search engines to ..., while
the only gold edit inserts a.
</bodyText>
<page confidence="0.812359">
1463
</page>
<table confidence="0.982634428571429">
Original
Error type Beam search ILP
P R F P R
Spelling 36.84 0.69 1.35 60.00 0.59
+ Article 19.84 12.59 15.40 18.54 14.75
+ Preposition 22.62 14.26 17.49 17.61 18.58
+ Punctuation 24.27 18.09 20.73 20.52 23.50
+ Noun number 30.28 19.17 23.48 20.54 27.93
Official
Beam search ILP
F P R F P R F
36.84 0.66 1.30 60.00 0.57 1.12
22.45 13.72 17.03 20.37 15.61 17.68
24.84 15.14 18.81 19.24 19.68 19.46
27.13 19.58 22.75 22.49 24.98 23.67
33.59 20.53 25.48 21.99 29.04 25.03
1.17
16.43
18.09
21.91
23.67
</table>
<tableCaption confidence="0.99787625">
Table 5: Comparison of the beam search decoder and ILP inference. ILP is equipped with all constraints
(MC, ANA, DR) and default parameters. Second order variables related to article and noun number error
types are also used in the last row.
Table 6: The effects of different constraints and second order variables.
</tableCaption>
<table confidence="0.65588125">
Setting
Official
Original
P R F
P R F
Art+Nn, 1st ord.
+ MC
+ ANA
+ DR
17.19 19.37 18.22
17.87 18.49 18.17
17.78 18.39 18.08
17.95 18.58 18.26
18.59 20.44 19.47
19.23 19.39 19.31
19.04 19.11 19.07
19.23 19.30 19.26
18.75 18.88 18.81
20.04 19.58 19.81
+ 2nd ord.
</table>
<sectionHeader confidence="0.971552" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999970444444444">
In this paper, we model grammatical error correc-
tion as a joint inference problem. The inference
problem is solved using integer linear program-
ming. We provide three sets of constraints to in-
corporate additional linguistic knowledge, and in-
troduce a further extension with second order vari-
ables. Experiments on the HOO 2011 shared task
show that ILP inference achieves state-of-the-art
performance on grammatical error correction.
</bodyText>
<sectionHeader confidence="0.99886" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9994825">
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998903243243243">
Francis Bond and Satoru Ikehara. 1996. When and
how to disambiguate? countability in machine trans-
lation. In Proceedings of the International Seminar
on Multimodal Interactive Disambiguation.
Francis Bond, Kentaro Ogura, and Tsukasa Kawaoka.
1995. Noun phrase reference in Japanese-to-English
machine translation. In Proceedings of the 6th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram corpus version 1.1. Technical report, Google
Research.
Koby Crammer, Mark Dredze, and Alex Kulesza.
2009. Multi-class confidence weighted algorithms.
In Proceedings of EMNLP.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical error correction with alternating structure opti-
mization. In Proceedings of ACL.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
Proceedings of EMNLP.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In Pro-
ceedings of NAACL.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the Seventh Workshop on Innovative Use of
NLP for Building Educational Applications, pages
54–62.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners&apos; writing. In Proceedings of
NAACL.
</reference>
<page confidence="0.997382">
1464
</page>
<bodyText confidence="0.999314">
Michael Gamon. 2011. High-order sequence model-
ing for language learner error detection. In Proceed-
ings of the Sixth Workshop on Innovative Use ofNLP
for Building Educational Applications.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of ACL.
</bodyText>
<reference confidence="0.999087591836735">
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2).
Julia Heine. 1998. Definiteness predictions for
Japanese noun phrases. In Proceedings of ACL-
COLING.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proceedings of AAAI.
Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun
Stiller, and Ming Zhou. 2010. SRL-based verb se-
lection for ESL. In Proceedings of EMNLP.
Andre Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of ACL-
IJCNLP.
Masaki Murata and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in
Japanese sentences for machine translation into En-
glish. In Proceedings of the 5th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation.
Y. Albert Park and Roger Levy. 2011. Automated
whole sentence grammar correction using a noisy
channel model. In Proceedings of ACL.
Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav
Zimak. 2005. Learning and inference over con-
strained output. In Proceedings of IJCAI.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective de-
pendency parsing. In Proceedings of EMNLP.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of EMNLP.
Alla Rozovskaya and Dan Roth. 2011. Algorithm
selection and model adaptation for ESL correction
tasks. In Proceedings of ACL.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois system in
HOO text correction shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and aspect error correction for
ESL learners using global context. In Proceedings
of ACL.
Joel R. Tetreault and Martin Chodorow. 2008. The
ups and downs of preposition error detection in ESL
writing. In Proceedings of COLING.
</reference>
<page confidence="0.992986">
1465
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.650347">
<title confidence="0.999612">Grammatical Error Correction Using Integer Linear Programming</title>
<author confidence="0.957217">Yuanbin</author>
<affiliation confidence="0.911572666666667">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.939905">Singapore</address>
<email confidence="0.99559">wuyb@comp.nus.edu.sg</email>
<abstract confidence="0.998689266666667">We propose a joint inference algorithm for grammatical error correction. Different from most previous work where different error types are corrected independently, our proposed inference process considers all possible errors in a unified framework. We use integer linear programming (ILP) to model the inference process, which can easily incorporate both the power of existing error classifiers and prior knowledge on grammatical error correction. Experimental results on the Helping Our Own shared task show that our method is competitive with state-of-the-art systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Satoru Ikehara</author>
</authors>
<title>When and how to disambiguate? countability in machine translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Seminar on Multimodal Interactive Disambiguation.</booktitle>
<contexts>
<context position="5568" citStr="Bond and Ikehara, 1996" startWordPosition="864" endWordPosition="867">erimental results show that the ILP formulation is competitive with stateof-the-art grammatical error correction systems. The remainder of this paper is organized as follows. Section 2 gives the related work. Section 3 introduces a basic ILP formulation. Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively. Section 6 presents the experimental results. Section 7 concludes the paper. 2 Related Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classification problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classification include surrounding words, part-of-spee</context>
</contexts>
<marker>Bond, Ikehara, 1996</marker>
<rawString>Francis Bond and Satoru Ikehara. 1996. When and how to disambiguate? countability in machine translation. In Proceedings of the International Seminar on Multimodal Interactive Disambiguation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Kentaro Ogura</author>
<author>Tsukasa Kawaoka</author>
</authors>
<title>Noun phrase reference in Japanese-to-English machine translation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th International Conference on Theoretical and Methodological Issues in Machine Translation.</booktitle>
<contexts>
<context position="5544" citStr="Bond et al., 1995" startWordPosition="860" endWordPosition="863">garriff, 2011). Experimental results show that the ILP formulation is competitive with stateof-the-art grammatical error correction systems. The remainder of this paper is organized as follows. Section 2 gives the related work. Section 3 introduces a basic ILP formulation. Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively. Section 6 presents the experimental results. Section 7 concludes the paper. 2 Related Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classification problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classification include surroun</context>
</contexts>
<marker>Bond, Ogura, Kawaoka, 1995</marker>
<rawString>Francis Bond, Kentaro Ogura, and Tsukasa Kawaoka. 1995. Noun phrase reference in Japanese-to-English machine translation. In Proceedings of the 6th International Conference on Theoretical and Methodological Issues in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram corpus version 1.1.</title>
<date>2006</date>
<tech>Technical report, Google Research.</tech>
<contexts>
<context position="26906" citStr="Brants and Franz, 2006" startWordPosition="4562" endWordPosition="4565">sitions corrected by our system is about, among, at, by, for, in, into, of, on, over, to, under, with, and within. Only selected positions in a sentence (determined by rules) undergo punctuation correction. The spelling correction candidates are given by a spell checker. We used GNU Aspell4 in our work. 3http://www.gnu.org/software/wdiff/ 4http://aspell.net 1462 6.2.2 Weights As described in Section 3.2, the weight of each variable is a linear combination of the language model score, three classifier confidence scores, and three classifier disagreement scores. We use the Web 1T 5-gram corpus (Brants and Franz, 2006) to compute the language model score for a sentence. Each of the three classifiers (article, preposition, and noun number) is trained with the multi-class confidence weighted algorithm (Crammer et al., 2009). The training data consists of all non-OCR papers in the ACL Anthology5, minus the documents that overlap with the HOO 2011 data set. The features used for the classifiers follow those in (Dahlmeier and Ng, 2012a), which include lexical and part-of-speech n-grams, lexical head words, web-scale n-gram counts, dependency heads and children, etc. Over 5 million training examples are extracted</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram corpus version 1.1. Technical report, Google Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Mark Dredze</author>
<author>Alex Kulesza</author>
</authors>
<title>Multi-class confidence weighted algorithms.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="27113" citStr="Crammer et al., 2009" startWordPosition="4593" endWordPosition="4597">e spelling correction candidates are given by a spell checker. We used GNU Aspell4 in our work. 3http://www.gnu.org/software/wdiff/ 4http://aspell.net 1462 6.2.2 Weights As described in Section 3.2, the weight of each variable is a linear combination of the language model score, three classifier confidence scores, and three classifier disagreement scores. We use the Web 1T 5-gram corpus (Brants and Franz, 2006) to compute the language model score for a sentence. Each of the three classifiers (article, preposition, and noun number) is trained with the multi-class confidence weighted algorithm (Crammer et al., 2009). The training data consists of all non-OCR papers in the ACL Anthology5, minus the documents that overlap with the HOO 2011 data set. The features used for the classifiers follow those in (Dahlmeier and Ng, 2012a), which include lexical and part-of-speech n-grams, lexical head words, web-scale n-gram counts, dependency heads and children, etc. Over 5 million training examples are extracted from the ACL Anthology for use as training data for the article and noun number classifiers, and over 1 million training examples for the preposition classifier. Finally, the language model score, classifie</context>
</contexts>
<marker>Crammer, Dredze, Kulesza, 2009</marker>
<rawString>Koby Crammer, Mark Dredze, and Alex Kulesza. 2009. Multi-class confidence weighted algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Grammatical error correction with alternating structure optimization.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6095" citStr="Dahlmeier and Ng, 2011" startWordPosition="947" endWordPosition="950">ical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classification problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classification include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, naive Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Le</context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammatical error correction with alternating structure optimization. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A beamsearch decoder for grammatical error correction.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6727" citStr="Dahlmeier and Ng, 2012" startWordPosition="1041" endWordPosition="1044"> used in classification include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, naive Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predefined generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to </context>
<context position="24538" citStr="Dahlmeier and Ng, 2012" startWordPosition="4155" endWordPosition="4158">System performance is measured by precision, recall, and F measure: _ # true edits = # true edits _ 2PR P R # system edits, # gold edits, F P + R . (26) The difficulty lies in how to generate the system edits from the system output. In the HOO 2011 shared task, participants can submit system edits directly or the corrected plain-text system output. In the latter case, the official HOO scorer will extract system edits based on the original (ungrammatical) input text and the corrected system output text, using GNU Wdiff�. Consider an input sentence The data is similar with test set. taken from (Dahlmeier and Ng, 2012a). The gold-standard edits are with → to and c → the. That is, the grammatically correct sentence should be The data is similar to the test set. Suppose the corrected output of a system to be evaluated is exactly this perfectly corrected sentence The data is similar to the test set. However, the official HOO scorer using GNU Wdiff will automatically extract only one system edit with → to the for this system output. Since this single system edit does not match any of the two gold-standard edits, the HOO scorer returns an F measure of 0, even though the system output is perfectly correct. In or</context>
<context position="27325" citStr="Dahlmeier and Ng, 2012" startWordPosition="4631" endWordPosition="4634">f each variable is a linear combination of the language model score, three classifier confidence scores, and three classifier disagreement scores. We use the Web 1T 5-gram corpus (Brants and Franz, 2006) to compute the language model score for a sentence. Each of the three classifiers (article, preposition, and noun number) is trained with the multi-class confidence weighted algorithm (Crammer et al., 2009). The training data consists of all non-OCR papers in the ACL Anthology5, minus the documents that overlap with the HOO 2011 data set. The features used for the classifiers follow those in (Dahlmeier and Ng, 2012a), which include lexical and part-of-speech n-grams, lexical head words, web-scale n-gram counts, dependency heads and children, etc. Over 5 million training examples are extracted from the ACL Anthology for use as training data for the article and noun number classifiers, and over 1 million training examples for the preposition classifier. Finally, the language model score, classifier confidence scores, and classifier disagreement scores are normalized to take values in [0, 1], based on the HOO 2011 development data. We use the following values for the coefficients: vLM = 1 (language model);</context>
<context position="29358" citStr="Dahlmeier and Ng, 2012" startWordPosition="4965" endWordPosition="4968">riginal gold-standard edits produced by the annotator, and the official gold5http://aclweb.org/anthology-new/ System Original F P Official F P R R UT Run1 40.86 11.21 17.59 54.61 14.57 23.00 Beam search 30.28 19.17 23.48 33.59 20.53 25.48 ILP 20.54 27.93 23.67 21.99 29.04 25.03 Table 4: Comparison of three grammatical error correction systems. standard edits which incorporated corrections proposed by the HOO 2011 shared task participants. All three systems listed in Table 4 use the M2 scorer to extract system edits. The results of the beam search decoder and UI Run1 are taken from Table 2 of (Dahlmeier and Ng, 2012a). Overall, ILP inference outperforms UI Run1 on both the original and official gold-standard edits, and the improvements are statistically significant at the level of significance 0.01. The performance of ILP inference is also competitive with the beam search decoder. The results indicate that a grammatical error correction system benefits from corrections made at a whole sentence level, and that joint correction of multiple error types achieves state-of-the-art performance. Table 5 provides the comparison of the beam search decoder and ILP inference in detail. The main difference between th</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beamsearch decoder for grammatical error correction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation for grammatical error correction.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="6727" citStr="Dahlmeier and Ng, 2012" startWordPosition="1041" endWordPosition="1044"> used in classification include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, naive Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predefined generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to </context>
<context position="24538" citStr="Dahlmeier and Ng, 2012" startWordPosition="4155" endWordPosition="4158">System performance is measured by precision, recall, and F measure: _ # true edits = # true edits _ 2PR P R # system edits, # gold edits, F P + R . (26) The difficulty lies in how to generate the system edits from the system output. In the HOO 2011 shared task, participants can submit system edits directly or the corrected plain-text system output. In the latter case, the official HOO scorer will extract system edits based on the original (ungrammatical) input text and the corrected system output text, using GNU Wdiff�. Consider an input sentence The data is similar with test set. taken from (Dahlmeier and Ng, 2012a). The gold-standard edits are with → to and c → the. That is, the grammatically correct sentence should be The data is similar to the test set. Suppose the corrected output of a system to be evaluated is exactly this perfectly corrected sentence The data is similar to the test set. However, the official HOO scorer using GNU Wdiff will automatically extract only one system edit with → to the for this system output. Since this single system edit does not match any of the two gold-standard edits, the HOO scorer returns an F measure of 0, even though the system output is perfectly correct. In or</context>
<context position="27325" citStr="Dahlmeier and Ng, 2012" startWordPosition="4631" endWordPosition="4634">f each variable is a linear combination of the language model score, three classifier confidence scores, and three classifier disagreement scores. We use the Web 1T 5-gram corpus (Brants and Franz, 2006) to compute the language model score for a sentence. Each of the three classifiers (article, preposition, and noun number) is trained with the multi-class confidence weighted algorithm (Crammer et al., 2009). The training data consists of all non-OCR papers in the ACL Anthology5, minus the documents that overlap with the HOO 2011 data set. The features used for the classifiers follow those in (Dahlmeier and Ng, 2012a), which include lexical and part-of-speech n-grams, lexical head words, web-scale n-gram counts, dependency heads and children, etc. Over 5 million training examples are extracted from the ACL Anthology for use as training data for the article and noun number classifiers, and over 1 million training examples for the preposition classifier. Finally, the language model score, classifier confidence scores, and classifier disagreement scores are normalized to take values in [0, 1], based on the HOO 2011 development data. We use the following values for the coefficients: vLM = 1 (language model);</context>
<context position="29358" citStr="Dahlmeier and Ng, 2012" startWordPosition="4965" endWordPosition="4968">riginal gold-standard edits produced by the annotator, and the official gold5http://aclweb.org/anthology-new/ System Original F P Official F P R R UT Run1 40.86 11.21 17.59 54.61 14.57 23.00 Beam search 30.28 19.17 23.48 33.59 20.53 25.48 ILP 20.54 27.93 23.67 21.99 29.04 25.03 Table 4: Comparison of three grammatical error correction systems. standard edits which incorporated corrections proposed by the HOO 2011 shared task participants. All three systems listed in Table 4 use the M2 scorer to extract system edits. The results of the beam search decoder and UI Run1 are taken from Table 2 of (Dahlmeier and Ng, 2012a). Overall, ILP inference outperforms UI Run1 on both the original and official gold-standard edits, and the improvements are statistically significant at the level of significance 0.01. The performance of ILP inference is also competitive with the beam search decoder. The results indicate that a grammatical error correction system benefits from corrections made at a whole sentence level, and that joint correction of multiple error types achieves state-of-the-art performance. Table 5 provides the comparison of the beam search decoder and ILP inference in detail. The main difference between th</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better evaluation for grammatical error correction. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Helping Our Own: The HOO 2011 pilot shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="1280" citStr="Dale and Kilgarriff, 2011" startWordPosition="187" endWordPosition="190">rimental results on the Helping Our Own shared task show that our method is competitive with state-of-the-art systems. 1 Introduction Grammatical error correction is an important task of natural language processing (NLP). It has many potential applications and may help millions of people who learn English as a second language (ESL). As a research field, it faces the challenge of processing ungrammatical language, which is different from other NLP tasks. The task has received much attention in recent years, and was the focus of two shared tasks on grammatical error correction in 2011 and 2012 (Dale and Kilgarriff, 2011; Dale et al., 2012). To detect and correct grammatical errors, two different approaches are typically used — knowledge engineering or machine learning. The first relies on handcrafting a set of rules. For example, the superlative adjective best is preceded by the article the. In contrast, the machine learning approach formulates the task as a classification problem based on learning from training data. For example, an article classifier takes a noun phrase (NP) as input and predicts its article using class labels a/an, the, or ɛ (no article). Both approaches have their advantages and disadvan</context>
<context position="4941" citStr="Dale and Kilgarriff, 2011" startWordPosition="767" endWordPosition="770">rrections, the objective function aims to select the best set of corrections, and the constraints help to enforce a valid and grammatical output. Furthermore, ILP not only provides a method to solve the inference problem, but also allows for a natural integration of grammatical constraints into a machine learning approach. We will show that ILP fully utilizes individual error classifiers, while prior knowledge on grammatical error correction can be easily expressed using linear constraints. We evaluate our proposed ILP approach on the test data from the Helping Our Own (HOO) 2011 shared task (Dale and Kilgarriff, 2011). Experimental results show that the ILP formulation is competitive with stateof-the-art grammatical error correction systems. The remainder of this paper is organized as follows. Section 2 gives the related work. Section 3 introduces a basic ILP formulation. Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively. Section 6 presents the experimental results. Section 7 concludes the paper. 2 Related Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1</context>
<context position="23712" citStr="Dale and Kilgarriff, 2011" startWordPosition="4008" endWordPosition="4011"> be in the same noun phrase. The set X of second order variables in Equation (24) is defined as follows: X ={Xu,v = Zu ∧ Zv|l1 = Art, l2 = Noun, s[p1], s[p2] are in the same noun phrase}, where l1, l2,p1,p2 are taken from Equation (15). 6 Experiments Our experiments mainly focus on two aspects: how our ILP approach performs compared to other grammatical error correction systems; and how the different constraints and the second order variables affect the ILP performance. 6.1 Evaluation Corpus and Metric We follow the evaluation setup in the HOO 2011 shared task on grammatical error correction (Dale and Kilgarriff, 2011). The development set and test set in the shared task consist of conference and workshop papers taken from the Association for Computational Linguistics (ACL). Table 3 gives an overview of the data sets. System performance is measured by precision, recall, and F measure: _ # true edits = # true edits _ 2PR P R # system edits, # gold edits, F P + R . (26) The difficulty lies in how to generate the system edits from the system output. In the HOO 2011 shared task, participants can submit system edits directly or the corrected plain-text system output. In the latter case, the official HOO scorer w</context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>Robert Dale and Adam Kilgarriff. 2011. Helping Our Own: The HOO 2011 pilot shared task. In Proceedings of the 13th European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ilya Anisimoff</author>
<author>George Narroway</author>
</authors>
<title>HOO 2012: A report on the preposition and determiner error correction shared task.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>54--62</pages>
<contexts>
<context position="1300" citStr="Dale et al., 2012" startWordPosition="191" endWordPosition="194">ping Our Own shared task show that our method is competitive with state-of-the-art systems. 1 Introduction Grammatical error correction is an important task of natural language processing (NLP). It has many potential applications and may help millions of people who learn English as a second language (ESL). As a research field, it faces the challenge of processing ungrammatical language, which is different from other NLP tasks. The task has received much attention in recent years, and was the focus of two shared tasks on grammatical error correction in 2011 and 2012 (Dale and Kilgarriff, 2011; Dale et al., 2012). To detect and correct grammatical errors, two different approaches are typically used — knowledge engineering or machine learning. The first relies on handcrafting a set of rules. For example, the superlative adjective best is preceded by the article the. In contrast, the machine learning approach formulates the task as a classification problem based on learning from training data. For example, an article classifier takes a noun phrase (NP) as input and predicts its article using class labels a/an, the, or ɛ (no article). Both approaches have their advantages and disadvantages. One can readi</context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>Robert Dale, Ilya Anisimoff, and George Narroway. 2012. HOO 2012: A report on the preposition and determiner error correction shared task. In Proceedings of the Seventh Workshop on Innovative Use of NLP for Building Educational Applications, pages 54–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Using mostly native data to correct errors in learners&apos; writing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="6212" citStr="Gamon, 2010" startWordPosition="964" endWordPosition="965">d by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classification problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classification include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, naive Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors</context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>Michael Gamon. 2010. Using mostly native data to correct errors in learners&apos; writing. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="5623" citStr="Han et al., 2006" startWordPosition="874" endWordPosition="877">ve with stateof-the-art grammatical error correction systems. The remainder of this paper is organized as follows. Section 2 gives the related work. Section 3 introduces a basic ILP formulation. Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively. Section 6 presents the experimental results. Section 7 concludes the paper. 2 Related Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classification problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classification include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Heine</author>
</authors>
<title>Definiteness predictions for Japanese noun phrases.</title>
<date>1998</date>
<booktitle>In Proceedings of ACLCOLING.</booktitle>
<contexts>
<context position="5582" citStr="Heine, 1998" startWordPosition="868" endWordPosition="869">hat the ILP formulation is competitive with stateof-the-art grammatical error correction systems. The remainder of this paper is organized as follows. Section 2 gives the related work. Section 3 introduces a basic ILP formulation. Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively. Section 6 presents the experimental results. Section 7 concludes the paper. 2 Related Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classification problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classification include surrounding words, part-of-speech tags, langu</context>
</contexts>
<marker>Heine, 1998</marker>
<rawString>Julia Heine. 1998. Definiteness predictions for Japanese noun phrases. In Proceedings of ACLCOLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Ishwar Chander</author>
</authors>
<title>Automated postediting of documents.</title>
<date>1994</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="6022" citStr="Knight and Chander, 1994" startWordPosition="935" endWordPosition="938">ted Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classification problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classification include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, naive Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with corr</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Bo Han</author>
<author>Kuan Li</author>
<author>Stephan Hyeonjun Stiller</author>
<author>Ming Zhou</author>
</authors>
<title>SRL-based verb selection for ESL.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="6543" citStr="Liu et al., 2010" startWordPosition="1012" endWordPosition="1015">blem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classification include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, naive Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predefined generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corre</context>
</contexts>
<marker>Liu, Han, Li, Stiller, Zhou, 2010</marker>
<rawString>Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun Stiller, and Ming Zhou. 2010. SRL-based verb selection for ESL. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP.</booktitle>
<contexts>
<context position="7582" citStr="Martins et al., 2009" startWordPosition="1178" endWordPosition="1181">s and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to the original inference problem, while ILP returns an exact solution to an approximate inference problem. Integer linear programming has been successfully applied to many NLP tasks, such as dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), semantic role labeling (Punyakanok et al., 2005), and event extraction (Riedel and McCallum, 2011). 3 Inference with First Order Variables The inference problem for grammatical error correction can be stated as follows: “Given an input sentence, choose a set of corrections which results in the best output sentence.” In this paper, this problem will be expressed and solved by integer linear programming (ILP). To express an NLP task in the framework of ILP requires the following steps: 1. Encode the output space of the NLP task using integer variables; 2. Express the inference objective as a l</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andre Martins, Noah Smith, and Eric Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings of ACLIJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Makoto Nagao</author>
</authors>
<title>Determination of referential property and number of nouns in Japanese sentences for machine translation into English.</title>
<date>1993</date>
<booktitle>In Proceedings of the 5th International Conference on Theoretical and Methodological Issues in Machine Translation.</booktitle>
<contexts>
<context position="5525" citStr="Murata and Nagao, 1993" startWordPosition="856" endWordPosition="859">hared task (Dale and Kilgarriff, 2011). Experimental results show that the ILP formulation is competitive with stateof-the-art grammatical error correction systems. The remainder of this paper is organized as follows. Section 2 gives the related work. Section 3 introduces a basic ILP formulation. Sections 4 and 5 improve the basic ILP formulation with more constraints and second order variables, respectively. Section 6 presents the experimental results. Section 7 concludes the paper. 2 Related Work The knowledge engineering approach has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classification problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classificat</context>
</contexts>
<marker>Murata, Nagao, 1993</marker>
<rawString>Masaki Murata and Makoto Nagao. 1993. Determination of referential property and number of nouns in Japanese sentences for machine translation into English. In Proceedings of the 5th International Conference on Theoretical and Methodological Issues in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Albert Park</author>
<author>Roger Levy</author>
</authors>
<title>Automated whole sentence grammar correction using a noisy channel model.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6703" citStr="Park and Levy, 2011" startWordPosition="1037" endWordPosition="1040">d Ng, 2011). Features used in classification include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, naive Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predefined generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an </context>
</contexts>
<marker>Park, Levy, 2011</marker>
<rawString>Y. Albert Park and Roger Levy. 2011. Automated whole sentence grammar correction using a noisy channel model. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
<author>Dav Zimak</author>
</authors>
<title>Learning and inference over constrained output.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="7632" citStr="Punyakanok et al., 2005" startWordPosition="1185" endWordPosition="1188">he work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to the original inference problem, while ILP returns an exact solution to an approximate inference problem. Integer linear programming has been successfully applied to many NLP tasks, such as dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), semantic role labeling (Punyakanok et al., 2005), and event extraction (Riedel and McCallum, 2011). 3 Inference with First Order Variables The inference problem for grammatical error correction can be stated as follows: “Given an input sentence, choose a set of corrections which results in the best output sentence.” In this paper, this problem will be expressed and solved by integer linear programming (ILP). To express an NLP task in the framework of ILP requires the following steps: 1. Encode the output space of the NLP task using integer variables; 2. Express the inference objective as a linear objective function; and 3. Introduce problem</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2005</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav Zimak. 2005. Learning and inference over constrained output. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>James Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7559" citStr="Riedel and Clarke, 2006" startWordPosition="1174" endWordPosition="1177">produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to the original inference problem, while ILP returns an exact solution to an approximate inference problem. Integer linear programming has been successfully applied to many NLP tasks, such as dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), semantic role labeling (Punyakanok et al., 2005), and event extraction (Riedel and McCallum, 2011). 3 Inference with First Order Variables The inference problem for grammatical error correction can be stated as follows: “Given an input sentence, choose a set of corrections which results in the best output sentence.” In this paper, this problem will be expressed and solved by integer linear programming (ILP). To express an NLP task in the framework of ILP requires the following steps: 1. Encode the output space of the NLP task using integer variables; 2. Express the inf</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>Sebastian Riedel and James Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Fast and robust joint models for biomedical event extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7682" citStr="Riedel and McCallum, 2011" startWordPosition="1192" endWordPosition="1196"> the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence to arrive at the best corrected output. The difference between their work and our ILP approach is that the beam-search decoder returns an approximate solution to the original inference problem, while ILP returns an exact solution to an approximate inference problem. Integer linear programming has been successfully applied to many NLP tasks, such as dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), semantic role labeling (Punyakanok et al., 2005), and event extraction (Riedel and McCallum, 2011). 3 Inference with First Order Variables The inference problem for grammatical error correction can be stated as follows: “Given an input sentence, choose a set of corrections which results in the best output sentence.” In this paper, this problem will be expressed and solved by integer linear programming (ILP). To express an NLP task in the framework of ILP requires the following steps: 1. Encode the output space of the NLP task using integer variables; 2. Express the inference objective as a linear objective function; and 3. Introduce problem-specific constraints to refine the feasible outpu</context>
</contexts>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011. Fast and robust joint models for biomedical event extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Dan Roth</author>
</authors>
<title>Algorithm selection and model adaptation for ESL correction tasks.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6425" citStr="Rozovskaya and Roth, 2011" startWordPosition="993" endWordPosition="996">ammatical error correction. Previous work in the machine learning approach typically formulates the task as a classification problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classification include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, naive Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predefined generative model produces correct sentences and errors are added through a noise model. The work of (Dahlm</context>
</contexts>
<marker>Rozovskaya, Roth, 2011</marker>
<rawString>Alla Rozovskaya and Dan Roth. 2011. Algorithm selection and model adaptation for ESL correction tasks. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Mark Sammons</author>
<author>Joshua Gioja</author>
<author>Dan Roth</author>
</authors>
<title>University of Illinois system in HOO text correction shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="28532" citStr="Rozovskaya et al., 2011" startWordPosition="4827" endWordPosition="4830">(language model); At = 1 (classifier confidence); and µt = −1 (classifier disagreement). 6.2.3 Constraints In Section 4, three sets of constraints are introduced: modification count (MC), article-noun agreement (ANA), and dependency relation (DR) constraints. The values for the modification count parameters are set as follows: NA, = 3, NP,.ep = 2, NNoun = 2, and NSpell = 1. 6.3 Experimental Results We compare our ILP approach with two other systems: the beam search decoder of (Dahlmeier and Ng, 2012a) which achieves the best published performance to date on the HOO 2011 data set, and UI Run1 (Rozovskaya et al., 2011) which achieves the best performance among all participating systems at the HOO 2011 shared task. The results are given in Table 4. The HOO 2011 shared task provides two sets of gold-standard edits: the original gold-standard edits produced by the annotator, and the official gold5http://aclweb.org/anthology-new/ System Original F P Official F P R R UT Run1 40.86 11.21 17.59 54.61 14.57 23.00 Beam search 30.28 19.17 23.48 33.59 20.53 25.48 ILP 20.54 27.93 23.67 21.99 29.04 25.03 Table 4: Comparison of three grammatical error correction systems. standard edits which incorporated corrections prop</context>
</contexts>
<marker>Rozovskaya, Sammons, Gioja, Roth, 2011</marker>
<rawString>Alla Rozovskaya, Mark Sammons, Joshua Gioja, and Dan Roth. 2011. University of Illinois system in HOO text correction shared task. In Proceedings of the 13th European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshikazu Tajiri</author>
<author>Mamoru Komachi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Tense and aspect error correction for ESL learners using global context.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6565" citStr="Tajiri et al., 2012" startWordPosition="1016" endWordPosition="1019">preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classification include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, naive Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model, where a predefined generative model produces correct sentences and errors are added through a noise model. The work of (Dahlmeier and Ng, 2012a) is probably the closest to our current work. It uses a beamsearch decoder, which iteratively corrects an input sentence </context>
</contexts>
<marker>Tajiri, Komachi, Matsumoto, 2012</marker>
<rawString>Toshikazu Tajiri, Mamoru Komachi, and Yuji Matsumoto. 2012. Tense and aspect error correction for ESL learners using global context. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in ESL writing.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="6070" citStr="Tetreault and Chodorow, 2008" startWordPosition="943" endWordPosition="946">has been used in early grammatical error correction systems (Murata and Nagao, 1993; Bond et al., 1995; Bond and Ikehara, 1996; Heine, 1998). However, as noted by (Han et al., 2006), rules usually have exceptions, and it is hard to utilize corpus statistics in handcrafted rules. As such, the machine learning approach has become the dominant approach in grammatical error correction. Previous work in the machine learning approach typically formulates the task as a classification problem. Article and preposition errors are the two main research topics (Knight and Chander, 1994; Han et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in classification include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al., 2010). Learning algorithms used include maximum entropy (Han et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, naive Bayes (Rozovskaya and Roth, 2011), etc. Besides article and preposition errors, verb form errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Several research efforts have started to deal with correcting different errors in an integrated manner </context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel R. Tetreault and Martin Chodorow. 2008. The ups and downs of preposition error detection in ESL writing. In Proceedings of COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>