<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.970895">
Improving Summaries by Revising Them
</title>
<author confidence="0.732796">
Inderjeet Mani and Barbara Gates and Eric Bloedorn
</author>
<affiliation confidence="0.477041">
The MITRE Corporation
</affiliation>
<address confidence="0.304679">
11493 Sunset Hills Rd.
Reston, VA 22090, USA
</address>
<email confidence="0.916992">
{imani,blgates,bloedorn}@mitre.org
</email>
<sectionHeader confidence="0.992338" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913166666667">
This paper describes a program which revises a
draft text by aggregating together descriptions
of discourse entities, in addition to deleting ex-
traneous information. In contrast to knowledge-
rich sentence aggregation approaches explored
in the past, this approach exploits statistical
parsing and robust coreference detection. In
an evaluation involving revision of topic-related
summaries using informativeness measures from
the TIPSTER SUMMAC evaluation, the results
show gains in informativeness without compro-
mising readability.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999930707692308">
Writing improves with revision. Authors are fa-
miliar with the process of condensing a long pa-
per into a shorter one: this is an iterative pro-
cess, with the results improved over successive
drafts. Professional abstractors carry out sub-
stantial revision and editing of abstracts (Crem-
mins 1996). We therefore expect revision to be
useful in automatic text summarization. Prior
research exploring the use of revision in sum-
marization, e.g., (Gabriel 1988), (Robin 1994),
(McKeown et al. 1995) has focused mainly on
structured data as the input. Here, we exam-
ine the use of revision in summarization of text
input.
First, we review some summarization termi-
nology. In revising draft summaries, these con-
densation operations, as well as stylistic reword-
ing of sentences, play an important role. Sum-
maries can be used to indicate what topics are
addressed in the source text, and thus can be
used to alert the user as to the source con-
tent (the indicative function). Summaries can
also be used to cover the concepts in the source
text to the extent possible given the compres-
sion requirements for the summary (the infor-
motive function). Summaries can be tailored to
a reader&apos;s interests and expertise, yielding topic-
related summaries, or they can be aimed at a
particular - usually broad - readership commu-
nity, as in the case of (so-called) generic sum-
maries. Revision here applies to generic and
topic-related informative summaries, intended
for publishing and dissemination.
Summarization can be viewed as a text-to-
text reduction operation involving three main
condensation operations: selection of salient
portions of the text, aggregation of information
from different portions of the text, and abstrac-
tion of specific information with more general
information (Mani and Maybury 1999). Our
approach to revision is to construct an initial
draft summary of a source text and then to add
to the draft additional background information.
Rather than concatenate material in the draft
(as surface-oriented, sentence extraction sum-
marizers do), information in the draft is com-
bined and excised based on revision rules in-
volving aggregation (Dalianis and Hovy 1996)
and elimination operations. Elimination can
increase the amount of compression (summary
length/source length) available, while aggrega-
tion can potentially gather and draw in relevant
background information, in the form of descrip-
tions of discourse entities from different parts of
the source. We therefore hypothesize that these
operations can result in packing in more infor-
mation per unit compression than possible by
concatenation. Rather than opportunistically
adding as much background information that
can fit in the available compression, as in (Robin
1994), our approach adds background informa-
tion from the source text to the draft based on
an information weighting function.
Our revision approach assumes input sen-
tences are represented as syntactic trees whose
</bodyText>
<page confidence="0.995988">
558
</page>
<bodyText confidence="0.999905130434783">
nodes are annotated with coreference informa-
tion. In order to provide open-domain cover-
age the approach does not assume a meaning-
level representation of each sentence, and so, un-
like many generation systems, the system does
not represent and reason about what is being
said&apos;. Meaning-dependent revision operations
are restricted to situations where it is clear from
coreference that the same entity is being talked
about.
There are several criteria our revision model
needs to satisfy. The final draft needs to be
informative, coherent, and grammatically well-
formed. Informativeness is explored in Sec-
tion 4.2. We can also strive to guarantee, based
on our revision rule set, that each revision will
be syntactically well-formed. Regarding coher-
ence, revision alters rhetorical structure in a
way which can produce disfluencies. As rhetori-
cal structure is hard to extract from the source2,
our program instead uses coreference to guide
the revision, and attempts to patch the coher-
ence by adjusting references in revised drafts.
</bodyText>
<sectionHeader confidence="0.945991" genericHeader="method">
2 The Revision Program
</sectionHeader>
<bodyText confidence="0.976400974358974">
The summary revision program takes as input
a source document, a draft summary specifi-
cation, and a target compression rate. Using
revision rules, it generates a revised summary
draft whose compression rate is no more than S
above the target compression rate. The initial
draft summary (and background) are specified
in terms of a task-dependent weighting function
which indicates the relative importance of each
of the source document sentences. The program
repeatedly selects the highest weighted sentence
from the source and adds it to the initial draft
until the given compression percentage of the
source has been extracted, rounded to the near-
est sentence. Next, for each rule in the sequence
of revision rules, the program repeatedly applies
the rule until it can no longer be applied. Each
rule application results in a revised draft. The
program selects sentences for rule application by
giving preference to higher weighted sentences.
&apos;Note that professional abstractors do not attempt to
fully &amp;quot;understand&amp;quot; the text - often extremely technical
material, but use surface-level features as above as well
as the overall discourse structure of the text (Cremmins
1996).
21-lowever, recent progress on this problem (Marcu
1997) is encouraging.
A unary rule applies to a single sentence. A bi-
nary rule applies to a pair of sentences, at least
one of which must be in the draft, and where the
first sentence precedes the second in the input.
Control over sentence complexity is imposed by
failing rule application when the draft sentence
is too long, the parse tree is too deep3, or if more
than two relative clauses would be stacked to-
gether. The program terminates when there are
no more rules to apply or when the revised draft
exceeds the required compression rate by more
than 5.
The syntactic structure of each source sen-
tence is extracted using Apple Pie 7.2 (Sekine
1998), a statistical parser trained on Penn Tree-
bank data. It was evaluated by (Sekine 1998)
as having 79% F-score accuracy (parseval) on
short sentences (less than 40 words) from the
Treebank. An informal assessment we made of
the accuracy of the parser (based on intuitive
judgments) on our own data sets of news ar-
ticles suggests about 66% of the parses were
acceptable, with almost half of the remain-
ing parsing errors being due to part-of-speech
tagging errors, many of which could be fixed
by preprocessing the text. To establish coref-
erence between proper names, named entities
are extracted from the document, along with
coreference relations using SRA&apos;s NameTag 2.0
(Krupka 1995), a MUC-6 fielded system. In ad-
dition, we implemented our own coreference ex-
tension: A singular definite NP (e.g., beginning
with &amp;quot;the&amp;quot;, and not marked as a proper name)
is marked by our program as coreferential (i.e.,
in the same coreference equivalence class) with
the last singular definite or singular indefinite
atomic NP with the same head, provided they
are within a distance -y of each other. On a cor-
pus of 90 documents, drawn from the TIPSTER
evaluation, described in Section 4.1 below, this
coreference extension scored 94% precision (470
valid coreference classes/501 total coreference
classes) on definite NP coreference. Also, &amp;quot;he&amp;quot;
(likewise &amp;quot;she&amp;quot;) is marked, subject to 7, as
coreferential with the last person name men-
tioned, with gender agreement enforced when
the person&apos;s first name&apos;s gender is known (from
NameTag&apos;s list of common first names)4. Most
3Lengths or depths greater than two standard devia-
tions beyond the mean are treated as too long or deep.
4However, this very naive method was excluded from
</bodyText>
<page confidence="0.944325">
559
</page>
<equation confidence="0.988978157894737">
rule-name: rel-clause-intro-which-1
patterns:
?X1 ; # first sentence pattern
?Y1 ?Y2 ?Y3 # second sentence pattern
tests:
label-NP ?X1 ; not entity-class ?X1 person ;
label-S ?Y1 ;
root ?Y1 ;
label-NP ?Y2 ;
label-VP ?Y3 ;
adjacent-sibling ?Y2 ?Y3;
parent-child ?Y1 ?Y2;
parent-child ?Y1 ?Y3;
coref ?X1 ?Y2
actions:
subs ?X1 (NP ?X1 (,-COMMA-)
(SBAR (WHNP (WP which))
(S ?Y3)) (, -COMMA-)) ;
elim-root-of ?Y1 # removes second sentence
</equation>
<figureCaption confidence="0.581844333333333">
Figure 2: Relative Clause Introduction Rule
showing Aggregation and Elimination opera-
tions.
</figureCaption>
<bodyText confidence="0.9999045">
of the errors were caused by different sequences
of words between the determiner and the noun
phrase head word (e.g., &amp;quot;the factory&amp;quot; = &amp;quot;the
cramped five-story pre-1915 factory&amp;quot; is OK, but
&amp;quot;the virus program&amp;quot;= &amp;quot;the graduate computer
science program&amp;quot; isn&apos;t).
</bodyText>
<sectionHeader confidence="0.992384" genericHeader="method">
3 Revision Rules
</sectionHeader>
<bodyText confidence="0.99991185">
The revision rules carry out three types of op-
erations. Elimination operations eliminate con-
stituents from a sentence. These include elim-
ination of parentheticals, and sentence-initial
PPs and adverbial phrases satisfying lexical
tests (such as &amp;quot;In particular,&amp;quot;, &amp;quot;Accordingly,&amp;quot;
&amp;quot;In conclusion,&amp;quot; etc.)5.
Aggregation operations combine constituents
from two sentences, at least one of which must
be a sentence in the draft, into a new con-
stituent which is inserted into the draft sen-
tence. The basis for combining sentences is that
of referential identity: if there is an NP in sen-
tence i which is coreferential with an NP in
sentence j, then sentences i and j are candi-
dates for aggregation. The most common form
of aggregation is expressed as tree-adjunction
(Joshi 1998) (Dras 1999). Figures 1 and 2
show a relative clause introduction rule which
turns a VP of a (non-embedded) sentence whose
</bodyText>
<footnote confidence="0.743855333333333">
our analysis because of a system bug.
5Such lexical tests help avoid misrepresenting the
meaning of the sentence.
</footnote>
<bodyText confidence="0.999898130434783">
subject is coreferential with an NP of an ear-
lier (draft) sentence into a relative clause mod-
ifier of the draft sentence NP. Other appositive
phrase insertion rules include copying and in-
serting nonrestrictive relative clause modifiers
(e.g., &amp;quot;Smith, who...,&amp;quot;), appositive modifiers of
proper names (e.g., &amp;quot;Peter G. Neumann, a com-
puter security expert familiar with the case,...&amp;quot;),
and proper name appositive modifiers of definite
NPs (e.g., &amp;quot;The network, named ARPANET, is
operated by ..&amp;quot; ).
Smoothing operations apply to a single sen-
tence, performing transformations so as to ar-
rive at more compact, stylistically preferred sen-
tences. There are two types of smoothing.
Reduction operations simplify coordinated con-
stituents. Ellipsis rules include subject ellipsis,
which lowers the coordination from a pair of
clauses with coreferential subjects to their VPs
(e.g., &amp;quot;The rogue computer program destroyed
files over a five month period and the program
infected close to 100 computers at NASA fa-
cilities&amp;quot; == &amp;quot;The rogue computer program de-
stroyed files over a five month period and in-
fected close to 100 computers at NASA facil-
ities&amp;quot;). It usually applies to the result of an
aggregation rule which conjoins clauses whose
subjects are coreferential. Relative clause re-
duction includes rules which apply to clauses
whose VPs begin with &amp;quot;be&amp;quot; (e.g., &amp;quot;which is&amp;quot;
is deleted) or &amp;quot;have&amp;quot; (e.g., &amp;quot;which have&amp;quot;
&amp;quot;with&amp;quot;), as well as for other verbs, a rule delet-
ing the relative pronoun and replacing the verb
with its present participle (i.e., &amp;quot;which V&amp;quot;
&amp;quot;V-Fing&amp;quot;). Coordination rules include relative
clause coordination. Reference Adjustment op-
erations fix up the results of other revision oper-
ations in order to improve discourse-level coher-
ence, and as a result, they are run lase. They
include substitution of a proper name with a
name alias if the name is mentioned earlier, ex-
pansion of a pronoun with a coreferential proper
name in a parenthetical (&amp;quot;pronoun expansion&amp;quot;),
and (&amp;quot;indefinitization&amp;quot;) replacement of a def-
inite NP with a coreferential indefinite if the
definite occurs without a prior indefinite.
</bodyText>
<footnote confidence="0.8587465">
&apos;Such operations have been investigated earlier by
(Robin 1994).
</footnote>
<page confidence="0.948342">
560
</page>
<figure confidence="0.698906333333333">
Draft sentence Other sentence
S al
/\ /\
NP VP NPI VP1
(=NP)
Result
sentence
S
WHIVP vp1
</figure>
<figureCaption confidence="0.998769">
Figure 1: Relative Clause Introduction showing tree NP2 being adjoined into tree S
</figureCaption>
<sectionHeader confidence="0.997356" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999952642857143">
Evaluation of text summarization and other
such NLP technologies where there may be
many acceptable outputs, is a difficult task. Re-
cently, the U.S. government conducted a large-
scale evaluation of summarization systems as
part of its TIPSTER text processing program
(Mani et al. 1999), which included both an
extrinsic (relevance assessment) evaluation, as
well as an intrinsic (coverage of key ideas)
evaluation. The test set used in the latter
(Q&amp;A) evaluation along with several automat-
ically scored measures of informativeness has
been reused in evaluating the informativeness
of our revision component.
</bodyText>
<subsectionHeader confidence="0.6116935">
4.1 Background: TIPSTER Q&amp;A
Evaluation
</subsectionHeader>
<bodyText confidence="0.999990432432433">
In this Q&amp;A evaluation, the summarization sys-
tem, given a document and a topic, needed to
produce an informative, topic-related summary
that contained the correct answers found in that
document to a set of topic-related questions.
These questions covered &amp;quot;obligatory&amp;quot; informa-
tion that has to be provided in any document
judged relevant to the topic. The topics chosen
(3 in all) were drawn from the TREC (Harman
and Voorhees 1996) data sets. For each topic,
30 relevant TREC documents were chosen as
the source texts for topic-related summariza-
tion. The principal tasks of each Q&amp;A evaluator
were to prepare the questions and answer keys
and to score the system summaries. To con-
struct the answer key, each evaluator marked
off any passages in the text that provided an an-
swer to a question (example shown in Table 1).
Two kinds of scoring were carried out. In
the first, a manual method, the answer to each
question was judged Correct, Partially Correct,
or Missing based on guidelines involving a hu-
man comparison of the summary of a docu-
ment against the set of tagged passages for that
question in the answer key for that document.
The second method of scoring was an automatic
method. This program&apos; took as input a key file
and a summary to be scored, and returns an
informativeness score on four different metrics.
The key file includes tags identifying passages
in the file which answer certain questions. The
scoring uses the overlap measures shown in Ta-
ble 28. The automatically computed V4 thru
V7 informativeness scores were strongly corre-
lated with the human-evaluated scores (Pearson
r&gt; .97, ce &lt; 0.0001). Given this correlation, we
decided to use these informativeness measures.
</bodyText>
<subsectionHeader confidence="0.936818">
4.2 Revision Evaluation:
Informativeness
</subsectionHeader>
<bodyText confidence="0.999933">
To evaluate the revised summaries, we first con-
verted each summary into a weighting function
which scored each full-text sentence in the sum-
mary&apos;s source in terms of its similarity to the
most similar summary sentence. The weight
of a source document sentence s given a sum-
</bodyText>
<footnote confidence="0.998535">
7The program was reimplemented by us for use in the
revision evaluation.
8Passage matching here involves a sequential match
with stop words and punctuation removed.
</footnote>
<page confidence="0.989643">
561
</page>
<table confidence="0.415849789473684">
Title: Computer Security
Description : Identify instances of illegal entry into sensitive
computer networks by nonauthorized personnel.
Narrative : Illegal entry into sensitive computer networks
is a serious and potentially menacing problem. Both &apos;hackers&apos; and
foreign agents have been known to acquire unauthorized entry into
various networks. Items relative this subject would include but not
be limited to instances of illegally entering networks containing
information of a sensitive nature to specific countries, such as
defense or technology information, international banking, etc. Items
of a personal nature (e.g. credit card fraud, changing of college
test scores) should not be considered relevant.
Questions
1)Who is the known or suspected hacker accessing a sensitive computer or computer network?
2) How is the hacking accomplished or putatively achieved?
3) Who is the apparent target of the hacker?
4) What did the hacker accomplish once the violation occurred?
What was the purpose in performing the violation?
5) What is the time period over which the breakins were occurring?
</table>
<tableCaption confidence="0.725896888888889">
As a federal grand jury decides whether he should be prosecuted, &lt;Q1&gt;a graduate
student&lt;/Q1&gt; linked to a &amp;quot;virus&amp;quot; that disrupted computers nationwide &lt;Q5&gt;last
month&lt;/Q5&gt;has been teaching his lawyer about the technical subject and turning down
offers for his life story. ....No charges have been filed against &lt;Q1&gt;Morris&lt;/Q1&gt;,
who reportedly told friends that he designed the virus that temporarily clogged about
&lt;Q3&gt;6,000 university and military computers&lt;/Q3&gt; &lt;Q2&gt;linked to the Pentagon&apos;s Arpanet
network&lt;/Q2&gt;.
Table 1: Q&amp;A Topic 258, topic-related questions, and part of a relevant source document showing
answer key annotations.
</tableCaption>
<table confidence="0.999684545454545">
Overlap Metric Definition
V4 full credit if the text spans for all tagged key passages
are found in their entirety in the summary
V5 full credit if the text spans for all tagged key passages
are found in their entirety in the summary;
half credit if the text spans for all tagged key passages
are found in some combination of full or truncated form in the summary
V6 full credit if the text spans for all tagged key passages
are found in some combination of full or truncated form in the summary
V7 percentage of credit assigned that is commensurate with the extent to which
the text spans for tagged key passages are present in the summary
</table>
<tableCaption confidence="0.9971135">
Table 2: Informativeness measures for Automatic Scoring of each question that has an answer
according to the key.
</tableCaption>
<table confidence="0.999782">
Party FOG Kincaid
Before After Before After
CGI/CMU 16.49 15.50 13.22 12.23
Cornell/SabIR 15.51 15.08 12.15 11.71
GE 15.43 15.14 12.13 11.87
ISI 19.57 17.94 16.18 14.51
NMSU 16.54 15.52 13.32 12.30
SRA 15.59 15.29 12.26 11.99
UPenn 16.29 16.21 12.93 12.83
Mean 16.48 15.82 13.15 12.51
</table>
<tableCaption confidence="0.960889">
Table 3: Readability of Summaries Before (Original Summary) and After Revision (A+E). Overall,
both FOG and Kincaid scores show a slight but statistically significant drop on revision (a &lt; 0.05).
</tableCaption>
<page confidence="0.967288">
562
</page>
<figure confidence="0.999237333333333">
100% 0 Lose
90% • Maintain
80% Win
70%
60%
50%
40%
30%
20%
10%
0%
E A A+E E A A+E E A A+E E A A+E
</figure>
<figureCaption confidence="0.973909666666667">
Figure 3: Gains in Compression-Normalized Informativeness of revised summaries compared to
initial drafts. E = elimination, A = aggregation. A, E, and A-1-E are shown in the order V4, V5,
V6, and V7.
</figureCaption>
<bodyText confidence="0.6956239375">
&lt;sl&gt; Researchers today tried to trace a &amp;quot;virus&amp;quot; that infected computer systems nationwide,
&lt;Q4&gt; slowing machines in universities, a NASA and nuclear weapons lab and other federal
research centers linked by a Defense Department computer network. &lt;/Q4&gt; &lt;s3&gt;
Authorities said the virus , which &lt;FROM s16&gt; &lt;Q3&gt; the virus infected only unclassified
computers &lt;/Q3&gt; and &lt;FROM s15&gt; &lt;Q3&gt; the virus affected the unclassified,
non-secured computer systems &lt;/Q3&gt; (and which &lt;FROM S19&gt; &lt;Q4&gt; the virus was &amp;quot;mainly just
slowing down systems ) and slowing data &amp;quot;, &lt;/Q4&gt; apparently &lt;Q4&gt; destroyed no data but temporarily
halted some research. &lt;/Q4&gt; &lt;s14&gt;. The computer problem also was discovered late
Wednesday at the &lt;Q3&gt; Lawrence Livermore National Laboratory in Livermore, Calif. &lt;/Q3&gt;
&lt;s15&gt; &lt;s20&gt; &apos;The developer was clearly a very high order hacker,&amp;quot;, &lt;FROM S25&gt; &lt;Q1&gt; a
graduate student &lt;/Q1&gt; &lt;Q2&gt; who made making a programming error in designing the
virus,eausing the program to replicate faster than expected &lt;/Q2&gt; or computer buff, said
John McAfee, chairman of the Computer Virus Industry Association in Santa Clara, Calif..
&lt;s24&gt; The Times reported today that the anonymous caller an anonymous caller to
the paper said his associate was responsible for the attack and had meant it to
be harmless.
</bodyText>
<figureCaption confidence="0.9868415">
Figure 4: A revised summary specified in terms of an original draft (plain text) with added (bold-
face) and deleted (italics) spans. Sentence &lt;s&gt; and Answer Key &lt;Q&gt; tags are overlaid.
</figureCaption>
<bodyText confidence="0.9998049">
mary is the match score of s&apos;s best-matching
summary sentence, where the match score is
the percentage of content word occurrences in
s that are also found in the summary sentence.
Thus, we constructed an idealized model of
each summary as a sentence extraction function.
Since some of the participants truncated and
occasionally mangled the source text (in addi-
tion, Penn carried out pronoun expansion), we
wanted to avoid having to parse and apply revi-
sion rules to such relatively ill-formed material.
This idealization is highly appropriate, for each
of the summarizers considered9 did carry out
sentence extraction; in addition, it helps level
the playing field, avoiding penalization of indi-
vidual summarizers simply because we didn&apos;t
cater to the particular form of their summary.
Each summary was revised by calling the re-
vision program with the full-text source, the
original compression rate of the summary, and
</bodyText>
<footnote confidence="0.714008">
9TextWise, which extracted named entities rather
than passages, was excluded.
</footnote>
<page confidence="0.997769">
563
</page>
<bodyText confidence="0.999959096774194">
the summary weighting function (i.e., with the
weight for each source sentence). The 630 re-
vised summaries (3 topics x 30 documents per
topic x 7 participant summaries per document)
were then scored against the answer keys using
the overlap measures above. The documents
consisted of AP, Wall Street Journal, and Fi-
nancial Times news articles from the TREC
(Harman and Voorhees 1996) collection.
The rules used in the system are very gen-
eral, and were not modified for the evaluation
except for turning off most of the reference ad-
justment rules, as we wished to evaluate that
component separately. Since the answer keys
typically do not contain names of commenta-
tors, we wanted to focus the algorithm away
from such names (otherwise, it would aggregate
information around those commentators). As
a result, special rules were written in the revi-
sion rule language to detect commentator names
in reported speech (&amp;quot;X said that ..&amp;quot;, &amp;quot;X said
...&amp;quot; , &amp;quot;, said X..&amp;quot;, &amp;quot;, said X..&amp;quot;, etc.), and these
names were added to a stoplist for use in enti-
tyhood and coreference tests during regular re-
vision rule application.
Figure 3 shows percentage of losses, main-
tains, and wins in informativeness against the
initial draft (i.e., the result of applying the com-
pression to the sentence weighting function).
Informativeness using V7 is measured by V71-°
normalized for compression as:
</bodyText>
<equation confidence="0.926364333333333">
sl
nV7 = V7 * (1 — --)
sO
</equation>
<bodyText confidence="0.998998976190476">
where sl is summary length and sO is the source
length. This initial draft is in itself not as in-
formative as the original summary: in all cases
except for Penn on 257, the initial draft either
maintains or loses informativeness compared to
the original summary.
As Figure 3 reveals (e.g., for nV7), revising
the initial draft using elimination rules only (E)
results in summaries which are less informative
than the initial draft 65% of the time, suggest-
ing that these rules are removing informative
material. Revising the initial draft using aggre-
gation rules alone (A), by contrast, results in
more informative summaries 47% of the time,
and equally informative summaries another 13%
&apos;°V7 computes for each question the percentage of
its answer passages completely covered by the summary.
This normalization is extended similarly for V4 thru V6.
of the time. This is due to aggregation folding in
additional informative material into the initial
draft when it can. Inspection of the output sum-
maries, an example of which is shown in Fig-
ure 4, confirms the folding in behavior of aggre-
gation. Finally, revising the initial draft using
both aggregation and elimination rules (A+E)
does no more than maintain the informative-
ness of the initial draft, suggesting A and E are
canceling each other out. The same trend is ob-
serving for nV4 thru nV6, confirming that the
relative gain in informativeness due to aggrega-
tion is robust across a variety of (closely related)
measures. Of course, if the revised summaries
were instead radically different in wording from
the original drafts, such informativeness mea-
sures would, perhaps, fall short.
It is also worth noting the impact of aggrega-
tion is modulated by the current control strat-
egy; we don&apos;t know what the upper bound is
on how well revision could do given other con-
trol regimes. Overall, then, while the results
are hardly dramatic, they are certainly encour-
aging&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.995882">
4.3 Revision Evaluation: Readability
</subsectionHeader>
<bodyText confidence="0.95566384">
Inspection of the results of revision indicates
that the syntactic well-formedness revision cri-
terion is satisfied to a very great extent. Im-
proper extraction from coordinated NPs is an
issue (see Figure 4), but we expect additional
revision rules to handle such cases. Coher-
ence disfluencies do occur; for example, since we
don&apos;t resolve possessive pronouns or plural def-
inites, we can get infelicitous revisions like &amp;quot;A
computer virus, which entered *their comput-
ers through ARPANET, infected systems from
MIT.&amp;quot; Other limitations in definite NP coref-
erence can and do result in infelicitous refer-
ence adjustments. For one thing, we don&apos;t link
definites to proper name antecedents, result-
ing in inappropriate indefinitization (e.g., &amp;quot;Bill
Gates ... *A computer tycoon&amp;quot;). In addition,
the &amp;quot;same head word&amp;quot; test doesn&apos;t of course ad-
dress inferential relationships between the defi-
nite NP and its antecedent (even when the an-
tecedent is explicitly mentioned), again result-
ing in inappropriate indefinitization (e.g., &amp;quot;The
program ... *a developer, and &amp;quot;The developer
&amp;quot;Similar results hold while using a variety of other
compression normalization metrics.
</bodyText>
<equation confidence="0.543477">
(1)
</equation>
<page confidence="0.991518">
564
</page>
<bodyText confidence="0.999932052631579">
... An anonymous caller said *a very high order
hacker was a graduate student&amp;quot;).
To measure fluency without conducting an
elaborate experiment involving human judg-
ments, we fell back on some extremely coarse
measures based on word and sentence length
computed by the (gnu) unix program style
(Cherry 1981). The FOG index sums the av-
erage sentence length with the percentage of
words over 3 syllables, with a &amp;quot;grade&amp;quot; level over
12 indicating difficulty for the average reader.
The Kincaid index, intended for technical text,
computes a weighted sum of sentence length and
word length. As can be seen from Table 3, there
is a slight but significant lowering of scores on
both metrics, revealing that according to these
metrics revision is not resulting in more com-
plex text. This suggests that elimination rather
than aggregation is mainly responsible for this.
</bodyText>
<sectionHeader confidence="0.997805" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999835">
This paper demonstrates that recent advances
in information extraction and robust parsing
can be exploited effectively in an open-domain
model of revision inspired by work in natural
language generation. In the future, instead of
relying on adjustment rules for coherence, it
may be useful to incorporate a level of text plan-
ning. We also hope to enrich the background
information by merging information from mul-
tiple text and structured data sources.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999663241379311">
Cherry, L.L., and Vesterman, W. Writing Tools:
The STYLE and DICTION programs, Com-
puter Science Technical Report 91, Bell Lab-
oratories, Murray Hill, N.J. (1981).
Cremmins, E. T. 1996. The Art of Abstracting.
Information Resources Press.
Dalianis, H., and Hov, E. 1996. Aggregation in
Natural Language Generation. In Zock, M.,
and Adorni, G., eds., Trends in Natural Lan-
guage Generation: an Artificial Intelligence
Perspective, pp.88-105. Lecture Notes in Ar-
tificial Intelligence, Number 1036, Springer
Verlag, Berlin.
Dras, M. 1999. Tree Adjoining Grammar and
the Reluctant Paraphrasing of Text, Ph.D.
Thesis, Macquarie University, Australia.
Gabriel, R. 1988. Deliberate Writing. In Mc-
Donald, D.D., and Bolc, L., eds., Natu-
ral Language Generation Systems, Springer-
Verlag, NY.
Harman, D.K. and E.M. Voorhees. 1996. The
fifth text retrieval conference (trec-5). Na-
tional Institute of Standards and Technology
NIST SP 500-238.
Joshi, A. K. and Schabes, Y. 1996. &amp;quot;Tree-
Adjoining Grammars&amp;quot;. In Rosenberg, G., and
Salomaa, A., eds., Handbook of Formal Lan-
guages, Vol. 3, 69-123. Springer-Verlag, NY.
Krupka, G. 1995. &amp;quot;SRA: Description of the SRA
System as Used for MUC-6&amp;quot;, Proceedings of
the Sixth Message Understanding Conference
(MUC-6), Columbia, Maryland, November
1995.
Marcu, D. 1997. From discourse structures to
text summaries. in Mani, I. and Maybury,
M., eds., Proceedings of the ACL/EACL&apos;97
Workshop on Intelligent Scalable Text Sum-
marization.
Mani, I. and M. Maybury, eds. 1999. Ad-
vances in Automatic Text Summarization.
MIT Press.
Mani, I., Firmin, T., House, D., Klein, G.,
Hirschman, L., and Sundheim, B. 1999.
&amp;quot;The TIPSTER SUMMAC Text Summariza-
tion Evaluation&amp;quot;, Proceedings of EACL&apos;99,
Bergen, Norway, June 8-12, 1999.
McKeown, K., J. Robin, and K. Kukich. 1995.
Generating Concise Natural Language Sum-
maries. Information Processing and Manage-
ment, 31,5, 703-733.
Robin, J. 1994. Revision-based generation of
natural language summaries providing his-
torical background: corpus-based analysis,
design and implementation. Ph.D. Thesis,
Columbia University.
Sekine, S. 1998. Corpus-based Parsing and Sub-
language Studies. Ph.D. Dissertation, New
York University, 1998.
</reference>
<page confidence="0.998492">
565
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.943241">
<title confidence="0.999563">Improving Summaries by Revising Them</title>
<author confidence="0.998464">Mani Gates Bloedorn</author>
<affiliation confidence="0.99821">The MITRE Corporation</affiliation>
<address confidence="0.999462">11493 Sunset Hills Rd. Reston, VA 22090, USA</address>
<email confidence="0.99599">imani@mitre.org</email>
<email confidence="0.99599">blgates@mitre.org</email>
<email confidence="0.99599">bloedorn@mitre.org</email>
<abstract confidence="0.996221846153846">This paper describes a program which revises a draft text by aggregating together descriptions of discourse entities, in addition to deleting exinformation. contrast knowledgerich sentence aggregation approaches explored in the past, this approach exploits statistical parsing and robust coreference detection. In an evaluation involving revision of topic-related summaries using informativeness measures from the TIPSTER SUMMAC evaluation, the results show gains in informativeness without compromising readability.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L L Cherry</author>
<author>W Vesterman</author>
</authors>
<title>Writing Tools: The STYLE and DICTION programs,</title>
<date>1981</date>
<journal>Computer Science</journal>
<tech>Technical Report 91,</tech>
<location>Bell Laboratories, Murray Hill, N.J.</location>
<marker>Cherry, Vesterman, 1981</marker>
<rawString>Cherry, L.L., and Vesterman, W. Writing Tools: The STYLE and DICTION programs, Computer Science Technical Report 91, Bell Laboratories, Murray Hill, N.J. (1981).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Cremmins</author>
</authors>
<title>The Art of Abstracting. Information</title>
<date>1996</date>
<publisher>Resources Press.</publisher>
<contexts>
<context position="1039" citStr="Cremmins 1996" startWordPosition="144" endWordPosition="146"> explored in the past, this approach exploits statistical parsing and robust coreference detection. In an evaluation involving revision of topic-related summaries using informativeness measures from the TIPSTER SUMMAC evaluation, the results show gains in informativeness without compromising readability. 1 Introduction Writing improves with revision. Authors are familiar with the process of condensing a long paper into a shorter one: this is an iterative process, with the results improved over successive drafts. Professional abstractors carry out substantial revision and editing of abstracts (Cremmins 1996). We therefore expect revision to be useful in automatic text summarization. Prior research exploring the use of revision in summarization, e.g., (Gabriel 1988), (Robin 1994), (McKeown et al. 1995) has focused mainly on structured data as the input. Here, we examine the use of revision in summarization of text input. First, we review some summarization terminology. In revising draft summaries, these condensation operations, as well as stylistic rewording of sentences, play an important role. Summaries can be used to indicate what topics are addressed in the source text, and thus can be used to</context>
<context position="5906" citStr="Cremmins 1996" startWordPosition="905" endWordPosition="906">l the given compression percentage of the source has been extracted, rounded to the nearest sentence. Next, for each rule in the sequence of revision rules, the program repeatedly applies the rule until it can no longer be applied. Each rule application results in a revised draft. The program selects sentences for rule application by giving preference to higher weighted sentences. &apos;Note that professional abstractors do not attempt to fully &amp;quot;understand&amp;quot; the text - often extremely technical material, but use surface-level features as above as well as the overall discourse structure of the text (Cremmins 1996). 21-lowever, recent progress on this problem (Marcu 1997) is encouraging. A unary rule applies to a single sentence. A binary rule applies to a pair of sentences, at least one of which must be in the draft, and where the first sentence precedes the second in the input. Control over sentence complexity is imposed by failing rule application when the draft sentence is too long, the parse tree is too deep3, or if more than two relative clauses would be stacked together. The program terminates when there are no more rules to apply or when the revised draft exceeds the required compression rate by</context>
</contexts>
<marker>Cremmins, 1996</marker>
<rawString>Cremmins, E. T. 1996. The Art of Abstracting. Information Resources Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dalianis</author>
<author>E Hov</author>
</authors>
<title>Aggregation in Natural Language Generation.</title>
<date>1996</date>
<booktitle>Trends in Natural Language Generation: an Artificial Intelligence Perspective, pp.88-105. Lecture Notes in Artificial Intelligence, Number 1036,</booktitle>
<editor>In Zock, M., and Adorni, G., eds.,</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin.</location>
<marker>Dalianis, Hov, 1996</marker>
<rawString>Dalianis, H., and Hov, E. 1996. Aggregation in Natural Language Generation. In Zock, M., and Adorni, G., eds., Trends in Natural Language Generation: an Artificial Intelligence Perspective, pp.88-105. Lecture Notes in Artificial Intelligence, Number 1036, Springer Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dras</author>
</authors>
<title>Tree Adjoining Grammar and the Reluctant Paraphrasing of Text,</title>
<date>1999</date>
<tech>Ph.D. Thesis,</tech>
<institution>Macquarie University,</institution>
<contexts>
<context position="9910" citStr="Dras 1999" startWordPosition="1559" endWordPosition="1560">nce-initial PPs and adverbial phrases satisfying lexical tests (such as &amp;quot;In particular,&amp;quot;, &amp;quot;Accordingly,&amp;quot; &amp;quot;In conclusion,&amp;quot; etc.)5. Aggregation operations combine constituents from two sentences, at least one of which must be a sentence in the draft, into a new constituent which is inserted into the draft sentence. The basis for combining sentences is that of referential identity: if there is an NP in sentence i which is coreferential with an NP in sentence j, then sentences i and j are candidates for aggregation. The most common form of aggregation is expressed as tree-adjunction (Joshi 1998) (Dras 1999). Figures 1 and 2 show a relative clause introduction rule which turns a VP of a (non-embedded) sentence whose our analysis because of a system bug. 5Such lexical tests help avoid misrepresenting the meaning of the sentence. subject is coreferential with an NP of an earlier (draft) sentence into a relative clause modifier of the draft sentence NP. Other appositive phrase insertion rules include copying and inserting nonrestrictive relative clause modifiers (e.g., &amp;quot;Smith, who...,&amp;quot;), appositive modifiers of proper names (e.g., &amp;quot;Peter G. Neumann, a computer security expert familiar with the case,</context>
</contexts>
<marker>Dras, 1999</marker>
<rawString>Dras, M. 1999. Tree Adjoining Grammar and the Reluctant Paraphrasing of Text, Ph.D. Thesis, Macquarie University, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gabriel</author>
</authors>
<title>Deliberate Writing.</title>
<date>1988</date>
<journal>Natural Language Generation Systems,</journal>
<editor>In McDonald, D.D., and Bolc, L., eds.,</editor>
<publisher>SpringerVerlag, NY.</publisher>
<contexts>
<context position="1199" citStr="Gabriel 1988" startWordPosition="169" endWordPosition="170">s using informativeness measures from the TIPSTER SUMMAC evaluation, the results show gains in informativeness without compromising readability. 1 Introduction Writing improves with revision. Authors are familiar with the process of condensing a long paper into a shorter one: this is an iterative process, with the results improved over successive drafts. Professional abstractors carry out substantial revision and editing of abstracts (Cremmins 1996). We therefore expect revision to be useful in automatic text summarization. Prior research exploring the use of revision in summarization, e.g., (Gabriel 1988), (Robin 1994), (McKeown et al. 1995) has focused mainly on structured data as the input. Here, we examine the use of revision in summarization of text input. First, we review some summarization terminology. In revising draft summaries, these condensation operations, as well as stylistic rewording of sentences, play an important role. Summaries can be used to indicate what topics are addressed in the source text, and thus can be used to alert the user as to the source content (the indicative function). Summaries can also be used to cover the concepts in the source text to the extent possible g</context>
</contexts>
<marker>Gabriel, 1988</marker>
<rawString>Gabriel, R. 1988. Deliberate Writing. In McDonald, D.D., and Bolc, L., eds., Natural Language Generation Systems, SpringerVerlag, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Harman</author>
<author>E M Voorhees</author>
</authors>
<date>1996</date>
<booktitle>The fifth text retrieval conference (trec-5). National Institute of Standards and Technology NIST SP</booktitle>
<pages>500--238</pages>
<contexts>
<context position="13568" citStr="Harman and Voorhees 1996" startWordPosition="2136" endWordPosition="2139">evaluation along with several automatically scored measures of informativeness has been reused in evaluating the informativeness of our revision component. 4.1 Background: TIPSTER Q&amp;A Evaluation In this Q&amp;A evaluation, the summarization system, given a document and a topic, needed to produce an informative, topic-related summary that contained the correct answers found in that document to a set of topic-related questions. These questions covered &amp;quot;obligatory&amp;quot; information that has to be provided in any document judged relevant to the topic. The topics chosen (3 in all) were drawn from the TREC (Harman and Voorhees 1996) data sets. For each topic, 30 relevant TREC documents were chosen as the source texts for topic-related summarization. The principal tasks of each Q&amp;A evaluator were to prepare the questions and answer keys and to score the system summaries. To construct the answer key, each evaluator marked off any passages in the text that provided an answer to a question (example shown in Table 1). Two kinds of scoring were carried out. In the first, a manual method, the answer to each question was judged Correct, Partially Correct, or Missing based on guidelines involving a human comparison of the summary</context>
<context position="21437" citStr="Harman and Voorhees 1996" startWordPosition="3404" endWordPosition="3407">ar form of their summary. Each summary was revised by calling the revision program with the full-text source, the original compression rate of the summary, and 9TextWise, which extracted named entities rather than passages, was excluded. 563 the summary weighting function (i.e., with the weight for each source sentence). The 630 revised summaries (3 topics x 30 documents per topic x 7 participant summaries per document) were then scored against the answer keys using the overlap measures above. The documents consisted of AP, Wall Street Journal, and Financial Times news articles from the TREC (Harman and Voorhees 1996) collection. The rules used in the system are very general, and were not modified for the evaluation except for turning off most of the reference adjustment rules, as we wished to evaluate that component separately. Since the answer keys typically do not contain names of commentators, we wanted to focus the algorithm away from such names (otherwise, it would aggregate information around those commentators). As a result, special rules were written in the revision rule language to detect commentator names in reported speech (&amp;quot;X said that ..&amp;quot;, &amp;quot;X said ...&amp;quot; , &amp;quot;, said X..&amp;quot;, &amp;quot;, said X..&amp;quot;, etc.), and</context>
</contexts>
<marker>Harman, Voorhees, 1996</marker>
<rawString>Harman, D.K. and E.M. Voorhees. 1996. The fifth text retrieval conference (trec-5). National Institute of Standards and Technology NIST SP 500-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>TreeAdjoining Grammars&amp;quot;.</title>
<date>1996</date>
<booktitle>Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>69--123</pages>
<editor>In Rosenberg, G., and Salomaa, A., eds.,</editor>
<publisher>Springer-Verlag, NY.</publisher>
<marker>Joshi, Schabes, 1996</marker>
<rawString>Joshi, A. K. and Schabes, Y. 1996. &amp;quot;TreeAdjoining Grammars&amp;quot;. In Rosenberg, G., and Salomaa, A., eds., Handbook of Formal Languages, Vol. 3, 69-123. Springer-Verlag, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Krupka</author>
</authors>
<title>SRA: Description of the SRA System as Used for MUC-6&amp;quot;,</title>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<location>Columbia, Maryland,</location>
<contexts>
<context position="7297" citStr="Krupka 1995" startWordPosition="1142" endWordPosition="1143">by (Sekine 1998) as having 79% F-score accuracy (parseval) on short sentences (less than 40 words) from the Treebank. An informal assessment we made of the accuracy of the parser (based on intuitive judgments) on our own data sets of news articles suggests about 66% of the parses were acceptable, with almost half of the remaining parsing errors being due to part-of-speech tagging errors, many of which could be fixed by preprocessing the text. To establish coreference between proper names, named entities are extracted from the document, along with coreference relations using SRA&apos;s NameTag 2.0 (Krupka 1995), a MUC-6 fielded system. In addition, we implemented our own coreference extension: A singular definite NP (e.g., beginning with &amp;quot;the&amp;quot;, and not marked as a proper name) is marked by our program as coreferential (i.e., in the same coreference equivalence class) with the last singular definite or singular indefinite atomic NP with the same head, provided they are within a distance -y of each other. On a corpus of 90 documents, drawn from the TIPSTER evaluation, described in Section 4.1 below, this coreference extension scored 94% precision (470 valid coreference classes/501 total coreference cl</context>
</contexts>
<marker>Krupka, 1995</marker>
<rawString>Krupka, G. 1995. &amp;quot;SRA: Description of the SRA System as Used for MUC-6&amp;quot;, Proceedings of the Sixth Message Understanding Conference (MUC-6), Columbia, Maryland, November 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>From discourse structures to text summaries.</title>
<date>1997</date>
<booktitle>Proceedings of the ACL/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization.</booktitle>
<editor>in Mani, I. and Maybury, M., eds.,</editor>
<contexts>
<context position="5964" citStr="Marcu 1997" startWordPosition="913" endWordPosition="914">racted, rounded to the nearest sentence. Next, for each rule in the sequence of revision rules, the program repeatedly applies the rule until it can no longer be applied. Each rule application results in a revised draft. The program selects sentences for rule application by giving preference to higher weighted sentences. &apos;Note that professional abstractors do not attempt to fully &amp;quot;understand&amp;quot; the text - often extremely technical material, but use surface-level features as above as well as the overall discourse structure of the text (Cremmins 1996). 21-lowever, recent progress on this problem (Marcu 1997) is encouraging. A unary rule applies to a single sentence. A binary rule applies to a pair of sentences, at least one of which must be in the draft, and where the first sentence precedes the second in the input. Control over sentence complexity is imposed by failing rule application when the draft sentence is too long, the parse tree is too deep3, or if more than two relative clauses would be stacked together. The program terminates when there are no more rules to apply or when the revised draft exceeds the required compression rate by more than 5. The syntactic structure of each source sente</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Marcu, D. 1997. From discourse structures to text summaries. in Mani, I. and Maybury, M., eds., Proceedings of the ACL/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>M Maybury</author>
<author>eds</author>
</authors>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12775" citStr="Mani et al. 1999" startWordPosition="2014" endWordPosition="2017"> a coreferential indefinite if the definite occurs without a prior indefinite. &apos;Such operations have been investigated earlier by (Robin 1994). 560 Draft sentence Other sentence S al /\ /\ NP VP NPI VP1 (=NP) Result sentence S WHIVP vp1 Figure 1: Relative Clause Introduction showing tree NP2 being adjoined into tree S 4 Evaluation Evaluation of text summarization and other such NLP technologies where there may be many acceptable outputs, is a difficult task. Recently, the U.S. government conducted a largescale evaluation of summarization systems as part of its TIPSTER text processing program (Mani et al. 1999), which included both an extrinsic (relevance assessment) evaluation, as well as an intrinsic (coverage of key ideas) evaluation. The test set used in the latter (Q&amp;A) evaluation along with several automatically scored measures of informativeness has been reused in evaluating the informativeness of our revision component. 4.1 Background: TIPSTER Q&amp;A Evaluation In this Q&amp;A evaluation, the summarization system, given a document and a topic, needed to produce an informative, topic-related summary that contained the correct answers found in that document to a set of topic-related questions. These </context>
</contexts>
<marker>Mani, Maybury, eds, 1999</marker>
<rawString>Mani, I. and M. Maybury, eds. 1999. Advances in Automatic Text Summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>T Firmin</author>
<author>D House</author>
<author>G Klein</author>
<author>L Hirschman</author>
<author>B Sundheim</author>
</authors>
<title>The TIPSTER SUMMAC Text Summarization Evaluation&amp;quot;,</title>
<date>1999</date>
<booktitle>Proceedings of EACL&apos;99,</booktitle>
<location>Bergen, Norway,</location>
<contexts>
<context position="12775" citStr="Mani et al. 1999" startWordPosition="2014" endWordPosition="2017"> a coreferential indefinite if the definite occurs without a prior indefinite. &apos;Such operations have been investigated earlier by (Robin 1994). 560 Draft sentence Other sentence S al /\ /\ NP VP NPI VP1 (=NP) Result sentence S WHIVP vp1 Figure 1: Relative Clause Introduction showing tree NP2 being adjoined into tree S 4 Evaluation Evaluation of text summarization and other such NLP technologies where there may be many acceptable outputs, is a difficult task. Recently, the U.S. government conducted a largescale evaluation of summarization systems as part of its TIPSTER text processing program (Mani et al. 1999), which included both an extrinsic (relevance assessment) evaluation, as well as an intrinsic (coverage of key ideas) evaluation. The test set used in the latter (Q&amp;A) evaluation along with several automatically scored measures of informativeness has been reused in evaluating the informativeness of our revision component. 4.1 Background: TIPSTER Q&amp;A Evaluation In this Q&amp;A evaluation, the summarization system, given a document and a topic, needed to produce an informative, topic-related summary that contained the correct answers found in that document to a set of topic-related questions. These </context>
</contexts>
<marker>Mani, Firmin, House, Klein, Hirschman, Sundheim, 1999</marker>
<rawString>Mani, I., Firmin, T., House, D., Klein, G., Hirschman, L., and Sundheim, B. 1999. &amp;quot;The TIPSTER SUMMAC Text Summarization Evaluation&amp;quot;, Proceedings of EACL&apos;99, Bergen, Norway, June 8-12, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>J Robin</author>
<author>K Kukich</author>
</authors>
<title>Generating Concise Natural Language Summaries.</title>
<date>1995</date>
<journal>Information Processing and Management,</journal>
<volume>31</volume>
<pages>703--733</pages>
<contexts>
<context position="1236" citStr="McKeown et al. 1995" startWordPosition="173" endWordPosition="176">es from the TIPSTER SUMMAC evaluation, the results show gains in informativeness without compromising readability. 1 Introduction Writing improves with revision. Authors are familiar with the process of condensing a long paper into a shorter one: this is an iterative process, with the results improved over successive drafts. Professional abstractors carry out substantial revision and editing of abstracts (Cremmins 1996). We therefore expect revision to be useful in automatic text summarization. Prior research exploring the use of revision in summarization, e.g., (Gabriel 1988), (Robin 1994), (McKeown et al. 1995) has focused mainly on structured data as the input. Here, we examine the use of revision in summarization of text input. First, we review some summarization terminology. In revising draft summaries, these condensation operations, as well as stylistic rewording of sentences, play an important role. Summaries can be used to indicate what topics are addressed in the source text, and thus can be used to alert the user as to the source content (the indicative function). Summaries can also be used to cover the concepts in the source text to the extent possible given the compression requirements for</context>
</contexts>
<marker>McKeown, Robin, Kukich, 1995</marker>
<rawString>McKeown, K., J. Robin, and K. Kukich. 1995. Generating Concise Natural Language Summaries. Information Processing and Management, 31,5, 703-733.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robin</author>
</authors>
<title>Revision-based generation of natural language summaries providing historical background: corpus-based analysis, design and implementation.</title>
<date>1994</date>
<tech>Ph.D. Thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="1213" citStr="Robin 1994" startWordPosition="171" endWordPosition="172">iveness measures from the TIPSTER SUMMAC evaluation, the results show gains in informativeness without compromising readability. 1 Introduction Writing improves with revision. Authors are familiar with the process of condensing a long paper into a shorter one: this is an iterative process, with the results improved over successive drafts. Professional abstractors carry out substantial revision and editing of abstracts (Cremmins 1996). We therefore expect revision to be useful in automatic text summarization. Prior research exploring the use of revision in summarization, e.g., (Gabriel 1988), (Robin 1994), (McKeown et al. 1995) has focused mainly on structured data as the input. Here, we examine the use of revision in summarization of text input. First, we review some summarization terminology. In revising draft summaries, these condensation operations, as well as stylistic rewording of sentences, play an important role. Summaries can be used to indicate what topics are addressed in the source text, and thus can be used to alert the user as to the source content (the indicative function). Summaries can also be used to cover the concepts in the source text to the extent possible given the compr</context>
<context position="3467" citStr="Robin 1994" startWordPosition="523" endWordPosition="524">involving aggregation (Dalianis and Hovy 1996) and elimination operations. Elimination can increase the amount of compression (summary length/source length) available, while aggregation can potentially gather and draw in relevant background information, in the form of descriptions of discourse entities from different parts of the source. We therefore hypothesize that these operations can result in packing in more information per unit compression than possible by concatenation. Rather than opportunistically adding as much background information that can fit in the available compression, as in (Robin 1994), our approach adds background information from the source text to the draft based on an information weighting function. Our revision approach assumes input sentences are represented as syntactic trees whose 558 nodes are annotated with coreference information. In order to provide open-domain coverage the approach does not assume a meaninglevel representation of each sentence, and so, unlike many generation systems, the system does not represent and reason about what is being said&apos;. Meaning-dependent revision operations are restricted to situations where it is clear from coreference that the s</context>
<context position="12300" citStr="Robin 1994" startWordPosition="1937" endWordPosition="1938">ordination rules include relative clause coordination. Reference Adjustment operations fix up the results of other revision operations in order to improve discourse-level coherence, and as a result, they are run lase. They include substitution of a proper name with a name alias if the name is mentioned earlier, expansion of a pronoun with a coreferential proper name in a parenthetical (&amp;quot;pronoun expansion&amp;quot;), and (&amp;quot;indefinitization&amp;quot;) replacement of a definite NP with a coreferential indefinite if the definite occurs without a prior indefinite. &apos;Such operations have been investigated earlier by (Robin 1994). 560 Draft sentence Other sentence S al /\ /\ NP VP NPI VP1 (=NP) Result sentence S WHIVP vp1 Figure 1: Relative Clause Introduction showing tree NP2 being adjoined into tree S 4 Evaluation Evaluation of text summarization and other such NLP technologies where there may be many acceptable outputs, is a difficult task. Recently, the U.S. government conducted a largescale evaluation of summarization systems as part of its TIPSTER text processing program (Mani et al. 1999), which included both an extrinsic (relevance assessment) evaluation, as well as an intrinsic (coverage of key ideas) evaluat</context>
</contexts>
<marker>Robin, 1994</marker>
<rawString>Robin, J. 1994. Revision-based generation of natural language summaries providing historical background: corpus-based analysis, design and implementation. Ph.D. Thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
</authors>
<title>Corpus-based Parsing and Sublanguage Studies.</title>
<date>1998</date>
<location>Ph.D. Dissertation, New York University,</location>
<contexts>
<context position="6614" citStr="Sekine 1998" startWordPosition="1030" endWordPosition="1031">to a single sentence. A binary rule applies to a pair of sentences, at least one of which must be in the draft, and where the first sentence precedes the second in the input. Control over sentence complexity is imposed by failing rule application when the draft sentence is too long, the parse tree is too deep3, or if more than two relative clauses would be stacked together. The program terminates when there are no more rules to apply or when the revised draft exceeds the required compression rate by more than 5. The syntactic structure of each source sentence is extracted using Apple Pie 7.2 (Sekine 1998), a statistical parser trained on Penn Treebank data. It was evaluated by (Sekine 1998) as having 79% F-score accuracy (parseval) on short sentences (less than 40 words) from the Treebank. An informal assessment we made of the accuracy of the parser (based on intuitive judgments) on our own data sets of news articles suggests about 66% of the parses were acceptable, with almost half of the remaining parsing errors being due to part-of-speech tagging errors, many of which could be fixed by preprocessing the text. To establish coreference between proper names, named entities are extracted from t</context>
</contexts>
<marker>Sekine, 1998</marker>
<rawString>Sekine, S. 1998. Corpus-based Parsing and Sublanguage Studies. Ph.D. Dissertation, New York University, 1998.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>