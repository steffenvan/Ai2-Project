<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008849">
<title confidence="0.997828">
Integration of Multiple Bilingually-Learned Segmentation Schemes
into Statistical Machine Translation
</title>
<author confidence="0.615025">
Michael Paul and Andrew Finch and Eiichiro Sumita
</author>
<affiliation confidence="0.392787333333333">
MASTAR Project
National Institute of Information and Communications Technology
Hikaridai 2-2-2, Keihanna Science City
</affiliation>
<address confidence="0.531478">
619-0288 Kyoto, Japan
</address>
<email confidence="0.997545">
michael.paul@nict.go.jp
</email>
<sectionHeader confidence="0.993855" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999784214285714">
This paper proposes an unsupervised
word segmentation algorithm that identi-
fies word boundaries in continuous source
language text in order to improve the
translation quality of statistical machine
translation (SMT) approaches. The
method can be applied to any language
pair where the source language is unseg-
mented and the target language segmen-
tation is known. First, an iterative boot-
strap method is applied to learn multi-
ple segmentation schemes that are consis-
tent with the phrasal segmentations of an
SMT system trained on the resegmented
bitext. In the second step, multiple seg-
mentation schemes are integrated into a
single SMT system by characterizing the
source language side and merging iden-
tical translation pairs of differently seg-
mented SMT models. Experimental re-
sults translating five Asian languages into
English revealed that the method of in-
tegrating multiple segmentation schemes
outperforms SMT models trained on any
of the learned word segmentations and
performs comparably to available state-of-
the-art monolingually-built segmentation
tools.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97063906122449">
The task of word segmentation, i.e., identifying
word boundaries in continuous text, is one of the
fundamental preprocessing steps of data-driven
NLP applications like Machine Translation (MT).
In contrast to Indo-European languages like En-
glish, many Asian languages like Chinese do not
use a whitespace character to separate meaningful
word units. The problems of word segmentation
are:
(1) ambiguity, e.g., for Chinese, a single charac-
ter can be a word component in one context,
but a word by itself in another context.
(2) unknown words, i.e., existing words can be
combined into new words such as proper
nouns, e.g. “White House”.
Purely dictionary-based approaches like (Cheng
et al., 1999) addressed these problems by max-
imum matching heuristics. Recent research on
unsupervised word segmentation focuses on ap-
proaches based on probabilistic methods. For ex-
ample, (Brent, 1999) proposes a probabilistic seg-
mentation model based on unigram word distri-
butions, whereas (Venkataraman, 2001) uses stan-
dard n-gram language models. An alternative non-
parametric Bayesian inference approach based on
the Dirichlet process incorporating unigram and
bigram word dependencies is introduced in (Gold-
water et al., 2006).
The focus of this paper, however, is to
learn word segmentations that are consistent with
phrasal segmentations of SMT translation mod-
els. In case of small translation units, e.g. sin-
gle Chinese or Japanese characters, it is likely
that such tokens have been seen in the training
corpus, thus these tokens can be translated by
an SMT engine. However, the contextual infor-
mation provided by these tokens might not be
enough to obtain a good translation. For exam-
ple, a Japanese-English SMT engine might trans-
late the two successive characters “ ” (“white”)
and “ ” (“bird”) as “white bird”, while a human
would translate “ ” as “swan”. Therefore, the
longer the translation unit, the more context can be
exploited to find a meaningful translation. On the
other hand, the longer the translation unit, the less
likely it is that such a token will occur in the train-
ing data due to data sparseness of the language
resources utilized to train the statistical translation
models. Therefore, a word segmentation that is
</bodyText>
<page confidence="0.960535">
400
</page>
<note confidence="0.961861">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 400–408,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999890225352113">
“consistent with SMT models” is one that identi-
fies translation units that are small enough to be
translatable, but large enough to be meaningful in
the context of the given input sentence, achieving a
trade-off between the coverage and the translation
task complexity of the statistical models in order to
improve translation quality.
The use of monolingual probabilistic models
does not necessarily yield a better MT perfor-
mance (Chang et al., 2008). However, improve-
ments have been reported for approaches taking
into account not only monolingual, but also bilin-
gual information, to derive a word segmentation
suitable for SMT. Due to the availability of lan-
guage resources, most recent research has focused
on optimizing Chinese word segmentation (CWS)
for Chinese-to-English SMT. For example, (Xu et
al., 2008) proposes a Bayesian Semi-Supervised
approach for CWS that builds on (Goldwater et al.,
2006). The generative model first segments Chi-
nese text using an off-the-shelf segmenter and then
learns new word types and word distributions suit-
able for SMT. Similarly, a dynamic programming-
based variational Bayes approach using bilingual
information to improve MT is proposed in (Chung
and Gildea, 2009). Concerning other languages,
for example, (Kikui and Yamamoto, 2002) ex-
tended Hidden-Markov-Models, where hidden n-
gram probabilities were affected by co-occurring
words in the target language part for Japanese
word segmentation.
Recent research on SMT is also focusing on the
usage of multiple word segmentation schemes for
the source language to improve translation qual-
ity. For example, (Zhang et al., 2008) combines
dictionary-based and CRF-based approaches for
Chinese word segmentation in order to avoid out-
of-vocabulary (OOV) words. Moreover, the com-
bination of different morphological decomposi-
tion of highly inflected languages like Arabic or
Finnish is proposed in (de Gispert et al., 2009) to
reduce the data sparseness problem of SMT ap-
proaches. Similarly, (Nakov et al., 2009) utilizes
SMT engines trained on different word segmenta-
tion schemes and combines the translation outputs
using system combination techniques as a post-
process to SMT decoding.
In order to integrate multiple word segmenta-
tion schemes into the SMT decoder, (Dyer et al.,
2008) proposed to generate word lattices covering
all possible segmentations of the input sentence
and to decode the lattice input. An extended ver-
sion of the lattice approach that does not require
the use (and existence) of monolingual segmenta-
tion tools was proposed in (Dyer, 2009) where a
maximum entropy model is used to assign prob-
abilities to the segmentations of an input word to
generate diverse segmentation lattices from a sin-
gle automatically learned model.
The method of (Ma and Way, 2009) also uses
a word lattice decoding approach, but they itera-
tively extract multiple word segmentation schemes
from the training bitext. This dictionary-based
approach uses heuristics based on the maximum
matching algorithm to obtain an agglomeration of
segments that are covered by the dictionary. It uses
all possible source segmentations that are consis-
tent with the extracted dictionary to create a word
lattice for decoding.
The method proposed in this papers differs from
previous approaches in the following points:
</bodyText>
<listItem confidence="0.946111421052631">
• it works for any language pair where the
source language is unsegmented and the tar-
get language segmentation is known.
• it can be applied for the translation of a
source language where no linguistically mo-
tivated word segmentation tools are available.
• it applies machine learning techniques to
identify segmentation schemes that improve
translation quality for a given language pair.
• it decodes directly from unsegmented text us-
ing segmentation information implicit in the
phrase-table to generate the target and thus
avoids issues of consistency between phrase-
table and input representation.
• it uses segmentations at all iterative levels of
the bootstrap process, rather than only those
from the final iteration allowing the consid-
eration of segmentations from many levels of
granularity.
</listItem>
<bodyText confidence="0.998600454545455">
Word segmentations are learned using a parallel
corpus by aligning character-wise source language
sentences to word units separated by a white-
space in the target language. Successive characters
aligned to the same target words are merged into a
larger source language unit. Therefore, the granu-
larity of the translation unit is defined in the given
bitext context. In order to minimize the side ef-
fects of alignment errors and to achieve segmenta-
tion consistency, a Maximum-Entropy (ME) algo-
rithm is applied to learn the source language word
</bodyText>
<page confidence="0.997829">
401
</page>
<bodyText confidence="0.999869807692308">
segmentation that is consistent with the transla-
tion model of an SMT system trained on the re-
segmented bitext. The process is iterated until
no further improvement in translation quality is
achieved. In order to integrate multiple word seg-
mentation into a single SMT system, the statisti-
cal translation models trained on differently seg-
mented source language corpora are merged by
characterizing the source side of each translation
model, summing up the probabilities of identical
phrase translation pairs, and rescoring the merged
translation model (see Section 2).
The proposed segmentation method is applied
to the translation of five Asian languages, i.e.,
Japanese, Korean, Thai, and two Chinese dialects
(Standard Mandarin and Taiwanese Mandarin),
into English. The utilized language resources
and the outline of the experiments are summa-
rized in Section 3. The experimental results re-
vealed that the proposed method outperforms not
only a baseline system that translates character-
ized source language sentences, but also all SMT
models trained on any of the learned word seg-
mentations. In addition, the proposed method
achieves translation results comparable to SMT
models trained on linguistically segmented bitext.
</bodyText>
<sectionHeader confidence="0.939148" genericHeader="method">
2 Word Segmentation
</sectionHeader>
<bodyText confidence="0.978173037037037">
The word segmentation method proposed in this
paper is an unsupervised, language-independent
approach that treats the task of word segmentation
as a phrase-boundary tagging task. This method
uses a parallel text corpus consisting of initially
unigram segmented source language character se-
quences and whitespace-separated target language
words. The initial bitext is used to train a stan-
dard phrase-based SMT system (SMT�hr). The
character-to-word alignment results of the SMT
training procedure) are exploited to identify suc-
cessive source language characters aligned to the
same target language word in the respective bitext
and to merge these characters into larger transla-
tion units, defining its granularity in the given bi-
text context.
The obtained translation units are then used to
learn the word segmentation that is most consis-
tent with the phrase alignments of the given SMT
system. First, each character of the source lan-
guage text is annotated with a word-boundary in-
&apos;For the experiments presented in Section 3, the GIZA++
toolkit was used.
dicator where only two tags are used, i.e, `E”
(end-of-word character tag) and `I” (in-word
character tag). The annotations are derived from
the SMT training corpus as described in Figure 1.
</bodyText>
<listItem confidence="0.997624272727273">
(1) proc annotate-phrase-boundaries( Bitext) ;
(2) begin
(3) for each (Src, Trg) in {Bitext} do
(4) A +-- align(Src, Trg) ;
(5) for each i in {1, ... , len(Src)-1} do
(6) Trgi +-- get-target(Src[i], A) ;
(7) Trgi+1 +-- get-target(Src[i+1], A) ;
(8) if null(Trgi) or Trgi # Trgi+1 then
(9) (* aligned to none or different target *)
(10) SrcME +-- assign-tag(Src[i],&apos; E&apos;) ;
(11) else
(12) (* aligned to the same target *)
(13) SrcME +-- assign-tag(Src[i],&apos; I&apos;) ;
(14) fi ;
(15) CorpusME +-- add(SrcME) ;
(16) od ;
(17) (* last source token *)
(18) LastSrcME +-- assign-tag(Src[len(Src)],&apos; E&apos;) ;
(19) CorpusME +-- add(LastSrcME) ;
(20) od ;
(21) return( CorpusME ) ;
(22) end ;
</listItem>
<figureCaption confidence="0.998678">
Figure 1: ME Training Data Annotation
</figureCaption>
<bodyText confidence="0.999845666666667">
Using these alignment-based word boundary
annotations, a Maximum-Entropy (ME) method is
applied to learn the word segmentation consistent
with the SMT translation model (see Section 2.1),
to resegment the original source language corpus,
and to retrain a phrase-based SMT engine that will
hopefully achieve a better translation performance
than the initial SMT engine. This process should
be repeated as long as an improvement in transla-
tion quality is achieved. Eventually, the concate-
nation of succeeding translation units will result in
overfitting, i.e., the newly created token can only
be translated in the context of rare training data ex-
amples. Therefore, a lower translation quality due
to an increase of untranslatable source language
phrases is to be expected (see Section 2.2).
However, in order to increase the coverage and
to reduce the translation task complexity of the
statistical models, the proposed method integrates
multiple segmentation schemes into the statistical
translation models of a single SMT engine so that
longer translation units are preferred for transla-
tion, if available, and smaller translation units can
be used otherwise (see Section 2.3).
</bodyText>
<subsectionHeader confidence="0.992467">
2.1 Maximum-Entropy Tagging Model
</subsectionHeader>
<bodyText confidence="0.9989295">
ME models provide a general purpose machine
learning technique for classification and predic-
</bodyText>
<page confidence="0.995544">
402
</page>
<table confidence="0.99959975">
Lexical Context Features &lt; t0, w−2 &gt; &lt; t0, w−1 &gt;
&lt; t0, w0 &gt;
&lt; t0, w+1 &gt; &lt; t0, w+2 &gt;
Tag Context Features &lt; t0, t−1 &gt; &lt; t0, t−1, t−2 &gt;
</table>
<tableCaption confidence="0.999664">
Table 1: Feature Set of ME Tagging Model
</tableCaption>
<bodyText confidence="0.999934466666667">
tion. They are versatile tools that can handle
large numbers of features, and have shown them-
selves to be highly effective in a broad range of
NLP tasks including sentence boundary detection
or part-of-speech tagging (Berger et al., 1996).
A maximum entropy classifier is an exponential
model consisting of a number of binary feature
functions and their weights (Pietra et al., 1997).
The model is trained by adjusting the weights to
maximize the entropy of the probabilistic model
given constraints imposed by the training data. In
our experiments, we use a conditional maximum
entropy model, where the conditional probability
of the outcome given the set of features is modeled
(Ratnaparkhi, 1996). The model has the form:
</bodyText>
<equation confidence="0.977621">
α��(��t)
k �p0
</equation>
<bodyText confidence="0.999371210526316">
where:
t is the tag being predicted;
c is the context of t;
γ is a normalization coefficient;
K is the number of features in the model;
fk are binary feature functions;
ak is the weight of feature function fk;
p0 is the default model.
The feature set is given in Table 1. The lexical
context features consist of target words annotated
with a tag t. w0 denotes the word being tagged and
w_2, ... , w+2 the surrounding words. t0 denotes
the current tag, t_1 the previous tag, etc. The tag
context features supply information about the con-
text of previous tag sequences. This conditional
model can be used as a classifier. The model is
trained iteratively, and we used the improved iter-
ative scaling algorithm (IIS) (Berger et al., 1996)
for the experiments presented in Section 3.
</bodyText>
<subsectionHeader confidence="0.997602">
2.2 Iterative Bootstrap Method
</subsectionHeader>
<bodyText confidence="0.999917">
The proposed iterative bootstrap method to learn
the word segmentation that is consistent with an
SMT engine is summarized in Figure 2. After
the ME tagging model is learned from the ini-
tial character-to-word alignments of the respec-
tive bitext ((1)-(4)), the obtained ME tagger is
</bodyText>
<figureCaption confidence="0.993541">
Figure 2: Iterative Bootstrap Method
</figureCaption>
<bodyText confidence="0.999447896551724">
applied to resegment the source language side of
the unsegmented parallel text corpus ((5)). This
results in a resegmented bitext that can be used
to retrain and reevaluate another engine SMT1
((6)), achieving what is hoped to be a better trans-
lation performance than the initial SMT engine
(SMTMI).
The unsupervised ME tagging method can also
be applied to the token-to-word alignments ex-
tracted during the training of the SMT1 engine
to obtain an ME tagging model ME1 capable of
handling longer translation units ((7)-(8)). Such
a bootstrap method iteratively creates a sequence
of SMT engines SMTi ((9)-(J)), each of which
reduces the translation complexity, because larger
chunks can be translated in a single step leading
to fewer word order or word disambiguation er-
rors. However, at some point, the increased length
of translation units learned from the training cor-
pus will lead to overfitting, resulting in reduced
translation performance when translating unseen
sentences. Therefore, the bootstrap method stops
when the Jth resegmentation of the training cor-
pus results in a lower automatic evaluation score
for the unseen sentences than the one for the previ-
ous iteration. The ME tagging model MEJ_1 that
achieved the highest automatic translation scores
is then selected as the best single-iteration word
segmenter.
</bodyText>
<subsectionHeader confidence="0.999474">
2.3 Integration of Multiple Segmentations
</subsectionHeader>
<bodyText confidence="0.999935666666667">
The integration of multiple word segmentation
schemes is carried out by merging the transla-
tion models of the SMT engines trained on the
characterized and iteratively learned segmentation
schemes. This process is performed by linearly in-
terpolating the model probabilities of each of the
</bodyText>
<figure confidence="0.98063878125">
K
p(t, c) = γ 11
k=0
...
Iteration 1
segmented SRC
Iteration 2
segmented SRC
Iteration J-1
segmented SRC
unigram
segmented SRC
TRG text
SRC text
(1) characterize
(5)
resegment
(9)
resegment
...
(4)
annotate
SMTchr
SRCtoken
TRGword
alignment
ME1
classifier
(3) extract
(2) train
(8)
annotate
...
SRCtoken
TRGword
alignment
SMT1
ME2
classifier
(7) extract
(6) train
...
SMTJ-1
SRCtoken
TRGword
alignment
MEJ-1
classifier
(J-1) train
extract
SMTJ
(J) train
Selected Word
Segmenter
decode
decode
decode
evalchr
eval1
evalJ-1
evalJ
worse
better
better
</figure>
<page confidence="0.998072">
403
</page>
<bodyText confidence="0.999984829268293">
models. In our experiments, equal weights were
used; however, it might be interesting to inves-
tigate varying the weights according to iteration
number, as the latter iterations may contain more
useful segmentations.
In addition, we also remove the internal seg-
mentation of the source phrases. The advantages
are twofold. Primarily it allows decoding directly
from unsegmented text. Moreover, the segmenta-
tion of the source phrase can differ between mod-
els at differing iterations; removing the source seg-
mentation at this stage makes the phrase pairs in
the translations models at various stages in the it-
erative process consistent with one another. Con-
sequently, duplicate bilingual phrase pairs appear
in the phrase table. These duplicates are combined
by normalizing their model probabilities prior to
model interpolation.
The rescored translation model covers all trans-
lation pairs that were learned by any of the
iterative models. Therefore, the selection of
longer translation units during decoding can re-
duce the complexity of the translation task. On the
other hand, overfitting problems of single-iteration
models can be avoided because multiple smaller
source language translation units can be exploited
to cover the given input parts and to generate trans-
lation hypotheses based on the concatenation of
associated target phrase expressions. Moreover,
the merging process increases the translation prob-
abilities of the source/target translation parts that
cover the same surface string but differ only in
the segmentation of the source language phrase.
Therefore, the more often such a translation pair is
learned by different iterative models, the more of-
ten the respective target language expression will
be exploited by the SMT decoder.
The translation of unseen data using the merged
translation models is carried out by (1) character-
izing the input text and (2) applying the SMT de-
coding in a standard way.
</bodyText>
<sectionHeader confidence="0.99981" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.998131666666667">
The effects of using different word segmentations
and integrating them into an SMT engine are in-
vestigated using the multilingual Basic Travel Ex-
pressions Corpus (BTEC), which is a collection
of sentences that bilingual travel experts consider
useful for people going to or coming from other
countries (Kikui et al., 2006). For the word seg-
mentation experiments, we selected five Asian
languages that do not naturally separate word
</bodyText>
<table confidence="0.995610416666667">
BTEC train set dev set test set
# of sen 160,000 1,000 1,000
en voc 15,390 1,262 1,292
len 7.5 7.1 7.2
ja voc 17,168 1,407 1,408
len 8.5 8.2 8.2
ko voc 17,246 1,366 1,365
len 8.0 7.7 7.8
th voc 7,354 1,081 1,053
len 7.8 7.3 7.4
zh voc 11,084 1,312 1,301
len 7.1 6.4 6.5
</table>
<tableCaption confidence="0.997384">
Table 2: Language Resources
</tableCaption>
<bodyText confidence="0.975151875">
units, i.e., Japanese (ja), Korean (ko), Thai (th),
and two dialects of Chinese (Standard Mandarin
(zh) and Taiwanese Mandarin (tw)).
Table 2 summarizes the characteristics of the
BTEC corpus used for the training (train) of the
SMT models, the tuning of model weights and
stop conditions of the iterative bootstrap method
(dev), and the evaluation of translation quality
(test). Besides the number of sentences (sen)
and the vocabulary (voc), the sentence length
(len) is also given as the average number of
words per sentence. The given statistics are ob-
tained using commonly-used linguistic segmenta-
tion tools available for the respective language,
i.e., CHASEN (ja), WORDCUT (th), ICTCLAS
(zh), HanTagger (ko). No segmentation was avail-
able for Taiwanese Mandarin and therefore no
meaningful statistics could be obtained.
For the training of the SMT models, standard
word alignment (Och and Ney, 2003) and lan-
guage modeling (Stolcke, 2002) tools were used.
Minimum error rate training (MERT) was used to
tune the decoder’s parameters and performed on
the dev set using the technique proposed in (Och
and Ney, 2003). For the translation, a multi-stack
phrase-based decoder was used.
For the evaluation of translation quality, we ap-
plied standard automatic metrics, i.e., BLEU (Pap-
ineni et al., 2002) and METEOR (Lavie and Agar-
wal, 2007). We have tested the statistical signif-
cance of our results2 using the bootstrap method
reported in (Zhang et al., 2004) that (1) performs a
random sampling with replacement from the eval-
uation data set, (2) calculates the evaluation metric
score of each engine for the sampled test sentences
and the difference between the two MT system
scores, (3) repeats the sampling/scoring step itera-
22000 iterations were used for the analysis of the auto-
matic evaluation results in this paper. All reported differences
in evaluation scores are statistically significant.
</bodyText>
<page confidence="0.998432">
404
</page>
<bodyText confidence="0.999905090909091">
tively, and (4) applies the Student’s t-test at a sig-
nificance level of 95% confidence to test whether
the score differences are significant.
In addition, human assessment of translation
quality was carried out using the Ranking metrics.
For the Ranking evaluation, a human grader was
asked to “rank each whole sentence translation
from Best to Worst relative to the other choices
(ties are allowed)” (Callison-Burch et al., 2007).
The Ranking scores were obtained as the average
number of times that a system was judged better
than any other system and the normalized ranks
(NormRank) were calculated on a per-judge ba-
sis for each translation task using the method of
(Blatz et al., 2003).
Section 3.1 compares the proposed method to
the baseline system that translates characterized
source language sentences and to the SMT en-
gines that are trained on iteratively learned as well
as language-dependent linguistic word segmenta-
tions. The effects of the iterative learning method
are summarized in Section 3.2.
</bodyText>
<subsectionHeader confidence="0.999771">
3.1 Effects of Word Segmentation
</subsectionHeader>
<bodyText confidence="0.996580405063291">
The automatic evaluation scores of the SMT en-
gines trained on the differently segmented source
language resources are given in Table 3, where
“character” refers to the baseline system of using
character-segmented source text; “single-best”3 is
the SMT engine that is trained on the corpus seg-
mented by the best-performing iteration of the
bootstrap approach; “proposed” is the SMT engine
whose models integrate multiple word segmen-
tation schemes; and “linguistic” uses language-
dependent linguistically motivated word segmen-
tation tools. The reported scores are calculated as
the mean score of all metric scores obtained for the
iterative sampling method used for statistical sig-
nificance testing and listed as percentage figures.
The results show that the proposed method out-
performs the character (single-best) system for
each of the involved languages achieving gains
of 2.0 to 9.1 (0.4 to 1.6) BLEU points and 2.0
to 5.9 (0.7 to 4.6) METEOR points, respectively.
However, the improvements depend on the source
language. For example, the smallest gains were
obtained for Standard Mandarin, because single
characters frequently form words of their own,
thus resulting in more ambiguity than Japanese,
3This approximates the approach of (Ma and Way, 2009)
and is given as a way of showing the effect of segmentation
at multiple levels of granularity.
where consecutive hiragana or katakana charac-
ters can form larger meaningful units.
Comparing the proposed method towards lin-
guistically motivated segmenters, the results show
that the proposed method outperforms the SMT
engines using linguistic segmentation tools for
tasks such as translating Korean and Standard
Mandarin into English. Slightly lower evaluation
scores were achieved for the automatically learned
word segmentation for Japanese, although the re-
sults of the proposed method are quite similar.
This is a suprisingly strong result, given the ma-
turity of the linguistically motivated segmenters,
and given that our segmenters use only the bilin-
gual corpus used to train the SMT systems.
The Thai-English experiments expose some is-
sues that are related to the definition of what
a “character” is. Our segmentation schemes
are learned directly from the bitext without any
language-specific information, and can cope well
with most languages. However, Thai seems to be
an exceptional case in our experiments, because
(1) the Thai script is a segmental writing system
which is based on consonants but in which vowel
notation is obligatory, so that the characterization
of the baseline system affects vowel dependen-
cies, (2) it uses tone markers that are placed above
the consonant, but are treated as a single charac-
ter in our approach, and (3) vowels sounding after
a consonant are non-sequential and can occur be-
fore, after, above, or below a consonant increasing
the number of word form variations in the training
corpus and reducing the accuracy of the learned
ME tagging models. This is an interesting result
that motivates further study on how to incorpo-
rate features on language scripts into our machine
learning framework. For example, Japanese is
written in three different scripts (kanji, hiragana,
katakana). Therefore, the script class of each char-
acter could be used as an additional feature to ob-
tain the initial segmentation of the training corpus.
Finally, the results for Taiwanese Mandarin,
where no linguistic tool was available to segment
the source language text, shows that the proposed
method can be applied successfully for the trans-
lation of any language where no linguistically-
motivated segmentation tools are available.
Table 4 summarizes the subjective evaluation
results which were carried out by a paid evalua-
tion expert who is a native speaker of English. The
NormRank results confirm the findings of the au-
</bodyText>
<page confidence="0.99783">
405
</page>
<table confidence="0.9856241875">
BLEU
source character word segmentation proposed linguistic
language single-best
ja 36.93 39.65 41.25 41.46
ko 34.72 37.32 38.51 37.19
th 41.42 50.16 50.53 56.68
zh 36.59 37.02 38.61 38.13
tw 45.71 50.95 52.21 –
METEOR
source character word segmentation proposed linguistic
language single-best
ja 59.78 60.95 65.45 66.03
ko 58.45 60.06 64.31 63.04
th 67.22 71.22 72.58 79.02
zh 61.77 62.38 63.80 62.72
tw 70.14 73.64 74.38 –
</table>
<tableCaption confidence="0.9575">
Table 3: Automatic Evaluation
</tableCaption>
<table confidence="0.99279625">
NormRank
source character word segmentation proposed linguistic
language single-best
ja 2.76 2.85 3.18 3.12
ko 2.68 2.90 3.17 3.09
th 2.65 2.95 3.05 3.43
zh 2.87 3.01 3.07 3.04
tw 2.83 2.86 3.24 –
</table>
<tableCaption confidence="0.998689">
Table 4: Subjective Evaluation
</tableCaption>
<bodyText confidence="0.98422675">
tomatic evaluation. In addition, for Japanese, the
translation outputs of the proposed method were
judged better than those of the linguistically seg-
mented SMT model.
</bodyText>
<subsectionHeader confidence="0.999956">
3.2 Effects of Bootstrap Iteration
</subsectionHeader>
<bodyText confidence="0.99998480952381">
In order to get an idea of the robustness of the pro-
posed method, the changes in system performance
for each source language during the iterative boot-
strap method is given in Figure 3. The results for
BLEU and METEOR show that all languages reach
their best performance after the first or second it-
eration and then slightly, but consistently decrease
with the increased number of iterations. The rea-
son for this is the effect of overfitting caused by
the concatenation of source tokens that are aligned
to longer target phrases, resulting in the segmenta-
tion of longer translation units.
The changes in the vocabulary size and the word
length are summarized in Figure 4. The amount of
words extracted by the proposed method is much
larger than the one of the baseline system, increas-
ing the vocabulary size by a factor of 10 for Stan-
dard Mandarin and Taiwanese Mandarin, 30 for
Japanese and Korean, and 100 for Thai. It is also
larger than the vocabulary obtained for the linguis-
tic tools by a factor of 1.5 to 2.5 for all investigated
</bodyText>
<figureCaption confidence="0.999845">
Figure 3: Change in System Performance
Figure 4: Change in Vocabulary Size and Length
</figureCaption>
<bodyText confidence="0.999851833333333">
languages. The average vocabulary length also in-
creased for each iteration whereby the length of
the translation units learned after 10 iterations al-
most doubles the word size of the initial iteration.
The overfitting problem of the iterative boot-
strap method is illustrated in the increase of out-
of-vocabulary words, i.e. source language words
contained in the unseen evaluation data set that
cannot be translated by the respective SMT. The
results given in Figure 5 show a large increase
in OOV for the first three iterations, resulting in
lower translation qualities as listed in Figure 3.
Table 5 illustrates translation examples using
different segmentation schemes for the Japanese-
English translation task. The SMT engines that
output the best translations are marked with an as-
terisk. In the first example, the concatenation of “
” (already midnight) by the single-best
segmentation scheme leads to an OOV word, thus
only a partial translation can be achieved. How-
ever, the problem can be resolved using the pro-
posed method. The second example is best trans-
lated using the single-best word segmentation that
correctly handles the sentence coordination. The
</bodyText>
<figure confidence="0.998925075">
BLEU (%)
METEOR (%)
60.00
50.00
40.00
30.00
20.00
70.00
60.00
50.00
40.00
80.00
chr 1 2 3 4 5 6 7 8 9 10
chr 1 2 3 4 5 6 7 8 9 10
iteration
Change in METEOR
Change in BLEU
iteration
Change in Vocabulary Size
120000
Vocabulary Size
100000
60000
40000
20000
80000
0
chr 1 2 3 4 5 6 7 8 9 10
iteration
Change in Average Vocabulary Length
Vocabulary Length
30.0
25.0
20.0
15.0
10.0
5.0
0.0
chr 1 2 3 4 5 6 7 8 9 10
iteration
</figure>
<page confidence="0.932833">
406
</page>
<figureCaption confidence="0.998996">
Figure 5: Change in Out-of-Vocabulary Size
</figureCaption>
<bodyText confidence="0.9999695">
baseline system omits the sentence coordination
information, resulting in an unacceptable transla-
tion. The third examples illustrates that longer to-
kens reduce the translation complexity and thus
can be translated better than the other segmenta-
tion that cause more ambiguities.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999974228571428">
This paper proposes a new language-independent
method to segment languages that do not use
whitespace characters to separate meaningful
word units in an unsupervised manner in order to
improve the performance of a state-of-the-art SMT
system. The proposed method does not need any
linguistic information about the source language
which is important when building SMT systems
for the translation of relatively resource-poor lan-
guages which frequently lack morphological anal-
ysis tools. In addition, the development costs
are far less than those for developing linguistic
word segmentation tools or even paying humans
to segment the data sets manually, since only the
bilingual corpus used to train the SMT system is
needed to train the segmenter.
The effectiveness of the proposed method was
investigated for the translation of Japanese, Ko-
rean, Thai, and two Chinese dialects (Standard
Mandarin and Taiwanese Mandarin) into English
for the domain of travel conversations. The auto-
matic evaluation of the translation results showed
consistent improvements of 2.0 to 9.1 BLEU points
and 2.0 to 5.9 METEOR points compared to a
baseline system that translates characterized input.
Moreover, it improves the best performing SMT
engine of the iterative learning procedure by 0.4
to 1.6 BLEU points and 0.7 to 4.6 METEOR points.
In addition, the proposed method achieved
translation results similar to SMT models trained
on bitext segmented with linguistically motivated
tools, even outperforming these for Korean, Chi-
nese, and Japanese in the human evaluation, al-
though no external information and only the given
bitext was used to train the segmentation models.
</bodyText>
<table confidence="0.9996014">
linguistic seg: / / / / / / /
trans: Yes. Let’s see. It’s midnight.
character* seg: / / / / / / / / / /✔ / / /
/ /
trans: Yes. Well, it’s already midnight.
single-best seg: / / / / /
trans: Yes. Let ’s see.
proposed* seg: / / / / / / / / / /✔ / / /
/ /
trans: Yes. Well, it’s already midnight.
linguistic seg: / / /
/ /☆
/✯ / /✱ / / / /
/
trans: I’d like a pair of jeans.
Could you recommend a good shop?
character seg: / / / / /☆ / / / / / / / /
/ /✯ / /✱ / / / / / / /
trans: Could you recommend a good ’d like
a pair of jeans.
✦ /
single-best* seg: / / /
/✯ / / /
trans: I’d like some jeans.
Could you recommend a good shop?
proposed seg: / / / / /☆ / / / / / / / /
/ /✯ / /✱ / / / / / / /
trans: I ’d like a pair of jeans and
could you recommend a good shop?
linguistic seg: / /✾✿/ / / / / /
trans: Will it be ready by this afternoon?
character seg: ✼ / / /✾ /✿ / / / / / / / /
/
trans: It’ll be ready by this afternoon?
single-best seg: /✾ / / /
trans: Will it be ready by this afternoon?
proposed* seg: ✼ / / /✾ /✿ / / / / / / / /
/
trans: Can you have these ready by this
afternoon?
</table>
<tableCaption confidence="0.998282">
Table 5: Sample Translations
</tableCaption>
<bodyText confidence="0.999910714285714">
The experiments using Thai are interesting be-
cause the script is a segmental writing system us-
ing tone markers and vowel dependencies. This
exposed some issues that are related to the defini-
tion of what a “character” is and motivates further
study on how to incorporate features on language
scripts into our machine learning framework.
</bodyText>
<sectionHeader confidence="0.999277" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9987565">
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to
NLP. Computational Linguistics, 22(1):39–71.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence es-
timation for statistical machine translation. In Final
Report of the JHU Summer Workshop.
Michael Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34:71–105.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Jan Schroeder. 2007.
(Meta-) Evaluation of Machine Translation. In Pro-
ceedings of the 2nd Workshop on SMT, pages 136–
158, Prague, Czech Republic.
</reference>
<figure confidence="0.998894">
OOV Words
600
500
400
300
200
100
0
chr 1 2 3 4 5 6 7 8 9 10
Change in OOV Words
iteration
</figure>
<page confidence="0.979665">
407
</page>
<reference confidence="0.999415595238095">
Pi-Chuan Chang, Michel Galley, and Christopher Man-
ning. 2008. Optimizing Chinese Word Segmen-
tation for Machine Translation Performance. In
Proc. of the 3rd Workshop on SMT, pages 224–232,
Columbus, USA.
Kwok-Shing Cheng, Gilbert Young, and Kam-Fai
Wong. 1999. A study on word-based and integrat-
bit Chinese text compression algorithms. American
Society ofInformation Science, 50(3):218–228.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised Tokenization for Machine Translation. In
Proc. of the EMNLP, pages 718–726, Singapore.
Adrian de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes Risk Com-
bination of Translation Hypotheses from Alternative
Morphological Decompositions. In Proc. of HLT,
Companion Volume, pages 73–76, Boulder, USA.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing Word Lattice Transla-
tion. In Proc. ofACL, pages 1012–1020, Columbus,
USA.
Christopher Dyer. 2009. Using a maximum entropy
model to build segmentation lattices for MT. In
Proc. ofHLT, pages 406–414, Boulder, USA.
Sharon Goldwater, Thomas Griffith, and Mark John-
son. 2006. Contextual Dependencies in Unsuper-
vised Word Segmentation. In Proc. of the ACL,
pages 673–680, Sydney, Australia.
Geninchiro Kikui and Hirofumi Yamamoto. 2002.
Finding Translation Pairs from English-Japanese
Untokenized Aligned Corpora. In Proc. of the Work-
shop on Speech-to-Speech Translation, pages 23–30,
Philadephia, USA.
Geninchiro Kikui, Seiichi Yamamoto, Toshiyuki
Takezawa, and Eiichiro Sumita. 2006. Com-
parative study on corpora for speech translation.
IEEE Transactions on Audio, Speech and Language,
14(5):1674–1682.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In
Proc. of the 2nd Workshop on SMT, pages 228–231,
Prague, Czech Republic.
Yanjun Ma and Andy Way. 2009. Bilingually Moti-
vated Domain-Adapted Word Segmentation for Sta-
tistical Machine Translation. In Proc. of the 12th
EACL, pages 549–557, Athens, Greece.
Preslav Nakov, Chang Liu, Wei Lu, and Hwee Tou Ng.
2009. The NUS SMT System for IWSLT 2009. In
Proc. ofIWSLT, pages 91–98, Tokyo, Japan.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Statistical Alignment Models. Com-
putational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proc. of the
40th ACL, pages 311–318, Philadelphia, USA.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing Features of Random
Fields. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 19(4):380–393.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Proc. of the
EMNLP, pages 133–142, Pennsylvania, USA.
Andreas Stolcke. 2002. SRILM an extensible lan-
guage modeling toolkit. In Proc. of ICSLP, pages
901–904, Denver, USA.
Anand Venkataraman. 2001. A statistical model for
word discovery in transcribed speech. Computa-
tional Linguistics, 27(3):351–372.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian Semi-Supervised Chi-
nese Word Segmentation for SMT. In Proc. of the
COLING, pages 1017–1024, Manchester, UK.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting Bleu/NIST Scores: How Much Im-
provement do We Need to Have a Better System? In
Proc. of the LREC, pages 2051–2054, Lisbon, Por-
tugal.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved Statistical Machine Translation by
Multiple Chinese Word Segmentation. In Proc. of
the 3rd Workshop on SMT, pages 216–223, Colum-
bus, USA.
</reference>
<page confidence="0.997883">
408
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.364885">
<title confidence="0.999039">Integration of Multiple Bilingually-Learned Segmentation into Statistical Machine Translation</title>
<author confidence="0.955069">Michael Paul</author>
<author confidence="0.955069">Andrew Finch</author>
<author confidence="0.955069">Eiichiro</author>
<affiliation confidence="0.796476">MASTAR National Institute of Information and Communications</affiliation>
<address confidence="0.6243695">Hikaridai 2-2-2, Keihanna Science 619-0288 Kyoto,</address>
<email confidence="0.972719">michael.paul@nict.go.jp</email>
<abstract confidence="0.998547413793103">This paper proposes an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches. method can be applied to any language pair where the source language is unsegmented and the target language segmentation is known. First, an iterative bootstrap method is applied to learn multiple segmentation schemes that are consistent with the phrasal segmentations of an SMT system trained on the resegmented bitext. In the second step, multiple segmentation schemes are integrated into a single SMT system by characterizing the source language side and merging identical translation pairs of differently segmented SMT models. Experimental results translating five Asian languages into English revealed that the method of integrating multiple segmentation schemes outperforms SMT models trained on any of the learned word segmentations and performs comparably to available state-ofthe-art monolingually-built segmentation tools.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to NLP.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="13385" citStr="Berger et al., 1996" startWordPosition="2079" endWordPosition="2082">able, and smaller translation units can be used otherwise (see Section 2.3). 2.1 Maximum-Entropy Tagging Model ME models provide a general purpose machine learning technique for classification and predic402 Lexical Context Features &lt; t0, w−2 &gt; &lt; t0, w−1 &gt; &lt; t0, w0 &gt; &lt; t0, w+1 &gt; &lt; t0, w+2 &gt; Tag Context Features &lt; t0, t−1 &gt; &lt; t0, t−1, t−2 &gt; Table 1: Feature Set of ME Tagging Model tion. They are versatile tools that can handle large numbers of features, and have shown themselves to be highly effective in a broad range of NLP tasks including sentence boundary detection or part-of-speech tagging (Berger et al., 1996). A maximum entropy classifier is an exponential model consisting of a number of binary feature functions and their weights (Pietra et al., 1997). The model is trained by adjusting the weights to maximize the entropy of the probabilistic model given constraints imposed by the training data. In our experiments, we use a conditional maximum entropy model, where the conditional probability of the outcome given the set of features is modeled (Ratnaparkhi, 1996). The model has the form: α��(��t) k �p0 where: t is the tag being predicted; c is the context of t; γ is a normalization coefficient; K is</context>
<context position="14621" citStr="Berger et al., 1996" startWordPosition="2291" endWordPosition="2294">eatures in the model; fk are binary feature functions; ak is the weight of feature function fk; p0 is the default model. The feature set is given in Table 1. The lexical context features consist of target words annotated with a tag t. w0 denotes the word being tagged and w_2, ... , w+2 the surrounding words. t0 denotes the current tag, t_1 the previous tag, etc. The tag context features supply information about the context of previous tag sequences. This conditional model can be used as a classifier. The model is trained iteratively, and we used the improved iterative scaling algorithm (IIS) (Berger et al., 1996) for the experiments presented in Section 3. 2.2 Iterative Bootstrap Method The proposed iterative bootstrap method to learn the word segmentation that is consistent with an SMT engine is summarized in Figure 2. After the ME tagging model is learned from the initial character-to-word alignments of the respective bitext ((1)-(4)), the obtained ME tagger is Figure 2: Iterative Bootstrap Method applied to resegment the source language side of the unsegmented parallel text corpus ((5)). This results in a resegmented bitext that can be used to retrain and reevaluate another engine SMT1 ((6)), achie</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to NLP. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Final Report of the JHU Summer Workshop.</booktitle>
<contexts>
<context position="22484" citStr="Blatz et al., 2003" startWordPosition="3537" endWordPosition="3540">f 95% confidence to test whether the score differences are significant. In addition, human assessment of translation quality was carried out using the Ranking metrics. For the Ranking evaluation, a human grader was asked to “rank each whole sentence translation from Best to Worst relative to the other choices (ties are allowed)” (Callison-Burch et al., 2007). The Ranking scores were obtained as the average number of times that a system was judged better than any other system and the normalized ranks (NormRank) were calculated on a per-judge basis for each translation task using the method of (Blatz et al., 2003). Section 3.1 compares the proposed method to the baseline system that translates characterized source language sentences and to the SMT engines that are trained on iteratively learned as well as language-dependent linguistic word segmentations. The effects of the iterative learning method are summarized in Section 3.2. 3.1 Effects of Word Segmentation The automatic evaluation scores of the SMT engines trained on the differently segmented source language resources are given in Table 3, where “character” refers to the baseline system of using character-segmented source text; “single-best”3 is t</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2003. Confidence estimation for statistical machine translation. In Final Report of the JHU Summer Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="2289" citStr="Brent, 1999" startWordPosition="332" endWordPosition="333"> Chinese do not use a whitespace character to separate meaningful word units. The problems of word segmentation are: (1) ambiguity, e.g., for Chinese, a single character can be a word component in one context, but a word by itself in another context. (2) unknown words, i.e., existing words can be combined into new words such as proper nouns, e.g. “White House”. Purely dictionary-based approaches like (Cheng et al., 1999) addressed these problems by maximum matching heuristics. Recent research on unsupervised word segmentation focuses on approaches based on probabilistic methods. For example, (Brent, 1999) proposes a probabilistic segmentation model based on unigram word distributions, whereas (Venkataraman, 2001) uses standard n-gram language models. An alternative nonparametric Bayesian inference approach based on the Dirichlet process incorporating unigram and bigram word dependencies is introduced in (Goldwater et al., 2006). The focus of this paper, however, is to learn word segmentations that are consistent with phrasal segmentations of SMT translation models. In case of small translation units, e.g. single Chinese or Japanese characters, it is likely that such tokens have been seen in th</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>Michael Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Jan Schroeder</author>
</authors>
<title>(Meta-) Evaluation of Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2nd Workshop on SMT,</booktitle>
<pages>136--158</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="22225" citStr="Callison-Burch et al., 2007" startWordPosition="3492" endWordPosition="3495">ing/scoring step itera22000 iterations were used for the analysis of the automatic evaluation results in this paper. All reported differences in evaluation scores are statistically significant. 404 tively, and (4) applies the Student’s t-test at a significance level of 95% confidence to test whether the score differences are significant. In addition, human assessment of translation quality was carried out using the Ranking metrics. For the Ranking evaluation, a human grader was asked to “rank each whole sentence translation from Best to Worst relative to the other choices (ties are allowed)” (Callison-Burch et al., 2007). The Ranking scores were obtained as the average number of times that a system was judged better than any other system and the normalized ranks (NormRank) were calculated on a per-judge basis for each translation task using the method of (Blatz et al., 2003). Section 3.1 compares the proposed method to the baseline system that translates characterized source language sentences and to the SMT engines that are trained on iteratively learned as well as language-dependent linguistic word segmentations. The effects of the iterative learning method are summarized in Section 3.2. 3.1 Effects of Word</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Jan Schroeder. 2007. (Meta-) Evaluation of Machine Translation. In Proceedings of the 2nd Workshop on SMT, pages 136– 158, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher Manning</author>
</authors>
<title>Optimizing Chinese Word Segmentation for Machine Translation Performance.</title>
<date>2008</date>
<booktitle>In Proc. of the 3rd Workshop on SMT,</booktitle>
<pages>224--232</pages>
<location>Columbus, USA.</location>
<contexts>
<context position="4276" citStr="Chang et al., 2008" startWordPosition="646" endWordPosition="649">oint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 400–408, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics “consistent with SMT models” is one that identifies translation units that are small enough to be translatable, but large enough to be meaningful in the context of the given input sentence, achieving a trade-off between the coverage and the translation task complexity of the statistical models in order to improve translation quality. The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al., 2008). However, improvements have been reported for approaches taking into account not only monolingual, but also bilingual information, to derive a word segmentation suitable for SMT. Due to the availability of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suita</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher Manning. 2008. Optimizing Chinese Word Segmentation for Machine Translation Performance. In Proc. of the 3rd Workshop on SMT, pages 224–232, Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kwok-Shing Cheng</author>
<author>Gilbert Young</author>
<author>Kam-Fai Wong</author>
</authors>
<title>A study on word-based and integratbit Chinese text compression algorithms.</title>
<date>1999</date>
<journal>American Society ofInformation Science,</journal>
<volume>50</volume>
<issue>3</issue>
<contexts>
<context position="2101" citStr="Cheng et al., 1999" startWordPosition="303" endWordPosition="306">s text, is one of the fundamental preprocessing steps of data-driven NLP applications like Machine Translation (MT). In contrast to Indo-European languages like English, many Asian languages like Chinese do not use a whitespace character to separate meaningful word units. The problems of word segmentation are: (1) ambiguity, e.g., for Chinese, a single character can be a word component in one context, but a word by itself in another context. (2) unknown words, i.e., existing words can be combined into new words such as proper nouns, e.g. “White House”. Purely dictionary-based approaches like (Cheng et al., 1999) addressed these problems by maximum matching heuristics. Recent research on unsupervised word segmentation focuses on approaches based on probabilistic methods. For example, (Brent, 1999) proposes a probabilistic segmentation model based on unigram word distributions, whereas (Venkataraman, 2001) uses standard n-gram language models. An alternative nonparametric Bayesian inference approach based on the Dirichlet process incorporating unigram and bigram word dependencies is introduced in (Goldwater et al., 2006). The focus of this paper, however, is to learn word segmentations that are consist</context>
</contexts>
<marker>Cheng, Young, Wong, 1999</marker>
<rawString>Kwok-Shing Cheng, Gilbert Young, and Kam-Fai Wong. 1999. A study on word-based and integratbit Chinese text compression algorithms. American Society ofInformation Science, 50(3):218–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Unsupervised Tokenization for Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of the EMNLP,</booktitle>
<pages>718--726</pages>
<contexts>
<context position="5035" citStr="Chung and Gildea, 2009" startWordPosition="761" endWordPosition="764">erive a word segmentation suitable for SMT. Due to the availability of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suitable for SMT. Similarly, a dynamic programmingbased variational Bayes approach using bilingual information to improve MT is proposed in (Chung and Gildea, 2009). Concerning other languages, for example, (Kikui and Yamamoto, 2002) extended Hidden-Markov-Models, where hidden ngram probabilities were affected by co-occurring words in the target language part for Japanese word segmentation. Recent research on SMT is also focusing on the usage of multiple word segmentation schemes for the source language to improve translation quality. For example, (Zhang et al., 2008) combines dictionary-based and CRF-based approaches for Chinese word segmentation in order to avoid outof-vocabulary (OOV) words. Moreover, the combination of different morphological decompo</context>
</contexts>
<marker>Chung, Gildea, 2009</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2009. Unsupervised Tokenization for Machine Translation. In Proc. of the EMNLP, pages 718–726, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian de Gispert</author>
<author>Sami Virpioja</author>
<author>Mikko Kurimo</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes Risk Combination of Translation Hypotheses from Alternative Morphological Decompositions.</title>
<date>2009</date>
<booktitle>In Proc. of HLT, Companion Volume,</booktitle>
<pages>73--76</pages>
<location>Boulder, USA.</location>
<marker>de Gispert, Virpioja, Kurimo, Byrne, 2009</marker>
<rawString>Adrian de Gispert, Sami Virpioja, Mikko Kurimo, and William Byrne. 2009. Minimum Bayes Risk Combination of Translation Hypotheses from Alternative Morphological Decompositions. In Proc. of HLT, Companion Volume, pages 73–76, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing Word Lattice Translation. In</title>
<date>2008</date>
<booktitle>Proc. ofACL,</booktitle>
<pages>1012--1020</pages>
<location>Columbus, USA.</location>
<contexts>
<context position="6098" citStr="Dyer et al., 2008" startWordPosition="922" endWordPosition="925">based approaches for Chinese word segmentation in order to avoid outof-vocabulary (OOV) words. Moreover, the combination of different morphological decomposition of highly inflected languages like Arabic or Finnish is proposed in (de Gispert et al., 2009) to reduce the data sparseness problem of SMT approaches. Similarly, (Nakov et al., 2009) utilizes SMT engines trained on different word segmentation schemes and combines the translation outputs using system combination techniques as a postprocess to SMT decoding. In order to integrate multiple word segmentation schemes into the SMT decoder, (Dyer et al., 2008) proposed to generate word lattices covering all possible segmentations of the input sentence and to decode the lattice input. An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. The method of (Ma and Way, 2009) also uses a word lattice decoding approach, but they iteratively extract multiple word segmentation sc</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing Word Lattice Translation. In Proc. ofACL, pages 1012–1020, Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
</authors>
<title>Using a maximum entropy model to build segmentation lattices for MT.</title>
<date>2009</date>
<booktitle>In Proc. ofHLT,</booktitle>
<pages>406--414</pages>
<location>Boulder, USA.</location>
<contexts>
<context position="6377" citStr="Dyer, 2009" startWordPosition="969" endWordPosition="970">problem of SMT approaches. Similarly, (Nakov et al., 2009) utilizes SMT engines trained on different word segmentation schemes and combines the translation outputs using system combination techniques as a postprocess to SMT decoding. In order to integrate multiple word segmentation schemes into the SMT decoder, (Dyer et al., 2008) proposed to generate word lattices covering all possible segmentations of the input sentence and to decode the lattice input. An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. The method of (Ma and Way, 2009) also uses a word lattice decoding approach, but they iteratively extract multiple word segmentation schemes from the training bitext. This dictionary-based approach uses heuristics based on the maximum matching algorithm to obtain an agglomeration of segments that are covered by the dictionary. It uses all possible source segmentations that are consistent with the extracted dict</context>
</contexts>
<marker>Dyer, 2009</marker>
<rawString>Christopher Dyer. 2009. Using a maximum entropy model to build segmentation lattices for MT. In Proc. ofHLT, pages 406–414, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas Griffith</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual Dependencies in Unsupervised Word Segmentation.</title>
<date>2006</date>
<booktitle>In Proc. of the ACL,</booktitle>
<pages>673--680</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2618" citStr="Goldwater et al., 2006" startWordPosition="376" endWordPosition="380">rds such as proper nouns, e.g. “White House”. Purely dictionary-based approaches like (Cheng et al., 1999) addressed these problems by maximum matching heuristics. Recent research on unsupervised word segmentation focuses on approaches based on probabilistic methods. For example, (Brent, 1999) proposes a probabilistic segmentation model based on unigram word distributions, whereas (Venkataraman, 2001) uses standard n-gram language models. An alternative nonparametric Bayesian inference approach based on the Dirichlet process incorporating unigram and bigram word dependencies is introduced in (Goldwater et al., 2006). The focus of this paper, however, is to learn word segmentations that are consistent with phrasal segmentations of SMT translation models. In case of small translation units, e.g. single Chinese or Japanese characters, it is likely that such tokens have been seen in the training corpus, thus these tokens can be translated by an SMT engine. However, the contextual information provided by these tokens might not be enough to obtain a good translation. For example, a Japanese-English SMT engine might translate the two successive characters “ ” (“white”) and “ ” (“bird”) as “white bird”, while a </context>
<context position="4733" citStr="Goldwater et al., 2006" startWordPosition="715" endWordPosition="718">al models in order to improve translation quality. The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al., 2008). However, improvements have been reported for approaches taking into account not only monolingual, but also bilingual information, to derive a word segmentation suitable for SMT. Due to the availability of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suitable for SMT. Similarly, a dynamic programmingbased variational Bayes approach using bilingual information to improve MT is proposed in (Chung and Gildea, 2009). Concerning other languages, for example, (Kikui and Yamamoto, 2002) extended Hidden-Markov-Models, where hidden ngram probabilities were affected by co-occurring words in the target language part for Japanese word segmentation. Recent research on SMT is also focusing on the usage of multiple wor</context>
</contexts>
<marker>Goldwater, Griffith, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas Griffith, and Mark Johnson. 2006. Contextual Dependencies in Unsupervised Word Segmentation. In Proc. of the ACL, pages 673–680, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geninchiro Kikui</author>
<author>Hirofumi Yamamoto</author>
</authors>
<title>Finding Translation Pairs from English-Japanese Untokenized Aligned Corpora.</title>
<date>2002</date>
<booktitle>In Proc. of the Workshop on Speech-to-Speech Translation,</booktitle>
<pages>23--30</pages>
<location>Philadephia, USA.</location>
<contexts>
<context position="5104" citStr="Kikui and Yamamoto, 2002" startWordPosition="770" endWordPosition="773"> of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suitable for SMT. Similarly, a dynamic programmingbased variational Bayes approach using bilingual information to improve MT is proposed in (Chung and Gildea, 2009). Concerning other languages, for example, (Kikui and Yamamoto, 2002) extended Hidden-Markov-Models, where hidden ngram probabilities were affected by co-occurring words in the target language part for Japanese word segmentation. Recent research on SMT is also focusing on the usage of multiple word segmentation schemes for the source language to improve translation quality. For example, (Zhang et al., 2008) combines dictionary-based and CRF-based approaches for Chinese word segmentation in order to avoid outof-vocabulary (OOV) words. Moreover, the combination of different morphological decomposition of highly inflected languages like Arabic or Finnish is propos</context>
</contexts>
<marker>Kikui, Yamamoto, 2002</marker>
<rawString>Geninchiro Kikui and Hirofumi Yamamoto. 2002. Finding Translation Pairs from English-Japanese Untokenized Aligned Corpora. In Proc. of the Workshop on Speech-to-Speech Translation, pages 23–30, Philadephia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geninchiro Kikui</author>
<author>Seiichi Yamamoto</author>
<author>Toshiyuki Takezawa</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Comparative study on corpora for speech translation.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech and Language,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="19483" citStr="Kikui et al., 2006" startWordPosition="3045" endWordPosition="3048">erative models, the more often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 3 Experiments The effects of using different word segmentations and integrating them into an SMT engine are investigated using the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of sentences that bilingual travel experts consider useful for people going to or coming from other countries (Kikui et al., 2006). For the word segmentation experiments, we selected five Asian languages that do not naturally separate word BTEC train set dev set test set # of sen 160,000 1,000 1,000 en voc 15,390 1,262 1,292 len 7.5 7.1 7.2 ja voc 17,168 1,407 1,408 len 8.5 8.2 8.2 ko voc 17,246 1,366 1,365 len 8.0 7.7 7.8 th voc 7,354 1,081 1,053 len 7.8 7.3 7.4 zh voc 11,084 1,312 1,301 len 7.1 6.4 6.5 Table 2: Language Resources units, i.e., Japanese (ja), Korean (ko), Thai (th), and two dialects of Chinese (Standard Mandarin (zh) and Taiwanese Mandarin (tw)). Table 2 summarizes the characteristics of the BTEC corpus </context>
</contexts>
<marker>Kikui, Yamamoto, Takezawa, Sumita, 2006</marker>
<rawString>Geninchiro Kikui, Seiichi Yamamoto, Toshiyuki Takezawa, and Eiichiro Sumita. 2006. Comparative study on corpora for speech translation. IEEE Transactions on Audio, Speech and Language, 14(5):1674–1682.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Proc. of the 2nd Workshop on SMT,</booktitle>
<pages>228--231</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="21230" citStr="Lavie and Agarwal, 2007" startWordPosition="3334" endWordPosition="3338">No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with replacement from the evaluation data set, (2) calculates the evaluation metric score of each engine for the sampled test sentences and the difference between the two MT system scores, (3) repeats the sampling/scoring step itera22000 iterations were used for the analysis of the automatic evaluation results in this paper. All reported differences in evaluation scores are statistically significant. 404 tively, and (4) applies the Student</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proc. of the 2nd Workshop on SMT, pages 228–231, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Andy Way</author>
</authors>
<title>Bilingually Motivated Domain-Adapted Word Segmentation for Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of the 12th EACL,</booktitle>
<pages>549--557</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="6595" citStr="Ma and Way, 2009" startWordPosition="1004" endWordPosition="1007">cess to SMT decoding. In order to integrate multiple word segmentation schemes into the SMT decoder, (Dyer et al., 2008) proposed to generate word lattices covering all possible segmentations of the input sentence and to decode the lattice input. An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. The method of (Ma and Way, 2009) also uses a word lattice decoding approach, but they iteratively extract multiple word segmentation schemes from the training bitext. This dictionary-based approach uses heuristics based on the maximum matching algorithm to obtain an agglomeration of segments that are covered by the dictionary. It uses all possible source segmentations that are consistent with the extracted dictionary to create a word lattice for decoding. The method proposed in this papers differs from previous approaches in the following points: • it works for any language pair where the source language is unsegmented and t</context>
<context position="24094" citStr="Ma and Way, 2009" startWordPosition="3782" endWordPosition="3785">sampling method used for statistical significance testing and listed as percentage figures. The results show that the proposed method outperforms the character (single-best) system for each of the involved languages achieving gains of 2.0 to 9.1 (0.4 to 1.6) BLEU points and 2.0 to 5.9 (0.7 to 4.6) METEOR points, respectively. However, the improvements depend on the source language. For example, the smallest gains were obtained for Standard Mandarin, because single characters frequently form words of their own, thus resulting in more ambiguity than Japanese, 3This approximates the approach of (Ma and Way, 2009) and is given as a way of showing the effect of segmentation at multiple levels of granularity. where consecutive hiragana or katakana characters can form larger meaningful units. Comparing the proposed method towards linguistically motivated segmenters, the results show that the proposed method outperforms the SMT engines using linguistic segmentation tools for tasks such as translating Korean and Standard Mandarin into English. Slightly lower evaluation scores were achieved for the automatically learned word segmentation for Japanese, although the results of the proposed method are quite sim</context>
</contexts>
<marker>Ma, Way, 2009</marker>
<rawString>Yanjun Ma and Andy Way. 2009. Bilingually Motivated Domain-Adapted Word Segmentation for Statistical Machine Translation. In Proc. of the 12th EACL, pages 549–557, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Chang Liu</author>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
</authors>
<date>2009</date>
<booktitle>The NUS SMT System for IWSLT 2009. In Proc. ofIWSLT,</booktitle>
<pages>91--98</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="5824" citStr="Nakov et al., 2009" startWordPosition="879" endWordPosition="882">in the target language part for Japanese word segmentation. Recent research on SMT is also focusing on the usage of multiple word segmentation schemes for the source language to improve translation quality. For example, (Zhang et al., 2008) combines dictionary-based and CRF-based approaches for Chinese word segmentation in order to avoid outof-vocabulary (OOV) words. Moreover, the combination of different morphological decomposition of highly inflected languages like Arabic or Finnish is proposed in (de Gispert et al., 2009) to reduce the data sparseness problem of SMT approaches. Similarly, (Nakov et al., 2009) utilizes SMT engines trained on different word segmentation schemes and combines the translation outputs using system combination techniques as a postprocess to SMT decoding. In order to integrate multiple word segmentation schemes into the SMT decoder, (Dyer et al., 2008) proposed to generate word lattices covering all possible segmentations of the input sentence and to decode the lattice input. An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assig</context>
</contexts>
<marker>Nakov, Liu, Lu, Ng, 2009</marker>
<rawString>Preslav Nakov, Chang Liu, Wei Lu, and Hwee Tou Ng. 2009. The NUS SMT System for IWSLT 2009. In Proc. ofIWSLT, pages 91–98, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="20796" citStr="Och and Ney, 2003" startWordPosition="3264" endWordPosition="3267">s of the iterative bootstrap method (dev), and the evaluation of translation quality (test). Besides the number of sentences (sen) and the vocabulary (voc), the sentence length (len) is also given as the average number of words per sentence. The given statistics are obtained using commonly-used linguistic segmentation tools available for the respective language, i.e., CHASEN (ja), WORDCUT (th), ICTCLAS (zh), HanTagger (ko). No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with repl</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A Systematic Comparison of Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="21193" citStr="Papineni et al., 2002" startWordPosition="3327" endWordPosition="3331">th), ICTCLAS (zh), HanTagger (ko). No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with replacement from the evaluation data set, (2) calculates the evaluation metric score of each engine for the sampled test sentences and the difference between the two MT system scores, (3) repeats the sampling/scoring step itera22000 iterations were used for the analysis of the automatic evaluation results in this paper. All reported differences in evaluation scores are statistically significant. 40</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proc. of the 40th ACL, pages 311–318, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing Features of Random Fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="13530" citStr="Pietra et al., 1997" startWordPosition="2102" endWordPosition="2105">e machine learning technique for classification and predic402 Lexical Context Features &lt; t0, w−2 &gt; &lt; t0, w−1 &gt; &lt; t0, w0 &gt; &lt; t0, w+1 &gt; &lt; t0, w+2 &gt; Tag Context Features &lt; t0, t−1 &gt; &lt; t0, t−1, t−2 &gt; Table 1: Feature Set of ME Tagging Model tion. They are versatile tools that can handle large numbers of features, and have shown themselves to be highly effective in a broad range of NLP tasks including sentence boundary detection or part-of-speech tagging (Berger et al., 1996). A maximum entropy classifier is an exponential model consisting of a number of binary feature functions and their weights (Pietra et al., 1997). The model is trained by adjusting the weights to maximize the entropy of the probabilistic model given constraints imposed by the training data. In our experiments, we use a conditional maximum entropy model, where the conditional probability of the outcome given the set of features is modeled (Ratnaparkhi, 1996). The model has the form: α��(��t) k �p0 where: t is the tag being predicted; c is the context of t; γ is a normalization coefficient; K is the number of features in the model; fk are binary feature functions; ak is the weight of feature function fk; p0 is the default model. The feat</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing Features of Random Fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-Of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proc. of the EMNLP,</booktitle>
<pages>133--142</pages>
<location>Pennsylvania, USA.</location>
<contexts>
<context position="13846" citStr="Ratnaparkhi, 1996" startWordPosition="2153" endWordPosition="2154">e shown themselves to be highly effective in a broad range of NLP tasks including sentence boundary detection or part-of-speech tagging (Berger et al., 1996). A maximum entropy classifier is an exponential model consisting of a number of binary feature functions and their weights (Pietra et al., 1997). The model is trained by adjusting the weights to maximize the entropy of the probabilistic model given constraints imposed by the training data. In our experiments, we use a conditional maximum entropy model, where the conditional probability of the outcome given the set of features is modeled (Ratnaparkhi, 1996). The model has the form: α��(��t) k �p0 where: t is the tag being predicted; c is the context of t; γ is a normalization coefficient; K is the number of features in the model; fk are binary feature functions; ak is the weight of feature function fk; p0 is the default model. The feature set is given in Table 1. The lexical context features consist of target words annotated with a tag t. w0 denotes the word being tagged and w_2, ... , w+2 the surrounding words. t0 denotes the current tag, t_1 the previous tag, etc. The tag context features supply information about the context of previous tag se</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part-Of-Speech Tagging. In Proc. of the EMNLP, pages 133–142, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP,</booktitle>
<pages>901--904</pages>
<location>Denver, USA.</location>
<contexts>
<context position="20834" citStr="Stolcke, 2002" startWordPosition="3272" endWordPosition="3273"> and the evaluation of translation quality (test). Besides the number of sentences (sen) and the vocabulary (voc), the sentence length (len) is also given as the average number of words per sentence. The given statistics are obtained using commonly-used linguistic segmentation tools available for the respective language, i.e., CHASEN (ja), WORDCUT (th), ICTCLAS (zh), HanTagger (ko). No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained. For the training of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with replacement from the evaluation data set, </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM an extensible language modeling toolkit. In Proc. of ICSLP, pages 901–904, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Venkataraman</author>
</authors>
<title>A statistical model for word discovery in transcribed speech.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="2399" citStr="Venkataraman, 2001" startWordPosition="347" endWordPosition="348">entation are: (1) ambiguity, e.g., for Chinese, a single character can be a word component in one context, but a word by itself in another context. (2) unknown words, i.e., existing words can be combined into new words such as proper nouns, e.g. “White House”. Purely dictionary-based approaches like (Cheng et al., 1999) addressed these problems by maximum matching heuristics. Recent research on unsupervised word segmentation focuses on approaches based on probabilistic methods. For example, (Brent, 1999) proposes a probabilistic segmentation model based on unigram word distributions, whereas (Venkataraman, 2001) uses standard n-gram language models. An alternative nonparametric Bayesian inference approach based on the Dirichlet process incorporating unigram and bigram word dependencies is introduced in (Goldwater et al., 2006). The focus of this paper, however, is to learn word segmentations that are consistent with phrasal segmentations of SMT translation models. In case of small translation units, e.g. single Chinese or Japanese characters, it is likely that such tokens have been seen in the training corpus, thus these tokens can be translated by an SMT engine. However, the contextual information p</context>
</contexts>
<marker>Venkataraman, 2001</marker>
<rawString>Anand Venkataraman. 2001. A statistical model for word discovery in transcribed speech. Computational Linguistics, 27(3):351–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann Ney</author>
</authors>
<title>Bayesian Semi-Supervised Chinese Word Segmentation for SMT.</title>
<date>2008</date>
<booktitle>In Proc. of the COLING,</booktitle>
<pages>1017--1024</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="4640" citStr="Xu et al., 2008" startWordPosition="701" endWordPosition="704"> a trade-off between the coverage and the translation task complexity of the statistical models in order to improve translation quality. The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al., 2008). However, improvements have been reported for approaches taking into account not only monolingual, but also bilingual information, to derive a word segmentation suitable for SMT. Due to the availability of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT. For example, (Xu et al., 2008) proposes a Bayesian Semi-Supervised approach for CWS that builds on (Goldwater et al., 2006). The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suitable for SMT. Similarly, a dynamic programmingbased variational Bayes approach using bilingual information to improve MT is proposed in (Chung and Gildea, 2009). Concerning other languages, for example, (Kikui and Yamamoto, 2002) extended Hidden-Markov-Models, where hidden ngram probabilities were affected by co-occurring words in the target language part for Jap</context>
</contexts>
<marker>Xu, Gao, Toutanova, Ney, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann Ney. 2008. Bayesian Semi-Supervised Chinese Word Segmentation for SMT. In Proc. of the COLING, pages 1017–1024, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting Bleu/NIST Scores: How Much Improvement do We Need to Have a Better System?</title>
<date>2004</date>
<booktitle>In Proc. of the LREC,</booktitle>
<pages>2051--2054</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="21350" citStr="Zhang et al., 2004" startWordPosition="3355" endWordPosition="3358">ng of the SMT models, standard word alignment (Och and Ney, 2003) and language modeling (Stolcke, 2002) tools were used. Minimum error rate training (MERT) was used to tune the decoder’s parameters and performed on the dev set using the technique proposed in (Och and Ney, 2003). For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). We have tested the statistical signifcance of our results2 using the bootstrap method reported in (Zhang et al., 2004) that (1) performs a random sampling with replacement from the evaluation data set, (2) calculates the evaluation metric score of each engine for the sampled test sentences and the difference between the two MT system scores, (3) repeats the sampling/scoring step itera22000 iterations were used for the analysis of the automatic evaluation results in this paper. All reported differences in evaluation scores are statistically significant. 404 tively, and (4) applies the Student’s t-test at a significance level of 95% confidence to test whether the score differences are significant. In addition, </context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting Bleu/NIST Scores: How Much Improvement do We Need to Have a Better System? In Proc. of the LREC, pages 2051–2054, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Keiji Yasuda</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Improved Statistical Machine Translation by Multiple Chinese Word Segmentation.</title>
<date>2008</date>
<booktitle>In Proc. of the 3rd Workshop on SMT,</booktitle>
<pages>216--223</pages>
<location>Columbus, USA.</location>
<contexts>
<context position="5445" citStr="Zhang et al., 2008" startWordPosition="822" endWordPosition="825">learns new word types and word distributions suitable for SMT. Similarly, a dynamic programmingbased variational Bayes approach using bilingual information to improve MT is proposed in (Chung and Gildea, 2009). Concerning other languages, for example, (Kikui and Yamamoto, 2002) extended Hidden-Markov-Models, where hidden ngram probabilities were affected by co-occurring words in the target language part for Japanese word segmentation. Recent research on SMT is also focusing on the usage of multiple word segmentation schemes for the source language to improve translation quality. For example, (Zhang et al., 2008) combines dictionary-based and CRF-based approaches for Chinese word segmentation in order to avoid outof-vocabulary (OOV) words. Moreover, the combination of different morphological decomposition of highly inflected languages like Arabic or Finnish is proposed in (de Gispert et al., 2009) to reduce the data sparseness problem of SMT approaches. Similarly, (Nakov et al., 2009) utilizes SMT engines trained on different word segmentation schemes and combines the translation outputs using system combination techniques as a postprocess to SMT decoding. In order to integrate multiple word segmentat</context>
</contexts>
<marker>Zhang, Yasuda, Sumita, 2008</marker>
<rawString>Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 2008. Improved Statistical Machine Translation by Multiple Chinese Word Segmentation. In Proc. of the 3rd Workshop on SMT, pages 216–223, Columbus, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>