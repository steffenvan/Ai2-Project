<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001781">
<title confidence="0.996809">
HMM Revises Low Marginal Probability by CRF
for Chinese Word Segmentation*
</title>
<author confidence="0.99897">
Degen Huang, Deqin Tong, Yanyan Luo
</author>
<affiliation confidence="0.9999675">
Department of Computer Science and Engineering
Dalian University of Technology
</affiliation>
<email confidence="0.987097">
huangdg@dlut.edu.cn, {tongdeqin, ziyanluoyu}@gmail.com
</email>
<sectionHeader confidence="0.997339" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942833333333">
This paper presents a Chinese word
segmentation system for CIPS-SIGHAN
2010 Chinese language processing task.
Firstly, based on Conditional Random
Field (CRF) model, with local features
and global features, the character-based
tagging model is designed. Secondly,
Hidden Markov Models (HMM) is used
to revise the substrings with low marginal
probability by CRF. Finally, confidence
measure is used to regenerate the result
and simple rules to deal with the strings
within letters and numbers. As is well
known that character-based approach has
outstanding capability of discovering
out-of-vocabulary (OOV) word, but ex-
ternal information of word lost. HMM
makes use of word information to in-
crease in-vocabulary (IV) recall. We par-
ticipate in the simplified Chinese word
segmentation both closed and open test
on all four corpora, which belong to dif-
ferent domains. Our system achieves bet-
ter performance.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9962763">
Chinese Word Segmentation (CWS) has wit-
nessed a prominent progress in the first four
SIGHAN Bakeoffs. Since Xue (2003) used
character-based tagging, this method has at-
tracted more and more attention. Some previous
work (Peng et al., 2004; Tseng et al., 2005; Low
et al., 2005) illustrated the effectiveness of using
characters as tagging units, while literatures
(Zhang et al., 2006; Zhao and Kit, 2007a; Zhang
and Clark, 2007) focus on employing lexical
words or subwords as tagging units. Because the
word-based models can capture the word-level
contextual information and IV knowledge. Be-
sides, many strategies are proposed to balance
the IV and OOV performance (Wang et al.,
2008).
CRF has been widely used in sequence label-
ing tasks and has a good performance (Lafferty
et al., 2001). Zhao and Kit (2007b; 2008) at-
tempt to integrate global information with local
information to further improve CRF-based tag-
ging method of CWS, which provides a solid
foundation for strengthening CRF learning with
unsupervised learning outcomes.
In order to increase the accuracy of tagging
using CRF, we adopt the strategy, which is: if the
marginal probability of characters is lower than a
threshold, the modified component based on
HMM will be trigged; combining the confidence
measure the results will be regenerated.
</bodyText>
<sectionHeader confidence="0.979804" genericHeader="method">
2 Our word segmentation system
</sectionHeader>
<bodyText confidence="0.984382375">
In this section, we describe our system in more
details. Three modules are included in our sys-
tem: a basic character-based CRF tagger, HMM
which revises the substrings with low marginal
probability and confidence measure which com-
bines them to regenerate the result. In addition,
we also use some rules to deal with the strings
within letters and numbers.
</bodyText>
<subsectionHeader confidence="0.979384">
2.1 Character-based CRF tagger
</subsectionHeader>
<bodyText confidence="0.988214942857143">
Tag Set A 6-tag set is adopted in our system. It
includes six tags: B, B2, B3, M, E and S. Here,
Tag B and E stand for the first and the last posi-
tion in a multi-character word, respectively. S
stands for a single-character word. B2 and B3
stand for the second and the third position in a
* The work described in this paper is supported by Microsoft Research Asia Funded Project.
multi-character word. M stands for the fourth or
more rear position in a multi-character word
with more than four characters. The 6-tag set is
proved to work more effectively than other tag
sets in improving the segmentation performance
of CRFs by Zhao et al. (2006).
Feature templates In our system, six n-gram
templates, namely, C-1, C0, C1, C-1C0, C0C1,
C-1C1 are selected as features, where C stands for
a character and the subscripts -1, 0 and 1 stand
for the previous, current and next character, re-
spectively. Furthermore, another one is character
type feature template T-1T0T1. We use four
classes of character sets which are predefined as:
class N represents numbers, class L represents
non-Chinese letters, class P represents punctua-
tion labels and class C represents Chinese char-
acters.
Except for the character feature, we also em-
ploy global word feature templates. The basic
idea of using global word information for CWS
is to inform the supervised learner how likely it
is that the subsequence can be a word candidate.
The accessor variety (AV) (Feng et al., 2005) is
opted as global word feature, which is integrated
into CRF successfully in literatures (Zhao and
Kit, 2007b; Zhao and Kit, 2008). The AV value
of a substring s is defined as:
</bodyText>
<equation confidence="0.922975">
AV(s) = min{ Lav (s), Rav (s)} (1)
</equation>
<bodyText confidence="0.9988419">
Where the left and right AV values Lav (s)
and Rav (s) are defined, respectively, as the
number of its distinct predecessors and the
number of its distinct successors.
Multiple feature templates are used to repre-
sent word candidates of various lengths identi-
fied by the AV criterion. Meanwhile, in order to
alleviate the sparse data problem, we follow the
feature function definition for a word candidate
s with a score AV (s) in Zhao and Kit (2008),
</bodyText>
<equation confidence="0.871804">
namely:
t 1
fn (s) = t, 2 ≤ &lt; (2)
( ) 2 t +
AV s
</equation>
<bodyText confidence="0.999743736842105">
In order to improve the efficiency, all candi-
dates longer than five characters are given up.
The AV features of word candidates can’t di-
rectly be utilized to direct CRF learning before
being transferred to the information of characters.
So we only choose the one with the greatest AV
score to activate the above feature function for
that character.
In the open test, we only add another feature
of ‘FRE’, the basic idea of which is if a string
matches a word in an existing dictionary, it may
be a clue that the string is likely a true word.
Then more word boundary information can be
obtained, which may be helpful for CRF learn-
ing on CWS. The dictionary we used is
downloaded from the Internet① and consists of
108,750 words with length of one to four char-
acters. We get FRE features similar to the AV
features.
</bodyText>
<subsectionHeader confidence="0.790752">
2.2 HMM revises substrings with low mar-
ginal probability
</subsectionHeader>
<bodyText confidence="0.999673571428571">
The MP (short for marginal probability) of each
character labeled with one of the six tags can be
got separately through the basic CRF tagger. Here,
B replaces ‘B’ and ‘S’ , and I represents other
tags (‘B2’, ‘B3’, ‘M’, ‘E’). So each character has
corresponding new MP as defined in formula (3)
and (4).
</bodyText>
<equation confidence="0.949112125">
( S
P P
+ B) (3)
=∑ Pt
( + P+P +P
PB2B3ME )
∑ Pt
Where t∈ { S,B,B2,B3,M,E} and Pt can be
</equation>
<bodyText confidence="0.982679">
calculated by using forward-backward algorithm
and more details are in Lafferty et al. (2001).
A low confident word refers to a word with
word boundary ambiguity which can be reflected
by the MP of the first character of a word. That
is, it’s a low confident word if the MP of the first
character of the word is lower than a threshold
β (it’s an empirical value and can be obtained
by experiments). After getting the new MP, all
these low confident candidate words are recom-
bined with their direct predecessors until the
occurrence of a word that the MP of its first
character is above the threshold β , and then a
new substring is generated for post processing.
Then, we use class-based HMM to re-segment
the substrings mentioned above. Given a word
</bodyText>
<footnote confidence="0.882465">
①http://ccl.pku.edu.cn/doubtfire/Course/Chinese%20Inform
ation%20Processing/Source_Code/Chapter_8/Lexicon_full.
zip
</footnote>
<equation confidence="0.41964325">
PB
PI
=
(4)
</equation>
<bodyText confidence="0.7138785">
wi, a word class ci is the word itself. Let W be
the word sequence, let C be its class sequence,
W be the segmentation result with the
#
maximum likelihood. Then, a class-based HMM
model (Liu, 2004) can be got.
</bodyText>
<equation confidence="0.999789071428571">
W =
# arg max ( )
P W
W
= arg max (  |) ( )
P W C P C
W
m
= arg max ∏ p &apos;(wi  |ci)P(ci  |ci−1)
w w w i
1 2 ... m = 1
m
= arg max ∏ P(ci  |ci−1) (5)
w1w2...wm i=1
</equation>
<bodyText confidence="0.978943">
Where P(ci  |ci−1) indicates the transitive
probability from one class to another and it can
be obtained from training corpora.
The word boundary of results from HMM is
also represented by tag ‘B’ and ‘I’ which mean-
ing are the same as mentioned in above.
</bodyText>
<subsectionHeader confidence="0.719201">
2.3 Confidence measure and post process-
</subsectionHeader>
<bodyText confidence="0.976423142857143">
ing for final result
There are two segmentation results for substrings
with low MP candidates after reprocessing using
HMM. Analyzing experiments data, we find
wrong tags labeled by CRF are mainly: OOV
words in test data, IV words and incorrect words
recognized by CRF. Rectifying the tags with
lower MP simply may produce an even worse
performance in some case. For example, some
OOV words are recognized correctly by CRF but
with low MP. So, we can’t accept the revised
results completely. A confidence measure ap-
proach is used to resolve this problem. Its calcu-
lation is defined as:
</bodyText>
<equation confidence="0.9543405">
PC = PCo + λ − PCo (6)
(1 )
</equation>
<bodyText confidence="0.9998472">
PCo is the MP of the character as ‘I’, λ is the
premium coefficient. Based on the new value, a
threshold t was used, if the value was lower
than t , the original tag ‘I’ will be rejected and
changed into the tag ‘B’ which is labeled by
HMM.
At last, we use a simple rule to post-process the
result directed at the strings that containing letters,
numbers and punctuations. If the punctuation (not
all punctuations) is half-width and the string be-
fore or after are composed of letters and numbers,
combine all into a string as a whole. For an ex-
ample, ‘.’, ‘/’, ‘:’, ‘%’ and ‘\’ are usually recog-
nized as split tokens. So, it needs handling addi-
tionally.
</bodyText>
<sectionHeader confidence="0.977261" genericHeader="method">
3 Experiments results and analysis
</sectionHeader>
<bodyText confidence="0.9776655">
We evaluate our system on the corpora given by
CIPS-SIGHAN 2010. There are four test corpora
which belong to different domains. The details
are showed in table 1.
</bodyText>
<table confidence="0.9981568">
Domain Testing Data OOV rate
A 149K 0.069
B 165K 0.152
C 151K 0.110
D 157K 0.087
</table>
<tableCaption confidence="0.999846">
Table 1. Test corpora details
</tableCaption>
<bodyText confidence="0.997979666666667">
A, B, C and D represent literature, computer
science, medical science and finance, respec-
tively.
</bodyText>
<subsectionHeader confidence="0.99511">
3.1 Closed test
</subsectionHeader>
<bodyText confidence="0.999666555555556">
The rule for the closed test in Bakeoff is that no
additional information beyond training corpora is
allowed. Following the rule, the closed test is
designed to compare our system with other CWS
systems. Five metrics of SIGHAN Bakeoff are
used to evaluate the segmentation results: F-score
(F), recall (R), precision (P), the recall on IV
words (RIV) and the recall on OOV words (Roov).
The closed test results are presented in table 2.
</bodyText>
<table confidence="0.999054">
Domain R P F Roov R②IV
A 0.932 0.936 0.934 0.662 0.952
0.940 0.942 0.941 0.649 0.961
B 0.950 0.948 0.949 0.831 0.971
0.953 0.950 0.951 0.827 0.975
C 0.934 0.932 0.933 0.751 0.957
0.942 0.936 0.939 0.750 0.965
D 0.955 0.957 0.956 0.837 0.966
0.959 0.960 0.959 0.827 0.972
</table>
<tableCaption confidence="0.999958">
Table 2. Evaluation closed results on all data sets
</tableCaption>
<footnote confidence="0.797066333333333">
② In order to analyze our results, we got value of RIV from
the organizers because it can’t be obtained from the scoring
system on http://nlp.ict.ac.cn/demo/CIPS-SIGHAN2010/#.
</footnote>
<bodyText confidence="0.955181571428571">
and let
In each domain, the first line shows the results
of our basic CRF segmenter and the second one
shows the final results dealt with HMM through
confidence measure, which make it clear that
using the confidence measure can improve the
overall F-score by increasing value of R and P.
</bodyText>
<table confidence="0.999921538461538">
Domain ID R P F Roov RIV
A 5 0.945 0.946 0.946 0.816 0.954
our 0.940 0.942 0.941 0.649 0.961
12 0.937 0.937 0.937 0.652 0.958
B our 0.953 0.950 0.951 0.827 0.975
11 0.948 0.945 0.947 0.853 0.965
12 0.941 0.940 0.940 0.757 0.974
C our 0.942 0.936 0.939 0.750 0.965
18 0.937 0.934 0.936 0.761 0.959
5 0.940 0.928 0.934 0.761 0.962
D our 0.959 0.960 0.959 0.827 0.972
12 0.957 0.956 0.957 0.813 0.971
9 0.956 0.955 0.956 0.857 0.965
</table>
<tableCaption confidence="0.999805">
Table 3. Comparison our closed results with the top three in all test sets
</tableCaption>
<bodyText confidence="0.999941">
Next, we compare it with other top three sys-
tems. From the table 3 we can see that our system
achieves better performance on closed test. In
contrast, the values of RIV of our method are su-
perior to others’, which contributes to the model
we use. Whether the features of AV for charac-
ter-based CRF tagger or HMM revising, they all
make good use of word information of training
corpora.
</bodyText>
<subsectionHeader confidence="0.999381">
3.2 Open test
</subsectionHeader>
<bodyText confidence="0.985661944444444">
In the open test, the only additional source we
use is the dictionary mentioned above. We get
one first and two third best. Our result is showed
in table 4. Compared with closed test, the value
of RIV is increased in all test corpora. But we
only get the higher value of F in domain of lit-
erature. The reasons will be analyzed as follows:
In the open test, the OOV words are split into
pieces because our model may be more depend-
ent on the dictionary information. Consequently,
we get higher value of R but lower P. The train-
ing corpora are the same as closed test, but it is
different that FRE features are added. The addi-
tional features enhance the original information
of IV words, so the value of RIV is improved to
some extent. However, they have side effects for
OOV segmentation. We will continue to solve
this problem in the future work.
</bodyText>
<table confidence="0.999200444444444">
Domain R P F Roov RIV
A 0.956 0.947 0.952 0.636 0.980
0.958 0.953 0.955 0.655 0.981
B 0.943 0.921 0.932 0.716 0.985
0.948 0.929 0.939 0.735 0.986
C 0.947 0.915 0.931 0.659 0.983
0.951 0.92 0.935 0.67 0.986
D 0.962 0.948 0.955 0.760 0.981
0.964 0.95 0.957 0.763 0.983
</table>
<tableCaption confidence="0.999889">
Table 4. Evaluation open results on all test sets
</tableCaption>
<sectionHeader confidence="0.993173" genericHeader="conclusions">
4 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.9999527">
In this paper, a detailed description on a Chinese
segmentation system is presented. Based on
intermediate results from a CRF tagger, which
employs local features and global features, we
use class-based HMM to revise the substrings
with low marginal probabilities. Then, a confi-
dence measure is introduced to combine the two
results. Finally, we post process the strings
within letters, numbers and punctuations using
simple rules. The results above show that our
system achieves the state-of-the-art performance.
The MP plays the important role in our method
and HMM revises some errors identified by CRF.
Besides, the word features are proved to be in-
formative cues in obtaining high quality MP.
Therefore, our future work will focus on how to
make CRF generate more reliable MP of char-
acters, including exploring other word informa-
tion or more unsupervised segmentation infor-
mation.
</bodyText>
<sectionHeader confidence="0.998355" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998889969230769">
Feng Haodi, Kang Chen, Chuyu Kit, Xiaotie Deng.
2005. Unsupervised segmentation of Chinese cor-
pus using accessor variety, In: Natural Language
Processing IJCNLP, pages 694-703, Sanya, China.
Lafferty John, Andrew McCallum and Fernando
Pereira. 2001. Conditional Random Fields: prob-
abilistic models for segmenting and labeling se-
quence data, In: Proceedings of ICML-18, pages
282-289, Williams College, USA.
Liu Qun, Huaping Zhang, Hongkui Yu and Xueqi
Chen. 2004. Chinese lexical analysis using cas-
caded Hidden Markov Model, Journal of computer
research and development 41(8): 1421-1429.
Low Kiat Jin, Hwee Tou Ng and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word
Segmentation. In: Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Proc-
essing, pages 161-164, Jeju Island, Korea.
Peng Fuchun, Fangfang Feng and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using Conditional Random Fields, In: COL-
ING 2004, pages 562-568, Geneva, Switzerland.
Tseng Huihsin, Pichuan Chang et al. 2005. A Condi-
tional Random Field Word Segmenter for SIGHAN
Bakeoff 2005. In: Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Proc-
essing, pages 168-171, Jeju Island, Korea.
Wang Zhenxing, Changning Huang and Jingbo Zhu.
2008. Which perform better on in-vocabulary word
segmentation: based on word or character? In:
Processing of the Sixth SIGHAN Workshop on
Chinese Language Processing, pages 61-68, Hy-
derabad, India.
Xue Nianwen. 2003. Chinese word segmentation as
character tagging, Computational Linguistics and
Chinese Language Processing 8(1): 29-48.
Zhang Yue and Stephen Clark. 2007. Chinese Seg-
mentation with a Word-Based Perceptron Algo-
rithm. In: Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics,
pages 840-847, Prague, Czech Republic.
Zhang Ruiqiang, Genichiro Kikui and Eiichiro Sumita.
2006. Subword-based tagging by Conditional
Random Fields for Chinese word segmentation, In:
Proceedings of the Human Language
Technology Conference of the NAACL, pages
193-196, New York, USA.
Zhao Hai, Changning Huang, Mu Li and Baoliang Lu.
2006. Effective tag set selection in Chinese word
segmentation via Conditional Random Field mod-
eling, In: PACLIC-20, pages 87-94, Wuhan, China.
Zhao Hai and Chunyu Kit. 2007a. Effective subse-
quence based tagging for Chinese word segmenta-
tion, Journal of Chinese Information Processing
21(5): 8-13.
Zhao Hai and Chunyu Kit. 2007b. Incorporating
global information into supervised learning for
Chinese word segmentation, In: PACLING-2007,
pages 66-74, Melbourne, Australia.
Zhao Hai and Chunyu Kit. 2008. Unsupervised seg-
mentation helps supervised learning of character
tagging for word segmentation and named entity
recognition, In: Proceedings of the Six SIGHAN
Workshop on Chinese Language Processing, pages
106-111, Hyderabad, India.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.596149">
<title confidence="0.900513">HMM Revises Low Marginal Probability by Chinese Word</title>
<author confidence="0.994989">Degen Huang</author>
<author confidence="0.994989">Deqin Tong</author>
<author confidence="0.994989">Yanyan</author>
<affiliation confidence="0.9989335">Department of Computer Science and Dalian University of</affiliation>
<email confidence="0.99473">huangdg@dlut.edu.cn,{tongdeqin,ziyanluoyu}@gmail.com</email>
<abstract confidence="0.98290608">This paper presents a Chinese word segmentation system for CIPS-SIGHAN 2010 Chinese language processing task. Firstly, based on Conditional Random Field (CRF) model, with local features and global features, the character-based tagging model is designed. Secondly, Hidden Markov Models (HMM) is used to revise the substrings with low marginal probability by CRF. Finally, confidence measure is used to regenerate the result and simple rules to deal with the strings within letters and numbers. As is well known that character-based approach has outstanding capability of discovering out-of-vocabulary (OOV) word, but external information of word lost. HMM makes use of word information to increase in-vocabulary (IV) recall. We participate in the simplified Chinese word segmentation both closed and open test on all four corpora, which belong to different domains. Our system achieves better performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Feng Haodi</author>
<author>Kang Chen</author>
<author>Chuyu Kit</author>
<author>Xiaotie Deng</author>
</authors>
<title>Unsupervised segmentation of Chinese corpus using accessor variety, In: Natural Language Processing IJCNLP,</title>
<date>2005</date>
<pages>694--703</pages>
<location>Sanya, China.</location>
<marker>Haodi, Chen, Kit, Deng, 2005</marker>
<rawString>Feng Haodi, Kang Chen, Chuyu Kit, Xiaotie Deng. 2005. Unsupervised segmentation of Chinese corpus using accessor variety, In: Natural Language Processing IJCNLP, pages 694-703, Sanya, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lafferty John</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: probabilistic models for segmenting and labeling sequence data, In:</title>
<date>2001</date>
<booktitle>Proceedings of ICML-18,</booktitle>
<pages>282--289</pages>
<location>Williams College, USA.</location>
<marker>John, McCallum, Pereira, 2001</marker>
<rawString>Lafferty John, Andrew McCallum and Fernando Pereira. 2001. Conditional Random Fields: probabilistic models for segmenting and labeling sequence data, In: Proceedings of ICML-18, pages 282-289, Williams College, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liu Qun</author>
<author>Huaping Zhang</author>
<author>Hongkui Yu</author>
<author>Xueqi Chen</author>
</authors>
<title>Chinese lexical analysis using cascaded Hidden Markov Model,</title>
<date>2004</date>
<journal>Journal of computer research and development</journal>
<volume>41</volume>
<issue>8</issue>
<pages>1421--1429</pages>
<marker>Qun, Zhang, Yu, Chen, 2004</marker>
<rawString>Liu Qun, Huaping Zhang, Hongkui Yu and Xueqi Chen. 2004. Chinese lexical analysis using cascaded Hidden Markov Model, Journal of computer research and development 41(8): 1421-1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Low Kiat Jin</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A Maximum Entropy Approach to Chinese Word Segmentation. In:</title>
<date>2005</date>
<booktitle>Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<location>Jeju Island,</location>
<marker>Jin, Ng, Guo, 2005</marker>
<rawString>Low Kiat Jin, Hwee Tou Ng and Wenyuan Guo. 2005. A Maximum Entropy Approach to Chinese Word Segmentation. In: Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 161-164, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Fuchun</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using Conditional Random Fields, In: COLING</title>
<date>2004</date>
<pages>562--568</pages>
<location>Geneva, Switzerland.</location>
<marker>Fuchun, Feng, McCallum, 2004</marker>
<rawString>Peng Fuchun, Fangfang Feng and Andrew McCallum. 2004. Chinese segmentation and new word detection using Conditional Random Fields, In: COLING 2004, pages 562-568, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tseng Huihsin</author>
<author>Pichuan Chang</author>
</authors>
<title>A Conditional Random Field Word Segmenter for SIGHAN Bakeoff</title>
<date>2005</date>
<booktitle>In: Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>168--171</pages>
<location>Jeju Island,</location>
<marker>Huihsin, Chang, 2005</marker>
<rawString>Tseng Huihsin, Pichuan Chang et al. 2005. A Conditional Random Field Word Segmenter for SIGHAN Bakeoff 2005. In: Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 168-171, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Zhenxing</author>
<author>Changning Huang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Which perform better on in-vocabulary word segmentation: based on word or character? In:</title>
<date>2008</date>
<booktitle>Processing of the Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>61--68</pages>
<location>Hyderabad, India.</location>
<marker>Zhenxing, Huang, Zhu, 2008</marker>
<rawString>Wang Zhenxing, Changning Huang and Jingbo Zhu. 2008. Which perform better on in-vocabulary word segmentation: based on word or character? In: Processing of the Sixth SIGHAN Workshop on Chinese Language Processing, pages 61-68, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xue Nianwen</author>
</authors>
<title>Chinese word segmentation as character tagging,</title>
<date>2003</date>
<journal>Computational Linguistics and Chinese Language Processing</journal>
<volume>8</volume>
<issue>1</issue>
<pages>29--48</pages>
<marker>Nianwen, 2003</marker>
<rawString>Xue Nianwen. 2003. Chinese word segmentation as character tagging, Computational Linguistics and Chinese Language Processing 8(1): 29-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhang Yue</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese Segmentation with a Word-Based Perceptron Algorithm. In:</title>
<date>2007</date>
<booktitle>Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>840--847</pages>
<location>Prague, Czech Republic.</location>
<marker>Yue, Clark, 2007</marker>
<rawString>Zhang Yue and Stephen Clark. 2007. Chinese Segmentation with a Word-Based Perceptron Algorithm. In: Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 840-847, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhang Ruiqiang</author>
</authors>
<title>Genichiro Kikui and Eiichiro Sumita.</title>
<date>2006</date>
<booktitle>Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>193--196</pages>
<location>New York, USA.</location>
<marker>Ruiqiang, 2006</marker>
<rawString>Zhang Ruiqiang, Genichiro Kikui and Eiichiro Sumita. 2006. Subword-based tagging by Conditional Random Fields for Chinese word segmentation, In: Proceedings of the Human Language Technology Conference of the NAACL, pages 193-196, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhao Hai</author>
<author>Changning Huang</author>
<author>Mu Li</author>
<author>Baoliang Lu</author>
</authors>
<title>Effective tag set selection in Chinese word segmentation via Conditional Random Field modeling, In:</title>
<date>2006</date>
<booktitle>PACLIC-20,</booktitle>
<pages>87--94</pages>
<location>Wuhan, China.</location>
<marker>Hai, Huang, Li, Lu, 2006</marker>
<rawString>Zhao Hai, Changning Huang, Mu Li and Baoliang Lu. 2006. Effective tag set selection in Chinese word segmentation via Conditional Random Field modeling, In: PACLIC-20, pages 87-94, Wuhan, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhao Hai</author>
<author>Chunyu Kit</author>
</authors>
<title>Effective subsequence based tagging for Chinese word segmentation,</title>
<date>2007</date>
<journal>Journal of Chinese Information Processing</journal>
<volume>21</volume>
<issue>5</issue>
<pages>8--13</pages>
<marker>Hai, Kit, 2007</marker>
<rawString>Zhao Hai and Chunyu Kit. 2007a. Effective subsequence based tagging for Chinese word segmentation, Journal of Chinese Information Processing 21(5): 8-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhao Hai</author>
<author>Chunyu Kit</author>
</authors>
<title>Incorporating global information into supervised learning for Chinese word segmentation, In:</title>
<date>2007</date>
<booktitle>PACLING-2007,</booktitle>
<pages>66--74</pages>
<location>Melbourne, Australia.</location>
<marker>Hai, Kit, 2007</marker>
<rawString>Zhao Hai and Chunyu Kit. 2007b. Incorporating global information into supervised learning for Chinese word segmentation, In: PACLING-2007, pages 66-74, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhao Hai</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition, In:</title>
<date>2008</date>
<booktitle>Proceedings of the Six SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>106--111</pages>
<location>Hyderabad, India.</location>
<marker>Hai, Kit, 2008</marker>
<rawString>Zhao Hai and Chunyu Kit. 2008. Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition, In: Proceedings of the Six SIGHAN Workshop on Chinese Language Processing, pages 106-111, Hyderabad, India.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>