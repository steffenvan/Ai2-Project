<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.971911">
Affinity Measures based on the Graph Laplacian
</title>
<author confidence="0.994963">
Delip Rao
</author>
<affiliation confidence="0.927684">
Dept. of Computer Science
Johns Hopkins University
</affiliation>
<email confidence="0.997865">
delip@cs.jhu.edu
</email>
<author confidence="0.99219">
David Yarowsky
</author>
<affiliation confidence="0.927369">
Dept. of Computer Science
Johns Hopkins University
</affiliation>
<email confidence="0.998136">
yarowsky@cs.jhu.edu
</email>
<author confidence="0.979754">
Chris Callison-Burch
</author>
<affiliation confidence="0.9266135">
Dept. of Computer Science
Johns Hopkins University
</affiliation>
<email confidence="0.998594">
ccb@cs.jhu.edu
</email>
<sectionHeader confidence="0.994796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999871666666667">
Several language processing tasks can be
inherently represented by a weighted graph
where the weights are interpreted as a
measure of relatedness between two ver-
tices. Measuring similarity between ar-
bitary pairs of vertices is essential in solv-
ing several language processing problems
on these datasets. Random walk based
measures perform better than other path
based measures like shortest-path. We
evaluate several random walk measures
and propose a new measure based on com-
mute time. We use the psuedo inverse
of the Laplacian to derive estimates for
commute times in graphs. Further, we
show that this pseudo inverse based mea-
sure could be improved by discarding the
least significant eigenvectors, correspond-
ing to the noise in the graph construction
process, using singular value decomposi-
tion.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999823">
Natural language data lend themselves to a graph
based representation. Words could be linked by
explicit relations as in WordNet (Fellbaum, 1989)
or documents could be linked to one another via
hyperlinks. Even in the absence of such a straight-
forward representation it is possible to derive
meaningful graphs such as the nearest neighbor
graphs as done in certain manifold learning meth-
ods (Roweis and Saul, 2000; Belkin and Niyogi,
</bodyText>
<note confidence="0.727667">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.984791">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<listItem confidence="0.956732818181818">
2001). All of these graphs share the following
properties:
• They are edge-weighted.
• The edge weight encodes some notion of re-
latedness between the vertices.
• The relation represented by edges is at least
weakly transitive. Examples of such rela-
tions include, “is similar to”, “is more general
than”, and so on. It is important that the re-
lations selected are transitive for the random
walk to make sense.
</listItem>
<bodyText confidence="0.999618">
Such graphs present several possibilities in solv-
ing language problems on the data. One such task
is, given two vertices in the graph we would like
to know how related the two vertices are. There
is an abundance of literature on this topic, some
of which will be reviewed here. Finding similarity
between vertices in a graph could be an end in it-
self, as in the lexical similarity task, or could be a
stage before solving other problems like clustering
and classification.
</bodyText>
<sectionHeader confidence="0.791344" genericHeader="introduction">
2 Contributions of this paper
</sectionHeader>
<bodyText confidence="0.960708">
The major contributions of this paper are
</bodyText>
<listItem confidence="0.996459142857143">
• A comprehensive evaluation of various ran-
dom walk based measures
• Propose a new similarity measure based on
commute time.
• An improvement to the above measure by
eliminating noisy features via singular value
decomposition.
</listItem>
<page confidence="0.988713">
41
</page>
<note confidence="0.673767">
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 41–48
</note>
<keyword confidence="0.258619">
Manchester, August 2008
</keyword>
<sectionHeader confidence="0.938838" genericHeader="method">
3 Problem setting
</sectionHeader>
<bodyText confidence="0.999801714285714">
Consider an undirected graph G(V, E, W) with
vertices V , edges E, and W = [wzj] be the sym-
metric adjacency weight matrix with wzj as the
weight of the edge connecting vertices i and j. The
weight, wzj = 0 for vertices i and j that are not
neighbors and when wzj &gt; 0 it is interpreted as an
indication of relatedness between i and j. In our
case, we consider uniformly weighted graphs, i.e,
wzj = 1 for neighbors but this need not be the case.
Let n = |V  |be the order of the graph. We define
a relation sim : V x V —* R+ such that sim(i, j)
is the relatedness between vertices i and j. There
are several ways to define sim; the ones explored
in this paper are:
</bodyText>
<listItem confidence="0.999495692307692">
• simG(i, j) is the reciprocal of the shortest
path length between vertices i and j. Note
that this is not a random walk based mea-
sure but a useful baseline for comparison pur-
poses.
• simB(i, j) is the probability of a random walk
from vertex i to vertex j using all paths of
length less than m.
• simP(i, j) is the probability of a random walk
from vertex i to vertex j defined via a pager-
ank model.
• simC(i, j) is a function of the commute time
between vertex i and vertex j.
</listItem>
<sectionHeader confidence="0.69533" genericHeader="method">
4 Data and Evaluation
</sectionHeader>
<bodyText confidence="0.999982411764706">
We evaluate each of the similarity measure we
consider by using a linguistically motivated task
of finding lexical similarity. Deriving lexical
relatedness between terms has been a topic of
interest with applications in word sense disam-
biguation (Patwardhan et al., 2005), paraphras-
ing (Kauchak and Barzilay, 2006), question an-
swering (Prager et al., 2001), and machine trans-
lation (Blatz et al., 2004) to name a few. Lex-
ical relatedness between terms could be derived
either from a thesaurus like WordNet or from
raw monolingual corpora via distributional simi-
larity (Pereira et al., 1993). WordNet is an inter-
esting graph-structured thesaurus where the ver-
tices are the words and the edges represent rela-
tions between the words. For the purpose of this
work, we only consider relations like hypernymy,
hyponymy, and synonymy. The importance of this
problem has generated copious literature in the
past – see (Pedersen et al., 2004) or (Budanitsky
and Hirst, 2006) for a detailed review of various
lexical relatedness measures on WordNet. Our fo-
cus in this paper is not to derive the best similar-
ity measure for WordNet but to use WordNet and
the lexical relatedness task as a method to evalu-
ate the various random walk based similarity mea-
sures. Following the tradition in previous litera-
ture we evaluate on the Miller and Charles (1991)
dataset. This data consists of 30 word-pairs along
with human judgements which is a real value be-
tween 1 and 4. For every measure we consider,
we derive similarity scores and compare with the
human judgements using the Spearman rank cor-
relation coefficient.
</bodyText>
<sectionHeader confidence="0.982483" genericHeader="method">
5 Graph construction
</sectionHeader>
<bodyText confidence="0.99986475">
For the purpose of evaluation of the random walk
measures, we construct a graph for every pair of
words for which similarity has to be computed.
This graph is derived from WordNet as follows:
</bodyText>
<listItem confidence="0.9651806">
• For each word w in the pair (w1, w2):
– Add an edge between w and all of its
parts of speech. For example, if the word
is coast, add edges between coast and
coast#noun and coast#verb.
</listItem>
<bodyText confidence="0.89179325">
– For each word#pos combination,
add edges to all of its senses (For
example, coast#noun#1 through
coast#noun#4.
– For each word sense, add edges to all of
its hyponyms
– For each word sense, add edges to all of
its hypernyms recursively.
In this paper we consider uniform weights on all
edges as our main aim is to illustrate the differ-
ent random walk measures rather than fine tune the
graph construction process.
</bodyText>
<sectionHeader confidence="0.876664" genericHeader="method">
6 Shortest path based measure
</sectionHeader>
<bodyText confidence="0.999912625">
The most obvious measure of distance in a graph is
the shortest path between the vertices which is de-
fined as the minimum number of intervening edges
between two vertices. This is also known as the
geodesic distance. To convert this distance mea-
sure to a similarity measure, we take the recipro-
cal of the shortest-path length. We refer to this as
the geodesic similarity. This is not a random walk
</bodyText>
<page confidence="0.998504">
42
</page>
<figureCaption confidence="0.999813">
Figure 1: Shortest path distances on graphs
</figureCaption>
<bodyText confidence="0.642774">
measure but will serve as an important baseline for
our work. As can be observed from Table 1, the
</bodyText>
<table confidence="0.8686555">
Method Spearman correlation
Geodesic 0.275
</table>
<tableCaption confidence="0.998782">
Table 1: Similarity using shortest-path measure.
</tableCaption>
<bodyText confidence="0.993923">
correlation is rather poor for the shortest path mea-
sure.
</bodyText>
<sectionHeader confidence="0.622179" genericHeader="method">
7 Why are shortest path distances bad?
</sectionHeader>
<bodyText confidence="0.9997725">
While shortest-path distances are useful in many
applications, it fails to capture the following obser-
vation. Consider the subgraph of WordNet shown
in Figure 1. The term moon is connected to the
terms religious leader and satellite1.
Observe that both religious leader and
satellite are at the same shortest path dis-
tance from moon. However, the connectivity
structure of the graph would suggest satellite
to be “more” similar than religious leader
as there are multiple senses, and hence multiple
paths, connecting satellite and moon.
Thus it is desirable to have a measure that cap-
tures not only path lengths but also the connectiv-
ity structure of the graph. This notion is elegantly
captured using random walks on graphs.
</bodyText>
<subsectionHeader confidence="0.998225">
7.1 Similarity via Random walks
</subsectionHeader>
<bodyText confidence="0.999971375">
A random walk is a stochastic process that consists
of a sequence of discrete steps taken at random de-
fined by a distribution. Random walks have inter-
esting connections to Brownian motion, heat diffu-
sion and have been used in semi-supervised learn-
ing – for example, see (Zhu et al., 2003). Certain
properties of random walks are defined for ergodic
processes only2. In our work, we assume these
</bodyText>
<footnote confidence="0.98434925">
1The religious leader sense of moon is due to Sun
Myung Moon, a US religious leader.
2A stochastic process is ergodic if the underlying Markov
chain is irreducible and aperiodic. A Markov chain is irre-
</footnote>
<bodyText confidence="0.9505885">
hold true as the graphs we deal with are connected,
undirected, and non-bipartite.
</bodyText>
<subsectionHeader confidence="0.829859">
7.1.1 Bounded length walks
</subsectionHeader>
<bodyText confidence="0.9992222">
As our first random walk measure, we consider
the bounded length walk – i.e., all random walks of
length less than or equal to a bound m. We derive
a probability transition matrix P from the weight
matrix W as follows:
</bodyText>
<equation confidence="0.992558">
P = D−sW
</equation>
<bodyText confidence="0.606046">
where, D is a diagonal matrix with dii =
Enj = 1 wij. Observe that:
</bodyText>
<listItem confidence="0.987375">
• pij = P[i, j] ? 0, and
• �nj = 1 pij = 1
</listItem>
<bodyText confidence="0.9994386">
Hence pij can be interpreted as the probability
of transition from vertex i to vertex j in one step. It
is easy to observe that Pk gives the transition prob-
ability from vertex i to vertex j in k steps. This
leads to the following similarity measure:
</bodyText>
<equation confidence="0.992935">
S = P + P2 + P3 + ... + Pm
</equation>
<bodyText confidence="0.98704125">
Observe that S[i, j] derives the total probability of
transition from vertex i to vertex j in at most m
steps3. Given S, we can derive several measures of
similarity:
</bodyText>
<listItem confidence="0.998817">
1. Bounded Walk: S[i, j]
2. Bounded Walk Cosine: dot product of
rowvectors Si and Sj.
</listItem>
<bodyText confidence="0.999780666666667">
When we evaluate these measures on the Miller-
Charles data the results shown in Table 2. are ob-
served. For this experiment, we consider all walks
that are at most 20 steps long, i.e., m = 20. Ob-
serve that these results are significantly better than
the Geodesic similarity based on shortest-paths.
</bodyText>
<footnote confidence="0.99124725">
ducible if there exists a path between any two states and it is
aperiodic if the GCD of all cycle lengths is one.
3The matrix S is row normalized to ensure that the entries
can be interpreted as probabilities.
</footnote>
<page confidence="0.997781">
43
</page>
<table confidence="0.997888666666667">
Method Spearman correlation
Bounded Walk 0.346
Bounded Walk Cosine 0.365
</table>
<tableCaption confidence="0.981773">
Table 2: Similarity using bounded random walks
</tableCaption>
<bodyText confidence="0.391143">
(m = 20).
</bodyText>
<subsectionHeader confidence="0.321">
7.1.2 How many paths are sufficient?
</subsectionHeader>
<bodyText confidence="0.9890866">
In the previous experiment, we arbitrarily fixed
m = 20. However, as observed in Figure 2. , be-
yond a certain value the choice of m does not affect
the result as the random walk converges to its sta-
tionary distribution. The choice of m depends on
</bodyText>
<figureCaption confidence="0.998455">
Figure 2: Effect of m in Bounded walk
</figureCaption>
<bodyText confidence="0.997982428571428">
the amount of computation available. A reason-
ably large value of m (m &gt; 10) should be suffi-
cient for most purposes and one could use lower
values of m to derive an approximation for this
measure. One could derive an upper bound on the
value of m using the mixing time of the underlying
Markov chain (Aldous and Fill, 2001).
</bodyText>
<subsectionHeader confidence="0.676021">
7.1.3 Similarity via pagerank
</subsectionHeader>
<bodyText confidence="0.999011">
Pagerank (Page et al., 1998) is the celebrated ci-
tation ranking algorithm that has been applied to
several natural language problems from summa-
rization (Erkan and Radev, 2004) to opinion min-
ing (Esuli and Sebastiani, 2007) to our task of
lexical relatedness (Hughes and Ramage, 2007).
Pagerank is yet another random walk model with a
difference that it allows the random walk to “jump”
to its initial state with a nonzero probability (α).
Given the probability transition matrix P as defined
above, a stationary distribution vector for any ver-
tex (say i) could be derived as follows:
</bodyText>
<listItem confidence="0.981026714285714">
1. Let ei be a vector of all zeros with ei(i) = 1
2. Let v0 = ei
3. Repeat until �vt − vt_111F &lt; 2
• vt+1 = αvtP + (1 − α)v0
• t = t + 1
4. Assign vt+1 as the stationary distribution for
vertex i.
</listItem>
<bodyText confidence="0.988282857142857">
Armed with the stationary distribution vectors for
vertices i and j, we define pagerank similarity ei-
ther as the cosine of the stationary distribution vec-
tors or the reciprocal Jensen-Shannon (JS) diver-
gence4 between them. Table 3. shows results on
the Miller-Charles data. We use α = 0.1, the best
value on this data. Observe that these results are
</bodyText>
<table confidence="0.998613">
Method Spearman correlation
Pagerank JS-Divergence 0.379
Pagerank Cosine 0.393
</table>
<tableCaption confidence="0.999754">
Table 3: Similarity via pagerank (α = 0.1).
</tableCaption>
<bodyText confidence="0.999286">
better than the best bounded walk result. We fur-
ther note that our results are different from that
of (Hughes and Ramage, 2007) as they use exten-
sive feature engineering and weight tuning during
the graph generation process that we have not been
able to reproduce. Hence for simplicity we stuck to
a simpler graph generation process. Nevertheless,
the result in Table 3. is still useful as we are in-
terested in the performance of the various spectral
similarity measures rather than achieving the best
performance on the lexical relatedness task. The
graphs we use in all methods are identical making
comparisons across methods possible.
</bodyText>
<subsectionHeader confidence="0.994014">
7.2 Similarity via Hitting Time
</subsectionHeader>
<bodyText confidence="0.999901833333333">
Given a graph with the transition probability ma-
trix P as defined above, the hitting time between
vertices i and j, denoted as h(i, j), is defined as
the expected number of steps taken by a random
walker to first encounter vertex j starting from ver-
tex i. This can be recursively defined as follows:
</bodyText>
<equation confidence="0.980138">
pikh(k, j) if i =� j
h(i, j) =
0 ifi=j
(1)
</equation>
<footnote confidence="0.9670534">
4The Jensen-Shannon divergence between two distribu-
tions p and q is defined as D(p I I a)+D(q I I a), where D(. I I
.) is the Kullback-Liebler divergence and a = (p + q)/2.
Note that unlike KL-divergence this measure is symmetric.
See (Lin, 1991) for additional details.
</footnote>
<equation confidence="0.97684375">
{
�
1 +
k : wik &gt; 0
</equation>
<page confidence="0.982334">
44
</page>
<bodyText confidence="0.999944571428571">
The lower the hitting times of two vertices, the
more similar they are. It can be easily verified
that hitting time is not a symmetric relation hence
graph theory literature suggests another symmet-
ric measure – the commute time.5 The commute
time, c(i, j), is the expected number of steps taken
to leave vertex i, reach vertex j, and return back to
</bodyText>
<equation confidence="0.937158">
i. Thus,
c(i, j) = h(i, j) + h(j, i) (2)
</equation>
<bodyText confidence="0.966562952380952">
Observe that, the commute time is a metric in that
it is positive definite, symmetric, and satisifies tri-
angle inequality. Hence, commute time could be
used as a distance measure as well. We derive a
similarity measure from this distance measure us-
ing the following lemma.
Lemma 1. For every edge (i, j), c(i, j) &lt; 2l
where l = |E|, the number of edges.
Proof. This can be easily observed by defining a
Markov chain on the edges with probability tran-
sition matrix Q with 2l states, such that Qe1e2 =
1/degree(e1 n e2). Since this matrix is doubly
stochastic, the stationary distribution on this chain
will be uniform with a probability 1/2l. Now
c(i, j) = h(i, j)+h(j, i), is the expected time for a
walk to start at i, visit j, and return back to i. When
the stationary probability at each edge is 1/2l, this
expected time evaluates to 2l. Hence the commute
time can be at most 2l.
This lemma allows us to define a similarity mea-
sure as follows:
</bodyText>
<equation confidence="0.990378333333333">
c(i, j)
simC(i,j) = 1 − (3)
2l
</equation>
<bodyText confidence="0.99400646875">
Observe that the measure defined in Equation 3 is
a metric and further its range is defined in [0, 1].
We now only need a way to compute the commute
times to use Equation 3. One could compute the
hitting times and hence the commute times from
the Equations 1 and 2 using dynamic program-
ming, akin to shortest paths in graphs. In this pa-
per, we instead choose to derive commute times
via the graph Laplacian. This also allows us to
handle “noise” in the graph construction process
which cannot be taken care by naive dynamic pro-
gramming.
5Note that distance measures, in general, need not be sym-
metric but we interpret distance as proximity which mandates
symmetry.
Chandra et. al. (1989) show that the commute
time between two vertices is equal to the resis-
tance distance between them. Resistance distance,
as proposed by Klein and Randic (1993), is the
effective resistance between two vertices in the
electrical network represented by the graph, where
the edges have resistance 1/wij. Xiao and Gut-
man (2003), show the relation between resistance
distances in graphs to the Laplacian spectrum, thus
enabling a way to derive commute times from the
graph Laplacian in closed form.
We now introduce graph Laplacians, which are
interesting in their own right besides being related
to commute time. The Laplacian of a graph could
be viewed as a discrete version of the Laplace-
Beltrami operator on Riemannian manifolds. It is
defined as
</bodyText>
<equation confidence="0.742551">
L = D − W
</equation>
<bodyText confidence="0.999948428571429">
The graph Laplacian has interesting properties and
a wide range of applications, in semi-supervised
learning (Zhu et al., 2003), non-linear dimension-
ality reduction (Roweis and Saul, 2000; Belkin and
Niyogi, 2001), and so on. See (Chung, 1997) for
a thorough introduction on Laplacians and their
properties. We depend on the fact that L is:
</bodyText>
<listItem confidence="0.928953">
1. symmetric (since D and W are for undirected
graphs)
2. positive-semidefinite : since it is symmet-
ric, all of the eigenvalues are real and by
the Greshgorin circle theorem, the eigenval-
ues must also be non-negative and hence L is
positive-semidefinite.
</listItem>
<bodyText confidence="0.88617">
Throughout this paper we use normalized Lapla-
cians as defined below:
</bodyText>
<equation confidence="0.991308">
L = D−1/2LD−1/2 = I − D−1/2WD−1/2
</equation>
<bodyText confidence="0.960921833333333">
The normalized Laplacians preserve all properties
of the Laplacian by construction.
As noted in Xiao and Gutman (2003), the re-
sistance distances can be derived from the gener-
alized Moore-Penrose pseudo-inverse of the graph
Laplacian(L†) – also called the inverse Laplacian.
Like Laplacians, their pseudo inverse counterparts
are also symmetric, and positive semi-definite.
Lemma 2. L† is symmetric
Proof. The Moore-Penrose pseudo-inverse is de-
fined as L† = (LTL)−1LT. From this definition,
it is clear that (L†)T = (LT)†. By the symmetry
</bodyText>
<page confidence="0.9982">
45
</page>
<bodyText confidence="0.8324036">
property of graph Laplacians, LT = L. Hence,
(L†)T = L†.
Lemma 3. L† is positive semi-definite
Proof. We make use of the following properties
from (Chung, 1997):
</bodyText>
<listItem confidence="0.977302">
• The Laplacian, L, is positive semi-definite
(also shown above).
• If the Eigen-decomposition of L is QAQT,
then the Eigen-decomposition of the pseudo-
inverse L† is QA−1QT. If any of the eigenval-
ues of L is zero then the corresponding eigen-
value for L† is also zero.
</listItem>
<bodyText confidence="0.942469">
Since L is positive semi-definite, and the eigen-
values of L† have the same sign as L, the pseudo
inverse L† has to be positive semi-definite.
Lemma 4. The inverse Laplacian is a gram matrix
Proof. To prove this, we use the fact that the
Laplacian Matrix is symmetric and positive semi-
definite. Hence by Cholesky decomposition we
can write L = UUT.
</bodyText>
<equation confidence="0.710491">
Therefore L† = (UT)†U† = (U†)T(U†).
Hence L† is a matrix of dot-products or a gram-
matrix.
</equation>
<bodyText confidence="0.8991455">
Thus, from Lemmas 2, 3 and 4, the inverse
Laplacian L† is a valid Kernel.
</bodyText>
<subsectionHeader confidence="0.567557">
7.2.1 Similarity measures from the Laplacian
</subsectionHeader>
<bodyText confidence="0.999851">
The pseudo inverse of the Laplacian allows us
to compute the following similarity measures.
</bodyText>
<listItem confidence="0.970122">
1. Since L† is a kernel, L† ijcan be interpreted a
similarity value of vertices i and j.
2. Commute time: This is due to (Aldous and
Fill, 2001). The commute time, c(i, j) a
(L†ii + L†jj − 2L† ij). This allows us to derive
similarities using Equation 3.
</listItem>
<bodyText confidence="0.99626225">
Evaluating the above measures with the Miller-
Charles data yields results shown in Table 4.
Again, these results are better than the other ran-
dom walk methods compared in the paper.
</bodyText>
<table confidence="0.9915255">
Method Spearman correlation
L† 0.469
ij
Commute Time (simC) 0.520
</table>
<tableCaption confidence="0.999664">
Table 4: Similarity via inverse Laplacian.
</tableCaption>
<subsubsectionHeader confidence="0.46919">
7.2.2 Noise in the graph construction process
</subsubsectionHeader>
<bodyText confidence="0.99982225">
The graph construction process outlined in Sec-
tion 5 is not necessarily the best one. In fact, any
method that constructs graphs from existing data
incorporates “noise” or extraneous features. These
could be spurious edges between vertices, miss-
ing edges, or even improper edge weights. It is
however impossible to know any of this a priori
and some noise is inevitable. The derivation of
commute times via the pseudo inverse of a noisy
Laplacian matrix makes it even worse because the
pseudo inverse amplifies the noise in the original
matrix. This is because the largest singular value
of the pseudo inverse of a matrix is equal to the in-
verse of the smallest singular value of the original
matrix. A standard technique in signal processing
and information retrieval to eliminate noise or han-
dle missing values is to use singular value decom-
position (Deerwester et al., 1990). We apply SVD
to handle noise in the graph construction process.
For a given matrix A, SVD decomposes A into
three matrices U, S, and V such that A = USVT ,
where S is a diagonal matrix of eigenvalues of A,
and U and V are orthonormal matrices containing
the left and the right eigenvectors respectively. The
top-k eigenvectors and eigenvalues are computed
using the iterative method by Lanczos-Arnoldi (us-
ing LAPACK) and the product of these matrices
represents a “smoothed” version of the original
Laplacian. The pseudo inverse is then computed
on this smooth Laplacian. Table 5., shows the im-
provements obtained by discarding bottom 20% of
the eigenvalues.
</bodyText>
<table confidence="0.998662">
Method Original After SVD
L† 0.469 0.472
ij
Commute Time (simC) 0.520 0.542
</table>
<tableCaption confidence="0.999684">
Table 5: Denoising graph Laplacian via SVD
</tableCaption>
<bodyText confidence="0.987920285714286">
Figure 3. shows the dependence on the num-
ber of eigenvalues selected. As can be observed in
both curves there is a reduction in performance by
adding the last few eigenvectors and hence may be
safely discarded. This observation is true in other
text processing tasks like document clustering or
classification using Latent Semantic Indexing.
</bodyText>
<sectionHeader confidence="0.999841" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999849">
Apart from the related work cited throughout this
paper, we would also like to note the paper by Yen
</bodyText>
<page confidence="0.999305">
46
</page>
<figureCaption confidence="0.999168">
Figure 3: Noise reduction via SVD.
</figureCaption>
<bodyText confidence="0.999963375">
et al (2007) on using sigmoid commute time kernel
on a graph for document clustering but our work
differs in that our goal was to study various ran-
dom walk measures rather than a specific task and
we provide a new similarity measure (ref. Eqn
3) based on an upper bound on the commute time
(Lemma 1). Our work also suggests a way to han-
dle noise in the graph construction process.
</bodyText>
<sectionHeader confidence="0.986691" genericHeader="conclusions">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999596875">
This paper presented an evaluation of random
walk based similarity measures on weighted undi-
rected graphs. We provided an intuitive explana-
tion of why random walk based measures perform
better than shortest-path or geodesic measures,
and backed it with empirical evidence. The ran-
dom walk measures we consider include bounded
length walks, pagerank based measures, and a new
measure based on the commute times in graphs.
We derived the commute times via pseudo inverse
of the graph Laplacian. This enables a new method
of graph similarity using SVD that is robust to the
noise in the graph construction process. Further,
the inverse Laplacian is also interesting in that it is
a kernel by itself and could be used for other tasks
like word clustering, for example.
</bodyText>
<sectionHeader confidence="0.996179" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99973925">
The authors would like to thank David Smith and
Petros Drineas for useful discussions and to Fan
Chung for the wonderful book on Spectral Graph
theory.
</bodyText>
<sectionHeader confidence="0.997354" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999689576923077">
Aldous and Fill. 2001. Reversible Markov Chains and
Random Walks on Graphs. In preparation.
Belkin, Mikhail and Partha Niyogi. 2001. Laplacian
eigenmaps and spectral techniques for embedding
and clustering. In Proceedings of the NIPS.
Blatz, John, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto San-
chis, and Nicola Ueffing. 2004. Confidence estima-
tion for machine translation. In Proceeding of the
COLING.
Budanitsky, Alexander and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13–
47.
Chandra, Ashok, Prabhakar Raghavan, Walter Ruzzo,
Roman Smolensky, and Prasoon Tiwari. 1989. The
electrical resistance of a graph captures its commute
and cover times. In Proceedings of the STOC.
Chung, Fan. 1997. Spectral graph theory. In CBMS:
Conference Board of the Mathematical Sciences, Re-
gional Conference Series.
Deerwester, Scott, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41.
Erkan, G¨unes and Dragomir Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal ofArtificial Intelligence Re-
search (JAIR), 22:457–479.
Esuli, Andrea and Fabrizio Sebastiani. 2007. Pager-
anking wordnet synsets: An application to opinion
mining. In Proceedings of the ACL, pages 424–431.
Fellbaum, Christaine, editor. 1989. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Hughes, Thad and Daniel Ramage. 2007. Lexical
semantic relatedness with random graph walks. In
Proceedings of the EMNLP.
Kauchak, David and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
HLT-NAACL.
Klein, D. and M. Randic. 1993. Resistance distance.
Journal of Mathematical Chemistry, 12:81–95.
Lin, Jianhua. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory, 37(1).
Miller, G. and W. Charles. 1991. Contextual correlates
of semantic similarity. In Language and Cognitive
Process.
Page, Larry, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1998. The pagerank citation ranking:
Bringing order to the web. Technical report, Stan-
ford University, Stanford, CA.
</reference>
<page confidence="0.986812">
47
</page>
<reference confidence="0.999498379310345">
Patwardhan, Siddharth, Satanjeev Banerjee, and Ted
Pedersen. 2005. Senserelate:: Targetword-A gen-
eralized framework for word sense disambiguation.
In Proceedings of the ACL.
Pedersen, Ted, Siddharth Patwardhan, and Jason
Michelizzi. 2004. Wordnet::similarity -measuring
the relatedness of concepts. In Proceedings of the
AAAI.
Pereira, Fernando, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the ACL.
Prager, John M., Jennifer Chu-Carroll, and Krzysztof
Czuba. 2001. Use of wordnet hypernyms for an-
swering what-is questions. In Proceedings of the
Text REtrieval Conference.
Roweis, Sam and Lawrence Saul. 2000. Nonlinear di-
mensionality reduction by locally linear embedding.
Science, 290:2323–2326.
Xiao, W. and I. Gutman. 2003. Resistance distance and
laplacian spectrum. Theoretical Chemistry Associa-
tion, 110:284–289.
Yen, Luh, Francois Fouss, Christine Decaestecker, Pas-
cal Francq, and Marco Saerens. 2007. Graph nodes
clustering based on the commute-time kernel. In
Proceedings of the PAKDD.
Zhu, Xiaojin, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of the
ICML.
</reference>
<page confidence="0.999346">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.021710">
<title confidence="0.481686">Affinity Measures based on the Graph Laplacian Delip Dept. of Computer</title>
<author confidence="0.463071">Johns Hopkins</author>
<email confidence="0.999625">delip@cs.jhu.edu</email>
<author confidence="0.864071">David</author>
<affiliation confidence="0.7411975">Dept. of Computer Johns Hopkins</affiliation>
<email confidence="0.999317">yarowsky@cs.jhu.edu</email>
<author confidence="0.893214">Chris</author>
<affiliation confidence="0.6964225">Dept. of Computer Johns Hopkins</affiliation>
<email confidence="0.999565">ccb@cs.jhu.edu</email>
<abstract confidence="0.988941954545455">Several language processing tasks can be inherently represented by a weighted graph where the weights are interpreted as a measure of relatedness between two vertices. Measuring similarity between arbitary pairs of vertices is essential in solving several language processing problems on these datasets. Random walk based measures perform better than other path based measures like shortest-path. We evaluate several random walk measures and propose a new measure based on commute time. We use the psuedo inverse of the Laplacian to derive estimates for commute times in graphs. Further, we show that this pseudo inverse based measure could be improved by discarding the least significant eigenvectors, corresponding to the noise in the graph construction process, using singular value decomposition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aldous</author>
<author>Fill</author>
</authors>
<title>Reversible Markov Chains and Random Walks on Graphs.</title>
<date>2001</date>
<booktitle>In preparation.</booktitle>
<contexts>
<context position="11039" citStr="Aldous and Fill, 2001" startWordPosition="1906" endWordPosition="1909">re sufficient? In the previous experiment, we arbitrarily fixed m = 20. However, as observed in Figure 2. , beyond a certain value the choice of m does not affect the result as the random walk converges to its stationary distribution. The choice of m depends on Figure 2: Effect of m in Bounded walk the amount of computation available. A reasonably large value of m (m &gt; 10) should be sufficient for most purposes and one could use lower values of m to derive an approximation for this measure. One could derive an upper bound on the value of m using the mixing time of the underlying Markov chain (Aldous and Fill, 2001). 7.1.3 Similarity via pagerank Pagerank (Page et al., 1998) is the celebrated citation ranking algorithm that has been applied to several natural language problems from summarization (Erkan and Radev, 2004) to opinion mining (Esuli and Sebastiani, 2007) to our task of lexical relatedness (Hughes and Ramage, 2007). Pagerank is yet another random walk model with a difference that it allows the random walk to “jump” to its initial state with a nonzero probability (α). Given the probability transition matrix P as defined above, a stationary distribution vector for any vertex (say i) could be deri</context>
<context position="18904" citStr="Aldous and Fill, 2001" startWordPosition="3291" endWordPosition="3294">n is a gram matrix Proof. To prove this, we use the fact that the Laplacian Matrix is symmetric and positive semidefinite. Hence by Cholesky decomposition we can write L = UUT. Therefore L† = (UT)†U† = (U†)T(U†). Hence L† is a matrix of dot-products or a grammatrix. Thus, from Lemmas 2, 3 and 4, the inverse Laplacian L† is a valid Kernel. 7.2.1 Similarity measures from the Laplacian The pseudo inverse of the Laplacian allows us to compute the following similarity measures. 1. Since L† is a kernel, L† ijcan be interpreted a similarity value of vertices i and j. 2. Commute time: This is due to (Aldous and Fill, 2001). The commute time, c(i, j) a (L†ii + L†jj − 2L† ij). This allows us to derive similarities using Equation 3. Evaluating the above measures with the MillerCharles data yields results shown in Table 4. Again, these results are better than the other random walk methods compared in the paper. Method Spearman correlation L† 0.469 ij Commute Time (simC) 0.520 Table 4: Similarity via inverse Laplacian. 7.2.2 Noise in the graph construction process The graph construction process outlined in Section 5 is not necessarily the best one. In fact, any method that constructs graphs from existing data incorp</context>
</contexts>
<marker>Aldous, Fill, 2001</marker>
<rawString>Aldous and Fill. 2001. Reversible Markov Chains and Random Walks on Graphs. In preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
</authors>
<title>Laplacian eigenmaps and spectral techniques for embedding and clustering.</title>
<date>2001</date>
<booktitle>In Proceedings of the NIPS.</booktitle>
<contexts>
<context position="16655" citStr="Belkin and Niyogi, 2001" startWordPosition="2905" endWordPosition="2908">tween resistance distances in graphs to the Laplacian spectrum, thus enabling a way to derive commute times from the graph Laplacian in closed form. We now introduce graph Laplacians, which are interesting in their own right besides being related to commute time. The Laplacian of a graph could be viewed as a discrete version of the LaplaceBeltrami operator on Riemannian manifolds. It is defined as L = D − W The graph Laplacian has interesting properties and a wide range of applications, in semi-supervised learning (Zhu et al., 2003), non-linear dimensionality reduction (Roweis and Saul, 2000; Belkin and Niyogi, 2001), and so on. See (Chung, 1997) for a thorough introduction on Laplacians and their properties. We depend on the fact that L is: 1. symmetric (since D and W are for undirected graphs) 2. positive-semidefinite : since it is symmetric, all of the eigenvalues are real and by the Greshgorin circle theorem, the eigenvalues must also be non-negative and hence L is positive-semidefinite. Throughout this paper we use normalized Laplacians as defined below: L = D−1/2LD−1/2 = I − D−1/2WD−1/2 The normalized Laplacians preserve all properties of the Laplacian by construction. As noted in Xiao and Gutman (2</context>
</contexts>
<marker>Belkin, Niyogi, 2001</marker>
<rawString>Belkin, Mikhail and Partha Niyogi. 2001. Laplacian eigenmaps and spectral techniques for embedding and clustering. In Proceedings of the NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In Proceeding of the COLING.</booktitle>
<contexts>
<context position="4648" citStr="Blatz et al., 2004" startWordPosition="771" endWordPosition="774">ess than m. • simP(i, j) is the probability of a random walk from vertex i to vertex j defined via a pagerank model. • simC(i, j) is a function of the commute time between vertex i and vertex j. 4 Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this problem has generated copious literature in the past – see (Pedersen et al., 2004) or (Budanitsky and Hirst, 2006) for a detailed review of various lex</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>Blatz, John, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence estimation for machine translation. In Proceeding of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating wordnet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>47</pages>
<contexts>
<context position="5211" citStr="Budanitsky and Hirst, 2006" startWordPosition="863" endWordPosition="866">ager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this problem has generated copious literature in the past – see (Pedersen et al., 2004) or (Budanitsky and Hirst, 2006) for a detailed review of various lexical relatedness measures on WordNet. Our focus in this paper is not to derive the best similarity measure for WordNet but to use WordNet and the lexical relatedness task as a method to evaluate the various random walk based similarity measures. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. This data consists of 30 word-pairs along with human judgements which is a real value between 1 and 4. For every measure we consider, we derive similarity scores and compare with the human judgements using the Spearm</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Budanitsky, Alexander and Graeme Hirst. 2006. Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13– 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashok Chandra</author>
<author>Prabhakar Raghavan</author>
<author>Walter Ruzzo</author>
<author>Roman Smolensky</author>
<author>Prasoon Tiwari</author>
</authors>
<title>The electrical resistance of a graph captures its commute and cover times.</title>
<date>1989</date>
<booktitle>In Proceedings of the STOC.</booktitle>
<marker>Chandra, Raghavan, Ruzzo, Smolensky, Tiwari, 1989</marker>
<rawString>Chandra, Ashok, Prabhakar Raghavan, Walter Ruzzo, Roman Smolensky, and Prasoon Tiwari. 1989. The electrical resistance of a graph captures its commute and cover times. In Proceedings of the STOC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Chung</author>
</authors>
<title>Spectral graph theory.</title>
<date>1997</date>
<booktitle>In CBMS: Conference Board of the Mathematical Sciences, Regional Conference Series.</booktitle>
<contexts>
<context position="16685" citStr="Chung, 1997" startWordPosition="2913" endWordPosition="2914">e Laplacian spectrum, thus enabling a way to derive commute times from the graph Laplacian in closed form. We now introduce graph Laplacians, which are interesting in their own right besides being related to commute time. The Laplacian of a graph could be viewed as a discrete version of the LaplaceBeltrami operator on Riemannian manifolds. It is defined as L = D − W The graph Laplacian has interesting properties and a wide range of applications, in semi-supervised learning (Zhu et al., 2003), non-linear dimensionality reduction (Roweis and Saul, 2000; Belkin and Niyogi, 2001), and so on. See (Chung, 1997) for a thorough introduction on Laplacians and their properties. We depend on the fact that L is: 1. symmetric (since D and W are for undirected graphs) 2. positive-semidefinite : since it is symmetric, all of the eigenvalues are real and by the Greshgorin circle theorem, the eigenvalues must also be non-negative and hence L is positive-semidefinite. Throughout this paper we use normalized Laplacians as defined below: L = D−1/2LD−1/2 = I − D−1/2WD−1/2 The normalized Laplacians preserve all properties of the Laplacian by construction. As noted in Xiao and Gutman (2003), the resistance distances</context>
</contexts>
<marker>Chung, 1997</marker>
<rawString>Chung, Fan. 1997. Spectral graph theory. In CBMS: Conference Board of the Mathematical Sciences, Regional Conference Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>George Furnas</author>
<author>Thomas Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<contexts>
<context position="20227" citStr="Deerwester et al., 1990" startWordPosition="3513" endWordPosition="3516">s, or even improper edge weights. It is however impossible to know any of this a priori and some noise is inevitable. The derivation of commute times via the pseudo inverse of a noisy Laplacian matrix makes it even worse because the pseudo inverse amplifies the noise in the original matrix. This is because the largest singular value of the pseudo inverse of a matrix is equal to the inverse of the smallest singular value of the original matrix. A standard technique in signal processing and information retrieval to eliminate noise or handle missing values is to use singular value decomposition (Deerwester et al., 1990). We apply SVD to handle noise in the graph construction process. For a given matrix A, SVD decomposes A into three matrices U, S, and V such that A = USVT , where S is a diagonal matrix of eigenvalues of A, and U and V are orthonormal matrices containing the left and the right eigenvectors respectively. The top-k eigenvectors and eigenvalues are computed using the iterative method by Lanczos-Arnoldi (using LAPACK) and the product of these matrices represents a “smoothed” version of the original Laplacian. The pseudo inverse is then computed on this smooth Laplacian. Table 5., shows the improv</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, Scott, Susan Dumais, George Furnas, Thomas Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal ofArtificial Intelligence Research (JAIR),</journal>
<pages>22--457</pages>
<contexts>
<context position="11246" citStr="Erkan and Radev, 2004" startWordPosition="1938" endWordPosition="1941">ts stationary distribution. The choice of m depends on Figure 2: Effect of m in Bounded walk the amount of computation available. A reasonably large value of m (m &gt; 10) should be sufficient for most purposes and one could use lower values of m to derive an approximation for this measure. One could derive an upper bound on the value of m using the mixing time of the underlying Markov chain (Aldous and Fill, 2001). 7.1.3 Similarity via pagerank Pagerank (Page et al., 1998) is the celebrated citation ranking algorithm that has been applied to several natural language problems from summarization (Erkan and Radev, 2004) to opinion mining (Esuli and Sebastiani, 2007) to our task of lexical relatedness (Hughes and Ramage, 2007). Pagerank is yet another random walk model with a difference that it allows the random walk to “jump” to its initial state with a nonzero probability (α). Given the probability transition matrix P as defined above, a stationary distribution vector for any vertex (say i) could be derived as follows: 1. Let ei be a vector of all zeros with ei(i) = 1 2. Let v0 = ei 3. Repeat until �vt − vt_111F &lt; 2 • vt+1 = αvtP + (1 − α)v0 • t = t + 1 4. Assign vt+1 as the stationary distribution for vert</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Erkan, G¨unes and Dragomir Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. Journal ofArtificial Intelligence Research (JAIR), 22:457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Pageranking wordnet synsets: An application to opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>424--431</pages>
<contexts>
<context position="11293" citStr="Esuli and Sebastiani, 2007" startWordPosition="1946" endWordPosition="1949">m depends on Figure 2: Effect of m in Bounded walk the amount of computation available. A reasonably large value of m (m &gt; 10) should be sufficient for most purposes and one could use lower values of m to derive an approximation for this measure. One could derive an upper bound on the value of m using the mixing time of the underlying Markov chain (Aldous and Fill, 2001). 7.1.3 Similarity via pagerank Pagerank (Page et al., 1998) is the celebrated citation ranking algorithm that has been applied to several natural language problems from summarization (Erkan and Radev, 2004) to opinion mining (Esuli and Sebastiani, 2007) to our task of lexical relatedness (Hughes and Ramage, 2007). Pagerank is yet another random walk model with a difference that it allows the random walk to “jump” to its initial state with a nonzero probability (α). Given the probability transition matrix P as defined above, a stationary distribution vector for any vertex (say i) could be derived as follows: 1. Let ei be a vector of all zeros with ei(i) = 1 2. Let v0 = ei 3. Repeat until �vt − vt_111F &lt; 2 • vt+1 = αvtP + (1 − α)v0 • t = t + 1 4. Assign vt+1 as the stationary distribution for vertex i. Armed with the stationary distribution ve</context>
</contexts>
<marker>Esuli, Sebastiani, 2007</marker>
<rawString>Esuli, Andrea and Fabrizio Sebastiani. 2007. Pageranking wordnet synsets: An application to opinion mining. In Proceedings of the ACL, pages 424–431.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1989</date>
<editor>Fellbaum, Christaine, editor.</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="15687" citStr="(1989)" startWordPosition="2751" endWordPosition="2751">is defined in [0, 1]. We now only need a way to compute the commute times to use Equation 3. One could compute the hitting times and hence the commute times from the Equations 1 and 2 using dynamic programming, akin to shortest paths in graphs. In this paper, we instead choose to derive commute times via the graph Laplacian. This also allows us to handle “noise” in the graph construction process which cannot be taken care by naive dynamic programming. 5Note that distance measures, in general, need not be symmetric but we interpret distance as proximity which mandates symmetry. Chandra et. al. (1989) show that the commute time between two vertices is equal to the resistance distance between them. Resistance distance, as proposed by Klein and Randic (1993), is the effective resistance between two vertices in the electrical network represented by the graph, where the edges have resistance 1/wij. Xiao and Gutman (2003), show the relation between resistance distances in graphs to the Laplacian spectrum, thus enabling a way to derive commute times from the graph Laplacian in closed form. We now introduce graph Laplacians, which are interesting in their own right besides being related to commut</context>
</contexts>
<marker>1989</marker>
<rawString>Fellbaum, Christaine, editor. 1989. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thad Hughes</author>
<author>Daniel Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="11354" citStr="Hughes and Ramage, 2007" startWordPosition="1956" endWordPosition="1959"> computation available. A reasonably large value of m (m &gt; 10) should be sufficient for most purposes and one could use lower values of m to derive an approximation for this measure. One could derive an upper bound on the value of m using the mixing time of the underlying Markov chain (Aldous and Fill, 2001). 7.1.3 Similarity via pagerank Pagerank (Page et al., 1998) is the celebrated citation ranking algorithm that has been applied to several natural language problems from summarization (Erkan and Radev, 2004) to opinion mining (Esuli and Sebastiani, 2007) to our task of lexical relatedness (Hughes and Ramage, 2007). Pagerank is yet another random walk model with a difference that it allows the random walk to “jump” to its initial state with a nonzero probability (α). Given the probability transition matrix P as defined above, a stationary distribution vector for any vertex (say i) could be derived as follows: 1. Let ei be a vector of all zeros with ei(i) = 1 2. Let v0 = ei 3. Repeat until �vt − vt_111F &lt; 2 • vt+1 = αvtP + (1 − α)v0 • t = t + 1 4. Assign vt+1 as the stationary distribution for vertex i. Armed with the stationary distribution vectors for vertices i and j, we define pagerank similarity eit</context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>Hughes, Thad and Daniel Ramage. 2007. Lexical semantic relatedness with random graph walks. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings HLT-NAACL.</booktitle>
<contexts>
<context position="4560" citStr="Kauchak and Barzilay, 2006" startWordPosition="756" endWordPosition="759">(i, j) is the probability of a random walk from vertex i to vertex j using all paths of length less than m. • simP(i, j) is the probability of a random walk from vertex i to vertex j defined via a pagerank model. • simC(i, j) is a function of the commute time between vertex i and vertex j. 4 Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this problem has generated copious literature in the past – see (Ped</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>Kauchak, David and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>M Randic</author>
</authors>
<title>Resistance distance.</title>
<date>1993</date>
<journal>Journal of Mathematical Chemistry,</journal>
<pages>12--81</pages>
<contexts>
<context position="15845" citStr="Klein and Randic (1993)" startWordPosition="2774" endWordPosition="2777">e commute times from the Equations 1 and 2 using dynamic programming, akin to shortest paths in graphs. In this paper, we instead choose to derive commute times via the graph Laplacian. This also allows us to handle “noise” in the graph construction process which cannot be taken care by naive dynamic programming. 5Note that distance measures, in general, need not be symmetric but we interpret distance as proximity which mandates symmetry. Chandra et. al. (1989) show that the commute time between two vertices is equal to the resistance distance between them. Resistance distance, as proposed by Klein and Randic (1993), is the effective resistance between two vertices in the electrical network represented by the graph, where the edges have resistance 1/wij. Xiao and Gutman (2003), show the relation between resistance distances in graphs to the Laplacian spectrum, thus enabling a way to derive commute times from the graph Laplacian in closed form. We now introduce graph Laplacians, which are interesting in their own right besides being related to commute time. The Laplacian of a graph could be viewed as a discrete version of the LaplaceBeltrami operator on Riemannian manifolds. It is defined as L = D − W The</context>
</contexts>
<marker>Klein, Randic, 1993</marker>
<rawString>Klein, D. and M. Randic. 1993. Resistance distance. Journal of Mathematical Chemistry, 12:81–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on the shannon entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="13584" citStr="Lin, 1991" startWordPosition="2358" endWordPosition="2359">ity via Hitting Time Given a graph with the transition probability matrix P as defined above, the hitting time between vertices i and j, denoted as h(i, j), is defined as the expected number of steps taken by a random walker to first encounter vertex j starting from vertex i. This can be recursively defined as follows: pikh(k, j) if i =� j h(i, j) = 0 ifi=j (1) 4The Jensen-Shannon divergence between two distributions p and q is defined as D(p I I a)+D(q I I a), where D(. I I .) is the Kullback-Liebler divergence and a = (p + q)/2. Note that unlike KL-divergence this measure is symmetric. See (Lin, 1991) for additional details. { � 1 + k : wik &gt; 0 44 The lower the hitting times of two vertices, the more similar they are. It can be easily verified that hitting time is not a symmetric relation hence graph theory literature suggests another symmetric measure – the commute time.5 The commute time, c(i, j), is the expected number of steps taken to leave vertex i, reach vertex j, and return back to i. Thus, c(i, j) = h(i, j) + h(j, i) (2) Observe that, the commute time is a metric in that it is positive definite, symmetric, and satisifies triangle inequality. Hence, commute time could be used as a </context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Lin, Jianhua. 1991. Divergence measures based on the shannon entropy. IEEE Transactions on Information Theory, 37(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>W Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>In Language and Cognitive Process.</booktitle>
<contexts>
<context position="5585" citStr="Miller and Charles (1991)" startWordPosition="929" endWordPosition="932">tween the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this problem has generated copious literature in the past – see (Pedersen et al., 2004) or (Budanitsky and Hirst, 2006) for a detailed review of various lexical relatedness measures on WordNet. Our focus in this paper is not to derive the best similarity measure for WordNet but to use WordNet and the lexical relatedness task as a method to evaluate the various random walk based similarity measures. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. This data consists of 30 word-pairs along with human judgements which is a real value between 1 and 4. For every measure we consider, we derive similarity scores and compare with the human judgements using the Spearman rank correlation coefficient. 5 Graph construction For the purpose of evaluation of the random walk measures, we construct a graph for every pair of words for which similarity has to be computed. This graph is derived from WordNet as follows: • For each word w in the pair (w1, w2): – Add an edge between w and all of its parts of speech. For example, if the word is coas</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>Miller, G. and W. Charles. 1991. Contextual correlates of semantic similarity. In Language and Cognitive Process.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Stanford University,</institution>
<location>Stanford, CA.</location>
<contexts>
<context position="11099" citStr="Page et al., 1998" startWordPosition="1915" endWordPosition="1918">m = 20. However, as observed in Figure 2. , beyond a certain value the choice of m does not affect the result as the random walk converges to its stationary distribution. The choice of m depends on Figure 2: Effect of m in Bounded walk the amount of computation available. A reasonably large value of m (m &gt; 10) should be sufficient for most purposes and one could use lower values of m to derive an approximation for this measure. One could derive an upper bound on the value of m using the mixing time of the underlying Markov chain (Aldous and Fill, 2001). 7.1.3 Similarity via pagerank Pagerank (Page et al., 1998) is the celebrated citation ranking algorithm that has been applied to several natural language problems from summarization (Erkan and Radev, 2004) to opinion mining (Esuli and Sebastiani, 2007) to our task of lexical relatedness (Hughes and Ramage, 2007). Pagerank is yet another random walk model with a difference that it allows the random walk to “jump” to its initial state with a nonzero probability (α). Given the probability transition matrix P as defined above, a stationary distribution vector for any vertex (say i) could be derived as follows: 1. Let ei be a vector of all zeros with ei(i</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>Page, Larry, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1998. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford University, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Senserelate:: Targetword-A generalized framework for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="4517" citStr="Patwardhan et al., 2005" startWordPosition="750" endWordPosition="753">baseline for comparison purposes. • simB(i, j) is the probability of a random walk from vertex i to vertex j using all paths of length less than m. • simP(i, j) is the probability of a random walk from vertex i to vertex j defined via a pagerank model. • simC(i, j) is a function of the commute time between vertex i and vertex j. 4 Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this problem has generate</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2005</marker>
<rawString>Patwardhan, Siddharth, Satanjeev Banerjee, and Ted Pedersen. 2005. Senserelate:: Targetword-A generalized framework for word sense disambiguation. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity -measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI.</booktitle>
<contexts>
<context position="5179" citStr="Pedersen et al., 2004" startWordPosition="858" endWordPosition="861">06), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this problem has generated copious literature in the past – see (Pedersen et al., 2004) or (Budanitsky and Hirst, 2006) for a detailed review of various lexical relatedness measures on WordNet. Our focus in this paper is not to derive the best similarity measure for WordNet but to use WordNet and the lexical relatedness task as a method to evaluate the various random walk based similarity measures. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. This data consists of 30 word-pairs along with human judgements which is a real value between 1 and 4. For every measure we consider, we derive similarity scores and compare with the h</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Pedersen, Ted, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity -measuring the relatedness of concepts. In Proceedings of the AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="4836" citStr="Pereira et al., 1993" startWordPosition="802" endWordPosition="805">ertex j. 4 Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this problem has generated copious literature in the past – see (Pedersen et al., 2004) or (Budanitsky and Hirst, 2006) for a detailed review of various lexical relatedness measures on WordNet. Our focus in this paper is not to derive the best similarity measure for WordNet but to use WordNet and the lexical relatedness task as a method to ev</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, Fernando, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of english words. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Prager</author>
<author>Jennifer Chu-Carroll</author>
<author>Krzysztof Czuba</author>
</authors>
<title>Use of wordnet hypernyms for answering what-is questions.</title>
<date>2001</date>
<booktitle>In Proceedings of the Text REtrieval Conference.</booktitle>
<contexts>
<context position="4602" citStr="Prager et al., 2001" startWordPosition="763" endWordPosition="766">ertex i to vertex j using all paths of length less than m. • simP(i, j) is the probability of a random walk from vertex i to vertex j defined via a pagerank model. • simC(i, j) is a function of the commute time between vertex i and vertex j. 4 Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this problem has generated copious literature in the past – see (Pedersen et al., 2004) or (Budanitsky and Hir</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, 2001</marker>
<rawString>Prager, John M., Jennifer Chu-Carroll, and Krzysztof Czuba. 2001. Use of wordnet hypernyms for answering what-is questions. In Proceedings of the Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Roweis</author>
<author>Lawrence Saul</author>
</authors>
<title>Nonlinear dimensionality reduction by locally linear embedding.</title>
<date>2000</date>
<journal>Science,</journal>
<pages>290--2323</pages>
<contexts>
<context position="1536" citStr="Roweis and Saul, 2000" startWordPosition="228" endWordPosition="231">eudo inverse based measure could be improved by discarding the least significant eigenvectors, corresponding to the noise in the graph construction process, using singular value decomposition. 1 Introduction Natural language data lend themselves to a graph based representation. Words could be linked by explicit relations as in WordNet (Fellbaum, 1989) or documents could be linked to one another via hyperlinks. Even in the absence of such a straightforward representation it is possible to derive meaningful graphs such as the nearest neighbor graphs as done in certain manifold learning methods (Roweis and Saul, 2000; Belkin and Niyogi, © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 2001). All of these graphs share the following properties: • They are edge-weighted. • The edge weight encodes some notion of relatedness between the vertices. • The relation represented by edges is at least weakly transitive. Examples of such relations include, “is similar to”, “is more general than”, and so on. It is important that the relations selected are transitive for the random walk to make</context>
<context position="16629" citStr="Roweis and Saul, 2000" startWordPosition="2901" endWordPosition="2904">), show the relation between resistance distances in graphs to the Laplacian spectrum, thus enabling a way to derive commute times from the graph Laplacian in closed form. We now introduce graph Laplacians, which are interesting in their own right besides being related to commute time. The Laplacian of a graph could be viewed as a discrete version of the LaplaceBeltrami operator on Riemannian manifolds. It is defined as L = D − W The graph Laplacian has interesting properties and a wide range of applications, in semi-supervised learning (Zhu et al., 2003), non-linear dimensionality reduction (Roweis and Saul, 2000; Belkin and Niyogi, 2001), and so on. See (Chung, 1997) for a thorough introduction on Laplacians and their properties. We depend on the fact that L is: 1. symmetric (since D and W are for undirected graphs) 2. positive-semidefinite : since it is symmetric, all of the eigenvalues are real and by the Greshgorin circle theorem, the eigenvalues must also be non-negative and hence L is positive-semidefinite. Throughout this paper we use normalized Laplacians as defined below: L = D−1/2LD−1/2 = I − D−1/2WD−1/2 The normalized Laplacians preserve all properties of the Laplacian by construction. As n</context>
</contexts>
<marker>Roweis, Saul, 2000</marker>
<rawString>Roweis, Sam and Lawrence Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323–2326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Xiao</author>
<author>I Gutman</author>
</authors>
<title>Resistance distance and laplacian spectrum. Theoretical Chemistry Association,</title>
<date>2003</date>
<pages>110--284</pages>
<contexts>
<context position="16009" citStr="Xiao and Gutman (2003)" startWordPosition="2799" endWordPosition="2803">the graph Laplacian. This also allows us to handle “noise” in the graph construction process which cannot be taken care by naive dynamic programming. 5Note that distance measures, in general, need not be symmetric but we interpret distance as proximity which mandates symmetry. Chandra et. al. (1989) show that the commute time between two vertices is equal to the resistance distance between them. Resistance distance, as proposed by Klein and Randic (1993), is the effective resistance between two vertices in the electrical network represented by the graph, where the edges have resistance 1/wij. Xiao and Gutman (2003), show the relation between resistance distances in graphs to the Laplacian spectrum, thus enabling a way to derive commute times from the graph Laplacian in closed form. We now introduce graph Laplacians, which are interesting in their own right besides being related to commute time. The Laplacian of a graph could be viewed as a discrete version of the LaplaceBeltrami operator on Riemannian manifolds. It is defined as L = D − W The graph Laplacian has interesting properties and a wide range of applications, in semi-supervised learning (Zhu et al., 2003), non-linear dimensionality reduction (R</context>
<context position="17259" citStr="Xiao and Gutman (2003)" startWordPosition="3006" endWordPosition="3009"> and Niyogi, 2001), and so on. See (Chung, 1997) for a thorough introduction on Laplacians and their properties. We depend on the fact that L is: 1. symmetric (since D and W are for undirected graphs) 2. positive-semidefinite : since it is symmetric, all of the eigenvalues are real and by the Greshgorin circle theorem, the eigenvalues must also be non-negative and hence L is positive-semidefinite. Throughout this paper we use normalized Laplacians as defined below: L = D−1/2LD−1/2 = I − D−1/2WD−1/2 The normalized Laplacians preserve all properties of the Laplacian by construction. As noted in Xiao and Gutman (2003), the resistance distances can be derived from the generalized Moore-Penrose pseudo-inverse of the graph Laplacian(L†) – also called the inverse Laplacian. Like Laplacians, their pseudo inverse counterparts are also symmetric, and positive semi-definite. Lemma 2. L† is symmetric Proof. The Moore-Penrose pseudo-inverse is defined as L† = (LTL)−1LT. From this definition, it is clear that (L†)T = (LT)†. By the symmetry 45 property of graph Laplacians, LT = L. Hence, (L†)T = L†. Lemma 3. L† is positive semi-definite Proof. We make use of the following properties from (Chung, 1997): • The Laplacian</context>
</contexts>
<marker>Xiao, Gutman, 2003</marker>
<rawString>Xiao, W. and I. Gutman. 2003. Resistance distance and laplacian spectrum. Theoretical Chemistry Association, 110:284–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luh Yen</author>
<author>Francois Fouss</author>
<author>Christine Decaestecker</author>
<author>Pascal Francq</author>
<author>Marco Saerens</author>
</authors>
<title>Graph nodes clustering based on the commute-time kernel.</title>
<date>2007</date>
<booktitle>In Proceedings of the PAKDD.</booktitle>
<marker>Yen, Fouss, Decaestecker, Francq, Saerens, 2007</marker>
<rawString>Yen, Luh, Francois Fouss, Christine Decaestecker, Pascal Francq, and Marco Saerens. 2007. Graph nodes clustering based on the commute-time kernel. In Proceedings of the PAKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised learning using gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings of the ICML.</booktitle>
<contexts>
<context position="8468" citStr="Zhu et al., 2003" startWordPosition="1425" endWordPosition="1428">han religious leader as there are multiple senses, and hence multiple paths, connecting satellite and moon. Thus it is desirable to have a measure that captures not only path lengths but also the connectivity structure of the graph. This notion is elegantly captured using random walks on graphs. 7.1 Similarity via Random walks A random walk is a stochastic process that consists of a sequence of discrete steps taken at random defined by a distribution. Random walks have interesting connections to Brownian motion, heat diffusion and have been used in semi-supervised learning – for example, see (Zhu et al., 2003). Certain properties of random walks are defined for ergodic processes only2. In our work, we assume these 1The religious leader sense of moon is due to Sun Myung Moon, a US religious leader. 2A stochastic process is ergodic if the underlying Markov chain is irreducible and aperiodic. A Markov chain is irrehold true as the graphs we deal with are connected, undirected, and non-bipartite. 7.1.1 Bounded length walks As our first random walk measure, we consider the bounded length walk – i.e., all random walks of length less than or equal to a bound m. We derive a probability transition matrix P </context>
<context position="16569" citStr="Zhu et al., 2003" startWordPosition="2893" endWordPosition="2896">e the edges have resistance 1/wij. Xiao and Gutman (2003), show the relation between resistance distances in graphs to the Laplacian spectrum, thus enabling a way to derive commute times from the graph Laplacian in closed form. We now introduce graph Laplacians, which are interesting in their own right besides being related to commute time. The Laplacian of a graph could be viewed as a discrete version of the LaplaceBeltrami operator on Riemannian manifolds. It is defined as L = D − W The graph Laplacian has interesting properties and a wide range of applications, in semi-supervised learning (Zhu et al., 2003), non-linear dimensionality reduction (Roweis and Saul, 2000; Belkin and Niyogi, 2001), and so on. See (Chung, 1997) for a thorough introduction on Laplacians and their properties. We depend on the fact that L is: 1. symmetric (since D and W are for undirected graphs) 2. positive-semidefinite : since it is symmetric, all of the eigenvalues are real and by the Greshgorin circle theorem, the eigenvalues must also be non-negative and hence L is positive-semidefinite. Throughout this paper we use normalized Laplacians as defined below: L = D−1/2LD−1/2 = I − D−1/2WD−1/2 The normalized Laplacians pr</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Zhu, Xiaojin, Zoubin Ghahramani, and John Lafferty. 2003. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>