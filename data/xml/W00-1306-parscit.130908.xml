<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000113">
<title confidence="0.99169">
Sample Selection for Statistical Grammar Induction *
</title>
<author confidence="0.99696">
Rebecca Hwa
</author>
<affiliation confidence="0.983984">
Division of Engineering and Applied Sciences
Harvard University
</affiliation>
<address confidence="0.710688">
Cambridge, MA 02138 USA
</address>
<email confidence="0.998391">
rebecca@eecs.harvard.edu
</email>
<sectionHeader confidence="0.993885" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99978025">
Corpus-based grammar induction relies on us-
ing many hand-parsed sentences as training
examples. However, the construction of a
training corpus with detailed syntactic analy-
sis for every sentence is a labor-intensive task.
We propose to use sample selection methods
to minimize the amount of annotation needed
in the training data, thereby reducing the
workload of the human annotators. This pa-
per shows that the amount of annotated train-
ing data can be reduced by 36% without de-
grading the quality of the induced grammars.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999859565217391">
Many learning problems in the domain of
natural language processing need supervised
training. For instance, it is difficult to induce
a grammar from a corpus of raw text; but the
task becomes much easier when the training
sentences are supplemented with their parse
trees. However, appropriate supervised train-
ing data may be difficult to obtain. Existing
corpora might not contain the relevant type
of supervision, and the data might not be
in the domain of interest. For example, one
might need morphological analyses of the lex-
icon in addition to the parse trees for inducing
a grammar, or one might be interested in pro-
cessing non-English languages for which there
is no annotated corpus. Because supervised
training typically demands significant human
involvement (e.g., annotating the parse trees
of sentences by hand), building a new corpus is
a labor-intensive task. Therefore, it is worth-
while to consider ways of minimizing the size
of the corpus to reduce the effort spent by an-
notators.
</bodyText>
<listItem confidence="0.833767333333333">
• This material is based upon work supported by the
National Science Foundation under Grant No. IRI
9712068. We thank Wheeler Rural for his plotting
tool; and Stuart Shieber, Lillian Lee, Ric Crabbe, and
the anonymous reviewers for their comments on the
paper.
</listItem>
<bodyText confidence="0.999950195121951">
There are two possible directions: one
might attempt to reduce the amount of anno-
tations in each sentence, as was explored by
Hwa (1999); alternatively, one might attempt
to reduce the number of training sentences.
In this paper, we consider the latter approach
using sample selection, an interactive learning
method in which the machine takes the initia-
tive of selecting potentially beneficial train-
ing examples for the humans to annotate. If
the system could accurately identify a subset
of examples with high Training Utility Values
(TUV) out of a pool of unlabeled data, the
annotators would not need to waste time on
processing uninformative examples.
We show that sample selection can be
applied to grammar induction to produce
high quality grammars with fewer annotated
training sentences. Our approach is to use
uncertainty-based evaluation functions that
estimate the TUV of a sentence by quantify-
ing the grammar&apos;s uncertainty about assign-
ing a parse tree to this sentence. We have
considered two functions. The first is a sim-
ple heuristic that approximates the grammar&apos;s
uncertainty in terms of sentence lengths. The
second computes uncertainty in terms of the
tree entropy of the sentence. This metric is
described in detail later.
This paper presents an empirical study
measuring the effectiveness of our evaluation
functions at selecting training sentences from
the Wall Street Journal (WSJ) corpus (Mar-
cus et al., 1993) for inducing grammars. Con-
ducting the experiments with training pools
of different sizes, we have found that sample
selection based on tree entropy reduces a large
training pool by 36% and a small training pool
by 27%. These results suggest that sample se-
lection can significantly reduce -human effort
exerted in building training corpora.
</bodyText>
<page confidence="0.999232">
45
</page>
<sectionHeader confidence="0.961122" genericHeader="introduction">
2 Sample Selection
</sectionHeader>
<bodyText confidence="0.997288055555556">
Unlike traditional learning systems that re-
ceive training examples indiscriminately, a
learning system that uses sample selection
actively influences its progress by choosing
new examples to incorporate into its training
set. Sample selection works with two types
of learning systems: a committee of learners
or a single learner. The committee-based se-
lection algorithm works with multiple learn-
ers, each maintaining a different hypothesis
(perhaps pertaining to different aspects of the
problem). The candidate examples that led
to the most disagreements among the differ-
ent learners are considered to have the high-
est TUV (Cohn et al., 1994; Freund et al.,
1997). For computationally intensive prob-
lems such as grammar induction, maintaining
multiple learners may be an impracticality. In
this work, we explore sample selection with a
single learner that keeps just one working hy-
pothesis at all times.
Figure 1 outlines the single-learner sample
selection training loop in pseudo-code. Ini-
tially, the training set, L, consists of a small
number of labeled examples, based on which
the learner proposes its first hypothesis of
the target concept, C. Also available to the
learner is a large pool of unlabeled training
candidates, U. In each training iteration, the
selection algorithm, Select(n,U,C, f), ranks
the candidates of U according to their ex-
pected TUVs and returns the n candidates
with the highest values. The algorithm com-
putes the expected TUV of each candidate,
u E U, with an evaluation function, f (u,C).
This function may possibly rely on the hy-
pothesis concept C to estimate the utility of
a candidate u. The set of the n chosen candi-
dates are then labeled by human and added
to the existing training set. Running the
learning algorithm, Train(L), on the updated
training set, the system proposes a new hy-
pothesis consistent with all the examples seen
thus far. The loop continues until one of three
stopping conditions is met: the hypothesis is
considered close enough to the target concept,
all candidates are labeled, or all human re-
sources are exhausted.
Sample selection may be beneficial for many
learning tasks in natural language process-
ing. Although there exist abundant collec-
tions of raw text, the high expense of man-
ually annotating the text sets a severe lim-
itation for many learning algorithms in nat-
</bodyText>
<figureCaption confidence="0.9576535">
Figure 1: The pseudo-code for the sample se-
lection learning algorithm
</figureCaption>
<bodyText confidence="0.999854805555556">
ural language processing. Sample selection
presents an attractive solution to offset this
labeled data sparsity problem. Thus far, it
has been successfully applied to several classi-
fication applications. Some examples include
text categorization (Lewis and Gale, 1994),
part-of-speech tagging (Engelson and Dagan,
1996), word-sense disambiguation (Fujii et al.,
1998), and prepositional-phrase attachment
(Hwa, 2000).
More difficult are learning problems whose
objective is not classification, but generation
of complex structures. One example in this di-
rection is applying sample selection to seman-
tic parsing (Thompson et al., 1999), in which
sentences are paired with their semantic rep-
resentation using a deterministic shift-reduce
parser. Our work focuses on another complex
natural language learning problem: inducing
a stochastic context-free grammar that can
generate syntactic parse trees for novel test
sentences.
Although abstractly, parsing with a gram-
mar can be seen as a classification task of de-
termining the structure of a sentence by se-
lecting one tree out of a set of possible parse
trees, there are two major distinctions that
differentiate it from typical classification prob-
lems. First, a classifier usually chooses from
a fixed set of categories, but in our domain,
every sentence has a different set of possible
parse trees. Second, for most classification
problems, the the number of the possible cate-
gories is relatively small, whereas the number
of potential parse trees for a sentence is expo-
nential with respect to the sentence length.
</bodyText>
<figure confidence="0.530898909090909">
U is a set of unlabeled candidates.
L is a set of labeled training examples.
C is the current hypothesis.
Initialize:
C Train(L).
Repeat
N E- Select(n,U,C, f).
U U — N.
L L U Label(N).
C Train(L).
Until (C. Ctrue)or (U = 0) or (human stops)
</figure>
<page confidence="0.998513">
46
</page>
<sectionHeader confidence="0.994361" genericHeader="method">
3 Grammar Induction
</sectionHeader>
<bodyText confidence="0.999924416666667">
The degree of difficulty of the task of learning
a grammar from data depends on the quantity
and quality of the training supervision. When
the training corpus consists of a large reservoir
of fully annotated parse trees, it is possible
to directly extract a grammar based on these
parse trees. The success of recent high-quality
parsers (Charnia.k, 1997; Collins, 1997) relies
on the availability of such treebank corpora.
To work with smaller training corpora, the
learning system would require even more in-
formation about the examples than their syn-
tactic parse trees. For instance, Hermjakob
and Mooney (1997) have described a learning
system that can build a deterministic shift-
reduce parser from a small set of training
examples with the aid of detailed morpho-
logical, syntactical, and semantic knowledge
databases and step-by-step guidance from hu-
man experts.
The induction task becomes more chal-
lenging as the amount of supervision in the
training data and background knowledge de-
creases. To compensate for the missing infor-
mation, the learning process requires heuristic
search to find locally optimal grammars. One
form of partially supervised data might spec-
ify the phrasal boundaries without specify-
ing their labels by bracketing each constituent
unit with a pair of parentheses (McNaughton,
1967). For example, the parse tree for the sen-
tence &amp;quot;Several fund managers expect a rough
market this morning before prices stablize.&amp;quot;
is labeled as &amp;quot;((Several fund managers) (ex-
pect ((a rough market) (this morning)) (be-
fore (prices stabilize))).)&amp;quot; As shown in Pereira
and Schabes (1992), an essentially unsuper-
vised learning algorithm such as the Inside-
Outside re-estimation process (Baker, 1979;
Lan i and Young, 1990) can be modified to take
advantage of these bracketing constraints.
For our sample selection experiment, we
chose to work under the more stringent con-
dition of partially supervised training data, as
described above, because our ultimate goal is
to minimize the amount of annotation done
by humans in terms of both the number of
sentences and the number of brackets within
the sentences. Thus, the quality of our in-
duced grammars should not be compared to
those extracted from a fully annotated train-
ing corpus. The learning algorithm we use is
a variant of the Inside-Outside algorithm that
induces grammars expressed in the Probabilis-
tic Lexicalized Tree Insertion Grammar rep-
resentation (Schabes and Waters, 1993; Hwa,
1998). This formalism&apos;s context-free equiva-
lence and its lexicalized representation make
the training process efficient and computa-
tionally plausible.
</bodyText>
<sectionHeader confidence="0.901069" genericHeader="method">
4 Selective Sampling Evaluation
Functions
</sectionHeader>
<bodyText confidence="0.999962888888889">
In this paper, we propose two uncertainty-
based evaluation functions for estimating the
training utilities of the candidate sentences.
The first is a simple heuristic that uses the
length of a sentence to estimate uncertain-
ties. The second function computes uncer-
tainty in terms of the entropy of the parse
trees that the hypothesis-grammar generated
for the sentence.
</bodyText>
<subsectionHeader confidence="0.999359">
4.1 Sentence Length
</subsectionHeader>
<bodyText confidence="0.999059857142857">
Let us first consider a simple evaluation
function that estimates the training utility
of a candidate without consulting the cur-
rent hypothesis-grammar, G. The function
hen (s, G) coarsely approximates the uncer-
tainty of a candidate sentence s with its
length:
</bodyText>
<equation confidence="0.707919">
fien(s, G) = length(s).
</equation>
<bodyText confidence="0.999745166666667">
The intuition behind this function is based
on the general observation that longer sen-
tences tend to have complex structures and
introduce more opportunities for ambiguous
parses. Since the scoring only depends on
sentence lengths, this naive evaluation func-
tion orders the training pool deterministically
regardless of either the current state of the
grammar or the annotation of previous train-
ing sentences. This approach has one major
advantage: it is easy to compute and takes
negligible processing time.
</bodyText>
<subsectionHeader confidence="0.998544">
4.2 Tree Entropy
</subsectionHeader>
<bodyText confidence="0.999929538461538">
Sentence length is not a very reliable indi-
cator of uncertainty. To measure the un-
certainty of a sentence more accurately, the
evaluation function must base its estimation
on the outcome of testing the sentence on
the hypothesis-grammar When a stochastic
grammar parses a sentence, it generates a set
of possible trees and associates a likelihood
value with each. Typically, the most likely
tree is taken to be the best parse for the sen-
tence.
We propose an evaluation function that
considers the probabilities of all parses. The
</bodyText>
<page confidence="0.996819">
47
</page>
<bodyText confidence="0.965806464285714">
set of probabilities of the possible parse trees
for a sentence defines a distribution that in-
dicates the grammar&apos;s uncertainty about the
structure of the sentence. For example, a uni-
form distribution signifies that the grammar
is at its highest uncertainty because all the
parses are equally likely; whereas a distribu-
tion resembling an impulse function suggests
that the grammar is very certain because it
finds one parse much more likely than all oth-
ers. To quantitatively characterize a distribu-
tion, we compute its entropy.
Entropy measures the uncertainty of assign-
ing a value to a random variable over a dis-
tribution. Informally speaking, it is the ex-
pected number of bits needed to encode the
assignment. A higher entropy value signifies
a higher degree of uncertainty. At the highest
uncertainty, the random variable is assigned
one of n values over a uniform distribution,
and the outcome would require 10g2(n) bits to
encode.
More formally, let V be a discrete random
variable that can take any possible outcome
in set V. Let p(v) be the density function
p(v) = Pr(V = v), v E V. The entropy H (V)
is the expected negative log likelihood of ran-
dom variable V:
</bodyText>
<equation confidence="0.976585666666667">
H (V) = —EX(log2(p(V))).
= E p(v) log2(p(v)).
vEV
</equation>
<bodyText confidence="0.999938961538462">
Further details about the properties of en-
tropy can be found in textbooks on informa-
tion theory (Cover and Thomas, 1991).
Determining the parse tree for a sentence
from a set of possible parses can be viewed as
assigning a value to a random variable. Thus,
a direct application of the entropy definition
to the probability distribution of the parses for
sentence s in grammar G computes its tree en-
tropy, TE(s,G), the expected number of bits
needed to encode the distribution of possible
parses for s. Note that we cannot compare
sentences of different lengths by their entropy.
For two sentences of unequal lengths, both
with uniform distributions, the entropy of the
longer one is higher. To normalize for sen-
tence length, we define an evaluation function
that computes the similarity between the ac-
tual probability distribution and the uniform
distribution for a sentence of that length. For
a sentence s of length 1, there can be at most
0(21) equally likely parse trees and its maxi-
mal entropy is 0(1) bits (Cover and Thomas,
1991). Therefore, we define the evaluation
function, fte(s,G) to be the tree entropy di-
vided by the sentence length.
</bodyText>
<equation confidence="0.987398666666667">
TE(s,G)
fte(s,G) =
length(s).
</equation>
<bodyText confidence="0.999986428571429">
We now derive the expression for TE(s,G).
Suppose that a sentence s can be generated by
a grammar G with some non-zero probability,
Pr(s I G). Let V be the set of possible parses
that G generated for s. Then the probability
that sentence s is generated by G is the sum
of the probabilities of its parses. That is:
</bodyText>
<equation confidence="0.96598">
Pr(s I G) = E Pr(v I G).
vEV
</equation>
<bodyText confidence="0.999626714285714">
Note that Pr(v G) reflects the probability of
one particular parse tree, v, in the grammar
out of all possible parse trees for all possible
sentences that G accepts. But in order to ap-
ply the entropy definition from above, we need
to specify a distribution of probabilities for the
parses of sentence s such that
</bodyText>
<equation confidence="0.990778">
E Pr(vJ s,G) = 1.
vEV
</equation>
<bodyText confidence="0.987382555555556">
Pr(v I s, C) indicates the likelihood that v is
the correct parse tree out of a set of possible
parses for $ according to grammar G. It is
also the density function, p(v), for the distri-
bution (i.e., the probability of assigning v to
a random variable V). Using Bayes Rule and
noting that Pr(v,s G) = Pr(v J G) (because
the existence of tree v implies the existence of
sentence s), we get:
</bodyText>
<equation confidence="0.982932333333333">
Pr(v,s I G) Pr(v G)
p(v) = Pr(v s,G) =
Pr(s(G) Pr(s ( G).
</equation>
<bodyText confidence="0.999684333333333">
Replacing the generic density function term
in the entropy definition, we derive the expres-
sion for TE(s,G), the tree entropy of s:
</bodyText>
<equation confidence="0.952919952380952">
TE(s,G) = H(V)
— Ep(v) log2 p(v)
vEV
Pr(v G) Pr(v G)1
Pr(s G) 4()g2 Pr(s IG)I
vEV
Pr(v I G) pr(v
Pr(s G) 82
vEV
z Pr(v I G)
(s G)g2 Pr(s I G)
Pr
vEV
48
EvEy Pr(t) I G) log2 Pr(v I G)
Pr(s I G)
+ log2 Pr(s G)EvEy Pr(v I G)
Pr(s I G)
EvEv Pr(v I G) log2 Pr(v I G)
Pr(s I G)
+ log2 Pr(s G)
</equation>
<bodyText confidence="0.999234125">
Using the bottom-up, dynamic program-
ming technique of computing Inside Proba-
bilities (Lan i and Young, 1990), we can ef-
ficiently compute the probability of the sen-
tence, Pr(s G). Similarly, the algorithm
can be modified to compute the quantity
EvEv Pr(v I G) log2(Pr(v f G)) (see Ap-
pendix A).
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99998196">
To determine the effectiveness of selecting
training examples with the two proposed eval-
uation functions, we compare them against
a baseline of random selection (frend(s, G) =
rand()). The task is to induce grammars from
selected sentences in the Wall Street Journal
(WSJ) corpus, and to parse unseen test sen-
tences with the trained grammars Because
the vocabulary size (and the grammar size
by extension) is very large, we have substi-
tuted the words with their part-of-speech tags
to avoid additional computational complexity
in training the grammar After replacing the
words with part-of-speech tags, the vocabu-
lary size of the corpus is reduced to 47 tags.
We repeat the study for two different
candidate-pool sizes. For the first experiment,
we assume that there exists an abundant sup-
ply of unlabeled data. Based on empirical ob-
servations (as will be shown in Section 6), for
the task we are considering, the induction al-
gorithm typically reaches its asymptotic limit
after training with 2600 sentences; therefore,
it is sufficient to allow for a candidate-pool size
of U = 3500 unlabeled WSJ sentences. In the
second experiment, we restrict the size of the
candidate-pool such that U contains only 900
unlabeled sentences. This experiment studies
how the paucity of training data affects the
evaluation functions.
For both experiments, each of the three
evaluation functions:
dfranch hen, and fte, is
applied to the sample selection learning algo-
rithm shown in Figure 1, where concept C is
the current hypothesis-grammar G, and L, the
set of labeled training data, initially consists
of 100 sentences. In every iteration, n = 100
new sentences are picked from U to be added
to L, and a new C is induced from the updated
L. After the hypothesis-grammar is updated,
it is tested. The quality of the induced gram-
mar is judged by its ability to generate cor-
rect parses for unseen test sentences. We use
the consistent bracketing metric (i.e., the per-
centage of brackets in the proposed parse not
crossing brackets of the true parse) to mea-
sure parsing accuracy&apos;. To ensure the statis-
tical significance of the results, we report the
average of ten trials for each experiment2.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.964953675675675">
The results of the two experiments are graph-
ically depicted in Figure 2. We plot learning
rates of the induction processes using train-
ing sentences selected by the three evaluation
functions. The learning rate relates the qual-
ity of the induced grammars to the amount of
supervised training data available. In order
for the induced grammar to parse test sen-
tences with higher accuracy (x-axis), more su-
pervision (y-axis) is needed. The amount of
supervision is measured in terms of the num-
ber of brackets rather than sentences because
it more accurately quantifies the effort spent
by the human annotator. Longer sentences
tend to require more brackets than short ones,
and thus take more time to analyze. We deem
one evaluation function more effective than
another if the smallest set of sentences it se-
lected can train a grammar that performs at
least as well as the grammar trained under the
other function and if the selected data con-
tains considerably fewer brackets than that of
the other function.
Figure 2(a) presents the outcomes of the
first experiment, in which the evaluation func-
tions select training examples out of a large
candidate-pool. We see that overall, sample
selection has a positive effect on the learning
&apos;The unsupervised induction algorithm induces
grammars that generate binary branching trees so that
the number of proposed brackets in a sentence is al-
ways one fewer than the length of the sentence. The
WSJ corpus, on the other hand, favors a more flat-
tened tree structure with considerably fewer brackets
per sentence. The consistent bracketing metric does
not unfairly penalize a proposed parse tree for being
binary branching.
</bodyText>
<footnote confidence="0.94866675">
2We generate different candidate-pools by moving
a fixed-size window across WSJ sections 02 through
05, advancing 400 sentences for each trial. Secti9n 23
is always used for testing.
</footnote>
<page confidence="0.99748">
49
</page>
<figure confidence="0.571825333333333">
Number of brackets in the training net 39,009 Number of brackets in the training set
9.090 -
Van -
10,099
5.900
boaiine basOin.
69 72 75 81 2 79 76 61
Parsing accuracy on the test set Parsing accuracy on the test set
(a) (b)
</figure>
<figureCaption confidence="0.994486666666667">
Figure 2: The learning rates of the induction processes using examples selected by the three
evaluation functions for (a) when the candidate-pool is large, and (b) when the candidate-pool
is small.
</figureCaption>
<table confidence="0.974588">
grammar set avg. training brackets t-test on bracket avg. avg. score t-test on score avg.
baseline-26 33355 N/A 80.3 N/A
length-17 30288 better 80.3 not sig. worse
tree entropy-14 21236 better 80.4 not sig. worse
</table>
<tableCaption confidence="0.991759">
Table 1: Summary of pair-wise t-test with 95% confidence comparing the best set of grammars
</tableCaption>
<bodyText confidence="0.96480248">
induced with the baseline (after 26 selection iterations) to the sets of grammars induced under
the proposed evaluation functions (lien after 17 iterations, fie after 14 iterations).
rate of the induction process. For the base-
line case, the induction process uses frand,
in which training sentences are randomly se-
lected. The resulting grammars achieves an
average parsing accuracy of 80.3% on the test
sentences after seeing an average of 33355
brackets in the training data. The learning
rate of the tree entropy evaluation function,
fie, progresses much faster than the baseline.
To induce a grammar that reaches the same
80.3% parsing accuracy with the examples se-
lected by fte, the learner requires, on average,
21236 training brackets, reducing the amount
of annotation by 36% comparing to the base-
line. While the simplistic sentence length
evaluation function, lien, is less helpful, its
learning rate still improves slightly faster than
the baseline. A grammar of comparable qual-
ity can be induced from a set of training exam-
ples selected by hen containing an average of
30288 brackets. This provides a small reduc-
tion of 9% from the baseline 3. We consider a
set of grammars to be comparable to the base-
</bodyText>
<footnote confidence="0.59093125">
3In terms of the number of sentences, the baseline
frand used 2600 randomly chosen training sentences;
hen selected the 1700 longest sentences as training
data; and ft, selected 1400 sentences.
</footnote>
<bodyText confidence="0.998778296296296">
line if its mean test score is at least as high
as that of the baseline and if the difference of
the means is not statistically significant (us-
ing pair-wise t-test at 95% confidence). Ta-
ble 1 summarizes the statistical significance of
comparing the best set of baseline grammars
with those of of lien and fte.
Figure 2(b) presents the results of the sec-
ond experiment, in which the evaluation func-
tions only have access to a small candidate
pool. Similar to the previous experiment,
grammars induced from training examples se-
lected by fee require significantly less annota-
tions than the baseline. Under the baseline,
hand, to train grammars with 78.5% parsing
accuracy on test data, an average of 11699
brackets (in 900 sentences) is required. In con-.
trast, fte can induce a comparable grammar
with an average of 8559 brackets (in 600 sen-
tences), providing a saving of 27% in the num-
ber of training brackets. The simpler evalua-
tion function hen out-performs the baseline
as well; the 600 sentences it selected have an
average of 9935 brackets. Table 2 shows the
statistical significance of these comparisons.
A somewhat surprising outcome of the sec-
ond study is that the grammars induced from
</bodyText>
<page confidence="0.956903">
50
</page>
<table confidence="0.997757">
grammar set avg. training brackets t-test on bracket avg. avg. score t-test on score avg.
baseline-9 11699 N/A 78.5 N/A
length-6 9936 better 78.5 not sig. worse
tree entropy-6 8559 better 78.5 not sig. worse
tree entropy-8 11242 better 79.1 better
</table>
<tableCaption confidence="0.987595">
Table 2: Summary of pair-wise t-test with 95% confidence comparing the best set of grammars
</tableCaption>
<bodyText confidence="0.9671549375">
induced with the baseline (after 9 selection iterations) to the sets of grammars induced under
the proposed evaluation functions (fie, after 6 iterations, ft, after 6 and 8 iterations).
the three methods did not parse with the same
accuracy when all the sentences from the un-
labeled pool have been added to the training
set. Presenting the training examples in dif-
ferent orders changes the search path of the
induction process. Trained on data selected
by fte, the induced grammar parses the test
sentences with 79.1% accuracy, a small but
statistically significant improvement over the
baseline. This suggests that, when faced with
a dearth of training candidates, ft, can make
good use of the available data to induce gram-
mars that are comparable to those directly in-
duced from more data.
</bodyText>
<sectionHeader confidence="0.987938" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999959810810811">
This empirical study indicates that sample se-
lection can significantly reduce the human ef-
fort in parsing sentences for inducing gram-
mars. Our proposed evaluation function using
tree entropy selects helpful training examples.
Choosing from a large pool of unlabeled can-
didates, it significantly reduces the amount of
training annotations needed (by 36% in the
experiment). Although the reduction is less
dramatic when the pool of candidates is small
(by 27% in the experiment), the training ex-
amples it selected helped to induce slightly
better grammars.
The current work suggests many potential
research directions on selective sampling for
grammar induction. First, since the ideas be-
hind the proposed evaluation functions are
general and independent of formalisms, we
would like to empirically determine their ef-
fect on other parsers. Next, we shall explore
alternative formulations of evaluation func-
tions for the single-learner system. The cur-
rent approach uses uncertainty-based evalua-
tion functions; we hope to consider other fac-
tors such as confidence about the parameters
of the grammars and domain knowledge. We
also plan to focus on the constituent units
within a sentence as training examples. Thus,
the evaluation functions could estimate the
training utilities of constituent units rather
than full sentences. Another area of interest
is to experiment with committee-based sam-
ple selection using multiple learners. Finally,
we are interested in applying sample selection
to other natural language learning algorithms
that have been limited by the sparsity of an-
notated data.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988991189189189">
James K. Baker. 1979. Trainable grammars for
speech recognition. In Proceedings of the Spring
Conference of the Acoustical Society of Amer-
ica, pages 547-550, Boston, MA, June.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proceedings of the AAAI, pages 598-603, Prov-
idence, RI. AAAI Press/MIT Press.
David Cohn, Les Atlas, and Richard Ladner. 1994.
Improving generalization with active learning.
Machine Learning, 15(2):201-221.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the ACL,
pages 16-23, Madrid, Spain.
Thomas M. Cover and Joy A. Thomas. 1991. El-
ements of Information Theory. John Wiley.
Sean P. Engelson and Ido Dagan. 1996. Minimiz-
ing manual annotation cost in supervised train-
ing from copora. In Proceedings of the 34th An-
nual Meeting of the ACL, pages 319-326.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and
Naftali Tishby. 1997. Selective sampling using
the query by committee algorithm. Machine
Learning, 28(2-3):133-168.
Atsushi Fujii, Kentaro Inui, Takenobu Tokunaga,
and Hozumi Tanaka. 1998. Selective sampling
for example-based word sense disambiguation.
Computational Linguistics, 24(4):573-598, De-
cember.
Ulf Hermjakob and Raymond J. Mooney. 1997.
Learning parse and translation decisions from
examples with rich context. In Proceedings of
the Association for Computational Linguistics,
pages 482-489.
Rebecca Hwa. 1998. An empirical evaluation
of probabilistic lexicalized tree insertion gram-
</reference>
<page confidence="0.992954">
51
</page>
<reference confidence="0.98635715">
mars. In Proceedings of COLING-ACL, vol-
ume 1, pages 557-563.
Rebecca Hwa. 1999. Supervised grammar in-
duction using training data with limited con-
stituent information. In Proceedings of 37th An-
nual Meeting of the ACL, pages 73-79, June.
Rebecca Hwa. 2000. Learning Probabilistic Lex-
icalized Grammars for Natural Language Pro-
cessing. Ph.D. thesis, Harvard University.
Forthcoming.
K. Lan i and S.J. Young. 1990. The estimation
of stochastic context-free grammars using the
inside-outside algorithm. Computer Speech and
Language, 4:35-56.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers.
In Proceedings of the 17th Annual International
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 3-12.
Mitchell Marcus, Beatrice Santorini, and
Mary Ann MarcinIdewicz. 1993. Building
a large annotated corpus of English: the
Penn lleebank. Computational Linguistics,
19(2):313-330.
Robert McNaughton. 1967. Parenthesis gram-
mars. Journal of the ACM, 2(3):490-500.
Fernando Pereira and Yves Schabes. 1992. Inside-
Outside reestimation from partially bracketed
corpora. In Proceedings of the 30th Annual
Meeting of the ACL, pages 128-135, Newark,
Delaware.
Yves Schabes and Richard Waters. 1993. Stochas-
tic lexicalized context-free grammar. In Pro-
ceedings of the Third International Workshop
on Parsing Technologies, pages 257-266.
Cynthia A. Thompson, Mary Elaine Califf, and
Raymond J. Mooney. 1999. Active learning
for natural language parsing and information
extraction. In Proceedings of ICML-99, pages
406-414, Bled, Slovenia.
</reference>
<sectionHeader confidence="0.7300315" genericHeader="conclusions">
A Efficient Computation of Tree
Entropy
</sectionHeader>
<bodyText confidence="0.966265972222222">
The tree entropy of a sentence depends on the
quantity Evev Pr(v I G) log2(Pr(v J G)) de-
scribed in Section 4.2, a sum of an exponential
number of parses. Fortunately, through a dy-
namic programming algorithm similar to the
computation of the Inside Probabilities, this
quantity can be efficiently computed. The ba-
sic idea is to compose the tree entropy of the
entire sentence from the tree entropy of the
subtrees.
For illustrative purposes, we describe the
computation process using a PCFG grammar
expressed in Chomsky Normal Form, in which
each rule can have two forms: X -4 Y Z
or X —&gt; a, where X, Y, Z are variables over
non-terminal symbols and a is a variable over
terminal symbols. Moreover, let the sym-
bol S be the start symbol of the grammar
G. Following the notation of Lan i and Young,
we denote the inside probability as e(X, j),
which represents the probability that a non-
terminal X . tap Similarly, we define
a new function h(X, i, j) to represent the cor-
responding entropy for the set of subtrees.
h(X,i, j) = — E Pr(v G) log2(Pr(v I G)).
vEX
Therefore, EvEv Pr(v I G) log2 Pr(v I G) can
be expressed as h(S,1,n).
We compute all possible h(X, i, j) re-
cursively. The base case is h(X, i, i) =--
—e(X, i, i) log2 (e(X , i, i)) since a non-terminal
X can generate the symbol wi in exactly one
way. For the more general case, h(X,i, j), we
consider all the possible rules with X on the
left hand side that might have contributed to
build X &apos;3 wi wj.
</bodyText>
<equation confidence="0.975892">
j-1
h(X,i, j) = E E j).
k=i (X-+Y Z)
</equation>
<bodyText confidence="0.998694818181818">
The function hy,z,k (X, i, j) is a portion of
h(X, j) where Y wk and Z
wk+i wi. The non-terminals Y and Z may,
in turn, generate their substrings with mul-
tiple parses. Let there be a parses for Y
wi • • • Wk and 13 parses for Z Wk+1 • • • wi•
Let x denote the event of X —&gt; Y Z; y E
, , ya; and z E z1, . . . , zfl. The proba-
bility of one of the a x fl possible parses is
Pr(x)Pr(y)Pr(z), and hy,z,k is computed by
summing over all possible parses:
</bodyText>
<equation confidence="0.9471155">
hY,z,k (X, i, .i)
= — Ey,z Pr(x)Pr(y)Pr(z) x
log2 (Pr (x)Pr (y)Pr (z))
= — Ey,z Pr(x)Pr(y)Pr(z)x
[log2 Pr(x) + log2 Pr(y) + log2 Pr(z)]
—Pr(x) log2 Pr(x)e(Y, i, k)e(Z,k+1, j)
+Pr(x)h(Y,i, k)e(Z , k + 1,j)
+Pr(x)e(Y, k)h(Z, k + 1,j).
</equation>
<bodyText confidence="0.98555525">
These equations can be modified to compute
the tree entropy of sentences using a Prob-
abilistic Lexicalized Tree Insertion Grammar
(Hwa, 2000).
</bodyText>
<page confidence="0.998056">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.514156">
<title confidence="0.996096">Sample Selection for Statistical Grammar Induction *</title>
<author confidence="0.890294">Rebecca</author>
<affiliation confidence="0.7806935">Division of Engineering and Applied Harvard</affiliation>
<address confidence="0.999966">Cambridge, MA 02138</address>
<email confidence="0.999886">rebecca@eecs.harvard.edu</email>
<abstract confidence="0.996310230769231">Corpus-based grammar induction relies on using many hand-parsed sentences as training examples. However, the construction of a training corpus with detailed syntactic analysis for every sentence is a labor-intensive task. We propose to use sample selection methods to minimize the amount of annotation needed in the training data, thereby reducing the workload of the human annotators. This paper shows that the amount of annotated traindata can be reduced by degrading the quality of the induced grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Proceedings of the Spring Conference of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<location>Boston, MA,</location>
<contexts>
<context position="9712" citStr="Baker, 1979" startWordPosition="1537" endWordPosition="1538"> find locally optimal grammars. One form of partially supervised data might specify the phrasal boundaries without specifying their labels by bracketing each constituent unit with a pair of parentheses (McNaughton, 1967). For example, the parse tree for the sentence &amp;quot;Several fund managers expect a rough market this morning before prices stablize.&amp;quot; is labeled as &amp;quot;((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)&amp;quot; As shown in Pereira and Schabes (1992), an essentially unsupervised learning algorithm such as the InsideOutside re-estimation process (Baker, 1979; Lan i and Young, 1990) can be modified to take advantage of these bracketing constraints. For our sample selection experiment, we chose to work under the more stringent condition of partially supervised training data, as described above, because our ultimate goal is to minimize the amount of annotation done by humans in terms of both the number of sentences and the number of brackets within the sentences. Thus, the quality of our induced grammars should not be compared to those extracted from a fully annotated training corpus. The learning algorithm we use is a variant of the Inside-Outside </context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James K. Baker. 1979. Trainable grammars for speech recognition. In Proceedings of the Spring Conference of the Acoustical Society of America, pages 547-550, Boston, MA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the AAAI,</booktitle>
<pages>598--603</pages>
<publisher>AAAI Press/MIT Press.</publisher>
<location>Providence, RI.</location>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the AAAI, pages 598-603, Providence, RI. AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Les Atlas</author>
<author>Richard Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<pages>15--2</pages>
<contexts>
<context position="4423" citStr="Cohn et al., 1994" startWordPosition="692" endWordPosition="695">ng systems that receive training examples indiscriminately, a learning system that uses sample selection actively influences its progress by choosing new examples to incorporate into its training set. Sample selection works with two types of learning systems: a committee of learners or a single learner. The committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem). The candidate examples that led to the most disagreements among the different learners are considered to have the highest TUV (Cohn et al., 1994; Freund et al., 1997). For computationally intensive problems such as grammar induction, maintaining multiple learners may be an impracticality. In this work, we explore sample selection with a single learner that keeps just one working hypothesis at all times. Figure 1 outlines the single-learner sample selection training loop in pseudo-code. Initially, the training set, L, consists of a small number of labeled examples, based on which the learner proposes its first hypothesis of the target concept, C. Also available to the learner is a large pool of unlabeled training candidates, U. In each</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>David Cohn, Les Atlas, and Richard Ladner. 1994. Improving generalization with active learning. Machine Learning, 15(2):201-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="8382" citStr="Collins, 1997" startWordPosition="1332" endWordPosition="1333">eled candidates. L is a set of labeled training examples. C is the current hypothesis. Initialize: C Train(L). Repeat N E- Select(n,U,C, f). U U — N. L L U Label(N). C Train(L). Until (C. Ctrue)or (U = 0) or (human stops) 46 3 Grammar Induction The degree of difficulty of the task of learning a grammar from data depends on the quantity and quality of the training supervision. When the training corpus consists of a large reservoir of fully annotated parse trees, it is possible to directly extract a grammar based on these parse trees. The success of recent high-quality parsers (Charnia.k, 1997; Collins, 1997) relies on the availability of such treebank corpora. To work with smaller training corpora, the learning system would require even more information about the examples than their syntactic parse trees. For instance, Hermjakob and Mooney (1997) have described a learning system that can build a deterministic shiftreduce parser from a small set of training examples with the aid of detailed morphological, syntactical, and semantic knowledge databases and step-by-step guidance from human experts. The induction task becomes more challenging as the amount of supervision in the training data and backg</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the ACL, pages 16-23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley.</publisher>
<contexts>
<context position="13723" citStr="Cover and Thomas, 1991" startWordPosition="2186" endWordPosition="2189">her entropy value signifies a higher degree of uncertainty. At the highest uncertainty, the random variable is assigned one of n values over a uniform distribution, and the outcome would require 10g2(n) bits to encode. More formally, let V be a discrete random variable that can take any possible outcome in set V. Let p(v) be the density function p(v) = Pr(V = v), v E V. The entropy H (V) is the expected negative log likelihood of random variable V: H (V) = —EX(log2(p(V))). = E p(v) log2(p(v)). vEV Further details about the properties of entropy can be found in textbooks on information theory (Cover and Thomas, 1991). Determining the parse tree for a sentence from a set of possible parses can be viewed as assigning a value to a random variable. Thus, a direct application of the entropy definition to the probability distribution of the parses for sentence s in grammar G computes its tree entropy, TE(s,G), the expected number of bits needed to encode the distribution of possible parses for s. Note that we cannot compare sentences of different lengths by their entropy. For two sentences of unequal lengths, both with uniform distributions, the entropy of the longer one is higher. To normalize for sentence len</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean P Engelson</author>
<author>Ido Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from copora.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL,</booktitle>
<pages>319--326</pages>
<contexts>
<context position="6504" citStr="Engelson and Dagan, 1996" startWordPosition="1027" endWordPosition="1030">y be beneficial for many learning tasks in natural language processing. Although there exist abundant collections of raw text, the high expense of manually annotating the text sets a severe limitation for many learning algorithms in natFigure 1: The pseudo-code for the sample selection learning algorithm ural language processing. Sample selection presents an attractive solution to offset this labeled data sparsity problem. Thus far, it has been successfully applied to several classification applications. Some examples include text categorization (Lewis and Gale, 1994), part-of-speech tagging (Engelson and Dagan, 1996), word-sense disambiguation (Fujii et al., 1998), and prepositional-phrase attachment (Hwa, 2000). More difficult are learning problems whose objective is not classification, but generation of complex structures. One example in this direction is applying sample selection to semantic parsing (Thompson et al., 1999), in which sentences are paired with their semantic representation using a deterministic shift-reduce parser. Our work focuses on another complex natural language learning problem: inducing a stochastic context-free grammar that can generate syntactic parse trees for novel test senten</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Sean P. Engelson and Ido Dagan. 1996. Minimizing manual annotation cost in supervised training from copora. In Proceedings of the 34th Annual Meeting of the ACL, pages 319-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>H Sebastian Seung</author>
<author>Eli Shamir</author>
<author>Naftali Tishby</author>
</authors>
<title>Selective sampling using the query by committee algorithm.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>28--2</pages>
<contexts>
<context position="4445" citStr="Freund et al., 1997" startWordPosition="696" endWordPosition="699">eive training examples indiscriminately, a learning system that uses sample selection actively influences its progress by choosing new examples to incorporate into its training set. Sample selection works with two types of learning systems: a committee of learners or a single learner. The committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem). The candidate examples that led to the most disagreements among the different learners are considered to have the highest TUV (Cohn et al., 1994; Freund et al., 1997). For computationally intensive problems such as grammar induction, maintaining multiple learners may be an impracticality. In this work, we explore sample selection with a single learner that keeps just one working hypothesis at all times. Figure 1 outlines the single-learner sample selection training loop in pseudo-code. Initially, the training set, L, consists of a small number of labeled examples, based on which the learner proposes its first hypothesis of the target concept, C. Also available to the learner is a large pool of unlabeled training candidates, U. In each training iteration, t</context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28(2-3):133-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Kentaro Inui</author>
<author>Takenobu Tokunaga</author>
<author>Hozumi Tanaka</author>
</authors>
<title>Selective sampling for example-based word sense disambiguation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--4</pages>
<contexts>
<context position="6552" citStr="Fujii et al., 1998" startWordPosition="1033" endWordPosition="1036">guage processing. Although there exist abundant collections of raw text, the high expense of manually annotating the text sets a severe limitation for many learning algorithms in natFigure 1: The pseudo-code for the sample selection learning algorithm ural language processing. Sample selection presents an attractive solution to offset this labeled data sparsity problem. Thus far, it has been successfully applied to several classification applications. Some examples include text categorization (Lewis and Gale, 1994), part-of-speech tagging (Engelson and Dagan, 1996), word-sense disambiguation (Fujii et al., 1998), and prepositional-phrase attachment (Hwa, 2000). More difficult are learning problems whose objective is not classification, but generation of complex structures. One example in this direction is applying sample selection to semantic parsing (Thompson et al., 1999), in which sentences are paired with their semantic representation using a deterministic shift-reduce parser. Our work focuses on another complex natural language learning problem: inducing a stochastic context-free grammar that can generate syntactic parse trees for novel test sentences. Although abstractly, parsing with a grammar</context>
</contexts>
<marker>Fujii, Inui, Tokunaga, Tanaka, 1998</marker>
<rawString>Atsushi Fujii, Kentaro Inui, Takenobu Tokunaga, and Hozumi Tanaka. 1998. Selective sampling for example-based word sense disambiguation. Computational Linguistics, 24(4):573-598, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning parse and translation decisions from examples with rich context.</title>
<date>1997</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>482--489</pages>
<contexts>
<context position="8625" citStr="Hermjakob and Mooney (1997)" startWordPosition="1368" endWordPosition="1371">mmar Induction The degree of difficulty of the task of learning a grammar from data depends on the quantity and quality of the training supervision. When the training corpus consists of a large reservoir of fully annotated parse trees, it is possible to directly extract a grammar based on these parse trees. The success of recent high-quality parsers (Charnia.k, 1997; Collins, 1997) relies on the availability of such treebank corpora. To work with smaller training corpora, the learning system would require even more information about the examples than their syntactic parse trees. For instance, Hermjakob and Mooney (1997) have described a learning system that can build a deterministic shiftreduce parser from a small set of training examples with the aid of detailed morphological, syntactical, and semantic knowledge databases and step-by-step guidance from human experts. The induction task becomes more challenging as the amount of supervision in the training data and background knowledge decreases. To compensate for the missing information, the learning process requires heuristic search to find locally optimal grammars. One form of partially supervised data might specify the phrasal boundaries without specifyin</context>
</contexts>
<marker>Hermjakob, Mooney, 1997</marker>
<rawString>Ulf Hermjakob and Raymond J. Mooney. 1997. Learning parse and translation decisions from examples with rich context. In Proceedings of the Association for Computational Linguistics, pages 482-489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>An empirical evaluation of probabilistic lexicalized tree insertion grammars.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<volume>1</volume>
<pages>557--563</pages>
<contexts>
<context position="10462" citStr="Hwa, 1998" startWordPosition="1659" endWordPosition="1660"> work under the more stringent condition of partially supervised training data, as described above, because our ultimate goal is to minimize the amount of annotation done by humans in terms of both the number of sentences and the number of brackets within the sentences. Thus, the quality of our induced grammars should not be compared to those extracted from a fully annotated training corpus. The learning algorithm we use is a variant of the Inside-Outside algorithm that induces grammars expressed in the Probabilistic Lexicalized Tree Insertion Grammar representation (Schabes and Waters, 1993; Hwa, 1998). This formalism&apos;s context-free equivalence and its lexicalized representation make the training process efficient and computationally plausible. 4 Selective Sampling Evaluation Functions In this paper, we propose two uncertaintybased evaluation functions for estimating the training utilities of the candidate sentences. The first is a simple heuristic that uses the length of a sentence to estimate uncertainties. The second function computes uncertainty in terms of the entropy of the parse trees that the hypothesis-grammar generated for the sentence. 4.1 Sentence Length Let us first consider a </context>
</contexts>
<marker>Hwa, 1998</marker>
<rawString>Rebecca Hwa. 1998. An empirical evaluation of probabilistic lexicalized tree insertion grammars. In Proceedings of COLING-ACL, volume 1, pages 557-563.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Supervised grammar induction using training data with limited constituent information.</title>
<date>1999</date>
<booktitle>In Proceedings of 37th Annual Meeting of the ACL,</booktitle>
<pages>73--79</pages>
<contexts>
<context position="2121" citStr="Hwa (1999)" startWordPosition="336" endWordPosition="337"> the parse trees of sentences by hand), building a new corpus is a labor-intensive task. Therefore, it is worthwhile to consider ways of minimizing the size of the corpus to reduce the effort spent by annotators. • This material is based upon work supported by the National Science Foundation under Grant No. IRI 9712068. We thank Wheeler Rural for his plotting tool; and Stuart Shieber, Lillian Lee, Ric Crabbe, and the anonymous reviewers for their comments on the paper. There are two possible directions: one might attempt to reduce the amount of annotations in each sentence, as was explored by Hwa (1999); alternatively, one might attempt to reduce the number of training sentences. In this paper, we consider the latter approach using sample selection, an interactive learning method in which the machine takes the initiative of selecting potentially beneficial training examples for the humans to annotate. If the system could accurately identify a subset of examples with high Training Utility Values (TUV) out of a pool of unlabeled data, the annotators would not need to waste time on processing uninformative examples. We show that sample selection can be applied to grammar induction to produce hi</context>
</contexts>
<marker>Hwa, 1999</marker>
<rawString>Rebecca Hwa. 1999. Supervised grammar induction using training data with limited constituent information. In Proceedings of 37th Annual Meeting of the ACL, pages 73-79, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Learning Probabilistic Lexicalized Grammars for Natural Language Processing.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University. Forthcoming.</institution>
<contexts>
<context position="6601" citStr="Hwa, 2000" startWordPosition="1040" endWordPosition="1041">s of raw text, the high expense of manually annotating the text sets a severe limitation for many learning algorithms in natFigure 1: The pseudo-code for the sample selection learning algorithm ural language processing. Sample selection presents an attractive solution to offset this labeled data sparsity problem. Thus far, it has been successfully applied to several classification applications. Some examples include text categorization (Lewis and Gale, 1994), part-of-speech tagging (Engelson and Dagan, 1996), word-sense disambiguation (Fujii et al., 1998), and prepositional-phrase attachment (Hwa, 2000). More difficult are learning problems whose objective is not classification, but generation of complex structures. One example in this direction is applying sample selection to semantic parsing (Thompson et al., 1999), in which sentences are paired with their semantic representation using a deterministic shift-reduce parser. Our work focuses on another complex natural language learning problem: inducing a stochastic context-free grammar that can generate syntactic parse trees for novel test sentences. Although abstractly, parsing with a grammar can be seen as a classification task of determin</context>
</contexts>
<marker>Hwa, 2000</marker>
<rawString>Rebecca Hwa. 2000. Learning Probabilistic Lexicalized Grammars for Natural Language Processing. Ph.D. thesis, Harvard University. Forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan i</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm.</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<pages>4--35</pages>
<contexts>
<context position="9736" citStr="i and Young, 1990" startWordPosition="1540" endWordPosition="1543">imal grammars. One form of partially supervised data might specify the phrasal boundaries without specifying their labels by bracketing each constituent unit with a pair of parentheses (McNaughton, 1967). For example, the parse tree for the sentence &amp;quot;Several fund managers expect a rough market this morning before prices stablize.&amp;quot; is labeled as &amp;quot;((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)&amp;quot; As shown in Pereira and Schabes (1992), an essentially unsupervised learning algorithm such as the InsideOutside re-estimation process (Baker, 1979; Lan i and Young, 1990) can be modified to take advantage of these bracketing constraints. For our sample selection experiment, we chose to work under the more stringent condition of partially supervised training data, as described above, because our ultimate goal is to minimize the amount of annotation done by humans in terms of both the number of sentences and the number of brackets within the sentences. Thus, the quality of our induced grammars should not be compared to those extracted from a fully annotated training corpus. The learning algorithm we use is a variant of the Inside-Outside algorithm that induces g</context>
<context position="16434" citStr="i and Young, 1990" startWordPosition="2697" endWordPosition="2700">xistence of sentence s), we get: Pr(v,s I G) Pr(v G) p(v) = Pr(v s,G) = Pr(s(G) Pr(s ( G). Replacing the generic density function term in the entropy definition, we derive the expression for TE(s,G), the tree entropy of s: TE(s,G) = H(V) — Ep(v) log2 p(v) vEV Pr(v G) Pr(v G)1 Pr(s G) 4()g2 Pr(s IG)I vEV Pr(v I G) pr(v Pr(s G) 82 vEV z Pr(v I G) (s G)g2 Pr(s I G) Pr vEV 48 EvEy Pr(t) I G) log2 Pr(v I G) Pr(s I G) + log2 Pr(s G)EvEy Pr(v I G) Pr(s I G) EvEv Pr(v I G) log2 Pr(v I G) Pr(s I G) + log2 Pr(s G) Using the bottom-up, dynamic programming technique of computing Inside Probabilities (Lan i and Young, 1990), we can efficiently compute the probability of the sentence, Pr(s G). Similarly, the algorithm can be modified to compute the quantity EvEv Pr(v I G) log2(Pr(v f G)) (see Appendix A). 5 Experimental Setup To determine the effectiveness of selecting training examples with the two proposed evaluation functions, we compare them against a baseline of random selection (frend(s, G) = rand()). The task is to induce grammars from selected sentences in the Wall Street Journal (WSJ) corpus, and to parse unseen test sentences with the trained grammars Because the vocabulary size (and the grammar size by</context>
</contexts>
<marker>i, Young, 1990</marker>
<rawString>K. Lan i and S.J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>3--12</pages>
<contexts>
<context position="6453" citStr="Lewis and Gale, 1994" startWordPosition="1021" endWordPosition="1024">an resources are exhausted. Sample selection may be beneficial for many learning tasks in natural language processing. Although there exist abundant collections of raw text, the high expense of manually annotating the text sets a severe limitation for many learning algorithms in natFigure 1: The pseudo-code for the sample selection learning algorithm ural language processing. Sample selection presents an attractive solution to offset this labeled data sparsity problem. Thus far, it has been successfully applied to several classification applications. Some examples include text categorization (Lewis and Gale, 1994), part-of-speech tagging (Engelson and Dagan, 1996), word-sense disambiguation (Fujii et al., 1998), and prepositional-phrase attachment (Hwa, 2000). More difficult are learning problems whose objective is not classification, but generation of complex structures. One example in this direction is applying sample selection to semantic parsing (Thompson et al., 1999), in which sentences are paired with their semantic representation using a deterministic shift-reduce parser. Our work focuses on another complex natural language learning problem: inducing a stochastic context-free grammar that can g</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A sequential algorithm for training text classifiers. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann MarcinIdewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn lleebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="3418" citStr="Marcus et al., 1993" startWordPosition="536" endWordPosition="540">to use uncertainty-based evaluation functions that estimate the TUV of a sentence by quantifying the grammar&apos;s uncertainty about assigning a parse tree to this sentence. We have considered two functions. The first is a simple heuristic that approximates the grammar&apos;s uncertainty in terms of sentence lengths. The second computes uncertainty in terms of the tree entropy of the sentence. This metric is described in detail later. This paper presents an empirical study measuring the effectiveness of our evaluation functions at selecting training sentences from the Wall Street Journal (WSJ) corpus (Marcus et al., 1993) for inducing grammars. Conducting the experiments with training pools of different sizes, we have found that sample selection based on tree entropy reduces a large training pool by 36% and a small training pool by 27%. These results suggest that sample selection can significantly reduce -human effort exerted in building training corpora. 45 2 Sample Selection Unlike traditional learning systems that receive training examples indiscriminately, a learning system that uses sample selection actively influences its progress by choosing new examples to incorporate into its training set. Sample sele</context>
</contexts>
<marker>Marcus, Santorini, MarcinIdewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann MarcinIdewicz. 1993. Building a large annotated corpus of English: the Penn lleebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert McNaughton</author>
</authors>
<title>Parenthesis grammars.</title>
<date>1967</date>
<journal>Journal of the ACM,</journal>
<pages>2--3</pages>
<contexts>
<context position="9321" citStr="McNaughton, 1967" startWordPosition="1477" endWordPosition="1478">rser from a small set of training examples with the aid of detailed morphological, syntactical, and semantic knowledge databases and step-by-step guidance from human experts. The induction task becomes more challenging as the amount of supervision in the training data and background knowledge decreases. To compensate for the missing information, the learning process requires heuristic search to find locally optimal grammars. One form of partially supervised data might specify the phrasal boundaries without specifying their labels by bracketing each constituent unit with a pair of parentheses (McNaughton, 1967). For example, the parse tree for the sentence &amp;quot;Several fund managers expect a rough market this morning before prices stablize.&amp;quot; is labeled as &amp;quot;((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)&amp;quot; As shown in Pereira and Schabes (1992), an essentially unsupervised learning algorithm such as the InsideOutside re-estimation process (Baker, 1979; Lan i and Young, 1990) can be modified to take advantage of these bracketing constraints. For our sample selection experiment, we chose to work under the more stringent condition of partially supervised trai</context>
</contexts>
<marker>McNaughton, 1967</marker>
<rawString>Robert McNaughton. 1967. Parenthesis grammars. Journal of the ACM, 2(3):490-500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>InsideOutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the ACL,</booktitle>
<pages>128--135</pages>
<location>Newark, Delaware.</location>
<contexts>
<context position="9603" citStr="Pereira and Schabes (1992)" startWordPosition="1520" endWordPosition="1523">background knowledge decreases. To compensate for the missing information, the learning process requires heuristic search to find locally optimal grammars. One form of partially supervised data might specify the phrasal boundaries without specifying their labels by bracketing each constituent unit with a pair of parentheses (McNaughton, 1967). For example, the parse tree for the sentence &amp;quot;Several fund managers expect a rough market this morning before prices stablize.&amp;quot; is labeled as &amp;quot;((Several fund managers) (expect ((a rough market) (this morning)) (before (prices stabilize))).)&amp;quot; As shown in Pereira and Schabes (1992), an essentially unsupervised learning algorithm such as the InsideOutside re-estimation process (Baker, 1979; Lan i and Young, 1990) can be modified to take advantage of these bracketing constraints. For our sample selection experiment, we chose to work under the more stringent condition of partially supervised training data, as described above, because our ultimate goal is to minimize the amount of annotation done by humans in terms of both the number of sentences and the number of brackets within the sentences. Thus, the quality of our induced grammars should not be compared to those extrac</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. InsideOutside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the ACL, pages 128-135, Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Richard Waters</author>
</authors>
<title>Stochastic lexicalized context-free grammar.</title>
<date>1993</date>
<booktitle>In Proceedings of the Third International Workshop on Parsing Technologies,</booktitle>
<pages>257--266</pages>
<contexts>
<context position="10450" citStr="Schabes and Waters, 1993" startWordPosition="1655" endWordPosition="1658">on experiment, we chose to work under the more stringent condition of partially supervised training data, as described above, because our ultimate goal is to minimize the amount of annotation done by humans in terms of both the number of sentences and the number of brackets within the sentences. Thus, the quality of our induced grammars should not be compared to those extracted from a fully annotated training corpus. The learning algorithm we use is a variant of the Inside-Outside algorithm that induces grammars expressed in the Probabilistic Lexicalized Tree Insertion Grammar representation (Schabes and Waters, 1993; Hwa, 1998). This formalism&apos;s context-free equivalence and its lexicalized representation make the training process efficient and computationally plausible. 4 Selective Sampling Evaluation Functions In this paper, we propose two uncertaintybased evaluation functions for estimating the training utilities of the candidate sentences. The first is a simple heuristic that uses the length of a sentence to estimate uncertainties. The second function computes uncertainty in terms of the entropy of the parse trees that the hypothesis-grammar generated for the sentence. 4.1 Sentence Length Let us first</context>
</contexts>
<marker>Schabes, Waters, 1993</marker>
<rawString>Yves Schabes and Richard Waters. 1993. Stochastic lexicalized context-free grammar. In Proceedings of the Third International Workshop on Parsing Technologies, pages 257-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Mary Elaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Active learning for natural language parsing and information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of ICML-99,</booktitle>
<pages>406--414</pages>
<location>Bled, Slovenia.</location>
<contexts>
<context position="6819" citStr="Thompson et al., 1999" startWordPosition="1071" endWordPosition="1074">e processing. Sample selection presents an attractive solution to offset this labeled data sparsity problem. Thus far, it has been successfully applied to several classification applications. Some examples include text categorization (Lewis and Gale, 1994), part-of-speech tagging (Engelson and Dagan, 1996), word-sense disambiguation (Fujii et al., 1998), and prepositional-phrase attachment (Hwa, 2000). More difficult are learning problems whose objective is not classification, but generation of complex structures. One example in this direction is applying sample selection to semantic parsing (Thompson et al., 1999), in which sentences are paired with their semantic representation using a deterministic shift-reduce parser. Our work focuses on another complex natural language learning problem: inducing a stochastic context-free grammar that can generate syntactic parse trees for novel test sentences. Although abstractly, parsing with a grammar can be seen as a classification task of determining the structure of a sentence by selecting one tree out of a set of possible parse trees, there are two major distinctions that differentiate it from typical classification problems. First, a classifier usually choos</context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>Cynthia A. Thompson, Mary Elaine Califf, and Raymond J. Mooney. 1999. Active learning for natural language parsing and information extraction. In Proceedings of ICML-99, pages 406-414, Bled, Slovenia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>