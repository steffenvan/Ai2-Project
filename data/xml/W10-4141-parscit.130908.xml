<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001263">
<title confidence="0.998924">
Chinese Word Segmentation with Conditional Support Vector In-
spired Markov Models
</title>
<author confidence="0.997924">
Yu-Chieh Wu Jie-Chi Yang Yue-Shi Lee
</author>
<affiliation confidence="0.8112925">
1Dep. of Computer Science and Graduate Institute of Net- Dep. of Computer Science
Information Engineering; work Learning and Information Engineering
National Central University National Central University Ming Chuan University
2Finance Department Taoyuan, Taiwan Taoyuan, Taiwan
Ming Chuan University yang@cl.ncu.edu.tw leeys@mcu.edu.tw
Taipei, Taiwan
</affiliation>
<email confidence="0.480892">
bcbb@db.csie.ncu.edu.
tw
</email>
<sectionHeader confidence="0.980217" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999647095238095">
In this paper, we present the proposed me-
thod of participating SIGHAN-2010 Chi-
nese word segmentation bake-off. In this
year, our focus aims to quick train and test
the given data. Unlike the most structural
learning algorithms, such as conditional
random fields, we design an in-house devel-
opment conditional support vector Markov
model (CMM) framework. The method is
very quick to train and also show better per-
formance in accuracy than CRF. To give a
fair comparison, we compare our method to
CRF with three additional tasks, namely,
CoNLL-2000 chunking, SIGHAN-3 Chi-
nese word segmentation. The results were
encourage and indicated that the proposed
CMM produces better not only accuracy but
also training time efficiency. The official re-
sults in SIGHAN-2010 also demonstrates
that our method perform very well in tradi-
tional Chinese with fine-tuned features set.
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989625882353">
Since 2006 Chinese word segmentation bakeoff
in SIGHAN-3 (Levow, 2006), this is the third
time to join the competition (Wu et al., 2006,
2007). In this year, we join the SIGHAN bakeoff
task in both traditional and simplified Chinese
closed word segmentation. Unlike most western
languages, there is no explicit space between
words. The goal of word segmentation is to iden-
tify words given the sentence. This technique
provides important features for downstream pur-
poses. Examples include Chinese part-of-speech
(POS) tagging (Wu et al., 2007), Chinese word
dependency parsing (Wu et al., 2007, 2008).
With the rapid growth of structural learning
algorithms, such as conditional random fields
(CRFs) (Lafferty et al., 2001) and maximum-
margin Markov models (M3N) (Taskar et al.,
2003) have received a great attention and be-
come a prominent learning algorithm to many
sequential labeling tasks. Examples include part-
of-speech (POS) tagging (Shen et al., 2007) and
syntactic phrase chunking (Suzuki et al., 2007).
The Chinese word segmentation can also be
treated as a character-based tagging task in (Xue
and Converse, 2002). One feature of sequential
labeling is that it aims at finding non-recursive
chunk fragments in a given sentence. Among
these approaches, CRF has been wildly used in
recent SIGHAN bakeoff tasks (Jin and Chen,
2008; Levow, 2006).
Although these approaches do not suffer from
so-called label-bias problems (Lafferty et al.,
2001), one limitation is that they are inefficient
to train with large-scale, especially large catego-
ry data. On the other hand, non-structural learn-
ing approaches (e.g. maximum entropy models)
which learn local predictors usually cost much
better training time performance than structural
learning algorithms. These methods condition on
local context features and incorporate fix-length
history information. Although higher order fea-
ture (longer history) maybe useful to some tasks,
the exponential scaled inference time is also in-
tractable in practice.
Support vector machines (SVMs) which is one
of the state-of-the-art supervised learning algo-
rithms have been widely employed as local clas-
sifiers to many sequential labeling tasks (Taku
and Matsumoto, 2001; Wu et al., 2006, 2008).
Specially, the training time of linear kernel SVM
with either L1-norm (Joachims, 2006; Keerthi et
al., 2008) or L2-norm (Keerthi and DeCoste,
2005; Hsieh et al., 2008) can now be obtained in
linear time. Even local classifier-based ap-
proaches have the drawbacks of label-bias prob-
lems, training nonstructural linear SVM is scala-
ble to large-scale data. By means of so-called
one-versus-all multiclass SVM training, it is also
scalable to large-category data.
In this paper, we present our Chinese word
segmentation based on the proposed conditional
support vector Markov models for sequential
labeling tasks, especially Chinese word segmen-
tation. Unlike structural learning algorithms, our
method can be simply trained without consider-
ing the entire structures and hence the training
time scales linearly with the number of training
examples. In this framework, to alleviate the ease
of label-bias problems, the state transition proba-
bility is ignored. Instead, we merely utilize the
property of label relationships between chunks
(Wu et al., 2008). To demonstrate our method,
we compare to several well-known structural
learning algorithms, like CRF (Kudo et al., 2004),
and SVM-HMM (Joachims et al., 2009) on two
well-known data, namely, CoNLL-2000 syntac-
tic chunking, SIGHAN-3 Chinese word segmen-
tation tasks. By following this, we apply the
model to the Chinese word segmentation tasks of
SIGHAN-2010 this year. The empirical results
showed that our method is not only fast but also
achieving more superior accuracy than structural
learning methods. In traditional Chinese, our me-
thod also achieves the state-of-the-art perfor-
mance in accuracy with fined-tune features.
</bodyText>
<sectionHeader confidence="0.967237" genericHeader="method">
2 Conditional support vector Markov
models
</sectionHeader>
<bodyText confidence="0.996757666666667">
Traditional conditional Markov models (CMM)
is to assign the tag sequence which maximizes
the observation sequence.
</bodyText>
<equation confidence="0.748561">
P s s s n o o o n
( 1 , 2 ,...,  |1 , 2 ,..., )
</equation>
<bodyText confidence="0.998674333333333">
Where si is the tag of word i. For the first order
left-to-right CMM, the chain rule decomposes
the probabilistic function as:
</bodyText>
<equation confidence="0.983416666666667">
n
P(s1,s2, ... ,sn |o1,o2, ... , on) =fjP(si  |si-1,oi) (1)
i 1
</equation>
<bodyText confidence="0.997748875">
Therefore, we can employ a local classifier to
predict P(si  |si-1, oi) and the optimal tag sequence
can be efficiently searched by using conventional
Viterbi algorithm.
The graphic illustration of the K-th order left-
to-right CMM is shown in Figure 1. The chain
probability decompositions of the other K-th or-
der CMM in Figure 1 are:
</bodyText>
<equation confidence="0.998600882352941">
n
P s o
( , ) = fj P si oi
(  |) (2)
i=
1
n
P s o
( , ) = fjP(si  |oi , si-1) (3)
i=2
n
P s o
( , ) = fjP(si  |oi , si-1, si-2) (4)
i 3
n
P(s,o)=fjP(si |oi,si-1,si-1) (5)
i=3
</equation>
<bodyText confidence="0.988664">
Equations (2), (3), and (4) are merely standard
zero, first and second order decompositions,
while equation (5) is the proposed greedy second
order CMM decomposition which will be dis-
cussed in next section.
</bodyText>
<figureCaption confidence="0.98201925">
Figure 1: K-th order conditional Markov models: (a)
the standard 0(zero) order CMM, (b) first order CMM,
(c) second order CMM, and (d) the proposed second
order CMM
</figureCaption>
<bodyText confidence="0.999987">
The above decompositions merge the transi-
tion and emission probability with single func-
tion. McCallum et al. (2000) further combined
the locally trained maximum entropy with the
infered transition score. However, our condition-
al support vector Markov models make different
chain probability. We replace the original transi-
tion probability with transition validity score, i.e.
</bodyText>
<equation confidence="0.990266111111111">
n
~
P s o
( , ) = fjP(si  |si-1 )P(si |oi) (6)
i=2
n
P s o
( , ) Ö= fl P(si  |si-1 )P(si  |oi , si-1, si-1) (7)
i=3
</equation>
<bodyText confidence="0.9999285">
The transition validity score is merely a Boo-
lean flag which indicates the relationships be-
tween two neighbor labels. Equation (6) and (7)
are zero-order and our second order chain prob-
abilities. We will introduce the proposed infe-
rence algorithm and how to obtain the transition
validity score automatically without concerning
the change of chunk representation.
</bodyText>
<subsectionHeader confidence="0.997245">
2.1 Tag transitions
</subsectionHeader>
<bodyText confidence="0.9999807">
In this paper, we do not explicitly adopt the state
transitions for our CMM. Instead, a chunk-
relation pair is used. Nevertheless, one important
property to sequential chunk labeling is that there
is only one phrase type in a chunk. For example,
if the previous word is tagged as begin of noun
phrase (B-NP), the current word must not be end
of the other phrase (E-VP, E-PP, etc). Therefore,
we only model relationships between chunk tags
to generate valid phrase structure.
Wu et al. (2007, 2008) presented an automatic
chunk pair relation construction algorithm which
can handle so-called IOB1/IOB2/IOE1/IOE2
(Kudo and Matsumoto, 2001) chunk representa-
tion structures with either left-to-right or right-to-
left directions. Here, we extend this idea and ge-
neralize to fit to more chunk tags. That is we can
model the S-tag, B2, B3 tags with dividing the
leading tags into two categories. For details can
refer the literatures.
</bodyText>
<sectionHeader confidence="0.990563" genericHeader="method">
3 Empirical Results
</sectionHeader>
<bodyText confidence="0.999730833333333">
Three large-scale and large-category dataset is
used to evaluate the proposed method, namely,
CoNLL-2000 syntactic chunking (Tjong Kim
Sang and Buchholz, 2000), Chinese POS tagging,
and three of SIGHAN-3 word segmentation tasks.
Table 1 shows the statistics of those datasets.
</bodyText>
<equation confidence="0.512382333333333">
Feature CoNLL-2000 SIGHAN-3
type
Unigram w-2~w+2 w-2~w+2
</equation>
<bodyText confidence="0.997737466666667">
CoNLL-2000 chunking task is a well-known
and widely evaluated in many literatures (Suzuki
et al., 2007; Ando and Zhang, 2005; Kudo and
Matsumoto, 2001; Wu et al., 2008; Daumé III
and Marcu, 2005). The training data was derived
from Treebank WSJ section 15-18 while section
20 was used for testing. The goal is to find the
non-recursive phrase structures in a sentence,
such as noun phrase (NP), verb phrase (VP), etc.
There are 11 phrase types in this dataset. We fol-
low the previous best settings for SVMs (Kudo
and Matsumoto, 2001; Wu et al., 2008). The
IOE2 is used to represent the phrase structure
and tagged the data with backward direction.
The training and testing data of the Chinese
POS tagging is mainly derived from the Aca-
demic Sinica¶s balanced corpus (version 3.0).
Seventy-five percent out of the data is used for
training while the remaining 25% is used for test-
ing. However, the task of the Chinese POS tag-
ging is very different from classical English POS
tagging in that there is no word boundary infor-
mation in Chinese text. To achieve this, Ng and
Low (2004) gave a successful study on Chinese
POS tagging. Just as English phrase chunking,
the IOB-tags can be used to represent the Chi-
nese word and its part-of-speech tag. For exam-
ple, the tag B-ADJ means the first character of a
Chinese word which POS tag is ADJ (adjective).
n this task, we simply use the IOB2 to represent
the chunk structure. In this way, the tagger needs
to recognize the chunk tag by considering 118
(59*2) categories at once.
As discussed in (Zhou and Kit, 2007), using
more complex chunk representation bring better
segmentation accuracy in several Chinese word
segmentation benchmarks. It is very useful in
particular to represent long Chinese word (in par-
ticular proper nouns). By following this line, we
apply the six tags B, BI, I, IE, E, and S to
represent the Chinese word. BI and IE are the
interior after begin and interior before end of a
chunk. B/I/E/S tags indicate the be-
gin/interior/end/single of a chunk. Figure 2 lists
the used feature set in both experiments.
</bodyText>
<equation confidence="0.977377">
(w-2,w-1),(w-1,w0),
(w0,w+1),
(w+1,w+2),(w+1,w-1)
ams
(Zhou and Kit, 2007)
Bigram (w-2,w-1),(w-1,w0),
(w0,w+1),
(w+1,w+2),(w+1,w-1)
POS p-2~p+2
POS bigram (p-2,p-1),(p-1,p0),
(p0,p+1),(p+1,p+2),
(p+1,p-1)
POS trigram (p-2,p-1,p0),
(p-1,p0,p+1),(p-3,p-2,p-1),
(p0,p+1,p+2), (p+1,p+2,p+3)
(Word+POS)
</equation>
<footnote confidence="0.9332308">
bigram
Other
features 2~4 prefix letters
Orthographic feature
(Wu et al., 2008)
</footnote>
<subsectionHeader confidence="0.988068">
3.1 Settings
</subsectionHeader>
<bodyText confidence="0.999925375">
We included the Liblinear with square loss
(Hsieh et al., 2008) into our conditional Markov
models as classification algorithms. In basic, the
SVM was designed for binary classification
problems. To port to multiclass problems, we
adopted the well-known one-versus-all (OVA)
method. One good property of OVA is that pa-
rameter estimation process can be trained indivi-
</bodyText>
<equation confidence="0.9629">
(w-1,p0),(w-2,p-1) (w0,p+1),
(w+1,p+2)
</equation>
<page confidence="0.4649">
2~4 suffix letters AV feature of 2~6 gr
</page>
<figureCaption confidence="0.989989">
Figure 2: Feature templates used in experiments
</figureCaption>
<tableCaption confidence="0.980315">
Table 2: SIGHAN-3 word segmentation results
</tableCaption>
<table confidence="0.999814777777778">
SIGHAN-3 UPUC MSRA CityU
Method F(p) Training Testing F(p) Training Testing F(p) Training Testing
Time Time Time Time Time Time
Our method 93.86 0.06 hr 15.15 s 96.22 0.45 hr 15.41 s 97.26 0.26 hr 25.32 s
CRF 93.76 1.17 hr 23.48 s 96.11 3.63 hr 17.06 s 97.29 4.34 hr 31.29 s
SVM-HMM Out-of-memory Out-of-memory Out-of-memory
Best approach (Zhou and 94.28 N/A N/A 96.34 N/A N/A 97.43 N/A N/A
Kit, 2007)
Second best approach 93.30 N/A N/A 96.30 N/A N/A 97.20 N/A N/A
</table>
<tableCaption confidence="0.988021">
Table 3: Official evaluation results of the traditional and simplified Chinese word segmentation tasks
</tableCaption>
<table confidence="0.997338375">
Task Literature Computer
Recall Precision F1 OOV-RR IV-RR Recall Precision F1 OOV-RR IV-RR
Traditional 0.942 0.942 0.942 0.788 0.958 0.948 0.957 0.952 0.666 0.977
Simplified 0.936 0.932 0.934 0.564 0.964 0.915 0.915 0.915 0.594 0.972
Task Medicine Finance
Recall Precision F1 OOV-RR IV-RR Recall Precision F1 OOV-RR IV-RR
Traditional 0.953 0.957 0.955 0.798 0.966 0.964 0.962 0.963 0.812 0.975
Simplified 0.933 0.915 0.924 0.642 0.969 0.945 0.941 0.943 0.666 0.972
</table>
<bodyText confidence="0.997996833333333">
dually. This is in particularly useful to the tasks
which involve training large number of features
and categories (Wu et al., 2008). To obtain the
probability output from SVM, we employ the
sigmoid function with fixed parameter A=-2 and
B=0 as (Platt, 1999).
</bodyText>
<subsectionHeader confidence="0.99904">
3.2 Comparison to structural learning
</subsectionHeader>
<bodyText confidence="0.999846071428572">
The overall experimental results are summarized
in Table 1. Column &amp;quot;All&amp;quot; denotes as the F(p)
score of all chunk types, while &amp;quot;NP&amp;quot; is the F(p)
score of the noun phrase only. The final two col-
umns list the entire training and testing times.
As shown in Table 1, it is surprising that the
proposed CMM outperforms the other structural
learning methods, CRF and SVM-HMM. In
terms of training time, our method shows sub-
stantial faster than CRF. However, in terms of
testing time, our method is worse than CRF. The
main reason is that we do not optimize the code
and implementation. We trust this can be further
improved.
</bodyText>
<tableCaption confidence="0.9983755">
Table 1: Syntactic chunking results of the proposed
CMM and the selected structural learning methods.
</tableCaption>
<table confidence="0.994991">
Method All NP Training Time Testing Time
Our method 94.51 94.95 0.15 hr 13.72 s
CRF 93.67 93.93 0.88 hr 6.20 s
SVM-HMM 93.90 94.20 0.20 hr 13.60 s
</table>
<bodyText confidence="0.996370944444445">
Table 2 shows the experimental results of the
SIGHAN-3 bake-off tasks. We ran and con-
ducted the experiments with UPUC, MSRA, and
CityU datasets. The final two rows in Table 5 list
the top 1 and 2 scores of published papers.
Here, the SVM-HMM still suffer from the sca-
lability problems. Similar to the findings in the
Chinese POS tagging task, the zero-order CMM
achieved the optimal accuracy among first-order,
full second order and the proposed inference al-
gorithms. The training time is still very efficient
for most CMMs. In comparison to CRF, our me-
thod did clearly perform better accuracy (ex-
cepted for the CityU) and require much less
training time. For example, for the CityU dataset,
our 0-order CMM took less than 15 minutes to
train, while the CRF takes 4.34 hours in training.
However, we observe that our CMM yielded
better testing time speed than CRF in this task.
We further exploit the trained SVM models and
found that the produced weights were not as
dense as CRF which produces many nonzero
weights per category. In addition, we observed
that our implementation worked very efficient in
the small category tasks.
For the three datasets, our method produced
very competitive results as previous best ap-
proach which also made use of CRF as classifiers.
Although we use the same techniques to de-
rive global features (assessor variety (AV) fea-
ture with 2~6 grams) from both training and test-
ing data, our CMMs and the conducted CRF
could not perform as well as (Zhou and Kit,
2007). In our experiments, both CRF and CMMs
received the same training set. Hence the CRF
and our CMMs is comparable in this experiment.
</bodyText>
<subsectionHeader confidence="0.999156">
3.3 Official Results in SIGHAN-2010
</subsectionHeader>
<bodyText confidence="0.984891388888889">
To apply CMM to SIGHAN-2010, we design the
following strategy. First the classifier parameters,
feature set should be improved. To achieve this,
1/4 of the training data was used as development
set, while the remaining 3/4 training data was
used to train the classifier. Second, we combine
multi-classifier to enhance the accuracy. The
CRF and our CMM with basic feature set were
trained to predict the initial labels of the testing
data. Then the predicted labels were included as
features to train the final-stage classifier. The
final classifier is still our CMM. Third, the post-
processing method (Low et al., 2005) is em-
ployed to enhance the unknown word segmenta-
tion.
Table 4 lists the empirical results of the devel-
opment set. By validate with development data,
we found that C=1.25 and use the E-BIES repre-
sentation method (Wu et al., 2008) yields better
accuracy than B-BIES (Zhou and Kit, 2007).
Meanwhile, CRF seems to be suitable for B-
BIES representation method.
The classifier parameters were fixed and then
we try to search the optimal feature set via the
incremental add-and-check method. That is, we
use the initial feature set as basis and add one
feature type from the pool and verify the good-
ness of the feature with the development data.
Figure 3 figures out the used features of each
pass.
In this year, the process was completely run-
through for the traditional Chinese task. Unfor-
tunately we have insufficient time to apply the
same technique to Simplified Chinese task. Table
3 lists the official results in the SIGHAN 2010
Chinese word segmentation bake-off.
</bodyText>
<tableCaption confidence="0.9891175">
Table 4: Empirical results of the development set of
single CRF and our CMM
</tableCaption>
<table confidence="0.999559">
Development Traditional Simplified
dataset Chinese Chinese
B-BIES E-BIES B-BIES E-BIES
Our method 97.40 97.42 97.34 97.37
CRF 97.07 97.10 97.07 96.96
</table>
<sectionHeader confidence="0.992979" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99992088">
In this paper, we investigate the issues of sequen-
tial chunk labeling and present the conditional
support vector Markov models for this purpose.
The experiments were conducted with two well-
known datasets, includes CoNLL-2000 text
chunking and SIGHAN-3 Chinese word segmen-
tation. The experimental results showed that our
method scales very well while achieving surpris-
ing good accuracy than structural learning me-
thods. On the SIGHAN-3 task, the proposed me-
thod outperformed CRF, while substantially re-
duced the training time. We also apply such me-
thod to the SIGHAN-2010 traditional Chinese
segmentation with fined tuned feature set. The
result was also encouraged. Our approach ob-
tains the best accuracy in this task. In terms of
Simplified Chinese, we achieve mid-rank place
due to the very limited time-constraint. In the
future, we plan to completely adopt this method
to the Simplified Chinese word segmentation
with the elaborated feature selection metrics and
the same post-processing method.
The full online demonstration of the proposed
conditional support vector Markov models can
be found at the web site1.
</bodyText>
<table confidence="0.985888444444444">
Feature Pass1: CRF/CMM Pass2: CMM
Name
Character w-2—w+2 Feature set of
Pass1
Character (w-2,w-1),(w-1,w0),
N-gram (w0,w+1),(w+1,w+2),(w+
1,w-1)
Special w-2—w+2
Character
flags (Low
et al., 2005)
Others 2AV feature and its 2- 2AV feature
gram combinations and its 2-gram
and 3-gram
combinations
Future N/A t+1, t+2, t+3,(t0,t+
flags1 2),(t+1,t+2),(w0,t+
1),(w0,t+2)
</table>
<figureCaption confidence="0.794352">
Future flags: the predicted tags of previous classifier
Figure 3: Feature templates used in experiments
</figureCaption>
<sectionHeader confidence="0.995777" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998199675925926">
Rie K. Ando, and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking, In Proc. of ACL, pp. 1-9.
Bernhard E. Boser, Isabelle M. Guyon, Vladimir N.
Vapnik. 1992. A training algorithm for optimal
margin classifiers, In Proc. of COLT, pp. 144-152.
Andrew McCallum, Dayne Freitag, and Fernando C.
N. Pereira. 2000. Maximum entropy Markov
models for information extraction and
segmentation, In Proc. of ICML, pp. 591.598.
Hal Daumé III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin
methods for structured prediction, In Proc. of
ICML, pp. 169-176.
Guangjin Jin and Xiao Chen. 2008.
The Fourth International Chinese Language Proces
sing Bakeoff: Chinese Word Segmentation Named
Entity Recognition and Chinese POS Tagging. In
Proc. of the SIGHAN Workshop on Chinese
Language Processing, pp. 69-81.
1 http://140.115.112.118/bcbb/Chunking.htm
Thorsten Joachims. 2006. Training linear SVMs in
linear time, In Proc. of KDD, pp. 217-226.
Thorsten Joachims, Thomas Finley, and Chun-Nam
Yu. 2009. Cutting-Plane Training of Structural
SVMs, Machine Learning Journal, to appear.
Sathiya Keerthi and Dennis DeCoste. 2005. A
modified finite Newton method for fast solution of
large scale linear SVMs, JMLR, 6: 341-361.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proc. of NAACL,
pp. 192-199.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis, In Proc. of
EMNLP, pp. 230-237.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields:
probabilistic models for segmenting and labeling
sequence data, In Proc. of ICML, pp. 282-289.
Gina-Anne Levow. 2006. The third international
Chinese language processing bakeoff: Word
segmentation and named entity recognition. In
Proc. of the SIGHAN Workshop on Chinese
Language Processing, pp. 108±117.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.
2005. A maximum entropy approach to Chinese
word segmentation. In Proc. of the SIGHAN
Workshop on Chinese Language Processing, pp.
161-164.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging. one-at-a-time or all-at-once?
word-based or character-based? In Proc. of
EMNLP, pp. 277-284.
John Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized
likelihood methods, In Advances in Large Margin
Classifiers.
Jun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007.
Semi-supervised structural output learning based
on a hybrid generative and discriminative approach,
In Proc. of EMNLP-CoNLL, pp. 791-800.
Jun Suzuki and Hideki Isozaki. 2008. Semi-
Supervised Sequential Labeling and Segmentation
using Giga-word Scale Unlabeled Data. In Proc. of
ACL, pp. 665-673.
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin Markov networks, In Proc. of
NIPS.
Eric F. Tjong Kim Sang, and Sabine Buchholz. 2000.
Introduction to the CoNLL-2000 shared task:
chunking. In Proc. of CoNLL, pp. 127-132.
Yu-Chieh Wu, Jie-Chi Yang, Yue-Shi Lee, and
Show-Jane Yen. 2006. Efficient and robust phrase
chunking using support vector machines, In Asia
Information Retrieval Symposium (AIRS), pp. 350-
361.
Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee. 2008.
Description of the NCU Chinese Word
Segmentation and Part-of-Speech Tagging for
SIGHAN Bakeoff 2008, In Proc. of the SIGHAN
Workshop on Chinese Language Processing, pp.
161-166, 2008.
Yu-Chieh Wu, Yue-Shi Lee, and Jie-Chi Yang.
Robust and efficient multiclass SVM models for
phrase pattern recognition, Pattern recognition,
41(9): 2874-2889, 2008.
Yu-Chieh Wu, Jie-Chi Yang, and Qian Xiang Lin.
2006. Description of the NCU Chinese word
segmentation and named entity recognition System
for SIGHAN Bakeoff 2006, In Proc. of the
SIGHAN Workshop on Chinese Language
Processing, pp. 209-212.
Yu-Chieh Wu, Yue-Shi Lee, and Jie-Chi Yang. 2008.
Robust and efficient Chinese word dependency
analysis with linear kernel support vector machines,
In Proc. of the COLING, pp. 135-138.
Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee. 2007.
Multilingual deterministic dependency parsing
framework using modified finite Newton method
Support Vector Machines. In Proc. of the
EMNLP/CoNLL, pp.1175-1181.
Tong Zhang, Fred Damerau, and David Johnson.
2002. Text chunking based on a generalization
Winnow, JMLR, 2: 615-637.
Hai Zhao and Chunyu Kit. 2007. Incorporating global
information into supervised learning for Chinese
word segmentation, In Proc. of PACLIC, pp.66-74.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.185089">
<title confidence="0.996738">Chinese Word Segmentation with Conditional Support Vector Inspired Markov Models</title>
<author confidence="0.989052">Yu-Chieh Jie-Chi Yue-Shi</author>
<affiliation confidence="0.544479">of Computer Science Institute of Dep. of Computer Information work and Information National Central National Central Ming Chuan Ming Chuan Taoyuan, Taiwan Taoyuan, Taiwan</affiliation>
<address confidence="0.458669">Taipei, Taiwan yang@cl.ncu.edu.tw leeys@mcu.edu.tw</address>
<email confidence="0.7177985">bcbb@db.csie.ncu.edu.tw</email>
<abstract confidence="0.995185181818182">In this paper, we present the proposed method of participating SIGHAN-2010 Chinese word segmentation bake-off. In this year, our focus aims to quick train and test the given data. Unlike the most structural learning algorithms, such as conditional random fields, we design an in-house development conditional support vector Markov model (CMM) framework. The method is very quick to train and also show better performance in accuracy than CRF. To give a fair comparison, we compare our method to CRF with three additional tasks, namely, CoNLL-2000 chunking, SIGHAN-3 Chinese word segmentation. The results were encourage and indicated that the proposed CMM produces better not only accuracy but also training time efficiency. The official results in SIGHAN-2010 also demonstrates that our method perform very well in traditional Chinese with fine-tuned features set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie K Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A highperformance semi-supervised learning method for text chunking,</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="8839" citStr="Ando and Zhang, 2005" startWordPosition="1403" endWordPosition="1406"> tags. That is we can model the S-tag, B2, B3 tags with dividing the leading tags into two categories. For details can refer the literatures. 3 Empirical Results Three large-scale and large-category dataset is used to evaluate the proposed method, namely, CoNLL-2000 syntactic chunking (Tjong Kim Sang and Buchholz, 2000), Chinese POS tagging, and three of SIGHAN-3 word segmentation tasks. Table 1 shows the statistics of those datasets. Feature CoNLL-2000 SIGHAN-3 type Unigram w-2~w+2 w-2~w+2 CoNLL-2000 chunking task is a well-known and widely evaluated in many literatures (Suzuki et al., 2007; Ando and Zhang, 2005; Kudo and Matsumoto, 2001; Wu et al., 2008; Daumé III and Marcu, 2005). The training data was derived from Treebank WSJ section 15-18 while section 20 was used for testing. The goal is to find the non-recursive phrase structures in a sentence, such as noun phrase (NP), verb phrase (VP), etc. There are 11 phrase types in this dataset. We follow the previous best settings for SVMs (Kudo and Matsumoto, 2001; Wu et al., 2008). The IOE2 is used to represent the phrase structure and tagged the data with backward direction. The training and testing data of the Chinese POS tagging is mainly derived f</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie K. Ando, and Tong Zhang. 2005. A highperformance semi-supervised learning method for text chunking, In Proc. of ACL, pp. 1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard E Boser</author>
<author>Isabelle M Guyon</author>
<author>Vladimir N Vapnik</author>
</authors>
<title>A training algorithm for optimal margin classifiers,</title>
<date>1992</date>
<booktitle>In Proc. of COLT,</booktitle>
<pages>144--152</pages>
<marker>Boser, Guyon, Vapnik, 1992</marker>
<rawString>Bernhard E. Boser, Isabelle M. Guyon, Vladimir N. Vapnik. 1992. A training algorithm for optimal margin classifiers, In Proc. of COLT, pp. 144-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation,</title>
<date>2000</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>591--598</pages>
<contexts>
<context position="6677" citStr="McCallum et al. (2000)" startWordPosition="1053" endWordPosition="1056">= 1 n P s o ( , ) = fjP(si |oi , si-1) (3) i=2 n P s o ( , ) = fjP(si |oi , si-1, si-2) (4) i 3 n P(s,o)=fjP(si |oi,si-1,si-1) (5) i=3 Equations (2), (3), and (4) are merely standard zero, first and second order decompositions, while equation (5) is the proposed greedy second order CMM decomposition which will be discussed in next section. Figure 1: K-th order conditional Markov models: (a) the standard 0(zero) order CMM, (b) first order CMM, (c) second order CMM, and (d) the proposed second order CMM The above decompositions merge the transition and emission probability with single function. McCallum et al. (2000) further combined the locally trained maximum entropy with the infered transition score. However, our conditional support vector Markov models make different chain probability. We replace the original transition probability with transition validity score, i.e. n ~ P s o ( , ) = fjP(si |si-1 )P(si |oi) (6) i=2 n P s o ( , ) Ö= fl P(si |si-1 )P(si |oi , si-1, si-1) (7) i=3 The transition validity score is merely a Boolean flag which indicates the relationships between two neighbor labels. Equation (6) and (7) are zero-order and our second order chain probabilities. We will introduce the proposed</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando C. N. Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation, In Proc. of ICML, pp. 591.598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daumé</author>
<author>Daniel Marcu</author>
</authors>
<title>Learning as search optimization: approximate large margin methods for structured prediction,</title>
<date>2005</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>169--176</pages>
<marker>Daumé, Marcu, 2005</marker>
<rawString>Hal Daumé III and Daniel Marcu. 2005. Learning as search optimization: approximate large margin methods for structured prediction, In Proc. of ICML, pp. 169-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangjin Jin</author>
<author>Xiao Chen</author>
</authors>
<title>The Fourth International Chinese Language Proces sing Bakeoff: Chinese Word Segmentation Named Entity Recognition and Chinese POS Tagging.</title>
<date>2008</date>
<booktitle>In Proc. of the SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>69--81</pages>
<contexts>
<context position="2720" citStr="Jin and Chen, 2008" startWordPosition="407" endWordPosition="410">2001) and maximummargin Markov models (M3N) (Taskar et al., 2003) have received a great attention and become a prominent learning algorithm to many sequential labeling tasks. Examples include partof-speech (POS) tagging (Shen et al., 2007) and syntactic phrase chunking (Suzuki et al., 2007). The Chinese word segmentation can also be treated as a character-based tagging task in (Xue and Converse, 2002). One feature of sequential labeling is that it aims at finding non-recursive chunk fragments in a given sentence. Among these approaches, CRF has been wildly used in recent SIGHAN bakeoff tasks (Jin and Chen, 2008; Levow, 2006). Although these approaches do not suffer from so-called label-bias problems (Lafferty et al., 2001), one limitation is that they are inefficient to train with large-scale, especially large category data. On the other hand, non-structural learning approaches (e.g. maximum entropy models) which learn local predictors usually cost much better training time performance than structural learning algorithms. These methods condition on local context features and incorporate fix-length history information. Although higher order feature (longer history) maybe useful to some tasks, the exp</context>
</contexts>
<marker>Jin, Chen, 2008</marker>
<rawString>Guangjin Jin and Xiao Chen. 2008. The Fourth International Chinese Language Proces sing Bakeoff: Chinese Word Segmentation Named Entity Recognition and Chinese POS Tagging. In Proc. of the SIGHAN Workshop on Chinese Language Processing, pp. 69-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear SVMs in linear time,</title>
<date>2006</date>
<booktitle>In Proc. of KDD,</booktitle>
<pages>217--226</pages>
<contexts>
<context position="3700" citStr="Joachims, 2006" startWordPosition="552" endWordPosition="553">time performance than structural learning algorithms. These methods condition on local context features and incorporate fix-length history information. Although higher order feature (longer history) maybe useful to some tasks, the exponential scaled inference time is also intractable in practice. Support vector machines (SVMs) which is one of the state-of-the-art supervised learning algorithms have been widely employed as local classifiers to many sequential labeling tasks (Taku and Matsumoto, 2001; Wu et al., 2006, 2008). Specially, the training time of linear kernel SVM with either L1-norm (Joachims, 2006; Keerthi et al., 2008) or L2-norm (Keerthi and DeCoste, 2005; Hsieh et al., 2008) can now be obtained in linear time. Even local classifier-based approaches have the drawbacks of label-bias problems, training nonstructural linear SVM is scalable to large-scale data. By means of so-called one-versus-all multiclass SVM training, it is also scalable to large-category data. In this paper, we present our Chinese word segmentation based on the proposed conditional support vector Markov models for sequential labeling tasks, especially Chinese word segmentation. Unlike structural learning algorithms,</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear SVMs in linear time, In Proc. of KDD, pp. 217-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>Thomas Finley</author>
<author>Chun-Nam Yu</author>
</authors>
<title>Cutting-Plane Training of Structural SVMs,</title>
<date>2009</date>
<journal>Machine Learning Journal,</journal>
<note>to appear.</note>
<contexts>
<context position="4822" citStr="Joachims et al., 2009" startWordPosition="720" endWordPosition="723">uential labeling tasks, especially Chinese word segmentation. Unlike structural learning algorithms, our method can be simply trained without considering the entire structures and hence the training time scales linearly with the number of training examples. In this framework, to alleviate the ease of label-bias problems, the state transition probability is ignored. Instead, we merely utilize the property of label relationships between chunks (Wu et al., 2008). To demonstrate our method, we compare to several well-known structural learning algorithms, like CRF (Kudo et al., 2004), and SVM-HMM (Joachims et al., 2009) on two well-known data, namely, CoNLL-2000 syntactic chunking, SIGHAN-3 Chinese word segmentation tasks. By following this, we apply the model to the Chinese word segmentation tasks of SIGHAN-2010 this year. The empirical results showed that our method is not only fast but also achieving more superior accuracy than structural learning methods. In traditional Chinese, our method also achieves the state-of-the-art performance in accuracy with fined-tune features. 2 Conditional support vector Markov models Traditional conditional Markov models (CMM) is to assign the tag sequence which maximizes </context>
</contexts>
<marker>Joachims, Finley, Yu, 2009</marker>
<rawString>Thorsten Joachims, Thomas Finley, and Chun-Nam Yu. 2009. Cutting-Plane Training of Structural SVMs, Machine Learning Journal, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sathiya Keerthi</author>
<author>Dennis DeCoste</author>
</authors>
<title>A modified finite Newton method for fast solution of large scale linear SVMs,</title>
<date>2005</date>
<journal>JMLR,</journal>
<volume>6</volume>
<pages>341--361</pages>
<contexts>
<context position="3761" citStr="Keerthi and DeCoste, 2005" startWordPosition="560" endWordPosition="563">ms. These methods condition on local context features and incorporate fix-length history information. Although higher order feature (longer history) maybe useful to some tasks, the exponential scaled inference time is also intractable in practice. Support vector machines (SVMs) which is one of the state-of-the-art supervised learning algorithms have been widely employed as local classifiers to many sequential labeling tasks (Taku and Matsumoto, 2001; Wu et al., 2006, 2008). Specially, the training time of linear kernel SVM with either L1-norm (Joachims, 2006; Keerthi et al., 2008) or L2-norm (Keerthi and DeCoste, 2005; Hsieh et al., 2008) can now be obtained in linear time. Even local classifier-based approaches have the drawbacks of label-bias problems, training nonstructural linear SVM is scalable to large-scale data. By means of so-called one-versus-all multiclass SVM training, it is also scalable to large-category data. In this paper, we present our Chinese word segmentation based on the proposed conditional support vector Markov models for sequential labeling tasks, especially Chinese word segmentation. Unlike structural learning algorithms, our method can be simply trained without considering the ent</context>
</contexts>
<marker>Keerthi, DeCoste, 2005</marker>
<rawString>Sathiya Keerthi and Dennis DeCoste. 2005. A modified finite Newton method for fast solution of large scale linear SVMs, JMLR, 6: 341-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="8071" citStr="Kudo and Matsumoto, 2001" startWordPosition="1284" endWordPosition="1287">er, we do not explicitly adopt the state transitions for our CMM. Instead, a chunkrelation pair is used. Nevertheless, one important property to sequential chunk labeling is that there is only one phrase type in a chunk. For example, if the previous word is tagged as begin of noun phrase (B-NP), the current word must not be end of the other phrase (E-VP, E-PP, etc). Therefore, we only model relationships between chunk tags to generate valid phrase structure. Wu et al. (2007, 2008) presented an automatic chunk pair relation construction algorithm which can handle so-called IOB1/IOB2/IOE1/IOE2 (Kudo and Matsumoto, 2001) chunk representation structures with either left-to-right or right-toleft directions. Here, we extend this idea and generalize to fit to more chunk tags. That is we can model the S-tag, B2, B3 tags with dividing the leading tags into two categories. For details can refer the literatures. 3 Empirical Results Three large-scale and large-category dataset is used to evaluate the proposed method, namely, CoNLL-2000 syntactic chunking (Tjong Kim Sang and Buchholz, 2000), Chinese POS tagging, and three of SIGHAN-3 word segmentation tasks. Table 1 shows the statistics of those datasets. Feature CoNLL</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proc. of NAACL, pp. 192-199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis,</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>230--237</pages>
<contexts>
<context position="4785" citStr="Kudo et al., 2004" startWordPosition="714" endWordPosition="717">port vector Markov models for sequential labeling tasks, especially Chinese word segmentation. Unlike structural learning algorithms, our method can be simply trained without considering the entire structures and hence the training time scales linearly with the number of training examples. In this framework, to alleviate the ease of label-bias problems, the state transition probability is ignored. Instead, we merely utilize the property of label relationships between chunks (Wu et al., 2008). To demonstrate our method, we compare to several well-known structural learning algorithms, like CRF (Kudo et al., 2004), and SVM-HMM (Joachims et al., 2009) on two well-known data, namely, CoNLL-2000 syntactic chunking, SIGHAN-3 Chinese word segmentation tasks. By following this, we apply the model to the Chinese word segmentation tasks of SIGHAN-2010 this year. The empirical results showed that our method is not only fast but also achieving more superior accuracy than structural learning methods. In traditional Chinese, our method also achieves the state-of-the-art performance in accuracy with fined-tune features. 2 Conditional support vector Markov models Traditional conditional Markov models (CMM) is to ass</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis, In Proc. of EMNLP, pp. 230-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data,</title>
<date>2001</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2107" citStr="Lafferty et al., 2001" startWordPosition="309" endWordPosition="312">in the competition (Wu et al., 2006, 2007). In this year, we join the SIGHAN bakeoff task in both traditional and simplified Chinese closed word segmentation. Unlike most western languages, there is no explicit space between words. The goal of word segmentation is to identify words given the sentence. This technique provides important features for downstream purposes. Examples include Chinese part-of-speech (POS) tagging (Wu et al., 2007), Chinese word dependency parsing (Wu et al., 2007, 2008). With the rapid growth of structural learning algorithms, such as conditional random fields (CRFs) (Lafferty et al., 2001) and maximummargin Markov models (M3N) (Taskar et al., 2003) have received a great attention and become a prominent learning algorithm to many sequential labeling tasks. Examples include partof-speech (POS) tagging (Shen et al., 2007) and syntactic phrase chunking (Suzuki et al., 2007). The Chinese word segmentation can also be treated as a character-based tagging task in (Xue and Converse, 2002). One feature of sequential labeling is that it aims at finding non-recursive chunk fragments in a given sentence. Among these approaches, CRF has been wildly used in recent SIGHAN bakeoff tasks (Jin a</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data, In Proc. of ICML, pp. 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
</authors>
<title>The third international Chinese language processing bakeoff: Word segmentation and named entity recognition.</title>
<date>2006</date>
<booktitle>In Proc. of the SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>108--117</pages>
<contexts>
<context position="1455" citStr="Levow, 2006" startWordPosition="209" endWordPosition="210">MM) framework. The method is very quick to train and also show better performance in accuracy than CRF. To give a fair comparison, we compare our method to CRF with three additional tasks, namely, CoNLL-2000 chunking, SIGHAN-3 Chinese word segmentation. The results were encourage and indicated that the proposed CMM produces better not only accuracy but also training time efficiency. The official results in SIGHAN-2010 also demonstrates that our method perform very well in traditional Chinese with fine-tuned features set. 1 Introduction Since 2006 Chinese word segmentation bakeoff in SIGHAN-3 (Levow, 2006), this is the third time to join the competition (Wu et al., 2006, 2007). In this year, we join the SIGHAN bakeoff task in both traditional and simplified Chinese closed word segmentation. Unlike most western languages, there is no explicit space between words. The goal of word segmentation is to identify words given the sentence. This technique provides important features for downstream purposes. Examples include Chinese part-of-speech (POS) tagging (Wu et al., 2007), Chinese word dependency parsing (Wu et al., 2007, 2008). With the rapid growth of structural learning algorithms, such as cond</context>
<context position="2734" citStr="Levow, 2006" startWordPosition="411" endWordPosition="412">gin Markov models (M3N) (Taskar et al., 2003) have received a great attention and become a prominent learning algorithm to many sequential labeling tasks. Examples include partof-speech (POS) tagging (Shen et al., 2007) and syntactic phrase chunking (Suzuki et al., 2007). The Chinese word segmentation can also be treated as a character-based tagging task in (Xue and Converse, 2002). One feature of sequential labeling is that it aims at finding non-recursive chunk fragments in a given sentence. Among these approaches, CRF has been wildly used in recent SIGHAN bakeoff tasks (Jin and Chen, 2008; Levow, 2006). Although these approaches do not suffer from so-called label-bias problems (Lafferty et al., 2001), one limitation is that they are inefficient to train with large-scale, especially large category data. On the other hand, non-structural learning approaches (e.g. maximum entropy models) which learn local predictors usually cost much better training time performance than structural learning algorithms. These methods condition on local context features and incorporate fix-length history information. Although higher order feature (longer history) maybe useful to some tasks, the exponential scale</context>
</contexts>
<marker>Levow, 2006</marker>
<rawString>Gina-Anne Levow. 2006. The third international Chinese language processing bakeoff: Word segmentation and named entity recognition. In Proc. of the SIGHAN Workshop on Chinese Language Processing, pp. 108±117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A maximum entropy approach to Chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proc. of the SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<contexts>
<context position="16194" citStr="Low et al., 2005" startWordPosition="2620" endWordPosition="2623">o apply CMM to SIGHAN-2010, we design the following strategy. First the classifier parameters, feature set should be improved. To achieve this, 1/4 of the training data was used as development set, while the remaining 3/4 training data was used to train the classifier. Second, we combine multi-classifier to enhance the accuracy. The CRF and our CMM with basic feature set were trained to predict the initial labels of the testing data. Then the predicted labels were included as features to train the final-stage classifier. The final classifier is still our CMM. Third, the postprocessing method (Low et al., 2005) is employed to enhance the unknown word segmentation. Table 4 lists the empirical results of the development set. By validate with development data, we found that C=1.25 and use the E-BIES representation method (Wu et al., 2008) yields better accuracy than B-BIES (Zhou and Kit, 2007). Meanwhile, CRF seems to be suitable for BBIES representation method. The classifier parameters were fixed and then we try to search the optimal feature set via the incremental add-and-check method. That is, we use the initial feature set as basis and add one feature type from the pool and verify the goodness of </context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A maximum entropy approach to Chinese word segmentation. In Proc. of the SIGHAN Workshop on Chinese Language Processing, pp. 161-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese partof-speech tagging. one-at-a-time or all-at-once? word-based or character-based?</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>277--284</pages>
<contexts>
<context position="9794" citStr="Ng and Low (2004)" startWordPosition="1572" endWordPosition="1575">e follow the previous best settings for SVMs (Kudo and Matsumoto, 2001; Wu et al., 2008). The IOE2 is used to represent the phrase structure and tagged the data with backward direction. The training and testing data of the Chinese POS tagging is mainly derived from the Academic Sinica¶s balanced corpus (version 3.0). Seventy-five percent out of the data is used for training while the remaining 25% is used for testing. However, the task of the Chinese POS tagging is very different from classical English POS tagging in that there is no word boundary information in Chinese text. To achieve this, Ng and Low (2004) gave a successful study on Chinese POS tagging. Just as English phrase chunking, the IOB-tags can be used to represent the Chinese word and its part-of-speech tag. For example, the tag B-ADJ means the first character of a Chinese word which POS tag is ADJ (adjective). n this task, we simply use the IOB2 to represent the chunk structure. In this way, the tagger needs to recognize the chunk tag by considering 118 (59*2) categories at once. As discussed in (Zhou and Kit, 2007), using more complex chunk representation bring better segmentation accuracy in several Chinese word segmentation benchma</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese partof-speech tagging. one-at-a-time or all-at-once? word-based or character-based? In Proc. of EMNLP, pp. 277-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods,</title>
<date>1999</date>
<booktitle>In Advances in Large Margin Classifiers.</booktitle>
<contexts>
<context position="13007" citStr="Platt, 1999" startWordPosition="2077" endWordPosition="2078">942 0.942 0.788 0.958 0.948 0.957 0.952 0.666 0.977 Simplified 0.936 0.932 0.934 0.564 0.964 0.915 0.915 0.915 0.594 0.972 Task Medicine Finance Recall Precision F1 OOV-RR IV-RR Recall Precision F1 OOV-RR IV-RR Traditional 0.953 0.957 0.955 0.798 0.966 0.964 0.962 0.963 0.812 0.975 Simplified 0.933 0.915 0.924 0.642 0.969 0.945 0.941 0.943 0.666 0.972 dually. This is in particularly useful to the tasks which involve training large number of features and categories (Wu et al., 2008). To obtain the probability output from SVM, we employ the sigmoid function with fixed parameter A=-2 and B=0 as (Platt, 1999). 3.2 Comparison to structural learning The overall experimental results are summarized in Table 1. Column &amp;quot;All&amp;quot; denotes as the F(p) score of all chunk types, while &amp;quot;NP&amp;quot; is the F(p) score of the noun phrase only. The final two columns list the entire training and testing times. As shown in Table 1, it is surprising that the proposed CMM outperforms the other structural learning methods, CRF and SVM-HMM. In terms of training time, our method shows substantial faster than CRF. However, in terms of testing time, our method is worse than CRF. The main reason is that we do not optimize the code and</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods, In Advances in Large Margin Classifiers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Akinori Fujino</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semi-supervised structural output learning based on a hybrid generative and discriminative approach,</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>791--800</pages>
<contexts>
<context position="2393" citStr="Suzuki et al., 2007" startWordPosition="354" endWordPosition="357"> the sentence. This technique provides important features for downstream purposes. Examples include Chinese part-of-speech (POS) tagging (Wu et al., 2007), Chinese word dependency parsing (Wu et al., 2007, 2008). With the rapid growth of structural learning algorithms, such as conditional random fields (CRFs) (Lafferty et al., 2001) and maximummargin Markov models (M3N) (Taskar et al., 2003) have received a great attention and become a prominent learning algorithm to many sequential labeling tasks. Examples include partof-speech (POS) tagging (Shen et al., 2007) and syntactic phrase chunking (Suzuki et al., 2007). The Chinese word segmentation can also be treated as a character-based tagging task in (Xue and Converse, 2002). One feature of sequential labeling is that it aims at finding non-recursive chunk fragments in a given sentence. Among these approaches, CRF has been wildly used in recent SIGHAN bakeoff tasks (Jin and Chen, 2008; Levow, 2006). Although these approaches do not suffer from so-called label-bias problems (Lafferty et al., 2001), one limitation is that they are inefficient to train with large-scale, especially large category data. On the other hand, non-structural learning approaches </context>
<context position="8817" citStr="Suzuki et al., 2007" startWordPosition="1399" endWordPosition="1402"> to fit to more chunk tags. That is we can model the S-tag, B2, B3 tags with dividing the leading tags into two categories. For details can refer the literatures. 3 Empirical Results Three large-scale and large-category dataset is used to evaluate the proposed method, namely, CoNLL-2000 syntactic chunking (Tjong Kim Sang and Buchholz, 2000), Chinese POS tagging, and three of SIGHAN-3 word segmentation tasks. Table 1 shows the statistics of those datasets. Feature CoNLL-2000 SIGHAN-3 type Unigram w-2~w+2 w-2~w+2 CoNLL-2000 chunking task is a well-known and widely evaluated in many literatures (Suzuki et al., 2007; Ando and Zhang, 2005; Kudo and Matsumoto, 2001; Wu et al., 2008; Daumé III and Marcu, 2005). The training data was derived from Treebank WSJ section 15-18 while section 20 was used for testing. The goal is to find the non-recursive phrase structures in a sentence, such as noun phrase (NP), verb phrase (VP), etc. There are 11 phrase types in this dataset. We follow the previous best settings for SVMs (Kudo and Matsumoto, 2001; Wu et al., 2008). The IOE2 is used to represent the phrase structure and tagged the data with backward direction. The training and testing data of the Chinese POS taggi</context>
</contexts>
<marker>Suzuki, Fujino, Isozaki, 2007</marker>
<rawString>Jun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007. Semi-supervised structural output learning based on a hybrid generative and discriminative approach, In Proc. of EMNLP-CoNLL, pp. 791-800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>SemiSupervised Sequential Labeling and Segmentation using Giga-word Scale Unlabeled Data.</title>
<date>2008</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>665--673</pages>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2008. SemiSupervised Sequential Labeling and Segmentation using Giga-word Scale Unlabeled Data. In Proc. of ACL, pp. 665-673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Max-margin Markov networks,</title>
<date>2003</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="2167" citStr="Taskar et al., 2003" startWordPosition="319" endWordPosition="322">oin the SIGHAN bakeoff task in both traditional and simplified Chinese closed word segmentation. Unlike most western languages, there is no explicit space between words. The goal of word segmentation is to identify words given the sentence. This technique provides important features for downstream purposes. Examples include Chinese part-of-speech (POS) tagging (Wu et al., 2007), Chinese word dependency parsing (Wu et al., 2007, 2008). With the rapid growth of structural learning algorithms, such as conditional random fields (CRFs) (Lafferty et al., 2001) and maximummargin Markov models (M3N) (Taskar et al., 2003) have received a great attention and become a prominent learning algorithm to many sequential labeling tasks. Examples include partof-speech (POS) tagging (Shen et al., 2007) and syntactic phrase chunking (Suzuki et al., 2007). The Chinese word segmentation can also be treated as a character-based tagging task in (Xue and Converse, 2002). One feature of sequential labeling is that it aims at finding non-recursive chunk fragments in a given sentence. Among these approaches, CRF has been wildly used in recent SIGHAN bakeoff tasks (Jin and Chen, 2008; Levow, 2006). Although these approaches do no</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-margin Markov networks, In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: chunking.</title>
<date>2000</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="8540" citStr="Sang and Buchholz, 2000" startWordPosition="1358" endWordPosition="1361">l. (2007, 2008) presented an automatic chunk pair relation construction algorithm which can handle so-called IOB1/IOB2/IOE1/IOE2 (Kudo and Matsumoto, 2001) chunk representation structures with either left-to-right or right-toleft directions. Here, we extend this idea and generalize to fit to more chunk tags. That is we can model the S-tag, B2, B3 tags with dividing the leading tags into two categories. For details can refer the literatures. 3 Empirical Results Three large-scale and large-category dataset is used to evaluate the proposed method, namely, CoNLL-2000 syntactic chunking (Tjong Kim Sang and Buchholz, 2000), Chinese POS tagging, and three of SIGHAN-3 word segmentation tasks. Table 1 shows the statistics of those datasets. Feature CoNLL-2000 SIGHAN-3 type Unigram w-2~w+2 w-2~w+2 CoNLL-2000 chunking task is a well-known and widely evaluated in many literatures (Suzuki et al., 2007; Ando and Zhang, 2005; Kudo and Matsumoto, 2001; Wu et al., 2008; Daumé III and Marcu, 2005). The training data was derived from Treebank WSJ section 15-18 while section 20 was used for testing. The goal is to find the non-recursive phrase structures in a sentence, such as noun phrase (NP), verb phrase (VP), etc. There a</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Eric F. Tjong Kim Sang, and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 shared task: chunking. In Proc. of CoNLL, pp. 127-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Chieh Wu</author>
<author>Jie-Chi Yang</author>
<author>Yue-Shi Lee</author>
<author>Show-Jane Yen</author>
</authors>
<title>Efficient and robust phrase chunking using support vector machines,</title>
<date>2006</date>
<booktitle>In Asia Information Retrieval Symposium (AIRS),</booktitle>
<pages>350--361</pages>
<contexts>
<context position="1520" citStr="Wu et al., 2006" startWordPosition="220" endWordPosition="223"> better performance in accuracy than CRF. To give a fair comparison, we compare our method to CRF with three additional tasks, namely, CoNLL-2000 chunking, SIGHAN-3 Chinese word segmentation. The results were encourage and indicated that the proposed CMM produces better not only accuracy but also training time efficiency. The official results in SIGHAN-2010 also demonstrates that our method perform very well in traditional Chinese with fine-tuned features set. 1 Introduction Since 2006 Chinese word segmentation bakeoff in SIGHAN-3 (Levow, 2006), this is the third time to join the competition (Wu et al., 2006, 2007). In this year, we join the SIGHAN bakeoff task in both traditional and simplified Chinese closed word segmentation. Unlike most western languages, there is no explicit space between words. The goal of word segmentation is to identify words given the sentence. This technique provides important features for downstream purposes. Examples include Chinese part-of-speech (POS) tagging (Wu et al., 2007), Chinese word dependency parsing (Wu et al., 2007, 2008). With the rapid growth of structural learning algorithms, such as conditional random fields (CRFs) (Lafferty et al., 2001) and maximumm</context>
<context position="3606" citStr="Wu et al., 2006" startWordPosition="536" endWordPosition="539">s (e.g. maximum entropy models) which learn local predictors usually cost much better training time performance than structural learning algorithms. These methods condition on local context features and incorporate fix-length history information. Although higher order feature (longer history) maybe useful to some tasks, the exponential scaled inference time is also intractable in practice. Support vector machines (SVMs) which is one of the state-of-the-art supervised learning algorithms have been widely employed as local classifiers to many sequential labeling tasks (Taku and Matsumoto, 2001; Wu et al., 2006, 2008). Specially, the training time of linear kernel SVM with either L1-norm (Joachims, 2006; Keerthi et al., 2008) or L2-norm (Keerthi and DeCoste, 2005; Hsieh et al., 2008) can now be obtained in linear time. Even local classifier-based approaches have the drawbacks of label-bias problems, training nonstructural linear SVM is scalable to large-scale data. By means of so-called one-versus-all multiclass SVM training, it is also scalable to large-category data. In this paper, we present our Chinese word segmentation based on the proposed conditional support vector Markov models for sequentia</context>
</contexts>
<marker>Wu, Yang, Lee, Yen, 2006</marker>
<rawString>Yu-Chieh Wu, Jie-Chi Yang, Yue-Shi Lee, and Show-Jane Yen. 2006. Efficient and robust phrase chunking using support vector machines, In Asia Information Retrieval Symposium (AIRS), pp. 350-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Chieh Wu</author>
<author>Jie-Chi Yang</author>
<author>Yue-Shi Lee</author>
</authors>
<title>Description of the NCU Chinese Word Segmentation and Part-of-Speech Tagging for SIGHAN Bakeoff</title>
<date>2008</date>
<booktitle>In Proc. of the SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--166</pages>
<contexts>
<context position="4663" citStr="Wu et al., 2008" startWordPosition="696" endWordPosition="699">le to large-category data. In this paper, we present our Chinese word segmentation based on the proposed conditional support vector Markov models for sequential labeling tasks, especially Chinese word segmentation. Unlike structural learning algorithms, our method can be simply trained without considering the entire structures and hence the training time scales linearly with the number of training examples. In this framework, to alleviate the ease of label-bias problems, the state transition probability is ignored. Instead, we merely utilize the property of label relationships between chunks (Wu et al., 2008). To demonstrate our method, we compare to several well-known structural learning algorithms, like CRF (Kudo et al., 2004), and SVM-HMM (Joachims et al., 2009) on two well-known data, namely, CoNLL-2000 syntactic chunking, SIGHAN-3 Chinese word segmentation tasks. By following this, we apply the model to the Chinese word segmentation tasks of SIGHAN-2010 this year. The empirical results showed that our method is not only fast but also achieving more superior accuracy than structural learning methods. In traditional Chinese, our method also achieves the state-of-the-art performance in accuracy </context>
<context position="8882" citStr="Wu et al., 2008" startWordPosition="1411" endWordPosition="1414">gs with dividing the leading tags into two categories. For details can refer the literatures. 3 Empirical Results Three large-scale and large-category dataset is used to evaluate the proposed method, namely, CoNLL-2000 syntactic chunking (Tjong Kim Sang and Buchholz, 2000), Chinese POS tagging, and three of SIGHAN-3 word segmentation tasks. Table 1 shows the statistics of those datasets. Feature CoNLL-2000 SIGHAN-3 type Unigram w-2~w+2 w-2~w+2 CoNLL-2000 chunking task is a well-known and widely evaluated in many literatures (Suzuki et al., 2007; Ando and Zhang, 2005; Kudo and Matsumoto, 2001; Wu et al., 2008; Daumé III and Marcu, 2005). The training data was derived from Treebank WSJ section 15-18 while section 20 was used for testing. The goal is to find the non-recursive phrase structures in a sentence, such as noun phrase (NP), verb phrase (VP), etc. There are 11 phrase types in this dataset. We follow the previous best settings for SVMs (Kudo and Matsumoto, 2001; Wu et al., 2008). The IOE2 is used to represent the phrase structure and tagged the data with backward direction. The training and testing data of the Chinese POS tagging is mainly derived from the Academic Sinica¶s balanced corpus (</context>
<context position="11165" citStr="Wu et al., 2008" startWordPosition="1782" endWordPosition="1785">E, E, and S to represent the Chinese word. BI and IE are the interior after begin and interior before end of a chunk. B/I/E/S tags indicate the begin/interior/end/single of a chunk. Figure 2 lists the used feature set in both experiments. (w-2,w-1),(w-1,w0), (w0,w+1), (w+1,w+2),(w+1,w-1) ams (Zhou and Kit, 2007) Bigram (w-2,w-1),(w-1,w0), (w0,w+1), (w+1,w+2),(w+1,w-1) POS p-2~p+2 POS bigram (p-2,p-1),(p-1,p0), (p0,p+1),(p+1,p+2), (p+1,p-1) POS trigram (p-2,p-1,p0), (p-1,p0,p+1),(p-3,p-2,p-1), (p0,p+1,p+2), (p+1,p+2,p+3) (Word+POS) bigram Other features 2~4 prefix letters Orthographic feature (Wu et al., 2008) 3.1 Settings We included the Liblinear with square loss (Hsieh et al., 2008) into our conditional Markov models as classification algorithms. In basic, the SVM was designed for binary classification problems. To port to multiclass problems, we adopted the well-known one-versus-all (OVA) method. One good property of OVA is that parameter estimation process can be trained indivi(w-1,p0),(w-2,p-1) (w0,p+1), (w+1,p+2) 2~4 suffix letters AV feature of 2~6 gr Figure 2: Feature templates used in experiments Table 2: SIGHAN-3 word segmentation results SIGHAN-3 UPUC MSRA CityU Method F(p) Training Tes</context>
<context position="12881" citStr="Wu et al., 2008" startWordPosition="2054" endWordPosition="2057">segmentation tasks Task Literature Computer Recall Precision F1 OOV-RR IV-RR Recall Precision F1 OOV-RR IV-RR Traditional 0.942 0.942 0.942 0.788 0.958 0.948 0.957 0.952 0.666 0.977 Simplified 0.936 0.932 0.934 0.564 0.964 0.915 0.915 0.915 0.594 0.972 Task Medicine Finance Recall Precision F1 OOV-RR IV-RR Recall Precision F1 OOV-RR IV-RR Traditional 0.953 0.957 0.955 0.798 0.966 0.964 0.962 0.963 0.812 0.975 Simplified 0.933 0.915 0.924 0.642 0.969 0.945 0.941 0.943 0.666 0.972 dually. This is in particularly useful to the tasks which involve training large number of features and categories (Wu et al., 2008). To obtain the probability output from SVM, we employ the sigmoid function with fixed parameter A=-2 and B=0 as (Platt, 1999). 3.2 Comparison to structural learning The overall experimental results are summarized in Table 1. Column &amp;quot;All&amp;quot; denotes as the F(p) score of all chunk types, while &amp;quot;NP&amp;quot; is the F(p) score of the noun phrase only. The final two columns list the entire training and testing times. As shown in Table 1, it is surprising that the proposed CMM outperforms the other structural learning methods, CRF and SVM-HMM. In terms of training time, our method shows substantial faster than</context>
<context position="16423" citStr="Wu et al., 2008" startWordPosition="2661" endWordPosition="2664"> data was used to train the classifier. Second, we combine multi-classifier to enhance the accuracy. The CRF and our CMM with basic feature set were trained to predict the initial labels of the testing data. Then the predicted labels were included as features to train the final-stage classifier. The final classifier is still our CMM. Third, the postprocessing method (Low et al., 2005) is employed to enhance the unknown word segmentation. Table 4 lists the empirical results of the development set. By validate with development data, we found that C=1.25 and use the E-BIES representation method (Wu et al., 2008) yields better accuracy than B-BIES (Zhou and Kit, 2007). Meanwhile, CRF seems to be suitable for BBIES representation method. The classifier parameters were fixed and then we try to search the optimal feature set via the incremental add-and-check method. That is, we use the initial feature set as basis and add one feature type from the pool and verify the goodness of the feature with the development data. Figure 3 figures out the used features of each pass. In this year, the process was completely runthrough for the traditional Chinese task. Unfortunately we have insufficient time to apply th</context>
</contexts>
<marker>Wu, Yang, Lee, 2008</marker>
<rawString>Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee. 2008. Description of the NCU Chinese Word Segmentation and Part-of-Speech Tagging for SIGHAN Bakeoff 2008, In Proc. of the SIGHAN Workshop on Chinese Language Processing, pp. 161-166, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Chieh Wu</author>
<author>Yue-Shi Lee</author>
<author>Jie-Chi Yang</author>
</authors>
<title>Robust and efficient multiclass SVM models for phrase pattern recognition,</title>
<date>2008</date>
<journal>Pattern recognition,</journal>
<volume>41</volume>
<issue>9</issue>
<pages>2874--2889</pages>
<contexts>
<context position="4663" citStr="Wu et al., 2008" startWordPosition="696" endWordPosition="699">le to large-category data. In this paper, we present our Chinese word segmentation based on the proposed conditional support vector Markov models for sequential labeling tasks, especially Chinese word segmentation. Unlike structural learning algorithms, our method can be simply trained without considering the entire structures and hence the training time scales linearly with the number of training examples. In this framework, to alleviate the ease of label-bias problems, the state transition probability is ignored. Instead, we merely utilize the property of label relationships between chunks (Wu et al., 2008). To demonstrate our method, we compare to several well-known structural learning algorithms, like CRF (Kudo et al., 2004), and SVM-HMM (Joachims et al., 2009) on two well-known data, namely, CoNLL-2000 syntactic chunking, SIGHAN-3 Chinese word segmentation tasks. By following this, we apply the model to the Chinese word segmentation tasks of SIGHAN-2010 this year. The empirical results showed that our method is not only fast but also achieving more superior accuracy than structural learning methods. In traditional Chinese, our method also achieves the state-of-the-art performance in accuracy </context>
<context position="8882" citStr="Wu et al., 2008" startWordPosition="1411" endWordPosition="1414">gs with dividing the leading tags into two categories. For details can refer the literatures. 3 Empirical Results Three large-scale and large-category dataset is used to evaluate the proposed method, namely, CoNLL-2000 syntactic chunking (Tjong Kim Sang and Buchholz, 2000), Chinese POS tagging, and three of SIGHAN-3 word segmentation tasks. Table 1 shows the statistics of those datasets. Feature CoNLL-2000 SIGHAN-3 type Unigram w-2~w+2 w-2~w+2 CoNLL-2000 chunking task is a well-known and widely evaluated in many literatures (Suzuki et al., 2007; Ando and Zhang, 2005; Kudo and Matsumoto, 2001; Wu et al., 2008; Daumé III and Marcu, 2005). The training data was derived from Treebank WSJ section 15-18 while section 20 was used for testing. The goal is to find the non-recursive phrase structures in a sentence, such as noun phrase (NP), verb phrase (VP), etc. There are 11 phrase types in this dataset. We follow the previous best settings for SVMs (Kudo and Matsumoto, 2001; Wu et al., 2008). The IOE2 is used to represent the phrase structure and tagged the data with backward direction. The training and testing data of the Chinese POS tagging is mainly derived from the Academic Sinica¶s balanced corpus (</context>
<context position="11165" citStr="Wu et al., 2008" startWordPosition="1782" endWordPosition="1785">E, E, and S to represent the Chinese word. BI and IE are the interior after begin and interior before end of a chunk. B/I/E/S tags indicate the begin/interior/end/single of a chunk. Figure 2 lists the used feature set in both experiments. (w-2,w-1),(w-1,w0), (w0,w+1), (w+1,w+2),(w+1,w-1) ams (Zhou and Kit, 2007) Bigram (w-2,w-1),(w-1,w0), (w0,w+1), (w+1,w+2),(w+1,w-1) POS p-2~p+2 POS bigram (p-2,p-1),(p-1,p0), (p0,p+1),(p+1,p+2), (p+1,p-1) POS trigram (p-2,p-1,p0), (p-1,p0,p+1),(p-3,p-2,p-1), (p0,p+1,p+2), (p+1,p+2,p+3) (Word+POS) bigram Other features 2~4 prefix letters Orthographic feature (Wu et al., 2008) 3.1 Settings We included the Liblinear with square loss (Hsieh et al., 2008) into our conditional Markov models as classification algorithms. In basic, the SVM was designed for binary classification problems. To port to multiclass problems, we adopted the well-known one-versus-all (OVA) method. One good property of OVA is that parameter estimation process can be trained indivi(w-1,p0),(w-2,p-1) (w0,p+1), (w+1,p+2) 2~4 suffix letters AV feature of 2~6 gr Figure 2: Feature templates used in experiments Table 2: SIGHAN-3 word segmentation results SIGHAN-3 UPUC MSRA CityU Method F(p) Training Tes</context>
<context position="12881" citStr="Wu et al., 2008" startWordPosition="2054" endWordPosition="2057">segmentation tasks Task Literature Computer Recall Precision F1 OOV-RR IV-RR Recall Precision F1 OOV-RR IV-RR Traditional 0.942 0.942 0.942 0.788 0.958 0.948 0.957 0.952 0.666 0.977 Simplified 0.936 0.932 0.934 0.564 0.964 0.915 0.915 0.915 0.594 0.972 Task Medicine Finance Recall Precision F1 OOV-RR IV-RR Recall Precision F1 OOV-RR IV-RR Traditional 0.953 0.957 0.955 0.798 0.966 0.964 0.962 0.963 0.812 0.975 Simplified 0.933 0.915 0.924 0.642 0.969 0.945 0.941 0.943 0.666 0.972 dually. This is in particularly useful to the tasks which involve training large number of features and categories (Wu et al., 2008). To obtain the probability output from SVM, we employ the sigmoid function with fixed parameter A=-2 and B=0 as (Platt, 1999). 3.2 Comparison to structural learning The overall experimental results are summarized in Table 1. Column &amp;quot;All&amp;quot; denotes as the F(p) score of all chunk types, while &amp;quot;NP&amp;quot; is the F(p) score of the noun phrase only. The final two columns list the entire training and testing times. As shown in Table 1, it is surprising that the proposed CMM outperforms the other structural learning methods, CRF and SVM-HMM. In terms of training time, our method shows substantial faster than</context>
<context position="16423" citStr="Wu et al., 2008" startWordPosition="2661" endWordPosition="2664"> data was used to train the classifier. Second, we combine multi-classifier to enhance the accuracy. The CRF and our CMM with basic feature set were trained to predict the initial labels of the testing data. Then the predicted labels were included as features to train the final-stage classifier. The final classifier is still our CMM. Third, the postprocessing method (Low et al., 2005) is employed to enhance the unknown word segmentation. Table 4 lists the empirical results of the development set. By validate with development data, we found that C=1.25 and use the E-BIES representation method (Wu et al., 2008) yields better accuracy than B-BIES (Zhou and Kit, 2007). Meanwhile, CRF seems to be suitable for BBIES representation method. The classifier parameters were fixed and then we try to search the optimal feature set via the incremental add-and-check method. That is, we use the initial feature set as basis and add one feature type from the pool and verify the goodness of the feature with the development data. Figure 3 figures out the used features of each pass. In this year, the process was completely runthrough for the traditional Chinese task. Unfortunately we have insufficient time to apply th</context>
</contexts>
<marker>Wu, Lee, Yang, 2008</marker>
<rawString>Yu-Chieh Wu, Yue-Shi Lee, and Jie-Chi Yang. Robust and efficient multiclass SVM models for phrase pattern recognition, Pattern recognition, 41(9): 2874-2889, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Chieh Wu</author>
<author>Jie-Chi Yang</author>
<author>Qian Xiang Lin</author>
</authors>
<title>Description of the NCU Chinese word segmentation and named entity recognition System for SIGHAN Bakeoff</title>
<date>2006</date>
<booktitle>In Proc. of the SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>209--212</pages>
<contexts>
<context position="1520" citStr="Wu et al., 2006" startWordPosition="220" endWordPosition="223"> better performance in accuracy than CRF. To give a fair comparison, we compare our method to CRF with three additional tasks, namely, CoNLL-2000 chunking, SIGHAN-3 Chinese word segmentation. The results were encourage and indicated that the proposed CMM produces better not only accuracy but also training time efficiency. The official results in SIGHAN-2010 also demonstrates that our method perform very well in traditional Chinese with fine-tuned features set. 1 Introduction Since 2006 Chinese word segmentation bakeoff in SIGHAN-3 (Levow, 2006), this is the third time to join the competition (Wu et al., 2006, 2007). In this year, we join the SIGHAN bakeoff task in both traditional and simplified Chinese closed word segmentation. Unlike most western languages, there is no explicit space between words. The goal of word segmentation is to identify words given the sentence. This technique provides important features for downstream purposes. Examples include Chinese part-of-speech (POS) tagging (Wu et al., 2007), Chinese word dependency parsing (Wu et al., 2007, 2008). With the rapid growth of structural learning algorithms, such as conditional random fields (CRFs) (Lafferty et al., 2001) and maximumm</context>
<context position="3606" citStr="Wu et al., 2006" startWordPosition="536" endWordPosition="539">s (e.g. maximum entropy models) which learn local predictors usually cost much better training time performance than structural learning algorithms. These methods condition on local context features and incorporate fix-length history information. Although higher order feature (longer history) maybe useful to some tasks, the exponential scaled inference time is also intractable in practice. Support vector machines (SVMs) which is one of the state-of-the-art supervised learning algorithms have been widely employed as local classifiers to many sequential labeling tasks (Taku and Matsumoto, 2001; Wu et al., 2006, 2008). Specially, the training time of linear kernel SVM with either L1-norm (Joachims, 2006; Keerthi et al., 2008) or L2-norm (Keerthi and DeCoste, 2005; Hsieh et al., 2008) can now be obtained in linear time. Even local classifier-based approaches have the drawbacks of label-bias problems, training nonstructural linear SVM is scalable to large-scale data. By means of so-called one-versus-all multiclass SVM training, it is also scalable to large-category data. In this paper, we present our Chinese word segmentation based on the proposed conditional support vector Markov models for sequentia</context>
</contexts>
<marker>Wu, Yang, Lin, 2006</marker>
<rawString>Yu-Chieh Wu, Jie-Chi Yang, and Qian Xiang Lin. 2006. Description of the NCU Chinese word segmentation and named entity recognition System for SIGHAN Bakeoff 2006, In Proc. of the SIGHAN Workshop on Chinese Language Processing, pp. 209-212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Chieh Wu</author>
<author>Yue-Shi Lee</author>
<author>Jie-Chi Yang</author>
</authors>
<title>Robust and efficient Chinese word dependency analysis with linear kernel support vector machines,</title>
<date>2008</date>
<booktitle>In Proc. of the COLING,</booktitle>
<pages>135--138</pages>
<contexts>
<context position="4663" citStr="Wu et al., 2008" startWordPosition="696" endWordPosition="699">le to large-category data. In this paper, we present our Chinese word segmentation based on the proposed conditional support vector Markov models for sequential labeling tasks, especially Chinese word segmentation. Unlike structural learning algorithms, our method can be simply trained without considering the entire structures and hence the training time scales linearly with the number of training examples. In this framework, to alleviate the ease of label-bias problems, the state transition probability is ignored. Instead, we merely utilize the property of label relationships between chunks (Wu et al., 2008). To demonstrate our method, we compare to several well-known structural learning algorithms, like CRF (Kudo et al., 2004), and SVM-HMM (Joachims et al., 2009) on two well-known data, namely, CoNLL-2000 syntactic chunking, SIGHAN-3 Chinese word segmentation tasks. By following this, we apply the model to the Chinese word segmentation tasks of SIGHAN-2010 this year. The empirical results showed that our method is not only fast but also achieving more superior accuracy than structural learning methods. In traditional Chinese, our method also achieves the state-of-the-art performance in accuracy </context>
<context position="8882" citStr="Wu et al., 2008" startWordPosition="1411" endWordPosition="1414">gs with dividing the leading tags into two categories. For details can refer the literatures. 3 Empirical Results Three large-scale and large-category dataset is used to evaluate the proposed method, namely, CoNLL-2000 syntactic chunking (Tjong Kim Sang and Buchholz, 2000), Chinese POS tagging, and three of SIGHAN-3 word segmentation tasks. Table 1 shows the statistics of those datasets. Feature CoNLL-2000 SIGHAN-3 type Unigram w-2~w+2 w-2~w+2 CoNLL-2000 chunking task is a well-known and widely evaluated in many literatures (Suzuki et al., 2007; Ando and Zhang, 2005; Kudo and Matsumoto, 2001; Wu et al., 2008; Daumé III and Marcu, 2005). The training data was derived from Treebank WSJ section 15-18 while section 20 was used for testing. The goal is to find the non-recursive phrase structures in a sentence, such as noun phrase (NP), verb phrase (VP), etc. There are 11 phrase types in this dataset. We follow the previous best settings for SVMs (Kudo and Matsumoto, 2001; Wu et al., 2008). The IOE2 is used to represent the phrase structure and tagged the data with backward direction. The training and testing data of the Chinese POS tagging is mainly derived from the Academic Sinica¶s balanced corpus (</context>
<context position="11165" citStr="Wu et al., 2008" startWordPosition="1782" endWordPosition="1785">E, E, and S to represent the Chinese word. BI and IE are the interior after begin and interior before end of a chunk. B/I/E/S tags indicate the begin/interior/end/single of a chunk. Figure 2 lists the used feature set in both experiments. (w-2,w-1),(w-1,w0), (w0,w+1), (w+1,w+2),(w+1,w-1) ams (Zhou and Kit, 2007) Bigram (w-2,w-1),(w-1,w0), (w0,w+1), (w+1,w+2),(w+1,w-1) POS p-2~p+2 POS bigram (p-2,p-1),(p-1,p0), (p0,p+1),(p+1,p+2), (p+1,p-1) POS trigram (p-2,p-1,p0), (p-1,p0,p+1),(p-3,p-2,p-1), (p0,p+1,p+2), (p+1,p+2,p+3) (Word+POS) bigram Other features 2~4 prefix letters Orthographic feature (Wu et al., 2008) 3.1 Settings We included the Liblinear with square loss (Hsieh et al., 2008) into our conditional Markov models as classification algorithms. In basic, the SVM was designed for binary classification problems. To port to multiclass problems, we adopted the well-known one-versus-all (OVA) method. One good property of OVA is that parameter estimation process can be trained indivi(w-1,p0),(w-2,p-1) (w0,p+1), (w+1,p+2) 2~4 suffix letters AV feature of 2~6 gr Figure 2: Feature templates used in experiments Table 2: SIGHAN-3 word segmentation results SIGHAN-3 UPUC MSRA CityU Method F(p) Training Tes</context>
<context position="12881" citStr="Wu et al., 2008" startWordPosition="2054" endWordPosition="2057">segmentation tasks Task Literature Computer Recall Precision F1 OOV-RR IV-RR Recall Precision F1 OOV-RR IV-RR Traditional 0.942 0.942 0.942 0.788 0.958 0.948 0.957 0.952 0.666 0.977 Simplified 0.936 0.932 0.934 0.564 0.964 0.915 0.915 0.915 0.594 0.972 Task Medicine Finance Recall Precision F1 OOV-RR IV-RR Recall Precision F1 OOV-RR IV-RR Traditional 0.953 0.957 0.955 0.798 0.966 0.964 0.962 0.963 0.812 0.975 Simplified 0.933 0.915 0.924 0.642 0.969 0.945 0.941 0.943 0.666 0.972 dually. This is in particularly useful to the tasks which involve training large number of features and categories (Wu et al., 2008). To obtain the probability output from SVM, we employ the sigmoid function with fixed parameter A=-2 and B=0 as (Platt, 1999). 3.2 Comparison to structural learning The overall experimental results are summarized in Table 1. Column &amp;quot;All&amp;quot; denotes as the F(p) score of all chunk types, while &amp;quot;NP&amp;quot; is the F(p) score of the noun phrase only. The final two columns list the entire training and testing times. As shown in Table 1, it is surprising that the proposed CMM outperforms the other structural learning methods, CRF and SVM-HMM. In terms of training time, our method shows substantial faster than</context>
<context position="16423" citStr="Wu et al., 2008" startWordPosition="2661" endWordPosition="2664"> data was used to train the classifier. Second, we combine multi-classifier to enhance the accuracy. The CRF and our CMM with basic feature set were trained to predict the initial labels of the testing data. Then the predicted labels were included as features to train the final-stage classifier. The final classifier is still our CMM. Third, the postprocessing method (Low et al., 2005) is employed to enhance the unknown word segmentation. Table 4 lists the empirical results of the development set. By validate with development data, we found that C=1.25 and use the E-BIES representation method (Wu et al., 2008) yields better accuracy than B-BIES (Zhou and Kit, 2007). Meanwhile, CRF seems to be suitable for BBIES representation method. The classifier parameters were fixed and then we try to search the optimal feature set via the incremental add-and-check method. That is, we use the initial feature set as basis and add one feature type from the pool and verify the goodness of the feature with the development data. Figure 3 figures out the used features of each pass. In this year, the process was completely runthrough for the traditional Chinese task. Unfortunately we have insufficient time to apply th</context>
</contexts>
<marker>Wu, Lee, Yang, 2008</marker>
<rawString>Yu-Chieh Wu, Yue-Shi Lee, and Jie-Chi Yang. 2008. Robust and efficient Chinese word dependency analysis with linear kernel support vector machines, In Proc. of the COLING, pp. 135-138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Chieh Wu</author>
<author>Jie-Chi Yang</author>
<author>Yue-Shi Lee</author>
</authors>
<title>Multilingual deterministic dependency parsing framework using modified finite Newton method Support Vector Machines.</title>
<date>2007</date>
<booktitle>In Proc. of the EMNLP/CoNLL,</booktitle>
<pages>1175--1181</pages>
<contexts>
<context position="1927" citStr="Wu et al., 2007" startWordPosition="282" endWordPosition="285"> well in traditional Chinese with fine-tuned features set. 1 Introduction Since 2006 Chinese word segmentation bakeoff in SIGHAN-3 (Levow, 2006), this is the third time to join the competition (Wu et al., 2006, 2007). In this year, we join the SIGHAN bakeoff task in both traditional and simplified Chinese closed word segmentation. Unlike most western languages, there is no explicit space between words. The goal of word segmentation is to identify words given the sentence. This technique provides important features for downstream purposes. Examples include Chinese part-of-speech (POS) tagging (Wu et al., 2007), Chinese word dependency parsing (Wu et al., 2007, 2008). With the rapid growth of structural learning algorithms, such as conditional random fields (CRFs) (Lafferty et al., 2001) and maximummargin Markov models (M3N) (Taskar et al., 2003) have received a great attention and become a prominent learning algorithm to many sequential labeling tasks. Examples include partof-speech (POS) tagging (Shen et al., 2007) and syntactic phrase chunking (Suzuki et al., 2007). The Chinese word segmentation can also be treated as a character-based tagging task in (Xue and Converse, 2002). One feature of sequ</context>
<context position="7924" citStr="Wu et al. (2007" startWordPosition="1266" endWordPosition="1269">obtain the transition validity score automatically without concerning the change of chunk representation. 2.1 Tag transitions In this paper, we do not explicitly adopt the state transitions for our CMM. Instead, a chunkrelation pair is used. Nevertheless, one important property to sequential chunk labeling is that there is only one phrase type in a chunk. For example, if the previous word is tagged as begin of noun phrase (B-NP), the current word must not be end of the other phrase (E-VP, E-PP, etc). Therefore, we only model relationships between chunk tags to generate valid phrase structure. Wu et al. (2007, 2008) presented an automatic chunk pair relation construction algorithm which can handle so-called IOB1/IOB2/IOE1/IOE2 (Kudo and Matsumoto, 2001) chunk representation structures with either left-to-right or right-toleft directions. Here, we extend this idea and generalize to fit to more chunk tags. That is we can model the S-tag, B2, B3 tags with dividing the leading tags into two categories. For details can refer the literatures. 3 Empirical Results Three large-scale and large-category dataset is used to evaluate the proposed method, namely, CoNLL-2000 syntactic chunking (Tjong Kim Sang and</context>
</contexts>
<marker>Wu, Yang, Lee, 2007</marker>
<rawString>Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee. 2007. Multilingual deterministic dependency parsing framework using modified finite Newton method Support Vector Machines. In Proc. of the EMNLP/CoNLL, pp.1175-1181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
<author>Fred Damerau</author>
<author>David Johnson</author>
</authors>
<title>Text chunking based on a generalization Winnow,</title>
<date>2002</date>
<journal>JMLR,</journal>
<volume>2</volume>
<pages>615--637</pages>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>Tong Zhang, Fred Damerau, and David Johnson. 2002. Text chunking based on a generalization Winnow, JMLR, 2: 615-637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Incorporating global information into supervised learning for Chinese word segmentation,</title>
<date>2007</date>
<booktitle>In Proc. of PACLIC,</booktitle>
<pages>66--74</pages>
<marker>Zhao, Kit, 2007</marker>
<rawString>Hai Zhao and Chunyu Kit. 2007. Incorporating global information into supervised learning for Chinese word segmentation, In Proc. of PACLIC, pp.66-74.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>