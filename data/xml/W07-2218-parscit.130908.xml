<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019408">
<title confidence="0.996997">
A Latent Variable Model for Generative Dependency Parsing
</title>
<author confidence="0.997973">
Ivan Titov
</author>
<affiliation confidence="0.998575">
University of Geneva
</affiliation>
<address confidence="0.930171">
24, rue G´en´eral Dufour
CH-1211 Gen`eve 4, Switzerland
</address>
<email confidence="0.99782">
ivan.titov@cui.unige.ch
</email>
<author confidence="0.989278">
James Henderson
</author>
<affiliation confidence="0.997769">
University of Edinburgh
</affiliation>
<address confidence="0.7839315">
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
</address>
<email confidence="0.998563">
james.henderson@ed.ac.uk
</email>
<sectionHeader confidence="0.995627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954461538462">
We propose a generative dependency pars-
ing model which uses binary latent variables
to induce conditioning features. To define
this model we use a recently proposed class
of Bayesian Networks for structured predic-
tion, Incremental Sigmoid Belief Networks.
We demonstrate that the proposed model
achieves state-of-the-art results on three dif-
ferent languages. We also demonstrate that
the features induced by the ISBN’s latent
variables are crucial to this success, and
show that the proposed model is particularly
good on long dependencies.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999736176470589">
Dependency parsing has been a topic of active re-
search in natural language processing during the last
several years. The CoNLL-X shared task (Buch-
holz and Marsi, 2006) made a wide selection of
standardized treebanks for different languages avail-
able for the research community and allowed for
easy comparison between various statistical meth-
ods on a standardized benchmark. One of the sur-
prising things discovered by this evaluation is that
the best results are achieved by methods which
are quite different from state-of-the-art models for
constituent parsing, e.g. the deterministic parsing
method of (Nivre et al., 2006) and the minimum
spanning tree parser of (McDonald et al., 2006).
All the most accurate dependency parsing models
are fully discriminative, unlike constituent parsing
where all the state of the art methods have a genera-
tive component (Charniak and Johnson, 2005; Hen-
derson, 2004; Collins, 2000). Another surprising
thing is the lack of latent variable models among
the methods used in the shared task. Latent vari-
able models would allow complex features to be in-
duced automatically, which would be highly desir-
able in multilingual parsing, where manual feature
selection might be very difficult and time consum-
ing, especially for languages unknown to the parser
developer.
In this paper we propose a generative latent vari-
able model for dependency parsing. It is based on
Incremental Sigmoid Belief Networks (ISBNs), a
class of directed graphical model for structure pre-
diction problems recently proposed in (Titov and
Henderson, 2007), where they were demonstrated
to achieve competitive results on the constituent
parsing task. As discussed in (Titov and Hender-
son, 2007), computing the conditional probabili-
ties which we need for parsing is in general in-
tractable with ISBNs, but they can be approximated
efficiently in several ways. In particular, the neu-
ral network constituent parsers in (Henderson, 2003)
and (Henderson, 2004) can be regarded as coarse ap-
proximations to their corresponding ISBN model.
ISBNs use history-based probability models. The
most common approach to handling the unbounded
nature of the parse histories in these models is to
choose a pre-defined set of features which can be
unambiguously derived from the history (e.g. (Char-
niak, 2000; Collins, 1999; Nivre et al., 2004)). De-
cision probabilities are then assumed to be indepen-
dent of all information not represented by this finite
set of features. ISBNs instead use a vector of binary
</bodyText>
<page confidence="0.979443">
144
</page>
<note confidence="0.923286">
Proceedings of the 10th Conference on Parsing Technologies, pages 144–155,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999956923913044">
latent variables to encode the information about the
parser history. This history vector is similar to the
hidden state of a Hidden Markov Model. But un-
like the graphical model for an HMM, which speci-
fies conditional dependency edges only between ad-
jacent states in the sequence, the ISBN graphical
model can specify conditional dependency edges be-
tween states which are arbitrarily far apart in the
parse history. The source state of such an edge is de-
termined by the partial output structure built at the
time of the destination state, thereby allowing the
conditional dependency edges to be appropriate for
the structural nature of the problem being modeled.
This structure sensitivity is possible because ISBNs
are a constrained form of switching model (Mur-
phy, 2002), where each output decision switches the
model structure used for the remaining decisions.
We build an ISBN model of dependency parsing
using the parsing order proposed in (Nivre et al.,
2004). However, instead of performing determin-
istic parsing as in (Nivre et al., 2004), we use this
ordering to define a generative history-based model,
by integrating word prediction operations into the
set of parser actions. Then we propose a simple, lan-
guage independent set of relations which determine
how latent variable vectors are interconnected by
conditional dependency edges in the ISBN model.
ISBNs also condition the latent variable vectors on a
set of explicit features, which we vary in the experi-
ments.
In experiments we evaluate both the performance
of the ISBN dependency parser compared to previ-
ous work, and the ability of the ISBN model to in-
duce complex history features. Our model achieves
state-of-the-art performance on the languages we
test, significantly outperforming the model of (Nivre
et al., 2006) on two languages out of three and
demonstrating about the same results on the third.
In order to test the model’s feature induction abili-
ties, we train models with two different sets of ex-
plicit conditioning features: the feature set individu-
ally tuned by (Nivre et al., 2006) for each considered
language, and a minimal set of local features. These
models achieve comparable accuracy, unlike with
the discriminative SVM-based approach of (Nivre et
al., 2006), where careful feature selection appears to
be crucial. We also conduct a controlled experiment
where we used the tuned features of (Nivre et al.,
2006) but disable the feature induction abilities of
our model by elimination of the edges connecting
latent state vectors. This restricted model achieves
far worse results, showing that it is exactly the ca-
pacity of ISBNs to induce history features which is
the key to its success. It also motivates further re-
search into how feature induction techniques can be
exploited in discriminative parsing methods.
We analyze how the relation accuracy changes
with the length of the head-dependent relation,
demonstrating that our model very significantly out-
performs the state-of-the-art baseline of (Nivre et
al., 2006) on long dependencies. Additional exper-
iments suggest that both feature induction abilities
and use of the beam search contribute to this im-
provement.
The fact that our model defines a probability
model over parse trees, unlike the previous state-of-
the-art methods (Nivre et al., 2006; McDonald et al.,
2006), makes it easier to use this model in appli-
cations which require probability estimates, e.g. in
language processing pipelines. Also, as with any
generative model, it may be easy to improve the
parser’s accuracy by using discriminative retraining
techniques (Henderson, 2004) or data-defined ker-
nels (Henderson and Titov, 2005), with or even with-
out introduction of any additional linguistic features.
In addition, there are some applications, such as lan-
guage modeling, which require generative models.
Another advantage of generative models is that they
do not suffer from the label bias problems (Bot-
tou, 1991), which is a potential problem for con-
ditional or deterministic history-based models, such
as (Nivre et al., 2004).
In the remainder of this paper, we will first review
general ISBNs and how they can be approximated.
Then we will define the generative parsing model,
based on the algorithm of (Nivre et al., 2004), and
propose an ISBN for this model. The empirical part
of the paper then evaluates both the overall accuracy
of this method and the importance of the model’s
capacity to induce features. Additional related work
will be discussed in the last section before conclud-
ing.
</bodyText>
<page confidence="0.999304">
145
</page>
<sectionHeader confidence="0.985572" genericHeader="introduction">
2 The Latent Variable Architecture
</sectionHeader>
<bodyText confidence="0.9999574">
In this section we will begin by briefly introduc-
ing the class of graphical models we will be us-
ing, Incremental Sigmoid Belief Networks (Titov
and Henderson, 2007). ISBNs are designed specif-
ically for modeling structured data. They are latent
variable models which are not tractable to compute
exactly, but two approximations exist which have
been shown to be effective for constituent parsing
(Titov and Henderson, 2007). Finally, we present
how these approximations can be trained.
</bodyText>
<subsectionHeader confidence="0.988247">
2.1 Incremental Sigmoid Belief Networks
</subsectionHeader>
<bodyText confidence="0.99994875">
An ISBN is a form of Sigmoid Belief Network
(SBN) (Neal, 1992). SBNs are Bayesian Networks
with binary variables and conditional probability
distributions in the form:
</bodyText>
<equation confidence="0.9883815">
P(Si = 1�Par(Si)) = Q( 1] JijSj),
SjEPar(Si)
</equation>
<bodyText confidence="0.9999472">
where Si are the variables, Par(Si) are the variables
which Si depends on (its parents), Q denotes the lo-
gistic sigmoid function, and Jij is the weight for the
edge from variable Sj to variable Si in the graphi-
cal model. SBNs are similar to feed-forward neural
networks, but unlike neural networks, SBNs have a
precise probabilistic semantics for their hidden vari-
ables. ISBNs are based on a generalized version of
SBNs where variables with any range of discrete val-
ues are allowed. The normalized exponential func-
tion (’soft-max’) is used to define the conditional
probability distributions at these nodes.
To extend SBNs for processing arbitrarily long se-
quences, such as a parser’s sequence of decisions
D1,..., Dm, SBNs are extended to a form of Dy-
namic Bayesian Network (DBN). In DBNs, a new
set of variables is instantiated for each position in
the sequence, but the edges and weights are the same
for each position in the sequence. The edges which
connect variables instantiated for different positions
must be directed forward in the sequence, thereby
allowing a temporal interpretation of the sequence.
Incremental Sigmoid Belief Networks (Titov and
Henderson, 2007) differ from simple dynamic SBNs
in that they allow the model structure to depend on
the output variable values. Specifically, a decision is
allowed to effect the placement of any edge whose
destination is after the decision. This results in a
form of switching model (Murphy, 2002), where
each decision switches the model structure used for
the remaining decisions. The incoming edges for
a given position are a discrete function of the se-
quence of decisions which precede that position.
This makes the ISBN an “incremental” model, not
just a dynamic model. The structure of the model is
determined incrementally as the decision sequence
proceeds.
ISBNs are designed to allow the model structure
to depend on the output values without overly com-
plicating the inference of the desired conditional
probabilities P(DtJD1, . . . , Dt�1), the probability
of the next decision given the history of previous de-
cisions. In particular, it is never necessary to sum
over all possible model structures, which in general
would make inference intractable.
</bodyText>
<subsectionHeader confidence="0.999341">
2.2 Modeling Structures with ISBNs
</subsectionHeader>
<bodyText confidence="0.999899857142857">
ISBNs are designed for modeling structured data
where the output structure is not given as part of
the input. In dependency parsing, this means they
can model the probability of an output dependency
structure when the input only specifies the sequence
of words (i.e. parsing). The difficulty with such
problems is that the statistical dependencies in the
dependency structure are local in the structure, and
not necessarily local in the word sequence. ISBNs
allow us to capture these statistical dependencies in
the model structure by having model edges depend
on the output variables which specify the depen-
dency structure. For example, if an output specifies
that there is a dependency arc from word wi to word
wj, then any future decision involving wj can di-
rectly depend on its head wi. This allows the head
wi to be treated as local to the dependent wj even if
they are far apart in the sentence.
This structurally-defined notion of locality is par-
ticularly important for the model’s latent variables.
When the structurally-defined model edges connect
latent variables, information can be propagated be-
tween latent variables, thereby providing an even
larger structural domain of locality than that pro-
vided by single edges. This provides a poten-
tially powerful form of feature induction, which is
nonetheless biased toward a notion of locality which
is appropriate for the structure of the problem.
</bodyText>
<page confidence="0.997791">
146
</page>
<subsectionHeader confidence="0.99901">
2.3 Approximating ISBNs
</subsectionHeader>
<bodyText confidence="0.999966608695652">
(Titov and Henderson, 2007) proposes two approxi-
mations for inference in ISBNs, both based on vari-
ational methods. The main idea of variational meth-
ods (Jordan et al., 1999) is, roughly, to construct a
tractable approximate model with a number of free
parameters. The values of the free parameters are set
so that the resulting approximate model is as close as
possible to the original graphical model for a given
inference problem.
The simplest example of a variation method is the
mean field method, which uses a fully factorized dis-
tribution Q(HIV) = ni Qi(hi|V ) as the approxi-
mate model, where V are the visible (i.e. known)
variables, H = hi, ... , hl are the hidden (i.e. la-
tent) variables, and each Qi is the distribution of an
individual latent variable hi. The free parameters of
this approximate model are the means µi of the dis-
tributions Qi.
(Titov and Henderson, 2007) proposes two ap-
proximate models based on the variational approach.
First, they show that the neural network of (Hen-
derson, 2003) can be viewed as a coarse mean field
approximation of ISBNs, which they call the feed-
forward approximation. This approximation im-
poses the constraint that the free parameters µi of
the approximate model are only allowed to depend
on the distributions of their parent variables. This
constraint increases the potential for a large approx-
imation error, but it significantly simplifies the com-
putations by allowing all the free parameters to be
set in a single pass over the model.
The second approximation proposed in (Titov and
Henderson, 2007) takes into consideration the fact
that, after each decision is made, all the preceding
latent variables should have their means µi updated.
This approximation extends the feed-forward ap-
proximation to account for the most important com-
ponents of this update. They call this approxima-
tion the mean field approximation, because a mean
field approximation is applied to handle the statisti-
cal dependencies introduced by the new decisions.
This approximation was shown to be a more accu-
rate approximation of ISBNs than the feed-forward
approximation, but remain tractable. It was also
shown to achieve significantly better accuracy on
constituent parsing.
</bodyText>
<subsectionHeader confidence="0.997433">
2.4 Learning
</subsectionHeader>
<bodyText confidence="0.999965611111111">
Training these approximations of ISBNs is done to
maximize the fit of the approximate models to the
data. We use gradient descent, and a regularized
maximum likelihood objective function. Gaussian
regularization is applied, which is equivalent to the
weight decay standardly used in neural networks.
Regularization was reduced through the course of
learning.
Gradient descent requires computing the deriva-
tives of the objective function with respect to the
model parameters. In the feed-forward approxima-
tion, this can be done with the standard Backpropa-
gation learning used with neural networks. For the
mean field approximation, propagating the error all
the way back through the structure of the graphical
model requires a more complicated calculation, but
it can still be done efficiently (see (Titov and Hen-
derson, 2007) for details).
</bodyText>
<sectionHeader confidence="0.970915" genericHeader="method">
3 The Dependency Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.998089346153846">
The sequences of decisions Dl,..., D&amp;quot;1z which we
will be modeling with ISBNs are the sequences of
decisions made by a dependency parser. For this we
use the parsing strategy for projective dependency
parsing introduced in (Nivre et al., 2004), which
is similar to a standard shift-reduce algorithm for
context-free grammars (Aho et al., 1986). It can
be viewed as a mixture of bottom-up and top-down
parsing strategies, where left dependencies are con-
structed in a bottom-up fashion and right dependen-
cies are constructed top-down. For details we refer
the reader to (Nivre et al., 2004). In this section we
briefly describe the algorithm and explain how we
use it to define our history-based probability model.
In this paper, as in the CoNLL-X shared task,
we consider labeled dependency parsing. The state
of the parser is defined by the current stack 5, the
queue I of remaining input words and the partial la-
beled dependency structure constructed by previous
parser decisions. The parser starts with an empty
stack 5 and terminates when it reaches a configura-
tion with an empty queue I. The algorithm uses 4
types of decisions:
1. The decision Left-Arc, adds a dependency arc
from the next input word w3 to the word wi on
top of the stack and selects the label r for the
</bodyText>
<page confidence="0.990692">
147
</page>
<bodyText confidence="0.9587545">
relation between wi and wj. Word wi is then
popped from the stack.
</bodyText>
<listItem confidence="0.990408">
2. The decision Right-Arcr adds an arc from the
word wi on top of the stack to the next input
word wj and selects the label r for the relation
between wi and wj.
3. The decision Reduce pops the word wi from
the stack.
4. The decision Shiftwj shifts the word wj from
the queue to the stack.
</listItem>
<bodyText confidence="0.955714">
Unlike the original definition in (Nivre et al., 2004)
the Right-Arcr decision does not shift wj to the
stack. However, the only thing the parser can do
after a Right-Arcr decision is to choose the Shiftwj
decision. This subtle modification does not change
the actual parsing order, but it does simplify the def-
inition of our graphical model, as explained in sec-
tion 4.
We use a history-based probability model, which
decomposes the probability of the parse according
to the parser decisions:
</bodyText>
<equation confidence="0.9874445">
P (T ) = P (D1, ..., Dm) = � P (Dt|D1,...,Dt−1),
t
</equation>
<bodyText confidence="0.9988866">
where T is the parse tree and D1, ... , Dm is its
equivalent sequence of parser decisions. Since we
need a generative model, the action Shiftwj also pre-
dicts the next word in the queue I, wj+1, thus the
P(Shiftwi|D1,..., Dt−1) is a probability both of
the shift operation and the word wj+1 conditioned
on current parsing history.1
Instead of treating each Dt as an atomic decision,
it is convenient to split it into a sequence of elemen-
tary decisions Dt = dt1, ... , dtn:
</bodyText>
<equation confidence="0.91897175">
P D D ,...,Dt�) II
( tI =
— P(dtk|h(t, k)),
k
</equation>
<bodyText confidence="0.999569909090909">
&apos;In preliminary experiments, we also considered look-
ahead, where the word is predicted earlier than it appears at the
head of the queue I, and “anti-look-ahead”, where the word is
predicted only when it is shifted to the stack S. Early predic-
tion allows conditioning decision probabilities on the words in
the look-ahead and, thus, speeds up the search for an optimal
decision sequence. However, the loss of accuracy with look-
ahead was quite significant. The described method, where a
new word is predicted when it appears at the head of the queue,
led to the most accurate model and quite efficient search. The
anti-look-ahead model was both less accurate and slower.
</bodyText>
<figureCaption confidence="0.998373">
Figure 1: An ISBN for estimating P(dtk|h(t, k)).
</figureCaption>
<bodyText confidence="0.9999749">
where h(t, k) denotes the parsing history
D1, ... , Dt−1, dt1, ... , dtk−1. We split Left-Arcr
and Right-Arcr each into two elementary decisions:
first, the parser decides to create the corresponding
arc, then, it decides to assign a relation r to the
arc. Similarly, we decompose the decision Shiftwj
into an elementary decision to shift a word and a
prediction of the word wj+1. In our experiments we
use datasets from the CoNLL-X shared task, which
provide additional properties for each word token,
such as its part-of-speech tag and some fine-grain
features. This information implicitly induces word
clustering, which we use in our model: first we
predict a part-of-speech tag for the word, then a set
of word features, treating feature combination as an
atomic value, and only then a particular word form.
This approach allows us to both decrease the effect
of sparsity and to avoid normalization across all the
words in the vocabulary, significantly reducing the
computational expense of word prediction.
</bodyText>
<sectionHeader confidence="0.984798" genericHeader="method">
4 An ISBN for Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999907071428571">
In this section we define the ISBN model we use for
dependency parsing. An example of this ISBN for
estimating P(dtk|h(t, k)) is illustrated in figure 1. It
is organized into vectors of variables: latent state
variable vectors St0 = st01 , ... , st0n, representing an
intermediate state at position t0, and decision vari-
able vectors Dt0, representing a decision at position
t0, where t0 ≤ t. Variables whose value are given at
the current decision (t, k) are shaded in figure 1, la-
tent and current decision variables are left unshaded.
As illustrated by the edges in figure 1, the prob-
ability of each state variable st0 i(the individual cir-
cles in St0) depends on all the variables in a finite
set of relevant previous state and decision vectors,
</bodyText>
<page confidence="0.992831">
148
</page>
<bodyText confidence="0.999449435897436">
but there are no direct dependencies between the dif-
ferent variables in a single state vector. For each
relevant decision vector, the precise set of decision
variables which are connected in this way can be
adapted to a particular language. As long as these
connected decisions include all the new information
about the parse, the performance of the model is not
very sensitive to this choice. This is because ISBNs
have the ability to induce their own complex features
of the parse history, as demonstrated in the experi-
ments in section 6.
The most important design decision in building
an ISBN model is choosing the finite set of relevant
previous state vectors for the current decision. By
connecting to a previous state, we place that state in
the local context of the current decision. This speci-
fication of the domain of locality determines the in-
ductive bias of learning with ISBNs. When deciding
what information to store in its latent variables, an
ISBN is more likely to choose information which
is immediately local to the current decision. This
stored information then becomes local to any fol-
lowing connected decision, where it again has some
chance of being chosen as relevant to that decision.
In this way, the information available to a given deci-
sion can come from arbitrarily far away in the chain
of interconnected states, but it is much more likely
to come from a state which is relatively local. Thus,
we need to choose the set of local (i.e. connected)
states in accordance with our prior knowledge about
which previous decisions are likely to be particularly
relevant to the current decision.
To choose which previous decisions are particu-
larly relevant to the current decision, we make use
of the partial dependency structure which has been
decided so far in the parse. Specifically, the current
latent state vector is connected to a set of 7 previous
latent state vectors (if they exist) according to the
following relationships:
</bodyText>
<listItem confidence="0.960339647058824">
1. Input Context: the last previous state with the
same queue I.
2. Stack Context: the last previous state with the
same stack S.
3. Right Child of Top of S: the last previous state
where the rightmost right child of the current
stack top was on top of the stack.
4. Left Child of Top of S: the last previous state
where the leftmost left child of the current stack
top was on top of the stack.
5. Left Child of Front of I2 : the last previous
state where the leftmost child of the front ele-
ment of I was on top of the stack.
6. Head of Top: the last previous state where the
head word of the current stack top was on top
of the stack.
7. Top of S at Front of I: the last previous state
</listItem>
<bodyText confidence="0.995281625">
where the current stack top was at the front of
the queue.
Each of these 7 relations has its own distinct weight
matrix for the resulting edges in the ISBN, but the
same weight matrix is used at each position where
the relation is relevant.
All these relations but the last one are motivated
by linguistic considerations. The current decision is
primarily about what to do with the current word on
the top of the stack and the current word on the front
of the queue. The Input Context and Stack Context
relationships connect to the most recent states used
for making decisions about each of these words. The
Right Child of Top of S relationship connects to a
state used for making decisions about the most re-
cently attached dependent of the stack top. Simi-
larly, the Left Child of Front of I relationship con-
nects to a state for the most recently attached depen-
dent of the queue front. The Left Child of Top of S
is the first dependent of the stack top, which is a par-
ticularly informative dependent for many languages.
Likewise, the Head of Top can tell us a lot about the
stack top, if it has been chosen already.
A second motivation for including a state in the
local context of a decision is that it might contain in-
formation which has no other route for reaching the
current decision. In particular, it is generally a good
idea to ensure that the immediately preceding state is
always included somewhere in the set of connected
states. This requirement ensures that information, at
least theoretically, can pass between any two states
in the decision sequence, thereby avoiding any hard
</bodyText>
<footnote confidence="0.994877666666667">
2We refer to the head of the queue as the front, to avoid
unnecessary ambiguity of the word head in the context of de-
pendency parsing.
</footnote>
<page confidence="0.998846">
149
</page>
<bodyText confidence="0.9993583">
independence assumptions. The last relation, Top of
5 at Front of I, is included mainly to fulfill this re-
quirement. Otherwise, after a Shiftwj operation, the
preceding state would not be selected by any of the
relationships.
As indicated in figure 1, the probability of each
elementary decision dt0k depends both on the current
state vector 5t0 and on the previously chosen ele-
mentary action dt0k−1 from Dt0. This probability dis-
tribution has the form of a normalized exponential:
</bodyText>
<equation confidence="0.999385">
, &apos;bh(t0 k) (d) ey-j Wdjst0
j
P(dt0k = d|5t , dtk−1) = t0
Ed0kh(t0,k) (dl) eEj Wd0jsj
</equation>
<bodyText confidence="0.999961583333333">
where 4bh(t0,k) is the indicator function of the set of
elementary decisions that may possibly follow the
last decision in the history h(t&apos;, k), and the Wdj are
the weights. Now it is easy to see why the origi-
nal decision Right-Arcr (Nivre et al., 2004) had to
be decomposed into two distinct decisions: the de-
cision to construct a labeled arc and the decision to
shift the word. Use of this composite Right-Arcr
would have required the introduction of individual
parameters for each pair (w, r), where w is an arbi-
trary word in the lexicon and r - an arbitrary depen-
dency relation.
</bodyText>
<sectionHeader confidence="0.919742" genericHeader="method">
5 Searching for the Best Tree
</sectionHeader>
<bodyText confidence="0.999941375">
ISBNs define a probability model which does not
make any a-priori assumptions of independence be-
tween any decision variables. As we discussed in
section 4 use of relations based on partial output
structure makes it possible to take into account sta-
tistical interdependencies between decisions closely
related in the output structure, but separated by mul-
tiple decisions in the input structure. This property
leads to exponential complexity of complete search.
However, the success of the deterministic parsing
strategy which uses the same parsing order (Nivre et
al., 2006), suggests that it should be relatively easy
to find an accurate approximation to the best parse
with heuristic search methods. Unlike (Nivre et al.,
2006), we can not use a lookahead in our generative
model, as was discussed in section 3, so a greedy
method is unlikely to lead to a good approximation.
Instead we use a pruning strategy similar to that de-
scribed in (Henderson, 2003), where it was applied
to a considerably harder search problem: constituent
parsing with a left-corner parsing order.
We apply fixed beam pruning after each deci-
sion Shiftwj, because knowledge of the next word
in the queue I helps distinguish unlikely decision
sequences. We could have used best-first search be-
tween Shiftwj operations, but this still leads to rela-
tively expensive computations, especially when the
set of dependency relations is large. However, most
of the word pairs can possibly participate only in a
very limited number of distinct relations. Thus, we
pursue only a fixed number of relations r after each
Left-Arcr and Right-Arcr operation.
Experiments with a variety of post-shift beam
widths confirmed that very small validation perfor-
mance gains are achieved with widths larger than 30,
and sometimes even a beam of 5 was sufficient. We
found also that allowing 5 different relations after
each dependency prediction operation was enough
that it had virtually no effect on the validation accu-
racy.
</bodyText>
<sectionHeader confidence="0.995912" genericHeader="method">
6 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.999984105263158">
In this section we evaluate the ISBN model for
dependency parsing on three treebanks from the
CoNLL-X shared task. We compare our genera-
tive models with the best parsers from the CoNLL-
X task, including the SVM-based parser of (Nivre et
al., 2006) (the MALT parser), which uses the same
parsing algorithm. To test the feature induction abil-
ities of our model we compare results with two fea-
ture sets, the feature set tuned individually for each
language by (Nivre et al., 2006), and another fea-
ture set which includes only obvious local features.
This simple feature set comprises only features of
the word on top of the stack 5 and the front word
of the queue I. We compare the gain from using
tuned features with the similar gain obtained by the
MALT parser. To obtain these results we train the
MALT parser with the same two feature sets.3
In order to distinguish the contribution of ISBN’s
feature induction abilities from the contribution of
</bodyText>
<footnote confidence="0.995765666666667">
3The tuned feature sets were obtained from
http://w3.msi.vxu.se/˜nivre/research/MaltParser.html. We
removed lookahead features for ISBN experiments but
preserved them for experiments with the MALT parser. Anal-
ogously, we extended simple features with 3 words lookahead
for the MALT parser experiments.
</footnote>
<page confidence="0.998272">
150
</page>
<bodyText confidence="0.9999229">
our estimation method and search, we perform an-
other experiment. We use the tuned feature set and
disable the feature induction abilities of the model
by removing all the edges between latent variables
vectors. Comparison of this restricted model with
the full ISBN model shows how important the fea-
ture induction is. Also, comparison of this restricted
model with the MALT parser, which uses the same
set of features, indicates whether our generative esti-
mation method and use of beam search is beneficial.
</bodyText>
<subsectionHeader confidence="0.989098">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.997912619047619">
We used the CoNLL-X distributions of Danish
DDT treebank (Kromann, 2003), Dutch Alpino tree-
bank (van der Beek et al., 2002) and Slovene SDT
treebank (Dzeroski et al., 2006). The choice of these
treebanks was motivated by the fact that they all
are freely distributed and have very different sizes
of their training sets: 195,069 tokens for Dutch,
94,386 tokens for Danish and only 28,750 tokens for
Slovene. As it is generally believed that discrimina-
tive models win over generative models with a large
amount of training data, so we expected to see simi-
lar trend in our results. Test sets are about equal and
contain about 5,000 scoring tokens.
We followed the experimental setup of the shared
task and used all the information provided for the
languages: gold standard part-of-speech tags and
coarse part-of-speech tags, word form, word lemma
(lemma information was not available for Danish)
and a set of fine-grain word features. As we ex-
plained in section 3, we treated these sets of fine-
grain features as an atomic value when predicting
a word. However, when conditioning on words, we
treated each component of this composite feature in-
dividually, as it proved to be useful on the develop-
ment set. We used frequency cutoffs: we ignored
any property (e.g., word form, feature or even part-
of-speech tag4) which occurs in the training set less
than 5 times. Following (Nivre et al., 2006), we used
pseudo-projective transformation they proposed to
cast non-projective parsing tasks as projective.
ISBN models were trained using a small devel-
opment set taken out from the training set, which
was used for tuning learning parameters and for
4Part-of-speech tags for multi-word units in the Danish tree-
bank were formed as concatenation of tags of the words, which
led to quite sparse set of part-of-speech tags.
early stopping. The sizes of the development sets
were: 4,988 tokens for larger Dutch corpus, 2,504
tokens for Danish and 2,033 tokens for Slovene.
The MALT parser was trained always using the en-
tire training set. We expect that the mean field ap-
proximation should demonstrate better results than
feed-forward approximation on this task as it is the-
oretically expected and confirmed on the constituent
parsing task (Titov and Henderson, 2007). How-
ever, the sizes of testing sets would not allow us
to perform any conclusive analysis, so we decided
not to perform these comparisons here. Instead we
used the mean field approximation for the smaller
two corpora and used the feed-forward approxima-
tion for the larger one. Training the mean field ap-
proximations on the larger Dutch treebank is feasi-
ble, but would significantly reduce the possibilities
for tuning the learning parameters on the develop-
ment set and, thus, would increase the randomness
of model comparisons.
All model selection was performed on the devel-
opment set and a single model of each type was
applied to the testing set. We used a state vari-
able vector consisting of 80 binary variables, as it
proved sufficient on the preliminary experiments.
For the MALT parser we replicated the parameters
from (Nivre et al., 2006) as described in detail on
their web site.
The labeled attachment scores for the ISBN with
tuned features (TF) and local features (LF) and
ISBN with tuned features and no edges connect-
ing latent variable vectors (TF-NA) are presented
in table 1, along with results for the MALT parser
both with tuned and local feature, the MST parser
(McDonald et al., 2006), and the average score
(Aver) across all systems in the CoNLL-X shared
task. The MST parser is included because it demon-
strated the best overall result in the task, non signif-
icantly outperforming the MALT parser, which, in
turn, achieved the second best overall result. The la-
beled attachment score is computed using the same
method as in the CoNLL-X shared task, i.e. ignor-
ing punctuation. Note, that though we tried to com-
pletely replicate training of the MALT parser with
the tuned features, we obtained slightly different re-
sults. The original published results for the MALT
parser with tuned features were 84.8% for Danish,
78.6% for Dutch and 70.3% for Slovene. The im-
</bodyText>
<page confidence="0.993829">
151
</page>
<table confidence="0.9997165">
Danish Dutch Slovene
ISBN TF 85.0 79.6 72.9
LF 84.5 79.5 72.4
TF-NA 83.5 76.4 71.7
MALT TF 85.1 78.2 70.5
LF 79.8 74.5 66.8
MST 84.8 79.2 73.4
Aver 78.3 70.7 65.2
</table>
<tableCaption confidence="0.7290345">
Table 1: Labeled attachment score on the testing sets
of Danish, Dutch and Slovene treebanks.
</tableCaption>
<bodyText confidence="0.967817">
provement of the ISBN models (TF and LF) over
the MALT parser is statistically significant for Dutch
and Slovene. Differences between their results on
Danish are not statistically significant.
</bodyText>
<subsectionHeader confidence="0.999701">
6.2 Discussion of Results
</subsectionHeader>
<bodyText confidence="0.999579903225806">
The ISBN with tuned features (TF) achieved signif-
icantly better accuracy than the MALT parser on 2
languages (Dutch and Slovene), and demonstrated
essentially the same accuracy on Danish. The results
of the ISBN are among the two top published results
on all three languages, including the best published
results on Dutch. All three models, MST, MALT and
ISBN, demonstrate much better results than the av-
erage result in the CoNLL-X shared task. These re-
sults suggest that our generative model is quite com-
petitive with respect to the best models, which are
both discriminative.5 We would expect further im-
provement of ISBN results if we applied discrimina-
tive retraining (Henderson, 2004) or reranking with
data-defined kernels (Henderson and Titov, 2005),
even without introduction of any additional features.
We can see that the ISBN parser achieves about
the same results with local features (LF). Local fea-
tures by themselves are definitely not sufficient for
the construction of accurate models, as seen from
the results of the MALT parser with local features
(and look-ahead). This result demonstrates that IS-
BNs are a powerful model for feature induction.
The results of the ISBN without edges connecting
latent state vectors is slightly surprising and suggest
that without feature induction the ISBN is signifi-
cantly worse than the best models. This shows that
5Note that the development set accuracy predicted correctly
the testing set ranking of ISBN TF, LF and TF-NA models on
each of the datasets, so it is fair to compare the best ISBN result
among the three with other parsers.
</bodyText>
<table confidence="0.9985318">
to root 1 2 3 - 6 &gt; 6
Da ISBN 95.1 95.7 90.1 84.1 74.7
MALT 95.4 96.0 90.8 84.0 71.6
Du ISBN 79.8 92.4 86.2 81.4 71.1
MALT 73.1 91.9 85.0 76.2 64.3
Sl ISBN 76.1 92.5 85.6 79.6 54.3
MALT 59.9 92.1 85.0 78.4 47.1
Av ISBN 83.6 93.5 87.3 81.7 66.7
MALT 76.2 93.3 87.0 79.5 61.0
Improv 7.5 0.2 0.4 2.2 5.7
</table>
<tableCaption confidence="0.725267333333333">
Table 2: F1 score of labeled attachment as a function
of dependency length on the testing sets of Danish,
Dutch and Slovene.
</tableCaption>
<bodyText confidence="0.9998985">
the improvement is coming mostly from the abil-
ity of the ISBN to induce complex features and not
from either using beam search or from the estima-
tion procedure. It might also suggest that genera-
tive models are probably worse for the dependency
parsing task than discriminative approaches (at least
for larger datasets). This motivates further research
into methods which combine powerful feature in-
duction properties with the advantage of discrimina-
tive training. Although discriminative reranking of
the generative model is likely to help, the derivation
of fully discriminative feature induction methods is
certainly more challenging.
In order to better understand differences in per-
formance between ISBN and MALT, we analyzed
how relation accuracy changes with the length of
the head-dependent relation. The harmonic mean
between precision and recall of labeled attachment,
F1 measure, for the ISBN and MALT parsers with
tuned features is presented in table 2. F1 score is
computed for four different ranges of lengths and
for attachments directly to root. Along with the re-
sults for each of the languages, the table includes
their mean (Av) and the absolute improvement of
the ISBN model over MALT (Improv). It is easy
to see that accuracy of both models is generally sim-
ilar for small distances (1 and 2), but as the distance
grows the ISBN parser starts to significantly outper-
form MALT, achieving 5.7% average improvement
on dependencies longer than 6 word tokens. When
the MALT parser does not manage to recover a long
dependency, the highest scoring action it can choose
is to reduce the dependent from the stack without
specifying its head, thereby attaching the dependent
</bodyText>
<page confidence="0.994584">
152
</page>
<bodyText confidence="0.999977785714286">
to the root by default. This explains the relatively
low F1 scores for attachments to root (evident for
Dutch and Slovene): though recall of attachment to
root is comparable to that of the ISBN parser (82.4%
for MALT against 84.2% for ISBN, on average over
3 languages), precision for the MALT parser is much
worse (71.5% for MALT against 83.1% for ISBN,
on average).
The considerably worse accuracy of the MALT
parser on longer dependencies might be explained
both by use of a non-greedy search method in the
ISBN and the ability of ISBNs to induce history fea-
tures. To capture a long dependency, the MALT
parser should keep a word on the stack during a
long sequence of decision. If at any point during
the intermediate steps this choice seems not to be
locally optimal, then the MALT parser will choose
the alternative and lose the possibility of the long
dependency.6 By using a beam search, the ISBN
parser can maintain the possibility of the long de-
pendency in its beam even when other alternatives
seem locally preferable. Also, long dependences are
often more difficult, and may be systematically dif-
ferent from local dependencies. The designer of a
MALT parser needs to discover predictive features
for long dependencies by hand, whereas the ISBN
model can automatically discover them. Thus we
expect that the feature induction abilities of ISBNs
have a strong effect on the accuracy of long depen-
dences. This prediction is confirmed by the differ-
ences between the results of the normal ISBN (TF)
and the restricted ISBN (TF-NA) model. The TF-
NA model, like the MALT parser, is biased toward
attachment to root; it attaches to root 12.0% more
words on average than the normal ISBN, without
any improvement of recall and with a great loss of
precision. The F1 score on long dependences for the
TF-NA model is also negatively effected in the same
way as for the MALT parser. This confirms that the
ability of the ISBN model to induce features is a ma-
jor factor in improving accuracy of long dependen-
cies.
</bodyText>
<footnote confidence="0.917790833333333">
6The MALT parser is trained to keep the word as long as
possible: ifboth Shift and Reduce decisions are possible during
training, it always prefers to shift. Though this strategy should
generally reduce the described problem, it is evident from the
low precision score for attachment to root, that it can not com-
pletely eliminate it.
</footnote>
<sectionHeader confidence="0.998721" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999946">
There has not been much previous work on latent
variable models for dependency parsing. Depen-
dency parsing with Dynamic Bayesian Networks
was considered in (Peshkin and Savova, 2005), with
limited success. Roughly, the model considered
the whole sentence at a time, with the DBN being
used to decide which words correspond to leaves
of the tree. The chosen words are then removed
from the sentence and the model is recursively ap-
plied to the reduced sentence. Recently several la-
tent variable models for constituent parsing have
been proposed (Koo and Collins, 2005; Matsuzaki
et al., 2005; Prescher, 2005; Riezler et al., 2002).
In (Matsuzaki et al., 2005) non-terminals in a stan-
dard PCFG model are augmented with latent vari-
ables. A similar model of (Prescher, 2005) uses a
head-driven PCFG with latent heads, thus restrict-
ing the flexibility of the latent-variable model by us-
ing explicit linguistic constraints. While the model
of (Matsuzaki et al., 2005) significantly outperforms
the constrained model of (Prescher, 2005), they both
are well below the state-of-the-art in constituent
parsing. In (Koo and Collins, 2005), an undirected
graphical model for constituent parse reranking uses
dependency relations to define the edges. Thus, it
should be easy to apply a similar method to rerank-
ing dependency trees.
Undirected graphical models, in particular Condi-
tional Random Fields, are the standard tools for shal-
low parsing (Sha and Pereira, 2003). However, shal-
low parsing is effectively a sequence labeling prob-
lem and therefore differs significantly from full pars-
ing. As discussed in (Titov and Henderson, 2007),
undirected graphical models do not seem to be suit-
able for history-based parsing models.
Sigmoid Belief Networks (SBNs) were used orig-
inally for character recognition tasks, but later a dy-
namic modification of this model was applied to the
reinforcement learning task (Sallans, 2002). How-
ever, their graphical model, approximation method,
and learning method differ significantly from those
of this paper. The extension of dynamic SBNs with
incrementally specified model structure (i.e. Incre-
mental Sigmoid Belief Networks, used in this pa-
per) was proposed and applied to constituent parsing
in (Titov and Henderson, 2007).
</bodyText>
<page confidence="0.99889">
153
</page>
<sectionHeader confidence="0.998833" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999994615384615">
We proposed a latent variable dependency parsing
model based on Incremental Sigmoid Belief Net-
works. Unlike state-of-the-art dependency parsers,
it uses a generative history-based model. We demon-
strated that it achieves state-of-the-art results on a
selection of languages from the CoNLL-X shared
task. The parser uses a vector of latent variables
to represent an intermediate state and uses rela-
tions defined on the output structure to construct the
edges between latent state vectors. These proper-
ties make it a powerful feature induction method
for dependency parsing, and it achieves competi-
tive results even with very simple explicit features.
The ISBN model is especially accurate at modeling
long dependences, achieving average improvement
of 5.7% over the state-of-the-art baseline on depen-
dences longer than 6 words. Empirical evaluation
demonstrates that competitive results are achieved
mostly because of the ability of the model to in-
duce complex features and not because of the use of
a generative probability model or a specific search
method. As with other generative models, it can be
further improved by the application of discrimina-
tive reranking techniques. Discriminative methods
are likely to allow it to significantly improve over
the current state-of-the-art in dependency parsing.?
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998482666666667">
This work was funded by Swiss NSF grant 200020-
109685, UK EPSRC grant EP/E019501/1, and EU
FP6 grant 507802 for project TALK. We thank
Joakim Nivre and Sandra K¨ubler for an excellent
tutorial on dependency parsing given at COLING-
ACL 2006.
</bodyText>
<sectionHeader confidence="0.999229" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993632228070175">
Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. 1986.
Compilers: Principles, Techniques and Tools. Addi-
son Wesley.
Leon Bottou. 1991. Une approche th´eoretique de
l’apprentissage connexionniste: Applications a` la re-
connaissance de la parole. Ph.D. thesis, Universit´e de
Paris XI, Paris, France.
7The ISBN dependency parser will be soon made download-
able from the authors’ web-page.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, New York, USA.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. 43rd Meeting ofAssociation for Compu-
tational Linguistics, pages 173–180, Ann Arbor, MI.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. 1st Meeting of North American
Chapter ofAssociation for Computational Linguistics,
pages 132–139, Seattle, Washington.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. 17th Int. Conf. on Ma-
chine Learning, pages 175–182, Stanford, CA.
S. Dzeroski, T. Erjavec, N. Ledinek, P. Pajas, Z. Zabokrt-
sky, and A. Zele. 2006. Towards a Slovene depen-
dency treebank. In Proc. Int. Conf. on Language Re-
sources and Evaluation (LREC), Genoa, Italy.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilis-
tic models. In Proc. 43rd Meeting of Association for
Computational Linguistics, Ann Arbor, MI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
joint meeting of North American Chapter of the Asso-
ciation for Computational Linguistics and the Human
Language Technology Conf., pages 103–110, Edmon-
ton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. 42nd
Meeting ofAssociation for Computational Linguistics,
Barcelona, Spain.
M. I. Jordan, Z.Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods for
graphical models. In Michael I. Jordan, editor, Learn-
ing in Graphical Models. MIT Press, Cambridge, MA.
Terry Koo and Michael Collins. 2005. Hidden-variable
models for discriminative reranking. In Proc. Conf. on
Empirical Methods in Natural Language Processing,
Vancouver, B.C., Canada.
Matthias T. Kromann. 2003. The Danish dependency
treebank and the underlying linguistic theory. In Pro-
ceedings of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), Vaxjo, Sweden.
</reference>
<page confidence="0.997485">
154
</page>
<reference confidence="0.999640924528302">
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the ACL,
Ann Arbor, MI.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proc. of the Tenth Con-
ference on Computational Natural Language Learn-
ing, New York, USA.
Kevin P. Murphy. 2002. Dynamic Belief Networks:
Representation, Inference and Learning. Ph.D. thesis,
University of California, Berkeley, CA.
Radford Neal. 1992. Connectionist learning of belief
networks. Artificial Intelligence, 56:71–113.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proc. of the
Eighth Conference on Computational Natural Lan-
guage Learning, pages 49–56, Boston, USA.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Pseudo-projective de-
pendency parsing with support vector machines. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, pages 221–225, New York,
USA.
Leon Peshkin and Virginia Savova. 2005. Dependency
parsing with dynamic Bayesian network. In AAAI,
20th National Conference on Artificial Intelligence,
Pittsburgh, Pennsylvania.
Detlef Prescher. 2005. Head-driven PCFGs with latent-
head statistics. In Proc. 9th Int. Workshop on Parsing
Technologies, Vancouver, Canada.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proc. 40th Meeting ofAssocia-
tion for Computational Linguistics, Philadelphia, PA.
Brian Sallans. 2002. Reinforcement Learning for Fac-
tored Markov Decision Processes. Ph.D. thesis, Uni-
versity of Toronto, Toronto, Canada.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proc. joint meet-
ing of North American Chapter of the Association for
Computational Linguistics and the Human Language
Technology Conf., Edmonton, Canada.
Ivan Titov and James Henderson. 2007. Constituent
parsing with incremental sigmoid belief networks. In
Proc. 45th Meeting of Association for Computational
Linguistics, Prague, Czech Republic.
L. van der Beek, G. Bouma, J. Daciuk, T. Gaustad,
R. Malouf, G van Noord, R. Prins, and B. Villada.
2002. The Alpino dependency treebank. Computa-
tional Linguistic in the Netherlands (CLIN).
</reference>
<page confidence="0.999006">
155
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.336124">
<title confidence="0.999978">A Latent Variable Model for Generative Dependency Parsing</title>
<author confidence="0.991517">Ivan</author>
<affiliation confidence="0.996795">University of</affiliation>
<address confidence="0.5744495">24, rue G´en´eral CH-1211 Gen`eve 4,</address>
<email confidence="0.748043">ivan.titov@cui.unige.ch</email>
<author confidence="0.892159">James</author>
<affiliation confidence="0.9867005">University of 2 Buccleuch</affiliation>
<address confidence="0.956957">Edinburgh EH8 9LW, United</address>
<email confidence="0.995127">james.henderson@ed.ac.uk</email>
<abstract confidence="0.998890285714286">We propose a generative dependency parsing model which uses binary latent variables to induce conditioning features. To define this model we use a recently proposed class of Bayesian Networks for structured prediction, Incremental Sigmoid Belief Networks. We demonstrate that the proposed model achieves state-of-the-art results on three different languages. We also demonstrate that the features induced by the ISBN’s latent variables are crucial to this success, and show that the proposed model is particularly good on long dependencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Ravi Sethi</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1986</date>
<booktitle>Compilers: Principles, Techniques and Tools.</booktitle>
<publisher>Addison Wesley.</publisher>
<contexts>
<context position="15922" citStr="Aho et al., 1986" startWordPosition="2521" endWordPosition="2524"> the mean field approximation, propagating the error all the way back through the structure of the graphical model requires a more complicated calculation, but it can still be done efficiently (see (Titov and Henderson, 2007) for details). 3 The Dependency Parsing Algorithm The sequences of decisions Dl,..., D&amp;quot;1z which we will be modeling with ISBNs are the sequences of decisions made by a dependency parser. For this we use the parsing strategy for projective dependency parsing introduced in (Nivre et al., 2004), which is similar to a standard shift-reduce algorithm for context-free grammars (Aho et al., 1986). It can be viewed as a mixture of bottom-up and top-down parsing strategies, where left dependencies are constructed in a bottom-up fashion and right dependencies are constructed top-down. For details we refer the reader to (Nivre et al., 2004). In this section we briefly describe the algorithm and explain how we use it to define our history-based probability model. In this paper, as in the CoNLL-X shared task, we consider labeled dependency parsing. The state of the parser is defined by the current stack 5, the queue I of remaining input words and the partial labeled dependency structure con</context>
</contexts>
<marker>Aho, Sethi, Ullman, 1986</marker>
<rawString>Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. 1986. Compilers: Principles, Techniques and Tools. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Bottou</author>
</authors>
<title>Une approche th´eoretique de l’apprentissage connexionniste: Applications a` la reconnaissance de la parole.</title>
<date>1991</date>
<booktitle>Ph.D. thesis, Universit´e de</booktitle>
<location>Paris XI, Paris, France.</location>
<contexts>
<context position="7437" citStr="Bottou, 1991" startWordPosition="1154" endWordPosition="1156">, makes it easier to use this model in applications which require probability estimates, e.g. in language processing pipelines. Also, as with any generative model, it may be easy to improve the parser’s accuracy by using discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without introduction of any additional linguistic features. In addition, there are some applications, such as language modeling, which require generative models. Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al., 2004). In the remainder of this paper, we will first review general ISBNs and how they can be approximated. Then we will define the generative parsing model, based on the algorithm of (Nivre et al., 2004), and propose an ISBN for this model. The empirical part of the paper then evaluates both the overall accuracy of this method and the importance of the model’s capacity to induce features. Additional related work will be discussed in the last section before concluding. 145 2 The Latent </context>
</contexts>
<marker>Bottou, 1991</marker>
<rawString>Leon Bottou. 1991. Une approche th´eoretique de l’apprentissage connexionniste: Applications a` la reconnaissance de la parole. Ph.D. thesis, Universit´e de Paris XI, Paris, France.</rawString>
</citation>
<citation valid="false">
<title>7The ISBN dependency parser will be soon made downloadable from the authors’ web-page.</title>
<marker></marker>
<rawString>7The ISBN dependency parser will be soon made downloadable from the authors’ web-page.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<location>New York, USA.</location>
<contexts>
<context position="1019" citStr="Buchholz and Marsi, 2006" startWordPosition="143" endWordPosition="147">itioning features. To define this model we use a recently proposed class of Bayesian Networks for structured prediction, Incremental Sigmoid Belief Networks. We demonstrate that the proposed model achieves state-of-the-art results on three different languages. We also demonstrate that the features induced by the ISBN’s latent variables are crucial to this success, and show that the proposed model is particularly good on long dependencies. 1 Introduction Dependency parsing has been a topic of active research in natural language processing during the last several years. The CoNLL-X shared task (Buchholz and Marsi, 2006) made a wide selection of standardized treebanks for different languages available for the research community and allowed for easy comparison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unli</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of the Tenth Conference on Computational Natural Language Learning, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. 43rd Meeting ofAssociation for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1736" citStr="Charniak and Johnson, 2005" startWordPosition="254" endWordPosition="257">research community and allowed for easy comparison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unlike constituent parsing where all the state of the art methods have a generative component (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000). Another surprising thing is the lack of latent variable models among the methods used in the shared task. Latent variable models would allow complex features to be induced automatically, which would be highly desirable in multilingual parsing, where manual feature selection might be very difficult and time consuming, especially for languages unknown to the parser developer. In this paper we propose a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for stru</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proc. 43rd Meeting ofAssociation for Computational Linguistics, pages 173–180, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. 1st Meeting of North American Chapter ofAssociation for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="3142" citStr="Charniak, 2000" startWordPosition="475" endWordPosition="477">and Henderson, 2007), computing the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories in these models is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999; Nivre et al., 2004)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. ISBNs instead use a vector of binary 144 Proceedings of the 10th Conference on Parsing Technologies, pages 144–155, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics latent variables to encode the information about the parser history. This history vector is similar to the hidden state of a Hidden Markov Model. But unlike the graphical model for an HMM, which specifies conditional dependency edges on</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proc. 1st Meeting of North American Chapter ofAssociation for Computational Linguistics, pages 132–139, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="3157" citStr="Collins, 1999" startWordPosition="478" endWordPosition="479">007), computing the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories in these models is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999; Nivre et al., 2004)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. ISBNs instead use a vector of binary 144 Proceedings of the 10th Conference on Parsing Technologies, pages 144–155, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics latent variables to encode the information about the parser history. This history vector is similar to the hidden state of a Hidden Markov Model. But unlike the graphical model for an HMM, which specifies conditional dependency edges only between adja</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. 17th Int. Conf. on Machine Learning,</booktitle>
<pages>175--182</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="1769" citStr="Collins, 2000" startWordPosition="261" endWordPosition="262">rison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unlike constituent parsing where all the state of the art methods have a generative component (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000). Another surprising thing is the lack of latent variable models among the methods used in the shared task. Latent variable models would allow complex features to be induced automatically, which would be highly desirable in multilingual parsing, where manual feature selection might be very difficult and time consuming, especially for languages unknown to the parser developer. In this paper we propose a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recentl</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proc. 17th Int. Conf. on Machine Learning, pages 175–182, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dzeroski</author>
<author>T Erjavec</author>
<author>N Ledinek</author>
<author>P Pajas</author>
<author>Z Zabokrtsky</author>
<author>A Zele</author>
</authors>
<title>Towards a Slovene dependency treebank.</title>
<date>2006</date>
<booktitle>In Proc. Int. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="30303" citStr="Dzeroski et al., 2006" startWordPosition="5004" endWordPosition="5007"> feature set and disable the feature induction abilities of the model by removing all the edges between latent variables vectors. Comparison of this restricted model with the full ISBN model shows how important the feature induction is. Also, comparison of this restricted model with the MALT parser, which uses the same set of features, indicates whether our generative estimation method and use of beam search is beneficial. 6.1 Experimental Setup We used the CoNLL-X distributions of Danish DDT treebank (Kromann, 2003), Dutch Alpino treebank (van der Beek et al., 2002) and Slovene SDT treebank (Dzeroski et al., 2006). The choice of these treebanks was motivated by the fact that they all are freely distributed and have very different sizes of their training sets: 195,069 tokens for Dutch, 94,386 tokens for Danish and only 28,750 tokens for Slovene. As it is generally believed that discriminative models win over generative models with a large amount of training data, so we expected to see similar trend in our results. Test sets are about equal and contain about 5,000 scoring tokens. We followed the experimental setup of the shared task and used all the information provided for the languages: gold standard p</context>
</contexts>
<marker>Dzeroski, Erjavec, Ledinek, Pajas, Zabokrtsky, Zele, 2006</marker>
<rawString>S. Dzeroski, T. Erjavec, N. Ledinek, P. Pajas, Z. Zabokrtsky, and A. Zele. 2006. Towards a Slovene dependency treebank. In Proc. Int. Conf. on Language Resources and Evaluation (LREC), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Ivan Titov</author>
</authors>
<title>Data-defined kernels for parse reranking derived from probabilistic models.</title>
<date>2005</date>
<booktitle>In Proc. 43rd Meeting of Association for Computational Linguistics,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="7151" citStr="Henderson and Titov, 2005" startWordPosition="1108" endWordPosition="1111">dependencies. Additional experiments suggest that both feature induction abilities and use of the beam search contribute to this improvement. The fact that our model defines a probability model over parse trees, unlike the previous state-ofthe-art methods (Nivre et al., 2006; McDonald et al., 2006), makes it easier to use this model in applications which require probability estimates, e.g. in language processing pipelines. Also, as with any generative model, it may be easy to improve the parser’s accuracy by using discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without introduction of any additional linguistic features. In addition, there are some applications, such as language modeling, which require generative models. Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al., 2004). In the remainder of this paper, we will first review general ISBNs and how they can be approximated. Then we will define the generative parsing model, based on the algorithm of (Nivre et al., 2004),</context>
<context position="35500" citStr="Henderson and Titov, 2005" startWordPosition="5868" endWordPosition="5871"> and demonstrated essentially the same accuracy on Danish. The results of the ISBN are among the two top published results on all three languages, including the best published results on Dutch. All three models, MST, MALT and ISBN, demonstrate much better results than the average result in the CoNLL-X shared task. These results suggest that our generative model is quite competitive with respect to the best models, which are both discriminative.5 We would expect further improvement of ISBN results if we applied discriminative retraining (Henderson, 2004) or reranking with data-defined kernels (Henderson and Titov, 2005), even without introduction of any additional features. We can see that the ISBN parser achieves about the same results with local features (LF). Local features by themselves are definitely not sufficient for the construction of accurate models, as seen from the results of the MALT parser with local features (and look-ahead). This result demonstrates that ISBNs are a powerful model for feature induction. The results of the ISBN without edges connecting latent state vectors is slightly surprising and suggest that without feature induction the ISBN is significantly worse than the best models. Th</context>
</contexts>
<marker>Henderson, Titov, 2005</marker>
<rawString>James Henderson and Ivan Titov. 2005. Data-defined kernels for parse reranking derived from probabilistic models. In Proc. 43rd Meeting of Association for Computational Linguistics, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Inducing history representations for broad coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In Proc. joint meeting of North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conf.,</booktitle>
<pages>103--110</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2786" citStr="Henderson, 2003" startWordPosition="420" endWordPosition="421">se a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in (Titov and Henderson, 2007), where they were demonstrated to achieve competitive results on the constituent parsing task. As discussed in (Titov and Henderson, 2007), computing the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories in these models is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999; Nivre et al., 2004)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. ISBNs instead use a vector of binary 144 Proceedings of the 10th Conference on P</context>
<context position="13495" citStr="Henderson, 2003" startWordPosition="2145" endWordPosition="2147"> model for a given inference problem. The simplest example of a variation method is the mean field method, which uses a fully factorized distribution Q(HIV) = ni Qi(hi|V ) as the approximate model, where V are the visible (i.e. known) variables, H = hi, ... , hl are the hidden (i.e. latent) variables, and each Qi is the distribution of an individual latent variable hi. The free parameters of this approximate model are the means µi of the distributions Qi. (Titov and Henderson, 2007) proposes two approximate models based on the variational approach. First, they show that the neural network of (Henderson, 2003) can be viewed as a coarse mean field approximation of ISBNs, which they call the feedforward approximation. This approximation imposes the constraint that the free parameters µi of the approximate model are only allowed to depend on the distributions of their parent variables. This constraint increases the potential for a large approximation error, but it significantly simplifies the computations by allowing all the free parameters to be set in a single pass over the model. The second approximation proposed in (Titov and Henderson, 2007) takes into consideration the fact that, after each deci</context>
<context position="27303" citStr="Henderson, 2003" startWordPosition="4521" endWordPosition="4522">arated by multiple decisions in the input structure. This property leads to exponential complexity of complete search. However, the success of the deterministic parsing strategy which uses the same parsing order (Nivre et al., 2006), suggests that it should be relatively easy to find an accurate approximation to the best parse with heuristic search methods. Unlike (Nivre et al., 2006), we can not use a lookahead in our generative model, as was discussed in section 3, so a greedy method is unlikely to lead to a good approximation. Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order. We apply fixed beam pruning after each decision Shiftwj, because knowledge of the next word in the queue I helps distinguish unlikely decision sequences. We could have used best-first search between Shiftwj operations, but this still leads to relatively expensive computations, especially when the set of dependency relations is large. However, most of the word pairs can possibly participate only in a very limited number of distinct relations. Thus, we pursue only a fixed number </context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>James Henderson. 2003. Inducing history representations for broad coverage statistical parsing. In Proc. joint meeting of North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conf., pages 103–110, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In Proc. 42nd Meeting ofAssociation for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1753" citStr="Henderson, 2004" startWordPosition="258" endWordPosition="260">ed for easy comparison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unlike constituent parsing where all the state of the art methods have a generative component (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000). Another surprising thing is the lack of latent variable models among the methods used in the shared task. Latent variable models would allow complex features to be induced automatically, which would be highly desirable in multilingual parsing, where manual feature selection might be very difficult and time consuming, especially for languages unknown to the parser developer. In this paper we propose a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction </context>
<context position="7099" citStr="Henderson, 2004" startWordPosition="1102" endWordPosition="1103"> baseline of (Nivre et al., 2006) on long dependencies. Additional experiments suggest that both feature induction abilities and use of the beam search contribute to this improvement. The fact that our model defines a probability model over parse trees, unlike the previous state-ofthe-art methods (Nivre et al., 2006; McDonald et al., 2006), makes it easier to use this model in applications which require probability estimates, e.g. in language processing pipelines. Also, as with any generative model, it may be easy to improve the parser’s accuracy by using discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without introduction of any additional linguistic features. In addition, there are some applications, such as language modeling, which require generative models. Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al., 2004). In the remainder of this paper, we will first review general ISBNs and how they can be approximated. Then we will define the generative parsing mo</context>
<context position="35433" citStr="Henderson, 2004" startWordPosition="5861" endWordPosition="5862"> than the MALT parser on 2 languages (Dutch and Slovene), and demonstrated essentially the same accuracy on Danish. The results of the ISBN are among the two top published results on all three languages, including the best published results on Dutch. All three models, MST, MALT and ISBN, demonstrate much better results than the average result in the CoNLL-X shared task. These results suggest that our generative model is quite competitive with respect to the best models, which are both discriminative.5 We would expect further improvement of ISBN results if we applied discriminative retraining (Henderson, 2004) or reranking with data-defined kernels (Henderson and Titov, 2005), even without introduction of any additional features. We can see that the ISBN parser achieves about the same results with local features (LF). Local features by themselves are definitely not sufficient for the construction of accurate models, as seen from the results of the MALT parser with local features (and look-ahead). This result demonstrates that ISBNs are a powerful model for feature induction. The results of the ISBN without edges connecting latent state vectors is slightly surprising and suggest that without feature</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Discriminative training of a neural network statistical parser. In Proc. 42nd Meeting ofAssociation for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M I Jordan</author>
<author>T S Jaakkola Z Ghahramani</author>
<author>L K Saul</author>
</authors>
<title>An introduction to variational methods for graphical models.</title>
<date>1999</date>
<booktitle>Learning in Graphical Models.</booktitle>
<editor>In Michael I. Jordan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="12657" citStr="Jordan et al., 1999" startWordPosition="1998" endWordPosition="2001">’s latent variables. When the structurally-defined model edges connect latent variables, information can be propagated between latent variables, thereby providing an even larger structural domain of locality than that provided by single edges. This provides a potentially powerful form of feature induction, which is nonetheless biased toward a notion of locality which is appropriate for the structure of the problem. 146 2.3 Approximating ISBNs (Titov and Henderson, 2007) proposes two approximations for inference in ISBNs, both based on variational methods. The main idea of variational methods (Jordan et al., 1999) is, roughly, to construct a tractable approximate model with a number of free parameters. The values of the free parameters are set so that the resulting approximate model is as close as possible to the original graphical model for a given inference problem. The simplest example of a variation method is the mean field method, which uses a fully factorized distribution Q(HIV) = ni Qi(hi|V ) as the approximate model, where V are the visible (i.e. known) variables, H = hi, ... , hl are the hidden (i.e. latent) variables, and each Qi is the distribution of an individual latent variable hi. The fr</context>
</contexts>
<marker>Jordan, Ghahramani, Saul, 1999</marker>
<rawString>M. I. Jordan, Z.Ghahramani, T. S. Jaakkola, and L. K. Saul. 1999. An introduction to variational methods for graphical models. In Michael I. Jordan, editor, Learning in Graphical Models. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Hidden-variable models for discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="41375" citStr="Koo and Collins, 2005" startWordPosition="6866" endWordPosition="6869">that it can not completely eliminate it. 7 Related Work There has not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-driven PCFG with latent heads, thus restricting the flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for con</context>
</contexts>
<marker>Koo, Collins, 2005</marker>
<rawString>Terry Koo and Michael Collins. 2005. Hidden-variable models for discriminative reranking. In Proc. Conf. on Empirical Methods in Natural Language Processing, Vancouver, B.C., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias T Kromann</author>
</authors>
<title>The Danish dependency treebank and the underlying linguistic theory.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd Workshop on Treebanks and Linguistic Theories (TLT), Vaxjo,</booktitle>
<contexts>
<context position="30203" citStr="Kromann, 2003" startWordPosition="4988" endWordPosition="4989">ments. 150 our estimation method and search, we perform another experiment. We use the tuned feature set and disable the feature induction abilities of the model by removing all the edges between latent variables vectors. Comparison of this restricted model with the full ISBN model shows how important the feature induction is. Also, comparison of this restricted model with the MALT parser, which uses the same set of features, indicates whether our generative estimation method and use of beam search is beneficial. 6.1 Experimental Setup We used the CoNLL-X distributions of Danish DDT treebank (Kromann, 2003), Dutch Alpino treebank (van der Beek et al., 2002) and Slovene SDT treebank (Dzeroski et al., 2006). The choice of these treebanks was motivated by the fact that they all are freely distributed and have very different sizes of their training sets: 195,069 tokens for Dutch, 94,386 tokens for Danish and only 28,750 tokens for Slovene. As it is generally believed that discriminative models win over generative models with a large amount of training data, so we expected to see similar trend in our results. Test sets are about equal and contain about 5,000 scoring tokens. We followed the experiment</context>
</contexts>
<marker>Kromann, 2003</marker>
<rawString>Matthias T. Kromann. 2003. The Danish dependency treebank and the underlying linguistic theory. In Proceedings of the 2nd Workshop on Treebanks and Linguistic Theories (TLT), Vaxjo, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="41399" citStr="Matsuzaki et al., 2005" startWordPosition="6870" endWordPosition="6873">ely eliminate it. 7 Related Work There has not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-driven PCFG with latent heads, thus restricting the flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings of the 43rd Annual Meeting of the ACL, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a twostage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proc. of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<location>New York, USA.</location>
<contexts>
<context position="1539" citStr="McDonald et al., 2006" startWordPosition="224" endWordPosition="227"> language processing during the last several years. The CoNLL-X shared task (Buchholz and Marsi, 2006) made a wide selection of standardized treebanks for different languages available for the research community and allowed for easy comparison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unlike constituent parsing where all the state of the art methods have a generative component (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000). Another surprising thing is the lack of latent variable models among the methods used in the shared task. Latent variable models would allow complex features to be induced automatically, which would be highly desirable in multilingual parsing, where manual feature selection might be very difficult and time consuming, especially for languages unknown to the parser de</context>
<context position="6824" citStr="McDonald et al., 2006" startWordPosition="1059" endWordPosition="1062"> further research into how feature induction techniques can be exploited in discriminative parsing methods. We analyze how the relation accuracy changes with the length of the head-dependent relation, demonstrating that our model very significantly outperforms the state-of-the-art baseline of (Nivre et al., 2006) on long dependencies. Additional experiments suggest that both feature induction abilities and use of the beam search contribute to this improvement. The fact that our model defines a probability model over parse trees, unlike the previous state-ofthe-art methods (Nivre et al., 2006; McDonald et al., 2006), makes it easier to use this model in applications which require probability estimates, e.g. in language processing pipelines. Also, as with any generative model, it may be easy to improve the parser’s accuracy by using discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without introduction of any additional linguistic features. In addition, there are some applications, such as language modeling, which require generative models. Another advantage of generative models is that they do not suffer from the label bias problems (</context>
<context position="33589" citStr="McDonald et al., 2006" startWordPosition="5551" endWordPosition="5554">et and a single model of each type was applied to the testing set. We used a state variable vector consisting of 80 binary variables, as it proved sufficient on the preliminary experiments. For the MALT parser we replicated the parameters from (Nivre et al., 2006) as described in detail on their web site. The labeled attachment scores for the ISBN with tuned features (TF) and local features (LF) and ISBN with tuned features and no edges connecting latent variable vectors (TF-NA) are presented in table 1, along with results for the MALT parser both with tuned and local feature, the MST parser (McDonald et al., 2006), and the average score (Aver) across all systems in the CoNLL-X shared task. The MST parser is included because it demonstrated the best overall result in the task, non significantly outperforming the MALT parser, which, in turn, achieved the second best overall result. The labeled attachment score is computed using the same method as in the CoNLL-X shared task, i.e. ignoring punctuation. Note, that though we tried to completely replicate training of the MALT parser with the tuned features, we obtained slightly different results. The original published results for the MALT parser with tuned f</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a twostage discriminative parser. In Proc. of the Tenth Conference on Computational Natural Language Learning, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
</authors>
<title>Dynamic Belief Networks: Representation, Inference and Learning.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California,</institution>
<location>Berkeley, CA.</location>
<contexts>
<context position="4277" citStr="Murphy, 2002" startWordPosition="656" endWordPosition="658">he graphical model for an HMM, which specifies conditional dependency edges only between adjacent states in the sequence, the ISBN graphical model can specify conditional dependency edges between states which are arbitrarily far apart in the parse history. The source state of such an edge is determined by the partial output structure built at the time of the destination state, thereby allowing the conditional dependency edges to be appropriate for the structural nature of the problem being modeled. This structure sensitivity is possible because ISBNs are a constrained form of switching model (Murphy, 2002), where each output decision switches the model structure used for the remaining decisions. We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al., 2004). However, instead of performing deterministic parsing as in (Nivre et al., 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. Then we propose a simple, language independent set of relations which determine how latent variable vectors are interconnected by conditional dependency edges in the ISBN model. ISBNs a</context>
<context position="10254" citStr="Murphy, 2002" startWordPosition="1612" endWordPosition="1613">ion in the sequence, but the edges and weights are the same for each position in the sequence. The edges which connect variables instantiated for different positions must be directed forward in the sequence, thereby allowing a temporal interpretation of the sequence. Incremental Sigmoid Belief Networks (Titov and Henderson, 2007) differ from simple dynamic SBNs in that they allow the model structure to depend on the output variable values. Specifically, a decision is allowed to effect the placement of any edge whose destination is after the decision. This results in a form of switching model (Murphy, 2002), where each decision switches the model structure used for the remaining decisions. The incoming edges for a given position are a discrete function of the sequence of decisions which precede that position. This makes the ISBN an “incremental” model, not just a dynamic model. The structure of the model is determined incrementally as the decision sequence proceeds. ISBNs are designed to allow the model structure to depend on the output values without overly complicating the inference of the desired conditional probabilities P(DtJD1, . . . , Dt�1), the probability of the next decision given the </context>
</contexts>
<marker>Murphy, 2002</marker>
<rawString>Kevin P. Murphy. 2002. Dynamic Belief Networks: Representation, Inference and Learning. Ph.D. thesis, University of California, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford Neal</author>
</authors>
<title>Connectionist learning of belief networks.</title>
<date>1992</date>
<journal>Artificial Intelligence,</journal>
<pages>56--71</pages>
<contexts>
<context position="8646" citStr="Neal, 1992" startWordPosition="1352" endWordPosition="1353">t Variable Architecture In this section we will begin by briefly introducing the class of graphical models we will be using, Incremental Sigmoid Belief Networks (Titov and Henderson, 2007). ISBNs are designed specifically for modeling structured data. They are latent variable models which are not tractable to compute exactly, but two approximations exist which have been shown to be effective for constituent parsing (Titov and Henderson, 2007). Finally, we present how these approximations can be trained. 2.1 Incremental Sigmoid Belief Networks An ISBN is a form of Sigmoid Belief Network (SBN) (Neal, 1992). SBNs are Bayesian Networks with binary variables and conditional probability distributions in the form: P(Si = 1�Par(Si)) = Q( 1] JijSj), SjEPar(Si) where Si are the variables, Par(Si) are the variables which Si depends on (its parents), Q denotes the logistic sigmoid function, and Jij is the weight for the edge from variable Sj to variable Si in the graphical model. SBNs are similar to feed-forward neural networks, but unlike neural networks, SBNs have a precise probabilistic semantics for their hidden variables. ISBNs are based on a generalized version of SBNs where variables with any rang</context>
</contexts>
<marker>Neal, 1992</marker>
<rawString>Radford Neal. 1992. Connectionist learning of belief networks. Artificial Intelligence, 56:71–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proc. of the Eighth Conference on Computational Natural Language Learning,</booktitle>
<pages>49--56</pages>
<location>Boston, USA.</location>
<contexts>
<context position="3178" citStr="Nivre et al., 2004" startWordPosition="480" endWordPosition="483"> the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories in these models is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999; Nivre et al., 2004)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. ISBNs instead use a vector of binary 144 Proceedings of the 10th Conference on Parsing Technologies, pages 144–155, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics latent variables to encode the information about the parser history. This history vector is similar to the hidden state of a Hidden Markov Model. But unlike the graphical model for an HMM, which specifies conditional dependency edges only between adjacent states in the se</context>
<context position="4470" citStr="Nivre et al., 2004" startWordPosition="686" endWordPosition="689"> between states which are arbitrarily far apart in the parse history. The source state of such an edge is determined by the partial output structure built at the time of the destination state, thereby allowing the conditional dependency edges to be appropriate for the structural nature of the problem being modeled. This structure sensitivity is possible because ISBNs are a constrained form of switching model (Murphy, 2002), where each output decision switches the model structure used for the remaining decisions. We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al., 2004). However, instead of performing deterministic parsing as in (Nivre et al., 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. Then we propose a simple, language independent set of relations which determine how latent variable vectors are interconnected by conditional dependency edges in the ISBN model. ISBNs also condition the latent variable vectors on a set of explicit features, which we vary in the experiments. In experiments we evaluate both the performance of the ISBN dependency parser compared</context>
<context position="7551" citStr="Nivre et al., 2004" startWordPosition="1171" endWordPosition="1174">ocessing pipelines. Also, as with any generative model, it may be easy to improve the parser’s accuracy by using discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without introduction of any additional linguistic features. In addition, there are some applications, such as language modeling, which require generative models. Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al., 2004). In the remainder of this paper, we will first review general ISBNs and how they can be approximated. Then we will define the generative parsing model, based on the algorithm of (Nivre et al., 2004), and propose an ISBN for this model. The empirical part of the paper then evaluates both the overall accuracy of this method and the importance of the model’s capacity to induce features. Additional related work will be discussed in the last section before concluding. 145 2 The Latent Variable Architecture In this section we will begin by briefly introducing the class of graphical models we will b</context>
<context position="15822" citStr="Nivre et al., 2004" startWordPosition="2506" endWordPosition="2509">roximation, this can be done with the standard Backpropagation learning used with neural networks. For the mean field approximation, propagating the error all the way back through the structure of the graphical model requires a more complicated calculation, but it can still be done efficiently (see (Titov and Henderson, 2007) for details). 3 The Dependency Parsing Algorithm The sequences of decisions Dl,..., D&amp;quot;1z which we will be modeling with ISBNs are the sequences of decisions made by a dependency parser. For this we use the parsing strategy for projective dependency parsing introduced in (Nivre et al., 2004), which is similar to a standard shift-reduce algorithm for context-free grammars (Aho et al., 1986). It can be viewed as a mixture of bottom-up and top-down parsing strategies, where left dependencies are constructed in a bottom-up fashion and right dependencies are constructed top-down. For details we refer the reader to (Nivre et al., 2004). In this section we briefly describe the algorithm and explain how we use it to define our history-based probability model. In this paper, as in the CoNLL-X shared task, we consider labeled dependency parsing. The state of the parser is defined by the cu</context>
<context position="17270" citStr="Nivre et al., 2004" startWordPosition="2767" endWordPosition="2770">h an empty queue I. The algorithm uses 4 types of decisions: 1. The decision Left-Arc, adds a dependency arc from the next input word w3 to the word wi on top of the stack and selects the label r for the 147 relation between wi and wj. Word wi is then popped from the stack. 2. The decision Right-Arcr adds an arc from the word wi on top of the stack to the next input word wj and selects the label r for the relation between wi and wj. 3. The decision Reduce pops the word wi from the stack. 4. The decision Shiftwj shifts the word wj from the queue to the stack. Unlike the original definition in (Nivre et al., 2004) the Right-Arcr decision does not shift wj to the stack. However, the only thing the parser can do after a Right-Arcr decision is to choose the Shiftwj decision. This subtle modification does not change the actual parsing order, but it does simplify the definition of our graphical model, as explained in section 4. We use a history-based probability model, which decomposes the probability of the parse according to the parser decisions: P (T ) = P (D1, ..., Dm) = � P (Dt|D1,...,Dt−1), t where T is the parse tree and D1, ... , Dm is its equivalent sequence of parser decisions. Since we need a gen</context>
<context position="25986" citStr="Nivre et al., 2004" startWordPosition="4299" endWordPosition="4302">y of the relationships. As indicated in figure 1, the probability of each elementary decision dt0k depends both on the current state vector 5t0 and on the previously chosen elementary action dt0k−1 from Dt0. This probability distribution has the form of a normalized exponential: , &apos;bh(t0 k) (d) ey-j Wdjst0 j P(dt0k = d|5t , dtk−1) = t0 Ed0kh(t0,k) (dl) eEj Wd0jsj where 4bh(t0,k) is the indicator function of the set of elementary decisions that may possibly follow the last decision in the history h(t&apos;, k), and the Wdj are the weights. Now it is easy to see why the original decision Right-Arcr (Nivre et al., 2004) had to be decomposed into two distinct decisions: the decision to construct a labeled arc and the decision to shift the word. Use of this composite Right-Arcr would have required the introduction of individual parameters for each pair (w, r), where w is an arbitrary word in the lexicon and r - an arbitrary dependency relation. 5 Searching for the Best Tree ISBNs define a probability model which does not make any a-priori assumptions of independence between any decision variables. As we discussed in section 4 use of relations based on partial output structure makes it possible to take into acc</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proc. of the Eighth Conference on Computational Natural Language Learning, pages 49–56, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Gulsen Eryigit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proc. of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<pages>221--225</pages>
<location>New York, USA.</location>
<contexts>
<context position="1475" citStr="Nivre et al., 2006" startWordPosition="213" endWordPosition="216">ndency parsing has been a topic of active research in natural language processing during the last several years. The CoNLL-X shared task (Buchholz and Marsi, 2006) made a wide selection of standardized treebanks for different languages available for the research community and allowed for easy comparison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unlike constituent parsing where all the state of the art methods have a generative component (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000). Another surprising thing is the lack of latent variable models among the methods used in the shared task. Latent variable models would allow complex features to be induced automatically, which would be highly desirable in multilingual parsing, where manual feature selection might be very difficult and t</context>
<context position="5294" citStr="Nivre et al., 2006" startWordPosition="816" endWordPosition="819"> parser actions. Then we propose a simple, language independent set of relations which determine how latent variable vectors are interconnected by conditional dependency edges in the ISBN model. ISBNs also condition the latent variable vectors on a set of explicit features, which we vary in the experiments. In experiments we evaluate both the performance of the ISBN dependency parser compared to previous work, and the ability of the ISBN model to induce complex history features. Our model achieves state-of-the-art performance on the languages we test, significantly outperforming the model of (Nivre et al., 2006) on two languages out of three and demonstrating about the same results on the third. In order to test the model’s feature induction abilities, we train models with two different sets of explicit conditioning features: the feature set individually tuned by (Nivre et al., 2006) for each considered language, and a minimal set of local features. These models achieve comparable accuracy, unlike with the discriminative SVM-based approach of (Nivre et al., 2006), where careful feature selection appears to be crucial. We also conduct a controlled experiment where we used the tuned features of (Nivre </context>
<context position="6516" citStr="Nivre et al., 2006" startWordPosition="1010" endWordPosition="1013">al., 2006) but disable the feature induction abilities of our model by elimination of the edges connecting latent state vectors. This restricted model achieves far worse results, showing that it is exactly the capacity of ISBNs to induce history features which is the key to its success. It also motivates further research into how feature induction techniques can be exploited in discriminative parsing methods. We analyze how the relation accuracy changes with the length of the head-dependent relation, demonstrating that our model very significantly outperforms the state-of-the-art baseline of (Nivre et al., 2006) on long dependencies. Additional experiments suggest that both feature induction abilities and use of the beam search contribute to this improvement. The fact that our model defines a probability model over parse trees, unlike the previous state-ofthe-art methods (Nivre et al., 2006; McDonald et al., 2006), makes it easier to use this model in applications which require probability estimates, e.g. in language processing pipelines. Also, as with any generative model, it may be easy to improve the parser’s accuracy by using discriminative retraining techniques (Henderson, 2004) or data-defined </context>
<context position="26919" citStr="Nivre et al., 2006" startWordPosition="4452" endWordPosition="4455">dency relation. 5 Searching for the Best Tree ISBNs define a probability model which does not make any a-priori assumptions of independence between any decision variables. As we discussed in section 4 use of relations based on partial output structure makes it possible to take into account statistical interdependencies between decisions closely related in the output structure, but separated by multiple decisions in the input structure. This property leads to exponential complexity of complete search. However, the success of the deterministic parsing strategy which uses the same parsing order (Nivre et al., 2006), suggests that it should be relatively easy to find an accurate approximation to the best parse with heuristic search methods. Unlike (Nivre et al., 2006), we can not use a lookahead in our generative model, as was discussed in section 3, so a greedy method is unlikely to lead to a good approximation. Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order. We apply fixed beam pruning after each decision Shiftwj, because knowledge of the next word in the</context>
<context position="28594" citStr="Nivre et al., 2006" startWordPosition="4727" endWordPosition="4730">s with a variety of post-shift beam widths confirmed that very small validation performance gains are achieved with widths larger than 30, and sometimes even a beam of 5 was sufficient. We found also that allowing 5 different relations after each dependency prediction operation was enough that it had virtually no effect on the validation accuracy. 6 Empirical Evaluation In this section we evaluate the ISBN model for dependency parsing on three treebanks from the CoNLL-X shared task. We compare our generative models with the best parsers from the CoNLLX task, including the SVM-based parser of (Nivre et al., 2006) (the MALT parser), which uses the same parsing algorithm. To test the feature induction abilities of our model we compare results with two feature sets, the feature set tuned individually for each language by (Nivre et al., 2006), and another feature set which includes only obvious local features. This simple feature set comprises only features of the word on top of the stack 5 and the front word of the queue I. We compare the gain from using tuned features with the similar gain obtained by the MALT parser. To obtain these results we train the MALT parser with the same two feature sets.3 In o</context>
<context position="31521" citStr="Nivre et al., 2006" startWordPosition="5210" endWordPosition="5213">part-of-speech tags and coarse part-of-speech tags, word form, word lemma (lemma information was not available for Danish) and a set of fine-grain word features. As we explained in section 3, we treated these sets of finegrain features as an atomic value when predicting a word. However, when conditioning on words, we treated each component of this composite feature individually, as it proved to be useful on the development set. We used frequency cutoffs: we ignored any property (e.g., word form, feature or even partof-speech tag4) which occurs in the training set less than 5 times. Following (Nivre et al., 2006), we used pseudo-projective transformation they proposed to cast non-projective parsing tasks as projective. ISBN models were trained using a small development set taken out from the training set, which was used for tuning learning parameters and for 4Part-of-speech tags for multi-word units in the Danish treebank were formed as concatenation of tags of the words, which led to quite sparse set of part-of-speech tags. early stopping. The sizes of the development sets were: 4,988 tokens for larger Dutch corpus, 2,504 tokens for Danish and 2,033 tokens for Slovene. The MALT parser was trained alw</context>
<context position="33231" citStr="Nivre et al., 2006" startWordPosition="5489" endWordPosition="5492">he feed-forward approximation for the larger one. Training the mean field approximations on the larger Dutch treebank is feasible, but would significantly reduce the possibilities for tuning the learning parameters on the development set and, thus, would increase the randomness of model comparisons. All model selection was performed on the development set and a single model of each type was applied to the testing set. We used a state variable vector consisting of 80 binary variables, as it proved sufficient on the preliminary experiments. For the MALT parser we replicated the parameters from (Nivre et al., 2006) as described in detail on their web site. The labeled attachment scores for the ISBN with tuned features (TF) and local features (LF) and ISBN with tuned features and no edges connecting latent variable vectors (TF-NA) are presented in table 1, along with results for the MALT parser both with tuned and local feature, the MST parser (McDonald et al., 2006), and the average score (Aver) across all systems in the CoNLL-X shared task. The MST parser is included because it demonstrated the best overall result in the task, non significantly outperforming the MALT parser, which, in turn, achieved th</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Eryigit, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit, and Svetoslav Marinov. 2006. Pseudo-projective dependency parsing with support vector machines. In Proc. of the Tenth Conference on Computational Natural Language Learning, pages 221–225, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Peshkin</author>
<author>Virginia Savova</author>
</authors>
<title>Dependency parsing with dynamic Bayesian network.</title>
<date>2005</date>
<booktitle>In AAAI, 20th National Conference on Artificial Intelligence,</booktitle>
<location>Pittsburgh, Pennsylvania.</location>
<contexts>
<context position="40992" citStr="Peshkin and Savova, 2005" startWordPosition="6802" endWordPosition="6805">e ISBN model to induce features is a major factor in improving accuracy of long dependencies. 6The MALT parser is trained to keep the word as long as possible: ifboth Shift and Reduce decisions are possible during training, it always prefers to shift. Though this strategy should generally reduce the described problem, it is evident from the low precision score for attachment to root, that it can not completely eliminate it. 7 Related Work There has not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-</context>
</contexts>
<marker>Peshkin, Savova, 2005</marker>
<rawString>Leon Peshkin and Virginia Savova. 2005. Dependency parsing with dynamic Bayesian network. In AAAI, 20th National Conference on Artificial Intelligence, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detlef Prescher</author>
</authors>
<title>Head-driven PCFGs with latenthead statistics.</title>
<date>2005</date>
<booktitle>In Proc. 9th Int. Workshop on Parsing Technologies,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="41415" citStr="Prescher, 2005" startWordPosition="6874" endWordPosition="6875">ted Work There has not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-driven PCFG with latent heads, thus restricting the flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking uses dependency</context>
</contexts>
<marker>Prescher, 2005</marker>
<rawString>Detlef Prescher. 2005. Head-driven PCFGs with latenthead statistics. In Proc. 9th Int. Workshop on Parsing Technologies, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proc. 40th Meeting ofAssociation for Computational Linguistics,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="41438" citStr="Riezler et al., 2002" startWordPosition="6876" endWordPosition="6879">as not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-driven PCFG with latent heads, thus restricting the flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking uses dependency relations to define th</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proc. 40th Meeting ofAssociation for Computational Linguistics, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Sallans</author>
</authors>
<title>Reinforcement Learning for Factored Markov Decision Processes.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Toronto,</institution>
<location>Toronto, Canada.</location>
<contexts>
<context position="42717" citStr="Sallans, 2002" startWordPosition="7076" endWordPosition="7077">anking dependency trees. Undirected graphical models, in particular Conditional Random Fields, are the standard tools for shallow parsing (Sha and Pereira, 2003). However, shallow parsing is effectively a sequence labeling problem and therefore differs significantly from full parsing. As discussed in (Titov and Henderson, 2007), undirected graphical models do not seem to be suitable for history-based parsing models. Sigmoid Belief Networks (SBNs) were used originally for character recognition tasks, but later a dynamic modification of this model was applied to the reinforcement learning task (Sallans, 2002). However, their graphical model, approximation method, and learning method differ significantly from those of this paper. The extension of dynamic SBNs with incrementally specified model structure (i.e. Incremental Sigmoid Belief Networks, used in this paper) was proposed and applied to constituent parsing in (Titov and Henderson, 2007). 153 8 Conclusions We proposed a latent variable dependency parsing model based on Incremental Sigmoid Belief Networks. Unlike state-of-the-art dependency parsers, it uses a generative history-based model. We demonstrated that it achieves state-of-the-art resu</context>
</contexts>
<marker>Sallans, 2002</marker>
<rawString>Brian Sallans. 2002. Reinforcement Learning for Factored Markov Decision Processes. Ph.D. thesis, University of Toronto, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. joint meeting of North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conf.,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="42264" citStr="Sha and Pereira, 2003" startWordPosition="7004" endWordPosition="7007">e flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking uses dependency relations to define the edges. Thus, it should be easy to apply a similar method to reranking dependency trees. Undirected graphical models, in particular Conditional Random Fields, are the standard tools for shallow parsing (Sha and Pereira, 2003). However, shallow parsing is effectively a sequence labeling problem and therefore differs significantly from full parsing. As discussed in (Titov and Henderson, 2007), undirected graphical models do not seem to be suitable for history-based parsing models. Sigmoid Belief Networks (SBNs) were used originally for character recognition tasks, but later a dynamic modification of this model was applied to the reinforcement learning task (Sallans, 2002). However, their graphical model, approximation method, and learning method differ significantly from those of this paper. The extension of dynamic</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proc. joint meeting of North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conf., Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Constituent parsing with incremental sigmoid belief networks.</title>
<date>2007</date>
<booktitle>In Proc. 45th Meeting of Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2410" citStr="Titov and Henderson, 2007" startWordPosition="360" endWordPosition="363">ising thing is the lack of latent variable models among the methods used in the shared task. Latent variable models would allow complex features to be induced automatically, which would be highly desirable in multilingual parsing, where manual feature selection might be very difficult and time consuming, especially for languages unknown to the parser developer. In this paper we propose a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in (Titov and Henderson, 2007), where they were demonstrated to achieve competitive results on the constituent parsing task. As discussed in (Titov and Henderson, 2007), computing the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories </context>
<context position="8223" citStr="Titov and Henderson, 2007" startWordPosition="1285" endWordPosition="1288">t review general ISBNs and how they can be approximated. Then we will define the generative parsing model, based on the algorithm of (Nivre et al., 2004), and propose an ISBN for this model. The empirical part of the paper then evaluates both the overall accuracy of this method and the importance of the model’s capacity to induce features. Additional related work will be discussed in the last section before concluding. 145 2 The Latent Variable Architecture In this section we will begin by briefly introducing the class of graphical models we will be using, Incremental Sigmoid Belief Networks (Titov and Henderson, 2007). ISBNs are designed specifically for modeling structured data. They are latent variable models which are not tractable to compute exactly, but two approximations exist which have been shown to be effective for constituent parsing (Titov and Henderson, 2007). Finally, we present how these approximations can be trained. 2.1 Incremental Sigmoid Belief Networks An ISBN is a form of Sigmoid Belief Network (SBN) (Neal, 1992). SBNs are Bayesian Networks with binary variables and conditional probability distributions in the form: P(Si = 1�Par(Si)) = Q( 1] JijSj), SjEPar(Si) where Si are the variables</context>
<context position="9972" citStr="Titov and Henderson, 2007" startWordPosition="1563" endWordPosition="1566"> the conditional probability distributions at these nodes. To extend SBNs for processing arbitrarily long sequences, such as a parser’s sequence of decisions D1,..., Dm, SBNs are extended to a form of Dynamic Bayesian Network (DBN). In DBNs, a new set of variables is instantiated for each position in the sequence, but the edges and weights are the same for each position in the sequence. The edges which connect variables instantiated for different positions must be directed forward in the sequence, thereby allowing a temporal interpretation of the sequence. Incremental Sigmoid Belief Networks (Titov and Henderson, 2007) differ from simple dynamic SBNs in that they allow the model structure to depend on the output variable values. Specifically, a decision is allowed to effect the placement of any edge whose destination is after the decision. This results in a form of switching model (Murphy, 2002), where each decision switches the model structure used for the remaining decisions. The incoming edges for a given position are a discrete function of the sequence of decisions which precede that position. This makes the ISBN an “incremental” model, not just a dynamic model. The structure of the model is determined </context>
<context position="12511" citStr="Titov and Henderson, 2007" startWordPosition="1973" endWordPosition="1976">cal to the dependent wj even if they are far apart in the sentence. This structurally-defined notion of locality is particularly important for the model’s latent variables. When the structurally-defined model edges connect latent variables, information can be propagated between latent variables, thereby providing an even larger structural domain of locality than that provided by single edges. This provides a potentially powerful form of feature induction, which is nonetheless biased toward a notion of locality which is appropriate for the structure of the problem. 146 2.3 Approximating ISBNs (Titov and Henderson, 2007) proposes two approximations for inference in ISBNs, both based on variational methods. The main idea of variational methods (Jordan et al., 1999) is, roughly, to construct a tractable approximate model with a number of free parameters. The values of the free parameters are set so that the resulting approximate model is as close as possible to the original graphical model for a given inference problem. The simplest example of a variation method is the mean field method, which uses a fully factorized distribution Q(HIV) = ni Qi(hi|V ) as the approximate model, where V are the visible (i.e. know</context>
<context position="14039" citStr="Titov and Henderson, 2007" startWordPosition="2232" endWordPosition="2235">ariational approach. First, they show that the neural network of (Henderson, 2003) can be viewed as a coarse mean field approximation of ISBNs, which they call the feedforward approximation. This approximation imposes the constraint that the free parameters µi of the approximate model are only allowed to depend on the distributions of their parent variables. This constraint increases the potential for a large approximation error, but it significantly simplifies the computations by allowing all the free parameters to be set in a single pass over the model. The second approximation proposed in (Titov and Henderson, 2007) takes into consideration the fact that, after each decision is made, all the preceding latent variables should have their means µi updated. This approximation extends the feed-forward approximation to account for the most important components of this update. They call this approximation the mean field approximation, because a mean field approximation is applied to handle the statistical dependencies introduced by the new decisions. This approximation was shown to be a more accurate approximation of ISBNs than the feed-forward approximation, but remain tractable. It was also shown to achieve s</context>
<context position="15530" citStr="Titov and Henderson, 2007" startWordPosition="2458" endWordPosition="2462">gularization is applied, which is equivalent to the weight decay standardly used in neural networks. Regularization was reduced through the course of learning. Gradient descent requires computing the derivatives of the objective function with respect to the model parameters. In the feed-forward approximation, this can be done with the standard Backpropagation learning used with neural networks. For the mean field approximation, propagating the error all the way back through the structure of the graphical model requires a more complicated calculation, but it can still be done efficiently (see (Titov and Henderson, 2007) for details). 3 The Dependency Parsing Algorithm The sequences of decisions Dl,..., D&amp;quot;1z which we will be modeling with ISBNs are the sequences of decisions made by a dependency parser. For this we use the parsing strategy for projective dependency parsing introduced in (Nivre et al., 2004), which is similar to a standard shift-reduce algorithm for context-free grammars (Aho et al., 1986). It can be viewed as a mixture of bottom-up and top-down parsing strategies, where left dependencies are constructed in a bottom-up fashion and right dependencies are constructed top-down. For details we ref</context>
<context position="32384" citStr="Titov and Henderson, 2007" startWordPosition="5347" endWordPosition="5350">eters and for 4Part-of-speech tags for multi-word units in the Danish treebank were formed as concatenation of tags of the words, which led to quite sparse set of part-of-speech tags. early stopping. The sizes of the development sets were: 4,988 tokens for larger Dutch corpus, 2,504 tokens for Danish and 2,033 tokens for Slovene. The MALT parser was trained always using the entire training set. We expect that the mean field approximation should demonstrate better results than feed-forward approximation on this task as it is theoretically expected and confirmed on the constituent parsing task (Titov and Henderson, 2007). However, the sizes of testing sets would not allow us to perform any conclusive analysis, so we decided not to perform these comparisons here. Instead we used the mean field approximation for the smaller two corpora and used the feed-forward approximation for the larger one. Training the mean field approximations on the larger Dutch treebank is feasible, but would significantly reduce the possibilities for tuning the learning parameters on the development set and, thus, would increase the randomness of model comparisons. All model selection was performed on the development set and a single m</context>
<context position="42432" citStr="Titov and Henderson, 2007" startWordPosition="7030" endWordPosition="7033">rained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking uses dependency relations to define the edges. Thus, it should be easy to apply a similar method to reranking dependency trees. Undirected graphical models, in particular Conditional Random Fields, are the standard tools for shallow parsing (Sha and Pereira, 2003). However, shallow parsing is effectively a sequence labeling problem and therefore differs significantly from full parsing. As discussed in (Titov and Henderson, 2007), undirected graphical models do not seem to be suitable for history-based parsing models. Sigmoid Belief Networks (SBNs) were used originally for character recognition tasks, but later a dynamic modification of this model was applied to the reinforcement learning task (Sallans, 2002). However, their graphical model, approximation method, and learning method differ significantly from those of this paper. The extension of dynamic SBNs with incrementally specified model structure (i.e. Incremental Sigmoid Belief Networks, used in this paper) was proposed and applied to constituent parsing in (Ti</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Ivan Titov and James Henderson. 2007. Constituent parsing with incremental sigmoid belief networks. In Proc. 45th Meeting of Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L van der Beek</author>
<author>G Bouma</author>
<author>J Daciuk</author>
<author>T Gaustad</author>
<author>R Malouf</author>
<author>G van Noord</author>
<author>R Prins</author>
<author>B Villada</author>
</authors>
<title>The Alpino dependency treebank.</title>
<date>2002</date>
<booktitle>Computational Linguistic in the Netherlands (CLIN).</booktitle>
<marker>van der Beek, Bouma, Daciuk, Gaustad, Malouf, van Noord, Prins, Villada, 2002</marker>
<rawString>L. van der Beek, G. Bouma, J. Daciuk, T. Gaustad, R. Malouf, G van Noord, R. Prins, and B. Villada. 2002. The Alpino dependency treebank. Computational Linguistic in the Netherlands (CLIN).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>