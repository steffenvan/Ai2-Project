<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9968455">
Inducing a Semantically Annotated Lexicon
via EM-Based Clustering
</title>
<author confidence="0.6147304">
Mats Rooth
Stefan Riezler
Detlef Prescher
Glenn Carroll
Franz Beil
</author>
<affiliation confidence="0.986509">
Institut fiir Maschinelle Sprachverarbeitung
University of Stuttgart, Germany
</affiliation>
<sectionHeader confidence="0.980148" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999765083333333">
We present a technique for automatic induction
of slot annotations for subcategorization frames,
based on induction of hidden classes in the EM
framework of statistical estimation. The models
are empirically evalutated by a general decision
test. Induction of slot labeling for subcategoriza-
tion frames is accomplished by a further applica-
tion of EM, and applied experimentally on frame
observations derived from parsing large corpora.
We outline an interpretation of the learned rep-
resentations as theoretical-linguistic decomposi-
tional lexical entries.
</bodyText>
<sectionHeader confidence="0.998431" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978666666667">
An important challenge in computational lin-
guistics concerns the construction of large-scale
computational lexicons for the numerous natu-
ral languages where very large samples of lan-
guage use are now available. Resnik (1993) ini-
tiated research into the automatic acquisition
of semantic selectional restrictions. Ribas (1994)
presented an approach which takes into account
the syntactic position of the elements whose se-
mantic relation is to be acquired. However, those
and most of the following approaches require as
a prerequisite a fixed taxonomy of semantic rela-
tions. This is a problem because (i) entailment
hierarchies are presently available for few lan-
guages, and (ii) we regard it as an open ques-
tion whether and to what degree existing designs
for lexical hierarchies are appropriate for repre-
senting lexical meaning. Both of these consid-
erations suggest the relevance of inductive and
experimental approaches to the construction of
lexicons with semantic information.
This paper presents a method for automatic
induction of semantically annotated subcatego-
rization frames from unannotated corpora. We
use a statistical subcat-induction system which
estimates probability distributions and corpus
frequencies for pairs of a head and a subcat
frame (Carroll and Rooth, 1998). The statistical
parser can also collect frequencies for the nomi-
nal fillers of slots in a subcat frame. The induc-
tion of labels for slots in a frame is based upon
estimation of a probability distribution over tu-
ples consisting of a class label, a selecting head,
a grammatical relation, and a filler head. The
class label is treated as hidden data in the EM-
framework for statistical estimation.
</bodyText>
<sectionHeader confidence="0.990231" genericHeader="method">
2 EM-Based Clustering
</sectionHeader>
<bodyText confidence="0.999991481481481">
In our clustering approach, classes are derived
directly from distributional data—a sample of
pairs of verbs and nouns, gathered by pars-
ing an unannotated corpus and extracting the
fillers of grammatical relations. Semantic classes
corresponding to such pairs are viewed as hid-
den variables or unobserved data in the context
of maximum likelihood estimation from incom-
plete data via the EM algorithm. This approach
allows us to work in a mathematically well-
defined framework of statistical inference, i.e.,
standard monotonicity and convergence results
for the EM algorithm extend to our method.
The two main tasks of EM-based clustering are
i) the induction of a smooth probability model
on the data, and ii) the automatic discovery of
class-structure in the data. Both of these aspects
are respected in our application of lexicon in-
duction. The basic ideas of our EM-based clus-
tering approach were presented in Rooth (Ms).
Our approach constrasts with the merely heuris-
tic and empirical justification of similarity-based
approaches to clustering (Dagan et al., to ap-
pear) for which so far no clear probabilistic
interpretation has been given. The probability
model we use can be found earlier in Pereira
et al. (1993). However, in contrast to this ap-
</bodyText>
<page confidence="0.997175">
104
</page>
<table confidence="0.971026">
Class 17 0000 00 0 itl. CO 0 0 in 00 0 . 05 CO ni t- 01 . . 0 00 1.-- ■0 in 00 I-- t- V&apos; . o
PROS 0.0265 O 0 O O 0 n n n 0 n n n n
010. .0000 000 000 0 0 00000 0 00
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
do 0 ci c; 0 0 c; 0 0 0 0 0 O 0 0 0 0 0 0 c; c; c; c; 0 0 c; 0 do
0
u
u c c g
-o ea &apos;13 0
&apos; ai
ill . 0 E .....
tot...zg t .,-g.-s.e. ,-.F&amp;quot;,§151, -.., a. ,, -.
0-,..0&gt;E7.,..0. 41,9&apos;1..&apos;ut01.52Eg.E00&apos;34°
0.0437 increase.as:s
0.0392 increase.aso:o
0.0344 fall.as:s 00000000 • • • • • • • • • • • • • • •
0.0337 pay.aso:o • • • • • • • • • • • • • • a
0.0329 reduce.aso:o
0.0257 rise.as:s
0.0196 exceed.aso:o
• • • • • • 00000
0.0177 exceed.aso:s
0.0169 affect.asom oo o : •
0.0156 grow.as:s • • • • • 00000 ••• • • •
0.0134 include.aso:s
0.0129 reach.aso:s
• • • • • • • •
0.0120 decline.as:s 00000 • • • • • • • • • • • • • •
0.0102 lose.aso:o • • • • • • • 0000000 • • • • • • • • •
0.0099 act.aso:s • • a • • • • •
0.0099 improve.aso:o • • • • • • • • • • • •
0.0088 include.aso:o oo
0.0088 cut.aso:o 00000 • • • • • • • • • • • •
0.0080 show.aso:o
• • •
0.0078 vary.as:s 000000 • • • • • • • • • •
</table>
<figureCaption confidence="0.998326">
Figure 1: Class 17: scalar change
</figureCaption>
<bodyText confidence="0.999823055555555">
proach, our statistical inference method for clus-
tering is formalized clearly as an EM-algorithm.
Approaches to probabilistic clustering similar to
ours were presented recently in Saul and Pereira
(1997) and Hofmann and Puzicha (1998). There
also EM-algorithms for similar probability mod-
els have been derived, but applied only to sim-
pler tasks not involving a combination of EM-
based clustering models as in our lexicon induc-
tion experiment. For further applications of our
clustering model see Rooth et al. (1998).
We seek to derive a joint distribution of verb-
noun pairs from a large sample of pairs of verbs
v E V and nouns n E N. The key idea is to view
v and n as conditioned on a hidden class c e C,
where the classes are given no prior interpreta-
tion. The semantically smoothed probability of
a pair (v, n) is defined to be:
</bodyText>
<equation confidence="0.6723995">
p(v, n) = p(c, v, n) = Ep(c)p(vic)p(nic)
cEC cEC
</equation>
<bodyText confidence="0.970150916666666">
The joint distribution p(c, v, n) is defined by
p(c, v , n) = p(c)p(vic)p(nic). Note that by con-
struction, conditioning of v and n on each other
is solely made through the classes c.
In the framework of the EM algorithm
(Dempster et al., 1977), we can formalize clus-
tering as an estimation problem for a latent class
(LC) model as follows. We are given: (i) a sam-
ple space)) of observed, incomplete data, corre-
sponding to pairs from V x N, (ii) a sample space
X of unobserved, complete data, corresponding
to triples from Cx V x N, (iii) a set X(y) = {x E
X I x = (c, y), c E CI of complete data related
to the observation y, (iv) a complete-data speci-
fication po(x), corresponding to the joint proba-
bility p(c, v, n) over Cx V x N, with parameter-
vector 0 = (Oc,Ov,OncIc E C, v E V, n E N), (v)
an incomplete data specification P0(Y) which is
related to the complete-data specification as the
marginal probability P0 (Y) = Ex(y)po(x). &apos;
The EM algorithm is directed at finding a
value a of 0 that maximizes the incomplete-
data log-likelihood function L as a func-
tion of 0 for a given sample )), i.e., a =
arg max L(0) where L(0) = ln Fly po (y).
0
As prescribed by the EM algorithm, the pa-
rameters of L(0) are estimated indirectly by pro-
ceeding iteratively in terms of complete-data es-
timation for the auxiliary function Q(0; 9(0),
which is the conditional expectation of the
complete-data log-likelihood lnpo(x) given the
observed data y and the current fit of the pa-
rameter values 0(0 (E-step). This auxiliary func-
tion is iteratively maximized as a function of
0 (M-step), where each iteration is defined by
</bodyText>
<equation confidence="0.8611325">
the map 0(t+1) = M(0(t) = arg max Q(0; 0(0)
0
</equation>
<bodyText confidence="0.9822005">
Note that our application is an instance of the
EM-algorithm for context-free models (Baum et
</bodyText>
<page confidence="0.995289">
105
</page>
<table confidence="0.75660735483871">
Class 5 .0 •.t. 01 00 ..t r. ..?&apos; 0 00 t- 0 .-. 00 0000 00 .0 .0 t- 0 0 0 et CO CI 01 01
PROS 0.0412 •o■ oo co t- t- t- us .1 cr ce. ..t.d. ct•tt, 00 00 01010101.0 0101.0 00.1000101.0
8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8
6 6 6 6 6 6 6 6 6 6 . 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
0 0
-al o&gt;&apos; :.5
F
E 2 8-8 .4 t; 2 tl &apos;8 -&apos;.b:o d &gt;&apos; .9, A g a..2 v.
0.0542 ask.as:s
0.0340 nod.as:s • •
0.0299 think.as:s
0.0287 shake.aso:s
0.0264 smile.as:s
0.0213 laugh.as:s
0.0207 reply.as:s • • •
0.0167 shrug.as:s s S •
0.0148 wonder.as:s
• • •
0.0141 feel.aso:s e • • •
0.0133 take.aso:s
0.0121 sigh.as:s 000000 • • • • • •• • OOOOOO •••
0.0110 watch.aso:s • • • 0000000 • 0000000 • • • • •
0.0106 ask.aso:s • • • • •
0.0104 tell.aso:s
0.0094 look.as:s • • S • •
0.0092 give.aso:s • • • •
0.0089 hear.aso:s • • • • • • •
•
0.0083 grin.as:s 00000000 • • • • • • •
0.0083 answer.as:s 00000000 • • • • . .
9 .
</table>
<figureCaption confidence="0.996044">
Figure 2: Class 5: communicative action
</figureCaption>
<bodyText confidence="0.994384">
al., 1970), from which the following particular-
ily simple reestimation formulae can be derived.
Let x = (c, y) for fixed c and y. Then
probabilistic context-free grammar of (Carroll
and Rooth, 1998) gave for the British National
Corpus (117 million words).
</bodyText>
<equation confidence="0.896372">
M(Ovc) EyE{v}xNP0(x1Y)
M(Onc) EyPo(xly) &apos;
M(0) EyEVx{n}Pe(XIY)
EyP9(xly) &apos;
EyPe(x1Y)
IYI
</equation>
<bodyText confidence="0.999826157894737">
Intuitively, the conditional expectation of the
number of times a particular v, n, or c choice
is made during the derivation is prorated by the
conditionally expected total number of times a
choice of the same kind is made. As shown by
Baum et al. (1970), these expectations can be
calculated efficiently using dynamic program-
ming techniques. Every such maximization step
increases the log-likelihood function L, and a se-
quence of re-estimates eventually converges to a
(local) maximum of L.
In the following, we will present some exam-
ples of induced clusters. Input to the clustering
algorithm was a training corpus of 1280715 to-
kens (608850 types) of verb-noun pairs partici-
pating in the grammatical relations of intransi-
tive and transitive verbs and their subject- and
object-fillers. The data were gathered from the
maximal-probability parses the head-lexicalized
</bodyText>
<figureCaption confidence="0.999626">
Figure 3: Evaluation of pseudo-disambiguation
</figureCaption>
<bodyText confidence="0.999313375">
Fig. 2 shows an induced semantic class out of
a model with 35 classes. At the top are listed the
20 most probable nouns in the p(n5) distribu-
tion and their probabilities, and at left are the 30
most probable verbs in the p(v15) distribution. 5
is the class index. Those verb-noun pairs which
were seen in the training data appear with a dot
in the class matrix. Verbs with suffix .as : s in-
dicate the subject slot of an active intransitive.
Similarily .aso: s denotes the subject slot of an
active transitive, and .aso: o denotes the object
slot of an active transitive. Thus v in the above
discussion actually consists of a combination of
a verb with a subcat frame slot as : s, aso : s,
or aso : o. Induced classes often have a basis
in lexical semantics; class 5 can be interpreted
</bodyText>
<page confidence="0.926141">
106
</page>
<bodyText confidence="0.898438888888889">
as clustering agents, denoted by proper names,
&amp;quot;man&amp;quot;, and &amp;quot;woman&amp;quot;, together with verbs denot-
ing communicative action. Fig. 1 shows a clus-
ter involving verbs of scalar change and things
which can move along scales. Fig. 5 can be in-
terpreted as involving different dispositions and
modes of their execution.
60 100 160 200 260 300
nix*. el cleeee•
</bodyText>
<sectionHeader confidence="0.776322" genericHeader="method">
3 Evaluation of Clustering Models
</sectionHeader>
<subsectionHeader confidence="0.999594">
3.1 Pseudo-Disambiguation
</subsectionHeader>
<bodyText confidence="0.999845024390244">
We evaluated our clustering models on a pseudo-
disambiguation task similar to that performed
in Pereira et al. (1993), but differing in detail.
The task is to judge which of two verbs v and
v&apos; is more likely to take a given noun n as its
argument where the pair (v, n) has been cut out
of the original corpus and the pair (v&apos;, n) is con-
structed by pairing 71 with a randomly chosen
verb v&apos; such that the combination (v&apos;, n) is com-
pletely unseen. Thus this test evaluates how well
the models generalize over unseen verbs.
The data for this test were built as follows.
We constructed an evaluation corpus of (v, n, v&apos;)
triples by randomly cutting a test corpus of 3000
(v, n) pairs out of the original corpus of 1280712
tokens, leaving a training corpus of 1178698 to-
kens. Each noun n in the test corpus was com-
bined with a verb v&apos; which was randomly cho-
sen according to its frequency such that the pair
(v&apos;, n) did appear neither in the training nor in
the test corpus. However, the elements v, v&apos;, and
n were required to be part of the training corpus.
Furthermore, we restricted the verbs and nouns
in the evalutation corpus to the ones which oc-
cured at least 30 times and at most 3000 times
with some verb-functor v in the training cor-
pus. The resulting 1337 evaluation triples were
used to evaluate a sequence of clustering models
trained from the training corpus.
The clustering models we evaluated were
parametrized in starting values of the training
algorithm, in the number of classes of the model,
and in the number of iteration steps, resulting
in a sequence of 3 x 10 x 6 models. Starting
from a lower bound of 50 % random choice, ac-
curacy was calculated as the number of times
the model decided for p(nlv) &gt; p(n1v1) out of all
choices made. Fig. 3 shows the evaluation results
for models trained with 50 iterations, averaged
over starting values, and plotted against class
cardinality. Different starting values had an ef-
</bodyText>
<figureCaption confidence="0.999026">
Figure 4: Evaluation on smoothing task
</figureCaption>
<bodyText confidence="0.9997538">
fect of 2 % on the performance of the test.
We obtained a value of about 80 % accuracy for
models between 25 and 100 classes. Models with
more than 100 classes show a small but stable
overfitting effect.
</bodyText>
<subsectionHeader confidence="0.999849">
3.2 Smoothing Power
</subsectionHeader>
<bodyText confidence="0.999996153846154">
A second experiment addressed the smoothing
power of the model by counting the number of
(v, n) pairs in the set V xN of all possible combi-
nations of verbs and nouns which received a pos-
itive joint probability by the model. The V x N-
space for the above clustering models included
about 425 million (v, n) combinations; we ap-
proximated the smoothing size of a model by
randomly sampling 1000 pairs from V x N and
returning the percentage of positively assigned
pairs in the random sample. Fig. 4 plots the
smoothing results for the above models against
the number of classes. Starting values had an in-
fluence of ± 1 % on performance. Given the pro-
portion of the number of types in the training
corpus to the V x N-space, without clustering
we have a smoothing power of 0.14 % whereas
for example a model with 50 classes and 50 it-
erations has a smoothing power of about 93 %.
Corresponding to the maximum likelihood
paradigm, the number of training iterations had
a decreasing effect on the smoothing perfor-
mance whereas the accuracy of the pseudo-
disambiguation was increasing in the number of
iterations. We found a number of 50 iterations
to be a good compromise in this trade-off.
</bodyText>
<sectionHeader confidence="0.9423315" genericHeader="method">
4 Lexicon Induction Based on
Latent Classes
</sectionHeader>
<bodyText confidence="0.99441525">
The goal of the following experiment was to de-
rive a lexicon of several hundred intransitive and
transitive verbs with subcat slots labeled with
latent classes.
</bodyText>
<page confidence="0.993689">
107
</page>
<subsectionHeader confidence="0.997998">
4.1 Probabilistic Labeling with Latent
Classes using EM-estimation
</subsectionHeader>
<bodyText confidence="0.999973714285714">
To induce latent classes for the subject slot of
a fixed intransitive verb the following statisti-
cal inference step was performed. Given a la-
tent class model pLc(.) for verb-noun pairs, and
a sample n1, , nm of subjects for a fixed in-
transitive verb, we calculate the probability of
an arbitrary subject n E N by:
</bodyText>
<equation confidence="0.9962715">
p(n) = p(c,n) = EpcopLc(nle)-
cEC cEC
</equation>
<bodyText confidence="0.999940125">
The estimation of the parameter-vector 0 =
(Ocic e C) can be formalized in the EM frame-
work by viewing p(n) or p(c, n) as a function of
0 for fixed pLc(.). The re-estimation formulae
resulting from the incomplete data estimation
for these probability functions have the follow-
ing form (1(n) is the frequency of n in the sam-
ple of subjects of the fixed verb):
</bodyText>
<equation confidence="0.9874635">
EnEN f (n)po(cln)
EnEN (n)
</equation>
<bodyText confidence="0.999450375">
A similar EM induction process can be applied
also to pairs of nouns, thus enabling induction of
latent semantic annotations for transitive verb
frames. Given a LC model NA.) for verb-noun
pairs, and a sample (ni, n2)1, , (n, n) of
noun arguments (ni subjects, and n2 direct ob-
jects) for a fixed transitive verb, we calculate the
probability of its noun argument pairs by:
</bodyText>
<equation confidence="0.957230222222222">
p(ni, n2) = Ecl,„Ec p(ci, c2, n1, n2)
= Ec1,c2ec c2)PLc(nil
, ci )
PLc (n2 I c2)
Again, estimation of the parameter-vector
(Oci c
=- ci , C2
E C) can be formalized
2 I
</equation>
<bodyText confidence="0.999435285714286">
in an EM framework by viewing p(ni, n2) or
p(ci, c2, ni, n2) as a function of 0 for fixed
pLc(.). The re-estimation formulae resulting
from this incomplete data estimation problem
have the following simple form (f(ni, n2) is the
frequency of (ni, n2) in the sample of noun ar-
gument pairs of the fixed verb):
</bodyText>
<equation confidence="0.932921">
, En
1,n 2EN
f(ni,n2)pe(ci, c2Ini, n2)
M(0i2) =
f (ni, n2)
</equation>
<bodyText confidence="0.623302933333333">
Note that the class distributions p(c) and
p(ci, c2) for intransitive and transitive models
can be computed also for verbs unseen in the
LC model.
blush 5 0.982975 snarl 5 0.962094
constance 3 mandeyille 2
christina 3 jinkwa 2
willie 2.99737 man 1.99859
ronni 2 scott 1.99761
claudia 2 omalley 1.99755
gabriel 2 shamlou 1
maggie 2 angalo 1
bathsheba 2 corbett 1
sarah 2 southgate 1
girl 1.9977 ace 1
</bodyText>
<figureCaption confidence="0.968095875">
Figure 6: Lexicon entries: blush, snarl
increase 17 0.923698
number 134.147 proportion 23.8699
demand 30.7322 size 22.8108
pressure 30.5844 rate 20.9593
temperature 25.9691 level 20.7651
cost 23.9431 price 17.9996
Figure 7: Scalar motion increase.
</figureCaption>
<subsectionHeader confidence="0.909562">
4.2 Lexicon Induction Experiment
</subsectionHeader>
<bodyText confidence="0.965952814814815">
Experiments used a model with 35 classes. From
maximal probability parses for the British Na-
tional Corpus derived with a statistical parser
(Carroll and Rooth, 1998), we extracted fre-
quency tables for intransitve verb/subject pairs
and transitive verb/subject/object triples. The
500 most frequent verbs were selected for slot
labeling. Fig. 6 shows two verbs v for which
the most probable class label is 5, a class
which we earlier described as communicative ac-
tion, together with the estimated frequencies of
f (n)pe(cln) for those ten nouns n for which this
estimated frequency is highest.
Fig. 7 shows corresponding data for an intran-
sitive scalar motion sense of increase.
Fig. 8 shows the intransitive verbs which take
17 as the most probable label. Intuitively, the
verbs are semantically coherent. When com-
pared to Levin (1993)&apos;s 48 top-level verb classes,
we found an agreement of our classification with
her class of &amp;quot;verbs of changes of state&amp;quot; except for
the last three verbs in the list in Fig. 8 which is
sorted by probability of the class label.
Similar results for German intransitive scalar
motion verbs are shown in Fig. 9. The data
for these experiments were extracted from the
maximal-probability parses of a 4.1 million word
</bodyText>
<page confidence="0.996189">
108
</page>
<table confidence="0.993451418604651">
Class 8 In C•1 t.- .-.1 In 1-1 (f) 00 t-- us 01 01 . 0 0 00 t.- C- CD •-■ 0 0C..- 0 0
PROB 0.0369 00 0 us 0 C.- C.- to to to 0 ,0 bi., 0 0 it, 0 NV .0 ..0&apos; V&apos; V •cr V cr CO CO CO CO CO CO
c.o... 000 oo 0000 o 00000 o 000 0000000
000 o coo oo 0000 o 00000 o 000 0000000
O ■::; O O 000 ci c; 0000 ci 000000 000 0000000
40&apos; I,0
0 .I.,
.., C) :43 g 0
a. 1&gt; -,9 . ;g, g fi
00 &apos;&apos;&apos;,4: 2, :5 .2 ,-, aj P g 1.3 4, :C■ r; 8 ,, g t ..; i t, .0 a■ E c
CI &apos;6&apos; . .,,, &apos;ib 8 &apos; . . . . . . . 4, 0
.
CP 11 &apos;&apos; 6&apos;, .P. 1.1 -,1 -; &apos;2
g&amp;quot; . t. t).&apos; li t;&apos; 8 . 2
&apos;0-3(1. El .a.
0.0539 require.aso:o • • •
0.0469 show.aso:o
0.0439 need.aso:o
0.0383 involve.aso:o •
0.0270 produce.aso:o • eee
0.0255 occur.as:s
0.0192 cause.aso:s 0000000 • • • • •
0.0189 cause.aso:o • • • • •
0.0179 affect.aso:s
0.0162 require.aso:s
0.0150 mean.aso:o 00000 • 0000000 • • • • • • 00000
0.0140 suggest.aso:o
0.0138 produce.aso:s •
0.0109 demand.aso:o • • 00000 • • • •
0.0109 reduce.aso:s • • •
0.0097 reflect.aso:o
0.0092 involve.aso:s • • • • • • • • • o
0.0091 undergo.aso:o • • • • • • • • • • • • • •
Figure 5: Class 8: dispositions
0.977992 decrease 0.560727 drop
0.948099 double 0.476524 grow
0.923698 increase 0.42842 vary
0.908378 decline 0.365586 improve
0.877338 rise 0.365374 climb
0.876083 soar 0.292716 flow
0.803479 fall 0.280183 cut
0.672409 slow 0.238182 mount
0.583314 diminish
</table>
<figureCaption confidence="0.997136">
Figure 8: Scalar motion verbs
</figureCaption>
<bodyText confidence="0.999768526315789">
corpus of German subordinate clauses, yielding
418290 tokens (318086 types) of pairs of verbs
or adjectives and nouns. The lexicalized proba-
bilistic grammar for German used is described
in Beil et al. (1999). We compared the Ger-
man example of scalar motion verbs to the lin-
guistic classification of verbs given by Schuh-
macher (1986) and found an agreement of our
classification with the class of &amp;quot;einfache An-
derungsverben&amp;quot; (simple verbs of change) except
for the verbs anwachsen (increase) and stag-
nieren(stagnate) which were not classified there
at all.
Fig. 10 shows the most probable pair of classes
for increase as a transitive verb, together with
estimated frequencies for the head filler pair.
Note that the object label 17 is the class found
with intransitive scalar motion verbs; this cor-
respondence is exploited in the next section.
</bodyText>
<table confidence="0.9983875">
0.741467 ansteigen (go up)
0.720221 steigen (rise)
0.693922 absinken (sink)
0.656021 sinken (go down)
0.438486 schrumpfen (shrink)
0.375039 zuriickgehen (decrease)
0.316081 anwachsen (increase)
0.215156 stagnieren (stagnate)
0.160317 wachsen (grow)
0.154633 hinzukommen (be added)
</table>
<figureCaption confidence="0.755223">
Figure 9: German intransitive scalar motion
verbs
Figure 10: Transitive increase with estimated
frequencies for filler pairs.
</figureCaption>
<sectionHeader confidence="0.991965" genericHeader="method">
5 Linguistic Interpretation
</sectionHeader>
<bodyText confidence="0.9998435">
In some linguistic accounts, multi-place verbs
are decomposed into representations involv-
ing (at least) one predicate or relation
per argument. For instance, the transitive
causative/inchoative verb increase, is composed
of an actor/causative verb combining with a
</bodyText>
<equation confidence="0.255930666666667">
increase (8, 17) 0.3097650
development - pressure
fat - risk
communication - awareness
supplementation - concentration
increase - number
</equation>
<page confidence="0.638815666666667">
2.3055
2.11807
2.04227
1.98918
1.80559
109
</page>
<table confidence="0.871021">
VP
NP V
R,, A increase„
NP V NP V
R.„, A increase„
</table>
<figureCaption confidence="0.959648571428571">
Figure 11: First tree: linguistic lexical entry for
transitive verb increase. Second: corresponding
lexical entry with induced classes as relational
constants. Third: indexed open class root added
as conjunct in transitive scalar motion increase.
Fourth: induced entry for related intransitive in-
crease.
</figureCaption>
<bodyText confidence="0.992328289473685">
one-place predicate in the structure on the left in
Fig. 11. Linguistically, such representations are
motivated by argument alternations (diathesis),
case linking and deep word order, language ac-
quistion, scope ambiguity, by the desire to repre-
sent aspects of lexical meaning, and by the fact
that in some languages, the postulated decom-
posed representations are overt, with each primi-
tive predicate corresponding to a morpheme. For
references and recent discussion of this kind of
theory see Hale and Keyser (1993) and Kural
(1996).
We will sketch an understanding of the lexi-
cal representations induced by latent-class label-
ing in terms of the linguistic theories mentioned
above, aiming at an interpretation which com-
bines computational learnability, linguistic mo-
tivation, and denotational-semantic adequacy.
The basic idea is that latent classes are compu-
tational models of the atomic relation symbols
occurring in lexical-semantic representations. As
a first implementation, consider replacing the re-
lation symbols in the first tree in Fig. 11 with
relation symbols derived from the latent class la-
beling. In the second tree in Fig 11, R17 and R8
are relation symbols with indices derived from
the labeling procedure of Sect. 4. Such represen-
tations can be semantically interpreted in stan-
dard ways, for instance by interpreting relation
symbols as denoting relations between events
and individuals.
Such representations are semantically inad-
equate for reasons given in philosophical cri-
tiques of decomposed linguistic representations;
see Fodor (1998) for recent discussion. A lex-
icon estimated in the above way has as many
primitive relations as there are latent classes. We
guess there should be a few hundred classes in an
approximately complete lexicon (which would
have to be estimated from a corpus of hun-
dreds of millions of words or more). Fodor&apos;s ar-
guments, which are based on the very limited de-
gree of genuine interdefinability of lexical items
and on Putnam&apos;s arguments for contextual de-
termination of lexical meaning, indicate that the
number of basic concepts has the order of mag-
nitude of the lexicon itself. More concretely, a
lexicon constructed along the above principles
would identify verbs which are labelled with the
same latent classes; for instance it might identify
the representations of grab and touch.
For these reasons, a semantically adequate
lexicon must include additional relational con-
stants. We meet this requirement in a simple
way, by including as a conjunct a unique con-
stant derived from the open-class root, as in
the third tree in Fig. 11. We introduce index-
ing of the open class root (copied from the class
index) in order that homophony of open class
roots not result in common conjuncts in seman-
tic representations—for instance, we don&apos;t want
the two senses of decline exemplified in decline
the proposal and decline five percent to have an
common entailment represented by a common
conjunct. This indexing method works as long
as the labeling process produces different latent
class labels for the different senses.
The last tree in Fig. 11 is the learned represen-
tation for the scalar motion sense of the intran-
sitive verb increase. In our approach, learning
the argument alternation (diathesis) relating the
transitive increase (in its scalar motion sense)
to the intransitive increase (in its scalar motion
sense) amounts to learning representations with
a common component Ri7 A increaser,. In this
case, this is achieved.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.997883">
We have proposed a procedure which maps
observations of subcategorization frames with
their complement fillers to structured lexical
entries. We believe the method is scientifically
interesting, practically useful, and flexible be-
cause:
1. The algorithms and implementation are ef-
ficient enough to map a corpus of a hundred
million words to a lexicon.
</bodyText>
<figure confidence="0.991992714285714">
VP
NP VI
A
VP V
A AT
NP V
increase
</figure>
<page confidence="0.987106">
110
</page>
<bodyText confidence="0.67232848">
2. The model and induction algorithm have
foundations in the theory of parameter-
ized families of probability distributions
and statistical estimation. As exemplified
in the paper, learning, disambiguation, and
evaluation can be given simple, motivated
formulations.
3. The derived lexical representations are lin-
guistically interpretable. This suggests the
possibility of large-scale modeling and ob-
servational experiments bearing on ques-
tions arising in linguistic theories of the lex-
icon.
4. Because a simple probabilistic model is
used, the induced lexical entries could be
incorporated in lexicalized syntax-based
probabilistic language models, in particular
in head-lexicalized models. This provides
for potential application in many areas.
5. The method is applicable to any natural
language where text samples of sufficient
size, computational morphology, and a ro-
bust parser capable of extracting subcate-
gorization frames with their fillers are avail-
able.
</bodyText>
<sectionHeader confidence="0.992653" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.949191887323944">
Leonard E. Baum, Ted Petrie, George Soules,
and Norman Weiss. 1970. A maximiza-
tion technique occuring in the statistical
analysis of probabilistic functions of Markov
chains. The Annals of Mathematical Statis-
tics, 41(1):164-171.
Franz Beil, Glenn Carroll, Detlef Prescher, Ste-
fan Riezler, and Mats Rooth. 1999. Inside-
outside estimation of a lexicalized PCFG for
German. In Proceedings of the 37th Annual
Meeting of the ACL, Maryland.
Glenn Carroll and Mats Rooth. 1998. Valence
induction with a head-lexicalized PCFG. In
Proceedings of EMNLP-3, Granada.
Ido Dagan, Lillian Lee, and Fernando Pereira.
to appear. Similarity-based models of word
cooccurence probabilities. Machine Learning.
A. P. Dempster, N. M. Laird, and D. B. Rubin.
1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the
Royal Statistical Society, 39(B):1-38.
Jerry A. Fodor. 1998. Concepts : Where Cogni-
tive Science Went Wrong. Oxford Cognitive
Science Series, Oxford.
K. Hale and S.J. Keyser. 1993. Argument struc-
ture and the lexical expression of syntactic re-
lations. In K. Hale and S.J. Keyser, editors,
The View from Building 20. MIT Press, Cam-
bridge, MA.
Thomas Hofmann and Jan Puzicha. 1998. Un-
supervised learning from dyadic data. Tech-
nical Report TR-98-042, International Com-
puter Science Insitute, Berkeley, CA.
Murat Kural. 1996. Verb Incorporation and El-
ementary Predicates. Ph.D. thesis, University
of California, Los Angeles.
Beth Levin. 1993. English Verb Classes
and Alternations. A Preliminary Investiga-
tion. The University of Chicago Press,
Chicago/London.
Fernando Pereira, Naftali Tishby, and Lillian
Lee. 1993. Distributional clustering of en-
glish words. In Proceedings of the 31th Annual
Meeting of the ACL, Columbus, Ohio.
Philip Resnik. 1993. Selection and information:
A class-bases approach to lexical relationships.
Ph.D. thesis, University of Pennsylvania, CIS
Department.
Francecso Ribas. 1994. An experiment on learn-
ing appropriate selectional restrictions from a
parsed corpus. In Proceedings of COLING-94,
Kyoto, Japan.
Mats Rooth, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1998. EM-
based clustering for NLP applications. In
Inducing Lexicons with the EM Algorithm.
AIMS Report 4(3), Institut ffir Maschinelle
Sprachverarbeitung, Universitat Stuttgart.
Mats Rooth. Ms. Two-dimensional clusters in
grammatical relations. In Symposium on Rep-
resentation and Acquisition of Lexical Knowl-
edge: Polysemy, Ambiguity, and Generativity.
AAAI 1995 Spring Symposium Series, Stan-
ford University.
Lawrence K. Saul and Fernando Pereira. 1997.
Aggregate and mixed-order Markov models
for statistical language processing. In Pro-
ceedings of EMNLP-2.
Helmut Schuhmacher. 1986. Verben in Feldern.
Valenzworterbuch zur Syntax und Semantik
deutscher Verben. de Gruyter, Berlin.
</reference>
<page confidence="0.998755">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.257604">
<title confidence="0.955482666666667">Inducing a Semantically Annotated Lexicon via EM-Based Clustering Mats Rooth</title>
<author confidence="0.766025">Stefan Riezler</author>
<title confidence="0.608719">Detlef Prescher</title>
<author confidence="0.9958795">Glenn Carroll Franz Beil</author>
<affiliation confidence="0.920763">Institut fiir Maschinelle Sprachverarbeitung University of Stuttgart, Germany</affiliation>
<abstract confidence="0.997087230769231">We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evalutated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
<author>Ted Petrie</author>
<author>George Soules</author>
<author>Norman Weiss</author>
</authors>
<title>A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains.</title>
<date>1970</date>
<journal>The Annals of Mathematical Statistics,</journal>
<pages>41--1</pages>
<contexts>
<context position="9100" citStr="Baum et al. (1970)" startWordPosition="1729" endWordPosition="1732">municative action al., 1970), from which the following particularily simple reestimation formulae can be derived. Let x = (c, y) for fixed c and y. Then probabilistic context-free grammar of (Carroll and Rooth, 1998) gave for the British National Corpus (117 million words). M(Ovc) EyE{v}xNP0(x1Y) M(Onc) EyPo(xly) &apos; M(0) EyEVx{n}Pe(XIY) EyP9(xly) &apos; EyPe(x1Y) IYI Intuitively, the conditional expectation of the number of times a particular v, n, or c choice is made during the derivation is prorated by the conditionally expected total number of times a choice of the same kind is made. As shown by Baum et al. (1970), these expectations can be calculated efficiently using dynamic programming techniques. Every such maximization step increases the log-likelihood function L, and a sequence of re-estimates eventually converges to a (local) maximum of L. In the following, we will present some examples of induced clusters. Input to the clustering algorithm was a training corpus of 1280715 tokens (608850 types) of verb-noun pairs participating in the grammatical relations of intransitive and transitive verbs and their subject- and object-fillers. The data were gathered from the maximal-probability parses the hea</context>
</contexts>
<marker>Baum, Petrie, Soules, Weiss, 1970</marker>
<rawString>Leonard E. Baum, Ted Petrie, George Soules, and Norman Weiss. 1970. A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains. The Annals of Mathematical Statistics, 41(1):164-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Beil</author>
<author>Glenn Carroll</author>
<author>Detlef Prescher</author>
<author>Stefan Riezler</author>
<author>Mats Rooth</author>
</authors>
<title>Insideoutside estimation of a lexicalized PCFG for German.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL,</booktitle>
<location>Maryland.</location>
<contexts>
<context position="19774" citStr="Beil et al. (1999)" startWordPosition="3656" endWordPosition="3659"> 0.0092 involve.aso:s • • • • • • • • • o 0.0091 undergo.aso:o • • • • • • • • • • • • • • Figure 5: Class 8: dispositions 0.977992 decrease 0.560727 drop 0.948099 double 0.476524 grow 0.923698 increase 0.42842 vary 0.908378 decline 0.365586 improve 0.877338 rise 0.365374 climb 0.876083 soar 0.292716 flow 0.803479 fall 0.280183 cut 0.672409 slow 0.238182 mount 0.583314 diminish Figure 8: Scalar motion verbs corpus of German subordinate clauses, yielding 418290 tokens (318086 types) of pairs of verbs or adjectives and nouns. The lexicalized probabilistic grammar for German used is described in Beil et al. (1999). We compared the German example of scalar motion verbs to the linguistic classification of verbs given by Schuhmacher (1986) and found an agreement of our classification with the class of &amp;quot;einfache Anderungsverben&amp;quot; (simple verbs of change) except for the verbs anwachsen (increase) and stagnieren(stagnate) which were not classified there at all. Fig. 10 shows the most probable pair of classes for increase as a transitive verb, together with estimated frequencies for the head filler pair. Note that the object label 17 is the class found with intransitive scalar motion verbs; this correspondence</context>
</contexts>
<marker>Beil, Carroll, Prescher, Riezler, Rooth, 1999</marker>
<rawString>Franz Beil, Glenn Carroll, Detlef Prescher, Stefan Riezler, and Mats Rooth. 1999. Insideoutside estimation of a lexicalized PCFG for German. In Proceedings of the 37th Annual Meeting of the ACL, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Proceedings of EMNLP-3,</booktitle>
<location>Granada.</location>
<contexts>
<context position="2074" citStr="Carroll and Rooth, 1998" startWordPosition="296" endWordPosition="299">nguages, and (ii) we regard it as an open question whether and to what degree existing designs for lexical hierarchies are appropriate for representing lexical meaning. Both of these considerations suggest the relevance of inductive and experimental approaches to the construction of lexicons with semantic information. This paper presents a method for automatic induction of semantically annotated subcategorization frames from unannotated corpora. We use a statistical subcat-induction system which estimates probability distributions and corpus frequencies for pairs of a head and a subcat frame (Carroll and Rooth, 1998). The statistical parser can also collect frequencies for the nominal fillers of slots in a subcat frame. The induction of labels for slots in a frame is based upon estimation of a probability distribution over tuples consisting of a class label, a selecting head, a grammatical relation, and a filler head. The class label is treated as hidden data in the EMframework for statistical estimation. 2 EM-Based Clustering In our clustering approach, classes are derived directly from distributional data—a sample of pairs of verbs and nouns, gathered by parsing an unannotated corpus and extracting the </context>
<context position="8698" citStr="Carroll and Rooth, 1998" startWordPosition="1663" endWordPosition="1666">0148 wonder.as:s • • • 0.0141 feel.aso:s e • • • 0.0133 take.aso:s 0.0121 sigh.as:s 000000 • • • • • •• • OOOOOO ••• 0.0110 watch.aso:s • • • 0000000 • 0000000 • • • • • 0.0106 ask.aso:s • • • • • 0.0104 tell.aso:s 0.0094 look.as:s • • S • • 0.0092 give.aso:s • • • • 0.0089 hear.aso:s • • • • • • • • 0.0083 grin.as:s 00000000 • • • • • • • 0.0083 answer.as:s 00000000 • • • • . . 9 . Figure 2: Class 5: communicative action al., 1970), from which the following particularily simple reestimation formulae can be derived. Let x = (c, y) for fixed c and y. Then probabilistic context-free grammar of (Carroll and Rooth, 1998) gave for the British National Corpus (117 million words). M(Ovc) EyE{v}xNP0(x1Y) M(Onc) EyPo(xly) &apos; M(0) EyEVx{n}Pe(XIY) EyP9(xly) &apos; EyPe(x1Y) IYI Intuitively, the conditional expectation of the number of times a particular v, n, or c choice is made during the derivation is prorated by the conditionally expected total number of times a choice of the same kind is made. As shown by Baum et al. (1970), these expectations can be calculated efficiently using dynamic programming techniques. Every such maximization step increases the log-likelihood function L, and a sequence of re-estimates eventual</context>
<context position="17099" citStr="Carroll and Rooth, 1998" startWordPosition="3135" endWordPosition="3138">wa 2 willie 2.99737 man 1.99859 ronni 2 scott 1.99761 claudia 2 omalley 1.99755 gabriel 2 shamlou 1 maggie 2 angalo 1 bathsheba 2 corbett 1 sarah 2 southgate 1 girl 1.9977 ace 1 Figure 6: Lexicon entries: blush, snarl increase 17 0.923698 number 134.147 proportion 23.8699 demand 30.7322 size 22.8108 pressure 30.5844 rate 20.9593 temperature 25.9691 level 20.7651 cost 23.9431 price 17.9996 Figure 7: Scalar motion increase. 4.2 Lexicon Induction Experiment Experiments used a model with 35 classes. From maximal probability parses for the British National Corpus derived with a statistical parser (Carroll and Rooth, 1998), we extracted frequency tables for intransitve verb/subject pairs and transitive verb/subject/object triples. The 500 most frequent verbs were selected for slot labeling. Fig. 6 shows two verbs v for which the most probable class label is 5, a class which we earlier described as communicative action, together with the estimated frequencies of f (n)pe(cln) for those ten nouns n for which this estimated frequency is highest. Fig. 7 shows corresponding data for an intransitive scalar motion sense of increase. Fig. 8 shows the intransitive verbs which take 17 as the most probable label. Intuitive</context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Glenn Carroll and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings of EMNLP-3, Granada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>to appear. Similarity-based models of word cooccurence probabilities.</title>
<journal>Machine Learning.</journal>
<marker>Dagan, Lee, Pereira, </marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando Pereira. to appear. Similarity-based models of word cooccurence probabilities. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="6031" citStr="Dempster et al., 1977" startWordPosition="1098" endWordPosition="1101">t al. (1998). We seek to derive a joint distribution of verbnoun pairs from a large sample of pairs of verbs v E V and nouns n E N. The key idea is to view v and n as conditioned on a hidden class c e C, where the classes are given no prior interpretation. The semantically smoothed probability of a pair (v, n) is defined to be: p(v, n) = p(c, v, n) = Ep(c)p(vic)p(nic) cEC cEC The joint distribution p(c, v, n) is defined by p(c, v , n) = p(c)p(vic)p(nic). Note that by construction, conditioning of v and n on each other is solely made through the classes c. In the framework of the EM algorithm (Dempster et al., 1977), we can formalize clustering as an estimation problem for a latent class (LC) model as follows. We are given: (i) a sample space)) of observed, incomplete data, corresponding to pairs from V x N, (ii) a sample space X of unobserved, complete data, corresponding to triples from Cx V x N, (iii) a set X(y) = {x E X I x = (c, y), c E CI of complete data related to the observation y, (iv) a complete-data specification po(x), corresponding to the joint probability p(c, v, n) over Cx V x N, with parametervector 0 = (Oc,Ov,OncIc E C, v E V, n E N), (v) an incomplete data specification P0(Y) which is </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(B):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry A Fodor</author>
</authors>
<title>Concepts : Where Cognitive Science Went Wrong. Oxford Cognitive Science Series,</title>
<date>1998</date>
<location>Oxford.</location>
<contexts>
<context position="23202" citStr="Fodor (1998)" startWordPosition="4166" endWordPosition="4167">ations. As a first implementation, consider replacing the relation symbols in the first tree in Fig. 11 with relation symbols derived from the latent class labeling. In the second tree in Fig 11, R17 and R8 are relation symbols with indices derived from the labeling procedure of Sect. 4. Such representations can be semantically interpreted in standard ways, for instance by interpreting relation symbols as denoting relations between events and individuals. Such representations are semantically inadequate for reasons given in philosophical critiques of decomposed linguistic representations; see Fodor (1998) for recent discussion. A lexicon estimated in the above way has as many primitive relations as there are latent classes. We guess there should be a few hundred classes in an approximately complete lexicon (which would have to be estimated from a corpus of hundreds of millions of words or more). Fodor&apos;s arguments, which are based on the very limited degree of genuine interdefinability of lexical items and on Putnam&apos;s arguments for contextual determination of lexical meaning, indicate that the number of basic concepts has the order of magnitude of the lexicon itself. More concretely, a lexicon </context>
</contexts>
<marker>Fodor, 1998</marker>
<rawString>Jerry A. Fodor. 1998. Concepts : Where Cognitive Science Went Wrong. Oxford Cognitive Science Series, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hale</author>
<author>S J Keyser</author>
</authors>
<title>Argument structure and the lexical expression of syntactic relations.</title>
<date>1993</date>
<booktitle>The View from Building 20.</booktitle>
<editor>In K. Hale and S.J. Keyser, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="22159" citStr="Hale and Keyser (1993)" startWordPosition="4009" endWordPosition="4012">as conjunct in transitive scalar motion increase. Fourth: induced entry for related intransitive increase. one-place predicate in the structure on the left in Fig. 11. Linguistically, such representations are motivated by argument alternations (diathesis), case linking and deep word order, language acquistion, scope ambiguity, by the desire to represent aspects of lexical meaning, and by the fact that in some languages, the postulated decomposed representations are overt, with each primitive predicate corresponding to a morpheme. For references and recent discussion of this kind of theory see Hale and Keyser (1993) and Kural (1996). We will sketch an understanding of the lexical representations induced by latent-class labeling in terms of the linguistic theories mentioned above, aiming at an interpretation which combines computational learnability, linguistic motivation, and denotational-semantic adequacy. The basic idea is that latent classes are computational models of the atomic relation symbols occurring in lexical-semantic representations. As a first implementation, consider replacing the relation symbols in the first tree in Fig. 11 with relation symbols derived from the latent class labeling. In </context>
</contexts>
<marker>Hale, Keyser, 1993</marker>
<rawString>K. Hale and S.J. Keyser. 1993. Argument structure and the lexical expression of syntactic relations. In K. Hale and S.J. Keyser, editors, The View from Building 20. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
<author>Jan Puzicha</author>
</authors>
<title>Unsupervised learning from dyadic data.</title>
<date>1998</date>
<tech>Technical Report TR-98-042,</tech>
<institution>International Computer Science Insitute,</institution>
<location>Berkeley, CA.</location>
<contexts>
<context position="5141" citStr="Hofmann and Puzicha (1998)" startWordPosition="929" endWordPosition="932"> reach.aso:s • • • • • • • • 0.0120 decline.as:s 00000 • • • • • • • • • • • • • • 0.0102 lose.aso:o • • • • • • • 0000000 • • • • • • • • • 0.0099 act.aso:s • • a • • • • • 0.0099 improve.aso:o • • • • • • • • • • • • 0.0088 include.aso:o oo 0.0088 cut.aso:o 00000 • • • • • • • • • • • • 0.0080 show.aso:o • • • 0.0078 vary.as:s 000000 • • • • • • • • • • Figure 1: Class 17: scalar change proach, our statistical inference method for clustering is formalized clearly as an EM-algorithm. Approaches to probabilistic clustering similar to ours were presented recently in Saul and Pereira (1997) and Hofmann and Puzicha (1998). There also EM-algorithms for similar probability models have been derived, but applied only to simpler tasks not involving a combination of EMbased clustering models as in our lexicon induction experiment. For further applications of our clustering model see Rooth et al. (1998). We seek to derive a joint distribution of verbnoun pairs from a large sample of pairs of verbs v E V and nouns n E N. The key idea is to view v and n as conditioned on a hidden class c e C, where the classes are given no prior interpretation. The semantically smoothed probability of a pair (v, n) is defined to be: p(</context>
</contexts>
<marker>Hofmann, Puzicha, 1998</marker>
<rawString>Thomas Hofmann and Jan Puzicha. 1998. Unsupervised learning from dyadic data. Technical Report TR-98-042, International Computer Science Insitute, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murat Kural</author>
</authors>
<title>Verb Incorporation and Elementary Predicates.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California,</institution>
<location>Los Angeles.</location>
<contexts>
<context position="22176" citStr="Kural (1996)" startWordPosition="4014" endWordPosition="4015">calar motion increase. Fourth: induced entry for related intransitive increase. one-place predicate in the structure on the left in Fig. 11. Linguistically, such representations are motivated by argument alternations (diathesis), case linking and deep word order, language acquistion, scope ambiguity, by the desire to represent aspects of lexical meaning, and by the fact that in some languages, the postulated decomposed representations are overt, with each primitive predicate corresponding to a morpheme. For references and recent discussion of this kind of theory see Hale and Keyser (1993) and Kural (1996). We will sketch an understanding of the lexical representations induced by latent-class labeling in terms of the linguistic theories mentioned above, aiming at an interpretation which combines computational learnability, linguistic motivation, and denotational-semantic adequacy. The basic idea is that latent classes are computational models of the atomic relation symbols occurring in lexical-semantic representations. As a first implementation, consider replacing the relation symbols in the first tree in Fig. 11 with relation symbols derived from the latent class labeling. In the second tree i</context>
</contexts>
<marker>Kural, 1996</marker>
<rawString>Murat Kural. 1996. Verb Incorporation and Elementary Predicates. Ph.D. thesis, University of California, Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations. A Preliminary Investigation.</title>
<date>1993</date>
<publisher>The University of Chicago Press, Chicago/London.</publisher>
<contexts>
<context position="17769" citStr="Levin (1993)" startWordPosition="3245" endWordPosition="3246">ct pairs and transitive verb/subject/object triples. The 500 most frequent verbs were selected for slot labeling. Fig. 6 shows two verbs v for which the most probable class label is 5, a class which we earlier described as communicative action, together with the estimated frequencies of f (n)pe(cln) for those ten nouns n for which this estimated frequency is highest. Fig. 7 shows corresponding data for an intransitive scalar motion sense of increase. Fig. 8 shows the intransitive verbs which take 17 as the most probable label. Intuitively, the verbs are semantically coherent. When compared to Levin (1993)&apos;s 48 top-level verb classes, we found an agreement of our classification with her class of &amp;quot;verbs of changes of state&amp;quot; except for the last three verbs in the list in Fig. 8 which is sorted by probability of the class label. Similar results for German intransitive scalar motion verbs are shown in Fig. 9. The data for these experiments were extracted from the maximal-probability parses of a 4.1 million word 108 Class 8 In C•1 t.- .-.1 In 1-1 (f) 00 t-- us 01 01 . 0 0 00 t.- C- CD •-■ 0 0C..- 0 0 PROB 0.0369 00 0 us 0 C.- C.- to to to 0 ,0 bi., 0 0 it, 0 NV .0 ..0&apos; V&apos; V •cr V cr CO CO CO CO CO C</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations. A Preliminary Investigation. The University of Chicago Press, Chicago/London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31th Annual Meeting of the ACL,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="3712" citStr="Pereira et al. (1993)" startWordPosition="562" endWordPosition="565">e two main tasks of EM-based clustering are i) the induction of a smooth probability model on the data, and ii) the automatic discovery of class-structure in the data. Both of these aspects are respected in our application of lexicon induction. The basic ideas of our EM-based clustering approach were presented in Rooth (Ms). Our approach constrasts with the merely heuristic and empirical justification of similarity-based approaches to clustering (Dagan et al., to appear) for which so far no clear probabilistic interpretation has been given. The probability model we use can be found earlier in Pereira et al. (1993). However, in contrast to this ap104 Class 17 0000 00 0 itl. CO 0 0 in 00 0 . 05 CO ni t- 01 . . 0 00 1.-- ■0 in 00 I-- t- V&apos; . o PROS 0.0265 O 0 O O 0 n n n 0 n n n n 010. .0000 000 000 0 0 00000 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 do 0 ci c; 0 0 c; 0 0 0 0 0 O 0 0 0 0 0 0 c; c; c; c; 0 0 c; 0 do 0 u u c c g -o ea &apos;13 0 &apos; ai ill . 0 E ..... tot...zg t .,-g.-s.e. ,-.F&amp;quot;,§151, -.., a. ,, -. 0-,..0&gt;E7.,..0. 41,9&apos;1..&apos;ut01.52Eg.E00&apos;34° 0.0437 increase.as:s 0.0392 increase.aso:o 0.0344 fall.as:s 00000000 • • • • • • • • • • • • • • • 0.0337 pay.aso:o • • • • • • • • • • </context>
<context position="11071" citStr="Pereira et al. (1993)" startWordPosition="2059" endWordPosition="2062">r aso : o. Induced classes often have a basis in lexical semantics; class 5 can be interpreted 106 as clustering agents, denoted by proper names, &amp;quot;man&amp;quot;, and &amp;quot;woman&amp;quot;, together with verbs denoting communicative action. Fig. 1 shows a cluster involving verbs of scalar change and things which can move along scales. Fig. 5 can be interpreted as involving different dispositions and modes of their execution. 60 100 160 200 260 300 nix*. el cleeee• 3 Evaluation of Clustering Models 3.1 Pseudo-Disambiguation We evaluated our clustering models on a pseudodisambiguation task similar to that performed in Pereira et al. (1993), but differing in detail. The task is to judge which of two verbs v and v&apos; is more likely to take a given noun n as its argument where the pair (v, n) has been cut out of the original corpus and the pair (v&apos;, n) is constructed by pairing 71 with a randomly chosen verb v&apos; such that the combination (v&apos;, n) is completely unseen. Thus this test evaluates how well the models generalize over unseen verbs. The data for this test were built as follows. We constructed an evaluation corpus of (v, n, v&apos;) triples by randomly cutting a test corpus of 3000 (v, n) pairs out of the original corpus of 1280712</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of english words. In Proceedings of the 31th Annual Meeting of the ACL, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and information: A class-bases approach to lexical relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania, CIS Department.</institution>
<contexts>
<context position="1014" citStr="Resnik (1993)" startWordPosition="139" endWordPosition="140">The models are empirically evalutated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries. 1 Introduction An important challenge in computational linguistics concerns the construction of large-scale computational lexicons for the numerous natural languages where very large samples of language use are now available. Resnik (1993) initiated research into the automatic acquisition of semantic selectional restrictions. Ribas (1994) presented an approach which takes into account the syntactic position of the elements whose semantic relation is to be acquired. However, those and most of the following approaches require as a prerequisite a fixed taxonomy of semantic relations. This is a problem because (i) entailment hierarchies are presently available for few languages, and (ii) we regard it as an open question whether and to what degree existing designs for lexical hierarchies are appropriate for representing lexical mean</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Philip Resnik. 1993. Selection and information: A class-bases approach to lexical relationships. Ph.D. thesis, University of Pennsylvania, CIS Department.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francecso Ribas</author>
</authors>
<title>An experiment on learning appropriate selectional restrictions from a parsed corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING-94,</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="1115" citStr="Ribas (1994)" startWordPosition="152" endWordPosition="153">tegorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries. 1 Introduction An important challenge in computational linguistics concerns the construction of large-scale computational lexicons for the numerous natural languages where very large samples of language use are now available. Resnik (1993) initiated research into the automatic acquisition of semantic selectional restrictions. Ribas (1994) presented an approach which takes into account the syntactic position of the elements whose semantic relation is to be acquired. However, those and most of the following approaches require as a prerequisite a fixed taxonomy of semantic relations. This is a problem because (i) entailment hierarchies are presently available for few languages, and (ii) we regard it as an open question whether and to what degree existing designs for lexical hierarchies are appropriate for representing lexical meaning. Both of these considerations suggest the relevance of inductive and experimental approaches to t</context>
</contexts>
<marker>Ribas, 1994</marker>
<rawString>Francecso Ribas. 1994. An experiment on learning appropriate selectional restrictions from a parsed corpus. In Proceedings of COLING-94, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carroll</author>
<author>Franz Beil</author>
</authors>
<title>EMbased clustering for NLP applications.</title>
<date>1998</date>
<booktitle>In Inducing Lexicons with the EM Algorithm. AIMS</booktitle>
<tech>Report 4(3),</tech>
<institution>Institut ffir Maschinelle Sprachverarbeitung, Universitat Stuttgart.</institution>
<contexts>
<context position="5421" citStr="Rooth et al. (1998)" startWordPosition="975" endWordPosition="978"> • 0.0080 show.aso:o • • • 0.0078 vary.as:s 000000 • • • • • • • • • • Figure 1: Class 17: scalar change proach, our statistical inference method for clustering is formalized clearly as an EM-algorithm. Approaches to probabilistic clustering similar to ours were presented recently in Saul and Pereira (1997) and Hofmann and Puzicha (1998). There also EM-algorithms for similar probability models have been derived, but applied only to simpler tasks not involving a combination of EMbased clustering models as in our lexicon induction experiment. For further applications of our clustering model see Rooth et al. (1998). We seek to derive a joint distribution of verbnoun pairs from a large sample of pairs of verbs v E V and nouns n E N. The key idea is to view v and n as conditioned on a hidden class c e C, where the classes are given no prior interpretation. The semantically smoothed probability of a pair (v, n) is defined to be: p(v, n) = p(c, v, n) = Ep(c)p(vic)p(nic) cEC cEC The joint distribution p(c, v, n) is defined by p(c, v , n) = p(c)p(vic)p(nic). Note that by construction, conditioning of v and n on each other is solely made through the classes c. In the framework of the EM algorithm (Dempster et </context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1998</marker>
<rawString>Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1998. EMbased clustering for NLP applications. In Inducing Lexicons with the EM Algorithm. AIMS Report 4(3), Institut ffir Maschinelle Sprachverarbeitung, Universitat Stuttgart.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ms</author>
</authors>
<title>Two-dimensional clusters in grammatical relations. In</title>
<booktitle>Symposium on Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity. AAAI 1995 Spring Symposium Series,</booktitle>
<institution>Stanford University.</institution>
<marker>Ms, </marker>
<rawString>Mats Rooth. Ms. Two-dimensional clusters in grammatical relations. In Symposium on Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity. AAAI 1995 Spring Symposium Series, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence K Saul</author>
<author>Fernando Pereira</author>
</authors>
<title>Aggregate and mixed-order Markov models for statistical language processing.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP-2.</booktitle>
<contexts>
<context position="5110" citStr="Saul and Pereira (1997)" startWordPosition="924" endWordPosition="927"> 0.0134 include.aso:s 0.0129 reach.aso:s • • • • • • • • 0.0120 decline.as:s 00000 • • • • • • • • • • • • • • 0.0102 lose.aso:o • • • • • • • 0000000 • • • • • • • • • 0.0099 act.aso:s • • a • • • • • 0.0099 improve.aso:o • • • • • • • • • • • • 0.0088 include.aso:o oo 0.0088 cut.aso:o 00000 • • • • • • • • • • • • 0.0080 show.aso:o • • • 0.0078 vary.as:s 000000 • • • • • • • • • • Figure 1: Class 17: scalar change proach, our statistical inference method for clustering is formalized clearly as an EM-algorithm. Approaches to probabilistic clustering similar to ours were presented recently in Saul and Pereira (1997) and Hofmann and Puzicha (1998). There also EM-algorithms for similar probability models have been derived, but applied only to simpler tasks not involving a combination of EMbased clustering models as in our lexicon induction experiment. For further applications of our clustering model see Rooth et al. (1998). We seek to derive a joint distribution of verbnoun pairs from a large sample of pairs of verbs v E V and nouns n E N. The key idea is to view v and n as conditioned on a hidden class c e C, where the classes are given no prior interpretation. The semantically smoothed probability of a p</context>
</contexts>
<marker>Saul, Pereira, 1997</marker>
<rawString>Lawrence K. Saul and Fernando Pereira. 1997. Aggregate and mixed-order Markov models for statistical language processing. In Proceedings of EMNLP-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schuhmacher</author>
</authors>
<date>1986</date>
<booktitle>Verben in Feldern. Valenzworterbuch zur Syntax und Semantik deutscher Verben. de Gruyter,</booktitle>
<location>Berlin.</location>
<contexts>
<context position="19899" citStr="Schuhmacher (1986)" startWordPosition="3679" endWordPosition="3681">977992 decrease 0.560727 drop 0.948099 double 0.476524 grow 0.923698 increase 0.42842 vary 0.908378 decline 0.365586 improve 0.877338 rise 0.365374 climb 0.876083 soar 0.292716 flow 0.803479 fall 0.280183 cut 0.672409 slow 0.238182 mount 0.583314 diminish Figure 8: Scalar motion verbs corpus of German subordinate clauses, yielding 418290 tokens (318086 types) of pairs of verbs or adjectives and nouns. The lexicalized probabilistic grammar for German used is described in Beil et al. (1999). We compared the German example of scalar motion verbs to the linguistic classification of verbs given by Schuhmacher (1986) and found an agreement of our classification with the class of &amp;quot;einfache Anderungsverben&amp;quot; (simple verbs of change) except for the verbs anwachsen (increase) and stagnieren(stagnate) which were not classified there at all. Fig. 10 shows the most probable pair of classes for increase as a transitive verb, together with estimated frequencies for the head filler pair. Note that the object label 17 is the class found with intransitive scalar motion verbs; this correspondence is exploited in the next section. 0.741467 ansteigen (go up) 0.720221 steigen (rise) 0.693922 absinken (sink) 0.656021 sinke</context>
</contexts>
<marker>Schuhmacher, 1986</marker>
<rawString>Helmut Schuhmacher. 1986. Verben in Feldern. Valenzworterbuch zur Syntax und Semantik deutscher Verben. de Gruyter, Berlin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>