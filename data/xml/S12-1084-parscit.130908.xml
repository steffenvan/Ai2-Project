<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.127379">
<title confidence="0.995779">
Tiantianzhu7:System Description of Semantic Textual Similarity (STS) in
the SemEval-2012 (Task 6)
</title>
<author confidence="0.999438">
Tiantian Zhu Man Lan
</author>
<affiliation confidence="0.956001">
Department of Computer Science and Department of Computer Science and
Technology Technology
East China Normal University East China Normal University
</affiliation>
<email confidence="0.996038">
51111201046@student.ecnu.edu.cn mlan@cs.ecnu.edu.cn
</email>
<sectionHeader confidence="0.995573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999626">
This paper briefly reports our submissions to
the Semantic Textual Similarity (STS) task
in the SemEval 2012 (Task 6). We first use
knowledge-based methods to compute word
semantic similarity as well as Word Sense Dis-
ambiguation (WSD). We also consider word
order similarity from the structure of the sen-
tence. Finally we sum up several aspects of
similarity with different coefficients and get
the sentence similarity score.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969860465116">
The task of semantic textual similarity (STS) is to
measure the degree of semantic equivalence between
two sentences. It plays an increasingly important
role in several text-related research and applications,
such as text mining, Web page retrieval, automatic
question-answering, text summarization, and ma-
chine translation. The goal of the Semeval 2012 STS
task (task 6) is to build a unified framework for the
evaluation of semantic textual similarity modules for
different systems and to characterize their impact on
NLP applications.
Generally, there are two ways to measure sim-
ilarity of two sentences, i.e, corpus-based meth-
ods and knowledge-based methods. The corpus-
based method typically computes sentence similar-
ity based on the frequency of word occurrence or the
co-occurrence between collocated words. For ex-
ample, in (Islam and Inkpen, 2008) they proposed a
corpus-based sentence similarity measure as a func-
tion of string similarity, word similarity and com-
mon word order similarity (CWO). The knowledge-
based method computes sentence similarity based
on the semantic information collected from knowl-
edge bases. With the aid of a number of success-
ful computational linguistic projects, many seman-
tic knowledge bases are readily available, for ex-
ample, WordNet, Spatial Date Transfer Standard,
Gene Ontology, etc. Among them, the most widely
used one is WordNet, which is organized by mean-
ings and developed at Princeton University. Sev-
eral methods computed word similarity by using
WordNet, such as the Lesk method in (Banerjee and
Pedersen, 2003), the lch method in (Leacock and
Chodorow, 1998)and the wup method in (Wu and
Palmer, 1994). Generally, although the knowledge-
based methods heavily depend on the knowledge
bases, they performed much better than the corpus-
based methods in most cases. Therefore, in our STS
system, we use a knowledge-based method to com-
pute word similarity.
The rest of this paper is organized as follows. Sec-
tion 2 describes our system. Section 3 presents the
results of our system.
</bodyText>
<sectionHeader confidence="0.973841" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.9999636">
Usually, a sentence is composed of some nouns,
verbs, adjectives, adverbs and/or some stop words.
We found that these words carry a lot of informa-
tion, especially the nouns and verbs. Although the
adjectives and adverbs also make contribution to the
semantic meaning of the sentence, they are much
weaker than the nouns and verbs. So we consider
to measure the sentence semantic similarities from
three aspects. We define the following three types of
similarity from two compared sentences to measure
</bodyText>
<page confidence="0.975739">
575
</page>
<note confidence="0.5270665">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 575–578,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999966166666667">
the semantic similarity: (1) Noun Similarity to mea-
sure the similarity between the nouns from the two
compared sentences, (2) Verb Similarity to measure
the similarity between Verbs, (3) ADJ-ADV Simi-
larity to measure the similarity between the adjec-
tives and adverbs from each sentence. Besides the
semantic information similarity, we also found that
the structure of the sentences carry some informa-
tion which cannot be ignored. Therefore, we define
the last aspect of the sentence similarity as Word Or-
der Similarity. In the following we will introduce the
different components of our system.
</bodyText>
<subsectionHeader confidence="0.90793">
2.1 POS
</subsectionHeader>
<bodyText confidence="0.9999725">
As a basic natural language processing technique,
part of speech tagging is to identify the part of
speech of individual words in the sentence. In or-
der to compute the three above semantic similari-
ties, we first identify the nouns, verbs, adjectives,
and adverbs in the sentence. Then we can calculate
the Noun Similarity, Verb Similarity and ADJ-ADV
Similarity from two sentences.
</bodyText>
<subsectionHeader confidence="0.999421">
2.2 Semantic similarity between words
</subsectionHeader>
<bodyText confidence="0.999906864864865">
The word similarity measurement have important
impact on the performance of sentence similarity.
Currently, many lexical resources based approaches
perform comparatively well to compute semantic
word similarities. However, the exact resources they
are based are quite different. For example, some are
based on dictionary and/or thesaurus, and others are
based on WordNet.
WordNet is a machine-readable lexical database.
The words in Wordnet are classified into four cat-
egories, i.e., nouns, verbs, adjectives and adverbs.
WordNet groups these words into sets of syn-
onyms called synsets, provides short definitions, and
records the various semantic relations between these
synsets. The synsets are interlinked by means of
conceptual-semantic and lexical relations. Word-
Net also provides the most common relationships
include Hyponym/Hypernym (i.e., is-a relationships)
and Meronym/Holonym (i.e., part-of relationships).
Nouns and verbs are organized into hierarchies
based on the hyponymy/hypernym relation between
synsets while adjectives and adverbs are not.
In this paper, we adopt the wup method in (Wu
and Palmer, 1994) to estimate the semantic similar-
ity between two words, which estimates the seman-
tic similarity between two words based on the depth
of the two words in WordNet and the depth of their
least common subsumer (LCS), where LCS is de-
fined as the common ancestor deepest in the taxon-
omy.
For example, given two words, w1 and w2, the
semantic similarity s(w1,w2) is the function of their
depth in the taxonomy and the depth of their least
common subsumer. If d1 and d2 are the depth of
w1 and w2 in WordNet, and h is the depth of their
least common subsumer in WordNet, the semantic
similarity can be written as:
</bodyText>
<equation confidence="0.997795666666667">
2.0 * h
s(w1� w2) = (1)
d1 + d2
</equation>
<subsectionHeader confidence="0.989633">
2.3 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999407928571429">
Word Sense Disambiguation (WSD) is to identify
the actual meaning of a word according to the con-
text. In our word similarity method, we take the
nearest meaning of two words into consideration
rather than their actual meaning. More impor-
tantly, the nearest meaning does not always repre-
sent the actual meaning. In our system, we used
a WSD algorithm proposed by (Ted Pedersen et
al.,2005), which computes semantic relatedness of
word senses using extended gloss overlaps of their
dictionary definitions. We utilize this WSD algo-
rithm for each sentence to get the actual meaning of
each word before computing the word semantic sim-
ilarity.
</bodyText>
<subsectionHeader confidence="0.997939">
2.4 Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.999994727272727">
We adopt a similar way to compute the three types of
semantic similarities. Here we take Noun Similarity
as an example.
Suppose sentence s1 and s2 are the two sentences
to be compared, s1 has a nouns while s2 has b nouns.
Then we get a * b noun pairs and use the word sim-
ilarity method mentioned in section 2.2 to compute
the Noun Similarity of each noun pair. After that,
for each noun, we choose its highest score in noun
pairs as its similarity score. Then we use the formula
below to compute the Noun Similarity.
</bodyText>
<equation confidence="0.996329">
(�c 1 nz) * (a + b) (2)
Z=
imN°ten = 2ab
</equation>
<page confidence="0.985063">
576
</page>
<bodyText confidence="0.999948166666667">
where c represents the number of noun words in
sequence a and sequence b, c = min(a, b); ni rep-
resents the highest matching similarity score of i-th
word in the shorter sequence with respect to one of
the words in the longer sequence; and E&apos;=1 ni rep-
resents the sum of the highest matching similarity
score between the words in sequence a and sequence
b. Similarly, we can get SimV erb. Since there is no
Hyponym/Hypernym relation for adjectives and ad-
verbs in WordNet, we just compute ADJ-ADV Sim-
ilarity based on the frequency of overlap of simple
words.
</bodyText>
<subsectionHeader confidence="0.996281">
2.5 Word Order Similarity
</subsectionHeader>
<bodyText confidence="0.999478333333333">
We believe that word order information also make
contributions to sentence similarity. In most cases,
the longer common sequence (LCS) the two sen-
tences have, the higher similarity score the sentences
get. For example the pair of sentences s1 and s2, we
remove all the punctuation from the sentences:
</bodyText>
<listItem confidence="0.9951738">
• s1: But other sources close to the sale said
Vivendi was keeping the door open to further
bids and hoped to see bidders interested in in-
dividual assets team up
• s2: But other sources close to the sale said
</listItem>
<bodyText confidence="0.822707">
Vivendi was keeping the door open for further
bids in the next day or two
Since the length of the longest common sequence
is 14, we use the following formula to compute the
word order similarity.
</bodyText>
<equation confidence="0.9882625">
lengthofLCS 3 ( )
SimWordOrder = shorterlength
</equation>
<bodyText confidence="0.999971">
where the shorter length means the length of the
shorter sentence.
</bodyText>
<subsectionHeader confidence="0.998524">
2.6 Overall Similarity
</subsectionHeader>
<bodyText confidence="0.9999776">
After we have the Noun Similarity, Verb Similar-
ity, ADJ-ADV Similarity and Word Order Similar-
ity, we calculate the Overall Similarity of two com-
pared sentences based on these four scores of simi-
larity. We combine them in the following way:
</bodyText>
<equation confidence="0.997523">
Simsent = aSimNoun + bSimV erb+ (4)
cSimADJ−ADV + dSimWordOrder
</equation>
<bodyText confidence="0.999944">
Where a, b, c and d are the coefficients which
denote the contribution of each aspect to the over-
all sentence similarity, For different data collections,
we empirically set different coefficients, for exam-
ple, for the MSR Paraphrase data, the four coeffi-
cients are set as 0.5, 0.3, 0.1, 0.1, because it is hard
to get the highest score 5 even when the two sen-
tences are almost the same meaning, We empirically
set a threshold, if the score exceeds the threshold we
set the score 5.
</bodyText>
<sectionHeader confidence="0.968516" genericHeader="evaluation">
3 Experiment and Results on STS
</sectionHeader>
<bodyText confidence="0.999895722222222">
Firstly, Stanford parser1 is used to parse each
sentence and to tag each word with a part of
speech(POS). Secondly, WordNet SenseRelate All-
Words2, a WSD tool from CPAN is used to disam-
biguate and to assign a sense for each word based on
the assigned POS.
We submitted three runs: run 1 with WSD, run 2
without WSD, run 3 removing stop words and with-
out WSD. The stoplist is available online3. Table 1
lists the performance of these three systems as well
as the baseline and the rank 1 results on STS task in
SemEval 2012.
We can see that run1 gets the best result, which
means WSD has improved the accuracy of sentence
similarity. Run3 gets better result than run2, which
proves that stop words do disturb the computation of
sentence similarity, removing them is a better choice
in our system.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999909545454545">
In our work, we adopt a knowledge-based word sim-
ilarity method with WSD to measure the seman-
tic similarity between two sentences from four as-
pects: Noun Similarity, Verb Similarity, ADJ-ADV
Similarity and Word Order Similarity. The results
show that WSD improves the pearson coefficient at
some degree. However, our system did not get a
good rank. It indicates there still exists many prob-
lems such as wrong POS tag and wrong WSD which
might lead to wrong meaning of one word in a sen-
tence.
</bodyText>
<footnote confidence="0.9527278">
1http://nlp.stanford.edu/software/lex-parser.shtml
2http://search.cpan.org/Tedpederse/WordNet-SenseRelate-
AllWords-0.19
3http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-
smart-stop-list/english.stop
</footnote>
<page confidence="0.995306">
577
</page>
<tableCaption confidence="0.999785">
Table 1: STS system configuration and results on STS task.
</tableCaption>
<table confidence="0.998861166666667">
Run ALL ALLnrm Mean MSRpar MSRvid SMTeur OnWN SMTnews
rank 1 .7790 .8579 .6773 .6830 .8739 .5280 .6641 .4937
baseline .3110 .6732 .4356 .4334 .2996 .4542 .5864 .3908
1 .4533 .7134 .4192 .4184 .5630 .2083 .4822 .2745
2 .4157 .7099 .3960 .4260 .5628 .1546 .4552 .1923
3 .4446 .7097 .3740 .3411 .5946 .1868 .4029 .1823
</table>
<sectionHeader confidence="0.997272" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999822666666667">
The authors would like to thank the organizers for
their invaluable support making STS a first-rank and
interesting international event.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871323529412">
Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-
dul Kadir, Shyamala C. Doraisamy. 2010. Word Sense
Disambiguation-based Sentence Similarity. In Proc.
COLING-ACL, Beijing.
Jin Feng, Yiming Zhou, Trevor Martin. 2008. Sen-
tence Similarity based on Relevance. Proceedings of
IPMU’08, Torremolinos.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O’Shea, and Keeley Crockett. 2009. Sentence Simi-
larity Based on Semantic Nets and Corpus Statistics.
LIN LI, XIA HU, BI-YUN HU, JUN WANG, YI-MING
ZHOU. 2009. MEASURING SENTENCE SIMILAR-
ITY FROM DIFFERENT ASPECTS.
Islam Aminul and Diana Inkpen. 2008. Semantic Text
Similarity Using Corpus-Based Word Similarity and
String Similarity. ACM Transactions on Knowledge
Discovery from Data.
Banerjee and Pedersen. 2003. Extended gloss overlaps
as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence (IJCAI-03), pages805C810,
Acapulco, Mexico.
Leacock and Chodorow. 1998. Combining local con-
text and WordNet similarity for word sense identifica-
tion. In Christiane Fellbaum, editor, WordNet: An
Electronic Lexical Database. The MIT Press, Cam-
bridge,MA.
Z.Wu and M.Palmer. 1994. Verbs semantics and
lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computional Linguis-
tics,Morristown, NJ, USA.
Ted Pedersen, Satanjeev Banerjee, Siddharth Patward-
han. 2005. Maximizing Semantic Relatedness to Per-
form Word Sense Disambiguation.
</reference>
<page confidence="0.996704">
578
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.401540">
<title confidence="0.6293645">Tiantianzhu7:System Description of Semantic Textual Similarity (STS) the SemEval-2012 (Task 6)</title>
<author confidence="0.999677">Tiantian Zhu Man Lan</author>
<affiliation confidence="0.998257">Department of Computer Science and Department of Computer Science and Technology Technology East China Normal University East China Normal University</affiliation>
<address confidence="0.696995">51111201046@student.ecnu.edu.cn mlan@cs.ecnu.edu.cn</address>
<abstract confidence="0.999240545454545">This paper briefly reports our submissions to the Semantic Textual Similarity (STS) task in the SemEval 2012 (Task 6). We first use knowledge-based methods to compute word semantic similarity as well as Word Sense Disambiguation (WSD). We also consider word order similarity from the structure of the sentence. Finally we sum up several aspects of similarity with different coefficients and get the sentence similarity score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chukfong Ho</author>
</authors>
<title>Masrah Azrifah Azmi Murad, Rabiah Abdul Kadir, Shyamala</title>
<date>2010</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<location>Beijing.</location>
<marker>Ho, 2010</marker>
<rawString>Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Abdul Kadir, Shyamala C. Doraisamy. 2010. Word Sense Disambiguation-based Sentence Similarity. In Proc. COLING-ACL, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Feng</author>
<author>Yiming Zhou</author>
<author>Trevor Martin</author>
</authors>
<title>Sentence Similarity based on Relevance.</title>
<date>2008</date>
<booktitle>Proceedings of IPMU’08,</booktitle>
<location>Torremolinos.</location>
<marker>Feng, Zhou, Martin, 2008</marker>
<rawString>Jin Feng, Yiming Zhou, Trevor Martin. 2008. Sentence Similarity based on Relevance. Proceedings of IPMU’08, Torremolinos.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O’Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence Similarity Based on Semantic Nets and Corpus Statistics.</title>
<date>2009</date>
<marker>Li, McLean, Bandar, O’Shea, Crockett, 2009</marker>
<rawString>Yuhua Li, David McLean, Zuhair A. Bandar, James D. O’Shea, and Keeley Crockett. 2009. Sentence Similarity Based on Semantic Nets and Corpus Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LIN LI</author>
<author>XIA HU</author>
<author>BI-YUN HU</author>
<author>JUN WANG</author>
<author>YI-MING ZHOU</author>
</authors>
<date>2009</date>
<journal>MEASURING SENTENCE SIMILARITY FROM DIFFERENT ASPECTS.</journal>
<marker>LI, HU, HU, WANG, ZHOU, 2009</marker>
<rawString>LIN LI, XIA HU, BI-YUN HU, JUN WANG, YI-MING ZHOU. 2009. MEASURING SENTENCE SIMILARITY FROM DIFFERENT ASPECTS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Aminul</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic Text Similarity Using Corpus-Based Word Similarity and String Similarity.</title>
<date>2008</date>
<journal>ACM Transactions on Knowledge Discovery from Data.</journal>
<marker>Aminul, Inkpen, 2008</marker>
<rawString>Islam Aminul and Diana Inkpen. 2008. Semantic Text Similarity Using Corpus-Based Word Similarity and String Similarity. ACM Transactions on Knowledge Discovery from Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Banerjee</author>
<author>Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03),</booktitle>
<pages>805--810</pages>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="2331" citStr="Banerjee and Pedersen, 2003" startWordPosition="343" endWordPosition="346">ing similarity, word similarity and common word order similarity (CWO). The knowledgebased method computes sentence similarity based on the semantic information collected from knowledge bases. With the aid of a number of successful computational linguistic projects, many semantic knowledge bases are readily available, for example, WordNet, Spatial Date Transfer Standard, Gene Ontology, etc. Among them, the most widely used one is WordNet, which is organized by meanings and developed at Princeton University. Several methods computed word similarity by using WordNet, such as the Lesk method in (Banerjee and Pedersen, 2003), the lch method in (Leacock and Chodorow, 1998)and the wup method in (Wu and Palmer, 1994). Generally, although the knowledgebased methods heavily depend on the knowledge bases, they performed much better than the corpusbased methods in most cases. Therefore, in our STS system, we use a knowledge-based method to compute word similarity. The rest of this paper is organized as follows. Section 2 describes our system. Section 3 presents the results of our system. 2 System Description Usually, a sentence is composed of some nouns, verbs, adjectives, adverbs and/or some stop words. We found that t</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Banerjee and Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), pages805C810, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leacock</author>
<author>Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database. The</booktitle>
<publisher>MIT Press, Cambridge,MA.</publisher>
<contexts>
<context position="2379" citStr="Leacock and Chodorow, 1998" startWordPosition="351" endWordPosition="354">rder similarity (CWO). The knowledgebased method computes sentence similarity based on the semantic information collected from knowledge bases. With the aid of a number of successful computational linguistic projects, many semantic knowledge bases are readily available, for example, WordNet, Spatial Date Transfer Standard, Gene Ontology, etc. Among them, the most widely used one is WordNet, which is organized by meanings and developed at Princeton University. Several methods computed word similarity by using WordNet, such as the Lesk method in (Banerjee and Pedersen, 2003), the lch method in (Leacock and Chodorow, 1998)and the wup method in (Wu and Palmer, 1994). Generally, although the knowledgebased methods heavily depend on the knowledge bases, they performed much better than the corpusbased methods in most cases. Therefore, in our STS system, we use a knowledge-based method to compute word similarity. The rest of this paper is organized as follows. Section 2 describes our system. Section 3 presents the results of our system. 2 System Description Usually, a sentence is composed of some nouns, verbs, adjectives, adverbs and/or some stop words. We found that these words carry a lot of information, especiall</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Leacock and Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database. The MIT Press, Cambridge,MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wu</author>
<author>M Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computional Linguistics,Morristown,</booktitle>
<location>NJ, USA.</location>
<contexts>
<context position="2422" citStr="Wu and Palmer, 1994" startWordPosition="359" endWordPosition="362">omputes sentence similarity based on the semantic information collected from knowledge bases. With the aid of a number of successful computational linguistic projects, many semantic knowledge bases are readily available, for example, WordNet, Spatial Date Transfer Standard, Gene Ontology, etc. Among them, the most widely used one is WordNet, which is organized by meanings and developed at Princeton University. Several methods computed word similarity by using WordNet, such as the Lesk method in (Banerjee and Pedersen, 2003), the lch method in (Leacock and Chodorow, 1998)and the wup method in (Wu and Palmer, 1994). Generally, although the knowledgebased methods heavily depend on the knowledge bases, they performed much better than the corpusbased methods in most cases. Therefore, in our STS system, we use a knowledge-based method to compute word similarity. The rest of this paper is organized as follows. Section 2 describes our system. Section 3 presents the results of our system. 2 System Description Usually, a sentence is composed of some nouns, verbs, adjectives, adverbs and/or some stop words. We found that these words carry a lot of information, especially the nouns and verbs. Although the adjecti</context>
<context position="5638" citStr="Wu and Palmer, 1994" startWordPosition="849" endWordPosition="852">tives and adverbs. WordNet groups these words into sets of synonyms called synsets, provides short definitions, and records the various semantic relations between these synsets. The synsets are interlinked by means of conceptual-semantic and lexical relations. WordNet also provides the most common relationships include Hyponym/Hypernym (i.e., is-a relationships) and Meronym/Holonym (i.e., part-of relationships). Nouns and verbs are organized into hierarchies based on the hyponymy/hypernym relation between synsets while adjectives and adverbs are not. In this paper, we adopt the wup method in (Wu and Palmer, 1994) to estimate the semantic similarity between two words, which estimates the semantic similarity between two words based on the depth of the two words in WordNet and the depth of their least common subsumer (LCS), where LCS is defined as the common ancestor deepest in the taxonomy. For example, given two words, w1 and w2, the semantic similarity s(w1,w2) is the function of their depth in the taxonomy and the depth of their least common subsumer. If d1 and d2 are the depth of w1 and w2 in WordNet, and h is the depth of their least common subsumer in WordNet, the semantic similarity can be writte</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Z.Wu and M.Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computional Linguistics,Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Satanjeev Banerjee</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Maximizing Semantic Relatedness to Perform Word Sense Disambiguation.</title>
<date>2005</date>
<marker>Pedersen, Banerjee, Patwardhan, 2005</marker>
<rawString>Ted Pedersen, Satanjeev Banerjee, Siddharth Patwardhan. 2005. Maximizing Semantic Relatedness to Perform Word Sense Disambiguation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>