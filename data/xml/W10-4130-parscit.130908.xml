<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9986165">
A Chinese Word Segmentation System Based on Structured
Support Vector Machine Utilization of Unlabeled Text Corpus
</title>
<author confidence="0.974075">
Chongyang Zhang
</author>
<affiliation confidence="0.9634012">
Anhui Province
Engineering Laboratory
of Speech and Language,
University of Science and
Technology of China
</affiliation>
<address confidence="0.760152">
cyzhang9
</address>
<email confidence="0.995356">
@mail.ustc.edu.cn
</email>
<author confidence="0.914513">
Zhigang Chen
</author>
<affiliation confidence="0.9445504">
Anhui Province
Engineering Laboratory
of Speech and Language,
University of Science and
Technology of China
</affiliation>
<address confidence="0.465835">
Chenzhigang
</address>
<email confidence="0.996819">
@ustc.edu
</email>
<author confidence="0.949664">
Guoping Hu
</author>
<affiliation confidence="0.9555786">
Anhui Province
Engineering Laboratory
of Speech and Language,
University of Science and
Technology of China
</affiliation>
<address confidence="0.513356">
Applecore
</address>
<email confidence="0.996761">
@ustc.edu
</email>
<sectionHeader confidence="0.996625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998606">
Character-based tagging method has
achieved great success in Chinese Word
Segmentation (CWS). This paper
proposes a new approach to improve the
CWS tagging accuracy by structured
support vector machine (SVM)
utilization of unlabeled text corpus. First,
character N-grams in unlabeled text
corpus are mapped into low-dimensional
space by adopting SOM algorithm. Then
new features extracted from these maps
and another kind of feature based on
entropy for each N-gram are integrated
into the structured SVM methods for
CWS. We took part in two tracks of the
Word Segmentation for Simplified
Chinese Text in bakeoff-2010: Closed
track and Open track. The test corpora
cover four domains: Literature,
Computer Science, Medicine and
Finance. Our system achieved good
performance, especially in the open
track on the domain of medicine, our
system got the highest score among 18
systems.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998902291139241">
In the last decade, many statistics-based
methods for automatic Chinese word
segmentation (CWS) have been proposed with
development of machine learning and statistical
method (Huang and Zhao, 2007). Especially,
character-based tagging method which was
proposed by Nianwen Xue (2003) achieves
great success in the second International
Chinese word segmentation Bakeoff in 2005
(Low et al., 2005). The character-based tagging
method formulates the CWS problem as a task
of predicting a tag for each character in the
sentence, i.e. every character is considered as
one of four different types in 4-tag set: B (begin
of word), M (middle of word), E (end of word),
and S (single-character word).
Most of these works train tagging models
only on limited labeled training sets, without
using any unsupervised learning outcomes from
unlabeled text. But in recent years, researchers
begin to exploit the value of enormous
unlabeled corpus for CWS, such as some
statistics information on co-occurrence of sub-
sequences in the whole text has been extracted
from unlabeled data and been employed as input
features for tagging model training (Zhao and
Kit , 2007).
Word clustering is a common method to
utilize unlabeled corpus in language processing
research to enhance the generalization ability,
such as part-of-speech clustering and semantic
clustering (Lee et al., 1999 and B Wang and H
Wang 2006). Character-based tagging method
usually employs N-gram features, where an N-
gram is an N-character segment of a string. We
believe that there are also semantic or
grammatical relationships between most of N-
grams and these relationships will be useful in
CWS. Intuitively, assuming the training data
contains the bigram “ 色 / 列 ”(The last two
characters of the word “Israel” in Chinese), not
contain the bigram “ 耳 / 其 ”(The last two
characters of the word “Turkey” in Chinese), if
we could cluster the two bigrams together
according to unlabeled corpus and employ it as
a feature for supervised training of tagging
model, then maybe we will know that there
should be a word boundary after “耳/其” though
we only find the existence of word boundary
after “ 色 / 列 ” in the training data. So we
investigate how to apply clustering method onto
unlabeled data for the purpose of improving
CWS accuracy in this paper.
This paper proposes a novel method of
using unlabeled data for CWS, which employs
Self-Organizing Map (SOM) (Kohonen 1982)
to organize Chinese character N-grams on a
two-dimensional array, named as “N-gram
cluster map” (NGCM), in which the character
N-grams similar in grammatical structure and
semantic meaning are organized in the same or
adjacent position. Two different arrays are built
based the N-gram’s preceding context and
succeeding context respectively because
normally N-gram is just part of Chinese word
and doesn’t share similar preceding and
succeeding context in the same time. Then
NGCM-based features are extracted and applied
to tagging model of CWS. Another kind of
feature based on entropy for each N-gram is
also introduced for improving the performance
of CWS.
The rest of this paper is organized as
follows: Section 2 describes our system; Section
3 describes structured SVM and the features
which are obtained from labeled corpus and also
unlabeled corpus; Section 4 shows experimental
results on Bakeoff-2010 and Section 5 gives our
conclusion.
</bodyText>
<sectionHeader confidence="0.986683" genericHeader="method">
2 System description
</sectionHeader>
<subsectionHeader confidence="0.998232">
2.1 Open track:
</subsectionHeader>
<bodyText confidence="0.999478214285714">
The architecture of our system for open track is
shown in Figure 1. For improving the cross-
domain performance, we train and test with
dictionary-based word segmentation outputs.
On large-scale unlabeled corpus we use Self-
Organizing Map (SOM) (Kohonen 1982) to
organize Chinese character N-grams on a two-
dimensional array, named as “N-gram cluster
map” (NGCM), in which the character N-grams
similar in grammatical structure and semantic
meaning are organized in the same or adjacent
position. Then new features are extracted from
these maps and integrated into the structured
SVM methods for CWS.
</bodyText>
<figureCaption confidence="0.997633">
Figure 1: Open track system
</figureCaption>
<subsectionHeader confidence="0.861465">
2.2 Closed track:
</subsectionHeader>
<figureCaption confidence="0.995243">
Figure 2: closed track system
</figureCaption>
<bodyText confidence="0.89741">
Because the large-scale unlabeled corpus is
forbidden to be used on closed track. We trained
the SOM only on the data provided by
</bodyText>
<figure confidence="0.998591434782609">
Structured SVM
Results
Model
test text
Dictionary Based
CWS
Training text
The large-scale
unlabeled corpus
Dictionary Based
CWS
Som
Labeled
data
NGCM
Labeled
data
NGCM Entropy
Som
Structured SVM
Training text test text
Statistic
Model
</figure>
<sectionHeader confidence="0.473784" genericHeader="evaluation">
Results
</sectionHeader>
<bodyText confidence="0.9967212">
organizers. To make up for the deficiency of the
sparse data on SOM, we add entropy-based
features (ETF) for every N-gram to structured
SVM model. The architecture of our system for
close track is shown in Figure 2.
</bodyText>
<sectionHeader confidence="0.97621" genericHeader="evaluation">
3 Learning algorithm
</sectionHeader>
<subsectionHeader confidence="0.99984">
3.1 Structured support vector machine
</subsectionHeader>
<bodyText confidence="0.999659733333333">
The structured support vector machine can learn
to predict structured y , such as trees sequences
or sets, from x based on large-margin approach.
We employ a structured SVM that can predict a
sequence of labels y = (y1,..., yT) for a given
observation sequences x=(x1,...,xT) , where
yt ∈ ∑ , ∑ is the label set for y.
There are two types of features in the
structured SVM: transition features (interactions
between neighboring labels along the chain),
emission features (interactions between
attributes of the observation vectors and a
specific label).we can represent the input-output
pairs via joint feature map (JFM)
where
</bodyText>
<equation confidence="0.991331">
Λ ≡
c ( ) ( ( , ), ( , ),..., ( , ))&apos;
y δ y y y y
δ δ y y
1 2 K
∈ {0K
,1} , y∈ {y1,y2, ... , yK} = ∑
</equation>
<bodyText confidence="0.6873265">
denotes an arbitrary feature representation
of the inputs. The sign
</bodyText>
<figure confidence="0.474859">
expresses tensor
product defined as
Rd
Rk
[
=
the length of an
observation sequence.
0 is a scaling factor
</figure>
<bodyText confidence="0.7785685">
which balances the two types of contributions.
Note that both transition features and
</bodyText>
<equation confidence="0.927386166666667">
emission features can
φ(x)
&amp;quot;⊗&amp;quot;
⊗:
×
→R,
dk
] i (j 1)d
a b + −
⊗
[a]i[b]j. T is
η≥
</equation>
<bodyText confidence="0.9859852">
be extended by including
higher-order interdependencies of labels (e.g.
Λc (yt) ⊗Λc (yt+ 1) ⊗Λc (yt+2) ),by including
input features from a window centered at the
current position (e.g. replacing ( )
</bodyText>
<equation confidence="0.968965555555556">
φ x with
t
φ(xt−r, , xt, xt+r) )or by combining higher-
order output features with input features (e.g.
1
∑ ⊗ Λ ⊗ Λ
( ) ( ) ( )
t c t c t
t φ x y y + )
</equation>
<bodyText confidence="0.893281">
The w-parametrized discriminant function
Training the parameters can
be formulated
as the following optimization problem.
</bodyText>
<equation confidence="0.997594571428571">
I
2 n
min 1 w, w + C
ξiw,ξ
i=1
s.t.∀i,∀y∈ Y :
w,ψi(xi,yi)− ψi(xi,y) ≥ Δ(yi,y)
</equation>
<bodyText confidence="0.99996025">
where n is the number of the training set, ξi is
a slack variable , C ≥ 0 is a constant
controlling the tradeoff between training error
minimization and margin maximization,
</bodyText>
<equation confidence="0.9494735">
Δ(y, y) is the loss function ,usually the
1
</equation>
<bodyText confidence="0.941888">
number of misclassified tags in the sentence.
</bodyText>
<subsectionHeader confidence="0.995712">
3.2 Features set for tagging model
</subsectionHeader>
<bodyText confidence="0.946764">
For a training sample denoted as
</bodyText>
<equation confidence="0.982689">
x = x x and
( ,..., )
1 T y = (y1,..., yT) . We chose
</equation>
<bodyText confidence="0.999625428571428">
first-order interdependencies of labels to be
transition features, and dependencies between
labels and N-grams (n=1, 2, 3, 4) at current
position in observed input sequence to be
emission features.
So our JFM is the concatenation of the
follow vectors
</bodyText>
<equation confidence="0.9930566">
T
−1
∑ c t 1
c t
y y +
Λ ⊗ Λ
( ) ( )
φ(xt+m)⊗Λ c(yt),m∈ {−1,0,1}
φ(xt+mxt+m+1)⊗Λ c(yt),m∈ {−2,−1,0,1}
F: X × Y→ R interpreted as measuring the
</equation>
<bodyText confidence="0.8656814">
compatibility of x and y is defined as:
F ( x , y ; w ) = w , ψ ( x , y )
So we can maximize this function over the
response variable to make a prediction
f (x ) arg max ( , , )
</bodyText>
<equation confidence="0.969414384615384">
= F x y w
y Y
∈
1,i= j
0 ≠j
(xt+m−1xt+mxt+m+1) ⊗Λc (yt),
− −
∈ { 2, 1,0,1,2}
m
T
⎛ ⎞
⎜∑φ(x t) ⊗Λc (yt) ⎟
t 1
= ⎜ = ⎟
⎜ T−1 ⎟
⎜ ηyy+
∑Λ⎝ t=1 ⎠
ψ ( , )
x y
Kronecker delta ,
δ δ =
i j,
{
n
− ξi
t
=
1
T
∑
t
=
1
T
∑
t=
1
T
∑ φ
</equation>
<page confidence="0.419588">
t=1
</page>
<figure confidence="0.917304217391305">
T
φ
L
t
=1
m
清/晨
傍/晚
下/午
三/餐
中/国
埃/塞
英/格
俄/罗
巴/基
(xt+m−1xt+mxt+m+1xt+m+2) ⊗Ac (yt )
− − −
E { 3, 2, 1,0,1,2}
0,0
1,0 2,0
14,0
0, 1,1 2,1
14,1
</figure>
<figureCaption confidence="0.8960785">
Figure 3 shows the transition features and
the emission features of N-grams (n=1, 2) at y3.
</figureCaption>
<bodyText confidence="0.967125">
The emission features of 3-grams and 4-grams
are not shown here because of the large number
of the dependencies.
</bodyText>
<equation confidence="0.8613025">
y1 y2 y3 y4
x5
</equation>
<figureCaption confidence="0.97753">
Figure 3: the transition features and the
emission features at y3 for structured SVM
</figureCaption>
<subsectionHeader confidence="0.979718">
3.3 SOM-based N-gram cluster maps
</subsectionHeader>
<bodyText confidence="0.990073965517242">
and the NGCM mapping feature
The Self-Organizing Map (SOM) (Kohonen
1982), sometimes called Kohonen map, was
developed by Teuvo Kohonen in the early
1980s.
Self-organizing semantic maps (Ritter and
Kohonen 1989, 1990) are SOMs that have been
organized according to word similarities,
measured by the similarity of the short contexts
of the words. Our algorithm of building N-gram
cluster maps is similar to self-organizing
semantic maps. Because normally N-gram is
just part of Chinese word and do not share
similar preceding and succeeding context in the
same time, so we build two different maps
according to the preceding context and the
succeeding context of N-gram individually. In
the end we build two NGCMs: NGCMP
(NGCM according to preceding context) and
NGCMS (NGCM according to succeeding
context).
Due to the limitation of our computer and
time we only get two 15 x 1 5 size 2GCMs for
open track system from large-scale unlabeled
corpus which was obtained easily from websites
like Sohu, Netease, Sina and People Daily.
The 2GCMP and 2GCMS we got for the
open track task are shown in Figure 4 and
Figure 5 respectively.
</bodyText>
<figure confidence="0.8417935">
0,2 1,2 2,2
0,14 1,14 2,14
</figure>
<figureCaption confidence="0.991427">
Figure 4: 2GCMP
</figureCaption>
<figure confidence="0.993775454545454">
丈/夫
父/亲
已/婚
女/性
色/列
耳/其
苏/丹
中/国
美/国
...
...
</figure>
<figureCaption confidence="0.999958">
Figure 5: 2GCMS
</figureCaption>
<bodyText confidence="0.999880538461538">
After checking the results, we find that the
2GCMS have following characters:1) most of
the meaningless bigrams that contain characters
from more than one word, such as the bigram &amp;quot;
京天&amp;quot; in &amp;quot;...北京天坛...&amp;quot; , are organized into the
same neurons in the map, 2) most of the first or
last bigrams of the country names are organized
into a few adjacent neurons, such as “色/列”,
“耳/其”, “中/国” and “美/国”in 2GCMS , “巴/
基”, “埃/塞”, “英/格”, “俄/罗” , and “中/国” in
2GCMP.
Two 20 x 1 size 2GCMs are trained for the
closed track system only on the data provided
by organizers. The results are not as good as the
results of the 15 x 1 5 size 2GCMs because of
the less training data. The second character
described above is no longer apparent as well as
the 15 x 1 5 size 2GCMs, but it still kept the first
character.
Then we adopt the position of the neurons
which current N-gram mapped in the NGCM as
a new feature. So every feature has D
dimensions (D equals to the dimension of the
NGCM, every dimension is corresponding to
the coordinate value in the NGCM). In this way,
N-gram which is originally represented as a
</bodyText>
<figure confidence="0.908356923076923">
y5
x1 x2 x3 x4
冲/突
误/会
抗/体
诊/治
14,14
14,2
美/国
以/色
日/本
印/度
0,0
1,0 2,0
14,0
日/本
德/国
印/尼
班/牙
0,1 1,1 2,1
14,1
...
0,2 1,2 2,2
14,2
同/志
士/兵
美/军
台/胞
0,14 1,14 2,14
14,14
T
∑
ϕ 2GCMS (xt+mxt+m+1)⊗Λ c(yt),m∈ {−2,−1}
∑
H(P |N=xNgram)⊗Λc(yt),
T
∑
H(S|N=xNgram)⊗Λc(yt),
1
</figure>
<equation confidence="0.98644544">
t
1
t
T
xNgram ∈
x x x x x x x x x
2 1 − − −
3 2 1 − − − −
4 3 2 1
{ ,
− −
t t t t t t t t t
, }
T
∑
I(N = xNgram) ⊗ Λc (yt )
T
1
t
∑
η 2GCMS (xt+mxt+m+1)⊗Λ c(yt),m∈ {−2,−1}
∑
η2GCMP (xt+mxt+m+1) ⊗ Λ c (yt ), m ∈ {0,1}
−∑
p(xt  |xNgram) log p(xt  |xNgram)
</equation>
<bodyText confidence="0.9995528">
high dimensional vector based on its context is
mapped into a very low-dimensional space. We
call it NGCM mapping feature. So our previous
JFM in section 3.2 is concatenated with the
following features:
</bodyText>
<equation confidence="0.526476">
x ∈
</equation>
<bodyText confidence="0.949766166666667">
Ngram all the ngrams used in section 3.2
Where P and S denote the set of the preceding
and succeeding characters respectively. The
entropy: H(X  |N = xNgram) =
where ϕ 2GCMS( x) and ϕ2GCMP( x)
∈ {0,1,...,14} denote the NGCM mapping
2
feature from 2GCMS and 2GCMP respectively.
ηNGCM( x) denotes the quantization error of
current N-gram x on its NGCM.
As an example, the process of import features
from NGCMs at y3 is presented in Figure 6.
</bodyText>
<figureCaption confidence="0.966771">
Figure 6: Using 2GCMS and 2GCMP as input
to structured SVM
</figureCaption>
<equation confidence="0.7741124">
2GCMS 2GCMP
y1 y2 y3 y4
x1 x2 x3 x4
y5
x5
</equation>
<subsectionHeader confidence="0.336477">
3.4 Entropy-based features
</subsectionHeader>
<bodyText confidence="0.932976571428571">
On closed track, the entropy of the preceding
and succeeding characters conditional on the N-
gram and also the self-information of the N-
gram are used as features for the structured
SVM methods. Then our previous JFM in
section 3.2 is concatenated with the following
features:
</bodyText>
<figure confidence="0.977026428571429">
X xt
∈
The self-information of the N-gram N = x:
Ngram
I (xNgram) = −log
p(xNgram )
4 Applications and Experiments
</figure>
<subsectionHeader confidence="0.617288">
4.1 Text Preprocessing
</subsectionHeader>
<bodyText confidence="0.9966648">
Text is usually mixed up with numerical or
alphabetic characters in Chinese natural
language, such as “我在 office 上班到晚上 9
点”. These numerical or alphabetic characters
are barely segmented in CWS. Hence, we treat
these symbols as a whole “character” according
to the following two preprocessing steps. First
replace one alphabetic character to four
continuous alphabetic characters with E1 to E4
respectively, five or more alphabetic characters
with E5. Then replace one numerical number to
four numerical numbers with N1 to N4 and five
or more numerical numbers with N5. After text
preprocessing, the above examples will be “我
在 E5 上班到晚上 N1 点”.
</bodyText>
<subsectionHeader confidence="0.77625">
4.2 Character-based tagging method
</subsectionHeader>
<bodyText confidence="0.957956">
for CWS
Previous works show that 6-tag set achieved
a better CWS performance (Zhao et al.,
2006). Thus, we opt for this tag set. This 6-
tag set adds ‘B2’ and ’B3’ to 4-tag set
which stand for the type of the second and
the third character in a Chinese word
respectively. For example, the tag sequence
for the sentence “上海世博会/将/持续/半
</bodyText>
<equation confidence="0.85130680952381">
t=
xNgram ∈
T
1
1
T
t=
t
1
t t
1 2 1 2 3 1 2 3 4
t t
{ x x x x x x x x x
t t
+ + + + + + + + +
t t t
, , }
ϕ2GCMP (xt+mxt+m+1) ⊗ Λ c (yt ), m ∈ {0,1}
∑
1
t=
</equation>
<bodyText confidence="0.746642">
年(Shanghai World Expo / will / last / six
months)” will be “B B2 B3 M E S B E B E”.
</bodyText>
<subsectionHeader confidence="0.797266">
4.3 Results in the bakeoff-2010
</subsectionHeader>
<bodyText confidence="0.972261173913044">
We use hmm
svm version 3.1 to build our
structured SVM models. The cut-off threshold is
set to 2. The precision parameter is set to 0.1.
The tradeoff between training error
minimization and margin maximization is set to
1000.
We took part in two tracks of the Word
Segmentation for Simplified Chinese Text in
bakeoff-2010: c (Closed track), o (Open track).
The test corpora cover four domains: A
(Literature), B (Computer Science), C
(Medicine), D (Finance).
Precision(P),Recall(R),F-measure(F),Out-
Of-Vocabulary Word Recall(OOV RR) and In-
Vocabulary Word Recall(IV RR) are adopted to
measure the performance of word segmentation
system.
Table 1 shows the results of our system on
the word segmentation task for simplified
Chinese text in bakeoff-2010. Table 2 shows the
comparision between our system results and
best results in bakeoff-2010.
</bodyText>
<table confidence="0.999729111111111">
R P F1 OOV RR IV RR
A c 0.932 0.935 0.933 0.654 0.953
o 0.942 0.943 0.942 0.702 0.959
B c 0.935 0.934 0.935 0.792 0.961
o 0.948 0.946 0.947 0.812 0.973
C c 0.937 0.934 0.936 0.761 0.959
o 0.941 0.935 0.938 0.787 0.96
D c 0.955 0.956 0.955 0.848 0.965
o 0.948 0.955 0.951 0.853 0.957
</table>
<tableCaption confidence="0.998175">
Table 1: The results of our systems
</tableCaption>
<table confidence="0.998667333333333">
F1(Bakeoff-2010) F1(Our system)
A c 0.946 0.933
o 0.955 0.942
B c 0.951 0.935
o 0.95 0.947
C c 0.939 0.936
o 0.938 0.938
D c 0.959 0.955
o 0.96 0.951
</table>
<tableCaption confidence="0.7254275">
Tabel 2: The comparision between our system
results and best results in bakeoff-2010
</tableCaption>
<bodyText confidence="0.999788">
It is obvious that our systems are stable and
reliable even in the domain of medicine when
the F-measure of the best results was decreased.
Our open track system performs better than
closed track system, demonstrating the benefit
of the dictionary-based word segmentation
outputs and the NGCMs which are training on
large-scale unlabeled corpus.
</bodyText>
<sectionHeader confidence="0.998755" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999940727272727">
This paper proposes a new approach to improve
the CWS tagging accuracy by structured support
vector machine (SVM) utilization of unlabeled
text corpus. We use SOM to organize Chinese
character N-grams on a two-dimensional array,
so that the N-grams similar in grammatical
structure and semantic meaning are organized in
the same or adjacent position. Then new
features extracted from these maps and another
kind of feature based on entropy for each N-
gram are integrated into the structured SVM
methods for CWS. Our system achieved good
performance, especially in the open track on the
domain of medicine, our system got the
highest score among 18 systems.
In future work, we will try to organizing all
the N-grams on a much larger array, so that
every neuron will be labeled by a single N-gram.
The ultimate objective is to reduce the
dimension of input features for supervised CWS
learning by replacing N-gram features with two-
dimensional NGCM mapping features.
</bodyText>
<sectionHeader confidence="0.999312" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996889360465116">
B.Wang, H.Wang 2006.A Comparative Study on
Chinese Word Clustering. Computer Processing
of Oriental Languages. Beyond the Orient: The
Research Challenges Ahead, pages 157-164
Chang-Ning Huang and Hai Zhao. 2007. Chinese
word segmentation: A decade review. Journal of
Chinese Information Processing, 21(3):8–20.
Chung-Hong Lee &amp; Hsin-Chang Yang.1999, A Web
Text Mining Approach Based on Self-Organizing
Map, ACM-library
G.Bakir, T.Hofmann, B.Scholkopf, A.Smola, B.
Taskar, and S. V. N. Vishwanathan, editors. 2007
Predicting Structured Data. MIT Press,
Cambridge, Massachusetts.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-
Liang Lu. 2006. Effective tag set selection
inChinese word segmentation via conditional
random field modeling. In Proceedings of
PACLIC-20, pages 87–94. Wuhan, China.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006.An
improved Chinese word segmentation system with
conditional random field. In SIGHAN-5, pages
162–165, Sydney, Australia, July 22-23.
Hai Zhao and Chunyu Kit. 2007. Incorporating
global information into supervised learning for
Chinese word segmentation. In PACLING-2007,
pages 66–74, Melbourne,Australia, September 19-
21.
H.Ritter, and T.Kohonen, 1989. Self-organizing
semantic maps. Biological Cybernetics, vol. 61,
no. 4, pp. 241-254.
I.Tsochantaridis,T.Joachims,T.Hofmann,and Y.Altun.
2005. Large Margin Methods for Structured and
Interdependent Output Variables, Journal of
Machine Learning Research (JMLR),
6(Sep):1453-1484.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan
Guo.2005. A maximum entropy approach to
Chinese word segmentation. In Proceedings of the
Fourth SIGHAN Workshop on Chinese Language
Processing, pages 161–164. Jeju Island,Korea.
J.Lafferty,A.McCallum, F.Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data. In
Proceedings of the International Conference on
Machine Learning (ICML). San Francisco:
Morgan Kaufmann Publishers, 282−289.
Nianwen Xue and Susan P. Converse., 2002,
Combining Classifiers for Chinese Word
Segmentation, In Proceedings of First SIGHAN
Workshop on Chinese Language Processing.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29–48.
R.Sproat and T.Emerson. 2003.The first
international Chinese word segmentation bakeoff.
In The Second SIGHAN Workshop on Chinese
Language Processing, pages 133–143.Sapporo,
Japan.
S.Haykin, 1994. Neural Networks: A Comprehensive
Foundation. NewYork: MacMillan.
T.Joachims, T.Finley, Chun-Nam Yu. 2009, Cutting-
Plane Training of Structural SVMs, Machine
Learning Journal,77(1):27-59.
http://www.cs.cornell.edu/People/tj/svm_light/sv
m_hmm.html
T.Honkela, 1997. Self-Organizing Maps in Natural
Language Processing. PhD thesis, Helsinki
University of Technology, Department of
Computer Science and Engineering, Laboratory of
Computer and Information Science.
T.Kohonen. 1982.Self-organized formation of
topologically correct feature maps. Biological
Cybernetics, 43, pp. 59-69.
T.Kohonen., J.Hynninen, J.Kangas, J.Laaksonen,
1996 ,SOM_PAK: The Self-Organizing Map
Program Package,Technical Report A31,
Helsinki University of Technology ,
http://www.cis.hut.fi/nnrc/nnrc-programs.html
Y.Altun, I.Tsochantaridis, T.Hofmann. 2003. Hidden
Markov Support Vector Machines. In Proceedings
of International Conference on Machine Learning
(ICML).
T.Joachims. 2008 . svm Sequence Tagging with
hmm
Structural Support Vector Machines,
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.002026">
<title confidence="0.831716666666667">A Chinese Word Segmentation System Based on Structured Support Vector Machine Utilization of Unlabeled Text Corpus Chongyang</title>
<author confidence="0.235576">Anhui</author>
<affiliation confidence="0.853575">Engineering of Speech and University of Science Technology of China</affiliation>
<address confidence="0.668623">cyzhang9</address>
<email confidence="0.839397">@mail.ustc.edu.cn</email>
<author confidence="0.314943">Zhigang</author>
<affiliation confidence="0.7886806">Anhui Engineering of Speech and University of Science Technology of China</affiliation>
<email confidence="0.999108">@ustc.edu</email>
<affiliation confidence="0.682083666666667">Guoping Anhui Engineering of Speech and University of Science Technology of China</affiliation>
<email confidence="0.999795">@ustc.edu</email>
<abstract confidence="0.993611230769231">Character-based tagging method has achieved great success in Chinese Word Segmentation (CWS). This paper proposes a new approach to improve the CWS tagging accuracy by structured support vector machine (SVM) utilization of unlabeled text corpus. First, character N-grams in unlabeled text corpus are mapped into low-dimensional space by adopting SOM algorithm. Then new features extracted from these maps and another kind of feature based on entropy for each N-gram are integrated into the structured SVM methods for CWS. We took part in two tracks of the Word Segmentation for Simplified Chinese Text in bakeoff-2010: Closed track and Open track. The test corpora cover four domains: Literature, Computer Science, Medicine and Finance. Our system achieved good performance, especially in the open track on the domain of medicine, our system got the highest score among 18 systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>B Wang</author>
</authors>
<title>H.Wang 2006.A Comparative Study on Chinese Word Clustering. Computer Processing of Oriental Languages. Beyond the Orient: The Research Challenges Ahead,</title>
<pages>157--164</pages>
<marker>Wang, </marker>
<rawString>B.Wang, H.Wang 2006.A Comparative Study on Chinese Word Clustering. Computer Processing of Oriental Languages. Beyond the Orient: The Research Challenges Ahead, pages 157-164</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang-Ning Huang</author>
<author>Hai Zhao</author>
</authors>
<title>Chinese word segmentation: A decade review.</title>
<date>2007</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="1651" citStr="Huang and Zhao, 2007" startWordPosition="235" endWordPosition="238">egrated into the structured SVM methods for CWS. We took part in two tracks of the Word Segmentation for Simplified Chinese Text in bakeoff-2010: Closed track and Open track. The test corpora cover four domains: Literature, Computer Science, Medicine and Finance. Our system achieved good performance, especially in the open track on the domain of medicine, our system got the highest score among 18 systems. 1 Introduction In the last decade, many statistics-based methods for automatic Chinese word segmentation (CWS) have been proposed with development of machine learning and statistical method (Huang and Zhao, 2007). Especially, character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al., 2005). The character-based tagging method formulates the CWS problem as a task of predicting a tag for each character in the sentence, i.e. every character is considered as one of four different types in 4-tag set: B (begin of word), M (middle of word), E (end of word), and S (single-character word). Most of these works train tagging models only on limited labeled training sets, without using any unsuperv</context>
</contexts>
<marker>Huang, Zhao, 2007</marker>
<rawString>Chang-Ning Huang and Hai Zhao. 2007. Chinese word segmentation: A decade review. Journal of Chinese Information Processing, 21(3):8–20.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chung-Hong Lee</author>
</authors>
<title>Hsin-Chang Yang.1999, A Web Text Mining Approach Based on Self-Organizing Map,</title>
<location>ACM-library</location>
<marker>Lee, </marker>
<rawString>Chung-Hong Lee &amp; Hsin-Chang Yang.1999, A Web Text Mining Approach Based on Self-Organizing Map, ACM-library</rawString>
</citation>
<citation valid="true">
<title>Predicting Structured Data.</title>
<date>2007</date>
<editor>G.Bakir, T.Hofmann, B.Scholkopf, A.Smola, B. Taskar, and S. V. N. Vishwanathan, editors.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>2007</marker>
<rawString>G.Bakir, T.Hofmann, B.Scholkopf, A.Smola, B. Taskar, and S. V. N. Vishwanathan, editors. 2007 Predicting Structured Data. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>BaoLiang Lu</author>
</authors>
<title>Effective tag set selection inChinese word segmentation via conditional random field modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of PACLIC-20,</booktitle>
<pages>87--94</pages>
<location>Wuhan, China.</location>
<contexts>
<context position="14277" citStr="Zhao et al., 2006" startWordPosition="2531" endWordPosition="2534">aracters are barely segmented in CWS. Hence, we treat these symbols as a whole “character” according to the following two preprocessing steps. First replace one alphabetic character to four continuous alphabetic characters with E1 to E4 respectively, five or more alphabetic characters with E5. Then replace one numerical number to four numerical numbers with N1 to N4 and five or more numerical numbers with N5. After text preprocessing, the above examples will be “我 在 E5 上班到晚上 N1 点”. 4.2 Character-based tagging method for CWS Previous works show that 6-tag set achieved a better CWS performance (Zhao et al., 2006). Thus, we opt for this tag set. This 6- tag set adds ‘B2’ and ’B3’ to 4-tag set which stand for the type of the second and the third character in a Chinese word respectively. For example, the tag sequence for the sentence “上海世博会/将/持续/半 t= xNgram ∈ T 1 1 T t= t 1 t t 1 2 1 2 3 1 2 3 4 t t { x x x x x x x x x t t + + + + + + + + + t t t , , } ϕ2GCMP (xt+mxt+m+1) ⊗ Λ c (yt ), m ∈ {0,1} ∑ 1 t= 年(Shanghai World Expo / will / last / six months)” will be “B B2 B3 M E S B E B E”. 4.3 Results in the bakeoff-2010 We use hmm svm version 3.1 to build our structured SVM models. The cut-off threshold is se</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and BaoLiang Lu. 2006. Effective tag set selection inChinese word segmentation via conditional random field modeling. In Proceedings of PACLIC-20, pages 87–94. Wuhan, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
</authors>
<title>improved Chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In SIGHAN-5,</booktitle>
<pages>162--165</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="14277" citStr="Zhao et al., 2006" startWordPosition="2531" endWordPosition="2534">aracters are barely segmented in CWS. Hence, we treat these symbols as a whole “character” according to the following two preprocessing steps. First replace one alphabetic character to four continuous alphabetic characters with E1 to E4 respectively, five or more alphabetic characters with E5. Then replace one numerical number to four numerical numbers with N1 to N4 and five or more numerical numbers with N5. After text preprocessing, the above examples will be “我 在 E5 上班到晚上 N1 点”. 4.2 Character-based tagging method for CWS Previous works show that 6-tag set achieved a better CWS performance (Zhao et al., 2006). Thus, we opt for this tag set. This 6- tag set adds ‘B2’ and ’B3’ to 4-tag set which stand for the type of the second and the third character in a Chinese word respectively. For example, the tag sequence for the sentence “上海世博会/将/持续/半 t= xNgram ∈ T 1 1 T t= t 1 t t 1 2 1 2 3 1 2 3 4 t t { x x x x x x x x x t t + + + + + + + + + t t t , , } ϕ2GCMP (xt+mxt+m+1) ⊗ Λ c (yt ), m ∈ {0,1} ∑ 1 t= 年(Shanghai World Expo / will / last / six months)” will be “B B2 B3 M E S B E B E”. 4.3 Results in the bakeoff-2010 We use hmm svm version 3.1 to build our structured SVM models. The cut-off threshold is se</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, and Mu Li. 2006.An improved Chinese word segmentation system with conditional random field. In SIGHAN-5, pages 162–165, Sydney, Australia, July 22-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Incorporating global information into supervised learning for Chinese word segmentation.</title>
<date>2007</date>
<booktitle>In PACLING-2007,</booktitle>
<pages>66--74</pages>
<location>Melbourne,Australia,</location>
<marker>Zhao, Kit, 2007</marker>
<rawString>Hai Zhao and Chunyu Kit. 2007. Incorporating global information into supervised learning for Chinese word segmentation. In PACLING-2007, pages 66–74, Melbourne,Australia, September 19-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ritter</author>
<author>T Kohonen</author>
</authors>
<title>Self-organizing semantic maps.</title>
<date>1989</date>
<journal>Biological Cybernetics,</journal>
<volume>61</volume>
<pages>241--254</pages>
<contexts>
<context position="9679" citStr="Ritter and Kohonen 1989" startWordPosition="1678" endWordPosition="1681">+m+2) ⊗Ac (yt ) − − − E { 3, 2, 1,0,1,2} 0,0 1,0 2,0 14,0 0, 1,1 2,1 14,1 Figure 3 shows the transition features and the emission features of N-grams (n=1, 2) at y3. The emission features of 3-grams and 4-grams are not shown here because of the large number of the dependencies. y1 y2 y3 y4 x5 Figure 3: the transition features and the emission features at y3 for structured SVM 3.3 SOM-based N-gram cluster maps and the NGCM mapping feature The Self-Organizing Map (SOM) (Kohonen 1982), sometimes called Kohonen map, was developed by Teuvo Kohonen in the early 1980s. Self-organizing semantic maps (Ritter and Kohonen 1989, 1990) are SOMs that have been organized according to word similarities, measured by the similarity of the short contexts of the words. Our algorithm of building N-gram cluster maps is similar to self-organizing semantic maps. Because normally N-gram is just part of Chinese word and do not share similar preceding and succeeding context in the same time, so we build two different maps according to the preceding context and the succeeding context of N-gram individually. In the end we build two NGCMs: NGCMP (NGCM according to preceding context) and NGCMS (NGCM according to succeeding context). D</context>
</contexts>
<marker>Ritter, Kohonen, 1989</marker>
<rawString>H.Ritter, and T.Kohonen, 1989. Self-organizing semantic maps. Biological Cybernetics, vol. 61, no. 4, pp. 241-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims I Tsochantaridis</author>
<author>T Hofmann</author>
<author>Y Altun</author>
</authors>
<title>Large Margin Methods for Structured and Interdependent Output Variables,</title>
<date>2005</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>6--1453</pages>
<marker>Tsochantaridis, Hofmann, Altun, 2005</marker>
<rawString>I.Tsochantaridis,T.Joachims,T.Hofmann,and Y.Altun. 2005. Large Margin Methods for Structured and Interdependent Output Variables, Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jin Kiat Low</author>
</authors>
<title>Hwee Tou Ng, and Wenyuan Guo.2005. A maximum entropy approach to Chinese word segmentation.</title>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<note>Jeju Island,Korea.</note>
<marker>Low, </marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.2005. A maximum entropy approach to Chinese word segmentation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 161–164. Jeju Island,Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum J Lafferty</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Francisco:</location>
<marker>Lafferty, Pereira, 2001</marker>
<rawString>J.Lafferty,A.McCallum, F.Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning (ICML). San Francisco: Morgan Kaufmann Publishers, 282−289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Susan P Converse</author>
</authors>
<title>Combining Classifiers for Chinese Word Segmentation,</title>
<date>2002</date>
<booktitle>In Proceedings of First SIGHAN Workshop on Chinese Language Processing.</booktitle>
<marker>Xue, Converse, 2002</marker>
<rawString>Nianwen Xue and Susan P. Converse., 2002, Combining Classifiers for Chinese Word Segmentation, In Proceedings of First SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1736" citStr="Xue (2003)" startWordPosition="248" endWordPosition="249">tion for Simplified Chinese Text in bakeoff-2010: Closed track and Open track. The test corpora cover four domains: Literature, Computer Science, Medicine and Finance. Our system achieved good performance, especially in the open track on the domain of medicine, our system got the highest score among 18 systems. 1 Introduction In the last decade, many statistics-based methods for automatic Chinese word segmentation (CWS) have been proposed with development of machine learning and statistical method (Huang and Zhao, 2007). Especially, character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al., 2005). The character-based tagging method formulates the CWS problem as a task of predicting a tag for each character in the sentence, i.e. every character is considered as one of four different types in 4-tag set: B (begin of word), M (middle of word), E (end of word), and S (single-character word). Most of these works train tagging models only on limited labeled training sets, without using any unsupervised learning outcomes from unlabeled text. But in recent years, researchers begin to</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
</authors>
<title>T.Emerson. 2003.The first international Chinese word segmentation bakeoff.</title>
<date></date>
<booktitle>In The Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>133--143</pages>
<marker>Sproat, </marker>
<rawString>R.Sproat and T.Emerson. 2003.The first international Chinese word segmentation bakeoff. In The Second SIGHAN Workshop on Chinese Language Processing, pages 133–143.Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Haykin</author>
</authors>
<title>Neural Networks: A Comprehensive Foundation.</title>
<date>1994</date>
<publisher>NewYork: MacMillan.</publisher>
<marker>Haykin, 1994</marker>
<rawString>S.Haykin, 1994. Neural Networks: A Comprehensive Foundation. NewYork: MacMillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Finley T Joachims</author>
<author>Chun-Nam Yu</author>
</authors>
<title>CuttingPlane Training of Structural SVMs,</title>
<date>2009</date>
<journal>Machine Learning Journal,77(1):27-59.</journal>
<marker>Joachims, Yu, 2009</marker>
<rawString>T.Joachims, T.Finley, Chun-Nam Yu. 2009, CuttingPlane Training of Structural SVMs, Machine Learning Journal,77(1):27-59.</rawString>
</citation>
<citation valid="false">
<note>http://www.cs.cornell.edu/People/tj/svm_light/sv m_hmm.html</note>
<marker></marker>
<rawString>http://www.cs.cornell.edu/People/tj/svm_light/sv m_hmm.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Honkela</author>
</authors>
<title>Self-Organizing Maps in Natural Language Processing.</title>
<date>1997</date>
<tech>PhD thesis,</tech>
<institution>Helsinki University of Technology, Department of Computer Science and Engineering, Laboratory of Computer and Information Science.</institution>
<marker>Honkela, 1997</marker>
<rawString>T.Honkela, 1997. Self-Organizing Maps in Natural Language Processing. PhD thesis, Helsinki University of Technology, Department of Computer Science and Engineering, Laboratory of Computer and Information Science.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Kohonen</author>
</authors>
<title>1982.Self-organized formation of topologically correct feature maps.</title>
<journal>Biological Cybernetics,</journal>
<volume>43</volume>
<pages>59--69</pages>
<marker>Kohonen, </marker>
<rawString>T.Kohonen. 1982.Self-organized formation of topologically correct feature maps. Biological Cybernetics, 43, pp. 59-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hynninen T Kohonen</author>
<author>J Laaksonen J Kangas</author>
</authors>
<title>SOM_PAK: The Self-Organizing Map Program</title>
<date>1996</date>
<tech>Package,Technical Report A31,</tech>
<institution>Helsinki University of Technology</institution>
<note>http://www.cis.hut.fi/nnrc/nnrc-programs.html</note>
<marker>Kohonen, Kangas, 1996</marker>
<rawString>T.Kohonen., J.Hynninen, J.Kangas, J.Laaksonen, 1996 ,SOM_PAK: The Self-Organizing Map Program Package,Technical Report A31, Helsinki University of Technology , http://www.cis.hut.fi/nnrc/nnrc-programs.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis Y Altun</author>
<author>T Hofmann</author>
</authors>
<title>Hidden Markov Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of International Conference on Machine Learning (ICML).</booktitle>
<marker>Altun, Hofmann, 2003</marker>
<rawString>Y.Altun, I.Tsochantaridis, T.Hofmann. 2003. Hidden Markov Support Vector Machines. In Proceedings of International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>svm Sequence Tagging with hmm Structural Support Vector Machines,</title>
<date>2008</date>
<marker>Joachims, 2008</marker>
<rawString>T.Joachims. 2008 . svm Sequence Tagging with hmm Structural Support Vector Machines,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>