<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002642">
<title confidence="0.993065">
Compositional Distributional Semantics with Long Short Term Memory
</title>
<author confidence="0.99359">
Phong Le and Willem Zuidema
</author>
<affiliation confidence="0.997136">
Institute for Logic, Language and Computation
University of Amsterdam, the Netherlands
</affiliation>
<email confidence="0.996197">
{p.le,zuidema}@uva.nl
</email>
<sectionHeader confidence="0.996623" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999396153846154">
We are proposing an extension of the recur-
sive neural network that makes use of a vari-
ant of the long short-term memory architec-
ture. The extension allows information low
in parse trees to be stored in a memory reg-
ister (the ‘memory cell’) and used much later
higher up in the parse tree. This provides a so-
lution to the vanishing gradient problem and
allows the network to capture long range de-
pendencies. Experimental results show that
our composition outperformed the traditional
neural-network composition on the Stanford
Sentiment Treebank.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967222222222">
Moving from lexical to compositional semantics in
vector-based semantics requires answers to two dif-
ficult questions: (i) what is the nature of the com-
position functions (given that the lambda calculus
for variable binding is no longer applicable), and (ii)
how do we learn the parameters of those functions
(if they have any) from data? A number of classes of
functions have been proposed in answer to the first
question, including simple linear functions like vec-
tor addition (Mitchell and Lapata, 2009), non-linear
functions like those defined by multi-layer neural
networks (Socher et al., 2010), and vector matrix
multiplication and tensor linear mapping (Baroni et
al., 2013). The matrix and tensor-based functions
have the advantage of allowing a relatively straight-
forward comparison with formal semantics, but the
fact that multi-layer neural networks with non-linear
activation functions like sigmoid can approximate
</bodyText>
<page confidence="0.968926">
10
</page>
<bodyText confidence="0.999141029411765">
any continuous function (Cybenko, 1989) already
make them an attractive choice.
In trying to answer the second question, the ad-
vantages of approaches based on neural network ar-
chitectures, such as the recursive neural network
(RNN) model (Socher et al., 2013b) and the con-
volutional neural network model (Kalchbrenner et
al., 2014), are even clearer. Models in this paradigm
can take advantage of general learning procedures
based on back-propagation, and with the rise of
‘deep learning’, of a variety of efficient algorithms
and tricks to further improve training.
Since the first success of the RNN model (Socher
et al., 2011b) in constituent parsing, two classes of
extensions have been proposed. One class is to en-
hance its compositionality by using tensor product
(Socher et al., 2013b) or concatenating RNNs hor-
izontally to make a deeper net (Irsoy and Cardie,
2014). The other is to extend its topology in order to
fulfill a wider range of tasks, like Le and Zuidema
(2014a) for dependency parsing and Paulus et al.
(2014) for context-dependence sentiment analysis.
Our proposal in this paper is an extension of the
RNN model to improve compositionality. Our mo-
tivation is that, like training recurrent neural net-
works, training RNNs on deep trees can suffer from
the vanishing gradient problem (Hochreiter et al.,
2001), i.e., that errors propagated back to the leaf
nodes shrink exponentially. In addition, information
sent from a leaf node to the root can be obscured
if the path between them is long, thus leading to
the problem how to capture long range dependen-
cies. We therefore borrow the long short-term mem-
ory (LSTM) architecture (Hochreiter and Schmidhu-
</bodyText>
<note confidence="0.9813905">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 10–19,
Denver, Colorado, June 4–5, 2015.
</note>
<figureCaption confidence="0.985829666666667">
Figure 1: Multi-layer neural network (left) and Recursive
neural network (right). Bias vectors are removed for the
simplicity.
</figureCaption>
<bodyText confidence="0.999914090909091">
ber, 1997) from recurrent neural network research
to tackle those two problems. The main idea is to
allow information low in a parse tree to be stored
in a memory cell and used much later higher up in
the parse tree, by recursively adding up all mem-
ory into memory cells in a bottom-up manner. In
this way, errors propagated back through structure
do not vanish. And information from leaf nodes is
still (loosely) preserved and can be used directly at
any higher nodes in the hierarchy. We then apply
this composition to sentiment analysis. Experimen-
tal results show that the new composition works bet-
ter than the traditional neural-network-based com-
position.
The outline of the rest of the paper is as fol-
lows. We first, in Section 2, give a brief background
on neural networks, including the multi-layer neural
network, recursive neural network, recurrent neural
network, and LSTM. We then propose the LSTM for
recursive neural networks in Section 3, and its appli-
cation to sentiment analysis in Section 4. Section 5
shows our experiments.
</bodyText>
<sectionHeader confidence="0.996089" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999621">
2.1 Multi-layer Neural Network
</subsectionHeader>
<bodyText confidence="0.999739">
In a multi-layer neural network (MLN), neurons are
organized in layers (see Figure 1-left). A neuron in
layer i receives signal from neurons in layer i − 1
and transmits its output to neurons in layer i + 1. 1
The computation is given by
</bodyText>
<equation confidence="0.994914">
yi = g(Wi−1,iyi−1 + bi)
</equation>
<footnote confidence="0.9979435">
1This is a simplified definition. In practice, any layer j &lt; i
can connect to layer i.
</footnote>
<equation confidence="0.97311575">
1+e−x,
2x
tanh(x) = e2x+1, softsign(x) = x
1+|x|.
</equation>
<bodyText confidence="0.999936555555555">
where real vector yi contains the activations of the
neurons in layer i; Wi−1,i E R|yz|x|yz−1 |is the ma-
trix of weights of connections from layer i − 1 to
layer i; bi E R|yz |is the vector of biases of the
neurons in layer i; g is an activation function, e.g.
sigmoid, tanh, or softsign (see Figure 2).
For classification tasks, we put a softmax layer on
the top of the network, and compute the probability
of assigning a class c to an input x by
</bodyText>
<equation confidence="0.996890666666667">
eu(c,ytop)
Pr(c|x) = softmax(c) = u(c, ytop) (1)
Ec/EC e
</equation>
<bodyText confidence="0.999097909090909">
where [u(c1, ytop), ..., u(c|C|, ytop)]T = Wytop +
b; C is the set of all possible classes; W E
R|C|x|yt p|, b E R|C |are a weight matrix and a bias
vector.
Training an MLN is to minimize an objective
function J(θ) where θ is the parameter set (for clas-
sification, J(θ) is often a negative log likelihood).
Thanks to the back-propagation algorithm (Rumel-
hart et al., 1988), the gradient ∂J/∂θ is efficiently
computed; the gradient descent method thus is used
to minimize J.
</bodyText>
<subsectionHeader confidence="0.998742">
2.2 Recursive Neural Network
</subsectionHeader>
<bodyText confidence="0.9974895">
A recursive neural network (RNN) (Goller and
K¨uchler, 1996) is an MLN where, given a tree struc-
ture, we recursively apply the same weight matri-
ces at each inner node in a bottom-up manner. In
order to see how an RNN works, consider the fol-
lowing example. Assume that there is a constituent
</bodyText>
<figureCaption confidence="0.998677">
Figure 2: Activation functions: sigmoid(x) = 1
</figureCaption>
<page confidence="0.997757">
11
</page>
<bodyText confidence="0.97618075">
with parse tree (p2 (p1 x y) z) (Figure 1-right), and
that x, y, z E Rd are the vectorial representations
of the three words x, y and z, respectively. We use
a neural network which consists of a weight matrix
W1 E Rd×d for left children and a weight matrix
W2 E Rd×d for right children to compute the vec-
tor for a parent node in a bottom up manner. Thus,
we compute p1
</bodyText>
<equation confidence="0.999404">
p1 = g(W1x + W2y + b) (2)
</equation>
<bodyText confidence="0.998185666666667">
where b is a bias vector and g is an activation func-
tion. Having computed p1, we can then move one
level up in the hierarchy and compute p2:
</bodyText>
<equation confidence="0.999695">
p2 = g(W1p1 + W2z + b) (3)
</equation>
<bodyText confidence="0.995944266666667">
This process is continued until we reach the root
node.
Like training an MLN, training an RNN uses the
gradient descent method to minimize an objective
function J(O). The gradient aJ/aO is efficiently
computed thanks to the back-propagation through
structure algorithm (Goller and K¨uchler, 1996).
The RNN model and its extensions have been em-
ployed successfully to solve a wide range of prob-
lems: from parsing (constituent parsing (Socher et
al., 2013a), dependency parsing (Le and Zuidema,
2014a)) to classification (e.g. sentiment analysis
(Socher et al., 2013b; Irsoy and Cardie, 2014), para-
phrase detection (Socher et al., 2011a), semantic
role labelling (Le and Zuidema, 2014b)).
</bodyText>
<subsectionHeader confidence="0.988258">
2.3 Recurrent Networks and Long Short-Term
Memory
</subsectionHeader>
<bodyText confidence="0.999246857142857">
A neural network is recurrent if it has at least one
directed ring in its structure. In the natural lan-
guage processing field, the simple recurrent neu-
ral network (SRN) proposed by Elman (1990) (see
Figure 3-left) and its extensions are used to tackle
sequence-related problems, such as machine transla-
tion (Sutskever et al., 2014) and language modelling
(Mikolov et al., 2010).
In an SRN, an input xt is fed to the network
at each time t. The hidden layer h, which has
activation ht−1 right before xt comes in, plays a
role as a memory store capturing the whole history
(x0, ..., xt−1). When xt comes in, the hidden layer
updates its activation by
</bodyText>
<equation confidence="0.727266">
ht = g(Wxxt + Whht−1 + b)
</equation>
<figureCaption confidence="0.985806">
Figure 3: Simple recurrent neural network (left) and long
short-term memory (right). Bias vectors are removed for
the simplicity.
</figureCaption>
<bodyText confidence="0.999844592592593">
where Wx E R|h|×|xt|, Wh E R|h|×|h|, b E R|h|
are weight matrices and a bias vector; g is an activa-
tion.
This network model thus, in theory, can be used
to estimate probabilities conditioning on long histo-
ries. And computing gradients is efficient thanks to
the back-propagation through time algorithm (Wer-
bos, 1990). In practice, however, training recurrent
neural networks with the gradient descent method is
challenging because gradients aJt/ahj (j G t, Jt is
the objective function at time t) vanish quickly af-
ter a few back-propagation steps (Hochreiter et al.,
2001). In addition, it is difficult to capture long
range dependencies, i.e. the output at time t depends
on some inputs that happened very long time ago.
One solution for this, proposed by Hochreiter and
Schmidhuber (1997) and enhanced by Gers (2001),
is long short-term memory (LSTM).
Long Short-Term Memory The main idea of the
LSTM architecture is to maintain a memory of
all inputs the hidden layer received over time, by
adding up all (gated) inputs to the hidden layer
through time to a memory cell. In this way, er-
rors propagated back through time do not vanish
and even inputs received a very long time ago are
still (approximately) preserved and can play a role
in computing the output of the network (see the il-
</bodyText>
<page confidence="0.991895">
12
</page>
<bodyText confidence="0.9923872">
lustration in Graves (2012, Chapter 4)).
An LSTM cell (see Figure 3-right) consists of a
memory cell c, an input gate i, a forget gate f, an
output gate o. Computations occur in this cell are
given below
</bodyText>
<equation confidence="0.967958">
it = Q(Wxixt + Whiht−1 + Wcict−1 + bi)
ft = Q(Wxfxt + Whfht−1 + Wcfct−1 + bf)
ct = ft O ct−1+
it O tanh (Wxcxt + Whcht−1 + bc)
ot = Q(Wxoxt + Whoht−1 + Wcoct + bo )
ht = ot O tanh(ct)
</equation>
<bodyText confidence="0.999953538461539">
where Q is the sigmoid function; it, ft, ot are the
outputs (i.e. activations) of the corresponding gates;
ct is the state of the memory cell; O denotes the
element-wise multiplication operator; W’s and b’s
are weight matrices and bias vectors.
Because the sigmoid function has the output range
(0, 1) (see Figure 2), activations of those gates can
be seen as normalized weights. Therefore, intu-
itively, the network can learn to use the input gate
to decide when to memorize information, and simi-
larly learn to use the output gate to decide when to
access that memory. The forget gate, finally, is to
reset the memory.
</bodyText>
<sectionHeader confidence="0.915646" genericHeader="method">
3 Long Short-Term Memory in RNNs
</sectionHeader>
<bodyText confidence="0.999984">
In this section, we propose an extension of the
LSTM for the RNN model (see Figure 4). A key
feature of the RNN is to hierarchically combine in-
formation from two children to compute the parent
vector; the idea in this section is to extend the LSTM
such that not only the output from each of the chil-
dren is used, but also the contents of their memory
cells. This way, the network has the option to store
information when processing constituents low in the
parse tree, and make it available later on when it is
processing constituents high in the parse tree.
For the simplicity 2, we assume that the parent
node p has two children a and b. The LSTM at p
thus has two input gates i1, i2 and two forget gates
f1, f2 for the two children. Computations occuring
in this LSTM are:
</bodyText>
<footnote confidence="0.881041">
2Extending our LSTM for n-ary trees is trivial.
</footnote>
<figureCaption confidence="0.9430825">
Figure 4: Long short-term memory for recursive neural
network.
</figureCaption>
<equation confidence="0.9993475">
i1 = Q(Wi1x + Wi2y + Wci1cx + Wci2cy + bi)
i2 = Q(Wi1y + Wi2x + Wci1cy + Wci2cx + bi)
f1 = Q(Wf1x + Wf2y + Wcf1cx + Wcf2cy + bf)
f2 = Q(Wf1y + Wf2x + Wcf1cy + Wcf2cx + bf)
cp = f1 O cx + f2 O cy+
g(Wc1x O i1 + Wc2y (D i2 + bc)
o = Q(Wo1x + Wo2y + Wcoc + bo)
p = o O g(cp)
</equation>
<bodyText confidence="0.999981">
where u and cu are the output and the state of the
memory cell at node u; i1, i2, f1, f2, o are the acti-
vations of the corresponding gates; W’s and b’s are
weight matrices and bias vectors; and g is an activa-
tion function.
Intuitively, the input gate ij lets the LSTM at the
parent node decide how important the output at the
j-th child is. If it is important, the input gate ij
will have an activation close to 1. Moreover, the
LSTM controls, using the forget gate fj, the degree
to which information from the memory of the j-th
child should be added to its memory.
Using one input gate and one forget gate for each
child makes the LSTM flexible in storing memory
and computing composition. For instance, in a com-
</bodyText>
<page confidence="0.995636">
13
</page>
<bodyText confidence="0.999986857142857">
plex sentence containing a main clause and a depen-
dent clause it could be beneficial if only information
about the main clause is passed on to higher lev-
els. This can be achieved by having low values for
the input gate and the forget gate for the child node
that covers the dependent clause, and high values for
the gates corresponding to the child node covering
(a part of) the main clause. More interestingly, this
LSTM can even allow a child to contribute to com-
position by activating the corresponding input gate,
but ignore the child’s memory by deactivating the
corresponding forget gate. This happens when the
information given by the child is temporarily impor-
tant only.
</bodyText>
<sectionHeader confidence="0.6989135" genericHeader="method">
4 LSTM-RNN model for Sentiment
Analysis 3
</sectionHeader>
<bodyText confidence="0.999746173913043">
In this section, we introduce a model using the pro-
posed LSTM for sentiment analysis. Our model,
named LSTM-RNN, is an extension of the tradi-
tional RNN model (see Section 2.2) where tradi-
tional composition function g’s in Equations 2- 3 are
replaced by our proposed LSTM (see Figure 5). On
top of the node covering a phrase/word, if its sen-
timent class (e.g. positive, negative, or neutral) is
available, we put a softmax layer (see Equation 1) to
compute the probability of assigning a class to it.
The vector representations of words (i.e. word
embeddings) can be initialized randomly, or pre-
trained. The memory of any leaf node w, i.e. cw,
is 0.
Similarly to Irsoy and Cardie (2014), we ‘untie’
leaf nodes and inner nodes: we use one weight ma-
trix set for leaf nodes and another set for inner nodes.
Hence, let dw and d respectively be the dimensions
of word embeddings (leaf nodes) and vector repre-
sentations of phrases (inner nodes), all weight ma-
trices from a leaf node to an inner node have size
d × dw, and all weight matrices from an inner node
to another inner node have size d × d.
</bodyText>
<footnote confidence="0.989024">
3The LSTM architecture was already applied to the
sentiment analysis task, for instance in the model proposed
at http://deeplearning.net/tutorial/lstm.
html. Independently from and concurrently with our work,
Tai et al. (2015) and Zhu et al. (2015) have developed very
similar models applying LTSM to RNNs.
</footnote>
<bodyText confidence="0.99959125">
Training Training this model is to minimize the
following objective function, which is the cross-
entropy over training sentence set D plus an L2-
norm regularization term
</bodyText>
<equation confidence="0.8735785">
A
log Pr(cp|p) + 2||0||2
</equation>
<bodyText confidence="0.999954181818182">
where 0 is the parameter set, cp is the sentiment class
of phrase p, p is the vector representation at the node
covering p, Pr(cp|p) is computed by the softmax
function, and A is the regularization parameter. Like
training an RNN, we use the mini-batch gradient
descent method to minimize J, where the gradient
aJ/a0 is computed efficiently thanks to the back-
propagation through structure (Goller and K¨uchler,
1996). We use the AdaGrad method (Duchi et al.,
2011) to automatically update the learning rate for
each parameter.
</bodyText>
<subsectionHeader confidence="0.936587">
4.1 Complexity
</subsectionHeader>
<bodyText confidence="0.9650644">
We analyse the complexities of the RNN and LSTM-
RNN models in the forward phase, i.e. computing
vector representations for inner nodes and classifi-
cation probabilities. The complexities in the back-
ward phase, i.e. computing gradients aJ/a0, can be
analysed similarly.
The complexities of the two models are domi-
nated by the matrix-vector multiplications that are
carried out. Since the number of sentiment classes
is very small (5 or 2 in our experiments) compared
to d and dw, we only consider those matrix-vector
multiplications which are for computing vector rep-
resentations at the inner nodes.
For a sentence consisting of N words, assuming
that its parse tree is binarized without any unary
branch (as in the data set we use in our experiments),
there are N −1 inner nodes, N links from leaf nodes
to inner nodes, and N − 2 links from inner nodes to
other inner nodes. The complexity of RNN in the
forward phase is thus approximately
N × d × dw + (N − 2) × d × d
The complexity of LSTM-RNN is approximately
N×6×d×dw+(N−2)×10×d×d+(N−1)×d×d
If dw ≈ d, the complexity of LSTM-RNN is about
8.5 times higher than the complexity of RNN.
</bodyText>
<equation confidence="0.948796">
1 �
J(0) = −
|D|
s∈D
�
p∈s
</equation>
<page confidence="0.98784">
14
</page>
<figureCaption confidence="0.999915">
Figure 5: The RNN model (left) and LSTM-RNN model (right) for sentiment analysis.
</figureCaption>
<bodyText confidence="0.999968833333333">
In our experiments, this difference is not a prob-
lem because training and evaluating the LSTM-
RNN model is very fast: it took us, on a single core
of a modern computer, about 10 minutes to train the
model (d = 50, dw = 100) on 8544 sentences, and
about 2 seconds to evaluate it on 2210 sentences.
</bodyText>
<sectionHeader confidence="0.999454" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.908692">
5.1 Dataset
</subsectionHeader>
<bodyText confidence="0.998403692307692">
We used the Stanford Sentiment Treebank4 (Socher
et al., 2013b) which consists of 5-way fine-grained
sentiment labels (very negative, negative, neutral,
positive, very positive) for 215,154 phrases of
11,855 sentences. The standard splitting is also
given: 8544 sentences for training, 1101 for devel-
opment, and 2210 for testing. The average sentence
length is 19.1.
In addition, the treebank also supports binary sen-
timent (positive, negative) classification by remov-
ing neutral labels, leading to: 6920 sentences for
training, 872 for development, and 1821 for testing.
The evaluation metric is the accuracy, given by
</bodyText>
<equation confidence="0.299332">
100x#correct
#total .
</equation>
<subsectionHeader confidence="0.99579">
5.2 LSTM-RNN vs. RNN
</subsectionHeader>
<bodyText confidence="0.91441925">
Setting We initialized the word vectors by the
100-D GloVe5 word embeddings (Pennington et
al., 2014), which were trained on a 6B-word cor-
pus. The initial values for a weight matrix were
uniformly sampled from the symmetric interval
�− 11 ] where n is the number of total input
√n, √n
units.
</bodyText>
<footnote confidence="0.995484">
4http://nlp.stanford.edu/sentiment/
treebank.html
5http://nlp.stanford.edu/projects/GloVe/
</footnote>
<figureCaption confidence="0.999780333333333">
Figure 6: Boxplots of accuracies of 10 runs of RNN and
LSTM-RNN on the test set in the fine-grained classifica-
tion task. (LSTM stands for LSTM-RNN.)
</figureCaption>
<bodyText confidence="0.994913941176471">
For each model (RNN and LSTM-RNN), we
tested three activation functions: softmax, tanh, and
softsign, leading to six sub-models. Tuning those
sub-models on the development set, we chose the
dimensions of vector representations at inner nodes
d = 50, learning rate 0.05, regularization parameter
A = 10−3, and mini-batch-size 5.
On each task, we run each sub-model 10 times.
Each time, we trained the sub-model in 20 epochs
and selected the network achieving the highest ac-
curacy on the development set.
Results Figure 6 and 7 show the statistics of the
accuracies of the final networks on the test set in the
fine-grained classification task and binary classifica-
tion task, respectively.
It can be seen that LSTM-RNN outperformed
RNN when using the tanh or softsign activation
</bodyText>
<page confidence="0.992047">
15
</page>
<table confidence="0.998562454545455">
Model Fine-grained Binary
BiNB 41.9 83.1
RNTN 45.7 85.4
CNN 48.0 88.1
DCNN 48.5 86.8
PV 48.7 87.8
DRNN 49.8 86.6
with GloVe-100D
LSTM-RNN 48.0 86.2
with GloVe-300D
LSTM-RNN 49.9 88.0
</table>
<tableCaption confidence="0.808696">
Table 1: Accuracies of the (tanh) LSTM-RNN compared
with other models.
</tableCaption>
<figureCaption confidence="0.998873333333333">
Figure 7: Boxplot of accuracies of 10 runs of RNN and
LSTM-RNN on the test set in the binary classification
task. (LSTM stands for LSTM-RNN.)
</figureCaption>
<bodyText confidence="0.999653909090909">
functions. With the sigmoid activation function, the
difference is not so clear, but it seems that LSTM-
RNN performed slightly better. Tanh-LSTM-RNN
and softsign-LSTM-RNN have the highest median
accuracies (48.1 and 86.4) in the fine-grained clas-
sification task and in the binary classification task,
respectively.
With the RNN model, it is surprising to see that
the sigmoid function performed well, comparably
with the other two functions in the fine-grained task,
and even better than the softsign function in the bi-
nary task, given that it was not often chosen in recent
work. The softsign function, which was shown to
work better than tanh for deep networks (Glorot and
Bengio, 2010), however, did not yield improvements
in this experiment.
With the LSTM-RNN model, the tanh function,
in general, worked best whereas the sigmoid func-
tion was the worst. This result agrees with the
common choice for this activation function for the
LSTM architecture in recurrent network research
(Gers, 2001; Sutskever et al., 2014).
</bodyText>
<subsectionHeader confidence="0.999077">
5.3 Compared against other Models
</subsectionHeader>
<bodyText confidence="0.9764790625">
We compare LSTM-RNN (using tanh) in the pre-
vious experiment against existing models: Naive
Bayes with bag of bigram features (BiNB), Re-
cursive neural tensor network (RNTN) (Socher et
al., 2013b), Convolutional neural network (CNN)
(Kim, 2014), Dynamic convolutional neural network
(DCNN) (Kalchbrenner et al., 2014), paragraph vec-
tors (PV) (Le and Mikolov, 2014), and Deep RNN
(DRNN) (Irsoy and Cardie, 2014).
Among them, BiNB is the only one that is not a
neural net model. RNTN and DRNN are two ex-
tensions of RNN. Whereas RNTN, which keeps the
structure of the RNN, uses both matrix-vector multi-
plication and tensor product for the composition pur-
pose, DRNN makes the net deeper by concatenat-
ing more than one RNNs horizontally. CNN, DCNN
and PV do not rely on syntactic trees. CNN uses a
convolutional layer and a max-pooling layer to han-
dle sequences with different lengths. DCNN is hi-
erarchical in the sense that it stacks more than one
convolutional layers with k-max pooling layers in
between. In PV, a sentence (or document) is rep-
resented as an input vector to predict which words
appear in it.
Table 1 (above the dashed line) shows the accura-
cies of those models. The accuracies of LSTM-RNN
was taken from the network achieving the highest
performance out of 10 runs on the development set.
The accuracies of the other models are copied from
the corresponding papers. LSTM-RNN clearly per-
formed worse than DCNN, PV, DRNN in both tasks,
and worse than CNN in the binary task.
</bodyText>
<subsectionHeader confidence="0.9948055">
5.4 Toward State-of-the-art with Better Word
Embeddings
</subsectionHeader>
<bodyText confidence="0.99923825">
We focus on DRNN, which is the most similar
to LSTM-RNN among those four models CNN,
DCNN, PV and DRNN. In fact, from the results re-
ported in Irsoy and Cardie (2014, Table 1a), LSTM-
</bodyText>
<page confidence="0.993525">
16
</page>
<bodyText confidence="0.999906787878788">
RNN performed on par6 with their 1-layer-DRNN
(d = 340) using dropout, which is to randomly
remove some neurons during training. Dropout is
a powerful technique to train neural networks, not
only because it plays a role as a strong regulariza-
tion method to prohibit neurons co-adapting, but it
is also considered a technique to efficiently make an
ensemble of a large number of shared weight neu-
ral networks (Srivastava et al., 2014). Thanks to
dropout, Irsoy and Cardie (2014) boosted the accu-
racy of a 3-layer-DRNN with d = 200 from 46.06
to 49.5 in the fine-grained task.
In the second experiment, we tried to boost the
accuracy of the LSTM-RNN model. Inspired by Ir-
soy and Cardie (2014), we tried using dropout and
better word embeddings. Dropout, however, did
not work with LSTM. The reason might be that
dropout corrupted its memory, thus making train-
ing more difficult. Better word embeddings did pay
off, however. We used 300-D GloVe word embed-
dings trained on a 840B-word corpus. Testing on the
development set, we chose the same values for the
hyper-parameters as in the first experiment, except
setting learning rate 0.01. We also run the model
10 times and selected the networks getting the high-
est accuracies on the development set. Table 1 (be-
low the dashed line) shows the results. Using the
300-D GloVe word embeddings was very helpful:
LSTM-RNN performed on par with DRNN in the
fine-grained task, and with CNN in the binary task.
Therefore, taking into account both tasks, LSTM-
RNN with the 300-D GloVe word embeddings out-
performed all other models.
</bodyText>
<sectionHeader confidence="0.985563" genericHeader="discussions">
6 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.998699886363636">
We proposed a new composition method for the re-
cursive neural network (RNN) model by extending
the long short-term memory (LSTM) architecture
which is widely used in recurrent neural network re-
search.
6Irsoy and Cardie (2014) used the 300-D word2vec word
embeddings trained on a 100B-word corpus whereas we used
the 100-D GloVe word embeddings trained on a 6B-word cor-
pus. From the fact that they achieved the accuracy 46.1 with
an RNN (d = 50) in the fine-grained task and 85.3 in the
binary task, and our implementation of RNN (d = 50) per-
formed worse (see Table 6 and 7), we conclude that the 100-D
GloVe word embeddings are not more suitable than the 300-D
word2vec word embeddings.
The question is why LSTM-RNN performed bet-
ter than the traditional RNN. Here, based on the fact
that the LSTM for RNNs should work very sim-
ilarly to LSTM for recurrent neural networks, we
borrow the argument given in Bengio et al. (2013,
Section 3.2) to answer the question. Bengio explains
that the LSTM behaves like low-pass filter “hence
they can be used to focus certain units on differ-
ent frequency regions of the data”. This suggests
that the LSTM plays a role as a lossy compressor
which is to keep global information by focusing on
low frequency regions and remove noise by ignor-
ing high frequency regions. So composition in this
case could be seen as compression, like the recursive
auto-encoder (RAE) (Socher et al., 2011a). Because
pre-training an RNN as an RAE can boost the over-
all performance (Socher et al., 2011a; Socher et al.,
2011c), seeing LSTM as a compressor might explain
why the LSTM-RNN worked better than RNN with-
out pre-training.
Comparing LSTM-RNN against DRNN (Irsoy
and Cardie, 2014) gives us a hint about how to im-
prove our model. From the experimental results,
LSTM-RNN without the 300-D GloVe word embed-
dings performed worse than DRNN, while DRNN
gained a significant improvement thanks to dropout.
Finding a method like dropout that does not corrupt
the LSTM memory might boost the overall perfor-
mance significantly and will be a topic for our future
work.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999314">
We thank three anonymous reviewers for helpful
comments.
</bodyText>
<sectionHeader confidence="0.997946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999413545454546">
Marco Baroni, Raffaella Bernardi, and Roberto Zampar-
elli. 2013. Frege in space: A program for composi-
tional distributional semantics. In A. Zaenen, B. Web-
ber, and M. Palmer, editors, Linguistic Issues in Lan-
guage Technologies. CSLI Publications, Stanford, CA.
Yoshua Bengio, Nicolas Boulanger-Lewandowski, and
Razvan Pascanu. 2013. Advances in optimizing re-
current networks. In Acoustics, Speech and Signal
Processing (ICASSP), 2013 IEEE International Con-
ference on, pages 8624–8628. IEEE.
George Cybenko. 1989. Approximation by superposi-
</reference>
<page confidence="0.995341">
17
</page>
<reference confidence="0.998482254716981">
tions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303–314.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, pages 2121–2159.
Jeffrey L. Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179–211.
Felix Gers. 2001. Long short-term memory in recur-
rent neural networks. Unpublished PhD dissertation,
´Ecole Polytechnique F´ed´erale de Lausanne, Lausanne,
Switzerland.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In International conference on artificial in-
telligence and statistics, pages 249–256.
Christoph Goller and Andreas K¨uchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In International Con-
ference on Neural Networks, pages 347–352. IEEE.
Alex Graves. 2012. Supervised sequence labelling with
recurrent neural networks, volume 385. Springer.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735–
1780.
S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhu-
ber. 2001. Gradient flow in recurrent nets: the diffi-
culty of learning long-term dependencies. In Kremer
and Kolen, editors, A Field Guide to Dynamical Re-
current Neural Networks. IEEE Press.
Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language. In
Advances in Neural Information Processing Systems,
pages 2096–2104.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for mod-
elling sentences. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 655–665, Balti-
more, Maryland, June. Association for Computational
Linguistics.
Yoon Kim. 2014. Convolutional neural networks for sen-
tence classification. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1746–1751, Doha, Qatar,
October. Association for Computational Linguistics.
Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of the 31st International Conference on Machine
Learning (ICML-14), pages 1188–1196.
Phong Le and Willem Zuidema. 2014a. The inside-
outside recursive neural network model for depen-
dency parsing. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.
Phong Le and Willem Zuidema. 2014b. Inside-outside
semantics: A framework for neural models of semantic
composition. In NIPS 2014 Workshop on Deep Learn-
ing and Representation Learning.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cer-
nock`y, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In INTER-
SPEECH, pages 1045–1048.
Jeff Mitchell and Mirella Lapata. 2009. Language mod-
els based on semantic composition. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 430–439.
Romain Paulus, Richard Socher, and Christopher D Man-
ning. 2014. Global belief recursive neural networks.
In Advances in Neural Information Processing Sys-
tems, pages 2888–2896.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. Proceedings of the Empiricial Methods in
Natural Language Processing (EMNLP 2014), 12.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1988. Learning representations by back-
propagating errors. Cognitive modeling, 5.
Richard Socher, Christopher D. Manning, and Andrew Y.
Ng. 2010. Learning continuous phrase representa-
tions and syntactic parsing with recursive neural net-
works. In Proceedings of the NIPS-2010 Deep Learn-
ing and Unsupervised Feature Learning Workshop.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011a. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. Advances in Neural Infor-
mation Processing Systems, 24:801–809.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christo-
pher D. Manning. 2011b. Parsing natural scenes and
natural language with recursive neural networks. In
Proceedings of the 26th International Conference on
Machine Learning, volume 2.
Richard Socher, Jeffrey Pennington, Eric H Huang, An-
drew Y Ng, and Christopher D Manning. 2011c.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 151–161.
Richard Socher, John Bauer, Christopher D Manning, and
Andrew Y Ng. 2013a. Parsing with compositional
vector grammars. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics, pages 455–465.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
</reference>
<page confidence="0.984757">
18
</page>
<reference confidence="0.9997645">
Christopher Potts. 2013b. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings EMNLP.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems, pages 3104–3112.
Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. arXiv preprint arXiv:1503.00075.
Paul J Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10):1550–1560.
Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015.
Long short-term memory over tree structures. arXiv
preprint arXiv:1503.04881.
</reference>
<page confidence="0.999328">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.594611">
<title confidence="0.979447">Compositional Distributional Semantics with Long Short Term Memory</title>
<author confidence="0.989311">Le_Zuidema</author>
<affiliation confidence="0.9631845">Institute for Logic, Language and Computation University of Amsterdam, the Netherlands</affiliation>
<abstract confidence="0.995744615384615">We are proposing an extension of the recursive neural network that makes use of a variant of the long short-term memory architecture. The extension allows information low in parse trees to be stored in a memory register (the ‘memory cell’) and used much later higher up in the parse tree. This provides a solution to the vanishing gradient problem and allows the network to capture long range dependencies. Experimental results show that our composition outperformed the traditional neural-network composition on the Stanford</abstract>
<intro confidence="0.667305">Sentiment Treebank.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Frege in space: A program for compositional distributional semantics.</title>
<date>2013</date>
<editor>In A. Zaenen, B. Webber, and M. Palmer, editors,</editor>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="1455" citStr="Baroni et al., 2013" startWordPosition="221" endWordPosition="224">d semantics requires answers to two difficult questions: (i) what is the nature of the composition functions (given that the lambda calculus for variable binding is no longer applicable), and (ii) how do we learn the parameters of those functions (if they have any) from data? A number of classes of functions have been proposed in answer to the first question, including simple linear functions like vector addition (Mitchell and Lapata, 2009), non-linear functions like those defined by multi-layer neural networks (Socher et al., 2010), and vector matrix multiplication and tensor linear mapping (Baroni et al., 2013). The matrix and tensor-based functions have the advantage of allowing a relatively straightforward comparison with formal semantics, but the fact that multi-layer neural networks with non-linear activation functions like sigmoid can approximate 10 any continuous function (Cybenko, 1989) already make them an attractive choice. In trying to answer the second question, the advantages of approaches based on neural network architectures, such as the recursive neural network (RNN) model (Socher et al., 2013b) and the convolutional neural network model (Kalchbrenner et al., 2014), are even clearer. </context>
</contexts>
<marker>Baroni, Bernardi, Zamparelli, 2013</marker>
<rawString>Marco Baroni, Raffaella Bernardi, and Roberto Zamparelli. 2013. Frege in space: A program for compositional distributional semantics. In A. Zaenen, B. Webber, and M. Palmer, editors, Linguistic Issues in Language Technologies. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Nicolas Boulanger-Lewandowski</author>
<author>Razvan Pascanu</author>
</authors>
<title>Advances in optimizing recurrent networks.</title>
<date>2013</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,</booktitle>
<pages>8624--8628</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="24886" citStr="Bengio et al. (2013" startWordPosition="4281" endWordPosition="4284">we used the 100-D GloVe word embeddings trained on a 6B-word corpus. From the fact that they achieved the accuracy 46.1 with an RNN (d = 50) in the fine-grained task and 85.3 in the binary task, and our implementation of RNN (d = 50) performed worse (see Table 6 and 7), we conclude that the 100-D GloVe word embeddings are not more suitable than the 300-D word2vec word embeddings. The question is why LSTM-RNN performed better than the traditional RNN. Here, based on the fact that the LSTM for RNNs should work very similarly to LSTM for recurrent neural networks, we borrow the argument given in Bengio et al. (2013, Section 3.2) to answer the question. Bengio explains that the LSTM behaves like low-pass filter “hence they can be used to focus certain units on different frequency regions of the data”. This suggests that the LSTM plays a role as a lossy compressor which is to keep global information by focusing on low frequency regions and remove noise by ignoring high frequency regions. So composition in this case could be seen as compression, like the recursive auto-encoder (RAE) (Socher et al., 2011a). Because pre-training an RNN as an RAE can boost the overall performance (Socher et al., 2011a; Socher</context>
</contexts>
<marker>Bengio, Boulanger-Lewandowski, Pascanu, 2013</marker>
<rawString>Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu. 2013. Advances in optimizing recurrent networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8624–8628. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Cybenko</author>
</authors>
<title>Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems,</title>
<date>1989</date>
<pages>2--4</pages>
<contexts>
<context position="1743" citStr="Cybenko, 1989" startWordPosition="262" endWordPosition="263">es of functions have been proposed in answer to the first question, including simple linear functions like vector addition (Mitchell and Lapata, 2009), non-linear functions like those defined by multi-layer neural networks (Socher et al., 2010), and vector matrix multiplication and tensor linear mapping (Baroni et al., 2013). The matrix and tensor-based functions have the advantage of allowing a relatively straightforward comparison with formal semantics, but the fact that multi-layer neural networks with non-linear activation functions like sigmoid can approximate 10 any continuous function (Cybenko, 1989) already make them an attractive choice. In trying to answer the second question, the advantages of approaches based on neural network architectures, such as the recursive neural network (RNN) model (Socher et al., 2013b) and the convolutional neural network model (Kalchbrenner et al., 2014), are even clearer. Models in this paradigm can take advantage of general learning procedures based on back-propagation, and with the rise of ‘deep learning’, of a variety of efficient algorithms and tricks to further improve training. Since the first success of the RNN model (Socher et al., 2011b) in const</context>
</contexts>
<marker>Cybenko, 1989</marker>
<rawString>George Cybenko. 1989. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>2121--2159</pages>
<contexts>
<context position="15569" citStr="Duchi et al., 2011" startWordPosition="2716" endWordPosition="2719">ize the following objective function, which is the crossentropy over training sentence set D plus an L2- norm regularization term A log Pr(cp|p) + 2||0||2 where 0 is the parameter set, cp is the sentiment class of phrase p, p is the vector representation at the node covering p, Pr(cp|p) is computed by the softmax function, and A is the regularization parameter. Like training an RNN, we use the mini-batch gradient descent method to minimize J, where the gradient aJ/a0 is computed efficiently thanks to the backpropagation through structure (Goller and K¨uchler, 1996). We use the AdaGrad method (Duchi et al., 2011) to automatically update the learning rate for each parameter. 4.1 Complexity We analyse the complexities of the RNN and LSTMRNN models in the forward phase, i.e. computing vector representations for inner nodes and classification probabilities. The complexities in the backward phase, i.e. computing gradients aJ/a0, can be analysed similarly. The complexities of the two models are dominated by the matrix-vector multiplications that are carried out. Since the number of sentiment classes is very small (5 or 2 in our experiments) compared to d and dw, we only consider those matrix-vector multipli</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, pages 2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="7970" citStr="Elman (1990)" startWordPosition="1338" endWordPosition="1339">ts extensions have been employed successfully to solve a wide range of problems: from parsing (constituent parsing (Socher et al., 2013a), dependency parsing (Le and Zuidema, 2014a)) to classification (e.g. sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al., 2011a), semantic role labelling (Le and Zuidema, 2014b)). 2.3 Recurrent Networks and Long Short-Term Memory A neural network is recurrent if it has at least one directed ring in its structure. In the natural language processing field, the simple recurrent neural network (SRN) proposed by Elman (1990) (see Figure 3-left) and its extensions are used to tackle sequence-related problems, such as machine translation (Sutskever et al., 2014) and language modelling (Mikolov et al., 2010). In an SRN, an input xt is fed to the network at each time t. The hidden layer h, which has activation ht−1 right before xt comes in, plays a role as a memory store capturing the whole history (x0, ..., xt−1). When xt comes in, the hidden layer updates its activation by ht = g(Wxxt + Whht−1 + b) Figure 3: Simple recurrent neural network (left) and long short-term memory (right). Bias vectors are removed for the </context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L. Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Gers</author>
</authors>
<title>Long short-term memory in recurrent neural networks.</title>
<date>2001</date>
<booktitle>Unpublished PhD dissertation, ´Ecole Polytechnique F´ed´erale de</booktitle>
<location>Lausanne, Lausanne, Switzerland.</location>
<contexts>
<context position="9400" citStr="Gers (2001)" startWordPosition="1580" endWordPosition="1581">es. And computing gradients is efficient thanks to the back-propagation through time algorithm (Werbos, 1990). In practice, however, training recurrent neural networks with the gradient descent method is challenging because gradients aJt/ahj (j G t, Jt is the objective function at time t) vanish quickly after a few back-propagation steps (Hochreiter et al., 2001). In addition, it is difficult to capture long range dependencies, i.e. the output at time t depends on some inputs that happened very long time ago. One solution for this, proposed by Hochreiter and Schmidhuber (1997) and enhanced by Gers (2001), is long short-term memory (LSTM). Long Short-Term Memory The main idea of the LSTM architecture is to maintain a memory of all inputs the hidden layer received over time, by adding up all (gated) inputs to the hidden layer through time to a memory cell. In this way, errors propagated back through time do not vanish and even inputs received a very long time ago are still (approximately) preserved and can play a role in computing the output of the network (see the il12 lustration in Graves (2012, Chapter 4)). An LSTM cell (see Figure 3-right) consists of a memory cell c, an input gate i, a for</context>
<context position="20582" citStr="Gers, 2001" startWordPosition="3544" endWordPosition="3545">erformed well, comparably with the other two functions in the fine-grained task, and even better than the softsign function in the binary task, given that it was not often chosen in recent work. The softsign function, which was shown to work better than tanh for deep networks (Glorot and Bengio, 2010), however, did not yield improvements in this experiment. With the LSTM-RNN model, the tanh function, in general, worked best whereas the sigmoid function was the worst. This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014). 5.3 Compared against other Models We compare LSTM-RNN (using tanh) in the previous experiment against existing models: Naive Bayes with bag of bigram features (BiNB), Recursive neural tensor network (RNTN) (Socher et al., 2013b), Convolutional neural network (CNN) (Kim, 2014), Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014). Among them, BiNB is the only one that is not a neural net model. RNTN and DRNN are two extensions of RNN. Whereas RNTN, which kee</context>
</contexts>
<marker>Gers, 2001</marker>
<rawString>Felix Gers. 2001. Long short-term memory in recurrent neural networks. Unpublished PhD dissertation, ´Ecole Polytechnique F´ed´erale de Lausanne, Lausanne, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Yoshua Bengio</author>
</authors>
<title>Understanding the difficulty of training deep feedforward neural networks.</title>
<date>2010</date>
<booktitle>In International conference on artificial intelligence and statistics,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="20274" citStr="Glorot and Bengio, 2010" startWordPosition="3494" endWordPosition="3497"> is not so clear, but it seems that LSTMRNN performed slightly better. Tanh-LSTM-RNN and softsign-LSTM-RNN have the highest median accuracies (48.1 and 86.4) in the fine-grained classification task and in the binary classification task, respectively. With the RNN model, it is surprising to see that the sigmoid function performed well, comparably with the other two functions in the fine-grained task, and even better than the softsign function in the binary task, given that it was not often chosen in recent work. The softsign function, which was shown to work better than tanh for deep networks (Glorot and Bengio, 2010), however, did not yield improvements in this experiment. With the LSTM-RNN model, the tanh function, in general, worked best whereas the sigmoid function was the worst. This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014). 5.3 Compared against other Models We compare LSTM-RNN (using tanh) in the previous experiment against existing models: Naive Bayes with bag of bigram features (BiNB), Recursive neural tensor network (RNTN) (Socher et al., 2013b), Convolutional neural network (CNN) </context>
</contexts>
<marker>Glorot, Bengio, 2010</marker>
<rawString>Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In International conference on artificial intelligence and statistics, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas K¨uchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In International Conference on Neural Networks,</booktitle>
<pages>347--352</pages>
<publisher>IEEE.</publisher>
<marker>Goller, K¨uchler, 1996</marker>
<rawString>Christoph Goller and Andreas K¨uchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In International Conference on Neural Networks, pages 347–352. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Supervised sequence labelling with recurrent neural networks,</title>
<date>2012</date>
<volume>385</volume>
<publisher>Springer.</publisher>
<contexts>
<context position="9900" citStr="Graves (2012" startWordPosition="1670" endWordPosition="1671"> long time ago. One solution for this, proposed by Hochreiter and Schmidhuber (1997) and enhanced by Gers (2001), is long short-term memory (LSTM). Long Short-Term Memory The main idea of the LSTM architecture is to maintain a memory of all inputs the hidden layer received over time, by adding up all (gated) inputs to the hidden layer through time to a memory cell. In this way, errors propagated back through time do not vanish and even inputs received a very long time ago are still (approximately) preserved and can play a role in computing the output of the network (see the il12 lustration in Graves (2012, Chapter 4)). An LSTM cell (see Figure 3-right) consists of a memory cell c, an input gate i, a forget gate f, an output gate o. Computations occur in this cell are given below it = Q(Wxixt + Whiht−1 + Wcict−1 + bi) ft = Q(Wxfxt + Whfht−1 + Wcfct−1 + bf) ct = ft O ct−1+ it O tanh (Wxcxt + Whcht−1 + bc) ot = Q(Wxoxt + Whoht−1 + Wcoct + bo ) ht = ot O tanh(ct) where Q is the sigmoid function; it, ft, ot are the outputs (i.e. activations) of the corresponding gates; ct is the state of the memory cell; O denotes the element-wise multiplication operator; W’s and b’s are weight matrices and bias ve</context>
</contexts>
<marker>Graves, 2012</marker>
<rawString>Alex Graves. 2012. Supervised sequence labelling with recurrent neural networks, volume 385. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<journal>Neural computation,</journal>
<volume>9</volume>
<issue>8</issue>
<pages>1780</pages>
<contexts>
<context position="9372" citStr="Hochreiter and Schmidhuber (1997)" startWordPosition="1573" endWordPosition="1576">stimate probabilities conditioning on long histories. And computing gradients is efficient thanks to the back-propagation through time algorithm (Werbos, 1990). In practice, however, training recurrent neural networks with the gradient descent method is challenging because gradients aJt/ahj (j G t, Jt is the objective function at time t) vanish quickly after a few back-propagation steps (Hochreiter et al., 2001). In addition, it is difficult to capture long range dependencies, i.e. the output at time t depends on some inputs that happened very long time ago. One solution for this, proposed by Hochreiter and Schmidhuber (1997) and enhanced by Gers (2001), is long short-term memory (LSTM). Long Short-Term Memory The main idea of the LSTM architecture is to maintain a memory of all inputs the hidden layer received over time, by adding up all (gated) inputs to the hidden layer through time to a memory cell. In this way, errors propagated back through time do not vanish and even inputs received a very long time ago are still (approximately) preserved and can play a role in computing the output of the network (see the il12 lustration in Graves (2012, Chapter 4)). An LSTM cell (see Figure 3-right) consists of a memory ce</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735– 1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hochreiter</author>
<author>Y Bengio</author>
<author>P Frasconi</author>
<author>J Schmidhuber</author>
</authors>
<title>Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.</title>
<date>2001</date>
<editor>In Kremer and Kolen, editors, A Field</editor>
<publisher>IEEE Press.</publisher>
<contexts>
<context position="3032" citStr="Hochreiter et al., 2001" startWordPosition="470" endWordPosition="473"> class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al. (2014) for context-dependence sentiment analysis. Our proposal in this paper is an extension of the RNN model to improve compositionality. Our motivation is that, like training recurrent neural networks, training RNNs on deep trees can suffer from the vanishing gradient problem (Hochreiter et al., 2001), i.e., that errors propagated back to the leaf nodes shrink exponentially. In addition, information sent from a leaf node to the root can be obscured if the path between them is long, thus leading to the problem how to capture long range dependencies. We therefore borrow the long short-term memory (LSTM) architecture (Hochreiter and SchmidhuProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 10–19, Denver, Colorado, June 4–5, 2015. Figure 1: Multi-layer neural network (left) and Recursive neural network (right). Bias vectors are removed for the</context>
<context position="9154" citStr="Hochreiter et al., 2001" startWordPosition="1537" endWordPosition="1540">ight). Bias vectors are removed for the simplicity. where Wx E R|h|×|xt|, Wh E R|h|×|h|, b E R|h| are weight matrices and a bias vector; g is an activation. This network model thus, in theory, can be used to estimate probabilities conditioning on long histories. And computing gradients is efficient thanks to the back-propagation through time algorithm (Werbos, 1990). In practice, however, training recurrent neural networks with the gradient descent method is challenging because gradients aJt/ahj (j G t, Jt is the objective function at time t) vanish quickly after a few back-propagation steps (Hochreiter et al., 2001). In addition, it is difficult to capture long range dependencies, i.e. the output at time t depends on some inputs that happened very long time ago. One solution for this, proposed by Hochreiter and Schmidhuber (1997) and enhanced by Gers (2001), is long short-term memory (LSTM). Long Short-Term Memory The main idea of the LSTM architecture is to maintain a memory of all inputs the hidden layer received over time, by adding up all (gated) inputs to the hidden layer through time to a memory cell. In this way, errors propagated back through time do not vanish and even inputs received a very lon</context>
</contexts>
<marker>Hochreiter, Bengio, Frasconi, Schmidhuber, 2001</marker>
<rawString>S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. 2001. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. In Kremer and Kolen, editors, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep recursive neural networks for compositionality in language.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2096--2104</pages>
<contexts>
<context position="2577" citStr="Irsoy and Cardie, 2014" startWordPosition="395" endWordPosition="398">l., 2013b) and the convolutional neural network model (Kalchbrenner et al., 2014), are even clearer. Models in this paradigm can take advantage of general learning procedures based on back-propagation, and with the rise of ‘deep learning’, of a variety of efficient algorithms and tricks to further improve training. Since the first success of the RNN model (Socher et al., 2011b) in constituent parsing, two classes of extensions have been proposed. One class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al. (2014) for context-dependence sentiment analysis. Our proposal in this paper is an extension of the RNN model to improve compositionality. Our motivation is that, like training recurrent neural networks, training RNNs on deep trees can suffer from the vanishing gradient problem (Hochreiter et al., 2001), i.e., that errors propagated back to the leaf nodes shrink exponentially. In addition, information sent from a leaf node to the root can be obs</context>
<context position="7630" citStr="Irsoy and Cardie, 2014" startWordPosition="1281" endWordPosition="1284">te p2: p2 = g(W1p1 + W2z + b) (3) This process is continued until we reach the root node. Like training an MLN, training an RNN uses the gradient descent method to minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). The RNN model and its extensions have been employed successfully to solve a wide range of problems: from parsing (constituent parsing (Socher et al., 2013a), dependency parsing (Le and Zuidema, 2014a)) to classification (e.g. sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al., 2011a), semantic role labelling (Le and Zuidema, 2014b)). 2.3 Recurrent Networks and Long Short-Term Memory A neural network is recurrent if it has at least one directed ring in its structure. In the natural language processing field, the simple recurrent neural network (SRN) proposed by Elman (1990) (see Figure 3-left) and its extensions are used to tackle sequence-related problems, such as machine translation (Sutskever et al., 2014) and language modelling (Mikolov et al., 2010). In an SRN, an input xt is fed to the network at each time t. The hidden la</context>
<context position="14195" citStr="Irsoy and Cardie (2014)" startWordPosition="2482" endWordPosition="2485">for sentiment analysis. Our model, named LSTM-RNN, is an extension of the traditional RNN model (see Section 2.2) where traditional composition function g’s in Equations 2- 3 are replaced by our proposed LSTM (see Figure 5). On top of the node covering a phrase/word, if its sentiment class (e.g. positive, negative, or neutral) is available, we put a softmax layer (see Equation 1) to compute the probability of assigning a class to it. The vector representations of words (i.e. word embeddings) can be initialized randomly, or pretrained. The memory of any leaf node w, i.e. cw, is 0. Similarly to Irsoy and Cardie (2014), we ‘untie’ leaf nodes and inner nodes: we use one weight matrix set for leaf nodes and another set for inner nodes. Hence, let dw and d respectively be the dimensions of word embeddings (leaf nodes) and vector representations of phrases (inner nodes), all weight matrices from a leaf node to an inner node have size d × dw, and all weight matrices from an inner node to another inner node have size d × d. 3The LSTM architecture was already applied to the sentiment analysis task, for instance in the model proposed at http://deeplearning.net/tutorial/lstm. html. Independently from and concurrentl</context>
<context position="21051" citStr="Irsoy and Cardie, 2014" startWordPosition="3613" endWordPosition="3616"> was the worst. This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014). 5.3 Compared against other Models We compare LSTM-RNN (using tanh) in the previous experiment against existing models: Naive Bayes with bag of bigram features (BiNB), Recursive neural tensor network (RNTN) (Socher et al., 2013b), Convolutional neural network (CNN) (Kim, 2014), Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014). Among them, BiNB is the only one that is not a neural net model. RNTN and DRNN are two extensions of RNN. Whereas RNTN, which keeps the structure of the RNN, uses both matrix-vector multiplication and tensor product for the composition purpose, DRNN makes the net deeper by concatenating more than one RNNs horizontally. CNN, DCNN and PV do not rely on syntactic trees. CNN uses a convolutional layer and a max-pooling layer to handle sequences with different lengths. DCNN is hierarchical in the sense that it stacks more than one convolutional layers with k-max pooling layers in between. In PV, </context>
<context position="22348" citStr="Irsoy and Cardie (2014" startWordPosition="3843" endWordPosition="3846"> words appear in it. Table 1 (above the dashed line) shows the accuracies of those models. The accuracies of LSTM-RNN was taken from the network achieving the highest performance out of 10 runs on the development set. The accuracies of the other models are copied from the corresponding papers. LSTM-RNN clearly performed worse than DCNN, PV, DRNN in both tasks, and worse than CNN in the binary task. 5.4 Toward State-of-the-art with Better Word Embeddings We focus on DRNN, which is the most similar to LSTM-RNN among those four models CNN, DCNN, PV and DRNN. In fact, from the results reported in Irsoy and Cardie (2014, Table 1a), LSTM16 RNN performed on par6 with their 1-layer-DRNN (d = 340) using dropout, which is to randomly remove some neurons during training. Dropout is a powerful technique to train neural networks, not only because it plays a role as a strong regularization method to prohibit neurons co-adapting, but it is also considered a technique to efficiently make an ensemble of a large number of shared weight neural networks (Srivastava et al., 2014). Thanks to dropout, Irsoy and Cardie (2014) boosted the accuracy of a 3-layer-DRNN with d = 200 from 46.06 to 49.5 in the fine-grained task. In th</context>
<context position="24188" citStr="Irsoy and Cardie (2014)" startWordPosition="4154" endWordPosition="4157">g the highest accuracies on the development set. Table 1 (below the dashed line) shows the results. Using the 300-D GloVe word embeddings was very helpful: LSTM-RNN performed on par with DRNN in the fine-grained task, and with CNN in the binary task. Therefore, taking into account both tasks, LSTMRNN with the 300-D GloVe word embeddings outperformed all other models. 6 Discussion and Conclusion We proposed a new composition method for the recursive neural network (RNN) model by extending the long short-term memory (LSTM) architecture which is widely used in recurrent neural network research. 6Irsoy and Cardie (2014) used the 300-D word2vec word embeddings trained on a 100B-word corpus whereas we used the 100-D GloVe word embeddings trained on a 6B-word corpus. From the fact that they achieved the accuracy 46.1 with an RNN (d = 50) in the fine-grained task and 85.3 in the binary task, and our implementation of RNN (d = 50) performed worse (see Table 6 and 7), we conclude that the 100-D GloVe word embeddings are not more suitable than the 300-D word2vec word embeddings. The question is why LSTM-RNN performed better than the traditional RNN. Here, based on the fact that the LSTM for RNNs should work very si</context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Advances in Neural Information Processing Systems, pages 2096–2104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>655--665</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="2035" citStr="Kalchbrenner et al., 2014" startWordPosition="307" endWordPosition="310">nd tensor linear mapping (Baroni et al., 2013). The matrix and tensor-based functions have the advantage of allowing a relatively straightforward comparison with formal semantics, but the fact that multi-layer neural networks with non-linear activation functions like sigmoid can approximate 10 any continuous function (Cybenko, 1989) already make them an attractive choice. In trying to answer the second question, the advantages of approaches based on neural network architectures, such as the recursive neural network (RNN) model (Socher et al., 2013b) and the convolutional neural network model (Kalchbrenner et al., 2014), are even clearer. Models in this paradigm can take advantage of general learning procedures based on back-propagation, and with the rise of ‘deep learning’, of a variety of efficient algorithms and tricks to further improve training. Since the first success of the RNN model (Socher et al., 2011b) in constituent parsing, two classes of extensions have been proposed. One class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill </context>
<context position="20958" citStr="Kalchbrenner et al., 2014" startWordPosition="3597" endWordPosition="3600">With the LSTM-RNN model, the tanh function, in general, worked best whereas the sigmoid function was the worst. This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014). 5.3 Compared against other Models We compare LSTM-RNN (using tanh) in the previous experiment against existing models: Naive Bayes with bag of bigram features (BiNB), Recursive neural tensor network (RNTN) (Socher et al., 2013b), Convolutional neural network (CNN) (Kim, 2014), Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014). Among them, BiNB is the only one that is not a neural net model. RNTN and DRNN are two extensions of RNN. Whereas RNTN, which keeps the structure of the RNN, uses both matrix-vector multiplication and tensor product for the composition purpose, DRNN makes the net deeper by concatenating more than one RNNs horizontally. CNN, DCNN and PV do not rely on syntactic trees. CNN uses a convolutional layer and a max-pooling layer to handle sequences with different lengths. DCNN is hierarchical in the sense th</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 655–665, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1746--1751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="20885" citStr="Kim, 2014" startWordPosition="3590" endWordPosition="3591"> however, did not yield improvements in this experiment. With the LSTM-RNN model, the tanh function, in general, worked best whereas the sigmoid function was the worst. This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014). 5.3 Compared against other Models We compare LSTM-RNN (using tanh) in the previous experiment against existing models: Naive Bayes with bag of bigram features (BiNB), Recursive neural tensor network (RNTN) (Socher et al., 2013b), Convolutional neural network (CNN) (Kim, 2014), Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014). Among them, BiNB is the only one that is not a neural net model. RNTN and DRNN are two extensions of RNN. Whereas RNTN, which keeps the structure of the RNN, uses both matrix-vector multiplication and tensor product for the composition purpose, DRNN makes the net deeper by concatenating more than one RNNs horizontally. CNN, DCNN and PV do not rely on syntactic trees. CNN uses a convolutional layer and a max-pooling layer to hand</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning (ICML-14),</booktitle>
<pages>1188--1196</pages>
<contexts>
<context position="21005" citStr="Mikolov, 2014" startWordPosition="3607" endWordPosition="3608">ked best whereas the sigmoid function was the worst. This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014). 5.3 Compared against other Models We compare LSTM-RNN (using tanh) in the previous experiment against existing models: Naive Bayes with bag of bigram features (BiNB), Recursive neural tensor network (RNTN) (Socher et al., 2013b), Convolutional neural network (CNN) (Kim, 2014), Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014). Among them, BiNB is the only one that is not a neural net model. RNTN and DRNN are two extensions of RNN. Whereas RNTN, which keeps the structure of the RNN, uses both matrix-vector multiplication and tensor product for the composition purpose, DRNN makes the net deeper by concatenating more than one RNNs horizontally. CNN, DCNN and PV do not rely on syntactic trees. CNN uses a convolutional layer and a max-pooling layer to handle sequences with different lengths. DCNN is hierarchical in the sense that it stacks more than one convolutional layers</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1188–1196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le and Willem Zuidema</author>
</authors>
<title>The insideoutside recursive neural network model for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2684" citStr="Zuidema (2014" startWordPosition="418" endWordPosition="419">aradigm can take advantage of general learning procedures based on back-propagation, and with the rise of ‘deep learning’, of a variety of efficient algorithms and tricks to further improve training. Since the first success of the RNN model (Socher et al., 2011b) in constituent parsing, two classes of extensions have been proposed. One class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al. (2014) for context-dependence sentiment analysis. Our proposal in this paper is an extension of the RNN model to improve compositionality. Our motivation is that, like training recurrent neural networks, training RNNs on deep trees can suffer from the vanishing gradient problem (Hochreiter et al., 2001), i.e., that errors propagated back to the leaf nodes shrink exponentially. In addition, information sent from a leaf node to the root can be obscured if the path between them is long, thus leading to the problem how to capture long range dependencies.</context>
<context position="7537" citStr="Zuidema, 2014" startWordPosition="1270" endWordPosition="1271">ction. Having computed p1, we can then move one level up in the hierarchy and compute p2: p2 = g(W1p1 + W2z + b) (3) This process is continued until we reach the root node. Like training an MLN, training an RNN uses the gradient descent method to minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). The RNN model and its extensions have been employed successfully to solve a wide range of problems: from parsing (constituent parsing (Socher et al., 2013a), dependency parsing (Le and Zuidema, 2014a)) to classification (e.g. sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al., 2011a), semantic role labelling (Le and Zuidema, 2014b)). 2.3 Recurrent Networks and Long Short-Term Memory A neural network is recurrent if it has at least one directed ring in its structure. In the natural language processing field, the simple recurrent neural network (SRN) proposed by Elman (1990) (see Figure 3-left) and its extensions are used to tackle sequence-related problems, such as machine translation (Sutskever et al., 2014) and language modelling (Miko</context>
</contexts>
<marker>Zuidema, 2014</marker>
<rawString>Phong Le and Willem Zuidema. 2014a. The insideoutside recursive neural network model for dependency parsing. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le and Willem Zuidema</author>
</authors>
<title>Inside-outside semantics: A framework for neural models of semantic composition.</title>
<date>2014</date>
<booktitle>In NIPS 2014 Workshop on Deep Learning and Representation Learning.</booktitle>
<contexts>
<context position="2684" citStr="Zuidema (2014" startWordPosition="418" endWordPosition="419">aradigm can take advantage of general learning procedures based on back-propagation, and with the rise of ‘deep learning’, of a variety of efficient algorithms and tricks to further improve training. Since the first success of the RNN model (Socher et al., 2011b) in constituent parsing, two classes of extensions have been proposed. One class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al. (2014) for context-dependence sentiment analysis. Our proposal in this paper is an extension of the RNN model to improve compositionality. Our motivation is that, like training recurrent neural networks, training RNNs on deep trees can suffer from the vanishing gradient problem (Hochreiter et al., 2001), i.e., that errors propagated back to the leaf nodes shrink exponentially. In addition, information sent from a leaf node to the root can be obscured if the path between them is long, thus leading to the problem how to capture long range dependencies.</context>
<context position="7537" citStr="Zuidema, 2014" startWordPosition="1270" endWordPosition="1271">ction. Having computed p1, we can then move one level up in the hierarchy and compute p2: p2 = g(W1p1 + W2z + b) (3) This process is continued until we reach the root node. Like training an MLN, training an RNN uses the gradient descent method to minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). The RNN model and its extensions have been employed successfully to solve a wide range of problems: from parsing (constituent parsing (Socher et al., 2013a), dependency parsing (Le and Zuidema, 2014a)) to classification (e.g. sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al., 2011a), semantic role labelling (Le and Zuidema, 2014b)). 2.3 Recurrent Networks and Long Short-Term Memory A neural network is recurrent if it has at least one directed ring in its structure. In the natural language processing field, the simple recurrent neural network (SRN) proposed by Elman (1990) (see Figure 3-left) and its extensions are used to tackle sequence-related problems, such as machine translation (Sutskever et al., 2014) and language modelling (Miko</context>
</contexts>
<marker>Zuidema, 2014</marker>
<rawString>Phong Le and Willem Zuidema. 2014b. Inside-outside semantics: A framework for neural models of semantic composition. In NIPS 2014 Workshop on Deep Learning and Representation Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Language models based on semantic composition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>430--439</pages>
<contexts>
<context position="1279" citStr="Mitchell and Lapata, 2009" startWordPosition="196" endWordPosition="199">r composition outperformed the traditional neural-network composition on the Stanford Sentiment Treebank. 1 Introduction Moving from lexical to compositional semantics in vector-based semantics requires answers to two difficult questions: (i) what is the nature of the composition functions (given that the lambda calculus for variable binding is no longer applicable), and (ii) how do we learn the parameters of those functions (if they have any) from data? A number of classes of functions have been proposed in answer to the first question, including simple linear functions like vector addition (Mitchell and Lapata, 2009), non-linear functions like those defined by multi-layer neural networks (Socher et al., 2010), and vector matrix multiplication and tensor linear mapping (Baroni et al., 2013). The matrix and tensor-based functions have the advantage of allowing a relatively straightforward comparison with formal semantics, but the fact that multi-layer neural networks with non-linear activation functions like sigmoid can approximate 10 any continuous function (Cybenko, 1989) already make them an attractive choice. In trying to answer the second question, the advantages of approaches based on neural network a</context>
</contexts>
<marker>Mitchell, Lapata, 2009</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2009. Language models based on semantic composition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Romain Paulus</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Global belief recursive neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2888--2896</pages>
<contexts>
<context position="2734" citStr="Paulus et al. (2014)" startWordPosition="424" endWordPosition="427">ing procedures based on back-propagation, and with the rise of ‘deep learning’, of a variety of efficient algorithms and tricks to further improve training. Since the first success of the RNN model (Socher et al., 2011b) in constituent parsing, two classes of extensions have been proposed. One class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al. (2014) for context-dependence sentiment analysis. Our proposal in this paper is an extension of the RNN model to improve compositionality. Our motivation is that, like training recurrent neural networks, training RNNs on deep trees can suffer from the vanishing gradient problem (Hochreiter et al., 2001), i.e., that errors propagated back to the leaf nodes shrink exponentially. In addition, information sent from a leaf node to the root can be obscured if the path between them is long, thus leading to the problem how to capture long range dependencies. We therefore borrow the long short-term memory (L</context>
</contexts>
<marker>Paulus, Socher, Manning, 2014</marker>
<rawString>Romain Paulus, Richard Socher, and Christopher D Manning. 2014. Global belief recursive neural networks. In Advances in Neural Information Processing Systems, pages 2888–2896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12</pages>
<contexts>
<context position="17980" citStr="Pennington et al., 2014" startWordPosition="3125" endWordPosition="3128">ve, neutral, positive, very positive) for 215,154 phrases of 11,855 sentences. The standard splitting is also given: 8544 sentences for training, 1101 for development, and 2210 for testing. The average sentence length is 19.1. In addition, the treebank also supports binary sentiment (positive, negative) classification by removing neutral labels, leading to: 6920 sentences for training, 872 for development, and 1821 for testing. The evaluation metric is the accuracy, given by 100x#correct #total . 5.2 LSTM-RNN vs. RNN Setting We initialized the word vectors by the 100-D GloVe5 word embeddings (Pennington et al., 2014), which were trained on a 6B-word corpus. The initial values for a weight matrix were uniformly sampled from the symmetric interval �− 11 ] where n is the number of total input √n, √n units. 4http://nlp.stanford.edu/sentiment/ treebank.html 5http://nlp.stanford.edu/projects/GloVe/ Figure 6: Boxplots of accuracies of 10 runs of RNN and LSTM-RNN on the test set in the fine-grained classification task. (LSTM stands for LSTM-RNN.) For each model (RNN and LSTM-RNN), we tested three activation functions: softmax, tanh, and softsign, leading to six sub-models. Tuning those sub-models on the developme</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning representations by backpropagating errors.</title>
<date>1988</date>
<booktitle>Cognitive modeling,</booktitle>
<pages>5</pages>
<contexts>
<context position="6006" citStr="Rumelhart et al., 1988" startWordPosition="988" endWordPosition="992">, e.g. sigmoid, tanh, or softsign (see Figure 2). For classification tasks, we put a softmax layer on the top of the network, and compute the probability of assigning a class c to an input x by eu(c,ytop) Pr(c|x) = softmax(c) = u(c, ytop) (1) Ec/EC e where [u(c1, ytop), ..., u(c|C|, ytop)]T = Wytop + b; C is the set of all possible classes; W E R|C|x|yt p|, b E R|C |are a weight matrix and a bias vector. Training an MLN is to minimize an objective function J(θ) where θ is the parameter set (for classification, J(θ) is often a negative log likelihood). Thanks to the back-propagation algorithm (Rumelhart et al., 1988), the gradient ∂J/∂θ is efficiently computed; the gradient descent method thus is used to minimize J. 2.2 Recursive Neural Network A recursive neural network (RNN) (Goller and K¨uchler, 1996) is an MLN where, given a tree structure, we recursively apply the same weight matrices at each inner node in a bottom-up manner. In order to see how an RNN works, consider the following example. Assume that there is a constituent Figure 2: Activation functions: sigmoid(x) = 1 11 with parse tree (p2 (p1 x y) z) (Figure 1-right), and that x, y, z E Rd are the vectorial representations of the three words x, </context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1988</marker>
<rawString>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1988. Learning representations by backpropagating errors. Cognitive modeling, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<contexts>
<context position="1373" citStr="Socher et al., 2010" startWordPosition="209" endWordPosition="212">bank. 1 Introduction Moving from lexical to compositional semantics in vector-based semantics requires answers to two difficult questions: (i) what is the nature of the composition functions (given that the lambda calculus for variable binding is no longer applicable), and (ii) how do we learn the parameters of those functions (if they have any) from data? A number of classes of functions have been proposed in answer to the first question, including simple linear functions like vector addition (Mitchell and Lapata, 2009), non-linear functions like those defined by multi-layer neural networks (Socher et al., 2010), and vector matrix multiplication and tensor linear mapping (Baroni et al., 2013). The matrix and tensor-based functions have the advantage of allowing a relatively straightforward comparison with formal semantics, but the fact that multi-layer neural networks with non-linear activation functions like sigmoid can approximate 10 any continuous function (Cybenko, 1989) already make them an attractive choice. In trying to answer the second question, the advantages of approaches based on neural network architectures, such as the recursive neural network (RNN) model (Socher et al., 2013b) and the </context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>24--801</pages>
<contexts>
<context position="2332" citStr="Socher et al., 2011" startWordPosition="355" endWordPosition="358">nuous function (Cybenko, 1989) already make them an attractive choice. In trying to answer the second question, the advantages of approaches based on neural network architectures, such as the recursive neural network (RNN) model (Socher et al., 2013b) and the convolutional neural network model (Kalchbrenner et al., 2014), are even clearer. Models in this paradigm can take advantage of general learning procedures based on back-propagation, and with the rise of ‘deep learning’, of a variety of efficient algorithms and tricks to further improve training. Since the first success of the RNN model (Socher et al., 2011b) in constituent parsing, two classes of extensions have been proposed. One class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al. (2014) for context-dependence sentiment analysis. Our proposal in this paper is an extension of the RNN model to improve compositionality. Our motivation is that, like training recurrent neural networks, </context>
<context position="7673" citStr="Socher et al., 2011" startWordPosition="1288" endWordPosition="1291">is continued until we reach the root node. Like training an MLN, training an RNN uses the gradient descent method to minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). The RNN model and its extensions have been employed successfully to solve a wide range of problems: from parsing (constituent parsing (Socher et al., 2013a), dependency parsing (Le and Zuidema, 2014a)) to classification (e.g. sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al., 2011a), semantic role labelling (Le and Zuidema, 2014b)). 2.3 Recurrent Networks and Long Short-Term Memory A neural network is recurrent if it has at least one directed ring in its structure. In the natural language processing field, the simple recurrent neural network (SRN) proposed by Elman (1990) (see Figure 3-left) and its extensions are used to tackle sequence-related problems, such as machine translation (Sutskever et al., 2014) and language modelling (Mikolov et al., 2010). In an SRN, an input xt is fed to the network at each time t. The hidden layer h, which has activation ht−1 right befo</context>
<context position="25381" citStr="Socher et al., 2011" startWordPosition="4365" endWordPosition="4368">r RNNs should work very similarly to LSTM for recurrent neural networks, we borrow the argument given in Bengio et al. (2013, Section 3.2) to answer the question. Bengio explains that the LSTM behaves like low-pass filter “hence they can be used to focus certain units on different frequency regions of the data”. This suggests that the LSTM plays a role as a lossy compressor which is to keep global information by focusing on low frequency regions and remove noise by ignoring high frequency regions. So composition in this case could be seen as compression, like the recursive auto-encoder (RAE) (Socher et al., 2011a). Because pre-training an RNN as an RAE can boost the overall performance (Socher et al., 2011a; Socher et al., 2011c), seeing LSTM as a compressor might explain why the LSTM-RNN worked better than RNN without pre-training. Comparing LSTM-RNN against DRNN (Irsoy and Cardie, 2014) gives us a hint about how to improve our model. From the experimental results, LSTM-RNN without the 300-D GloVe word embeddings performed worse than DRNN, while DRNN gained a significant improvement thanks to dropout. Finding a method like dropout that does not corrupt the LSTM memory might boost the overall perform</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. Advances in Neural Information Processing Systems, 24:801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning,</booktitle>
<volume>2</volume>
<contexts>
<context position="2332" citStr="Socher et al., 2011" startWordPosition="355" endWordPosition="358">nuous function (Cybenko, 1989) already make them an attractive choice. In trying to answer the second question, the advantages of approaches based on neural network architectures, such as the recursive neural network (RNN) model (Socher et al., 2013b) and the convolutional neural network model (Kalchbrenner et al., 2014), are even clearer. Models in this paradigm can take advantage of general learning procedures based on back-propagation, and with the rise of ‘deep learning’, of a variety of efficient algorithms and tricks to further improve training. Since the first success of the RNN model (Socher et al., 2011b) in constituent parsing, two classes of extensions have been proposed. One class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al. (2014) for context-dependence sentiment analysis. Our proposal in this paper is an extension of the RNN model to improve compositionality. Our motivation is that, like training recurrent neural networks, </context>
<context position="7673" citStr="Socher et al., 2011" startWordPosition="1288" endWordPosition="1291">is continued until we reach the root node. Like training an MLN, training an RNN uses the gradient descent method to minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). The RNN model and its extensions have been employed successfully to solve a wide range of problems: from parsing (constituent parsing (Socher et al., 2013a), dependency parsing (Le and Zuidema, 2014a)) to classification (e.g. sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al., 2011a), semantic role labelling (Le and Zuidema, 2014b)). 2.3 Recurrent Networks and Long Short-Term Memory A neural network is recurrent if it has at least one directed ring in its structure. In the natural language processing field, the simple recurrent neural network (SRN) proposed by Elman (1990) (see Figure 3-left) and its extensions are used to tackle sequence-related problems, such as machine translation (Sutskever et al., 2014) and language modelling (Mikolov et al., 2010). In an SRN, an input xt is fed to the network at each time t. The hidden layer h, which has activation ht−1 right befo</context>
<context position="25381" citStr="Socher et al., 2011" startWordPosition="4365" endWordPosition="4368">r RNNs should work very similarly to LSTM for recurrent neural networks, we borrow the argument given in Bengio et al. (2013, Section 3.2) to answer the question. Bengio explains that the LSTM behaves like low-pass filter “hence they can be used to focus certain units on different frequency regions of the data”. This suggests that the LSTM plays a role as a lossy compressor which is to keep global information by focusing on low frequency regions and remove noise by ignoring high frequency regions. So composition in this case could be seen as compression, like the recursive auto-encoder (RAE) (Socher et al., 2011a). Because pre-training an RNN as an RAE can boost the overall performance (Socher et al., 2011a; Socher et al., 2011c), seeing LSTM as a compressor might explain why the LSTM-RNN worked better than RNN without pre-training. Comparing LSTM-RNN against DRNN (Irsoy and Cardie, 2014) gives us a hint about how to improve our model. From the experimental results, LSTM-RNN without the 300-D GloVe word embeddings performed worse than DRNN, while DRNN gained a significant improvement thanks to dropout. Finding a method like dropout that does not corrupt the LSTM memory might boost the overall perform</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. 2011b. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 26th International Conference on Machine Learning, volume 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="2332" citStr="Socher et al., 2011" startWordPosition="355" endWordPosition="358">nuous function (Cybenko, 1989) already make them an attractive choice. In trying to answer the second question, the advantages of approaches based on neural network architectures, such as the recursive neural network (RNN) model (Socher et al., 2013b) and the convolutional neural network model (Kalchbrenner et al., 2014), are even clearer. Models in this paradigm can take advantage of general learning procedures based on back-propagation, and with the rise of ‘deep learning’, of a variety of efficient algorithms and tricks to further improve training. Since the first success of the RNN model (Socher et al., 2011b) in constituent parsing, two classes of extensions have been proposed. One class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al. (2014) for context-dependence sentiment analysis. Our proposal in this paper is an extension of the RNN model to improve compositionality. Our motivation is that, like training recurrent neural networks, </context>
<context position="7673" citStr="Socher et al., 2011" startWordPosition="1288" endWordPosition="1291">is continued until we reach the root node. Like training an MLN, training an RNN uses the gradient descent method to minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). The RNN model and its extensions have been employed successfully to solve a wide range of problems: from parsing (constituent parsing (Socher et al., 2013a), dependency parsing (Le and Zuidema, 2014a)) to classification (e.g. sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al., 2011a), semantic role labelling (Le and Zuidema, 2014b)). 2.3 Recurrent Networks and Long Short-Term Memory A neural network is recurrent if it has at least one directed ring in its structure. In the natural language processing field, the simple recurrent neural network (SRN) proposed by Elman (1990) (see Figure 3-left) and its extensions are used to tackle sequence-related problems, such as machine translation (Sutskever et al., 2014) and language modelling (Mikolov et al., 2010). In an SRN, an input xt is fed to the network at each time t. The hidden layer h, which has activation ht−1 right befo</context>
<context position="25381" citStr="Socher et al., 2011" startWordPosition="4365" endWordPosition="4368">r RNNs should work very similarly to LSTM for recurrent neural networks, we borrow the argument given in Bengio et al. (2013, Section 3.2) to answer the question. Bengio explains that the LSTM behaves like low-pass filter “hence they can be used to focus certain units on different frequency regions of the data”. This suggests that the LSTM plays a role as a lossy compressor which is to keep global information by focusing on low frequency regions and remove noise by ignoring high frequency regions. So composition in this case could be seen as compression, like the recursive auto-encoder (RAE) (Socher et al., 2011a). Because pre-training an RNN as an RAE can boost the overall performance (Socher et al., 2011a; Socher et al., 2011c), seeing LSTM as a compressor might explain why the LSTM-RNN worked better than RNN without pre-training. Comparing LSTM-RNN against DRNN (Irsoy and Cardie, 2014) gives us a hint about how to improve our model. From the experimental results, LSTM-RNN without the 300-D GloVe word embeddings performed worse than DRNN, while DRNN gained a significant improvement thanks to dropout. Finding a method like dropout that does not corrupt the LSTM memory might boost the overall perform</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011c. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>455--465</pages>
<contexts>
<context position="1962" citStr="Socher et al., 2013" startWordPosition="296" endWordPosition="299">networks (Socher et al., 2010), and vector matrix multiplication and tensor linear mapping (Baroni et al., 2013). The matrix and tensor-based functions have the advantage of allowing a relatively straightforward comparison with formal semantics, but the fact that multi-layer neural networks with non-linear activation functions like sigmoid can approximate 10 any continuous function (Cybenko, 1989) already make them an attractive choice. In trying to answer the second question, the advantages of approaches based on neural network architectures, such as the recursive neural network (RNN) model (Socher et al., 2013b) and the convolutional neural network model (Kalchbrenner et al., 2014), are even clearer. Models in this paradigm can take advantage of general learning procedures based on back-propagation, and with the rise of ‘deep learning’, of a variety of efficient algorithms and tricks to further improve training. Since the first success of the RNN model (Socher et al., 2011b) in constituent parsing, two classes of extensions have been proposed. One class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy an</context>
<context position="7493" citStr="Socher et al., 2013" startWordPosition="1262" endWordPosition="1265">here b is a bias vector and g is an activation function. Having computed p1, we can then move one level up in the hierarchy and compute p2: p2 = g(W1p1 + W2z + b) (3) This process is continued until we reach the root node. Like training an MLN, training an RNN uses the gradient descent method to minimize an objective function J(O). The gradient aJ/aO is efficiently computed thanks to the back-propagation through structure algorithm (Goller and K¨uchler, 1996). The RNN model and its extensions have been employed successfully to solve a wide range of problems: from parsing (constituent parsing (Socher et al., 2013a), dependency parsing (Le and Zuidema, 2014a)) to classification (e.g. sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al., 2011a), semantic role labelling (Le and Zuidema, 2014b)). 2.3 Recurrent Networks and Long Short-Term Memory A neural network is recurrent if it has at least one directed ring in its structure. In the natural language processing field, the simple recurrent neural network (SRN) proposed by Elman (1990) (see Figure 3-left) and its extensions are used to tackle sequence-related problems, such as machine translation (Sutskeve</context>
<context position="17277" citStr="Socher et al., 2013" startWordPosition="3020" endWordPosition="3023">N×6×d×dw+(N−2)×10×d×d+(N−1)×d×d If dw ≈ d, the complexity of LSTM-RNN is about 8.5 times higher than the complexity of RNN. 1 � J(0) = − |D| s∈D � p∈s 14 Figure 5: The RNN model (left) and LSTM-RNN model (right) for sentiment analysis. In our experiments, this difference is not a problem because training and evaluating the LSTMRNN model is very fast: it took us, on a single core of a modern computer, about 10 minutes to train the model (d = 50, dw = 100) on 8544 sentences, and about 2 seconds to evaluate it on 2210 sentences. 5 Experiments 5.1 Dataset We used the Stanford Sentiment Treebank4 (Socher et al., 2013b) which consists of 5-way fine-grained sentiment labels (very negative, negative, neutral, positive, very positive) for 215,154 phrases of 11,855 sentences. The standard splitting is also given: 8544 sentences for training, 1101 for development, and 2210 for testing. The average sentence length is 19.1. In addition, the treebank also supports binary sentiment (positive, negative) classification by removing neutral labels, leading to: 6920 sentences for training, 872 for development, and 1821 for testing. The evaluation metric is the accuracy, given by 100x#correct #total . 5.2 LSTM-RNN vs. RN</context>
<context position="20835" citStr="Socher et al., 2013" startWordPosition="3582" endWordPosition="3585">tter than tanh for deep networks (Glorot and Bengio, 2010), however, did not yield improvements in this experiment. With the LSTM-RNN model, the tanh function, in general, worked best whereas the sigmoid function was the worst. This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014). 5.3 Compared against other Models We compare LSTM-RNN (using tanh) in the previous experiment against existing models: Naive Bayes with bag of bigram features (BiNB), Recursive neural tensor network (RNTN) (Socher et al., 2013b), Convolutional neural network (CNN) (Kim, 2014), Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014). Among them, BiNB is the only one that is not a neural net model. RNTN and DRNN are two extensions of RNN. Whereas RNTN, which keeps the structure of the RNN, uses both matrix-vector multiplication and tensor product for the composition purpose, DRNN makes the net deeper by concatenating more than one RNNs horizontally. CNN, DCNN and PV do not rely on syntactic trees. CNN uses a c</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013a. Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455–465.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<location>and</location>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, </marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings EMNLP.</booktitle>
<marker>Potts, 2013</marker>
<rawString>Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="8108" citStr="Sutskever et al., 2014" startWordPosition="1357" endWordPosition="1360">l., 2013a), dependency parsing (Le and Zuidema, 2014a)) to classification (e.g. sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al., 2011a), semantic role labelling (Le and Zuidema, 2014b)). 2.3 Recurrent Networks and Long Short-Term Memory A neural network is recurrent if it has at least one directed ring in its structure. In the natural language processing field, the simple recurrent neural network (SRN) proposed by Elman (1990) (see Figure 3-left) and its extensions are used to tackle sequence-related problems, such as machine translation (Sutskever et al., 2014) and language modelling (Mikolov et al., 2010). In an SRN, an input xt is fed to the network at each time t. The hidden layer h, which has activation ht−1 right before xt comes in, plays a role as a memory store capturing the whole history (x0, ..., xt−1). When xt comes in, the hidden layer updates its activation by ht = g(Wxxt + Whht−1 + b) Figure 3: Simple recurrent neural network (left) and long short-term memory (right). Bias vectors are removed for the simplicity. where Wx E R|h|×|xt|, Wh E R|h|×|h|, b E R|h| are weight matrices and a bias vector; g is an activation. This network model th</context>
<context position="20607" citStr="Sutskever et al., 2014" startWordPosition="3546" endWordPosition="3549">l, comparably with the other two functions in the fine-grained task, and even better than the softsign function in the binary task, given that it was not often chosen in recent work. The softsign function, which was shown to work better than tanh for deep networks (Glorot and Bengio, 2010), however, did not yield improvements in this experiment. With the LSTM-RNN model, the tanh function, in general, worked best whereas the sigmoid function was the worst. This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014). 5.3 Compared against other Models We compare LSTM-RNN (using tanh) in the previous experiment against existing models: Naive Bayes with bag of bigram features (BiNB), Recursive neural tensor network (RNTN) (Socher et al., 2013b), Convolutional neural network (CNN) (Kim, 2014), Dynamic convolutional neural network (DCNN) (Kalchbrenner et al., 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014). Among them, BiNB is the only one that is not a neural net model. RNTN and DRNN are two extensions of RNN. Whereas RNTN, which keeps the structure of the R</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075.</title>
<date>2015</date>
<contexts>
<context position="14829" citStr="Tai et al. (2015)" startWordPosition="2592" endWordPosition="2595"> nodes and inner nodes: we use one weight matrix set for leaf nodes and another set for inner nodes. Hence, let dw and d respectively be the dimensions of word embeddings (leaf nodes) and vector representations of phrases (inner nodes), all weight matrices from a leaf node to an inner node have size d × dw, and all weight matrices from an inner node to another inner node have size d × d. 3The LSTM architecture was already applied to the sentiment analysis task, for instance in the model proposed at http://deeplearning.net/tutorial/lstm. html. Independently from and concurrently with our work, Tai et al. (2015) and Zhu et al. (2015) have developed very similar models applying LTSM to RNNs. Training Training this model is to minimize the following objective function, which is the crossentropy over training sentence set D plus an L2- norm regularization term A log Pr(cp|p) + 2||0||2 where 0 is the parameter set, cp is the sentiment class of phrase p, p is the vector representation at the node covering p, Pr(cp|p) is computed by the softmax function, and A is the regularization parameter. Like training an RNN, we use the mini-batch gradient descent method to minimize J, where the gradient aJ/a0 is comp</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul J Werbos</author>
</authors>
<title>Backpropagation through time: what it does and how to do it.</title>
<date>1990</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>78--10</pages>
<contexts>
<context position="8898" citStr="Werbos, 1990" startWordPosition="1498" endWordPosition="1500">in, plays a role as a memory store capturing the whole history (x0, ..., xt−1). When xt comes in, the hidden layer updates its activation by ht = g(Wxxt + Whht−1 + b) Figure 3: Simple recurrent neural network (left) and long short-term memory (right). Bias vectors are removed for the simplicity. where Wx E R|h|×|xt|, Wh E R|h|×|h|, b E R|h| are weight matrices and a bias vector; g is an activation. This network model thus, in theory, can be used to estimate probabilities conditioning on long histories. And computing gradients is efficient thanks to the back-propagation through time algorithm (Werbos, 1990). In practice, however, training recurrent neural networks with the gradient descent method is challenging because gradients aJt/ahj (j G t, Jt is the objective function at time t) vanish quickly after a few back-propagation steps (Hochreiter et al., 2001). In addition, it is difficult to capture long range dependencies, i.e. the output at time t depends on some inputs that happened very long time ago. One solution for this, proposed by Hochreiter and Schmidhuber (1997) and enhanced by Gers (2001), is long short-term memory (LSTM). Long Short-Term Memory The main idea of the LSTM architecture </context>
</contexts>
<marker>Werbos, 1990</marker>
<rawString>Paul J Werbos. 1990. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550–1560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Parinaz Sobhani</author>
<author>Hongyu Guo</author>
</authors>
<title>Long short-term memory over tree structures. arXiv preprint arXiv:1503.04881.</title>
<date>2015</date>
<contexts>
<context position="14851" citStr="Zhu et al. (2015)" startWordPosition="2597" endWordPosition="2600">: we use one weight matrix set for leaf nodes and another set for inner nodes. Hence, let dw and d respectively be the dimensions of word embeddings (leaf nodes) and vector representations of phrases (inner nodes), all weight matrices from a leaf node to an inner node have size d × dw, and all weight matrices from an inner node to another inner node have size d × d. 3The LSTM architecture was already applied to the sentiment analysis task, for instance in the model proposed at http://deeplearning.net/tutorial/lstm. html. Independently from and concurrently with our work, Tai et al. (2015) and Zhu et al. (2015) have developed very similar models applying LTSM to RNNs. Training Training this model is to minimize the following objective function, which is the crossentropy over training sentence set D plus an L2- norm regularization term A log Pr(cp|p) + 2||0||2 where 0 is the parameter set, cp is the sentiment class of phrase p, p is the vector representation at the node covering p, Pr(cp|p) is computed by the softmax function, and A is the regularization parameter. Like training an RNN, we use the mini-batch gradient descent method to minimize J, where the gradient aJ/a0 is computed efficiently thank</context>
</contexts>
<marker>Zhu, Sobhani, Guo, 2015</marker>
<rawString>Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long short-term memory over tree structures. arXiv preprint arXiv:1503.04881.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>