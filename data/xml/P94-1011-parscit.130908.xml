<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<sectionHeader confidence="0.4477315" genericHeader="abstract">
PRECISE N-GRAM PROBABILITIES FROM
STOCHASTIC CONTEXT-FREE GRAMMARS
</sectionHeader>
<author confidence="0.992617">
Andreas Stolcke and Jonathan Segal
</author>
<affiliation confidence="0.969139">
University of California, Berkeley
and
International Computer Science Institute
</affiliation>
<address confidence="0.885057">
1947 Center Street
Berkeley, CA 94704
</address>
<email confidence="0.9988">
fstolcke,jsegall@icsi.berkeley.edu
</email>
<sectionHeader confidence="0.996658" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999972777777778">
We present an algorithm for computing n-gram probabil-
ities from stochastic context-free grammars, a procedure
that can alleviate some of the standard problems associated
with n-grams (estimation from sparse data, lack of linguis-
tic structure, among others). The method operates via the
computation of substring expectations, which in turn is ac-
complished by solving systems of linear equations derived
from the grammar. The procedure is fully implemented and
has proved viable and useful in practice.
</bodyText>
<sectionHeader confidence="0.997909" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999886457142857">
Probabilistic language modeling with n-gram grammars
(particularly bigram and trigram) has proven extremely use-
ful for such tasks as automated speech recognition, part-of-
speech tagging, and word-sense disambiguation, and lead to
simple, efficient algorithms. Unfortunately, working with
these grammars can be problematic for several reasons: they
have large numbers of parameters, so reliable estimation
requires a very large training corpus and/or sophisticated
smoothing techniques (Church and Gale, 1991); it is very
hard to directly model linguistic knowledge (and thus these
grammars are practically incomprehensible to human inspec-
tion); and the models are not easily extensible, i.e., if a new
word is added to the vocabulary, none of the information
contained in an existing n-gram will tell anything about the
n-grams containing the new item. Stochastic context-free
grammars (SCFGs), on the other hand, are not as suscep-
tible to these problems: they have many fewer parameters
(so can be reasonably trained with smaller corpora); they
capture linguistic generalizations, and are easily understood
and written, by linguists; and they can be extended straight-
forwardly based on the underlying linguistic knowledge.
In this paper, we present a technique for computing an
n-gram grammar from an existing SCFG—an attempt to get
the best of both worlds. Besides developing the mathematics
involved in the computation, we also discuss efficiency and
implementation issues, and briefly report on our experience
confirming its practical feasibility and utility.
The technique of compiling higher-level grammat-
ical models into lower-level ones has precedents:
Zue et al. (1991) report building a word-pair grammar from
more elaborate language models to achieve good coverage,
by random generation of sentences. In our own group,
the current approach was predated by an alternative one
that essentially relied on approximating bigram probabili-
ties through Monte-Carlo sampling from SCFGs.
</bodyText>
<sectionHeader confidence="0.883189" genericHeader="background">
PRELIMINARIES
</sectionHeader>
<bodyText confidence="0.999756461538462">
An n-gram grammar is a set of probabil-
ities F(wn I wi w2 ton--i), giving the probability that wn
follows a word string WI W2. . . , for each possible com-
bination of the w&apos;s in the vocabulary of the language. So
for a 5000 word vocabulary, a bigram grammar would have
approximately 5000 x 5000 = 25, 000, 000 free parameters,
and a trigram grammar would have 125, 000, 000, 000.
This is what we mean when we say n-gram grammars have
many parameters.
A SCFG is a set of phrase-structure rules, annotated with
probabilities of choosing a certain production given the left-
hand side nonterminal. For example, if we have a simple
CFG, we can augment it with the probabilities specified:
</bodyText>
<table confidence="0.9948543">
S -4 NP VP 1.0
NP N 0.4
NP -+ Det N 0.6
V P -+ V 0.8
VP -4 V N P 0.2
Det -4 the 0.4
Det -4 a 0.6
N —&gt; book 1.0
V —&gt; close 0.3
V -4 open 0.7
</table>
<bodyText confidence="0.864286666666667">
The language this grammar generates contains 5 words.
Including markers for sentence beginning and end, a bigram
grammar would contain 6 x 6 probabilities, or 6 x 5 = 30
</bodyText>
<page confidence="0.997787">
74
</page>
<bodyText confidence="0.999840684210526">
free parameters (since probabilities have to sum to one). A
trigram grammar would come with (5 x 6 + 1) x 5 = 155
parameters. Yet, the above SCFG has only 10 probabilities,
only 4 of which are free parameters. The divergence between
these two types of models generally grows as the vocabulary
size increases, although this depends on the productions in
the SCFG.
The reason for this discrepancy, of course, is that the struc-
ture of the SCFG itself is a discrete (hyper-)parameter with a
lot of potential variation, but one that has been fixed before-
hand. The point is that such a structure is comprehensible by
humans, and can in many cases be constrained using prior
knowledge, thereby reducing the estimation problem for the
remaining probabilities. The problem of estimating SCFG
parameters from data is solved with standard techniques,
usually by way of likelihood maximization and a variant of
the Baum-Welch (EM) algorithm (Baker, 1979). A tutorial
introduction to SCFGs and standard algorithms can be found
in Jelinek et al. (1992).
</bodyText>
<sectionHeader confidence="0.94615" genericHeader="method">
MOTIVATION
</sectionHeader>
<bodyText confidence="0.99995075">
There are good arguments that SCFGs are in principle not ad-
equate probabilistic models for natural languages, due to the
conditional independence assumptions they embody (Mager-
man and Marcus, 1991; Jones and Eisner, 1992; Briscoe and
Carroll, 1993). Such shortcomings can be partly remedied
by using SCFGs with very specific, semantically oriented
categories and rules (Jurafsky et al., 1994). If the goal is to
use n-grams nevertheless, then their their computation from
a more constrained SCFG is still useful since the results can
be interpolated with raw n-gram estimates for smoothing.
An experiment illustrating this approach is reported later in
the paper.
On the other hand, even if vastly more sophisticated lan-
guage models give better results, n-grams will most likely
still be important in applications such as speech recogni-
tion. The standard speech decoding technique of frame-
synchronous dynamic programming (Ney, 1984) is based
on a first-order Markov assumption, which is satisfied by bi-
grams models (as well as by Hidden Markov Models), but not
by more complex models incorporating non-local or higher-
order constraints (including SCFGs). A standard approach is
therefore to use simple language models to generate a prelim-
inary set of candidate hypotheses. These hypotheses, e.g.,
represented as word lattices or N-best lists (Schwartz and
Chow, 1990), are re-evaluated later using additional criteria
that can afford to be more costly due to the more constrained
outcomes. In this type of setting, the techniques developed
in this paper can be used to compile probabilistic knowledge
present in the more elaborate language models into n-gram
estimates that improve the quality of the hypotheses gener-
ated by the decoder.
Finally, comparing directly estimated, reliable n-grams
with those compiled from other language models is a poten-
tially useful method for evaluating the models in question.
For the purpose of this paper, then, we assume that comput-
ing n-grams from SCFGs is of either practical or theoretical
interest and concentrate on the computational aspects of the
problem.
It should be noted that there are alternative, unrelated
methods for addressing the problem of the large parameter
space in n-gram models. For example, Brown et al. (1992)
describe an approach based on grouping words into classes,
thereby reducing the number of conditional probabilities in
the model.
</bodyText>
<subsectionHeader confidence="0.5856365">
THE ALGORITHM
Normal form for SCFGs
</subsectionHeader>
<bodyText confidence="0.998453">
A grammar is in Chomsky Normal Form (CNF) if every
production is of the form A ./3 C or A terminal.
Any CFG or SCFG can be converted into one in CNF which
generates exactly the same language, each of the sentences
with exactly the same probability, and for which any parse in
the original grammar would be reconstructible from a parse
in the CNF grammar. In short, we can, without loss of
generality, assume that the SCFGs we are dealing with are
in CNF. In fact, our algorithm generalizes straightforwardly
to the more general Canonical Two-Form (Graham et al.,
1980) format, and in the case of bigrams (n = 2) it can even
be modified to work directly for arbitrary SCFGs. Still, the
CNF form is convenient, and to keep the exposition simple
we assume all SCFGs to be in CNF.
</bodyText>
<subsectionHeader confidence="0.931766">
Probabilities from expectations
</subsectionHeader>
<bodyText confidence="0.999938333333333">
The first key insight towards a solution is that the n-gram
probabilities can be obtained from the associated expected
frequencies for n-grams and (n — 1)-grams:
</bodyText>
<equation confidence="0.983345">
c(wi wri IL)
P(wnlwiw2 • • • wn-1) = (1)
c(wi IL)
</equation>
<bodyText confidence="0.998761">
where c(wIL) stands for the expected count of occurrences
of the substring w in a sentence of L.&apos;
Proof Write the expectation for n-grams recursively in
terms of those of order n — 1 and the conditional n-gram
probabilities:
</bodyText>
<equation confidence="0.532017">
IL) = c(wi . . . wn_ilL)P(wn Iwt w2 • • • wn-1).
</equation>
<bodyText confidence="0.999731333333333">
So if we can compute c(wIG) for all substrings w of
lengths n and n — 1 for a SCFG G, we immediately have an
n-gram grammar for the language generated by G.
</bodyText>
<subsectionHeader confidence="0.96965">
Computing expectations
</subsectionHeader>
<bodyText confidence="0.887479777777778">
Our goal now is to compute the substring expectations for
a given grammar. Formalisms such as SCFGs that have a
recursive rule structure suggest a divide-and-conquer algo-
rithm that follows the recursive structure of the grammar.2
We generalize the problem by considering e(w IX), the
expected number of (possibly overlapping) occurrences of
&apos;The only counts appearing in this paper are expectations, so
be will not be using special notation to make a distinction between
observed and expected values.
</bodyText>
<footnote confidence="0.9474895">
2A similar, even simpler approach applies to probabilistic finite
state (i.e., Hidden Markov) models.
</footnote>
<page confidence="0.994077">
75
</page>
<figure confidence="0.998243">
(a) (b) (c)
</figure>
<figureCaption confidence="0.999952">
Figure 1: Three ways of generating a substring w from a nonterminal X.
</figureCaption>
<bodyText confidence="0.822373066666667">
w = wi wn in strings generated by an arbitrary nonter-
minal X. The special case c(w1S) = c(w IL) is the solution
sought, where S is the start symbol for the grammar.
Now consider all possible ways that nonterminal X can
generate string w = wi wn as a substring, denoted by
X 4. ...wi tun ..., and the associated probabilities. For
each production of X we have to distinguish two main cases,
assuming the grammar is in CNF. If the string in question is
of length 1, w = wi , and if X happens to have a production
X WI, then that production adds exactly P(X WO to
the expectation c(w IX).
If X has non-terminal productions, say, X —+ Y Z then
w might also be generated by recursive expansion of the
right-hand side. Here, for each production, there are three
subcases.
</bodyText>
<listItem confidence="0.96045825">
(a) First, Y can by itself generate the complete w (see
Figure 1(a)).
(b) Likewise, Z itself can generate w (Figure 1(b)).
(c) Finally, Y could generate w1 wi as a suffix (Y
</listItem>
<equation confidence="0.9097275">
W1 • • •W3) and Z, wi±i wn as a prefix (Z
wi+) • • • wn), thereby resulting in a single occurrence
</equation>
<bodyText confidence="0.98221675">
of w (Figure 1(c)).3
Each of these cases will have an expectation for generating
Wi . wn as a substring, and the total expectation c(w1X)
will be the sum of these partial expectations. The total
expectations for the first two cases (that of the substring
being completely generated by Y or Z) are given recursively:
c(w IY) and c(wlY) respectively. The expectation for the
third case is
</bodyText>
<equation confidence="0.949849">
n-1
EPcy R W1 • • •WAP(Z L Ipj-I-1 • • • Wn), (2)
3=1
</equation>
<bodyText confidence="0.993422">
where one has to sum over all possible split points j of the
string w.
</bodyText>
<footnote confidence="0.514463">
3We use the notation X 4R a to denote that non-terminal X
</footnote>
<bodyText confidence="0.967669714285714">
generates the string a as a suffix, and X a to denote that X
generates a as a prefix. Thus P(X r a) and P(X iii a) are
the probabilities associated with those events.
To compute the total expectation c(w ( X), then, we have
to sum over all these choices: the production used (weighted
by the rule probabilities), and for each nonterminal rule the
three cases above. This gives
</bodyText>
<equation confidence="0.999866625">
c(wIX) = P(X --÷ w)
+ E P(X -4 Y Z)
X -).YZ
(C(WIY) C(WIZ)
n-1
+E POT W1 • • wi)
j.1
P(Z w3+1 • • • Wn))
</equation>
<bodyText confidence="0.9986875">
In the important special case of bigrams, this summation
simplifies quite a bit, since the terminal productions are ruled
out and splitting into prefix and suffix allows but one possi-
bility:
</bodyText>
<equation confidence="0.995620875">
c(wi w21x) E P(X Y Z)
X—&gt;YZ
(c(wiw21Y) + c(wiwziZ)
+P(Y R Illi)P(Z L W2))
For unigrams equation (3) simplifies even more:
c(wiPC) = P(X wi)
+ E P(X Y Z)(c(wilY) + c(wilZ))
X--*Y Z
</equation>
<bodyText confidence="0.997540428571429">
We now have a recursive specification of the quantities
c(w I X) we need to compute. Alas, the recursion does not
necessarily bottom out, since the c(wlY) and c(w1Z) quanti-
ties on the right side of equation (3) may depend themselves
on c(wIX). Fortunately, the recurrence is linear, so for each
string w, we can find the solution by solving the linear system
formed by all equations of type (3). Notice there are exactly
</bodyText>
<page confidence="0.927534">
76
</page>
<bodyText confidence="0.99049056097561">
as many equations as variables, equal to the number of non-
terminals in the grammar. The solution of these systems is
further discussed below.
Computing prefix and suffix probabilities
The only substantial problem left at this point is the com-
putation of the constants in equation (3). These are derived
from the rule probabilities P(X w) and P(X YZ),
as well as the prefix/suffix generation probabilities P(Y
Wi . .W3) and P(Z in-3+1 • • • wn)•
The computation of prefix probabilities for SCFGs is gen-
erally useful for applications, and has been solved with
the LRI algorithm (Jelinek and Lafferty, 1991). Recently,
Stoleke (1993) has shown how to perform this computation
efficiently for sparsely parameterized SCFGs using a proba-
bilistic version of Earley&apos;s parser (Earley, 1970). Computing
suffix probabilities is obviously a symmetrical task; for ex-
ample, one could create a &apos;mirrored&apos; SCFG (reversing the
order of right-hand side symbols in all productions) and then
run any prefix probability computation on that mirror gram-
mar.
Note that in the case of bigrams, only a particularly simple
form of prefix/suffix probabilities are required, namely, the
left-corner&apos; and `right-corner&apos; probabilities, P(X L WI)
and P(Y W2), which can each be obtained from a single
matrix inversion (Jelinek and Lafferty, 1991).
It should be mentioned that there are some technical con-
ditions that have to be met for a SCFG to be well-defined
and consistent (Booth and Thompson, 1973). These condi-
tion are also sufficient to guarantee that the linear equations
given by (3) have positive probabilities as solutions. The
details of this are discussed in the Appendix.
Finally, it is interesting to compare the relative ease with
which one can solve the substring expectation problem to the
seemingly similar problem of finding substring probabilities:
the probability that X generates (one or more instances of)
w. The latter problem is studied by Corazza et al. (1991),
and shown to lead to a non-linear system of equations. The
crucial difference here is that expectations are additive with
respect to the cases in Figure 1, whereas the corresponding
probabilities are not, since the three cases can occur simul-
taneously.
</bodyText>
<sectionHeader confidence="0.995338" genericHeader="method">
EFFICIENCY AND COMPLEXITY ISSUES
</sectionHeader>
<bodyText confidence="0.999937833333333">
Summarizing from the previous section, we can compute
any n-gram probability by solving two linear systems of
equations of the form (3), one with w being the n-gram itself
and one for the (n — 1)-gram prefix wi n_.The latter
computation can be shared among all n-grams with the same
prefix, so that essentially one system needs to be solved for
each n-gram we are interested in. The good news here is that
the work required is linear in the number of n-grams, and
correspondingly limited if one needs probabilities for only
a subset of the possible n-grams. For example, one could
compute these probabilities on demand and cache the results.
Let us examine these systems of equations one more time.
</bodyText>
<equation confidence="0.9759305">
Each can be written in matrix notation in the form
(I — A)c = b (6)
</equation>
<bodyText confidence="0.9626878">
where I is the identity matrix, A = (axu) is a coefficient
matrix, b = (bx) is the right-hand side vector, and c rep-
resents the vector of unknowns, c(w(X). All of these are
indexed by nonterminals X, U.
We get
</bodyText>
<equation confidence="0.999864777777778">
axu =E P(X Y Z)(6(Y,U) + o(Z,U))(7)
X--0
bx = P(X 111)
+ E P(X Y Z)
x—).Yz
n-1
EP(/ W1 • • •W.2)
j=--1
P(Z L w3+1 wn) (8)
</equation>
<bodyText confidence="0.99998175">
where 6(X, Y) -= 1 if X -= Y, and 0 otherwise. The
expression I — A arises from bringing the variables c(w I Y)
and c(w1Z) to the other side in equation (3) in order to collect
the coefficients.
We can see that all dependencies on the particular bigram,
w, are in the right-hand side vector b, while the coefficient
matrix I — A depends only on the grammar. This, together
with the standard method of LU decomposition (see, e.g.,
Press et al. (1988)) enables us to solve for each bigram in
time 0(N2), rather than the standard 0(N3) for a full sys-
tem (N being the number of nonterminals/variables). The
LU decomposition itself is cubic, but is incurred only once.
The full computation is therefore dominated by the quadratic
effort of solving the system for each n-gram. Furthermore,
the quadratic cost is a worst-case figure that would be in-
curred only if the grammar contained every possible rule;
empirically we have found this computation to be linear in the
number of nonterminals, for grammars that are sparse, i.e.,
where each nonterminal makes reference only to a bounded
number of other nonterminals.
</bodyText>
<sectionHeader confidence="0.978256" genericHeader="method">
SUMMARY
</sectionHeader>
<bodyText confidence="0.999455">
Listed below are the steps of the complete computation. For
concreteness we give the version specific to bigrams (n = 2).
</bodyText>
<listItem confidence="0.997721">
1. Compute the prefix (left-corner) and suffix (right-
corner) probabilities for each (nonterminal,word) pair.
2. Compute the coefficient matrix and right-hand sides for
the systems of linear equations, as per equations (4)
and (5).
3. LU decompose the coefficient matrix.
4. Compute the unigram expectations for each word in the
grammar, by solving the LU system for the unigram
right-hand sides computed in step 2.
5. Compute the bigram expectations for each word pair by
solving the LU system for the bigram right-hand sides
computed in step 2.
</listItem>
<page confidence="0.93824">
77
</page>
<listItem confidence="0.464531666666667">
6. Compute each bigram probability P(w2I /Di ), by divid-
ing the bigram expectation c(wi w2 IS) by the unigram
expectation c(w I IS).
</listItem>
<sectionHeader confidence="0.663971" genericHeader="evaluation">
EXPERIMENTS
</sectionHeader>
<bodyText confidence="0.999858487179487">
The algorithm described here has been implemented, and
is being used to generate bigrams for a speech recognizer
that is part of the BeRP spoken-language system (Jurafsky
et al., 1994). An early prototype of BeRP was used in an
experiment to assess the benefit of using bigram probabili-
ties obtained through SCFGs versus estimating them directly
from the available training corpus.4 The system&apos;s domain are
inquiries about restaurants in the city of Berkeley. The train-
ing corpus used had only 2500 sentences, with an average
length of about 4.8 words/sentence.
Our experiments made use of a context-free grammar
hand-written for the BeRP domain. With 1200 rules and
a vocabulary of 1100 words, this grammar was able to parse
60% of the training corpus. Computing the bigram proba-
bilities from this SCFG takes about 24 hours on a SPARC-
station 2-class machine.5
In experiment 1, the recognizer used bigrams that were
estimated directly from the training corpus, without any
smoothing, resulting in a word error rate of 35.1%. In ex-
periment 2, a different set of bigram probabilities was used,
computed from the context-free grammar, whose probabil-
ities had previously been estimated from the same training
corpus, using standard EM techniques. This resulted in a
word error rate of 35.3%. This may seem surprisingly good
given the low coverage of the underlying CFGs, but notice
that the conversion into bigrams is bound to result in a less
constraining language model, effectively increasing cover-
age.
Finally, in experiment 3, the bigrams generated from the
SCFG were augmented by those from the raw training data,
in a proportion of 200,000: 2500. We have not attempted to
optimize this mixture proportion, e.g., by deleted interpola-
tion (Jelinek and Mercer, 1980).6 With the bigram estimates
thus obtained, the word error rate dropped to 33.5%. (All
error rates were measured on a separate test corpus.)
The experiment therefore supports our earlier argument
that more sophisticated language models, even if far from
perfect, can improve n-gram estimates obtained directly
from sample data.
</bodyText>
<footnote confidence="0.916602928571429">
4Corpus and grammar sizes, as well as the recognition per-
formance figures reported here are not up-to-date with respect to
the latest version of BeRP. For ACL-94 we expect to have revised
results available that reflect the current performance of the system.
5Unlike the rest of BeRP, this computation is implemented in
Lisp/CLOS and could be speeded up considerably if necessary.
6This proportion comes about because in the original system,
predating the method described in this paper, bigrams had to be
estimated from the SCFG by random sampling. Generating 200,000
sentence samples was found to give good converging estimates for
the bigrams. The bigrams from the raw training sentences were then
simply added to the randomly generated ones. We later verified that
the bigrams estimated from the SCFG were indeed identical to the
ones computed directly using the method described here.
</footnote>
<sectionHeader confidence="0.776883" genericHeader="conclusions">
CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999886">
We. have described an algorithm to compute in closed form
the distribution of n-grams for a probabilistic language
given by a stochastic context-free grammar. Our method
is based on computing substring expectations, which can be
expressed as systems of linear equations derived from the
grammar. The algorithm was used successfully and found
to be practical in dealing with context-free grammars and
bigram models for a medium-scale speech recognition task,
where it helped to improve bigram estimates obtained from
relatively small amounts of data.
Deriving n-gram probabilities from more sophisticated
language models appears to be a generally useful technique
which can both improve upon direct estimation of n-grams,
and allow available higher-level linguistic knowledge to be
effectively integrated into the speech decoding task.
</bodyText>
<sectionHeader confidence="0.999363" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.998068111111111">
Dan Jurafsky wrote the BeRP grammar, carried out the recog-
nition experiments, and was generally indispensable. Steve
Omohundro planted the seed for our n-gram algorithm dur-
ing lunch at the California Dream Café by suggesting sub-
string expectations as an interesting computational linguis-
tics problem. Thanks also to Jerry Feldman and Lokendra
Shastri for improving the presentation with their comments.
This research has been supported by ICSI and ARPA con-
tract #N0000 1493 CO249.
</bodyText>
<sectionHeader confidence="0.998384" genericHeader="references">
APPENDIX: CONSISTENCY OF SCFGS
</sectionHeader>
<bodyText confidence="0.998460333333333">
Blindly applying the n-gram algorithm (and many others)
to a SCFG with arbitrary probabilities can lead to surprising
results. Consider the following simple grammar
</bodyText>
<equation confidence="0.7068055">
S x [p]
S -4 SS [q = 1 — p]
</equation>
<bodyText confidence="0.816998">
What is the expected frequency of unigram x? Using the
abbreviation c = c(XIS) and equation 5, we see that
</bodyText>
<equation confidence="0.9725026">
c = P(S x) + P(S S S)(c + c)
= p + 2qc
This leads to
c= (10)
1 — 2q 2p — 1
</equation>
<bodyText confidence="0.999170076923077">
Now, for p = 0.5 this becomes infinity, and for probabilities
p &lt; 0.5, the solution is negative! This is a rather striking
manifestation of the failure of this grammar, for p &lt; 0.5,
to be consistent. A grammar is said to be inconsistent if
the underlying stochastic derivation process has non-zero
probability of not terminating (Booth and Thompson, 1973).
The expected length of the generated strings should therefore
be infinite in this case.
Fortunately, Booth and Thompson derive a criterion for
checking the consistency of a SCFG: Find the first-order ex-
pectancy matrix E = (exy), where ex y is the expected
number of occurrences of nonterminal Y in a one-step ex-
pansion of nonterminal X, and make sure its powers Ek
</bodyText>
<figure confidence="0.472993">
(9)
</figure>
<page confidence="0.989575">
78
</page>
<bodyText confidence="0.995787444444444">
converge to 0 as k oo. If so, the grammar is consistent,
otherwise it is not.7
For the grammar in (9), E is the 1 x 1 matrix (2q). Thus
we can confirm our earlier observation by noting that (2q)k
converges to 0 iff q &lt;0.5, or p&gt; 0.5.
Now, it so happens that E is identical to the matrix A that
occurs in the linear equations (6) for the n-gram computation.
The actual coefficient matrix is I - A, and its inverse, if it
exists, can be written as the geometric sum
</bodyText>
<equation confidence="0.932374">
(I - Arl = 1+ A + A2 + A3 +
</equation>
<bodyText confidence="0.996979571428571">
This series converges precisely if Ak converges to 0. We
have thus shown that the existence of a solution for the n-
gram problem is equivalent to the consistency of the grammar
in question. Furthermore, the solution vector c = (I -
A)&apos; b will always consist of non-negative numbers: it is
the sum and product of the non-negative values given by
equations (7) and (8).
</bodyText>
<sectionHeader confidence="0.998576" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.863225363636364">
James K. Baker. 1979. Trainable grammars for speech
recognition. In Jared J. Wolf and Dennis H. Klatt, editors,
Speech Communication Papers for the 97th Meeting of
the Acoustical Society of America, pages 547-550, MIT,
Cambridge, Mass.
Taylor L. Booth and Richard A. Thompson. 1973. Ap-
plying probability measures to abstract languages. IEEE
Transactions on Computers, C-22(5):442-450.
Ted Briscoe and John Carroll. 1993. Generalized prob-
abilistic LR parsing of natural language (corpora) with
unification-based grammars. Computational Linguistics,
19(1):25-59.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based
n-gram models of natural language. Computational Lin-
guistics, 18(4):467-479.
Kenneth W. Church and William A. Gale. 1991. A compar-
ison of the enhanced Good-Turing and deleted estimation
methods for estimating probabilities of English bigrams.
Computer Speech and Language, 5:19-54.
Anna Corazza, Renato De Mori, Roberto Gretter, and Gior-
gio Satta. 1991. Computation of probabilities for an
island-driven parser. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 13(9):936-950.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451-455.
Susan L. Graham, Michael A. Harrison, and Walter L.
Ruzzo. 1980. An improved context-free recognizer. ACM
Transactions on Programming Languages and Systems,
2(3):415-462.
7A further version of this criterion is to check the magnitude of
the largest of E&apos;s eigenvalues (its spectral radius). If that value is
&gt; 1, the grammar is inconsistent; if &lt; 1, it is consistent.
Frederick Jelinek and John D. Lafferty. 1991. Computa-
tion of the probability of initial substring generation by
stochastic context-free grammars. Computational Lin-
guistics, 17(3):315-323.
Frederick Jelinek and Robert L. Mercer. 1980. Interpo-
lated estimation of Markov source parameters from sparse
data. In Proceedings Workshop on Pattern Recognition in
Practice, pages 381-397, Amsterdam.
Frederick Jelinek, John D. Lafferty, and Robert L. Mer-
cer. 1992. Basic methods of probabilistic context free
grammars. In Pietro Laface and Renato De Mori, editors,
Speech Recognition and Understanding. Recent Advances,
Trends, and Applications, volume F75 of NATO Advanced
Sciences Institutes Series, pages 345-360. Springer Ver-
lag, Berlin. Proceedings of the NATO Advanced Study
Institute, Cetraro, Italy, July 1990.
Mark A. Jones and Jason M. Eisner. 1992. A probabilistic
parser applied to software testing documents. In Proceed-
ings of the 8th National Conference on Artificial Intelli-
gence, pages 332-328, San Jose, CA. AAAI Press.
Daniel Jurafsky, Chuck Wooters, Gary Tajchman, Jonathan
Segal, Andreas Stolcke, and Nelson Morgan. 1994. In-
tegrating grammatical, phonological, and dialect/accent
information with a speech recognizer in the Berkeley
Restaurant Project. In Paul McKevitt, editor, AAAI Work-
shop on the Integration of Natural Language and Speech
Processing, Seattle, WA. To appear.
David M. Magerman and Mitchell P. Marcus. 1991. Pearl:
A probabilistic chart parser. In Proceedings of the 6th
Conference of the European Chapter of the Association
for Computational Linguistics, Berlin, Germany.
Hermann Ney. 1984. The use of a one-stage dynamic
programming algorithm for connected word recognition.
IEEE Transactions on Acoustics, Speech, and Signal Pro-
cessing, 32(2):263-271.
William H. Press, Brian P. Flannery, Saul A. Teukolsky, and
William T. Vetterling. 1988. Numerical Recipes in C: The
Art of Scientific Computing. Cambridge University Press,
Cambridge.
Richard Schwartz and Yen-Lu Chow. 1990. The N-best
algorithm: An efficient and exact procedure for finding the
n most likely sentence hypotheses. In Proceedings IEEE
Conference on Acoustics, Speech and Signal Processing,
volume 1, pages 81-84, Albuquerque, NM.
Andreas Stolcke. 1993. An efficient probabilistic context-
free parsing algorithm that computes prefix probabilities.
Technical Report TR-93-065, International Computer Sci-
ence Institute, Berkeley, CA. To appear in Computational
Linguistics.
Victor Zue, James Glass, David Goodine, Hong Leung,
Michael Phillips, Joseph Polifroni, and Stephanie Sen-
eff. 1991. Integration of speech recognition and natu-
ral language processing in the MIT Voyager system. In
Proceedings IEEE Conference on Acoustics, Speech and
Signal Processing, volume 1, pages 713-716, Toronto.
</reference>
<page confidence="0.999006">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.910388">
<title confidence="0.990059">PRECISE N-GRAM PROBABILITIES FROM STOCHASTIC CONTEXT-FREE GRAMMARS</title>
<author confidence="0.988295">Stolcke Segal</author>
<affiliation confidence="0.981477">University of California, Berkeley and International Computer Science Institute</affiliation>
<address confidence="0.9989425">1947 Center Street Berkeley, CA 94704</address>
<email confidence="0.999746">fstolcke,jsegall@icsi.berkeley.edu</email>
<abstract confidence="0.9989423">We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others). The method operates via the computation of substring expectations, which in turn is accomplished by solving systems of linear equations derived from the grammar. The procedure is fully implemented and has proved viable and useful in practice.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition. In</title>
<date>1979</date>
<booktitle>Speech Communication Papers for the 97th Meeting of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<editor>Jared J. Wolf and Dennis H. Klatt, editors,</editor>
<publisher>MIT,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="4716" citStr="Baker, 1979" startWordPosition="751" endWordPosition="752"> on the productions in the SCFG. The reason for this discrepancy, of course, is that the structure of the SCFG itself is a discrete (hyper-)parameter with a lot of potential variation, but one that has been fixed beforehand. The point is that such a structure is comprehensible by humans, and can in many cases be constrained using prior knowledge, thereby reducing the estimation problem for the remaining probabilities. The problem of estimating SCFG parameters from data is solved with standard techniques, usually by way of likelihood maximization and a variant of the Baum-Welch (EM) algorithm (Baker, 1979). A tutorial introduction to SCFGs and standard algorithms can be found in Jelinek et al. (1992). MOTIVATION There are good arguments that SCFGs are in principle not adequate probabilistic models for natural languages, due to the conditional independence assumptions they embody (Magerman and Marcus, 1991; Jones and Eisner, 1992; Briscoe and Carroll, 1993). Such shortcomings can be partly remedied by using SCFGs with very specific, semantically oriented categories and rules (Jurafsky et al., 1994). If the goal is to use n-grams nevertheless, then their their computation from a more constrained </context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James K. Baker. 1979. Trainable grammars for speech recognition. In Jared J. Wolf and Dennis H. Klatt, editors, Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, pages 547-550, MIT, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor L Booth</author>
<author>Richard A Thompson</author>
</authors>
<title>Applying probability measures to abstract languages.</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<pages>22--5</pages>
<contexts>
<context position="13783" citStr="Booth and Thompson, 1973" startWordPosition="2312" endWordPosition="2315"> for example, one could create a &apos;mirrored&apos; SCFG (reversing the order of right-hand side symbols in all productions) and then run any prefix probability computation on that mirror grammar. Note that in the case of bigrams, only a particularly simple form of prefix/suffix probabilities are required, namely, the left-corner&apos; and `right-corner&apos; probabilities, P(X L WI) and P(Y W2), which can each be obtained from a single matrix inversion (Jelinek and Lafferty, 1991). It should be mentioned that there are some technical conditions that have to be met for a SCFG to be well-defined and consistent (Booth and Thompson, 1973). These condition are also sufficient to guarantee that the linear equations given by (3) have positive probabilities as solutions. The details of this are discussed in the Appendix. Finally, it is interesting to compare the relative ease with which one can solve the substring expectation problem to the seemingly similar problem of finding substring probabilities: the probability that X generates (one or more instances of) w. The latter problem is studied by Corazza et al. (1991), and shown to lead to a non-linear system of equations. The crucial difference here is that expectations are additi</context>
<context position="22676" citStr="Booth and Thompson, 1973" startWordPosition="3789" endWordPosition="3792">o surprising results. Consider the following simple grammar S x [p] S -4 SS [q = 1 — p] What is the expected frequency of unigram x? Using the abbreviation c = c(XIS) and equation 5, we see that c = P(S x) + P(S S S)(c + c) = p + 2qc This leads to c= (10) 1 — 2q 2p — 1 Now, for p = 0.5 this becomes infinity, and for probabilities p &lt; 0.5, the solution is negative! This is a rather striking manifestation of the failure of this grammar, for p &lt; 0.5, to be consistent. A grammar is said to be inconsistent if the underlying stochastic derivation process has non-zero probability of not terminating (Booth and Thompson, 1973). The expected length of the generated strings should therefore be infinite in this case. Fortunately, Booth and Thompson derive a criterion for checking the consistency of a SCFG: Find the first-order expectancy matrix E = (exy), where ex y is the expected number of occurrences of nonterminal Y in a one-step expansion of nonterminal X, and make sure its powers Ek (9) 78 converge to 0 as k oo. If so, the grammar is consistent, otherwise it is not.7 For the grammar in (9), E is the 1 x 1 matrix (2q). Thus we can confirm our earlier observation by noting that (2q)k converges to 0 iff q &lt;0.5, or </context>
</contexts>
<marker>Booth, Thompson, 1973</marker>
<rawString>Taylor L. Booth and Richard A. Thompson. 1973. Applying probability measures to abstract languages. IEEE Transactions on Computers, C-22(5):442-450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="5073" citStr="Briscoe and Carroll, 1993" startWordPosition="804" endWordPosition="807">edge, thereby reducing the estimation problem for the remaining probabilities. The problem of estimating SCFG parameters from data is solved with standard techniques, usually by way of likelihood maximization and a variant of the Baum-Welch (EM) algorithm (Baker, 1979). A tutorial introduction to SCFGs and standard algorithms can be found in Jelinek et al. (1992). MOTIVATION There are good arguments that SCFGs are in principle not adequate probabilistic models for natural languages, due to the conditional independence assumptions they embody (Magerman and Marcus, 1991; Jones and Eisner, 1992; Briscoe and Carroll, 1993). Such shortcomings can be partly remedied by using SCFGs with very specific, semantically oriented categories and rules (Jurafsky et al., 1994). If the goal is to use n-grams nevertheless, then their their computation from a more constrained SCFG is still useful since the results can be interpolated with raw n-gram estimates for smoothing. An experiment illustrating this approach is reported later in the paper. On the other hand, even if vastly more sophisticated language models give better results, n-grams will most likely still be important in applications such as speech recognition. The st</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Ted Briscoe and John Carroll. 1993. Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<contexts>
<context position="7095" citStr="Brown et al. (1992)" startWordPosition="1120" endWordPosition="1123">n-gram estimates that improve the quality of the hypotheses generated by the decoder. Finally, comparing directly estimated, reliable n-grams with those compiled from other language models is a potentially useful method for evaluating the models in question. For the purpose of this paper, then, we assume that computing n-grams from SCFGs is of either practical or theoretical interest and concentrate on the computational aspects of the problem. It should be noted that there are alternative, unrelated methods for addressing the problem of the large parameter space in n-gram models. For example, Brown et al. (1992) describe an approach based on grouping words into classes, thereby reducing the number of conditional probabilities in the model. THE ALGORITHM Normal form for SCFGs A grammar is in Chomsky Normal Form (CNF) if every production is of the form A ./3 C or A terminal. Any CFG or SCFG can be converted into one in CNF which generates exactly the same language, each of the sentences with exactly the same probability, and for which any parse in the original grammar would be reconstructible from a parse in the CNF grammar. In short, we can, without loss of generality, assume that the SCFGs we are dea</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--19</pages>
<contexts>
<context position="1285" citStr="Church and Gale, 1991" startWordPosition="169" endWordPosition="172">ed from the grammar. The procedure is fully implemented and has proved viable and useful in practice. INTRODUCTION Probabilistic language modeling with n-gram grammars (particularly bigram and trigram) has proven extremely useful for such tasks as automated speech recognition, part-ofspeech tagging, and word-sense disambiguation, and lead to simple, efficient algorithms. Unfortunately, working with these grammars can be problematic for several reasons: they have large numbers of parameters, so reliable estimation requires a very large training corpus and/or sophisticated smoothing techniques (Church and Gale, 1991); it is very hard to directly model linguistic knowledge (and thus these grammars are practically incomprehensible to human inspection); and the models are not easily extensible, i.e., if a new word is added to the vocabulary, none of the information contained in an existing n-gram will tell anything about the n-grams containing the new item. Stochastic context-free grammars (SCFGs), on the other hand, are not as susceptible to these problems: they have many fewer parameters (so can be reasonably trained with smaller corpora); they capture linguistic generalizations, and are easily understood </context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Kenneth W. Church and William A. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Corazza</author>
<author>Renato De Mori</author>
<author>Roberto Gretter</author>
<author>Giorgio Satta</author>
</authors>
<title>Computation of probabilities for an island-driven parser.</title>
<date>1991</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>13--9</pages>
<marker>Corazza, De Mori, Gretter, Satta, 1991</marker>
<rawString>Anna Corazza, Renato De Mori, Roberto Gretter, and Giorgio Satta. 1991. Computation of probabilities for an island-driven parser. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(9):936-950.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>6--8</pages>
<contexts>
<context position="13093" citStr="Earley, 1970" startWordPosition="2204" endWordPosition="2205">x probabilities The only substantial problem left at this point is the computation of the constants in equation (3). These are derived from the rule probabilities P(X w) and P(X YZ), as well as the prefix/suffix generation probabilities P(Y Wi . .W3) and P(Z in-3+1 • • • wn)• The computation of prefix probabilities for SCFGs is generally useful for applications, and has been solved with the LRI algorithm (Jelinek and Lafferty, 1991). Recently, Stoleke (1993) has shown how to perform this computation efficiently for sparsely parameterized SCFGs using a probabilistic version of Earley&apos;s parser (Earley, 1970). Computing suffix probabilities is obviously a symmetrical task; for example, one could create a &apos;mirrored&apos; SCFG (reversing the order of right-hand side symbols in all productions) and then run any prefix probability computation on that mirror grammar. Note that in the case of bigrams, only a particularly simple form of prefix/suffix probabilities are required, namely, the left-corner&apos; and `right-corner&apos; probabilities, P(X L WI) and P(Y W2), which can each be obtained from a single matrix inversion (Jelinek and Lafferty, 1991). It should be mentioned that there are some technical conditions t</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 6(8):451-455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan L Graham</author>
<author>Michael A Harrison</author>
<author>Walter L Ruzzo</author>
</authors>
<title>An improved context-free recognizer.</title>
<date>1980</date>
<journal>ACM Transactions on Programming Languages and Systems,</journal>
<pages>2--3</pages>
<contexts>
<context position="7830" citStr="Graham et al., 1980" startWordPosition="1246" endWordPosition="1249">s in the model. THE ALGORITHM Normal form for SCFGs A grammar is in Chomsky Normal Form (CNF) if every production is of the form A ./3 C or A terminal. Any CFG or SCFG can be converted into one in CNF which generates exactly the same language, each of the sentences with exactly the same probability, and for which any parse in the original grammar would be reconstructible from a parse in the CNF grammar. In short, we can, without loss of generality, assume that the SCFGs we are dealing with are in CNF. In fact, our algorithm generalizes straightforwardly to the more general Canonical Two-Form (Graham et al., 1980) format, and in the case of bigrams (n = 2) it can even be modified to work directly for arbitrary SCFGs. Still, the CNF form is convenient, and to keep the exposition simple we assume all SCFGs to be in CNF. Probabilities from expectations The first key insight towards a solution is that the n-gram probabilities can be obtained from the associated expected frequencies for n-grams and (n — 1)-grams: c(wi wri IL) P(wnlwiw2 • • • wn-1) = (1) c(wi IL) where c(wIL) stands for the expected count of occurrences of the substring w in a sentence of L.&apos; Proof Write the expectation for n-grams recursive</context>
</contexts>
<marker>Graham, Harrison, Ruzzo, 1980</marker>
<rawString>Susan L. Graham, Michael A. Harrison, and Walter L. Ruzzo. 1980. An improved context-free recognizer. ACM Transactions on Programming Languages and Systems, 2(3):415-462.</rawString>
</citation>
<citation valid="false">
<title>7A further version of this criterion is to check the magnitude of the largest of E&apos;s eigenvalues (its spectral radius). If that value is &gt; 1, the grammar is inconsistent; if &lt; 1, it is consistent.</title>
<marker></marker>
<rawString>7A further version of this criterion is to check the magnitude of the largest of E&apos;s eigenvalues (its spectral radius). If that value is &gt; 1, the grammar is inconsistent; if &lt; 1, it is consistent.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
</authors>
<title>Computation of the probability of initial substring generation by stochastic context-free grammars.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--3</pages>
<contexts>
<context position="12916" citStr="Jelinek and Lafferty, 1991" startWordPosition="2177" endWordPosition="2180">e there are exactly 76 as many equations as variables, equal to the number of nonterminals in the grammar. The solution of these systems is further discussed below. Computing prefix and suffix probabilities The only substantial problem left at this point is the computation of the constants in equation (3). These are derived from the rule probabilities P(X w) and P(X YZ), as well as the prefix/suffix generation probabilities P(Y Wi . .W3) and P(Z in-3+1 • • • wn)• The computation of prefix probabilities for SCFGs is generally useful for applications, and has been solved with the LRI algorithm (Jelinek and Lafferty, 1991). Recently, Stoleke (1993) has shown how to perform this computation efficiently for sparsely parameterized SCFGs using a probabilistic version of Earley&apos;s parser (Earley, 1970). Computing suffix probabilities is obviously a symmetrical task; for example, one could create a &apos;mirrored&apos; SCFG (reversing the order of right-hand side symbols in all productions) and then run any prefix probability computation on that mirror grammar. Note that in the case of bigrams, only a particularly simple form of prefix/suffix probabilities are required, namely, the left-corner&apos; and `right-corner&apos; probabilities,</context>
</contexts>
<marker>Jelinek, Lafferty, 1991</marker>
<rawString>Frederick Jelinek and John D. Lafferty. 1991. Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics, 17(3):315-323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Proceedings Workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<location>Amsterdam.</location>
<contexts>
<context position="19354" citStr="Jelinek and Mercer, 1980" startWordPosition="3257" endWordPosition="3260">babilities had previously been estimated from the same training corpus, using standard EM techniques. This resulted in a word error rate of 35.3%. This may seem surprisingly good given the low coverage of the underlying CFGs, but notice that the conversion into bigrams is bound to result in a less constraining language model, effectively increasing coverage. Finally, in experiment 3, the bigrams generated from the SCFG were augmented by those from the raw training data, in a proportion of 200,000: 2500. We have not attempted to optimize this mixture proportion, e.g., by deleted interpolation (Jelinek and Mercer, 1980).6 With the bigram estimates thus obtained, the word error rate dropped to 33.5%. (All error rates were measured on a separate test corpus.) The experiment therefore supports our earlier argument that more sophisticated language models, even if far from perfect, can improve n-gram estimates obtained directly from sample data. 4Corpus and grammar sizes, as well as the recognition performance figures reported here are not up-to-date with respect to the latest version of BeRP. For ACL-94 we expect to have revised results available that reflect the current performance of the system. 5Unlike the re</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Frederick Jelinek and Robert L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Proceedings Workshop on Pattern Recognition in Practice, pages 381-397, Amsterdam.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
</authors>
<title>Basic methods of probabilistic context free grammars.</title>
<date>1992</date>
<booktitle>Speech Recognition and Understanding. Recent Advances, Trends, and Applications, volume F75 of NATO Advanced Sciences Institutes Series,</booktitle>
<pages>345--360</pages>
<editor>In Pietro Laface and Renato De Mori, editors,</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="4812" citStr="Jelinek et al. (1992)" startWordPosition="765" endWordPosition="768">e structure of the SCFG itself is a discrete (hyper-)parameter with a lot of potential variation, but one that has been fixed beforehand. The point is that such a structure is comprehensible by humans, and can in many cases be constrained using prior knowledge, thereby reducing the estimation problem for the remaining probabilities. The problem of estimating SCFG parameters from data is solved with standard techniques, usually by way of likelihood maximization and a variant of the Baum-Welch (EM) algorithm (Baker, 1979). A tutorial introduction to SCFGs and standard algorithms can be found in Jelinek et al. (1992). MOTIVATION There are good arguments that SCFGs are in principle not adequate probabilistic models for natural languages, due to the conditional independence assumptions they embody (Magerman and Marcus, 1991; Jones and Eisner, 1992; Briscoe and Carroll, 1993). Such shortcomings can be partly remedied by using SCFGs with very specific, semantically oriented categories and rules (Jurafsky et al., 1994). If the goal is to use n-grams nevertheless, then their their computation from a more constrained SCFG is still useful since the results can be interpolated with raw n-gram estimates for smoothi</context>
</contexts>
<marker>Jelinek, Lafferty, Mercer, 1992</marker>
<rawString>Frederick Jelinek, John D. Lafferty, and Robert L. Mercer. 1992. Basic methods of probabilistic context free grammars. In Pietro Laface and Renato De Mori, editors, Speech Recognition and Understanding. Recent Advances, Trends, and Applications, volume F75 of NATO Advanced Sciences Institutes Series, pages 345-360. Springer Verlag, Berlin. Proceedings of the NATO Advanced Study Institute, Cetraro, Italy, July 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Jones</author>
<author>Jason M Eisner</author>
</authors>
<title>A probabilistic parser applied to software testing documents.</title>
<date>1992</date>
<booktitle>In Proceedings of the 8th National Conference on Artificial Intelligence,</booktitle>
<pages>332--328</pages>
<publisher>AAAI Press.</publisher>
<location>San Jose, CA.</location>
<contexts>
<context position="5045" citStr="Jones and Eisner, 1992" startWordPosition="800" endWordPosition="803">rained using prior knowledge, thereby reducing the estimation problem for the remaining probabilities. The problem of estimating SCFG parameters from data is solved with standard techniques, usually by way of likelihood maximization and a variant of the Baum-Welch (EM) algorithm (Baker, 1979). A tutorial introduction to SCFGs and standard algorithms can be found in Jelinek et al. (1992). MOTIVATION There are good arguments that SCFGs are in principle not adequate probabilistic models for natural languages, due to the conditional independence assumptions they embody (Magerman and Marcus, 1991; Jones and Eisner, 1992; Briscoe and Carroll, 1993). Such shortcomings can be partly remedied by using SCFGs with very specific, semantically oriented categories and rules (Jurafsky et al., 1994). If the goal is to use n-grams nevertheless, then their their computation from a more constrained SCFG is still useful since the results can be interpolated with raw n-gram estimates for smoothing. An experiment illustrating this approach is reported later in the paper. On the other hand, even if vastly more sophisticated language models give better results, n-grams will most likely still be important in applications such a</context>
</contexts>
<marker>Jones, Eisner, 1992</marker>
<rawString>Mark A. Jones and Jason M. Eisner. 1992. A probabilistic parser applied to software testing documents. In Proceedings of the 8th National Conference on Artificial Intelligence, pages 332-328, San Jose, CA. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Chuck Wooters</author>
<author>Gary Tajchman</author>
<author>Jonathan Segal</author>
<author>Andreas Stolcke</author>
<author>Nelson Morgan</author>
</authors>
<title>Integrating grammatical, phonological, and dialect/accent information with a speech recognizer in the Berkeley Restaurant Project.</title>
<date>1994</date>
<booktitle>AAAI Workshop on the Integration of Natural Language and Speech Processing,</booktitle>
<editor>In Paul McKevitt, editor,</editor>
<location>Seattle, WA.</location>
<note>To appear.</note>
<contexts>
<context position="5217" citStr="Jurafsky et al., 1994" startWordPosition="825" endWordPosition="828">andard techniques, usually by way of likelihood maximization and a variant of the Baum-Welch (EM) algorithm (Baker, 1979). A tutorial introduction to SCFGs and standard algorithms can be found in Jelinek et al. (1992). MOTIVATION There are good arguments that SCFGs are in principle not adequate probabilistic models for natural languages, due to the conditional independence assumptions they embody (Magerman and Marcus, 1991; Jones and Eisner, 1992; Briscoe and Carroll, 1993). Such shortcomings can be partly remedied by using SCFGs with very specific, semantically oriented categories and rules (Jurafsky et al., 1994). If the goal is to use n-grams nevertheless, then their their computation from a more constrained SCFG is still useful since the results can be interpolated with raw n-gram estimates for smoothing. An experiment illustrating this approach is reported later in the paper. On the other hand, even if vastly more sophisticated language models give better results, n-grams will most likely still be important in applications such as speech recognition. The standard speech decoding technique of framesynchronous dynamic programming (Ney, 1984) is based on a first-order Markov assumption, which is satis</context>
<context position="17772" citStr="Jurafsky et al., 1994" startWordPosition="3001" endWordPosition="3004">icient matrix. 4. Compute the unigram expectations for each word in the grammar, by solving the LU system for the unigram right-hand sides computed in step 2. 5. Compute the bigram expectations for each word pair by solving the LU system for the bigram right-hand sides computed in step 2. 77 6. Compute each bigram probability P(w2I /Di ), by dividing the bigram expectation c(wi w2 IS) by the unigram expectation c(w I IS). EXPERIMENTS The algorithm described here has been implemented, and is being used to generate bigrams for a speech recognizer that is part of the BeRP spoken-language system (Jurafsky et al., 1994). An early prototype of BeRP was used in an experiment to assess the benefit of using bigram probabilities obtained through SCFGs versus estimating them directly from the available training corpus.4 The system&apos;s domain are inquiries about restaurants in the city of Berkeley. The training corpus used had only 2500 sentences, with an average length of about 4.8 words/sentence. Our experiments made use of a context-free grammar hand-written for the BeRP domain. With 1200 rules and a vocabulary of 1100 words, this grammar was able to parse 60% of the training corpus. Computing the bigram probabili</context>
</contexts>
<marker>Jurafsky, Wooters, Tajchman, Segal, Stolcke, Morgan, 1994</marker>
<rawString>Daniel Jurafsky, Chuck Wooters, Gary Tajchman, Jonathan Segal, Andreas Stolcke, and Nelson Morgan. 1994. Integrating grammatical, phonological, and dialect/accent information with a speech recognizer in the Berkeley Restaurant Project. In Paul McKevitt, editor, AAAI Workshop on the Integration of Natural Language and Speech Processing, Seattle, WA. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Pearl: A probabilistic chart parser.</title>
<date>1991</date>
<booktitle>In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Berlin, Germany.</location>
<contexts>
<context position="5021" citStr="Magerman and Marcus, 1991" startWordPosition="795" endWordPosition="799"> can in many cases be constrained using prior knowledge, thereby reducing the estimation problem for the remaining probabilities. The problem of estimating SCFG parameters from data is solved with standard techniques, usually by way of likelihood maximization and a variant of the Baum-Welch (EM) algorithm (Baker, 1979). A tutorial introduction to SCFGs and standard algorithms can be found in Jelinek et al. (1992). MOTIVATION There are good arguments that SCFGs are in principle not adequate probabilistic models for natural languages, due to the conditional independence assumptions they embody (Magerman and Marcus, 1991; Jones and Eisner, 1992; Briscoe and Carroll, 1993). Such shortcomings can be partly remedied by using SCFGs with very specific, semantically oriented categories and rules (Jurafsky et al., 1994). If the goal is to use n-grams nevertheless, then their their computation from a more constrained SCFG is still useful since the results can be interpolated with raw n-gram estimates for smoothing. An experiment illustrating this approach is reported later in the paper. On the other hand, even if vastly more sophisticated language models give better results, n-grams will most likely still be importan</context>
</contexts>
<marker>Magerman, Marcus, 1991</marker>
<rawString>David M. Magerman and Mitchell P. Marcus. 1991. Pearl: A probabilistic chart parser. In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
</authors>
<title>The use of a one-stage dynamic programming algorithm for connected word recognition.</title>
<date>1984</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<pages>32--2</pages>
<contexts>
<context position="5757" citStr="Ney, 1984" startWordPosition="911" endWordPosition="912">ic, semantically oriented categories and rules (Jurafsky et al., 1994). If the goal is to use n-grams nevertheless, then their their computation from a more constrained SCFG is still useful since the results can be interpolated with raw n-gram estimates for smoothing. An experiment illustrating this approach is reported later in the paper. On the other hand, even if vastly more sophisticated language models give better results, n-grams will most likely still be important in applications such as speech recognition. The standard speech decoding technique of framesynchronous dynamic programming (Ney, 1984) is based on a first-order Markov assumption, which is satisfied by bigrams models (as well as by Hidden Markov Models), but not by more complex models incorporating non-local or higherorder constraints (including SCFGs). A standard approach is therefore to use simple language models to generate a preliminary set of candidate hypotheses. These hypotheses, e.g., represented as word lattices or N-best lists (Schwartz and Chow, 1990), are re-evaluated later using additional criteria that can afford to be more costly due to the more constrained outcomes. In this type of setting, the techniques dev</context>
</contexts>
<marker>Ney, 1984</marker>
<rawString>Hermann Ney. 1984. The use of a one-stage dynamic programming algorithm for connected word recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing, 32(2):263-271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Brian P Flannery</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
</authors>
<title>Numerical Recipes in C: The Art of Scientific Computing.</title>
<date>1988</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="16103" citStr="Press et al. (1988)" startWordPosition="2727" endWordPosition="2730"> these are indexed by nonterminals X, U. We get axu =E P(X Y Z)(6(Y,U) + o(Z,U))(7) X--0 bx = P(X 111) + E P(X Y Z) x—).Yz n-1 EP(/ W1 • • •W.2) j=--1 P(Z L w3+1 wn) (8) where 6(X, Y) -= 1 if X -= Y, and 0 otherwise. The expression I — A arises from bringing the variables c(w I Y) and c(w1Z) to the other side in equation (3) in order to collect the coefficients. We can see that all dependencies on the particular bigram, w, are in the right-hand side vector b, while the coefficient matrix I — A depends only on the grammar. This, together with the standard method of LU decomposition (see, e.g., Press et al. (1988)) enables us to solve for each bigram in time 0(N2), rather than the standard 0(N3) for a full system (N being the number of nonterminals/variables). The LU decomposition itself is cubic, but is incurred only once. The full computation is therefore dominated by the quadratic effort of solving the system for each n-gram. Furthermore, the quadratic cost is a worst-case figure that would be incurred only if the grammar contained every possible rule; empirically we have found this computation to be linear in the number of nonterminals, for grammars that are sparse, i.e., where each nonterminal mak</context>
</contexts>
<marker>Press, Flannery, Teukolsky, Vetterling, 1988</marker>
<rawString>William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. 1988. Numerical Recipes in C: The Art of Scientific Computing. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Schwartz</author>
<author>Yen-Lu Chow</author>
</authors>
<title>The N-best algorithm: An efficient and exact procedure for finding the n most likely sentence hypotheses.</title>
<date>1990</date>
<booktitle>In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing,</booktitle>
<volume>1</volume>
<pages>81--84</pages>
<location>Albuquerque, NM.</location>
<contexts>
<context position="6191" citStr="Schwartz and Chow, 1990" startWordPosition="977" endWordPosition="980">er results, n-grams will most likely still be important in applications such as speech recognition. The standard speech decoding technique of framesynchronous dynamic programming (Ney, 1984) is based on a first-order Markov assumption, which is satisfied by bigrams models (as well as by Hidden Markov Models), but not by more complex models incorporating non-local or higherorder constraints (including SCFGs). A standard approach is therefore to use simple language models to generate a preliminary set of candidate hypotheses. These hypotheses, e.g., represented as word lattices or N-best lists (Schwartz and Chow, 1990), are re-evaluated later using additional criteria that can afford to be more costly due to the more constrained outcomes. In this type of setting, the techniques developed in this paper can be used to compile probabilistic knowledge present in the more elaborate language models into n-gram estimates that improve the quality of the hypotheses generated by the decoder. Finally, comparing directly estimated, reliable n-grams with those compiled from other language models is a potentially useful method for evaluating the models in question. For the purpose of this paper, then, we assume that comp</context>
</contexts>
<marker>Schwartz, Chow, 1990</marker>
<rawString>Richard Schwartz and Yen-Lu Chow. 1990. The N-best algorithm: An efficient and exact procedure for finding the n most likely sentence hypotheses. In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, volume 1, pages 81-84, Albuquerque, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic contextfree parsing algorithm that computes prefix probabilities.</title>
<date>1993</date>
<tech>Technical Report TR-93-065,</tech>
<institution>International Computer Science Institute,</institution>
<location>Berkeley, CA.</location>
<note>To appear in Computational Linguistics.</note>
<marker>Stolcke, 1993</marker>
<rawString>Andreas Stolcke. 1993. An efficient probabilistic contextfree parsing algorithm that computes prefix probabilities. Technical Report TR-93-065, International Computer Science Institute, Berkeley, CA. To appear in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Zue</author>
<author>James Glass</author>
<author>David Goodine</author>
<author>Hong Leung</author>
<author>Michael Phillips</author>
<author>Joseph Polifroni</author>
<author>Stephanie Seneff</author>
</authors>
<title>Integration of speech recognition and natural language processing in the MIT Voyager system.</title>
<date>1991</date>
<booktitle>In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing,</booktitle>
<volume>1</volume>
<pages>713--716</pages>
<location>Toronto.</location>
<contexts>
<context position="2455" citStr="Zue et al. (1991)" startWordPosition="347" endWordPosition="350">tic generalizations, and are easily understood and written, by linguists; and they can be extended straightforwardly based on the underlying linguistic knowledge. In this paper, we present a technique for computing an n-gram grammar from an existing SCFG—an attempt to get the best of both worlds. Besides developing the mathematics involved in the computation, we also discuss efficiency and implementation issues, and briefly report on our experience confirming its practical feasibility and utility. The technique of compiling higher-level grammatical models into lower-level ones has precedents: Zue et al. (1991) report building a word-pair grammar from more elaborate language models to achieve good coverage, by random generation of sentences. In our own group, the current approach was predated by an alternative one that essentially relied on approximating bigram probabilities through Monte-Carlo sampling from SCFGs. PRELIMINARIES An n-gram grammar is a set of probabilities F(wn I wi w2 ton--i), giving the probability that wn follows a word string WI W2. . . , for each possible combination of the w&apos;s in the vocabulary of the language. So for a 5000 word vocabulary, a bigram grammar would have approxim</context>
</contexts>
<marker>Zue, Glass, Goodine, Leung, Phillips, Polifroni, Seneff, 1991</marker>
<rawString>Victor Zue, James Glass, David Goodine, Hong Leung, Michael Phillips, Joseph Polifroni, and Stephanie Seneff. 1991. Integration of speech recognition and natural language processing in the MIT Voyager system. In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, volume 1, pages 713-716, Toronto.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>