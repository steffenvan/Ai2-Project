<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000908">
<title confidence="0.991582">
Bayesian Kernel Methods for Natural Language Processing
</title>
<author confidence="0.998458">
Daniel Beck
</author>
<affiliation confidence="0.941609">
Department of Computer Science
University of Sheffield
Sheffield, United Kingdom
</affiliation>
<email confidence="0.990257">
debeck1@sheffield.ac.uk
</email>
<sectionHeader confidence="0.993721" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999811214285714">
Kernel methods are heavily used in Natu-
ral Language Processing (NLP). Frequen-
tist approaches like Support Vector Ma-
chines are the state-of-the-art in many
tasks. However, these approaches lack
efficient procedures for model selection,
which hinders the usage of more advanced
kernels. In this work, we propose the
use of a Bayesian approach for kernel
methods, Gaussian Processes, which allow
easy model fitting even for complex kernel
combinations. Our goal is to employ this
approach to improve results in a number of
regression and classification tasks in NLP.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936431818182">
In the last years, kernel methods have been suc-
cessfully employed in many Natural Language
Processing tasks. These methods allow the build-
ing of non-parametric models which make less as-
sumptions about the underlying pattern in the data.
Another advantage of kernels is that they can be
defined in arbitrary structures like strings or trees,
which greatly reduce the need for careful feature
engineering in these structures.
The properties cited above make kernel meth-
ods ideal for problems where we do not have
much prior knowledge about how the data be-
haves. This is a common setting in NLP, where
they have been mostly applied in the form of Sup-
port Vector Machines (SVMs). Systems based on
SVMs have been the state-of-the-art in classifica-
tion tasks like Text Categorization (Lodhi et al.,
2002), Sentiment Analysis (Johansson and Mos-
chitti, 2013; P´erez-Rosas and Mihalcea, 2013) and
Question Classification (Moschitti, 2006; Croce et
al., 2011). Recently, they were also employed in
regression settings like Machine Translation Qual-
ity Estimation (Specia and Farzindar, 2010; Bojar
et al., 2013) and structured prediction (Chang et
al., 2013).
SVMs are a frequentist method: they aim to find
an approximation to the exact latent function that
explains the data. This is in contrast to Bayesian
settings, which define a prior distribution on this
function and perform inference by marginalizing
over all its possible values. Although there is some
discussion about which approach is better (Mur-
phy, 2012, Sec. 6.6.4), Bayesian methods offer
many useful theoretical properties. In fact, they
have been used before in NLP, especially in gram-
mar induction (Cohn et al., 2010) and word seg-
mentation (Goldwater et al., 2009). However, only
very recently kernel methods have been applied in
NLP using the Bayesian approach.
Gaussian Processes (GPs) are the Bayesian
counterpart of kernel methods and are widely con-
sidered the state-of-the-art for inference on func-
tions (Hensman et al., 2013). They have a number
of advantages which are very useful in NLP:
</bodyText>
<listItem confidence="0.632004">
• Kernels in general can be combined and pa-
rameterized in many ways. This parame-
</listItem>
<bodyText confidence="0.94639875">
terization lead to the problem of model se-
lection, which is difficult in frequentist ap-
proaches (mainly based on cross validation).
The Bayesian formulation of GPs let them
deal with model selection in a much more
more efficient and elegant way: by maximiz-
ing the likelihood on the training data. This
opens the door for the use of heavily param-
eterized kernel combinations, like multi-task
kernels for example.
• Being a probabilistic framework, they are
able to naturally encode uncertainty in the
predictions, which can be propagated if the
task is part of a larger system pipeline.
Besides these properties, GPs have also been
applied sucessfully in many Machine Learning
</bodyText>
<page confidence="0.810806">
1
</page>
<note confidence="0.869123">
Proceedings of the ACL 2014 Student Research Workshop, pages 1–9,
</note>
<affiliation confidence="0.376653">
Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics
</affiliation>
<note confidence="0.4799818">
tasks. Examples include Robotics (Ko et al.,
2007), Bioinformatics (Chu et al., 2005; Polaj-
nar et al., 2011), Geolocation (Schwaighofer et
al., 2004) and Computer Vision (Sinz et al., 2004;
Riihim¨aki et al., 2013). In NLP, GPs have been
</note>
<bodyText confidence="0.986354">
used only very recently and focused on regression
tasks (Cohn and Specia, 2013; Preotiuc-Pietro and
Cohn, 2013). In this work, we propose to combine
GPs with recent kernel developments to advance
the state-of-the-art in a number of NLP tasks.
</bodyText>
<sectionHeader confidence="0.993367" genericHeader="method">
2 Gaussian Processes
</sectionHeader>
<bodyText confidence="0.995576428571429">
In this Section, we follow closely the definition of
Rasmussen and Williams (2006). Consider a ma-
chine learning setting, where we have a dataset {
X = (x1, y1), (x2, y2), ... , (xn, yn)} and our
goal is to infer the underlying function f(x) that
best explains the data. A GP model assumes a
prior stochastic process over this function:
</bodyText>
<equation confidence="0.981608">
f(x) ∼ GP(µ(x), k(x, x&apos;)), (1)
</equation>
<bodyText confidence="0.998376833333333">
where µ(x) is the mean function, which is usu-
ally the 0 constant, and k(x, x&apos;) is the kernel or
covariance function. In this sense, they are analo-
gous to Gaussian distributions, which are also de-
fined in terms of a mean and a variance values, or
in the case of multivariate Gaussians, a mean vec-
tor and a covariance matrix. In fact, a GP can be
interpreted as an infinite-dimensional multivariate
Gaussian distribution.
The full model uses Bayes’ rule to define a pos-
terior over f, combining the GP prior with the data
likelihood:
</bodyText>
<equation confidence="0.997824">
p(f|X, y) = p(y|X, f)p(f)
p(y|X) , (2)
</equation>
<bodyText confidence="0.999765">
where X and y are the training inputs and outputs,
respectively. The posterior is then used to predict
the label for an unseen input x* by marginalizing
over all possible latent functions:
</bodyText>
<equation confidence="0.809771">
p(y* |x*,X, y) = if
p(y*|x*,X,f)p(f|X,y)df.
(3)
</equation>
<bodyText confidence="0.996987">
where y* is the predicted output. The choice of
the likelihood distribution depends if the task is re-
gression, classification or other prediction setting.
</bodyText>
<subsectionHeader confidence="0.953393">
2.1 GP Regression
</subsectionHeader>
<bodyText confidence="0.999622555555556">
In a regression setting, we assume that the output
values are equal to noisy latent function evalua-
tions, i.e., yi = f(xi) + η, where η ∼ N(0, σ2n) is
the added white noise. We also usually assume a
Gaussian likelihood, because this able us to solve
the integral in Equation 3 analytically. Substitut-
ing the likelihood and the prior in both Equations
2 and 3 and manipulating the result, we compute
the posterior also as a Gaussian distribution:
</bodyText>
<equation confidence="0.9986145">
y* ∼ N(k*(K + σnI)−1yT, (4)
k(x*, x*) − kT* (K + σnI)−1k*).
</equation>
<bodyText confidence="0.9998552">
where K is the Gram matrix corre-
sponding to the training inputs and
k* = [hx1, x*i, hx2, x*i, ... , hxn, x*i] is the
vector of kernel evaluations between the test input
and each training input.
</bodyText>
<subsectionHeader confidence="0.985816">
2.2 GP Classification
</subsectionHeader>
<bodyText confidence="0.999850666666667">
Consider binary classification using −1 and +1
as labels1. The model in this case use the ac-
tual, noiseless latent function evaluations f and
“squash” them through the [−1, +1] interval to ob-
tain the outputs. The posterior over the outputs is
then defined as:
</bodyText>
<equation confidence="0.988404">
p(y* = +1|x*, X, y) =
if a(f*)p(f* |x*, X, y)df*, (5)
*
</equation>
<bodyText confidence="0.9990996">
where σ(f*) is a squashing function. Two com-
mon choices are the logistic function and the pro-
bit function. The distribution over the latent values
f* is obtained by integrating out the latent func-
tion:
</bodyText>
<equation confidence="0.992675333333333">
p(f* |x*, X, y) = if
p(f* |x*, X, f )p(f |X, y)df.
(6)
</equation>
<bodyText confidence="0.9996808">
Because the likelihood is not Gaussian, the re-
sulting posterior integral is not analytically avail-
able anymore. The most common solution to this
problem is to approximate the posterior p(f|X, y)
with a Gaussian q(f|X, y). Two such approxi-
mation algorithms are the Laplace approximation
(Williams and Barber, 1998) and the Expectation
Propagation (Minka, 2001). Another option is to
use Markov Chain Monte Carlo sampling methods
on the true posterior (Neal, 1998).
</bodyText>
<subsectionHeader confidence="0.99622">
2.3 Hyperparameter Optimization
</subsectionHeader>
<bodyText confidence="0.843931">
The GP prior used in the models described above
usually have a number of hyperparameters. The
</bodyText>
<footnote confidence="0.970778">
1Extensions to multi-class settings are possible.
</footnote>
<page confidence="0.995488">
2
</page>
<bodyText confidence="0.9990376">
most important ones are the kernel ones but they
can also include others like the white noise vari-
ance σ2n used in regression. A key property of GPs
is their ability to easily fit these hyperparameters
to the data by maximizing the marginal likelihood:
</bodyText>
<equation confidence="0.7406195">
p(y|X, 0) = if
p(y|X, 0, f)p(f), (7)
</equation>
<bodyText confidence="0.998100333333333">
where 0 represents the full set of hyperparameters
(which was suppressed from all conditionals until
now for brevity). Optimization involves deriving
the gradients of the marginal log likelihood w.r.t.
the hyperparameters and then employ a gradient
ascent procedure. Gradients can be found ana-
litically for regression and by approximations for
classification, using methods similar to the ones
used for prediction.
</bodyText>
<subsectionHeader confidence="0.995851">
2.4 Sparse Approximations for GPs
</subsectionHeader>
<bodyText confidence="0.9991696">
SVMs are naturally sparse models which use only
a subset of data points to make predictions. This
results in important speed-ups which is one of
the reasons for their success. On the other hand,
canonical GPs are not sparse, making use of all
data points. This results in a training complexity
of O(n3) (due to the Gram matrix inversion) and
O(n) for predictions.
Sparse GPs tackle this problem by approximat-
ing the Gram matrix using only a subset of m in-
ducing inputs. Without loss of generalization, con-
sider these m inputs as the first ones in the train-
ing data and (n — m) the remaining ones. Then
we can partition the Gram matrix in the following
way (Rasmussen and Williams, 2006, Sec. 8.1):
</bodyText>
<equation confidence="0.790425333333333">
K=
IK(n−m)m
Kmm Km(n−m)1 K(n−m)(n−m)
</equation>
<bodyText confidence="0.9999496">
where each block corresponds to a matrix of ker-
nel evaluations between two sets of inputs. For
brevity, we will refer Km(n−m) as Kmn and its
transpose as Knm. The block structure of K forms
the base of the so-called Nystr¨om approximation:
</bodyText>
<equation confidence="0.991865">
K˜ = KnmK−1
mmKmn. (8)
which result in( the following predictive posterior:
y* — Ar(kTm*˜G−1Kmny, (9)
k(x*, x*) — kTm*K−1mmkm*+
2 T1
σnkm*˜G km*),
</equation>
<bodyText confidence="0.999982315789474">
where G˜ = σ2nKmm + KmnKnm and km* is the
vector of kernel evaluations between test input x*
and the m inducing inputs. The resulting com-
plexities for training and prediction are O(m2n)
and O(m), respectively.
The remaining question is how to choose the in-
ducing inputs. Seeger et al. (2003) use an iterative
method that starts with some random data points
and adds new ones based on a greedy procedure,
in an active learning fashion. Snelson and Ghahra-
mani (2006) use a different approach: it defines a
fixed m a priori and use pseudo-inputs which can
be optimized as regular hyperparameters. Later,
Titsias (2009) also used pseudo-inputs but per-
form optimization using a variational method in-
stead. Recently, Hensman et al. (2013) modified
this method to allow Stochastic Variational Infer-
ence (Hoffman et al., 2013), which reduces the
training complexity to O(m3).
</bodyText>
<sectionHeader confidence="0.998506" genericHeader="method">
3 Kernels
</sectionHeader>
<bodyText confidence="0.9999855">
The core of a GP model is the kernel function. A
kernel k(x, x&apos;) is a symmetric and positive semi-
definite function which returns a similarity score
between two inputs in some feature space (Shawe-
Taylor and Cristianini, 2004). Probably the most
used kernel in general is the Radial Basis Func-
tion (RBF) kernel, which is defined over two real-
valued vectors. Our focus in this work is on two
different types of kernels which can be applied for
NLP settings and allow richer parameterizations.
</bodyText>
<subsectionHeader confidence="0.961242">
3.1 Kernels for Discrete Structures
</subsectionHeader>
<bodyText confidence="0.999375071428571">
In NLP, discrete structures like strings or trees are
common in training data. To apply a vectorial
kernel like the RBF, one can always extract real-
valued features from these structures. However,
kernels can be defined directly on these structures,
potentially reducing the need for feature engineer-
ing. The string and tree kernels we define here
are based on the theory of Convolution kernels
of Haussler (1999), which calculate the similar-
ity between two structures based on the number
of substructures they have in common. Other ap-
proaches include random walk kernels (G¨artner et
al., 2003; Vishwanathan et al., 2010) and Fisher
kernels (Jaakkola et al., 2000).
</bodyText>
<subsectionHeader confidence="0.877922">
3.1.1 String Kernels
</subsectionHeader>
<bodyText confidence="0.874233333333333">
Consider a function φs(x) that counts the number
of times a substring s appears in x. A string kernel
,
</bodyText>
<page confidence="0.790373">
3
</page>
<bodyText confidence="0.683456">
is defined as:
</bodyText>
<equation confidence="0.9774455">
Ek(x, x&apos;) = wsos(x)os(x&apos;), (10)
sEE∗
</equation>
<bodyText confidence="0.999987833333334">
where ws is a non-negative weight for substring s
and E* is the set of all possible strings over the
symbol alphabet E.
Usually in NLP, each word is considered a sym-
bol, although some previous work also considered
characters as symbols (Lodhi et al., 2002). If we
restrict s to be only single words we end up hav-
ing a bag-of-words (BOW) representation. Allow-
ing longer substrings lead us to the Word Sequence
Kernels of Cancedda et al. (2003), which also al-
low gaps between words.
One extension of these kernels is to allow soft
matching between substrings. This is done by
defining a similarity matrix S, which encode sym-
bol similarities. This matrix can be defined by ex-
ternal resources, like WordNet, or be inferred from
data using Latent Semantic Analysis (Deerwester
et al., 1990) for example.
</bodyText>
<subsectionHeader confidence="0.915483">
3.1.2 Tree Kernels
</subsectionHeader>
<bodyText confidence="0.999933727272727">
Collins and Duffy (2001) first introduced Tree
Kernels, which measure the similarity between
two trees by counting the number of fragments
they share, in a very similar way to string kernels.
Consider two trees T1 and T2. We define the set
of nodes in these two trees as N1 and N2 respec-
tively. Consider also F the full set of possible tree
fragments (similar to E* in the case of strings). We
define Ii(n) as an indicator function that returns 1
if fragment fi E F has root n and 0 otherwise. A
Tree Kernel can then be defined as:
</bodyText>
<equation confidence="0.993109333333333">
E
k(T1, T2) =
n1EN1
where:
A(n1,n2) = E |S |Asize(i)Ii(n1)Ii(n2).
i=1
</equation>
<bodyText confidence="0.999912578947369">
Here, 0 &lt; A &lt; 1 is a decay factor that penalizes
contributions from larger fragments cf. smaller
ones.
Again, we can put restrictions on the type of
tree fragment considered for comparison. Collins
and Duffy (2001) defined Subtree kernels, which
considered only subtrees as fragments, and Subset
Tree Kernels (SSTK), where fragments can have
non-terminals as leaves. Later, Moschitti (2006)
introduced the Partial Tree Kernels (PTK), by al-
lowing fragments with partial rule expansions.
Tree kernels were used in a variety of tasks, in-
cluding Relation Extraction (Bloehdorn and Mos-
chitti, 2007; Plank and Moschitti, 2013), Ques-
tion Classification (Moschitti, 2006; Croce et al.,
2011) and Quality Estimation (Hardmeier, 2011;
Hardmeier et al., 2012). Furthermore, soft match-
ing approaches were also used by Bloehdorn and
Moschitti (2007) and Croce et al. (2011).
</bodyText>
<subsectionHeader confidence="0.9958">
3.2 Multi-task Kernels
</subsectionHeader>
<bodyText confidence="0.9995784">
Kernels can also be extended to deal with set-
tings where we want to predict a vector of val-
ues ( ´Alvarez et al., 2012). These settings are use-
ful in multi-task and domain adaptation problems.
Kernels for vector-valued functions are known as
coregionalization kernels in the literature. Here
we are going to refer them as multi-task kernels.
One of the simplest ways to define a kernel for
a multi-task setting is the Intrinsic Coregionaliza-
tion Model (ICM):
</bodyText>
<equation confidence="0.642672">
K(x, x&apos;) = B ® k(x, x&apos;).
</equation>
<bodyText confidence="0.999970473684211">
where ® denotes the Kronecker product and B
is the coregionalization matrix, encoding task co-
variances. We also denote the resulting kernel
function as K(x, x&apos;) to stress out that its result is
now a matrix instead of a scalar.
Cohn and Specia (2013) used the ICM to model
annotator bias in Quality Estimation datasets.
They parameterize B in a number of differ-
ent ways and get significant improvements over
single-task baselines, especially in post-editing
time prediction. They also point out that the well
known EasyAdapt method (Daum´e III, 2007) for
domain adaptation can be modeled by the ICM us-
ing B = 1+I, i.e., a coregionalization matrix with
its diagonal elements equal to 2 and remaining el-
ements equal to 1.
An extension of the ICM is the Linear Model of
Coregionalization (LMC), which assume a sum of
kernels with different coregionalization matrices:
</bodyText>
<equation confidence="0.984236">
EK(x,x&apos;) = Bp ® kp(x, x&apos;).
k,EP
</equation>
<bodyText confidence="0.9994814">
where P is the set of different kernels employed.
´Alvarez et al. (2012) argue that the LMC is much
more flexible than the ICM because the latter as-
sumes that each kernel contributes equally to the
task covariances.
</bodyText>
<equation confidence="0.9630375">
E A(n1, n2),
n2EN2
</equation>
<page confidence="0.984208">
4
</page>
<sectionHeader confidence="0.998697" genericHeader="method">
4 Planned Work
</sectionHeader>
<bodyText confidence="0.999962">
Our goal in this proposal is to employ GPs and
the kernels introduced in Section 3 to advance
the state-of-the-art in regression and classification
NLP tasks. It would be unfeasible though, at least
for a single thesis, to address all possible tasks so
we are going to focus on three of them where ker-
nel methods were already successfully applied.
</bodyText>
<subsectionHeader confidence="0.997168">
4.1 Quality Estimation
</subsectionHeader>
<bodyText confidence="0.999813424242424">
The purpose of Machine Translation Quality Esti-
mation is to provide a quality prediction for new,
unseen machine translated texts, without relying
on reference translations (Blatz et al., 2004; Bojar
et al., 2013). A common use of quality predictions
is the decision between post-editing a given ma-
chine translated sentence and translating its source
from scratch.
GP regression models were recently success-
fully employed for post-editing time (Cohn and
Specia, 2013) and HTER2 prediction (Beck et al.,
2013). Both used RBF kernels as the covariance
function so a natural extension is to apply the
structured kernels of Section 3.1. This was already
been done with tree kernels by Hardmeier (2011)
in the context of SVMs.
Multi-task kernels can also be applied for Qual-
ity Estimation in several ways. The model used
by Cohn and Specia (2013) for modelling annota-
tor bias can be further extended for settings with
dozens or even hundreds of annotators. This is a
common setting in crowdsourcing platforms like
Amazon’s Mechanical Turk3.
Another plan is to use multi-task kernels to
combine different datasets. Quality annotation is
usually expensive, requiring post-editing or sub-
jective scoring. Possibilities include combining
datasets from different language pairs or different
machine translation systems. Available datasets
include those used in the WMT12 and WMT13
QE shared tasks (Callison-burch et al., 2012; Bo-
jar et al., 2013) and others (Specia et al., 2009;
Specia, 2011; Koponen et al., 2012).
</bodyText>
<subsectionHeader confidence="0.992154">
4.2 Question Classification
</subsectionHeader>
<bodyText confidence="0.99991">
A Question Classifier is a module that aims to re-
strict the answer hypotheses generated by a Ques-
tion Answering system by applying a label to the
input question (Li and Roth, 2002; Li and Roth,
</bodyText>
<footnote confidence="0.99866">
2Human Translation Error Rate (Snover et al., 2006).
3www.mturk.com
</footnote>
<bodyText confidence="0.999736307692308">
2005). This task can be seen as an instance of text
classification, where the inputs are usually com-
posed of only one sentence.
Much of previous work in Question Classifica-
tion largely used SVMs combined with structured
kernels. Zhang and Lee (2003) compares String
Kernels based on BOW and n-gram representa-
tions with the Subset Tree Kernel on constituent
trees. Moschitti (2006) show improved results by
using the Partial Tree Kernel and dependency trees
instead of constituency ones. Bloehdorn and Mos-
chitti (2007) combines a SSTK with different soft
matching approaches to encode lexical similarity
on tree leaves. The same soft matching idea is
used by Croce et al. (2011), but applied to PTKs
instead and permitting soft matches between any
nodes in each tree (which is sensible when using
kernels on dependency trees).
Our work proposes to address this task by em-
ploying tree kernels and GPs. Unlike Quality Esti-
mation, this is a classification setting and our pur-
pose is to find if this combination can also improve
the state-of-the-art for tasks of this kind. We will
use the TREC dataset provided by Li and Roth
(2002), which assigns 6000 questions with both a
coarse and a fine-grained label.
</bodyText>
<subsectionHeader confidence="0.999632">
4.3 Multi-domain Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.998530434782609">
Sentiment Analysis is defined as “the computa-
tional treatment of opinion, sentiment and subjec-
tivity in text” (Pang and Lee, 2008). In this pro-
posal, we focus on the specific task of polarity de-
tection, where the goal is to label a text as hav-
ing positive or negative sentiment. State-of-the-art
methods for this task use SVMs as the learning al-
gorithm and vary between the feature sets used.
Polarity predictions can be heavily biased on
the text domain. Consider the example showed
by Turney (2002): the word “unpredictable” usu-
ally has a positive meaning in a movie review but
a negative one when applied to an automotive re-
view (in a phrase like “unpredictable steering”, for
instance). One of the first methods to tackle this
issue is the Structural Correspondence Learning
of Blitzer et al. (2007). Their method uses pivot
words shared between domains to find correspon-
dencies in words that are not shared.
A previous work that used structured kernels in
Sentiment Analysis is the approach of Wu et al.
(2009). Their method uses tree kernels on phrase
dependency trees and outperforms bag-of-words
</bodyText>
<page confidence="0.985108">
5
</page>
<bodyText confidence="0.9999070625">
and word dependency approaches. They also show
good results in cross-domain experiments.
We propose to apply GPs with a combination
of structured and multi-task kernels for this task.
The results showed by Wu et al. (2009) suggest
that tree kernels on dependency trees are a good
approach but we also plan to employ string ker-
nels on this task. This is because string kernels
have demonstrated promising results for text cate-
gorization in past work. Also, considering model
selection is easily dealt by GPs, we can combine
all those kernels in complex and heavily param-
eterized ways, an unfeasible setting for SVMs.
We will use the Multi-Domain Sentiment Dataset
(Blitzer et al., 2007), composed of Amazon prod-
uct reviews in different categories.
</bodyText>
<subsectionHeader confidence="0.98104">
4.4 Research Directions
</subsectionHeader>
<bodyText confidence="0.999987027777778">
In Section 2.3 we saw how the Bayesian formu-
lation of GPs let us do model selection by maxi-
mizing the marginal likelihood. In fact, one of our
main research directions in this proposal revolves
around this crucial point: because we can easily fit
hyperparameters to the data we have much more
freedom to use richer kernel parameterizations and
kernel combinations. Multi-task kernels are one
example where we usually have a large number of
hyperparameters because we need to fit all the el-
ements of the coregionalization matrix. This num-
ber can get even larger if we have a LMC model,
with multiple coregionalization matrices. Struc-
tured kernels can also be redefined in a richer way:
tree kernels between constituency trees could have
multiple decay hyperparameters, one for each POS
tag. A more extreme example would be to treat
all weights in a string kernel as hyperparameters.
Thus, we plan to investigate these possibilities in
the context of the three tasks detailed before.
As another research direction we also want to
address the issue of scalability. Although GPs al-
ready showed promising results they can be slow
when compared to other well established meth-
ods like SVM. Fortunately there has been a lot
of advancements in the field of sparse GPs in the
last years and we plan to employ them in our
work. A key question is how to combine sparse
GPs with the structured kernels we presented be-
fore. Although it is perfectly possible to select in-
ducing points using greedy methods, it would be
much more interesting to use the pseudo-inputs
approach. However, it is not clear how to do that
in conjunction with non-vectorial inputs, like the
ones we plan to use in structured kernels, and this
is a key direction that we also plan to investigate.
</bodyText>
<subsectionHeader confidence="0.989148">
4.5 GP Toolkits
</subsectionHeader>
<bodyText confidence="0.999921727272727">
Available toolkits for GP modelling include
GPML4 (Rasmussen and Williams, 2006) and GP-
stuff5 (Vanhatalo et al., 2013), which are written
in Matlab. Our experiments will mainly use GPy6,
an open source toolkit written in Python. It imple-
ments models for regression and binary classifi-
cation, including sparse approximations and many
vectorial kernels. We plan to contribute to GPy
by implementing the structured kernels of Section
3.1, effectively extending it to a GP framework for
NLP.
</bodyText>
<sectionHeader confidence="0.993757" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999799">
In this work we showed a proposal for advancing
the state-of-the-art in a number of NLP tasks by
combining Gaussian Process with structured and
multi-task kernels. Our hypothesis is that highly
parameterized kernel combinations allied with the
fitting methods provided by GPs will result in bet-
ter models for these tasks. We also detailed the
future plans for experiments, including available
datasets and toolkits.
Further research directions that can be explored
by this proposal include the use of GPs in different
learning settings. Models for ordinal regression
(Chu and Ghahramani, 2005) and structured pre-
diction (Altun et al., 2004; Brati`eres et al., 2013)
were already proposed in the GP literature and a
natural extension is to apply these models for their
corresponding NLP tasks. Another extension is to
employ other kinds of kernels. The literature on
that subject is quite vast, with many approaches
showing promising results.
</bodyText>
<sectionHeader confidence="0.97021" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9990925">
This work was supported by funding from
CNPq/Brazil (No. 237999/2012-9) and from the
EU FP7-ICT QTLaunchPad project (No. 296347).
The author would also like to thank Yahoo for the
financial support and the anonymous reviewers for
their excellent comments.
</bodyText>
<footnote confidence="0.9940952">
4www.gaussianprocess.org/gpml/code/
matlab
5becs.aalto.fi/en/research/bayes/
gpstuff
6github.com/SheffieldML/GPy
</footnote>
<page confidence="0.998188">
6
</page>
<sectionHeader confidence="0.931566" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997902009803921">
Yasemin Altun, Thomas Hofmann, and Alexander J.
Smola. 2004. Gaussian Process Classification for
Segmenting and Annotating Sequences. In Proceed-
ings of ICML, page 8, New York, New York, USA.
ACM Press.
Mauricio A. ´Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for Vector-Valued Func-
tions: a Review. Foundations and Trends in Ma-
chine Learning, pages 1–37.
Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia
Specia. 2013. SHEF-Lite : When Less is More for
Translation Quality Estimation. In Proceedings of
WMT13, pages 337–342.
John Blatz, Erin Fitzgerald, and George Foster. 2004.
Confidence estimation for machine translation. In
Proceedings of the 20th Conference on Computa-
tional Linguistics, pages 315–321.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes and
Blenders: Domain Adaptation for Sentiment Clas-
sification. In Proceedings of ACL.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Exploiting Structure and Semantics for Expressive
Text Kernels. In Proceedings of CIKM.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proceedings
of WMT13, pages 1–44.
S´ebastien Brati`eres, Novi Quadrianto, and Zoubin
Ghahramani. 2013. Bayesian Structured Prediction
using Gaussian Processes. arXiv:1307.3846, pages
1–17.
Chris Callison-burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of 7th Workshop
on Statistical Machine Translation.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean-Michel Renders. 2003. Word-Sequence Ker-
nels. The Journal of Machine Learning Research,
3:1059–1082.
Kai-Wei Chang, Vivek Srikumar, and Dan Roth. 2013.
Multi-core Structural SVM Training. In Proceed-
ings of ECML-PHDD.
Wei Chu and Zoubin Ghahramani. 2005. Gaussian
Processes for Ordinal Regression. Journal of Ma-
chine Learning Research, 6:1019–1041.
Wei Chu, Zoubin Ghahramani, Francesco Falciani, and
David L Wild. 2005. Biomarker discovery in mi-
croarray gene expression data with Gaussian pro-
cesses. Bioinformatics, 21(16):3385–93, August.
Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of ACL.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. The
Journal of Machine Learning, 11:3053–3096.
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Advances in Neu-
ral Information Processing Systems.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured Lexical Similarity via Con-
volution Kernels on Dependency Trees. In Proc. of
EMNLP.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of
the American Society For Information Science, 41.
Thomas G¨artner, Peter Flach, and Stefan Wrobel.
2003. On Graph Kernels: Hardness Results and Ef-
ficient Alternatives. LNAI, 2777:129–143.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21–54, July.
Christian Hardmeier, Joakim Nivre, and J¨org Tiede-
mann. 2012. Tree Kernels for Machine Transla-
tion Quality Estimation. In Proceedings of WMT12,
number 2011, pages 109–113.
Christian Hardmeier. 2011. Improving Machine
Translation Quality Prediction with Syntactic Tree
Kernels. In Proceedings of EAMT, number May.
David Haussler. 1999. Convolution Kernels on Dis-
crete Structures. Technical report.
James Hensman, Nicol`o Fusi, and Neil D. Lawrence.
2013. Gaussian Processes for Big Data. In Pro-
ceedings of UAI.
Matt Hoffman, David M. Blei, Chong Wang, and John
Paisley. 2013. Stochastic Variational Inference. The
Journal of Machine Learning Research.
Tommi Jaakkola, Mark Diekhans, and David Haussler.
2000. A discriminative framework for detecting re-
mote protein homologies. Journal of Computational
Biology, 7:95–114.
Richard Johansson and Alessandro Moschitti. 2013.
Relational Features in Fine-Grained Opinion Analy-
sis. Computational Linguistics, 39(3):473–509.
</reference>
<page confidence="0.996002">
7
</page>
<reference confidence="0.999536653846154">
Jonathan Ko, Daniel J. Klein, Dieter Fox, and Dirk
Haehnel. 2007. Gaussian Processes and Reinforce-
ment Learning for Identification and Control of an
Autonomous Blimp. In Proceedings of IEEE Inter-
national Conference on Robotics and Automation,
pages 742–747. Ieee, April.
Maarit Koponen, Wilker Aziz, Luciana Ramos, and
Lucia Specia. 2012. Post-editing time as a measure
of cognitive effort. In Proceedings of WPTP.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of COLING, volume 1, pages
1–7.
Xin Li and Dan Roth. 2005. Learning Question Clas-
sifiers: the Role of Semantic Information. Natural
Language Engineering, 1(1).
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
Classification using String Kernels. The Journal of
Machine Learning Research, 2:419–444.
Thomas P. Minka. 2001. A family of algorithms for
approximate Bayesian inference. Ph.D. thesis.
Alessandro Moschitti. 2006. Efficient Convolution
Kernels for Dependency and Constituent Syntactic
Trees. In Proceedings of ECML.
Kevin P. Murphy. 2012. Machine Learning: a Proba-
bilistic Perspective.
Radford M. Neal. 1998. Regression and Classification
Using Gaussian Process Priors. Bayesian Statistics,
6.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in In-
formation Retrieval, 2(1–2):1–135.
Ver´onica P´erez-Rosas and Rada Mihalcea. 2013. Sen-
timent Analysis of Online Spoken Reviews. In Pro-
ceedings of Interspeech.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding Semantic Similarity in Tree Kernels for Do-
main Adaptation of Relation Extraction. In Pro-
ceedings of ACL, pages 1498–1507.
Tamara Polajnar, Simon Rogers, and Mark Girolami.
2011. Protein interaction detection in sentences via
Gaussian Processes: a preliminary evaluation. Inter-
national Journal of Data Mining and Bioinformat-
ics, 5(1):52–72, January.
Daniel Preotiuc-Pietro and Trevor Cohn. 2013. A tem-
poral model of text periodicities using Gaussian Pro-
cesses. In Proceedings of EMNLP.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian processes for machine
learning, volume 1. MIT Press Cambridge.
Jaakko Riihim¨aki, Pasi Jyl¨anki, and Aki Vehtari. 2013.
Nested Expectation Propagation for Gaussian Pro-
cess Classification with a Multinomial Probit Like-
lihood. Journal of Machine Learning Research,
14:75–109.
Anton Schwaighofer, Marian Grigoras, Volker Tresp,
and Clemens Hoffmann. 2004. GPPS: A Gaussian
Process Positioning System for Cellular Networks.
In Proceedings of NIPS.
Matthias Seeger, Christopher K. I. Williams, and
Neil D. Lawrence. 2003. Fast Forward Selection
to Speed Up Sparse Gaussian Process Regression.
In Proceedings ofAISTATS.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel methods for pattern analysis. Cambridge.
Fabian H. Sinz, Joaquin Qui˜nonero Candela,
G¨okhan H. Bakır, Carl E. Rasmussen, and
Matthias O. Franz. 2004. Learning Depth from
Stereo. Pattern Recognition, pages 1–8.
Edward Snelson and Zoubin Ghahramani. 2006.
Sparse Gaussian Processes using Pseudo-inputs. In
Proceedings of NIPS.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAMTA.
Lucia Specia and Atefeh Farzindar. 2010. Estimating
machine translation post-editing effort with HTER.
In Proceedings of AMTA Workshop Bringing MT to
the User: MT Research and the Translation Indus-
try.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of EAMT, pages 28–35.
Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of EAMT.
Michalis K. Titsias. 2009. Variational Learning of In-
ducing Variables in Sparse Gaussian Processes. In
Proceedings ofAISTATS, volume 5, pages 567–574.
Peter D. Turney. 2002. Thumbs Up or Thumbs
Down?: Semantic Orientation Applied to Unsuper-
vised Classification of Reviews. In Proceedings of
ACL, number July, pages 417–424.
Jarno Vanhatalo, Jaakko Riihim¨aki, Jouni Hartikainen,
Pasi Jyl¨anki, Ville Tolvanen, and Aki Vehtari. 2013.
GPstuff: Bayesian Modeling with Gaussian Pro-
cesses. The Journal of Machine Learning Research,
14:1175–1179.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi
Kondor, and Karsten M. Borgwardt. 2010. Graph
Kernels. Journal of Machine Learning Research,
11:1201–1242.
</reference>
<page confidence="0.97611">
8
</page>
<reference confidence="0.999708">
Christopher K. I. Williams and David Barber. 1998.
Bayesian Classification with Gaussian Processes.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 20(12):1342–1351.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase Dependency Parsing for Opinion Min-
ing. In Proceedings of EMNLP, pages 1533–1541.
Dell Zhang and Wee Sun Lee. 2003. Question classi-
fication using support vector machines. In Proceed-
ings of SIGIR, New York, New York, USA. ACM
Press.
</reference>
<page confidence="0.99706">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647707">
<title confidence="0.999963">Bayesian Kernel Methods for Natural Language Processing</title>
<author confidence="0.992714">Daniel</author>
<affiliation confidence="0.9977415">Department of Computer University of</affiliation>
<address confidence="0.67956">Sheffield, United</address>
<email confidence="0.925507">debeck1@sheffield.ac.uk</email>
<abstract confidence="0.9985006">Kernel methods are heavily used in Natural Language Processing (NLP). Frequentist approaches like Support Vector Machines are the state-of-the-art in many tasks. However, these approaches lack efficient procedures for model selection, which hinders the usage of more advanced kernels. In this work, we propose the use of a Bayesian approach for kernel methods, Gaussian Processes, which allow easy model fitting even for complex kernel combinations. Our goal is to employ this approach to improve results in a number of regression and classification tasks in NLP.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasemin Altun</author>
<author>Thomas Hofmann</author>
<author>Alexander J Smola</author>
</authors>
<title>Gaussian Process Classification for Segmenting and Annotating Sequences.</title>
<date>2004</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>8</pages>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="23779" citStr="Altun et al., 2004" startWordPosition="3942" endWordPosition="3945">d a proposal for advancing the state-of-the-art in a number of NLP tasks by combining Gaussian Process with structured and multi-task kernels. Our hypothesis is that highly parameterized kernel combinations allied with the fitting methods provided by GPs will result in better models for these tasks. We also detailed the future plans for experiments, including available datasets and toolkits. Further research directions that can be explored by this proposal include the use of GPs in different learning settings. Models for ordinal regression (Chu and Ghahramani, 2005) and structured prediction (Altun et al., 2004; Brati`eres et al., 2013) were already proposed in the GP literature and a natural extension is to apply these models for their corresponding NLP tasks. Another extension is to employ other kinds of kernels. The literature on that subject is quite vast, with many approaches showing promising results. Acknowledgements This work was supported by funding from CNPq/Brazil (No. 237999/2012-9) and from the EU FP7-ICT QTLaunchPad project (No. 296347). The author would also like to thank Yahoo for the financial support and the anonymous reviewers for their excellent comments. 4www.gaussianprocess.org</context>
</contexts>
<marker>Altun, Hofmann, Smola, 2004</marker>
<rawString>Yasemin Altun, Thomas Hofmann, and Alexander J. Smola. 2004. Gaussian Process Classification for Segmenting and Annotating Sequences. In Proceedings of ICML, page 8, New York, New York, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauricio A ´Alvarez</author>
<author>Lorenzo Rosasco</author>
<author>Neil D Lawrence</author>
</authors>
<title>Kernels for Vector-Valued Functions: a Review. Foundations and Trends</title>
<date>2012</date>
<booktitle>in Machine Learning,</booktitle>
<pages>1--37</pages>
<marker>´Alvarez, Rosasco, Lawrence, 2012</marker>
<rawString>Mauricio A. ´Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. 2012. Kernels for Vector-Valued Functions: a Review. Foundations and Trends in Machine Learning, pages 1–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Beck</author>
<author>Kashif Shah</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>SHEF-Lite : When Less is More for Translation Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of WMT13,</booktitle>
<pages>337--342</pages>
<contexts>
<context position="16444" citStr="Beck et al., 2013" startWordPosition="2737" endWordPosition="2740">oing to focus on three of them where kernel methods were already successfully applied. 4.1 Quality Estimation The purpose of Machine Translation Quality Estimation is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch. GP regression models were recently successfully employed for post-editing time (Cohn and Specia, 2013) and HTER2 prediction (Beck et al., 2013). Both used RBF kernels as the covariance function so a natural extension is to apply the structured kernels of Section 3.1. This was already been done with tree kernels by Hardmeier (2011) in the context of SVMs. Multi-task kernels can also be applied for Quality Estimation in several ways. The model used by Cohn and Specia (2013) for modelling annotator bias can be further extended for settings with dozens or even hundreds of annotators. This is a common setting in crowdsourcing platforms like Amazon’s Mechanical Turk3. Another plan is to use multi-task kernels to combine different datasets.</context>
</contexts>
<marker>Beck, Shah, Cohn, Specia, 2013</marker>
<rawString>Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. SHEF-Lite : When Less is More for Translation Quality Estimation. In Proceedings of WMT13, pages 337–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th Conference on Computational Linguistics,</booktitle>
<pages>315--321</pages>
<contexts>
<context position="16128" citStr="Blatz et al., 2004" startWordPosition="2688" endWordPosition="2691"> task covariances. E A(n1, n2), n2EN2 4 4 Planned Work Our goal in this proposal is to employ GPs and the kernels introduced in Section 3 to advance the state-of-the-art in regression and classification NLP tasks. It would be unfeasible though, at least for a single thesis, to address all possible tasks so we are going to focus on three of them where kernel methods were already successfully applied. 4.1 Quality Estimation The purpose of Machine Translation Quality Estimation is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch. GP regression models were recently successfully employed for post-editing time (Cohn and Specia, 2013) and HTER2 prediction (Beck et al., 2013). Both used RBF kernels as the covariance function so a natural extension is to apply the structured kernels of Section 3.1. This was already been done with tree kernels by Hardmeier (2011) in the context of SVMs. Multi-task kernels can also be applied for Quality Estimation in sever</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, and George Foster. 2004. Confidence estimation for machine translation. In Proceedings of the 20th Conference on Computational Linguistics, pages 315–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="19775" citStr="Blitzer et al. (2007)" startWordPosition="3284" endWordPosition="3287"> polarity detection, where the goal is to label a text as having positive or negative sentiment. State-of-the-art methods for this task use SVMs as the learning algorithm and vary between the feature sets used. Polarity predictions can be heavily biased on the text domain. Consider the example showed by Turney (2002): the word “unpredictable” usually has a positive meaning in a movie review but a negative one when applied to an automotive review (in a phrase like “unpredictable steering”, for instance). One of the first methods to tackle this issue is the Structural Correspondence Learning of Blitzer et al. (2007). Their method uses pivot words shared between domains to find correspondencies in words that are not shared. A previous work that used structured kernels in Sentiment Analysis is the approach of Wu et al. (2009). Their method uses tree kernels on phrase dependency trees and outperforms bag-of-words 5 and word dependency approaches. They also show good results in cross-domain experiments. We propose to apply GPs with a combination of structured and multi-task kernels for this task. The results showed by Wu et al. (2009) suggest that tree kernels on dependency trees are a good approach but we a</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Exploiting Structure and Semantics for Expressive Text Kernels.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="13639" citStr="Bloehdorn and Moschitti, 2007" startWordPosition="2270" endWordPosition="2274">2) = E |S |Asize(i)Ii(n1)Ii(n2). i=1 Here, 0 &lt; A &lt; 1 is a decay factor that penalizes contributions from larger fragments cf. smaller ones. Again, we can put restrictions on the type of tree fragment considered for comparison. Collins and Duffy (2001) defined Subtree kernels, which considered only subtrees as fragments, and Subset Tree Kernels (SSTK), where fragments can have non-terminals as leaves. Later, Moschitti (2006) introduced the Partial Tree Kernels (PTK), by allowing fragments with partial rule expansions. Tree kernels were used in a variety of tasks, including Relation Extraction (Bloehdorn and Moschitti, 2007; Plank and Moschitti, 2013), Question Classification (Moschitti, 2006; Croce et al., 2011) and Quality Estimation (Hardmeier, 2011; Hardmeier et al., 2012). Furthermore, soft matching approaches were also used by Bloehdorn and Moschitti (2007) and Croce et al. (2011). 3.2 Multi-task Kernels Kernels can also be extended to deal with settings where we want to predict a vector of values ( ´Alvarez et al., 2012). These settings are useful in multi-task and domain adaptation problems. Kernels for vector-valued functions are known as coregionalization kernels in the literature. Here we are going to</context>
<context position="18249" citStr="Bloehdorn and Moschitti (2007)" startWordPosition="3024" endWordPosition="3028">ut question (Li and Roth, 2002; Li and Roth, 2Human Translation Error Rate (Snover et al., 2006). 3www.mturk.com 2005). This task can be seen as an instance of text classification, where the inputs are usually composed of only one sentence. Much of previous work in Question Classification largely used SVMs combined with structured kernels. Zhang and Lee (2003) compares String Kernels based on BOW and n-gram representations with the Subset Tree Kernel on constituent trees. Moschitti (2006) show improved results by using the Partial Tree Kernel and dependency trees instead of constituency ones. Bloehdorn and Moschitti (2007) combines a SSTK with different soft matching approaches to encode lexical similarity on tree leaves. The same soft matching idea is used by Croce et al. (2011), but applied to PTKs instead and permitting soft matches between any nodes in each tree (which is sensible when using kernels on dependency trees). Our work proposes to address this task by employing tree kernels and GPs. Unlike Quality Estimation, this is a classification setting and our purpose is to find if this combination can also improve the state-of-the-art for tasks of this kind. We will use the TREC dataset provided by Li and </context>
</contexts>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Stephan Bloehdorn and Alessandro Moschitti. 2007. Exploiting Structure and Semantics for Expressive Text Kernels. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of WMT13,</booktitle>
<pages>1--44</pages>
<contexts>
<context position="1861" citStr="Bojar et al., 2013" startWordPosition="282" endWordPosition="285">problems where we do not have much prior knowledge about how the data behaves. This is a common setting in NLP, where they have been mostly applied in the form of Support Vector Machines (SVMs). Systems based on SVMs have been the state-of-the-art in classification tasks like Text Categorization (Lodhi et al., 2002), Sentiment Analysis (Johansson and Moschitti, 2013; P´erez-Rosas and Mihalcea, 2013) and Question Classification (Moschitti, 2006; Croce et al., 2011). Recently, they were also employed in regression settings like Machine Translation Quality Estimation (Specia and Farzindar, 2010; Bojar et al., 2013) and structured prediction (Chang et al., 2013). SVMs are a frequentist method: they aim to find an approximation to the exact latent function that explains the data. This is in contrast to Bayesian settings, which define a prior distribution on this function and perform inference by marginalizing over all its possible values. Although there is some discussion about which approach is better (Murphy, 2012, Sec. 6.6.4), Bayesian methods offer many useful theoretical properties. In fact, they have been used before in NLP, especially in grammar induction (Cohn et al., 2010) and word segmentation (</context>
<context position="16149" citStr="Bojar et al., 2013" startWordPosition="2692" endWordPosition="2695"> A(n1, n2), n2EN2 4 4 Planned Work Our goal in this proposal is to employ GPs and the kernels introduced in Section 3 to advance the state-of-the-art in regression and classification NLP tasks. It would be unfeasible though, at least for a single thesis, to address all possible tasks so we are going to focus on three of them where kernel methods were already successfully applied. 4.1 Quality Estimation The purpose of Machine Translation Quality Estimation is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch. GP regression models were recently successfully employed for post-editing time (Cohn and Specia, 2013) and HTER2 prediction (Beck et al., 2013). Both used RBF kernels as the covariance function so a natural extension is to apply the structured kernels of Section 3.1. This was already been done with tree kernels by Hardmeier (2011) in the context of SVMs. Multi-task kernels can also be applied for Quality Estimation in several ways. The model us</context>
<context position="17371" citStr="Bojar et al., 2013" startWordPosition="2881" endWordPosition="2885">by Cohn and Specia (2013) for modelling annotator bias can be further extended for settings with dozens or even hundreds of annotators. This is a common setting in crowdsourcing platforms like Amazon’s Mechanical Turk3. Another plan is to use multi-task kernels to combine different datasets. Quality annotation is usually expensive, requiring post-editing or subjective scoring. Possibilities include combining datasets from different language pairs or different machine translation systems. Available datasets include those used in the WMT12 and WMT13 QE shared tasks (Callison-burch et al., 2012; Bojar et al., 2013) and others (Specia et al., 2009; Specia, 2011; Koponen et al., 2012). 4.2 Question Classification A Question Classifier is a module that aims to restrict the answer hypotheses generated by a Question Answering system by applying a label to the input question (Li and Roth, 2002; Li and Roth, 2Human Translation Error Rate (Snover et al., 2006). 3www.mturk.com 2005). This task can be seen as an instance of text classification, where the inputs are usually composed of only one sentence. Much of previous work in Question Classification largely used SVMs combined with structured kernels. Zhang and </context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of WMT13, pages 1–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S´ebastien Brati`eres</author>
<author>Novi Quadrianto</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Bayesian Structured Prediction using Gaussian Processes.</title>
<date>2013</date>
<pages>1--17</pages>
<marker>Brati`eres, Quadrianto, Ghahramani, 2013</marker>
<rawString>S´ebastien Brati`eres, Novi Quadrianto, and Zoubin Ghahramani. 2013. Bayesian Structured Prediction using Gaussian Processes. arXiv:1307.3846, pages 1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of 7th Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="17350" citStr="Callison-burch et al., 2012" startWordPosition="2877" endWordPosition="2880">several ways. The model used by Cohn and Specia (2013) for modelling annotator bias can be further extended for settings with dozens or even hundreds of annotators. This is a common setting in crowdsourcing platforms like Amazon’s Mechanical Turk3. Another plan is to use multi-task kernels to combine different datasets. Quality annotation is usually expensive, requiring post-editing or subjective scoring. Possibilities include combining datasets from different language pairs or different machine translation systems. Available datasets include those used in the WMT12 and WMT13 QE shared tasks (Callison-burch et al., 2012; Bojar et al., 2013) and others (Specia et al., 2009; Specia, 2011; Koponen et al., 2012). 4.2 Question Classification A Question Classifier is a module that aims to restrict the answer hypotheses generated by a Question Answering system by applying a label to the input question (Li and Roth, 2002; Li and Roth, 2Human Translation Error Rate (Snover et al., 2006). 3www.mturk.com 2005). This task can be seen as an instance of text classification, where the inputs are usually composed of only one sentence. Much of previous work in Question Classification largely used SVMs combined with structure</context>
</contexts>
<marker>Callison-burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of 7th Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Cancedda</author>
</authors>
<title>Eric Gaussier, Cyril Goutte, and Jean-Michel Renders.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1059</pages>
<marker>Cancedda, 2003</marker>
<rawString>Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean-Michel Renders. 2003. Word-Sequence Kernels. The Journal of Machine Learning Research, 3:1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Vivek Srikumar</author>
<author>Dan Roth</author>
</authors>
<title>Multi-core Structural SVM Training.</title>
<date>2013</date>
<booktitle>In Proceedings of ECML-PHDD.</booktitle>
<contexts>
<context position="1908" citStr="Chang et al., 2013" startWordPosition="289" endWordPosition="292">dge about how the data behaves. This is a common setting in NLP, where they have been mostly applied in the form of Support Vector Machines (SVMs). Systems based on SVMs have been the state-of-the-art in classification tasks like Text Categorization (Lodhi et al., 2002), Sentiment Analysis (Johansson and Moschitti, 2013; P´erez-Rosas and Mihalcea, 2013) and Question Classification (Moschitti, 2006; Croce et al., 2011). Recently, they were also employed in regression settings like Machine Translation Quality Estimation (Specia and Farzindar, 2010; Bojar et al., 2013) and structured prediction (Chang et al., 2013). SVMs are a frequentist method: they aim to find an approximation to the exact latent function that explains the data. This is in contrast to Bayesian settings, which define a prior distribution on this function and perform inference by marginalizing over all its possible values. Although there is some discussion about which approach is better (Murphy, 2012, Sec. 6.6.4), Bayesian methods offer many useful theoretical properties. In fact, they have been used before in NLP, especially in grammar induction (Cohn et al., 2010) and word segmentation (Goldwater et al., 2009). However, only very rec</context>
</contexts>
<marker>Chang, Srikumar, Roth, 2013</marker>
<rawString>Kai-Wei Chang, Vivek Srikumar, and Dan Roth. 2013. Multi-core Structural SVM Training. In Proceedings of ECML-PHDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Chu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Gaussian Processes for Ordinal Regression.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1019</pages>
<contexts>
<context position="23733" citStr="Chu and Ghahramani, 2005" startWordPosition="3934" endWordPosition="3937">. 5 Conclusions and Future Work In this work we showed a proposal for advancing the state-of-the-art in a number of NLP tasks by combining Gaussian Process with structured and multi-task kernels. Our hypothesis is that highly parameterized kernel combinations allied with the fitting methods provided by GPs will result in better models for these tasks. We also detailed the future plans for experiments, including available datasets and toolkits. Further research directions that can be explored by this proposal include the use of GPs in different learning settings. Models for ordinal regression (Chu and Ghahramani, 2005) and structured prediction (Altun et al., 2004; Brati`eres et al., 2013) were already proposed in the GP literature and a natural extension is to apply these models for their corresponding NLP tasks. Another extension is to employ other kinds of kernels. The literature on that subject is quite vast, with many approaches showing promising results. Acknowledgements This work was supported by funding from CNPq/Brazil (No. 237999/2012-9) and from the EU FP7-ICT QTLaunchPad project (No. 296347). The author would also like to thank Yahoo for the financial support and the anonymous reviewers for thei</context>
</contexts>
<marker>Chu, Ghahramani, 2005</marker>
<rawString>Wei Chu and Zoubin Ghahramani. 2005. Gaussian Processes for Ordinal Regression. Journal of Machine Learning Research, 6:1019–1041.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Chu</author>
<author>Zoubin Ghahramani</author>
<author>Francesco Falciani</author>
<author>David L Wild</author>
</authors>
<title>Biomarker discovery in microarray gene expression data with Gaussian processes.</title>
<date>2005</date>
<journal>Bioinformatics,</journal>
<volume>21</volume>
<issue>16</issue>
<contexts>
<context position="3815" citStr="Chu et al., 2005" startWordPosition="595" endWordPosition="598">pens the door for the use of heavily parameterized kernel combinations, like multi-task kernels for example. • Being a probabilistic framework, they are able to naturally encode uncertainty in the predictions, which can be propagated if the task is part of a larger system pipeline. Besides these properties, GPs have also been applied sucessfully in many Machine Learning 1 Proceedings of the ACL 2014 Student Research Workshop, pages 1–9, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics tasks. Examples include Robotics (Ko et al., 2007), Bioinformatics (Chu et al., 2005; Polajnar et al., 2011), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004; Riihim¨aki et al., 2013). In NLP, GPs have been used only very recently and focused on regression tasks (Cohn and Specia, 2013; Preotiuc-Pietro and Cohn, 2013). In this work, we propose to combine GPs with recent kernel developments to advance the state-of-the-art in a number of NLP tasks. 2 Gaussian Processes In this Section, we follow closely the definition of Rasmussen and Williams (2006). Consider a machine learning setting, where we have a dataset { X = (x1, y1), (x2, y2), ... , (xn, </context>
</contexts>
<marker>Chu, Ghahramani, Falciani, Wild, 2005</marker>
<rawString>Wei Chu, Zoubin Ghahramani, Francesco Falciani, and David L Wild. 2005. Biomarker discovery in microarray gene expression data with Gaussian processes. Bioinformatics, 21(16):3385–93, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4047" citStr="Cohn and Specia, 2013" startWordPosition="634" endWordPosition="637">agated if the task is part of a larger system pipeline. Besides these properties, GPs have also been applied sucessfully in many Machine Learning 1 Proceedings of the ACL 2014 Student Research Workshop, pages 1–9, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics tasks. Examples include Robotics (Ko et al., 2007), Bioinformatics (Chu et al., 2005; Polajnar et al., 2011), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004; Riihim¨aki et al., 2013). In NLP, GPs have been used only very recently and focused on regression tasks (Cohn and Specia, 2013; Preotiuc-Pietro and Cohn, 2013). In this work, we propose to combine GPs with recent kernel developments to advance the state-of-the-art in a number of NLP tasks. 2 Gaussian Processes In this Section, we follow closely the definition of Rasmussen and Williams (2006). Consider a machine learning setting, where we have a dataset { X = (x1, y1), (x2, y2), ... , (xn, yn)} and our goal is to infer the underlying function f(x) that best explains the data. A GP model assumes a prior stochastic process over this function: f(x) ∼ GP(µ(x), k(x, x&apos;)), (1) where µ(x) is the mean function, which is usual</context>
<context position="14666" citStr="Cohn and Specia (2013)" startWordPosition="2443" endWordPosition="2446">, 2012). These settings are useful in multi-task and domain adaptation problems. Kernels for vector-valued functions are known as coregionalization kernels in the literature. Here we are going to refer them as multi-task kernels. One of the simplest ways to define a kernel for a multi-task setting is the Intrinsic Coregionalization Model (ICM): K(x, x&apos;) = B ® k(x, x&apos;). where ® denotes the Kronecker product and B is the coregionalization matrix, encoding task covariances. We also denote the resulting kernel function as K(x, x&apos;) to stress out that its result is now a matrix instead of a scalar. Cohn and Specia (2013) used the ICM to model annotator bias in Quality Estimation datasets. They parameterize B in a number of different ways and get significant improvements over single-task baselines, especially in post-editing time prediction. They also point out that the well known EasyAdapt method (Daum´e III, 2007) for domain adaptation can be modeled by the ICM using B = 1+I, i.e., a coregionalization matrix with its diagonal elements equal to 2 and remaining elements equal to 1. An extension of the ICM is the Linear Model of Coregionalization (LMC), which assume a sum of kernels with different coregionaliza</context>
<context position="16403" citStr="Cohn and Specia, 2013" startWordPosition="2730" endWordPosition="2733">is, to address all possible tasks so we are going to focus on three of them where kernel methods were already successfully applied. 4.1 Quality Estimation The purpose of Machine Translation Quality Estimation is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch. GP regression models were recently successfully employed for post-editing time (Cohn and Specia, 2013) and HTER2 prediction (Beck et al., 2013). Both used RBF kernels as the covariance function so a natural extension is to apply the structured kernels of Section 3.1. This was already been done with tree kernels by Hardmeier (2011) in the context of SVMs. Multi-task kernels can also be applied for Quality Estimation in several ways. The model used by Cohn and Specia (2013) for modelling annotator bias can be further extended for settings with dozens or even hundreds of annotators. This is a common setting in crowdsourcing platforms like Amazon’s Mechanical Turk3. Another plan is to use multi-ta</context>
</contexts>
<marker>Cohn, Specia, 2013</marker>
<rawString>Trevor Cohn and Lucia Specia. 2013. Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Sharon Goldwater</author>
</authors>
<title>Inducing tree-substitution grammars.</title>
<date>2010</date>
<booktitle>The Journal of Machine Learning,</booktitle>
<pages>11--3053</pages>
<contexts>
<context position="2437" citStr="Cohn et al., 2010" startWordPosition="374" endWordPosition="377">a and Farzindar, 2010; Bojar et al., 2013) and structured prediction (Chang et al., 2013). SVMs are a frequentist method: they aim to find an approximation to the exact latent function that explains the data. This is in contrast to Bayesian settings, which define a prior distribution on this function and perform inference by marginalizing over all its possible values. Although there is some discussion about which approach is better (Murphy, 2012, Sec. 6.6.4), Bayesian methods offer many useful theoretical properties. In fact, they have been used before in NLP, especially in grammar induction (Cohn et al., 2010) and word segmentation (Goldwater et al., 2009). However, only very recently kernel methods have been applied in NLP using the Bayesian approach. Gaussian Processes (GPs) are the Bayesian counterpart of kernel methods and are widely considered the state-of-the-art for inference on functions (Hensman et al., 2013). They have a number of advantages which are very useful in NLP: • Kernels in general can be combined and parameterized in many ways. This parameterization lead to the problem of model selection, which is difficult in frequentist approaches (mainly based on cross validation). The Bayes</context>
</contexts>
<marker>Cohn, Blunsom, Goldwater, 2010</marker>
<rawString>Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. The Journal of Machine Learning, 11:3053–3096.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="12469" citStr="Collins and Duffy (2001)" startWordPosition="2070" endWordPosition="2073">acters as symbols (Lodhi et al., 2002). If we restrict s to be only single words we end up having a bag-of-words (BOW) representation. Allowing longer substrings lead us to the Word Sequence Kernels of Cancedda et al. (2003), which also allow gaps between words. One extension of these kernels is to allow soft matching between substrings. This is done by defining a similarity matrix S, which encode symbol similarities. This matrix can be defined by external resources, like WordNet, or be inferred from data using Latent Semantic Analysis (Deerwester et al., 1990) for example. 3.1.2 Tree Kernels Collins and Duffy (2001) first introduced Tree Kernels, which measure the similarity between two trees by counting the number of fragments they share, in a very similar way to string kernels. Consider two trees T1 and T2. We define the set of nodes in these two trees as N1 and N2 respectively. Consider also F the full set of possible tree fragments (similar to E* in the case of strings). We define Ii(n) as an indicator function that returns 1 if fragment fi E F has root n and 0 otherwise. A Tree Kernel can then be defined as: E k(T1, T2) = n1EN1 where: A(n1,n2) = E |S |Asize(i)Ii(n1)Ii(n2). i=1 Here, 0 &lt; A &lt; 1 is a d</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution Kernels for Natural Language. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>Structured Lexical Similarity via Convolution Kernels on Dependency Trees.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1710" citStr="Croce et al., 2011" startWordPosition="260" endWordPosition="263"> or trees, which greatly reduce the need for careful feature engineering in these structures. The properties cited above make kernel methods ideal for problems where we do not have much prior knowledge about how the data behaves. This is a common setting in NLP, where they have been mostly applied in the form of Support Vector Machines (SVMs). Systems based on SVMs have been the state-of-the-art in classification tasks like Text Categorization (Lodhi et al., 2002), Sentiment Analysis (Johansson and Moschitti, 2013; P´erez-Rosas and Mihalcea, 2013) and Question Classification (Moschitti, 2006; Croce et al., 2011). Recently, they were also employed in regression settings like Machine Translation Quality Estimation (Specia and Farzindar, 2010; Bojar et al., 2013) and structured prediction (Chang et al., 2013). SVMs are a frequentist method: they aim to find an approximation to the exact latent function that explains the data. This is in contrast to Bayesian settings, which define a prior distribution on this function and perform inference by marginalizing over all its possible values. Although there is some discussion about which approach is better (Murphy, 2012, Sec. 6.6.4), Bayesian methods offer many</context>
<context position="13730" citStr="Croce et al., 2011" startWordPosition="2284" endWordPosition="2287">rom larger fragments cf. smaller ones. Again, we can put restrictions on the type of tree fragment considered for comparison. Collins and Duffy (2001) defined Subtree kernels, which considered only subtrees as fragments, and Subset Tree Kernels (SSTK), where fragments can have non-terminals as leaves. Later, Moschitti (2006) introduced the Partial Tree Kernels (PTK), by allowing fragments with partial rule expansions. Tree kernels were used in a variety of tasks, including Relation Extraction (Bloehdorn and Moschitti, 2007; Plank and Moschitti, 2013), Question Classification (Moschitti, 2006; Croce et al., 2011) and Quality Estimation (Hardmeier, 2011; Hardmeier et al., 2012). Furthermore, soft matching approaches were also used by Bloehdorn and Moschitti (2007) and Croce et al. (2011). 3.2 Multi-task Kernels Kernels can also be extended to deal with settings where we want to predict a vector of values ( ´Alvarez et al., 2012). These settings are useful in multi-task and domain adaptation problems. Kernels for vector-valued functions are known as coregionalization kernels in the literature. Here we are going to refer them as multi-task kernels. One of the simplest ways to define a kernel for a multi-</context>
<context position="18409" citStr="Croce et al. (2011)" startWordPosition="3052" endWordPosition="3055">fication, where the inputs are usually composed of only one sentence. Much of previous work in Question Classification largely used SVMs combined with structured kernels. Zhang and Lee (2003) compares String Kernels based on BOW and n-gram representations with the Subset Tree Kernel on constituent trees. Moschitti (2006) show improved results by using the Partial Tree Kernel and dependency trees instead of constituency ones. Bloehdorn and Moschitti (2007) combines a SSTK with different soft matching approaches to encode lexical similarity on tree leaves. The same soft matching idea is used by Croce et al. (2011), but applied to PTKs instead and permitting soft matches between any nodes in each tree (which is sensible when using kernels on dependency trees). Our work proposes to address this task by employing tree kernels and GPs. Unlike Quality Estimation, this is a classification setting and our purpose is to find if this combination can also improve the state-of-the-art for tasks of this kind. We will use the TREC dataset provided by Li and Roth (2002), which assigns 6000 questions with both a coarse and a fine-grained label. 4.3 Multi-domain Sentiment Analysis Sentiment Analysis is defined as “the</context>
</contexts>
<marker>Croce, Moschitti, Basili, 2011</marker>
<rawString>Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured Lexical Similarity via Convolution Kernels on Dependency Trees. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society For Information Science,</journal>
<volume>41</volume>
<contexts>
<context position="12412" citStr="Deerwester et al., 1990" startWordPosition="2061" endWordPosition="2064"> symbol, although some previous work also considered characters as symbols (Lodhi et al., 2002). If we restrict s to be only single words we end up having a bag-of-words (BOW) representation. Allowing longer substrings lead us to the Word Sequence Kernels of Cancedda et al. (2003), which also allow gaps between words. One extension of these kernels is to allow soft matching between substrings. This is done by defining a similarity matrix S, which encode symbol similarities. This matrix can be defined by external resources, like WordNet, or be inferred from data using Latent Semantic Analysis (Deerwester et al., 1990) for example. 3.1.2 Tree Kernels Collins and Duffy (2001) first introduced Tree Kernels, which measure the similarity between two trees by counting the number of fragments they share, in a very similar way to string kernels. Consider two trees T1 and T2. We define the set of nodes in these two trees as N1 and N2 respectively. Consider also F the full set of possible tree fragments (similar to E* in the case of strings). We define Ii(n) as an indicator function that returns 1 if fragment fi E F has root n and 0 otherwise. A Tree Kernel can then be defined as: E k(T1, T2) = n1EN1 where: A(n1,n2)</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society For Information Science, 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G¨artner</author>
<author>Peter Flach</author>
<author>Stefan Wrobel</author>
</authors>
<title>On Graph Kernels: Hardness Results and Efficient Alternatives.</title>
<date>2003</date>
<booktitle>LNAI,</booktitle>
<pages>2777--129</pages>
<marker>G¨artner, Flach, Wrobel, 2003</marker>
<rawString>Thomas G¨artner, Peter Flach, and Stefan Wrobel. 2003. On Graph Kernels: Hardness Results and Efficient Alternatives. LNAI, 2777:129–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="2484" citStr="Goldwater et al., 2009" startWordPosition="382" endWordPosition="385"> and structured prediction (Chang et al., 2013). SVMs are a frequentist method: they aim to find an approximation to the exact latent function that explains the data. This is in contrast to Bayesian settings, which define a prior distribution on this function and perform inference by marginalizing over all its possible values. Although there is some discussion about which approach is better (Murphy, 2012, Sec. 6.6.4), Bayesian methods offer many useful theoretical properties. In fact, they have been used before in NLP, especially in grammar induction (Cohn et al., 2010) and word segmentation (Goldwater et al., 2009). However, only very recently kernel methods have been applied in NLP using the Bayesian approach. Gaussian Processes (GPs) are the Bayesian counterpart of kernel methods and are widely considered the state-of-the-art for inference on functions (Hensman et al., 2013). They have a number of advantages which are very useful in NLP: • Kernels in general can be combined and parameterized in many ways. This parameterization lead to the problem of model selection, which is difficult in frequentist approaches (mainly based on cross validation). The Bayesian formulation of GPs let them deal with model</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Joakim Nivre</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Tree Kernels for Machine Translation Quality Estimation.</title>
<date>2012</date>
<booktitle>In Proceedings of WMT12, number</booktitle>
<pages>109--113</pages>
<contexts>
<context position="13795" citStr="Hardmeier et al., 2012" startWordPosition="2293" endWordPosition="2296">rictions on the type of tree fragment considered for comparison. Collins and Duffy (2001) defined Subtree kernels, which considered only subtrees as fragments, and Subset Tree Kernels (SSTK), where fragments can have non-terminals as leaves. Later, Moschitti (2006) introduced the Partial Tree Kernels (PTK), by allowing fragments with partial rule expansions. Tree kernels were used in a variety of tasks, including Relation Extraction (Bloehdorn and Moschitti, 2007; Plank and Moschitti, 2013), Question Classification (Moschitti, 2006; Croce et al., 2011) and Quality Estimation (Hardmeier, 2011; Hardmeier et al., 2012). Furthermore, soft matching approaches were also used by Bloehdorn and Moschitti (2007) and Croce et al. (2011). 3.2 Multi-task Kernels Kernels can also be extended to deal with settings where we want to predict a vector of values ( ´Alvarez et al., 2012). These settings are useful in multi-task and domain adaptation problems. Kernels for vector-valued functions are known as coregionalization kernels in the literature. Here we are going to refer them as multi-task kernels. One of the simplest ways to define a kernel for a multi-task setting is the Intrinsic Coregionalization Model (ICM): K(x,</context>
</contexts>
<marker>Hardmeier, Nivre, Tiedemann, 2012</marker>
<rawString>Christian Hardmeier, Joakim Nivre, and J¨org Tiedemann. 2012. Tree Kernels for Machine Translation Quality Estimation. In Proceedings of WMT12, number 2011, pages 109–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
</authors>
<title>Improving Machine Translation Quality Prediction with Syntactic Tree Kernels.</title>
<date>2011</date>
<booktitle>In Proceedings of EAMT, number</booktitle>
<contexts>
<context position="13770" citStr="Hardmeier, 2011" startWordPosition="2291" endWordPosition="2292">, we can put restrictions on the type of tree fragment considered for comparison. Collins and Duffy (2001) defined Subtree kernels, which considered only subtrees as fragments, and Subset Tree Kernels (SSTK), where fragments can have non-terminals as leaves. Later, Moschitti (2006) introduced the Partial Tree Kernels (PTK), by allowing fragments with partial rule expansions. Tree kernels were used in a variety of tasks, including Relation Extraction (Bloehdorn and Moschitti, 2007; Plank and Moschitti, 2013), Question Classification (Moschitti, 2006; Croce et al., 2011) and Quality Estimation (Hardmeier, 2011; Hardmeier et al., 2012). Furthermore, soft matching approaches were also used by Bloehdorn and Moschitti (2007) and Croce et al. (2011). 3.2 Multi-task Kernels Kernels can also be extended to deal with settings where we want to predict a vector of values ( ´Alvarez et al., 2012). These settings are useful in multi-task and domain adaptation problems. Kernels for vector-valued functions are known as coregionalization kernels in the literature. Here we are going to refer them as multi-task kernels. One of the simplest ways to define a kernel for a multi-task setting is the Intrinsic Coregional</context>
<context position="16633" citStr="Hardmeier (2011)" startWordPosition="2771" endWordPosition="2772">diction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch. GP regression models were recently successfully employed for post-editing time (Cohn and Specia, 2013) and HTER2 prediction (Beck et al., 2013). Both used RBF kernels as the covariance function so a natural extension is to apply the structured kernels of Section 3.1. This was already been done with tree kernels by Hardmeier (2011) in the context of SVMs. Multi-task kernels can also be applied for Quality Estimation in several ways. The model used by Cohn and Specia (2013) for modelling annotator bias can be further extended for settings with dozens or even hundreds of annotators. This is a common setting in crowdsourcing platforms like Amazon’s Mechanical Turk3. Another plan is to use multi-task kernels to combine different datasets. Quality annotation is usually expensive, requiring post-editing or subjective scoring. Possibilities include combining datasets from different language pairs or different machine translati</context>
</contexts>
<marker>Hardmeier, 2011</marker>
<rawString>Christian Hardmeier. 2011. Improving Machine Translation Quality Prediction with Syntactic Tree Kernels. In Proceedings of EAMT, number May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution Kernels on Discrete Structures.</title>
<date>1999</date>
<tech>Technical report.</tech>
<contexts>
<context position="11194" citStr="Haussler (1999)" startWordPosition="1853" endWordPosition="1854">ned over two realvalued vectors. Our focus in this work is on two different types of kernels which can be applied for NLP settings and allow richer parameterizations. 3.1 Kernels for Discrete Structures In NLP, discrete structures like strings or trees are common in training data. To apply a vectorial kernel like the RBF, one can always extract realvalued features from these structures. However, kernels can be defined directly on these structures, potentially reducing the need for feature engineering. The string and tree kernels we define here are based on the theory of Convolution kernels of Haussler (1999), which calculate the similarity between two structures based on the number of substructures they have in common. Other approaches include random walk kernels (G¨artner et al., 2003; Vishwanathan et al., 2010) and Fisher kernels (Jaakkola et al., 2000). 3.1.1 String Kernels Consider a function φs(x) that counts the number of times a substring s appears in x. A string kernel , 3 is defined as: Ek(x, x&apos;) = wsos(x)os(x&apos;), (10) sEE∗ where ws is a non-negative weight for substring s and E* is the set of all possible strings over the symbol alphabet E. Usually in NLP, each word is considered a symbo</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution Kernels on Discrete Structures. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Hensman</author>
<author>Nicol`o Fusi</author>
<author>Neil D Lawrence</author>
</authors>
<title>Gaussian Processes for Big Data.</title>
<date>2013</date>
<booktitle>In Proceedings of UAI.</booktitle>
<contexts>
<context position="2751" citStr="Hensman et al., 2013" startWordPosition="423" endWordPosition="426">rm inference by marginalizing over all its possible values. Although there is some discussion about which approach is better (Murphy, 2012, Sec. 6.6.4), Bayesian methods offer many useful theoretical properties. In fact, they have been used before in NLP, especially in grammar induction (Cohn et al., 2010) and word segmentation (Goldwater et al., 2009). However, only very recently kernel methods have been applied in NLP using the Bayesian approach. Gaussian Processes (GPs) are the Bayesian counterpart of kernel methods and are widely considered the state-of-the-art for inference on functions (Hensman et al., 2013). They have a number of advantages which are very useful in NLP: • Kernels in general can be combined and parameterized in many ways. This parameterization lead to the problem of model selection, which is difficult in frequentist approaches (mainly based on cross validation). The Bayesian formulation of GPs let them deal with model selection in a much more more efficient and elegant way: by maximizing the likelihood on the training data. This opens the door for the use of heavily parameterized kernel combinations, like multi-task kernels for example. • Being a probabilistic framework, they are</context>
<context position="10110" citStr="Hensman et al. (2013)" startWordPosition="1672" endWordPosition="1675"> inputs. The resulting complexities for training and prediction are O(m2n) and O(m), respectively. The remaining question is how to choose the inducing inputs. Seeger et al. (2003) use an iterative method that starts with some random data points and adds new ones based on a greedy procedure, in an active learning fashion. Snelson and Ghahramani (2006) use a different approach: it defines a fixed m a priori and use pseudo-inputs which can be optimized as regular hyperparameters. Later, Titsias (2009) also used pseudo-inputs but perform optimization using a variational method instead. Recently, Hensman et al. (2013) modified this method to allow Stochastic Variational Inference (Hoffman et al., 2013), which reduces the training complexity to O(m3). 3 Kernels The core of a GP model is the kernel function. A kernel k(x, x&apos;) is a symmetric and positive semidefinite function which returns a similarity score between two inputs in some feature space (ShaweTaylor and Cristianini, 2004). Probably the most used kernel in general is the Radial Basis Function (RBF) kernel, which is defined over two realvalued vectors. Our focus in this work is on two different types of kernels which can be applied for NLP settings </context>
</contexts>
<marker>Hensman, Fusi, Lawrence, 2013</marker>
<rawString>James Hensman, Nicol`o Fusi, and Neil D. Lawrence. 2013. Gaussian Processes for Big Data. In Proceedings of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Hoffman</author>
<author>David M Blei</author>
<author>Chong Wang</author>
<author>John Paisley</author>
</authors>
<title>Stochastic Variational Inference.</title>
<date>2013</date>
<journal>The Journal of Machine Learning Research.</journal>
<contexts>
<context position="10196" citStr="Hoffman et al., 2013" startWordPosition="1685" endWordPosition="1688">espectively. The remaining question is how to choose the inducing inputs. Seeger et al. (2003) use an iterative method that starts with some random data points and adds new ones based on a greedy procedure, in an active learning fashion. Snelson and Ghahramani (2006) use a different approach: it defines a fixed m a priori and use pseudo-inputs which can be optimized as regular hyperparameters. Later, Titsias (2009) also used pseudo-inputs but perform optimization using a variational method instead. Recently, Hensman et al. (2013) modified this method to allow Stochastic Variational Inference (Hoffman et al., 2013), which reduces the training complexity to O(m3). 3 Kernels The core of a GP model is the kernel function. A kernel k(x, x&apos;) is a symmetric and positive semidefinite function which returns a similarity score between two inputs in some feature space (ShaweTaylor and Cristianini, 2004). Probably the most used kernel in general is the Radial Basis Function (RBF) kernel, which is defined over two realvalued vectors. Our focus in this work is on two different types of kernels which can be applied for NLP settings and allow richer parameterizations. 3.1 Kernels for Discrete Structures In NLP, discre</context>
</contexts>
<marker>Hoffman, Blei, Wang, Paisley, 2013</marker>
<rawString>Matt Hoffman, David M. Blei, Chong Wang, and John Paisley. 2013. Stochastic Variational Inference. The Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tommi Jaakkola</author>
<author>Mark Diekhans</author>
<author>David Haussler</author>
</authors>
<title>A discriminative framework for detecting remote protein homologies.</title>
<date>2000</date>
<journal>Journal of Computational Biology,</journal>
<pages>7--95</pages>
<contexts>
<context position="11446" citStr="Jaakkola et al., 2000" startWordPosition="1891" endWordPosition="1894">r trees are common in training data. To apply a vectorial kernel like the RBF, one can always extract realvalued features from these structures. However, kernels can be defined directly on these structures, potentially reducing the need for feature engineering. The string and tree kernels we define here are based on the theory of Convolution kernels of Haussler (1999), which calculate the similarity between two structures based on the number of substructures they have in common. Other approaches include random walk kernels (G¨artner et al., 2003; Vishwanathan et al., 2010) and Fisher kernels (Jaakkola et al., 2000). 3.1.1 String Kernels Consider a function φs(x) that counts the number of times a substring s appears in x. A string kernel , 3 is defined as: Ek(x, x&apos;) = wsos(x)os(x&apos;), (10) sEE∗ where ws is a non-negative weight for substring s and E* is the set of all possible strings over the symbol alphabet E. Usually in NLP, each word is considered a symbol, although some previous work also considered characters as symbols (Lodhi et al., 2002). If we restrict s to be only single words we end up having a bag-of-words (BOW) representation. Allowing longer substrings lead us to the Word Sequence Kernels of</context>
</contexts>
<marker>Jaakkola, Diekhans, Haussler, 2000</marker>
<rawString>Tommi Jaakkola, Mark Diekhans, and David Haussler. 2000. A discriminative framework for detecting remote protein homologies. Journal of Computational Biology, 7:95–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Relational Features in Fine-Grained Opinion Analysis.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="1610" citStr="Johansson and Moschitti, 2013" startWordPosition="246" endWordPosition="250">ern in the data. Another advantage of kernels is that they can be defined in arbitrary structures like strings or trees, which greatly reduce the need for careful feature engineering in these structures. The properties cited above make kernel methods ideal for problems where we do not have much prior knowledge about how the data behaves. This is a common setting in NLP, where they have been mostly applied in the form of Support Vector Machines (SVMs). Systems based on SVMs have been the state-of-the-art in classification tasks like Text Categorization (Lodhi et al., 2002), Sentiment Analysis (Johansson and Moschitti, 2013; P´erez-Rosas and Mihalcea, 2013) and Question Classification (Moschitti, 2006; Croce et al., 2011). Recently, they were also employed in regression settings like Machine Translation Quality Estimation (Specia and Farzindar, 2010; Bojar et al., 2013) and structured prediction (Chang et al., 2013). SVMs are a frequentist method: they aim to find an approximation to the exact latent function that explains the data. This is in contrast to Bayesian settings, which define a prior distribution on this function and perform inference by marginalizing over all its possible values. Although there is so</context>
</contexts>
<marker>Johansson, Moschitti, 2013</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2013. Relational Features in Fine-Grained Opinion Analysis. Computational Linguistics, 39(3):473–509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Ko</author>
<author>Daniel J Klein</author>
<author>Dieter Fox</author>
<author>Dirk Haehnel</author>
</authors>
<title>Gaussian Processes and Reinforcement Learning for Identification and Control of an Autonomous Blimp.</title>
<date>2007</date>
<booktitle>In Proceedings of IEEE International Conference on Robotics and Automation,</booktitle>
<pages>742--747</pages>
<publisher>Ieee,</publisher>
<contexts>
<context position="3781" citStr="Ko et al., 2007" startWordPosition="590" endWordPosition="593">ihood on the training data. This opens the door for the use of heavily parameterized kernel combinations, like multi-task kernels for example. • Being a probabilistic framework, they are able to naturally encode uncertainty in the predictions, which can be propagated if the task is part of a larger system pipeline. Besides these properties, GPs have also been applied sucessfully in many Machine Learning 1 Proceedings of the ACL 2014 Student Research Workshop, pages 1–9, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics tasks. Examples include Robotics (Ko et al., 2007), Bioinformatics (Chu et al., 2005; Polajnar et al., 2011), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004; Riihim¨aki et al., 2013). In NLP, GPs have been used only very recently and focused on regression tasks (Cohn and Specia, 2013; Preotiuc-Pietro and Cohn, 2013). In this work, we propose to combine GPs with recent kernel developments to advance the state-of-the-art in a number of NLP tasks. 2 Gaussian Processes In this Section, we follow closely the definition of Rasmussen and Williams (2006). Consider a machine learning setting, where we have a dataset { X</context>
</contexts>
<marker>Ko, Klein, Fox, Haehnel, 2007</marker>
<rawString>Jonathan Ko, Daniel J. Klein, Dieter Fox, and Dirk Haehnel. 2007. Gaussian Processes and Reinforcement Learning for Identification and Control of an Autonomous Blimp. In Proceedings of IEEE International Conference on Robotics and Automation, pages 742–747. Ieee, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarit Koponen</author>
<author>Wilker Aziz</author>
<author>Luciana Ramos</author>
<author>Lucia Specia</author>
</authors>
<title>Post-editing time as a measure of cognitive effort.</title>
<date>2012</date>
<booktitle>In Proceedings of WPTP.</booktitle>
<contexts>
<context position="17440" citStr="Koponen et al., 2012" startWordPosition="2894" endWordPosition="2897">er extended for settings with dozens or even hundreds of annotators. This is a common setting in crowdsourcing platforms like Amazon’s Mechanical Turk3. Another plan is to use multi-task kernels to combine different datasets. Quality annotation is usually expensive, requiring post-editing or subjective scoring. Possibilities include combining datasets from different language pairs or different machine translation systems. Available datasets include those used in the WMT12 and WMT13 QE shared tasks (Callison-burch et al., 2012; Bojar et al., 2013) and others (Specia et al., 2009; Specia, 2011; Koponen et al., 2012). 4.2 Question Classification A Question Classifier is a module that aims to restrict the answer hypotheses generated by a Question Answering system by applying a label to the input question (Li and Roth, 2002; Li and Roth, 2Human Translation Error Rate (Snover et al., 2006). 3www.mturk.com 2005). This task can be seen as an instance of text classification, where the inputs are usually composed of only one sentence. Much of previous work in Question Classification largely used SVMs combined with structured kernels. Zhang and Lee (2003) compares String Kernels based on BOW and n-gram representa</context>
</contexts>
<marker>Koponen, Aziz, Ramos, Specia, 2012</marker>
<rawString>Maarit Koponen, Wilker Aziz, Luciana Ramos, and Lucia Specia. 2012. Post-editing time as a measure of cognitive effort. In Proceedings of WPTP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING,</booktitle>
<volume>1</volume>
<pages>1--7</pages>
<contexts>
<context position="17649" citStr="Li and Roth, 2002" startWordPosition="2930" endWordPosition="2933">nt datasets. Quality annotation is usually expensive, requiring post-editing or subjective scoring. Possibilities include combining datasets from different language pairs or different machine translation systems. Available datasets include those used in the WMT12 and WMT13 QE shared tasks (Callison-burch et al., 2012; Bojar et al., 2013) and others (Specia et al., 2009; Specia, 2011; Koponen et al., 2012). 4.2 Question Classification A Question Classifier is a module that aims to restrict the answer hypotheses generated by a Question Answering system by applying a label to the input question (Li and Roth, 2002; Li and Roth, 2Human Translation Error Rate (Snover et al., 2006). 3www.mturk.com 2005). This task can be seen as an instance of text classification, where the inputs are usually composed of only one sentence. Much of previous work in Question Classification largely used SVMs combined with structured kernels. Zhang and Lee (2003) compares String Kernels based on BOW and n-gram representations with the Subset Tree Kernel on constituent trees. Moschitti (2006) show improved results by using the Partial Tree Kernel and dependency trees instead of constituency ones. Bloehdorn and Moschitti (2007)</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of COLING, volume 1, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning Question Classifiers: the Role of Semantic Information.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>1</issue>
<marker>Li, Roth, 2005</marker>
<rawString>Xin Li and Dan Roth. 2005. Learning Question Classifiers: the Role of Semantic Information. Natural Language Engineering, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text Classification using String Kernels.</title>
<date>2002</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="1559" citStr="Lodhi et al., 2002" startWordPosition="240" endWordPosition="243">ess assumptions about the underlying pattern in the data. Another advantage of kernels is that they can be defined in arbitrary structures like strings or trees, which greatly reduce the need for careful feature engineering in these structures. The properties cited above make kernel methods ideal for problems where we do not have much prior knowledge about how the data behaves. This is a common setting in NLP, where they have been mostly applied in the form of Support Vector Machines (SVMs). Systems based on SVMs have been the state-of-the-art in classification tasks like Text Categorization (Lodhi et al., 2002), Sentiment Analysis (Johansson and Moschitti, 2013; P´erez-Rosas and Mihalcea, 2013) and Question Classification (Moschitti, 2006; Croce et al., 2011). Recently, they were also employed in regression settings like Machine Translation Quality Estimation (Specia and Farzindar, 2010; Bojar et al., 2013) and structured prediction (Chang et al., 2013). SVMs are a frequentist method: they aim to find an approximation to the exact latent function that explains the data. This is in contrast to Bayesian settings, which define a prior distribution on this function and perform inference by marginalizing</context>
<context position="11883" citStr="Lodhi et al., 2002" startWordPosition="1970" endWordPosition="1973">umber of substructures they have in common. Other approaches include random walk kernels (G¨artner et al., 2003; Vishwanathan et al., 2010) and Fisher kernels (Jaakkola et al., 2000). 3.1.1 String Kernels Consider a function φs(x) that counts the number of times a substring s appears in x. A string kernel , 3 is defined as: Ek(x, x&apos;) = wsos(x)os(x&apos;), (10) sEE∗ where ws is a non-negative weight for substring s and E* is the set of all possible strings over the symbol alphabet E. Usually in NLP, each word is considered a symbol, although some previous work also considered characters as symbols (Lodhi et al., 2002). If we restrict s to be only single words we end up having a bag-of-words (BOW) representation. Allowing longer substrings lead us to the Word Sequence Kernels of Cancedda et al. (2003), which also allow gaps between words. One extension of these kernels is to allow soft matching between substrings. This is done by defining a similarity matrix S, which encode symbol similarities. This matrix can be defined by external resources, like WordNet, or be inferred from data using Latent Semantic Analysis (Deerwester et al., 1990) for example. 3.1.2 Tree Kernels Collins and Duffy (2001) first introdu</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text Classification using String Kernels. The Journal of Machine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P Minka</author>
</authors>
<title>A family of algorithms for approximate Bayesian inference.</title>
<date>2001</date>
<tech>Ph.D. thesis.</tech>
<contexts>
<context position="7234" citStr="Minka, 2001" startWordPosition="1195" endWordPosition="1196">e σ(f*) is a squashing function. Two common choices are the logistic function and the probit function. The distribution over the latent values f* is obtained by integrating out the latent function: p(f* |x*, X, y) = if p(f* |x*, X, f )p(f |X, y)df. (6) Because the likelihood is not Gaussian, the resulting posterior integral is not analytically available anymore. The most common solution to this problem is to approximate the posterior p(f|X, y) with a Gaussian q(f|X, y). Two such approximation algorithms are the Laplace approximation (Williams and Barber, 1998) and the Expectation Propagation (Minka, 2001). Another option is to use Markov Chain Monte Carlo sampling methods on the true posterior (Neal, 1998). 2.3 Hyperparameter Optimization The GP prior used in the models described above usually have a number of hyperparameters. The 1Extensions to multi-class settings are possible. 2 most important ones are the kernel ones but they can also include others like the white noise variance σ2n used in regression. A key property of GPs is their ability to easily fit these hyperparameters to the data by maximizing the marginal likelihood: p(y|X, 0) = if p(y|X, 0, f)p(f), (7) where 0 represents the full</context>
</contexts>
<marker>Minka, 2001</marker>
<rawString>Thomas P. Minka. 2001. A family of algorithms for approximate Bayesian inference. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.</title>
<date>2006</date>
<booktitle>In Proceedings of ECML.</booktitle>
<contexts>
<context position="1689" citStr="Moschitti, 2006" startWordPosition="258" endWordPosition="259">ures like strings or trees, which greatly reduce the need for careful feature engineering in these structures. The properties cited above make kernel methods ideal for problems where we do not have much prior knowledge about how the data behaves. This is a common setting in NLP, where they have been mostly applied in the form of Support Vector Machines (SVMs). Systems based on SVMs have been the state-of-the-art in classification tasks like Text Categorization (Lodhi et al., 2002), Sentiment Analysis (Johansson and Moschitti, 2013; P´erez-Rosas and Mihalcea, 2013) and Question Classification (Moschitti, 2006; Croce et al., 2011). Recently, they were also employed in regression settings like Machine Translation Quality Estimation (Specia and Farzindar, 2010; Bojar et al., 2013) and structured prediction (Chang et al., 2013). SVMs are a frequentist method: they aim to find an approximation to the exact latent function that explains the data. This is in contrast to Bayesian settings, which define a prior distribution on this function and perform inference by marginalizing over all its possible values. Although there is some discussion about which approach is better (Murphy, 2012, Sec. 6.6.4), Bayesi</context>
<context position="13437" citStr="Moschitti (2006)" startWordPosition="2241" endWordPosition="2242">e of strings). We define Ii(n) as an indicator function that returns 1 if fragment fi E F has root n and 0 otherwise. A Tree Kernel can then be defined as: E k(T1, T2) = n1EN1 where: A(n1,n2) = E |S |Asize(i)Ii(n1)Ii(n2). i=1 Here, 0 &lt; A &lt; 1 is a decay factor that penalizes contributions from larger fragments cf. smaller ones. Again, we can put restrictions on the type of tree fragment considered for comparison. Collins and Duffy (2001) defined Subtree kernels, which considered only subtrees as fragments, and Subset Tree Kernels (SSTK), where fragments can have non-terminals as leaves. Later, Moschitti (2006) introduced the Partial Tree Kernels (PTK), by allowing fragments with partial rule expansions. Tree kernels were used in a variety of tasks, including Relation Extraction (Bloehdorn and Moschitti, 2007; Plank and Moschitti, 2013), Question Classification (Moschitti, 2006; Croce et al., 2011) and Quality Estimation (Hardmeier, 2011; Hardmeier et al., 2012). Furthermore, soft matching approaches were also used by Bloehdorn and Moschitti (2007) and Croce et al. (2011). 3.2 Multi-task Kernels Kernels can also be extended to deal with settings where we want to predict a vector of values ( ´Alvarez</context>
<context position="18112" citStr="Moschitti (2006)" startWordPosition="3006" endWordPosition="3007"> module that aims to restrict the answer hypotheses generated by a Question Answering system by applying a label to the input question (Li and Roth, 2002; Li and Roth, 2Human Translation Error Rate (Snover et al., 2006). 3www.mturk.com 2005). This task can be seen as an instance of text classification, where the inputs are usually composed of only one sentence. Much of previous work in Question Classification largely used SVMs combined with structured kernels. Zhang and Lee (2003) compares String Kernels based on BOW and n-gram representations with the Subset Tree Kernel on constituent trees. Moschitti (2006) show improved results by using the Partial Tree Kernel and dependency trees instead of constituency ones. Bloehdorn and Moschitti (2007) combines a SSTK with different soft matching approaches to encode lexical similarity on tree leaves. The same soft matching idea is used by Croce et al. (2011), but applied to PTKs instead and permitting soft matches between any nodes in each tree (which is sensible when using kernels on dependency trees). Our work proposes to address this task by employing tree kernels and GPs. Unlike Quality Estimation, this is a classification setting and our purpose is t</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Proceedings of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
</authors>
<date>2012</date>
<booktitle>Machine Learning: a Probabilistic Perspective.</booktitle>
<contexts>
<context position="2268" citStr="Murphy, 2012" startWordPosition="348" endWordPosition="350">n Classification (Moschitti, 2006; Croce et al., 2011). Recently, they were also employed in regression settings like Machine Translation Quality Estimation (Specia and Farzindar, 2010; Bojar et al., 2013) and structured prediction (Chang et al., 2013). SVMs are a frequentist method: they aim to find an approximation to the exact latent function that explains the data. This is in contrast to Bayesian settings, which define a prior distribution on this function and perform inference by marginalizing over all its possible values. Although there is some discussion about which approach is better (Murphy, 2012, Sec. 6.6.4), Bayesian methods offer many useful theoretical properties. In fact, they have been used before in NLP, especially in grammar induction (Cohn et al., 2010) and word segmentation (Goldwater et al., 2009). However, only very recently kernel methods have been applied in NLP using the Bayesian approach. Gaussian Processes (GPs) are the Bayesian counterpart of kernel methods and are widely considered the state-of-the-art for inference on functions (Hensman et al., 2013). They have a number of advantages which are very useful in NLP: • Kernels in general can be combined and parameteriz</context>
</contexts>
<marker>Murphy, 2012</marker>
<rawString>Kevin P. Murphy. 2012. Machine Learning: a Probabilistic Perspective.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Regression and Classification Using Gaussian Process Priors.</title>
<date>1998</date>
<journal>Bayesian Statistics,</journal>
<volume>6</volume>
<contexts>
<context position="7337" citStr="Neal, 1998" startWordPosition="1212" endWordPosition="1213">he distribution over the latent values f* is obtained by integrating out the latent function: p(f* |x*, X, y) = if p(f* |x*, X, f )p(f |X, y)df. (6) Because the likelihood is not Gaussian, the resulting posterior integral is not analytically available anymore. The most common solution to this problem is to approximate the posterior p(f|X, y) with a Gaussian q(f|X, y). Two such approximation algorithms are the Laplace approximation (Williams and Barber, 1998) and the Expectation Propagation (Minka, 2001). Another option is to use Markov Chain Monte Carlo sampling methods on the true posterior (Neal, 1998). 2.3 Hyperparameter Optimization The GP prior used in the models described above usually have a number of hyperparameters. The 1Extensions to multi-class settings are possible. 2 most important ones are the kernel ones but they can also include others like the white noise variance σ2n used in regression. A key property of GPs is their ability to easily fit these hyperparameters to the data by maximizing the marginal likelihood: p(y|X, 0) = if p(y|X, 0, f)p(f), (7) where 0 represents the full set of hyperparameters (which was suppressed from all conditionals until now for brevity). Optimizatio</context>
</contexts>
<marker>Neal, 1998</marker>
<rawString>Radford M. Neal. 1998. Regression and Classification Using Gaussian Process Priors. Bayesian Statistics, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="19102" citStr="Pang and Lee, 2008" startWordPosition="3168" endWordPosition="3171">s in each tree (which is sensible when using kernels on dependency trees). Our work proposes to address this task by employing tree kernels and GPs. Unlike Quality Estimation, this is a classification setting and our purpose is to find if this combination can also improve the state-of-the-art for tasks of this kind. We will use the TREC dataset provided by Li and Roth (2002), which assigns 6000 questions with both a coarse and a fine-grained label. 4.3 Multi-domain Sentiment Analysis Sentiment Analysis is defined as “the computational treatment of opinion, sentiment and subjectivity in text” (Pang and Lee, 2008). In this proposal, we focus on the specific task of polarity detection, where the goal is to label a text as having positive or negative sentiment. State-of-the-art methods for this task use SVMs as the learning algorithm and vary between the feature sets used. Polarity predictions can be heavily biased on the text domain. Consider the example showed by Turney (2002): the word “unpredictable” usually has a positive meaning in a movie review but a negative one when applied to an automotive review (in a phrase like “unpredictable steering”, for instance). One of the first methods to tackle this</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval, 2(1–2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ver´onica P´erez-Rosas</author>
<author>Rada Mihalcea</author>
</authors>
<title>Sentiment Analysis of Online Spoken Reviews.</title>
<date>2013</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<marker>P´erez-Rosas, Mihalcea, 2013</marker>
<rawString>Ver´onica P´erez-Rosas and Rada Mihalcea. 2013. Sentiment Analysis of Online Spoken Reviews. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1498--1507</pages>
<contexts>
<context position="13667" citStr="Plank and Moschitti, 2013" startWordPosition="2275" endWordPosition="2278">. i=1 Here, 0 &lt; A &lt; 1 is a decay factor that penalizes contributions from larger fragments cf. smaller ones. Again, we can put restrictions on the type of tree fragment considered for comparison. Collins and Duffy (2001) defined Subtree kernels, which considered only subtrees as fragments, and Subset Tree Kernels (SSTK), where fragments can have non-terminals as leaves. Later, Moschitti (2006) introduced the Partial Tree Kernels (PTK), by allowing fragments with partial rule expansions. Tree kernels were used in a variety of tasks, including Relation Extraction (Bloehdorn and Moschitti, 2007; Plank and Moschitti, 2013), Question Classification (Moschitti, 2006; Croce et al., 2011) and Quality Estimation (Hardmeier, 2011; Hardmeier et al., 2012). Furthermore, soft matching approaches were also used by Bloehdorn and Moschitti (2007) and Croce et al. (2011). 3.2 Multi-task Kernels Kernels can also be extended to deal with settings where we want to predict a vector of values ( ´Alvarez et al., 2012). These settings are useful in multi-task and domain adaptation problems. Kernels for vector-valued functions are known as coregionalization kernels in the literature. Here we are going to refer them as multi-task ke</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Barbara Plank and Alessandro Moschitti. 2013. Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction. In Proceedings of ACL, pages 1498–1507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Polajnar</author>
<author>Simon Rogers</author>
<author>Mark Girolami</author>
</authors>
<title>Protein interaction detection in sentences via Gaussian Processes: a preliminary evaluation.</title>
<date>2011</date>
<journal>International Journal of Data Mining and Bioinformatics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="3839" citStr="Polajnar et al., 2011" startWordPosition="599" endWordPosition="603">the use of heavily parameterized kernel combinations, like multi-task kernels for example. • Being a probabilistic framework, they are able to naturally encode uncertainty in the predictions, which can be propagated if the task is part of a larger system pipeline. Besides these properties, GPs have also been applied sucessfully in many Machine Learning 1 Proceedings of the ACL 2014 Student Research Workshop, pages 1–9, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics tasks. Examples include Robotics (Ko et al., 2007), Bioinformatics (Chu et al., 2005; Polajnar et al., 2011), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004; Riihim¨aki et al., 2013). In NLP, GPs have been used only very recently and focused on regression tasks (Cohn and Specia, 2013; Preotiuc-Pietro and Cohn, 2013). In this work, we propose to combine GPs with recent kernel developments to advance the state-of-the-art in a number of NLP tasks. 2 Gaussian Processes In this Section, we follow closely the definition of Rasmussen and Williams (2006). Consider a machine learning setting, where we have a dataset { X = (x1, y1), (x2, y2), ... , (xn, yn)} and our goal is to </context>
</contexts>
<marker>Polajnar, Rogers, Girolami, 2011</marker>
<rawString>Tamara Polajnar, Simon Rogers, and Mark Girolami. 2011. Protein interaction detection in sentences via Gaussian Processes: a preliminary evaluation. International Journal of Data Mining and Bioinformatics, 5(1):52–72, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Preotiuc-Pietro</author>
<author>Trevor Cohn</author>
</authors>
<title>A temporal model of text periodicities using Gaussian Processes.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4080" citStr="Preotiuc-Pietro and Cohn, 2013" startWordPosition="638" endWordPosition="641">art of a larger system pipeline. Besides these properties, GPs have also been applied sucessfully in many Machine Learning 1 Proceedings of the ACL 2014 Student Research Workshop, pages 1–9, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics tasks. Examples include Robotics (Ko et al., 2007), Bioinformatics (Chu et al., 2005; Polajnar et al., 2011), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004; Riihim¨aki et al., 2013). In NLP, GPs have been used only very recently and focused on regression tasks (Cohn and Specia, 2013; Preotiuc-Pietro and Cohn, 2013). In this work, we propose to combine GPs with recent kernel developments to advance the state-of-the-art in a number of NLP tasks. 2 Gaussian Processes In this Section, we follow closely the definition of Rasmussen and Williams (2006). Consider a machine learning setting, where we have a dataset { X = (x1, y1), (x2, y2), ... , (xn, yn)} and our goal is to infer the underlying function f(x) that best explains the data. A GP model assumes a prior stochastic process over this function: f(x) ∼ GP(µ(x), k(x, x&apos;)), (1) where µ(x) is the mean function, which is usually the 0 constant, and k(x, x&apos;) i</context>
</contexts>
<marker>Preotiuc-Pietro, Cohn, 2013</marker>
<rawString>Daniel Preotiuc-Pietro and Trevor Cohn. 2013. A temporal model of text periodicities using Gaussian Processes. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Edward Rasmussen</author>
<author>Christopher K I Williams</author>
</authors>
<title>Gaussian processes for machine learning, volume 1.</title>
<date>2006</date>
<publisher>MIT Press</publisher>
<location>Cambridge.</location>
<contexts>
<context position="4315" citStr="Rasmussen and Williams (2006)" startWordPosition="676" endWordPosition="679">14 Association for Computational Linguistics tasks. Examples include Robotics (Ko et al., 2007), Bioinformatics (Chu et al., 2005; Polajnar et al., 2011), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004; Riihim¨aki et al., 2013). In NLP, GPs have been used only very recently and focused on regression tasks (Cohn and Specia, 2013; Preotiuc-Pietro and Cohn, 2013). In this work, we propose to combine GPs with recent kernel developments to advance the state-of-the-art in a number of NLP tasks. 2 Gaussian Processes In this Section, we follow closely the definition of Rasmussen and Williams (2006). Consider a machine learning setting, where we have a dataset { X = (x1, y1), (x2, y2), ... , (xn, yn)} and our goal is to infer the underlying function f(x) that best explains the data. A GP model assumes a prior stochastic process over this function: f(x) ∼ GP(µ(x), k(x, x&apos;)), (1) where µ(x) is the mean function, which is usually the 0 constant, and k(x, x&apos;) is the kernel or covariance function. In this sense, they are analogous to Gaussian distributions, which are also defined in terms of a mean and a variance values, or in the case of multivariate Gaussians, a mean vector and a covariance</context>
<context position="8941" citStr="Rasmussen and Williams, 2006" startWordPosition="1476" endWordPosition="1479">of data points to make predictions. This results in important speed-ups which is one of the reasons for their success. On the other hand, canonical GPs are not sparse, making use of all data points. This results in a training complexity of O(n3) (due to the Gram matrix inversion) and O(n) for predictions. Sparse GPs tackle this problem by approximating the Gram matrix using only a subset of m inducing inputs. Without loss of generalization, consider these m inputs as the first ones in the training data and (n — m) the remaining ones. Then we can partition the Gram matrix in the following way (Rasmussen and Williams, 2006, Sec. 8.1): K= IK(n−m)m Kmm Km(n−m)1 K(n−m)(n−m) where each block corresponds to a matrix of kernel evaluations between two sets of inputs. For brevity, we will refer Km(n−m) as Kmn and its transpose as Knm. The block structure of K forms the base of the so-called Nystr¨om approximation: K˜ = KnmK−1 mmKmn. (8) which result in( the following predictive posterior: y* — Ar(kTm*˜G−1Kmny, (9) k(x*, x*) — kTm*K−1mmkm*+ 2 T1 σnkm*˜G km*), where G˜ = σ2nKmm + KmnKnm and km* is the vector of kernel evaluations between test input x* and the m inducing inputs. The resulting complexities for training and</context>
<context position="22702" citStr="Rasmussen and Williams, 2006" startWordPosition="3773" endWordPosition="3776">cements in the field of sparse GPs in the last years and we plan to employ them in our work. A key question is how to combine sparse GPs with the structured kernels we presented before. Although it is perfectly possible to select inducing points using greedy methods, it would be much more interesting to use the pseudo-inputs approach. However, it is not clear how to do that in conjunction with non-vectorial inputs, like the ones we plan to use in structured kernels, and this is a key direction that we also plan to investigate. 4.5 GP Toolkits Available toolkits for GP modelling include GPML4 (Rasmussen and Williams, 2006) and GPstuff5 (Vanhatalo et al., 2013), which are written in Matlab. Our experiments will mainly use GPy6, an open source toolkit written in Python. It implements models for regression and binary classification, including sparse approximations and many vectorial kernels. We plan to contribute to GPy by implementing the structured kernels of Section 3.1, effectively extending it to a GP framework for NLP. 5 Conclusions and Future Work In this work we showed a proposal for advancing the state-of-the-art in a number of NLP tasks by combining Gaussian Process with structured and multi-task kernels</context>
</contexts>
<marker>Rasmussen, Williams, 2006</marker>
<rawString>Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian processes for machine learning, volume 1. MIT Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaakko Riihim¨aki</author>
<author>Pasi Jyl¨anki</author>
<author>Aki Vehtari</author>
</authors>
<title>Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood.</title>
<date>2013</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>14--75</pages>
<marker>Riihim¨aki, Jyl¨anki, Vehtari, 2013</marker>
<rawString>Jaakko Riihim¨aki, Pasi Jyl¨anki, and Aki Vehtari. 2013. Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood. Journal of Machine Learning Research, 14:75–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Schwaighofer</author>
<author>Marian Grigoras</author>
<author>Volker Tresp</author>
<author>Clemens Hoffmann</author>
</authors>
<title>GPPS: A Gaussian Process Positioning System for Cellular Networks.</title>
<date>2004</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="3880" citStr="Schwaighofer et al., 2004" startWordPosition="605" endWordPosition="608">el combinations, like multi-task kernels for example. • Being a probabilistic framework, they are able to naturally encode uncertainty in the predictions, which can be propagated if the task is part of a larger system pipeline. Besides these properties, GPs have also been applied sucessfully in many Machine Learning 1 Proceedings of the ACL 2014 Student Research Workshop, pages 1–9, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics tasks. Examples include Robotics (Ko et al., 2007), Bioinformatics (Chu et al., 2005; Polajnar et al., 2011), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004; Riihim¨aki et al., 2013). In NLP, GPs have been used only very recently and focused on regression tasks (Cohn and Specia, 2013; Preotiuc-Pietro and Cohn, 2013). In this work, we propose to combine GPs with recent kernel developments to advance the state-of-the-art in a number of NLP tasks. 2 Gaussian Processes In this Section, we follow closely the definition of Rasmussen and Williams (2006). Consider a machine learning setting, where we have a dataset { X = (x1, y1), (x2, y2), ... , (xn, yn)} and our goal is to infer the underlying function f(x) that b</context>
</contexts>
<marker>Schwaighofer, Grigoras, Tresp, Hoffmann, 2004</marker>
<rawString>Anton Schwaighofer, Marian Grigoras, Volker Tresp, and Clemens Hoffmann. 2004. GPPS: A Gaussian Process Positioning System for Cellular Networks. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Seeger</author>
<author>Christopher K I Williams</author>
<author>Neil D Lawrence</author>
</authors>
<title>Fast Forward Selection to Speed Up Sparse Gaussian Process Regression.</title>
<date>2003</date>
<booktitle>In Proceedings ofAISTATS.</booktitle>
<contexts>
<context position="9669" citStr="Seeger et al. (2003)" startWordPosition="1600" endWordPosition="1603">ions between two sets of inputs. For brevity, we will refer Km(n−m) as Kmn and its transpose as Knm. The block structure of K forms the base of the so-called Nystr¨om approximation: K˜ = KnmK−1 mmKmn. (8) which result in( the following predictive posterior: y* — Ar(kTm*˜G−1Kmny, (9) k(x*, x*) — kTm*K−1mmkm*+ 2 T1 σnkm*˜G km*), where G˜ = σ2nKmm + KmnKnm and km* is the vector of kernel evaluations between test input x* and the m inducing inputs. The resulting complexities for training and prediction are O(m2n) and O(m), respectively. The remaining question is how to choose the inducing inputs. Seeger et al. (2003) use an iterative method that starts with some random data points and adds new ones based on a greedy procedure, in an active learning fashion. Snelson and Ghahramani (2006) use a different approach: it defines a fixed m a priori and use pseudo-inputs which can be optimized as regular hyperparameters. Later, Titsias (2009) also used pseudo-inputs but perform optimization using a variational method instead. Recently, Hensman et al. (2013) modified this method to allow Stochastic Variational Inference (Hoffman et al., 2013), which reduces the training complexity to O(m3). 3 Kernels The core of a</context>
</contexts>
<marker>Seeger, Williams, Lawrence, 2003</marker>
<rawString>Matthias Seeger, Christopher K. I. Williams, and Neil D. Lawrence. 2003. Fast Forward Selection to Speed Up Sparse Gaussian Process Regression. In Proceedings ofAISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel methods for pattern analysis.</title>
<date>2004</date>
<location>Cambridge.</location>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel methods for pattern analysis. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian H Sinz</author>
<author>Joaquin Qui˜nonero Candela</author>
<author>G¨okhan H Bakır</author>
<author>Carl E Rasmussen</author>
<author>Matthias O Franz</author>
</authors>
<title>Learning Depth from Stereo. Pattern Recognition,</title>
<date>2004</date>
<pages>1--8</pages>
<contexts>
<context position="3919" citStr="Sinz et al., 2004" startWordPosition="612" endWordPosition="615">mple. • Being a probabilistic framework, they are able to naturally encode uncertainty in the predictions, which can be propagated if the task is part of a larger system pipeline. Besides these properties, GPs have also been applied sucessfully in many Machine Learning 1 Proceedings of the ACL 2014 Student Research Workshop, pages 1–9, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics tasks. Examples include Robotics (Ko et al., 2007), Bioinformatics (Chu et al., 2005; Polajnar et al., 2011), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004; Riihim¨aki et al., 2013). In NLP, GPs have been used only very recently and focused on regression tasks (Cohn and Specia, 2013; Preotiuc-Pietro and Cohn, 2013). In this work, we propose to combine GPs with recent kernel developments to advance the state-of-the-art in a number of NLP tasks. 2 Gaussian Processes In this Section, we follow closely the definition of Rasmussen and Williams (2006). Consider a machine learning setting, where we have a dataset { X = (x1, y1), (x2, y2), ... , (xn, yn)} and our goal is to infer the underlying function f(x) that best explains the data. A GP model assum</context>
</contexts>
<marker>Sinz, Candela, Bakır, Rasmussen, Franz, 2004</marker>
<rawString>Fabian H. Sinz, Joaquin Qui˜nonero Candela, G¨okhan H. Bakır, Carl E. Rasmussen, and Matthias O. Franz. 2004. Learning Depth from Stereo. Pattern Recognition, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Snelson</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Sparse Gaussian Processes using Pseudo-inputs.</title>
<date>2006</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="9842" citStr="Snelson and Ghahramani (2006)" startWordPosition="1629" endWordPosition="1633">om approximation: K˜ = KnmK−1 mmKmn. (8) which result in( the following predictive posterior: y* — Ar(kTm*˜G−1Kmny, (9) k(x*, x*) — kTm*K−1mmkm*+ 2 T1 σnkm*˜G km*), where G˜ = σ2nKmm + KmnKnm and km* is the vector of kernel evaluations between test input x* and the m inducing inputs. The resulting complexities for training and prediction are O(m2n) and O(m), respectively. The remaining question is how to choose the inducing inputs. Seeger et al. (2003) use an iterative method that starts with some random data points and adds new ones based on a greedy procedure, in an active learning fashion. Snelson and Ghahramani (2006) use a different approach: it defines a fixed m a priori and use pseudo-inputs which can be optimized as regular hyperparameters. Later, Titsias (2009) also used pseudo-inputs but perform optimization using a variational method instead. Recently, Hensman et al. (2013) modified this method to allow Stochastic Variational Inference (Hoffman et al., 2013), which reduces the training complexity to O(m3). 3 Kernels The core of a GP model is the kernel function. A kernel k(x, x&apos;) is a symmetric and positive semidefinite function which returns a similarity score between two inputs in some feature spa</context>
</contexts>
<marker>Snelson, Ghahramani, 2006</marker>
<rawString>Edward Snelson and Zoubin Ghahramani. 2006. Sparse Gaussian Processes using Pseudo-inputs. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA.</booktitle>
<contexts>
<context position="17715" citStr="Snover et al., 2006" startWordPosition="2941" endWordPosition="2944"> post-editing or subjective scoring. Possibilities include combining datasets from different language pairs or different machine translation systems. Available datasets include those used in the WMT12 and WMT13 QE shared tasks (Callison-burch et al., 2012; Bojar et al., 2013) and others (Specia et al., 2009; Specia, 2011; Koponen et al., 2012). 4.2 Question Classification A Question Classifier is a module that aims to restrict the answer hypotheses generated by a Question Answering system by applying a label to the input question (Li and Roth, 2002; Li and Roth, 2Human Translation Error Rate (Snover et al., 2006). 3www.mturk.com 2005). This task can be seen as an instance of text classification, where the inputs are usually composed of only one sentence. Much of previous work in Question Classification largely used SVMs combined with structured kernels. Zhang and Lee (2003) compares String Kernels based on BOW and n-gram representations with the Subset Tree Kernel on constituent trees. Moschitti (2006) show improved results by using the Partial Tree Kernel and dependency trees instead of constituency ones. Bloehdorn and Moschitti (2007) combines a SSTK with different soft matching approaches to encode</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Atefeh Farzindar</author>
</authors>
<title>Estimating machine translation post-editing effort with HTER.</title>
<date>2010</date>
<booktitle>In Proceedings of AMTA Workshop Bringing MT to the User: MT Research and the Translation Industry.</booktitle>
<contexts>
<context position="1840" citStr="Specia and Farzindar, 2010" startWordPosition="278" endWordPosition="281">ke kernel methods ideal for problems where we do not have much prior knowledge about how the data behaves. This is a common setting in NLP, where they have been mostly applied in the form of Support Vector Machines (SVMs). Systems based on SVMs have been the state-of-the-art in classification tasks like Text Categorization (Lodhi et al., 2002), Sentiment Analysis (Johansson and Moschitti, 2013; P´erez-Rosas and Mihalcea, 2013) and Question Classification (Moschitti, 2006; Croce et al., 2011). Recently, they were also employed in regression settings like Machine Translation Quality Estimation (Specia and Farzindar, 2010; Bojar et al., 2013) and structured prediction (Chang et al., 2013). SVMs are a frequentist method: they aim to find an approximation to the exact latent function that explains the data. This is in contrast to Bayesian settings, which define a prior distribution on this function and perform inference by marginalizing over all its possible values. Although there is some discussion about which approach is better (Murphy, 2012, Sec. 6.6.4), Bayesian methods offer many useful theoretical properties. In fact, they have been used before in NLP, especially in grammar induction (Cohn et al., 2010) an</context>
</contexts>
<marker>Specia, Farzindar, 2010</marker>
<rawString>Lucia Specia and Atefeh Farzindar. 2010. Estimating machine translation post-editing effort with HTER. In Proceedings of AMTA Workshop Bringing MT to the User: MT Research and the Translation Industry.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Marco Turchi</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the sentence-level quality of machine translation systems.</title>
<date>2009</date>
<booktitle>In Proceedings of EAMT,</booktitle>
<pages>28--35</pages>
<contexts>
<context position="17403" citStr="Specia et al., 2009" startWordPosition="2888" endWordPosition="2891">delling annotator bias can be further extended for settings with dozens or even hundreds of annotators. This is a common setting in crowdsourcing platforms like Amazon’s Mechanical Turk3. Another plan is to use multi-task kernels to combine different datasets. Quality annotation is usually expensive, requiring post-editing or subjective scoring. Possibilities include combining datasets from different language pairs or different machine translation systems. Available datasets include those used in the WMT12 and WMT13 QE shared tasks (Callison-burch et al., 2012; Bojar et al., 2013) and others (Specia et al., 2009; Specia, 2011; Koponen et al., 2012). 4.2 Question Classification A Question Classifier is a module that aims to restrict the answer hypotheses generated by a Question Answering system by applying a label to the input question (Li and Roth, 2002; Li and Roth, 2Human Translation Error Rate (Snover et al., 2006). 3www.mturk.com 2005). This task can be seen as an instance of text classification, where the inputs are usually composed of only one sentence. Much of previous work in Question Classification largely used SVMs combined with structured kernels. Zhang and Lee (2003) compares String Kerne</context>
</contexts>
<marker>Specia, Cancedda, Dymetman, Turchi, Cristianini, 2009</marker>
<rawString>Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the sentence-level quality of machine translation systems. In Proceedings of EAMT, pages 28–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
</authors>
<title>Exploiting objective annotations for measuring translation post-editing effort.</title>
<date>2011</date>
<booktitle>In Proceedings of EAMT.</booktitle>
<contexts>
<context position="17417" citStr="Specia, 2011" startWordPosition="2892" endWordPosition="2893">s can be further extended for settings with dozens or even hundreds of annotators. This is a common setting in crowdsourcing platforms like Amazon’s Mechanical Turk3. Another plan is to use multi-task kernels to combine different datasets. Quality annotation is usually expensive, requiring post-editing or subjective scoring. Possibilities include combining datasets from different language pairs or different machine translation systems. Available datasets include those used in the WMT12 and WMT13 QE shared tasks (Callison-burch et al., 2012; Bojar et al., 2013) and others (Specia et al., 2009; Specia, 2011; Koponen et al., 2012). 4.2 Question Classification A Question Classifier is a module that aims to restrict the answer hypotheses generated by a Question Answering system by applying a label to the input question (Li and Roth, 2002; Li and Roth, 2Human Translation Error Rate (Snover et al., 2006). 3www.mturk.com 2005). This task can be seen as an instance of text classification, where the inputs are usually composed of only one sentence. Much of previous work in Question Classification largely used SVMs combined with structured kernels. Zhang and Lee (2003) compares String Kernels based on BO</context>
</contexts>
<marker>Specia, 2011</marker>
<rawString>Lucia Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In Proceedings of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michalis K Titsias</author>
</authors>
<title>Variational Learning of Inducing Variables in Sparse Gaussian Processes.</title>
<date>2009</date>
<booktitle>In Proceedings ofAISTATS,</booktitle>
<volume>5</volume>
<pages>567--574</pages>
<contexts>
<context position="9993" citStr="Titsias (2009)" startWordPosition="1656" endWordPosition="1657">here G˜ = σ2nKmm + KmnKnm and km* is the vector of kernel evaluations between test input x* and the m inducing inputs. The resulting complexities for training and prediction are O(m2n) and O(m), respectively. The remaining question is how to choose the inducing inputs. Seeger et al. (2003) use an iterative method that starts with some random data points and adds new ones based on a greedy procedure, in an active learning fashion. Snelson and Ghahramani (2006) use a different approach: it defines a fixed m a priori and use pseudo-inputs which can be optimized as regular hyperparameters. Later, Titsias (2009) also used pseudo-inputs but perform optimization using a variational method instead. Recently, Hensman et al. (2013) modified this method to allow Stochastic Variational Inference (Hoffman et al., 2013), which reduces the training complexity to O(m3). 3 Kernels The core of a GP model is the kernel function. A kernel k(x, x&apos;) is a symmetric and positive semidefinite function which returns a similarity score between two inputs in some feature space (ShaweTaylor and Cristianini, 2004). Probably the most used kernel in general is the Radial Basis Function (RBF) kernel, which is defined over two r</context>
</contexts>
<marker>Titsias, 2009</marker>
<rawString>Michalis K. Titsias. 2009. Variational Learning of Inducing Variables in Sparse Gaussian Processes. In Proceedings ofAISTATS, volume 5, pages 567–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs Up or Thumbs Down?: Semantic Orientation Applied to Unsupervised Classification of Reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL, number July,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="19472" citStr="Turney (2002)" startWordPosition="3235" endWordPosition="3236">), which assigns 6000 questions with both a coarse and a fine-grained label. 4.3 Multi-domain Sentiment Analysis Sentiment Analysis is defined as “the computational treatment of opinion, sentiment and subjectivity in text” (Pang and Lee, 2008). In this proposal, we focus on the specific task of polarity detection, where the goal is to label a text as having positive or negative sentiment. State-of-the-art methods for this task use SVMs as the learning algorithm and vary between the feature sets used. Polarity predictions can be heavily biased on the text domain. Consider the example showed by Turney (2002): the word “unpredictable” usually has a positive meaning in a movie review but a negative one when applied to an automotive review (in a phrase like “unpredictable steering”, for instance). One of the first methods to tackle this issue is the Structural Correspondence Learning of Blitzer et al. (2007). Their method uses pivot words shared between domains to find correspondencies in words that are not shared. A previous work that used structured kernels in Sentiment Analysis is the approach of Wu et al. (2009). Their method uses tree kernels on phrase dependency trees and outperforms bag-of-wo</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs Up or Thumbs Down?: Semantic Orientation Applied to Unsupervised Classification of Reviews. In Proceedings of ACL, number July, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jarno Vanhatalo</author>
<author>Jaakko Riihim¨aki</author>
<author>Jouni Hartikainen</author>
<author>Pasi Jyl¨anki</author>
<author>Ville Tolvanen</author>
<author>Aki Vehtari</author>
</authors>
<title>GPstuff: Bayesian Modeling with Gaussian Processes.</title>
<date>2013</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>14--1175</pages>
<marker>Vanhatalo, Riihim¨aki, Hartikainen, Jyl¨anki, Tolvanen, Vehtari, 2013</marker>
<rawString>Jarno Vanhatalo, Jaakko Riihim¨aki, Jouni Hartikainen, Pasi Jyl¨anki, Ville Tolvanen, and Aki Vehtari. 2013. GPstuff: Bayesian Modeling with Gaussian Processes. The Journal of Machine Learning Research, 14:1175–1179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>Nicol N Schraudolph</author>
<author>Risi Kondor</author>
<author>Karsten M Borgwardt</author>
</authors>
<title>Graph Kernels.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--1201</pages>
<contexts>
<context position="11403" citStr="Vishwanathan et al., 2010" startWordPosition="1884" endWordPosition="1887">ures In NLP, discrete structures like strings or trees are common in training data. To apply a vectorial kernel like the RBF, one can always extract realvalued features from these structures. However, kernels can be defined directly on these structures, potentially reducing the need for feature engineering. The string and tree kernels we define here are based on the theory of Convolution kernels of Haussler (1999), which calculate the similarity between two structures based on the number of substructures they have in common. Other approaches include random walk kernels (G¨artner et al., 2003; Vishwanathan et al., 2010) and Fisher kernels (Jaakkola et al., 2000). 3.1.1 String Kernels Consider a function φs(x) that counts the number of times a substring s appears in x. A string kernel , 3 is defined as: Ek(x, x&apos;) = wsos(x)os(x&apos;), (10) sEE∗ where ws is a non-negative weight for substring s and E* is the set of all possible strings over the symbol alphabet E. Usually in NLP, each word is considered a symbol, although some previous work also considered characters as symbols (Lodhi et al., 2002). If we restrict s to be only single words we end up having a bag-of-words (BOW) representation. Allowing longer substri</context>
</contexts>
<marker>Vishwanathan, Schraudolph, Kondor, Borgwardt, 2010</marker>
<rawString>S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi Kondor, and Karsten M. Borgwardt. 2010. Graph Kernels. Journal of Machine Learning Research, 11:1201–1242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher K I Williams</author>
<author>David Barber</author>
</authors>
<title>Bayesian Classification with Gaussian Processes.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<issue>12</issue>
<contexts>
<context position="7188" citStr="Williams and Barber, 1998" startWordPosition="1187" endWordPosition="1190">p(y* = +1|x*, X, y) = if a(f*)p(f* |x*, X, y)df*, (5) * where σ(f*) is a squashing function. Two common choices are the logistic function and the probit function. The distribution over the latent values f* is obtained by integrating out the latent function: p(f* |x*, X, y) = if p(f* |x*, X, f )p(f |X, y)df. (6) Because the likelihood is not Gaussian, the resulting posterior integral is not analytically available anymore. The most common solution to this problem is to approximate the posterior p(f|X, y) with a Gaussian q(f|X, y). Two such approximation algorithms are the Laplace approximation (Williams and Barber, 1998) and the Expectation Propagation (Minka, 2001). Another option is to use Markov Chain Monte Carlo sampling methods on the true posterior (Neal, 1998). 2.3 Hyperparameter Optimization The GP prior used in the models described above usually have a number of hyperparameters. The 1Extensions to multi-class settings are possible. 2 most important ones are the kernel ones but they can also include others like the white noise variance σ2n used in regression. A key property of GPs is their ability to easily fit these hyperparameters to the data by maximizing the marginal likelihood: p(y|X, 0) = if p(y</context>
</contexts>
<marker>Williams, Barber, 1998</marker>
<rawString>Christopher K. I. Williams and David Barber. 1998. Bayesian Classification with Gaussian Processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12):1342–1351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanbin Wu</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
<author>Lide Wu</author>
</authors>
<title>Phrase Dependency Parsing for Opinion Mining.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1533--1541</pages>
<contexts>
<context position="19987" citStr="Wu et al. (2009)" startWordPosition="3320" endWordPosition="3323">ty predictions can be heavily biased on the text domain. Consider the example showed by Turney (2002): the word “unpredictable” usually has a positive meaning in a movie review but a negative one when applied to an automotive review (in a phrase like “unpredictable steering”, for instance). One of the first methods to tackle this issue is the Structural Correspondence Learning of Blitzer et al. (2007). Their method uses pivot words shared between domains to find correspondencies in words that are not shared. A previous work that used structured kernels in Sentiment Analysis is the approach of Wu et al. (2009). Their method uses tree kernels on phrase dependency trees and outperforms bag-of-words 5 and word dependency approaches. They also show good results in cross-domain experiments. We propose to apply GPs with a combination of structured and multi-task kernels for this task. The results showed by Wu et al. (2009) suggest that tree kernels on dependency trees are a good approach but we also plan to employ string kernels on this task. This is because string kernels have demonstrated promising results for text categorization in past work. Also, considering model selection is easily dealt by GPs, w</context>
</contexts>
<marker>Wu, Zhang, Huang, Wu, 2009</marker>
<rawString>Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009. Phrase Dependency Parsing for Opinion Mining. In Proceedings of EMNLP, pages 1533–1541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>Wee Sun Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="17981" citStr="Zhang and Lee (2003)" startWordPosition="2984" endWordPosition="2987">al., 2013) and others (Specia et al., 2009; Specia, 2011; Koponen et al., 2012). 4.2 Question Classification A Question Classifier is a module that aims to restrict the answer hypotheses generated by a Question Answering system by applying a label to the input question (Li and Roth, 2002; Li and Roth, 2Human Translation Error Rate (Snover et al., 2006). 3www.mturk.com 2005). This task can be seen as an instance of text classification, where the inputs are usually composed of only one sentence. Much of previous work in Question Classification largely used SVMs combined with structured kernels. Zhang and Lee (2003) compares String Kernels based on BOW and n-gram representations with the Subset Tree Kernel on constituent trees. Moschitti (2006) show improved results by using the Partial Tree Kernel and dependency trees instead of constituency ones. Bloehdorn and Moschitti (2007) combines a SSTK with different soft matching approaches to encode lexical similarity on tree leaves. The same soft matching idea is used by Croce et al. (2011), but applied to PTKs instead and permitting soft matches between any nodes in each tree (which is sensible when using kernels on dependency trees). Our work proposes to ad</context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>Dell Zhang and Wee Sun Lee. 2003. Question classification using support vector machines. In Proceedings of SIGIR, New York, New York, USA. ACM Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>