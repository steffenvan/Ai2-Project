<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004771">
<title confidence="0.998006">
Generating Easy References:
the Case of Document Deixis
</title>
<author confidence="0.989988">
Ivandr´e Paraboni and Kees van Deemter
</author>
<affiliation confidence="0.9960435">
Information Technology Research Institute
University of Brighton
</affiliation>
<address confidence="0.66641">
United Kingdom
</address>
<email confidence="0.990829">
ivandre@terra.com.br, Kees van.Deemter@itri.brighton.ac.uk
</email>
<sectionHeader confidence="0.995489" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999428866666667">
This paper argues that algorithms for the
generation of referring expressions should
aim to make it easy for hearers and readers
to find the referent of the expressions that
are generated. To illustrate this claim, an
algorithm is described for the generation
of expressions that refer across a hierar-
chically ordered domain, and which takes
search effort into account by adding log-
ically redundant information. To support
the ideas underlying the algorithm, a psy-
cholinguistic experiment is presented that
confirms readers’ preference for the gen-
erated, logically redundant expressions
over non-redundant alternatives.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99478715">
Common sense suggests that unnecessarily complex
utterences are best avoided by speakers and writers
who want to get their message across and, broadly
speaking, this idea is confirmed by empirical re-
search (Clark 1992, Cremers 1996). The present
paper will discuss the complexity of referring ex-
pressions, and the implications that arise for Natu-
ral Language Generation (NLG) if unnecessary com-
plexity of referring expressions is to be avoided.
We focus on the complexity of search, bypassing
complexity of interpretation (i.e., determining the
meaning or logical form of the referring expression).
Search is understood here as the effort needed to
‘find’ the referent of an expression once the mean-
ing of the expression has been determined. Note that
complexity of search and interpretation are often in-
versely related. The expression (b), for example, is
longer and more difficult to interpret than (a), but
the additional material in (b) makes search easier
once interpretation is successfully completed. Both
</bodyText>
<figure confidence="0.78376575">
(a) and (b) determine their referent uniquely.
a. 68 Lincoln Street, Brighton
b. 68 Lincoln Street, in the middle of
Brighton’s Hannover area
</figure>
<bodyText confidence="0.929636272727273">
We will explore how an NLG program can generate
the kind of redundancy exemplified in (b), so as to
simplify the search for a referent.&apos; We focus on a
domain whose inhabitants are parts of a document
rather than of the ‘real’ world. (See the Conclusion
section for some remarks about other types of do-
mains.) Examples of the expressions generated are
c. picture 1
d. picture 1, in section 2
We focus on the first time an entity is mentioned,
disregarding anaphoric references (e.g., ‘it’).
</bodyText>
<sectionHeader confidence="0.6057105" genericHeader="method">
2 Generating references that are easy to
resolve
</sectionHeader>
<bodyText confidence="0.99473997260274">
Generation of Referring Expressions (GRE) is a key
task of NLG systems (e.g., Reiter and Dale 2000,
section 5.4). The task of a GRE algorithm is to find
combinations of properties that allow the generator
to refer uniquely to an entity, called the target of the
algorithm, and to express these properties in a lin-
guistic description. We will focus on the first part
of the problem, which involves determining the se-
mantic content of a description, paying no attention
&apos;Compare Jordan 2000, Jordan (forthcoming), where other
factors triggering logically redundant material in referring ex-
pressions are explored.
to linguistic realization (e.g., Malouf 2000). When
there is no risk of confusion, we will use the term
‘description’ loosely to refer to either the combina-
tion of properties or it linguistic realization.
GRE algorithms take as their input a knowledge base
that is shared between writer and reader, generating
unique descriptions of entities whenever the knowl-
edge base allows it (van Deemter 2002). These al-
gorithms are designed in such a way that generation
is made easy (i.e., quick). The implications for the
reader, for example in terms of the difficulty offind-
ing the referent, are never taken into account. Note
that some algorithms do generate descriptions that
are optimally brief (Dale 1989), while others ap-
proximate optimal brevity (Dale and Reiter 1995),
and this can be argued to make interpretation easier.
As we have seen, however, a short description can
sometimes make search difficult.
To illustrate, the Incremental Algorithm works
roughly as follows. (See Dale and Reiter 1995 for
details). Attributes represented in the shared Knowl-
edge base are ordered in a linear preference order-
ing. The Attributes in the ordering are considered
one by one, to see if any of their Values contributes
something to the description, typically by removing
‘distractors’ (i.e., objects other than the referent); if
an Attribute (e.g., COLOUR) can contribute some-
thing then a suitable Value (e.g., RED) for this At-
tribute is selected as a part of the description. This
is repeated ‘incrementally’ until the logical conjunc-
tion of all selected Attribute/Value combinations re-
sult in a unique identification of the referent. There
is no backtracking, and this is what keep the com-
plexity of the algorithm linear.
While relying on the shared Knowledge Base, GRE
algorithms such as the Incremental Algorithm fail
to account for the time or effort that it takes a
reader to discover which properties (i.e., which At-
tribute/Value combinations) are true of a given ob-
ject. Once a description has been interpreted, at least
two things can make it difficult to find its referent:
the ‘opacity’ of the properties used in the descrip-
tion, and the size and structure of the search space.
To exemplify the first factor, we may refer to some-
one as ‘the person wearing green underwear’, but the
colour of someone’s underwear is not always easy
to assess. In the Incremental Algorithm, this can
be tackled easily, by taking the opacity of an At-
tribute into account in the preference ordering (see
above) of Attributes: the algorithm could give pref-
erence to COLOUR OF ONE’S COAT over COLOUR
OF ONE’S UNDERWEAR. As a result, the former
Attribute would be considered (and possibly added
to the description) before the latter. As a conse-
quence, COLOUR OF ONE’S UNDERWEAR would
only be considered if the referent cannot be iden-
tified uniquely without using a combination of more
preferred Attributes, including COLOUR OF ONE’S
COAT.
In what follows, we will focus on the second factor:
the size and structure of the search space.
</bodyText>
<sectionHeader confidence="0.983422" genericHeader="method">
3 Generating document-deictic
expressions
</sectionHeader>
<bodyText confidence="0.95440698630137">
Document-deictic references (DDXs, for short) are
expressions in a document which refer to a part of
the same document. The term document deixis par-
allels Webber (1991)’s discourse deixis : ‘deixis’ be-
cause the referent of a DDX depends on its spatial
co-ordinates (Lyons 1977); ‘document’ because its
referent lives in a document rather than a discourse
(Paraboni 2000). Examples of DDXs are the noun
phrases (c) and (d) of section 1. Such expressions
point the reader’s direction to a part of the document
that they are not presently reading. Sometimes they
also serve as an indirect way of characterizing an
entity in the world, as in ‘The objects mentioned in
section 2.3’. DDXs take the document itself as their
domain, which is hierarchically ordered (in sections,
subsections, etc.), allowing us to picture it as a tree.
Documents and the hierarchical relations that hold
them together can be modelled in different ways.
For present purposes, we stick with a simple At-
tribute/Value model.2 One Attribute, TYPE, says
ZAt first sight, a Dale and Haddock-style modeling based
on relational properties looks promising for the modeling of a
hierarchy, but appearances are deceptive. Consider, for exam-
ple, a description like ‘the vase on the table’, which can be said
when there are many tables, as long as only one of them sup-
ports a vase (Dale and Haddock 1991). By comparison, if a
document contains only two pictures, one of which occurs in
the only Appendix, while the other occurs in one of the many
what kind of entity it is (picture, table, section, sub-
section, part, etc.). Given its TYPE, the identity of
the referent within its parent node can usually be de-
termined by means of one other Attribute. For ex-
ample, a picture might be identified by the property
TYPE: PICTURE and the property PICTURENUm-
BER: 3, the combination of which may be realized
as ‘picture (iii)’. Often, of course, this will not be
enough to identify the referent within the document
as a whole, necessitating addition information, as in
‘picture (iii) in/of section 2’. Such additions will
play a crucial role in what follows. Note that we
will focus on ‘hierarchical’ properties of Document
Entities. For simplicity, (and also because they can-
not always be predicted by the author) page numbers
are not taken into account here.3
The search for the referent Ref of a document-
deictic reference is, of course, sensitive the loca-
tion of , but sometimes the words in cause the
‘starting point’ for the search to be somewhere else,
for example when contains a phrase of the form ‘in
section ...’. More precisely, let be of the form ‘
in/of ’, then can either be empty, in which case
equals , or can itself be a DDX. We will as-
sume that provides the starting point for search,
unless is empty, in which case is ’s parent. We
assume that search is structured as follows:
Note that this leaves the details of the search process
unspecified. In particular, Ancestral Search leaves
it open how the search within a given subtree (i.e.,
the subtree dominated by or , etc.) is carried out.
Yet, it has various nontrivial implications. In partic-
ular, it predicts that more elaborate descriptions do
sometimes, but not always, simplify search.
sections; then ‘the picture in the section’ is not an acceptable
description, whereas ‘the picture in the Appendix’ is.
3Page numbers offer a secondary ordering of the document,
alongside its hierarchical structure; if they are taken into ac-
count then the generation algorithm has to be adapted. (Cf.,
section 5 on nonhierarchical domains.)
Example: Suppose contains only one document
entity whose TYPE is PICTURE and whose PIC-
TURENUmBER is . Sections are identified (within
their parent node) by the Attribute SECTIONNUm-
BER, parts by their PARTID.
</bodyText>
<equation confidence="0.9615674">
D
section1 section2
/ \ /  |\
partA partB partA partB partC
 ||
</equation>
<bodyText confidence="0.995336705882353">
d picture 1
Suppose the referring expression occurs in part A
of section 1. Then some of the ways in which we
could uniquely identify the referent are:
. picture 1 in part C of section 2
. picture 1 in section 2
. picture 1 in part C
. picture 1
Each of these would require a certain amount of
search effort. As the reader may verify, Ancestral
Search predicts that the referring expression re-
quires less search than the shorter expressions
because section 2 includes part C. The referring
expression fares worst of all: It is a perfectly
unique description of its referent, and it does so in
the briefest possible way, but Ancestral Search pre-
dicts that it is difficult to interpret: the starting point
is part A of section 1, where Ref is not found; at
the next step, (the rest of) section 1 is searched and,
once again, Ref is not found. Next, all (the rest
of) is searched and finally Ref is found. By
contrast, adapting the example, if the picture and
both lived in part A of section 1 then the description
‘picture 1’ would require the same amount of search
as the description ‘picture 1 in part A ’.
Going back to the original example, note that a Ref-
erence like ‘part C’ (in ) is qualitatively worse
than ‘part C of section 2’ in , since it not only
forces a reader to search through an unnecessarily
large part of the document (i.e., the whole document
instead of only section 2): by forcing the reader to
‘travel’ beyond a point (i.e., beyond section 1) where
the referent might have been found, they can cause
confusion.4 Even if the reader eventually comes
</bodyText>
<footnote confidence="0.892461">
4The referent ‘might have been found’ in section 1 because
the Attribute PARTID is defined there, as is witnessed by the
</footnote>
<bodyText confidence="0.974806375">
Ancestral Search: First, search for Ref✟ in the
subtree dominated by the starting point . If Ref✟
is not found there then search for Ref✟ in the sub-
tree dominated by the parent of , which is called
. If Ref✟ is not found there then move up to the
parent of ,.., etc., until the root is reached. If,
at this point, Ref✟ is still not found, search fails.
across part C, she has no simple way of knowing
whether that document part is the intended refer-
ent (for perhaps part C of section 1 has been over-
looked). In cases like this, we speak of a lack of
orientation (LO). Evidence confirming that writers
tend to avoid LO is discussed in section 4.
An even more problematic situation occurs when a
DDX contains a ‘dead end’ (DE). Suppose the docu-
ment contains only two pictures, located in parts B
and C of section 2, respectively. Then for (in part
A of section 1) to refer to the one in part B as ‘the
picture in part B’ would be logically sufficient; nev-
ertheless, this description would be a recipe for dis-
aster: Ancestral Search predicts that the reader will
reach a point where she searches all of section 1, in
which she finds a section whose PartId is B, exactly
as required – and yet it is the wrong section, since
the intended referent lives in part B ofsection 2.5
The following generation algorithm reduces the
amount of search performed by an interpreter ifAn-
cestral Search holds. The document part is the
referent. is the list of properties delivered by the
algorithm, to be turned into an English description
by a language realization program.
two document parts and in that section.
</bodyText>
<footnote confidence="0.968100666666667">
5One can even question the correctness of ‘the picture in part
B’, under these circumstances. (Cf., Dale and Haddock 1991;
see also footnote 1).
</footnote>
<bodyText confidence="0.991642333333333">
‘Includes’ is taken to be a reflexive relation: ‘ in-
cludes ’ iff either is an ancestor of or
. Applied to our earlier example, in the refer-
ence to part C Full Inclusion first builds up the
list Type=Part, PartId=C , then expands it
to Type=Part, PartId=C, SectionNumber=2 .
Now section 2 has been identified within , and
since Parent , we reach STOP. can be real-
ized as ‘part of section 2’.
</bodyText>
<sectionHeader confidence="0.998828" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999886794117647">
We wanted to find out whether referring expressions
that reduce the complexity of search are actually
preferred. Dead ends (DE) and Lack of orientation
(LO) can only occur in fairly complex documents.
Instead of trying to find a large number of such docu-
ments, we opted for an experimental aproach. More
specifically, we asked subjects to compare different
document-deictic references and to choose the one
they found more suitable for each situation in a given
document. One group of DDXs was produced by the
Full Inclusion (FI); the other group of DDXs con-
tained no more information than what is minimally
required to make the DDX uniquely distinguishing;
the latter category included examples of DE and LO.
The experiment made use of a specially designed
schematic document (see Appendix). The document
contains a number of incomplete statements, to be
completed by the subject. The following is an (out of
context) example of such an incomplete statement:
The green star is shown in
( ) part C of section 2
( ) part C
For each statement of this type, two alternatives are
offered, one ‘minimally distinguishing’ description
and the other corresponding to the output of the FI
algorithm. Both alternatives are unambiguous ref-
erences to the same object, the only difference be-
ing that the minimally distinguishing alternative in-
volves a situation of DE or LO, which is prevented in
the FI alternative. Our research hypotheses were that
in situations of DE, and also in situations of LO, the
expression produced by the FI algorithm would be
chosen more often than the minimally distinguish-
ing alternative.
</bodyText>
<figure confidence="0.804337333333333">
Full Inclusion :
Initialize as the empty list
Identify
The function ‘Identify’ is defined recursively:
Identify :
Type(r) Include ’s Type in
, where identifies uniquely
within Parent
IF Parent
OR Parent includes Parent
THEN STOP
ELSE Identify(Parent )
</figure>
<bodyText confidence="0.99984993220339">
Design of experiment. The experiment made use
of 15 subjects, all of them with practice in document
authoring. The subjects were instructed to put them-
selves in the shoes of the authors and to complete the
referring expressions taking the (document-deictic)
context into account. The document itself was pre-
sented in a printed version (3 pages-long, with no
page numbers). Each subject was asked ques-
tions, four of which were potential LOs (depend-
ing on the expression chosen), four were potential
DEs, four were cases in which DE and LO do not
occur, and four were control questions (to break the
monotony of the task and to disguise the purpose of
the experiment).
Half of the questions, in each of the 4 groups, in-
volved backward references (i.e., to document parts
preceeding ), the other half involved forward ref-
erences. Within the LO and control groups, two of
the four questions involved references to pictures,
while the other two involved references to sections;
all the questions involving DEs involved references
to pictures. (Using DE references to sections would
have led to highly artificial structures.) The DDX
and the referent were always located on different
pages of the document; had the referring expression
and the referent occured on the same page of
a document then physical proximity might have ob-
scured navigational issues, leading to a bias towards
the shortest alternative. When and occur in docu-
ment parts with similar layout properties (e.g., when
the positions of both and in the document happen
to be subsections labelled as “C” in different sec-
tions of the document) there could be a bias towards
the most complete (i.e., the longest) description; to
avoid such a cheap victory, and were always put
in document parts whose layout properties are differ-
ent from each other. The presentational order of al-
ternatives (i.e., minimally distinguishing versus FI)
was evenly distributed.
Results. Situations of DE (100%) were always
avoided. In situations involving LO, the FI version
was chosen in 93% of cases, which is highly sig-
nificant (p = .002723). In both cases, the research
hypothesis was confirmed. In the cases not involv-
ing DE or LO, there was no significant preference
for or against logical redundancy.
Discussion. The experiment offers support for the
idea that references should include logically redun-
dant information to simplify search in those cases
where this makes the task of finding the referent
(for example, as specified by Ancestral Search) eas-
ier. Asking subjects to compare the adequacy of
candidate referring expressions, however, is an ad-
mittedly indirect way of gauging their reading be-
haviour, which might say as much about their writ-
ing behaviour as about their reading behaviour. An
interesting alternative would be to measure reading
times and/or text comprehension, with ‘type of re-
ferring expression’ as the independent variable.
</bodyText>
<sectionHeader confidence="0.9993" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985931034483">
This paper has discussed generation strategies that
reduce the search for a referent by adding logically
redundant properties. The properties that are added
contain information about the place of the refer-
ent within a hierarchically structured domain. Al-
gorithms for generating logically redundant refer-
ences along the lines described here have been im-
plemented in PROLOG. The descriptions generated
do not always minimize search: to do this, one would
have to revise Full Inclusion to ensure that the DDX
identifies the parent of the referent, even if it in-
cludes the Parent of the DDX. This would sometimes
lead to lengthy descriptions which would be diffi-
cult to interpret while requiring only slightly less
search.6
The cost of search is not determined by the ‘logi-
cal’ structure of the domain alone. A pilot corpus
study on twelve books by different authors and pub-
lishers and covering different fields of knowledge
suggests that short descriptions, without indication
of the place of the referent within the hierarchical
structure, are preferred when search is unproblem-
atic (i.e., when LO and DE do not play a role).
Suppose, for example, that pictures are enumerated
throughout the document. Then the description ‘pic-
ture 43’ makes its referent easy to find even though
the description does not wear locational information
on its sleeves (Paraboni 2000). Note that, in this
paper, issues like the average physical distance be-
</bodyText>
<footnote confidence="0.908207">
6An example arises in our example tree if the DDX lives
in part of section 2. In this case, a reference that minimizes
search would say ‘picture 1 in part C of section 2.
</footnote>
<bodyText confidence="0.999947810810811">
tween referring expression and referent have been
left out of consideration, but they are likely to play a
role. In the previous example, for instance, if there
are pictures on virtually every page then this makes
it easier to find the picture.
Other domains. Although we have focused on the
generation of DDXs, we expect that the main ideas
carry over to references in other domains and set-
tings. A number of cases may be distinguished.
Firstly (1), if a domain is hierarchically organized
and references are deictic, for example, (i.e., they
involve something analogous to our point , where
the DDX originates) then it is so similar to the one
studied here that we expect referring expressions to
behave in exactly the same way. An example that
comes to mind is that of a speaker who explains
the location of a house somewhere else in the same
town. Suppose, for example, that only one street
in Brighton has numbers above 2000 then ‘2568
Lincoln Street’ is much better than ‘the house with
street number 2568’. (Cf. examples (a) and (b) in
section 1.) Secondly (2), if the domain is hierarchi-
cal while references are not deictic then the starting
point for Ancestral Search (see section 3) defaults
to the root of the document (unless, of course, the
DDX indicates another starting point). But even if,
thirdly (3), the domain is not hierarchical, the goals
of making references easy does apply, and domain
structure can sometimes be exploited to achieve it.
Imagine a long line of people, for example, one of
which has on a grey coat. Then ‘the man in the grey
coat’ is a unique description, but ‘the man in the
grey coat, about 30 meters from here’ could make
it far easier to find the referent. In future work we
intend to address search in nonhierarchical domains,
and the question how this affects the generation of
referring expressions.
</bodyText>
<sectionHeader confidence="0.999614" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998855666666667">
This work is part of an ongoing PhD project of the
first author and it has been supported by the CNPq,
the Brazilian Research Council.
</bodyText>
<sectionHeader confidence="0.999079" genericHeader="references">
7 References
</sectionHeader>
<reference confidence="0.998438956521739">
Clark, Herbert. 1992. Arenas of Language Use.
CSLI Publications, Stanford, Ca.
Cremers, Anita. 1996. Reference to Objects; an
empirically based study of task-oriented dialogues.
Ph.D. thesis, University of Eindhoven.
Dale, Robert. 1989. Cooking up referring expres-
sions. In Procs. of 27th Annual meeting of the Asso-
ciation for Computational Linguistics (ACL-1989),
p.68-75.
Dale, Robert and Nicholas Haddock. 1991. Gen-
erating Referring Expressions involving Relations.
Procs. of EACL, Berlin, pages 161-166.
Dale, Robert and Ehud Reiter. 1995. Computational
Interpretations of the Gricean Maxims in the Gen-
eration of Referring Expressions. Cognitive Science
18: 233-263.
Lyons, John. 1977. Semantics. Cambridge Univer-
sity Press.
Jordan, Pamela W. 2000. Can Nominal Expres-
sions Achieve Multiple Goals?: An Empirical Study.
Procs. of ACL-2000, Hong Kong.
Jordan, Pamela W. (forthcoming). Contextual Influ-
ences on Attribute Selection for Repeated Descrip-
tions. To appear in Van Deemter and Kibble (Eds.)
Information Sharing: Reference and Presupposition
in Language Generation and Interpretation. CSLI
Publications, Stanford, Autumn 2002.
Malouf, Rob. 2000. The order of prenominal ad-
jectives in natural language generation. In procs. of
ACL-2000.
Paraboni, Ivandr´e. 2000. An algorithm for generat-
ing document-deictic references. Procs. of work-
shop Coherence in Generated Multimedia, associ-
ated with First Int. Conf. on Natural Language Gen-
eration (INLG-2000), Mitzpe Ramon. pp.27-31.
Reiter, Ehud and Robert Dale. 2000. Building Nat-
ural language Generation Systems. Cambridge Uni-
versity Press, Cambridge, UK.
van Deemter, Kees. 2002. Generating Referring
Expressions: Boolean Extensions of the Incremen-
tal Algorithm” Computational Linguistics 28 (1),
pp.37-52. March 2002.
Webber, Bonnie. 1991. Structure and Ostension
in the Interpretation of Discourse deixis. Language
and Cognitive Processes 6(2) May 1991, pp. 107-
135.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.272491">
<title confidence="0.9990945">Generating Easy the Case of Document Deixis</title>
<author confidence="0.999078">Paraboni van</author>
<affiliation confidence="0.881528">Information Technology Research University of United</affiliation>
<email confidence="0.358201">ivandre@terra.com.br,Keesvan.Deemter@itri.brighton.ac.uk</email>
<abstract confidence="0.9996184375">This paper argues that algorithms for the generation of referring expressions should aim to make it easy for hearers and readers the referent the expressions that are generated. To illustrate this claim, an algorithm is described for the generation of expressions that refer across a hierarchically ordered domain, and which takes search effort into account by adding logically redundant information. To support the ideas underlying the algorithm, a psycholinguistic experiment is presented that confirms readers’ preference for the generated, logically redundant expressions over non-redundant alternatives.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Herbert Clark</author>
</authors>
<title>Arenas of Language Use.</title>
<date>1992</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, Ca.</location>
<contexts>
<context position="1095" citStr="Clark 1992" startWordPosition="153" endWordPosition="154">or the generation of expressions that refer across a hierarchically ordered domain, and which takes search effort into account by adding logically redundant information. To support the ideas underlying the algorithm, a psycholinguistic experiment is presented that confirms readers’ preference for the generated, logically redundant expressions over non-redundant alternatives. 1 Introduction Common sense suggests that unnecessarily complex utterences are best avoided by speakers and writers who want to get their message across and, broadly speaking, this idea is confirmed by empirical research (Clark 1992, Cremers 1996). The present paper will discuss the complexity of referring expressions, and the implications that arise for Natural Language Generation (NLG) if unnecessary complexity of referring expressions is to be avoided. We focus on the complexity of search, bypassing complexity of interpretation (i.e., determining the meaning or logical form of the referring expression). Search is understood here as the effort needed to ‘find’ the referent of an expression once the meaning of the expression has been determined. Note that complexity of search and interpretation are often inversely relat</context>
</contexts>
<marker>Clark, 1992</marker>
<rawString>Clark, Herbert. 1992. Arenas of Language Use. CSLI Publications, Stanford, Ca.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anita Cremers</author>
</authors>
<title>Reference to Objects; an empirically based study of task-oriented dialogues.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Eindhoven.</institution>
<contexts>
<context position="1110" citStr="Cremers 1996" startWordPosition="155" endWordPosition="156">ation of expressions that refer across a hierarchically ordered domain, and which takes search effort into account by adding logically redundant information. To support the ideas underlying the algorithm, a psycholinguistic experiment is presented that confirms readers’ preference for the generated, logically redundant expressions over non-redundant alternatives. 1 Introduction Common sense suggests that unnecessarily complex utterences are best avoided by speakers and writers who want to get their message across and, broadly speaking, this idea is confirmed by empirical research (Clark 1992, Cremers 1996). The present paper will discuss the complexity of referring expressions, and the implications that arise for Natural Language Generation (NLG) if unnecessary complexity of referring expressions is to be avoided. We focus on the complexity of search, bypassing complexity of interpretation (i.e., determining the meaning or logical form of the referring expression). Search is understood here as the effort needed to ‘find’ the referent of an expression once the meaning of the expression has been determined. Note that complexity of search and interpretation are often inversely related. The express</context>
</contexts>
<marker>Cremers, 1996</marker>
<rawString>Cremers, Anita. 1996. Reference to Objects; an empirically based study of task-oriented dialogues. Ph.D. thesis, University of Eindhoven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Cooking up referring expressions.</title>
<date>1989</date>
<booktitle>In Procs. of 27th Annual meeting of the Association for Computational Linguistics (ACL-1989),</booktitle>
<pages>68--75</pages>
<contexts>
<context position="3884" citStr="Dale 1989" startWordPosition="605" endWordPosition="606">use the term ‘description’ loosely to refer to either the combination of properties or it linguistic realization. GRE algorithms take as their input a knowledge base that is shared between writer and reader, generating unique descriptions of entities whenever the knowledge base allows it (van Deemter 2002). These algorithms are designed in such a way that generation is made easy (i.e., quick). The implications for the reader, for example in terms of the difficulty offinding the referent, are never taken into account. Note that some algorithms do generate descriptions that are optimally brief (Dale 1989), while others approximate optimal brevity (Dale and Reiter 1995), and this can be argued to make interpretation easier. As we have seen, however, a short description can sometimes make search difficult. To illustrate, the Incremental Algorithm works roughly as follows. (See Dale and Reiter 1995 for details). Attributes represented in the shared Knowledge base are ordered in a linear preference ordering. The Attributes in the ordering are considered one by one, to see if any of their Values contributes something to the description, typically by removing ‘distractors’ (i.e., objects other than </context>
</contexts>
<marker>Dale, 1989</marker>
<rawString>Dale, Robert. 1989. Cooking up referring expressions. In Procs. of 27th Annual meeting of the Association for Computational Linguistics (ACL-1989), p.68-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Nicholas Haddock</author>
</authors>
<title>Generating Referring Expressions involving Relations. Procs. of EACL,</title>
<date>1991</date>
<pages>161--166</pages>
<location>Berlin,</location>
<contexts>
<context position="7603" citStr="Dale and Haddock 1991" startWordPosition="1217" endWordPosition="1220">cally ordered (in sections, subsections, etc.), allowing us to picture it as a tree. Documents and the hierarchical relations that hold them together can be modelled in different ways. For present purposes, we stick with a simple Attribute/Value model.2 One Attribute, TYPE, says ZAt first sight, a Dale and Haddock-style modeling based on relational properties looks promising for the modeling of a hierarchy, but appearances are deceptive. Consider, for example, a description like ‘the vase on the table’, which can be said when there are many tables, as long as only one of them supports a vase (Dale and Haddock 1991). By comparison, if a document contains only two pictures, one of which occurs in the only Appendix, while the other occurs in one of the many what kind of entity it is (picture, table, section, subsection, part, etc.). Given its TYPE, the identity of the referent within its parent node can usually be determined by means of one other Attribute. For example, a picture might be identified by the property TYPE: PICTURE and the property PICTURENUmBER: 3, the combination of which may be realized as ‘picture (iii)’. Often, of course, this will not be enough to identify the referent within the docume</context>
<context position="13469" citStr="Dale and Haddock 1991" startWordPosition="2262" endWordPosition="2265">l of section 1, in which she finds a section whose PartId is B, exactly as required – and yet it is the wrong section, since the intended referent lives in part B ofsection 2.5 The following generation algorithm reduces the amount of search performed by an interpreter ifAncestral Search holds. The document part is the referent. is the list of properties delivered by the algorithm, to be turned into an English description by a language realization program. two document parts and in that section. 5One can even question the correctness of ‘the picture in part B’, under these circumstances. (Cf., Dale and Haddock 1991; see also footnote 1). ‘Includes’ is taken to be a reflexive relation: ‘ includes ’ iff either is an ancestor of or . Applied to our earlier example, in the reference to part C Full Inclusion first builds up the list Type=Part, PartId=C , then expands it to Type=Part, PartId=C, SectionNumber=2 . Now section 2 has been identified within , and since Parent , we reach STOP. can be realized as ‘part of section 2’. 4 Experiment We wanted to find out whether referring expressions that reduce the complexity of search are actually preferred. Dead ends (DE) and Lack of orientation (LO) can only occur </context>
</contexts>
<marker>Dale, Haddock, 1991</marker>
<rawString>Dale, Robert and Nicholas Haddock. 1991. Generating Referring Expressions involving Relations. Procs. of EACL, Berlin, pages 161-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<date>1995</date>
<journal>Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions. Cognitive Science</journal>
<volume>18</volume>
<pages>233--263</pages>
<contexts>
<context position="3949" citStr="Dale and Reiter 1995" startWordPosition="613" endWordPosition="616">the combination of properties or it linguistic realization. GRE algorithms take as their input a knowledge base that is shared between writer and reader, generating unique descriptions of entities whenever the knowledge base allows it (van Deemter 2002). These algorithms are designed in such a way that generation is made easy (i.e., quick). The implications for the reader, for example in terms of the difficulty offinding the referent, are never taken into account. Note that some algorithms do generate descriptions that are optimally brief (Dale 1989), while others approximate optimal brevity (Dale and Reiter 1995), and this can be argued to make interpretation easier. As we have seen, however, a short description can sometimes make search difficult. To illustrate, the Incremental Algorithm works roughly as follows. (See Dale and Reiter 1995 for details). Attributes represented in the shared Knowledge base are ordered in a linear preference ordering. The Attributes in the ordering are considered one by one, to see if any of their Values contributes something to the description, typically by removing ‘distractors’ (i.e., objects other than the referent); if an Attribute (e.g., COLOUR) can contribute some</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Dale, Robert and Ehud Reiter. 1995. Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions. Cognitive Science 18: 233-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lyons</author>
</authors>
<title>Semantics.</title>
<date>1977</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6515" citStr="Lyons 1977" startWordPosition="1037" endWordPosition="1038">quence, COLOUR OF ONE’S UNDERWEAR would only be considered if the referent cannot be identified uniquely without using a combination of more preferred Attributes, including COLOUR OF ONE’S COAT. In what follows, we will focus on the second factor: the size and structure of the search space. 3 Generating document-deictic expressions Document-deictic references (DDXs, for short) are expressions in a document which refer to a part of the same document. The term document deixis parallels Webber (1991)’s discourse deixis : ‘deixis’ because the referent of a DDX depends on its spatial co-ordinates (Lyons 1977); ‘document’ because its referent lives in a document rather than a discourse (Paraboni 2000). Examples of DDXs are the noun phrases (c) and (d) of section 1. Such expressions point the reader’s direction to a part of the document that they are not presently reading. Sometimes they also serve as an indirect way of characterizing an entity in the world, as in ‘The objects mentioned in section 2.3’. DDXs take the document itself as their domain, which is hierarchically ordered (in sections, subsections, etc.), allowing us to picture it as a tree. Documents and the hierarchical relations that hol</context>
</contexts>
<marker>Lyons, 1977</marker>
<rawString>Lyons, John. 1977. Semantics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela W Jordan</author>
</authors>
<title>Can Nominal Expressions Achieve Multiple Goals?: An Empirical Study. Procs. of ACL-2000,</title>
<date>2000</date>
<location>Hong Kong.</location>
<contexts>
<context position="3060" citStr="Jordan 2000" startWordPosition="476" endWordPosition="477">e an entity is mentioned, disregarding anaphoric references (e.g., ‘it’). 2 Generating references that are easy to resolve Generation of Referring Expressions (GRE) is a key task of NLG systems (e.g., Reiter and Dale 2000, section 5.4). The task of a GRE algorithm is to find combinations of properties that allow the generator to refer uniquely to an entity, called the target of the algorithm, and to express these properties in a linguistic description. We will focus on the first part of the problem, which involves determining the semantic content of a description, paying no attention &apos;Compare Jordan 2000, Jordan (forthcoming), where other factors triggering logically redundant material in referring expressions are explored. to linguistic realization (e.g., Malouf 2000). When there is no risk of confusion, we will use the term ‘description’ loosely to refer to either the combination of properties or it linguistic realization. GRE algorithms take as their input a knowledge base that is shared between writer and reader, generating unique descriptions of entities whenever the knowledge base allows it (van Deemter 2002). These algorithms are designed in such a way that generation is made easy (i.e</context>
</contexts>
<marker>Jordan, 2000</marker>
<rawString>Jordan, Pamela W. 2000. Can Nominal Expressions Achieve Multiple Goals?: An Empirical Study. Procs. of ACL-2000, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela W Jordan</author>
</authors>
<title>Contextual Influences on Attribute Selection for Repeated Descriptions. To appear in Van Deemter and Kibble (Eds.) Information Sharing: Reference and Presupposition in Language Generation and Interpretation.</title>
<date>2002</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, Autumn</location>
<marker>Jordan, 2002</marker>
<rawString>Jordan, Pamela W. (forthcoming). Contextual Influences on Attribute Selection for Repeated Descriptions. To appear in Van Deemter and Kibble (Eds.) Information Sharing: Reference and Presupposition in Language Generation and Interpretation. CSLI Publications, Stanford, Autumn 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Malouf</author>
</authors>
<title>The order of prenominal adjectives in natural language generation. In procs. of ACL-2000.</title>
<date>2000</date>
<contexts>
<context position="3228" citStr="Malouf 2000" startWordPosition="497" endWordPosition="498"> a key task of NLG systems (e.g., Reiter and Dale 2000, section 5.4). The task of a GRE algorithm is to find combinations of properties that allow the generator to refer uniquely to an entity, called the target of the algorithm, and to express these properties in a linguistic description. We will focus on the first part of the problem, which involves determining the semantic content of a description, paying no attention &apos;Compare Jordan 2000, Jordan (forthcoming), where other factors triggering logically redundant material in referring expressions are explored. to linguistic realization (e.g., Malouf 2000). When there is no risk of confusion, we will use the term ‘description’ loosely to refer to either the combination of properties or it linguistic realization. GRE algorithms take as their input a knowledge base that is shared between writer and reader, generating unique descriptions of entities whenever the knowledge base allows it (van Deemter 2002). These algorithms are designed in such a way that generation is made easy (i.e., quick). The implications for the reader, for example in terms of the difficulty offinding the referent, are never taken into account. Note that some algorithms do ge</context>
</contexts>
<marker>Malouf, 2000</marker>
<rawString>Malouf, Rob. 2000. The order of prenominal adjectives in natural language generation. In procs. of ACL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivandr´e Paraboni</author>
</authors>
<title>An algorithm for generating document-deictic references.</title>
<date>2000</date>
<booktitle>Procs. of workshop Coherence in Generated Multimedia, associated with First Int. Conf. on Natural Language Generation (INLG-2000), Mitzpe Ramon.</booktitle>
<pages>27--31</pages>
<contexts>
<context position="6608" citStr="Paraboni 2000" startWordPosition="1051" endWordPosition="1052">ified uniquely without using a combination of more preferred Attributes, including COLOUR OF ONE’S COAT. In what follows, we will focus on the second factor: the size and structure of the search space. 3 Generating document-deictic expressions Document-deictic references (DDXs, for short) are expressions in a document which refer to a part of the same document. The term document deixis parallels Webber (1991)’s discourse deixis : ‘deixis’ because the referent of a DDX depends on its spatial co-ordinates (Lyons 1977); ‘document’ because its referent lives in a document rather than a discourse (Paraboni 2000). Examples of DDXs are the noun phrases (c) and (d) of section 1. Such expressions point the reader’s direction to a part of the document that they are not presently reading. Sometimes they also serve as an indirect way of characterizing an entity in the world, as in ‘The objects mentioned in section 2.3’. DDXs take the document itself as their domain, which is hierarchically ordered (in sections, subsections, etc.), allowing us to picture it as a tree. Documents and the hierarchical relations that hold them together can be modelled in different ways. For present purposes, we stick with a simp</context>
<context position="20024" citStr="Paraboni 2000" startWordPosition="3340" endWordPosition="3341">etermined by the ‘logical’ structure of the domain alone. A pilot corpus study on twelve books by different authors and publishers and covering different fields of knowledge suggests that short descriptions, without indication of the place of the referent within the hierarchical structure, are preferred when search is unproblematic (i.e., when LO and DE do not play a role). Suppose, for example, that pictures are enumerated throughout the document. Then the description ‘picture 43’ makes its referent easy to find even though the description does not wear locational information on its sleeves (Paraboni 2000). Note that, in this paper, issues like the average physical distance be6An example arises in our example tree if the DDX lives in part of section 2. In this case, a reference that minimizes search would say ‘picture 1 in part C of section 2. tween referring expression and referent have been left out of consideration, but they are likely to play a role. In the previous example, for instance, if there are pictures on virtually every page then this makes it easier to find the picture. Other domains. Although we have focused on the generation of DDXs, we expect that the main ideas carry over to r</context>
</contexts>
<marker>Paraboni, 2000</marker>
<rawString>Paraboni, Ivandr´e. 2000. An algorithm for generating document-deictic references. Procs. of workshop Coherence in Generated Multimedia, associated with First Int. Conf. on Natural Language Generation (INLG-2000), Mitzpe Ramon. pp.27-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="2670" citStr="Reiter and Dale 2000" startWordPosition="408" endWordPosition="411">ow an NLG program can generate the kind of redundancy exemplified in (b), so as to simplify the search for a referent.&apos; We focus on a domain whose inhabitants are parts of a document rather than of the ‘real’ world. (See the Conclusion section for some remarks about other types of domains.) Examples of the expressions generated are c. picture 1 d. picture 1, in section 2 We focus on the first time an entity is mentioned, disregarding anaphoric references (e.g., ‘it’). 2 Generating references that are easy to resolve Generation of Referring Expressions (GRE) is a key task of NLG systems (e.g., Reiter and Dale 2000, section 5.4). The task of a GRE algorithm is to find combinations of properties that allow the generator to refer uniquely to an entity, called the target of the algorithm, and to express these properties in a linguistic description. We will focus on the first part of the problem, which involves determining the semantic content of a description, paying no attention &apos;Compare Jordan 2000, Jordan (forthcoming), where other factors triggering logically redundant material in referring expressions are explored. to linguistic realization (e.g., Malouf 2000). When there is no risk of confusion, we w</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Reiter, Ehud and Robert Dale. 2000. Building Natural language Generation Systems. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
</authors>
<title>Generating Referring Expressions:</title>
<date>2002</date>
<journal>Boolean Extensions of the Incremental Algorithm” Computational Linguistics</journal>
<volume>28</volume>
<issue>1</issue>
<pages>37--52</pages>
<marker>van Deemter, 2002</marker>
<rawString>van Deemter, Kees. 2002. Generating Referring Expressions: Boolean Extensions of the Incremental Algorithm” Computational Linguistics 28 (1), pp.37-52. March 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
</authors>
<title>Structure and Ostension in the Interpretation of Discourse deixis.</title>
<date>1991</date>
<journal>Language and Cognitive Processes</journal>
<volume>6</volume>
<issue>2</issue>
<pages>107--135</pages>
<contexts>
<context position="6406" citStr="Webber (1991)" startWordPosition="1019" endWordPosition="1020"> the former Attribute would be considered (and possibly added to the description) before the latter. As a consequence, COLOUR OF ONE’S UNDERWEAR would only be considered if the referent cannot be identified uniquely without using a combination of more preferred Attributes, including COLOUR OF ONE’S COAT. In what follows, we will focus on the second factor: the size and structure of the search space. 3 Generating document-deictic expressions Document-deictic references (DDXs, for short) are expressions in a document which refer to a part of the same document. The term document deixis parallels Webber (1991)’s discourse deixis : ‘deixis’ because the referent of a DDX depends on its spatial co-ordinates (Lyons 1977); ‘document’ because its referent lives in a document rather than a discourse (Paraboni 2000). Examples of DDXs are the noun phrases (c) and (d) of section 1. Such expressions point the reader’s direction to a part of the document that they are not presently reading. Sometimes they also serve as an indirect way of characterizing an entity in the world, as in ‘The objects mentioned in section 2.3’. DDXs take the document itself as their domain, which is hierarchically ordered (in section</context>
</contexts>
<marker>Webber, 1991</marker>
<rawString>Webber, Bonnie. 1991. Structure and Ostension in the Interpretation of Discourse deixis. Language and Cognitive Processes 6(2) May 1991, pp. 107-135.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>