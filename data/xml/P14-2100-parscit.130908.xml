<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025457">
<title confidence="0.9980495">
Enriching Cold Start Personalized Language Model
Using Social Network Information
</title>
<author confidence="0.997472">
Yu-Yang Huang†, Rui Yan*, Tsung-Ting Kuo‡, Shou-De Lin†‡†Graduate Institute of Computer Science and Information Engineering,
</author>
<affiliation confidence="0.9995738">
National Taiwan University, Taipei, Taiwan
‡Graduate Institute of Network and Multimedia,
National Taiwan University, Taipei, Taiwan
*Computer and Information Science Department,
University of Pennsylvania, Philadelphia, PA 19104, U.S.A.
</affiliation>
<email confidence="0.989277">
{r02922050, d97944007, sdlin}@csie.ntu.edu.tw, ruiyan@seas.upenn.edu
</email>
<sectionHeader confidence="0.998561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999851818181818">
We introduce a generalized framework to enrich
the personalized language models for cold start
users. The cold start problem is solved with
content written by friends on social network
services. Our framework consists of a mixture
language model, whose mixture weights are es-
timated with a factor graph. The factor graph is
used to incorporate prior knowledge and heuris-
tics to identify the most appropriate weights.
The intrinsic and extrinsic experiments show
significant improvement on cold start users.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969772727273">
Personalized language models (PLM) on social
network services are useful in many aspects (Xue
et al., 2009; Wen et al., 2012; Clements, 2007),
For instance, if the authorship of a document is
in doubt, a PLM may be used as a generative
model to identify it. In this sense, a PLM serves
as a proxy of one’s writing style. Furthermore,
PLMs can improve the quality of information
retrieval and content-based recommendation sys-
tems, where documents or topics can be recom-
mended based on the generative probabilities.
However, it is challenging to build a PLM for
users who just entered the system, and whose
content is thus insufficient to characterize them.
These are called “cold start” users. Producing
better recommendations is even more critical for
cold start users to make them continue to use the
system. Therefore, this paper focuses on how to
overcome the cold start problem and obtain a
better PLM for cold start users.
The content written by friends on a social
network service, such as Facebook or Twitter, is
exploited. It can be either a reply to an original
post or posts by friends. Here the hypothesis is
that friends, who usually share common interests,
tend to discuss similar topics and use similar
words than non-friends. In other words, we be-
lieve that a cold start user’s language model can
be enriched and better personalized by incorpo-
rating content written by friends.
Intuitively, a linear combination of document-
level language models can be used to incorporate
content written by friends. However, it should be
noticed that some documents are more relevant
than others, and should be weighted higher. To
obtain better weights, some simple heuristics
could be exploited. For example, we can measure
the similarity or distance between a user lan-
guage model and a document language model. In
addition, documents that are shared frequently in
a social network are usually considered to be
more influential, and could contribute more to
the language model. More complex heuristics
can also be derived. For instance, if two docu-
ments are posted by the same person, their
weights should be more similar. The main chal-
lenge lies in how such heuristics can be utilized
in a systematic manner to infer the weights of
each document-level language model.
In this paper, we exploit the information on
social network services in two ways. First, we
impose the social dependency assumption via a
finite mixture model. We model the true, albeit
unknown, personalized language model as a
combination of a biased user language model and
a set of relevant document language models. Due
to the noise inevitably contained in social media
content, instead of using all available documents,
we argue that by properly specifying the set of
relevant documents, a better personalized lan-
guage model can be learnt. In other words, each
user language model is enriched by a personal-
ized collection of background documents.
Second, we propose a factor graph model
(FGM) to incorporate prior knowledge (e.g. the
heuristics described above) into our model. Each
</bodyText>
<page confidence="0.978066">
611
</page>
<bodyText confidence="0.9327635">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 611–617,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
mixture weight is represented by a random vari-
able in the factor graph, and an efficient algo-
rithm is proposed to optimize the model and infer
the marginal distribution of these variables. Use-
ful information about these variables is encoded
by a set of potential functions.
The main contributions of this work are sum-
marized below:
</bodyText>
<listItem confidence="0.9523955">
• To solve the cold start problem encountered
when estimating PLMs, a generalized frame-
work based on FGM is proposed. We incorpo-
rate social network information into user lan-
guage models through the use of FGM. An it-
erative optimization procedure utilizing per-
plexity is presented to learn the parameters.
To our knowledge, this is the first proposal to
use FGM to enrich language models.
• Perplexity is selected as an intrinsic evalua-
</listItem>
<bodyText confidence="0.81532125">
tion, and experiment on authorship attribution
is used as an extrinsic evaluation. The results
show that our model yields significant im-
provements for cold start users.
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="introduction">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.993353">
2.1 Social-Driven Personalized Language
Model
</subsectionHeader>
<bodyText confidence="0.999984375">
The language model of a collection of documents
can be estimated by normalizing the counts of
words in the entire collection (Zhai, 2008). To
build a user language model, one naïve way is to
first normalize word frequency 𝑐(𝑤, 𝑑) within
each document, and then average over all the
documents in a user’s document collection. The
resulting unigram user language model is:
</bodyText>
<equation confidence="0.9878504">
1 ∑ 𝑐(𝑤, 𝑑)
|𝒟𝑢 |𝑑∈𝒟𝑢 |𝑑 |(1)
1
|𝒟𝑢 |∑ 𝑃𝑑(𝑤)
𝑑∈𝒟𝑢
</equation>
<bodyText confidence="0.999675052631579">
where 𝑃𝑑 (𝑤) is the language model of a particu-
lar document, and 𝒟𝑢 is the user’s document col-
lection. This formulation is basically an equal-
weighted finite mixture model.
A simple yet effective way to smooth a lan-
guage model is to linearly interpolate with a
background language model (Chen and Good-
man, 1996; Zhai and Lafferty, 2001). In the line-
ar interpolation method, all background docu-
ments are treated equally. The entire document
collection is added to the user language model
𝑃𝑢 (𝑤) with the same interpolation coefficient.
Our main idea is to specify a set of relevant
documents for the target user using information
embedded in a social network, and enrich the
smoothing procedure with these documents. Let
𝒟𝑟𝑒𝑙 denote the content from relevant persons
(e.g. social neighbors) of u1, our idea can be con-
cisely expressed as:
</bodyText>
<equation confidence="0.978253333333333">
𝑃𝑢1
′ (𝑤) = 𝜆𝑢1𝑃𝑢1(𝑤) + ∑ 𝜆𝑑𝑖𝑃𝑑𝑖(𝑤)
𝑑𝑖∈𝒟𝑟𝑒𝑙
</equation>
<bodyText confidence="0.999134235294117">
where 𝜆𝑑𝑖 is the mixture weight of the language
model of document di, and 𝜆𝑢1 + ∑ 𝜆𝑑𝑖 = 1 .
Documents posted by irrelevant users are not
included as we believe the user language model
can be better personalized by exploiting the so-
cial relationship in a more structured way. In our
experiment, we choose the first degree neighbor
documents as 𝒟𝑟𝑒𝑙.
Also note that we have made no assumption
about how the “base” user language model
𝑃𝑢1(𝑤) is built. In practice, it need not be models
following maximum likelihood estimation, but
any language model can be integrated into our
framework to achieve a better refined model.
Furthermore, any smoothing method can be ap-
plied to the language model without degrading
the effectiveness.
</bodyText>
<subsectionHeader confidence="0.995473">
2.2 Factor Graph Model (FGM)
</subsectionHeader>
<bodyText confidence="0.99777335">
Now we discuss how the mixture weights can be
estimated. We introduce a factor graph model
(FGM) to make use of the diverse information on
a social network. Factor graph (Kschischang et
al., 2006) is a bipartite graph consisting of a set
of random variables and a set of factors which
signifies the relationships among the variables. It
is best suited in situations where the data is clear-
ly of a relational nature (Wang et al., 2012). The
joint distribution of the variables is factored ac-
cording to the graph structure. Using FGM, one
can incorporate the knowledge into the potential
function for optimization and perform joint in-
ference over documents. As shown in Figure 1,
the variables included in the model are described
as follows:
Candidate variables 𝑦𝑖 = 〈𝑢, 𝑑𝑖〉 . The ran-
dom variables in the top layer stand for the de-
grees of belief that a document di should be in-
cluded in the PLM of the target user 𝑢.
</bodyText>
<figureCaption confidence="0.998133">
Figure 1: A two-layered factor graph (FGM)
proposed to estimate the mixture weights.
</figureCaption>
<figure confidence="0.740939">
𝑃𝑢(𝑤) =
(2)
</figure>
<page confidence="0.981105">
612
</page>
<bodyText confidence="0.999635416666667">
Attribute variables xi. Local information is
stored as the random variables in the bottom lay-
er. For example, x, might represent the number
of common friends between the author of a doc-
ument di and our target user.
The potential functions in the FGM are:
Attribute-to-candidate function. This poten-
tial function captures the local dependencies of a
candidate variable to the relevant attributes. Let
the candidate variable yi correspond to a docu-
ment di, the attribute-to-candidate function of yi
is defined in a log-linear form:
</bodyText>
<equation confidence="0.9976045">
𝑓 (𝑦𝑖, 𝐴) = 𝑍𝛼
1 𝑒𝑥𝑝{𝛼𝑇𝐟(𝑦𝑖, 𝐴)} (3)
</equation>
<bodyText confidence="0.999559">
where A is the set of attributes of either the doc-
ument di or target user u; f is a vector of feature
functions which locally model the value of yi
with attributes in A; 𝑍𝛼 is the local partition
function and 𝛼 is the weight vector to be learnt.
In our experiment, we define the vector of
functions as 𝐟 = 〈𝑓𝑠𝑖𝑚,𝑓𝑜𝑜𝑣,𝑓𝑝𝑜𝑝,𝑓𝑐𝑚𝑓,𝑓𝑎𝑓〉𝑇 as:
</bodyText>
<listItem confidence="0.998755125">
• Similarity function 𝑓𝑠𝑖𝑚 . The similarity be-
tween language models of the target user and
a document should play an important role. We
use cosine similarity between two unigram
models in our experiments.
• Document quality function 𝑓𝑜𝑜𝑣. The out-of-
vocabulary (OOV) ratio is used to measure the
quality of a document. It is defined as
</listItem>
<equation confidence="0.972903333333333">
|{𝑤:𝑤∈ 𝑑𝑖∩ 𝑤∉ 𝑉}|
𝑓𝑜𝑜𝑣 = 1 − (4)
|𝑑𝑖|
</equation>
<bodyText confidence="0.99778">
where 𝑉 is the vocabulary set of the entire
corpus, with stop words excluded.
</bodyText>
<listItem confidence="0.952673">
• Document popularity function 𝑓𝑝𝑜𝑝 . This
function is defined as the number of times di is
shared to model the popularity of documents.
• Common friend function 𝑓𝑐𝑚 𝑓. It is defined
as the number of common friends between the
target user u, and the author of di.
• Author friendship function 𝑓𝑎𝑓. Assuming
that documents posted by a user with more
</listItem>
<bodyText confidence="0.999722">
friends are more influential, this function is
defined as the number of friends of di’s author.
Candidate-to-candidate function. This po-
tential function defines the correlation of a can-
didate variable yi with another candidate variable
yj in the factor graph. The function is defined as
</bodyText>
<equation confidence="0.98118">
1
𝑔(𝑦𝑖,𝑦𝑗) = 𝑒𝑥𝑝{𝛽𝑇𝐠(𝑦𝑖, 𝑦𝑗)} (5)
𝑍𝑖𝑗,𝛽
</equation>
<bodyText confidence="0.9996272">
where g is a vector of feature functions indicat-
ing whether two variables are correlated. If we
further denote the set of all related variables as
𝐺 (𝑦𝑖), then for any candidate variable yi, we
have the following brief expression:
</bodyText>
<equation confidence="0.986915">
𝑔(𝑦𝑖, 𝐺(𝑦𝑖)) = ∏ 𝑔(𝑦𝑖,𝑦𝑗) (6)
𝑦𝑗 ∈𝐺(𝑦𝑖)
</equation>
<bodyText confidence="0.997782">
For two candidate variables, let the corre-
sponding document be di and dj, respectively, we
define the vector 𝐠 = 〈𝑔𝑟𝑒𝑙, 𝑔𝑐𝑎𝑡〉𝑇 as:
</bodyText>
<listItem confidence="0.811782">
• User relationship function 𝑔𝑟𝑒𝑙. We assume
that two candidate variables have higher de-
pendency if they represent documents of the
</listItem>
<bodyText confidence="0.8922466">
same author or the two authors are friends.
The dependency should be even greater if two
documents are similar. Let 𝑎(𝑑) denote the
author of a document d and 𝒩 [𝑢] denote the
closed neighborhood of a user u, we define
</bodyText>
<equation confidence="0.949276">
𝑔𝑟𝑒𝑙 = 𝕀{𝑎(𝑑𝑗) ∈ 𝒩[𝑎(𝑑𝑖)]} × 𝑠𝑖𝑚(𝑑𝑖, 𝑑𝑗) (7)
</equation>
<listItem confidence="0.8943826">
• Co-category function 𝑔𝑐𝑎𝑡. For any two can-
didate variables, it is intuitive that the two var-
iables would have a higher correlation if di
and dj are of the same category. Let 𝑐(𝑑) de-
note the category of document d, we define
</listItem>
<equation confidence="0.988857">
𝑔𝑐𝑎𝑡 = 𝕀{𝑐(𝑑𝑖) = 𝑐(𝑑𝑗)} × 𝑠𝑖𝑚(𝑑𝑖, 𝑑𝑗) (8)
</equation>
<subsectionHeader confidence="0.998949">
2.3 Model Inference and Optimization
</subsectionHeader>
<bodyText confidence="0.99995975">
Let Y and X be the set of all candidate variables
and attribute variables, respectively. The joint
distribution encoded by the FGM is given by
multiplying all potential functions.
</bodyText>
<equation confidence="0.9313865">
𝑃(𝑌,𝑋) = ∏𝑓(𝑦𝑖, 𝐴)𝑔(𝑦𝑖,𝐺(𝑦𝑖)) (9)
𝑖
</equation>
<bodyText confidence="0.9999771">
The desired marginal distribution can be ob-
tained by marginalizing all other variables. Since
under most circumstances, however, the factor
graph is densely connected, the exact inference is
intractable and approximate inference is required.
After obtaining the marginal probabilities, the
mixture weights 𝜆𝑑𝑖 in Eq. 2 are estimated by
normalizing the corresponding marginal proba-
bilities 𝑃(𝑦𝑖) over all candidate variables, which
can be written as
</bodyText>
<equation confidence="0.9929005">
𝜆𝑑𝑖 = (1 − 𝜆𝑢1) 𝑃(𝑦𝑖) (10)
∑𝑗:𝑑𝑗∈𝒟𝑟𝑒𝑙𝑃(𝑦𝑗)
</equation>
<bodyText confidence="0.999954888888889">
where the constraint 𝜆𝑢1 + ∑ 𝜆𝑑𝑖 = 1 leads to a
valid probability distribution for our mixture
model.
A factor graph is normally optimized by gra-
dient-based methods. Unfortunately, since the
ground truth values of the mixture weights are
not available, we are prohibited from using su-
pervised approaches. Here we propose a two-step
iterative procedure to optimize our model. At
</bodyText>
<page confidence="0.99666">
613
</page>
<bodyText confidence="0.999947">
first, all the model parameters (i.e. a, fl, A�) are
randomly initialized. Then, we infer the marginal
probabilities of candidate variables. Given these
marginal probabilities, we can evaluate the per-
plexity of the user language model on a held-out
dataset, and search for better parameters. This
procedure is repeated until convergence. Also,
notice that by using FGM, we reduce the number
of parameters from 1 + |D,�1  |to 1 + |a  |+ |fl |,
lowering the risk of overfitting.
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="background">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998081">
3.1 Dataset and Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999503925925926">
We perform experiments on the Twitter dataset
collected by Galuba et al. (2010). Twitter data
have been used to verify models with different
purposes (Lin et al., 2011; Tan et al., 2011). To
emphasize on the cold start scenario, we random-
ly selected 15 users with about 35 tweets and 70
friends as candidates for an authorship attribution
task. Our experiment corpus consists of 4322
tweets. All words with less than 5 occurrences
are removed. Stop words and URLs are also re-
moved and all tweets are stemmed. We identify
the 100 most frequent terms as categories. The
size of the vocabulary set is 1377.
We randomly partitioned the tweets of each
user into training, validation and testing sets. The
reported result is the average of 10 random splits.
In all experiments, we vary the size of training
data from 1% to 15%, and hold out the same
number of tweets from each user as validation
and testing data. The statistics of our dataset,
given 15% training data, are shown in Table 1.
Loopy belief propagation (LBP) is used to ob-
tain the marginal probabilities of the variables
(Murphy et al., 1999). Parameters are searched
with the pattern search algorithm (Audet and
Dennis, 2002). To not lose generality, we use the
default configuration in all experiments.
</bodyText>
<table confidence="0.9986882">
# of Max. Min. Avg.
Tweets 70 19 35.4
Friends 139 24 68.9
Variables 467 97 252.7
Edges 9216 231 3427.1
</table>
<tableCaption confidence="0.998954">
Table 1: Dataset statistics
</tableCaption>
<subsectionHeader confidence="0.896131">
3.2 Baseline Methods
</subsectionHeader>
<bodyText confidence="0.999964875">
We compare our framework with two baseline
methods. The first (“Cosine”) is a straightfor-
ward implementation that sets all mixture
weights Ad, to the cosine similarity between the
probability mass vectors of the document and
user unigram language models. The second
(“PS”) uses the pattern search algorithm to per-
form constrained optimization over the mixture
weights. As mentioned in section 2.3, the main
difference between this method and ours
(“FGM”) is that we reduce the search space of
the parameters by FGM. Furthermore, social
network information is exploited in our frame-
work, while the PS method performs a direct
search over mixture weights, discarding valuable
knowledge.
Different from other smoothing methods that
are usually mutually exclusive, any other
smoothing methods can be easily merged into
our framework. In Eq. 2, the base language
model P,,(w) can be already smoothed by any
techniques before being plugged into our frame-
work. Our framework then enriches the user lan-
guage model with social network information.
We select four popular smoothing methods to
demonstrate such effect, namely additive
smoothing, absolute smoothing (Ney et al., 1995),
Jelinek-Mercer smoothing (Jelinek and Mercer,
1980) and Dirichlet smoothing (MacKay and
Peto, 1994). The results of using only the base
model (i.e. set Ad, = 0 in Eq. 2) are denoted as
“Base” in the following tables.
</bodyText>
<table confidence="0.999389083333333">
Train % Additive Absolute
Base Cosine PS FGM Base Cosine PS FGM
1% 900.4 712.6 725.5 537.5** 895.3 703.1 722.1 544.5**
5% 814.5 623.4 690.5 506.8** 782.4 607.9 678.4 510.2**
10% 757.7 566.6 684.8 481.2** 708.4 552.7 661.0 485.8**
15% 693.8 521.0 635.2 474.8** 647.4 504.3 622.3 474.1**
Train % Jelinek-Mercer Dirichlet
Base Cosine PS FGM Base Cosine PS FGM
1% 637.8 571.4 643.1 541.0** 638.5 571.3 643.1 541.0**
5% 593.9 526.1 602.9 505.4** 595.0 526.6 616.5 507.2**
10% 559.2 494.1 573.8 483.6** 560.4 494.9 579.6 486.0**
15% 535.3 473.4 560.2 473.0 535.7 473.6 563.2 474.4
</table>
<tableCaption confidence="0.973143">
Table 2: Testing set perplexity. ** indicates that the best score among all methods is significantly bet-
ter than the next highest score, by t-test at a significance level of 0.05.
</tableCaption>
<page confidence="0.996203">
614
</page>
<subsectionHeader confidence="0.993329">
3.3 Perplexity
</subsectionHeader>
<bodyText confidence="0.9999941">
As an intrinsic evaluation, we first compute the
perplexity of unseen sentences under each user
language model. The result is shown in Table 2.
Our method significantly outperforms all of
the methods in almost all settings. We observe
that the “PS” method takes a long time to con-
verge and is prone to overfitting, likely because
it has to search about a few hundred parameters
on average. As expected, the advantage of our
model is more apparent when the data is sparse.
</bodyText>
<subsectionHeader confidence="0.990818">
3.4 Authorship Attribution (AA)
</subsectionHeader>
<bodyText confidence="0.999158157894737">
The authorship attribution (AA) task is chosen as
the extrinsic evaluation metric. Here the goal is
not about comparing with the state-of-the-art ap-
proaches in AA, but showing that LM-based ap-
proaches can benefit from our framework.
To apply PLM on this task, a naïve Bayes
classifier is implemented (Peng et al., 2004). The
most probable author of a document d is the one
whose PLM yields the highest probability, and is
determined by 𝑢∗ = argmax𝑢{Fh𝑤Cd P𝑢(W)}.
The result is shown in Table 3. Our model im-
proves personalization and outperforms the base-
lines under cold start settings. When data is
sparse, the “PS” method tends to overfit the
noise, while the “Cosine” method contains too
few information and is severely biased. Our
method strikes a balance between model com-
plexity and the amount of information included,
and hence performs better than the others.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.9999665">
Personalization has long been studied in various
textual related tasks. Personalized search is es-
tablished by modeling user behavior when using
search engines (Shen et al., 2005; Xue et al.,
2009). Query language model could be also ex-
panded based on personalized user modeling
(Chirita et al., 2007). Personalization has also
been modeled in many NLP tasks such as sum-
marization (Yan et al., 2011) and recommenda-
tion (Yan et al., 2012). Different from our pur-
pose, these models do not aim at exploiting so-
cial media content to enrich a language model.
Wen et al. (2012) combines user-level language
models from a social network, but instead of fo-
cusing on the cold start problem, they try to im-
prove the speech recognition performance using
a mass amount of texts on social network. On the
other hand, our work explicitly models the more
sophisticated document-level relationships using
a probabilistic graphical model.
</bodyText>
<sectionHeader confidence="0.99777" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999993933333334">
The advantage of our model is threefold. First,
prior knowledge and heuristics about the social
network can be adapted in a structured way
through the use of FGM. Second, by exploiting a
well-studied graphical model, mature inference
techniques, such as LBP, can be applied in the
optimization procedure, making it much more
effective and efficient. Finally, different from
most smoothing methods that are mutually ex-
clusive, any other smoothing method can be in-
corporated into our framework to be further en-
hanced. Using only 1% of the training corpus,
our model can improve the perplexity of base
models by as much as 40% and the accuracy of
authorship attribution by at most 15%.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="acknowledgments">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.998762">
This work was sponsored by AOARD grant
number No. FA2386-13-1-4045 and National
Science Council, National Taiwan University
and Intel Corporation under Grants NSC102-
2911-I-002-001 and NTU103R7501 and grant
102-2923-E-002-007-MY2, 102-2221-E-002-170,
101-2628-E-002-028-MY2.
</bodyText>
<table confidence="0.984394333333333">
Train % Additive Absolute
Base Cosine PS FGM Base Cosine PS FGM
1% 54.67 58.27 61.07 63.74 49.47 57.60 58.27 64.27**
5% 61.47 63.20 62.67 68.40** 59.60 62.40 61.33 66.53**
10% 61.47 65.73 66.27 69.20** 61.47 65.20 64.67 71.87**
15% 64.27 67.07 62.13 70.40** 64.67 68.27 63.33 71.60**
Train % Jelinek-Mercer Dirichlet
Base Cosine PS FGM Base Cosine PS FGM
1% 54.00 60.93 62.00 64.80** 52.80 60.40 61.87 64.67**
5% 62.67 65.47 64.00 68.00 60.80 65.33 62.40 66.93
10% 63.87 68.00 67.87 68.53 62.53 67.87 66.40 68.53
15% 65.87 70.40 64.14 69.87 65.47 70.27 64.53 68.40
</table>
<tableCaption confidence="0.955764">
Table 3: Accuracy (%) of authorship attribution. ** indicates that the best score among all methods is
significantly better than the next highest score, by t-test at a significance level of 0.05.
</tableCaption>
<page confidence="0.998141">
615
</page>
<sectionHeader confidence="0.95526" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999879403669725">
Charles Audet and J. E. Dennis, Jr. 2002. Analysis of
generalized pattern searches. SIAM J. on Optimiza-
tion, 13(3):889–903, August.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ’96, pages 310–318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Paul Alexandru Chirita, Claudiu S. Firan, and Wolf-
gang Nejdl. 2007. Personalized query expansion
for the web. In Proceedings of the 30th Annual In-
ternational ACM SIGIR Conference on Research
and Development in Information Retrieval,
SIGIR ’07, pages 7–14, New York, NY, USA.
ACM.
Maarten Clements. 2007. Personalization of social
media. In Proceedings of the 1st BCS IRSG Con-
ference on Future Directions in Information Access,
FDIA’07, pages 14–14, Swinton, UK, UK. British
Computer Society.
Wojciech Galuba, Karl Aberer, Dipanjan Chakraborty,
Zoran Despotovic, and Wolfgang Kellerer. 2010.
Outtweeting the twitterers - predicting information
cascades in microblogs. In Proceedings of the 3rd
Conference on Online Social Networks, WOSN’10,
pages 3–3, Berkeley, CA, USA. USENIX Associa-
tion.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of markov source parameters
from sparse data. In In Proceedings of the Work-
shop on Pattern Recognition in Practice, pages
381–397, Amsterdam, The Netherlands: North-
Holland, May.
F. R. Kschischang, B. J. Frey, and H. A. Loeliger.
2006. Factor graphs and the sum-product algorithm.
IEEE Trans. Inf. Theor., 47(2):498–519, Septem-
ber.
Jimmy Lin, Rion Snow, and William Morgan. 2011.
Smoothing techniques for adaptive online language
models: Topic tracking in tweet streams. In Pro-
ceedings of the 17th ACM SIGKDD International
Conference on Knowledge Discovery and Data
Mining, KDD ’11, pages 422–429, New York, NY,
USA. ACM.
David J.C. MacKay and Linda C. Bauman Peto. 1994.
A hierarchical dirichlet language model. Natural
Language Engineering, 1:1–19.
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In Proceedings of the
Fifteenth Conference on Uncertainty in Artificial
Intelligence, UAI’99, pages 467–475, San Francis-
co, CA, USA. Morgan Kaufmann Publishers Inc.
Hermann Ney, Ute Essen, and Reinhard Kneser. 1995.
On the estimation of ’small’ probabilities by leav-
ing-one-out. IEEE Trans. Pattern Anal. Mach. In-
tell., 17(12):1202–1212, December.
Fuchun Peng, Dale Schuurmans, and Shaojun Wang.
2004. Augmenting naive bayes classifiers with sta-
tistical language models. Inf. Retr., 7(3-4):317–345,
September.
Xuehua Shen, Bin Tan, and ChengXiang Zhai. 2005.
Implicit user modeling for personalized search. In
Proceedings of the 14th ACM International Con-
ference on Information and Knowledge Manage-
ment, CIKM ’05, pages 824–831, New York, NY,
USA. ACM.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang,
Ming Zhou, and Ping Li. 2011. User-level senti-
ment analysis incorporating social networks. In
Proceedings of the 17th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and
Data Mining, KDD ’11, pages 1397–1405, New
York, NY, USA. ACM.
Zhichun Wang, Juanzi Li, Zhigang Wang, and Jie
Tang. 2012. Cross-lingual knowledge linking
across wiki knowledge bases. In Proceedings of the
21st International Conference on World Wide Web,
WWW ’12, pages 459–468, New York, NY, USA.
ACM.
Tsung-Hsien Wen, Hung-Yi Lee, Tai-Yuan Chen, and
Lin-Shan Lee. 2012. Personalized language model-
ing by crowd sourcing with social network data for
voice access of cloud applications. In Spoken Lan-
guage Technology Workshop (SLT), 2012 IEEE,
pages 188–193.
Gui-Rong Xue, Jie Han, Yong Yu, and Qiang Yang.
2009. User language model for collaborative per-
sonalized search. ACM Trans. Inf. Syst.,
27(2):11:1–11:28, March.
Rui Yan, Jian-Yun Nie, and Xiaoming Li. 2011.
Summarize what you are interested in: An optimi-
zation framework for interactive personalized
summarization. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’11, pages 1342–1351, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Rui Yan, Mirella Lapata, and Xiaoming Li. 2012.
Tweet recommendation with graph co-ranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Long Pa-
pers - Volume 1, ACL ’12, pages 516–525,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
ChengXiang Zhai. 2008. Statistical Language Models
for Information Retrieval. Now Publishers Inc.,
Hanover, MA, USA.
</reference>
<page confidence="0.985309">
616
</page>
<reference confidence="0.987118571428572">
Chengxiang Zhai and John Lafferty. 2001. A study of
smoothing methods for language models applied to
ad hoc information retrieval. In Proceedings of the
24th Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, SIGIR ’01, pages 334–342, New York, NY,
USA. ACM.
</reference>
<page confidence="0.997292">
617
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476177">
<title confidence="0.889009666666667">Enriching Cold Start Personalized Language Using Social Network Information Rui Tsung-Ting Shou-De Institute of Computer Science and Information</title>
<affiliation confidence="0.9419458">National Taiwan University, Taipei, Institute of Network and National Taiwan University, Taipei, and Information Science University of Pennsylvania, Philadelphia, PA 19104, U.S.A.</affiliation>
<email confidence="0.996839">r02922050@csie.ntu.edu.tw,ruiyan@seas.upenn.edu</email>
<email confidence="0.996839">d97944007@csie.ntu.edu.tw,ruiyan@seas.upenn.edu</email>
<email confidence="0.996839">sdlin@csie.ntu.edu.tw,ruiyan@seas.upenn.edu</email>
<abstract confidence="0.999606583333333">We introduce a generalized framework to enrich the personalized language models for cold start users. The cold start problem is solved with content written by friends on social network services. Our framework consists of a mixture language model, whose mixture weights are estimated with a factor graph. The factor graph is used to incorporate prior knowledge and heuristics to identify the most appropriate weights. The intrinsic and extrinsic experiments show significant improvement on cold start users.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charles Audet</author>
<author>J E Dennis</author>
</authors>
<title>Analysis of generalized pattern searches.</title>
<date>2002</date>
<journal>SIAM J. on Optimization,</journal>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="14318" citStr="Audet and Dennis, 2002" startWordPosition="2376" endWordPosition="2379">s. The size of the vocabulary set is 1377. We randomly partitioned the tweets of each user into training, validation and testing sets. The reported result is the average of 10 random splits. In all experiments, we vary the size of training data from 1% to 15%, and hold out the same number of tweets from each user as validation and testing data. The statistics of our dataset, given 15% training data, are shown in Table 1. Loopy belief propagation (LBP) is used to obtain the marginal probabilities of the variables (Murphy et al., 1999). Parameters are searched with the pattern search algorithm (Audet and Dennis, 2002). To not lose generality, we use the default configuration in all experiments. # of Max. Min. Avg. Tweets 70 19 35.4 Friends 139 24 68.9 Variables 467 97 252.7 Edges 9216 231 3427.1 Table 1: Dataset statistics 3.2 Baseline Methods We compare our framework with two baseline methods. The first (“Cosine”) is a straightforward implementation that sets all mixture weights Ad, to the cosine similarity between the probability mass vectors of the document and user unigram language models. The second (“PS”) uses the pattern search algorithm to perform constrained optimization over the mixture weights. </context>
</contexts>
<marker>Audet, Dennis, 2002</marker>
<rawString>Charles Audet and J. E. Dennis, Jr. 2002. Analysis of generalized pattern searches. SIAM J. on Optimization, 13(3):889–903, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL ’96,</booktitle>
<pages>310--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6043" citStr="Chen and Goodman, 1996" startWordPosition="956" endWordPosition="960">ntire collection (Zhai, 2008). To build a user language model, one naïve way is to first normalize word frequency 𝑐(𝑤, 𝑑) within each document, and then average over all the documents in a user’s document collection. The resulting unigram user language model is: 1 ∑ 𝑐(𝑤, 𝑑) |𝒟𝑢 |𝑑∈𝒟𝑢 |𝑑 |(1) 1 |𝒟𝑢 |∑ 𝑃𝑑(𝑤) 𝑑∈𝒟𝑢 where 𝑃𝑑 (𝑤) is the language model of a particular document, and 𝒟𝑢 is the user’s document collection. This formulation is basically an equalweighted finite mixture model. A simple yet effective way to smooth a language model is to linearly interpolate with a background language model (Chen and Goodman, 1996; Zhai and Lafferty, 2001). In the linear interpolation method, all background documents are treated equally. The entire document collection is added to the user language model 𝑃𝑢 (𝑤) with the same interpolation coefficient. Our main idea is to specify a set of relevant documents for the target user using information embedded in a social network, and enrich the smoothing procedure with these documents. Let 𝒟𝑟𝑒𝑙 denote the content from relevant persons (e.g. social neighbors) of u1, our idea can be concisely expressed as: 𝑃𝑢1 ′ (𝑤) = 𝜆𝑢1𝑃𝑢1(𝑤) + ∑ 𝜆𝑑𝑖𝑃𝑑𝑖(𝑤) 𝑑𝑖∈𝒟𝑟𝑒𝑙 where 𝜆𝑑𝑖 is the mixture weig</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL ’96, pages 310–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Alexandru Chirita</author>
<author>Claudiu S Firan</author>
<author>Wolfgang Nejdl</author>
</authors>
<title>Personalized query expansion for the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’07,</booktitle>
<pages>7--14</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="18401" citStr="Chirita et al., 2007" startWordPosition="3046" endWordPosition="3049">der cold start settings. When data is sparse, the “PS” method tends to overfit the noise, while the “Cosine” method contains too few information and is severely biased. Our method strikes a balance between model complexity and the amount of information included, and hence performs better than the others. 4 Related Work Personalization has long been studied in various textual related tasks. Personalized search is established by modeling user behavior when using search engines (Shen et al., 2005; Xue et al., 2009). Query language model could be also expanded based on personalized user modeling (Chirita et al., 2007). Personalization has also been modeled in many NLP tasks such as summarization (Yan et al., 2011) and recommendation (Yan et al., 2012). Different from our purpose, these models do not aim at exploiting social media content to enrich a language model. Wen et al. (2012) combines user-level language models from a social network, but instead of focusing on the cold start problem, they try to improve the speech recognition performance using a mass amount of texts on social network. On the other hand, our work explicitly models the more sophisticated document-level relationships using a probabilis</context>
</contexts>
<marker>Chirita, Firan, Nejdl, 2007</marker>
<rawString>Paul Alexandru Chirita, Claudiu S. Firan, and Wolfgang Nejdl. 2007. Personalized query expansion for the web. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’07, pages 7–14, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarten Clements</author>
</authors>
<title>Personalization of social media.</title>
<date>2007</date>
<booktitle>In Proceedings of the 1st BCS IRSG Conference on Future Directions in Information Access, FDIA’07,</booktitle>
<pages>14--14</pages>
<publisher>British Computer Society.</publisher>
<location>Swinton, UK, UK.</location>
<contexts>
<context position="1186" citStr="Clements, 2007" startWordPosition="159" endWordPosition="160">he personalized language models for cold start users. The cold start problem is solved with content written by friends on social network services. Our framework consists of a mixture language model, whose mixture weights are estimated with a factor graph. The factor graph is used to incorporate prior knowledge and heuristics to identify the most appropriate weights. The intrinsic and extrinsic experiments show significant improvement on cold start users. 1 Introduction Personalized language models (PLM) on social network services are useful in many aspects (Xue et al., 2009; Wen et al., 2012; Clements, 2007), For instance, if the authorship of a document is in doubt, a PLM may be used as a generative model to identify it. In this sense, a PLM serves as a proxy of one’s writing style. Furthermore, PLMs can improve the quality of information retrieval and content-based recommendation systems, where documents or topics can be recommended based on the generative probabilities. However, it is challenging to build a PLM for users who just entered the system, and whose content is thus insufficient to characterize them. These are called “cold start” users. Producing better recommendations is even more cr</context>
</contexts>
<marker>Clements, 2007</marker>
<rawString>Maarten Clements. 2007. Personalization of social media. In Proceedings of the 1st BCS IRSG Conference on Future Directions in Information Access, FDIA’07, pages 14–14, Swinton, UK, UK. British Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Galuba</author>
<author>Karl Aberer</author>
<author>Dipanjan Chakraborty</author>
<author>Zoran Despotovic</author>
<author>Wolfgang Kellerer</author>
</authors>
<title>Outtweeting the twitterers - predicting information cascades in microblogs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd Conference on Online Social Networks, WOSN’10,</booktitle>
<pages>3--3</pages>
<publisher>USENIX Association.</publisher>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="13213" citStr="Galuba et al. (2010)" startWordPosition="2185" endWordPosition="2188"> our model. At 613 first, all the model parameters (i.e. a, fl, A�) are randomly initialized. Then, we infer the marginal probabilities of candidate variables. Given these marginal probabilities, we can evaluate the perplexity of the user language model on a held-out dataset, and search for better parameters. This procedure is repeated until convergence. Also, notice that by using FGM, we reduce the number of parameters from 1 + |D,�1 |to 1 + |a |+ |fl |, lowering the risk of overfitting. 3 Experiments 3.1 Dataset and Experiment Setup We perform experiments on the Twitter dataset collected by Galuba et al. (2010). Twitter data have been used to verify models with different purposes (Lin et al., 2011; Tan et al., 2011). To emphasize on the cold start scenario, we randomly selected 15 users with about 35 tweets and 70 friends as candidates for an authorship attribution task. Our experiment corpus consists of 4322 tweets. All words with less than 5 occurrences are removed. Stop words and URLs are also removed and all tweets are stemmed. We identify the 100 most frequent terms as categories. The size of the vocabulary set is 1377. We randomly partitioned the tweets of each user into training, validation a</context>
</contexts>
<marker>Galuba, Aberer, Chakraborty, Despotovic, Kellerer, 2010</marker>
<rawString>Wojciech Galuba, Karl Aberer, Dipanjan Chakraborty, Zoran Despotovic, and Wolfgang Kellerer. 2010. Outtweeting the twitterers - predicting information cascades in microblogs. In Proceedings of the 3rd Conference on Online Social Networks, WOSN’10, pages 3–3, Berkeley, CA, USA. USENIX Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>Interpolated estimation of markov source parameters from sparse data. In</title>
<date>1980</date>
<booktitle>In Proceedings of the Workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<location>Amsterdam, The Netherlands: NorthHolland,</location>
<contexts>
<context position="15774" citStr="Jelinek and Mercer, 1980" startWordPosition="2603" endWordPosition="2606">method performs a direct search over mixture weights, discarding valuable knowledge. Different from other smoothing methods that are usually mutually exclusive, any other smoothing methods can be easily merged into our framework. In Eq. 2, the base language model P,,(w) can be already smoothed by any techniques before being plugged into our framework. Our framework then enriches the user language model with social network information. We select four popular smoothing methods to demonstrate such effect, namely additive smoothing, absolute smoothing (Ney et al., 1995), Jelinek-Mercer smoothing (Jelinek and Mercer, 1980) and Dirichlet smoothing (MacKay and Peto, 1994). The results of using only the base model (i.e. set Ad, = 0 in Eq. 2) are denoted as “Base” in the following tables. Train % Additive Absolute Base Cosine PS FGM Base Cosine PS FGM 1% 900.4 712.6 725.5 537.5** 895.3 703.1 722.1 544.5** 5% 814.5 623.4 690.5 506.8** 782.4 607.9 678.4 510.2** 10% 757.7 566.6 684.8 481.2** 708.4 552.7 661.0 485.8** 15% 693.8 521.0 635.2 474.8** 647.4 504.3 622.3 474.1** Train % Jelinek-Mercer Dirichlet Base Cosine PS FGM Base Cosine PS FGM 1% 637.8 571.4 643.1 541.0** 638.5 571.3 643.1 541.0** 5% 593.9 526.1 602.9 5</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Frederick Jelinek and Robert L. Mercer. 1980. Interpolated estimation of markov source parameters from sparse data. In In Proceedings of the Workshop on Pattern Recognition in Practice, pages 381–397, Amsterdam, The Netherlands: NorthHolland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F R Kschischang</author>
<author>B J Frey</author>
<author>H A Loeliger</author>
</authors>
<title>Factor graphs and the sum-product algorithm.</title>
<date>2006</date>
<journal>IEEE Trans. Inf. Theor.,</journal>
<volume>47</volume>
<issue>2</issue>
<contexts>
<context position="7567" citStr="Kschischang et al., 2006" startWordPosition="1215" endWordPosition="1218">r documents as 𝒟𝑟𝑒𝑙. Also note that we have made no assumption about how the “base” user language model 𝑃𝑢1(𝑤) is built. In practice, it need not be models following maximum likelihood estimation, but any language model can be integrated into our framework to achieve a better refined model. Furthermore, any smoothing method can be applied to the language model without degrading the effectiveness. 2.2 Factor Graph Model (FGM) Now we discuss how the mixture weights can be estimated. We introduce a factor graph model (FGM) to make use of the diverse information on a social network. Factor graph (Kschischang et al., 2006) is a bipartite graph consisting of a set of random variables and a set of factors which signifies the relationships among the variables. It is best suited in situations where the data is clearly of a relational nature (Wang et al., 2012). The joint distribution of the variables is factored according to the graph structure. Using FGM, one can incorporate the knowledge into the potential function for optimization and perform joint inference over documents. As shown in Figure 1, the variables included in the model are described as follows: Candidate variables 𝑦𝑖 = 〈𝑢, 𝑑𝑖〉 . The random variables </context>
</contexts>
<marker>Kschischang, Frey, Loeliger, 2006</marker>
<rawString>F. R. Kschischang, B. J. Frey, and H. A. Loeliger. 2006. Factor graphs and the sum-product algorithm. IEEE Trans. Inf. Theor., 47(2):498–519, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
<author>Rion Snow</author>
<author>William Morgan</author>
</authors>
<title>Smoothing techniques for adaptive online language models: Topic tracking in tweet streams.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11,</booktitle>
<pages>422--429</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13301" citStr="Lin et al., 2011" startWordPosition="2200" endWordPosition="2203">. Then, we infer the marginal probabilities of candidate variables. Given these marginal probabilities, we can evaluate the perplexity of the user language model on a held-out dataset, and search for better parameters. This procedure is repeated until convergence. Also, notice that by using FGM, we reduce the number of parameters from 1 + |D,�1 |to 1 + |a |+ |fl |, lowering the risk of overfitting. 3 Experiments 3.1 Dataset and Experiment Setup We perform experiments on the Twitter dataset collected by Galuba et al. (2010). Twitter data have been used to verify models with different purposes (Lin et al., 2011; Tan et al., 2011). To emphasize on the cold start scenario, we randomly selected 15 users with about 35 tweets and 70 friends as candidates for an authorship attribution task. Our experiment corpus consists of 4322 tweets. All words with less than 5 occurrences are removed. Stop words and URLs are also removed and all tweets are stemmed. We identify the 100 most frequent terms as categories. The size of the vocabulary set is 1377. We randomly partitioned the tweets of each user into training, validation and testing sets. The reported result is the average of 10 random splits. In all experime</context>
</contexts>
<marker>Lin, Snow, Morgan, 2011</marker>
<rawString>Jimmy Lin, Rion Snow, and William Morgan. 2011. Smoothing techniques for adaptive online language models: Topic tracking in tweet streams. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11, pages 422–429, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C MacKay</author>
<author>Linda C Bauman Peto</author>
</authors>
<title>A hierarchical dirichlet language model.</title>
<date>1994</date>
<journal>Natural Language Engineering,</journal>
<pages>1--1</pages>
<contexts>
<context position="15822" citStr="MacKay and Peto, 1994" startWordPosition="2610" endWordPosition="2613">s, discarding valuable knowledge. Different from other smoothing methods that are usually mutually exclusive, any other smoothing methods can be easily merged into our framework. In Eq. 2, the base language model P,,(w) can be already smoothed by any techniques before being plugged into our framework. Our framework then enriches the user language model with social network information. We select four popular smoothing methods to demonstrate such effect, namely additive smoothing, absolute smoothing (Ney et al., 1995), Jelinek-Mercer smoothing (Jelinek and Mercer, 1980) and Dirichlet smoothing (MacKay and Peto, 1994). The results of using only the base model (i.e. set Ad, = 0 in Eq. 2) are denoted as “Base” in the following tables. Train % Additive Absolute Base Cosine PS FGM Base Cosine PS FGM 1% 900.4 712.6 725.5 537.5** 895.3 703.1 722.1 544.5** 5% 814.5 623.4 690.5 506.8** 782.4 607.9 678.4 510.2** 10% 757.7 566.6 684.8 481.2** 708.4 552.7 661.0 485.8** 15% 693.8 521.0 635.2 474.8** 647.4 504.3 622.3 474.1** Train % Jelinek-Mercer Dirichlet Base Cosine PS FGM Base Cosine PS FGM 1% 637.8 571.4 643.1 541.0** 638.5 571.3 643.1 541.0** 5% 593.9 526.1 602.9 505.4** 595.0 526.6 616.5 507.2** 10% 559.2 494.1</context>
</contexts>
<marker>MacKay, Peto, 1994</marker>
<rawString>David J.C. MacKay and Linda C. Bauman Peto. 1994. A hierarchical dirichlet language model. Natural Language Engineering, 1:1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
<author>Yair Weiss</author>
<author>Michael I Jordan</author>
</authors>
<title>Loopy belief propagation for approximate inference: An empirical study.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI’99,</booktitle>
<pages>467--475</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="14234" citStr="Murphy et al., 1999" startWordPosition="2364" endWordPosition="2367"> and all tweets are stemmed. We identify the 100 most frequent terms as categories. The size of the vocabulary set is 1377. We randomly partitioned the tweets of each user into training, validation and testing sets. The reported result is the average of 10 random splits. In all experiments, we vary the size of training data from 1% to 15%, and hold out the same number of tweets from each user as validation and testing data. The statistics of our dataset, given 15% training data, are shown in Table 1. Loopy belief propagation (LBP) is used to obtain the marginal probabilities of the variables (Murphy et al., 1999). Parameters are searched with the pattern search algorithm (Audet and Dennis, 2002). To not lose generality, we use the default configuration in all experiments. # of Max. Min. Avg. Tweets 70 19 35.4 Friends 139 24 68.9 Variables 467 97 252.7 Edges 9216 231 3427.1 Table 1: Dataset statistics 3.2 Baseline Methods We compare our framework with two baseline methods. The first (“Cosine”) is a straightforward implementation that sets all mixture weights Ad, to the cosine similarity between the probability mass vectors of the document and user unigram language models. The second (“PS”) uses the pat</context>
</contexts>
<marker>Murphy, Weiss, Jordan, 1999</marker>
<rawString>Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. 1999. Loopy belief propagation for approximate inference: An empirical study. In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI’99, pages 467–475, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Ute Essen</author>
<author>Reinhard Kneser</author>
</authors>
<title>On the estimation of ’small’ probabilities by leaving-one-out.</title>
<date>1995</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>17</volume>
<issue>12</issue>
<contexts>
<context position="15721" citStr="Ney et al., 1995" startWordPosition="2597" endWordPosition="2600"> is exploited in our framework, while the PS method performs a direct search over mixture weights, discarding valuable knowledge. Different from other smoothing methods that are usually mutually exclusive, any other smoothing methods can be easily merged into our framework. In Eq. 2, the base language model P,,(w) can be already smoothed by any techniques before being plugged into our framework. Our framework then enriches the user language model with social network information. We select four popular smoothing methods to demonstrate such effect, namely additive smoothing, absolute smoothing (Ney et al., 1995), Jelinek-Mercer smoothing (Jelinek and Mercer, 1980) and Dirichlet smoothing (MacKay and Peto, 1994). The results of using only the base model (i.e. set Ad, = 0 in Eq. 2) are denoted as “Base” in the following tables. Train % Additive Absolute Base Cosine PS FGM Base Cosine PS FGM 1% 900.4 712.6 725.5 537.5** 895.3 703.1 722.1 544.5** 5% 814.5 623.4 690.5 506.8** 782.4 607.9 678.4 510.2** 10% 757.7 566.6 684.8 481.2** 708.4 552.7 661.0 485.8** 15% 693.8 521.0 635.2 474.8** 647.4 504.3 622.3 474.1** Train % Jelinek-Mercer Dirichlet Base Cosine PS FGM Base Cosine PS FGM 1% 637.8 571.4 643.1 541</context>
</contexts>
<marker>Ney, Essen, Kneser, 1995</marker>
<rawString>Hermann Ney, Ute Essen, and Reinhard Kneser. 1995. On the estimation of ’small’ probabilities by leaving-one-out. IEEE Trans. Pattern Anal. Mach. Intell., 17(12):1202–1212, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Dale Schuurmans</author>
<author>Shaojun Wang</author>
</authors>
<title>Augmenting naive bayes classifiers with statistical language models.</title>
<date>2004</date>
<journal>Inf. Retr.,</journal>
<pages>7--3</pages>
<contexts>
<context position="17537" citStr="Peng et al., 2004" startWordPosition="2903" endWordPosition="2906">most all settings. We observe that the “PS” method takes a long time to converge and is prone to overfitting, likely because it has to search about a few hundred parameters on average. As expected, the advantage of our model is more apparent when the data is sparse. 3.4 Authorship Attribution (AA) The authorship attribution (AA) task is chosen as the extrinsic evaluation metric. Here the goal is not about comparing with the state-of-the-art approaches in AA, but showing that LM-based approaches can benefit from our framework. To apply PLM on this task, a naïve Bayes classifier is implemented (Peng et al., 2004). The most probable author of a document d is the one whose PLM yields the highest probability, and is determined by 𝑢∗ = argmax𝑢{Fh𝑤Cd P𝑢(W)}. The result is shown in Table 3. Our model improves personalization and outperforms the baselines under cold start settings. When data is sparse, the “PS” method tends to overfit the noise, while the “Cosine” method contains too few information and is severely biased. Our method strikes a balance between model complexity and the amount of information included, and hence performs better than the others. 4 Related Work Personalization has long been studie</context>
</contexts>
<marker>Peng, Schuurmans, Wang, 2004</marker>
<rawString>Fuchun Peng, Dale Schuurmans, and Shaojun Wang. 2004. Augmenting naive bayes classifiers with statistical language models. Inf. Retr., 7(3-4):317–345, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuehua Shen</author>
<author>Bin Tan</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Implicit user modeling for personalized search.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM ’05,</booktitle>
<pages>824--831</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="18278" citStr="Shen et al., 2005" startWordPosition="3025" endWordPosition="3028">gmax𝑢{Fh𝑤Cd P𝑢(W)}. The result is shown in Table 3. Our model improves personalization and outperforms the baselines under cold start settings. When data is sparse, the “PS” method tends to overfit the noise, while the “Cosine” method contains too few information and is severely biased. Our method strikes a balance between model complexity and the amount of information included, and hence performs better than the others. 4 Related Work Personalization has long been studied in various textual related tasks. Personalized search is established by modeling user behavior when using search engines (Shen et al., 2005; Xue et al., 2009). Query language model could be also expanded based on personalized user modeling (Chirita et al., 2007). Personalization has also been modeled in many NLP tasks such as summarization (Yan et al., 2011) and recommendation (Yan et al., 2012). Different from our purpose, these models do not aim at exploiting social media content to enrich a language model. Wen et al. (2012) combines user-level language models from a social network, but instead of focusing on the cold start problem, they try to improve the speech recognition performance using a mass amount of texts on social ne</context>
</contexts>
<marker>Shen, Tan, Zhai, 2005</marker>
<rawString>Xuehua Shen, Bin Tan, and ChengXiang Zhai. 2005. Implicit user modeling for personalized search. In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM ’05, pages 824–831, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhao Tan</author>
<author>Lillian Lee</author>
<author>Jie Tang</author>
<author>Long Jiang</author>
<author>Ming Zhou</author>
<author>Ping Li</author>
</authors>
<title>User-level sentiment analysis incorporating social networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11,</booktitle>
<pages>1397--1405</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13320" citStr="Tan et al., 2011" startWordPosition="2204" endWordPosition="2207">he marginal probabilities of candidate variables. Given these marginal probabilities, we can evaluate the perplexity of the user language model on a held-out dataset, and search for better parameters. This procedure is repeated until convergence. Also, notice that by using FGM, we reduce the number of parameters from 1 + |D,�1 |to 1 + |a |+ |fl |, lowering the risk of overfitting. 3 Experiments 3.1 Dataset and Experiment Setup We perform experiments on the Twitter dataset collected by Galuba et al. (2010). Twitter data have been used to verify models with different purposes (Lin et al., 2011; Tan et al., 2011). To emphasize on the cold start scenario, we randomly selected 15 users with about 35 tweets and 70 friends as candidates for an authorship attribution task. Our experiment corpus consists of 4322 tweets. All words with less than 5 occurrences are removed. Stop words and URLs are also removed and all tweets are stemmed. We identify the 100 most frequent terms as categories. The size of the vocabulary set is 1377. We randomly partitioned the tweets of each user into training, validation and testing sets. The reported result is the average of 10 random splits. In all experiments, we vary the si</context>
</contexts>
<marker>Tan, Lee, Tang, Jiang, Zhou, Li, 2011</marker>
<rawString>Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming Zhou, and Ping Li. 2011. User-level sentiment analysis incorporating social networks. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11, pages 1397–1405, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhichun Wang</author>
<author>Juanzi Li</author>
<author>Zhigang Wang</author>
<author>Jie Tang</author>
</authors>
<title>Cross-lingual knowledge linking across wiki knowledge bases.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st International Conference on World Wide Web, WWW ’12,</booktitle>
<pages>459--468</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7805" citStr="Wang et al., 2012" startWordPosition="1258" endWordPosition="1261">r framework to achieve a better refined model. Furthermore, any smoothing method can be applied to the language model without degrading the effectiveness. 2.2 Factor Graph Model (FGM) Now we discuss how the mixture weights can be estimated. We introduce a factor graph model (FGM) to make use of the diverse information on a social network. Factor graph (Kschischang et al., 2006) is a bipartite graph consisting of a set of random variables and a set of factors which signifies the relationships among the variables. It is best suited in situations where the data is clearly of a relational nature (Wang et al., 2012). The joint distribution of the variables is factored according to the graph structure. Using FGM, one can incorporate the knowledge into the potential function for optimization and perform joint inference over documents. As shown in Figure 1, the variables included in the model are described as follows: Candidate variables 𝑦𝑖 = 〈𝑢, 𝑑𝑖〉 . The random variables in the top layer stand for the degrees of belief that a document di should be included in the PLM of the target user 𝑢. Figure 1: A two-layered factor graph (FGM) proposed to estimate the mixture weights. 𝑃𝑢(𝑤) = (2) 612 Attribute variabl</context>
</contexts>
<marker>Wang, Li, Wang, Tang, 2012</marker>
<rawString>Zhichun Wang, Juanzi Li, Zhigang Wang, and Jie Tang. 2012. Cross-lingual knowledge linking across wiki knowledge bases. In Proceedings of the 21st International Conference on World Wide Web, WWW ’12, pages 459–468, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsung-Hsien Wen</author>
<author>Hung-Yi Lee</author>
<author>Tai-Yuan Chen</author>
<author>Lin-Shan Lee</author>
</authors>
<title>Personalized language modeling by crowd sourcing with social network data for voice access of cloud applications.</title>
<date>2012</date>
<booktitle>In Spoken Language Technology Workshop (SLT), 2012 IEEE,</booktitle>
<pages>188--193</pages>
<contexts>
<context position="1169" citStr="Wen et al., 2012" startWordPosition="155" endWordPosition="158">mework to enrich the personalized language models for cold start users. The cold start problem is solved with content written by friends on social network services. Our framework consists of a mixture language model, whose mixture weights are estimated with a factor graph. The factor graph is used to incorporate prior knowledge and heuristics to identify the most appropriate weights. The intrinsic and extrinsic experiments show significant improvement on cold start users. 1 Introduction Personalized language models (PLM) on social network services are useful in many aspects (Xue et al., 2009; Wen et al., 2012; Clements, 2007), For instance, if the authorship of a document is in doubt, a PLM may be used as a generative model to identify it. In this sense, a PLM serves as a proxy of one’s writing style. Furthermore, PLMs can improve the quality of information retrieval and content-based recommendation systems, where documents or topics can be recommended based on the generative probabilities. However, it is challenging to build a PLM for users who just entered the system, and whose content is thus insufficient to characterize them. These are called “cold start” users. Producing better recommendation</context>
<context position="18671" citStr="Wen et al. (2012)" startWordPosition="3095" endWordPosition="3098">erforms better than the others. 4 Related Work Personalization has long been studied in various textual related tasks. Personalized search is established by modeling user behavior when using search engines (Shen et al., 2005; Xue et al., 2009). Query language model could be also expanded based on personalized user modeling (Chirita et al., 2007). Personalization has also been modeled in many NLP tasks such as summarization (Yan et al., 2011) and recommendation (Yan et al., 2012). Different from our purpose, these models do not aim at exploiting social media content to enrich a language model. Wen et al. (2012) combines user-level language models from a social network, but instead of focusing on the cold start problem, they try to improve the speech recognition performance using a mass amount of texts on social network. On the other hand, our work explicitly models the more sophisticated document-level relationships using a probabilistic graphical model. 5 Conclusion The advantage of our model is threefold. First, prior knowledge and heuristics about the social network can be adapted in a structured way through the use of FGM. Second, by exploiting a well-studied graphical model, mature inference te</context>
</contexts>
<marker>Wen, Lee, Chen, Lee, 2012</marker>
<rawString>Tsung-Hsien Wen, Hung-Yi Lee, Tai-Yuan Chen, and Lin-Shan Lee. 2012. Personalized language modeling by crowd sourcing with social network data for voice access of cloud applications. In Spoken Language Technology Workshop (SLT), 2012 IEEE, pages 188–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gui-Rong Xue</author>
<author>Jie Han</author>
<author>Yong Yu</author>
<author>Qiang Yang</author>
</authors>
<title>User language model for collaborative personalized search.</title>
<date>2009</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="1151" citStr="Xue et al., 2009" startWordPosition="151" endWordPosition="154"> a generalized framework to enrich the personalized language models for cold start users. The cold start problem is solved with content written by friends on social network services. Our framework consists of a mixture language model, whose mixture weights are estimated with a factor graph. The factor graph is used to incorporate prior knowledge and heuristics to identify the most appropriate weights. The intrinsic and extrinsic experiments show significant improvement on cold start users. 1 Introduction Personalized language models (PLM) on social network services are useful in many aspects (Xue et al., 2009; Wen et al., 2012; Clements, 2007), For instance, if the authorship of a document is in doubt, a PLM may be used as a generative model to identify it. In this sense, a PLM serves as a proxy of one’s writing style. Furthermore, PLMs can improve the quality of information retrieval and content-based recommendation systems, where documents or topics can be recommended based on the generative probabilities. However, it is challenging to build a PLM for users who just entered the system, and whose content is thus insufficient to characterize them. These are called “cold start” users. Producing bet</context>
<context position="18297" citStr="Xue et al., 2009" startWordPosition="3029" endWordPosition="3032"> The result is shown in Table 3. Our model improves personalization and outperforms the baselines under cold start settings. When data is sparse, the “PS” method tends to overfit the noise, while the “Cosine” method contains too few information and is severely biased. Our method strikes a balance between model complexity and the amount of information included, and hence performs better than the others. 4 Related Work Personalization has long been studied in various textual related tasks. Personalized search is established by modeling user behavior when using search engines (Shen et al., 2005; Xue et al., 2009). Query language model could be also expanded based on personalized user modeling (Chirita et al., 2007). Personalization has also been modeled in many NLP tasks such as summarization (Yan et al., 2011) and recommendation (Yan et al., 2012). Different from our purpose, these models do not aim at exploiting social media content to enrich a language model. Wen et al. (2012) combines user-level language models from a social network, but instead of focusing on the cold start problem, they try to improve the speech recognition performance using a mass amount of texts on social network. On the other</context>
</contexts>
<marker>Xue, Han, Yu, Yang, 2009</marker>
<rawString>Gui-Rong Xue, Jie Han, Yong Yu, and Qiang Yang. 2009. User language model for collaborative personalized search. ACM Trans. Inf. Syst., 27(2):11:1–11:28, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Jian-Yun Nie</author>
<author>Xiaoming Li</author>
</authors>
<title>Summarize what you are interested in: An optimization framework for interactive personalized summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1342--1351</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18499" citStr="Yan et al., 2011" startWordPosition="3063" endWordPosition="3066">osine” method contains too few information and is severely biased. Our method strikes a balance between model complexity and the amount of information included, and hence performs better than the others. 4 Related Work Personalization has long been studied in various textual related tasks. Personalized search is established by modeling user behavior when using search engines (Shen et al., 2005; Xue et al., 2009). Query language model could be also expanded based on personalized user modeling (Chirita et al., 2007). Personalization has also been modeled in many NLP tasks such as summarization (Yan et al., 2011) and recommendation (Yan et al., 2012). Different from our purpose, these models do not aim at exploiting social media content to enrich a language model. Wen et al. (2012) combines user-level language models from a social network, but instead of focusing on the cold start problem, they try to improve the speech recognition performance using a mass amount of texts on social network. On the other hand, our work explicitly models the more sophisticated document-level relationships using a probabilistic graphical model. 5 Conclusion The advantage of our model is threefold. First, prior knowledge </context>
</contexts>
<marker>Yan, Nie, Li, 2011</marker>
<rawString>Rui Yan, Jian-Yun Nie, and Xiaoming Li. 2011. Summarize what you are interested in: An optimization framework for interactive personalized summarization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1342–1351, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Mirella Lapata</author>
<author>Xiaoming Li</author>
</authors>
<title>Tweet recommendation with graph co-ranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>516--525</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18537" citStr="Yan et al., 2012" startWordPosition="3070" endWordPosition="3073">tion and is severely biased. Our method strikes a balance between model complexity and the amount of information included, and hence performs better than the others. 4 Related Work Personalization has long been studied in various textual related tasks. Personalized search is established by modeling user behavior when using search engines (Shen et al., 2005; Xue et al., 2009). Query language model could be also expanded based on personalized user modeling (Chirita et al., 2007). Personalization has also been modeled in many NLP tasks such as summarization (Yan et al., 2011) and recommendation (Yan et al., 2012). Different from our purpose, these models do not aim at exploiting social media content to enrich a language model. Wen et al. (2012) combines user-level language models from a social network, but instead of focusing on the cold start problem, they try to improve the speech recognition performance using a mass amount of texts on social network. On the other hand, our work explicitly models the more sophisticated document-level relationships using a probabilistic graphical model. 5 Conclusion The advantage of our model is threefold. First, prior knowledge and heuristics about the social networ</context>
</contexts>
<marker>Yan, Lapata, Li, 2012</marker>
<rawString>Rui Yan, Mirella Lapata, and Xiaoming Li. 2012. Tweet recommendation with graph co-ranking. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 516–525, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ChengXiang Zhai</author>
</authors>
<title>Statistical Language Models for Information Retrieval.</title>
<date>2008</date>
<publisher>Now Publishers Inc.,</publisher>
<location>Hanover, MA, USA.</location>
<contexts>
<context position="5450" citStr="Zhai, 2008" startWordPosition="852" endWordPosition="853">ugh the use of FGM. An iterative optimization procedure utilizing perplexity is presented to learn the parameters. To our knowledge, this is the first proposal to use FGM to enrich language models. • Perplexity is selected as an intrinsic evaluation, and experiment on authorship attribution is used as an extrinsic evaluation. The results show that our model yields significant improvements for cold start users. 2 Methodology 2.1 Social-Driven Personalized Language Model The language model of a collection of documents can be estimated by normalizing the counts of words in the entire collection (Zhai, 2008). To build a user language model, one naïve way is to first normalize word frequency 𝑐(𝑤, 𝑑) within each document, and then average over all the documents in a user’s document collection. The resulting unigram user language model is: 1 ∑ 𝑐(𝑤, 𝑑) |𝒟𝑢 |𝑑∈𝒟𝑢 |𝑑 |(1) 1 |𝒟𝑢 |∑ 𝑃𝑑(𝑤) 𝑑∈𝒟𝑢 where 𝑃𝑑 (𝑤) is the language model of a particular document, and 𝒟𝑢 is the user’s document collection. This formulation is basically an equalweighted finite mixture model. A simple yet effective way to smooth a language model is to linearly interpolate with a background language model (Chen and Goodman, 1996; Zhai </context>
</contexts>
<marker>Zhai, 2008</marker>
<rawString>ChengXiang Zhai. 2008. Statistical Language Models for Information Retrieval. Now Publishers Inc., Hanover, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to ad hoc information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’01,</booktitle>
<pages>334--342</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6069" citStr="Zhai and Lafferty, 2001" startWordPosition="961" endWordPosition="964">2008). To build a user language model, one naïve way is to first normalize word frequency 𝑐(𝑤, 𝑑) within each document, and then average over all the documents in a user’s document collection. The resulting unigram user language model is: 1 ∑ 𝑐(𝑤, 𝑑) |𝒟𝑢 |𝑑∈𝒟𝑢 |𝑑 |(1) 1 |𝒟𝑢 |∑ 𝑃𝑑(𝑤) 𝑑∈𝒟𝑢 where 𝑃𝑑 (𝑤) is the language model of a particular document, and 𝒟𝑢 is the user’s document collection. This formulation is basically an equalweighted finite mixture model. A simple yet effective way to smooth a language model is to linearly interpolate with a background language model (Chen and Goodman, 1996; Zhai and Lafferty, 2001). In the linear interpolation method, all background documents are treated equally. The entire document collection is added to the user language model 𝑃𝑢 (𝑤) with the same interpolation coefficient. Our main idea is to specify a set of relevant documents for the target user using information embedded in a social network, and enrich the smoothing procedure with these documents. Let 𝒟𝑟𝑒𝑙 denote the content from relevant persons (e.g. social neighbors) of u1, our idea can be concisely expressed as: 𝑃𝑢1 ′ (𝑤) = 𝜆𝑢1𝑃𝑢1(𝑤) + ∑ 𝜆𝑑𝑖𝑃𝑑𝑖(𝑤) 𝑑𝑖∈𝒟𝑟𝑒𝑙 where 𝜆𝑑𝑖 is the mixture weight of the language model o</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2001. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’01, pages 334–342, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>