<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.082741">
<title confidence="0.99823">
CMUQ-Hybrid: Sentiment Classification
By Feature Engineering and Parameter Tuning
</title>
<author confidence="0.9625385">
Kamla Al-Mannai1, Hanan Alshikhabobakr2,
Sabih Bin Wasi2, Rukhsar Neyaz2, Houda Bouamor2, Behrang Mohit2
</author>
<affiliation confidence="0.925026">
Texas A&amp;M University in Qatar1, Carnegie Mellon University in Qatar 2
</affiliation>
<email confidence="0.8298345">
almannaika@hotmail.com1
{halshikh, sabih, rukhsar, hbouamor, behrang}@cmu.edu
</email>
<sectionHeader confidence="0.993805" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971357142857">
This paper describes the system we sub-
mitted to the SemEval-2014 shared task
on sentiment analysis in Twitter. Our sys-
tem is a hybrid combination of two system
developed for a course project at CMU-
Qatar. We use an SVM classifier and cou-
ple a set of features from one system with
feature and parameter optimization frame-
work from the second system. Most of the
tuning and feature selection efforts were
originally aimed at task-A of the shared
task. We achieve an F-score of 84.4% for
task-A and 62.71% for task-B and the sys-
tems are ranked 3rd and 29th respectively.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986421052632">
With the proliferation of Web2.0, people increas-
ingly express and share their opinion through so-
cial media. For instance, microblogging websites
such as Twitter1 are becoming a very popular com-
munication tool. An analysis of this platform re-
veals a large amount of community messages ex-
pressing their opinions and sentiments on differ-
ent topics and aspects of life. This makes Twit-
ter a valuable source of subjective and opinionated
text that could be used in several NLP research
works on sentiment analysis. Many approaches
for detecting subjectivity and determining polarity
of opinions in Twitter have been proposed (Pang
and Lee, 2008; Davidov et al., 2010; Pak and
Paroubek, 2010; Tang et al., 2014). For instance,
the Twitter sentiment analysis shared task (Nakov
et al., 2013) is an interesting testbed to develop
and evaluate sentiment analysis systems on social
media text. Participants are asked to implement
</bodyText>
<footnote confidence="0.9277394">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1http://twitter.com
</footnote>
<bodyText confidence="0.9991305">
a system capable of determining whether a given
tweet expresses positive, negative or neutral sen-
timent. In this paper, we describe the CMUQ-
Hybrid system we developed to participate in the
two subtasks of SemEval 2014 Task 9 (Rosenthal
et al., 2014). Our system uses an SVM classifier
with a rich set of features and a parameter opti-
mization framework.
</bodyText>
<sectionHeader confidence="0.987133" genericHeader="method">
2 Data Preprocessing
</sectionHeader>
<bodyText confidence="0.99993175">
Working with tweets presents several challenges
for NLP, different from those encountered when
dealing with more traditional texts, such as
newswire data. Tweet messages usually contain
different kinds of orthographic and typographical
errors such as the use of special and decorative
characters, letter duplication used generally for
emphasis, word duplication, creative spelling and
punctuation, URLs, #hashtags as well as the use
of slangs and special abbreviations. Hence, before
building our classifier, we start with a preprocess-
ing step on the data, in order to normalize it. All
letters are converted to lower case and all words
are reduced to their root form using the WordNet
Lemmatizer in NLTK2 (Bird et al., 2009). We kept
only some punctuation marks: periods, commas,
semi-colons, and question and exclamation marks.
The excluded characters were identified to be per-
formance boosters using the best-first branch and
bound technique described in Section 3.
</bodyText>
<sectionHeader confidence="0.991691" genericHeader="method">
3 Feature Extraction
</sectionHeader>
<bodyText confidence="0.997707">
Out of a wide variety of features, we selected the
most effective features using the bestfirst branch
and bound method (Neapolitan, 2014), a search
tree technique for solving optimization problems.
We used this technique to determine which punc-
tuation marks to keep in the preprocessing step and
</bodyText>
<footnote confidence="0.9938725">
2http://www.nltk.org/api/nltk.stem.
html
</footnote>
<page confidence="0.917654">
181
</page>
<note confidence="0.736677">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 181–185,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.99957812244898">
in selecting features as well. In the feature selec-
tion step, the root node is represented by a bag of
words feature, referred as textual tokens.
At each level of the tree, we consider a set of
different features, and iteratively we carry out the
following steps: we process the current feature by
generating its successors, which are all the other
features. Then, we rank features according to the
f-score and we only process the best feature and
prune the rest. We pass all the current pruned fea-
tures as successors to the next level of the tree. The
process iterates until all partial solutions in the tree
are processed or terminated. The selected features
are the following:
Sentiment lexicons : we used the Bing Liu Lex-
icon (Hu and Liu, 2004), the MPQA Subjectivity
Lexicon (Wilson et al., 2005), and NRC Hashtag
Sentiment Lexicon (Mohammad et al., 2013). We
count the number of words in each class, result-
ing in three features: (a) positive words count, (b)
negative words count and (c) neutral words count.
Negative presence: presence of negative words
in a term/tweet using a list of negative words. The
list used is built from the Bing Liu Lexicon (Hu
and Liu, 2004).
Textual tokens: the target term/tweet is seg-
mented into tokens based on space. Token identity
features are created and assigned the value of 1.
Overall polarity score: we determine the polar-
ity scores of words in a target term/tweet using the
Sentiment140 Lexicon (Mohammad et al., 2013)
and the SentiWordNet lexicon (Baccianella et al.,
2010). The overall score is computed by adding
up all word scores.
Level of association: indicates whether the
overall polarity score of a term is greater than 0.2
or not. The threshold value was optimized on the
development set.
Sentiment frequency: indicates the most fre-
quent word sentiment in the tweet. We determine
the sentiment of words using an automatically
generated lexicon. The lexicon comprises 3,247
words and their sentiments. Words were obtained
from the provided training set for task-A and sen-
timents were generated using our expression-level
classifier.
We used slightly different features for Task-A
and Task-B. The features extracted for each task
are summarized in Table 1.
</bodyText>
<subsectionHeader confidence="0.559347">
Feature Task A Task B
</subsectionHeader>
<bodyText confidence="0.939046625">
Positive words count ✓
Negative words count ✓
Neutral words count ✓
Negative presence ✓ ✓
Textual tokens ✓ ✓
Overall polarity score ✓ ✓
Level of association ✓
Sentiment frequency ✓
</bodyText>
<tableCaption confidence="0.997521">
Table 1: Feature summary for each task.
</tableCaption>
<sectionHeader confidence="0.954997" genericHeader="method">
4 Modeling Kernel Functions
</sectionHeader>
<bodyText confidence="0.999988657142857">
Initially we experimented with both logistic
regression and the Support Vector Machine
(SVM) (Fan et al., 2008), using the Stochastic
Gradient Descent (SGD) algorithm for parame-
ter optimization. In our development experiments,
SVM outperformed and became our single classi-
fier. We used the LIBSVM package (Chang and
Lin, 2011) to train and test our classifier.
An SVM kernel function and associated param-
eters were optimized for best F-score on the de-
velopment set. In order to avoid the model over-
fitting the data, we select the optimal parameter
value only if there are smooth gaps between the
near neighbors of the corresponded F-score. Oth-
erwise, the search will continue to the second op-
timal value.
In machine learning, the difference between the
number of training samples, m, and the number
of features, n, is crucial in the selection process
of SVM kernel functions. The Gaussian kernel is
suggested when m is slightly larger than n. Other-
wise, the linear kernel is recommended. In Task-
B, the n : m ratio was 1 : 3 indicating a large
difference between the two numbers. Whereas in
Task-A, a ratio of 5 : 2 indicated a small differ-
ence between the two numbers. We selected the
theoretical types, after conducting an experimen-
tal verification to identify the best kernel function
according to the f-score.
We used a radical basis function kernel for the
expression-level task and the value of its gamma
parameter was adjusted to 0.319. Whereas, we
used a linear function kernel for the message-level
task and the value of its cost parameter was ad-
justed to 0.053.
</bodyText>
<page confidence="0.995795">
182
</page>
<sectionHeader confidence="0.994965" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.998857166666667">
In this section, we describe the data and the sev-
eral experiments we conducted for both tasks. We
train and evaluate our classifier with the training,
development and testing datasets provided for the
SemEval 2014 shared task. A short summary of
the data distribution is shown in Table 2.
</bodyText>
<table confidence="0.999931777777778">
Dataset Postive Negative Neutral
Task-A:
Train (9,451) 62% 33% 5%
Dev (1,135) 57% 38% 5%
Test (10,681) 60% 35% 5%
Task-B:
Train (9,684) 38% 15% 47%
Dev (1,654) 35% 21% 44%
Test (5,754) 45% 15% 40%
</table>
<tableCaption confidence="0.996898">
Table 2: Datasets distribution percentage per class.
</tableCaption>
<bodyText confidence="0.999951625">
Our test dataset is composed of five different
sets: The test dataset is composed of five dif-
ferent sets: Twitter2013 a set of tweets collected
for the SemEval2013 test set, Twitter2014, tweets
collected for this years version, LiveJournal2014
consisting of formal tweets, SMS2013, a collection
of sms messages, TwitterSarcasm, a collection of
sarcastic tweets.
</bodyText>
<subsectionHeader confidence="0.986797">
5.1 Task-A
</subsectionHeader>
<bodyText confidence="0.999844725">
For this task, we train our classifier on 10,586
terms (9,451 terms in the training set and 1,135
in the development set), tune it on 4,435 terms,
and evaluate it using 10,681 terms. The average
F-score of the positive and negative classes for
each dataset is given in the first part of Table 3.
The best F-score value of 88.94 is achieved on the
Twitter2013.
We conducted an ablation study illustrated in
the second part of Table 3 shows that all the se-
lected features contribute well in our system per-
formance. Other than the textual tokens feature,
which refers to a bag of preprocessed tokens, the
study highlights the role of the term polarity score
feature: −4.20 in the F-score, when this feature is
not considered on the TwitterSarcasm dataset.
Another study conducted is a feature correlation
analysis, in which we grouped features with sim-
ilar intuitions. Namely the two features negative
presence and negative words count are grouped
as “negative features”, and the features positive
words count and negative words count are grouped
as “words count”. We show in Table 4 the effect
on f-score after removing each group from the fea-
tures set. Also we show the f-score after remov-
ing each individual feature within the group. This
helps us see whether features within a group are
redundant or not. For the Twitter2014 dataset, we
notice that excluding one of the features in any of
the two groups leads to a significant drop, in com-
parison to the total drop by its group. The uncor-
related contributions of features within the same
group indicate that features are not redundant to
each other and that they are indeed capturing dif-
ferent information. However, in the case of the
TwitterSarcasm dataset, we observe that the neg-
ative presence feature is not only not contributing
to the system performance but also adding noise
to the feature space, specifically, to the negative
words count feature.
</bodyText>
<subsectionHeader confidence="0.997269">
5.2 Task-B
</subsectionHeader>
<bodyText confidence="0.999949133333333">
For this task, we trained our classifier on 11,338
tweets (9,684 terms in the training set and 1,654
in the development set), tuned it on 3,813 tweets,
and evaluated it using 8,987 tweets. Results for
different feature configurations are reported in Ta-
ble 5.
It is important to note that if we exclude the tex-
tual tokens feature, all datasets benefit the most
from the polarity score feature. It is interesting to
note that the bag of words, referred to as textual
tokens, is not helping in one of the datasets, the
TwitterSarcasm set. For all datasets, performance
could be improved by removing different features.
In Table 5, we observe that the Negative pres-
ence feature decreases the F-score on the Twitter-
Sarcasm dataset. This could be explained by the
fact that negative words do not usually appear in
a negative implication in sarcastic messages. For
example, this tweet: Such a fun Saturday catch-
ing up on hw. which has a negative sentiment, is
classified positive because of the absence of neg-
ative words. Table 5 shows that the textual tokens
feature increases the classifier’s performance up to
+21.07 for some datasets. However, using a large
number of features in comparison to the number
of training samples could increase data sparseness
and lower the classifier’s performance.
We conducted a post-competition experiment to
examine the relationship between the number of
features and the number of training samples. We
</bodyText>
<page confidence="0.997946">
183
</page>
<table confidence="0.990153">
Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013
F-score 84.40 76.99 84.21 88.94 87.98
Negative presence -0.45 0.00 -0.45 -0.23 +0.30
Positive words count -0.52 -1.37 -0.11 -0.02 +0.38
Negative words count -0.50 -2.20 -0.61 -0.47 -1.66
Polarity score -1.83 -4.20 -0.23 -2.14 -3.00
Level of association -0.18 0.00 -0.18 -0.07 +0.57
Textual tokens -8.74 -2.40 -3.02 -4.37 -6.06
</table>
<tableCaption confidence="0.9845585">
Table 3: Task-A feature ablation study. F-scores calculated on each set along with the effect when
removing one feature at a time.
</tableCaption>
<table confidence="0.9964465">
Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013
F-score 84.40 76.99 84.21 88.94 87.98
Negative features -1.53 -0.84 -3.05 -1.88 -0.67
Negative presence -0.45 0.00 -0.45 -0.23 +0.3
Negative words count -0.50 -2.20 -0.61 -0.47 -1.66
Words count -1.07 -2.2 -0.79 -0.62 -2.01
Positive words count -0.52 -1.37 -0.11 -0.02 +0.38
Negative words count -0.50 -2.20 -0.61 -0.47 -1.66
</table>
<tableCaption confidence="0.9923005">
Table 4: Task-A features correlation analysis. We grouped features with similar intuitions and we calcu-
lated F-scores on each set along with the effect when removing one feature at a time.
</tableCaption>
<bodyText confidence="0.999909777777778">
fixed the size of our training dataset. Then, we
compared the performance of our classifier using
only the bag of tokens feature, in two different
sizes. In the first experiment, we included all to-
kens collected from all tweets. In the second, we
only considered the top 20 ranked tokens from
each tweet. Tokens were ranked according to the
difference between their highest level of associa-
tion into one of the sentiments and the sum of the
rest. The level of associations for tokens were de-
termined using the Sentiment140 and SentiWord-
Net lexicons. The threshold number of tokens was
identified empirically for best performance. We
found that the classifier’s performance has been
improved by 2 f-score points when the size of to-
kens bag is smaller. The experiment indicates that
the contribution of the bag of words feature can be
increased by reducing the size of vocabulary list.
</bodyText>
<sectionHeader confidence="0.998574" genericHeader="method">
6 Error Analysis
</sectionHeader>
<bodyText confidence="0.99992">
Our efforts are mostly tuned towards task-A,
hence our inspection and analysis is focused on
task-A. The error rate calculated per sentiment
class: positive, negative and neutral are 6.8%,
14.9% and 93.8%, respectively. The highest error
rate in the neutral class, 93.8%, is mainly due to
the few neutral examples in the training data (only
5% of the data). Hence the system could not learn
from such a small set of neutral class examples.
In the case of negative class error rate, 14.9%,
most of which were classified as positive. An ex-
ample of such classification: I knew it was too
good to be true OTL. Since our system highly re-
lies on lexicon, hence looking at lexicon assigned
polarity to the phrase too good to be true which is
positive, happens because the positive words good
and true has dominating positive polarity.
Lastly for the positive error rate, which is rel-
atively lower, 6%, most of which were classified
negative instead of positive. An example of such
classification: Looks like we’re getting the heavi-
est snowfall in five years tomorrow. Awesome. I’ll
never get tired of winter. Although the phrase car-
ries a positive sentiment, the individual negative
words of the phrase never and tired again domi-
nates over the phrase.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999994333333333">
We described our systems for Twitter Sentiment
Analysis shared task. We participated in both
tasks, but were mostly focused on task-A. Our hy-
brid system was assembled by integrating a rich
set of lexical features into a framework of fea-
ture selection and parameter tuning, The polarity
</bodyText>
<page confidence="0.995982">
184
</page>
<table confidence="0.985275">
Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013
F-score 62.71 40.95 65.14 63.22 61.75
Negative presence -1.65 +1.26 -3.37 -3.66 -0.95
Neutral words count +0.05 0.00 -0.72 -0.57 -0.54
Polarity score -4.03 -6.92 -3.82 -3.83 -4.84
Sentiment frequency +0.10 0.00 +0.18 -0.12 -0.05
textual tokens -17.91 +6.5 -21.07 -19.97 -15.8
</table>
<tableCaption confidence="0.987546">
Table 5: Task B feature ablation study. F-scores calculated on each set along with the effect when
removing one feature at a time.
</tableCaption>
<bodyText confidence="0.999551166666667">
score feature was the most important feature for
our model in both tasks. The F-score results were
consistent across all datasets, except the Twitter-
Sarcasm dataset. It indicates that feature selection
and parameter tuning steps were effective in gen-
eralizing the model to unseen data.
</bodyText>
<sectionHeader confidence="0.970931" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999910666666667">
We would like to thank Kemal Oflazer and also the
shared task organizers for their support throughout
this work.
</bodyText>
<sectionHeader confidence="0.998118" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999784492957746">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh conference
on International Language Resources and Evalua-
tion (LREC’10), pages 2200–2204, Valletta, Malta.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media, Inc.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology, 2:27:1–27:27.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 241–249, Uppsala, Sweden.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Minqing Hu and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 168–
177, Seattle, WA, USA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-
the-Art in Sentiment Analysis of Tweets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321–327, Atlanta, Geor-
gia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312–
320, Atlanta, Georgia, USA.
Richard E. Neapolitan, 2014. Foundations of Algo-
rithms, pages 257–262. Jones &amp; Bartlett Learning.
Alexander Pak and Patrick Paroubek. 2010. Twitter
Based System: Using Twitter for Disambiguating
Sentiment Ambiguous Adjectives. In Proceedings
of the 5th International Workshop on Semantic Eval-
uation, pages 436–439, Uppsala, Sweden.
Bo Pang and Lillian Lee, 2008. Opinion Mining and
Sentiment Analysis, volume 2, pages 1–135. Now
Publishers Inc.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Proceedings of the
Eighth International Workshop on Semantic Evalu-
ation (SemEval’14), Dublin, Ireland.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning Sentiment-
Specific Word Embedding for Twitter Sentiment
Classification. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1555–1565, Baltimore, Maryland.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of the
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 347–354, Vancouver, B.C.,
Canada.
</reference>
<page confidence="0.998847">
185
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.515057">
<title confidence="0.977749">CMUQ-Hybrid: Sentiment By Feature Engineering and Parameter Tuning</title>
<author confidence="0.803187">Hanan Bin Rukhsar Houda Behrang</author>
<affiliation confidence="0.935636">A&amp;M University in Carnegie Mellon University in Qatar</affiliation>
<email confidence="0.961554">sabih,rukhsar,hbouamor,</email>
<abstract confidence="0.9954636">This paper describes the system we submitted to the SemEval-2014 shared task on sentiment analysis in Twitter. Our system is a hybrid combination of two system developed for a course project at CMU- Qatar. We use an SVM classifier and couple a set of features from one system with feature and parameter optimization framework from the second system. Most of the tuning and feature selection efforts were originally aimed at task-A of the shared task. We achieve an F-score of 84.4% for task-A and 62.71% for task-B and the systems are ranked 3rd and 29th respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<pages>2200--2204</pages>
<location>Valletta,</location>
<contexts>
<context position="5441" citStr="Baccianella et al., 2010" startWordPosition="854" endWordPosition="857">ch class, resulting in three features: (a) positive words count, (b) negative words count and (c) neutral words count. Negative presence: presence of negative words in a term/tweet using a list of negative words. The list used is built from the Bing Liu Lexicon (Hu and Liu, 2004). Textual tokens: the target term/tweet is segmented into tokens based on space. Token identity features are created and assigned the value of 1. Overall polarity score: we determine the polarity scores of words in a target term/tweet using the Sentiment140 Lexicon (Mohammad et al., 2013) and the SentiWordNet lexicon (Baccianella et al., 2010). The overall score is computed by adding up all word scores. Level of association: indicates whether the overall polarity score of a term is greater than 0.2 or not. The threshold value was optimized on the development set. Sentiment frequency: indicates the most frequent word sentiment in the tweet. We determine the sentiment of words using an automatically generated lexicon. The lexicon comprises 3,247 words and their sentiments. Words were obtained from the provided training set for task-A and sentiments were generated using our expression-level classifier. We used slightly different featu</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), pages 2200–2204, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python.</booktitle>
<publisher>O’Reilly Media, Inc.</publisher>
<contexts>
<context position="3177" citStr="Bird et al., 2009" startWordPosition="488" endWordPosition="491"> more traditional texts, such as newswire data. Tweet messages usually contain different kinds of orthographic and typographical errors such as the use of special and decorative characters, letter duplication used generally for emphasis, word duplication, creative spelling and punctuation, URLs, #hashtags as well as the use of slangs and special abbreviations. Hence, before building our classifier, we start with a preprocessing step on the data, in order to normalize it. All letters are converted to lower case and all words are reduced to their root form using the WordNet Lemmatizer in NLTK2 (Bird et al., 2009). We kept only some punctuation marks: periods, commas, semi-colons, and question and exclamation marks. The excluded characters were identified to be performance boosters using the best-first branch and bound technique described in Section 3. 3 Feature Extraction Out of a wide variety of features, we selected the most effective features using the bestfirst branch and bound method (Neapolitan, 2014), a search tree technique for solving optimization problems. We used this technique to determine which punctuation marks to keep in the preprocessing step and 2http://www.nltk.org/api/nltk.stem. htm</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A Library for Support Vector Machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology,</booktitle>
<pages>2--27</pages>
<contexts>
<context position="6729" citStr="Chang and Lin, 2011" startWordPosition="1059" endWordPosition="1062">summarized in Table 1. Feature Task A Task B Positive words count ✓ Negative words count ✓ Neutral words count ✓ Negative presence ✓ ✓ Textual tokens ✓ ✓ Overall polarity score ✓ ✓ Level of association ✓ Sentiment frequency ✓ Table 1: Feature summary for each task. 4 Modeling Kernel Functions Initially we experimented with both logistic regression and the Support Vector Machine (SVM) (Fan et al., 2008), using the Stochastic Gradient Descent (SGD) algorithm for parameter optimization. In our development experiments, SVM outperformed and became our single classifier. We used the LIBSVM package (Chang and Lin, 2011) to train and test our classifier. An SVM kernel function and associated parameters were optimized for best F-score on the development set. In order to avoid the model overfitting the data, we select the optimal parameter value only if there are smooth gaps between the near neighbors of the corresponded F-score. Otherwise, the search will continue to the second optimal value. In machine learning, the difference between the number of training samples, m, and the number of features, n, is crucial in the selection process of SVM kernel functions. The Gaussian kernel is suggested when m is slightl</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A Library for Support Vector Machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>241--249</pages>
<location>Uppsala,</location>
<contexts>
<context position="1587" citStr="Davidov et al., 2010" startWordPosition="245" endWordPosition="248">increasingly express and share their opinion through social media. For instance, microblogging websites such as Twitter1 are becoming a very popular communication tool. An analysis of this platform reveals a large amount of community messages expressing their opinions and sentiments on different topics and aspects of life. This makes Twitter a valuable source of subjective and opinionated text that could be used in several NLP research works on sentiment analysis. Many approaches for detecting subjectivity and determining polarity of opinions in Twitter have been proposed (Pang and Lee, 2008; Davidov et al., 2010; Pak and Paroubek, 2010; Tang et al., 2014). For instance, the Twitter sentiment analysis shared task (Nakov et al., 2013) is an interesting testbed to develop and evaluate sentiment analysis systems on social media text. Participants are asked to implement This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://twitter.com a system capable of determining whether a given tweet expresses positive, negative or neutral sentiment. In t</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 241–249, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="6514" citStr="Fan et al., 2008" startWordPosition="1027" endWordPosition="1030"> from the provided training set for task-A and sentiments were generated using our expression-level classifier. We used slightly different features for Task-A and Task-B. The features extracted for each task are summarized in Table 1. Feature Task A Task B Positive words count ✓ Negative words count ✓ Neutral words count ✓ Negative presence ✓ ✓ Textual tokens ✓ ✓ Overall polarity score ✓ ✓ Level of association ✓ Sentiment frequency ✓ Table 1: Feature summary for each task. 4 Modeling Kernel Functions Initially we experimented with both logistic regression and the Support Vector Machine (SVM) (Fan et al., 2008), using the Stochastic Gradient Descent (SGD) algorithm for parameter optimization. In our development experiments, SVM outperformed and became our single classifier. We used the LIBSVM package (Chang and Lin, 2011) to train and test our classifier. An SVM kernel function and associated parameters were optimized for best F-score on the development set. In order to avoid the model overfitting the data, we select the optimal parameter value only if there are smooth gaps between the near neighbors of the corresponded F-score. Otherwise, the search will continue to the second optimal value. In mac</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and Summarizing Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="4668" citStr="Hu and Liu, 2004" startWordPosition="726" endWordPosition="729">extual tokens. At each level of the tree, we consider a set of different features, and iteratively we carry out the following steps: we process the current feature by generating its successors, which are all the other features. Then, we rank features according to the f-score and we only process the best feature and prune the rest. We pass all the current pruned features as successors to the next level of the tree. The process iterates until all partial solutions in the tree are processed or terminated. The selected features are the following: Sentiment lexicons : we used the Bing Liu Lexicon (Hu and Liu, 2004), the MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013). We count the number of words in each class, resulting in three features: (a) positive words count, (b) negative words count and (c) neutral words count. Negative presence: presence of negative words in a term/tweet using a list of negative words. The list used is built from the Bing Liu Lexicon (Hu and Liu, 2004). Textual tokens: the target term/tweet is segmented into tokens based on space. Token identity features are created and assigned the value of 1. Overall polarity score: we</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and Summarizing Customer Reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168– 177, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the State-ofthe-Art in Sentiment Analysis of Tweets.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>321--327</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="4780" citStr="Mohammad et al., 2013" startWordPosition="743" endWordPosition="746"> out the following steps: we process the current feature by generating its successors, which are all the other features. Then, we rank features according to the f-score and we only process the best feature and prune the rest. We pass all the current pruned features as successors to the next level of the tree. The process iterates until all partial solutions in the tree are processed or terminated. The selected features are the following: Sentiment lexicons : we used the Bing Liu Lexicon (Hu and Liu, 2004), the MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013). We count the number of words in each class, resulting in three features: (a) positive words count, (b) negative words count and (c) neutral words count. Negative presence: presence of negative words in a term/tweet using a list of negative words. The list used is built from the Bing Liu Lexicon (Hu and Liu, 2004). Textual tokens: the target term/tweet is segmented into tokens based on space. Token identity features are created and assigned the value of 1. Overall polarity score: we determine the polarity scores of words in a target term/tweet using the Sentiment140 Lexicon (Mohammad et al., </context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the State-ofthe-Art in Sentiment Analysis of Tweets. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 321–327, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>SemEval-2013 Task 2: Sentiment Analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="1710" citStr="Nakov et al., 2013" startWordPosition="265" endWordPosition="268"> becoming a very popular communication tool. An analysis of this platform reveals a large amount of community messages expressing their opinions and sentiments on different topics and aspects of life. This makes Twitter a valuable source of subjective and opinionated text that could be used in several NLP research works on sentiment analysis. Many approaches for detecting subjectivity and determining polarity of opinions in Twitter have been proposed (Pang and Lee, 2008; Davidov et al., 2010; Pak and Paroubek, 2010; Tang et al., 2014). For instance, the Twitter sentiment analysis shared task (Nakov et al., 2013) is an interesting testbed to develop and evaluate sentiment analysis systems on social media text. Participants are asked to implement This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://twitter.com a system capable of determining whether a given tweet expresses positive, negative or neutral sentiment. In this paper, we describe the CMUQHybrid system we developed to participate in the two subtasks of SemEval 2014 Task 9 (Rosent</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. SemEval-2013 Task 2: Sentiment Analysis in Twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312– 320, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard E Neapolitan</author>
</authors>
<title>Foundations of Algorithms,</title>
<date>2014</date>
<pages>257--262</pages>
<publisher>Jones &amp; Bartlett Learning.</publisher>
<contexts>
<context position="3579" citStr="Neapolitan, 2014" startWordPosition="550" endWordPosition="551">e start with a preprocessing step on the data, in order to normalize it. All letters are converted to lower case and all words are reduced to their root form using the WordNet Lemmatizer in NLTK2 (Bird et al., 2009). We kept only some punctuation marks: periods, commas, semi-colons, and question and exclamation marks. The excluded characters were identified to be performance boosters using the best-first branch and bound technique described in Section 3. 3 Feature Extraction Out of a wide variety of features, we selected the most effective features using the bestfirst branch and bound method (Neapolitan, 2014), a search tree technique for solving optimization problems. We used this technique to determine which punctuation marks to keep in the preprocessing step and 2http://www.nltk.org/api/nltk.stem. html 181 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 181–185, Dublin, Ireland, August 23-24, 2014. in selecting features as well. In the feature selection step, the root node is represented by a bag of words feature, referred as textual tokens. At each level of the tree, we consider a set of different features, and iteratively we carry out the following st</context>
</contexts>
<marker>Neapolitan, 2014</marker>
<rawString>Richard E. Neapolitan, 2014. Foundations of Algorithms, pages 257–262. Jones &amp; Bartlett Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter Based System: Using Twitter for Disambiguating Sentiment Ambiguous Adjectives.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>436--439</pages>
<location>Uppsala,</location>
<contexts>
<context position="1611" citStr="Pak and Paroubek, 2010" startWordPosition="249" endWordPosition="252">nd share their opinion through social media. For instance, microblogging websites such as Twitter1 are becoming a very popular communication tool. An analysis of this platform reveals a large amount of community messages expressing their opinions and sentiments on different topics and aspects of life. This makes Twitter a valuable source of subjective and opinionated text that could be used in several NLP research works on sentiment analysis. Many approaches for detecting subjectivity and determining polarity of opinions in Twitter have been proposed (Pang and Lee, 2008; Davidov et al., 2010; Pak and Paroubek, 2010; Tang et al., 2014). For instance, the Twitter sentiment analysis shared task (Nakov et al., 2013) is an interesting testbed to develop and evaluate sentiment analysis systems on social media text. Participants are asked to implement This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://twitter.com a system capable of determining whether a given tweet expresses positive, negative or neutral sentiment. In this paper, we describe t</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter Based System: Using Twitter for Disambiguating Sentiment Ambiguous Adjectives. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 436–439, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis,</title>
<date>2008</date>
<volume>2</volume>
<pages>1--135</pages>
<publisher>Now Publishers Inc.</publisher>
<contexts>
<context position="1565" citStr="Pang and Lee, 2008" startWordPosition="241" endWordPosition="244">n of Web2.0, people increasingly express and share their opinion through social media. For instance, microblogging websites such as Twitter1 are becoming a very popular communication tool. An analysis of this platform reveals a large amount of community messages expressing their opinions and sentiments on different topics and aspects of life. This makes Twitter a valuable source of subjective and opinionated text that could be used in several NLP research works on sentiment analysis. Many approaches for detecting subjectivity and determining polarity of opinions in Twitter have been proposed (Pang and Lee, 2008; Davidov et al., 2010; Pak and Paroubek, 2010; Tang et al., 2014). For instance, the Twitter sentiment analysis shared task (Nakov et al., 2013) is an interesting testbed to develop and evaluate sentiment analysis systems on social media text. Participants are asked to implement This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://twitter.com a system capable of determining whether a given tweet expresses positive, negative or n</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee, 2008. Opinion Mining and Sentiment Analysis, volume 2, pages 1–135. Now Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>SemEval-2014 Task 9: Sentiment Analysis in Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighth International Workshop on Semantic Evaluation (SemEval’14),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2327" citStr="Rosenthal et al., 2014" startWordPosition="355" endWordPosition="358"> 2013) is an interesting testbed to develop and evaluate sentiment analysis systems on social media text. Participants are asked to implement This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://twitter.com a system capable of determining whether a given tweet expresses positive, negative or neutral sentiment. In this paper, we describe the CMUQHybrid system we developed to participate in the two subtasks of SemEval 2014 Task 9 (Rosenthal et al., 2014). Our system uses an SVM classifier with a rich set of features and a parameter optimization framework. 2 Data Preprocessing Working with tweets presents several challenges for NLP, different from those encountered when dealing with more traditional texts, such as newswire data. Tweet messages usually contain different kinds of orthographic and typographical errors such as the use of special and decorative characters, letter duplication used generally for emphasis, word duplication, creative spelling and punctuation, URLs, #hashtags as well as the use of slangs and special abbreviations. Hence</context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanov. 2014. SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Proceedings of the Eighth International Workshop on Semantic Evaluation (SemEval’14), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning SentimentSpecific Word Embedding for Twitter Sentiment Classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1555--1565</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="1631" citStr="Tang et al., 2014" startWordPosition="253" endWordPosition="256">hrough social media. For instance, microblogging websites such as Twitter1 are becoming a very popular communication tool. An analysis of this platform reveals a large amount of community messages expressing their opinions and sentiments on different topics and aspects of life. This makes Twitter a valuable source of subjective and opinionated text that could be used in several NLP research works on sentiment analysis. Many approaches for detecting subjectivity and determining polarity of opinions in Twitter have been proposed (Pang and Lee, 2008; Davidov et al., 2010; Pak and Paroubek, 2010; Tang et al., 2014). For instance, the Twitter sentiment analysis shared task (Nakov et al., 2013) is an interesting testbed to develop and evaluate sentiment analysis systems on social media text. Participants are asked to implement This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://twitter.com a system capable of determining whether a given tweet expresses positive, negative or neutral sentiment. In this paper, we describe the CMUQHybrid system</context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning SentimentSpecific Word Embedding for Twitter Sentiment Classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555–1565, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in Phraselevel Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="4721" citStr="Wilson et al., 2005" startWordPosition="734" endWordPosition="737">der a set of different features, and iteratively we carry out the following steps: we process the current feature by generating its successors, which are all the other features. Then, we rank features according to the f-score and we only process the best feature and prune the rest. We pass all the current pruned features as successors to the next level of the tree. The process iterates until all partial solutions in the tree are processed or terminated. The selected features are the following: Sentiment lexicons : we used the Bing Liu Lexicon (Hu and Liu, 2004), the MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013). We count the number of words in each class, resulting in three features: (a) positive words count, (b) negative words count and (c) neutral words count. Negative presence: presence of negative words in a term/tweet using a list of negative words. The list used is built from the Bing Liu Lexicon (Hu and Liu, 2004). Textual tokens: the target term/tweet is segmented into tokens based on space. Token identity features are created and assigned the value of 1. Overall polarity score: we determine the polarity scores of words in a target t</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in Phraselevel Sentiment Analysis. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 347–354, Vancouver, B.C., Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>