<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017031">
<title confidence="0.98604">
Topic Indexing and Retrieval for Factoid QA
</title>
<author confidence="0.994966">
Kisuh Ahn
</author>
<affiliation confidence="0.906245333333333">
School of Informatics
University of Edinburgh
Edinburgh, UK
</affiliation>
<email confidence="0.994204">
k.ahn@sms.ed.ac.uk
</email>
<author confidence="0.992269">
Bonnie Webber
</author>
<affiliation confidence="0.905836666666667">
School of Informatics
University of Edinburgh
Edinburgh, UK
</affiliation>
<email confidence="0.996821">
bonnie@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999208916666667">
The method of Topic Indexing and Re-
trieval for QA persented in this paper
enables fast and efficent QA for ques-
tions with named entity answers. This is
achieved by identifying all possible named
entity answers in a corpus off-line and
gathering all possible evidence for their di-
rect retrieval as answer candidates using
standard IR techniques. An evaluation of
this method on 377 TREC questions pro-
duced a score of 0.342 in Accuracy and
0.413 in Mean Reciprocal Rank (MRR).
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999699315789474">
Many textual QA systems use Information
Retrieval to retrieve a subset of the docu-
ments/passages from the source corpus in order to
reduce the amount of text that needs to be inves-
tigated in finding the correct answers. This use
of Information Retrieval (IR) plays an important
role, since it imposes an upper bound on the per-
formance of the entire QA system: Subsequent an-
swer extraction operations cannot make up for the
failure of IR to fetch text that contains correct an-
swers. Several techniques have been developed to
cut down the amount of text that must be retrieved
in order to ensure against the loss of answer mate-
rial, but processing any text for downstream oper-
ations still takes up valuable on-line time.
In this paper, we present a method, Topic Index-
ing and Retrieval for QA, that turns factoid Ques-
tion Answering into fine-grained Information Re-
trieval, where answer candidates are directly re-
</bodyText>
<footnote confidence="0.9031125">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.99663775">
trieved instead of documents/passages. The pri-
mary claim here is that for simple named entity
answers, this can make for fast and accurate re-
trieval.
</bodyText>
<sectionHeader confidence="0.970827" genericHeader="method">
2 The Overall Idea
</sectionHeader>
<bodyText confidence="0.999952875">
The answers to many factoid questions are named
entities — eg, “Who is the president of India?”,
“Where was Eric Clapton born?”, etc. The basic
idea of this paper’s central method, Topic Indexing
and Retrieval for Question Answering (or TOQA
subsequently), is to extract such expressions off-
line from a textual corpus as potential answers and
gather evidence that supports their direct retrieval
as answers to questions using off-the-shelf IR.
Central here is the notion of topics. Under
this method, any named entities (proper names)
found in a corpus are regarded as potential an-
swers. However, named entities are not just treated
as words or phrases but as topics with three kinds
of information useful for Question Answering.
First, as a locus of information, a topic has tex-
tual content which talks about this topic. This
comprises the set of all sentences from the cor-
pus that mention this topic. Textual content is im-
portant because it provides the means to judge the
topic’s suitability as an answer to a question via
textual similarity between the question and some
part of the topic’s textual content.
Second, a topic has an ontological type (or
types). This type information is very important for
QA because the question requires the answer to be
of certain type. A topic must be of the same type
(or some compatible type via ISA relation) in or-
der to be considered as an answer candidate. For
example, the question, “Who is the president of In-
dia?” requires the answer to be of type PERSON
(or more specifically, PRESIDENT).
</bodyText>
<page confidence="0.918295">
66
</page>
<note confidence="0.998452">
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 66–73
Manchester, UK. August 2008
</note>
<bodyText confidence="0.9997665625">
Finally, a topic has relations to other topics. For
example, the topic, “Dolly the sheep”, is closely
related to the topic, “Ian Wilmut”. While the pre-
cise nature of this relation may vary, the frequent
co-occurence of two topics in sentences can be re-
garded as an evidence that the two are related. Re-
lated topics are useful for question answering be-
cause they reduce the search space. For exam-
ple, the answer to the question, e.g. “Who cre-
ated Dolly the sheep?” can be found among all the
topics that are related to the topic contained in the
question (or question topic), e.g. “Dolly” here.
These three kinds of information are the base
material for Question Answering using topics:
they provide the means to directly retrieve answers
to questions.
</bodyText>
<sectionHeader confidence="0.994602" genericHeader="method">
3 Preprocessing
</sectionHeader>
<bodyText confidence="0.999985727272727">
This section describes the technical details of how
to collect these three kinds of information used for
topic based QA, and how to process and store them
off-line in order to enable fast and efficient on-
line question answering. The stored material con-
sists of (1) a Topic Repository, which stores topics
with their variant names and ontological types, (2)
a topic document collection that stores the textual
content of topics, and (3) a set of indices created
by indexing the topic document collection for fast
and efficient retrieval.
</bodyText>
<subsectionHeader confidence="0.999036">
3.1 The Make Up of Topic Repository
</subsectionHeader>
<bodyText confidence="0.999883567164179">
The Topic Repository stores topics, along their
variant names and their ontological types, in hash
tables for fast look-up. Building a topic repos-
itory requires identifying topics within the given
corpus. For this we have used the C&amp;C named en-
tity recogniser (Curran and Clark, 2003), which is
run on pos-tagged and chunked documents in the
corpus to identify and extract named entities as po-
tential topics. This also identifies the base type of a
subset of named entities as PERSON, LOCATION
and ORGANISATION. This is stored for later use
in building type-separated indices. When a named
entity is identified, we first check whether it repre-
sents a topic already found in the topic repository.
This is done by checking the topic-name hash ta-
ble in the repository, which serves as the main data
storage for the variant names of topics.
To resolve a target named entity to the appro-
priate topic, we use Wikipedia’s Redirect table,
which contains many common variant names for
the same topic. The topic-name hash table is up-
dated accordingly. Hash table entries consist of
pairs like (‘George Clooney’, 1745442), where the
name ‘George Clooney’ is one of the names that
belong to the unique topic with the ID number of
1745442. We currently do nothing to disambiguate
topics, so different individuals with the same name
will all be considered the same topic.
Fine-grained ontological types of topics are
identified and stored as well in a separate topic-
type table. In order to discover fine-grained
topic types, the ontology database Yago is used
(Suchanek et al., 2007). Yago contains such infor-
mation for Wikipedia topics, derived by mapping
the category information about target topic sup-
plied by a Wikipedia user to the appropriate Word-
Net concept. (Wikipedia categories are not consis-
tent and uniform, and they are more like tags that
characterise a topic rather than strictly classify it.)
Using this ontology to look up the type(s) of each
topic-type (i.e. the corresponding WordNet con-
cept) and by tracing up the WordNet concept hier-
archy, we created a fine-grained, multi-level (with
respect to ISA) topic-type hash table for all the top-
ics in the topic repository.
The topic-type hash table not only contains the
ontological type of a topic, but also a significant
amount of world knowledge typically associated to
the topic, due to the nature of Wikipedia categories
as descriptive tags. For example, ‘Bill Gates’ is
identified as ‘CEO’ (a title-role), and ‘Pusan’ as
‘a province of Korea’ (geographical knowledge).
Such diverse and significant knowledge, as well as
the breadth and the depth of the fine types con-
tained in the topic-type hash table, enable a very
powerful match between the answer type from a
question to that of a candidate topic.
The set of fine-grained answer types used here
differs from the set of answer types such as Li and
Roth (2002) used elsewhere in that the set is open-
ended, and new types can be added for an entity at
any time.
The topic repository is used in re-ranking an-
swer candidates by the fine-grained anwer type
and for question topic identification, as well as in
building topic document collection to be explained
next.
</bodyText>
<subsectionHeader confidence="0.995668">
3.2 The Topic Document Collection
</subsectionHeader>
<bodyText confidence="0.997832">
As noted, the textual content of a topic is the set of
all sentences in a corpus that mentions this topic.
</bodyText>
<page confidence="0.998984">
67
</page>
<bodyText confidence="0.931117727272727">
(Since anaphora resolution is not yet performed,
the sentences that only mention a particular topic
anaphorically are missed.) Such set of sentences is
assembled into one file per topic. This can then
be regarded as a document on its own with the
topic name as its title. We henceforth call such
a document, a topic document. Figure 1 illustrates
a topic document for the topic, Dolly the sheep.
The topic document collection thus created for all
topics identified can be regarded as a reorganised
corpus with respect to the original corpus as the
</bodyText>
<figureCaption confidence="0.934419">
Figure ?? illustrates.
Figure 1: An Example Topic Document: Dolly the
sheep
</figureCaption>
<bodyText confidence="0.999901459459459">
The topic document collection for the full set of
topics is a subset of the original corpus, reorga-
nized around topics. The process of creating the
topic document collection (which we refer to as
the topic document method) is actually performed
at the same time as the creation of Topic Reposi-
tory. Any sentence that contains identifiable topics
is appended to the topic document of each topic
it contains. The topic document collection so cre-
ated is central to our Question Answering because
retrieving a topic document (specifically, its topic)
equates to generating an answer candidate for a
given question. Hence, via topic documents, fine-
grained IR can be used to retrieve answers directly.
In order to facilitate such retrieval, however, a
topic document collection needs to be indexed.
In our implemented system (described in Section
5), this is done using the indexing module of the
Lemur Toolkit. For type specific retrieval, three
separate indices corresponding to PERSON, LO-
CATION and ORGANISATION are created ac-
cording to the base types of the topics identified
at the time of Named Entity Recognition. In ad-
dition, an index for all topic documents regardless
of types, TOTAL, is also created for questions from
which the answer type cannot be determined or for
which their answer types differ from the three base
types. Of note here is that separate indices are
created only for these base types, as we have not
explored separate indexing by fine-grained answer
types. These fine-types are only used for reranking
after the candidate topics have been retrieved from
the base indices.
At the time of retrieval, an appropriate index is
to be chosen depending on the answer type iden-
tified from the question. This is discussed in the
next section.
</bodyText>
<sectionHeader confidence="0.888061" genericHeader="method">
4 Topic Retrieval and Reranking for QA
</sectionHeader>
<bodyText confidence="0.999977875">
The goal is to retrieve a ranked list of topic doc-
uments (indicated by their topics) as answers to a
given question. In order to do this, the query for
the IR operation must be formulated from the ques-
tion, and the specific answer type must be identi-
fied both for retrieval and for any re-ranking of the
retrieved list of topics.
Thus, the first necessary operation is Question
Analysis. Question Analysis identifies the ques-
tion type (eg, definition question, factoid question,
list question, etc); the answer type, and the ques-
tion topics (if any) and produces a shallow parse
of the question text (pos-tagged and chunked) for
query formulation. (Identifying the question type
is a formality since the method only deals with fac-
toid questions.)
The question topic identification is straightfor-
ward: Any proper name present in a particular
question is a question topic. For answer type
identification, we use a simple rule based algo-
rithm that looks at the WH-word (e.g. “Where”
means location), the head noun of a WH-phrase
with “Which” or “What” (e.g. “Which president”
means the answer type is of president), and if the
main verb is a copula, the head of the post-copula
noun phrase (e.g. for “Who is the president ..”,
here again “president” is the answer type.) Word-
Net is used to identify the base type of the an-
swer type identified from the question when it is
not one of the base types (PERSON, LOCATION,
ORGANISATION). For example, “president” is
traced to its base type, “PERSON”.
</bodyText>
<page confidence="0.996703">
68
</page>
<bodyText confidence="0.995915722222222">
Next is the retrieval of topics as answer candi-
dates for a given question. This involves: (1) iden-
tifying the appropriate index, (2) formulating the
query, and (3) the actual retrieval operation. An ap-
propriate index is chosen based on the base answer
type. For example, for the question, “Who is the
president of Germany?”, the answer type is iden-
tified as ‘president’. But since the answer type,
‘president’, is not the base type, WordNet is used
to trace from ’president’ to a base type (PERSON)
and the corresponding index is selected (because
separate indices exist only for base types). If none
of the three base types is found by this process, the
total index is used.
Retrieval uses the InQuery retrieval system
within the Lemur Tool Kit (Ogilvie and Callan,
2002). InQuery supports a powerful and flexible
structured query language. Taking advantage of
this, a structured query is formulated from the tar-
get question. So for example, the parsed form of
the question, “Who is the president of Germany?”
is used to generate the following query
\sum(is president of germany
\phrase(president of germany)).
In this example, “president of germany” forms a
phrase, and it is inserted as part of the query el-
ement with the ‘\phrase’ operator. However, the
individual keywords are also included as bag of
words since we have found it to give better perfor-
mance in the trials that we have run. The overall
operator is then enclosed by the ‘\sum’ operator
that gives the overall score of the query with re-
spect to a document. With this query, search is
performed and a ranked list of topics is retrieved.
This ranked list is then run through the following
operations:
</bodyText>
<listItem confidence="0.983582142857143">
1. Filtering the retrieved list of topics to remove
question topics if present.
2. Re-ranking with respect to topic type, prefer-
ring the topic that matches the fine answer
type.
3. Choosing the highest ranking topic as the an-
swer to the question.
</listItem>
<bodyText confidence="0.989050111111111">
The question topic, in the above example, “Ger-
many”, is filtered out if it is found in the list of top-
ics retrieved (using topic-name hash table), which
can happen as it is one of the keywords in the
query. For the remaining topics in the list, the types
of each topic are fetched using the topic-type hash
table and matched up to the specific answer type.
Re-ranking is performed according to the follow-
ing rules:
</bodyText>
<listItem confidence="0.698152">
• Topics whose type precisely matches the an-
swer type are ranked higher than any other
topics whose types do not precisely match the
answer type.
• Topics whose type do not precisely match
the answer type but still matches the base
type traced from the answer type are ranked
higher than any other topics whose types do
not match the answer type at all.
</listItem>
<bodyText confidence="0.9994905">
Based on these simple rules, the highest-ranking
topic is chosen as the answer. Because of the de-
tailed and precise type information stored for each
topic, we find this simple procedure works well
enough. However, a more sophisticated answer
candidate reranking strategy is conceivable based
on giving different weights to different degree of
match for an answer type.
</bodyText>
<sectionHeader confidence="0.997406" genericHeader="method">
5 Bi-Topic Indexing
</sectionHeader>
<bodyText confidence="0.999953230769231">
The method described thus far ignores question
topics except for filtering them out during post-
processing. However, we mentioned in Section 2
that related topics can be exploited in answering
questions.
To take advantage of question topics within
Topic Indexing and Retrieval, we have adopted
the solution of constructing bi-topic documents in
contrast to the original topic documents with sin-
gle topics. An example of a bi-topic document is
the following Figure 2, which represents the two
topics (Dolly, Ian Wilmut). Such a bi-topic docu-
ment represents the general relation between two
topics via the context in which they co-occur. (As
already noted, the precise character of the rela-
tion is ignored.) The terms that more frequently
appear in such document characterise the relation
between the two topics in statistical fashion, and
this document would be given a higher score for
retrieval with respect to a question, if the ques-
tion contains such a relatively frequently appear-
ing term. For example, in scoring the bi-topic doc-
ument pertaining to (Dolly, Ian Wilmut) bi-topic
document with respect to the question, “Who cre-
ated the first cloned sheep, Dolly?”, the frequently
appearing term in the document, ‘cloned’ would
</bodyText>
<page confidence="0.996942">
69
</page>
<bodyText confidence="0.5797225">
give a very high mark for this document with re-
spect to this question.
</bodyText>
<figureCaption confidence="0.877984">
Figure 2: A Bi-Topic Document: (Dolly, Ian
Wilmut)
</figureCaption>
<bodyText confidence="0.99993641509434">
We construct a bi-topic document collection
is a recursive application of the topic document
method first on the original documents and then
to the resulting topic documents. So given a single
topic document, e.g. for “Dolly”, the same topic
document generating process is then applied to this
document. This generates a new set of topic doc-
uments that, in addition to having their own top-
ics, e.g. “Ian Wilmut”, will also contain the topic
“Dolly” since the original topic document has the
topic “Dolly” in its every sentence. The result-
ing bi-topic documents would comprise (Dolly, Ian
Wilmut), (Dolly, Bonnie), (Dolly, Roslin), etc., all
as bi-topic documents. These topic documents all
concern the topic “Dolly”, which we call the an-
chor topic, and indexing these amounts to creating
a “Dolly” (anchored) index. Separate indices for
base types as in the case of the single topic doc-
uments need not be created since the number of
bi-topic documents anchored to one topic is some
magnitude smaller compared to the number of to-
tal single topic documents.
QA using a bi-topic document index is essen-
tially the same as for the single topic document in-
dex, except in selecting the appropriate anchored
index using the question topic identified from the
question. So the “Dolly” index is chosen if the
question topic is “Dolly”, as in the question, e.g.
“Who created the fist cloned sheep, Dolly?”. Re-
ranking based on fine-grained answer types can
still be performed although question topic filtering
is no longer necessary.
This bi-topic method has the draw-back of gen-
erating a lot of documents and corresponding in-
dices since the number of bi-topic documents is
the product of the number of topics with all the
associated topics. This takes a lot of space for stor-
age and time for generating such a collection. For
the evaluation to be described in the next section,
we have created bi-topic documents and indices
that only pertain to questions (ie only for the ques-
tion topics within the test set) due to the limitation
of space. To be able to scale this method gener-
ally, XML information retrieval technique might
be applicable as this supports richer retrieval ele-
ments other than whole documents and therefore
the bi-topic documents pertaining to one anchor
topic could be all embedded within one topic doc-
ument. This is one area we would like to explore
further in the future.
The next section characterises and compares the
performance of single topic and bi-topic document
based methods.
</bodyText>
<sectionHeader confidence="0.999635" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999693">
6.1 The Evaluation Settings
</subsectionHeader>
<bodyText confidence="0.99999108">
Evaluation has been carried out to determine
whether Topic Indexing and Retrieval using a sim-
ple and efficient IR technique for direct answer re-
trieval can indeed make for an accurate QA sys-
tem. This has also iluminated those features of the
method that contribute to QA performance.
The questions and the corpus (AQUAINT) used
for the evaluation are taken from the TREC QA
track. 377 questions that have single proper names
as answers (ie, excluding list questions, “other”
questions and questions without answers) were se-
lected from the TREC 2003/2004/2005 questions.
Questions from TREC 2004 and TREC 2005 are
grouped around what are called targets. A tar-
get is basically the question topic, e.g. “When
was he born?” where “he” refers to the target,
e.g. “Fred Durst”. One of the experimental setups
takes account of these targets by employing the Bi-
topic method discussed in Section 5. This retrieval
strategy is also applied to questions from TREC
2003 (that come with no targets), by identifying
the question topic in a question and extracting it
as a target automatically, in order to see whether
it can benefit the QA performance even when the
target is not provided manually.
</bodyText>
<page confidence="0.995357">
70
</page>
<bodyText confidence="0.99992975">
The actual evaluation of the method consists of
three experiments, each of which tests a different
setting. The common elements for all three are
the core answer retrieval system. The aspects that
differentiate the three settings are: (1) whether or
not a fine-grained answer type is used for rerank-
ing, (2) whether single topic documents or bi-topic
documents are retrieved.
</bodyText>
<subsectionHeader confidence="0.998139">
6.2 The Core Evaluation System
</subsectionHeader>
<bodyText confidence="0.999928095238095">
The common core system that implements the an-
swer retrieval method comprises (1) a question
analysis module that analyses the question and
produces the question type, answer type, the ques-
tion topics and the shallow parse of the question
text and (2) a retrieval module that generates the
structured query, selects the appropriate index and
retrieves the top 100 topics as answer candidates.
This core system performs the basic retrieval op-
erations, to which we add further operations such
as answer-type based reranking and target specific
retrieval. The addition of some of these features
distinguish different setups for the evaluation.
Setup A involves just the core system on single
topic document indexing of the AQUAINT corpus,
as described in Section 3.2. The resulting topic
documents are divided into the three base types
(PERSON, LOCATION, ORGANISATION), plus
OTHER, as summarised in Table 1. Some ex-
amples of entities belong to type OTHER include
medicines, roller coasters and software.
</bodyText>
<table confidence="0.995812333333333">
KIND NUM
PERSON 117370
ORGANISATION 67559
LOCATION 48194
OTHER 17942
TOTAL 251065
</table>
<tableCaption confidence="0.999843">
Table 1: Number of Topic Docs per Types
</tableCaption>
<bodyText confidence="0.999601">
Setup B is basically the same as setup A, ex-
cept for the addition of fine-grained answer type
re-ranking on the one hundred topics retrieved as
answer candidates. That is, elements of this list
are re-ranked depending on whether their fine-
grained answer type matches the fine-grained an-
swer type identified from the question. Note here
that only the coarse answer type (PERSON, LO-
CATION, ORGANISATION, TOTAL) was used
for retrieval, as opposed to the fine-grained type
such as PRESIDENT or COMPANY, due to the
</bodyText>
<table confidence="0.999621">
A@N A B C
1 0.233:88 0.340:128 0.342:129
2 0.316:119 0.406:153 0.443:167
3 0.366:138 0.438:165 0.485:183
4 0.401:151 0.467:176 0.501:189
5 0.430:162 0.491:185 0.515:194
10 0.472:178 0.523:197 0.549:207
15 0.496:187 0.533:201 0.560:211
20 0.512:193 0.541:204 0.560:211
ACC 0.233 0.340 0.342
MRR 0.306 0.395 0.413
</table>
<tableCaption confidence="0.996458">
Table 2: Results for all setups for all questions
</tableCaption>
<bodyText confidence="0.999969592592593">
fact that separate indices exist only for these coarse
types. The identification of the fine type of a can-
didate topic is done by looking up this information
in the topic-type hash table as mentioned in Sec-
tion 3. Again the resulting top candidate is picked
as the definite answer.
The final setup is setup C. Setup C exploits ques-
tion topics (targets), as described in Section 5. Tar-
gets are explicitly provided in TREC 2004 and
TREC 2005 question set. For the TREC 2003, the
questions, which do not come with explicit targets,
the system automatically extracts a target from the
question using a very simple rule: any proper name
in the question is regarded as a target. The point of
this setup is to test the effectiveness of the bi-topic
method discussed in Section 5. The core retrieval
procedure is the same as in setup B, except that
the index on which the retrieval is performed is se-
lected based on the question topic. In Section 5,
we mentioned that a set of indices were built with
respect to ‘anchor topics’. So the question topic
identified from the question (or provided as de-
fault) acts as the anchor topic and the index that
corresponds to this anchor topic gets chosen. The
rest of the process is the same as setup B, and re-
trieved topics are re-ranked according to the fine-
grained answer type.
</bodyText>
<subsectionHeader confidence="0.999453">
6.3 Overall Results
</subsectionHeader>
<bodyText confidence="0.999577625">
Table 2 summarises the results of the experiments
across all setups and across all the questions eval-
uated. The leftmost column indicates the cut-off
point (ie, 5 indicates the top-5 answer candidates,
10 indicates the top-10 answer candidates, etc.).
The other columns indicate the A@N performance
score data for setup A, setup B and setup C respec-
tively at each cut-off point. Each entry comprises
</bodyText>
<page confidence="0.99762">
71
</page>
<table confidence="0.999861444444444">
A@N B-C C-B C ∩ B
1 60 61 68
2 61 75 92
3 61 79 104
4 63 76 113
5 66 75 119
10 69 79 128
15 68 78 133
20 69 76 135
</table>
<tableCaption confidence="0.999938">
Table 3: Overlap between B and C
</tableCaption>
<bodyText confidence="0.999909395348837">
two scores separated by a colon, representing the
ratio of correctly answered questions over all ques-
tions and the number of correctly answered ques-
tions. The last two rows summarise the results by
giving the accuracy (ACC), which is equivalent to
the correctness rate at A@1 and the Mean Recip-
rocal Rank score (MRR).
From this table, it can be seen that both setup B
and setup C produced results that are superior to
setup A in all measures: accuracy, A@N (for N up
to 20) and MRR.
In order to verify whether the differences in
scores indicate statistical significance, we have
performed Wilcoxon Matched Signed Rank Test
(Wilcoxon, 1945) on the test data (the differences
in ranks for all the questions between setups). This
test is suited for testing two related samples when
an underlying distribution cannot be assumed (un-
like t-test) as with the data here. The statistical
test shows that the difference between setup B and
setup A is indeed significant (p = 1.763e − 08,
for P threshold at 0.05) and that the difference
between setup C and setup A is also significant
(p = 4.244e−08). So setup B and setup C perform
significantly better than setup A.
Setup C performs slightly better than setup B,
both in accuracy (0.342 vs. 0.340) and in MRR
(0.413 vs 0.395), but the statistical test shows,
this difference is not statistically significant (p =
0.5729). However, as the Table 3 shows, setup
B and setup C correctly answered different ques-
tions. (Setup B answered most of the questions
that were correctly answered by setup A, as well
as questions that were not correctly answered by
setup A). Thus, a further investigation is needed
to understand performance differences between se-
tups B and C.
The execution time for each question takes less
than one second for both single-topic and bi-topic
document indices based retrieval on a single CPU
(P4 3.2 Mhz HT) with 512 MB of memory, and
the reranking operation did not add any significant
amount of time to it.
</bodyText>
<sectionHeader confidence="0.999835" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999953">
In this section, we discuss some of the works on
novel indexing techniques for QA that relate to this
work.
In predictive annotation (Prager et al., 1999), the
text of the target corpus is pre-processed in such
a way that phrases that are potential answers are
marked and annotated with respect to their answer
types (or QA-tokens as they call them) including
PERSON$1, DURATION$, etc. Then the text is
indexed not only with ordinary terms but also with
these QA-tokens as indexing elements. The main
advantage of this approach is that QA-tokens are
used as part of the query enhancing the passage re-
trieval performance. Our work in this paper uses
the same predictive annotation technique but dif-
fers in that the named entities are indexed as topics
and are retrieved directly as answer candidates.
Similar to our approach, Kim et al. (2001) ap-
plies predictive annotation method to retrieve an-
swers directly rather than supporting text. For ev-
ery potential answer in the corpus, a set of text
spans up to three sentences long (the sentence in
which it appears, plus whatever following sen-
tences that are linked to this sentence via lexical
chain totalling no more than three sentences in
size) is stored and later sued to retrieve a potential
answer. Although similar to our work, the main
difference is in the way the textual evidence is ag-
gregated. In Topic Indexing and Retrieval, all the
evidence (aka textual content) available through-
out the corpus for a possible answer is aggregated,
whereas Kim uses text spans up three sentences
long from a single document connected by a co-
reference chain for each answer candidate. Also,
topic relations are not exploited as in our work (via
Bi-topic documents).
Fleischman et al. (2002) also retrieves answers
directly. In what they call the answer repository
approach to Question Answering, highly precise
relational information is extracted from the text
collection using text mining techniques based on
part of speech patterns. The extracted concept-
instance pairs of person name-title such as (Bill
</bodyText>
<footnote confidence="0.9995915">
1In their notation, the Dollar sign at the end indicates that
this is a QA token rather than a term.
</footnote>
<page confidence="0.99778">
72
</page>
<bodyText confidence="0.999911793103448">
Gates, Chairman of Microsoft) are used either
solely or in conjunction with a common QA sys-
tem in producing answers. (Jijkoun et al. (2004)
follows a similar approach.) This basically Infor-
mation Extraction approach taken here can com-
plement our own work for the benefit of increased
precision for select types of questions.
In Clifton and Teahan (2004), their knowledge
framework based QA system, QITEKAT, prestores
possible answers along with their corresponding
question templates based on manual and automatic
regular expression patterns. That the potential
questions are stored as well the answers make this
approach different from our approach.
The bi-topic method in this paper has some sim-
ilarity to Katz and Lin (2000). Here, ternary re-
lations are extracted off-line using manually con-
structed regular expression patterns on a target text
and stored in a database for the use in Question
Answering such as in the START QA system (Katz
et al., 2002). With bi-topic documents in this pa-
per, instead of the precise relations between the
two topics, the aggregate context between two par-
ticular topics are captured by assembling all state-
ments that mention these two topics together in one
file. While this does not give the exact character-
istics of the relations involved, it does give some
statistical characterization between the two topics
to the benefit for QA.
</bodyText>
<sectionHeader confidence="0.997808" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9996131">
In this paper, we have presented the method of
Topic Indexing and Retrieval for QA. The method
effectively turns document retrieval of IR into di-
rect answer retrieval by indexing potential answers
(topics) via topic documents. We claimed that
the method can be applied in answering simple
named-entity questions. The evaluation results in-
deed show that the method is effective for this type
of question, with MRR of 0.413 and accuracy of
0.342 (best run: setup C).
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999934214285714">
Clifton, Terence and William Teahan. 2004. Bangor at
TREC 2004: Question answering track. In Proceed-
ings TREC 2004.
Curran, J. and S. Clark. 2003. Language independent
ner using a maximum entropy tagger. In Proceed-
ings of the Seventh Conference on Natual Language
Learning (CoNLL-03), pages 164–167.
Fleischman, Michael, Eduard Hovy, and Abdessamad
Echihabi. 2002. Offline strategies for online ques-
tion answering: Answering questions before they are
asked. In Proceedings of the 41th Annual Meeting of
the Association for Computational Linguistics (ACL-
2003).
Jijkoun, Valentin, Maarten de Rijke, and Jori Mur.
2004. Information extraction for question answer-
ing: improving recall through syntactic patterns.
In COLING ’04: Proceedings of the 20th inter-
national conference on Computational Linguistics,
page 1284, Morristown, NJ, USA. Association for
Computational Linguistics.
Katz, Boris and Jimmy Lin. 2000. REXTOR: A system
for generating relations from natural language. In
Proceedings of the ACL 2000 Workshop on Recent
Advances in NLP and IR.
Katz, B., S. Felshin, D. Yuret, A. Ibrahim, J. Lin,
G. Marton, A. McFarland, and B. Temelkuran. 2002.
Omnibase: Uniform access to heterogeneous data for
question answering.
Kim, Harksoo, Kyungsun Kim, Gary Geunbae Lee,
and Jungyun Seo. 2001. MAYA: A fast question-
answering system based on a predictive answer in-
dexer. In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics (ACL-
2001) Workshop on Open-Domain Question Answer-
ing.
Li, X. and D. Roth. 2002. Learning question clas-
sifiers. In Proceeding of the 19th International
Conference on Computational Linguistics (COL-
ING’02).
Ogilvie, P. and J. Callan. 2002. Experiments using
the lemur toolkit. In Proceeding of the 2001 Text
Retrieval Conference (TREC 2001), pages 103–108.
Prager, John, Dragomir Radev, Eric Brown, Anni Co-
den, and Valerie Samn. 1999. The use of predic-
tive annotation for question answering in TREC8. In
Proceedings of the Eighth Text REtrieval Conference
(TREC-8).
Suchanek, Fabian M., Gjergji Kasneci, and Ger-
hard Weikum. 2007. Yago: A core of seman-
tic knowledge - unifying WordNet and Wikipedia.
In Williamson, Carey L., Mary Ellen Zurko, and
Prashant J. Patel-Schneider, Peter F. Shenoy, edi-
tors, 16th International World Wide Web Conference
(WWW 2007), pages 697–706, Banff, Canada. ACM.
Wilcoxon, F. 1945. Individual comparisons by ranking
methods. Biometrics, (1):80–83.
</reference>
<page confidence="0.999299">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.162293">
<title confidence="0.992055">Topic Indexing and Retrieval for Factoid QA</title>
<author confidence="0.889199">Kisuh</author>
<affiliation confidence="0.915770333333333">School of University of Edinburgh,</affiliation>
<email confidence="0.992863">k.ahn@sms.ed.ac.uk</email>
<author confidence="0.955096">Bonnie</author>
<affiliation confidence="0.913260666666667">School of University of Edinburgh,</affiliation>
<email confidence="0.999447">bonnie@inf.ed.ac.uk</email>
<abstract confidence="0.935907230769231">method of Indexing and Refor QA in this paper enables fast and efficent QA for questions with named entity answers. This is achieved by identifying all possible named entity answers in a corpus off-line and gathering all possible evidence for their direct retrieval as answer candidates using standard IR techniques. An evaluation of this method on 377 TREC questions produced a score of 0.342 in Accuracy and 0.413 in Mean Reciprocal Rank (MRR).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Terence Clifton</author>
<author>William Teahan</author>
</authors>
<title>Bangor at TREC 2004: Question answering track.</title>
<date>2004</date>
<booktitle>In Proceedings TREC</booktitle>
<contexts>
<context position="29162" citStr="Clifton and Teahan (2004)" startWordPosition="4902" endWordPosition="4905">xtracted from the text collection using text mining techniques based on part of speech patterns. The extracted conceptinstance pairs of person name-title such as (Bill 1In their notation, the Dollar sign at the end indicates that this is a QA token rather than a term. 72 Gates, Chairman of Microsoft) are used either solely or in conjunction with a common QA system in producing answers. (Jijkoun et al. (2004) follows a similar approach.) This basically Information Extraction approach taken here can complement our own work for the benefit of increased precision for select types of questions. In Clifton and Teahan (2004), their knowledge framework based QA system, QITEKAT, prestores possible answers along with their corresponding question templates based on manual and automatic regular expression patterns. That the potential questions are stored as well the answers make this approach different from our approach. The bi-topic method in this paper has some similarity to Katz and Lin (2000). Here, ternary relations are extracted off-line using manually constructed regular expression patterns on a target text and stored in a database for the use in Question Answering such as in the START QA system (Katz et al., 2</context>
</contexts>
<marker>Clifton, Teahan, 2004</marker>
<rawString>Clifton, Terence and William Teahan. 2004. Bangor at TREC 2004: Question answering track. In Proceedings TREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curran</author>
<author>S Clark</author>
</authors>
<title>Language independent ner using a maximum entropy tagger.</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh Conference on Natual Language Learning (CoNLL-03),</booktitle>
<pages>164--167</pages>
<contexts>
<context position="5289" citStr="Curran and Clark, 2003" startWordPosition="867" endWordPosition="870">stored material consists of (1) a Topic Repository, which stores topics with their variant names and ontological types, (2) a topic document collection that stores the textual content of topics, and (3) a set of indices created by indexing the topic document collection for fast and efficient retrieval. 3.1 The Make Up of Topic Repository The Topic Repository stores topics, along their variant names and their ontological types, in hash tables for fast look-up. Building a topic repository requires identifying topics within the given corpus. For this we have used the C&amp;C named entity recogniser (Curran and Clark, 2003), which is run on pos-tagged and chunked documents in the corpus to identify and extract named entities as potential topics. This also identifies the base type of a subset of named entities as PERSON, LOCATION and ORGANISATION. This is stored for later use in building type-separated indices. When a named entity is identified, we first check whether it represents a topic already found in the topic repository. This is done by checking the topic-name hash table in the repository, which serves as the main data storage for the variant names of topics. To resolve a target named entity to the appropr</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>Curran, J. and S. Clark. 2003. Language independent ner using a maximum entropy tagger. In Proceedings of the Seventh Conference on Natual Language Learning (CoNLL-03), pages 164–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>Offline strategies for online question answering: Answering questions before they are asked.</title>
<date>2002</date>
<booktitle>In Proceedings of the 41th Annual Meeting of the Association for Computational Linguistics (ACL2003).</booktitle>
<contexts>
<context position="28389" citStr="Fleischman et al. (2002)" startWordPosition="4777" endWordPosition="4780">ed to this sentence via lexical chain totalling no more than three sentences in size) is stored and later sued to retrieve a potential answer. Although similar to our work, the main difference is in the way the textual evidence is aggregated. In Topic Indexing and Retrieval, all the evidence (aka textual content) available throughout the corpus for a possible answer is aggregated, whereas Kim uses text spans up three sentences long from a single document connected by a coreference chain for each answer candidate. Also, topic relations are not exploited as in our work (via Bi-topic documents). Fleischman et al. (2002) also retrieves answers directly. In what they call the answer repository approach to Question Answering, highly precise relational information is extracted from the text collection using text mining techniques based on part of speech patterns. The extracted conceptinstance pairs of person name-title such as (Bill 1In their notation, the Dollar sign at the end indicates that this is a QA token rather than a term. 72 Gates, Chairman of Microsoft) are used either solely or in conjunction with a common QA system in producing answers. (Jijkoun et al. (2004) follows a similar approach.) This basica</context>
</contexts>
<marker>Fleischman, Hovy, Echihabi, 2002</marker>
<rawString>Fleischman, Michael, Eduard Hovy, and Abdessamad Echihabi. 2002. Offline strategies for online question answering: Answering questions before they are asked. In Proceedings of the 41th Annual Meeting of the Association for Computational Linguistics (ACL2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Jijkoun</author>
<author>Maarten de Rijke</author>
<author>Jori Mur</author>
</authors>
<title>Information extraction for question answering: improving recall through syntactic patterns.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>1284</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Jijkoun, de Rijke, Mur, 2004</marker>
<rawString>Jijkoun, Valentin, Maarten de Rijke, and Jori Mur. 2004. Information extraction for question answering: improving recall through syntactic patterns. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 1284, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
</authors>
<title>REXTOR: A system for generating relations from natural language.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACL 2000 Workshop on Recent Advances in NLP and IR.</booktitle>
<contexts>
<context position="29536" citStr="Katz and Lin (2000)" startWordPosition="4958" endWordPosition="4961">answers. (Jijkoun et al. (2004) follows a similar approach.) This basically Information Extraction approach taken here can complement our own work for the benefit of increased precision for select types of questions. In Clifton and Teahan (2004), their knowledge framework based QA system, QITEKAT, prestores possible answers along with their corresponding question templates based on manual and automatic regular expression patterns. That the potential questions are stored as well the answers make this approach different from our approach. The bi-topic method in this paper has some similarity to Katz and Lin (2000). Here, ternary relations are extracted off-line using manually constructed regular expression patterns on a target text and stored in a database for the use in Question Answering such as in the START QA system (Katz et al., 2002). With bi-topic documents in this paper, instead of the precise relations between the two topics, the aggregate context between two particular topics are captured by assembling all statements that mention these two topics together in one file. While this does not give the exact characteristics of the relations involved, it does give some statistical characterization b</context>
</contexts>
<marker>Katz, Lin, 2000</marker>
<rawString>Katz, Boris and Jimmy Lin. 2000. REXTOR: A system for generating relations from natural language. In Proceedings of the ACL 2000 Workshop on Recent Advances in NLP and IR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Katz</author>
<author>S Felshin</author>
<author>D Yuret</author>
<author>A Ibrahim</author>
<author>J Lin</author>
<author>G Marton</author>
<author>A McFarland</author>
<author>B Temelkuran</author>
</authors>
<title>Omnibase: Uniform access to heterogeneous data for question answering.</title>
<date>2002</date>
<contexts>
<context position="29766" citStr="Katz et al., 2002" startWordPosition="4998" endWordPosition="5001"> Teahan (2004), their knowledge framework based QA system, QITEKAT, prestores possible answers along with their corresponding question templates based on manual and automatic regular expression patterns. That the potential questions are stored as well the answers make this approach different from our approach. The bi-topic method in this paper has some similarity to Katz and Lin (2000). Here, ternary relations are extracted off-line using manually constructed regular expression patterns on a target text and stored in a database for the use in Question Answering such as in the START QA system (Katz et al., 2002). With bi-topic documents in this paper, instead of the precise relations between the two topics, the aggregate context between two particular topics are captured by assembling all statements that mention these two topics together in one file. While this does not give the exact characteristics of the relations involved, it does give some statistical characterization between the two topics to the benefit for QA. 8 Conclusion In this paper, we have presented the method of Topic Indexing and Retrieval for QA. The method effectively turns document retrieval of IR into direct answer retrieval by in</context>
</contexts>
<marker>Katz, Felshin, Yuret, Ibrahim, Lin, Marton, McFarland, Temelkuran, 2002</marker>
<rawString>Katz, B., S. Felshin, D. Yuret, A. Ibrahim, J. Lin, G. Marton, A. McFarland, and B. Temelkuran. 2002. Omnibase: Uniform access to heterogeneous data for question answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harksoo Kim</author>
<author>Kyungsun Kim</author>
<author>Gary Geunbae Lee</author>
<author>Jungyun Seo</author>
</authors>
<title>MAYA: A fast questionanswering system based on a predictive answer indexer.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL2001) Workshop on Open-Domain Question Answering.</booktitle>
<contexts>
<context position="27498" citStr="Kim et al. (2001)" startWordPosition="4627" endWordPosition="4630">ases that are potential answers are marked and annotated with respect to their answer types (or QA-tokens as they call them) including PERSON$1, DURATION$, etc. Then the text is indexed not only with ordinary terms but also with these QA-tokens as indexing elements. The main advantage of this approach is that QA-tokens are used as part of the query enhancing the passage retrieval performance. Our work in this paper uses the same predictive annotation technique but differs in that the named entities are indexed as topics and are retrieved directly as answer candidates. Similar to our approach, Kim et al. (2001) applies predictive annotation method to retrieve answers directly rather than supporting text. For every potential answer in the corpus, a set of text spans up to three sentences long (the sentence in which it appears, plus whatever following sentences that are linked to this sentence via lexical chain totalling no more than three sentences in size) is stored and later sued to retrieve a potential answer. Although similar to our work, the main difference is in the way the textual evidence is aggregated. In Topic Indexing and Retrieval, all the evidence (aka textual content) available througho</context>
</contexts>
<marker>Kim, Kim, Lee, Seo, 2001</marker>
<rawString>Kim, Harksoo, Kyungsun Kim, Gary Geunbae Lee, and Jungyun Seo. 2001. MAYA: A fast questionanswering system based on a predictive answer indexer. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL2001) Workshop on Open-Domain Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proceeding of the 19th International Conference on Computational Linguistics (COLING’02).</booktitle>
<contexts>
<context position="7875" citStr="Li and Roth (2002)" startWordPosition="1300" endWordPosition="1303">ut also a significant amount of world knowledge typically associated to the topic, due to the nature of Wikipedia categories as descriptive tags. For example, ‘Bill Gates’ is identified as ‘CEO’ (a title-role), and ‘Pusan’ as ‘a province of Korea’ (geographical knowledge). Such diverse and significant knowledge, as well as the breadth and the depth of the fine types contained in the topic-type hash table, enable a very powerful match between the answer type from a question to that of a candidate topic. The set of fine-grained answer types used here differs from the set of answer types such as Li and Roth (2002) used elsewhere in that the set is openended, and new types can be added for an entity at any time. The topic repository is used in re-ranking answer candidates by the fine-grained anwer type and for question topic identification, as well as in building topic document collection to be explained next. 3.2 The Topic Document Collection As noted, the textual content of a topic is the set of all sentences in a corpus that mentions this topic. 67 (Since anaphora resolution is not yet performed, the sentences that only mention a particular topic anaphorically are missed.) Such set of sentences is as</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Li, X. and D. Roth. 2002. Learning question classifiers. In Proceeding of the 19th International Conference on Computational Linguistics (COLING’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ogilvie</author>
<author>J Callan</author>
</authors>
<title>Experiments using the lemur toolkit.</title>
<date>2002</date>
<booktitle>In Proceeding of the 2001 Text Retrieval Conference (TREC</booktitle>
<pages>103--108</pages>
<contexts>
<context position="13009" citStr="Ogilvie and Callan, 2002" startWordPosition="2171" endWordPosition="2174">e query, and (3) the actual retrieval operation. An appropriate index is chosen based on the base answer type. For example, for the question, “Who is the president of Germany?”, the answer type is identified as ‘president’. But since the answer type, ‘president’, is not the base type, WordNet is used to trace from ’president’ to a base type (PERSON) and the corresponding index is selected (because separate indices exist only for base types). If none of the three base types is found by this process, the total index is used. Retrieval uses the InQuery retrieval system within the Lemur Tool Kit (Ogilvie and Callan, 2002). InQuery supports a powerful and flexible structured query language. Taking advantage of this, a structured query is formulated from the target question. So for example, the parsed form of the question, “Who is the president of Germany?” is used to generate the following query \sum(is president of germany \phrase(president of germany)). In this example, “president of germany” forms a phrase, and it is inserted as part of the query element with the ‘\phrase’ operator. However, the individual keywords are also included as bag of words since we have found it to give better performance in the tri</context>
</contexts>
<marker>Ogilvie, Callan, 2002</marker>
<rawString>Ogilvie, P. and J. Callan. 2002. Experiments using the lemur toolkit. In Proceeding of the 2001 Text Retrieval Conference (TREC 2001), pages 103–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Dragomir Radev</author>
<author>Eric Brown</author>
<author>Anni Coden</author>
<author>Valerie Samn</author>
</authors>
<title>The use of predictive annotation for question answering in TREC8.</title>
<date>1999</date>
<booktitle>In Proceedings of the Eighth Text REtrieval Conference (TREC-8).</booktitle>
<contexts>
<context position="26810" citStr="Prager et al., 1999" startWordPosition="4511" endWordPosition="4514">ered by setup A, as well as questions that were not correctly answered by setup A). Thus, a further investigation is needed to understand performance differences between setups B and C. The execution time for each question takes less than one second for both single-topic and bi-topic document indices based retrieval on a single CPU (P4 3.2 Mhz HT) with 512 MB of memory, and the reranking operation did not add any significant amount of time to it. 7 Related Work In this section, we discuss some of the works on novel indexing techniques for QA that relate to this work. In predictive annotation (Prager et al., 1999), the text of the target corpus is pre-processed in such a way that phrases that are potential answers are marked and annotated with respect to their answer types (or QA-tokens as they call them) including PERSON$1, DURATION$, etc. Then the text is indexed not only with ordinary terms but also with these QA-tokens as indexing elements. The main advantage of this approach is that QA-tokens are used as part of the query enhancing the passage retrieval performance. Our work in this paper uses the same predictive annotation technique but differs in that the named entities are indexed as topics and</context>
</contexts>
<marker>Prager, Radev, Brown, Coden, Samn, 1999</marker>
<rawString>Prager, John, Dragomir Radev, Eric Brown, Anni Coden, and Valerie Samn. 1999. The use of predictive annotation for question answering in TREC8. In Proceedings of the Eighth Text REtrieval Conference (TREC-8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A core of semantic knowledge - unifying WordNet and Wikipedia.</title>
<date>2007</date>
<booktitle>16th International World Wide Web Conference (WWW 2007),</booktitle>
<pages>697--706</pages>
<editor>In Williamson, Carey L., Mary Ellen Zurko, and Prashant J. Patel-Schneider, Peter F. Shenoy, editors,</editor>
<publisher>ACM.</publisher>
<location>Banff, Canada.</location>
<contexts>
<context position="6574" citStr="Suchanek et al., 2007" startWordPosition="1083" endWordPosition="1086">any common variant names for the same topic. The topic-name hash table is updated accordingly. Hash table entries consist of pairs like (‘George Clooney’, 1745442), where the name ‘George Clooney’ is one of the names that belong to the unique topic with the ID number of 1745442. We currently do nothing to disambiguate topics, so different individuals with the same name will all be considered the same topic. Fine-grained ontological types of topics are identified and stored as well in a separate topictype table. In order to discover fine-grained topic types, the ontology database Yago is used (Suchanek et al., 2007). Yago contains such information for Wikipedia topics, derived by mapping the category information about target topic supplied by a Wikipedia user to the appropriate WordNet concept. (Wikipedia categories are not consistent and uniform, and they are more like tags that characterise a topic rather than strictly classify it.) Using this ontology to look up the type(s) of each topic-type (i.e. the corresponding WordNet concept) and by tracing up the WordNet concept hierarchy, we created a fine-grained, multi-level (with respect to ISA) topic-type hash table for all the topics in the topic reposit</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Suchanek, Fabian M., Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A core of semantic knowledge - unifying WordNet and Wikipedia. In Williamson, Carey L., Mary Ellen Zurko, and Prashant J. Patel-Schneider, Peter F. Shenoy, editors, 16th International World Wide Web Conference (WWW 2007), pages 697–706, Banff, Canada. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wilcoxon</author>
</authors>
<title>Individual comparisons by ranking methods.</title>
<date>1945</date>
<journal>Biometrics,</journal>
<contexts>
<context position="25310" citStr="Wilcoxon, 1945" startWordPosition="4252" endWordPosition="4253">a colon, representing the ratio of correctly answered questions over all questions and the number of correctly answered questions. The last two rows summarise the results by giving the accuracy (ACC), which is equivalent to the correctness rate at A@1 and the Mean Reciprocal Rank score (MRR). From this table, it can be seen that both setup B and setup C produced results that are superior to setup A in all measures: accuracy, A@N (for N up to 20) and MRR. In order to verify whether the differences in scores indicate statistical significance, we have performed Wilcoxon Matched Signed Rank Test (Wilcoxon, 1945) on the test data (the differences in ranks for all the questions between setups). This test is suited for testing two related samples when an underlying distribution cannot be assumed (unlike t-test) as with the data here. The statistical test shows that the difference between setup B and setup A is indeed significant (p = 1.763e − 08, for P threshold at 0.05) and that the difference between setup C and setup A is also significant (p = 4.244e−08). So setup B and setup C perform significantly better than setup A. Setup C performs slightly better than setup B, both in accuracy (0.342 vs. 0.340)</context>
</contexts>
<marker>Wilcoxon, 1945</marker>
<rawString>Wilcoxon, F. 1945. Individual comparisons by ranking methods. Biometrics, (1):80–83.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>