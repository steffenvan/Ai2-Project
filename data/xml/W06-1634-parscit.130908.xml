<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.99883">
Automatic Construction of Predicate-argument Structure Patterns
for Biomedical Information Extraction
</title>
<author confidence="0.990791">
Akane Yakushiji∗ † Yusuke Miyao∗ Tomoko Ohta∗ Yuka Tateisi∗ ‡ Jun’ichi Tsujii∗ §
</author>
<affiliation confidence="0.996324">
∗Department of Computer Science, University of Tokyo
</affiliation>
<address confidence="0.704452">
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
§ School of Informatics, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
</address>
<email confidence="0.996123">
{akane, yusuke, okap, yucca, tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991435">
This paper presents a method of automat-
ically constructing information extraction
patterns on predicate-argument structures
(PASs) obtained by full parsing from a
smaller training corpus. Because PASs
represent generalized structures for syn-
tactical variants, patterns on PASs are ex-
pected to be more generalized than those
on surface words. In addition, patterns
are divided into components to improve
recall and we introduce a Support Vector
Machine to learn a prediction model using
pattern matching results. In this paper, we
present experimental results and analyze
them on how well protein-protein interac-
tions were extracted from MEDLINE ab-
stracts. The results demonstrated that our
method improved accuracy compared to a
machine learning approach using surface
word/part-of-speech patterns.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996692">
One primitive approach to Information Extrac-
tion (IE) is to manually craft numerous extrac-
tion patterns for particular applications and this
is presently one of the main streams of biomedi-
cal IE (Blaschke and Valencia, 2002; Koike et al.,
2003). Although such IE attempts have demon-
strated near-practical performance, the same sets
of patterns cannot be applied to different kinds of
information. A real-world task requires several
kinds of IE, thus manually engineering extraction
</bodyText>
<footnote confidence="0.602783">
Current Affiliation:
† FUJITSU LABORATORIES LTD.
‡ Faculty of Informatics, Kogakuin University
</footnote>
<bodyText confidence="0.999724">
patterns, which is tedious and time-consuming
process, is not really practical.
Techniques based on machine learning (Zhou et
al., 2005; Hao et al., 2005; Bunescu and Mooney,
2006) are expected to alleviate this problem in
manually crafted IE. However, in most cases, the
cost of manually crafting patterns is simply trans-
ferred to that for constructing a large amount of
training data, which requires tedious amount of
manual labor to annotate text.
To systematically reduce the necessary amount
of training data, we divided the task of construct-
ing extraction patterns into a subtask that general
natural language processing techniques can solve
and a subtask that has specific properties accord-
ing to the information to be extracted. The former
subtask is of full parsing (i.e. recognizing syntactic
structures of sentences), and the latter subtask is of
constructing specific extraction patterns (i.e. find-
ing clue words to extract information) based on the
obtained syntactic structures.
We adopted full parsing from various levels
of parsing, because we believe that it offers the
best utility to generalize sentences into normal-
ized syntactic relations. We also divided patterns
into components to improve recall and we intro-
duced machine learning with a Support Vector
Machine (SVM) to learn a prediction model us-
ing the matching results of extraction patterns. As
an actual IE task, we extracted pairs of interacting
protein names from biomedical text.
</bodyText>
<sectionHeader confidence="0.994638" genericHeader="introduction">
2 Full Parsing
</sectionHeader>
<subsectionHeader confidence="0.718226">
2.1 Necessity for Full Parsing
</subsectionHeader>
<bodyText confidence="0.998631">
A technique that many previous approaches have
used is shallow parsing (Koike et al., 2003; Yao
et al., 2004; Zhou et al., 2005). Their assertion is
</bodyText>
<page confidence="0.971535">
284
</page>
<note confidence="0.9661245">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 284–292,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<table confidence="0.809833">
Distance Count (%) Sum (%)
−1 54 5.0 5.0
0 8 0.7 5.7
1 170 15.7 21.4
2–5 337 31.1 52.5
6–10 267 24.6 77.1
11– 248 22.9 100.0
</table>
<bodyText confidence="0.7311002">
Distance −1 means protein word has been annotated as in-
teracting with itself (e.g. “actin polymerization”). Distance 0
means words of the interacting proteins are directly next to
one another. Multi-word protein names are concatenated as
long as they do not cross tags to annotate proteins.
</bodyText>
<tableCaption confidence="0.990675">
Table 1: Distance between Interacting Proteins
</tableCaption>
<bodyText confidence="0.999682970588235">
that shallow parsers are more robust and would be
sufficient for IE. However, their claims that shal-
low parsers are sufficient, or that full parsers do
not contribute to application tasks, have not been
fully proved by experimental results.
Zhou et al. (2005) argued that most informa-
tion useful for IE derived from full parsing was
shallow. However, they only used dependency
trees and paths on full parse trees in their experi-
ment. Such structures did not include information
of semantic subjects/objects, which full parsing
can recognize. Additionally, most relations they
extracted from the ACE corpus (Linguistic Data
Consortium, 2005) on broadcasts and newswires
were within very short word-distance (70% where
two entities are embedded in each other or sep-
arated by at most one word), and therefore shal-
low information was beneficial. However, Table 1
shows that the word distance is long between in-
teracting protein names annotated on the AImed
corpus (Bunescu and Mooney, 2004), and we have
to treat long-distance relations for information like
protein-protein interactions.
Full parsing is more effective for acquiring gen-
eralized data from long-length words than shallow
parsing. The sentences at left in Figure 1 exem-
plify the advantages of full parsing. The gerund
“activating” in the last sentence takes a non-local
semantic subject “ENTITY]”, and shallow parsing
cannot recognize this relation because “ENTITY]”
and “activating” are in different phrases. Full pars-
ing, on the other hand, can identify both the sub-
ject of the whole sentence and the semantic subject
of “activating” have been shared.
</bodyText>
<subsectionHeader confidence="0.998908">
2.2 Predicate-argument Structures
</subsectionHeader>
<bodyText confidence="0.999963085714286">
We applied Enju (Tsujii Laboratory, 2005a) as
a full parser which outputs predicate-argument
structures (PASs). PASs are well normalized
forms that represent syntactic relations. Enju
is based on Head-driven Phrase Structure Gram-
mar (Sag and Wasow, 1999), and it has been
trained on the Penn Treebank (PTB) (Marcus et
al., 1994) and a biomedical corpus, the GENIA
Treebank (GTB) (Tsujii Laboratory, 2005b). We
used a part-of-speech (POS) tagger trained on the
GENIA corpus (Tsujii Laboratory, 2005b) as a
preprocessor for Enju. On predicate-argument re-
lations, Enju achieved 88.0% precision and 87.2%
recall on PTB, and 87.1% precision and 85.4% re-
call on GTB.
The illustration at right in Figure 1 is a PAS
example, which represents the relation between
“activate”, “ENTITY]” and “ENTITY2” of all sen-
tences to the left. The predicate and its argu-
ments are words converted to their base forms,
augmented by their POSs. The arrows denote
the connections from predicates to their arguments
and the types of arguments are indicated as arrow
labels, i.e., ARGn (n = 1, 2, ...), MOD. For ex-
ample, the semantic subject of a transitive verb is
ARG1 and the semantic object is ARG2.
What is important here is, thanks to the strong
normalization of syntactic variations, that we can
expect that the construction algorithm for extract-
ing patterns that works on PASs will need a much
smaller training corpus than those working on
surface-word sequences. Furthermore, because of
the reduced diversity of surface-word sequences at
the PAS level, any IE system at this level should
demonstrate improved recall.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999982294117647">
Sudo et al. (2003), Culotta and Sorensen (2004)
and Bunescu and Mooney (2005) acquired sub-
structures derived from dependency trees as ex-
traction patterns for IE in general domains. Their
approaches were similar to our approach using
PASs derived from full parsing. However, one
problem with their systems is that they could
not treat non-local dependencies such as seman-
tic subjects of gerund constructions (discussed in
Section 2), and thus rules acquired from the con-
structions were partial.
Bunescu and Mooney (2006) also learned ex-
traction patterns for protein-protein interactions
by SVM with a generalized subsequence kernel.
Their patterns are sequences of words, POSs, en-
tity types, etc., and they heuristically restricted
length and word positions of the patterns. Al-
</bodyText>
<page confidence="0.996385">
285
</page>
<bodyText confidence="0.8425068">
ENTITY] recognizes and activates ENTITY2.
ENTITY2 activated by ENTITY] are not well characterized.
The herpesvirus encodes a functional ENTITY] that activates human ENTITY2.
ENTITY] can functionally cooperate to synergistically activate ENTITY2.
The ENTITY] plays key roles by activating ENTITY2.
</bodyText>
<figureCaption confidence="0.993973">
Figure 1: Syntactical Variations of “activate”
</figureCaption>
<bodyText confidence="0.999655941176471">
though they achieved about 60% precision and
about 40% recall, these heuristic restrictions could
not be guaranteed to be applied to other IE tasks.
Hao et al. (2005) learned extraction patterns
for protein-protein interactions as sequences of
words, POSs, entity tags and gaps by dynamic
programming, and reduced/merged them using a
minimum description length-based algorithm. Al-
though they achieved 79.8% precision and 59.5%
recall, sentences in their test corpus have too
many positive instances and some of the pat-
terns they claimed to have been successfully con-
structed went against linguistic or biomedical in-
tuition. (e.g. “ENTITY] and interacts with EN-
TITY2” should be replaced by a more general pat-
tern because they aimed to reduce the number of
patterns.)
</bodyText>
<sectionHeader confidence="0.984712" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.999089708333334">
We automatically construct patterns to extract
protein-protein interactions from an annotated
training corpus. The corpus needs to be tagged to
denote which protein words are interacting pairs.
We follow five steps in constructing extraction
patterns from the training corpus. (1) Sentences
in the training corpus are parsed into PASs and
we extract raw patterns from the PASs. (2) We
divide the raw patterns to generate both combi-
nation and fragmental patterns. Because obtained
patterns include inappropriate ones (wrongly gen-
erated or too general), (3) we apply both kinds of
patterns to PASs of sentences in the training cor-
pus, (4) calculate the scores for matching results
of combination patterns, and (5) make a prediction
model with SVM using these matching results and
scores.
We extract pairs of interacting proteins from a
target text in the actual IE phase, in three steps.
(1) Sentences in the target corpus are parsed into
PASs. (2) We apply both kinds of extraction pat-
terns to these PASs and (3) calculate scores for
combination pattern matching. (4) We use the pre-
diction model to predict interacting pairs.
</bodyText>
<figureCaption confidence="0.998101">
Figure 2: Extraction of Raw Pattern
</figureCaption>
<subsectionHeader confidence="0.9913385">
4.1 Full Parsing and Extraction of Raw
Patterns
</subsectionHeader>
<bodyText confidence="0.999880076923077">
As the first step in both the construction phase and
application phase of extraction patterns, we parse
sentences into PASs using Enju.1 We label all
PASs of the protein names as protein PASs.
After parsing, we extract the smallest set of
PASs, which connect words that denote interact-
ing proteins, and make it a raw pattern. We take
the same method to extract and refine raw patterns
as Yakushiji et al. (2005). Connecting means we
can trace predicate-argument relations from one
protein word to the other in an interacting pair.
The procedure to obtain a raw pattern (p0, . . . , pn)
is as follows:
</bodyText>
<listItem confidence="0.8540186">
predicate(p): PASs that have p as their argument
argument(p): PASs that p has as its arguments
1. pi = p0 is the PAS of a word correspondent
to one of interacting proteins, and we obtain
candidates of the raw pattern as follows:
</listItem>
<bodyText confidence="0.87368">
1-1. If pi is of the word of the other interact-
ing protein, (p0, ... , pi) is a candidate
of the raw pattern.
1-2. If not, make pattern candidates
for each pi+1 E predicate(pi) U
argument(pi) − {p0, ... , pi} by
returning to 1-1.
2. Select the pattern candidate of the smallest
set as the raw pattern.
</bodyText>
<footnote confidence="0.951746666666667">
1Before parsing, we concatenate each multi-word protein
name into the one word as long as the concatenation does not
cross name boundaries.
</footnote>
<page confidence="0.996304">
286
</page>
<bodyText confidence="0.996095966666667">
3. Substitute variables (ENTITY1, ENTITY2) for
the predicates of PASs correspondent to the
interacting proteins.
The lower part of Figure 2 shows an example
of the extraction of a raw pattern. “CD4” and
“MHCII” are words representing interacting pro-
teins. First, we set the PAS of “CD4” as p0.
argument(p0) includes the PAS of “protein”, and
we set it as p1 (in other words, tracing the arrow
(1)). Next, predicate(p1) includes the PAS of “in-
teract” (tracing the arrow (2) back), so we set it
as p2. We continue similarly until we reach the
PAS of “MHCII” (p6). The result of the extracted
raw pattern is the set of p0, ... , p6 with substitut-
ing variables ENTITY1 and ENTITY2 for “CD4”
and “MHCII”.
There are some cases where an extracted raw
pattern is not appropriate and we need to re-
fine it. One case is when unnecessary coordi-
nations/parentheses are included in the pattern,
e.g. two interactions are described in a combined
representation (“ENTITY1 binds this protein and
ENTITY2”). Another is when two interacting pro-
teins are connected directly by a conjunction or
only one protein participates in an interaction. In
such cases, we refine patterns by unfolding of co-
ordinations/parentheses and extension of patterns,
respectively. We have omitted detailed explana-
tions because of space limitations. The details are
described in the work of Yakushiji et al. (2005).
</bodyText>
<subsectionHeader confidence="0.999464">
4.2 Division of Patterns
</subsectionHeader>
<bodyText confidence="0.9999268">
Division for generating combination patterns is
based on observation of Yakushiji et al. (2005) that
there are many cases where combinations of verbs
and certain nouns form IE patterns. In the work
of Yakushiji et al. (2005), we divided only patterns
that include only one verb. We have extended the
division process to also treat nominal patterns or
patterns that include more than one verb.
Combination patterns are not appropriate for
utilizing individual word information because they
are always used in rather strictly combined ways.
Therefore we have newly introduced fragmental
patterns which consist of independent PASs from
raw patterns, in order to use individual word infor-
mation for higher recall.
</bodyText>
<sectionHeader confidence="0.783413" genericHeader="method">
4.2.1 Division for Generating Combination
Patterns
</sectionHeader>
<bodyText confidence="0.984597">
Raw patterns are divided into some compo-
nents and the components are combined to con-
</bodyText>
<figureCaption confidence="0.997125">
Figure 3: Division of Raw Pattern into Combina-
tion Pattern Components (Entity-Main-Entity)
</figureCaption>
<bodyText confidence="0.965052">
struct combination patterns according to types of
the division. There are three types of division of
raw patterns for generating combination patterns.
These are:
</bodyText>
<listItem confidence="0.9968472">
(a) Two-entity Division
(a-1) Entity-Main-Entity Division
(a-2) Main-Entity-Entity Division
(b) Single-entity Division, and
(c) No Division (Naive Patterns).
</listItem>
<bodyText confidence="0.999829888888889">
Most raw patterns, where entities are at both
ends of the patterns, are divided into Entity-Main-
Entity. Main-Entity-Entity are for the cases where
there are PASs other than entities at the ends of
the patterns (e.g. “interaction between ENTITY1
and ENTITY2”). Single-entity is a special Main-
Entity-Entity for interactions with only one partic-
ipant (e.g. “ENTITY1 dimerization”).
There is an example of Entity-Main-Entity divi-
sion in Figure 3. First, the main component from
the raw pattern is the syntactic head PAS of the
raw pattern. If the raw pattern corresponds to a
sentence, the syntactic head PAS is the PAS of the
main verb. We underspecify the arguments of the
main component, to enable them to unify with the
PASs of any words with the same POSs. Next, if
there are PASs of prepositions connecting to the
main component, they become prep components.
If there is no PAS of a preposition next to the main
component on the connecting link from the main
component to an entity, we make the pseudo PAS
of a null preposition the prep component. The left
prep component ($X) in Figure 3 is a pseudo PAS
of a null preposition. We also underspecify the ar-
guments of prep components. Finally, the remain-
ing two parts, which are typically noun phrases, of
the raw pattern become entity components. PASs
</bodyText>
<page confidence="0.981679">
287
</page>
<bodyText confidence="0.999867181818182">
corresponding to the entities of the original pair
are labeled as only unifiable with the entities of
other pairs.
Main-Entity-Entity division is similar, except
we distinguish only one prep component as a
double-prep component and the PAS of the coor-
dinate conjunction between entities becomes the
coord component. Single-entity division is simi-
lar to Main-Entity-Entity division and the differ-
ence is that single-entity division produces no co-
ord and one entity component. Naive patterns are
patterns without division, where no division can be
applied (e.g. “ENTITY]/NN in/IN complexes/NN
with/IN ENTITY2/NN”).
All PASs on boundaries of components are la-
beled to determine which PAS on a boundary of
another component can be unified. Labels are rep-
resented by subscriptions in Figure 3. These re-
strictions on component connection are used in the
step of constructing combination patterns.
Constructing combination patterns by combin-
ing components is equal to reconstructing orig-
inal raw patterns with the original combination
of components, or constructing new raw patterns
with new combinations of components. For exam-
ple, an Entity-Main-Entity pattern is constructed
by combination of any main, any two prep and any
two entity components. Actually, this construction
process by combination is executed in the pattern
matching step. That is, we do not off-line con-
struct all possible combination patterns from the
components and only construct the combination
patterns that are able to match the target.
</bodyText>
<sectionHeader confidence="0.8608765" genericHeader="method">
4.2.2 Division for Generating Fragmental
Patterns
</sectionHeader>
<bodyText confidence="0.999989153846154">
A raw pattern is splitted into individual PASs
and each PAS becomes a fragmental pattern. We
also prepare underspecified patterns where one or
more of the arguments of the original are under-
specified, i.e., are able to match any words of
the same POSs and the same label of protein/not-
protein. We underspecify the PASs of entities in
fragmental patterns to enable them to unify with
any PASs with the same POSs and a protein la-
bel, although in combination patterns we retain the
PASs of entities as only unifiable with entities of
pairs. This is because fragmental patterns are de-
signed to be less strict than combination patterns.
</bodyText>
<subsectionHeader confidence="0.995455">
4.3 Pattern Matching
</subsectionHeader>
<bodyText confidence="0.996784285714286">
Matching of combination patterns is executed as
a process to match and combine combination pat-
tern components according to their division types
(Entity-Main-Entity, Main-Entity-Entity, Single-
entity and No Division). Fragmental matching is
matching all fragmental patterns to PASs derived
from sentences.
</bodyText>
<subsectionHeader confidence="0.97977">
4.4 Scoring for Combination Matching
</subsectionHeader>
<bodyText confidence="0.99853325">
We next calculate the score of each combination
matching to estimate the adequacy of the combina-
tion of components. This is because new combina-
tion of components may form inadequate patterns.
(e.g. “ENTITY] be ENTITY2” can be formed of
components from “ENTITY] be ENTITY2 recep-
tor”.) Scores are derived from the results of com-
bination matching to the source training corpus.
We apply the combination patterns to the train-
ing corpus, and count pairs of True Positives (TP)
and False Positives (FP). The scores are calculated
basically by the following formula:
</bodyText>
<equation confidence="0.791297">
Score = TP/(TP + FP) + α x TP
This formula is based on the precision of the pat-
</equation>
<bodyText confidence="0.998221315789473">
tern on the training corpus, i.e., an estimated pre-
cision on a test corpus. α works for smoothing,
that is, to accept only patterns of large TP when
FP = 0. α is set as 0.01 empirically. The formula
is similar to the Apriori algorithm (Agrawal and
Srikant, 1995) that learns association rules from a
database. The first term corresponds to the confi-
dence of the algorithm, and the second term corre-
sponds to the support.
For patterns where TP = FP = 0, which
are not matched to PASs in the training corpus
(i.e., newly produced by combinations of com-
ponents), we estimates TP′ and FP′ by using
the confidence of the main and entity compo-
nents. This is because main and entity components
tend to contain pattern meanings, whereas prep,
double-prep and coord components are rather
functional. The formulas to calculate the scores
for all cases are:
</bodyText>
<equation confidence="0.998791333333333">
TP/(TP + FP) + α x TP
(TP + FP ≠ 0)
TP′/(TP′ + FP′)
(TP=FP=0, TP′ + FP′ ≠0)
0 (TP = FP = TP′ = FP′ = 0)
Score = I
</equation>
<page confidence="0.994839">
288
</page>
<table confidence="0.7086115">
Precision
Combination Pattern
</table>
<listItem confidence="0.722555818181818">
(1) Combination of components in combination
matching
(2) Main component in combination matching
(3) Entity components in combination matching
(4) Score for combination matching (SCORE)
Fragmental Pattern
(5) Matched fragmental patterns
(6) Number of PASs of example that are not matched
in fragmental matching
Raw Pattern
(7) Length of raw pattern derived from example
</listItem>
<tableCaption confidence="0.4184635">
Table 2: Features for SVM Learning of Prediction
Model
</tableCaption>
<figure confidence="0.9994356">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
ALL
SCORE
ERK
</figure>
<figureCaption confidence="0.999328">
Figure 4: Results of IE Experiment
</figureCaption>
<table confidence="0.365400666666667">
TP′ = 8 TP′main + T P′entity1(+T P′entity2)
�� (for Two-entity, Single-entity)
�� 0 (for Naive)
</table>
<equation confidence="0.83324975">
TPmain:single/(TPmain:single + FPmain:single)
FP′ = (similar to TP′ but TPx′ is replaced by FP′x)
TPmain:two/(TPmain:two + FPmain:two)
TPmain:two + FPmain:two =� 0,
for Two-entity
TP′main =
TPmain:single + FPmain:single =� 0,
for Single-entity
0 (other cases)
{ TPentityi/(TPentityi + FPentityi)
( )
TPentityi + FPentityi =� 0
0 (other cases)
�
�similar to TPx′ but TPy ′ in the
FP x ′ =
</equation>
<bodyText confidence="0.51488">
numerators is replaced by FPy′
</bodyText>
<listItem confidence="0.999757666666667">
• TP: number of TPs by the combination of components
• TPmain:two: sum of TPs by two-entity combinations
that include the same main component
• TPmain:single: sum of TPs by single-entity combina-
tions that include the same main component
• TPentityi: sum of TPs by combinations that include
the same entity component which is not the straight en-
tity component
• FPx: similar to TPx but TP is replaced by FP
</listItem>
<bodyText confidence="0.99964">
The entity component “ENTITY/NN”, which
only consists of the PAS of an entity, adds no infor-
mation to combinations of components. We call
this component a straight entity component and
exclude its effect from the scores.
</bodyText>
<subsectionHeader confidence="0.999699">
4.5 Construction of Prediction Model
</subsectionHeader>
<bodyText confidence="0.996490833333333">
We use an SVM to learn a prediction model to de-
termine whether a new protein pair is interacting.
We used SV Mlight (Joachims, 1999) with an rbf
kernel, which is known as the best kernel for most
tasks. The prediction model is based on the fea-
tures of Table 2.
</bodyText>
<figureCaption confidence="0.9664445">
Figure 5: Example Demonstrating Advantages of
Full Parsing
</figureCaption>
<sectionHeader confidence="0.999501" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.978956">
5.1 Experimental Results on the AImed
Corpus
</subsectionHeader>
<bodyText confidence="0.999793962962963">
To evaluate extraction patterns automatically con-
structed with our method, we used the AImed cor-
pus, which consists of 225 MEDLINE (U.S. Na-
tional Library of Medicine, 2006) abstracts (1969
sentences) annotated with protein names and
protein-protein interactions, for the training/test
corpora. We used tags for the protein names given.
We measured the accuracy of the IE task using
the same criterion as Bunescu and Mooney (2006),
who used an SVM to construct extraction patterns
on word/POS/type sequences from the AImed cor-
pus. That is, an extracted interaction from an ab-
stract is correct if the proteins are tagged as inter-
acting with each other somewhere in that abstract
(document-level measure).
Figure 4 plots our 10-fold cross validation and
the results of Bunescu and Mooney (2006). The
line ALL represents results when we used all fea-
tures for SVM learning. The line SCORE repre-
sents results when we extracted pairs with higher
combination matching scores than various thresh-
old values. And the line ERK represents results
by Bunescu and Mooney (2006).
The line ALL obtained our best overall F-
measure 57.3%, with 71.8% precision and 48.4%
recall. Our method was significantly better than
Bunescu and Mooney (2006) for precision be-
</bodyText>
<equation confidence="0.7567315">
f
�
�
TP′entityi =
</equation>
<page confidence="0.995954">
289
</page>
<bodyText confidence="0.999957">
tween 50% and 80%. It also needs to be noted
that SCORE, which did not use SVM learning
and only used the combination patterns, achieved
performance comparable to that by Bunescu and
Mooney (2006) for the precision range from 50%
to 80%. And for this range, introducing the frag-
mental patterns with SVM learning raised the re-
call. This range of precision is practical for the
IE task, because precision is more important than
recall for significant interactions that tend to be
described in many abstracts (as shown by the
next experiment), and too-low recall accompa-
nying too-high precision requires an excessively
large source text.
Figure 5 shows the advantage of introducing
full parsing. “FGF-2” and “KGFR” is an interact-
ing protein pair. The pattern “ENTITY1 interact
with ENTITY2” based on PASs successfully ex-
tracts this pair. However, it is difficult to extract
this pair with patterns based on surface words, be-
cause there are 5 words between “FGF-2” and “in-
teract”.
</bodyText>
<subsectionHeader confidence="0.9331845">
5.2 Experimental Results on Abstracts of
MEDLINE
</subsectionHeader>
<bodyText confidence="0.999859523809524">
We also conducted an experiment to extract in-
teracting protein pairs from a large amount of
biomedical text, i.e. about 14 million titles and
8 million abstracts in MEDLINE. We constructed
combination patterns from all 225 abstracts of the
AImed corpus, and calculated a threshold value
of combination scores that produced about 70%
precision and 30% recall on the training corpus.
We extracted protein pairs with higher combi-
nation scores than the threshold value. We ex-
cluded single-protein interactions to reduce time
consumption and we used a protein name recog-
nizer in this experiment2.
We compared the extracted pairs with a man-
ually curated database, Reactome (Joshi-Tope et
al., 2005), which published 16,564 human pro-
tein interaction pairs as pairs of Entrez Gene
IDs (U.S. National Library of Medicine, 2006).
We converted our extracted protein pairs into pairs
of Entrez Gene IDs by the protein name recog-
nizer.3 Because there may be pairs missed by Re-
</bodyText>
<footnote confidence="0.973337428571429">
2Because protein names were recognized after the pars-
ing, multi-word protein names were not concatenated.
3Although the same protein names are used for humans
and other species, these are considered to be human proteins
without checking the context. This is a fair assumption be-
cause Reactome itself infers human interaction events from
experiments on model organisms such as mice.
</footnote>
<table confidence="0.999180625">
Total 89
Parsing Error/Failure 35
(Related to coordinations) (14)
Lack of Combination Pattern Component 33
Requiring Anaphora Resolution 9
Error in Prediction Model 8
Requiring Attributive Adjectives 5
Others 10
</table>
<tableCaption confidence="0.767353333333333">
More than one cause can occur in one error, thus the sum of
all causes is larger than the total number of False Negatives.
Table 3: Causes of Error for FNs
</tableCaption>
<bodyText confidence="0.999925">
actome or pairs that our processed text did not in-
clude, we excluded extracted pairs of IDs that are
not included in Reactome and excluded Reactome
pairs of IDs that do not co-occur in the sentences
of our processed text.
After this postprocessing, we found that we had
extracted 7775 human protein pairs. Of them, 155
pairs were also included in Reactome ([a] pseudo
TPs) and 7620 pairs were not included in Reac-
tome ([b] pseudo FPs). 947 pairs of Reactome
were not extracted by our system ([c] pseudo False
Negatives (FNs)). However, these results included
pairs that Reactome missed or those that only co-
occurred and were not interacting pairs in the text.
There may also have been errors with ID assign-
ment.
To determine such cases, a biologist investi-
gated 100 pairs randomly selected from pairs of
pseudo TPs, FPs and FNs retaining their ratio of
numbers. She also checked correctness of the as-
signed IDs. 2 pairs were selected from pseudo
TPs, 88 pairs were from pseudo FPs and 10 pairs
were from pseudo FNs. The biologist found that
57 pairs were actual TPs (2 pairs of pseudo TPs
and 55 pairs of pseudo FPs) and 32 pairs were ac-
tual FPs of the pseudo FPs. Thus, the precision
was 64.0% in this sample set. Furthermore, even
if we assume that all pseudo FNs are actual FNs,
the recall can be estimated by actual TPs / (actual
TPs + pseudo FNs) × 100 = 83.8%.
These results mean that the recall of an IE sys-
tem for interacting proteins is improved for a large
amount of text even if it is low for a small corpus.
Thus, this justifies our assertion that a high degree
of precision in the low-recall range is important.
</bodyText>
<subsectionHeader confidence="0.893591">
5.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999833333333333">
Tables 3 and 4 list causes of error for FNs/FPs on
a test set of the AImed corpus using the predic-
tion model with the best F-measure with all the
</bodyText>
<page confidence="0.985629">
290
</page>
<table confidence="0.999360857142857">
Total 35
Requiring Attributive Adjectives 13
Corpus Error 11
Error in Prediction Model 5
Requiring Negation Words 2
Parsing Error 1
Others 3
</table>
<tableCaption confidence="0.99958">
Table 4: Causes of Error for FPs
</tableCaption>
<bodyText confidence="0.9995228">
features. Different to Subsection 5.1, we individ-
ually checked each occurring pair of interacting
proteins. The biggest problems were parsing er-
ror/failure, lack of necessary patterns and learning
of inappropriate patterns.
</bodyText>
<subsectionHeader confidence="0.872676">
5.3.1 Parsing Error
</subsectionHeader>
<bodyText confidence="0.99998852631579">
As listed in Table 3, 14 (40%) of the 35 pars-
ing errors/failures were related to coordinations.
Many of these were caused by differences in the
characteristics of the PTB/GTB, the training cor-
pora for Enju, and the AImed Corpus. For ex-
ample, Enju failed to obtain the correct structure
for “the ENTITY1 / ENTITY1 complex” because
words in the PTB/GTB are not segmented with
“/” and Enju could not be trained on such a case.
One method to solve this problem is to avoid seg-
menting words with “/” and introducing extraction
patterns based on surface characters, such as “EN-
TITY1/ENTITY2 complex”.
Parsing errors are intrinsic problems to IE meth-
ods using parsing. However, from Table 3, we can
conclude that the key to gaining better accuracy
is refining of the method with which the PAS pat-
terns are constructed (there were 46 related FNs)
rather than improving parsing (there were 35 FNs).
</bodyText>
<subsectionHeader confidence="0.9910375">
5.3.2 Lack of Necessary Patterns and
Learning of Inappropriate Patterns
</subsectionHeader>
<bodyText confidence="0.999881166666667">
There are two different reasons causing the
problems with the lack of necessary patterns and
the learning of inappropriate patterns: (1) the
training corpus was not sufficiently large to sat-
urate IE accuracy and (2) our method of pattern
construction was too limited.
</bodyText>
<subsectionHeader confidence="0.853927">
Effect of Training Corpus Size To investigate
</subsectionHeader>
<bodyText confidence="0.999919714285714">
whether the training corpus was large enough to
maximize IE accuracy, we conducted experiments
on training corpora of various sizes. Figure 6 plots
graphs of F-measures by SCORE and Figure 7
plots the number of combination patterns on train-
ing corpora of various sizes. From Figures 6 and 7,
the training corpus (207 abstracts at a maximum)
</bodyText>
<figure confidence="0.9172445">
0 50 100 150 200
Training Corpus Size (Number of Abstracts)
</figure>
<figureCaption confidence="0.997859">
Figure 6: Effect of Training Corpus Size (1)
</figureCaption>
<figure confidence="0.999067">
600
500
400
300
200
100
0
0 50 100 150 200
Training Corpus Size (Number of Abstracts)
Raw Patterns (before division)
Main Component
Entity Component
Other Component
Naive Pattern
</figure>
<figureCaption confidence="0.999993">
Figure 7: Effect of Training Corpus Size (2)
</figureCaption>
<bodyText confidence="0.995589260869565">
is not large enough. Thus increasing corpus size
will further improve IE accuracy.
Limitation of the Present Pattern Construc-
tion The limitations with our pattern construc-
tion method are revealed by the fact that we
could not achieve a high precision like Bunescu
and Mooney (2006) within the high-recall range.
Compared to theirs, one of our problems is that our
method could not handle attributives. One exam-
ple is “binding property of ENTITY1 to ENTITY2”.
We could not obtain “binding” because the small-
est set of PASs connecting “ENTITY1” and “EN-
TITY2” includes only the PASs of “property”, “of”
and “to”. To handle these attributives, we need dis-
tinguish necessary attributives from those that are
general4 by semantic analysis or bootstrapping.
Another approach to improve our method is to
include local information in sentences, such as
surface words between protein names. Zhao and
Grishman (2005) reported that adding local infor-
mation to deep syntactic information improved IE
results. This approach is also applicable to IE in
other domains, where related entities are in a short
</bodyText>
<footnote confidence="0.982123">
4Consider the case where a source sentence for a pattern is
“ENTITY1 is an important homodimeric protein.” (“homod-
imeric” represents that two molecules of “ENTITY1” interact
with each other.)
</footnote>
<figure confidence="0.997927">
F-measure by SCORE 0.55
0.5
0.45
0.4
0.35
Number
</figure>
<page confidence="0.990327">
291
</page>
<bodyText confidence="0.962028">
distance like the work of Zhou et al. (2005).
</bodyText>
<sectionHeader confidence="0.997041" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999941909090909">
We proposed the use of PASs to construct pat-
terns as extraction rules, utilizing their ability to
abstract syntactical variants with the same rela-
tion. In addition, we divided the patterns for gen-
eralization, and used matching results for SVM
learning. In experiments on extracting of protein-
protein interactions, we obtained 71.8% precision
and 48.4% recall on a small corpus and 64.0% pre-
cision and 83.8% recall estimated on a large text,
which demonstrated the obvious advantages of our
method.
</bodyText>
<sectionHeader confidence="0.98763" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9974458">
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas “Sys-
tems Genomics” (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99993727027027">
R. Agrawal and R Srikant. 1995. Mining Sequential
Patterns. In Proc. the 11th International Conference
on Data Engineering, pages 3–14.
Christian Blaschke and Alfonso Valencia. 2002. The
Frame-Based Module of the SUISEKI Informa-
tion Extraction System. IEEE Intelligent Systems,
17(2):14–20.
Razvan Bunescu and Raymond J. Mooney. 2004.
Collective information extraction with relational
markov networks. In Proc. ACL’04, pages 439–446.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
Shortest Path Dependency Kernel for Relation Ex-
traction. In Proc. HLT/EMNLP 2005, pages 724–
731.
Razvan Bunescu and Raymond Mooney. 2006. Subse-
quence kernels for relation extraction. In Advances
in Neural Information Processing Systems 18, pages
171–178. MIT Press.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proc. ACL’04,
pages 423–429.
Yu Hao, Xiaoyan Zhu, Minlie Huang, and Ming
Li. 2005. Discovering patterns to extract protein-
protein interactions from the literature: Part II.
Bioinformatics, 21(15):3294–3300.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Advances in Kernel Methods
– Support Vector Learning. MIT-Press.
G Joshi-Tope, M Gillespie, I Vastrik, P D’Eustachio,
E Schmidt, B de Bono, B Jassal, GR Gopinath,
GR Wu, L Matthews, S Lewis, E Birney, and Stein
L. 2005. Reactome: a knowledgebase of biologi-
cal pathways. Nucleic Acids Research, 33(Database
Issue):D428–D432.
Asako Koike, Yoshiyuki Kobayashi, and Toshihisa
Takagi. 2003. Kinase Pathway Database: An
Integrated Protein-Kinase and NLP-Based Protein-
Interaction Resource. Genome Research, 13:1231–
1243.
Linguistic Data Consortium. 2005. ACE Program.
http://projects.ldc.upenn.edu/ace/.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proc. AAI ’94.
Ivan A. Sag and Thomas Wasow. 1999. Syntactic The-
ory. CSLI publications.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic IE pattern acquisition. In
Proc. ACL 2003, pages 224–231.
Tsujii Laboratory. 2005a. Enju - A practical HPSG
parser. http://www-tsujii.is.s.u-tokyo.ac.jp/enju/.
Tsujii Laboratory. 2005b. GENIA Project.
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/.
U.S. National Library of Medicine. 2006. PubMed.
http://www.pubmed.gov.
Akane Yakushiji, Yusuke Miyao, Yuka Tateisi, and
Jun’ichi Tsujii. 2005. Biomedical information ex-
traction with predicate-argument structure patterns.
In Proc. SMBM 2005, pages 60–69.
Daming Yao, Jingbo Wang, Yanmei Lu, Nathan No-
ble, Huandong Sun, Xiaoyan Zhu, Nan Lin, Don-
ald G. Payan, Ming Li, and Kunbin Qu. 2004. Path-
wayFinder: Paving The Way Towards Automatic
Pathway Extraction. In Bioinformatics 2004: Proc.
the 2nd APBC, volume 29 of CRPIT, pages 53–62.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proc. ACL’05, pages 419–426.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proc. ACL’05, pages 427–434.
</reference>
<page confidence="0.997491">
292
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.724277">
<title confidence="0.997321">Automatic Construction of Predicate-argument Structure for Biomedical Information Extraction</title>
<author confidence="0.92057">Tomoko</author>
<affiliation confidence="0.950375">of Computer Science, University of Tokyo</affiliation>
<address confidence="0.815757">Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN</address>
<affiliation confidence="0.969109">of Informatics, University of Manchester</affiliation>
<address confidence="0.989963">POBox 88, Sackville St, MANCHESTER M60 1QD, UK</address>
<email confidence="0.972881">yusuke,okap,yucca,</email>
<abstract confidence="0.999415476190476">This paper presents a method of automatconstructing information predicate-argument structures (PASs) obtained by full parsing from a smaller training corpus. Because PASs represent generalized structures for syntactical variants, patterns on PASs are expected to be more generalized than those on surface words. In addition, patterns are divided into components to improve recall and we introduce a Support Vector Machine to learn a prediction model using pattern matching results. In this paper, we present experimental results and analyze them on how well protein-protein interactions were extracted from MEDLINE abstracts. The results demonstrated that our method improved accuracy compared to a machine learning approach using surface word/part-of-speech patterns.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Agrawal</author>
<author>R Srikant</author>
</authors>
<title>Mining Sequential Patterns.</title>
<date>1995</date>
<booktitle>In Proc. the 11th International Conference on Data Engineering,</booktitle>
<pages>3--14</pages>
<contexts>
<context position="19200" citStr="Agrawal and Srikant, 1995" startWordPosition="3050" endWordPosition="3053">eptor”.) Scores are derived from the results of combination matching to the source training corpus. We apply the combination patterns to the training corpus, and count pairs of True Positives (TP) and False Positives (FP). The scores are calculated basically by the following formula: Score = TP/(TP + FP) + α x TP This formula is based on the precision of the pattern on the training corpus, i.e., an estimated precision on a test corpus. α works for smoothing, that is, to accept only patterns of large TP when FP = 0. α is set as 0.01 empirically. The formula is similar to the Apriori algorithm (Agrawal and Srikant, 1995) that learns association rules from a database. The first term corresponds to the confidence of the algorithm, and the second term corresponds to the support. For patterns where TP = FP = 0, which are not matched to PASs in the training corpus (i.e., newly produced by combinations of components), we estimates TP′ and FP′ by using the confidence of the main and entity components. This is because main and entity components tend to contain pattern meanings, whereas prep, double-prep and coord components are rather functional. The formulas to calculate the scores for all cases are: TP/(TP + FP) + </context>
</contexts>
<marker>Agrawal, Srikant, 1995</marker>
<rawString>R. Agrawal and R Srikant. 1995. Mining Sequential Patterns. In Proc. the 11th International Conference on Data Engineering, pages 3–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Blaschke</author>
<author>Alfonso Valencia</author>
</authors>
<title>The Frame-Based Module of the SUISEKI Information Extraction System.</title>
<date>2002</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>17</volume>
<issue>2</issue>
<contexts>
<context position="1480" citStr="Blaschke and Valencia, 2002" startWordPosition="206" endWordPosition="209">and we introduce a Support Vector Machine to learn a prediction model using pattern matching results. In this paper, we present experimental results and analyze them on how well protein-protein interactions were extracted from MEDLINE abstracts. The results demonstrated that our method improved accuracy compared to a machine learning approach using surface word/part-of-speech patterns. 1 Introduction One primitive approach to Information Extraction (IE) is to manually craft numerous extraction patterns for particular applications and this is presently one of the main streams of biomedical IE (Blaschke and Valencia, 2002; Koike et al., 2003). Although such IE attempts have demonstrated near-practical performance, the same sets of patterns cannot be applied to different kinds of information. A real-world task requires several kinds of IE, thus manually engineering extraction Current Affiliation: † FUJITSU LABORATORIES LTD. ‡ Faculty of Informatics, Kogakuin University patterns, which is tedious and time-consuming process, is not really practical. Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE. Ho</context>
</contexts>
<marker>Blaschke, Valencia, 2002</marker>
<rawString>Christian Blaschke and Alfonso Valencia. 2002. The Frame-Based Module of the SUISEKI Information Extraction System. IEEE Intelligent Systems, 17(2):14–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Collective information extraction with relational markov networks.</title>
<date>2004</date>
<booktitle>In Proc. ACL’04,</booktitle>
<pages>439--446</pages>
<contexts>
<context position="5128" citStr="Bunescu and Mooney, 2004" startWordPosition="781" endWordPosition="784">used dependency trees and paths on full parse trees in their experiment. Such structures did not include information of semantic subjects/objects, which full parsing can recognize. Additionally, most relations they extracted from the ACE corpus (Linguistic Data Consortium, 2005) on broadcasts and newswires were within very short word-distance (70% where two entities are embedded in each other or separated by at most one word), and therefore shallow information was beneficial. However, Table 1 shows that the word distance is long between interacting protein names annotated on the AImed corpus (Bunescu and Mooney, 2004), and we have to treat long-distance relations for information like protein-protein interactions. Full parsing is more effective for acquiring generalized data from long-length words than shallow parsing. The sentences at left in Figure 1 exemplify the advantages of full parsing. The gerund “activating” in the last sentence takes a non-local semantic subject “ENTITY]”, and shallow parsing cannot recognize this relation because “ENTITY]” and “activating” are in different phrases. Full parsing, on the other hand, can identify both the subject of the whole sentence and the semantic subject of “ac</context>
</contexts>
<marker>Bunescu, Mooney, 2004</marker>
<rawString>Razvan Bunescu and Raymond J. Mooney. 2004. Collective information extraction with relational markov networks. In Proc. ACL’04, pages 439–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A Shortest Path Dependency Kernel for Relation Extraction.</title>
<date>2005</date>
<booktitle>In Proc. HLT/EMNLP</booktitle>
<pages>724--731</pages>
<contexts>
<context position="7479" citStr="Bunescu and Mooney (2005)" startWordPosition="1154" endWordPosition="1157"> 2, ...), MOD. For example, the semantic subject of a transitive verb is ARG1 and the semantic object is ARG2. What is important here is, thanks to the strong normalization of syntactic variations, that we can expect that the construction algorithm for extracting patterns that works on PASs will need a much smaller training corpus than those working on surface-word sequences. Furthermore, because of the reduced diversity of surface-word sequences at the PAS level, any IE system at this level should demonstrate improved recall. 3 Related Work Sudo et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005) acquired substructures derived from dependency trees as extraction patterns for IE in general domains. Their approaches were similar to our approach using PASs derived from full parsing. However, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in Section 2), and thus rules acquired from the constructions were partial. Bunescu and Mooney (2006) also learned extraction patterns for protein-protein interactions by SVM with a generalized subsequence kernel. Their patterns are sequences of words, POSs, </context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. A Shortest Path Dependency Kernel for Relation Extraction. In Proc. HLT/EMNLP 2005, pages 724– 731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 18,</booktitle>
<pages>171--178</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2014" citStr="Bunescu and Mooney, 2006" startWordPosition="284" endWordPosition="287">and this is presently one of the main streams of biomedical IE (Blaschke and Valencia, 2002; Koike et al., 2003). Although such IE attempts have demonstrated near-practical performance, the same sets of patterns cannot be applied to different kinds of information. A real-world task requires several kinds of IE, thus manually engineering extraction Current Affiliation: † FUJITSU LABORATORIES LTD. ‡ Faculty of Informatics, Kogakuin University patterns, which is tedious and time-consuming process, is not really practical. Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE. However, in most cases, the cost of manually crafting patterns is simply transferred to that for constructing a large amount of training data, which requires tedious amount of manual labor to annotate text. To systematically reduce the necessary amount of training data, we divided the task of constructing extraction patterns into a subtask that general natural language processing techniques can solve and a subtask that has specific properties according to the information to be extracted. The former subtask is of full parsing (i.e.</context>
<context position="7921" citStr="Bunescu and Mooney (2006)" startWordPosition="1223" endWordPosition="1226">equences at the PAS level, any IE system at this level should demonstrate improved recall. 3 Related Work Sudo et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005) acquired substructures derived from dependency trees as extraction patterns for IE in general domains. Their approaches were similar to our approach using PASs derived from full parsing. However, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in Section 2), and thus rules acquired from the constructions were partial. Bunescu and Mooney (2006) also learned extraction patterns for protein-protein interactions by SVM with a generalized subsequence kernel. Their patterns are sequences of words, POSs, entity types, etc., and they heuristically restricted length and word positions of the patterns. Al285 ENTITY] recognizes and activates ENTITY2. ENTITY2 activated by ENTITY] are not well characterized. The herpesvirus encodes a functional ENTITY] that activates human ENTITY2. ENTITY] can functionally cooperate to synergistically activate ENTITY2. The ENTITY] plays key roles by activating ENTITY2. Figure 1: Syntactical Variations of “activ</context>
<context position="22492" citStr="Bunescu and Mooney (2006)" startWordPosition="3614" endWordPosition="3617">t tasks. The prediction model is based on the features of Table 2. Figure 5: Example Demonstrating Advantages of Full Parsing 5 Results and Discussion 5.1 Experimental Results on the AImed Corpus To evaluate extraction patterns automatically constructed with our method, we used the AImed corpus, which consists of 225 MEDLINE (U.S. National Library of Medicine, 2006) abstracts (1969 sentences) annotated with protein names and protein-protein interactions, for the training/test corpora. We used tags for the protein names given. We measured the accuracy of the IE task using the same criterion as Bunescu and Mooney (2006), who used an SVM to construct extraction patterns on word/POS/type sequences from the AImed corpus. That is, an extracted interaction from an abstract is correct if the proteins are tagged as interacting with each other somewhere in that abstract (document-level measure). Figure 4 plots our 10-fold cross validation and the results of Bunescu and Mooney (2006). The line ALL represents results when we used all features for SVM learning. The line SCORE represents results when we extracted pairs with higher combination matching scores than various threshold values. And the line ERK represents res</context>
<context position="30505" citStr="Bunescu and Mooney (2006)" startWordPosition="4966" endWordPosition="4969"> at a maximum) 0 50 100 150 200 Training Corpus Size (Number of Abstracts) Figure 6: Effect of Training Corpus Size (1) 600 500 400 300 200 100 0 0 50 100 150 200 Training Corpus Size (Number of Abstracts) Raw Patterns (before division) Main Component Entity Component Other Component Naive Pattern Figure 7: Effect of Training Corpus Size (2) is not large enough. Thus increasing corpus size will further improve IE accuracy. Limitation of the Present Pattern Construction The limitations with our pattern construction method are revealed by the fact that we could not achieve a high precision like Bunescu and Mooney (2006) within the high-recall range. Compared to theirs, one of our problems is that our method could not handle attributives. One example is “binding property of ENTITY1 to ENTITY2”. We could not obtain “binding” because the smallest set of PASs connecting “ENTITY1” and “ENTITY2” includes only the PASs of “property”, “of” and “to”. To handle these attributives, we need distinguish necessary attributives from those that are general4 by semantic analysis or bootstrapping. Another approach to improve our method is to include local information in sentences, such as surface words between protein names. </context>
</contexts>
<marker>Bunescu, Mooney, 2006</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2006. Subsequence kernels for relation extraction. In Advances in Neural Information Processing Systems 18, pages 171–178. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proc. ACL’04,</booktitle>
<pages>423--429</pages>
<contexts>
<context position="7449" citStr="Culotta and Sorensen (2004)" startWordPosition="1149" endWordPosition="1152">arrow labels, i.e., ARGn (n = 1, 2, ...), MOD. For example, the semantic subject of a transitive verb is ARG1 and the semantic object is ARG2. What is important here is, thanks to the strong normalization of syntactic variations, that we can expect that the construction algorithm for extracting patterns that works on PASs will need a much smaller training corpus than those working on surface-word sequences. Furthermore, because of the reduced diversity of surface-word sequences at the PAS level, any IE system at this level should demonstrate improved recall. 3 Related Work Sudo et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005) acquired substructures derived from dependency trees as extraction patterns for IE in general domains. Their approaches were similar to our approach using PASs derived from full parsing. However, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in Section 2), and thus rules acquired from the constructions were partial. Bunescu and Mooney (2006) also learned extraction patterns for protein-protein interactions by SVM with a generalized subsequence kernel. Their patterns </context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proc. ACL’04, pages 423–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Hao</author>
<author>Xiaoyan Zhu</author>
<author>Minlie Huang</author>
<author>Ming Li</author>
</authors>
<title>Discovering patterns to extract proteinprotein interactions from the literature: Part II.</title>
<date>2005</date>
<journal>Bioinformatics,</journal>
<volume>21</volume>
<issue>15</issue>
<contexts>
<context position="1987" citStr="Hao et al., 2005" startWordPosition="280" endWordPosition="283">ular applications and this is presently one of the main streams of biomedical IE (Blaschke and Valencia, 2002; Koike et al., 2003). Although such IE attempts have demonstrated near-practical performance, the same sets of patterns cannot be applied to different kinds of information. A real-world task requires several kinds of IE, thus manually engineering extraction Current Affiliation: † FUJITSU LABORATORIES LTD. ‡ Faculty of Informatics, Kogakuin University patterns, which is tedious and time-consuming process, is not really practical. Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE. However, in most cases, the cost of manually crafting patterns is simply transferred to that for constructing a large amount of training data, which requires tedious amount of manual labor to annotate text. To systematically reduce the necessary amount of training data, we divided the task of constructing extraction patterns into a subtask that general natural language processing techniques can solve and a subtask that has specific properties according to the information to be extracted. The former subta</context>
<context position="8692" citStr="Hao et al. (2005)" startWordPosition="1335" endWordPosition="1338">Ss, entity types, etc., and they heuristically restricted length and word positions of the patterns. Al285 ENTITY] recognizes and activates ENTITY2. ENTITY2 activated by ENTITY] are not well characterized. The herpesvirus encodes a functional ENTITY] that activates human ENTITY2. ENTITY] can functionally cooperate to synergistically activate ENTITY2. The ENTITY] plays key roles by activating ENTITY2. Figure 1: Syntactical Variations of “activate” though they achieved about 60% precision and about 40% recall, these heuristic restrictions could not be guaranteed to be applied to other IE tasks. Hao et al. (2005) learned extraction patterns for protein-protein interactions as sequences of words, POSs, entity tags and gaps by dynamic programming, and reduced/merged them using a minimum description length-based algorithm. Although they achieved 79.8% precision and 59.5% recall, sentences in their test corpus have too many positive instances and some of the patterns they claimed to have been successfully constructed went against linguistic or biomedical intuition. (e.g. “ENTITY] and interacts with ENTITY2” should be replaced by a more general pattern because they aimed to reduce the number of patterns.) </context>
</contexts>
<marker>Hao, Zhu, Huang, Li, 2005</marker>
<rawString>Yu Hao, Xiaoyan Zhu, Minlie Huang, and Ming Li. 2005. Discovering patterns to extract proteinprotein interactions from the literature: Part II. Bioinformatics, 21(15):3294–3300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large-Scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods – Support Vector Learning. MIT-Press.</booktitle>
<contexts>
<context position="21805" citStr="Joachims, 1999" startWordPosition="3504" endWordPosition="3505">ntity combinations that include the same main component • TPentityi: sum of TPs by combinations that include the same entity component which is not the straight entity component • FPx: similar to TPx but TP is replaced by FP The entity component “ENTITY/NN”, which only consists of the PAS of an entity, adds no information to combinations of components. We call this component a straight entity component and exclude its effect from the scores. 4.5 Construction of Prediction Model We use an SVM to learn a prediction model to determine whether a new protein pair is interacting. We used SV Mlight (Joachims, 1999) with an rbf kernel, which is known as the best kernel for most tasks. The prediction model is based on the features of Table 2. Figure 5: Example Demonstrating Advantages of Full Parsing 5 Results and Discussion 5.1 Experimental Results on the AImed Corpus To evaluate extraction patterns automatically constructed with our method, we used the AImed corpus, which consists of 225 MEDLINE (U.S. National Library of Medicine, 2006) abstracts (1969 sentences) annotated with protein names and protein-protein interactions, for the training/test corpora. We used tags for the protein names given. We mea</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making Large-Scale SVM Learning Practical. In Advances in Kernel Methods – Support Vector Learning. MIT-Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Joshi-Tope</author>
<author>M Gillespie</author>
<author>I Vastrik</author>
<author>P D’Eustachio</author>
<author>E Schmidt</author>
<author>B de Bono</author>
<author>B Jassal</author>
<author>GR Gopinath</author>
<author>GR Wu</author>
<author>L Matthews</author>
<author>S Lewis</author>
<author>E Birney</author>
<author>L Stein</author>
</authors>
<title>Reactome: a knowledgebase of biological pathways.</title>
<date>2005</date>
<booktitle>Nucleic Acids Research, 33(Database Issue):D428–D432.</booktitle>
<marker>Joshi-Tope, Gillespie, Vastrik, D’Eustachio, Schmidt, de Bono, Jassal, Gopinath, Wu, Matthews, Lewis, Birney, Stein, 2005</marker>
<rawString>G Joshi-Tope, M Gillespie, I Vastrik, P D’Eustachio, E Schmidt, B de Bono, B Jassal, GR Gopinath, GR Wu, L Matthews, S Lewis, E Birney, and Stein L. 2005. Reactome: a knowledgebase of biological pathways. Nucleic Acids Research, 33(Database Issue):D428–D432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asako Koike</author>
<author>Yoshiyuki Kobayashi</author>
<author>Toshihisa Takagi</author>
</authors>
<title>Kinase Pathway Database: An Integrated Protein-Kinase and NLP-Based ProteinInteraction Resource.</title>
<date>2003</date>
<journal>Genome Research,</journal>
<volume>13</volume>
<pages>1243</pages>
<contexts>
<context position="1501" citStr="Koike et al., 2003" startWordPosition="210" endWordPosition="213">ctor Machine to learn a prediction model using pattern matching results. In this paper, we present experimental results and analyze them on how well protein-protein interactions were extracted from MEDLINE abstracts. The results demonstrated that our method improved accuracy compared to a machine learning approach using surface word/part-of-speech patterns. 1 Introduction One primitive approach to Information Extraction (IE) is to manually craft numerous extraction patterns for particular applications and this is presently one of the main streams of biomedical IE (Blaschke and Valencia, 2002; Koike et al., 2003). Although such IE attempts have demonstrated near-practical performance, the same sets of patterns cannot be applied to different kinds of information. A real-world task requires several kinds of IE, thus manually engineering extraction Current Affiliation: † FUJITSU LABORATORIES LTD. ‡ Faculty of Informatics, Kogakuin University patterns, which is tedious and time-consuming process, is not really practical. Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE. However, in most cases,</context>
<context position="3432" citStr="Koike et al., 2003" startWordPosition="507" endWordPosition="510"> structures. We adopted full parsing from various levels of parsing, because we believe that it offers the best utility to generalize sentences into normalized syntactic relations. We also divided patterns into components to improve recall and we introduced machine learning with a Support Vector Machine (SVM) to learn a prediction model using the matching results of extraction patterns. As an actual IE task, we extracted pairs of interacting protein names from biomedical text. 2 Full Parsing 2.1 Necessity for Full Parsing A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005). Their assertion is 284 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 284–292, Sydney, July 2006. c�2006 Association for Computational Linguistics Distance Count (%) Sum (%) −1 54 5.0 5.0 0 8 0.7 5.7 1 170 15.7 21.4 2–5 337 31.1 52.5 6–10 267 24.6 77.1 11– 248 22.9 100.0 Distance −1 means protein word has been annotated as interacting with itself (e.g. “actin polymerization”). Distance 0 means words of the interacting proteins are directly next to one another. Multi-word protein names are concat</context>
</contexts>
<marker>Koike, Kobayashi, Takagi, 2003</marker>
<rawString>Asako Koike, Yoshiyuki Kobayashi, and Toshihisa Takagi. 2003. Kinase Pathway Database: An Integrated Protein-Kinase and NLP-Based ProteinInteraction Resource. Genome Research, 13:1231– 1243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<date>2005</date>
<note>ACE Program. http://projects.ldc.upenn.edu/ace/.</note>
<contexts>
<context position="4782" citStr="Consortium, 2005" startWordPosition="726" endWordPosition="727">e more robust and would be sufficient for IE. However, their claims that shallow parsers are sufficient, or that full parsers do not contribute to application tasks, have not been fully proved by experimental results. Zhou et al. (2005) argued that most information useful for IE derived from full parsing was shallow. However, they only used dependency trees and paths on full parse trees in their experiment. Such structures did not include information of semantic subjects/objects, which full parsing can recognize. Additionally, most relations they extracted from the ACE corpus (Linguistic Data Consortium, 2005) on broadcasts and newswires were within very short word-distance (70% where two entities are embedded in each other or separated by at most one word), and therefore shallow information was beneficial. However, Table 1 shows that the word distance is long between interacting protein names annotated on the AImed corpus (Bunescu and Mooney, 2004), and we have to treat long-distance relations for information like protein-protein interactions. Full parsing is more effective for acquiring generalized data from long-length words than shallow parsing. The sentences at left in Figure 1 exemplify the a</context>
</contexts>
<marker>Consortium, 2005</marker>
<rawString>Linguistic Data Consortium. 2005. ACE Program. http://projects.ldc.upenn.edu/ace/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proc. AAI ’94.</booktitle>
<contexts>
<context position="6118" citStr="Marcus et al., 1994" startWordPosition="931" endWordPosition="934">, and shallow parsing cannot recognize this relation because “ENTITY]” and “activating” are in different phrases. Full parsing, on the other hand, can identify both the subject of the whole sentence and the semantic subject of “activating” have been shared. 2.2 Predicate-argument Structures We applied Enju (Tsujii Laboratory, 2005a) as a full parser which outputs predicate-argument structures (PASs). PASs are well normalized forms that represent syntactic relations. Enju is based on Head-driven Phrase Structure Grammar (Sag and Wasow, 1999), and it has been trained on the Penn Treebank (PTB) (Marcus et al., 1994) and a biomedical corpus, the GENIA Treebank (GTB) (Tsujii Laboratory, 2005b). We used a part-of-speech (POS) tagger trained on the GENIA corpus (Tsujii Laboratory, 2005b) as a preprocessor for Enju. On predicate-argument relations, Enju achieved 88.0% precision and 87.2% recall on PTB, and 87.1% precision and 85.4% recall on GTB. The illustration at right in Figure 1 is a PAS example, which represents the relation between “activate”, “ENTITY]” and “ENTITY2” of all sentences to the left. The predicate and its arguments are words converted to their base forms, augmented by their POSs. The arrow</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proc. AAI ’94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Thomas Wasow</author>
</authors>
<title>Syntactic Theory.</title>
<date>1999</date>
<publisher>CSLI publications.</publisher>
<contexts>
<context position="6044" citStr="Sag and Wasow, 1999" startWordPosition="917" endWordPosition="920">ivating” in the last sentence takes a non-local semantic subject “ENTITY]”, and shallow parsing cannot recognize this relation because “ENTITY]” and “activating” are in different phrases. Full parsing, on the other hand, can identify both the subject of the whole sentence and the semantic subject of “activating” have been shared. 2.2 Predicate-argument Structures We applied Enju (Tsujii Laboratory, 2005a) as a full parser which outputs predicate-argument structures (PASs). PASs are well normalized forms that represent syntactic relations. Enju is based on Head-driven Phrase Structure Grammar (Sag and Wasow, 1999), and it has been trained on the Penn Treebank (PTB) (Marcus et al., 1994) and a biomedical corpus, the GENIA Treebank (GTB) (Tsujii Laboratory, 2005b). We used a part-of-speech (POS) tagger trained on the GENIA corpus (Tsujii Laboratory, 2005b) as a preprocessor for Enju. On predicate-argument relations, Enju achieved 88.0% precision and 87.2% recall on PTB, and 87.1% precision and 85.4% recall on GTB. The illustration at right in Figure 1 is a PAS example, which represents the relation between “activate”, “ENTITY]” and “ENTITY2” of all sentences to the left. The predicate and its arguments a</context>
</contexts>
<marker>Sag, Wasow, 1999</marker>
<rawString>Ivan A. Sag and Thomas Wasow. 1999. Syntactic Theory. CSLI publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Sudo</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>An improved extraction pattern representation model for automatic IE pattern acquisition.</title>
<date>2003</date>
<booktitle>In Proc. ACL</booktitle>
<pages>224--231</pages>
<contexts>
<context position="7420" citStr="Sudo et al. (2003)" startWordPosition="1145" endWordPosition="1148">ts are indicated as arrow labels, i.e., ARGn (n = 1, 2, ...), MOD. For example, the semantic subject of a transitive verb is ARG1 and the semantic object is ARG2. What is important here is, thanks to the strong normalization of syntactic variations, that we can expect that the construction algorithm for extracting patterns that works on PASs will need a much smaller training corpus than those working on surface-word sequences. Furthermore, because of the reduced diversity of surface-word sequences at the PAS level, any IE system at this level should demonstrate improved recall. 3 Related Work Sudo et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005) acquired substructures derived from dependency trees as extraction patterns for IE in general domains. Their approaches were similar to our approach using PASs derived from full parsing. However, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in Section 2), and thus rules acquired from the constructions were partial. Bunescu and Mooney (2006) also learned extraction patterns for protein-protein interactions by SVM with a generalized subseq</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003. An improved extraction pattern representation model for automatic IE pattern acquisition. In Proc. ACL 2003, pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsujii Laboratory</author>
</authors>
<title>Enju - A practical HPSG</title>
<date>2005</date>
<note>parser. http://www-tsujii.is.s.u-tokyo.ac.jp/enju/.</note>
<contexts>
<context position="5830" citStr="Laboratory, 2005" startWordPosition="888" endWordPosition="889">teractions. Full parsing is more effective for acquiring generalized data from long-length words than shallow parsing. The sentences at left in Figure 1 exemplify the advantages of full parsing. The gerund “activating” in the last sentence takes a non-local semantic subject “ENTITY]”, and shallow parsing cannot recognize this relation because “ENTITY]” and “activating” are in different phrases. Full parsing, on the other hand, can identify both the subject of the whole sentence and the semantic subject of “activating” have been shared. 2.2 Predicate-argument Structures We applied Enju (Tsujii Laboratory, 2005a) as a full parser which outputs predicate-argument structures (PASs). PASs are well normalized forms that represent syntactic relations. Enju is based on Head-driven Phrase Structure Grammar (Sag and Wasow, 1999), and it has been trained on the Penn Treebank (PTB) (Marcus et al., 1994) and a biomedical corpus, the GENIA Treebank (GTB) (Tsujii Laboratory, 2005b). We used a part-of-speech (POS) tagger trained on the GENIA corpus (Tsujii Laboratory, 2005b) as a preprocessor for Enju. On predicate-argument relations, Enju achieved 88.0% precision and 87.2% recall on PTB, and 87.1% precision and </context>
</contexts>
<marker>Laboratory, 2005</marker>
<rawString>Tsujii Laboratory. 2005a. Enju - A practical HPSG parser. http://www-tsujii.is.s.u-tokyo.ac.jp/enju/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsujii Laboratory</author>
</authors>
<date>2005</date>
<note>GENIA Project. http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/.</note>
<contexts>
<context position="5830" citStr="Laboratory, 2005" startWordPosition="888" endWordPosition="889">teractions. Full parsing is more effective for acquiring generalized data from long-length words than shallow parsing. The sentences at left in Figure 1 exemplify the advantages of full parsing. The gerund “activating” in the last sentence takes a non-local semantic subject “ENTITY]”, and shallow parsing cannot recognize this relation because “ENTITY]” and “activating” are in different phrases. Full parsing, on the other hand, can identify both the subject of the whole sentence and the semantic subject of “activating” have been shared. 2.2 Predicate-argument Structures We applied Enju (Tsujii Laboratory, 2005a) as a full parser which outputs predicate-argument structures (PASs). PASs are well normalized forms that represent syntactic relations. Enju is based on Head-driven Phrase Structure Grammar (Sag and Wasow, 1999), and it has been trained on the Penn Treebank (PTB) (Marcus et al., 1994) and a biomedical corpus, the GENIA Treebank (GTB) (Tsujii Laboratory, 2005b). We used a part-of-speech (POS) tagger trained on the GENIA corpus (Tsujii Laboratory, 2005b) as a preprocessor for Enju. On predicate-argument relations, Enju achieved 88.0% precision and 87.2% recall on PTB, and 87.1% precision and </context>
</contexts>
<marker>Laboratory, 2005</marker>
<rawString>Tsujii Laboratory. 2005b. GENIA Project. http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U S National</author>
</authors>
<title>Library of Medicine.</title>
<date>2006</date>
<note>PubMed. http://www.pubmed.gov.</note>
<marker>National, 2006</marker>
<rawString>U.S. National Library of Medicine. 2006. PubMed. http://www.pubmed.gov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akane Yakushiji</author>
<author>Yusuke Miyao</author>
<author>Yuka Tateisi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Biomedical information extraction with predicate-argument structure patterns.</title>
<date>2005</date>
<booktitle>In Proc. SMBM</booktitle>
<pages>60--69</pages>
<contexts>
<context position="10921" citStr="Yakushiji et al. (2005)" startWordPosition="1697" endWordPosition="1700">e PASs and (3) calculate scores for combination pattern matching. (4) We use the prediction model to predict interacting pairs. Figure 2: Extraction of Raw Pattern 4.1 Full Parsing and Extraction of Raw Patterns As the first step in both the construction phase and application phase of extraction patterns, we parse sentences into PASs using Enju.1 We label all PASs of the protein names as protein PASs. After parsing, we extract the smallest set of PASs, which connect words that denote interacting proteins, and make it a raw pattern. We take the same method to extract and refine raw patterns as Yakushiji et al. (2005). Connecting means we can trace predicate-argument relations from one protein word to the other in an interacting pair. The procedure to obtain a raw pattern (p0, . . . , pn) is as follows: predicate(p): PASs that have p as their argument argument(p): PASs that p has as its arguments 1. pi = p0 is the PAS of a word correspondent to one of interacting proteins, and we obtain candidates of the raw pattern as follows: 1-1. If pi is of the word of the other interacting protein, (p0, ... , pi) is a candidate of the raw pattern. 1-2. If not, make pattern candidates for each pi+1 E predicate(pi) U ar</context>
<context position="13158" citStr="Yakushiji et al. (2005)" startWordPosition="2084" endWordPosition="2087">rn is not appropriate and we need to refine it. One case is when unnecessary coordinations/parentheses are included in the pattern, e.g. two interactions are described in a combined representation (“ENTITY1 binds this protein and ENTITY2”). Another is when two interacting proteins are connected directly by a conjunction or only one protein participates in an interaction. In such cases, we refine patterns by unfolding of coordinations/parentheses and extension of patterns, respectively. We have omitted detailed explanations because of space limitations. The details are described in the work of Yakushiji et al. (2005). 4.2 Division of Patterns Division for generating combination patterns is based on observation of Yakushiji et al. (2005) that there are many cases where combinations of verbs and certain nouns form IE patterns. In the work of Yakushiji et al. (2005), we divided only patterns that include only one verb. We have extended the division process to also treat nominal patterns or patterns that include more than one verb. Combination patterns are not appropriate for utilizing individual word information because they are always used in rather strictly combined ways. Therefore we have newly introduced</context>
</contexts>
<marker>Yakushiji, Miyao, Tateisi, Tsujii, 2005</marker>
<rawString>Akane Yakushiji, Yusuke Miyao, Yuka Tateisi, and Jun’ichi Tsujii. 2005. Biomedical information extraction with predicate-argument structure patterns. In Proc. SMBM 2005, pages 60–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daming Yao</author>
<author>Jingbo Wang</author>
<author>Yanmei Lu</author>
<author>Nathan Noble</author>
<author>Huandong Sun</author>
<author>Xiaoyan Zhu</author>
<author>Nan Lin</author>
<author>Donald G Payan</author>
<author>Ming Li</author>
<author>Kunbin Qu</author>
</authors>
<title>PathwayFinder: Paving The Way Towards Automatic Pathway Extraction.</title>
<date>2004</date>
<booktitle>In Bioinformatics 2004: Proc. the 2nd APBC,</booktitle>
<volume>29</volume>
<pages>53--62</pages>
<contexts>
<context position="3450" citStr="Yao et al., 2004" startWordPosition="511" endWordPosition="514">ted full parsing from various levels of parsing, because we believe that it offers the best utility to generalize sentences into normalized syntactic relations. We also divided patterns into components to improve recall and we introduced machine learning with a Support Vector Machine (SVM) to learn a prediction model using the matching results of extraction patterns. As an actual IE task, we extracted pairs of interacting protein names from biomedical text. 2 Full Parsing 2.1 Necessity for Full Parsing A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005). Their assertion is 284 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 284–292, Sydney, July 2006. c�2006 Association for Computational Linguistics Distance Count (%) Sum (%) −1 54 5.0 5.0 0 8 0.7 5.7 1 170 15.7 21.4 2–5 337 31.1 52.5 6–10 267 24.6 77.1 11– 248 22.9 100.0 Distance −1 means protein word has been annotated as interacting with itself (e.g. “actin polymerization”). Distance 0 means words of the interacting proteins are directly next to one another. Multi-word protein names are concatenated as long as </context>
</contexts>
<marker>Yao, Wang, Lu, Noble, Sun, Zhu, Lin, Payan, Li, Qu, 2004</marker>
<rawString>Daming Yao, Jingbo Wang, Yanmei Lu, Nathan Noble, Huandong Sun, Xiaoyan Zhu, Nan Lin, Donald G. Payan, Ming Li, and Kunbin Qu. 2004. PathwayFinder: Paving The Way Towards Automatic Pathway Extraction. In Bioinformatics 2004: Proc. the 2nd APBC, volume 29 of CRPIT, pages 53–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In Proc. ACL’05,</booktitle>
<pages>419--426</pages>
<contexts>
<context position="31129" citStr="Zhao and Grishman (2005)" startWordPosition="5065" endWordPosition="5068"> within the high-recall range. Compared to theirs, one of our problems is that our method could not handle attributives. One example is “binding property of ENTITY1 to ENTITY2”. We could not obtain “binding” because the smallest set of PASs connecting “ENTITY1” and “ENTITY2” includes only the PASs of “property”, “of” and “to”. To handle these attributives, we need distinguish necessary attributives from those that are general4 by semantic analysis or bootstrapping. Another approach to improve our method is to include local information in sentences, such as surface words between protein names. Zhao and Grishman (2005) reported that adding local information to deep syntactic information improved IE results. This approach is also applicable to IE in other domains, where related entities are in a short 4Consider the case where a source sentence for a pattern is “ENTITY1 is an important homodimeric protein.” (“homodimeric” represents that two molecules of “ENTITY1” interact with each other.) F-measure by SCORE 0.55 0.5 0.45 0.4 0.35 Number 291 distance like the work of Zhou et al. (2005). 6 Conclusion We proposed the use of PASs to construct patterns as extraction rules, utilizing their ability to abstract syn</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proc. ACL’05, pages 419–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Jian Su</author>
<author>Jie Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proc. ACL’05,</booktitle>
<pages>427--434</pages>
<contexts>
<context position="1969" citStr="Zhou et al., 2005" startWordPosition="276" endWordPosition="279">patterns for particular applications and this is presently one of the main streams of biomedical IE (Blaschke and Valencia, 2002; Koike et al., 2003). Although such IE attempts have demonstrated near-practical performance, the same sets of patterns cannot be applied to different kinds of information. A real-world task requires several kinds of IE, thus manually engineering extraction Current Affiliation: † FUJITSU LABORATORIES LTD. ‡ Faculty of Informatics, Kogakuin University patterns, which is tedious and time-consuming process, is not really practical. Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE. However, in most cases, the cost of manually crafting patterns is simply transferred to that for constructing a large amount of training data, which requires tedious amount of manual labor to annotate text. To systematically reduce the necessary amount of training data, we divided the task of constructing extraction patterns into a subtask that general natural language processing techniques can solve and a subtask that has specific properties according to the information to be extracted</context>
<context position="3470" citStr="Zhou et al., 2005" startWordPosition="515" endWordPosition="518">rom various levels of parsing, because we believe that it offers the best utility to generalize sentences into normalized syntactic relations. We also divided patterns into components to improve recall and we introduced machine learning with a Support Vector Machine (SVM) to learn a prediction model using the matching results of extraction patterns. As an actual IE task, we extracted pairs of interacting protein names from biomedical text. 2 Full Parsing 2.1 Necessity for Full Parsing A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005). Their assertion is 284 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 284–292, Sydney, July 2006. c�2006 Association for Computational Linguistics Distance Count (%) Sum (%) −1 54 5.0 5.0 0 8 0.7 5.7 1 170 15.7 21.4 2–5 337 31.1 52.5 6–10 267 24.6 77.1 11– 248 22.9 100.0 Distance −1 means protein word has been annotated as interacting with itself (e.g. “actin polymerization”). Distance 0 means words of the interacting proteins are directly next to one another. Multi-word protein names are concatenated as long as they do not cross ta</context>
<context position="31604" citStr="Zhou et al. (2005)" startWordPosition="5143" endWordPosition="5146">roach to improve our method is to include local information in sentences, such as surface words between protein names. Zhao and Grishman (2005) reported that adding local information to deep syntactic information improved IE results. This approach is also applicable to IE in other domains, where related entities are in a short 4Consider the case where a source sentence for a pattern is “ENTITY1 is an important homodimeric protein.” (“homodimeric” represents that two molecules of “ENTITY1” interact with each other.) F-measure by SCORE 0.55 0.5 0.45 0.4 0.35 Number 291 distance like the work of Zhou et al. (2005). 6 Conclusion We proposed the use of PASs to construct patterns as extraction rules, utilizing their ability to abstract syntactical variants with the same relation. In addition, we divided the patterns for generalization, and used matching results for SVM learning. In experiments on extracting of proteinprotein interactions, we obtained 71.8% precision and 48.4% recall on a small corpus and 64.0% precision and 83.8% recall estimated on a large text, which demonstrated the obvious advantages of our method. Acknowledgement This work was partially supported by Grant-in-Aid for Scientific Resear</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction. In Proc. ACL’05, pages 427–434.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>