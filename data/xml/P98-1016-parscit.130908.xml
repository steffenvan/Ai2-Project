<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003072">
<title confidence="0.843982">
Redundancy: helping semantic disambiguation
</title>
<author confidence="0.990044">
Caroline Barriere
</author>
<affiliation confidence="0.995085">
School of Information Technology and Engineering
University of Ottawa
</affiliation>
<address confidence="0.653044">
Ottawa,Canada,K1N 7Z3
</address>
<email confidence="0.791285">
barriereOsite.uottawa.ca
</email>
<sectionHeader confidence="0.994062" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995724">
Redundancy is a good thing, at least in a learn-
ing process. To be a good teacher you must
say what you are going to say, say it, then say
what you have just said. Well, three times is
better than one. To acquire and learn knowl-
edge from text for building a lexical knowledge
base, we need to find a source of information
that states facts, and repeats them a few times
using slightly different sentence structures. A
technique is needed for gathering information
from that source and identify the redundant in-
formation. The extraction of the commonality
is an active learning of the knowledge expressed.
The proposed research is based on a clustering
method developed by Barriere and Popowich
(1996) which performs a gathering of related
information about a particular topic. Individ-
ual pieces of information are represented via the
Conceptual Graph (CG) formalism and the re-
sult of the clustering is a large CG embedding
all individual graphs. In the present paper, we
suggest that the identification of the redundant
information within the resulting graph is very
useful for disambiguation of the original infor-
mation at the semantic level.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999847404761905">
The construction of a Lexical Knowledge Base
(LKB), if performed automatically (or semi-
automatically), attempts at extracting knowl-
edge from text. The extraction can be viewed
as a learning process. Simplicity, clarity and re-
dundancy of the information given in the source
text are key features for a successful acquisition
of knowledge. We assume success is attained
when a sentence from the source text expressed
in natural language can be transformed into an
unambiguous internal representation. Using a
conceptual graph (CG) representation (Sowa,
1984) of sentences means that a successful ac-
quisition of knowledge corresponds to trans-
forming each sentence from the source text into
a set of unambiguous concepts (correct word
senses found) and unambiguous relations (cor-
rect semantic relations between concepts).
This paper will look at the idea of making
good use of the redundancy found in a text to
help the knowledge acquisition task. Things are
not always understood when they are first en-
countered. A sentence expressing new knowl-
edge might be ambiguous (at the level of the
concepts it introduces and/or at the level of the
semantic relations between those concepts). A
search through previously acquired knowledge
might help disambiguate the new sentence or it
might not. A repetition of the exact same sen-
tence would be of no help, but a slightly differ-
ent format of expression might reveal necessary
aspects for the comprehension. This is the av-
enue explored in this paper which will unfold
as follows. Section 2 will present briefly a pos-
sible good source of knowledge and a gather-
ing/clustering technique. Section 3 will present
how the redundancy resulting from the cluster-
ing process can be used in solving some types
of semantic ambiguity. Section 4 will emphasize
the importance of semantic relations for the pro-
cess of semantic disambiguation. Section 5 will
conclude.
</bodyText>
<sectionHeader confidence="0.5367315" genericHeader="method">
2 Source of information and
clustering technique
</sectionHeader>
<bodyText confidence="0.999975142857143">
To acquire and learn knowledge from text for
building a lexical knowledge base, we need to
find a source of information that states facts,
and repeats them a few times using slightly
different sentence structures. A technique is
needed for gathering information from that
source and identify the redundant information.
</bodyText>
<page confidence="0.998947">
103
</page>
<bodyText confidence="0.999551333333333">
These two aspects are discussed hereafter: (1)
the choice of a source of information and (2) the
information gathering technique.
</bodyText>
<subsectionHeader confidence="0.999573">
2.1 Choice of source of information
</subsectionHeader>
<bodyText confidence="0.999977678571429">
When we think of learning about words, we
think of textbooks and dictionaries. Redun-
dancy might be present but not always simplic-
ity. Any text is written at a level which assumes
some common knowledge among potential read-
ers. In a textbook on science, the author will
define the scientific terms but not the general
English vocabulary. In an adult&apos;s dictionary,
all words are defined, but a certain knowledge of
the &amp;quot;world&amp;quot; (common sense, typical situations)
is assumed as common adult knowledge, so the
emphasis of the definitions might not be on sim-
ple cases but on more ambiguous or infrequent
cases. To learn the basic vocabulary used in
day to day life, a very simple children&apos;s first dic-
tionary is a good place to start. In (Barriere,
1997), such a dictionary is used for an appli-
cation of LKB construction in which no prior
semantic knowledge was assumed. In the same
research the author explains how to use a multi-
stage process, to transform the sentences from
the dictionary into conceptual graph represen-
tations. This dictionary, the American Her-
itage First Dictionaryl (AHFD), is an ex-
ample of a good source of knowledge in terms
of simplicity, clarity and redundancy. Some def-
initions introduce concepts that are mentioned
again in other definitions.
</bodyText>
<subsectionHeader confidence="0.999985">
2.2 Gathering of information
</subsectionHeader>
<bodyText confidence="0.998284692307692">
Barriere and Popowich (1996) presented the
idea of concept clustering for knowledge integra-
tion. First, a Lexical Knowledge Base (LKB) is
built automatically and contains all the nouns
and verbs of the AHFD, each word having its
definition represented using the CG formalism.
Here is a brief summary of the clustering pro-
cess from there. It is not a statistical cluster-
ing but more a &amp;quot;graph matching&amp;quot; type of clus-
tering. A trigger word is chosen and the CG
representation of its defining sentences make up
the initial CCKG (Concept clustering knowl-
edge graph). The trigger word can be any word,
</bodyText>
<footnote confidence="0.814506333333333">
&apos;Copyright Â©1994 by Houghton Mifflin Company.
Reproduced by permission from THE AMERICAN
HERITAGE FIRST DICTIONARY.
</footnote>
<bodyText confidence="0.997834490566038">
but preferably it should be a semantically sig-
nificant word. A word is semantically signifi-
cant if it occurs less than a maximal number
of times in the text, therefore excluding gen-
eral words such as place, or person. The clus-
tering is really an iterative forward and back-
ward search within the LKB to find definitions
of words that are somewhat &amp;quot;related&amp;quot; to the
trigger word. A forward search looks at the def-
inition of the words used in the trigger word&apos;s
definition. A backward search looks at the defi-
nition of the words that use the trigger word to
be defined. A word becomes part of the cluster
if its CG representation shares a common sub-
graph of a minimal size with the CCKG. The
process is then extended to perform forward and
backward searches based on the words in the
cluster and not only on the trigger word.
The cluster becomes a set of words related to
the trigger word, and the CCKG presents the
trigger word within a large context by showing
all the links between all the words of the clus-
ter. The CCKG is a merge of all individual CGs
from the words in the cluster.
Table 1 shows examples of clusters found by
using the clustering technique on the AHFD. If
a word is followed by _#, it means the sense #
of that word. The CCKGs corresponding to the
clusters are not illustrated as it would require
much space to show all the links between all
the words in the clusters.2
The clustering method described is based on
the principle that information is acquired from
a machine readable dictionary (the AHFD), and
therefore each word is associated with some
knowledge pertaining to it. To extend this clus-
tering technique to a knowledge base containing
non-classified pieces of information, we would
need to use some indexing scheme allowing ac-
cess to all the sentences containing a particular
2The reader might wonder why such word as [rain-
bow] is associated with [needle_1] or why [kangaroo] is
associated with [stomach]. The AHFD tells the child
that &amp;quot;A rainbow looks like a ribbon of many colors across
the sky.&amp;quot; and &amp;quot;Kangaroo mothers carry their babies in a
pocket in front of their stomachs.&amp;quot; The threshold used
to define the minimal size of the common subgraph nec-
essary to include a new word in the cluster is established
experimentally. Changing that threshold will change
the size of the resulting cluster therefore affecting which
words will be included. The clustering technique, and
a derived extended clustering technique are explained in
much details in (Barriere and Fass, 1998).
</bodyText>
<page confidence="0.999344">
104
</page>
<tableCaption confidence="0.999286">
Table 1: Multiple clusters from different words
</tableCaption>
<bodyText confidence="0.952981789473684">
Trigger Cluster
word
needle_l {needle_1, sew, cloth, thread, wool,
handkerchief, pin, ribbon, string, rainbow}
sew {sew, cloth, needle_1, needle_2, thread,
button, patch_l, pin, pocket, wool,
ribbon, rug, string, nest, prize, rainbow}
kitchen {kitchen, stove, refrigerator, pan}
stove {stove, pan, kitchen, refrigerator, pot, clay}
stomach {stomach, kangaroo, pain, swallow, mouth}
airplane {airplane, wing, airport, fly_2, helicopter,
jet, kit, machine, pilot, plane}
elephant {elephant, skin, trunk_1, ear, zoo, bark,
leather, rhinoceros}
soap {soap, dirt, mix, bath, bubble, suds,
wash, boil, steam}
wash {wash, soap, bath, bathroom, suds,
bubble, boil, steam}
word in them.
</bodyText>
<sectionHeader confidence="0.995999" genericHeader="method">
3 Semantic disambiguation
</sectionHeader>
<bodyText confidence="0.999696">
We propose in this section a way to attempt at
solving different types of semantic ambiguities
by using the redundancy of information result-
ing from the clustering technique as briefly de-
scribed in the previous section. Going through
an example, we will look at three types of se-
mantic ambiguity: anaphora resolution, word
sense disambiguation, and relation disambigua-
tion.
In Figure 1, Definition 3.1 shows one sen-
tence in the definition of mail_l (taken from
the AHFD, as all other definitions in Figure 1)
with its corresponding CG representation. Def-
inition 3.2 shows one sentence in the definition
of stamp also with its CG representation. Using
the clustering technique briefly described in the
previous section, the two words are put together
into a cluster triggered by the concept [mail_1].
Result 3.1 shows the maximal join3 between
the two previous graphs around shared concept
[mail_1]. Combining the information from stamp
and maili, puts in evidence the redundant in-
formation. The reduction process for eliminat-
ing this redundancy will solve some ambigui-
ties. This process is based on the idea of find-
ing &amp;quot;compatible&amp;quot; concepts within a graph. Two
concepts are compatible if their semantic dis-
tance is small. That distance is often based on
</bodyText>
<footnote confidence="0.492952333333333">
3A maximal join is an operation defined within the
CC formalism to gather knowledge from two graphs
around a concept that they both share.
</footnote>
<bodyText confidence="0.999959387755102">
the relative positions of concepts within the con-
cept hierarchy (Delugach, 1993; Foo et al., 1992;
Resnik, 1995). For the present discussion we as-
sume that two concepts are compatible if they
share a semantically significant common super-
type, or if one concept is a supertype of the
other.
In Result 3.1, the concept [send] is present
twice, and also the concept [letter] is present
in two compatible forms: [letter] and [mes-
sage]. The compatibility comes from the pres-
ence in the type hierarchy4 of one sense of
[letter], [letter_2], as being a subtype of [mes-
sage]. These compatible forms actually allow
the disambiguation of concept [letter] into [let-
ter_2]. This should update the definition of
stamp shown in Definition 3.2. The other sense
of [letter], [letter_1] is a subtype of [symbol].
The pronoun they in Result 3.1 must refer to
some word, either previously mentioned in the
sentence, or assumed known (as a default) in the
LKB. Both (agent) relations attached to con-
cept [send] lead to compatible concepts: [they]
and [person]. We can therefore go back to the
graph definition of [stamp] in which the pronoun
[they] could have referred to the concepts [let-
ters], [packages], [people] or [stamps], and now
disambiguate it to [people].
Result 3.2 shows the internal join which es-
tablishes coreference links (shown by *x, *y,
*z) between compatible concepts that are in an
identical relation with another concept. The re-
duced join, after the redundancy is eliminated,
is shown in Result 3.3.
Two types of disambiguation (anaphora res-
olution and word sense disambiguation) were
shown up to now. The third type of disam-
biguation is at the level of the semantic re-
lations. For this type of ambiguity, we must
briefly introduce the idea of a relation hierar-
chy which is described and justified in more de-
tails in (Barriere, 1998). A relation hierarchy,
as presented in (Sowa, 1984), is simply a way
to establish an order between the possible rela-
tions. The idea is to include relations that cor-
respond to the English prepositions (it could be
the prepositions of any language studied) at the
top of the hierarchy, and consider them gener-
alizations of possible deeper semantic relations.
</bodyText>
<footnote confidence="0.5887495">
4The type hierarchy has been built automatically
from information extracted from the AHFD.
</footnote>
<page confidence="0.996135">
105
</page>
<figure confidence="0.99734640625">
Definition 3.1 -
MAIL1 : People send messages through the mail.
[send]-&gt;(agent)-&gt;[person:plural]
-&gt;(object)-&gt;[message:plural]
-&gt;(through)-&gt;[mail_1]
Definition 3.2 -
STAMP : People buy stamps to put on letters and packages they send through the mail.
[send]-&gt;(object)-&gt;[letter:plural]-&gt;(and)-&gt;[package:plural]
&lt;-(on)&lt;-[put]&lt;-(goal)&lt;-[buy]-&gt;(agent)-&gt;[person:plural]
-&gt;(object)-&gt;[stamp:plural]
-&gt;(agent)-&gt;[they]
-&gt;(through)-&gt;[mail_1]
Result 3.1 - Maximal join between mail_1 and stamp
[send]-&gt;(object)-&gt;[letterplural]-&gt;(and)-&gt;[package:plural]
&lt;-(on)&lt;-[put]&lt;-(goal)&lt;-[buy]-&gt;(agent)-&gt;[person:plural]
-&gt;(object)-&gt;[stamp:plural]
-&gt;(agent)-&gt;[they]
-&gt;(through)-&gt; [mail_1] &lt;-(through)&lt;-[send]-&gt;(object)-&gt;[message:plural]
-&gt;(agent)-&gt;[person:plural]
Result 3.2 - Internal Join on Graph maiLl/stamp
[send *y]-&gt;(object)-&gt;[letterplural *z]-&gt;(and)-&gt;[package:plural]
&lt;-(on)&lt;-[put]&lt;-(goal)&lt;-[buy]-&gt;(agent)-&gt;[person:plural]
-&gt;(object)-&gt;[stamp:plural]
-&gt;(agent)-&gt;[they *x]
-&gt;(through)-&gt;[mail_1]&lt;-(through)&lt;-[send *y]-&gt;(object)-&gt;[message:plural *z]
-&gt;(agent)-&gt;[person:plural *x]
Result 3.3 - After reduction of graph maiLl/stamp
petter_2:plurall-&gt;(and)-&gt;[package:plural]
&lt;-(object)&lt;-[send]-&gt;(agent)-&gt;[person:plural]
-&gt;(through)-&gt;[mail_1]
&lt;-(on)&lt;-[put]&lt;-(goal)&lt;-[buy]-&gt;(agent)-&gt;[person:plural]
-&gt;(object)-&gt;[stamp:plural]
</figure>
<figureCaption confidence="0.999965">
Figure 1: Example of ambiguity reduction
</figureCaption>
<page confidence="0.900969">
106
</page>
<figure confidence="0.997383428571429">
Definition 3.3 -
CARD_2 : You send cards to people in the mail.
[send]-&gt;(agent)-&gt;[you]
-&gt;(object)-&gt;[card.2:plural]
-&gt;(to)-&gt;[person:plural]
-&gt;(in)-&gt;[mail_1]
Result 3.4 - Graph maiLl/stamp joined to card (after internal join)
[letter_2:plural]-&gt;(and)-&gt;[package:plural]
&lt;-(object)&lt;-[send *y]-&gt;(agent)-&gt;[person:plural *z]
-&gt; (th rough )-&gt; &lt;-(in)&lt;-[send *y]-&gt; (to)- &gt; [person :plural]
-&gt;(agent)-&gt; [you *z]
-&gt;(object)-&gt;[card_2:plural]
&lt;-(on)&lt;-[put]&lt;-(goal)&lt;-[buy]-&gt;(agent)-&gt;[person:plural]
-&gt;(object)-&gt;[stamp:plural]
Result 3.5 - After reduction of graph maiLl/stamp/card_2
[letter_2:plural]-&gt;(and)-&gt;[package:plural]-&gt;(and)-&gt;[card_2:plural]
&lt;-(object)&lt;-[send]-&gt;(agent)-&gt;[person:plural]
-&gt;(manner)-&gt;[mail_1]
-&gt;(to)-&gt;[person:plural]
&lt;-(on)&lt;-[put]&lt;-(goal)&lt;-[buy]-&gt;(agent)-&gt;[person:plural]
-&gt;(object)-&gt; [stamp:plural]
</figure>
<figureCaption confidence="0.999998">
Figure 1: Example of ambiguity reduction (continued)
</figureCaption>
<bodyText confidence="0.999131780487805">
This relation hierarchy is important for the
comparison of graphs expressing similar ideas
but using different sentence patterns that are
reflected in the graphs by different prepositions
becoming relations. Let us look in Figure 1 at
Definition 3.3 which gives a sentence in the defi-
nition of [card_2] and Result 3.4 which gives the
maximal join with graph maiLl/stamp from re-
sult 3.3 around concept [mail_1].
Subgraphs [send]-&gt;(in)-&gt;[mail_1] and [send]-
&gt;(through)-&gt;tmaiLl] have compatible concepts
on both sides of two different relations. These
two prepositions are both supertypes of a re-
stricted set of semantic relations. On Figure 2
which shows a small part of the relation hierar-
chy, we highlighted the compatibility between
through and in. It shows that the two prepo-
sitions interact at manner (at location as well
but more indirectly). Therefore, we can estab-
lish the similarity of those two relations via the
manner relation, and the ambiguity is resolved
as shown in Result 3.5.
Note that the concept [person] is present
many time among the different graphs in Fig-
ure 1. This gives the reader an insight into the
complexity behind clustering. It all relies on
compatibility of concepts and relations. Com-
patibility of concepts alone might be sufficient if
the concepts are highly semantically significant,
but for general concepts like [person], [place],
[animal] we cannot assume so. In the graph pre-
sented in Result 3.5, there are buyers of stamps,
receivers and senders of letters and they are all
people, but not necessarily the same ones.
We saw the redundancy resulting from the
clustering process and how to exploit this re-
dundancy for semantic disambiguation. We see
how redundancy at the concept level without
the relations can be very misleading, and the
following section emphasize the importance of
semantic relations.
</bodyText>
<page confidence="0.9882">
107
</page>
<figure confidence="0.9234175">
4// \
destination direction source path
</figure>
<figureCaption confidence="0.965348">
Figure 2: Small part of relation taxonomy.
</figureCaption>
<figure confidence="0.989105857142857">
with through at on
about
accompaniment
instrument
point-in-time
location
manner
</figure>
<sectionHeader confidence="0.9155345" genericHeader="method">
4 The importance of semantic
relations
</sectionHeader>
<bodyText confidence="0.9999266">
Clusters are and have been used in different
applications for information retrieval and word
sense disambiguation. Clustering can be done
statistically by analyzing text corpora (Wilks
et al., 1989; Brown et al., 1992; Pereira et
al., 1995) and usually results in a set of words
or word senses. In this paper, we are using
the clustering method used in (Barriere and
Popowich, 1996) to present our view on re-
dundancy and disambiguation. The clustering
brings together a set of words but also builds a
CCKG which shows the actual links (semantic
relations) between the members of the cluster.
We suggest that those links are essential in an-
alyzing and disambiguating texts. When links
are redundant in a graph (that is we find two
identical links between two compatible concepts
at each end) we are able to reduce semantic am-
biguity relating to anaphora and word sense.
The counterpart to this, is that redundancy at
the concept level allows us to disambiguate the
semantic relations.
To show our argument of the importance of
links, we present an example. Example 4.1
shows a situation where an ambiguous word
chicken (sense 1 for the animal and sense 2
for the meat) is used in a graph and needs
to be disambiguated. If two graphs stored in
a LKB contain the word chicken in a disam-
biguated form they can help solving the ambi-
guity. In Example 4.1, Graph 4.1 and Graph 4.2
have two isolated concepts in common: eat and
chicken. Graph 4.1 and Graph 4.3 have the
same two concepts in common, but the addi-
tion of a compatible relation, creating the com-
mon subgraph [eat]-&gt;(object)-&gt;[chicken], makes
them more similar. The different relations be-
tween words have a large impact on the meaning
of a sentence. In Graph 4.1, the word chicken
can be disambiguated to chicken_2.
</bodyText>
<reference confidence="0.558737">
Example 4.1 -
John eats chicken with a fork.
Graph 4.1 -
[eat]-&gt;(agent)-&gt;[John]
-&gt;(with)-&gt;[fork]
-&gt;(object)-&gt;[chicken]
John&apos;s chicken eats grain.
Graph 4.2
[eat]-&gt;(agent)-&gt;[chicken_1]&lt;-(poss)&lt;-[John]
-&gt; (object)-&gt;[grain]
</reference>
<page confidence="0.896639">
108
</page>
<figure confidence="0.9553848">
John likes to eat chicken at noon.
Graph 4.3 -
[like]-&gt;(agent)-&gt;[John]
-&gt;(goal)-&gt;[eat]-&gt;(object)-&gt;[chicken_2]
-&gt;(time)-&gt;[noon]
</figure>
<bodyText confidence="0.99916455">
Only if we look at the relations between words
can we understand how different each statement
is. It&apos;s all in the links... Of course those links
might not be necessary at all levels of text anal-
ysis. If we try to cluster documents based on
keywords, well we don&apos;t need to go to such a
deep level of understanding. But when we are
analyzing one text and trying to understand the
meaning it conveys, we are probably within a
narrow domain and the relations between words
take all their importance. For example, if we
are trying to disambiguate the word baseball
(the sport or the ball), both senses of the words
will occur in the same context, therefore using
clusters of words that identify a context will not
allow us to disambiguate between both senses.
On the other hand, having a CCKG showing
the relations between the baseball_l (ball), the
bat, the player and the baseball_2 (sport), will
express the desired information.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999986181818182">
We presented the problem of semantic disam-
biguation as solving ambiguities at the concept
level (word sense and anaphora) but also at
the link level (the relations between concepts).
We showed that when gathering information
around a particular subject via a clustering
method, we tend to cumulate similar facts ex-
pressed in slightly different ways. That redun-
dancy is expressed by multiple copies of com-
patible/identical concepts and relations in the
resulting graph which is called a CCKG (Con-
cept Clustering Knowledge Graph). The re-
dundancy within the links (relations) helps dis-
ambiguate the concepts they connect and the
redundancy within the concepts helps disam-
biguate the links connecting them. Clustering
has been used a lot in previous research but only
at the concept level; we propose that it is essen-
tial to understand the links between the con-
cepts in the cluster if we want to disambiguate
between elements that share a similar context
of usage.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99886874">
C. Barriere and D. Fass. 1998. Dictionary vali-
dation through a clustering technique. To be
published in the Proceedings of Euralex&apos;98:
Eight EURALEX International Congress on
Lexicography, Belgium, August 1998.
C. Barriere and F. Popowich. 1996. Concept
clustering and knowledge integration from a
children&apos;s dictionary. In Proc. of the 16 th
COLING, Copenhagen, Danemark, August.
C. Barriere. 1997. From a Children&apos;s First Dic-
tionary to a Lexical Knowledge Base of Con-
ceptual Graphs. Ph.D. thesis, Simon Fraser
University, June.
C. Barriere. 1998. The relation hierarchy:
one key to representing natural language
using conceptual graphs. Submitted at
ICCS98: International Conference on Con-
ceptual Structures, to be held in Montpellier,
France, August 1998.
P. Brown, V.J. Della Pietra, P.V. deSouza, J.C.
Lai, and R.L. Mercer. 1992. Class-based ii-
gram models of natural language. Computa-
tional Linguistics, 18(4):467-480.
H. S. Delugach. 1993. An exploration into
semantic distance. In H. D. Pfeiffer and
T. E. Nagle, editors, Conceptual Structures:
Theory and Implementation, pages 119-124.
Springer, Berlin, Heidelberg.
N. Foo, B.J. Garner, A. Rao, and E. Tsui. 1992.
Semantic distance in conceptual graphs. In
T.E. Nagle, J.A. Nagle, L.L. Gerholz, and
P.W.Eklund, editors, Conceptual Structures:
Current Research and Practice, chapter 7,
pages 149-154. Ellis Horwood.
F. Pereira, N. Tishby, and L. Lee. 1995. Distri-
butional clustering of english words. In Proc.
of the 33 th ACL, Cambridge,MA.
Philip Resnik. 1995. Using information content
to evaluate semantic similarity in a taxonomy.
In Proc. of the 14 th IJCAI, volume 1, pages
448-453, Montreal, Canada.
J. Sowa. 1984. Conceptual Structures in Mind
and Machines. Addison-Wesley.
Y. Wilks, D. Fass, G-M Guo, J. McDonald,
T. Plate, and B. Slator. 1989. A tractable
machine dictionary as a resource for computa-
tional semantics. In Bran Boguraev and Ted
Briscoe, editors, Computational Lexicography
for Natural Language Processing, chapter 9,
pages 193-231. Longman Group UK Limited.
</reference>
<page confidence="0.998945">
109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924571">
<title confidence="0.999231">Redundancy: helping semantic disambiguation</title>
<author confidence="0.999943">Caroline Barriere</author>
<affiliation confidence="0.999617">School of Information Technology and Engineering University of Ottawa</affiliation>
<address confidence="0.97103">Ottawa,Canada,K1N 7Z3</address>
<email confidence="0.996099">barriereOsite.uottawa.ca</email>
<abstract confidence="0.998312807692308">Redundancy is a good thing, at least in a learning process. To be a good teacher you must say what you are going to say, say it, then say what you have just said. Well, three times is better than one. To acquire and learn knowledge from text for building a lexical knowledge base, we need to find a source of information that states facts, and repeats them a few times using slightly different sentence structures. A technique is needed for gathering information from that source and identify the redundant information. The extraction of the commonality is an active learning of the knowledge expressed. The proposed research is based on a clustering method developed by Barriere and Popowich (1996) which performs a gathering of related information about a particular topic. Individual pieces of information are represented via the Conceptual Graph (CG) formalism and the result of the clustering is a large CG embedding all individual graphs. In the present paper, we suggest that the identification of the redundant information within the resulting graph is very useful for disambiguation of the original information at the semantic level.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Example 4.1 -John eats chicken with a fork. Graph 4.1 -John&apos;s chicken eats grain.</title>
<journal>Graph</journal>
<volume>4</volume>
<marker></marker>
<rawString>Example 4.1 -John eats chicken with a fork. Graph 4.1 -John&apos;s chicken eats grain. Graph 4.2</rawString>
</citation>
<citation valid="false">
<note>[eat]-&gt;(agent)-&gt;[chicken_1]&lt;-(poss)&lt;-[John] -&gt; (object)-&gt;[grain]</note>
<marker></marker>
<rawString>[eat]-&gt;(agent)-&gt;[chicken_1]&lt;-(poss)&lt;-[John] -&gt; (object)-&gt;[grain]</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Barriere</author>
<author>D Fass</author>
</authors>
<title>Dictionary validation through a clustering technique. To be published</title>
<date>1998</date>
<booktitle>in the Proceedings of Euralex&apos;98: Eight EURALEX International Congress on Lexicography,</booktitle>
<location>Belgium,</location>
<contexts>
<context position="8285" citStr="Barriere and Fass, 1998" startWordPosition="1369" endWordPosition="1372">le_1] or why [kangaroo] is associated with [stomach]. The AHFD tells the child that &amp;quot;A rainbow looks like a ribbon of many colors across the sky.&amp;quot; and &amp;quot;Kangaroo mothers carry their babies in a pocket in front of their stomachs.&amp;quot; The threshold used to define the minimal size of the common subgraph necessary to include a new word in the cluster is established experimentally. Changing that threshold will change the size of the resulting cluster therefore affecting which words will be included. The clustering technique, and a derived extended clustering technique are explained in much details in (Barriere and Fass, 1998). 104 Table 1: Multiple clusters from different words Trigger Cluster word needle_l {needle_1, sew, cloth, thread, wool, handkerchief, pin, ribbon, string, rainbow} sew {sew, cloth, needle_1, needle_2, thread, button, patch_l, pin, pocket, wool, ribbon, rug, string, nest, prize, rainbow} kitchen {kitchen, stove, refrigerator, pan} stove {stove, pan, kitchen, refrigerator, pot, clay} stomach {stomach, kangaroo, pain, swallow, mouth} airplane {airplane, wing, airport, fly_2, helicopter, jet, kit, machine, pilot, plane} elephant {elephant, skin, trunk_1, ear, zoo, bark, leather, rhinoceros} soap </context>
</contexts>
<marker>Barriere, Fass, 1998</marker>
<rawString>C. Barriere and D. Fass. 1998. Dictionary validation through a clustering technique. To be published in the Proceedings of Euralex&apos;98: Eight EURALEX International Congress on Lexicography, Belgium, August 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Barriere</author>
<author>F Popowich</author>
</authors>
<title>Concept clustering and knowledge integration from a children&apos;s dictionary.</title>
<date>1996</date>
<booktitle>In Proc. of the 16 th COLING,</booktitle>
<location>Copenhagen, Danemark,</location>
<contexts>
<context position="887" citStr="Barriere and Popowich (1996)" startWordPosition="137" endWordPosition="140">d teacher you must say what you are going to say, say it, then say what you have just said. Well, three times is better than one. To acquire and learn knowledge from text for building a lexical knowledge base, we need to find a source of information that states facts, and repeats them a few times using slightly different sentence structures. A technique is needed for gathering information from that source and identify the redundant information. The extraction of the commonality is an active learning of the knowledge expressed. The proposed research is based on a clustering method developed by Barriere and Popowich (1996) which performs a gathering of related information about a particular topic. Individual pieces of information are represented via the Conceptual Graph (CG) formalism and the result of the clustering is a large CG embedding all individual graphs. In the present paper, we suggest that the identification of the redundant information within the resulting graph is very useful for disambiguation of the original information at the semantic level. 1 Introduction The construction of a Lexical Knowledge Base (LKB), if performed automatically (or semiautomatically), attempts at extracting knowledge from </context>
<context position="5095" citStr="Barriere and Popowich (1996)" startWordPosition="818" endWordPosition="821">ctionary is a good place to start. In (Barriere, 1997), such a dictionary is used for an application of LKB construction in which no prior semantic knowledge was assumed. In the same research the author explains how to use a multistage process, to transform the sentences from the dictionary into conceptual graph representations. This dictionary, the American Heritage First Dictionaryl (AHFD), is an example of a good source of knowledge in terms of simplicity, clarity and redundancy. Some definitions introduce concepts that are mentioned again in other definitions. 2.2 Gathering of information Barriere and Popowich (1996) presented the idea of concept clustering for knowledge integration. First, a Lexical Knowledge Base (LKB) is built automatically and contains all the nouns and verbs of the AHFD, each word having its definition represented using the CG formalism. Here is a brief summary of the clustering process from there. It is not a statistical clustering but more a &amp;quot;graph matching&amp;quot; type of clustering. A trigger word is chosen and the CG representation of its defining sentences make up the initial CCKG (Concept clustering knowledge graph). The trigger word can be any word, &apos;Copyright Â©1994 by Houghton Miff</context>
<context position="17361" citStr="Barriere and Popowich, 1996" startWordPosition="2632" endWordPosition="2635">e the importance of semantic relations. 107 4// \ destination direction source path Figure 2: Small part of relation taxonomy. with through at on about accompaniment instrument point-in-time location manner 4 The importance of semantic relations Clusters are and have been used in different applications for information retrieval and word sense disambiguation. Clustering can be done statistically by analyzing text corpora (Wilks et al., 1989; Brown et al., 1992; Pereira et al., 1995) and usually results in a set of words or word senses. In this paper, we are using the clustering method used in (Barriere and Popowich, 1996) to present our view on redundancy and disambiguation. The clustering brings together a set of words but also builds a CCKG which shows the actual links (semantic relations) between the members of the cluster. We suggest that those links are essential in analyzing and disambiguating texts. When links are redundant in a graph (that is we find two identical links between two compatible concepts at each end) we are able to reduce semantic ambiguity relating to anaphora and word sense. The counterpart to this, is that redundancy at the concept level allows us to disambiguate the semantic relations</context>
</contexts>
<marker>Barriere, Popowich, 1996</marker>
<rawString>C. Barriere and F. Popowich. 1996. Concept clustering and knowledge integration from a children&apos;s dictionary. In Proc. of the 16 th COLING, Copenhagen, Danemark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Barriere</author>
</authors>
<title>From a Children&apos;s First Dictionary to a Lexical Knowledge Base of Conceptual Graphs.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Simon Fraser University,</institution>
<contexts>
<context position="4521" citStr="Barriere, 1997" startWordPosition="728" endWordPosition="729">y. Any text is written at a level which assumes some common knowledge among potential readers. In a textbook on science, the author will define the scientific terms but not the general English vocabulary. In an adult&apos;s dictionary, all words are defined, but a certain knowledge of the &amp;quot;world&amp;quot; (common sense, typical situations) is assumed as common adult knowledge, so the emphasis of the definitions might not be on simple cases but on more ambiguous or infrequent cases. To learn the basic vocabulary used in day to day life, a very simple children&apos;s first dictionary is a good place to start. In (Barriere, 1997), such a dictionary is used for an application of LKB construction in which no prior semantic knowledge was assumed. In the same research the author explains how to use a multistage process, to transform the sentences from the dictionary into conceptual graph representations. This dictionary, the American Heritage First Dictionaryl (AHFD), is an example of a good source of knowledge in terms of simplicity, clarity and redundancy. Some definitions introduce concepts that are mentioned again in other definitions. 2.2 Gathering of information Barriere and Popowich (1996) presented the idea of con</context>
</contexts>
<marker>Barriere, 1997</marker>
<rawString>C. Barriere. 1997. From a Children&apos;s First Dictionary to a Lexical Knowledge Base of Conceptual Graphs. Ph.D. thesis, Simon Fraser University, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Barriere</author>
</authors>
<title>The relation hierarchy: one key to representing natural language using conceptual graphs. Submitted at</title>
<date>1998</date>
<booktitle>ICCS98: International Conference on Conceptual Structures,</booktitle>
<location>Montpellier, France,</location>
<note>to be held in</note>
<contexts>
<context position="12271" citStr="Barriere, 1998" startWordPosition="2008" endWordPosition="2009">sambiguate it to [people]. Result 3.2 shows the internal join which establishes coreference links (shown by *x, *y, *z) between compatible concepts that are in an identical relation with another concept. The reduced join, after the redundancy is eliminated, is shown in Result 3.3. Two types of disambiguation (anaphora resolution and word sense disambiguation) were shown up to now. The third type of disambiguation is at the level of the semantic relations. For this type of ambiguity, we must briefly introduce the idea of a relation hierarchy which is described and justified in more details in (Barriere, 1998). A relation hierarchy, as presented in (Sowa, 1984), is simply a way to establish an order between the possible relations. The idea is to include relations that correspond to the English prepositions (it could be the prepositions of any language studied) at the top of the hierarchy, and consider them generalizations of possible deeper semantic relations. 4The type hierarchy has been built automatically from information extracted from the AHFD. 105 Definition 3.1 - MAIL1 : People send messages through the mail. [send]-&gt;(agent)-&gt;[person:plural] -&gt;(object)-&gt;[message:plural] -&gt;(through)-&gt;[mail_1]</context>
</contexts>
<marker>Barriere, 1998</marker>
<rawString>C. Barriere. 1998. The relation hierarchy: one key to representing natural language using conceptual graphs. Submitted at ICCS98: International Conference on Conceptual Structures, to be held in Montpellier, France, August 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based iigram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<contexts>
<context position="17196" citStr="Brown et al., 1992" startWordPosition="2602" endWordPosition="2605">cy for semantic disambiguation. We see how redundancy at the concept level without the relations can be very misleading, and the following section emphasize the importance of semantic relations. 107 4// \ destination direction source path Figure 2: Small part of relation taxonomy. with through at on about accompaniment instrument point-in-time location manner 4 The importance of semantic relations Clusters are and have been used in different applications for information retrieval and word sense disambiguation. Clustering can be done statistically by analyzing text corpora (Wilks et al., 1989; Brown et al., 1992; Pereira et al., 1995) and usually results in a set of words or word senses. In this paper, we are using the clustering method used in (Barriere and Popowich, 1996) to present our view on redundancy and disambiguation. The clustering brings together a set of words but also builds a CCKG which shows the actual links (semantic relations) between the members of the cluster. We suggest that those links are essential in analyzing and disambiguating texts. When links are redundant in a graph (that is we find two identical links between two compatible concepts at each end) we are able to reduce sema</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P. Brown, V.J. Della Pietra, P.V. deSouza, J.C. Lai, and R.L. Mercer. 1992. Class-based iigram models of natural language. Computational Linguistics, 18(4):467-480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Delugach</author>
</authors>
<title>An exploration into semantic distance.</title>
<date>1993</date>
<booktitle>Conceptual Structures: Theory and Implementation,</booktitle>
<pages>119--124</pages>
<editor>In H. D. Pfeiffer and T. E. Nagle, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="10515" citStr="Delugach, 1993" startWordPosition="1712" endWordPosition="1713">s graphs around shared concept [mail_1]. Combining the information from stamp and maili, puts in evidence the redundant information. The reduction process for eliminating this redundancy will solve some ambiguities. This process is based on the idea of finding &amp;quot;compatible&amp;quot; concepts within a graph. Two concepts are compatible if their semantic distance is small. That distance is often based on 3A maximal join is an operation defined within the CC formalism to gather knowledge from two graphs around a concept that they both share. the relative positions of concepts within the concept hierarchy (Delugach, 1993; Foo et al., 1992; Resnik, 1995). For the present discussion we assume that two concepts are compatible if they share a semantically significant common supertype, or if one concept is a supertype of the other. In Result 3.1, the concept [send] is present twice, and also the concept [letter] is present in two compatible forms: [letter] and [message]. The compatibility comes from the presence in the type hierarchy4 of one sense of [letter], [letter_2], as being a subtype of [message]. These compatible forms actually allow the disambiguation of concept [letter] into [letter_2]. This should updat</context>
</contexts>
<marker>Delugach, 1993</marker>
<rawString>H. S. Delugach. 1993. An exploration into semantic distance. In H. D. Pfeiffer and T. E. Nagle, editors, Conceptual Structures: Theory and Implementation, pages 119-124. Springer, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Foo</author>
<author>B J Garner</author>
<author>A Rao</author>
<author>E Tsui</author>
</authors>
<title>Semantic distance in conceptual graphs.</title>
<date>1992</date>
<booktitle>Conceptual Structures: Current Research and Practice, chapter 7,</booktitle>
<pages>149--154</pages>
<editor>In T.E. Nagle, J.A. Nagle, L.L. Gerholz, and P.W.Eklund, editors,</editor>
<publisher>Ellis Horwood.</publisher>
<contexts>
<context position="10533" citStr="Foo et al., 1992" startWordPosition="1714" endWordPosition="1717">shared concept [mail_1]. Combining the information from stamp and maili, puts in evidence the redundant information. The reduction process for eliminating this redundancy will solve some ambiguities. This process is based on the idea of finding &amp;quot;compatible&amp;quot; concepts within a graph. Two concepts are compatible if their semantic distance is small. That distance is often based on 3A maximal join is an operation defined within the CC formalism to gather knowledge from two graphs around a concept that they both share. the relative positions of concepts within the concept hierarchy (Delugach, 1993; Foo et al., 1992; Resnik, 1995). For the present discussion we assume that two concepts are compatible if they share a semantically significant common supertype, or if one concept is a supertype of the other. In Result 3.1, the concept [send] is present twice, and also the concept [letter] is present in two compatible forms: [letter] and [message]. The compatibility comes from the presence in the type hierarchy4 of one sense of [letter], [letter_2], as being a subtype of [message]. These compatible forms actually allow the disambiguation of concept [letter] into [letter_2]. This should update the definition o</context>
</contexts>
<marker>Foo, Garner, Rao, Tsui, 1992</marker>
<rawString>N. Foo, B.J. Garner, A. Rao, and E. Tsui. 1992. Semantic distance in conceptual graphs. In T.E. Nagle, J.A. Nagle, L.L. Gerholz, and P.W.Eklund, editors, Conceptual Structures: Current Research and Practice, chapter 7, pages 149-154. Ellis Horwood.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1995</date>
<booktitle>In Proc. of the 33 th ACL, Cambridge,MA.</booktitle>
<contexts>
<context position="17219" citStr="Pereira et al., 1995" startWordPosition="2606" endWordPosition="2609">mbiguation. We see how redundancy at the concept level without the relations can be very misleading, and the following section emphasize the importance of semantic relations. 107 4// \ destination direction source path Figure 2: Small part of relation taxonomy. with through at on about accompaniment instrument point-in-time location manner 4 The importance of semantic relations Clusters are and have been used in different applications for information retrieval and word sense disambiguation. Clustering can be done statistically by analyzing text corpora (Wilks et al., 1989; Brown et al., 1992; Pereira et al., 1995) and usually results in a set of words or word senses. In this paper, we are using the clustering method used in (Barriere and Popowich, 1996) to present our view on redundancy and disambiguation. The clustering brings together a set of words but also builds a CCKG which shows the actual links (semantic relations) between the members of the cluster. We suggest that those links are essential in analyzing and disambiguating texts. When links are redundant in a graph (that is we find two identical links between two compatible concepts at each end) we are able to reduce semantic ambiguity relating</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1995</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1995. Distributional clustering of english words. In Proc. of the 33 th ACL, Cambridge,MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proc. of the 14 th IJCAI,</booktitle>
<volume>1</volume>
<pages>448--453</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="10548" citStr="Resnik, 1995" startWordPosition="1718" endWordPosition="1719">il_1]. Combining the information from stamp and maili, puts in evidence the redundant information. The reduction process for eliminating this redundancy will solve some ambiguities. This process is based on the idea of finding &amp;quot;compatible&amp;quot; concepts within a graph. Two concepts are compatible if their semantic distance is small. That distance is often based on 3A maximal join is an operation defined within the CC formalism to gather knowledge from two graphs around a concept that they both share. the relative positions of concepts within the concept hierarchy (Delugach, 1993; Foo et al., 1992; Resnik, 1995). For the present discussion we assume that two concepts are compatible if they share a semantically significant common supertype, or if one concept is a supertype of the other. In Result 3.1, the concept [send] is present twice, and also the concept [letter] is present in two compatible forms: [letter] and [message]. The compatibility comes from the presence in the type hierarchy4 of one sense of [letter], [letter_2], as being a subtype of [message]. These compatible forms actually allow the disambiguation of concept [letter] into [letter_2]. This should update the definition of stamp shown i</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proc. of the 14 th IJCAI, volume 1, pages 448-453, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sowa</author>
</authors>
<date>1984</date>
<booktitle>Conceptual Structures in Mind and Machines.</booktitle>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="1902" citStr="Sowa, 1984" startWordPosition="295" endWordPosition="296">inal information at the semantic level. 1 Introduction The construction of a Lexical Knowledge Base (LKB), if performed automatically (or semiautomatically), attempts at extracting knowledge from text. The extraction can be viewed as a learning process. Simplicity, clarity and redundancy of the information given in the source text are key features for a successful acquisition of knowledge. We assume success is attained when a sentence from the source text expressed in natural language can be transformed into an unambiguous internal representation. Using a conceptual graph (CG) representation (Sowa, 1984) of sentences means that a successful acquisition of knowledge corresponds to transforming each sentence from the source text into a set of unambiguous concepts (correct word senses found) and unambiguous relations (correct semantic relations between concepts). This paper will look at the idea of making good use of the redundancy found in a text to help the knowledge acquisition task. Things are not always understood when they are first encountered. A sentence expressing new knowledge might be ambiguous (at the level of the concepts it introduces and/or at the level of the semantic relations b</context>
<context position="12323" citStr="Sowa, 1984" startWordPosition="2016" endWordPosition="2017"> join which establishes coreference links (shown by *x, *y, *z) between compatible concepts that are in an identical relation with another concept. The reduced join, after the redundancy is eliminated, is shown in Result 3.3. Two types of disambiguation (anaphora resolution and word sense disambiguation) were shown up to now. The third type of disambiguation is at the level of the semantic relations. For this type of ambiguity, we must briefly introduce the idea of a relation hierarchy which is described and justified in more details in (Barriere, 1998). A relation hierarchy, as presented in (Sowa, 1984), is simply a way to establish an order between the possible relations. The idea is to include relations that correspond to the English prepositions (it could be the prepositions of any language studied) at the top of the hierarchy, and consider them generalizations of possible deeper semantic relations. 4The type hierarchy has been built automatically from information extracted from the AHFD. 105 Definition 3.1 - MAIL1 : People send messages through the mail. [send]-&gt;(agent)-&gt;[person:plural] -&gt;(object)-&gt;[message:plural] -&gt;(through)-&gt;[mail_1] Definition 3.2 - STAMP : People buy stamps to put o</context>
</contexts>
<marker>Sowa, 1984</marker>
<rawString>J. Sowa. 1984. Conceptual Structures in Mind and Machines. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
<author>D Fass</author>
<author>G-M Guo</author>
<author>J McDonald</author>
<author>T Plate</author>
<author>B Slator</author>
</authors>
<title>A tractable machine dictionary as a resource for computational semantics.</title>
<date>1989</date>
<booktitle>Computational Lexicography for Natural Language Processing, chapter 9,</booktitle>
<pages>193--231</pages>
<editor>In Bran Boguraev and Ted Briscoe, editors,</editor>
<publisher>Longman Group UK Limited.</publisher>
<contexts>
<context position="17176" citStr="Wilks et al., 1989" startWordPosition="2598" endWordPosition="2601">xploit this redundancy for semantic disambiguation. We see how redundancy at the concept level without the relations can be very misleading, and the following section emphasize the importance of semantic relations. 107 4// \ destination direction source path Figure 2: Small part of relation taxonomy. with through at on about accompaniment instrument point-in-time location manner 4 The importance of semantic relations Clusters are and have been used in different applications for information retrieval and word sense disambiguation. Clustering can be done statistically by analyzing text corpora (Wilks et al., 1989; Brown et al., 1992; Pereira et al., 1995) and usually results in a set of words or word senses. In this paper, we are using the clustering method used in (Barriere and Popowich, 1996) to present our view on redundancy and disambiguation. The clustering brings together a set of words but also builds a CCKG which shows the actual links (semantic relations) between the members of the cluster. We suggest that those links are essential in analyzing and disambiguating texts. When links are redundant in a graph (that is we find two identical links between two compatible concepts at each end) we are</context>
</contexts>
<marker>Wilks, Fass, Guo, McDonald, Plate, Slator, 1989</marker>
<rawString>Y. Wilks, D. Fass, G-M Guo, J. McDonald, T. Plate, and B. Slator. 1989. A tractable machine dictionary as a resource for computational semantics. In Bran Boguraev and Ted Briscoe, editors, Computational Lexicography for Natural Language Processing, chapter 9, pages 193-231. Longman Group UK Limited.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>