<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000725">
<title confidence="0.990403">
Pairwise Neural Machine Translation Evaluation
</title>
<author confidence="0.937795">
Francisco Guzm´an Shafiq Joty Lluis M`arquez and Preslav Nakov
</author>
<affiliation confidence="0.9327695">
ALT Research Group
Qatar Computing Research Institute — HBKU, Qatar Foundation
</affiliation>
<email confidence="0.995369">
{fguzman,sjoty,lmarquez,pnakov}@qf.org.qa
</email>
<sectionHeader confidence="0.993809" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999868">
We present a novel framework for ma-
chine translation evaluation using neural
networks in a pairwise setting, where the
goal is to select the better translation from
a pair of hypotheses, given the reference
translation. In this framework, lexical,
syntactic and semantic information from
the reference and the two hypotheses is
compacted into relatively small distributed
vector representations, and fed into a
multi-layer neural network that models the
interaction between each of the hypothe-
ses and the reference, as well as between
the two hypotheses. These compact repre-
sentations are in turn based on word and
sentence embeddings, which are learned
using neural networks. The framework is
flexible, allows for efficient learning and
classification, and yields correlation with
humans that rivals the state of the art.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999820636363636">
Automatic machine translation (MT) evaluation is
a necessary step when developing or comparing
MT systems. Reference-based MT evaluation, i.e.,
comparing the system output to one or more hu-
man reference translations, is the most common
approach. Existing MT evaluation measures typ-
ically output an absolute quality score by com-
puting the similarity between the machine and
the human translations. In the simplest case, the
similarity is computed by counting word n-gram
matches between the translation and the reference.
This is the case of BLEU (Papineni et al., 2002),
which has been the standard for MT evaluation for
years. Nonetheless, more recent evaluation mea-
sures take into account various aspects of linguis-
tic similarity, and achieve better correlation with
human judgments.
Having absolute quality scores at the sentence
level allows to rank alternative translations for a
given source sentence. This is useful, for instance,
for statistical machine translation (SMT) parame-
ter tuning, for system comparison, and for assess-
ing the progress during MT system development.
The quality of automatic MT evaluation metrics
is usually assessed by computing their correlation
with human judgments. To that end, quality rank-
ings of alternative translations have been created
by human judges. It is known that assigning an
absolute score to a translation is a difficult task
for humans. Hence, ranking-based evaluations,
where judges are asked to rank the output of 2 to 5
systems, have been used in recent years, which
has yielded much higher inter-annotator agree-
ment (Callison-Burch et al., 2007).
These human quality judgments can be used to
train automatic metrics. This supervised learning
can be oriented to predict absolute scores, e.g., us-
ing regression (Albrecht and Hwa, 2008), or rank-
ings (Duh, 2008; Song and Cohn, 2011). A partic-
ular case of the latter is used to learn in a pair-
wise setting, i.e., given a reference and two al-
ternative translations (or hypotheses), the task is
to decide which one is better. This setting em-
ulates closely how human judges perform evalu-
ation assessments in reality, and can be used to
produce rankings for an arbitrarily large number
of hypotheses. In this pairwise setting, the chal-
lenge is to learn, from a pair of hypotheses, which
are the features that help to discriminate the better
from the worse translation. Although the pairwise
setting does not produce absolute quality scores
(i.e., it is not an evaluation metric applicable to a
single translation), it is useful and arguably suf-
ficient for most evaluation and MT development
scenarios.1
</bodyText>
<footnote confidence="0.986310333333333">
1We do not argue that the pairwise approach is better
than the direct estimation of human quality scores. Both ap-
proaches have pros and cons; we see them as complementary.
</footnote>
<page confidence="0.9372">
805
</page>
<note confidence="0.8877985">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 805–814,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
Recently, Guzm´an et al. (2014a) presented a
</note>
<bodyText confidence="0.9996487">
learning framework for this pairwise setting, based
on preference kernels and support vector ma-
chines (SVM). They obtained promising results
using syntactic and discourse-based structures.
However, using convolution kernels over complex
structures comes at a high computational cost both
at training and at testing time because the use of
kernels requires that the SVM operate in the much
slower dual space. Thus, some simplification is
needed to make it practical. While there are some
solutions in the kernel-based learning framework
to alleviate the computational burden, in this pa-
per we explore an entirely different direction.
We present a novel neural-based architecture for
learning in the pairwise setting for MT evalua-
tion. Lexical, syntactic and semantic information
from the reference and the two hypotheses is com-
pacted into relatively small distributed vector rep-
resentations and fed into the input layer, together
with a set of individual real-valued features com-
ing from simple pre-existing MT evaluation met-
rics. A hidden layer, motivated by our intuitions
on the pairwise ranking problem, is used to cap-
ture interactions between the relevant input com-
ponents. Finally, we present a task-oriented cost
function, specifically tailored for this problem.
Our evaluation results on the WMT12 metrics
task benchmark datasets (Callison-Burch et al.,
2012) show very high correlation with human
judgments. These results clearly surpass (Guzm´an
et al., 2014a) and are comparable to the best pre-
viously reported results for this dataset, achieved
by DiscoTK (Joty et al., 2014), which is a much
heavier combination-based metric.
Another advantage of the proposed architecture
is efficiency. Due to the vector-based compres-
sion of the linguistic structure and the relatively
reduced size of the network, testing is fast, which
would greatly facilitate the practical use of this ap-
proach in real MT evaluation and development.
Finally, we empirically show that syntactically-
and semantically-oriented embeddings can be in-
corporated to produce sizeable and cumulative
gains in performance over a strong combination
of pre-existing MT evaluation measures (BLEU,
NIST, METEOR, and TER). This is promising ev-
idence towards our longer-term goal of defining a
general platform for integrating varied linguistic
information and for producing more informed MT
evaluation measures.
</bodyText>
<sectionHeader confidence="0.999341" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99977164">
Contemporary MT evaluation measures have
evolved beyond simple lexical matching, and
now take into account various aspects of lin-
guistic structures, including synonymy and para-
phrasing (Lavie and Denkowski, 2009), syn-
tax (Gim´enez and M`arquez, 2007; Popovi´c and
Ney, 2007; Liu and Gildea, 2005), seman-
tics (Gim´enez and M`arquez, 2007; Lo et al.,
2012), and even discourse (Comelles et al., 2010;
Wong and Kit, 2012; Guzm´an et al., 2014b; Joty
et al., 2014). The combination of several of
these aspects has led to improved results in metric
evaluation campaigns, such as the WMT metrics
task (Bojar et al., 2014).
In this paper, we present a general framework
for learning to rank translations in the pairwise
setting, using information from several linguistic
representations of the translations and references.
This work has connections with the ranking-based
approaches for learning to reproduce human judg-
ments of MT quality. In particular, our setting is
similar to that of Duh (2008), but differs from it
both in terms of the feature representation and of
the learning framework. For instance, we integrate
several layers of linguistic information, while Duh
(2008) only used lexical and POS matches as fea-
tures. Secondly, we use information about both
the reference and the two alternative translations
simultaneously in a neural-based learning frame-
work capable of modeling complex interactions
between the features.
Another related work is that of Kulesza and
Shieber (2004), in which lexical and syntactic fea-
tures, together with other metrics, e.g., BLEU and
NIST, are used in an SVM classifier to discrimi-
nate good from bad translations. However, their
setting is not pairwise comparison, but a classifi-
cation task to distinguish human- from machine-
produced translations. Moreover, in their work,
using syntactic features decreased the correla-
tion with human judgments dramatically (although
classification accuracy improved), while in our
case the effect is positive.
In our previous work (Guzm´an et al., 2014a),
we introduced a learning framework for the pair-
wise setting, based on preference kernels and
SVMs. We used lexical, POS, syntactic and
discourse-based information in the form of tree-
like structures to learn to differentiate better from
worse translations.
</bodyText>
<page confidence="0.997308">
806
</page>
<bodyText confidence="0.999989195121951">
However, in that work we used convolution ker-
nels, which is computationally expensive and does
not scale well to large datasets and complex struc-
tures such as graphs and enriched trees. This in-
efficiency arises both at training and testing time.
Thus, here we use neural embeddings and multi-
layer neural networks, which yields an efficient
learning framework that works significantly better
on the same datasets (although we are not using
exactly the same information for learning).
To the best of our knowledge, the application
of structured neural embeddings and a neural net-
work learning architecture for MT evaluation is
completely novel. This is despite the growing in-
terest in recent years for deep neural nets (NNs)
and word embeddings with application to a myr-
iad of NLP problems. For example, in SMT we
have observed an increased use of neural nets for
language modeling (Bengio et al., 2003; Mikolov
et al., 2010) as well as for improving the transla-
tion model (Devlin et al., 2014; Sutskever et al.,
2014).
Deep learning has spread beyond language
modeling. For example, recursive NNs have been
used for syntactic parsing (Socher et al., 2013a)
and sentiment analysis (Socher et al., 2013b). The
increased use of NNs by the NLP community is
in part due to (i) the emergence of tools such as
word2vec (Mikolov et al., 2013a) and GloVe (Pen-
nington et al., 2014), which have enabled NLP re-
searchers to learn word embeddings, and (ii) uni-
fied learning frameworks, e.g., (Collobert et al.,
2011), which cover a variety of NLP tasks such
as part-of-speech tagging, chunking, named entity
recognition, and semantic role labeling.
While in this work we make use of widely avail-
able pre-computed structured embeddings, the
novelty of our work goes beyond the type of infor-
mation considered as input, and resides on the way
it is integrated to a neural network architecture that
is inspired by our intuitions about MT evaluation.
</bodyText>
<sectionHeader confidence="0.998363" genericHeader="method">
3 Neural Ranking Model
</sectionHeader>
<bodyText confidence="0.99987275">
Our motivation for using neural networks for MT
evaluation is twofold. First, to take advantage of
their ability to model complex non-linear relation-
ships efficiently. Second, to have a framework
that allows for easy incorporation of rich syntac-
tic and semantic representations captured by word
embeddings, which are in turn learned using deep
learning.
</bodyText>
<subsectionHeader confidence="0.999868">
3.1 Learning Task
</subsectionHeader>
<bodyText confidence="0.9999808">
Given two translation hypotheses t1 and t2 (and a
reference translation r), we want to tell which of
the two is better.2 Thus, we have a binary classifi-
cation task, which is modeled by the class variable
y, defined as follows:
</bodyText>
<equation confidence="0.671290333333333">
=f 1 if t1 is better than t2 given r 1
y
t 0 if t1 is worse than t2 given r ( )
</equation>
<bodyText confidence="0.9980615">
We model this task using a feed-forward neural
network (NN) of the form:
</bodyText>
<equation confidence="0.992515">
p(y|t1, t2, r) = Ber(y|f(t1, t2, r)) (2)
which is a Bernoulli distribution of y with param-
eter Q = f(t1, t2, r), defined as follows:
f(t1, t2, r) = sig(wv O(t1, t2, r) + b„) (3)
</equation>
<bodyText confidence="0.9999595">
where sig is the sigmoid function, O(x) defines the
transformations of the input x through the hidden
layer, wv are the weights from the hidden layer to
the output layer, and b„ is a bias term.
</bodyText>
<subsectionHeader confidence="0.998698">
3.2 Network Architecture
</subsectionHeader>
<bodyText confidence="0.999952857142857">
In order to decide which hypothesis is better given
the tuple (t1, t2, r) as input, we first map the hy-
potheses and the reference to a fixed-length vec-
tor [xt1, xt2, xr], using syntactic and semantic em-
beddings. Then, we feed this vector as input to
our neural network, whose architecture is shown
in Figure 1.
</bodyText>
<figure confidence="0.226178">
sentences embeddings pairwise nodes pairwise features
</figure>
<figureCaption confidence="0.998119">
Figure 1: Overall architecture of the neural network.
</figureCaption>
<bodyText confidence="0.9998962">
In our architecture, we model three types of in-
teractions, using different groups of nodes in the
hidden layer. We have two evaluation groups h1r
and her that model how similar each hypothesis tz
is to the reference r.
</bodyText>
<footnote confidence="0.902908">
2In this work, we do not learn to predict ties, and ties are
excluded from our training data.
</footnote>
<figure confidence="0.741212">
h12
ψ(t1,r) ψ(t2,r)
xt1
t1
v
f(t1,t2,r)
output layer
xt2
xr
h1r
h2r
t2
r
</figure>
<page confidence="0.978465">
807
</page>
<bodyText confidence="0.983517692307692">
The vector representations of the hypothesis
(i.e., xt1 or xt2) together with the reference
(i.e., xr) constitute the input to the hidden nodes
in these two groups. The third group of hidden
nodes h12, which we call similarity group, mod-
els how close t1 and t2 are. This might be useful
as highly similar hypotheses are likely to be com-
parable in quality, irrespective of whether they are
good or bad in absolute terms.
The input to each of these groups is repre-
sented by concatenating the vector representations
of the two components participating in the inter-
action, i.e., x1r = [xt1, xr], x2r = [xt2, xr],
</bodyText>
<equation confidence="0.838219333333333">
x12 = [xt1, xt2]. In summary, the transformation
φ(t1, t2, r) = [h12, h1r, h2r] in our NN architec-
ture can be written as follows:
h1r = g(W1rx1r + b1r)
h2r = g(W2rx2r + b2r)
h12 = g(W12x12 + b12)
</equation>
<bodyText confidence="0.999965611111111">
where g(.) is a non-linear activation function (ap-
plied component-wise), W E RH×N are the asso-
ciated weights between the input layer and the hid-
den layer, and b are the corresponding bias terms.
In our experiments, we used tanh as an activation
function, rather than sig, to be consistent with how
parts of our input vectors were generated.3
In addition, our model allows to incorporate ex-
ternal sources of information by enabling skip arcs
that go directly from the input to the output, skip-
ping the hidden layer. In our setting, these arcs
represent pairwise similarity features between the
translation hypotheses and the reference (e.g., the
BLEU scores of the translations). We denote these
pairwise external feature sets as ψ1r = ψ(t1, r)
and ψ2r = ψ(t2, r). When we include the external
features in our architecture, the activation at the
output, i.e., eq. (3), can be rewritten as follows:
</bodyText>
<equation confidence="0.836758">
f(t1, t2, r) = sig(wTv [φ(t1, t2, r), ψ1r, ψ2r] + bv)
</equation>
<subsectionHeader confidence="0.995334">
3.3 Network Training
</subsectionHeader>
<bodyText confidence="0.99815225">
The negative log likelihood of the train-
ing data for the model parameters
θ = (W12, W1r, W2r, wv, b12, b1r, b2r, bv)
can be written as follows:
</bodyText>
<equation confidence="0.985357666666667">
�Jo = − yn log ˆyno + (1 − yn) log (1 − ˆyno)
n
(4)
</equation>
<bodyText confidence="0.992194615384615">
3Many of our input representations consist of word em-
beddings trained with neural networks that used tanh as an
activation function.
In the above formula, ˆyno = fn(t1, t2, r) is
the activation at the output layer for the n-th
data instance. It is also common to use a reg-
ularized cost function by adding a weight decay
penalty (e.g., L2 or L1 regularization) and to per-
form maximum aposteriori (MAP) estimation of
the parameters. We trained our network with
stochastic gradient descent (SGD), mini-batches
and adagrad updates (Duchi et al., 2011), using
Theano (Bergstra et al., 2010).
</bodyText>
<sectionHeader confidence="0.997084" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999971875">
In this section, we describe the different aspects
of our general experimental setup (we will discuss
some extensions thereof in Section 6), starting
with a description of the input representations we
use to capture the syntactic and semantic charac-
teristics of the two hypothesis translations and the
corresponding reference, as well as the datasets
used to evaluate the performance of our model.
</bodyText>
<subsectionHeader confidence="0.998459">
4.1 Word Embedding Vectors
</subsectionHeader>
<bodyText confidence="0.999046464285714">
Word embeddings play a crucial role in our model,
since they allow us to model complex relations
between the translations and the reference using
syntactic and semantic vector representations.
Syntactic vectors. We generate a syntactic vector
for each sentence using the Stanford neural parser
(Socher et al., 2013a), which generates a 25-
dimensional vector as a by-product of syntactic
parsing using a recursive NN. Below we will refer
to these vectors as SYNTAX25.
Semantic vectors. We compose a semantic vector
for a given sentence using the average of the em-
bedding vectors for the words it contains (Mitchell
and Lapata, 2010). We use pre-trained, fixed-
length word embedding vectors produced by
(i) GloVe (Pennington et al., 2014), (ii) COM-
POSES (Baroni et al., 2014), and (iii) word2vec
(Mikolov et al., 2013b).
Our primary representation is based on 50-
dimensional GloVe vectors, trained on Wikipedia
2014+Gigaword 5 (6B tokens), to which below we
will refer as WIKI-GW25.
Furthermore, we experiment with WIKI-
GW300, the 300-dimensional GloVe vectors
trained on the same data, as well as with the CC-
300-42B and CC-300-840B, 300-dimensional
GloVe vectors trained on 42B and on 840B tokens
from Common Crawl.
</bodyText>
<page confidence="0.990874">
808
</page>
<bodyText confidence="0.999976857142857">
We also experiment with the pre-trained, 300-
dimensional word2vec embedding vectors, or
WORD2VEC300, trained on 100B words from
Google News. Finally, we use COMPOSES400,
the 400-dimensional COMPOSES vectors trained
on 2.8 billion tokens from ukWaC, the English
Wikipedia, and the British National Corpus.
</bodyText>
<subsectionHeader confidence="0.996365">
4.2 Tuning and Evaluation Datasets
</subsectionHeader>
<bodyText confidence="0.999913">
We experiment with datasets of segment-level
human rankings of system outputs from the
WMT11, WMT12 and WMT13 Metrics shared
tasks (Callison-Burch et al., 2011; Callison-Burch
et al., 2012; Mach´aˇcek and Bojar, 2013). We focus
on translating into English, for which the WMT11
and WMT12 datasets can be split by source lan-
guage: Czech (cs), German (de), Spanish (es), and
French (fr); WMT13 also has Russian (ru).
</bodyText>
<subsectionHeader confidence="0.994457">
4.3 Evaluation Score
</subsectionHeader>
<bodyText confidence="0.9999827">
We evaluate our metrics in terms of correlation
with human judgments measured using Kendall’s
T. We report T for the individual languages as well
as macro-averaged across all languages.
Note that there were different versions of T at
WMT over the years. Prior to 2013, WMT used a
strict version, which was later relaxed at WMT13
and further revised at WMT14. See (Mach´aˇcek
and Bojar, 2014) for a discussion. Here we use the
strict version used at WMT11 and WMT12.
</bodyText>
<subsectionHeader confidence="0.984878">
4.4 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.99989295">
Datasets: We train our neural models on WMT11
and we evaluate them on WMT12. We further use
a random subset of 5,000 examples from WMT13
as a validation set to implement early stopping.
Early stopping: We train on WMT11 for up to
10,000 epochs, and we calculate Kendall’s T on
the development set after each epoch. We then se-
lect the model that achieves the highest T on the
validation set; in case of ties for the best T, we
select the latest epoch that achieved the highest T.
Network parameters: We train our neural net-
work using SGD with adagrad, an initial learning
rate of q = 0.01, mini-batches of size 30, and L2
regularization with a decay parameter A = 1e−4.
We initialize the weights for our matrices by sam-
pling from a uniform distribution following (Ben-
gio and Glorot, 2010). We further set the size
of each of our pairwise hidden layers H to four
nodes, and we normalize the input data using min-
max to map the feature values to the range [−1, 1].
</bodyText>
<sectionHeader confidence="0.991724" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9997878">
The main findings of our experiments are shown
in Table 1. Section I of Table 1 shows the re-
sults for four commonly-used metrics for MT eval-
uation that compare a translation hypothesis to
the reference(s) using primarily lexical informa-
tion like word and n-gram overlap (even though
some allow paraphrases): BLEU, NIST, TER,
and METEOR (Papineni et al., 2002; Doddington,
2002; Snover et al., 2006; Denkowski and Lavie,
2011). We will refer to the set of these four met-
rics as 4METRICS. These metrics are not tuned
and achieve Kendall’s T between 18.5 and 23.5.
Section II of Table 1 shows the results for multi-
layer neural networks trained on vectors from
word embeddings only: SYNTAX25 and WIKI-
GW25. These networks achieve modest T values
around 10, which should not be surprising: they
use very general vector representations and have
no access to word or n-gram overlap or to length
information, which are very important features to
compute similarity against the reference. How-
ever, as will be discussed below, their contribution
is complementary to the four previous evaluation
metrics and will lead to significant improvements
in combination with them.
Section III of Table 1 shows the results for neu-
ral networks that combine the four metrics from
4METRICS with SYNTAX25 and WIKI-GW25.
We can see that just combining the four metrics
in a flat neural net (i.e., no hidden layer), which
is equivalent to a logistic regression, yields a T of
27.06, which is better than the best of the four met-
rics by 3.5 points absolute, and also better by over
1.5 points absolute than the best metric that par-
ticipated at the WMT12 metrics task competition
(SPEDE07PP with T = 25.4). Indeed, 4METRICS
is a strong mix that involves not only simple lex-
ical overlap but also approximate matching, para-
phrases, edit distance, lengths, etc. Yet, adding to
4METRICS the embedding vectors yields sizeable
further improvements: +1.5 and +2.0 points abso-
lute when adding SYNTAX25 and WIKI-GW25,
respectively. Finally, adding both yields even
further improvements close to T of 30 (+2.64 T
points), showing that lexical semantics and syn-
tactic representations are complementary.
Section IV of Table 1 puts these numbers in per-
spective: it lists the T for the top three systems that
participated at WMT12, whose scores ranged be-
tween 22.9 and 25.4.
</bodyText>
<page confidence="0.993869">
809
</page>
<table confidence="0.9726035">
System Details Kendall’s τ
I 4METRICS: commonly-used individual metrics cz de es fr AVG
BLEU no learning
NIST no learning
TER no learning
METEOR no learning
II NN using embedding vectors: syntactic &amp; semantic
SYNTAX25 multi-layer NN
WIKI-GW25 multi-layer NN
III NN using 4METRICS+ embedding vectors
15.88 18.56 18.57 20.83 18.46
19.66 23.09 20.41 22.21 21.34
17.80 25.31 22.86 21.05 21.75
20.82 26.79 23.81 22.93 23.59
8.00 13.03 12.11 7.42 10.14
14.31 11.49 9.24 4.99 10.01
23.46 29.95 27.49 27.36 27.06
26.09 30.58 29.30 28.07 28.51
25.67 32.50 29.21 28.92 29.07
26.30 33.19 30.38 28.92 29.70
4METRICS logistic regression
4METRICS+SYNTAX25 multi-layer NN
4METRICS+WIKI-GW25 multi-layer NN
4METRICS+SYNTAX25+WIKI-GW25 multi-layer NN
IV Comparison to previous results on WMT12
DiscoTK (Joty et al., 2014) Best on the WMT12 dataset na na na na 30.5
SPEDE07PP 1st at the WMT12 competition 21.2 27.8 26.5 26.0 25.4
METEOR* 2nd at WMT12 the competition 21.2 27.5 24.9 25.1 24.7
(Guzm´an et al., 2014a) Preference kernel approach 23.1 25.8 22.6 23.2 23.7
AMBER 3rd at the WMT12 competition 19.1 24.8 23.1 24.5 22.9
</table>
<tableCaption confidence="0.986314333333333">
Table 1: Kendall’s tau (τ) on the WMT12 dataset for various metrics. Notes: (i) the version of METEOR that took part in the
WMT12 competition (marked with * in section IV of the table) is different from the one used in our experiments (section I of
the table), (ii) values marked as na were not reported by the authors.
</tableCaption>
<bodyText confidence="0.999952466666667">
We can see that 4METRICS is much stronger
than the winner at WMT12, and thus arguably a
baseline hard to improve upon. While our results
are slightly behind those of DiscoTK (Joty et al.,
2014), we should note that we only combine four
metrics, plus the vectors, while DiscoTK com-
bines over 20 metrics, many of which are costly
to compute.
On the other hand, we work in a ranking frame-
work, i.e., we are not interested in producing an
absolute score, but in making pairwise decisions
only. Mapping these pairwise decisions into an ab-
solute score is challenging and in our experiments
it leads to a slight drop in T (results omitted here
to save space).
The only other result on WMT12 by authors
working with our pairwise framework is our own
previous work (Guzm´an et al., 2014a), where we
used a preference kernel approach to combine syn-
tactic and discourse trees with lexical information;
as we can see, our earlier results are 6 absolute
points lower than those we achieve here. More-
over, our NN approach offers advantages over
SVMs in terms of computational cost.
Based on these results, we can conclude that
word embeddings, whether syntactic or semantic,
offer generalizations that efficiently complement
very strong metric combinations, and thus should
be considered when designing future MT evalua-
tion metrics.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999980125">
In this section, we explore how different parts of
our framework can be modified to improve its per-
formance, or how it can be extended for further
generalization. First, we explore variations of the
feature sets from the perspective of both the pair-
wise features and the embeddings. Then, we ana-
lyze the role of the network architecture and of the
cost function used for learning.
</bodyText>
<subsectionHeader confidence="0.995825">
6.1 Fine-Grained Pairwise Features
</subsectionHeader>
<bodyText confidence="0.999902526315789">
We have shown that our NN can integrate syntactic
and semantic vectors with scores from other met-
rics. In fact, ours is a more general framework,
where one can integrate the components of a met-
ric instead of its score, which could yield better
learning. Below, we demonstrate this for BLEU.
BLEU has different components: the n-gram
precisions, the n-gram matches, the total num-
ber of n-grams (n=1,2,3,4), the lengths of the hy-
potheses and of the reference, the length ratio be-
tween them, and BLEU’s brevity penalty. We will
refer to this decomposed BLEU as BLEUCOMP.
Some of these features were previously used in
SIMPBLEU (Song and Cohn, 2011).
The results of using the components of
BLEUCOMP as features are shown in Table 2. We
see that using a single-layer neural network, which
is equivalent to logistic regression, outperforms
BLEU by more than +1 T points absolute.
</bodyText>
<page confidence="0.98392">
810
</page>
<figure confidence="0.941670875">
Kendall’s T
System Details
BLEU no learning
BLEUCOMP logistic regression
BLEUCOMP+SYNTAX25 multi-layer NN
BLEUCOMP+WIKI-GW25 multi-layer NN
BLEUCOMP+SYNTAX25+WIKI-GW25 multi-layer NN
BLEU+SYNTAX25+WIKI-GW25 multi-layer NN
</figure>
<table confidence="0.980563571428571">
cz de es fr AVG
15.88 18.56 18.57 20.83 18.46
18.18 21.13 19.79 19.91 19.75
20.75 25.32 24.85 23.88 23.70
22.96 26.63 25.99 24.10 24.92
22.84 28.92 27.95 24.90 26.15
20.03 25.95 27.07 23.16 24.05
</table>
<tableCaption confidence="0.872859">
Table 2: Kendall’s T on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
the last line shows a combination using BLEU instead of BLEUCOMP.
</tableCaption>
<table confidence="0.941721857142857">
Source Alone Comb.
WIKI-GW25 10.01 29.70
WIKI-GW300 9.66 29.90
CC-300-42B 12.16 29.68
CC-300-840B 11.41 29.88
WORD2VEC300 7.72 29.13
COMPOSES400 12.35 28.54
</table>
<tableCaption confidence="0.7774536">
Table 3: Average Kendall’s T on WMT12 for semantic vec-
tors trained on different text collections. Shown are results
(i) when using the semantic vectors alone, and (ii) when com-
bining them with 4METRICS and SYNTAX25. The improve-
ments over WIKI-GW25 are marked in bold.
</tableCaption>
<bodyText confidence="0.9999851">
As before, adding SYNTAX25 and WIKI-
GW25 improves the results, but now by a more
sizable margin: +4 for the former and +5 for the
latter. Adding both yields +6.5 improvement over
BLEUCOMP, and almost 8 points over BLEU.
We see once again that the syntactic and seman-
tic word embeddings are complementary to the in-
formation sources used by metrics such as BLEU,
and that our framework can learn from richer pair-
wise feature sets such as BLEUCOMP.
</bodyText>
<subsectionHeader confidence="0.999331">
6.2 Larger Semantic Vectors
</subsectionHeader>
<bodyText confidence="0.999484133333333">
One interesting aspect to explore is the effect of
the dimensionality of the input embeddings. Here,
we studied the impact of using semantic vectors
of bigger sizes, trained on different and larger text
collections. The results are shown in Table 3.
We can see that, compared to the 50-dimensional
WIKI-GW25, 300-400 dimensional vectors are
generally better by 1-2 T points absolute when
used in isolation; however, when used in combina-
tion with 4METRICS+SYNTAX25, they do not of-
fer much gain (up to +0.2), and in some cases, we
observe a slight drop in performance. We suspect
that the variability across the different collections
is due to a domain mismatch. Yet, we defer this
question for future work.
</bodyText>
<table confidence="0.99751825">
Details Kendall’s T
cz de es fr AVG
single-layer 25.86 32.06 30.03 28.45 29.10
multi-layer 26.30 33.19 30.38 28.92 29.70
</table>
<tableCaption confidence="0.984939">
Table 4: Kendall’s tau (T) on the WMT12 dataset for al-
ternative architectures using 4METRICS+SYNTAX25+WIKI-
GW25 as input.
</tableCaption>
<subsectionHeader confidence="0.987331">
6.3 Deep vs. Flat Neural Network
</subsectionHeader>
<bodyText confidence="0.999977923076923">
One interesting question is how much of the learn-
ing is due to the rich input representations, and
how much happens because of the architecture of
the neural network. To answer this, we exper-
imented with two settings: a single-layer neural
network, where all input features are fed directly
to the output layer (which is logistic regression),
and our proposed multi-layer neural network.
The results are shown in Table 4. We can see
that switching from our multi-layer architecture to
a single-layer one yields an absolute drop of 0.6
T. This suggests that there is value in using the
deeper, pairwise layer architecture.
</bodyText>
<subsectionHeader confidence="0.996427">
6.4 Task-Specific Cost Function
</subsectionHeader>
<bodyText confidence="0.99985125">
Another question is whether the log-likelihood
cost function J(B) (see Section 3.3) is the most
appropriate for our ranking task, provided that it is
evaluated using Kendall’s T as defined below:
</bodyText>
<equation confidence="0.9954485">
T = concord. − disc. − ties (5)
concord + disc. + ties
</equation>
<bodyText confidence="0.999966111111111">
where concord., disc. and ties are the number of
concordant, disconcordant and tied pairs.
Given an input tuple (t1, t2, r), the logistic cost
function yields larger values of Q = f(t1, t2, r) if
y = 1, and smaller if y = 0, where 0 &lt; Q &lt; 1 is
the parameter of the Bernoulli distribution. How-
ever, it does not model directly the probability
when the order of the hypotheses in the tuple is
reversed, i.e., Q&apos; = f(t2, t1, r).
</bodyText>
<page confidence="0.995904">
811
</page>
<table confidence="0.8946204">
Kendall’s τ
Details cz de es fr AVG
Logistic 26.30 33.19 30.38 28.92 29.70
Kendall 27.04 33.60 29.48 28.54 29.53
Log.+Ken. 26.90 33.17 30.40 29.21 29.92
</table>
<tableCaption confidence="0.9850425">
Table 5: Kendall’s tau (τ) on WMT12 for alternative cost
functions using 4METRICS+SYNTAX25+WIKI-GW25.
</tableCaption>
<bodyText confidence="0.9999056">
For our specific task, given an input tuple
(t1, t2, r), we want to make sure that the difference
between the two output activations A = σ − σ&apos; is
positive when y = 1, and negative when y = 0.
Ensuring this would take us closer to the actual
objective, which is Kendall’s τ. One possible way
to do this is to introduce a task-specific cost func-
tion that penalizes the disagreements similarly to
the way Kendall’s τ does.4 In particular, we de-
fine a new Kendall cost as follows:
</bodyText>
<equation confidence="0.997764666666667">
�JB = − yn sig(−γAn) + (1 − yn) sig(γAn)
n
(6)
</equation>
<bodyText confidence="0.999909111111111">
where we use the sigmoid function sig as a differ-
entiable approximation to the step function.
The above cost function penalizes disconcor-
dances, i.e., cases where (i) y = 1 but A &lt; 0,
or (ii) when y = 0 but A &gt; 0. However, we also
need to make sure that we discourage ties. We do
so by adding a zero-mean Gaussian regularization
term exp(−βA2/2) that penalizes the value of A
getting close to zero. Note that the specific val-
ues for γ and β are not really important, as long
as they are large. In particular, in our experiments,
we used γ = β = 100.
Table 5 shows a comparison of the two cost
functions: (i) the standard logistic cost, and (ii) our
Kendall cost. We can see that using the Kendall
cost enables effective learning, although it is even-
tually outperformed by the logistic cost. Our in-
vestigation revealed that this was due to a combi-
nation of slower convergence and poor initializa-
tion. Therefore, we further experimented with a
setup where we first used the logistic cost to pre-
train the neural network, and then we switched to
the Kendall cost in order to perform some finer
tuning. As we can see in Table 5 (last row), do-
ing so yielded a sizable improvement over using
the Kendall cost only; it also improved over using
the logistic cost only.
</bodyText>
<footnote confidence="0.989482">
4Other variations for ranking tasks are possible, e.g., (Yih
et al., 2011).
</footnote>
<sectionHeader confidence="0.787074" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99996806">
We have presented a novel framework for learn-
ing a tunable MT evaluation metric in a pairwise
ranking setting, given pre-existing pairwise human
preference judgments.
In particular, we used a neural network, where
the input layer encodes lexical, syntactic and se-
mantic information from the reference and the two
translation hypotheses, which is efficiently com-
pacted into relatively small embeddings. The net-
work has a hidden layer, motivated by our intuition
about the problem, which captures the interactions
between the relevant input components. Unlike
previously proposed kernel-based approaches, our
framework allows us to do both training and in-
ference efficiently. Moreover, we have shown that
it can be trained to optimize a task-specific cost
function, which is more appropriate for the pair-
wise MT evaluation setting.
The evaluation results have shown that our NN
model yields state-of-the-art results when using
lexical, syntactic and semantic features (the latter
two based on compact embeddings). Moreover,
we have shown that the contribution of the differ-
ent information sources is additive, thus demon-
strating that the framework can effectively inte-
grate complementary information. Furthermore,
the framework is flexible enough to exploit dif-
ferent granularities of features such as n-gram
matches and other components of BLEU (which
individually work better than using the aggregated
BLEU score). Finally, we have presented evidence
suggesting that using the pairwise hidden layers is
advantageous over simpler flat models.
In future work, we would like to experiment
with an extension that allows for multiple refer-
ences. We further plan to incorporate features
from the source sentence. We believe that our
framework can support learning similarities be-
tween the two translations and the source, for an
improved MT evaluation. Variations of this ar-
chitecture might be useful for related tasks such
as Quality Estimation and hypothesis re-ranking
for Machine Translation, where no references are
available.
Other aspects worth studying as a complement
to the present work include (i) the impact of the
quality of the syntactic analysis (translations are
often just a “word salad”), (ii) differences across
language pairs, and (iii) the relevance of the do-
main the semantic representations are trained on.
</bodyText>
<page confidence="0.995509">
812
</page>
<sectionHeader confidence="0.990141" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999596657657658">
Joshua Albrecht and Rebecca Hwa. 2008. Regression
for machine translation evaluation at the sentence
level. Machine Translation, 22(1-2):1–27.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics, ACL ’14, pages
238–247, Baltimore, Maryland, USA.
Yoshua Bengio and Xavier Glorot. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Proceedings of AI &amp; Statistics 2010,
volume 9, pages 249–256, Chia Laguna Resort, Sar-
dinia, Italy.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference,
SciPy ’10, Austin, Texas.
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs
Tamchyna. 2014. Findings of the 2014 workshop
on statistical machine translation. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, WMT ’14, pages 12–58, Baltimore, Maryland,
USA.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, WMT ’07, pages 136–158,
Prague, Czech Republic.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ’11, pages 22–64, Edin-
burgh, Scotland.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation,
WMT ’12, pages 10–51, Montr´eal, Canada.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
Elisabet Comelles, Jes´us Gim´enez, Llu´ıs M`arquez,
Irene Castell´on, and Victoria Arranz. 2010.
Document-level automatic MT evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ’10, pages 333–
338, Uppsala, Sweden.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ’11, pages 85–91, Edin-
burgh, Scotland.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, ACL ’14, pages 1370–1380,
Baltimore, Maryland, USA.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, HLT ’02, pages 138–145,
San Francisco, California, USA. Morgan Kaufmann
Publishers.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.
Kevin Duh. 2008. Ranking vs. regression in ma-
chine translation evaluation. In Proceedings of the
Third Workshop on Statistical Machine Translation,
WMT ’08, pages 191–194, Columbus, Ohio, USA.
Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguis-
tic features for automatic evaluation of heterogenous
MT systems. In Proceedings of the Second Work-
shop on Statistical Machine Translation, WMT ’07,
pages 256–264, Prague, Czech Republic.
Francisco Guzm´an, Shafiq Joty, Llu´ıs M`arquez,
Alessandro Moschitti, Preslav Nakov, and Massimo
Nicosia. 2014a. Learning to differentiate bet-
ter from worse translations. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing, EMNLP ’14, pages 214–220,
Doha, Qatar.
Francisco Guzm´an, Shafiq Joty, Llu´ıs M`arquez, and
Preslav Nakov. 2014b. Using discourse structure
improves machine translation evaluation. In Pro-
ceedings of 52nd Annual Meeting of the Association
for Computational Linguistics, ACL ’14, pages 687–
698, Baltimore, Maryland, USA.
</reference>
<page confidence="0.989686">
813
</page>
<reference confidence="0.999815964912281">
Shafiq Joty, Francisco Guzm´an, Llu´ıs M`arquez, and
Preslav Nakov. 2014. DiscoTK: Using discourse
structure for machine translation evaluation. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation, WMT ’14, pages 402–408, Balti-
more, Maryland, USA.
Alex Kulesza and Stuart M. Shieber. 2004. A learn-
ing approach to improving sentence-level MT evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation.
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2–3):105–115.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 25–32, Ann Ar-
bor, Michigan, USA.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic MT evaluation. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, WMT ’12, pages 243–252,
Montr´eal, Canada.
Matous&amp;quot; Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, WMT ’13, pages 45–51, Sofia, Bulgaria.
Matous&amp;quot; Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2014. Results of
the WMT14 metrics shared task. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, WMT ’14, pages 293–301, Baltimore, Mary-
land, USA.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock´y, and Sanjeev Khudanpur. 2010. Re-
current neural network based language model.
In 11th Annual Conference of the International
Speech Communication Association, pages 1045–
1048, Makuhari, Chiba, Japan.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their composition-
ality. In Advances in Neural Information Process-
ing Systems 26, NIPS ’13, pages 3111–3119. Lake
Tahoe, California, USA.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT ’13, pages
746–751, Atlanta, Georgia, USA.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1439.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meting of the Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Philadelphia, Pennsylvania, USA.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’14, pages 1532–1543, Doha, Qatar.
Maja Popovi´c and Hermann Ney. 2007. Word error
rates: Decomposition over POS classes and applica-
tions for error analysis. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
WMT ’07, pages 48–55, Prague, Czech Republic.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA ’06, Cambridge, Massachusetts, USA.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013a. Parsing with compo-
sitional vector grammars. In Proceedings of the
51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
ACL ’13, pages 455–465, Sofia, Bulgaria.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’13, pages 1631–1642, Seattle, Wash-
ington, USA.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence-level MT
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, WMT ’11, pages
123–129, Edinburgh, Scotland.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the Neural Information
Processing Systems, NIPS ’14, Montreal, Canada.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 1060–1068, Jeju Island, Korea.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Pro-
ceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning, CoNLL ’11,
pages 247–256, Portland, Oregon, USA.
</reference>
<page confidence="0.998527">
814
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.534519">
<title confidence="0.999913">Pairwise Neural Machine Translation Evaluation</title>
<author confidence="0.994401">Guzm´an Shafiq Joty Lluis M`arquez</author>
<affiliation confidence="0.98718">ALT Research</affiliation>
<address confidence="0.543183">Qatar Computing Research Institute — HBKU, Qatar</address>
<abstract confidence="0.999477714285714">We present a novel framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypotheses and the reference, as well as between the two hypotheses. These compact representations are in turn based on word and sentence embeddings, which are learned using neural networks. The framework is flexible, allows for efficient learning and classification, and yields correlation with humans that rivals the state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joshua Albrecht</author>
<author>Rebecca Hwa</author>
</authors>
<title>Regression for machine translation evaluation at the sentence level.</title>
<date>2008</date>
<journal>Machine Translation,</journal>
<pages>22--1</pages>
<contexts>
<context position="2867" citStr="Albrecht and Hwa, 2008" startWordPosition="427" endWordPosition="430">r correlation with human judgments. To that end, quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In this pairwise setting, the challenge is to learn, from a pair of hypotheses, which are the features that help to discriminate the better from the worse translation. Although the pairwise </context>
</contexts>
<marker>Albrecht, Hwa, 2008</marker>
<rawString>Joshua Albrecht and Rebecca Hwa. 2008. Regression for machine translation evaluation at the sentence level. Machine Translation, 22(1-2):1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14,</booktitle>
<pages>238--247</pages>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="16536" citStr="Baroni et al., 2014" startWordPosition="2678" endWordPosition="2681">ic and semantic vector representations. Syntactic vectors. We generate a syntactic vector for each sentence using the Stanford neural parser (Socher et al., 2013a), which generates a 25- dimensional vector as a by-product of syntactic parsing using a recursive NN. Below we will refer to these vectors as SYNTAX25. Semantic vectors. We compose a semantic vector for a given sentence using the average of the embedding vectors for the words it contains (Mitchell and Lapata, 2010). We use pre-trained, fixedlength word embedding vectors produced by (i) GloVe (Pennington et al., 2014), (ii) COMPOSES (Baroni et al., 2014), and (iii) word2vec (Mikolov et al., 2013b). Our primary representation is based on 50- dimensional GloVe vectors, trained on Wikipedia 2014+Gigaword 5 (6B tokens), to which below we will refer as WIKI-GW25. Furthermore, we experiment with WIKIGW300, the 300-dimensional GloVe vectors trained on the same data, as well as with the CC300-42B and CC-300-840B, 300-dimensional GloVe vectors trained on 42B and on 840B tokens from Common Crawl. 808 We also experiment with the pre-trained, 300- dimensional word2vec embedding vectors, or WORD2VEC300, trained on 100B words from Google News. Finally, we </context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14, pages 238–247, Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Xavier Glorot</author>
</authors>
<title>Understanding the difficulty of training deep feedforward neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of AI &amp; Statistics</booktitle>
<volume>9</volume>
<pages>249--256</pages>
<location>Chia Laguna Resort, Sardinia, Italy.</location>
<contexts>
<context position="19037" citStr="Bengio and Glorot, 2010" startWordPosition="3090" endWordPosition="3094">lement early stopping. Early stopping: We train on WMT11 for up to 10,000 epochs, and we calculate Kendall’s T on the development set after each epoch. We then select the model that achieves the highest T on the validation set; in case of ties for the best T, we select the latest epoch that achieved the highest T. Network parameters: We train our neural network using SGD with adagrad, an initial learning rate of q = 0.01, mini-batches of size 30, and L2 regularization with a decay parameter A = 1e−4. We initialize the weights for our matrices by sampling from a uniform distribution following (Bengio and Glorot, 2010). We further set the size of each of our pairwise hidden layers H to four nodes, and we normalize the input data using minmax to map the feature values to the range [−1, 1]. 5 Experiments and Results The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and METEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; D</context>
</contexts>
<marker>Bengio, Glorot, 2010</marker>
<rawString>Yoshua Bengio and Xavier Glorot. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of AI &amp; Statistics 2010, volume 9, pages 249–256, Chia Laguna Resort, Sardinia, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="9745" citStr="Bengio et al., 2003" startWordPosition="1498" endWordPosition="1501"> and multilayer neural networks, which yields an efficient learning framework that works significantly better on the same datasets (although we are not using exactly the same information for learning). To the best of our knowledge, the application of structured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU math expression compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference, SciPy ’10,</booktitle>
<location>Austin, Texas.</location>
<contexts>
<context position="15315" citStr="Bergstra et al., 2010" startWordPosition="2485" endWordPosition="2488">ˆyno + (1 − yn) log (1 − ˆyno) n (4) 3Many of our input representations consist of word embeddings trained with neural networks that used tanh as an activation function. In the above formula, ˆyno = fn(t1, t2, r) is the activation at the output layer for the n-th data instance. It is also common to use a regularized cost function by adding a weight decay penalty (e.g., L2 or L1 regularization) and to perform maximum aposteriori (MAP) estimation of the parameters. We trained our network with stochastic gradient descent (SGD), mini-batches and adagrad updates (Duchi et al., 2011), using Theano (Bergstra et al., 2010). 4 Experimental Setup In this section, we describe the different aspects of our general experimental setup (we will discuss some extensions thereof in Section 6), starting with a description of the input representations we use to capture the syntactic and semantic characteristics of the two hypothesis translations and the corresponding reference, as well as the datasets used to evaluate the performance of our model. 4.1 Word Embedding Vectors Word embeddings play a crucial role in our model, since they allow us to model complex relations between the translations and the reference using syntac</context>
</contexts>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference, SciPy ’10, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Johannes Leveling</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
<author>Herve Saint-Amand</author>
</authors>
<title>Radu Soricut, Lucia Specia, and Aleˇs Tamchyna.</title>
<date>2014</date>
<journal>Findings of the</journal>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT ’14,</booktitle>
<pages>12--58</pages>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="7170" citStr="Bojar et al., 2014" startWordPosition="1095" endWordPosition="1098">ork Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to reproduce human judgments of MT quality. In particular, our setting is similar to that of Duh (2008), but differs from it both in terms of the feature representation and of the learning framework. For instance, we integrate several layers of linguistic information, while Duh (2008) only used lexical and POS matches as fea</context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, Saint-Amand, 2014</marker>
<rawString>Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT ’14, pages 12–58, Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(Meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07,</booktitle>
<pages>136--158</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2679" citStr="Callison-Burch et al., 2007" startWordPosition="398" endWordPosition="401"> (SMT) parameter tuning, for system comparison, and for assessing the progress during MT system development. The quality of automatic MT evaluation metrics is usually assessed by computing their correlation with human judgments. To that end, quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (Meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07, pages 136–158, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11,</booktitle>
<pages>22--64</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="17482" citStr="Callison-Burch et al., 2011" startWordPosition="2820" endWordPosition="2823"> as with the CC300-42B and CC-300-840B, 300-dimensional GloVe vectors trained on 42B and on 840B tokens from Common Crawl. 808 We also experiment with the pre-trained, 300- dimensional word2vec embedding vectors, or WORD2VEC300, trained on 100B words from Google News. Finally, we use COMPOSES400, the 400-dimensional COMPOSES vectors trained on 2.8 billion tokens from ukWaC, the English Wikipedia, and the British National Corpus. 4.2 Tuning and Evaluation Datasets We experiment with datasets of segment-level human rankings of system outputs from the WMT11, WMT12 and WMT13 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´aˇcek and Bojar, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s T. We report T for the individual languages as well as macro-averaged across all languages. Note that there were different versions of T at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at W</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, pages 22–64, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12,</booktitle>
<pages>10--51</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="5527" citStr="Callison-Burch et al., 2012" startWordPosition="843" endWordPosition="846">cal, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations and fed into the input layer, together with a set of individual real-valued features coming from simple pre-existing MT evaluation metrics. A hidden layer, motivated by our intuitions on the pairwise ranking problem, is used to capture interactions between the relevant input components. Finally, we present a task-oriented cost function, specifically tailored for this problem. Our evaluation results on the WMT12 metrics task benchmark datasets (Callison-Burch et al., 2012) show very high correlation with human judgments. These results clearly surpass (Guzm´an et al., 2014a) and are comparable to the best previously reported results for this dataset, achieved by DiscoTK (Joty et al., 2014), which is a much heavier combination-based metric. Another advantage of the proposed architecture is efficiency. Due to the vector-based compression of the linguistic structure and the relatively reduced size of the network, testing is fast, which would greatly facilitate the practical use of this approach in real MT evaluation and development. Finally, we empirically show tha</context>
<context position="17511" citStr="Callison-Burch et al., 2012" startWordPosition="2824" endWordPosition="2827">-300-840B, 300-dimensional GloVe vectors trained on 42B and on 840B tokens from Common Crawl. 808 We also experiment with the pre-trained, 300- dimensional word2vec embedding vectors, or WORD2VEC300, trained on 100B words from Google News. Finally, we use COMPOSES400, the 400-dimensional COMPOSES vectors trained on 2.8 billion tokens from ukWaC, the English Wikipedia, and the British National Corpus. 4.2 Tuning and Evaluation Datasets We experiment with datasets of segment-level human rankings of system outputs from the WMT11, WMT12 and WMT13 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´aˇcek and Bojar, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s T. We report T for the individual languages as well as macro-averaged across all languages. Note that there were different versions of T at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at WMT13 and further revised at W</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12, pages 10–51, Montr´eal, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, </marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.</rawString>
</citation>
<citation valid="true">
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<marker>2011</marker>
<rawString>2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Elisabet Comelles</author>
<author>Jes´us Gim´enez</author>
</authors>
<location>Llu´ıs M`arquez,</location>
<marker>Comelles, Gim´enez, </marker>
<rawString>Elisabet Comelles, Jes´us Gim´enez, Llu´ıs M`arquez,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Castell´on</author>
<author>Victoria Arranz</author>
</authors>
<title>Document-level automatic MT evaluation based on discourse representations.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10,</booktitle>
<pages>333--338</pages>
<location>Uppsala,</location>
<marker>Castell´on, Arranz, 2010</marker>
<rawString>Irene Castell´on, and Victoria Arranz. 2010. Document-level automatic MT evaluation based on discourse representations. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10, pages 333– 338, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11,</booktitle>
<pages>85--91</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="19662" citStr="Denkowski and Lavie, 2011" startWordPosition="3200" endWordPosition="3203">). We further set the size of each of our pairwise hidden layers H to four nodes, and we normalize the input data using minmax to map the feature values to the range [−1, 1]. 5 Experiments and Results The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and METEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; Denkowski and Lavie, 2011). We will refer to the set of these four metrics as 4METRICS. These metrics are not tuned and achieve Kendall’s T between 18.5 and 23.5. Section II of Table 1 shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX25 and WIKIGW25. These networks achieve modest T values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity against the reference. However, as will be discussed below, their con</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, pages 85–91, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14,</booktitle>
<pages>1370--1380</pages>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="9836" citStr="Devlin et al., 2014" startWordPosition="1515" endWordPosition="1518">gnificantly better on the same datasets (although we are not using exactly the same information for learning). To the best of our knowledge, the application of structured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entit</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14, pages 1370–1380, Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology Research, HLT ’02,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="19613" citStr="Doddington, 2002" startWordPosition="3194" endWordPosition="3195">tion following (Bengio and Glorot, 2010). We further set the size of each of our pairwise hidden layers H to four nodes, and we normalize the input data using minmax to map the feature values to the range [−1, 1]. 5 Experiments and Results The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and METEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; Denkowski and Lavie, 2011). We will refer to the set of these four metrics as 4METRICS. These metrics are not tuned and achieve Kendall’s T between 18.5 and 23.5. Section II of Table 1 shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX25 and WIKIGW25. These networks achieve modest T values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity against the referenc</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the Second International Conference on Human Language Technology Research, HLT ’02, pages 138–145,</rawString>
</citation>
<citation valid="false">
<authors>
<author>San Francisco</author>
</authors>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>California, USA.</location>
<marker>Francisco, </marker>
<rawString>San Francisco, California, USA. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="15277" citStr="Duchi et al., 2011" startWordPosition="2479" endWordPosition="2482">written as follows: �Jo = − yn log ˆyno + (1 − yn) log (1 − ˆyno) n (4) 3Many of our input representations consist of word embeddings trained with neural networks that used tanh as an activation function. In the above formula, ˆyno = fn(t1, t2, r) is the activation at the output layer for the n-th data instance. It is also common to use a regularized cost function by adding a weight decay penalty (e.g., L2 or L1 regularization) and to perform maximum aposteriori (MAP) estimation of the parameters. We trained our network with stochastic gradient descent (SGD), mini-batches and adagrad updates (Duchi et al., 2011), using Theano (Bergstra et al., 2010). 4 Experimental Setup In this section, we describe the different aspects of our general experimental setup (we will discuss some extensions thereof in Section 6), starting with a description of the input representations we use to capture the syntactic and semantic characteristics of the two hypothesis translations and the corresponding reference, as well as the datasets used to evaluate the performance of our model. 4.1 Word Embedding Vectors Word embeddings play a crucial role in our model, since they allow us to model complex relations between the trans</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
</authors>
<title>Ranking vs. regression in machine translation evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation, WMT ’08,</booktitle>
<pages>191--194</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="2891" citStr="Duh, 2008" startWordPosition="434" endWordPosition="435"> that end, quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In this pairwise setting, the challenge is to learn, from a pair of hypotheses, which are the features that help to discriminate the better from the worse translation. Although the pairwise setting does not produce</context>
<context position="7547" citStr="Duh (2008)" startWordPosition="1154" endWordPosition="1155">es et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to reproduce human judgments of MT quality. In particular, our setting is similar to that of Duh (2008), but differs from it both in terms of the feature representation and of the learning framework. For instance, we integrate several layers of linguistic information, while Duh (2008) only used lexical and POS matches as features. Secondly, we use information about both the reference and the two alternative translations simultaneously in a neural-based learning framework capable of modeling complex interactions between the features. Another related work is that of Kulesza and Shieber (2004), in which lexical and syntactic features, together with other metrics, e.g., BLEU and NIST, are used in a</context>
</contexts>
<marker>Duh, 2008</marker>
<rawString>Kevin Duh. 2008. Ranking vs. regression in machine translation evaluation. In Proceedings of the Third Workshop on Statistical Machine Translation, WMT ’08, pages 191–194, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Linguistic features for automatic evaluation of heterogenous MT systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07,</booktitle>
<pages>256--264</pages>
<location>Prague, Czech Republic.</location>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguistic features for automatic evaluation of heterogenous MT systems. In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07, pages 256–264, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Guzm´an</author>
<author>Shafiq Joty</author>
<author>Llu´ıs M`arquez</author>
<author>Alessandro Moschitti</author>
<author>Preslav Nakov</author>
<author>Massimo Nicosia</author>
</authors>
<title>Learning to differentiate better from worse translations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP ’14,</booktitle>
<pages>214--220</pages>
<location>Doha, Qatar.</location>
<marker>Guzm´an, Joty, M`arquez, Moschitti, Nakov, Nicosia, 2014</marker>
<rawString>Francisco Guzm´an, Shafiq Joty, Llu´ıs M`arquez, Alessandro Moschitti, Preslav Nakov, and Massimo Nicosia. 2014a. Learning to differentiate better from worse translations. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP ’14, pages 214–220, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Guzm´an</author>
<author>Shafiq Joty</author>
<author>Llu´ıs M`arquez</author>
<author>Preslav Nakov</author>
</authors>
<title>Using discourse structure improves machine translation evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14,</booktitle>
<pages>687--698</pages>
<location>Baltimore, Maryland, USA.</location>
<marker>Guzm´an, Joty, M`arquez, Nakov, 2014</marker>
<rawString>Francisco Guzm´an, Shafiq Joty, Llu´ıs M`arquez, and Preslav Nakov. 2014b. Using discourse structure improves machine translation evaluation. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14, pages 687– 698, Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Francisco Guzm´an</author>
<author>Llu´ıs M`arquez</author>
<author>Preslav Nakov</author>
</authors>
<title>DiscoTK: Using discourse structure for machine translation evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT ’14,</booktitle>
<pages>402--408</pages>
<location>Baltimore, Maryland, USA.</location>
<marker>Joty, Guzm´an, M`arquez, Nakov, 2014</marker>
<rawString>Shafiq Joty, Francisco Guzm´an, Llu´ıs M`arquez, and Preslav Nakov. 2014. DiscoTK: Using discourse structure for machine translation evaluation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT ’14, pages 402–408, Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Stuart M Shieber</author>
</authors>
<title>A learning approach to improving sentence-level MT evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation.</booktitle>
<contexts>
<context position="8041" citStr="Kulesza and Shieber (2004)" startWordPosition="1227" endWordPosition="1230">ased approaches for learning to reproduce human judgments of MT quality. In particular, our setting is similar to that of Duh (2008), but differs from it both in terms of the feature representation and of the learning framework. For instance, we integrate several layers of linguistic information, while Duh (2008) only used lexical and POS matches as features. Secondly, we use information about both the reference and the two alternative translations simultaneously in a neural-based learning framework capable of modeling complex interactions between the features. Another related work is that of Kulesza and Shieber (2004), in which lexical and syntactic features, together with other metrics, e.g., BLEU and NIST, are used in an SVM classifier to discriminate good from bad translations. However, their setting is not pairwise comparison, but a classification task to distinguish human- from machineproduced translations. Moreover, in their work, using syntactic features decreased the correlation with human judgments dramatically (although classification accuracy improved), while in our case the effect is positive. In our previous work (Guzm´an et al., 2014a), we introduced a learning framework for the pairwise sett</context>
</contexts>
<marker>Kulesza, Shieber, 2004</marker>
<rawString>Alex Kulesza and Stuart M. Shieber. 2004. A learning approach to improving sentence-level MT evaluation. In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael Denkowski</author>
</authors>
<title>The METEOR metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="6767" citStr="Lavie and Denkowski, 2009" startWordPosition="1026" endWordPosition="1029">yand semantically-oriented embeddings can be incorporated to produce sizeable and cumulative gains in performance over a strong combination of pre-existing MT evaluation measures (BLEU, NIST, METEOR, and TER). This is promising evidence towards our longer-term goal of defining a general platform for integrating varied linguistic information and for producing more informed MT evaluation measures. 2 Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and referenc</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael Denkowski. 2009. The METEOR metric for automatic evaluation of machine translation. Machine Translation, 23(2–3):105–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>25--32</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="6851" citStr="Liu and Gildea, 2005" startWordPosition="1040" endWordPosition="1043">ive gains in performance over a strong combination of pre-existing MT evaluation measures (BLEU, NIST, METEOR, and TER). This is promising evidence towards our longer-term goal of defining a general platform for integrating varied linguistic information and for producing more informed MT evaluation measures. 2 Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to repr</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 25–32, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Anand Karthik Tumuluru</author>
<author>Dekai Wu</author>
</authors>
<title>Fully automatic semantic MT evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12,</booktitle>
<pages>243--252</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="6909" citStr="Lo et al., 2012" startWordPosition="1050" endWordPosition="1053">ing MT evaluation measures (BLEU, NIST, METEOR, and TER). This is promising evidence towards our longer-term goal of defining a general platform for integrating varied linguistic information and for producing more informed MT evaluation measures. 2 Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to reproduce human judgments of MT quality. In particular, our se</context>
</contexts>
<marker>Lo, Tumuluru, Wu, 2012</marker>
<rawString>Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. 2012. Fully automatic semantic MT evaluation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12, pages 243–252, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>Results of the WMT13 metrics shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation, WMT ’13,</booktitle>
<pages>45--51</pages>
<location>Sofia, Bulgaria.</location>
<marker>Mach´acek, Bojar, 2013</marker>
<rawString>Matous&amp;quot; Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2013. Results of the WMT13 metrics shared task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, WMT ’13, pages 45–51, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>Results of the WMT14 metrics shared task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT ’14,</booktitle>
<pages>293--301</pages>
<location>Baltimore, Maryland, USA.</location>
<marker>Mach´acek, Bojar, 2014</marker>
<rawString>Matous&amp;quot; Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2014. Results of the WMT14 metrics shared task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT ’14, pages 293–301, Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>1045--1048</pages>
<location>Makuhari, Chiba, Japan.</location>
<marker>Mikolov, Karafi´at, Burget, Cernock´y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In 11th Annual Conference of the International Speech Communication Association, pages 1045– 1048, Makuhari, Chiba, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26, NIPS ’13,</booktitle>
<pages>3111--3119</pages>
<location>Lake Tahoe, California, USA.</location>
<contexts>
<context position="10177" citStr="Mikolov et al., 2013" startWordPosition="1573" endWordPosition="1576">nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes beyond the type of information considered as input, and resides on the way it is integrated to a neural network architecture that is inspired by our intuitions about MT evaluation. 3</context>
<context position="16578" citStr="Mikolov et al., 2013" startWordPosition="2685" endWordPosition="2688">ntactic vectors. We generate a syntactic vector for each sentence using the Stanford neural parser (Socher et al., 2013a), which generates a 25- dimensional vector as a by-product of syntactic parsing using a recursive NN. Below we will refer to these vectors as SYNTAX25. Semantic vectors. We compose a semantic vector for a given sentence using the average of the embedding vectors for the words it contains (Mitchell and Lapata, 2010). We use pre-trained, fixedlength word embedding vectors produced by (i) GloVe (Pennington et al., 2014), (ii) COMPOSES (Baroni et al., 2014), and (iii) word2vec (Mikolov et al., 2013b). Our primary representation is based on 50- dimensional GloVe vectors, trained on Wikipedia 2014+Gigaword 5 (6B tokens), to which below we will refer as WIKI-GW25. Furthermore, we experiment with WIKIGW300, the 300-dimensional GloVe vectors trained on the same data, as well as with the CC300-42B and CC-300-840B, 300-dimensional GloVe vectors trained on 42B and on 840B tokens from Common Crawl. 808 We also experiment with the pre-trained, 300- dimensional word2vec embedding vectors, or WORD2VEC300, trained on 100B words from Google News. Finally, we use COMPOSES400, the 400-dimensional COMPO</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013a. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, NIPS ’13, pages 3111–3119. Lake Tahoe, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’13,</booktitle>
<pages>746--751</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="10177" citStr="Mikolov et al., 2013" startWordPosition="1573" endWordPosition="1576">nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes beyond the type of information considered as input, and resides on the way it is integrated to a neural network architecture that is inspired by our intuitions about MT evaluation. 3</context>
<context position="16578" citStr="Mikolov et al., 2013" startWordPosition="2685" endWordPosition="2688">ntactic vectors. We generate a syntactic vector for each sentence using the Stanford neural parser (Socher et al., 2013a), which generates a 25- dimensional vector as a by-product of syntactic parsing using a recursive NN. Below we will refer to these vectors as SYNTAX25. Semantic vectors. We compose a semantic vector for a given sentence using the average of the embedding vectors for the words it contains (Mitchell and Lapata, 2010). We use pre-trained, fixedlength word embedding vectors produced by (i) GloVe (Pennington et al., 2014), (ii) COMPOSES (Baroni et al., 2014), and (iii) word2vec (Mikolov et al., 2013b). Our primary representation is based on 50- dimensional GloVe vectors, trained on Wikipedia 2014+Gigaword 5 (6B tokens), to which below we will refer as WIKI-GW25. Furthermore, we experiment with WIKIGW300, the 300-dimensional GloVe vectors trained on the same data, as well as with the CC300-42B and CC-300-840B, 300-dimensional GloVe vectors trained on 42B and on 840B tokens from Common Crawl. 808 We also experiment with the pre-trained, 300- dimensional word2vec embedding vectors, or WORD2VEC300, trained on 100B words from Google News. Finally, we use COMPOSES400, the 400-dimensional COMPO</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’13, pages 746–751, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="16395" citStr="Mitchell and Lapata, 2010" startWordPosition="2655" endWordPosition="2658">mbeddings play a crucial role in our model, since they allow us to model complex relations between the translations and the reference using syntactic and semantic vector representations. Syntactic vectors. We generate a syntactic vector for each sentence using the Stanford neural parser (Socher et al., 2013a), which generates a 25- dimensional vector as a by-product of syntactic parsing using a recursive NN. Below we will refer to these vectors as SYNTAX25. Semantic vectors. We compose a semantic vector for a given sentence using the average of the embedding vectors for the words it contains (Mitchell and Lapata, 2010). We use pre-trained, fixedlength word embedding vectors produced by (i) GloVe (Pennington et al., 2014), (ii) COMPOSES (Baroni et al., 2014), and (iii) word2vec (Mikolov et al., 2013b). Our primary representation is based on 50- dimensional GloVe vectors, trained on Wikipedia 2014+Gigaword 5 (6B tokens), to which below we will refer as WIKI-GW25. Furthermore, we experiment with WIKIGW300, the 300-dimensional GloVe vectors trained on the same data, as well as with the CC300-42B and CC-300-840B, 300-dimensional GloVe vectors trained on 42B and on 840B tokens from Common Crawl. 808 We also exper</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meting of the Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="1647" citStr="Papineni et al., 2002" startWordPosition="239" endWordPosition="242">ans that rivals the state of the art. 1 Introduction Automatic machine translation (MT) evaluation is a necessary step when developing or comparing MT systems. Reference-based MT evaluation, i.e., comparing the system output to one or more human reference translations, is the most common approach. Existing MT evaluation measures typically output an absolute quality score by computing the similarity between the machine and the human translations. In the simplest case, the similarity is computed by counting word n-gram matches between the translation and the reference. This is the case of BLEU (Papineni et al., 2002), which has been the standard for MT evaluation for years. Nonetheless, more recent evaluation measures take into account various aspects of linguistic similarity, and achieve better correlation with human judgments. Having absolute quality scores at the sentence level allows to rank alternative translations for a given source sentence. This is useful, for instance, for statistical machine translation (SMT) parameter tuning, for system comparison, and for assessing the progress during MT system development. The quality of automatic MT evaluation metrics is usually assessed by computing their c</context>
<context position="19595" citStr="Papineni et al., 2002" startWordPosition="3190" endWordPosition="3193">from a uniform distribution following (Bengio and Glorot, 2010). We further set the size of each of our pairwise hidden layers H to four nodes, and we normalize the input data using minmax to map the feature values to the range [−1, 1]. 5 Experiments and Results The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and METEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; Denkowski and Lavie, 2011). We will refer to the set of these four metrics as 4METRICS. These metrics are not tuned and achieve Kendall’s T between 18.5 and 23.5. Section II of Table 1 shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX25 and WIKIGW25. These networks achieve modest T values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity ag</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meting of the Association for Computational Linguistics, ACL ’02, pages 311–318, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’14,</booktitle>
<pages>1532--1543</pages>
<location>Doha, Qatar.</location>
<contexts>
<context position="10215" citStr="Pennington et al., 2014" startWordPosition="1579" endWordPosition="1583">h application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes beyond the type of information considered as input, and resides on the way it is integrated to a neural network architecture that is inspired by our intuitions about MT evaluation. 3 Neural Ranking Model Our motivation f</context>
<context position="16499" citStr="Pennington et al., 2014" startWordPosition="2671" endWordPosition="2674">nslations and the reference using syntactic and semantic vector representations. Syntactic vectors. We generate a syntactic vector for each sentence using the Stanford neural parser (Socher et al., 2013a), which generates a 25- dimensional vector as a by-product of syntactic parsing using a recursive NN. Below we will refer to these vectors as SYNTAX25. Semantic vectors. We compose a semantic vector for a given sentence using the average of the embedding vectors for the words it contains (Mitchell and Lapata, 2010). We use pre-trained, fixedlength word embedding vectors produced by (i) GloVe (Pennington et al., 2014), (ii) COMPOSES (Baroni et al., 2014), and (iii) word2vec (Mikolov et al., 2013b). Our primary representation is based on 50- dimensional GloVe vectors, trained on Wikipedia 2014+Gigaword 5 (6B tokens), to which below we will refer as WIKI-GW25. Furthermore, we experiment with WIKIGW300, the 300-dimensional GloVe vectors trained on the same data, as well as with the CC300-42B and CC-300-840B, 300-dimensional GloVe vectors trained on 42B and on 840B tokens from Common Crawl. 808 We also experiment with the pre-trained, 300- dimensional word2vec embedding vectors, or WORD2VEC300, trained on 100B</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’14, pages 1532–1543, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>Word error rates: Decomposition over POS classes and applications for error analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07,</booktitle>
<pages>48--55</pages>
<location>Prague, Czech Republic.</location>
<marker>Popovi´c, Ney, 2007</marker>
<rawString>Maja Popovi´c and Hermann Ney. 2007. Word error rates: Decomposition over POS classes and applications for error analysis. In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07, pages 48–55, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA ’06,</booktitle>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="19634" citStr="Snover et al., 2006" startWordPosition="3196" endWordPosition="3199">ngio and Glorot, 2010). We further set the size of each of our pairwise hidden layers H to four nodes, and we normalize the input data using minmax to map the feature values to the range [−1, 1]. 5 Experiments and Results The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and METEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; Denkowski and Lavie, 2011). We will refer to the set of these four metrics as 4METRICS. These metrics are not tuned and achieve Kendall’s T between 18.5 and 23.5. Section II of Table 1 shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX25 and WIKIGW25. These networks achieve modest T values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity against the reference. However, as will b</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA ’06, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL ’13,</booktitle>
<pages>455--465</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="9998" citStr="Socher et al., 2013" startWordPosition="1540" endWordPosition="1543">ructured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes bey</context>
<context position="16077" citStr="Socher et al., 2013" startWordPosition="2602" endWordPosition="2605">ereof in Section 6), starting with a description of the input representations we use to capture the syntactic and semantic characteristics of the two hypothesis translations and the corresponding reference, as well as the datasets used to evaluate the performance of our model. 4.1 Word Embedding Vectors Word embeddings play a crucial role in our model, since they allow us to model complex relations between the translations and the reference using syntactic and semantic vector representations. Syntactic vectors. We generate a syntactic vector for each sentence using the Stanford neural parser (Socher et al., 2013a), which generates a 25- dimensional vector as a by-product of syntactic parsing using a recursive NN. Below we will refer to these vectors as SYNTAX25. Semantic vectors. We compose a semantic vector for a given sentence using the average of the embedding vectors for the words it contains (Mitchell and Lapata, 2010). We use pre-trained, fixedlength word embedding vectors produced by (i) GloVe (Pennington et al., 2014), (ii) COMPOSES (Baroni et al., 2014), and (iii) word2vec (Mikolov et al., 2013b). Our primary representation is based on 50- dimensional GloVe vectors, trained on Wikipedia 2014</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013a. Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL ’13, pages 455–465, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP ’13,</booktitle>
<pages>1631--1642</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="9998" citStr="Socher et al., 2013" startWordPosition="1540" endWordPosition="1543">ructured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes bey</context>
<context position="16077" citStr="Socher et al., 2013" startWordPosition="2602" endWordPosition="2605">ereof in Section 6), starting with a description of the input representations we use to capture the syntactic and semantic characteristics of the two hypothesis translations and the corresponding reference, as well as the datasets used to evaluate the performance of our model. 4.1 Word Embedding Vectors Word embeddings play a crucial role in our model, since they allow us to model complex relations between the translations and the reference using syntactic and semantic vector representations. Syntactic vectors. We generate a syntactic vector for each sentence using the Stanford neural parser (Socher et al., 2013a), which generates a 25- dimensional vector as a by-product of syntactic parsing using a recursive NN. Below we will refer to these vectors as SYNTAX25. Semantic vectors. We compose a semantic vector for a given sentence using the average of the embedding vectors for the words it contains (Mitchell and Lapata, 2010). We use pre-trained, fixedlength word embedding vectors produced by (i) GloVe (Pennington et al., 2014), (ii) COMPOSES (Baroni et al., 2014), and (iii) word2vec (Mikolov et al., 2013b). Our primary representation is based on 50- dimensional GloVe vectors, trained on Wikipedia 2014</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP ’13, pages 1631–1642, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingyi Song</author>
<author>Trevor Cohn</author>
</authors>
<title>Regression and ranking based optimisation for sentence-level MT evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11,</booktitle>
<pages>123--129</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="2913" citStr="Song and Cohn, 2011" startWordPosition="436" endWordPosition="439">quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In this pairwise setting, the challenge is to learn, from a pair of hypotheses, which are the features that help to discriminate the better from the worse translation. Although the pairwise setting does not produce absolute quality scor</context>
<context position="25395" citStr="Song and Cohn, 2011" startWordPosition="4157" endWordPosition="4160"> can integrate syntactic and semantic vectors with scores from other metrics. In fact, ours is a more general framework, where one can integrate the components of a metric instead of its score, which could yield better learning. Below, we demonstrate this for BLEU. BLEU has different components: the n-gram precisions, the n-gram matches, the total number of n-grams (n=1,2,3,4), the lengths of the hypotheses and of the reference, the length ratio between them, and BLEU’s brevity penalty. We will refer to this decomposed BLEU as BLEUCOMP. Some of these features were previously used in SIMPBLEU (Song and Cohn, 2011). The results of using the components of BLEUCOMP as features are shown in Table 2. We see that using a single-layer neural network, which is equivalent to logistic regression, outperforms BLEU by more than +1 T points absolute. 810 Kendall’s T System Details BLEU no learning BLEUCOMP logistic regression BLEUCOMP+SYNTAX25 multi-layer NN BLEUCOMP+WIKI-GW25 multi-layer NN BLEUCOMP+SYNTAX25+WIKI-GW25 multi-layer NN BLEU+SYNTAX25+WIKI-GW25 multi-layer NN cz de es fr AVG 15.88 18.56 18.57 20.83 18.46 18.18 21.13 19.79 19.91 19.75 20.75 25.32 24.85 23.88 23.70 22.96 26.63 25.99 24.10 24.92 22.84 28.</context>
</contexts>
<marker>Song, Cohn, 2011</marker>
<rawString>Xingyi Song and Trevor Cohn. 2011. Regression and ranking based optimisation for sentence-level MT evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, pages 123–129, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the Neural Information Processing Systems, NIPS ’14,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="9861" citStr="Sutskever et al., 2014" startWordPosition="1519" endWordPosition="1522"> the same datasets (although we are not using exactly the same information for learning). To the best of our knowledge, the application of structured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semant</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of the Neural Information Processing Systems, NIPS ’14, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy Wong</author>
<author>Chunyu Kit</author>
</authors>
<title>Extending machine translation evaluation metrics with lexical cohesion to document level.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>1060--1068</pages>
<location>Jeju Island,</location>
<contexts>
<context position="6972" citStr="Wong and Kit, 2012" startWordPosition="1061" endWordPosition="1064">s is promising evidence towards our longer-term goal of defining a general platform for integrating varied linguistic information and for producing more informed MT evaluation measures. 2 Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to reproduce human judgments of MT quality. In particular, our setting is similar to that of Duh (2008), but differs from it bot</context>
</contexts>
<marker>Wong, Kit, 2012</marker>
<rawString>Billy Wong and Chunyu Kit. 2012. Extending machine translation evaluation metrics with lexical cohesion to document level. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 1060–1068, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Kristina Toutanova</author>
<author>John C Platt</author>
<author>Christopher Meek</author>
</authors>
<title>Learning discriminative projections for text similarity measures.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL ’11,</booktitle>
<pages>247--256</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="31551" citStr="Yih et al., 2011" startWordPosition="5215" endWordPosition="5218">ve learning, although it is eventually outperformed by the logistic cost. Our investigation revealed that this was due to a combination of slower convergence and poor initialization. Therefore, we further experimented with a setup where we first used the logistic cost to pretrain the neural network, and then we switched to the Kendall cost in order to perform some finer tuning. As we can see in Table 5 (last row), doing so yielded a sizable improvement over using the Kendall cost only; it also improved over using the logistic cost only. 4Other variations for ranking tasks are possible, e.g., (Yih et al., 2011). 7 Conclusions and Future Work We have presented a novel framework for learning a tunable MT evaluation metric in a pairwise ranking setting, given pre-existing pairwise human preference judgments. In particular, we used a neural network, where the input layer encodes lexical, syntactic and semantic information from the reference and the two translation hypotheses, which is efficiently compacted into relatively small embeddings. The network has a hidden layer, motivated by our intuition about the problem, which captures the interactions between the relevant input components. Unlike previously</context>
</contexts>
<marker>Yih, Toutanova, Platt, Meek, 2011</marker>
<rawString>Wen-tau Yih, Kristina Toutanova, John C. Platt, and Christopher Meek. 2011. Learning discriminative projections for text similarity measures. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL ’11, pages 247–256, Portland, Oregon, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>