<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.081104">
<title confidence="0.998488">
A Rhetorical Status Classifier for Legal Text Summarisation
</title>
<author confidence="0.998067">
Ben Hachey and Claire Grover
</author>
<affiliation confidence="0.99846">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.996175">
{bhachey,grover}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997356" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999799">
We describe a classifier which determines the
rhetorical status of sentences in texts from a corpus
of judgments of the UK House of Lords. Our sum-
marisation system is based on the work of Teufel
and Moens where sentences are classified for rhetor-
ical status to aid sentence selection. We experi-
ment with a variety of linguistic features with results
comparable to Teufel and Moens, thereby demon-
strating the feasibility of porting this kind of system
to a new domain.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985025316456">
Law reports form an interesting domain for auto-
matic summarisation. They are texts which record
the proceedings of a court and, due to the role that
precedents play in English law, easy access to them
is essential for a wide range of people. For this rea-
son, they are frequently manually summarised by
legal experts, with summaries varying according to
target audience (e.g. students, solicitors).
In the SUM project, we are exploring methods for
generating flexible summaries of legal documents,
taking as our point of departure the Teufel and
Moens (2002; 1999a; 1999b) approach to automatic
summarisation (henceforth T&amp;M). We have chosen
to work with law reports for three main reasons: (a)
the existence of manual summaries means that we
have evaluation material for the final summarisation
system; (b) the existence of differing target audi-
ences allows us to explore the issue of tailored sum-
maries; and (c) the texts have much in common with
the academic papers that T&amp;M worked with, while
remaining challengingly different in many respects.
Our general aims are comparable with those of the
SALOMON project (Moens et al., 1997), which also
deals with summarisation of legal texts, but our
choice of methodology is designed to test the porta-
bility of the T&amp;M approach to a new domain.
The T&amp;M approach is an instance of what Sp¨arck
Jones (1999) terms text extraction where a sum-
mary typically consists of sentences selected from
the source text, with some smoothing to increase
the coherence between the sentences. Since the aca-
demic texts they use are rather long and the aim
is to produce flexible summaries of varying length
and for various audiences, T&amp;M go beyond sim-
ple sentence selection and classify source sentences
according to their rhetorical status (e.g. a descrip-
tion of the main result, a criticism of someone else’s
work, etc.). With sentences classified in this man-
ner, different kinds of summaries can be generated.
Sentences can be reordered, since they have rhetor-
ical roles associated with them, or they can be sup-
pressed if a user is not interested in certain types of
rhetorical roles.
In the second stage of our project we will explore
techniques for sentence selection. Following the
T&amp;M methodology, we will annotate sentences in
the corpus for ‘relevance’. For our corpus we hope
to be able to compute relevance by using automatic
techniques to pair up sentences from manually cre-
ated abstracts with sentences in the source text. The
addition of this layer of annotation will provide the
training and testing material for sentence extraction,
with the rhetorical role labels helping to constrain
the type of summary generated.
In this paper we focus on our rhetorical status
classifier. This is a key part of the summarisation
process and our work can be thought of as a test of
portability of the T&amp;M approach to a new domain.
At the same time, our methods differ in important
respects from those of T&amp;M and in reporting our
work we will attempt to draw comparisons wher-
ever possible.
In Section 2 we describe the House of Lords cor-
pus we have gathered and annotated. We explain
the rhetorical role annotation scheme that we have
developed and contrast it with the T&amp;M scheme
for academic articles. We provide inter-annotation
agreement results for the annotation scheme. In
Section 2.3 we give an overview of the tools and
techniques we have used in the automatic linguis-
tic processing of the judgments. Section 3 describes
our sentence classifier. In Section 3.1 we review the
kinds of features that can be used by a classifier and
describe the set of features used in our experiments.
In Section 3.2 we present the results of experiments
with four classifiers and discuss the relative effec-
tiveness of the methods and the feature sets. Finally,
in Section 4 we draw some conclusions and outline
future work.
</bodyText>
<sectionHeader confidence="0.995112" genericHeader="method">
2 The HOLD Corpus
</sectionHeader>
<subsectionHeader confidence="0.977537">
2.1 Corpus Overview
</subsectionHeader>
<bodyText confidence="0.999976547619048">
The texts in our corpus are judgments of the House
of Lords1, which we refer to as HOLJ. These texts
contain a header providing structured information,
followed by a sequence of Law Lord’s judgments
consisting of free-running text. The structured part
of the document contains information such as the re-
spondent, appellant and the date of the hearing. The
decision is given in the opinions of the Law Lords,
at least one of which is a substantial speech. This
often starts with a statement of how the case came
before the court. Sometimes it will move to a reca-
pitulation of the facts, moving on to discuss one or
more points of law, and then offer a ruling.
We have gathered a corpus of 188 judgments
from the years 2001–2003 from the House of Lords
website. (For 153 of these, manually created sum-
maries are available2 and will be used for system
evaluation). The raw HTML documents are pro-
cessed through a sequence of modules which auto-
matically add layers of annotation. The first stage
converts the HTML to an XML format which we refer
to as HOLXML. In HOLXML, a House of Lords Judg-
ment is defined as a J element whose BODY element
is composed of a number of LORD elements (usu-
ally five). Each LORD element contains the judg-
ment of one individual lord and is composed of a
sequence of paragraphs (P elements) inherited from
the original HTML. The total number of words in
the BODY elements in the corpus is 2,887,037 and
the total number of sentences is 98,645. The aver-
age sentence length is approx. 29 words. A judg-
ment contains an average of 525 sentences while an
individual LORD speech contains an average of 105
sentences.
All annotation is computed automatically except
for manual annotation of sentences for their rhetori-
cal status. The automatic processing is divided into
two stages, tokenisation, which also includes part-
of-speech (POS) tagging and sentence boundary dis-
ambiguation, followed by linguistic annotation (de-
scribed in detail in Section 2.3 below). The human
annotation of rhetorical roles is performed on the
</bodyText>
<footnote confidence="0.999283666666667">
1http://www.parliament.uk/judicial_work/
judicial_work.cfm
2http://www.lawreports.co.uk/
</footnote>
<bodyText confidence="0.9999612">
documents after tokenisation has identified the sen-
tences. This annotation is work in progress and so
far we have 40 manually annotated documents. The
classifiers described in this paper have been trained
and evaluated on this manually annotated subset of
the corpus.
Our working subset of the corpus is similar in size
to the corpus reported in (Teufel and Moens, 2002):
the T&amp;M corpus consists of 80 conference articles
while ours consists of 40 HOLJ documents. The
T&amp;M corpus contains 12,188 sentences and 285,934
words while ours contains 10,169 sentences and
290,793 words. The experimental results reported
in this paper were obtained using 10-fold cross val-
idation over the 40 documents.
</bodyText>
<subsectionHeader confidence="0.999108">
2.2 Rhetorical Status Annotation
</subsectionHeader>
<bodyText confidence="0.999900483870968">
The rhetorical roles that it would be appropriate
to assign to sentences3 vary from domain to do-
main and reflect the argumentative structure of the
texts. Teufel and Moens (2002) describe a set of
labels which reflect regularities in the argumenta-
tive structure of research articles following from
the authors’ communicative goals. The scientific
article rhetorical roles include labels such as AIM,
which is assigned to sentences indicating the goals
of the paper, and BACKGROUND, which is assigned
to sentences describing generally accepted scientific
background.
For the legal domain, the communicative goal is
slightly different; the author’s primary communica-
tive goal is to convince his peers that his position is
legally sound, having considered the case with re-
gard to all relevant points of law. We have anal-
ysed the structure of typical documents in our do-
main and derived from this seven rhetorical role
categories, as illustrated in Table 1. The second
column shows the frequency of occurrence of each
label in the manually annotated subset of the cor-
pus. Apart from the OTHER category, the most in-
frequently assigned category is TEXTUAL while the
most frequent is BACKGROUND. The distribution
across categories is more uniform than that of the
T&amp;M labels: Teufel and Moens (2002) report that
their most frequent category (OWN) is assigned to
67% of sentences while three other labels (BASIS,
TEXTUAL and AIM) are each assigned to only 2%
of sentences.
</bodyText>
<footnote confidence="0.995782571428571">
3We take the sentence as the level of processing for rhetor-
ical role annotation. While clause-level annotation might al-
low more detailed discourse information, there are consider-
ably more clauses in the HOLD documents than sentences and
annotating at the clause level would be significantly more ex-
pensive. Moreover, clause boundary identification is less reli-
able than sentence boundary identification.
</footnote>
<figureCaption confidence="0.774431047619048">
Label Freq. Description
FACT 862 The sentence recounts the events or circumstances which gave rise
(8.5%) to legal proceedings.
E.g. On analysis the package was found to contain 152 milligrams
of heroin at 100% purity.
PROCEEDINGS 2434 The sentence describes legal proceedings taken in the lower courts.
(24%) E.g. After hearing much evidence, Her Honour Judge Sander, sitting at
Plymouth County Court, made findings offact on 1 November 2000.
BACKGROUND 2813 The sentence is a direct quotation or citation of source of law material.
(27.5%) E.g. Article 5 provides in paragraph 1 that a group ofproducers may
apply for registration ...
FRAMING 2309 The sentence is part of the law lord’s argumentation.
(23%) E.g. In my opinion, however, the present case cannot be brought within
the principle applied by the majority in the Wells case.
DISPOSAL 935 A sentence which either credits or discredits a claim or previous ruling.
(9%) E.g. I would allow the appeal and restore the order of the Divisional Court.
TEXTUAL 768 A sentence which has to do with the structure of the document or with
(7.5%) things unrelated to a case.
E.g. First, I should refer to the facts that have given rise to this litigation.
OTHER 48 A sentence which does not fit any of the above categories.
(0.5%) E.g. Here, as a matter of legal policy, the position seems to me straightforward.
</figureCaption>
<tableCaption confidence="0.986632">
Table 1: Rhetorical annotation scheme for legal judgments
</tableCaption>
<bodyText confidence="0.9614414">
The 40 judgments in our manually annotated sub-
set were annotated by two annotators using guide-
lines which were developed by one of the authors,
one of the annotators and a law professional. Eleven
files were doubly annotated in order to measure
inter-annotator agreement.4 We used the kappa co-
efficient of agreement as a measure of reliability.
This showed that the human annotators distinguish
the seven categories with a reproducibility of K=.83
(N=1,955, k=2; where K is the kappa co-efficient,
N is the number of sentences and k is the number
of annotators). This is slightly higher than that re-
ported by T&amp;M and above the .80 mark which Krip-
pendorf (1980) suggests is the cut-off for good reli-
ability.
In striving to achieve high quality summarisation,
it is tempting to consider using an annotation sys-
tem which reflects a more sophisticated analysis of
rhetorical roles. However, our kappa co-efficient is
currently on the bottom end of the range suggested
as indicating ‘good’ reliability. Therefore, we sus-
pect that the methods we are using may not scale to
more refined distinctions. The decisions we made
with the annotation scheme reflect a desire to bal-
ance quality of annotation against detail. Also, as
mentioned earlier, the cost of annotation for these
complex legal documents is not insignificant.
4The doubly annotated files were used only for computing
kappa. For the experiments, we trained and tested on the 40
annotated files produced by the main annotator.
</bodyText>
<subsectionHeader confidence="0.999467">
2.3 Linguistic Analysis
</subsectionHeader>
<bodyText confidence="0.999992233333333">
One of the aims of the SUM project is to create an
annotated corpus in the legal domain which will be
available to NLP researchers. With this aim in mind
we have used the HOLXML format for the corpus
and we encode all the results of linguistic process-
ing as XML annotations. Figure 1 shows the broad
details of the automatic processing that we perform,
with the processing divided into an initial tokenisa-
tion module and a later linguistic annotation mod-
ule. The architecture of our system is one where a
range of NLP tools is used in a modular, pipelined
way to add linguistic knowledge to the XML docu-
ment markup.
In the tokenisation module we convert from the
source HTML to HOLXML and then pass the data
through a sequence of calls to a variety of XML-
based tools from the LT TTT and LT XML toolsets
(Grover et al., 2000; Thompson et al., 1997). The
core program in our pipelines is the LT TTT pro-
gram fsgmatch, a general purpose transducer which
processes an input stream and adds annotations us-
ing rules provided in a hand-written grammar file.
The other main LT TTT program is ltpos, a statisti-
cal combined part-of-speech (POS) tagger and sen-
tence boundary disambiguation module (Mikheev,
1997). The first step in the tokenisation modules
uses fsgmatch to segment the contents of the para-
graphs into word tokens encoded in the XML as W
elements. Once the word tokens have been iden-
tified, the next step uses ltpos to mark up the sen-
</bodyText>
<figure confidence="0.980479833333333">
HTML
document
Automatically
annotated
HOLXML
document
</figure>
<figureCaption confidence="0.99998">
Figure 1: HOLJ processing stages
</figureCaption>
<bodyText confidence="0.999405325301205">
tences as SENT elements and to add part of speech
attributes to word tokens.
The motivation for the module that performs fur-
ther linguistic analysis is to compute information to
be used to provide features for the sentence classi-
fier. However, the information we compute is gen-
eral purpose, making the data useful for a range of
NLP research activities.
The first step in the linguistic analysis module
lemmatises the inflected words using Minnen et al.’s
(2000) morpha lemmatiser. This program is not
XML-aware so we use xmlperl (McKelvie, 1999) to
provide a wrapper so that it can be incorporated in
the XML pipeline. We use a similar mechanism for
the other non-XML components.
The next stage, described in Figure 1 as Named
Entity Recognition, is in fact a more complex layer-
ing of two kinds of named entity recognition. The
documents in our domain contain the standard kinds
of entities familiar from the MUC and CoNLL com-
petitions (Chinchor, 1998; Roth and van den Bosch,
2002; Daelemans and Osborne, 2003), such as per-
son, organisation, location and date. However, they
also contain entities which are are specific to the do-
main. Table 2 shows examples of the entities we
have marked up in the corpus (in our annotation
scheme these are noun groups (NG) with specific
type and subtype attributes). In the top two blocks
of the table are examples of domain-specific entities
such as courts, judges, acts and judgments, while in
the third block we show examples of non-domain-
specific entity types. We use different strategies
for the identification of the two classes of entities:
for the domain-specific ones we use hand-crafted
LT TTT rules, while for the non-domain-specific
ones we use the C&amp;C named entity tagger (Curran
and Clark, 2003) trained on the MUC7 data set. For
some entities, the two approaches provide compet-
ing analyses and in all cases the domain-specific la-
bel is to be preferred since it provides finer-grained
information. However, while the rule-based recog-
niser can operate incrementally over data which al-
ready contains some entity markup, the C&amp;C tagger
is trained to operate over unlabelled sentences. For
this reason we run the C&amp;C tagger first and encode
its results as attributes on the words. We then run the
domain-specific tagger, encoding its results as XML
elements enclosing the words, and finish with a sim-
ilar encoding of whichever C&amp;C entities can still be
realised in the unlabelled subparts of the sentences
(these are labelled as subtype=‘fromCC’).
Part of the rule-based entity recognition com-
ponent builds an ‘on-the-fly’ lexicon from names
found in the header of the document. Here the
names of the lords who are judging the case are
listed as well as the names of the respondent and
appellant. Since instances of these three entities oc-
curring in the body of the judgment are likely to be
distributed differently across sentences with differ-
ent rhetorical roles, it is useful to mark them up ex-
plicitly. We create an expanded lexicon from the
‘on-the-fly’ lexicon containing entries for consecu-
tive substrings of the original entry in order to per-
form a more flexible lexical look-up. Thus the entity
Commission is recognised as an appellant substring
entity in the document where Northern Ireland Hu-
man Rights Commission has been identified as an
appellant entity.
As future work, we plan to create a named entity
gold standard for the HOLJ domain and evaluate the
named entity recognition we are performing. For
now, we can use rhetorical status classification as a
task-based evaluation to estimate the utility of entity
recognition. The generic C&amp;C entity recognition to-
gether with the hand-crafted rules for the HOLJ do-
main prove to be the third most effective feature set
after the cue phrase and location features (Table 3).
The next stage in the linguistic analysis module
performs noun group and verb group chunking us-
ing fsgmatch with the specialised hand-written rule
sets which were the core part of LT CHUNK (Finch
and Mikheev, 1997). The noun group and verb
group mark-up plus POS tags provide the relevant
</bodyText>
<figure confidence="0.685860333333333">
POS Tagging
&amp; Sentence
Identification
Conversion to
HOLXML
TOKENISATION MODULE
LINGUISTIC ANALYSIS MODULE
Lemmati−
sation
Named
Entity
Recognition
Chunking
&amp; Clause
Identification
Verb &amp;
subject
features
</figure>
<table confidence="0.992143958333333">
Tokenisation
&lt;NG type=‘enamex-pers’ subtype=‘committee-lord’&gt; Lord Rodger of Earlsferry
Lord Hutton
&lt;NG type=‘caseent’ subtype=‘appellant’&gt; Northern Ireland Human Rights Commission
&lt;NG type=‘caseentsub’ subtype=‘appellant’&gt; Commission
&lt;NG type=‘caseent’ subtype=‘respondent’&gt; URATEMP VENTURES LIMITED
&lt;NG type=‘caseentsub’ subtype=‘respondent’&gt; Uratemp Ventures
&lt;NG type=‘enamex-pers’ subtype=‘judge’&gt; Collins J
Potter and Hale LJJ
&lt;NG type=‘enamex-org’ subtype=‘court’&gt; European Court ofJustice
Bristol County Court
&lt;NG type=‘legal-ent’ subtype=‘act’&gt; Value Added Tax Act 1994
Adoption Act 1976
&lt;NG type=‘legal-ent’ subtype=‘section’&gt; section 18(1)(a)
para 3.1
&lt;NG type=‘legal-ent’ subtype=‘judgment’&gt; Turner J [1996] STC 1469
Apple and Pear Development Council v Commissioners
of Customs and Excise (Case 102/86) [1988] STC 221
&lt;NG type=‘enamex-loc’ subtype=‘fromCC’&gt; Oakdene Road
Kuwait Airport
&lt;NG type=‘enamex-pers’ subtype=‘fromCC’&gt; Irfan Choudhry
John MacDermott
&lt;NG type=‘enamex-org’ subtype=‘fromCC’&gt; Powergen
Grayan Building Services Ltd
</table>
<tableCaption confidence="0.999489">
Table 2: Named entities in the corpus
</tableCaption>
<bodyText confidence="0.999897625">
features for the next processing step. In a previous
paper we showed that a range of information about
the main verb group of the sentence was likely to
provide important clues as to the rhetorical status
of the sentence (e.g. a present tense active verb will
correlate more highly with BACKGROUND or DIS-
POSAL sentences while a simple past tense sentence
is more likely to be found in a FACT sentence). In or-
der to find the main verb group of a sentence, how-
ever, we need to establish its clause structure. We
do this with a clause identifier (Hachey, 2002) built
using the CoNLL-2001 shared task data (Sang and
D´ejean, 2001). Clause identification is performed
in three steps. First, two maximum entropy classi-
fiers (Berger et al., 1996) are applied, where the first
predicts clause start labels and the second predicts
clause end labels. In the the third step clause seg-
mentation is inferred from the predicted starts and
ends using a maximum entropy model whose sole
purpose is to provide confidence values for poten-
tial clauses.
The final stages of linguistic processing use hand-
written LT TTT components to compute features of
verb and noun groups. For all verb groups, attributes
encoding tense, aspect, modality and negation are
added to the mark-up: for example, might not have
been brought is analysed as &lt;VG tense=‘pres’, as-
pect=‘perf’, voice=‘pass’, modaWyes’, neg=‘yes’&gt;.
In addition, subject noun groups are identified and
lemma information from the head noun of the sub-
ject and the head verb of the verb group are propa-
gated to the verb group attribute list.
</bodyText>
<sectionHeader confidence="0.987801" genericHeader="method">
3 The Sentence Classifier
</sectionHeader>
<subsectionHeader confidence="0.993334">
3.1 Feature Sets
</subsectionHeader>
<bodyText confidence="0.999866557894737">
The feature set described in Teufel and
Moens (2002) includes many of the features
which are typically used in sentence extraction
approaches to automatic summarisation as well as
certain other features developed specifically for
rhetorical role classification. Briefly, the T&amp;M
feature set includes such features as: location of a
sentence within the document and its subsections
and paragraphs; sentence length; whether the
sentence contains words from the title; whether it
contains significant terms as determined by tf*idf;
whether it contains a citation; linguistic features
of the first finite verb; and cue phrases (described
as meta-discourse features in Teufel and Moens,
2002). The features that we have been experiment-
ing with for the HOLJ domain are broadly similar
to those used by T&amp;M and are described in the
remainder of this section.
Location. For sentence extraction in the news do-
main, sentence location is an important feature and,
though it is less dominant for T&amp;M’s scientific arti-
cle domain, they did find it to be a useful indicator.
T&amp;M calculate the position of a sentence relative to
segments of the document as well as sections and
paragraphs. In our system, location is calculated
relative to the containing paragraph and LORD el-
ement and is encoded in six integer-valued features:
paragraph number after the beginning of the LORD
element, paragraph number before the end of the
LORD, sentence number after the beginning of the
LORD element, sentence number before the end of
the LORD, sentence number after the beginning of
the paragraph, and sentence number before the end
of the paragraph.
Thematic Words. This feature is intended to
capture the extent to which a sentence contains
terms which are significant, or thematic, in the doc-
ument. The thematic strength of a sentence is cal-
culated as a function of the tf*idf measure on words
(tf=‘term frequency’, idf=‘inverse document fre-
quency’): words which occur frequently in the doc-
ument but rarely in the corpus as a whole have a
high tf*idf score. The thematic words feature in
Teufel and Moens (2002) records whether a sen-
tence contains one or more of the 18 highest scoring
words. In our system we summarise the thematic
content of a sentence with a real-valued thematic
sentence feature, whose value is the average tf*idf
score of the sentence’s terms.
Sentence Length. In T&amp;M, this feature describes
sentences as short or long depending on whether
they are less than or more than twelve words in
length. We implement an integer-valued sentence
length feature which is a count of the number of to-
kens in the sentence.
Quotation. This feature, which does not have
a direct counterpart in T&amp;M, encodes the percent-
age of sentence tokens inside an in-line quote and
whether or not the sentence is inside a block quote.
Entities. T&amp;M do not incorporate full-scale
named entity recognition in their system, though
they do have a feature reflecting the presence or
absence of citations. We recognise a wide range
of named entities and generate binary-valued entity
type features which take the value 0 or 1.
Cue Phrases. The term ‘cue phrase’ covers the
kinds of stock phrases which are frequently good in-
dicators of rhetorical status (e.g. phrases such as The
aim of this study in the scientific article domain and
It seems to me that in the HOLJ domain). T&amp;M in-
vested a considerable amount of effort in compiling
lists of such cue phrases and building hand-crafted
lexicons where the cue phrases are assigned to one
of a number of fixed categories. A primary aim of
the current research is to investigate whether the ef-
fects of T&amp;M’s cue phrase features can be achieved
using automatically computable linguistic features.
If they can, then this helps to relieve the burden in-
volved in porting systems such as these to new do-
mains.
Our preliminary cue phrase feature set includes
syntactic features of the main verb (voice, tense, as-
pect, modality, negation), which we have shown to
be correlated with rhetorical status (Grover et al.,
2003). We also use features indicating sentence
initial part-of-speech and sentence initial word fea-
tures to roughly approximate formulaic expressions
which are sentence-level adverbial or prepositional
phrases. Subject features include the head lemma,
entity type, and entity subtype. These features ap-
proximate the hand-coded agent features of T&amp;M.
A main verb lemma feature simulates T&amp;M’s type
of action and a feature encoding the part-of-speech
after the main verb is meant to capture basic subcat-
egorisation information.
</bodyText>
<subsectionHeader confidence="0.999918">
3.2 Classifier Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999824933333333">
We ran experiments for four classifiers in the Weka
package:5 C4.5 decision trees, naive Bayes (NB)
incorporating nonparametric density estimation of
continuous variables, the Winnow6 algorithm for
mistake-driven learning of a linear separator, and
the sequential minimal optimization algorithm for
training support vector machines (SVM) using poly-
nomial kernels. Default parameter settings are used
for all algorithms.
Micro-averaged7 F-scores for each classifier are
presented in Table 3. The I columns contain in-
dividual scores for each feature type and the C
columns contain cumulative scores which incorpo-
rate features incrementally. C4.5 performs very well
(65.4%) with location features only, but is not able
to successfully incorporate other features for im-
proved performance. SVMs perform second best
(60.6%) with all features. NB is next (51.8%)
with all but thematic word features. Winnow has
the poorest performance with all features giving a
micro-averaged F-score of 41.4%.
For the most part, these scores are consider-
ably lower than T&amp;M, where they achieve a micro-
averaged F-score of 72. However, the picture is
slightly different when we consider the systems in
the context of their respective baselines. Teufel and
Moens (2002) report a macro-averaged F-score of
11 for always assigning the most frequent rhetori-
cal class, similar to the simple baseline they use in
earlier work. This score is 54 when micro-averaged
</bodyText>
<footnote confidence="0.994342714285714">
5http://www.cs.waikato.ac.nz/ml/weka/
6To evaluate Winnow, we use the Weka implementation of
the MDL discretization method which recursively splits inter-
vals at the cut-off point that minimizes entropy.
7Micro-averaging weights categories by their prior proba-
bility. By contrast, macro-averaging puts equal weight on each
class regardless of how sparsely populated it might be.
</footnote>
<table confidence="0.996934111111111">
C4.5 NB Winnow SVM
I C I C I C I C
Cue Phrases 47.8 47.8 39.6 39.6 31.1 31.1 52.1 52.1
Location 65.4 54.9 34.9 47.5 34.2 40.2 35.9 55.0
Entities 35.5 54.4 32.6 48.8 26.0 40.2 33.1 56.5
Sent. Length 27.2 55.1 20.0 49.1 27.0 40.4 12.0 56.8
Quotations 28.4 59.5 29.7 51.8 23.3 41.1 27.8 60.2
Them. Words 30.4 59.7 21.2 51.7 25.7 41.4 12.0 60.6
Baseline 12.0
</table>
<tableCaption confidence="0.999867">
Table 3: Micro-averaged F-score results for rhetorical classification
</tableCaption>
<bodyText confidence="0.998755490196079">
because of the skewed distribution of rhetorical cat-
egories (67% of sentences fall into the most frequent
category).$
With the more uniform distribution of rhetori-
cal categories in the HOLJ corpus, we get baseline
numbers of 6.2 (macro-averaged) and 12.0 (micro-
averaged). Thus, the actual per-sentence (micro-
averaged) F-score improvement is relatively high,
with our system achieving an improvement of be-
tween 29.4 and 53.4 points (to 41.4 and 65.4 respec-
tively for the optimal Winnow and C4.5) where the
T&amp;M system achieves an improvement of 18 points.
Like T&amp;M, our cue phrase features are the most
successful feature subset (excepting C4.5 decision
trees). We find these results very encouraging given
that we have not invested any time in developing the
hand-crafted cue phrase features that proved most
useful for T&amp;M, but rather have attempted to simu-
late these through fully automatic, largely domain-
independent linguistic information.
The fact that C4.5 decision trees outperform all
algorithms on location features led us to believe
we might be using an inferior representation for lo-
cation features. To test this, we encoded our lo-
cation features in the same way as T&amp;M. This
gave improved F scores for SVMs (41.5) and naive
Bayes (41.0) but worse scores for Winnow and dra-
matically worse scores for C4.5, indicating, as one
would expect, that the discrete T&amp;M location fea-
tures lose information present in our non-discretized
location features.
Maximum entropy (ME) modelling is another
machine learning method which allows the integra-
tion of diverse information sources. ME approaches
8T&amp;M use macro-averaging in order to down-weight their
largest category which was the least interesting for their sum-
maries. With our more uniform distribution of rhetorical cat-
egories and without any reason, as yet, to expect the number
of summary sentences coming from any one category to be far
out of proportion, we believe it better to report micro-averaged
scores. If we compare macro-averaged F scores, the SVM clas-
sifier achieves a score (52) a bit higher than T&amp;M (50). C4.5
outperforms both by a considerable amount, achieving a macro-
averaged F score of 58.
explicitly model the dependence between features
and have proven highly effective in similar natural
language tasks such as text categorisation, part-of-
speech tagging, and named entity recognition. The
next step in our research will be to experiment with
maximum entropy modelling and to compare it with
the techniques reported here.
</bodyText>
<sectionHeader confidence="0.999485" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999984309090909">
We have presented new work on the summarisation
of legal texts for which we are developing a new cor-
pus of UK House of Lords judgments with detailed
linguistic markup in addition to rhetorical status and
sentence extraction annotation.
We have effectively laid the ground work for de-
tailed experiments with robust and generic meth-
ods for capturing cue phrase information. This
is favourable as it can be automatically ported to
new text summarisation domains where the tools are
available for linguistic analysis, as opposed to rely-
ing on cue phrases which need to be hand-crafted
for each domain. Hand-crafted cue phrase lists are
necessarily more fragile and more susceptible to
over-fitting in large-scale applications.
Future experiments will use maximum entropy
modelling to incorporate our diverse range of sparse
linguistic and textual features. We plan to ex-
periment with maximum entropy for sentence-level
rhetorical status prediction in both standard classifi-
cation and sequence modelling frameworks.
We also intend to incorporate bootstrapped
named entity recognition systems. While generic
linguistic analysis tools (e.g. part-of-speech tag-
ging, chunking) are easy to come by in many lan-
guages, domain-specific named entity recognition
is not. We have invested a considerable amount
of time in writing named entity rules by hand for
the HOLJ domain. However, current research is in-
vestigating methods for bootstrapping named en-
tity systems from small amounts of seed data. Ef-
fective methods will make our linguistic features
fully domain-independent for domains and lan-
guages where linguistic analysis tools are available.
For future work, we are considering active learn-
ing and co-training. Active learning (Cohn et al.,
1994) would seem the appropriate starting point for
our task as we currently have no gold standard data
but we do have annotation resources. We may also
benefit from co-training (Blum and Mitchell, 1998)
and rule induction (Riloff and Jones, 1999) with the
seed data set from the initial annotation for active
learning.
We have also performed a preliminary experi-
ment with hypernym features for subject and verb
lemmas which should allow better generalisation
over cue phrase information. This is a rather noisy
feature as we are not performing word sense disam-
biguation, but adding all WordNet hypernyms of the
first three senses as features. Nevertheless, this has
shown an improvement with the naive Bayes classi-
fier from 24.75 for the cue phrase features sets (mi-
nus lemma features) to 27.45 when hypernyms are
included. Future work will further investigate hy-
pernym features.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999263666666667">
This work is supported by EPSRC grant GR/N35311.
The corpus annotation was carried out by Vasilis
Karaiskos and Hui-Mei Liao using the NITE XML
Toolkit (NXT), with assistance from Jonathan Kilgour in
configuring NXT. We are grateful to Stephen Clark and
three anonymous reviewers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.999543" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999848160919541">
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39–71.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the 11th Annual Conference on Compu-
tational Learning Theory, Madison, Wisconsin.
Nancy A. Chinchor. 1998. Proceedings of the Seventh
Message Understanding Conference (MUC-7). Fair-
fax, Virginia.
David Cohn, Les Atlas, and Richard Ladner. 1994. Im-
proving generalization with active learning. Machine
Learning, 15(2):201–221.
James R. Curran and Stephen Clark. 2003. Language
independent NER using a maximum entropy tagger.
In Proceedings of CoNLL-2003, pages 164–167. Ed-
monton, Canada.
Walter Daelemans and Miles Osborne. 2003. Proceed-
ings of the Seventh Workshop on Computational Lan-
guage Learning (CoNLL-2003). Edmonton, Canada.
Steve Finch and Andrei Mikheev. 1997. A workbench
for finding structure in texts. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing (ANLP-97). Washington D.C.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. LT TTT—a flexible tokenisation
tool. In LREC 2000—Proceedings of the 2nd Interna-
tional Conference on Language Resources and Evalu-
ation, pages 1147–1154.
Claire Grover, Ben Hachey, and Chris Korycinski. 2003.
Summarising legal texts: Sentential tense and argu-
mentative roles. In HLT-NAACL 2003 Workshop: Text
Summarization (DUC03), pages 33–40, Edmonton,
Canada.
Ben Hachey. 2002. Recognising clauses using symbolic
and machine learning approaches. Master’s thesis,
University of Edinburgh.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Sage Publications, Bev-
erly Hills, CA.
David McKelvie. 1999. Xmlperl 1.0.4 XML process-
ing software. http://www.cogsci.ed.ac.
uk/˜dmck/xmlperl.
Andrei Mikheev. 1997. Automatic rule induction for
unknown word guessing. Computational Linguistics,
23(3):405–423.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust, applied morphological generation. In Pro-
ceedings of 1st International Natural Language Gen-
eration Conference (INLG’2000).
Marie-Francine Moens, Caroline Uyttendaele, and Jos
Dumortier. 1997. Abstracting of legal cases: The SA-
LOMON experience. In Proceedings of the Sixth In-
ternational Conference on Artificial Intelligence and
Law, ACM, pages 114–122.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the 16th National Con-
ference on Artificial Intelligence (AIII-99), Orlando,
Florida.
Dan Roth and Antal van den Bosch. 2002. Proceedings
of the Sixth Workshop on Computational Language
Learning (CoNLL-2002). Taipei, Taiwan.
Erik Tjong Kim Sang and Herv´e D´ejean. 2001. Intro-
duction to the CoNLL-2001 shared task: clause iden-
tification. In Proceedings of the Fifth Workshop on
Computational Language Learning, pages 53–57.
Karen Sp¨arck-Jones. 1998. Automatic summarising:
factors and directions. In Advances in Automatic Text
Summarisation, pages 1–14. MIT Press.
Simone Teufel and Marc Moens. 1999a. Argumentative
classification of extracted sentences as a first step to-
wards flexible abstracting. In Advances in Automatic
Text Summarization, pages 137–175. MIT Press.
Simone Teufel and Marc Moens. 1999b. Discourse-level
argumentation in scientific articles: human and auto-
matic annotation. In Towards Standards and Tools for
Discourse Tagging, pages 84–93. ACL Workshop.
Simone Teufel and Marc Moens. 2002. Summaris-
ing scientific articles—experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409–445.
Henry Thompson, Richard Tobin, David McKelvie,
and Chris Brew. 1997. LT XML. software API
and XML toolkit. http://www.ltg.ed.ac.
uk/software/.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.979574">
<title confidence="0.999973">A Rhetorical Status Classifier for Legal Text Summarisation</title>
<author confidence="0.999335">Ben Hachey</author>
<author confidence="0.999335">Claire</author>
<affiliation confidence="0.99631">School of University of</affiliation>
<abstract confidence="0.998778727272727">We describe a classifier which determines the rhetorical status of sentences in texts from a corpus of judgments of the UK House of Lords. Our summarisation system is based on the work of Teufel and Moens where sentences are classified for rhetorical status to aid sentence selection. We experiment with a variety of linguistic features with results comparable to Teufel and Moens, thereby demonstrating the feasibility of porting this kind of system to a new domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="19816" citStr="Berger et al., 1996" startWordPosition="3223" endWordPosition="3226">oup of the sentence was likely to provide important clues as to the rhetorical status of the sentence (e.g. a present tense active verb will correlate more highly with BACKGROUND or DISPOSAL sentences while a simple past tense sentence is more likely to be found in a FACT sentence). In order to find the main verb group of a sentence, however, we need to establish its clause structure. We do this with a clause identifier (Hachey, 2002) built using the CoNLL-2001 shared task data (Sang and D´ejean, 2001). Clause identification is performed in three steps. First, two maximum entropy classifiers (Berger et al., 1996) are applied, where the first predicts clause start labels and the second predicts clause end labels. In the the third step clause segmentation is inferred from the predicted starts and ends using a maximum entropy model whose sole purpose is to provide confidence values for potential clauses. The final stages of linguistic processing use handwritten LT TTT components to compute features of verb and noun groups. For all verb groups, attributes encoding tense, aspect, modality and negation are added to the mark-up: for example, might not have been brought is analysed as &lt;VG tense=‘pres’, aspect</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory,</booktitle>
<location>Madison, Wisconsin.</location>
<contexts>
<context position="31981" citStr="Blum and Mitchell, 1998" startWordPosition="5164" endWordPosition="5167"> named entity rules by hand for the HOLJ domain. However, current research is investigating methods for bootstrapping named entity systems from small amounts of seed data. Effective methods will make our linguistic features fully domain-independent for domains and languages where linguistic analysis tools are available. For future work, we are considering active learning and co-training. Active learning (Cohn et al., 1994) would seem the appropriate starting point for our task as we currently have no gold standard data but we do have annotation resources. We may also benefit from co-training (Blum and Mitchell, 1998) and rule induction (Riloff and Jones, 1999) with the seed data set from the initial annotation for active learning. We have also performed a preliminary experiment with hypernym features for subject and verb lemmas which should allow better generalisation over cue phrase information. This is a rather noisy feature as we are not performing word sense disambiguation, but adding all WordNet hypernyms of the first three senses as features. Nevertheless, this has shown an improvement with the naive Bayes classifier from 24.75 for the cue phrase features sets (minus lemma features) to 27.45 when hy</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, Madison, Wisconsin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy A Chinchor</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<location>Fairfax, Virginia.</location>
<contexts>
<context position="14667" citStr="Chinchor, 1998" startWordPosition="2431" endWordPosition="2432">rch activities. The first step in the linguistic analysis module lemmatises the inflected words using Minnen et al.’s (2000) morpha lemmatiser. This program is not XML-aware so we use xmlperl (McKelvie, 1999) to provide a wrapper so that it can be incorporated in the XML pipeline. We use a similar mechanism for the other non-XML components. The next stage, described in Figure 1 as Named Entity Recognition, is in fact a more complex layering of two kinds of named entity recognition. The documents in our domain contain the standard kinds of entities familiar from the MUC and CoNLL competitions (Chinchor, 1998; Roth and van den Bosch, 2002; Daelemans and Osborne, 2003), such as person, organisation, location and date. However, they also contain entities which are are specific to the domain. Table 2 shows examples of the entities we have marked up in the corpus (in our annotation scheme these are noun groups (NG) with specific type and subtype attributes). In the top two blocks of the table are examples of domain-specific entities such as courts, judges, acts and judgments, while in the third block we show examples of non-domainspecific entity types. We use different strategies for the identificatio</context>
</contexts>
<marker>Chinchor, 1998</marker>
<rawString>Nancy A. Chinchor. 1998. Proceedings of the Seventh Message Understanding Conference (MUC-7). Fairfax, Virginia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Les Atlas</author>
<author>Richard Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="31783" citStr="Cohn et al., 1994" startWordPosition="5131" endWordPosition="5134"> tools (e.g. part-of-speech tagging, chunking) are easy to come by in many languages, domain-specific named entity recognition is not. We have invested a considerable amount of time in writing named entity rules by hand for the HOLJ domain. However, current research is investigating methods for bootstrapping named entity systems from small amounts of seed data. Effective methods will make our linguistic features fully domain-independent for domains and languages where linguistic analysis tools are available. For future work, we are considering active learning and co-training. Active learning (Cohn et al., 1994) would seem the appropriate starting point for our task as we currently have no gold standard data but we do have annotation resources. We may also benefit from co-training (Blum and Mitchell, 1998) and rule induction (Riloff and Jones, 1999) with the seed data set from the initial annotation for active learning. We have also performed a preliminary experiment with hypernym features for subject and verb lemmas which should allow better generalisation over cue phrase information. This is a rather noisy feature as we are not performing word sense disambiguation, but adding all WordNet hypernyms </context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>David Cohn, Les Atlas, and Richard Ladner. 1994. Improving generalization with active learning. Machine Learning, 15(2):201–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Language independent NER using a maximum entropy tagger.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<pages>164--167</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="15462" citStr="Curran and Clark, 2003" startWordPosition="2561" endWordPosition="2564"> the domain. Table 2 shows examples of the entities we have marked up in the corpus (in our annotation scheme these are noun groups (NG) with specific type and subtype attributes). In the top two blocks of the table are examples of domain-specific entities such as courts, judges, acts and judgments, while in the third block we show examples of non-domainspecific entity types. We use different strategies for the identification of the two classes of entities: for the domain-specific ones we use hand-crafted LT TTT rules, while for the non-domain-specific ones we use the C&amp;C named entity tagger (Curran and Clark, 2003) trained on the MUC7 data set. For some entities, the two approaches provide competing analyses and in all cases the domain-specific label is to be preferred since it provides finer-grained information. However, while the rule-based recogniser can operate incrementally over data which already contains some entity markup, the C&amp;C tagger is trained to operate over unlabelled sentences. For this reason we run the C&amp;C tagger first and encode its results as attributes on the words. We then run the domain-specific tagger, encoding its results as XML elements enclosing the words, and finish with a si</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Language independent NER using a maximum entropy tagger. In Proceedings of CoNLL-2003, pages 164–167. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Miles Osborne</author>
</authors>
<date>2003</date>
<booktitle>Proceedings of the Seventh Workshop on Computational Language Learning (CoNLL-2003).</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="14727" citStr="Daelemans and Osborne, 2003" startWordPosition="2439" endWordPosition="2442">ic analysis module lemmatises the inflected words using Minnen et al.’s (2000) morpha lemmatiser. This program is not XML-aware so we use xmlperl (McKelvie, 1999) to provide a wrapper so that it can be incorporated in the XML pipeline. We use a similar mechanism for the other non-XML components. The next stage, described in Figure 1 as Named Entity Recognition, is in fact a more complex layering of two kinds of named entity recognition. The documents in our domain contain the standard kinds of entities familiar from the MUC and CoNLL competitions (Chinchor, 1998; Roth and van den Bosch, 2002; Daelemans and Osborne, 2003), such as person, organisation, location and date. However, they also contain entities which are are specific to the domain. Table 2 shows examples of the entities we have marked up in the corpus (in our annotation scheme these are noun groups (NG) with specific type and subtype attributes). In the top two blocks of the table are examples of domain-specific entities such as courts, judges, acts and judgments, while in the third block we show examples of non-domainspecific entity types. We use different strategies for the identification of the two classes of entities: for the domain-specific on</context>
</contexts>
<marker>Daelemans, Osborne, 2003</marker>
<rawString>Walter Daelemans and Miles Osborne. 2003. Proceedings of the Seventh Workshop on Computational Language Learning (CoNLL-2003). Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Finch</author>
<author>Andrei Mikheev</author>
</authors>
<title>A workbench for finding structure in texts.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP-97). Washington D.C.</booktitle>
<contexts>
<context position="17712" citStr="Finch and Mikheev, 1997" startWordPosition="2933" endWordPosition="2936">andard for the HOLJ domain and evaluate the named entity recognition we are performing. For now, we can use rhetorical status classification as a task-based evaluation to estimate the utility of entity recognition. The generic C&amp;C entity recognition together with the hand-crafted rules for the HOLJ domain prove to be the third most effective feature set after the cue phrase and location features (Table 3). The next stage in the linguistic analysis module performs noun group and verb group chunking using fsgmatch with the specialised hand-written rule sets which were the core part of LT CHUNK (Finch and Mikheev, 1997). The noun group and verb group mark-up plus POS tags provide the relevant POS Tagging &amp; Sentence Identification Conversion to HOLXML TOKENISATION MODULE LINGUISTIC ANALYSIS MODULE Lemmati− sation Named Entity Recognition Chunking &amp; Clause Identification Verb &amp; subject features Tokenisation &lt;NG type=‘enamex-pers’ subtype=‘committee-lord’&gt; Lord Rodger of Earlsferry Lord Hutton &lt;NG type=‘caseent’ subtype=‘appellant’&gt; Northern Ireland Human Rights Commission &lt;NG type=‘caseentsub’ subtype=‘appellant’&gt; Commission &lt;NG type=‘caseent’ subtype=‘respondent’&gt; URATEMP VENTURES LIMITED &lt;NG type=‘caseentsub</context>
</contexts>
<marker>Finch, Mikheev, 1997</marker>
<rawString>Steve Finch and Andrei Mikheev. 1997. A workbench for finding structure in texts. In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP-97). Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Colin Matheson</author>
<author>Andrei Mikheev</author>
<author>Marc Moens</author>
</authors>
<title>LT TTT—a flexible tokenisation tool.</title>
<date>2000</date>
<booktitle>In LREC 2000—Proceedings of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<pages>1147--1154</pages>
<contexts>
<context position="13013" citStr="Grover et al., 2000" startWordPosition="2151" endWordPosition="2154">we encode all the results of linguistic processing as XML annotations. Figure 1 shows the broad details of the automatic processing that we perform, with the processing divided into an initial tokenisation module and a later linguistic annotation module. The architecture of our system is one where a range of NLP tools is used in a modular, pipelined way to add linguistic knowledge to the XML document markup. In the tokenisation module we convert from the source HTML to HOLXML and then pass the data through a sequence of calls to a variety of XMLbased tools from the LT TTT and LT XML toolsets (Grover et al., 2000; Thompson et al., 1997). The core program in our pipelines is the LT TTT program fsgmatch, a general purpose transducer which processes an input stream and adds annotations using rules provided in a hand-written grammar file. The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module (Mikheev, 1997). The first step in the tokenisation modules uses fsgmatch to segment the contents of the paragraphs into word tokens encoded in the XML as W elements. Once the word tokens have been identified, the next step uses ltpos to </context>
</contexts>
<marker>Grover, Matheson, Mikheev, Moens, 2000</marker>
<rawString>Claire Grover, Colin Matheson, Andrei Mikheev, and Marc Moens. 2000. LT TTT—a flexible tokenisation tool. In LREC 2000—Proceedings of the 2nd International Conference on Language Resources and Evaluation, pages 1147–1154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Ben Hachey</author>
<author>Chris Korycinski</author>
</authors>
<title>Summarising legal texts: Sentential tense and argumentative roles.</title>
<date>2003</date>
<booktitle>In HLT-NAACL 2003 Workshop: Text Summarization (DUC03),</booktitle>
<pages>33--40</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="24718" citStr="Grover et al., 2003" startWordPosition="4034" endWordPosition="4037">f such cue phrases and building hand-crafted lexicons where the cue phrases are assigned to one of a number of fixed categories. A primary aim of the current research is to investigate whether the effects of T&amp;M’s cue phrase features can be achieved using automatically computable linguistic features. If they can, then this helps to relieve the burden involved in porting systems such as these to new domains. Our preliminary cue phrase feature set includes syntactic features of the main verb (voice, tense, aspect, modality, negation), which we have shown to be correlated with rhetorical status (Grover et al., 2003). We also use features indicating sentence initial part-of-speech and sentence initial word features to roughly approximate formulaic expressions which are sentence-level adverbial or prepositional phrases. Subject features include the head lemma, entity type, and entity subtype. These features approximate the hand-coded agent features of T&amp;M. A main verb lemma feature simulates T&amp;M’s type of action and a feature encoding the part-of-speech after the main verb is meant to capture basic subcategorisation information. 3.2 Classifier Results and Discussion We ran experiments for four classifiers </context>
</contexts>
<marker>Grover, Hachey, Korycinski, 2003</marker>
<rawString>Claire Grover, Ben Hachey, and Chris Korycinski. 2003. Summarising legal texts: Sentential tense and argumentative roles. In HLT-NAACL 2003 Workshop: Text Summarization (DUC03), pages 33–40, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
</authors>
<title>Recognising clauses using symbolic and machine learning approaches. Master’s thesis,</title>
<date>2002</date>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="19634" citStr="Hachey, 2002" startWordPosition="3197" endWordPosition="3198">ding Services Ltd Table 2: Named entities in the corpus features for the next processing step. In a previous paper we showed that a range of information about the main verb group of the sentence was likely to provide important clues as to the rhetorical status of the sentence (e.g. a present tense active verb will correlate more highly with BACKGROUND or DISPOSAL sentences while a simple past tense sentence is more likely to be found in a FACT sentence). In order to find the main verb group of a sentence, however, we need to establish its clause structure. We do this with a clause identifier (Hachey, 2002) built using the CoNLL-2001 shared task data (Sang and D´ejean, 2001). Clause identification is performed in three steps. First, two maximum entropy classifiers (Berger et al., 1996) are applied, where the first predicts clause start labels and the second predicts clause end labels. In the the third step clause segmentation is inferred from the predicted starts and ends using a maximum entropy model whose sole purpose is to provide confidence values for potential clauses. The final stages of linguistic processing use handwritten LT TTT components to compute features of verb and noun groups. Fo</context>
</contexts>
<marker>Hachey, 2002</marker>
<rawString>Ben Hachey. 2002. Recognising clauses using symbolic and machine learning approaches. Master’s thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to its Methodology. Sage Publications,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to its Methodology. Sage Publications, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McKelvie</author>
</authors>
<date>1999</date>
<booktitle>Xmlperl 1.0.4 XML processing software. http://www.cogsci.ed.ac. uk/˜dmck/xmlperl.</booktitle>
<contexts>
<context position="14261" citStr="McKelvie, 1999" startWordPosition="2360" endWordPosition="2361">matically annotated HOLXML document Figure 1: HOLJ processing stages tences as SENT elements and to add part of speech attributes to word tokens. The motivation for the module that performs further linguistic analysis is to compute information to be used to provide features for the sentence classifier. However, the information we compute is general purpose, making the data useful for a range of NLP research activities. The first step in the linguistic analysis module lemmatises the inflected words using Minnen et al.’s (2000) morpha lemmatiser. This program is not XML-aware so we use xmlperl (McKelvie, 1999) to provide a wrapper so that it can be incorporated in the XML pipeline. We use a similar mechanism for the other non-XML components. The next stage, described in Figure 1 as Named Entity Recognition, is in fact a more complex layering of two kinds of named entity recognition. The documents in our domain contain the standard kinds of entities familiar from the MUC and CoNLL competitions (Chinchor, 1998; Roth and van den Bosch, 2002; Daelemans and Osborne, 2003), such as person, organisation, location and date. However, they also contain entities which are are specific to the domain. Table 2 s</context>
</contexts>
<marker>McKelvie, 1999</marker>
<rawString>David McKelvie. 1999. Xmlperl 1.0.4 XML processing software. http://www.cogsci.ed.ac. uk/˜dmck/xmlperl.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
</authors>
<title>Automatic rule induction for unknown word guessing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="13390" citStr="Mikheev, 1997" startWordPosition="2214" endWordPosition="2215">e to the XML document markup. In the tokenisation module we convert from the source HTML to HOLXML and then pass the data through a sequence of calls to a variety of XMLbased tools from the LT TTT and LT XML toolsets (Grover et al., 2000; Thompson et al., 1997). The core program in our pipelines is the LT TTT program fsgmatch, a general purpose transducer which processes an input stream and adds annotations using rules provided in a hand-written grammar file. The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module (Mikheev, 1997). The first step in the tokenisation modules uses fsgmatch to segment the contents of the paragraphs into word tokens encoded in the XML as W elements. Once the word tokens have been identified, the next step uses ltpos to mark up the senHTML document Automatically annotated HOLXML document Figure 1: HOLJ processing stages tences as SENT elements and to add part of speech attributes to word tokens. The motivation for the module that performs further linguistic analysis is to compute information to be used to provide features for the sentence classifier. However, the information we compute is g</context>
<context position="17712" citStr="Mikheev, 1997" startWordPosition="2935" endWordPosition="2936"> the HOLJ domain and evaluate the named entity recognition we are performing. For now, we can use rhetorical status classification as a task-based evaluation to estimate the utility of entity recognition. The generic C&amp;C entity recognition together with the hand-crafted rules for the HOLJ domain prove to be the third most effective feature set after the cue phrase and location features (Table 3). The next stage in the linguistic analysis module performs noun group and verb group chunking using fsgmatch with the specialised hand-written rule sets which were the core part of LT CHUNK (Finch and Mikheev, 1997). The noun group and verb group mark-up plus POS tags provide the relevant POS Tagging &amp; Sentence Identification Conversion to HOLXML TOKENISATION MODULE LINGUISTIC ANALYSIS MODULE Lemmati− sation Named Entity Recognition Chunking &amp; Clause Identification Verb &amp; subject features Tokenisation &lt;NG type=‘enamex-pers’ subtype=‘committee-lord’&gt; Lord Rodger of Earlsferry Lord Hutton &lt;NG type=‘caseent’ subtype=‘appellant’&gt; Northern Ireland Human Rights Commission &lt;NG type=‘caseentsub’ subtype=‘appellant’&gt; Commission &lt;NG type=‘caseent’ subtype=‘respondent’&gt; URATEMP VENTURES LIMITED &lt;NG type=‘caseentsub</context>
</contexts>
<marker>Mikheev, 1997</marker>
<rawString>Andrei Mikheev. 1997. Automatic rule induction for unknown word guessing. Computational Linguistics, 23(3):405–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Robust, applied morphological generation.</title>
<date>2000</date>
<booktitle>In Proceedings of 1st International Natural Language Generation Conference (INLG’2000).</booktitle>
<marker>Minnen, Carroll, Pearce, 2000</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2000. Robust, applied morphological generation. In Proceedings of 1st International Natural Language Generation Conference (INLG’2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Francine Moens</author>
<author>Caroline Uyttendaele</author>
<author>Jos Dumortier</author>
</authors>
<title>Abstracting of legal cases: The SALOMON experience.</title>
<date>1997</date>
<booktitle>In Proceedings of the Sixth International Conference on Artificial Intelligence and Law, ACM,</booktitle>
<pages>114--122</pages>
<contexts>
<context position="1791" citStr="Moens et al., 1997" startWordPosition="283" endWordPosition="286">t of departure the Teufel and Moens (2002; 1999a; 1999b) approach to automatic summarisation (henceforth T&amp;M). We have chosen to work with law reports for three main reasons: (a) the existence of manual summaries means that we have evaluation material for the final summarisation system; (b) the existence of differing target audiences allows us to explore the issue of tailored summaries; and (c) the texts have much in common with the academic papers that T&amp;M worked with, while remaining challengingly different in many respects. Our general aims are comparable with those of the SALOMON project (Moens et al., 1997), which also deals with summarisation of legal texts, but our choice of methodology is designed to test the portability of the T&amp;M approach to a new domain. The T&amp;M approach is an instance of what Sp¨arck Jones (1999) terms text extraction where a summary typically consists of sentences selected from the source text, with some smoothing to increase the coherence between the sentences. Since the academic texts they use are rather long and the aim is to produce flexible summaries of varying length and for various audiences, T&amp;M go beyond simple sentence selection and classify source sentences ac</context>
</contexts>
<marker>Moens, Uyttendaele, Dumortier, 1997</marker>
<rawString>Marie-Francine Moens, Caroline Uyttendaele, and Jos Dumortier. 1997. Abstracting of legal cases: The SALOMON experience. In Proceedings of the Sixth International Conference on Artificial Intelligence and Law, ACM, pages 114–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference on Artificial Intelligence (AIII-99),</booktitle>
<location>Orlando, Florida.</location>
<contexts>
<context position="32025" citStr="Riloff and Jones, 1999" startWordPosition="5171" endWordPosition="5174">in. However, current research is investigating methods for bootstrapping named entity systems from small amounts of seed data. Effective methods will make our linguistic features fully domain-independent for domains and languages where linguistic analysis tools are available. For future work, we are considering active learning and co-training. Active learning (Cohn et al., 1994) would seem the appropriate starting point for our task as we currently have no gold standard data but we do have annotation resources. We may also benefit from co-training (Blum and Mitchell, 1998) and rule induction (Riloff and Jones, 1999) with the seed data set from the initial annotation for active learning. We have also performed a preliminary experiment with hypernym features for subject and verb lemmas which should allow better generalisation over cue phrase information. This is a rather noisy feature as we are not performing word sense disambiguation, but adding all WordNet hypernyms of the first three senses as features. Nevertheless, this has shown an improvement with the naive Bayes classifier from 24.75 for the cue phrase features sets (minus lemma features) to 27.45 when hypernyms are included. Future work will furth</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the 16th National Conference on Artificial Intelligence (AIII-99), Orlando, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Antal van den Bosch</author>
</authors>
<date>2002</date>
<booktitle>Proceedings of the Sixth Workshop on Computational Language Learning (CoNLL-2002).</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Roth, van den Bosch, 2002</marker>
<rawString>Dan Roth and Antal van den Bosch. 2002. Proceedings of the Sixth Workshop on Computational Language Learning (CoNLL-2002). Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
<author>Herv´e D´ejean</author>
</authors>
<title>Introduction to the CoNLL-2001 shared task: clause identification.</title>
<date>2001</date>
<booktitle>In Proceedings of the Fifth Workshop on Computational Language Learning,</booktitle>
<pages>53--57</pages>
<marker>Sang, D´ejean, 2001</marker>
<rawString>Erik Tjong Kim Sang and Herv´e D´ejean. 2001. Introduction to the CoNLL-2001 shared task: clause identification. In Proceedings of the Fifth Workshop on Computational Language Learning, pages 53–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sp¨arck-Jones</author>
</authors>
<title>Automatic summarising: factors and directions.</title>
<date>1998</date>
<booktitle>In Advances in Automatic Text Summarisation,</booktitle>
<pages>1--14</pages>
<publisher>MIT Press.</publisher>
<marker>Sp¨arck-Jones, 1998</marker>
<rawString>Karen Sp¨arck-Jones. 1998. Automatic summarising: factors and directions. In Advances in Automatic Text Summarisation, pages 1–14. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Argumentative classification of extracted sentences as a first step towards flexible abstracting.</title>
<date>1999</date>
<booktitle>In Advances in Automatic Text Summarization,</booktitle>
<pages>137--175</pages>
<publisher>MIT Press.</publisher>
<marker>Teufel, Moens, 1999</marker>
<rawString>Simone Teufel and Marc Moens. 1999a. Argumentative classification of extracted sentences as a first step towards flexible abstracting. In Advances in Automatic Text Summarization, pages 137–175. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Discourse-level argumentation in scientific articles: human and automatic annotation.</title>
<date>1999</date>
<booktitle>In Towards Standards and Tools for Discourse Tagging,</booktitle>
<pages>84--93</pages>
<publisher>ACL Workshop.</publisher>
<marker>Teufel, Moens, 1999</marker>
<rawString>Simone Teufel and Marc Moens. 1999b. Discourse-level argumentation in scientific articles: human and automatic annotation. In Towards Standards and Tools for Discourse Tagging, pages 84–93. ACL Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarising scientific articles—experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="1213" citStr="Teufel and Moens (2002" startWordPosition="190" endWordPosition="193">ty of porting this kind of system to a new domain. 1 Introduction Law reports form an interesting domain for automatic summarisation. They are texts which record the proceedings of a court and, due to the role that precedents play in English law, easy access to them is essential for a wide range of people. For this reason, they are frequently manually summarised by legal experts, with summaries varying according to target audience (e.g. students, solicitors). In the SUM project, we are exploring methods for generating flexible summaries of legal documents, taking as our point of departure the Teufel and Moens (2002; 1999a; 1999b) approach to automatic summarisation (henceforth T&amp;M). We have chosen to work with law reports for three main reasons: (a) the existence of manual summaries means that we have evaluation material for the final summarisation system; (b) the existence of differing target audiences allows us to explore the issue of tailored summaries; and (c) the texts have much in common with the academic papers that T&amp;M worked with, while remaining challengingly different in many respects. Our general aims are comparable with those of the SALOMON project (Moens et al., 1997), which also deals wit</context>
<context position="7038" citStr="Teufel and Moens, 2002" startWordPosition="1165" endWordPosition="1168">boundary disambiguation, followed by linguistic annotation (described in detail in Section 2.3 below). The human annotation of rhetorical roles is performed on the 1http://www.parliament.uk/judicial_work/ judicial_work.cfm 2http://www.lawreports.co.uk/ documents after tokenisation has identified the sentences. This annotation is work in progress and so far we have 40 manually annotated documents. The classifiers described in this paper have been trained and evaluated on this manually annotated subset of the corpus. Our working subset of the corpus is similar in size to the corpus reported in (Teufel and Moens, 2002): the T&amp;M corpus consists of 80 conference articles while ours consists of 40 HOLJ documents. The T&amp;M corpus contains 12,188 sentences and 285,934 words while ours contains 10,169 sentences and 290,793 words. The experimental results reported in this paper were obtained using 10-fold cross validation over the 40 documents. 2.2 Rhetorical Status Annotation The rhetorical roles that it would be appropriate to assign to sentences3 vary from domain to domain and reflect the argumentative structure of the texts. Teufel and Moens (2002) describe a set of labels which reflect regularities in the argu</context>
<context position="8688" citStr="Teufel and Moens (2002)" startWordPosition="1426" endWordPosition="1429">goal is to convince his peers that his position is legally sound, having considered the case with regard to all relevant points of law. We have analysed the structure of typical documents in our domain and derived from this seven rhetorical role categories, as illustrated in Table 1. The second column shows the frequency of occurrence of each label in the manually annotated subset of the corpus. Apart from the OTHER category, the most infrequently assigned category is TEXTUAL while the most frequent is BACKGROUND. The distribution across categories is more uniform than that of the T&amp;M labels: Teufel and Moens (2002) report that their most frequent category (OWN) is assigned to 67% of sentences while three other labels (BASIS, TEXTUAL and AIM) are each assigned to only 2% of sentences. 3We take the sentence as the level of processing for rhetorical role annotation. While clause-level annotation might allow more detailed discourse information, there are considerably more clauses in the HOLD documents than sentences and annotating at the clause level would be significantly more expensive. Moreover, clause boundary identification is less reliable than sentence boundary identification. Label Freq. Description</context>
<context position="20746" citStr="Teufel and Moens (2002)" startWordPosition="3376" endWordPosition="3379">ages of linguistic processing use handwritten LT TTT components to compute features of verb and noun groups. For all verb groups, attributes encoding tense, aspect, modality and negation are added to the mark-up: for example, might not have been brought is analysed as &lt;VG tense=‘pres’, aspect=‘perf’, voice=‘pass’, modaWyes’, neg=‘yes’&gt;. In addition, subject noun groups are identified and lemma information from the head noun of the subject and the head verb of the verb group are propagated to the verb group attribute list. 3 The Sentence Classifier 3.1 Feature Sets The feature set described in Teufel and Moens (2002) includes many of the features which are typically used in sentence extraction approaches to automatic summarisation as well as certain other features developed specifically for rhetorical role classification. Briefly, the T&amp;M feature set includes such features as: location of a sentence within the document and its subsections and paragraphs; sentence length; whether the sentence contains words from the title; whether it contains significant terms as determined by tf*idf; whether it contains a citation; linguistic features of the first finite verb; and cue phrases (described as meta-discourse </context>
<context position="22781" citStr="Teufel and Moens (2002)" startWordPosition="3705" endWordPosition="3708">element, sentence number before the end of the LORD, sentence number after the beginning of the paragraph, and sentence number before the end of the paragraph. Thematic Words. This feature is intended to capture the extent to which a sentence contains terms which are significant, or thematic, in the document. The thematic strength of a sentence is calculated as a function of the tf*idf measure on words (tf=‘term frequency’, idf=‘inverse document frequency’): words which occur frequently in the document but rarely in the corpus as a whole have a high tf*idf score. The thematic words feature in Teufel and Moens (2002) records whether a sentence contains one or more of the 18 highest scoring words. In our system we summarise the thematic content of a sentence with a real-valued thematic sentence feature, whose value is the average tf*idf score of the sentence’s terms. Sentence Length. In T&amp;M, this feature describes sentences as short or long depending on whether they are less than or more than twelve words in length. We implement an integer-valued sentence length feature which is a count of the number of tokens in the sentence. Quotation. This feature, which does not have a direct counterpart in T&amp;M, encode</context>
<context position="26522" citStr="Teufel and Moens (2002)" startWordPosition="4301" endWordPosition="4304">es incrementally. C4.5 performs very well (65.4%) with location features only, but is not able to successfully incorporate other features for improved performance. SVMs perform second best (60.6%) with all features. NB is next (51.8%) with all but thematic word features. Winnow has the poorest performance with all features giving a micro-averaged F-score of 41.4%. For the most part, these scores are considerably lower than T&amp;M, where they achieve a microaveraged F-score of 72. However, the picture is slightly different when we consider the systems in the context of their respective baselines. Teufel and Moens (2002) report a macro-averaged F-score of 11 for always assigning the most frequent rhetorical class, similar to the simple baseline they use in earlier work. This score is 54 when micro-averaged 5http://www.cs.waikato.ac.nz/ml/weka/ 6To evaluate Winnow, we use the Weka implementation of the MDL discretization method which recursively splits intervals at the cut-off point that minimizes entropy. 7Micro-averaging weights categories by their prior probability. By contrast, macro-averaging puts equal weight on each class regardless of how sparsely populated it might be. C4.5 NB Winnow SVM I C I C I C I</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarising scientific articles—experiments with relevance and rhetorical status. Computational Linguistics, 28(4):409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Thompson</author>
<author>Richard Tobin</author>
<author>David McKelvie</author>
<author>Chris Brew</author>
</authors>
<date>1997</date>
<booktitle>LT XML. software API and XML toolkit. http://www.ltg.ed.ac. uk/software/.</booktitle>
<contexts>
<context position="13037" citStr="Thompson et al., 1997" startWordPosition="2155" endWordPosition="2158">ults of linguistic processing as XML annotations. Figure 1 shows the broad details of the automatic processing that we perform, with the processing divided into an initial tokenisation module and a later linguistic annotation module. The architecture of our system is one where a range of NLP tools is used in a modular, pipelined way to add linguistic knowledge to the XML document markup. In the tokenisation module we convert from the source HTML to HOLXML and then pass the data through a sequence of calls to a variety of XMLbased tools from the LT TTT and LT XML toolsets (Grover et al., 2000; Thompson et al., 1997). The core program in our pipelines is the LT TTT program fsgmatch, a general purpose transducer which processes an input stream and adds annotations using rules provided in a hand-written grammar file. The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module (Mikheev, 1997). The first step in the tokenisation modules uses fsgmatch to segment the contents of the paragraphs into word tokens encoded in the XML as W elements. Once the word tokens have been identified, the next step uses ltpos to mark up the senHTML docu</context>
</contexts>
<marker>Thompson, Tobin, McKelvie, Brew, 1997</marker>
<rawString>Henry Thompson, Richard Tobin, David McKelvie, and Chris Brew. 1997. LT XML. software API and XML toolkit. http://www.ltg.ed.ac. uk/software/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>