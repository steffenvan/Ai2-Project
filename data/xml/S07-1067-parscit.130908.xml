<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001163">
<title confidence="0.903654">
SWAT-MP: The SemEval-2007 Systems for Task 5 and Task 14
</title>
<author confidence="0.998971">
Phil Katz, Matthew Singleton, Richard Wicentowski
</author>
<affiliation confidence="0.943834">
Department of Computer Science
Swarthmore College
</affiliation>
<address confidence="0.793051">
Swarthmore, PA
</address>
<email confidence="0.999607">
{katz,msingle1,richardw}@cs.swarthmore.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930533333333">
In this paper, we describe our two SemEval-
2007 entries. Our first entry, for Task 5:
Multilingual Chinese-English Lexical Sam-
ple Task, is a supervised system that decides
the most appropriate English translation of
a Chinese target word. This system uses a
combination of Naive Bayes, nearest neigh-
bor cosine, decision lists, and latent seman-
tic analysis. Our second entry, for Task 14:
Affective Text, is a supervised system that
annotates headlines using a predefined list of
emotions. This system uses synonym expan-
sion and matches lemmatized unigrams in
the test headlines against a corpus of hand-
annotated headlines.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999914285714286">
This paper describes our two entries in SemEval-
2007. The first entry, a supervised system used in the
Multilingual Chinese-English Lexical Sample task
(Task 5), is an extension of the system described in
(Wicentowski et al., 2004). We implement five dif-
ferent classifiers: a Naive Bayes classifier, a decision
list classifier, two different nearest neighbor cosine
classifiers, and a classifier based on Latent Seman-
tic Analysis. Section 2.2 describes each of the in-
dividual classifiers, Section 2.3 describes our clas-
sifier combination system, and Section 2.4 presents
our results.
The second entry, a supervised system used in the
Affective Text task (Task 14), uses a corpus of head-
lines hand-annotated by non-experts. It also uses an
online thesaurus to match synonyms and antonyms
of the sense labels (Thesaurus.com, 2007). Section
3.1 describes the creation of the annotated training
corpus, Section 3.2 describes our method for assign-
ing scores to the headlines, and Section 3.3 presents
our results.
</bodyText>
<sectionHeader confidence="0.968592" genericHeader="method">
2 Task 5: Multilingual Chinese-English LS
</sectionHeader>
<bodyText confidence="0.999959444444445">
This task presents a single Chinese word in context
which must be disambiguated. Rather than asking
participants to provide a sense label corresponding
to a pre-defined sense inventory, the goal here is to
label each ambiguous word with its correct English
translation. Since the task is quite similar to more
traditional lexical sample tasks, we extend an ap-
proach used successfully in multiple Senseval-3 lex-
ical sample tasks (Wicentowski et al., 2004).
</bodyText>
<subsectionHeader confidence="0.826876">
2.1 Features
</subsectionHeader>
<bodyText confidence="0.99986">
Each of our classifiers uses the same set of context
features, taken directly from the data provided by the
task organizers. The features we used included:
</bodyText>
<listItem confidence="0.99643925">
• Bag-of-words (unigrams)
• Bigrams and trigrams around the target word
• Weighted unigrams surrounding the target
word
</listItem>
<bodyText confidence="0.999876">
The weighted unigram features increased the fre-
quencies of the ten words before and after the tar-
get word by inserting them multiple times into the
bag-of-words.
</bodyText>
<page confidence="0.984061">
308
</page>
<bodyText confidence="0.962291375">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 308–313,
Prague, June 2007. c�2007 Association for Computational Linguistics
Many words in the Chinese data were broken up
into “subwords”: since we were unsure how to han-
dle these and since their appearance seemed incon-
sistent, we decided to simply treat each subword as a
word for the purposes of creating bigrams, trigrams,
and weighted unigrams.
</bodyText>
<subsectionHeader confidence="0.997464">
2.2 Classifiers
</subsectionHeader>
<bodyText confidence="0.999933428571429">
Our system consists of five unique classifiers. Three
of the classifiers were selected by our combination
system, while the other two were found to be detri-
mental to its performance. We describe the con-
tributing classifiers first. Table 1 shows the results
of each classifier, as well as our classifier combina-
tion system.
</bodyText>
<subsectionHeader confidence="0.585275">
2.2.1 Naive Bayes
</subsectionHeader>
<bodyText confidence="0.999577666666667">
The Naive Bayes classifier is based on Bayes’ the-
orem, which allows us to define the similarity be-
tween an instance, I, and a sense class, 5j, as:
</bodyText>
<equation confidence="0.926085">
5im(I, 5j) = Pr(I, 5j) = Pr(5j) * Pr(I|5j)
</equation>
<bodyText confidence="0.9993145">
We then choose the sense with the maximum sim-
ilarity to the test instance.
</bodyText>
<subsectionHeader confidence="0.932484">
Additive Smoothing
</subsectionHeader>
<bodyText confidence="0.999922">
Additive smoothing is a technique that is used
to attempt to improve the information gained from
low-frequency words, in tasks such as speech pat-
tern recognition (Chen and Goodman, 1998). We
used additive smoothing in the Naive Bayes classi-
fier. To implement additive smoothing, we added a
very small number, S, to the frequency count of each
feature (and divided the final product by this S value
times the size of the feature set to maintain accurate
probabilities). This small number has almost no ef-
fect on more frequent words, but boosts the score
of less common, yet potentially equally informative,
words.
</bodyText>
<subsectionHeader confidence="0.733862">
2.2.2 Decision List
</subsectionHeader>
<bodyText confidence="0.999836">
The decision list classifier uses the log-likelihood
of correspondence between each context feature and
each sense, using additive smoothing (Yarowsky,
1994). The decision list was created by ordering
the correspondences from strongest to weakest. In-
stances that did not match any rule in the decision
list were assigned the most frequent sense, as calcu-
lated from the training data.
</bodyText>
<subsectionHeader confidence="0.832744">
2.2.3 Nearest Neighbor Cosine
</subsectionHeader>
<bodyText confidence="0.952518619047619">
The nearest neighbor cosine classifier required the
creation of a term-document matrix, which contains
a row for each training instance of an ambiguous
word, and a column for each feature that can occur
in the context of an ambiguous word. The rows of
this matrix are referred to as sense vectors because
each row represents a combination of the features of
all ambiguous words that share the same sense.
The nearest neighbor cosine classifier compares
each of the training vectors to each ambiguous in-
stance vector. The cosine between the ambiguous
vector and each of the sense vectors is calculated,
and the sense that is the “nearest” (largest cosine, or
smallest angle) is selected by the classifier.
TF-IDF
TF-IDF (Term Frequency-Inverse Document Fre-
quency) is a method for automatically adjusting the
frequency of words based on their semantic impor-
tance to a document in a corpus. TF-IDF decreases
the value of words that occur in more different doc-
uments. The equation we used for TF-IDF is:
</bodyText>
<equation confidence="0.9943795">
( |D |/
tfi · idfi = ni · log |D : tiǫD|
</equation>
<bodyText confidence="0.999403">
where ni is the number of occurrences of a term ti,
and D is the set of all training documents.
TF-IDF is used in an attempt to minimize the
noise from words such as “and” that are extremely
common, but, since they are common across all
training instances, carry little semantic content.
</bodyText>
<subsectionHeader confidence="0.510818">
2.2.4 Non-contributing Classifiers
</subsectionHeader>
<bodyText confidence="0.999982111111111">
We implemented a classifier based on Latent Se-
mantic Analysis (Landauer et al., 1998). To do
the calculations required for LSA, we used the
SVDLIBC library1. Because this classifier actu-
ally weakened our combination system (in cross-
validation), our classifier combination (Section 2.3)
does not include it.
We also implemented a k-Nearest Neighbors clas-
sifier, which treats each individual training instance
</bodyText>
<footnote confidence="0.988414">
1http://tedlab.mit.edu/-dr/SVDLIBC/
</footnote>
<page confidence="0.998676">
309
</page>
<bodyText confidence="0.99987625">
as a separate vector (instead of treating each set of
training instances that makes up a given sense as a
single vector), and finds the k-nearest training in-
stances to the test instance. The most frequent sense
among the k-nearest to the test instance is the se-
lected sense. Unfortunately, the k-NN classifier did
not improve the results of our combined system and
so it is not included in our classifier combination.
</bodyText>
<subsectionHeader confidence="0.990754">
2.3 Classifier Combination
</subsectionHeader>
<bodyText confidence="0.999996608695652">
The classifier combination algorithm that we imple-
ment is based on a simple voting system. Each clas-
sifier returns a score for each sense: the Naive Bayes
classifier returns a probability, the cosine-based clas-
sifiers (including LSA) return a cosine distance, and
the decision list classifier returns the weight asso-
ciated with the chosen feature (if no feature is se-
lected, the frequency of the most frequent sense is
used). The scores from each classifier are normal-
ized to the range [0,1], multiplied by an empirically
determined weight for that classifier, and summed
for each sense. The combiner then chooses the sense
with the highest score. We used cross validation to
determine the weight for each classifier, and it was
during that test that we discovered that the best con-
stant for the LSA and k-NN classifiers was zero. The
most likely explanation for this is that the LSA and
k-NN are doing similar, only less accurate, classi-
fications as the nearest neighbor classifier, and so
have little new knowledge to add to the combiner.
We also implemented a simple majority voting sys-
tem, where the chosen sense is the sense chosen by
the most classifiers, but found it to be less accurate.
</bodyText>
<subsectionHeader confidence="0.949311">
2.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.9987296">
To increase the accuracy of our system, we needed to
optimize various parameters by running the training
data through 10-way cross-validation and averaging
the scores from each set. Table 2 shows the results of
this cross-validation in determining the δ value used
in the additive smoothing for both the Naive Bayes
classifier and for the decision list classifier.
We also experimented with different feature sets.
The results of these experiments are shown in Ta-
ble 3.
</bodyText>
<table confidence="0.999498909090909">
Classifier Cross-Validation Score
MFS 34.99%
LSA 38.61%
k-NN Cosine 61.54%
Naive Bayes 58.60%
Decision List 64.37%
NN Cosine 65.56%
Simple Combined 65.89%
Weighted Combined 67.38%
Classifier Competition Score
SWAT-MP 65.78%
</table>
<tableCaption confidence="0.981752">
Table 1: The (micro-averaged) precision of each of
our classifiers in cross-validation, plus the actual re-
sults from our entry in SemEval-2007.
</tableCaption>
<table confidence="0.999741">
Naive Bayes
δ precision
10 −1 53.01%
10 −2 58.60%
10 −3 60.80%
10 −4 61.09%
10 −5 60.95%
10 −6 61.06%
10 −7 61.08%
</table>
<tableCaption confidence="0.885998666666667">
Table 2: On cross-validated training data, system
precision when using different smoothing parame-
ters in the Naive Bayes and decision list classifiers.
</tableCaption>
<subsectionHeader confidence="0.919931">
2.5 Conclusion
</subsectionHeader>
<bodyText confidence="0.9999558">
We presented a supervised system that used simple
n-gram features and a combination of five different
classifiers. The methods used are applicable to any
lexical sample task, and have been applied to lexical
sample tasks in previous Senseval competitions.
</bodyText>
<sectionHeader confidence="0.997851" genericHeader="method">
3 Task 14
</sectionHeader>
<bodyText confidence="0.999211285714286">
The goal of Task 14: Affective Text is to take a list of
headlines and meaningfully annotate their emotional
content. Each headline was scored along seven axes:
six predefined emotions (Anger, Disgust, Fear, Joy,
Sadness, and Surprise) on a scale from 0 to 100, and
the negative/positive polarity (valence) of the head-
line on a scale from −100 to +100.
</bodyText>
<figure confidence="0.9986785">
Decision List
δ precision
1.0 64.14%
0.5 64.37%
0.1 64.59%
0.05 64.48%
0.005 64.37%
0.001 64.37%
</figure>
<page confidence="0.98892">
310
</page>
<table confidence="0.9995812">
Naive Bayes Feature Dec. List
55.36% word trigrams 59.98%
55.55% word bigrams 59.98%
58.50% weighted unigrams 62.77%
58.60% all features 64.37%
NN-Cosine Feature Combined
60.39% word trigrams 62.03%
60.42% word bigrams 62.66%
65.56% weighted unigrams 64.56%
62.92% all features 67.38%
</table>
<tableCaption confidence="0.998487">
Table 3: On cross-validated training data, the preci-
</tableCaption>
<bodyText confidence="0.98264375">
sion when using different features with each classi-
fier, and with the combination of all classifiers. All
feature sets include a simple, unweighted bag-of-
words in addition to the feature listed.
</bodyText>
<subsectionHeader confidence="0.998572">
3.1 Training Data Collection
</subsectionHeader>
<bodyText confidence="0.973930857142857">
Our system is trained on a set of pre-annotated head-
lines, building up a knowledge-base of individual
words and their emotional significance.
We were initially provided with a trial-set of 250
annotated headlines. We ran 5-way cross-validation
with a preliminary version of our system, and found
that a dataset of that size was too sparse to effec-
tively tag new headlines. In order to generate a
more meaningful knowledge-base, we created a sim-
ple web interface for human annotation of headlines.
We used untrained, non-experts to annotate an addi-
tional 1,000 headlines for use as a training set. The
headlines were taken from a randomized collection
of headlines from the Associated Press.
We included a subset of the original test set in
the set that we put online so that we could get a
rough estimate of the consistency of human annota-
tion. We found that consistency varied greatly across
the emotions. As can be seen in Table 4, our annota-
tors were very consistent with the trial data annota-
tors on some emotions, while inconsistent on others.
In ad-hoc, post-annotation interviews, our anno-
tators commented that the task was very difficult.
What we had initially expected to be a tedious but
mindless exercise turned out to be rather involved.
They also reported that some emotions were consis-
tently harder to annotate than others. The results in
Table 4 seem to bear this out as well.
</bodyText>
<table confidence="0.99945325">
Emotion Correlation
Valence 0.83
Sadness 0.81
Joy 0.79
Disgust 0.38
Anger 0.32
Fear 0.19
Surprise 0.19
</table>
<tableCaption confidence="0.879934">
Table 4: Pearson correlations between trial data an-
notators and our human annotators.
</tableCaption>
<bodyText confidence="0.999528166666667">
One difficulty reported by our annotators was de-
termining whether to label the emotion experienced
by the reader or by the subject of the headline. For
example, the headline “White House surprised at
reaction to attorney firings” clearly states that the
White House was surprised, but the reader might not
have been.
Another of the major difficulties in properly an-
notating headlines is that many headlines can be an-
notated in vastly different ways depending on the
viewpoint of the annotator. For example, while the
headline “Hundreds killed in earthquake” would be
universally accepted as negative, the headline “Italy
defeats France in World Cup Final,” can be seen as
positive, negative, or even neutral depending on the
viewpoint of the reader. These types of problems
made it very difficult for our annotators to provide
consistent labels.
</bodyText>
<subsectionHeader confidence="0.999388">
3.2 Data Processing
</subsectionHeader>
<bodyText confidence="0.999399">
Before we can process a headline and determine its
emotions and valence, we convert our list of tagged
headlines into a useful knowledge base. To this end,
we create a word-emotion mapping.
</bodyText>
<subsectionHeader confidence="0.971254">
3.2.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.9999644">
The first step is to lemmatize every word in every
headline, in an attempt to reduce the sparseness of
our data. We use the CELEX2 (Baayen et al., 1996)
data to perform this lemmatization. There are unfor-
tunate cases where lemmatizing actually changes the
emotional content of a word (unfortunate becomes
fortunate), but without lemmatization, our data is
simply too sparse to be of any use. Once we have
our list of lemmatized words, we score the emotions
and valence of each word as the average of the emo-
</bodyText>
<page confidence="0.996376">
311
</page>
<bodyText confidence="0.998123">
tions and valence of every headline, H, in which that
word, w, appears, ignoring non-content words:
</bodyText>
<equation confidence="0.9680185">
�Score(Em, w) � Score(Em, H)
H:wEH
</equation>
<bodyText confidence="0.999989416666667">
In the final step of pre-processing, we add the
synonyms and antonyms of the sense labels them-
selves to our word-emotion mapping. We queried
the web interface for Roget’s New Millennium The-
saurus (Thesaurus.com, 2007) and added every word
in the first 8 entries for each sense label to our map,
with a score of 100 (the maximum possible score)
for that sense. We also added every word in the first
4 antonym entries with a score of −40. For exam-
ple, for the emotion Joy, we added alleviation and
amusement with a score of 100, and we added de-
spair and misery with a score of −40.
</bodyText>
<subsectionHeader confidence="0.697203">
3.2.2 Processing
</subsectionHeader>
<bodyText confidence="0.999973">
After creating our word-emotion mapping, pre-
dicting the emotions and valence of a given headline
is straightforward. We treat each headline as a bag-
of-words and lemmatize each word. Then we look
up each word in the headline in our word-emotion
map, and average the emotion and valence scores of
each word in our map that occurs in the headline.
We ignore words that were not present in the train-
ing data.
</bodyText>
<subsectionHeader confidence="0.832328">
3.3 Evaluation
</subsectionHeader>
<table confidence="0.999868">
Emotion Training 250 Size (Headlines)
100 1000
Valence 19.07 32.07 35.25
Anger 8.42 13.38 24.51
Disgust 11.22 23.45 18.55
Fear 14.43 18.56 32.52
Joy 31.87 46.03 26.11
Sadness 16.32 35.09 38.98
Surprise 1.15 11.12 11.82
</table>
<tableCaption confidence="0.997636">
Table 5: A comparison of results on the provided
</tableCaption>
<bodyText confidence="0.859901384615385">
trial data as headlines are added to the training
set. The scores are given as Pearson correlations of
scores for training sets of size 100, 250, and 1000
headlines.
As can be seen in Table 5, four out of six emotions
and the valence increase along with training set size.
This leads us to believe that further increases in the
size of the training set would continue to improve
results. Lack of time prevents a full analysis that
can explain the sudden drop of Disgust and Joy.
Table 6 shows our full results from this task. Our
system finished third out of five in the valence sub-
task and second out of three in the emotion sub-task.
</bodyText>
<table confidence="0.998721555555556">
Emotion Fine Coarse -Grained R
A P
Valence 35.25 53.20 45.71 3.42
Anger 24.51 92.10 12.00 5.00
Disgust 18.55 97.20 0.00 0.00
Fear 32.52 84.80 25.00 14.40
Joy 26.11 80.60 35.41 9.44
Sadness 38.98 87.70 32.50 11.92
Surprise 11.82 89.10 11.86 10.93
</table>
<tableCaption confidence="0.78067">
Table 6: Our full results from SemEval-2007, Task
</tableCaption>
<bodyText confidence="0.996857">
14, as reported by the task organizers. Fine-grained
scores are given as Pearson correlations. Coarse-
grained scores are given as accuracy (A), preci-
sion (P), and recall (R).
</bodyText>
<subsectionHeader confidence="0.693415">
3.4 Conclusion
</subsectionHeader>
<bodyText confidence="0.999945">
We presented a supervised system that used a un-
igram model to annotate the emotional content of
headlines. We also used synonym expansion on the
emotion label words. Our annotators encountered
significant difficulty while tagging training data, due
to ambiguity in definition of the task.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999584583333333">
R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1996.
CELEX2. LDC96L14, Linguistic Data Consortium,
Philadelphia.
S. F. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal Report TR-10-98, Harvard University.
T.K. Landauer, Foltz P.W, and D. Laham. 1998. Intro-
duction to latent semantic analysis. Discourse Pro-
cesses, 25:259–284.
Thesaurus.com. 2007. Roget’s New Millennium The-
saurus, 1st ed. (v 1.3.1). Lexico Publishing Group,
LLC,http://thesaurus.reference.com.
</reference>
<page confidence="0.9833">
312
</page>
<reference confidence="0.9986003">
Richard Wicentowski, Emily Thomforde, and
Adrian Packel. 2004. The Swarthmore College
SENSEVAL-3 System. In Proceedings ofSenseval-3,
Third International Workshop on Evaluating Word
Sense Disambiguation Systems.
David Yarowsky. 1994. Decision lists for lexical am-
biguity resolution: Application to accent restoration
in Spanish and French. In Proceedings of the 32nd
Annual Meeting of the Association for Computational
Linguistics, pages 88–95.
</reference>
<page confidence="0.999512">
313
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.634366">
<title confidence="0.715652">SWAT-MP: The SemEval-2007 Systems for Task 5 and Task 14</title>
<author confidence="0.999815">Phil Katz</author>
<author confidence="0.999815">Matthew Singleton</author>
<author confidence="0.999815">Richard Wicentowski</author>
<affiliation confidence="0.9992745">Department of Computer Science Swarthmore College</affiliation>
<address confidence="0.94067">Swarthmore, PA</address>
<abstract confidence="0.99584875">In this paper, we describe our two SemEval- 2007 entries. Our first entry, for Task 5: Multilingual Chinese-English Lexical Sample Task, is a supervised system that decides the most appropriate English translation of a Chinese target word. This system uses a combination of Naive Bayes, nearest neighbor cosine, decision lists, and latent semantic analysis. Our second entry, for Task 14: Affective Text, is a supervised system that annotates headlines using a predefined list of emotions. This system uses synonym expansion and matches lemmatized unigrams in the test headlines against a corpus of handannotated headlines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>R Piepenbrock</author>
<author>L Gulikers</author>
</authors>
<date>1996</date>
<booktitle>CELEX2. LDC96L14, Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="13631" citStr="Baayen et al., 1996" startWordPosition="2201" endWordPosition="2204">line “Italy defeats France in World Cup Final,” can be seen as positive, negative, or even neutral depending on the viewpoint of the reader. These types of problems made it very difficult for our annotators to provide consistent labels. 3.2 Data Processing Before we can process a headline and determine its emotions and valence, we convert our list of tagged headlines into a useful knowledge base. To this end, we create a word-emotion mapping. 3.2.1 Pre-processing The first step is to lemmatize every word in every headline, in an attempt to reduce the sparseness of our data. We use the CELEX2 (Baayen et al., 1996) data to perform this lemmatization. There are unfortunate cases where lemmatizing actually changes the emotional content of a word (unfortunate becomes fortunate), but without lemmatization, our data is simply too sparse to be of any use. Once we have our list of lemmatized words, we score the emotions and valence of each word as the average of the emo311 tions and valence of every headline, H, in which that word, w, appears, ignoring non-content words: �Score(Em, w) � Score(Em, H) H:wEH In the final step of pre-processing, we add the synonyms and antonyms of the sense labels themselves to ou</context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1996</marker>
<rawString>R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1996. CELEX2. LDC96L14, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="4091" citStr="Chen and Goodman, 1998" startWordPosition="640" endWordPosition="643">scribe the contributing classifiers first. Table 1 shows the results of each classifier, as well as our classifier combination system. 2.2.1 Naive Bayes The Naive Bayes classifier is based on Bayes’ theorem, which allows us to define the similarity between an instance, I, and a sense class, 5j, as: 5im(I, 5j) = Pr(I, 5j) = Pr(5j) * Pr(I|5j) We then choose the sense with the maximum similarity to the test instance. Additive Smoothing Additive smoothing is a technique that is used to attempt to improve the information gained from low-frequency words, in tasks such as speech pattern recognition (Chen and Goodman, 1998). We used additive smoothing in the Naive Bayes classifier. To implement additive smoothing, we added a very small number, S, to the frequency count of each feature (and divided the final product by this S value times the size of the feature set to maintain accurate probabilities). This small number has almost no effect on more frequent words, but boosts the score of less common, yet potentially equally informative, words. 2.2.2 Decision List The decision list classifier uses the log-likelihood of correspondence between each context feature and each sense, using additive smoothing (Yarowsky, 1</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. F. Chen and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>Introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="6401" citStr="Landauer et al., 1998" startWordPosition="1026" endWordPosition="1029">ased on their semantic importance to a document in a corpus. TF-IDF decreases the value of words that occur in more different documents. The equation we used for TF-IDF is: ( |D |/ tfi · idfi = ni · log |D : tiǫD| where ni is the number of occurrences of a term ti, and D is the set of all training documents. TF-IDF is used in an attempt to minimize the noise from words such as “and” that are extremely common, but, since they are common across all training instances, carry little semantic content. 2.2.4 Non-contributing Classifiers We implemented a classifier based on Latent Semantic Analysis (Landauer et al., 1998). To do the calculations required for LSA, we used the SVDLIBC library1. Because this classifier actually weakened our combination system (in crossvalidation), our classifier combination (Section 2.3) does not include it. We also implemented a k-Nearest Neighbors classifier, which treats each individual training instance 1http://tedlab.mit.edu/-dr/SVDLIBC/ 309 as a separate vector (instead of treating each set of training instances that makes up a given sense as a single vector), and finds the k-nearest training instances to the test instance. The most frequent sense among the k-nearest to the</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T.K. Landauer, Foltz P.W, and D. Laham. 1998. Introduction to latent semantic analysis. Discourse Processes, 25:259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thesaurus com</author>
</authors>
<title>Roget’s New Millennium Thesaurus, 1st ed. (v 1.3.1).</title>
<date>2007</date>
<pages>LLC,http://thesaurus.reference.com.</pages>
<publisher>Lexico Publishing Group,</publisher>
<contexts>
<context position="1688" citStr="com, 2007" startWordPosition="254" endWordPosition="255">et al., 2004). We implement five different classifiers: a Naive Bayes classifier, a decision list classifier, two different nearest neighbor cosine classifiers, and a classifier based on Latent Semantic Analysis. Section 2.2 describes each of the individual classifiers, Section 2.3 describes our classifier combination system, and Section 2.4 presents our results. The second entry, a supervised system used in the Affective Text task (Task 14), uses a corpus of headlines hand-annotated by non-experts. It also uses an online thesaurus to match synonyms and antonyms of the sense labels (Thesaurus.com, 2007). Section 3.1 describes the creation of the annotated training corpus, Section 3.2 describes our method for assigning scores to the headlines, and Section 3.3 presents our results. 2 Task 5: Multilingual Chinese-English LS This task presents a single Chinese word in context which must be disambiguated. Rather than asking participants to provide a sense label corresponding to a pre-defined sense inventory, the goal here is to label each ambiguous word with its correct English translation. Since the task is quite similar to more traditional lexical sample tasks, we extend an approach used succes</context>
<context position="14342" citStr="com, 2007" startWordPosition="2321" endWordPosition="2322">he emotional content of a word (unfortunate becomes fortunate), but without lemmatization, our data is simply too sparse to be of any use. Once we have our list of lemmatized words, we score the emotions and valence of each word as the average of the emo311 tions and valence of every headline, H, in which that word, w, appears, ignoring non-content words: �Score(Em, w) � Score(Em, H) H:wEH In the final step of pre-processing, we add the synonyms and antonyms of the sense labels themselves to our word-emotion mapping. We queried the web interface for Roget’s New Millennium Thesaurus (Thesaurus.com, 2007) and added every word in the first 8 entries for each sense label to our map, with a score of 100 (the maximum possible score) for that sense. We also added every word in the first 4 antonym entries with a score of −40. For example, for the emotion Joy, we added alleviation and amusement with a score of 100, and we added despair and misery with a score of −40. 3.2.2 Processing After creating our word-emotion mapping, predicting the emotions and valence of a given headline is straightforward. We treat each headline as a bagof-words and lemmatize each word. Then we look up each word in the headl</context>
</contexts>
<marker>com, 2007</marker>
<rawString>Thesaurus.com. 2007. Roget’s New Millennium Thesaurus, 1st ed. (v 1.3.1). Lexico Publishing Group, LLC,http://thesaurus.reference.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Wicentowski</author>
<author>Emily Thomforde</author>
<author>Adrian Packel</author>
</authors>
<title>The Swarthmore College SENSEVAL-3 System.</title>
<date>2004</date>
<booktitle>In Proceedings ofSenseval-3, Third International Workshop on Evaluating Word Sense Disambiguation Systems.</booktitle>
<contexts>
<context position="1091" citStr="Wicentowski et al., 2004" startWordPosition="160" endWordPosition="163"> system uses a combination of Naive Bayes, nearest neighbor cosine, decision lists, and latent semantic analysis. Our second entry, for Task 14: Affective Text, is a supervised system that annotates headlines using a predefined list of emotions. This system uses synonym expansion and matches lemmatized unigrams in the test headlines against a corpus of handannotated headlines. 1 Introduction This paper describes our two entries in SemEval2007. The first entry, a supervised system used in the Multilingual Chinese-English Lexical Sample task (Task 5), is an extension of the system described in (Wicentowski et al., 2004). We implement five different classifiers: a Naive Bayes classifier, a decision list classifier, two different nearest neighbor cosine classifiers, and a classifier based on Latent Semantic Analysis. Section 2.2 describes each of the individual classifiers, Section 2.3 describes our classifier combination system, and Section 2.4 presents our results. The second entry, a supervised system used in the Affective Text task (Task 14), uses a corpus of headlines hand-annotated by non-experts. It also uses an online thesaurus to match synonyms and antonyms of the sense labels (Thesaurus.com, 2007). S</context>
<context position="2365" citStr="Wicentowski et al., 2004" startWordPosition="358" endWordPosition="361">d training corpus, Section 3.2 describes our method for assigning scores to the headlines, and Section 3.3 presents our results. 2 Task 5: Multilingual Chinese-English LS This task presents a single Chinese word in context which must be disambiguated. Rather than asking participants to provide a sense label corresponding to a pre-defined sense inventory, the goal here is to label each ambiguous word with its correct English translation. Since the task is quite similar to more traditional lexical sample tasks, we extend an approach used successfully in multiple Senseval-3 lexical sample tasks (Wicentowski et al., 2004). 2.1 Features Each of our classifiers uses the same set of context features, taken directly from the data provided by the task organizers. The features we used included: • Bag-of-words (unigrams) • Bigrams and trigrams around the target word • Weighted unigrams surrounding the target word The weighted unigram features increased the frequencies of the ten words before and after the target word by inserting them multiple times into the bag-of-words. 308 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 308–313, Prague, June 2007. c�2007 Association for </context>
</contexts>
<marker>Wicentowski, Thomforde, Packel, 2004</marker>
<rawString>Richard Wicentowski, Emily Thomforde, and Adrian Packel. 2004. The Swarthmore College SENSEVAL-3 System. In Proceedings ofSenseval-3, Third International Workshop on Evaluating Word Sense Disambiguation Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="4695" citStr="Yarowsky, 1994" startWordPosition="738" endWordPosition="739">dman, 1998). We used additive smoothing in the Naive Bayes classifier. To implement additive smoothing, we added a very small number, S, to the frequency count of each feature (and divided the final product by this S value times the size of the feature set to maintain accurate probabilities). This small number has almost no effect on more frequent words, but boosts the score of less common, yet potentially equally informative, words. 2.2.2 Decision List The decision list classifier uses the log-likelihood of correspondence between each context feature and each sense, using additive smoothing (Yarowsky, 1994). The decision list was created by ordering the correspondences from strongest to weakest. Instances that did not match any rule in the decision list were assigned the most frequent sense, as calculated from the training data. 2.2.3 Nearest Neighbor Cosine The nearest neighbor cosine classifier required the creation of a term-document matrix, which contains a row for each training instance of an ambiguous word, and a column for each feature that can occur in the context of an ambiguous word. The rows of this matrix are referred to as sense vectors because each row represents a combination of t</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>David Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 88–95.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>