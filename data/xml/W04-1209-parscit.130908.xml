<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000824">
<title confidence="0.998104">
Support Vector Machine Approach to Extracting
Gene References into Function from Biological Documents
</title>
<author confidence="0.982414">
Chih Lee, Wen-Juan Hou and Hsin-Hsi Chen
</author>
<affiliation confidence="0.99274">
Natural Language Processing Laboratory
Department of Computer Science and Information Engineering
National Taiwan University
</affiliation>
<address confidence="0.927769">
1 Roosevelt Road, Section 4, Taipei, Taiwan, 106
</address>
<email confidence="0.991872">
{clee, wjhou}@nlg.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw
</email>
<sectionHeader confidence="0.992447" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969818181818">
In the biological domain, extracting newly
discovered functional features from the
massive literature is a major challenging issue.
To automatically annotate Gene References
into Function (GeneRIF) in a new literature is
the main goal of this paper. We tried to find
GRIF words in a training corpus, and then
applied these informative words to annotate the
GeneRIFs in abstracts with several different
weighting schemes. The experiments showed
that the Classic Dice score is at most 50.18%,
when the weighting schemes proposed in the
paper (Hou et al., 2003) were adopted. In
contrast, after employing Support Vector
Machines (SVMs) and the definition of classes
proposed by Jelier et al. (2003), the score
greatly improved to 56.86% for Classic Dice
(CD). Adopting the same features, SVMs
demonstrated advantage over the Naïve Bayes
Classifier. Finally, the combination of the
former two models attained a score of 59.51%
for CD.
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999537627906977">
Text Retrieval Conference (TREC) has been
dedicated to information retrieval and information
extraction for years. TREC 2003 introduced a new
track called Genomics Track (Hersh and
Bhupatiraju, 2003) to address the information
retrieval and information extraction issues in the
biomedical domain. For the information extraction
part, the goal was to automatically reproduce the
Gene Reference into Function (GeneRIF) resource
in the LocusLink database (Pruitt et al., 2000.)
GeneRIF associated with a gene is a sentence
describing the function of that gene, and is currently
manually generated.
This paper presents the post-conference work on
the information extraction task (i.e., secondary task).
In the official runs, our system (Hou et al., 2003)
adopted several weighting schemes (described in
Section 3.2) to deal with this problem. However,
we failed to beat the simple baseline approach,
which always picks the title of a publication as the
candidate GeneRIF. Bhalotia et al. (2003)
converted this task into a binary classification
problem and trained a Naïve Bayes classifier with
kernels, achieving 53.04% for CD. In their work,
the title and last sentence of an abstract were
concatenated and features were then extracted from
the resulting string. Jelier et al. (2003) observed the
distribution of target GeneRIFs in 9 sentence
positions and converted this task into a 9-class
classification problem, attaining 57.83% for CD.
Both works indicated that the sentence position is
of great importance. We therefore modified our
system to incorporate the position information with
the help of SVMs and we also investigated the
capability of SVMs versus Naïve Bayes on this
problem.
The rest of this paper is organized as follows.
Section 2 presents the architecture of our extracting
procedure. The basic idea and the experimental
methods in this study are introduced in Section 3.
Section 4 shows the results and makes some
discussions. Finally, Section 5 concludes the
remarks and lists some future works.
</bodyText>
<sectionHeader confidence="0.847383" genericHeader="method">
2 Architecture Overview
</sectionHeader>
<bodyText confidence="0.999931764705882">
A complete annotation system may be done at two
stages, including (1) extraction of molecular
function for a gene from a publication and (2)
alignment of this function with a GO term. Figure
1 shows an example. The left part is an MEDLINE
abstract with the function description highlighted.
The middle part is the corresponding GeneRIF.
The matching words are in bold, and the similar
words are underlined. The right part is the GO
annotation. This figure shows a possible solution of
maintaining the knowledge bases and ontology
using natural language processing technology. We
addressed automation of the first stage in this paper.
The overall architecture is shown in Figure 2.
First, we constructed a training corpus in such a
way that GeneRIFs were collected from LocusLink
and the corresponding abstracts were retrieved from
</bodyText>
<page confidence="0.972552">
54
</page>
<figure confidence="0.3700085">
alignment
MEDLINE abstract
</figure>
<bodyText confidence="0.993965461538462">
The Bcl10 gene was recently isolated
from the breakpoint region of
t(1;14)(p22;q32) in mucosa-associated
lymphoid tissue (MALT) lymphomas.
Somatic mutations of Bcl10 were found
in not only t(1;14)-bearing MALT
lymphomas, but also a wide range of
other tumors. ... ... Our results strongly
suggest that somatic mutations of Bcl10
are extremely rare in malignant
cartilaginous tumors and do not
commonly contribute to their molecular
pathogenesis.
</bodyText>
<figure confidence="0.544998785714286">
PMID: 11836626
GeneRIF
Mutations,
relatively
common in
lymphomas,
are extremely
rare in
malignant
cartilaginous
tumors.
GO annotation
• GO:0005515
term: protein binding
</figure>
<bodyText confidence="0.613472">
definition: Interacting selectively with any protein, or
protein complex (a complex of two or more proteins that
may include other nonprotein molecules).
</bodyText>
<listItem confidence="0.979514571428571">
• GO:0008181
term: tumor suppressor
• GO:0006917
term: induction of apoptosis
• GO:0005622
term: intracellular
• GO:0016329
</listItem>
<bodyText confidence="0.716923">
term: apoptosis regulator activity
definition: The function held by products which directly
regulate any step in the process of apoptosis.
</bodyText>
<figure confidence="0.819541333333333">
• GO:0045786
term: negative regulation of cell cycle
extraction
</figure>
<figureCaption confidence="0.999839">
Figure 1: An Example of Complete Annotation from a Literature to Gene Ontology
</figureCaption>
<bodyText confidence="0.9998034">
MEDLINE. “GRIF words” and their weights were
derived from the training corpus. Then Support
Vector Machines were trained using the derived
corpus. Given a new abstract, a sentence is selected
from the abstract to be the candidate GeneRIF.
</bodyText>
<sectionHeader confidence="0.998136" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999991727272727">
We adopted several weighting schemes to locate the
GeneRIF sentence in an abstract in the official runs
(Hou et al., 2003). Inspired by the work by Jelier et
al. (2003), we incorporated their definition of
classes into our weighting schemes, converting this
task into a classification problem using SVMs as
the classifier. We ran SVMs on both sets of
features proposed by Hou et al. (2003) and Jelier et
al. (2003), respectively. Finally, all the features
were combined and some feature selection methods
were applied to train the classifier.
</bodyText>
<subsectionHeader confidence="0.997254">
3.1 Training and test material preparation
</subsectionHeader>
<bodyText confidence="0.999896285714286">
Since GeneRIFs are often cited verbatim from
abstracts, we decided to reproduce the GeneRIF by
selecting one sentence in the abstract. Therefore,
for each abstract in our training corpus, the sentence
most similar to the GeneRIF was labelled as the
GeneRIF sentence using Classic Dice coefficient as
similarity measure. Totally, 259,244 abstracts were
</bodyText>
<figureCaption confidence="0.998605">
Figure 2: Architecture of Extracting Candidate GeneRIF
</figureCaption>
<bodyText confidence="0.971460333333333">
used, excluding the abstracts for testing. The test
data for evaluation are the 139 abstracts used in
TREC 2003 Genomics track.
</bodyText>
<sectionHeader confidence="0.577308" genericHeader="method">
3.2 GRIF words extraction and weighting
scheme
</sectionHeader>
<bodyText confidence="0.9942935">
We called the matched words between GeneRIF
and the selected sentence as GRIF words in this
paper. GRIF words represent the favorite
vocabulary that human experts use to describe gene
functions. After stop word removal and stemming
operation, 10,506 GRIF words were extracted.
In our previous work (Hou et al., 2003), we first
generated the weight for each GRIF word. Given
an abstract, the score of each sentence is the sum of
weights of all the GRIF words in this sentence.
Finally, the sentence with the highest score is
selected as the candidate GeneRIF. This method is
denoted as OUR weighting scheme, and several
heuristic weighting schemes were investigated.
Here, we only present the weighting scheme used in
SVMs classification. The weighting scheme is as
follows. For GRIF word i, the number of
occurrence G
ni in all the GeneRIF sentences and the
number of occurrence A
</bodyText>
<equation confidence="0.78535">
ni in all the abstracts were
computed and A
n i / n was assigned to GRIF word i
G
i
</equation>
<bodyText confidence="0.936371">
as its weight.
</bodyText>
<subsectionHeader confidence="0.950877">
3.3 Classification
3.3.1 Class definition and feature extraction
</subsectionHeader>
<bodyText confidence="0.999228125">
The distribution of GeneRIF sentences showed that
the position of a sentence in an abstract is an
important clue to where the answer sentence is.
Jelier et al. (2003) considered only the title, the first
three and the last five sentences, achieving the best
performance in TREC official runs. Their Naïve
Bayes model is as follows. An abstract a is
assigned a class vj by calculating vNB:
</bodyText>
<figure confidence="0.9857982">
GeneRIF
Sentence Locator
GRIF Word
Extractor
Weighted GRIF
Words
Existing
GeneRIFs
on
LocusLink
Training SVMs
Generating
Training Data
Candidate
GeneRIF
New
Abstract
Corresponding
Medline
Abstracts
</figure>
<page confidence="0.804842">
55
</page>
<equation confidence="0.68788575">
vNB = argmax P(vj) X F1 F1 P(wk i  |vj )
v V
je i S k W
e e a , i
</equation>
<bodyText confidence="0.99969132">
where vj is one of the nine positions aforementioned,
S is the set of 9 sentence positions, Wa,i is the set of
all word positions in sentence i in abstract a, wk,i is
the occurrence of the normalized word at position k
in sentence i and V is the set of 9 classes.
We, therefore, represented each abstract by a
feature vector composed of the scores of 9
sentences. Furthermore, with a list of our 10,506
GRIF words at hand, we also computed the
occurrences of these words in each sentence, given
an abstract. Each abstract is then represented by the
number of occurrences of these words ni the 9
sentences respectively, i.e., the feature vector is
94,554 in length. Classification based on this type
of features is denoted the sentence-wise bag of
words model in the rest of this paper. Combining
these two models, we got totally 94,563 features.
Since we are extracting sentences discussing gene
functions, it’s reasonable to expect gene or protein
names in the GeneRIF sentence. Therefore, we
employed Yapex (Olsson et al., 2002) and
GAPSCORE (Chang et al., 2004) protein/gene
name detectors to count the number of protein/gene
names in each of the 9 sentences, resulting in
94,581 features.
</bodyText>
<subsectionHeader confidence="0.956282">
3.3.2 Training SVMs
</subsectionHeader>
<bodyText confidence="0.999969307692308">
The whole process related to SVM was done via
LIBSVM – A Library for Support Vector Machines
(Hsu et al., 2003). Radial basis kernel was adopted
based on our previous experience. However,
further verification showed that the combined
model with either linear or polynomial kernel only
slightly surpassed the baseline, attaining 50.67% for
CD. In order to get the best-performing classifier,
we tuned two parameters, C and gamma. They are
the penalty coefficient in optimization and a
parameter for the radial basis kernel, respectively.
Four-fold cross validation accuracy was used to
select the best parameter pair.
</bodyText>
<subsectionHeader confidence="0.863453">
3.3.3 Picking up the answe r sentence
</subsectionHeader>
<bodyText confidence="0.9999935">
Test instances were first fed to the classifier to get
the predicted positions of GeneRIF sentences. In
case that the predicted position doesn’t have a
sentence, which would happen when the abstract
doesn’t have enough sentences, the sentence with
the highest score is picked for the weighting
scheme and the combined model, otherwise the title
is picked for the sentence-wise bag of words model.
</bodyText>
<sectionHeader confidence="0.998939" genericHeader="evaluation">
4 Results and Discussions
</sectionHeader>
<bodyText confidence="0.999824558823529">
The performance measures are based on Dice
coefficient, which calculates the overlap between
the candidate GeneRIF and actual GeneRIF.
Classic Dice (CD) is the classic Dice formula using
a common stop word list and the Porter stemming
algorithm. Due to lack of space, we referred you to
the Genomics track overview for the other three
modifications of CD (Hersh and Bhupatiraju, 2003).
The evaluation results are shown in Table 2. The
1st row shows the official run of Jelier’s team, the
first place in the official runs. The 2nd row shows
the performance when the Naïve Bayes classifier
adopted by Jelier is replaced with SVMs. The 3rd
row is the performance of our weighting scheme
without a classifier. The 4th row then lists the
performance when our weighting scheme is
combined with SVMs. The 5th row is the result
when our weighting scheme and the sentence-wise
bag of words model are combined together. The 6th
row is the result when two gene/protein name
detectors are incorporated into the combined model.
The next two rows were obtained after two feature
selection methods were applied. The 9th row shows
the performance when the classifier always
proposes a sentence most similar to the actual
GeneRIF. The last row lists the baseline, i.e., title
is always picked.
A comparative study on text categorization
(Joachims, 1998) showed that SVMs outperform
other classification methods, such as Naïve Bayes,
C4.5, and k-NN. The reasons would be that SVMs
are capable of handling large feature space, text
categorization has few irrelevant features, and
document vectors are sparse. The comparison
</bodyText>
<table confidence="0.982577181818182">
CD MUD MBD MBDP
1 Jelier (Sentence-wise bag of words + Naïve Bayes) 57.83% 59.63% 46.75% 49.11%
2 Sentence-wise bag of words + SVMs 58.92% 61.46% 47.86% 50.84%
3 OUR Weighting scheme 50.18% 46.71% 33.47% 38.83%
4 OUR Weighting scheme + SVMs 56.86% 58.81% 45.08% 48.10%
5 Combined 59.51% 62.16% 48.17% 51.25%
6 Combined + gene/protein names 57.59% 59.95% 46.69% 49.68%
7 Combined + BWRatio feature selection 57.59% 59.90% 47.11% 50.08%
8 Combined + Graphical feature selection 58.81% 61.09% 47.98% 50.92%
9 Optimal Classifier 67.60% 70.74% 59.28% 62.09%
10 Baseline 50.47% 52.60% 34.82% 37.91%
</table>
<tableCaption confidence="0.999756">
Table 2: Comparison of performances on the 139 abstracts
</tableCaption>
<page confidence="0.995725">
56
</page>
<bodyText confidence="0.999971555555556">
between SVMs and the Naïve Bayes classifier again
demonstrated the superiority of SVMs in text
categorization (rows 1, 2).
The performance greatly improved after
introducing position information (rows 3, 4),
showing the sentence position plays an important
role in locating the GeneRIF sentence. The 2%
difference between rows 2 and 4 indicates that the
features under sentence-wise bag of words model
are more informative than those under our
weighting scheme. However, with only 9 features,
our weighting scheme with SVMs performed fairly
well. Comparing the performance before and after
combining our weighting scheme and the sentence-
wise bag of words model (rows 2, 5 and rows 4, 5),
we can infer from the performance differences that
both models provide mutually exclusive
information in the combined model. The result
shown in row 6 indicates that the information of
gene/protein name occurrences did not help identify
the GeneRIF sentences in these 139 test abstracts.
We performed feature selection on the combined
model to reduce the dimension of feature space.
There were two methods applied: a supervised
heuristic method (denoted as BWRatio feature
selection in Table 2) (S. Dutoit et al., 2002) and
another unsupervised method (denoted as Graphical
feature selection in Table 2) (Chang et al., 2002).
The number of features was then reduced to about
4,000 for both methods. Unfortunately, the
performance did not improve after either method
was applied. This may be attributed to over-fitting
training data, because the cross-validation
accuracies are indeed higher than those without
feature selection. The result may also imply there
are little irrelevant features in this case.
</bodyText>
<sectionHeader confidence="0.984709" genericHeader="conclusions">
5 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.999894684210526">
This paper proposed an automatic approach to
locate the GeneRIF sentence in an abstract with the
assistance of SVMs, reducing the human effort in
updating and maintaining the GeneRIF field in the
LocusLink database.
We have to admit that the 139 abstracts provided
in TREC 2003 are too few to verify the
performance among models, and the results based
on these 139 abstracts may be slightly biased. Our
next step would aim at measuring the cross-
validation performances using Dice coefficient.
The syntactic information is worth exploring,
since the sentences describing gene functions may
share some common structural patterns. Moreover,
how the weighting scheme affects the performance
is also very interesting. We are currently trying to
obtain a weighting scheme that can best distinguish
GeneRIF sentence from non-GeneRIF sentence
without classifiers.
</bodyText>
<sectionHeader confidence="0.996386" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99593092">
G. Bhalotia, P.I. Nakov, A.S. Schwartz, and M.A.
Hearst. 2003. BioText Team Report for the TREC
2003 Genomics Track. TREC 2003 work notes:
158-166.
Y.C. I. Chang, H. Hsu and L.Y. Chou. 2002.
Graphical Features Selection Method. Intelligent
Data Engineering and Automated Learning,
Edited by H. Yin, N. Allinson, R. Freeman, J.
Keane, and S. Hubband.
J.T. Chang, H. Schutze, R.B. Altman. 2004.
GAPSCORE: finding gene and protein names one
word at a time. Bioinformatics, 20(2):216-225.
S. Dutoit, Y.H. Yang, M.J. Callow and T.P. Speed.
2002. Statistical methods for identifying
differentially expressed genes in replicated cDNA
microarray experiments. J. Amer. Statis. Assoc.
97:77-86.
W. Hersh and Ravi Teja Bhupatiraju. 2003. TREC
Genomics Track Overview. TREC 2003 work
notes.
W.J. Hou, C.Y. Teng, C. Lee and H.H. Chen. 2003.
SVM Approach to GeneRIF Annotation.
Proceedings of TREC 2003.
C.W. Hsu, C.C Chang and C.J. Lin. 2003. A
Practical Guide to Support Vector Classification.
http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.ht
ml.
R. Jelier, M. Schuemie, C.V.E. Eijk, M. Weeber,
E.V. Mulligen, B. Schijvenaars, B. Mons and J.
Kors. 2003. Searching for geneRIFs: concept-
based query expansion and Bayes classification.
TREC 2003 work notes: 167-174.
T. Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with Many
Relevant Features. Proceedings of ECML-98,
137-142.
F. Olsson, G. Eriksson, K. Franzén, L. Asker and P.
Lidén. 2002. Notions of Correctness when
Evaluating Protein Name Taggers. Proceedings of
the 19th International Conference on
Computational Linguistics 2002, 765-771.
K.D. Pruitt, K.S. Katz, H. Sicotte and D.R. Maglott.
2000. Introducing RefSeq and LocusLink:
Curated Human Genome Resources at the NCBI.
Trends Genet, 16(1):44-47.
T. Sekimizu, H.S. Park and J. Tsujji. 1998.
Identifying the Interaction Between Genes and
Gene Products Based on Frequently Seen Verbs
in Medline Abstracts. Genome Information, 9:62-
71
</reference>
<page confidence="0.999049">
57
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.255528">
<title confidence="0.9665325">Support Vector Machine Approach to Extracting Gene References into Function from Biological Documents</title>
<author confidence="0.984651">Wen-Juan Hou Lee</author>
<affiliation confidence="0.937013">Natural Language Processing Department of Computer Science and Information National Taiwan</affiliation>
<address confidence="0.983939">1 Roosevelt Road, Section 4, Taipei, Taiwan,</address>
<email confidence="0.987443">clee@nlg.csie.ntu.edu.tw,hh_chen@csie.ntu.edu.tw</email>
<email confidence="0.987443">wjhou@nlg.csie.ntu.edu.tw,hh_chen@csie.ntu.edu.tw</email>
<abstract confidence="0.979325590909091">In the biological domain, extracting newly discovered functional features from the massive literature is a major challenging issue. To automatically annotate Gene References into Function (GeneRIF) in a new literature is the main goal of this paper. We tried to find GRIF words in a training corpus, and then applied these informative words to annotate the GeneRIFs in abstracts with several different weighting schemes. The experiments showed that the Classic Dice score is at most 50.18%, when the weighting schemes proposed in the (Hou 2003) were adopted. In contrast, after employing Support Vector Machines (SVMs) and the definition of classes by Jelier (2003), the score greatly improved to 56.86% for Classic Dice (CD). Adopting the same features, SVMs demonstrated advantage over the Naïve Bayes Classifier. Finally, the combination of the former two models attained a score of 59.51%</abstract>
<intro confidence="0.565689">for CD.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Bhalotia</author>
<author>P I Nakov</author>
<author>A S Schwartz</author>
<author>M A Hearst</author>
</authors>
<date>2003</date>
<booktitle>BioText Team Report for the TREC 2003 Genomics Track. TREC 2003 work notes:</booktitle>
<pages>158--166</pages>
<contexts>
<context position="2321" citStr="Bhalotia et al. (2003)" startWordPosition="338" endWordPosition="341">oduce the Gene Reference into Function (GeneRIF) resource in the LocusLink database (Pruitt et al., 2000.) GeneRIF associated with a gene is a sentence describing the function of that gene, and is currently manually generated. This paper presents the post-conference work on the information extraction task (i.e., secondary task). In the official runs, our system (Hou et al., 2003) adopted several weighting schemes (described in Section 3.2) to deal with this problem. However, we failed to beat the simple baseline approach, which always picks the title of a publication as the candidate GeneRIF. Bhalotia et al. (2003) converted this task into a binary classification problem and trained a Naïve Bayes classifier with kernels, achieving 53.04% for CD. In their work, the title and last sentence of an abstract were concatenated and features were then extracted from the resulting string. Jelier et al. (2003) observed the distribution of target GeneRIFs in 9 sentence positions and converted this task into a 9-class classification problem, attaining 57.83% for CD. Both works indicated that the sentence position is of great importance. We therefore modified our system to incorporate the position information with th</context>
</contexts>
<marker>Bhalotia, Nakov, Schwartz, Hearst, 2003</marker>
<rawString>G. Bhalotia, P.I. Nakov, A.S. Schwartz, and M.A. Hearst. 2003. BioText Team Report for the TREC 2003 Genomics Track. TREC 2003 work notes: 158-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C I Chang</author>
<author>H Hsu</author>
<author>L Y Chou</author>
</authors>
<date>2002</date>
<booktitle>Graphical Features Selection Method. Intelligent Data Engineering and Automated Learning, Edited</booktitle>
<contexts>
<context position="14384" citStr="Chang et al., 2002" startWordPosition="2275" endWordPosition="2278">n infer from the performance differences that both models provide mutually exclusive information in the combined model. The result shown in row 6 indicates that the information of gene/protein name occurrences did not help identify the GeneRIF sentences in these 139 test abstracts. We performed feature selection on the combined model to reduce the dimension of feature space. There were two methods applied: a supervised heuristic method (denoted as BWRatio feature selection in Table 2) (S. Dutoit et al., 2002) and another unsupervised method (denoted as Graphical feature selection in Table 2) (Chang et al., 2002). The number of features was then reduced to about 4,000 for both methods. Unfortunately, the performance did not improve after either method was applied. This may be attributed to over-fitting training data, because the cross-validation accuracies are indeed higher than those without feature selection. The result may also imply there are little irrelevant features in this case. 5 Conclusion and Future work This paper proposed an automatic approach to locate the GeneRIF sentence in an abstract with the assistance of SVMs, reducing the human effort in updating and maintaining the GeneRIF field </context>
</contexts>
<marker>Chang, Hsu, Chou, 2002</marker>
<rawString>Y.C. I. Chang, H. Hsu and L.Y. Chou. 2002. Graphical Features Selection Method. Intelligent Data Engineering and Automated Learning, Edited by H. Yin, N. Allinson, R. Freeman, J. Keane, and S. Hubband.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Chang</author>
<author>H Schutze</author>
<author>R B Altman</author>
</authors>
<title>GAPSCORE: finding gene and protein names one word at a time.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<pages>20--2</pages>
<contexts>
<context position="9587" citStr="Chang et al., 2004" startWordPosition="1511" endWordPosition="1514">ccurrences of these words in each sentence, given an abstract. Each abstract is then represented by the number of occurrences of these words ni the 9 sentences respectively, i.e., the feature vector is 94,554 in length. Classification based on this type of features is denoted the sentence-wise bag of words model in the rest of this paper. Combining these two models, we got totally 94,563 features. Since we are extracting sentences discussing gene functions, it’s reasonable to expect gene or protein names in the GeneRIF sentence. Therefore, we employed Yapex (Olsson et al., 2002) and GAPSCORE (Chang et al., 2004) protein/gene name detectors to count the number of protein/gene names in each of the 9 sentences, resulting in 94,581 features. 3.3.2 Training SVMs The whole process related to SVM was done via LIBSVM – A Library for Support Vector Machines (Hsu et al., 2003). Radial basis kernel was adopted based on our previous experience. However, further verification showed that the combined model with either linear or polynomial kernel only slightly surpassed the baseline, attaining 50.67% for CD. In order to get the best-performing classifier, we tuned two parameters, C and gamma. They are the penalty c</context>
</contexts>
<marker>Chang, Schutze, Altman, 2004</marker>
<rawString>J.T. Chang, H. Schutze, R.B. Altman. 2004. GAPSCORE: finding gene and protein names one word at a time. Bioinformatics, 20(2):216-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dutoit</author>
<author>Y H Yang</author>
<author>M J Callow</author>
<author>T P Speed</author>
</authors>
<title>Statistical methods for identifying differentially expressed genes in replicated cDNA microarray experiments.</title>
<date>2002</date>
<journal>J. Amer. Statis. Assoc.</journal>
<pages>97--77</pages>
<contexts>
<context position="14279" citStr="Dutoit et al., 2002" startWordPosition="2259" endWordPosition="2262">er combining our weighting scheme and the sentencewise bag of words model (rows 2, 5 and rows 4, 5), we can infer from the performance differences that both models provide mutually exclusive information in the combined model. The result shown in row 6 indicates that the information of gene/protein name occurrences did not help identify the GeneRIF sentences in these 139 test abstracts. We performed feature selection on the combined model to reduce the dimension of feature space. There were two methods applied: a supervised heuristic method (denoted as BWRatio feature selection in Table 2) (S. Dutoit et al., 2002) and another unsupervised method (denoted as Graphical feature selection in Table 2) (Chang et al., 2002). The number of features was then reduced to about 4,000 for both methods. Unfortunately, the performance did not improve after either method was applied. This may be attributed to over-fitting training data, because the cross-validation accuracies are indeed higher than those without feature selection. The result may also imply there are little irrelevant features in this case. 5 Conclusion and Future work This paper proposed an automatic approach to locate the GeneRIF sentence in an abstr</context>
</contexts>
<marker>Dutoit, Yang, Callow, Speed, 2002</marker>
<rawString>S. Dutoit, Y.H. Yang, M.J. Callow and T.P. Speed. 2002. Statistical methods for identifying differentially expressed genes in replicated cDNA microarray experiments. J. Amer. Statis. Assoc. 97:77-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hersh</author>
<author>Ravi Teja Bhupatiraju</author>
</authors>
<title>TREC Genomics Track Overview. TREC</title>
<date>2003</date>
<note>work notes.</note>
<contexts>
<context position="1530" citStr="Hersh and Bhupatiraju, 2003" startWordPosition="218" endWordPosition="221"> proposed in the paper (Hou et al., 2003) were adopted. In contrast, after employing Support Vector Machines (SVMs) and the definition of classes proposed by Jelier et al. (2003), the score greatly improved to 56.86% for Classic Dice (CD). Adopting the same features, SVMs demonstrated advantage over the Naïve Bayes Classifier. Finally, the combination of the former two models attained a score of 59.51% for CD. 1 Introduction Text Retrieval Conference (TREC) has been dedicated to information retrieval and information extraction for years. TREC 2003 introduced a new track called Genomics Track (Hersh and Bhupatiraju, 2003) to address the information retrieval and information extraction issues in the biomedical domain. For the information extraction part, the goal was to automatically reproduce the Gene Reference into Function (GeneRIF) resource in the LocusLink database (Pruitt et al., 2000.) GeneRIF associated with a gene is a sentence describing the function of that gene, and is currently manually generated. This paper presents the post-conference work on the information extraction task (i.e., secondary task). In the official runs, our system (Hou et al., 2003) adopted several weighting schemes (described in </context>
<context position="11198" citStr="Hersh and Bhupatiraju, 2003" startWordPosition="1767" endWordPosition="1770">n when the abstract doesn’t have enough sentences, the sentence with the highest score is picked for the weighting scheme and the combined model, otherwise the title is picked for the sentence-wise bag of words model. 4 Results and Discussions The performance measures are based on Dice coefficient, which calculates the overlap between the candidate GeneRIF and actual GeneRIF. Classic Dice (CD) is the classic Dice formula using a common stop word list and the Porter stemming algorithm. Due to lack of space, we referred you to the Genomics track overview for the other three modifications of CD (Hersh and Bhupatiraju, 2003). The evaluation results are shown in Table 2. The 1st row shows the official run of Jelier’s team, the first place in the official runs. The 2nd row shows the performance when the Naïve Bayes classifier adopted by Jelier is replaced with SVMs. The 3rd row is the performance of our weighting scheme without a classifier. The 4th row then lists the performance when our weighting scheme is combined with SVMs. The 5th row is the result when our weighting scheme and the sentence-wise bag of words model are combined together. The 6th row is the result when two gene/protein name detectors are incorpo</context>
</contexts>
<marker>Hersh, Bhupatiraju, 2003</marker>
<rawString>W. Hersh and Ravi Teja Bhupatiraju. 2003. TREC Genomics Track Overview. TREC 2003 work notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Hou</author>
<author>C Y Teng</author>
<author>C Lee</author>
<author>H H Chen</author>
</authors>
<title>SVM Approach to GeneRIF Annotation.</title>
<date>2003</date>
<booktitle>Proceedings of TREC</booktitle>
<contexts>
<context position="943" citStr="Hou et al., 2003" startWordPosition="130" endWordPosition="133">hou}@nlg.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw Abstract In the biological domain, extracting newly discovered functional features from the massive literature is a major challenging issue. To automatically annotate Gene References into Function (GeneRIF) in a new literature is the main goal of this paper. We tried to find GRIF words in a training corpus, and then applied these informative words to annotate the GeneRIFs in abstracts with several different weighting schemes. The experiments showed that the Classic Dice score is at most 50.18%, when the weighting schemes proposed in the paper (Hou et al., 2003) were adopted. In contrast, after employing Support Vector Machines (SVMs) and the definition of classes proposed by Jelier et al. (2003), the score greatly improved to 56.86% for Classic Dice (CD). Adopting the same features, SVMs demonstrated advantage over the Naïve Bayes Classifier. Finally, the combination of the former two models attained a score of 59.51% for CD. 1 Introduction Text Retrieval Conference (TREC) has been dedicated to information retrieval and information extraction for years. TREC 2003 introduced a new track called Genomics Track (Hersh and Bhupatiraju, 2003) to address t</context>
<context position="5777" citStr="Hou et al., 2003" startWordPosition="870" endWordPosition="873">efinition: The function held by products which directly regulate any step in the process of apoptosis. • GO:0045786 term: negative regulation of cell cycle extraction Figure 1: An Example of Complete Annotation from a Literature to Gene Ontology MEDLINE. “GRIF words” and their weights were derived from the training corpus. Then Support Vector Machines were trained using the derived corpus. Given a new abstract, a sentence is selected from the abstract to be the candidate GeneRIF. 3 Methods We adopted several weighting schemes to locate the GeneRIF sentence in an abstract in the official runs (Hou et al., 2003). Inspired by the work by Jelier et al. (2003), we incorporated their definition of classes into our weighting schemes, converting this task into a classification problem using SVMs as the classifier. We ran SVMs on both sets of features proposed by Hou et al. (2003) and Jelier et al. (2003), respectively. Finally, all the features were combined and some feature selection methods were applied to train the classifier. 3.1 Training and test material preparation Since GeneRIFs are often cited verbatim from abstracts, we decided to reproduce the GeneRIF by selecting one sentence in the abstract. T</context>
<context position="7139" citStr="Hou et al., 2003" startWordPosition="1083" endWordPosition="1086">ice coefficient as similarity measure. Totally, 259,244 abstracts were Figure 2: Architecture of Extracting Candidate GeneRIF used, excluding the abstracts for testing. The test data for evaluation are the 139 abstracts used in TREC 2003 Genomics track. 3.2 GRIF words extraction and weighting scheme We called the matched words between GeneRIF and the selected sentence as GRIF words in this paper. GRIF words represent the favorite vocabulary that human experts use to describe gene functions. After stop word removal and stemming operation, 10,506 GRIF words were extracted. In our previous work (Hou et al., 2003), we first generated the weight for each GRIF word. Given an abstract, the score of each sentence is the sum of weights of all the GRIF words in this sentence. Finally, the sentence with the highest score is selected as the candidate GeneRIF. This method is denoted as OUR weighting scheme, and several heuristic weighting schemes were investigated. Here, we only present the weighting scheme used in SVMs classification. The weighting scheme is as follows. For GRIF word i, the number of occurrence G ni in all the GeneRIF sentences and the number of occurrence A ni in all the abstracts were comput</context>
</contexts>
<marker>Hou, Teng, Lee, Chen, 2003</marker>
<rawString>W.J. Hou, C.Y. Teng, C. Lee and H.H. Chen. 2003. SVM Approach to GeneRIF Annotation. Proceedings of TREC 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C W Hsu</author>
<author>C C Chang</author>
<author>C J Lin</author>
</authors>
<title>A Practical Guide to Support Vector Classification.</title>
<date>2003</date>
<note>http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.ht ml.</note>
<contexts>
<context position="9847" citStr="Hsu et al., 2003" startWordPosition="1555" endWordPosition="1558">res is denoted the sentence-wise bag of words model in the rest of this paper. Combining these two models, we got totally 94,563 features. Since we are extracting sentences discussing gene functions, it’s reasonable to expect gene or protein names in the GeneRIF sentence. Therefore, we employed Yapex (Olsson et al., 2002) and GAPSCORE (Chang et al., 2004) protein/gene name detectors to count the number of protein/gene names in each of the 9 sentences, resulting in 94,581 features. 3.3.2 Training SVMs The whole process related to SVM was done via LIBSVM – A Library for Support Vector Machines (Hsu et al., 2003). Radial basis kernel was adopted based on our previous experience. However, further verification showed that the combined model with either linear or polynomial kernel only slightly surpassed the baseline, attaining 50.67% for CD. In order to get the best-performing classifier, we tuned two parameters, C and gamma. They are the penalty coefficient in optimization and a parameter for the radial basis kernel, respectively. Four-fold cross validation accuracy was used to select the best parameter pair. 3.3.3 Picking up the answe r sentence Test instances were first fed to the classifier to get t</context>
</contexts>
<marker>Hsu, Chang, Lin, 2003</marker>
<rawString>C.W. Hsu, C.C Chang and C.J. Lin. 2003. A Practical Guide to Support Vector Classification. http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.ht ml.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jelier</author>
<author>M Schuemie</author>
<author>C V E Eijk</author>
<author>M Weeber</author>
<author>E V Mulligen</author>
<author>B Schijvenaars</author>
<author>B Mons</author>
<author>J Kors</author>
</authors>
<title>Searching for geneRIFs: conceptbased query expansion and Bayes classification. TREC</title>
<date>2003</date>
<pages>167--174</pages>
<contexts>
<context position="1080" citStr="Jelier et al. (2003)" startWordPosition="151" endWordPosition="154">rom the massive literature is a major challenging issue. To automatically annotate Gene References into Function (GeneRIF) in a new literature is the main goal of this paper. We tried to find GRIF words in a training corpus, and then applied these informative words to annotate the GeneRIFs in abstracts with several different weighting schemes. The experiments showed that the Classic Dice score is at most 50.18%, when the weighting schemes proposed in the paper (Hou et al., 2003) were adopted. In contrast, after employing Support Vector Machines (SVMs) and the definition of classes proposed by Jelier et al. (2003), the score greatly improved to 56.86% for Classic Dice (CD). Adopting the same features, SVMs demonstrated advantage over the Naïve Bayes Classifier. Finally, the combination of the former two models attained a score of 59.51% for CD. 1 Introduction Text Retrieval Conference (TREC) has been dedicated to information retrieval and information extraction for years. TREC 2003 introduced a new track called Genomics Track (Hersh and Bhupatiraju, 2003) to address the information retrieval and information extraction issues in the biomedical domain. For the information extraction part, the goal was to</context>
<context position="2611" citStr="Jelier et al. (2003)" startWordPosition="384" endWordPosition="387">xtraction task (i.e., secondary task). In the official runs, our system (Hou et al., 2003) adopted several weighting schemes (described in Section 3.2) to deal with this problem. However, we failed to beat the simple baseline approach, which always picks the title of a publication as the candidate GeneRIF. Bhalotia et al. (2003) converted this task into a binary classification problem and trained a Naïve Bayes classifier with kernels, achieving 53.04% for CD. In their work, the title and last sentence of an abstract were concatenated and features were then extracted from the resulting string. Jelier et al. (2003) observed the distribution of target GeneRIFs in 9 sentence positions and converted this task into a 9-class classification problem, attaining 57.83% for CD. Both works indicated that the sentence position is of great importance. We therefore modified our system to incorporate the position information with the help of SVMs and we also investigated the capability of SVMs versus Naïve Bayes on this problem. The rest of this paper is organized as follows. Section 2 presents the architecture of our extracting procedure. The basic idea and the experimental methods in this study are introduced in Se</context>
<context position="5823" citStr="Jelier et al. (2003)" startWordPosition="879" endWordPosition="882">ich directly regulate any step in the process of apoptosis. • GO:0045786 term: negative regulation of cell cycle extraction Figure 1: An Example of Complete Annotation from a Literature to Gene Ontology MEDLINE. “GRIF words” and their weights were derived from the training corpus. Then Support Vector Machines were trained using the derived corpus. Given a new abstract, a sentence is selected from the abstract to be the candidate GeneRIF. 3 Methods We adopted several weighting schemes to locate the GeneRIF sentence in an abstract in the official runs (Hou et al., 2003). Inspired by the work by Jelier et al. (2003), we incorporated their definition of classes into our weighting schemes, converting this task into a classification problem using SVMs as the classifier. We ran SVMs on both sets of features proposed by Hou et al. (2003) and Jelier et al. (2003), respectively. Finally, all the features were combined and some feature selection methods were applied to train the classifier. 3.1 Training and test material preparation Since GeneRIFs are often cited verbatim from abstracts, we decided to reproduce the GeneRIF by selecting one sentence in the abstract. Therefore, for each abstract in our training co</context>
<context position="8034" citStr="Jelier et al. (2003)" startWordPosition="1240" endWordPosition="1243">ighting scheme, and several heuristic weighting schemes were investigated. Here, we only present the weighting scheme used in SVMs classification. The weighting scheme is as follows. For GRIF word i, the number of occurrence G ni in all the GeneRIF sentences and the number of occurrence A ni in all the abstracts were computed and A n i / n was assigned to GRIF word i G i as its weight. 3.3 Classification 3.3.1 Class definition and feature extraction The distribution of GeneRIF sentences showed that the position of a sentence in an abstract is an important clue to where the answer sentence is. Jelier et al. (2003) considered only the title, the first three and the last five sentences, achieving the best performance in TREC official runs. Their Naïve Bayes model is as follows. An abstract a is assigned a class vj by calculating vNB: GeneRIF Sentence Locator GRIF Word Extractor Weighted GRIF Words Existing GeneRIFs on LocusLink Training SVMs Generating Training Data Candidate GeneRIF New Abstract Corresponding Medline Abstracts 55 vNB = argmax P(vj) X F1 F1 P(wk i |vj ) v V je i S k W e e a , i where vj is one of the nine positions aforementioned, S is the set of 9 sentence positions, Wa,i is the set of </context>
</contexts>
<marker>Jelier, Schuemie, Eijk, Weeber, Mulligen, Schijvenaars, Mons, Kors, 2003</marker>
<rawString>R. Jelier, M. Schuemie, C.V.E. Eijk, M. Weeber, E.V. Mulligen, B. Schijvenaars, B. Mons and J. Kors. 2003. Searching for geneRIFs: conceptbased query expansion and Bayes classification. TREC 2003 work notes: 167-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>Proceedings of ECML-98,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="12150" citStr="Joachims, 1998" startWordPosition="1928" endWordPosition="1929">n lists the performance when our weighting scheme is combined with SVMs. The 5th row is the result when our weighting scheme and the sentence-wise bag of words model are combined together. The 6th row is the result when two gene/protein name detectors are incorporated into the combined model. The next two rows were obtained after two feature selection methods were applied. The 9th row shows the performance when the classifier always proposes a sentence most similar to the actual GeneRIF. The last row lists the baseline, i.e., title is always picked. A comparative study on text categorization (Joachims, 1998) showed that SVMs outperform other classification methods, such as Naïve Bayes, C4.5, and k-NN. The reasons would be that SVMs are capable of handling large feature space, text categorization has few irrelevant features, and document vectors are sparse. The comparison CD MUD MBD MBDP 1 Jelier (Sentence-wise bag of words + Naïve Bayes) 57.83% 59.63% 46.75% 49.11% 2 Sentence-wise bag of words + SVMs 58.92% 61.46% 47.86% 50.84% 3 OUR Weighting scheme 50.18% 46.71% 33.47% 38.83% 4 OUR Weighting scheme + SVMs 56.86% 58.81% 45.08% 48.10% 5 Combined 59.51% 62.16% 48.17% 51.25% 6 Combined + gene/prote</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. Proceedings of ECML-98, 137-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Olsson</author>
<author>G Eriksson</author>
<author>K Franzén</author>
<author>L Asker</author>
<author>P Lidén</author>
</authors>
<date>2002</date>
<booktitle>Notions of Correctness when Evaluating Protein Name Taggers. Proceedings of the 19th International Conference on Computational Linguistics</booktitle>
<pages>765--771</pages>
<contexts>
<context position="9553" citStr="Olsson et al., 2002" startWordPosition="1505" endWordPosition="1508">rds at hand, we also computed the occurrences of these words in each sentence, given an abstract. Each abstract is then represented by the number of occurrences of these words ni the 9 sentences respectively, i.e., the feature vector is 94,554 in length. Classification based on this type of features is denoted the sentence-wise bag of words model in the rest of this paper. Combining these two models, we got totally 94,563 features. Since we are extracting sentences discussing gene functions, it’s reasonable to expect gene or protein names in the GeneRIF sentence. Therefore, we employed Yapex (Olsson et al., 2002) and GAPSCORE (Chang et al., 2004) protein/gene name detectors to count the number of protein/gene names in each of the 9 sentences, resulting in 94,581 features. 3.3.2 Training SVMs The whole process related to SVM was done via LIBSVM – A Library for Support Vector Machines (Hsu et al., 2003). Radial basis kernel was adopted based on our previous experience. However, further verification showed that the combined model with either linear or polynomial kernel only slightly surpassed the baseline, attaining 50.67% for CD. In order to get the best-performing classifier, we tuned two parameters, C</context>
</contexts>
<marker>Olsson, Eriksson, Franzén, Asker, Lidén, 2002</marker>
<rawString>F. Olsson, G. Eriksson, K. Franzén, L. Asker and P. Lidén. 2002. Notions of Correctness when Evaluating Protein Name Taggers. Proceedings of the 19th International Conference on Computational Linguistics 2002, 765-771.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K D Pruitt</author>
<author>K S Katz</author>
<author>H Sicotte</author>
<author>D R Maglott</author>
</authors>
<title>Introducing RefSeq and LocusLink: Curated Human Genome Resources at the NCBI. Trends Genet,</title>
<date>2000</date>
<pages>16--1</pages>
<contexts>
<context position="1803" citStr="Pruitt et al., 2000" startWordPosition="257" endWordPosition="260">ted advantage over the Naïve Bayes Classifier. Finally, the combination of the former two models attained a score of 59.51% for CD. 1 Introduction Text Retrieval Conference (TREC) has been dedicated to information retrieval and information extraction for years. TREC 2003 introduced a new track called Genomics Track (Hersh and Bhupatiraju, 2003) to address the information retrieval and information extraction issues in the biomedical domain. For the information extraction part, the goal was to automatically reproduce the Gene Reference into Function (GeneRIF) resource in the LocusLink database (Pruitt et al., 2000.) GeneRIF associated with a gene is a sentence describing the function of that gene, and is currently manually generated. This paper presents the post-conference work on the information extraction task (i.e., secondary task). In the official runs, our system (Hou et al., 2003) adopted several weighting schemes (described in Section 3.2) to deal with this problem. However, we failed to beat the simple baseline approach, which always picks the title of a publication as the candidate GeneRIF. Bhalotia et al. (2003) converted this task into a binary classification problem and trained a Naïve Baye</context>
</contexts>
<marker>Pruitt, Katz, Sicotte, Maglott, 2000</marker>
<rawString>K.D. Pruitt, K.S. Katz, H. Sicotte and D.R. Maglott. 2000. Introducing RefSeq and LocusLink: Curated Human Genome Resources at the NCBI. Trends Genet, 16(1):44-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sekimizu</author>
<author>H S Park</author>
<author>J Tsujji</author>
</authors>
<date>1998</date>
<booktitle>Identifying the Interaction Between Genes and Gene Products Based on Frequently Seen Verbs in Medline Abstracts. Genome Information,</booktitle>
<pages>9--62</pages>
<marker>Sekimizu, Park, Tsujji, 1998</marker>
<rawString>T. Sekimizu, H.S. Park and J. Tsujji. 1998. Identifying the Interaction Between Genes and Gene Products Based on Frequently Seen Verbs in Medline Abstracts. Genome Information, 9:62-71</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>