<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996737">
Learning Nonstructural Distance Metric
by Minimum Cluster Distortions
</title>
<author confidence="0.710671">
Daichi Mochihashi, Genichiro Kikui
</author>
<note confidence="0.367801">
ATR Spoken Language Translation
research laboratories
Hikaridai 2-2-2, Keihanna Science City
Kyoto 619-0288, Japan
</note>
<email confidence="0.9836465">
daichi.mochihashi@atr.jp
genichiro.kikui@atr.jp
</email>
<sectionHeader confidence="0.993766" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978368421053">
Much natural language processing still depends on
the Euclidean (cosine) distance function between
two feature vectors, but this has severe problems
with regard to feature weightings and feature cor-
relations. To answer these problems, we propose an
optimal metric distance that can be used as an alter-
native to the cosine distance, thus accommodating
the two problems at the same time. This metric is
optimal in the sense of global quadratic minimiza-
tion, and can be obtained from the clusters in the
training data in a supervised fashion.
We confirmed the effect of the proposed metric
distance by a synonymous sentence retrieval task,
document retrieval task and the K-means clustering
of general vectorial data. The results showed con-
stant improvement over the baseline method of Eu-
clid and tf.idf, and were especially prominent for
the sentence retrieval task, showing a 33% increase
in the 11-point average precision.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99982395">
Natural language processing involves many kinds of
linguistic expressions, such as sentences, phrases,
documents and the collection of documents. Com-
paring these expressions based on semantic proxim-
ity is a fundamental task and has many applications.
Generally, two basic approaches exist to compare
two expressions: (a) structural and (b) nonstruc-
tural. Structural approaches make use of syntactic
parsing or dependency analysis to make a rigorous
comparison; nonstructural approaches use vector
representation and provide a rough but fast compar-
ison that is required for search/retrieval from a vast
amount of corpora. While structural approaches
have recently become available in a kernel-based
sophisticated treatment (Collins and Duffy, 2001;
Suzuki et al., 2003), here we concentrate on non-
structural comparison. This is not only because non-
structural comparison constitutes an integral part
in structural methods (that is, even in hierarchi-
cal methods the leaf comparison is still atomic),
</bodyText>
<note confidence="0.769148">
Kenji Kita
Center for Advanced Information
Technology, Tokushima University
Minamijosanjima 2-1
Tokushima 770-8506, Japan
</note>
<email confidence="0.989692">
kita@is.tokushima-u.ac.jp
</email>
<bodyText confidence="0.999971823529412">
but because it is frequently embedded in many ap-
plications where structural parsings are not avail-
able or computationally too expensive. For exam-
ple, information retrieval has long used the ‘bag
of words’ approach (Baeza-Yates and Ribeiro-Neto,
1999; Sch¨utze, 1992) mainly due to a lack of scal-
able segmentation algorithms and the huge amount
of data involved. While segmentation algorithms,
such as TEXTTILING (Hearst, 1994) and its recent
successors using the inter-paragraph similarity ma-
trix (Choi, 2000), all themselves use nonstructural
cosine similarity as a measure of semantic proxim-
ity between paragraphs.
However, the distance function so far has been
largely defined and used ad hoc, usually by a tf.idf
weighting scheme (Salton and Yang, 1973) and a
simple cosine similarity, equivalently, an Euclidean
dot product. In this paper, we propose an optimal
distance function that is parameterized by a global
metric matrix. This metric is optimal in the sense of
global quadratic minimization, and can be learned
from the given clusters in the training data. These
clusters are often attributable with many forms, such
as paragraphs, documents or document collections,
as long as the items in the training data are not com-
pletely independent.
This paper is organized as follows. In section 2
we describe the issue of traditional Euclidean dis-
tances, and section 3 places it into general perspec-
tive with related works in machine learning. Section
4 introduces the proposed metric, and section 5 vali-
dates its effect on the task of sentence retrieval, doc-
ument retrieval and the K-means clustering. Sec-
tions 6 and 7 present discussions and the conclusion.
</bodyText>
<sectionHeader confidence="0.972614" genericHeader="introduction">
2 Issues with Euclidean distances
</sectionHeader>
<bodyText confidence="0.9990638">
When we address nonstructural matching, linguis-
tic expressions are often modeled by a feature vec-
tor x� E R&apos;, with its elements x1 ... x,,, correspond-
ing to the number of occurrences of i’th feature. If
features are simply words, this is called a ‘bag of
words’; but in general, features are not restricted to
this kind, and we will use the general term “feature”
in the rest of the paper.
To measure the distance between two vectors
u, v, a dot product or Euclidean distance
</bodyText>
<equation confidence="0.734124">
d(u, v)2 = (u − v)T (u − v) (1)
= �ni=1(ui − vi)2
</equation>
<bodyText confidence="0.9903628">
(where T denotes a transposition) has been em-
ployed so far 1, with a heuristic feature weighting
such as tf.idf in a preprocessing stage.
However, there are two main problems with this
distance:
</bodyText>
<listItem confidence="0.9970865">
(1) The correlation between features is ignored.
(2) Feature weighting is inevitably arbitrary.
</listItem>
<bodyText confidence="0.9759743125">
Problem (1) is especially important in languages,
because linguistic features (e.g., words) generally
have strong correlations between them, such as col-
locations or typical constructions. But this correla-
tion cannot be considered in a simple dot product.
While it is possible to address this with a specific
kernel function, such as polynomials (M¨uller et al.,
2001), this is not available for many problems, such
as information retrieval or question answering, that
do not fit classifications or cannot be easily “kernel-
ized”. Problem (2) is a more subtle but inherent one:
while tf.idf often works properly in practice, there
are several options, especially in tf such as logs or
square roots, but we have no principle with which
to choose from. Further, it has no theoretical basis
that gives any optimality as a distance function.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="method">
3 Related Works
</sectionHeader>
<bodyText confidence="0.9984666">
The issues above of feature correlations and fea-
ture weightings can be summarized as a problem of
defining an appropriate metric in the feature space,
based on the distribution of data. This problem has
recently been highlighted in the field of machine
learning research. (Xing et al., 2002) has an ob-
jective that is quite similar to that of this paper, and
gives a metric matrix that resembles ours based on
sample pairs of “similar points” as training data.
(Bach and Jordan, 2004) and (Schultz and Joachims,
2004) seek to answer the same problem with an ad-
ditional scenario of spectral clustering and relative
comparisons in Support Vector Machines, respec-
tively. In this aspect, our work is a straight succes-
sor of (Xing et al., 2002) where its general usage
in vector space is preserved. We offer a discussion
on the similarity to our method and our advantages
&apos;When we normalize the length of the vectors |u |= |v |= 1
as commonly adopted, (u − v)T (u − v) = |u|2 + |v|2 − 2u ·
v  −u · v = − cos(u, v) ; therefore, this includes a cosine
similarity (Manning and Sch¨utze, 1999).
in section 6. Finally, we note that the Fisher ker-
nel of (Jaakkola and Haussler, 1999) has the same
concept that gives an appropriate similarity of two
data through the Fisher information matrix obtained
from the empirical distribution of data. However, it
is often approximated by a unit matrix because of
its heavy computational demand.
In the field of information retrieval, (Jiang and
Berry, 1998) proposes a Riemannian SVD (R-SVD)
from the viewpoint of relevance feedback. This
work is close in spirit to our work, but is not aimed
at defining a permanent distance function and does
not utilize cluster structures existent in the training
data.
</bodyText>
<sectionHeader confidence="0.987969" genericHeader="method">
4 Defining an Optimal Metric
</sectionHeader>
<bodyText confidence="0.999866">
To solve the problems in section 2, we note the func-
tion that synonymous clusters play. There are many
levels of (more or less) synonymous clusters in lin-
guistic data: phrases, sentences, paragraphs, docu-
ments, and, in a web environment, the site that con-
tains the document. These kinds of clusters can of-
ten be attributed to linguistic expressions because
they nest in general so that each expression has a
parent cluster.
Since these clusters are synonymous, we can ex-
pect the vectors in each cluster to concentrate in the
ideal feature space. Based on this property, we can
introduce an optimal weighting and correlation in a
supervised fashion. We will describe this method
below.
</bodyText>
<subsectionHeader confidence="0.998471">
4.1 The Basic Idea
</subsectionHeader>
<bodyText confidence="0.9896026">
As stated above, vectors in the same cluster must
have a small distance between each other in the ideal
geometry. When we measure an L2-distance be-
tween u and v by a Mahalanobis distance param-
eterized by M:
</bodyText>
<equation confidence="0.999484333333333">
dM (u, v)2 = (u − v)TM(u − v) (2)
= �n�n j=1 mij(ui − vi)(uj − vj),
i=1
</equation>
<bodyText confidence="0.997197166666667">
where symmetric metric matrix M gives both cor-
responding feature weights and feature correlations.
When we take M = I (unit matrix), we recover the
original Euclidean distance (1).
Equation (2) can be rewritten as (3) because M is
symmetric:
</bodyText>
<equation confidence="0.583898">
dM(u, v)2 = (M1/2(u −v))T (M1/2(u − v)). (3)
</equation>
<bodyText confidence="0.999305689655172">
Therefore, this distance amounts to a Euclidean dis-
tance in M1/2-mapped space (Xing et al., 2002).
Note that this distance is global, and different
from the ordinary Mahalanobis distance in pattern
recognition (for example, (Duda et al., 2000)) that is
defined for each cluster one by one, using a cluster-
specific covariance matrix. That type of distance
cannot be generalized to new kinds of data; there-
fore, it has been used for local classifications. What
we want is a global distance metric that is generally
useful, not a measure for classification to predefined
clusters. In this respect, (Xing et al., 2002) shares
the same objective as ours.
Therefore, we require an optimization over all the
clusters in the training data. Generally, data in the
clusters are distributed as in figure 1(a), comprising
ellipsoidal forms that have high (co)variances for
some dimensions and low (co)variances for other di-
mensions. Further, the cluster is not usually aligned
to the axes of coordinates. When we find a global
metric matrix M that minimizes the cluster distor-
tions, namely, one that reduces high variances and
expands low variances for the data to make a spher-
ical form as good as possible in the M1/2-mapped
space (figure 1(b)), we can expect it to capture nec-
essary and unnecessary variations and correlations
on the features, combining information from many
clusters to produce a more reliable metric that is not
locally optimal. We will find this optimal M below.
</bodyText>
<figureCaption confidence="0.998298">
Figure 1: Geometry of feature space.
</figureCaption>
<subsectionHeader confidence="0.994928">
4.2 Global optimization over clusters
</subsectionHeader>
<bodyText confidence="0.979256">
Suppose that each data (for example, sentences or
documents) is a vector s  ][8n, and the whole cor-
pus can be divided into N clusters, X1 ... XN. That
is, each vector has a dimension n, and the number of
clusters is N. For each cluster Xi, cluster centroid
ci is calculated as ci = 1/|Xi |sXi s, where |X|
denotes the number of data in X. When necessary,
each element in sj or ci is referenced as sjk or cik
(k = 1... n).
The basic idea above is formulated as follows.
We seek the metric matrix M that minimizes the
distance between each data sj and the cluster cen-
troid ci, dM(sj,ci) for all clusters X1 ... XN.
Mathematically, this is formulated as a quadratic
minimization problem
</bodyText>
<equation confidence="0.99175725">
dM(sj,ci)2
(sj −ci)T M(sj −ci) (4)
under a scale constraint ( |·  |means determinant)
|M |= 1. (5)
</equation>
<bodyText confidence="0.998769142857143">
Scale constraint (5) is necessary for excluding a
degenerate solution M = O. 1 is an arbitrary con-
stant: when we replace 1 by c, c2M becomes a new
solution. This minimization problem is an exten-
sion to the method of MindReader (Ishikawa et al.,
1998) to multiple clusters, and has a unique solution
below.
</bodyText>
<equation confidence="0.649258333333333">
Theorem The matrix that solves the minimization
problem (4,5) is
M = |A|1/nA−1, (6)
</equation>
<bodyText confidence="0.768652">
where A = [akl] is defined by
</bodyText>
<equation confidence="0.5620695">
(sjl − cil)(sjk − cik) . (7)
Proof: See Appendix A.
</equation>
<bodyText confidence="0.984697833333333">
When A is singular, we can use as A−1 a Moore-
Penrose matrix pseudoinverse A+. Generally, A
consists of linguistic features and is very sparse, and
often singular. Therefore, A+ is nearly always nec-
essary for the above computation. For details, see
Appendix B.
</bodyText>
<subsectionHeader confidence="0.994023">
4.3 Generalization
</subsectionHeader>
<bodyText confidence="0.999367">
While we assumed through the above construction
that each cluster is equally important, this is not
the case in general. For example, clusters with a
small number of data may be considered weak, and
in the hierarchical clustering situation, a “grand-
mother” cluster may be weaker. If we have con-
fidences 1 ... N for the strength of clustering for
each cluster X1 ... XN, this information can be in-
corporated into (4) by a set of normalized cluster
</bodyText>
<equation confidence="0.901862666666667">
weights i :
M = arg min N  i (sj −ci)TM(sj − ci),
M i=1 sjXi
</equation>
<bodyText confidence="0.999854714285714">
where i = i/ N j=1 j , and we obtain a respec-
tively weighted solution in (7). Further, we note that
when N = 1, this metric recovers the ordinary Ma-
halanobis distance in pattern recognition. However,
we used equal weights for the experiments below
because the number of data in each cluster was ap-
proximately equal.
</bodyText>
<figure confidence="0.997673714285714">
High
covariance
xa
xa
Low
variance
x2
x2
High
variance
x1
(a) Original space
x1
(b) Mapped space
</figure>
<equation confidence="0.946067058823529">
M = arg min
= arg min
M
N

M i=1

sjXi

N
i=1
sjXi
N
i=1
akl =

sjXi
</equation>
<sectionHeader confidence="0.988533" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999987545454546">
We evaluated our metric distance on the three tasks
of synonymous sentence retrieval, document re-
trieval, and the K-means clustering of general vec-
torial data. After calculating M on the training data
of clusters, we applied it to the test data to see how
well its clusters could be recovered. As a measure of
cluster recovery, we use 11-point average precision
and R-precision for the distribution of items of the
same cluster in each retrieval result. Here, R equals
the cardinality of the cluster; therefore, R-precision
shows the precision of cluster recovery.
</bodyText>
<subsectionHeader confidence="0.985725">
5.1 Synonymous sentence retrieval
5.1.1 Sentence cluster corpus
</subsectionHeader>
<bodyText confidence="0.999976875">
We used a paraphrasing corpus of travel conversa-
tions (Sugaya et al., 2002) for sentence retrieval.
This corpus consists of 33,723,164 Japanese trans-
lations, each of which corresponds to one of the
original English sentences. By way of this cor-
respondence, Japanese sentences are divided into
10,610 clusters. Therefore, each cluster consists
of Japanese sentences that are possible translations
from the same English seed sentence that the clus-
ter has. From this corpus, we constructed 10 sets
of data. Each set contains random selection of 200
training clusters and 50 test clusters, and each clus-
ter contains a maximum of 100 sentences 2. Ex-
periments were conducted on these 10 datasets for
each level of dimensionality reduction (see below)
to produce average statistics.
</bodyText>
<subsubsectionHeader confidence="0.985772">
5.1.2 Features and dimensionality reduction
</subsubsectionHeader>
<bodyText confidence="0.99987225">
As a feature of a sentence, we adopted unigrams of
all words and bigrams of functional words from the
part-of-speech tags, because the sequence of func-
tional words is important in the conversational cor-
pus.
While the lexicon is limited for travel conversa-
tions, the number of features exceeds several thou-
sand or more. This may be prohibitive for the calcu-
lation of the metric matrix, therefore, we addition-
ally compressed the features with SVD, the same
method used in Latent Semantic Indexing (Deer-
wester et al., 1990).
</bodyText>
<subsectionHeader confidence="0.613672">
5.1.3 Sentence retrieval results
</subsectionHeader>
<bodyText confidence="0.9994162">
Qualitative result Figure 5 (last page) shows a sam-
ple retrieval result. A sentence with (*) mark at
the end is the correct answer, that is, a sentence
from the same original cluster as the query. We can
see that the results with the metric distance contain
</bodyText>
<footnote confidence="0.909262">
2When the number of data in the cluster exceeds this limit,
100 sentences are randomly sampled. All sampling are made
without replacement.
</footnote>
<bodyText confidence="0.999567071428571">
less noise than a standard Euclid baseline with tf.idf
weighting, achieving a high-precision retrieval. Al-
though the high rate of dimensionality reduction in
figure 6 shows degradation due to the dimension
contamination, the effect of metric distance is still
apparent despite bad conditions.
Quantitative result Figure 2 shows the averaged
precision-recall curves of retrieval and figure 3
shows 11-point average precisions, for each rate
of dimensionality reduction. Clearly, our method
achieves higher precision than the standard method,
and does not degrade much with feature compres-
sions unless we reduce the dimension too much, i.e.,
to &lt; 5%.
</bodyText>
<figureCaption confidence="0.965597">
Figure 3: 11-point average precision.
</figureCaption>
<subsectionHeader confidence="0.997833">
5.2 Document retrieval
</subsectionHeader>
<bodyText confidence="0.999932384615384">
As a method of tackling clusters of texts, the text
classification task has recently made great advances
with a Naive Bayes or SVM classifiers (for exam-
ple, (Joachims, 1998)). However, they all aim at
classifying texts into a few predefined clusters, and
cannot deal with a document that fits neither of the
clusters. For example, when we regard a website as
a cluster of documents, the possible clusters are nu-
merous and constantly increasing, which precludes
classificatory approaches. For these circumstances,
document clustering or retrieval will benefit from a
global distance metric that exploits the multitude of
cluster structures themselves.
</bodyText>
<subsubsectionHeader confidence="0.599533">
5.2.1 Newsgroup text dataset
</subsubsectionHeader>
<bodyText confidence="0.999749666666667">
For this purpose, we used the 20-Newsgroup dataset
(Lang, 1995). This is a standard text classification
dataset that has a relatively large number of classes,
</bodyText>
<figure confidence="0.9940635">
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall Recall
(a) Metric distance + (b) Euclidean + idf
idf
</figure>
<figureCaption confidence="0.756439">
Figure 2: Precision-recall of sentence retrieval.
</figureCaption>
<figure confidence="0.987806162162162">
0 5 10 15 20 25 30 35 40 45 50
Dimension Reduction(%)
Precision
0.8
0.6
0.4
0.2
0
1
1%
5%
10%
20%
50%
Precision
0.8
0.6
0.4
0.2
0
1
1%
5%
10%
20%
50%
Precision
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Metric distance +idf
Euclidean distance +idf
</figure>
<bodyText confidence="0.9982048">
20. Among the 20 newsgroups, we selected 16 clus-
ters of training data and 4 clusters of test data, and
performed 5-fold cross validation. The maximum
number of documents per cluster is 100, and when
it exceeds this limit, we made a random sampling of
100 documents as the sentence retrieval experiment.
Because our proposed metric is calculated from
the distribution of vectors in high-dimensional fea-
ture space, it becomes inappropriate if the norm
of the vectors (largely proportional to document
length) differs much from document to document.
3 Therefore, we used subsampling/oversampling to
form a median length (130 words) on training docu-
ments. Further, we preprocessed them with tf.idf as
a baseline method.
</bodyText>
<sectionHeader confidence="0.867453" genericHeader="method">
5.2.2 Results
</sectionHeader>
<bodyText confidence="0.9996787">
Table 1 shows R-precision and 11-point average
precision. Since the test data contains 4 clusters,
the baselines of precision are 0.25. We can see from
both results that metric distance produces a better
retrieval over the tf.idf and dot product. However,
refinements in precision are certain (average p =
0.0243) but subtle.
This can be thought of as the effect of the dimen-
sionality reduction performed. We first decompose
data matrix X by SVD: X = USV −1 and build
a k-dimensional compressed representation Xk =
VkX; where Vk denotes a k-largest submatrix of V .
From the equation (3), this means a Euclidean dis-
tance of M1/2Xk = M1/2VkX. Therefore, Vk may
subsume the effect of M in a preprocessing stage.
Close inspection of table 1 shows this effect as a
tradeoff between M and Vk. To make the most of
metric distance, we should consider metric induc-
tion and dimensionality reduction simultaneously,
or reconsider the problem in kernel Hilbert space.
</bodyText>
<table confidence="0.9991593">
Dim. Metric R-precision 11-pt Avr. Prec.
Red. Euclid Metric Euclid
0.5% 0.421 0.399 0.476 0.455
1% 0.388 0.368 0.450 0.430
2% 0.359 0.343 0.425 0.409
3% 0.344 0.330 0.411 0.399
4% 0.335 0.323 0.402 0.392
5% 0.329 0.318 0.397 0.388
10% 0.316 0.307 0.379 0.376
20% 0.343 0.297 0.397 0.365
</table>
<tableCaption confidence="0.865434166666667">
Table 1: Newsgroup text retrieval results.
3Normalizing documents to unit length effectively maps
them to a high-dimensional hypersphere; this proved to pro-
duce an unsatisfactory result. Defining metrics that work on
a hypersphere like spherical K-means (Dhillon and Modha,
2001) requires further research.
</tableCaption>
<subsectionHeader confidence="0.86578">
5.3 K-means clustering and general vectorial
data
</subsectionHeader>
<bodyText confidence="0.999930571428571">
Metric distance can also be used for clustering or
general vectorial data. Figure 4 shows the K-means
clustering result of applying our metric distance to
some of the UCI Machine Learning datasets (Blake
and Merz, 1998). K-means clustering was con-
ducted 100 times with a random start, where K
equals the known number of classes in the data 4.
Clustering precision was measured as an average
probability that a randomly picked pair of data will
conform to the true clustering (Xing et al., 2002).
We also conducted the same clustering for doc-
uments of the 20-Newsgroup dataset to get a small
increase in precision like the document retrieval ex-
periment in section 5.2.
</bodyText>
<figure confidence="0.99693075">
0.9
0.9
0.8
0.8
0.7
0.7
0.6
1 2 5 10 15 20
Dimension
(a) “wine” dataset (b) “protein” dataset
1
0.9
0.9
0.8
0.8
0.7
0.7
1 2 5 10 20 3 5
Dimension
(c) “iris” dataset (d) “soybean” dataset
</figure>
<figureCaption confidence="0.994999">
Figure 4: K-means clustering of UCI Machine
</figureCaption>
<bodyText confidence="0.9236984">
Learning dataset results. The horizontal axis shows
compressed dimensions (rightmost is original). The
right bar shows clustering precision using Metric
distance, and the left bar shows that using Euclidean
distance.
</bodyText>
<sectionHeader confidence="0.998963" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999765125">
In this paper, we proposed an optimal distance met-
ric based on the idea of minimum cluster distortion
in training data. Although vector distances have fre-
quently been used in natural language processing,
this is a rather neglected but recently highlighted
problem. Unlike recently proposed methods with
spectral methods or SVMs, our method assumes no
such additional scenarios and can be considered as
</bodyText>
<footnote confidence="0.9316905">
4Because of the small size of the dataset, we did not apply
cross-validation as in other experiments.
</footnote>
<figure confidence="0.519175083333333">
Precision
Precision
0.6
1
1
Precision
1 2 3 4
Dimension
Precision
1 2 5 10 13
Dimension
0.6
</figure>
<bodyText confidence="0.999735071428571">
a straight successor to (Xing et al., 2002)’s work.
Their work has the same perspective as ours, and
they calculate a metric matrix A that is similar to
ours based on a set 5 of vector pairs (9i, xj) that can
be regarded as similar. They report that the effec-
tiveness of A increases as the number of the training
pairs 5 increases; this requires O(n2) sample points
from n training data, and must be optimized by a
computationally expensive Newton-Raphson itera-
tion. On the other hand, our method uses only linear
algebra, and can induce an ideal metric using all the
training data at the same time. We believe this met-
ric can be useful for many vector-based language
processing methods that have used cosine similar-
ity.
There remains some future directions for re-
search. First, as we stated in section 4.3, the effect
of a cluster weighted generalized metric must be in-
vestigated and optimal weighting must be induced.
Second, as noted in section 5.2.1, the dimensional-
ity reduction required for linguistic data may con-
strain the performance of the metric distance. To
alleviate this problem, simultaneous dimensionality
reduction and metric induction may be necessary, or
the same idea in a kernel-based approach is worth
considering. The latter obviates the problem of di-
mensionality, while it restricts the usage to a situa-
tion where the kernel-based approach is available.
</bodyText>
<sectionHeader confidence="0.99216" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999825444444444">
We proposed a global metric distance that is use-
ful for clustering or retrieval where Euclidean dis-
tance has been used. This distance is optimal in the
sense of quadratic minimization over all the clus-
ters in the training data. Experiments on sentence
retrieval, document retrieval and K-means cluster-
ing all showed improvements over Euclidean dis-
tance, with a significant refinement with tight train-
ing clusters in sentence retrieval.
</bodyText>
<sectionHeader confidence="0.818416" genericHeader="method">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9998704">
The research reported here was supported in part by
a contract with the National Institute of Information
and Communications Technology entitled ”A study
of speech dialogue translation technology based on
a large corpus”.
</bodyText>
<sectionHeader confidence="0.992331" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.984003298850575">
Francis R. Bach and Michael I. Jordan. 2004.
Learning Spectral Clustering. In Advances in
Neural Information Processing Systems 16. MIT
Press.
Ricardo A. Baeza-Yates and Berthier A. Ribeiro-
Neto. 1999. Modern Information Retrieval.
ACM Press / Addison-Wesley.
C. L. Blake and C. J. Merz. 1998. UCI
Repository of machine learning databases.
http://www.ics.uci.edu/˜mlearn/MLRepository.html.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings
ofNAACL-00.
Michael Collins and Nigel Duffy. 2001. Convo-
lution Kernels for Natural Language. In NIPS
2001.
S. Deerwester, Susan T. Dumais, and George W.
Furnas. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society of In-
formation Science, 41(6):391–407.
Inderjit S. Dhillon and Dharmendra S. Modha.
2001. Concept Decompositions for Large Sparse
Text Data Using Clustering. Machine Learning,
42(1/2):143–175.
Richard O. Duda, Peter E. Hart, and David G. Stork.
2000. Pattern Classification *Second Edition.
John Wiley &amp; Sons.
Marti Hearst. 1994. Multi-paragraph segmentation
of expository text. In 32nd. Annual Meeting of
the Association for Computational Linguistics,
pages 9–16.
Yoshiharu Ishikawa, Ravishankar Subramanya, and
Christos Faloutsos. 1998. MindReader: Query-
ing Databases Through Multiple Examples. In
Proc. 24th Int. Conf. Very Large Data Bases,
pages 218–227.
Tommi S. Jaakkola and David Haussler. 1999. Ex-
ploiting generative models in discriminative clas-
sifiers. In Proc. of the 1998 Conference on Ad-
vances in Neural Information Processing Sys-
tems, pages 487–493.
Eric P. Jiang and Michael W. Berry. 1998. Infor-
mation Filtering Using the Riemannian SVD (R-
SVD). In Proc. of IRREGULAR ’98, pages 386–
395.
Thorsten Joachims. 1998. Text categorization with
support vector machines: learning with many
relevant features. In Proceedings of ECML-98,
number 1398, pages 137–142.
Ken Lang. 1995. Newsweeder: Learning to filter
netnews. In Proceedings of the Twelfth Interna-
tional Conference on Machine Learning, pages
331–339.
Christopher D. Manning and Hinrich Sch¨utze.
1999. Foundations of Statistical Natural Lan-
guage Processing. MIT Press.
K. R. M¨uller, S. Mika, G. Ratsch, and K. Tsuda.
2001. An introduction to kernel-based learning
algorithms. IEEE Neural Networks, 12(2):181–
201.
G. Salton and C. S. Yang. 1973. On the specifica-
tion of term values in automatic indexing. Jour-
nal ofDocumentation, 29:351–372.
Matthew Schultz and Thorsten Joachims. 2004.
Learning a Distance Metric from Relative Com-
parisons. In Advances in Neural Information
Processing Systems 16. MIT Press.
Hinrich Sch¨utze. 1992. Dimensions of Mean-
ing. In Proceedings ofSupercomputing’92, pages
787–796.
F. Sugaya, T. Takezawa, G. Kikui, and S. Ya-
mamoto. 2002. Proposal for a very-large-corpus
acquisition method by cell-formed registration.
In Proc. LREC-2002, volume I, pages 326–328.
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and
Eisaku Maeda. 2003. Hierarchical Directed
Acyclic Graph Kernel: Methods for Structured
Natural Language Data. In Proc. of the 41th An-
nual Meeting of Association for Computational
Linguistics (ACL2003), pages 32–39.
Eric W. Weisstein. 2004. Moore-Penrose Matrix
Inverse. http://mathworld.wolfram.com/Moore-
PenroseMatrixInverse.html.
Eric P. Xing, Andrew Y. Ng, Michael I. Jordan,
and Stuart Russell. 2002. Distance metric learn-
ing, with application to clustering with side-
information. In NIPS 2002.
</reference>
<sectionHeader confidence="0.967842" genericHeader="method">
Appendix A.
</sectionHeader>
<subsectionHeader confidence="0.708423">
Derivation of the metric matrix
</subsectionHeader>
<bodyText confidence="0.999453">
Here we prove theorem 1, namely deriving M that
satisfies the condition
</bodyText>
<equation confidence="0.9499062">
 (sj −ci)TM(sj − ci) , (8)
sjXi
under the constraint
Therefore
(−1)k+lmkl|Mkl |= n, (11)
</equation>
<bodyText confidence="0.985986833333333">
where Mkl denotes an adjugate matrix of mkl.
Therefore, we come to minimize (10) under the
constraint (11).
By introducing the Lagrange multiplier , we de-
fine
Differentiating by mkl and setting to zero, we obtain
</bodyText>
<equation confidence="0.95356175">
(sjk − cik)(sjl − cil)
− (−1)k+l|Mkl |= 0
 |Mkl |= i sj (sjk − cik)(sjl − cil) . (12)
(−1)k+l
</equation>
<bodyText confidence="0.642357">
Let us define M−1 = [m−1
</bodyText>
<equation confidence="0.969952125">
kl ]. Then,
(−1)k+l|Mkl|
|M|
= (−1)k+l|Mkl |(... (9))
=   sj(sjk − cik)(sjl − cil)
i
 (13)
(... (12))
</equation>
<bodyText confidence="0.864251">
Therefore, when we define
</bodyText>
<figure confidence="0.89400632">
A = [akl] (14)
n
i=1
min
M
n
l=1
n

k=1
L= N  (sjk − cik)mkl(sjl − cil)
i=1 sj 
k l
−  (−1)k+lmkl |Mkl |− n .
k l
=
i sj
L
mkl
−1
mkl =
|M |= 1. (9) as N  (sjl − cil)(sjk − cik) , (15)
i=1 sjXi
Expanding (8), we get akl =
 
</figure>
<equation confidence="0.8194095">
i sj
n n
(sjk − cik)mkl(sjl − cil) , (10)
k=1 l=1
from (13),
A = M−1
</equation>
<bodyText confidence="0.838009333333333">
... |A |= n|M−1 |= n
...  = |A|1/n ,
where A is defined by (14), (15).
and from (9), for all k
n (−1)k+lmkl|Mkl |= 1 .
l=1
</bodyText>
<sectionHeader confidence="0.987771" genericHeader="method">
Appendix B.
</sectionHeader>
<subsectionHeader confidence="0.695785">
Moore-Penrose Matrix Pseudoinverse
</subsectionHeader>
<bodyText confidence="0.983711666666667">
The Moore-Penrose matrix pseudoinverse A+ of A
is a unique matrix that has a property of normal in-
verse in that x = A+y is a shortest length least
squares solution to Ax = y even if A is singular
(Weisstein, 2004).
A+ can be calculated simply by a MATLAB
function pinv. Or alternatively (Ishikawa et al.,
1998), we can decompose A as
A = UEUT ,
where U is an orthonormal n x n matrix and E =
diag(Q1, ... , QR, 0, ... , 0) (R = rank(A)). Then,
A+ is calculated as
</bodyText>
<equation confidence="0.637053">
A+ = UE+UT ,
where E+ = diag(1/Q1, ... ,1/QR, 0, ... , 0).
Therefore,
M = (Q1Q2 ··· QR)1/RA+.
f!
(‘How much is the total?’)
</equation>
<figure confidence="0.631579">
Metric distance:
Euclidean distance:
(* denotes the right answers.)
</figure>
<figureCaption confidence="0.989836">
Figure 5: Sentence retrieval example.
</figureCaption>
<figure confidence="0.807297">
f!
(‘I’d like some fruit for dessert.’)
Metric distance:
</figure>
<figureCaption confidence="0.99637">
Figure 6: High rate of dimensionality reduction.
</figureCaption>
<figure confidence="0.999750464285714">
distance
synonymous sentence
*
*
*
*
*
*
*
*
*
0.2712
0.3444
0.3444
0.369
0.4377
0.4479
0.4505
0.4558
0.4602
0.4682
0.4729
0.4851
distance
synonymous sentence
*
*
*
0.1732
1.781
1.902
1.966
1.966
1.974
1.983
2.283
2.505
2.65
2.729
2.749
distance
synonymous sentence
0.3531
0.3709
0.596
0.6104
0.621
0.6255
0.6295
0.6343
0.6685
0.7966
Euclidean distance:
*
*
distance
</figure>
<page confidence="0.868812111111111">
1.036
1.421
1.491
1.499
1.535
1.622
1.622
:
2.787
</page>
<figure confidence="0.853776">
synonymous sentence
2.854
:
*
Query: “
Query: “
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.167554">
<title confidence="0.9632175">Learning Nonstructural Distance by Minimum Cluster Distortions</title>
<author confidence="0.927504">Daichi Mochihashi</author>
<author confidence="0.927504">Genichiro</author>
<affiliation confidence="0.514371">ATR Spoken Language</affiliation>
<email confidence="0.726701">research</email>
<note confidence="0.5906995">Hikaridai 2-2-2, Keihanna Science Kyoto 619-0288,</note>
<email confidence="0.924435">genichiro.kikui@atr.jp</email>
<abstract confidence="0.9991631">Much natural language processing still depends on the Euclidean (cosine) distance function between two feature vectors, but this has severe problems with regard to feature weightings and feature correlations. To answer these problems, we propose an optimal metric distance that can be used as an alternative to the cosine distance, thus accommodating the two problems at the same time. This metric is optimal in the sense of global quadratic minimization, and can be obtained from the clusters in the training data in a supervised fashion. We confirmed the effect of the proposed metric distance by a synonymous sentence retrieval task, document retrieval task and the K-means clustering of general vectorial data. The results showed constant improvement over the baseline method of Euclid and tf.idf, and were especially prominent for the sentence retrieval task, showing a 33% increase in the 11-point average precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis R Bach</author>
<author>Michael I Jordan</author>
</authors>
<title>Learning Spectral Clustering.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems 16.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6219" citStr="Bach and Jordan, 2004" startWordPosition="969" endWordPosition="972">rinciple with which to choose from. Further, it has no theoretical basis that gives any optimality as a distance function. 3 Related Works The issues above of feature correlations and feature weightings can be summarized as a problem of defining an appropriate metric in the feature space, based on the distribution of data. This problem has recently been highlighted in the field of machine learning research. (Xing et al., 2002) has an objective that is quite similar to that of this paper, and gives a metric matrix that resembles ours based on sample pairs of “similar points” as training data. (Bach and Jordan, 2004) and (Schultz and Joachims, 2004) seek to answer the same problem with an additional scenario of spectral clustering and relative comparisons in Support Vector Machines, respectively. In this aspect, our work is a straight successor of (Xing et al., 2002) where its general usage in vector space is preserved. We offer a discussion on the similarity to our method and our advantages &apos;When we normalize the length of the vectors |u |= |v |= 1 as commonly adopted, (u − v)T (u − v) = |u|2 + |v|2 − 2u · v  −u · v = − cos(u, v) ; therefore, this includes a cosine similarity (Manning and </context>
</contexts>
<marker>Bach, Jordan, 2004</marker>
<rawString>Francis R. Bach and Michael I. Jordan. 2004. Learning Spectral Clustering. In Advances in Neural Information Processing Systems 16. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo A Baeza-Yates</author>
<author>Berthier A RibeiroNeto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>ACM Press / Addison-Wesley.</publisher>
<marker>Baeza-Yates, RibeiroNeto, 1999</marker>
<rawString>Ricardo A. Baeza-Yates and Berthier A. RibeiroNeto. 1999. Modern Information Retrieval. ACM Press / Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Blake</author>
<author>C J Merz</author>
</authors>
<title>UCI Repository of machine learning databases.</title>
<date>1998</date>
<note>http://www.ics.uci.edu/˜mlearn/MLRepository.html.</note>
<contexts>
<context position="19854" citStr="Blake and Merz, 1998" startWordPosition="3277" endWordPosition="3280">.307 0.379 0.376 20% 0.343 0.297 0.397 0.365 Table 1: Newsgroup text retrieval results. 3Normalizing documents to unit length effectively maps them to a high-dimensional hypersphere; this proved to produce an unsatisfactory result. Defining metrics that work on a hypersphere like spherical K-means (Dhillon and Modha, 2001) requires further research. 5.3 K-means clustering and general vectorial data Metric distance can also be used for clustering or general vectorial data. Figure 4 shows the K-means clustering result of applying our metric distance to some of the UCI Machine Learning datasets (Blake and Merz, 1998). K-means clustering was conducted 100 times with a random start, where K equals the known number of classes in the data 4. Clustering precision was measured as an average probability that a randomly picked pair of data will conform to the true clustering (Xing et al., 2002). We also conducted the same clustering for documents of the 20-Newsgroup dataset to get a small increase in precision like the document retrieval experiment in section 5.2. 0.9 0.9 0.8 0.8 0.7 0.7 0.6 1 2 5 10 15 20 Dimension (a) “wine” dataset (b) “protein” dataset 1 0.9 0.9 0.8 0.8 0.7 0.7 1 2 5 10 20 3 5 Dimension (c) “</context>
</contexts>
<marker>Blake, Merz, 1998</marker>
<rawString>C. L. Blake and C. J. Merz. 1998. UCI Repository of machine learning databases. http://www.ics.uci.edu/˜mlearn/MLRepository.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<title>Advances in domain independent linear text segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL-00.</booktitle>
<contexts>
<context position="2869" citStr="Choi, 2000" startWordPosition="413" endWordPosition="414">hnology, Tokushima University Minamijosanjima 2-1 Tokushima 770-8506, Japan kita@is.tokushima-u.ac.jp but because it is frequently embedded in many applications where structural parsings are not available or computationally too expensive. For example, information retrieval has long used the ‘bag of words’ approach (Baeza-Yates and Ribeiro-Neto, 1999; Sch¨utze, 1992) mainly due to a lack of scalable segmentation algorithms and the huge amount of data involved. While segmentation algorithms, such as TEXTTILING (Hearst, 1994) and its recent successors using the inter-paragraph similarity matrix (Choi, 2000), all themselves use nonstructural cosine similarity as a measure of semantic proximity between paragraphs. However, the distance function so far has been largely defined and used ad hoc, usually by a tf.idf weighting scheme (Salton and Yang, 1973) and a simple cosine similarity, equivalently, an Euclidean dot product. In this paper, we propose an optimal distance function that is parameterized by a global metric matrix. This metric is optimal in the sense of global quadratic minimization, and can be learned from the given clusters in the training data. These clusters are often attributable wi</context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>Freddy Y. Y. Choi. 2000. Advances in domain independent linear text segmentation. In Proceedings ofNAACL-00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language. In</title>
<date>2001</date>
<booktitle>NIPS</booktitle>
<contexts>
<context position="1961" citStr="Collins and Duffy, 2001" startWordPosition="280" endWordPosition="283">llection of documents. Comparing these expressions based on semantic proximity is a fundamental task and has many applications. Generally, two basic approaches exist to compare two expressions: (a) structural and (b) nonstructural. Structural approaches make use of syntactic parsing or dependency analysis to make a rigorous comparison; nonstructural approaches use vector representation and provide a rough but fast comparison that is required for search/retrieval from a vast amount of corpora. While structural approaches have recently become available in a kernel-based sophisticated treatment (Collins and Duffy, 2001; Suzuki et al., 2003), here we concentrate on nonstructural comparison. This is not only because nonstructural comparison constitutes an integral part in structural methods (that is, even in hierarchical methods the leaf comparison is still atomic), Kenji Kita Center for Advanced Information Technology, Tokushima University Minamijosanjima 2-1 Tokushima 770-8506, Japan kita@is.tokushima-u.ac.jp but because it is frequently embedded in many applications where structural parsings are not available or computationally too expensive. For example, information retrieval has long used the ‘bag of wor</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution Kernels for Natural Language. In NIPS 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="14902" citStr="Deerwester et al., 1990" startWordPosition="2466" endWordPosition="2470">nality reduction (see below) to produce average statistics. 5.1.2 Features and dimensionality reduction As a feature of a sentence, we adopted unigrams of all words and bigrams of functional words from the part-of-speech tags, because the sequence of functional words is important in the conversational corpus. While the lexicon is limited for travel conversations, the number of features exceeds several thousand or more. This may be prohibitive for the calculation of the metric matrix, therefore, we additionally compressed the features with SVD, the same method used in Latent Semantic Indexing (Deerwester et al., 1990). 5.1.3 Sentence retrieval results Qualitative result Figure 5 (last page) shows a sample retrieval result. A sentence with (*) mark at the end is the correct answer, that is, a sentence from the same original cluster as the query. We can see that the results with the metric distance contain 2When the number of data in the cluster exceeds this limit, 100 sentences are randomly sampled. All sampling are made without replacement. less noise than a standard Euclid baseline with tf.idf weighting, achieving a high-precision retrieval. Although the high rate of dimensionality reduction in figure 6 s</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, 1990</marker>
<rawString>S. Deerwester, Susan T. Dumais, and George W. Furnas. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit S Dhillon</author>
<author>Dharmendra S Modha</author>
</authors>
<title>Concept Decompositions for Large Sparse Text Data Using Clustering.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>42--1</pages>
<contexts>
<context position="19557" citStr="Dhillon and Modha, 2001" startWordPosition="3231" endWordPosition="3234">ously, or reconsider the problem in kernel Hilbert space. Dim. Metric R-precision 11-pt Avr. Prec. Red. Euclid Metric Euclid 0.5% 0.421 0.399 0.476 0.455 1% 0.388 0.368 0.450 0.430 2% 0.359 0.343 0.425 0.409 3% 0.344 0.330 0.411 0.399 4% 0.335 0.323 0.402 0.392 5% 0.329 0.318 0.397 0.388 10% 0.316 0.307 0.379 0.376 20% 0.343 0.297 0.397 0.365 Table 1: Newsgroup text retrieval results. 3Normalizing documents to unit length effectively maps them to a high-dimensional hypersphere; this proved to produce an unsatisfactory result. Defining metrics that work on a hypersphere like spherical K-means (Dhillon and Modha, 2001) requires further research. 5.3 K-means clustering and general vectorial data Metric distance can also be used for clustering or general vectorial data. Figure 4 shows the K-means clustering result of applying our metric distance to some of the UCI Machine Learning datasets (Blake and Merz, 1998). K-means clustering was conducted 100 times with a random start, where K equals the known number of classes in the data 4. Clustering precision was measured as an average probability that a randomly picked pair of data will conform to the true clustering (Xing et al., 2002). We also conducted the same</context>
</contexts>
<marker>Dhillon, Modha, 2001</marker>
<rawString>Inderjit S. Dhillon and Dharmendra S. Modha. 2001. Concept Decompositions for Large Sparse Text Data Using Clustering. Machine Learning, 42(1/2):143–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard O Duda</author>
<author>Peter E Hart</author>
<author>David G Stork</author>
</authors>
<title>Pattern Classification *Second Edition.</title>
<date>2000</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="9042" citStr="Duda et al., 2000" startWordPosition="1457" endWordPosition="1460"> M: dM (u, v)2 = (u − v)TM(u − v) (2) = �n�n j=1 mij(ui − vi)(uj − vj), i=1 where symmetric metric matrix M gives both corresponding feature weights and feature correlations. When we take M = I (unit matrix), we recover the original Euclidean distance (1). Equation (2) can be rewritten as (3) because M is symmetric: dM(u, v)2 = (M1/2(u −v))T (M1/2(u − v)). (3) Therefore, this distance amounts to a Euclidean distance in M1/2-mapped space (Xing et al., 2002). Note that this distance is global, and different from the ordinary Mahalanobis distance in pattern recognition (for example, (Duda et al., 2000)) that is defined for each cluster one by one, using a clusterspecific covariance matrix. That type of distance cannot be generalized to new kinds of data; therefore, it has been used for local classifications. What we want is a global distance metric that is generally useful, not a measure for classification to predefined clusters. In this respect, (Xing et al., 2002) shares the same objective as ours. Therefore, we require an optimization over all the clusters in the training data. Generally, data in the clusters are distributed as in figure 1(a), comprising ellipsoidal forms that have high </context>
</contexts>
<marker>Duda, Hart, Stork, 2000</marker>
<rawString>Richard O. Duda, Peter E. Hart, and David G. Stork. 2000. Pattern Classification *Second Edition. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In 32nd. Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2786" citStr="Hearst, 1994" startWordPosition="401" endWordPosition="402"> the leaf comparison is still atomic), Kenji Kita Center for Advanced Information Technology, Tokushima University Minamijosanjima 2-1 Tokushima 770-8506, Japan kita@is.tokushima-u.ac.jp but because it is frequently embedded in many applications where structural parsings are not available or computationally too expensive. For example, information retrieval has long used the ‘bag of words’ approach (Baeza-Yates and Ribeiro-Neto, 1999; Sch¨utze, 1992) mainly due to a lack of scalable segmentation algorithms and the huge amount of data involved. While segmentation algorithms, such as TEXTTILING (Hearst, 1994) and its recent successors using the inter-paragraph similarity matrix (Choi, 2000), all themselves use nonstructural cosine similarity as a measure of semantic proximity between paragraphs. However, the distance function so far has been largely defined and used ad hoc, usually by a tf.idf weighting scheme (Salton and Yang, 1973) and a simple cosine similarity, equivalently, an Euclidean dot product. In this paper, we propose an optimal distance function that is parameterized by a global metric matrix. This metric is optimal in the sense of global quadratic minimization, and can be learned fro</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti Hearst. 1994. Multi-paragraph segmentation of expository text. In 32nd. Annual Meeting of the Association for Computational Linguistics, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshiharu Ishikawa</author>
<author>Ravishankar Subramanya</author>
<author>Christos Faloutsos</author>
</authors>
<title>MindReader: Querying Databases Through Multiple Examples.</title>
<date>1998</date>
<booktitle>In Proc. 24th Int. Conf. Very Large Data Bases,</booktitle>
<pages>218--227</pages>
<contexts>
<context position="11398" citStr="Ishikawa et al., 1998" startWordPosition="1866" endWordPosition="1869">. The basic idea above is formulated as follows. We seek the metric matrix M that minimizes the distance between each data sj and the cluster centroid ci, dM(sj,ci) for all clusters X1 ... XN. Mathematically, this is formulated as a quadratic minimization problem dM(sj,ci)2 (sj −ci)T M(sj −ci) (4) under a scale constraint ( |· |means determinant) |M |= 1. (5) Scale constraint (5) is necessary for excluding a degenerate solution M = O. 1 is an arbitrary constant: when we replace 1 by c, c2M becomes a new solution. This minimization problem is an extension to the method of MindReader (Ishikawa et al., 1998) to multiple clusters, and has a unique solution below. Theorem The matrix that solves the minimization problem (4,5) is M = |A|1/nA−1, (6) where A = [akl] is defined by (sjl − cil)(sjk − cik) . (7) Proof: See Appendix A. When A is singular, we can use as A−1 a MoorePenrose matrix pseudoinverse A+. Generally, A consists of linguistic features and is very sparse, and often singular. Therefore, A+ is nearly always necessary for the above computation. For details, see Appendix B. 4.3 Generalization While we assumed through the above construction that each cluster is equally important, this is not</context>
</contexts>
<marker>Ishikawa, Subramanya, Faloutsos, 1998</marker>
<rawString>Yoshiharu Ishikawa, Ravishankar Subramanya, and Christos Faloutsos. 1998. MindReader: Querying Databases Through Multiple Examples. In Proc. 24th Int. Conf. Very Large Data Bases, pages 218–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tommi S Jaakkola</author>
<author>David Haussler</author>
</authors>
<title>Exploiting generative models in discriminative classifiers.</title>
<date>1999</date>
<booktitle>In Proc. of the 1998 Conference on Advances in Neural Information Processing Systems,</booktitle>
<pages>487--493</pages>
<contexts>
<context position="6922" citStr="Jaakkola and Haussler, 1999" startWordPosition="1099" endWordPosition="1102">dditional scenario of spectral clustering and relative comparisons in Support Vector Machines, respectively. In this aspect, our work is a straight successor of (Xing et al., 2002) where its general usage in vector space is preserved. We offer a discussion on the similarity to our method and our advantages &apos;When we normalize the length of the vectors |u |= |v |= 1 as commonly adopted, (u − v)T (u − v) = |u|2 + |v|2 − 2u · v  −u · v = − cos(u, v) ; therefore, this includes a cosine similarity (Manning and Sch¨utze, 1999). in section 6. Finally, we note that the Fisher kernel of (Jaakkola and Haussler, 1999) has the same concept that gives an appropriate similarity of two data through the Fisher information matrix obtained from the empirical distribution of data. However, it is often approximated by a unit matrix because of its heavy computational demand. In the field of information retrieval, (Jiang and Berry, 1998) proposes a Riemannian SVD (R-SVD) from the viewpoint of relevance feedback. This work is close in spirit to our work, but is not aimed at defining a permanent distance function and does not utilize cluster structures existent in the training data. 4 Defining an Optimal Metric To solv</context>
</contexts>
<marker>Jaakkola, Haussler, 1999</marker>
<rawString>Tommi S. Jaakkola and David Haussler. 1999. Exploiting generative models in discriminative classifiers. In Proc. of the 1998 Conference on Advances in Neural Information Processing Systems, pages 487–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric P Jiang</author>
<author>Michael W Berry</author>
</authors>
<title>Information Filtering Using the Riemannian SVD (RSVD).</title>
<date>1998</date>
<booktitle>In Proc. of IRREGULAR ’98,</booktitle>
<pages>386--395</pages>
<contexts>
<context position="7237" citStr="Jiang and Berry, 1998" startWordPosition="1148" endWordPosition="1151">alize the length of the vectors |u |= |v |= 1 as commonly adopted, (u − v)T (u − v) = |u|2 + |v|2 − 2u · v  −u · v = − cos(u, v) ; therefore, this includes a cosine similarity (Manning and Sch¨utze, 1999). in section 6. Finally, we note that the Fisher kernel of (Jaakkola and Haussler, 1999) has the same concept that gives an appropriate similarity of two data through the Fisher information matrix obtained from the empirical distribution of data. However, it is often approximated by a unit matrix because of its heavy computational demand. In the field of information retrieval, (Jiang and Berry, 1998) proposes a Riemannian SVD (R-SVD) from the viewpoint of relevance feedback. This work is close in spirit to our work, but is not aimed at defining a permanent distance function and does not utilize cluster structures existent in the training data. 4 Defining an Optimal Metric To solve the problems in section 2, we note the function that synonymous clusters play. There are many levels of (more or less) synonymous clusters in linguistic data: phrases, sentences, paragraphs, documents, and, in a web environment, the site that contains the document. These kinds of clusters can often be attributed</context>
</contexts>
<marker>Jiang, Berry, 1998</marker>
<rawString>Eric P. Jiang and Michael W. Berry. 1998. Information Filtering Using the Riemannian SVD (RSVD). In Proc. of IRREGULAR ’98, pages 386– 395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of ECML-98,</booktitle>
<pages>1398--137</pages>
<contexts>
<context position="16217" citStr="Joachims, 1998" startWordPosition="2676" endWordPosition="2677"> despite bad conditions. Quantitative result Figure 2 shows the averaged precision-recall curves of retrieval and figure 3 shows 11-point average precisions, for each rate of dimensionality reduction. Clearly, our method achieves higher precision than the standard method, and does not degrade much with feature compressions unless we reduce the dimension too much, i.e., to &lt; 5%. Figure 3: 11-point average precision. 5.2 Document retrieval As a method of tackling clusters of texts, the text classification task has recently made great advances with a Naive Bayes or SVM classifiers (for example, (Joachims, 1998)). However, they all aim at classifying texts into a few predefined clusters, and cannot deal with a document that fits neither of the clusters. For example, when we regard a website as a cluster of documents, the possible clusters are numerous and constantly increasing, which precludes classificatory approaches. For these circumstances, document clustering or retrieval will benefit from a global distance metric that exploits the multitude of cluster structures themselves. 5.2.1 Newsgroup text dataset For this purpose, we used the 20-Newsgroup dataset (Lang, 1995). This is a standard text clas</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: learning with many relevant features. In Proceedings of ECML-98, number 1398, pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Lang</author>
</authors>
<title>Newsweeder: Learning to filter netnews.</title>
<date>1995</date>
<booktitle>In Proceedings of the Twelfth International Conference on Machine Learning,</booktitle>
<pages>331--339</pages>
<contexts>
<context position="16787" citStr="Lang, 1995" startWordPosition="2761" endWordPosition="2762">lassifiers (for example, (Joachims, 1998)). However, they all aim at classifying texts into a few predefined clusters, and cannot deal with a document that fits neither of the clusters. For example, when we regard a website as a cluster of documents, the possible clusters are numerous and constantly increasing, which precludes classificatory approaches. For these circumstances, document clustering or retrieval will benefit from a global distance metric that exploits the multitude of cluster structures themselves. 5.2.1 Newsgroup text dataset For this purpose, we used the 20-Newsgroup dataset (Lang, 1995). This is a standard text classification dataset that has a relatively large number of classes, 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Recall (a) Metric distance + (b) Euclidean + idf idf Figure 2: Precision-recall of sentence retrieval. 0 5 10 15 20 25 30 35 40 45 50 Dimension Reduction(%) Precision 0.8 0.6 0.4 0.2 0 1 1% 5% 10% 20% 50% Precision 0.8 0.6 0.4 0.2 0 1 1% 5% 10% 20% 50% Precision 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Metric distance +idf Euclidean distance +idf 20. Among the 20 newsgroups, we selected 16 clusters of training data and 4 cluster</context>
</contexts>
<marker>Lang, 1995</marker>
<rawString>Ken Lang. 1995. Newsweeder: Learning to filter netnews. In Proceedings of the Twelfth International Conference on Machine Learning, pages 331–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R M¨uller</author>
<author>S Mika</author>
<author>G Ratsch</author>
<author>K Tsuda</author>
</authors>
<title>An introduction to kernel-based learning algorithms.</title>
<date>2001</date>
<journal>IEEE Neural Networks,</journal>
<volume>12</volume>
<issue>2</issue>
<pages>201</pages>
<marker>M¨uller, Mika, Ratsch, Tsuda, 2001</marker>
<rawString>K. R. M¨uller, S. Mika, G. Ratsch, and K. Tsuda. 2001. An introduction to kernel-based learning algorithms. IEEE Neural Networks, 12(2):181– 201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C S Yang</author>
</authors>
<title>On the specification of term values in automatic indexing.</title>
<date>1973</date>
<journal>Journal ofDocumentation,</journal>
<pages>29--351</pages>
<contexts>
<context position="3117" citStr="Salton and Yang, 1973" startWordPosition="450" endWordPosition="453">. For example, information retrieval has long used the ‘bag of words’ approach (Baeza-Yates and Ribeiro-Neto, 1999; Sch¨utze, 1992) mainly due to a lack of scalable segmentation algorithms and the huge amount of data involved. While segmentation algorithms, such as TEXTTILING (Hearst, 1994) and its recent successors using the inter-paragraph similarity matrix (Choi, 2000), all themselves use nonstructural cosine similarity as a measure of semantic proximity between paragraphs. However, the distance function so far has been largely defined and used ad hoc, usually by a tf.idf weighting scheme (Salton and Yang, 1973) and a simple cosine similarity, equivalently, an Euclidean dot product. In this paper, we propose an optimal distance function that is parameterized by a global metric matrix. This metric is optimal in the sense of global quadratic minimization, and can be learned from the given clusters in the training data. These clusters are often attributable with many forms, such as paragraphs, documents or document collections, as long as the items in the training data are not completely independent. This paper is organized as follows. In section 2 we describe the issue of traditional Euclidean distance</context>
</contexts>
<marker>Salton, Yang, 1973</marker>
<rawString>G. Salton and C. S. Yang. 1973. On the specification of term values in automatic indexing. Journal ofDocumentation, 29:351–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Schultz</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning a Distance Metric from Relative Comparisons.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems 16.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6252" citStr="Schultz and Joachims, 2004" startWordPosition="974" endWordPosition="977">e from. Further, it has no theoretical basis that gives any optimality as a distance function. 3 Related Works The issues above of feature correlations and feature weightings can be summarized as a problem of defining an appropriate metric in the feature space, based on the distribution of data. This problem has recently been highlighted in the field of machine learning research. (Xing et al., 2002) has an objective that is quite similar to that of this paper, and gives a metric matrix that resembles ours based on sample pairs of “similar points” as training data. (Bach and Jordan, 2004) and (Schultz and Joachims, 2004) seek to answer the same problem with an additional scenario of spectral clustering and relative comparisons in Support Vector Machines, respectively. In this aspect, our work is a straight successor of (Xing et al., 2002) where its general usage in vector space is preserved. We offer a discussion on the similarity to our method and our advantages &apos;When we normalize the length of the vectors |u |= |v |= 1 as commonly adopted, (u − v)T (u − v) = |u|2 + |v|2 − 2u · v  −u · v = − cos(u, v) ; therefore, this includes a cosine similarity (Manning and Sch¨utze, 1999). in section 6. Fi</context>
</contexts>
<marker>Schultz, Joachims, 2004</marker>
<rawString>Matthew Schultz and Thorsten Joachims. 2004. Learning a Distance Metric from Relative Comparisons. In Advances in Neural Information Processing Systems 16. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Dimensions of Meaning.</title>
<date>1992</date>
<booktitle>In Proceedings ofSupercomputing’92,</booktitle>
<pages>787--796</pages>
<marker>Sch¨utze, 1992</marker>
<rawString>Hinrich Sch¨utze. 1992. Dimensions of Meaning. In Proceedings ofSupercomputing’92, pages 787–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sugaya</author>
<author>T Takezawa</author>
<author>G Kikui</author>
<author>S Yamamoto</author>
</authors>
<title>Proposal for a very-large-corpus acquisition method by cell-formed registration.</title>
<date>2002</date>
<booktitle>In Proc. LREC-2002, volume I,</booktitle>
<pages>326--328</pages>
<contexts>
<context position="13637" citStr="Sugaya et al., 2002" startWordPosition="2264" endWordPosition="2267">, document retrieval, and the K-means clustering of general vectorial data. After calculating M on the training data of clusters, we applied it to the test data to see how well its clusters could be recovered. As a measure of cluster recovery, we use 11-point average precision and R-precision for the distribution of items of the same cluster in each retrieval result. Here, R equals the cardinality of the cluster; therefore, R-precision shows the precision of cluster recovery. 5.1 Synonymous sentence retrieval 5.1.1 Sentence cluster corpus We used a paraphrasing corpus of travel conversations (Sugaya et al., 2002) for sentence retrieval. This corpus consists of 33,723,164 Japanese translations, each of which corresponds to one of the original English sentences. By way of this correspondence, Japanese sentences are divided into 10,610 clusters. Therefore, each cluster consists of Japanese sentences that are possible translations from the same English seed sentence that the cluster has. From this corpus, we constructed 10 sets of data. Each set contains random selection of 200 training clusters and 50 test clusters, and each cluster contains a maximum of 100 sentences 2. Experiments were conducted on the</context>
</contexts>
<marker>Sugaya, Takezawa, Kikui, Yamamoto, 2002</marker>
<rawString>F. Sugaya, T. Takezawa, G. Kikui, and S. Yamamoto. 2002. Proposal for a very-large-corpus acquisition method by cell-formed registration. In Proc. LREC-2002, volume I, pages 326–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Tsutomu Hirao</author>
<author>Yutaka Sasaki</author>
<author>Eisaku Maeda</author>
</authors>
<title>Hierarchical Directed Acyclic Graph Kernel: Methods for Structured Natural Language Data.</title>
<date>2003</date>
<booktitle>In Proc. of the 41th Annual Meeting of Association for Computational Linguistics (ACL2003),</booktitle>
<pages>32--39</pages>
<contexts>
<context position="1983" citStr="Suzuki et al., 2003" startWordPosition="284" endWordPosition="287">mparing these expressions based on semantic proximity is a fundamental task and has many applications. Generally, two basic approaches exist to compare two expressions: (a) structural and (b) nonstructural. Structural approaches make use of syntactic parsing or dependency analysis to make a rigorous comparison; nonstructural approaches use vector representation and provide a rough but fast comparison that is required for search/retrieval from a vast amount of corpora. While structural approaches have recently become available in a kernel-based sophisticated treatment (Collins and Duffy, 2001; Suzuki et al., 2003), here we concentrate on nonstructural comparison. This is not only because nonstructural comparison constitutes an integral part in structural methods (that is, even in hierarchical methods the leaf comparison is still atomic), Kenji Kita Center for Advanced Information Technology, Tokushima University Minamijosanjima 2-1 Tokushima 770-8506, Japan kita@is.tokushima-u.ac.jp but because it is frequently embedded in many applications where structural parsings are not available or computationally too expensive. For example, information retrieval has long used the ‘bag of words’ approach (Baeza-Ya</context>
</contexts>
<marker>Suzuki, Hirao, Sasaki, Maeda, 2003</marker>
<rawString>Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and Eisaku Maeda. 2003. Hierarchical Directed Acyclic Graph Kernel: Methods for Structured Natural Language Data. In Proc. of the 41th Annual Meeting of Association for Computational Linguistics (ACL2003), pages 32–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Weisstein</author>
</authors>
<date>2004</date>
<note>Moore-Penrose Matrix Inverse. http://mathworld.wolfram.com/MoorePenroseMatrixInverse.html.</note>
<marker>Weisstein, 2004</marker>
<rawString>Eric W. Weisstein. 2004. Moore-Penrose Matrix Inverse. http://mathworld.wolfram.com/MoorePenroseMatrixInverse.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric P Xing</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>Stuart Russell</author>
</authors>
<title>Distance metric learning, with application to clustering with sideinformation.</title>
<date>2002</date>
<booktitle>In NIPS</booktitle>
<contexts>
<context position="6027" citStr="Xing et al., 2002" startWordPosition="934" endWordPosition="937">d”. Problem (2) is a more subtle but inherent one: while tf.idf often works properly in practice, there are several options, especially in tf such as logs or square roots, but we have no principle with which to choose from. Further, it has no theoretical basis that gives any optimality as a distance function. 3 Related Works The issues above of feature correlations and feature weightings can be summarized as a problem of defining an appropriate metric in the feature space, based on the distribution of data. This problem has recently been highlighted in the field of machine learning research. (Xing et al., 2002) has an objective that is quite similar to that of this paper, and gives a metric matrix that resembles ours based on sample pairs of “similar points” as training data. (Bach and Jordan, 2004) and (Schultz and Joachims, 2004) seek to answer the same problem with an additional scenario of spectral clustering and relative comparisons in Support Vector Machines, respectively. In this aspect, our work is a straight successor of (Xing et al., 2002) where its general usage in vector space is preserved. We offer a discussion on the similarity to our method and our advantages &apos;When we normalize the le</context>
<context position="8896" citStr="Xing et al., 2002" startWordPosition="1435" endWordPosition="1438">all distance between each other in the ideal geometry. When we measure an L2-distance between u and v by a Mahalanobis distance parameterized by M: dM (u, v)2 = (u − v)TM(u − v) (2) = �n�n j=1 mij(ui − vi)(uj − vj), i=1 where symmetric metric matrix M gives both corresponding feature weights and feature correlations. When we take M = I (unit matrix), we recover the original Euclidean distance (1). Equation (2) can be rewritten as (3) because M is symmetric: dM(u, v)2 = (M1/2(u −v))T (M1/2(u − v)). (3) Therefore, this distance amounts to a Euclidean distance in M1/2-mapped space (Xing et al., 2002). Note that this distance is global, and different from the ordinary Mahalanobis distance in pattern recognition (for example, (Duda et al., 2000)) that is defined for each cluster one by one, using a clusterspecific covariance matrix. That type of distance cannot be generalized to new kinds of data; therefore, it has been used for local classifications. What we want is a global distance metric that is generally useful, not a measure for classification to predefined clusters. In this respect, (Xing et al., 2002) shares the same objective as ours. Therefore, we require an optimization over all </context>
<context position="20129" citStr="Xing et al., 2002" startWordPosition="3325" endWordPosition="3328">ke spherical K-means (Dhillon and Modha, 2001) requires further research. 5.3 K-means clustering and general vectorial data Metric distance can also be used for clustering or general vectorial data. Figure 4 shows the K-means clustering result of applying our metric distance to some of the UCI Machine Learning datasets (Blake and Merz, 1998). K-means clustering was conducted 100 times with a random start, where K equals the known number of classes in the data 4. Clustering precision was measured as an average probability that a randomly picked pair of data will conform to the true clustering (Xing et al., 2002). We also conducted the same clustering for documents of the 20-Newsgroup dataset to get a small increase in precision like the document retrieval experiment in section 5.2. 0.9 0.9 0.8 0.8 0.7 0.7 0.6 1 2 5 10 15 20 Dimension (a) “wine” dataset (b) “protein” dataset 1 0.9 0.9 0.8 0.8 0.7 0.7 1 2 5 10 20 3 5 Dimension (c) “iris” dataset (d) “soybean” dataset Figure 4: K-means clustering of UCI Machine Learning dataset results. The horizontal axis shows compressed dimensions (rightmost is original). The right bar shows clustering precision using Metric distance, and the left bar shows that usin</context>
<context position="21403" citStr="Xing et al., 2002" startWordPosition="3541" endWordPosition="3544">posed an optimal distance metric based on the idea of minimum cluster distortion in training data. Although vector distances have frequently been used in natural language processing, this is a rather neglected but recently highlighted problem. Unlike recently proposed methods with spectral methods or SVMs, our method assumes no such additional scenarios and can be considered as 4Because of the small size of the dataset, we did not apply cross-validation as in other experiments. Precision Precision 0.6 1 1 Precision 1 2 3 4 Dimension Precision 1 2 5 10 13 Dimension 0.6 a straight successor to (Xing et al., 2002)’s work. Their work has the same perspective as ours, and they calculate a metric matrix A that is similar to ours based on a set 5 of vector pairs (9i, xj) that can be regarded as similar. They report that the effectiveness of A increases as the number of the training pairs 5 increases; this requires O(n2) sample points from n training data, and must be optimized by a computationally expensive Newton-Raphson iteration. On the other hand, our method uses only linear algebra, and can induce an ideal metric using all the training data at the same time. We believe this metric can be useful for ma</context>
</contexts>
<marker>Xing, Ng, Jordan, Russell, 2002</marker>
<rawString>Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stuart Russell. 2002. Distance metric learning, with application to clustering with sideinformation. In NIPS 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>