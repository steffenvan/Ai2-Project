<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000100">
<title confidence="0.9982675">
Investigating Loss Functions and Optimization Methods for Discriminative
Learning of Label Sequences
</title>
<author confidence="0.991538">
Yasemin Altun
</author>
<affiliation confidence="0.9792575">
Computer Science
Brown University
</affiliation>
<address confidence="0.91756">
Providence, RI 02912
</address>
<email confidence="0.999112">
altun@cs.brown.edu
</email>
<author confidence="0.975183">
Mark Johnson
</author>
<affiliation confidence="0.943809">
Cognitive and Linguistic Sciences
Brown University
</affiliation>
<address confidence="0.886717">
Providence, RI 02912
</address>
<email confidence="0.936135">
Mark Johnson@brown.edu
</email>
<author confidence="0.995782">
Thomas Hofmann
</author>
<affiliation confidence="0.9781725">
Computer Science
Brown University
</affiliation>
<address confidence="0.956948">
Providence, RI 02912
</address>
<email confidence="0.999621">
th@cs.brown.edu
</email>
<sectionHeader confidence="0.995651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999943785714286">
Discriminative models have been of inter-
est in the NLP community in recent years.
Previous research has shown that they
are advantageous over generative mod-
els. In this paper, we investigate how dif-
ferent objective functions and optimiza-
tion methods affect the performance of the
classifiers in the discriminative learning
framework. We focus on the sequence la-
belling problem, particularly POS tagging
and NER tasks. Our experiments show
that changing the objective function is not
as effective as changing the features in-
cluded in the model.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952530612245">
Until recent years, generative models were the most
common approach for many NLP tasks. Recently,
there is a growing interest on discriminative mod-
els in the NLP community, and these models were
shown to be successful for different tasks(Lafferty
et al., 2001; Ratnaparkhi, 1999; Collins, 2000). Dis-
criminative models do not only have theoretical ad-
vantages over generative models, as we discuss in
Section 2, but they are also shown to be empirically
favorable over generative models when features and
objective functions are fixed (Klein and Manning,
2002).
In this paper, we use discriminative models to
investigate the optimization of different objective
functions by a variety of optimization methods. We
focus on label sequence learning tasks. Part-of-
Speech (POS) tagging and Named Entity Recogni-
tion (NER) are the most studied applications among
these tasks. However, there are many others, such
as chunking, pitch accent prediction and speech edit
detection. These tasks differ in many aspects, such
as the nature of the label sequences (chunks or indi-
vidual labels), their difficulty and evaluation meth-
ods. Given this variety, we think it is worthwhile to
investigate how optimizing different objective func-
tions affects performance. In this paper, we varied
the scale (exponential vs logarithmic) and the man-
ner of the optimization (sequential vs pointwise) and
using different combinations, we designed 4 differ-
ent objective functions. We optimized these func-
tions on NER and POS tagging tasks. Despite our
intuitions, our experiments show that optimizing ob-
jective functions that vary in scale and manner do
not affect accuracy much. Instead, the selection of
the features has a larger impact.
The choice of the optimization method is impor-
tant for many learning problems. We would like
to use optimization methods that can handle a large
number of features, converge fast and return sparse
classifiers. The importance of the features, and
therefore the importance of the ability to cope with
a larger number of features is well-known. Since
training discriminative models over large corpora
can be expensive, an optimization method that con-
verges fast might be advantageous over others. A
sparse classifier has a shorter test time than a denser
classifier. For applications in which the test time is
crucial, optimization methods that result in sparser
classifiers might be preferable over other methods
</bodyText>
<figure confidence="0.64543">
a) HMM b)CRF
</figure>
<figureCaption confidence="0.756776">
Figure 1: Graphical representation of HMMs and
CRFs. Shaded areas indicate variables that the
model conditions on.
</figureCaption>
<bodyText confidence="0.998758857142857">
even if their training time is longer. In this paper we
investigate these aspects for different optimization
methods, i.e. the number of features, training time
and sparseness, as well as the accuracy. In some
cases, an approximate optimization that is more ef-
ficient in one of these aspects might be preferable to
the exact method, if they have similar accuracy. We
experiment with exact versus approximate as well
as parallel versus sequential optimization methods.
For the exact methods, we use an off-the-shelf gradi-
ent based optimization routine. For the approximate
methods, we use a perceptron and a boosting algo-
rithm for sequence labelling which update the fea-
ture weights parallel and sequentially respectively.
</bodyText>
<sectionHeader confidence="0.981295" genericHeader="method">
2 Discriminative Modeling of Label
</sectionHeader>
<subsectionHeader confidence="0.888587">
Sequences Learning
</subsectionHeader>
<bodyText confidence="0.999796457627119">
Label sequence learning is, formally, the problem
of learning a function that maps a sequence of ob-
servations to a label sequence
, where each , the set of
individual labels. For example, in POS tagging, the
words ’s construct a sentence , and is the la-
belling of the sentence where is the part of speech
tag of the word . We are interested in the super-
vised learning setting, where we are given a corpus,
in order to learn
the classifier.
The most popular model for label sequence learn-
ing is the Hidden Markov Model (HMM). An HMM,
as a generative model, is trained by finding the joint
probability distribution over the observation and la-
bel sequences that explains the corpus the
best (Figure 1a). In this model, each random vari-
able is assumed to be independent of the other ran-
dom variables, given its parents. Because of the long
distance dependencies of natural languages that can-
not be modeled by sequences, this conditional inde-
pendence assumption is violated in many NLP tasks.
Another shortcoming of this model is that, due to its
generative nature, overlapping features are difficult
to use in HMMs. For this reason, HMMs have been
standardly used with current word-current label, and
previous label(s)-current label features. However,
if we incorporate information about the neighboring
words and/or information about more detailed char-
acteristics of the current word directly to our model,
rather than propagating it through the previous la-
bels, we may hope to learn a better classifier.
Many different models, such as Maximum En-
tropy Markov Models (MEMMs) (McCallum et al.,
2000), Projection based Markov Models (PMMs)
(Punyakanok and Roth, 2000) and Conditional Ran-
dom Fields (CRFs) (Lafferty et al., 2001), have been
proposed to overcome these problems. The common
property of these models is their discriminative ap-
proach. They model the probability distribution of
the label sequences given the observation sequences:
.
The best performing models of label sequence
learning are MEMMs or PMMs (also known as
Maximum Entropy models) whose features are care-
fully designed for the specific tasks (Ratnaparkhi,
1999; Toutanova and Manning, 2000). However,
maximum entropy models suffer from the so called
label bias problem, the problem of making local de-
cisions (Lafferty et al., 2001). Lafferty et al. (2001)
show that CRFs overcome the label-bias problem
and outperform MEMMs in POS tagging.
CRFs define a probability distribution over the
whole sequence , globally conditioning over the
whole observation sequence (Figure 1b). Be-
cause they condition on the observation (as opposed
to generating it), they can use overlapping features.
The features used in this paper are of the
form:
</bodyText>
<listItem confidence="0.852043">
1. Current label and information about the obser-
vation sequence, such as the identity or spelling
features of a word that is within a window
of the word currently labelled. Each of these
features corresponds to a choice of and
</listItem>
<bodyText confidence="0.771162">
where and is the
half window size
</bodyText>
<listItem confidence="0.971931333333333">
2. Current label and the neighbors of that label,
i.e. features that capture the inter-label depen-
dencies. Each of these features corresponds to
</listItem>
<equation confidence="0.999862">
y(t−1) y(t) y(t+1) y(t−1)
x(t−1) x(t) x(t+1)
� � � � � � � � � �
x(t−1) x(t)
y(t) y(t+1)
� � � � � �
x(t+1)
</equation>
<bodyText confidence="0.997941076923077">
The conditional probability distribution defined
by this model is :
where ’s are the parameters to be estimated from
the training corpus C and is a normaliza-
tion term to assure a proper probability distribu-
tion. In order to simplify the notation, we in-
troduce , which is the
number of times feature is observed in
pair and, , which is
the linear combination of all the features with
parameterization. is the sufficient statis-
tic of . Then, we can rewrite as:
.
</bodyText>
<sectionHeader confidence="0.921066" genericHeader="method">
3 Loss Functions for Label Sequences
</sectionHeader>
<bodyText confidence="0.999981454545455">
Given the theoretical advantages of discriminative
models over generative models and the empirical
support by (Klein and Manning, 2002), and that
CRFs are the state-of-the-art among discriminative
models for label sequences, we chose CRFs as our
model, and trained by optimizing various objective
functions with respect to the corpus .
The application of these models to the label
sequence problems vary widely. The individual
labels might constitute chunks (e.g. Named-Entity
Recognition, shallow parsing), or they may be
single entries (e.g. POS tagging). The difficulty,
therefore the accuracy of the tasks are very different
from each other. The evaluation of the systems
differ from one task to another, and the nature of the
statistical noise level is task and corpus dependent.
Given this variety, using objective functions tailored
for each task might result in better classifiers. We
consider two dimensions in designing objective
functions: exponential versus logarithmic loss func-
tions, and sequential versus pointwise optimization
functions.
</bodyText>
<subsectionHeader confidence="0.996376">
3.1 Exponential vs Logarithmic Loss functions
</subsectionHeader>
<bodyText confidence="0.9936625">
Most estimation procedures in NLP proceed by
maximizing the likelihood of the training data. To
</bodyText>
<figure confidence="0.9656671">
Penalization of loss functions
exp−loss
2
1.5
1
log−loss
0.5
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Pθ(yi|xi)
</figure>
<figureCaption confidence="0.972574">
Figure 2: Loss values of 0-1, exp and log loss func-
tions in a binary classification problem
</figureCaption>
<bodyText confidence="0.999820714285715">
overcome the numerical problems of working with
a product of a large number of small probabilities,
usually the logarithm of the likelihood of the data
is optimized. However, most of the time, these sys-
tems, sequence labelling systems in particular, are
tested with respect to their error rate on test data, i.e.
the fraction of times the function assigns a higher
score to a label sequence (such that ) than
the correct label sequence for every observation
in test data. Then, the rank loss of might be a
more natural objective to minimize.
is the total number of label sequences that
ranks higher than the correct label sequences for the
training instances in the corpus . Since optimizing
the rank loss is NP-complete, one can optimize an
upper bound instead, e.g. an exponential loss func-
tion:
The exponential loss function is well studied in
the Machine Learning domain. The advantage of
the exp-loss over the log-loss is its property of pe-
nalizing incorrect labellings very severely, whereas
it penalizes almost nothing when the label sequence
is correct. This is a very desirable property for a
classifier. Figure 2 shows this property of exp-loss
in contrast to log-loss in a binary classification prob-
lem. However this property also means that, exp-
loss has the disadvantage of being sensitive to noisy
data, since systems optimizing exp-loss spends more
</bodyText>
<figure confidence="0.979912142857143">
Loss
0−1 loss
a choice of and the neighbors of , e.g. in a
bigram model, .
3.5
3
2.5
</figure>
<bodyText confidence="0.985444">
effort on the outliers and tend to be vulnerable to
noisy data, especially label noise.
</bodyText>
<subsectionHeader confidence="0.999383">
3.2 Sequential vs Pointwise Loss functions
</subsectionHeader>
<bodyText confidence="0.999986">
In many applications it is very difficult to get the
whole label sequence correct since most of the time
classifiers are not perfect and as the sequences get
longer, the probability of predicting every label in
the sequence correctly decreases exponentially. For
this reason performance is usually measured point-
wise, i.e. in terms of the number of individual la-
bels that are correctly predicted. Most common op-
timization functions in the literature, however, treat
the whole label sequence as one label, penalizing
a label sequence that has one error and a label se-
quence that is all wrong in the same manner. We
may be able to develop better classifiers by using
a loss function more similar to the evaluation func-
tion. One possible way of accomplishing this may
be minimizing pointwise loss functions. Sequential
optimizations optimize the joint conditional proba-
bility distribution , whereas pointwise op-
timizations that we propose optimize the marginal
conditional probability distribution,
.
</bodyText>
<subsectionHeader confidence="0.973189">
3.3 Four Loss functions
</subsectionHeader>
<bodyText confidence="0.9998772">
We derive four loss functions by taking the cross
product of the two dimensions discussed above:
Sequential Log-loss function: This function,
based on the standard maximum likelihood op-
timization, is used with CRFs in (Lafferty et al.,
2001).
As shown in (Altun et al., 2002) it is possible
to sum over all label sequences by using a dy-
namic algorithm.
Note that the exponential loss function is just
the inverse conditional probability plus a con-
stant.
Pointwise Log-loss function: This function op-
timizes the marginal probability of the labels at
each position conditioning on the observation
sequence:
Obviously, this function reduces to the sequen-
tial log loss if the length of the sequence is.
Pointwise Exp-loss function: Following the
parallelism in log-loss vs exp-loss functions of
sequential optimization (log vs inverse condi-
tional probability), we propose minimizing the
pointwise exp-loss function below, which re-
duces to the standard multi-class exponential
loss when the length of the sequence is.
</bodyText>
<sectionHeader confidence="0.8509" genericHeader="method">
4 Comparison of the Four Loss Functions
</sectionHeader>
<bodyText confidence="0.999964111111111">
We now compare the performance of the four loss
functions described above. Although (Lafferty et
al., 2001) proposes a modification of the iterative
scaling algorithm for parameter estimation in se-
quential log-loss function optimization, gradient-
based methods have often found to be more efficient
for minimizing the convex loss function in Eq. (1)
(Minka, 2001). For this reason, we use a gradient
based method to optimize the above loss functions.
</bodyText>
<equation confidence="0.617338">
(2)
(1)
</equation>
<bodyText confidence="0.997088222222222">
Sequential Exp-loss function: This loss func-
tion, was first introduced in (Collins, 2000) for
NLP tasks with a structured output domain.
However, there, the sum is not over the whole
possible label sequence set, but over the
best label sequences generated by an external
mechanism. Here we include all possible la-
bel sequences; so we do not require an external
mechanism to identify the best sequences..
</bodyText>
<subsectionHeader confidence="0.994969">
4.1 Gradient Based Optimization
</subsectionHeader>
<bodyText confidence="0.9996755">
The gradients of the four loss function can be com-
puted as follows:
</bodyText>
<equation confidence="0.420302">
Sequential Log-loss function:
(3)
</equation>
<bodyText confidence="0.932756333333333">
where expectations are taken w.r.t. .
Thus at the optimum the empirical and ex-
pected values of the sufficient statistics are
equal. The loss function and the derivatives
can be calculated with one pass of the forward-
backward algorithm.
</bodyText>
<equation confidence="0.4407735">
Sequential Exp-loss function:
(4)
</equation>
<bodyText confidence="0.999701">
At the optimum the empirical values of the suf-
ficient statistics equals their conditional expec-
tations where the contribution of each instance
is weighted by the inverse conditional proba-
bility of the instance. Thus this loss function
focuses on the examples that have a lower con-
ditional probability, which are usually the ex-
amples that the model labels incorrectly. The
computational complexity is the same as the
log-loss case.
</bodyText>
<subsubsectionHeader confidence="0.704812">
Pointwise Log-loss function:
</subsubsectionHeader>
<bodyText confidence="0.999909555555555">
At the optimum the expected value of the suf-
ficient statistics conditioned on the observation
are equal to their expected value when also
conditioned on the correct label sequence .
The computations can be done using the dy-
namic programming described in (Kakade et
al., 2002), with the computational complexity
of the forward-backward algorithm scaled by a
constant.
</bodyText>
<subsubsectionHeader confidence="0.851046">
Pointwise Exp-loss function:
</subsubsectionHeader>
<bodyText confidence="0.9986875">
At the optimum the expected value of the suf-
ficient statistics conditioned on are equal to
the value when also conditioned on , where
each point is weighted by . Com-
putational complexity is the same as the log-
loss case.
</bodyText>
<subsectionHeader confidence="0.966199">
4.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.997848358974359">
Before presenting the experimental results of the
comparison of the four loss functions described
above, we describe our experimental setup. We ran
experiments on Part-of-Speech (POS) tagging and
Named-Entity-Recognition (NER) tasks.
For POS tagging, we used the Penn TreeBank cor-
pus. There are 47 individual labels in this corpus.
Following the convention in POS tagging, we used
a Tag Dictionary for frequent words. We used Sec-
tions 1-21 for training and Section 22 for testing.
For NER, we used a Spanish corpus which was
provided for the Special Session of CoNLL2002 on
NER. There are training and test data sets and the
training data consists of about 7200 sentences. The
individual label set in the corpus consists of 9 la-
bels: the beginning and continuation of Person, Or-
ganization, Location and Miscellaneous names and
nonname tags.
We used three different feature sets:
is the set of bigram features, i.e. the current
tag and the current word, the current tag and
previous tags.
consists of features and spelling fea-
tures of the current word (e.g. ”Is the current
word capitalized and the current tag is Person-
Beginning?”). Some of the spelling features,
which are mostly adapted from (Bikel et al.,
1999) are the last one, two and three letters of
the word; whether the first letter is lower case,
upper case or alphanumeric; whether the word
is capitalized and contains a dot; whether all the
letters are capitalized; whether the word con-
tains a hyphen.
includes features not only for the current
word but also for the words within a fixed win-
dow of size . is an instance of where
. An example of features for
is ”Does the previous word ends with a dot and
the current tag is Organization-Intermediate?”.
</bodyText>
<table confidence="0.980396333333333">
POS
94.91 94.57 94.90 94.66
95.68 95.25 95.71 95.31
</table>
<tableCaption confidence="0.9421605">
Table 1: Accuracy of POS tagging on Penn Tree-
Bank.
</tableCaption>
<bodyText confidence="0.999139285714286">
For NER, we used a window of size 3 (i.e. consid-
ered features for the previous and next words). Since
the Penn TreeBank is very large, including fea-
tures, i.e. incorporating the information in the neigh-
boring words directly to the model, is intractable.
Therefore, we limited our experiments to and
features for POS tagging.
</bodyText>
<subsectionHeader confidence="0.998546">
4.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999980387096774">
As a gradient based optimization method, we used
an off-the-shelf optimization tool that uses the
limited-memory updating method. We observed that
this method is faster to converge than the conju-
gate gradient descent method. It is well known that
optimizing log-loss functions may result in over-
fitting, especially with noisy data. For this rea-
son, we used a regularization term in our cost func-
tions. We experimented with different regularization
terms. As expected, we observed that the regular-
ization term increases the accuracy, especially when
the training data is small; but we did not observe
much difference when we used different regulariza-
tion terms. The results we report are with the Gaus-
sian prior regularization term described in (Johnson
et al., 1999).
Our goal in this paper is not to build the best tag-
ger or recognizer, but to compare different loss func-
tions and optimization methods. Since we did not
spend much effort on designing the most useful fea-
tures, our results are slightly worse than, but compa-
rable to the best performing models.
We extracted corpora of different sizes (ranging
from 300 sentences to the complete corpus) and ran
experiments optimizing the four loss functions us-
ing different feature sets. In Table 1 and Table 2,
we report the accuracy of predicting every individ-
ual label. It can be seen that the test accuracy ob-
tained by different loss functions lie within a rela-
tively small range and the best performance depends
on what kind of features are included in the model.
</bodyText>
<table confidence="0.99594875">
NER
59.92 59.68 56.73 58.26
69.75 67.30 68.28 69.51
73.62 72.11 73.17 73.82
</table>
<tableCaption confidence="0.903715">
Table 2: F1 measure of NER on Spanish newswire
corpus. The window size is 3 for .
</tableCaption>
<bodyText confidence="0.996934571428571">
We observed similar behavior when the training set
is smaller. The accuracy is highest when more fea-
tures are included to the model. From these results
we conclude that when the model is the same, opti-
mizing different loss functions does not have much
effect on the accuracy, but increasing the variety of
the features included in the model has more impact.
</bodyText>
<sectionHeader confidence="0.999772" genericHeader="method">
5 Optimization methods
</sectionHeader>
<bodyText confidence="0.99995625">
In Section 4, we showed that optimizing differ-
ent loss function does not have a large impact on
the accuracy. In this section, we investigate differ-
ent methods of optimization. The conjugate based
method used in Section 4 is an exact method. If
the training corpus is large, the training may take
a long time, especially when the number of features
are very large. In this method, the optimization is
done in a parallel fashion by updating all of the pa-
rameters at the same time. Therefore, the resulting
classifier uses all the features that are included in the
model and lacks sparseness.
We now consider two approximation methods to
optimize two of the loss functions described above.
We first present a perceptron algorithm for labelling
sequences. This algorithm performs parallel opti-
mization and is an approximation of the sequential
log-loss optimization. Then, we present a boosting
algorithm for label sequence learning. This algo-
rithm performs sequential optimization by updating
one parameter at a time. It optimizes the sequential
exp-loss function. We compare these methods with
the exact method using the experimental setup pre-
sented in Section 4.2.
</bodyText>
<subsectionHeader confidence="0.975791">
5.1 Perceptron Algorithm for Label Sequences
</subsectionHeader>
<bodyText confidence="0.990928444444445">
Calculating the gradients, i.e. the expectations of
features for every instance in the training corpus
can be computationally expensive if the corpus is
very large. In many cases, a single training instance
might be as informative as all of the corpus to update
the parameters. Then, an online algorithm which
makes updates by using one training example may
converge much faster than a batch algorithm. If the
distribution is peaked, one label is more likely than
others and the contribution of this label dominates
the expectation values. If we assume this is the case,
i.e. we make a Viterbi assumption, we can calculate
a good approximation of the gradients by consider-
ing only the most likely, i.e. the best label sequence
according to the current model. The following on-
line perceptron algorithm (Algorithm 1), presented
in (Collins, 2002), uses these two approximations:
Algorithm 1 Label sequence Perceptron algorithm .
At each iteration, the perceptron algorithm calcu-
lates an approximation of the gradient of the sequen-
tial log-loss function (Eq. 3) based on the current
training instance. The batch version of this algo-
rithm is a closer approximation of the optimization
of sequential log-loss, since the only approximation
is the Viterbi assumption. The stopping criteria may
be convergence, or a fixed number of iterations over
the training data.
</bodyText>
<subsectionHeader confidence="0.999467">
5.2 Boosting Algorithm for Label Sequences
</subsectionHeader>
<bodyText confidence="0.999643888888889">
The original boosting algorithm (AdaBoost), pre-
sented in (Schapire and Singer, 1999), is a sequen-
tial learning algorithm to induce classifiers for sin-
gle random variables. (Altun et al., 2002) presents a
boosting algorithm for learning classifiers to predict
label sequences. This algorithm minimizes an upper
bound on the sequential exp-loss function (Eq. 2).
As in AdaBoost, a distribution over observations is
defined:
</bodyText>
<page confidence="0.465145">
(5)
</page>
<table confidence="0.99917325">
NER Perceptron Boosting
59.92 59.77 59.68 48.23
69.75 69.29 67.30 66.11
73.62 72.97 72.11 71.07
</table>
<tableCaption confidence="0.999745">
Table 3: F1 of different methods for NER
</tableCaption>
<bodyText confidence="0.979894181818182">
This distribution which expresses the importance of
every training instance is updated at each round, and
the algorithm focuses on the more difficult exam-
ples. The sequence Boosting algorithm (Algorithm
2) optimizes an upper bound on the sequential exp-
loss function by using the convexity of the exponen-
tial function. is the maximum difference of the
sufficient statistic in any label sequence and the
correct label sequence of any observation .
has a similar meaning. .
Algorithm 2 Label sequence Boosting algorithm.
</bodyText>
<sectionHeader confidence="0.702511" genericHeader="method">
6: end for
</sectionHeader>
<bodyText confidence="0.917738">
9: Update
10: until stopping criteria
As it can be seen from Line 4 in Algorithm 2, the
feature that was added to the ensemble at each round
is determined by a function of the gradient of the se-
quential exp-loss function (Eq. 4). At each round,
one pass of the forward backward algorithm over the
training data is sufficient to calculate’s for all.
Considering the sparseness of the features in each
training instance, one can restrict the forward back-
ward pass only to the training instances that contain
the feature that is added to the ensemble in the last
round. The stopping criteria may be a fixed number
of rounds, or by cross-validation on a heldout cor-
pus.
</bodyText>
<figure confidence="0.994269090909091">
1: initialize
2: repeat
3: for all training patterns do
4: compute
5: if then
7: end if
8: end for
9: until stopping criteria
1: initialize
2: repeat
3: for all features do
</figure>
<sectionHeader confidence="0.952531" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999984756756757">
The results summarized in Table 3 compares the per-
ceptron and the boosting algorithm with the gradi-
ent based method. Performance of the standard per-
ceptron algorithm fluctuates a lot, whereas the aver-
age perceptron is more stable. We report the results
of the average perceptron here. Not surprisingly, it
does slightly worse than CRF, since it is an approx-
imation of CRFs. The advantage of the Perceptron
algorithm is its dual formulation. In the dual form,
explicit feature mapping can be avoided by using the
kernel trick and one can have a large number of fea-
tures efficiently. As we have seen in the previous
sections, the ability to incorporate more features has
a big impact on the accuracy. Therefore, a dual per-
ceptron algorithm may have a large advantage over
other methods.
When only HMM features are used, Boosting as
a sequential algorithm performs worse than the gra-
dient based method that optimizes in a parallel fash-
ion. This is because there is not much information
in the HMM features other than the identity of the
word to be labeled. Therefore, the boosting algo-
rithm needs to include almost all the features one by
one in the ensemble. When there are just a few more
informative features, the boosting algorithm makes
better use of them. This situation is more dramatic
in POS tagging. Boosting gets 89.42% and 94.92%
accuracy for and features, whereas the gra-
dient based method gets 94.57% and 95.25%. The
gradient based method uses all of the available fea-
tures, whereas boosting uses only about 10% of the
features. Due to the loose upper bound that Boosting
optimizes, the estimate of the updates are very con-
servative. Therefore, the same features are selected
many times. This negatively effects the convergence
time, and the other methods outperform Boosting in
terms of training time.
</bodyText>
<sectionHeader confidence="0.993845" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999970428571429">
In this paper, we investigated how different objec-
tive functions and optimization methods affect the
accuracy of the sequence labelling task in the dis-
criminative learning framework. Our experiments
show that optimizing different objective functions
does not have a large affect on the accuracy. Ex-
tending the feature space is more effective. We con-
clude that methods that can use large, possibly infi-
nite number of features may be advantageous over
others. We are running experiments where we use a
dual formulation of the perceptron algorithm which
has the property of being able to use infinitely many
features. Our future work includes using SVMs for
label sequence learning task.
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993147">
Y. Altun, T. Hofmann, and M. Johnson. 2002. Discriminative
learning for label sequences via boosting. In Proceedings of
NIPS*15.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what’s in a
name. Machine Learning, 34(1-3):211–231.
M. Collins. 2000. Discriminative reranking for natural lan-
guage parsing. In Proceedings ofICML 2002.
M. Collins. 2002. Ranking algorithms for named-entity extrac-
tion: Boosting and the voted perceptron. In Proceedings of
ACL’02.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler. 1999.
Estimators for stochastic unification-based grammars. In
Proceedings ofACL’99.
S. Kakade, Y.W. Teh, and S. Roweis. 2002. An alternative
objective function for Markovian fields. In Proceedings of
ICML 2002.
Dan Klein and Christopher D. Manning. 2002. Conditional
structure versus conditional estimation in nlp models. In
Proceedings ofEMNLP 2002.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings ofICML2001.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum En-
tropy Markov Models for Information Extraction and Seg-
mentation. In Proceedings ofICML 2000.
T. Minka. 2001. Algorithms for maximum-likelihood logistic
regression. Technical report, CMU, Department of Statis-
tics, TR 758.
V. Punyakanok and D. Roth. 2000. The use of classifiers in
sequential inference. In Proceedings ofNIPS*13.
Adwait Ratnaparkhi. 1999. Learning to parse natural language
with maximum entropy models. Machine Learning, 34(1-
3):151–175.
R. Schapire and Y. Singer. 1999. Improved boosting algo-
rithms using confidence-rated predictions. Machine Learn-
ing, 37(3):297–336.
Kristina Toutanova and Christopher Manning. 2000. Enrich-
ing the knowledge sources used in a maximum entropy pos
tagger. In Proceedings ofEMNLP 2000.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.371409">
<title confidence="0.99983">Investigating Loss Functions and Optimization Methods for Discriminative Learning of Label Sequences</title>
<author confidence="0.823263">Yasemin</author>
<affiliation confidence="0.86215">Computer Brown</affiliation>
<address confidence="0.777868">Providence, RI</address>
<email confidence="0.997658">altun@cs.brown.edu</email>
<author confidence="0.976712">Mark</author>
<affiliation confidence="0.834949">Cognitive and Linguistic Brown</affiliation>
<address confidence="0.798704">Providence, RI</address>
<author confidence="0.9480215">Mark Johnsonbrown edu Thomas</author>
<affiliation confidence="0.994744">Computer Brown University</affiliation>
<address confidence="0.999133">Providence, RI 02912</address>
<email confidence="0.99972">th@cs.brown.edu</email>
<abstract confidence="0.993399">Discriminative models have been of interest in the NLP community in recent years. Previous research has shown that they are advantageous over generative models. In this paper, we investigate how different objective functions and optimization methods affect the performance of the classifiers in the discriminative learning framework. We focus on the sequence labelling problem, particularly POS tagging and NER tasks. Our experiments show that changing the objective function is not as effective as changing the features included in the model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Altun</author>
<author>T Hofmann</author>
<author>M Johnson</author>
</authors>
<title>Discriminative learning for label sequences via boosting.</title>
<date>2002</date>
<booktitle>In Proceedings of NIPS*15.</booktitle>
<contexts>
<context position="12262" citStr="Altun et al., 2002" startWordPosition="1963" endWordPosition="1966">on more similar to the evaluation function. One possible way of accomplishing this may be minimizing pointwise loss functions. Sequential optimizations optimize the joint conditional probability distribution , whereas pointwise optimizations that we propose optimize the marginal conditional probability distribution, . 3.3 Four Loss functions We derive four loss functions by taking the cross product of the two dimensions discussed above: Sequential Log-loss function: This function, based on the standard maximum likelihood optimization, is used with CRFs in (Lafferty et al., 2001). As shown in (Altun et al., 2002) it is possible to sum over all label sequences by using a dynamic algorithm. Note that the exponential loss function is just the inverse conditional probability plus a constant. Pointwise Log-loss function: This function optimizes the marginal probability of the labels at each position conditioning on the observation sequence: Obviously, this function reduces to the sequential log loss if the length of the sequence is. Pointwise Exp-loss function: Following the parallelism in log-loss vs exp-loss functions of sequential optimization (log vs inverse conditional probability), we propose minimiz</context>
<context position="22436" citStr="Altun et al., 2002" startWordPosition="3626" endWordPosition="3629">ulates an approximation of the gradient of the sequential log-loss function (Eq. 3) based on the current training instance. The batch version of this algorithm is a closer approximation of the optimization of sequential log-loss, since the only approximation is the Viterbi assumption. The stopping criteria may be convergence, or a fixed number of iterations over the training data. 5.2 Boosting Algorithm for Label Sequences The original boosting algorithm (AdaBoost), presented in (Schapire and Singer, 1999), is a sequential learning algorithm to induce classifiers for single random variables. (Altun et al., 2002) presents a boosting algorithm for learning classifiers to predict label sequences. This algorithm minimizes an upper bound on the sequential exp-loss function (Eq. 2). As in AdaBoost, a distribution over observations is defined: (5) NER Perceptron Boosting 59.92 59.77 59.68 48.23 69.75 69.29 67.30 66.11 73.62 72.97 72.11 71.07 Table 3: F1 of different methods for NER This distribution which expresses the importance of every training instance is updated at each round, and the algorithm focuses on the more difficult examples. The sequence Boosting algorithm (Algorithm 2) optimizes an upper boun</context>
</contexts>
<marker>Altun, Hofmann, Johnson, 2002</marker>
<rawString>Y. Altun, T. Hofmann, and M. Johnson. 2002. Discriminative learning for label sequences via boosting. In Proceedings of NIPS*15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard L Schwartz</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="16619" citStr="Bikel et al., 1999" startWordPosition="2662" endWordPosition="2665">ng and test data sets and the training data consists of about 7200 sentences. The individual label set in the corpus consists of 9 labels: the beginning and continuation of Person, Organization, Location and Miscellaneous names and nonname tags. We used three different feature sets: is the set of bigram features, i.e. the current tag and the current word, the current tag and previous tags. consists of features and spelling features of the current word (e.g. ”Is the current word capitalized and the current tag is PersonBeginning?”). Some of the spelling features, which are mostly adapted from (Bikel et al., 1999) are the last one, two and three letters of the word; whether the first letter is lower case, upper case or alphanumeric; whether the word is capitalized and contains a dot; whether all the letters are capitalized; whether the word contains a hyphen. includes features not only for the current word but also for the words within a fixed window of size . is an instance of where . An example of features for is ”Does the previous word ends with a dot and the current tag is Organization-Intermediate?”. POS 94.91 94.57 94.90 94.66 95.68 95.25 95.71 95.31 Table 1: Accuracy of POS tagging on Penn TreeB</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel M. Bikel, Richard L. Schwartz, and Ralph M. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning, 34(1-3):211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings ofICML</booktitle>
<contexts>
<context position="1245" citStr="Collins, 2000" startWordPosition="178" endWordPosition="179">on methods affect the performance of the classifiers in the discriminative learning framework. We focus on the sequence labelling problem, particularly POS tagging and NER tasks. Our experiments show that changing the objective function is not as effective as changing the features included in the model. 1 Introduction Until recent years, generative models were the most common approach for many NLP tasks. Recently, there is a growing interest on discriminative models in the NLP community, and these models were shown to be successful for different tasks(Lafferty et al., 2001; Ratnaparkhi, 1999; Collins, 2000). Discriminative models do not only have theoretical advantages over generative models, as we discuss in Section 2, but they are also shown to be empirically favorable over generative models when features and objective functions are fixed (Klein and Manning, 2002). In this paper, we use discriminative models to investigate the optimization of different objective functions by a variety of optimization methods. We focus on label sequence learning tasks. Part-ofSpeech (POS) tagging and Named Entity Recognition (NER) are the most studied applications among these tasks. However, there are many othe</context>
<context position="13587" citStr="Collins, 2000" startWordPosition="2170" endWordPosition="2171">length of the sequence is. 4 Comparison of the Four Loss Functions We now compare the performance of the four loss functions described above. Although (Lafferty et al., 2001) proposes a modification of the iterative scaling algorithm for parameter estimation in sequential log-loss function optimization, gradientbased methods have often found to be more efficient for minimizing the convex loss function in Eq. (1) (Minka, 2001). For this reason, we use a gradient based method to optimize the above loss functions. (2) (1) Sequential Exp-loss function: This loss function, was first introduced in (Collins, 2000) for NLP tasks with a structured output domain. However, there, the sum is not over the whole possible label sequence set, but over the best label sequences generated by an external mechanism. Here we include all possible label sequences; so we do not require an external mechanism to identify the best sequences.. 4.1 Gradient Based Optimization The gradients of the four loss function can be computed as follows: Sequential Log-loss function: (3) where expectations are taken w.r.t. . Thus at the optimum the empirical and expected values of the sufficient statistics are equal. The loss function a</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>M. Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings ofICML 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02.</booktitle>
<contexts>
<context position="21686" citStr="Collins, 2002" startWordPosition="3513" endWordPosition="3514"> as all of the corpus to update the parameters. Then, an online algorithm which makes updates by using one training example may converge much faster than a batch algorithm. If the distribution is peaked, one label is more likely than others and the contribution of this label dominates the expectation values. If we assume this is the case, i.e. we make a Viterbi assumption, we can calculate a good approximation of the gradients by considering only the most likely, i.e. the best label sequence according to the current model. The following online perceptron algorithm (Algorithm 1), presented in (Collins, 2002), uses these two approximations: Algorithm 1 Label sequence Perceptron algorithm . At each iteration, the perceptron algorithm calculates an approximation of the gradient of the sequential log-loss function (Eq. 3) based on the current training instance. The batch version of this algorithm is a closer approximation of the optimization of sequential log-loss, since the only approximation is the Viterbi assumption. The stopping criteria may be convergence, or a fixed number of iterations over the training data. 5.2 Boosting Algorithm for Label Sequences The original boosting algorithm (AdaBoost)</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Ranking algorithms for named-entity extraction: Boosting and the voted perceptron. In Proceedings of ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Geman</author>
<author>S Canon</author>
<author>Z Chi</author>
<author>S Riezler</author>
</authors>
<title>Estimators for stochastic unification-based grammars.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL’99.</booktitle>
<contexts>
<context position="18340" citStr="Johnson et al., 1999" startWordPosition="2952" endWordPosition="2955">t this method is faster to converge than the conjugate gradient descent method. It is well known that optimizing log-loss functions may result in overfitting, especially with noisy data. For this reason, we used a regularization term in our cost functions. We experimented with different regularization terms. As expected, we observed that the regularization term increases the accuracy, especially when the training data is small; but we did not observe much difference when we used different regularization terms. The results we report are with the Gaussian prior regularization term described in (Johnson et al., 1999). Our goal in this paper is not to build the best tagger or recognizer, but to compare different loss functions and optimization methods. Since we did not spend much effort on designing the most useful features, our results are slightly worse than, but comparable to the best performing models. We extracted corpora of different sizes (ranging from 300 sentences to the complete corpus) and ran experiments optimizing the four loss functions using different feature sets. In Table 1 and Table 2, we report the accuracy of predicting every individual label. It can be seen that the test accuracy obtai</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler. 1999. Estimators for stochastic unification-based grammars. In Proceedings ofACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kakade</author>
<author>Y W Teh</author>
<author>S Roweis</author>
</authors>
<title>An alternative objective function for Markovian fields.</title>
<date>2002</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="15041" citStr="Kakade et al., 2002" startWordPosition="2403" endWordPosition="2406">ntribution of each instance is weighted by the inverse conditional probability of the instance. Thus this loss function focuses on the examples that have a lower conditional probability, which are usually the examples that the model labels incorrectly. The computational complexity is the same as the log-loss case. Pointwise Log-loss function: At the optimum the expected value of the sufficient statistics conditioned on the observation are equal to their expected value when also conditioned on the correct label sequence . The computations can be done using the dynamic programming described in (Kakade et al., 2002), with the computational complexity of the forward-backward algorithm scaled by a constant. Pointwise Exp-loss function: At the optimum the expected value of the sufficient statistics conditioned on are equal to the value when also conditioned on , where each point is weighted by . Computational complexity is the same as the logloss case. 4.2 Experimental Setup Before presenting the experimental results of the comparison of the four loss functions described above, we describe our experimental setup. We ran experiments on Part-of-Speech (POS) tagging and Named-Entity-Recognition (NER) tasks. Fo</context>
</contexts>
<marker>Kakade, Teh, Roweis, 2002</marker>
<rawString>S. Kakade, Y.W. Teh, and S. Roweis. 2002. An alternative objective function for Markovian fields. In Proceedings of ICML 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Conditional structure versus conditional estimation in nlp models.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP</booktitle>
<contexts>
<context position="1509" citStr="Klein and Manning, 2002" startWordPosition="218" endWordPosition="221">ve as changing the features included in the model. 1 Introduction Until recent years, generative models were the most common approach for many NLP tasks. Recently, there is a growing interest on discriminative models in the NLP community, and these models were shown to be successful for different tasks(Lafferty et al., 2001; Ratnaparkhi, 1999; Collins, 2000). Discriminative models do not only have theoretical advantages over generative models, as we discuss in Section 2, but they are also shown to be empirically favorable over generative models when features and objective functions are fixed (Klein and Manning, 2002). In this paper, we use discriminative models to investigate the optimization of different objective functions by a variety of optimization methods. We focus on label sequence learning tasks. Part-ofSpeech (POS) tagging and Named Entity Recognition (NER) are the most studied applications among these tasks. However, there are many others, such as chunking, pitch accent prediction and speech edit detection. These tasks differ in many aspects, such as the nature of the label sequences (chunks or individual labels), their difficulty and evaluation methods. Given this variety, we think it is worthw</context>
<context position="8122" citStr="Klein and Manning, 2002" startWordPosition="1293" endWordPosition="1296">tional probability distribution defined by this model is : where ’s are the parameters to be estimated from the training corpus C and is a normalization term to assure a proper probability distribution. In order to simplify the notation, we introduce , which is the number of times feature is observed in pair and, , which is the linear combination of all the features with parameterization. is the sufficient statistic of . Then, we can rewrite as: . 3 Loss Functions for Label Sequences Given the theoretical advantages of discriminative models over generative models and the empirical support by (Klein and Manning, 2002), and that CRFs are the state-of-the-art among discriminative models for label sequences, we chose CRFs as our model, and trained by optimizing various objective functions with respect to the corpus . The application of these models to the label sequence problems vary widely. The individual labels might constitute chunks (e.g. Named-Entity Recognition, shallow parsing), or they may be single entries (e.g. POS tagging). The difficulty, therefore the accuracy of the tasks are very different from each other. The evaluation of the systems differ from one task to another, and the nature of the stat</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. Conditional structure versus conditional estimation in nlp models. In Proceedings ofEMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings ofICML2001.</booktitle>
<contexts>
<context position="1210" citStr="Lafferty et al., 2001" startWordPosition="172" endWordPosition="175">fferent objective functions and optimization methods affect the performance of the classifiers in the discriminative learning framework. We focus on the sequence labelling problem, particularly POS tagging and NER tasks. Our experiments show that changing the objective function is not as effective as changing the features included in the model. 1 Introduction Until recent years, generative models were the most common approach for many NLP tasks. Recently, there is a growing interest on discriminative models in the NLP community, and these models were shown to be successful for different tasks(Lafferty et al., 2001; Ratnaparkhi, 1999; Collins, 2000). Discriminative models do not only have theoretical advantages over generative models, as we discuss in Section 2, but they are also shown to be empirically favorable over generative models when features and objective functions are fixed (Klein and Manning, 2002). In this paper, we use discriminative models to investigate the optimization of different objective functions by a variety of optimization methods. We focus on label sequence learning tasks. Part-ofSpeech (POS) tagging and Named Entity Recognition (NER) are the most studied applications among these </context>
<context position="5995" citStr="Lafferty et al., 2001" startWordPosition="935" endWordPosition="938">se in HMMs. For this reason, HMMs have been standardly used with current word-current label, and previous label(s)-current label features. However, if we incorporate information about the neighboring words and/or information about more detailed characteristics of the current word directly to our model, rather than propagating it through the previous labels, we may hope to learn a better classifier. Many different models, such as Maximum Entropy Markov Models (MEMMs) (McCallum et al., 2000), Projection based Markov Models (PMMs) (Punyakanok and Roth, 2000) and Conditional Random Fields (CRFs) (Lafferty et al., 2001), have been proposed to overcome these problems. The common property of these models is their discriminative approach. They model the probability distribution of the label sequences given the observation sequences: . The best performing models of label sequence learning are MEMMs or PMMs (also known as Maximum Entropy models) whose features are carefully designed for the specific tasks (Ratnaparkhi, 1999; Toutanova and Manning, 2000). However, maximum entropy models suffer from the so called label bias problem, the problem of making local decisions (Lafferty et al., 2001). Lafferty et al. (200</context>
<context position="12228" citStr="Lafferty et al., 2001" startWordPosition="1956" endWordPosition="1959">er classifiers by using a loss function more similar to the evaluation function. One possible way of accomplishing this may be minimizing pointwise loss functions. Sequential optimizations optimize the joint conditional probability distribution , whereas pointwise optimizations that we propose optimize the marginal conditional probability distribution, . 3.3 Four Loss functions We derive four loss functions by taking the cross product of the two dimensions discussed above: Sequential Log-loss function: This function, based on the standard maximum likelihood optimization, is used with CRFs in (Lafferty et al., 2001). As shown in (Altun et al., 2002) it is possible to sum over all label sequences by using a dynamic algorithm. Note that the exponential loss function is just the inverse conditional probability plus a constant. Pointwise Log-loss function: This function optimizes the marginal probability of the labels at each position conditioning on the observation sequence: Obviously, this function reduces to the sequential log loss if the length of the sequence is. Pointwise Exp-loss function: Following the parallelism in log-loss vs exp-loss functions of sequential optimization (log vs inverse conditiona</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings ofICML2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum Entropy Markov Models for Information Extraction and Segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings ofICML</booktitle>
<contexts>
<context position="5867" citStr="McCallum et al., 2000" startWordPosition="916" endWordPosition="919">many NLP tasks. Another shortcoming of this model is that, due to its generative nature, overlapping features are difficult to use in HMMs. For this reason, HMMs have been standardly used with current word-current label, and previous label(s)-current label features. However, if we incorporate information about the neighboring words and/or information about more detailed characteristics of the current word directly to our model, rather than propagating it through the previous labels, we may hope to learn a better classifier. Many different models, such as Maximum Entropy Markov Models (MEMMs) (McCallum et al., 2000), Projection based Markov Models (PMMs) (Punyakanok and Roth, 2000) and Conditional Random Fields (CRFs) (Lafferty et al., 2001), have been proposed to overcome these problems. The common property of these models is their discriminative approach. They model the probability distribution of the label sequences given the observation sequences: . The best performing models of label sequence learning are MEMMs or PMMs (also known as Maximum Entropy models) whose features are carefully designed for the specific tasks (Ratnaparkhi, 1999; Toutanova and Manning, 2000). However, maximum entropy models s</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum Entropy Markov Models for Information Extraction and Segmentation. In Proceedings ofICML 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Minka</author>
</authors>
<title>Algorithms for maximum-likelihood logistic regression.</title>
<date>2001</date>
<tech>Technical report, CMU,</tech>
<institution>Department of Statistics,</institution>
<contexts>
<context position="13402" citStr="Minka, 2001" startWordPosition="2140" endWordPosition="2141">ptimization (log vs inverse conditional probability), we propose minimizing the pointwise exp-loss function below, which reduces to the standard multi-class exponential loss when the length of the sequence is. 4 Comparison of the Four Loss Functions We now compare the performance of the four loss functions described above. Although (Lafferty et al., 2001) proposes a modification of the iterative scaling algorithm for parameter estimation in sequential log-loss function optimization, gradientbased methods have often found to be more efficient for minimizing the convex loss function in Eq. (1) (Minka, 2001). For this reason, we use a gradient based method to optimize the above loss functions. (2) (1) Sequential Exp-loss function: This loss function, was first introduced in (Collins, 2000) for NLP tasks with a structured output domain. However, there, the sum is not over the whole possible label sequence set, but over the best label sequences generated by an external mechanism. Here we include all possible label sequences; so we do not require an external mechanism to identify the best sequences.. 4.1 Gradient Based Optimization The gradients of the four loss function can be computed as follows: </context>
</contexts>
<marker>Minka, 2001</marker>
<rawString>T. Minka. 2001. Algorithms for maximum-likelihood logistic regression. Technical report, CMU, Department of Statistics, TR 758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2000</date>
<booktitle>In Proceedings ofNIPS*13.</booktitle>
<contexts>
<context position="5934" citStr="Punyakanok and Roth, 2000" startWordPosition="925" endWordPosition="928">to its generative nature, overlapping features are difficult to use in HMMs. For this reason, HMMs have been standardly used with current word-current label, and previous label(s)-current label features. However, if we incorporate information about the neighboring words and/or information about more detailed characteristics of the current word directly to our model, rather than propagating it through the previous labels, we may hope to learn a better classifier. Many different models, such as Maximum Entropy Markov Models (MEMMs) (McCallum et al., 2000), Projection based Markov Models (PMMs) (Punyakanok and Roth, 2000) and Conditional Random Fields (CRFs) (Lafferty et al., 2001), have been proposed to overcome these problems. The common property of these models is their discriminative approach. They model the probability distribution of the label sequences given the observation sequences: . The best performing models of label sequence learning are MEMMs or PMMs (also known as Maximum Entropy models) whose features are carefully designed for the specific tasks (Ratnaparkhi, 1999; Toutanova and Manning, 2000). However, maximum entropy models suffer from the so called label bias problem, the problem of making </context>
</contexts>
<marker>Punyakanok, Roth, 2000</marker>
<rawString>V. Punyakanok and D. Roth. 2000. The use of classifiers in sequential inference. In Proceedings ofNIPS*13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1229" citStr="Ratnaparkhi, 1999" startWordPosition="176" endWordPosition="177">ions and optimization methods affect the performance of the classifiers in the discriminative learning framework. We focus on the sequence labelling problem, particularly POS tagging and NER tasks. Our experiments show that changing the objective function is not as effective as changing the features included in the model. 1 Introduction Until recent years, generative models were the most common approach for many NLP tasks. Recently, there is a growing interest on discriminative models in the NLP community, and these models were shown to be successful for different tasks(Lafferty et al., 2001; Ratnaparkhi, 1999; Collins, 2000). Discriminative models do not only have theoretical advantages over generative models, as we discuss in Section 2, but they are also shown to be empirically favorable over generative models when features and objective functions are fixed (Klein and Manning, 2002). In this paper, we use discriminative models to investigate the optimization of different objective functions by a variety of optimization methods. We focus on label sequence learning tasks. Part-ofSpeech (POS) tagging and Named Entity Recognition (NER) are the most studied applications among these tasks. However, the</context>
<context position="6402" citStr="Ratnaparkhi, 1999" startWordPosition="999" endWordPosition="1000">rent models, such as Maximum Entropy Markov Models (MEMMs) (McCallum et al., 2000), Projection based Markov Models (PMMs) (Punyakanok and Roth, 2000) and Conditional Random Fields (CRFs) (Lafferty et al., 2001), have been proposed to overcome these problems. The common property of these models is their discriminative approach. They model the probability distribution of the label sequences given the observation sequences: . The best performing models of label sequence learning are MEMMs or PMMs (also known as Maximum Entropy models) whose features are carefully designed for the specific tasks (Ratnaparkhi, 1999; Toutanova and Manning, 2000). However, maximum entropy models suffer from the so called label bias problem, the problem of making local decisions (Lafferty et al., 2001). Lafferty et al. (2001) show that CRFs overcome the label-bias problem and outperform MEMMs in POS tagging. CRFs define a probability distribution over the whole sequence , globally conditioning over the whole observation sequence (Figure 1b). Because they condition on the observation (as opposed to generating it), they can use overlapping features. The features used in this paper are of the form: 1. Current label and inform</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34(1-3):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved boosting algorithms using confidence-rated predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="22328" citStr="Schapire and Singer, 1999" startWordPosition="3608" endWordPosition="3611"> approximations: Algorithm 1 Label sequence Perceptron algorithm . At each iteration, the perceptron algorithm calculates an approximation of the gradient of the sequential log-loss function (Eq. 3) based on the current training instance. The batch version of this algorithm is a closer approximation of the optimization of sequential log-loss, since the only approximation is the Viterbi assumption. The stopping criteria may be convergence, or a fixed number of iterations over the training data. 5.2 Boosting Algorithm for Label Sequences The original boosting algorithm (AdaBoost), presented in (Schapire and Singer, 1999), is a sequential learning algorithm to induce classifiers for single random variables. (Altun et al., 2002) presents a boosting algorithm for learning classifiers to predict label sequences. This algorithm minimizes an upper bound on the sequential exp-loss function (Eq. 2). As in AdaBoost, a distribution over observations is defined: (5) NER Perceptron Boosting 59.92 59.77 59.68 48.23 69.75 69.29 67.30 66.11 73.62 72.97 72.11 71.07 Table 3: F1 of different methods for NER This distribution which expresses the importance of every training instance is updated at each round, and the algorithm f</context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>R. Schapire and Y. Singer. 1999. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3):297–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy pos tagger.</title>
<date>2000</date>
<booktitle>In Proceedings ofEMNLP</booktitle>
<contexts>
<context position="6432" citStr="Toutanova and Manning, 2000" startWordPosition="1001" endWordPosition="1004">s Maximum Entropy Markov Models (MEMMs) (McCallum et al., 2000), Projection based Markov Models (PMMs) (Punyakanok and Roth, 2000) and Conditional Random Fields (CRFs) (Lafferty et al., 2001), have been proposed to overcome these problems. The common property of these models is their discriminative approach. They model the probability distribution of the label sequences given the observation sequences: . The best performing models of label sequence learning are MEMMs or PMMs (also known as Maximum Entropy models) whose features are carefully designed for the specific tasks (Ratnaparkhi, 1999; Toutanova and Manning, 2000). However, maximum entropy models suffer from the so called label bias problem, the problem of making local decisions (Lafferty et al., 2001). Lafferty et al. (2001) show that CRFs overcome the label-bias problem and outperform MEMMs in POS tagging. CRFs define a probability distribution over the whole sequence , globally conditioning over the whole observation sequence (Figure 1b). Because they condition on the observation (as opposed to generating it), they can use overlapping features. The features used in this paper are of the form: 1. Current label and information about the observation se</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher Manning. 2000. Enriching the knowledge sources used in a maximum entropy pos tagger. In Proceedings ofEMNLP 2000.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>