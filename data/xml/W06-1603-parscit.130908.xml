<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.989036">
Paraphrase Recognition via Dissimilarity Significance Classification
</title>
<author confidence="0.991518">
Long Qiu, Min-Yen Kan and Tat-Seng Chua
</author>
<affiliation confidence="0.998314">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.986222">
Singapore, 117543
</address>
<email confidence="0.999192">
{qiul,kanmy,chuats}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.994806" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999858263157895">
We propose a supervised, two-phase
framework to address the problem of para-
phrase recognition (PR). Unlike most PR
systems that focus on sentence similarity,
our framework detects dissimilarities be-
tween sentences and makes its paraphrase
judgment based on the significance of such
dissimilarities. The ability to differenti-
ate significant dissimilarities not only re-
veals what makes two sentences a non-
paraphrase, but also helps to recall addi-
tional paraphrases that contain extra but
insignificant information. Experimental
results show that while being accurate
at discerning non-paraphrasing dissimilar-
ities, our implemented system is able to
achieve higher paraphrase recall (93%), at
an overall performance comparable to the
alternatives.
</bodyText>
<sectionHeader confidence="0.998893" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999289">
The task of sentence-level paraphrase recognition
(PR) is to identify whether a set of sentences (typ-
ically, a pair) are semantically equivalent. In such
a task, “equivalence” takes on a relaxed meaning,
allowing sentence pairs with minor semantic dif-
ferences to still be considered as paraphrases.
PR can be thought of as synonym detection ex-
tended for sentences, and it can play an equally
important role in natural language applications.
As with synonym detection, applications such as
summarization can benefit from the recognition
and canonicalization of concepts and actions that
are shared across multiple documents. Automatic
construction of large paraphrase corpora could
mine alternative ways to express the same con-
cept, aiding machine translation and natural lan-
guage generation applications.
In our work on sentence-level PR, we have iden-
tified two main issues through observation of sam-
ple sentences. The first is to identify all discrete in-
formation nuggets, or individual semantic content
units, shared by the sentences. For a pair of sen-
tences to be deemed a paraphrase, they must share
a substantial amount of these nuggets. A trivial
case is when both sentences are identical, word
for word. However, paraphrases often employ dif-
ferent words or syntactic structures to express the
same concept. Figure 1 shows two sentence pairs,
in which the first pair is a paraphrase while the
second is not. The paraphrasing pair (also denoted
</bodyText>
<subsectionHeader confidence="0.34877875">
Paraphrase (+pp):
Authorities said a young man injured Richard Miller.
Richard Miller was hurt by a young man.
Non-Paraphrase (-pp):
</subsectionHeader>
<bodyText confidence="0.99221325">
The technology-laced Nasdaq Composite Index
.IXIC added 1.92 points, or 0.12 percent, at 1,647.94.
The technology-laced Nasdaq Composite Index
.IXIC dipped 0.08 of a point to 1,646.
</bodyText>
<figureCaption confidence="0.864407">
Figure 1: Examples: Paraphrasing &amp; Non-
paraphrasing
</figureCaption>
<bodyText confidence="0.999955166666667">
as the +pp class) use different words. Focusing
just on the matrix verbs, we note differences be-
tween “injured” and “hurt”. A paraphrase recogni-
tion system should be able to detect such semantic
similarities (despite the different syntactic struc-
tures). Otherwise, the two sentences could look
even less similar than two non-paraphrasing sen-
tences, such as the two in the second pair. Also in
the paraphrasing pair, the first sentence includes an
extra phrase “Authorities said”. Human annotators
tend to regard the pair as a paraphrase despite the
presence of this extra information nugget.
</bodyText>
<page confidence="0.980964">
18
</page>
<note confidence="0.8552255">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 18–26,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999895608695652">
This leads to the second issue: how to recognize
when such extra information is extraneous with
respect to the paraphrase judgment. Such para-
phrases are common in daily life. In news articles
describing the same event, paraphrases are widely
used, possibly with extraneous information.
We equate PR with solving these two issues,
presenting a natural two-phase architecture. In the
first phase, the nuggets shared by the sentences
are identified by a pairing process. In the second
phase, any unpaired nuggets are classified as sig-
nificant or not (leading to −pp and +pp classifica-
tions, respectively). If the sentences do not contain
unpaired nuggets, or if all unpaired nuggets are in-
significant, then the sentences are considered para-
phrases. Experiments on the widely-used MSR
corpus (Dolan et al., 2004) show favorable results.
We first review related work in Section 2. We
then present the overall methodology and describe
the implemented system in Section 3. Sections 4
and 5 detail the algorithms for the two phases re-
spectively. This is followed with our evaluation
and discussion of the results.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999732935064936">
Possibly the simplest approach to PR is an infor-
mation retrieval (IR) based “bag-of-words” strat-
egy. This strategy calculates a cosine similar-
ity score for the given sentence set, and if the
similarity exceeds a threshold (either empirically
determined or learned from supervised training
data), the sentences are paraphrases. PR systems
that can be broadly categorized as IR-based in-
clude (Corley and Mihalcea, 2005; Brockett and
Dolan, 2005). In the former work, the authors
defined a directional similarity formula reflect-
ing the semantic similarity of one text “with re-
spect to” another. A word contributes to the di-
rectional similarity only when its counterpart has
been identified in the opposing sentence. The as-
sociated word similarity scores, weighted by the
word’s specificity (represented as inverted docu-
ment frequency, idf), sum to make up the direc-
tional similarity. The mean of both directions
is the overall similarity of the pair. Brockett
and Dolan (2005) represented sentence pairs as
a feature vector, including features (among oth-
ers) for sentence length, edit distance, number of
shared words, morphologically similar word pairs,
synonym pairs (as suggested by WordNet and a
semi-automatically constructed thesaurus). A sup-
port vector machine is then trained to learn the
{+pp, −pp} classifier.
Strategies based on bags of words largely ig-
nore the semantic interactions between words.
Weeds et al. (2005) addressed this problem by
utilizing parses for PR. Their system for phrasal
paraphrases equates paraphrasing as distributional
similarity of the partial sub-parses of a candidate
text. Wu (2005)’s approach relies on the genera-
tive framework of Inversion Transduction Gram-
mar (ITG) to measure how similar two sentences
arrange their words based on edit distance.
Barzilay and Lee (2003) proposed to apply
multiple-sequence alignment (MSA) for tradi-
tional, sentence-level PR. Given multiple articles
on a certain type of event, sentence clusters are
first generated. Sentences within the same clus-
ter, presumably similar in structure and content,
are then used to construct a lattice with “back-
bone” nodes corresponding to words shared by the
majority and “slots” corresponding to different re-
alization of arguments. If sentences from differ-
ent clusters have shared arguments, the associated
lattices are claimed to be paraphrase. Likewise,
Shinyama et al. (2002) extracted paraphrases from
similar news articles, but use shared named enti-
ties as an indication of paraphrasing. It should be
noted that the latter two approaches are geared to-
wards acquiring paraphrases rather than detecting
them, and as such have the disadvantage of requir-
ing a certain level of repetition among candidates
for paraphrases to be recognized.
All past approaches invariably aim at a proper
similarity measure that accounts for all of the
words in the sentences in order to make a judg-
ment for PR. This is suitable for PR where in-
put sentences are precisely equivalent semanti-
cally. However, for many people the notion of
paraphrases also covers cases in which minor or
irrelevant information is added or omitted in can-
didate sentences, as observed in the earlier ex-
ample. Such extraneous content should not be a
barrier to PR if the main concepts are shared by
the sentences. Approaches that focus only on the
similarity of shared contents may fail when the
(human) criteria for PR include whether the un-
matched content is significant or not. Correctly
addressing this problem should increase accuracy.
In addition, if extraneous portions of sentences
can be identified, their confounding influence on
the sentence similarity judgment can be removed,
</bodyText>
<page confidence="0.998552">
19
</page>
<bodyText confidence="0.997342">
leading to more accurate modeling of semantic
similarity for both recognition and acquisition.
</bodyText>
<sectionHeader confidence="0.996593" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999858">
As noted earlier, for a pair of sentences to be a
paraphrase, they must possess two attributes:
</bodyText>
<listItem confidence="0.9392596">
1. similarity: they share a substantial amount of
information nuggets;
2. dissimilarities are extraneous: if extra infor-
mation in the sentences exists, the effect of
its removal is not significant.
</listItem>
<bodyText confidence="0.999731583333333">
A key decision for our two-phase PR framework
is to choose the representation of an information
nugget. A simple approach is to use representative
words as information nuggets, as is done in the
SimFinder system (Hatzivassiloglou et al., 2001).
Instead of using words, we choose to equate in-
formation nuggets with predicate argument tuples.
A predicate argument tuple is a structured repre-
sentation of a verb predicate together with its argu-
ments. Given a sentence from the example in Fig-
ure 1, its predicate argument tuple form in Prop-
Bank (Kingsbury et al., 2002) format is:
</bodyText>
<sectionHeader confidence="0.314435" genericHeader="method">
target(predicate): hurt
</sectionHeader>
<subsectionHeader confidence="0.712962">
arg0: a young man
arg1: Richard Miller
</subsectionHeader>
<bodyText confidence="0.999986260869566">
We feel that this is a better choice for the repre-
sentation of a nugget as it accounts for the action,
concepts and their relationships as a single unit.
In comparison, using fine-grained units such as
words, including nouns and verbs may result in in-
accuracy (sentences that share vocabulary may not
be paraphrases), while using coarser-grained units
may cause key differences to be missed. In the rest
of this paper, we use the term tuple for conciseness
when no ambiguity is introduced.
An overview of our paraphrase recognition sys-
tem is shown in Figure 2. A pair of sentences is
first fed to a syntactic parser (Charniak, 2000) and
then passed to a semantic role labeler (ASSERT;
(Pradhan et al., 2004)), to label predicate argu-
ment tuples. We then calculate normalized tuple
similarity scores over the tuple pairs using a met-
ric that accounts for similarities in both syntactic
structure and content of each tuple. A thesaurus
constructed from corpus statistics (Lin, 1998) is
utilized for the content similarity.
We utilize this metric to greedily pair together
the most similar predicate argument tuples across
</bodyText>
<figureCaption confidence="0.994789">
Figure 2: System architecture
</figureCaption>
<bodyText confidence="0.999911333333333">
sentences. Any remaining unpaired tuples repre-
sent extra information and are passed to a dissim-
ilarity classifier to decide whether such informa-
tion is significant. The dissimilarity classifier uses
supervised machine learning to make such a deci-
sion.
</bodyText>
<sectionHeader confidence="0.800253" genericHeader="method">
4 Similarity Detection and Pairing
</sectionHeader>
<bodyText confidence="0.999050692307692">
We illustrate this advantage of using predicate ar-
gument tuples from our running example. In Ta-
ble 1, one of the model sentences is shown in the
middle column. Two edited versions are shown on
the left and right columns. While it is clear that
the left modification is an example of a paraphrase
and the right is not, the version on the left in-
volves more changes in its syntactic structure and
vocabulary. Standard word or syntactic similar-
ity measures would assign the right modification a
higher similarity score, likely mislabeling one or
both modifications.
In contrast, semantic role labeling identifies the
dependencies between predicates and their argu-
ments, allowing a more precise measurement of
sentence similarity. Assuming that the arguments
in predicate argument tuples are assigned the same
role when their roles are comparables, we define
the similarity score of two tuples Ta and Tb as
the weighted sum of the pairwise similarities of
all their shared constituents C=1(ca, cb)I (c being
either the target or one of the arguments that both
&apos;ASSERT, which is trained on the Propbank, only guaran-
tees consistency of arg0 and arg1 slots, but we have found in
practice that aligning arg2 and above arguments do not cause
problems.
</bodyText>
<page confidence="0.982327">
20
</page>
<note confidence="0.984139333333333">
Sentence Modification 1: paraphrase Model Sentence Modification 2: non-paraphrase
Richard Miller was hurt by a Authorities said a young man in- Authorities said Richard Miller injured
young man. jured Richard Miller. a young man.
(Paired) target: said target: said
Tuples arg0: Authorities arg0: Authorities
arg1: a young man injured arg1: Richard Miller injured a
Richard Miller young man
target: hurt target: injured target: injured
arg0: arg0: Richard Miller
arg1: arg1: a young man
a young man arg0: a young man
Richard Miller arg1: Richard Miller
</note>
<tableCaption confidence="0.999273">
Table 1: Similarity Detection: pairing of predicate argument tuples
</tableCaption>
<figure confidence="0.7322132">
tuples have):
1 �
Sim(Ta, Tb) = Sim(ca, cb)~
a
c={target,argeharedl
</figure>
<bodyText confidence="0.9962075">
where normalization factor α is the sum of the
weights of constituents in C, i.e.:
</bodyText>
<equation confidence="0.921371">
a = Ifarg.haredlll + wtarget (2)
</equation>
<bodyText confidence="0.992951">
In our current implementation we reduce tar-
gets and their arguments to their syntactic head-
words. These headwords are then directly com-
pared using a corpus-based similarity thesaurus.
As we hypothesized that targets are more impor-
tant for predicate argument tuple similarity, we
multiply the target’s similarity by a weighting fac-
tor wtarget, whose value we have empirically de-
termined as 1.7, based on a 300-pair development
set from the MSR training set.
We then proceed to pair tuples in the two sen-
tences using a greedy iterative algorithm. The al-
gorithm locates the two most similar tuples from
each sentence, pairs them together and removes
them from futher consideration. The process stops
when subsequent best pairings are below the simi-
larity threshold or when all possible tuples are ex-
hausted. If unpaired tuples still exist in a given
sentence pair, we further examine the copular con-
structions and noun phrases in the opposing sen-
tence for possible pairings2. This results in a one-
2Copular constructions are not handled by ASSERT. Such
constructions account for a large proportion of the semantic
meaning in sentences. Consider the pair “Microsoft rose 50
cents” and “Microsoft was up 50 cents”, in which the second
is in copular form. Similarly, NPs can often be equivalent
to predicate argument tuples when actions are nominalized.
Consider an NP that reads “(be blamed for) frequent attacks
on soldiers” and a predicate argument tuple: “(be blamed for)
attacking soldiers”. Again, identical information is conveyed
but not captured by semantic role labeling. In such cases,
they can be paired if we allow a candidate tuple to pair with
the predicative argument (e.g., 50 cents) of a copula, or (the
head of) an NP in the opposing sentence. As these heuristic
matches may introduce errors, we resort to these methods of
matching tuple only in the contingency when there are un-
paired tuples.
to-one mapping with possibly some tuples left un-
paired. The curved arrows in Table 1 denote the
correct results of similarity pairing: two tuples are
paired up if their target and shared arguments are
identical or similar respectively, otherwise they re-
main unpaired even if the “bag of words” they con-
tain are the same.
</bodyText>
<sectionHeader confidence="0.9939485" genericHeader="method">
5 Dissimilarity Significance
Classification
</sectionHeader>
<bodyText confidence="0.999697833333333">
If some tuples remain unpaired, they are dissimilar
parts of the sentence that need to be labeled by the
dissimilarity classifier. Such unpaired informa-
tion could be extraneous or they could be semanti-
cally important, creating a barrier for paraphrase.
We frame this as a supervised machine learning
problem in which a set of features are used to
inform the classifier. A support vector machine,
SVMLZght, was chosen as the learning model as it
has shown to yield good performance over a wide
application range. We experimented with a wide
set of features of unpaired tuples, including inter-
nal counts of numeric expressions, named entities,
words, semantic roles, whether they are similar
to other tuples in the same sentence, and contex-
tual features like source/target sentence length and
paired tuple count. Currently, only two features
are correlated in improved classification, which
we detail now.
Syntactic Parse Tree Path: This is a series of
features that reflect how the unpaired tuple con-
nects with the context: the rest of the sentence.
It models the syntactic connection between the
constituents on both ends of the path (Gildea and
Palmer, 2002; Pradhan et al., 2004). Here, we
model the ends of the path as the unpaired tuple
and the paired tuple with the closest shared ances-
tor, and model the path itself as a sequence of con-
stituent category tags and directions to reach the
destination (the paired target) from the source (the
</bodyText>
<equation confidence="0.602188">
c==target
wtarget (1)
</equation>
<page confidence="0.99066">
21
</page>
<bodyText confidence="0.998498444444444">
unpaired target) via the the shared ancestor. When
no tuples have been paired in the sentence pair,
the destination defaults to the root of the syntactic
parse tree. For example, the tuples with target “in-
jured” are unpaired when the model sentence and
the non-paraphrasing modification in Table 1 are
being compared. A path “TV BD, TVP, TS, TSBAR
, TVP, IV BD” links a target “injured” to the paired
target “said”, as shown in Figure 3.
</bodyText>
<figureCaption confidence="0.990556">
Figure 3: Syntactic parse tree path
</figureCaption>
<bodyText confidence="0.965958206896552">
The syntactic path can act as partial evidence
in significance classification. In the above exam-
ple, the category tag “V BD” assigned to “injured”
indicates that the verb is in its past tense. Such
a predicate argument tuple bears the main con-
tent of the sentence and generally can not be ig-
nored if its meaning is missing in the opposing
sentence. Another example is shown in Figure
4. The second sentence has one unpaired target
“proposed” while the rest all find their counter-
part. The path we get from the syntactic parse tree
reads “TV BN, TNP, TS, ...”, showing that the un-
paired tuple (consisting of a single predicate) is a
modifier contained in an NP. It can be ignored if
there is no contradiction in the opposing sentence.
We represent a syntactic path by a set of n-gram
(n &lt; 4) features of subsequences of category tags
found in the path, along with the respective direc-
tion. We require these n-gram features to be no
more than four category tags away from the un-
paired target, as our primary concern is to model
what role the target plays in its sentence.
Sheena Young of Child, the national infertility sup-
port network, hoped the guidelines would lead to a more
“fair and equitable” service for infertility sufferers.
Sheena Young, a spokesman for Child, the national
infertility support network, said the proposed guide-
lines should lead to a more “fair and equitable” service
for infertility sufferers.
</bodyText>
<figureCaption confidence="0.9755945">
Figure 4: Unpaired predicate argument tuple as
modifier in a paraphrase
</figureCaption>
<bodyText confidence="0.967847">
Predicate: This is the lexical token of predi-
cate argument tuple’s target, as a text feature. As
this feature is liable to run into sparse data prob-
lems, the semantic category of the target would be
a more suitable feature. However, verb similar-
ity is generally regarded as difficult to measure,
both in terms of semantic relatedness as well as
in finding a consistent granularity for verb cate-
gories. We investigated using WordNet as well as
Levin’s classification (Levin, 1993) as additional
features on our validation data, but currently find
that using the lexical form of the target works best.
</bodyText>
<subsectionHeader confidence="0.985412">
5.1 Classifier Training Set Acquisition
</subsectionHeader>
<bodyText confidence="0.999981142857143">
Currently, no training corpus for predicate argu-
ment tuple significance exists. Such a corpus is in-
dispensable for training the classifier. Rather than
manually annotating training instances, we use
an automatic method to construct instances from
paraphrase corpora. This is possible as the para-
phrase judgments in the corpora can imply which
portion of the sentence(s) are significant barriers
to paraphrasing or not. Here, we exploit the simi-
larity detector implemented for the first phase for
this purpose. If unpaired tuples exist after greedy
pairing, we classify them along two dimensions:
whether the sentence pair is a (non-)paraphrase,
and the source of the unpaired tuples:
</bodyText>
<listItem confidence="0.995325333333333">
1. [PS] paraphrasing pairs and unpaired predicate argu-
ment tuples are only from a single sentence;
2. [NS] non-paraphrasing pairs and only one single un-
paired predicate argument tuple exists;
3. [PM] paraphrasing pairs and unpaired predicate argu-
ment tuples are from multiple (both) sentences;
4. [NM] non-paraphrasing pairs and multiple unpaired
predicate argument tuples (from either one or both sen-
tences) exist.
</listItem>
<bodyText confidence="0.999906363636364">
Assuming that similarity detector pairs tuples
correctly, for the first two categories, the para-
phrasing judgment is directly linked to the un-
paired tuples. PS tuple instances are therefore
used as insignificant class instances, and NS as
significant ones. The last two categories can-
not be used for training data, as it is unclear which
of the unpaired tuples is responsible for the (non-)
paraphrasing as the similarity measure may mis-
takenly leave some similar predicate argument tu-
ples unpaired.
</bodyText>
<sectionHeader confidence="0.998789" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.988237">
The goal of our evaluation is to show that our sys-
tem can reliably determine the cause(s) of non-
</bodyText>
<figure confidence="0.996452428571428">
VP
����� �����
VBD SBAR
said S
���� ����
NP VP
NNP
Richard
NNP
Miller
� � ��
VBD NP
injured
� � ��
</figure>
<page confidence="0.991437">
22
</page>
<table confidence="0.998891545454546">
MSR Corpus Label +pp -pp total
system prediction correct? T F T F
# sentence pairs (s-ps) 85 23 55 37 200
# labelings (H&amp;C agree) 80 19 53 35 187
# tuple pairs (t-ps) (S) 80 6 36 35 157
# correct t-ps (H&amp;S agree) 74 6 34 30 144
# missed t-ps (H) 11 10 5 5 31
# sig. unpaired tuples(H) 6 4 69 51 130
# sig. unpaired tuples(S) 0 32 70 0 102
# sig. unpaired tuples(H&amp;S) 0 4 43 0 47
# -pp for other reasons 0 0 5 2 7
</table>
<tableCaption confidence="0.98888">
Table 2: (H)uman annotations vs. (C)orpus anno-
tations and (S)ystem output
</tableCaption>
<bodyText confidence="0.994209666666667">
paraphrase examples, while maintaining the per-
formance level of the state-of-the-art PR systems.
For evaluation, we conduct both component
evaluations as well as a holistic one, resulting in
three separate experiments. In evaluating the first
tuple pairing component, we aim for high preci-
sion, so that sentences that have all tuples paired
can be safely assumed to be paraphrases. In evalu-
ating the dissimilarity classifier, we simply aim for
high accuracy. In our overall system evaluation,
we compare our system versus other PR systems
on standard corpora.
Experimental Data Set. For these experi-
ments, we utilized two widely-used corpora for
paraphrasing evaluation: the MSR and PASCAL
RTE corpora. The Microsoft Research Paraphrase
coupus (Dolan et al., 2004) consists of 5801
newswire sentence pairs, 3900 of which are an-
notated as semantically equivalent by human an-
notators. It reflects ordinary paraphrases that peo-
ple often encounter in news articles, and may be
viewed as a typical domain-general paraphrase
recognition task that downstream NLP systems
will need to deal with. The corpus comes divided
into standard training (70%) and testing (30%) di-
visions, a partition we follow in our experiments.
ASSERT (the semantic role labeler) shows for this
corpus a sentence contains 2.24 predicate argu-
ment tuples on average. The second corpus is
the paraphrase acquisition subset of the PASCAL
Recognizing Textual Entailment (RTE) Challenge
corpus (Dagan et al., 2005). This is much smaller,
consisting of 50 pairs, which we employ for test-
ing only to show portability.
To assess the component performance, we need
additional ground truth beyond the {+pp, −pp}
labels provided by the corpora. For the first eval-
uation, we need to ascertain whether a sentence
pair’s tuples are correctly paired, misidentified or
mispaired. For the second, which tuple(s) (if any)
are responsible for a −pp instance. However, cre-
ating ground truth by manual annotation is expen-
sive, and thus we only sampled the data set to get
an indicative assessment of performance. We sam-
pled 200 random instances from the total MSR
testing set, and first processed them through our
framework. Then, five human annotators (two au-
thors and three volunteers) annotated the ground
truth for tuple pairing and the semantic signifi-
cance of the unpaired tuples, while checking sys-
tem output. They also independently came up with
their own {+pp,-pp} judgment so we could assess
the reliability of the provided annotations.
The results of this annotation is shown in Ta-
ble 2. Examining this data, we can see that the
similarity detector performs well, despite its sim-
plicity and assumption of a one-to-one mapping.
Out of the 157 predicate argument tuple pairs
identified through similarity detection, annotators
agreed that 144 (92%) are semantically similar or
equivalent. However, 31 similar pairs were missed
by the system, resulting in 82% recall. We defer
discussion on the other details of this table to Sec-
tion 7.
To assess the dissimilarity classifier, we focus
on the −pp subset of 55 instances recognized by
the system. For 43 unpaired tuples from 40 sen-
tence pairs (73% of 55), the annotators’ judgments
agree with the classifier’s claim that they are sig-
nificant. For these cases, the system is able to both
recognize that the sentence pair is not a paraphrase
and further correctly establish a cause of the non-
paraphrase.
In addition to this ground truth sampled evalu-
ation, we also show the effectiveness of the clas-
sifier by examining its performance on PS and NS
tuples in the MSR corpus as described in Section
5. The test set consists of 413 randomly selected
PS and NS instances among which 145 are signif-
icant (leading to non-paraphrases). The classifier
predicts predicate argument tuple significance at
an accuracy of 71%, outperforms a majority clas-
sifier (65%), a gain which is marginally statisti-
cally significant (p &lt; .09).
insignificant
263 insignificant by classifier
33 5 significant by classifier
We can see the classifier classifies the majority
of insignificant tuples correctly (by outputting a
significant
</bodyText>
<page confidence="0.9253095">
112
23
</page>
<table confidence="0.99956475">
Algorithm 709 Sentence Pairs Without 1016 Sentence Pairs With Overall
Unpaired Tuples Unpaired Tuples (100% of Test set)
(41.1% of Test set) (58.9% of Test set)
Acc R P Acc R P Acc R P F1
Majority Classifier 79.5% 100% 79.5% 57.4% 100% 57.4% 66.5% 100% 66.5% 79.9%
SimFinder 82.2% 92.2% 86.4% 66.3% 84.9% 66.1% 72.9% 88.5% 75.1% 81.3%
CM05 - - - - - - 71.5% 92.5% 72.3% 81.2%
Our System 79.5% 100% 79.5% 66.7% 87.0% 66.0% 72.0% 93.4% 72.5% 81.6%
</table>
<tableCaption confidence="0.999105">
Table 3: Results on MSR test set
</tableCaption>
<table confidence="0.999881142857143">
17 Sentence Pairs Without 33 Sentence Pairs With
Algorithm Unpaired Tuples Unpaired Tuples Overall
(34% of Test set) (66% of Test set) (100% of Test set)
Acc R P Acc R P Acc R P F1
Majority Classifier 65% 100% 65% 42% 100% 42% 50% 100% 50% 67%
SimFinder 71% 91% 71% 42% 21% 27% 52% 52% 52% 52%
Our System 65% 100% 65% 48% 64% 43% 54% 80% 53% 64%
</table>
<tableCaption confidence="0.999914">
Table 4: Results on PASCAL PP test set
</tableCaption>
<bodyText confidence="0.998481428571429">
score greater than zero), which is effectively a
98% recall of insignificant tuples. However, the
precision is less satisfatory. We suspect this is par-
tially due the tuples that fail to be paired up with
their counterpart. Such noise is found among the
automatically collected PS instances used in train-
ing.
</bodyText>
<figureCaption confidence="0.987619">
Figure 5: Dissimilarity classifier performance
</figureCaption>
<bodyText confidence="0.99994908">
For the final system-wide evaluation, we imple-
mented two baseline systems: a majority classifier
and SimFinder (Hatzivassiloglou et al., 2001), a
bag-of-words sentence similarity module incorpo-
rating lexical, syntactic and semantic features. In
Table 3, precision and recall are measured with re-
spect to the paraphrasing class. The table shows
sentence pairs falling under the column “pairs
without unpaired tuples” are more likely to be
paraphrasing than an arbitrary pair (79.5% ver-
sus 66.5%), providing further validation for using
predicate argument tuples as information nuggets.
The results for the experiment benchmarking the
overall system performance are shown under the
“Overall” column: our approach performs compa-
rably to the baselines at both accuracy and para-
phrase recall. The system performance reported in
(CM05; (Corley and Mihalcea, 2005)), which is
among the best we are aware of, is also included
for comparison.
We also ran our system (trained on the MSR
corpus) on the 50 instances in the PASCAL para-
phrase acquisition subset. Again, the system per-
formance (as shown in Table 4) is comparable to
the baseline systems.
</bodyText>
<sectionHeader confidence="0.997087" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999827">
We have just shown that when two sentences have
perfectly matched predicate argument tuples, they
are more likely to be a paraphrase than a random
sentence pair drawn from the corpus. Further-
more, in the sampled human evaluation in Table
2, among the 88 non-paraphrasing instances with
whose MSR corpus labels our annotators agreed
(53 correctly and 35 incorrectly judged by our sys-
tem), the cause of the −pp is correctly attributed
in 81 cases to one or more predicate argument tu-
ples. The remaining 7 cases (as shown in the last
row) are caused by phenomenon that are not cap-
tured by our tuple representation. We feel this jus-
tifies using predicate argument tuples as informa-
tion nuggets, but we are currently considering ex-
panding our representation to account for some of
these cases.
The evaluation also confirms the difficulty of
making paraphrase judgements. Although the
</bodyText>
<figure confidence="0.996425571428571">
Frequency
180
160
140
120
100
80
40
20
60
0
Insignificant
Significant
SVM Prediction
</figure>
<page confidence="0.99491">
24
</page>
<bodyText confidence="0.999197243902439">
MSR corpus used strict means of resolving inter-
rater disagreements during its construction, the an-
notators agreed with the MSR corpus labels only
93.5% (187/200) of the time.
One weakness of our system is that we rely on a
thesaurus (Lin, 1998) for word similarity informa-
tion for predicate argument tuple pairing. How-
ever, it is designed to provide similarity scores
between pairs of individual words (rather than
phrases). If a predicate argument tuple’s target or
one argument is realized as a phrase (borrow —*
check out, for instance), the thesaurus is unable to
provide an accurate similarity score. For similarity
between predicate argument tuples, negation and
modality have yet to be addressed, although they
account for only a tiny fraction of the corpus.
We further estimated how the similarity detec-
tor can be affected when the semantic role labeler
makes mistakes (by failing to identify arguments
or assigning incorrect role names). Checking 94
pairs ground-truth similar tuples, we found that the
system mislabels 43 of them. The similarity detec-
tor failed to pair around 30% of them. In compar-
sion, all the tuple pairs free of labeling errors are
correctly paired. A saving grace is that labeling
errors rarely lead to incorrect pairing (one pairing
in all the examined sentences). The labeling er-
rors impact the whole system in two ways: 1) they
caused similar tuples that should have been paired
up to be added as noise in that dissimilarity clas-
sifier’s training set and 2) paired tuples with label-
ing errors erroneously increase the target weight
in Equation (1).
Some example paraphrasing cases that are prob-
lematic for our current system are:
1. Non-literal language issues such as implica-
ture, idiom, metaphor, etc. are not addressed in
our current system. When predicate argument tu-
ples imply each other, they are less similar than
what our system currently is trained for, causing
the pairing to fail:
</bodyText>
<construct confidence="0.9085424">
+pp, Later in the day, a standoff developed between French
soldiers and a Hema battlewagon that attempted to pass the
UN compound.
French soldiers later threatened to open fire on a Hema bat-
tlewagon that tried to pass near the UN compound.
</construct>
<reference confidence="0.657102461538462">
2. A paraphrasing pair may exceed the systems’
threshold for syntactic difference:
+pp, With the exception of dancing, physical activity did not
decrease the risk.
Dancing was the only physical activity associated with a
lower risk of dementia.
3. One or more unpaired tuples exist, but their
significance is not inferred correctly:
+pp, Inhibited children tend to be timid with new people,
objects, and situations, while uninhibited children sponta-
neously approach them.
Simply put, shy individuals tend to be more timid with new
people and situations.
</reference>
<bodyText confidence="0.999001428571429">
In the MSR corpus, the first error case is more
frequent than the other two. We identify these as
challenging cases where idiomatic processing is
needed.
Below we show some unpaired predicate ar-
gument tuples (underlined) that are significant
enough to be paraphrase barriers. These examples
give an indicative categorization of what signifi-
cant tuples are and their corpus frequency (when
predicate argument tuples are the reasons; we ex-
amined 40 such cases for this estimation). There
is one thing in common: every case involves sub-
stantial information that is difficult to infer from
context. Such tuples appear as:
</bodyText>
<listItem confidence="0.8934745">
1. (40%) The nucleus of the sentence (often the
matrix tuple):
</listItem>
<reference confidence="0.639673739130435">
Michael Hill, a Sun reporter who is a member of the
Washington-Baltimore Newspaper Guild’s bargaining com-
mittee, estimated meetings to last late Sunday.
2. (30%) A part of a coordination:
Security lights have also been installed and police have
swept the grounds for booby traps.
3. (13%) A predicate of a modifying clause:
Westermayer was 26 then, and a friend and former manager
who knew she was unhappy in her job tipped her to another
position.
4. (7%) An adjunct:
While waiting for a bomb squad to arrive, the bomb exploded,
killing Wells.
5. (7%) An embedded sentence:
Dean told reporters traveling on his 10-city “Sleepless
Summer” tour that he considered campaigning in Texas a
challenge.
6. (3%) Or factual content that conflicts with
the opposing sentence:
Total sales for the period declined 8.0 percent to USD1.99
billion from a year earlier.
Wal-Mart said sales at stores open at least a year rose 4.6
percent from a year earlier.
</reference>
<sectionHeader confidence="0.99841" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999172">
We have proposed a new approach to the para-
phrase recognition (PR) problem: a supervised,
</bodyText>
<page confidence="0.991238">
25
</page>
<bodyText confidence="0.999988621621621">
two-phase framework emphasizing dissimilarity
classification. To emulate human PR judgment
in which insignificant, extraneous information
nuggets are generally allowed for a paraphrase,
we estimate whether such additional information
nuggets affect the final paraphrasing status of a
sentence pair. This approach, unlike previous PR
approaches, has the key benefit of explaining the
cause of a non-paraphrase sentence pair.
In the first, similarity detection module, using
predicate argument tuples as the unit for compar-
ison, we pair them up in a greedy manner. Un-
paired tuples thus represent additional information
unrepresented in the opposing sentence. A second,
dissimilarity classification module uses the lexical
head of the predicates and the tuples’ path of at-
tachment as features to decide whether such tuples
are barriers to paraphrase.
Our evaluations show that the system obtains 1)
high accuracy for the similarity detector in pairing
predicate argument tuples, 2) robust dissimilar-
ity classification despite noisy training instances
and 3) comparable overall performance to current
state-of-the-art PR systems. To our knowledge this
is the first work that tackles the problem of identi-
fying what factors stop a sentence pair from being
a paraphrase.
We also presented corpus examples that illus-
trate the categories of errors that our framework
makes, suggesting future work in PR. While we
continue to explore more suitable representation
of unpaired predicate argument tuples, we plan to
augment the similarity measure for phrasal units
to reduce the error rate in the first component. An-
other direction is to detect semantic redundancy in
a sentence. Unpaired tuples that are semantically
redundant should also be regarded as insignificant.
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999508137931034">
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL 2003.
Chris Brockett and Bill Dolan. 2005. Support vector ma-
chines for paraphrase identification and corpus construc-
tion. In Proceedings of the 3rd International Workshop on
Paraphrasing.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the First Annual Meeting of the
North American Chapter of the Association for Computa-
tional Linguistics (NAACL’2000).
Courtney Corley and Rada Mihalcea. 2005. Measuring the
semantic similarity of texts. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equiva-
lence and Entailment, pages 13–18, Ann Arbor, USA.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. In
PASCAL Proceedings of the First Challenge Workshop–
Recognizing Textual Entailment, Southampton,UK.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Exploiting
massively parallel news sources. In Proceedings of the
20th International Conference on Computational Linguis-
tics, Geneva, Switzerland.
Daniel Gildea and Martha Palmer. 2002. The necessity of
parsing for predicate argument recognition. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, Philadelphia, USA.
Vassileios Hatzivassiloglou, Judith Klavans, Melissa Hol-
combe, Regina Barzilay, Min-Yen Kan, and Kathleen
McKeown. 2001. Simfinder: A flexible clustering tool
for summarization. In Proceedings of the NAACL Work-
shop on Automatic Summarization, pages 41–49.
Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the penn treebank. In Pro-
ceedings of the Human Language Technology Conference,
San Diego, USA.
Beth Levin. 1993. English verb classes and alternations: A
preliminary investigation. University of Chicago Press.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL ’98, pages
768–774, Montreal, Canada.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Mar-
tin, and Dan Jurafsky. 2004. Shallow semantic pars-
ing using support vector machines. In Proceedings of
HLT/NAACL, Boston, USA.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and Ralph
Grishman. 2002. Automatic paraphrase acquisition from
news articles. In Proceedings of the Human Language
Technology Conference, pages 40–46, San Diego, USA.
Julie Weeds, David Weir, and Bill Keller. 2005. The dis-
tributional similarity of sub-parses. In Proceedings of the
ACL Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment, pages 7–12, Ann Arbor, USA.
Dekai Wu. 2005. Recognizing paraphrases and textual en-
tailment using inversion transduction grammars. In Pro-
ceedings of the ACL Workshop on Empirical Modeling of
Semantic Equivalence and Entailment, Ann Arbor, USA.
</reference>
<page confidence="0.998004">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.594314">
<title confidence="0.999845">Paraphrase Recognition via Dissimilarity Significance Classification</title>
<author confidence="0.986729">Long Qiu</author>
<author confidence="0.986729">Min-Yen Kan</author>
<author confidence="0.986729">Tat-Seng</author>
<affiliation confidence="0.9997615">Department of Computer National University of</affiliation>
<address confidence="0.619428">Singapore,</address>
<abstract confidence="0.9985324">We propose a supervised, two-phase framework to address the problem of paraphrase recognition (PR). Unlike most PR systems that focus on sentence similarity, our framework detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimilarities. The ability to differentiate significant dissimilarities not only reveals what makes two sentences a nonparaphrase, but also helps to recall additional paraphrases that contain extra but insignificant information. Experimental results show that while being accurate at discerning non-paraphrasing dissimilarities, our implemented system is able to achieve higher paraphrase recall (93%), at an overall performance comparable to the alternatives.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>A paraphrasing pair may exceed the systems’ threshold for syntactic difference: +pp, With the exception of dancing, physical activity did not decrease the risk. Dancing was the only physical activity associated with a lower risk of dementia. 3. One or more unpaired tuples exist, but their significance is not inferred correctly: +pp, Inhibited children tend to be timid with new people, objects, and situations, while uninhibited children spontaneously approach them.</title>
<marker></marker>
<rawString>2. A paraphrasing pair may exceed the systems’ threshold for syntactic difference: +pp, With the exception of dancing, physical activity did not decrease the risk. Dancing was the only physical activity associated with a lower risk of dementia. 3. One or more unpaired tuples exist, but their significance is not inferred correctly: +pp, Inhibited children tend to be timid with new people, objects, and situations, while uninhibited children spontaneously approach them.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Simply put</author>
</authors>
<title>shy individuals tend to be more timid with new people and situations.</title>
<marker>put, </marker>
<rawString>Simply put, shy individuals tend to be more timid with new people and situations.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Hill</author>
</authors>
<title>a Sun reporter who is a member of the Washington-Baltimore Newspaper Guild’s bargaining committee, estimated meetings to last late Sunday. 2. (30%) A part of a coordination: Security lights have also been installed and police have swept the grounds for booby traps. 3. (13%) A predicate of a modifying clause: Westermayer was 26 then, and a friend and former manager who knew she was unhappy in her job tipped her to another position.</title>
<marker>Hill, </marker>
<rawString>Michael Hill, a Sun reporter who is a member of the Washington-Baltimore Newspaper Guild’s bargaining committee, estimated meetings to last late Sunday. 2. (30%) A part of a coordination: Security lights have also been installed and police have swept the grounds for booby traps. 3. (13%) A predicate of a modifying clause: Westermayer was 26 then, and a friend and former manager who knew she was unhappy in her job tipped her to another position.</rawString>
</citation>
<citation valid="false">
<title>An adjunct: While waiting for a bomb squad to arrive, the bomb exploded, killing Wells. 5. (7%) An embedded sentence: Dean told reporters traveling on his 10-city “Sleepless Summer” tour that he considered campaigning in Texas a challenge.</title>
<marker></marker>
<rawString>4. (7%) An adjunct: While waiting for a bomb squad to arrive, the bomb exploded, killing Wells. 5. (7%) An embedded sentence: Dean told reporters traveling on his 10-city “Sleepless Summer” tour that he considered campaigning in Texas a challenge.</rawString>
</citation>
<citation valid="false">
<title>Or factual content that conflicts with the opposing sentence: Total sales for the period declined 8.0 percent to USD1.99 billion from a year earlier.</title>
<marker></marker>
<rawString>6. (3%) Or factual content that conflicts with the opposing sentence: Total sales for the period declined 8.0 percent to USD1.99 billion from a year earlier.</rawString>
</citation>
<citation valid="false">
<title>Wal-Mart said sales at stores open at least a year rose 4.6 percent from a year earlier.</title>
<marker></marker>
<rawString>Wal-Mart said sales at stores open at least a year rose 4.6 percent from a year earlier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="6522" citStr="Barzilay and Lee (2003)" startWordPosition="990" endWordPosition="993">t and a semi-automatically constructed thesaurus). A support vector machine is then trained to learn the {+pp, −pp} classifier. Strategies based on bags of words largely ignore the semantic interactions between words. Weeds et al. (2005) addressed this problem by utilizing parses for PR. Their system for phrasal paraphrases equates paraphrasing as distributional similarity of the partial sub-parses of a candidate text. Wu (2005)’s approach relies on the generative framework of Inversion Transduction Grammar (ITG) to measure how similar two sentences arrange their words based on edit distance. Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. Given multiple articles on a certain type of event, sentence clusters are first generated. Sentences within the same cluster, presumably similar in structure and content, are then used to construct a lattice with “backbone” nodes corresponding to words shared by the majority and “slots” corresponding to different realization of arguments. If sentences from different clusters have shared arguments, the associated lattices are claimed to be paraphrase. Likewise, Shinyama et al. (2002) extracted paraphrases f</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. In Proceedings of HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
<author>Bill Dolan</author>
</authors>
<title>Support vector machines for paraphrase identification and corpus construction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 3rd International Workshop on Paraphrasing.</booktitle>
<contexts>
<context position="5153" citStr="Brockett and Dolan, 2005" startWordPosition="779" endWordPosition="782">ented system in Section 3. Sections 4 and 5 detail the algorithms for the two phases respectively. This is followed with our evaluation and discussion of the results. 2 Related Work Possibly the simplest approach to PR is an information retrieval (IR) based “bag-of-words” strategy. This strategy calculates a cosine similarity score for the given sentence set, and if the similarity exceeds a threshold (either empirically determined or learned from supervised training data), the sentences are paraphrases. PR systems that can be broadly categorized as IR-based include (Corley and Mihalcea, 2005; Brockett and Dolan, 2005). In the former work, the authors defined a directional similarity formula reflecting the semantic similarity of one text “with respect to” another. A word contributes to the directional similarity only when its counterpart has been identified in the opposing sentence. The associated word similarity scores, weighted by the word’s specificity (represented as inverted document frequency, idf), sum to make up the directional similarity. The mean of both directions is the overall similarity of the pair. Brockett and Dolan (2005) represented sentence pairs as a feature vector, including features (a</context>
</contexts>
<marker>Brockett, Dolan, 2005</marker>
<rawString>Chris Brockett and Bill Dolan. 2005. Support vector machines for paraphrase identification and corpus construction. In Proceedings of the 3rd International Workshop on Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL’2000).</booktitle>
<contexts>
<context position="10046" citStr="Charniak, 2000" startWordPosition="1562" endWordPosition="1563"> is a better choice for the representation of a nugget as it accounts for the action, concepts and their relationships as a single unit. In comparison, using fine-grained units such as words, including nouns and verbs may result in inaccuracy (sentences that share vocabulary may not be paraphrases), while using coarser-grained units may cause key differences to be missed. In the rest of this paper, we use the term tuple for conciseness when no ambiguity is introduced. An overview of our paraphrase recognition system is shown in Figure 2. A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al., 2004)), to label predicate argument tuples. We then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and content of each tuple. A thesaurus constructed from corpus statistics (Lin, 1998) is utilized for the content similarity. We utilize this metric to greedily pair together the most similar predicate argument tuples across Figure 2: System architecture sentences. Any remaining unpaired tuples represent extra information and are passed</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL’2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Corley</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring the semantic similarity of texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>13--18</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="5126" citStr="Corley and Mihalcea, 2005" startWordPosition="775" endWordPosition="778">ogy and describe the implemented system in Section 3. Sections 4 and 5 detail the algorithms for the two phases respectively. This is followed with our evaluation and discussion of the results. 2 Related Work Possibly the simplest approach to PR is an information retrieval (IR) based “bag-of-words” strategy. This strategy calculates a cosine similarity score for the given sentence set, and if the similarity exceeds a threshold (either empirically determined or learned from supervised training data), the sentences are paraphrases. PR systems that can be broadly categorized as IR-based include (Corley and Mihalcea, 2005; Brockett and Dolan, 2005). In the former work, the authors defined a directional similarity formula reflecting the semantic similarity of one text “with respect to” another. A word contributes to the directional similarity only when its counterpart has been identified in the opposing sentence. The associated word similarity scores, weighted by the word’s specificity (represented as inverted document frequency, idf), sum to make up the directional similarity. The mean of both directions is the overall similarity of the pair. Brockett and Dolan (2005) represented sentence pairs as a feature ve</context>
<context position="27738" citStr="Corley and Mihalcea, 2005" startWordPosition="4491" endWordPosition="4494"> features. In Table 3, precision and recall are measured with respect to the paraphrasing class. The table shows sentence pairs falling under the column “pairs without unpaired tuples” are more likely to be paraphrasing than an arbitrary pair (79.5% versus 66.5%), providing further validation for using predicate argument tuples as information nuggets. The results for the experiment benchmarking the overall system performance are shown under the “Overall” column: our approach performs comparably to the baselines at both accuracy and paraphrase recall. The system performance reported in (CM05; (Corley and Mihalcea, 2005)), which is among the best we are aware of, is also included for comparison. We also ran our system (trained on the MSR corpus) on the 50 instances in the PASCAL paraphrase acquisition subset. Again, the system performance (as shown in Table 4) is comparable to the baseline systems. 7 Discussion We have just shown that when two sentences have perfectly matched predicate argument tuples, they are more likely to be a paraphrase than a random sentence pair drawn from the corpus. Furthermore, in the sampled human evaluation in Table 2, among the 88 non-paraphrasing instances with whose MSR corpus </context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Courtney Corley and Rada Mihalcea. 2005. Measuring the semantic similarity of texts. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 13–18, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In PASCAL Proceedings of the First Challenge Workshop– Recognizing Textual Entailment, Southampton,UK.</booktitle>
<contexts>
<context position="23021" citStr="Dagan et al., 2005" startWordPosition="3708" endWordPosition="3711">lent by human annotators. It reflects ordinary paraphrases that people often encounter in news articles, and may be viewed as a typical domain-general paraphrase recognition task that downstream NLP systems will need to deal with. The corpus comes divided into standard training (70%) and testing (30%) divisions, a partition we follow in our experiments. ASSERT (the semantic role labeler) shows for this corpus a sentence contains 2.24 predicate argument tuples on average. The second corpus is the paraphrase acquisition subset of the PASCAL Recognizing Textual Entailment (RTE) Challenge corpus (Dagan et al., 2005). This is much smaller, consisting of 50 pairs, which we employ for testing only to show portability. To assess the component performance, we need additional ground truth beyond the {+pp, −pp} labels provided by the corpora. For the first evaluation, we need to ascertain whether a sentence pair’s tuples are correctly paired, misidentified or mispaired. For the second, which tuple(s) (if any) are responsible for a −pp instance. However, creating ground truth by manual annotation is expensive, and thus we only sampled the data set to get an indicative assessment of performance. We sampled 200 ra</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In PASCAL Proceedings of the First Challenge Workshop– Recognizing Textual Entailment, Southampton,UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="4397" citStr="Dolan et al., 2004" startWordPosition="659" endWordPosition="662">es describing the same event, paraphrases are widely used, possibly with extraneous information. We equate PR with solving these two issues, presenting a natural two-phase architecture. In the first phase, the nuggets shared by the sentences are identified by a pairing process. In the second phase, any unpaired nuggets are classified as significant or not (leading to −pp and +pp classifications, respectively). If the sentences do not contain unpaired nuggets, or if all unpaired nuggets are insignificant, then the sentences are considered paraphrases. Experiments on the widely-used MSR corpus (Dolan et al., 2004) show favorable results. We first review related work in Section 2. We then present the overall methodology and describe the implemented system in Section 3. Sections 4 and 5 detail the algorithms for the two phases respectively. This is followed with our evaluation and discussion of the results. 2 Related Work Possibly the simplest approach to PR is an information retrieval (IR) based “bag-of-words” strategy. This strategy calculates a cosine similarity score for the given sentence set, and if the similarity exceeds a threshold (either empirically determined or learned from supervised trainin</context>
<context position="22309" citStr="Dolan et al., 2004" startWordPosition="3597" endWordPosition="3600">tions as well as a holistic one, resulting in three separate experiments. In evaluating the first tuple pairing component, we aim for high precision, so that sentences that have all tuples paired can be safely assumed to be paraphrases. In evaluating the dissimilarity classifier, we simply aim for high accuracy. In our overall system evaluation, we compare our system versus other PR systems on standard corpora. Experimental Data Set. For these experiments, we utilized two widely-used corpora for paraphrasing evaluation: the MSR and PASCAL RTE corpora. The Microsoft Research Paraphrase coupus (Dolan et al., 2004) consists of 5801 newswire sentence pairs, 3900 of which are annotated as semantically equivalent by human annotators. It reflects ordinary paraphrases that people often encounter in news articles, and may be viewed as a typical domain-general paraphrase recognition task that downstream NLP systems will need to deal with. The corpus comes divided into standard training (70%) and testing (30%) divisions, a partition we follow in our experiments. ASSERT (the semantic role labeler) shows for this corpus a sentence contains 2.24 predicate argument tuples on average. The second corpus is the paraph</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th International Conference on Computational Linguistics, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The necessity of parsing for predicate argument recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="16329" citStr="Gildea and Palmer, 2002" startWordPosition="2576" endWordPosition="2579">wide set of features of unpaired tuples, including internal counts of numeric expressions, named entities, words, semantic roles, whether they are similar to other tuples in the same sentence, and contextual features like source/target sentence length and paired tuple count. Currently, only two features are correlated in improved classification, which we detail now. Syntactic Parse Tree Path: This is a series of features that reflect how the unpaired tuple connects with the context: the rest of the sentence. It models the syntactic connection between the constituents on both ends of the path (Gildea and Palmer, 2002; Pradhan et al., 2004). Here, we model the ends of the path as the unpaired tuple and the paired tuple with the closest shared ancestor, and model the path itself as a sequence of constituent category tags and directions to reach the destination (the paired target) from the source (the c==target wtarget (1) 21 unpaired target) via the the shared ancestor. When no tuples have been paired in the sentence pair, the destination defaults to the root of the syntactic parse tree. For example, the tuples with target “injured” are unpaired when the model sentence and the non-paraphrasing modification </context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Daniel Gildea and Martha Palmer. 2002. The necessity of parsing for predicate argument recognition. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vassileios Hatzivassiloglou</author>
<author>Judith Klavans</author>
<author>Melissa Holcombe</author>
<author>Regina Barzilay</author>
<author>Min-Yen Kan</author>
<author>Kathleen McKeown</author>
</authors>
<title>Simfinder: A flexible clustering tool for summarization.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL Workshop on Automatic Summarization,</booktitle>
<pages>41--49</pages>
<contexts>
<context position="9017" citStr="Hatzivassiloglou et al., 2001" startWordPosition="1385" endWordPosition="1388">accurate modeling of semantic similarity for both recognition and acquisition. 3 Methodology As noted earlier, for a pair of sentences to be a paraphrase, they must possess two attributes: 1. similarity: they share a substantial amount of information nuggets; 2. dissimilarities are extraneous: if extra information in the sentences exists, the effect of its removal is not significant. A key decision for our two-phase PR framework is to choose the representation of an information nugget. A simple approach is to use representative words as information nuggets, as is done in the SimFinder system (Hatzivassiloglou et al., 2001). Instead of using words, we choose to equate information nuggets with predicate argument tuples. A predicate argument tuple is a structured representation of a verb predicate together with its arguments. Given a sentence from the example in Figure 1, its predicate argument tuple form in PropBank (Kingsbury et al., 2002) format is: target(predicate): hurt arg0: a young man arg1: Richard Miller We feel that this is a better choice for the representation of a nugget as it accounts for the action, concepts and their relationships as a single unit. In comparison, using fine-grained units such as w</context>
<context position="27023" citStr="Hatzivassiloglou et al., 2001" startWordPosition="4385" endWordPosition="4388">r 71% 91% 71% 42% 21% 27% 52% 52% 52% 52% Our System 65% 100% 65% 48% 64% 43% 54% 80% 53% 64% Table 4: Results on PASCAL PP test set score greater than zero), which is effectively a 98% recall of insignificant tuples. However, the precision is less satisfatory. We suspect this is partially due the tuples that fail to be paired up with their counterpart. Such noise is found among the automatically collected PS instances used in training. Figure 5: Dissimilarity classifier performance For the final system-wide evaluation, we implemented two baseline systems: a majority classifier and SimFinder (Hatzivassiloglou et al., 2001), a bag-of-words sentence similarity module incorporating lexical, syntactic and semantic features. In Table 3, precision and recall are measured with respect to the paraphrasing class. The table shows sentence pairs falling under the column “pairs without unpaired tuples” are more likely to be paraphrasing than an arbitrary pair (79.5% versus 66.5%), providing further validation for using predicate argument tuples as information nuggets. The results for the experiment benchmarking the overall system performance are shown under the “Overall” column: our approach performs comparably to the base</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Holcombe, Barzilay, Kan, McKeown, 2001</marker>
<rawString>Vassileios Hatzivassiloglou, Judith Klavans, Melissa Holcombe, Regina Barzilay, Min-Yen Kan, and Kathleen McKeown. 2001. Simfinder: A flexible clustering tool for summarization. In Proceedings of the NAACL Workshop on Automatic Summarization, pages 41–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
<author>Mitch Marcus</author>
</authors>
<title>Adding semantic annotation to the penn treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<location>San Diego, USA.</location>
<contexts>
<context position="9339" citStr="Kingsbury et al., 2002" startWordPosition="1440" endWordPosition="1443">ces exists, the effect of its removal is not significant. A key decision for our two-phase PR framework is to choose the representation of an information nugget. A simple approach is to use representative words as information nuggets, as is done in the SimFinder system (Hatzivassiloglou et al., 2001). Instead of using words, we choose to equate information nuggets with predicate argument tuples. A predicate argument tuple is a structured representation of a verb predicate together with its arguments. Given a sentence from the example in Figure 1, its predicate argument tuple form in PropBank (Kingsbury et al., 2002) format is: target(predicate): hurt arg0: a young man arg1: Richard Miller We feel that this is a better choice for the representation of a nugget as it accounts for the action, concepts and their relationships as a single unit. In comparison, using fine-grained units such as words, including nouns and verbs may result in inaccuracy (sentences that share vocabulary may not be paraphrases), while using coarser-grained units may cause key differences to be missed. In the rest of this paper, we use the term tuple for conciseness when no ambiguity is introduced. An overview of our paraphrase recog</context>
</contexts>
<marker>Kingsbury, Palmer, Marcus, 2002</marker>
<rawString>Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002. Adding semantic annotation to the penn treebank. In Proceedings of the Human Language Technology Conference, San Diego, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English verb classes and alternations: A preliminary investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="19086" citStr="Levin, 1993" startWordPosition="3055" endWordPosition="3056">more “fair and equitable” service for infertility sufferers. Figure 4: Unpaired predicate argument tuple as modifier in a paraphrase Predicate: This is the lexical token of predicate argument tuple’s target, as a text feature. As this feature is liable to run into sparse data problems, the semantic category of the target would be a more suitable feature. However, verb similarity is generally regarded as difficult to measure, both in terms of semantic relatedness as well as in finding a consistent granularity for verb categories. We investigated using WordNet as well as Levin’s classification (Levin, 1993) as additional features on our validation data, but currently find that using the lexical form of the target works best. 5.1 Classifier Training Set Acquisition Currently, no training corpus for predicate argument tuple significance exists. Such a corpus is indispensable for training the classifier. Rather than manually annotating training instances, we use an automatic method to construct instances from paraphrase corpora. This is possible as the paraphrase judgments in the corpora can imply which portion of the sentence(s) are significant barriers to paraphrasing or not. Here, we exploit the</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English verb classes and alternations: A preliminary investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL ’98,</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="10393" citStr="Lin, 1998" startWordPosition="1617" endWordPosition="1618">ferences to be missed. In the rest of this paper, we use the term tuple for conciseness when no ambiguity is introduced. An overview of our paraphrase recognition system is shown in Figure 2. A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al., 2004)), to label predicate argument tuples. We then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and content of each tuple. A thesaurus constructed from corpus statistics (Lin, 1998) is utilized for the content similarity. We utilize this metric to greedily pair together the most similar predicate argument tuples across Figure 2: System architecture sentences. Any remaining unpaired tuples represent extra information and are passed to a dissimilarity classifier to decide whether such information is significant. The dissimilarity classifier uses supervised machine learning to make such a decision. 4 Similarity Detection and Pairing We illustrate this advantage of using predicate argument tuples from our running example. In Table 1, one of the model sentences is shown in th</context>
<context position="29246" citStr="Lin, 1998" startWordPosition="4751" endWordPosition="4752">sentation. We feel this justifies using predicate argument tuples as information nuggets, but we are currently considering expanding our representation to account for some of these cases. The evaluation also confirms the difficulty of making paraphrase judgements. Although the Frequency 180 160 140 120 100 80 40 20 60 0 Insignificant Significant SVM Prediction 24 MSR corpus used strict means of resolving interrater disagreements during its construction, the annotators agreed with the MSR corpus labels only 93.5% (187/200) of the time. One weakness of our system is that we rely on a thesaurus (Lin, 1998) for word similarity information for predicate argument tuple pairing. However, it is designed to provide similarity scores between pairs of individual words (rather than phrases). If a predicate argument tuple’s target or one argument is realized as a phrase (borrow —* check out, for instance), the thesaurus is unable to provide an accurate similarity score. For similarity between predicate argument tuples, negation and modality have yet to be addressed, although they account for only a tiny fraction of the corpus. We further estimated how the similarity detector can be affected when the sema</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL ’98, pages 768–774, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<location>Boston, USA.</location>
<contexts>
<context position="10121" citStr="Pradhan et al., 2004" startWordPosition="1573" endWordPosition="1576"> for the action, concepts and their relationships as a single unit. In comparison, using fine-grained units such as words, including nouns and verbs may result in inaccuracy (sentences that share vocabulary may not be paraphrases), while using coarser-grained units may cause key differences to be missed. In the rest of this paper, we use the term tuple for conciseness when no ambiguity is introduced. An overview of our paraphrase recognition system is shown in Figure 2. A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al., 2004)), to label predicate argument tuples. We then calculate normalized tuple similarity scores over the tuple pairs using a metric that accounts for similarities in both syntactic structure and content of each tuple. A thesaurus constructed from corpus statistics (Lin, 1998) is utilized for the content similarity. We utilize this metric to greedily pair together the most similar predicate argument tuples across Figure 2: System architecture sentences. Any remaining unpaired tuples represent extra information and are passed to a dissimilarity classifier to decide whether such information is signif</context>
<context position="16352" citStr="Pradhan et al., 2004" startWordPosition="2580" endWordPosition="2583">npaired tuples, including internal counts of numeric expressions, named entities, words, semantic roles, whether they are similar to other tuples in the same sentence, and contextual features like source/target sentence length and paired tuple count. Currently, only two features are correlated in improved classification, which we detail now. Syntactic Parse Tree Path: This is a series of features that reflect how the unpaired tuple connects with the context: the rest of the sentence. It models the syntactic connection between the constituents on both ends of the path (Gildea and Palmer, 2002; Pradhan et al., 2004). Here, we model the ends of the path as the unpaired tuple and the paired tuple with the closest shared ancestor, and model the path itself as a sequence of constituent category tags and directions to reach the destination (the paired target) from the source (the c==target wtarget (1) 21 unpaired target) via the the shared ancestor. When no tuples have been paired in the sentence pair, the destination defaults to the root of the syntactic parse tree. For example, the tuples with target “injured” are unpaired when the model sentence and the non-paraphrasing modification in Table 1 are being co</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proceedings of HLT/NAACL, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
<author>Ralph Grishman</author>
</authors>
<title>Automatic paraphrase acquisition from news articles.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<pages>40--46</pages>
<location>San Diego, USA.</location>
<contexts>
<context position="7098" citStr="Shinyama et al. (2002)" startWordPosition="1076" endWordPosition="1079">ased on edit distance. Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. Given multiple articles on a certain type of event, sentence clusters are first generated. Sentences within the same cluster, presumably similar in structure and content, are then used to construct a lattice with “backbone” nodes corresponding to words shared by the majority and “slots” corresponding to different realization of arguments. If sentences from different clusters have shared arguments, the associated lattices are claimed to be paraphrase. Likewise, Shinyama et al. (2002) extracted paraphrases from similar news articles, but use shared named entities as an indication of paraphrasing. It should be noted that the latter two approaches are geared towards acquiring paraphrases rather than detecting them, and as such have the disadvantage of requiring a certain level of repetition among candidates for paraphrases to be recognized. All past approaches invariably aim at a proper similarity measure that accounts for all of the words in the sentences in order to make a judgment for PR. This is suitable for PR where input sentences are precisely equivalent semantically.</context>
</contexts>
<marker>Shinyama, Sekine, Sudo, Grishman, 2002</marker>
<rawString>Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and Ralph Grishman. 2002. Automatic paraphrase acquisition from news articles. In Proceedings of the Human Language Technology Conference, pages 40–46, San Diego, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Bill Keller</author>
</authors>
<title>The distributional similarity of sub-parses.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>7--12</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="6136" citStr="Weeds et al. (2005)" startWordPosition="932" endWordPosition="935">rted document frequency, idf), sum to make up the directional similarity. The mean of both directions is the overall similarity of the pair. Brockett and Dolan (2005) represented sentence pairs as a feature vector, including features (among others) for sentence length, edit distance, number of shared words, morphologically similar word pairs, synonym pairs (as suggested by WordNet and a semi-automatically constructed thesaurus). A support vector machine is then trained to learn the {+pp, −pp} classifier. Strategies based on bags of words largely ignore the semantic interactions between words. Weeds et al. (2005) addressed this problem by utilizing parses for PR. Their system for phrasal paraphrases equates paraphrasing as distributional similarity of the partial sub-parses of a candidate text. Wu (2005)’s approach relies on the generative framework of Inversion Transduction Grammar (ITG) to measure how similar two sentences arrange their words based on edit distance. Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. Given multiple articles on a certain type of event, sentence clusters are first generated. Sentences within the same cluster,</context>
</contexts>
<marker>Weeds, Weir, Keller, 2005</marker>
<rawString>Julie Weeds, David Weir, and Bill Keller. 2005. The distributional similarity of sub-parses. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 7–12, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Recognizing paraphrases and textual entailment using inversion transduction grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="6331" citStr="Wu (2005)" startWordPosition="962" endWordPosition="963">e vector, including features (among others) for sentence length, edit distance, number of shared words, morphologically similar word pairs, synonym pairs (as suggested by WordNet and a semi-automatically constructed thesaurus). A support vector machine is then trained to learn the {+pp, −pp} classifier. Strategies based on bags of words largely ignore the semantic interactions between words. Weeds et al. (2005) addressed this problem by utilizing parses for PR. Their system for phrasal paraphrases equates paraphrasing as distributional similarity of the partial sub-parses of a candidate text. Wu (2005)’s approach relies on the generative framework of Inversion Transduction Grammar (ITG) to measure how similar two sentences arrange their words based on edit distance. Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. Given multiple articles on a certain type of event, sentence clusters are first generated. Sentences within the same cluster, presumably similar in structure and content, are then used to construct a lattice with “backbone” nodes corresponding to words shared by the majority and “slots” corresponding to different reali</context>
</contexts>
<marker>Wu, 2005</marker>
<rawString>Dekai Wu. 2005. Recognizing paraphrases and textual entailment using inversion transduction grammars. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, Ann Arbor, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>