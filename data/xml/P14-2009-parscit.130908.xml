<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000222">
<title confidence="0.9984375">
Adaptive Recursive Neural Network
for Target-dependent Twitter Sentiment Classification
</title>
<author confidence="0.978592">
Li Dong†* Furu Wei$ Chuanqi Tan†* Duyu TangT* Ming Zhou$ Ke Xu††Beihang University, Beijing, China
</author>
<affiliation confidence="0.925653">
$Microsoft Research, Beijing, China
HHarbin Institute of Technology, Harbin, China
</affiliation>
<email confidence="0.966416">
donglixp@gmail.com fuwei@microsoft.com {ysjtcq,tangduyu}@gmail.com
mingzhou@microsoft.com kexu@nlsde.buaa.edu.cn
</email>
<sectionHeader confidence="0.994649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933333333333">
We propose Adaptive Recursive Neural
Network (AdaRNN) for target-dependent
Twitter sentiment classification. AdaRNN
adaptively propagates the sentiments of
words to target depending on the context
and syntactic relationships between them.
It consists of more than one composition
functions, and we model the adaptive sen-
timent propagations as distributions over
these composition functions. The experi-
mental studies illustrate that AdaRNN im-
proves the baseline methods. Further-
more, we introduce a manually annotated
dataset for target-dependent Twitter senti-
ment analysis.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997708460317461">
Twitter becomes one of the most popular social
networking sites, which allows the users to read
and post messages (i.e. tweets) up to 140 charac-
ters. Among the great varieties of topics, people
in Twitter tend to express their opinions for the
brands, celebrities, products and public events. As
a result, it attracts much attention to estimate the
crowd’s sentiments in Twitter.
For the tweets, our task is to classify their senti-
ments for a given target as positive, negative, and
neutral. People may mention several entities (or
targets) in one tweet, which affects the availabil-
ities for most of existing methods. For example,
the tweet “@ballmer: windows phone is better
than ios!” has three targets (@ballmer, windows
phone, and ios). The user expresses neutral, pos-
itive, and negative sentiments for them, respec-
tively. If target information is ignored, it is diffi-
cult to obtain the correct sentiment for a specified
target. For target-dependent sentiment classifica-
tion, the manual evaluation of Jiang et al. (2011)
∗Contribution during internship at Microsoft Research.
show that about 40% of errors are caused by not
considering the targets in classification.
The features used in traditional learning-based
methods (Pang et al., 2002; Nakagawa et al., 2010)
are independent to the targets, hence the results
are computed despite what the targets are. Hu and
Liu (2004) regard the features of products as tar-
gets, and sentiments for them are heuristically de-
termined by the dominant opinion words. Jiang
et al. (2011) combine the target-independent fea-
tures (content and lexicon) and target-dependent
features (rules based on the dependency parsing
results) together in subjectivity classification and
polarity classification for tweets.
In this paper, we mainly focus on integrating
target information with Recursive Neural Network
(RNN) to leverage the ability of deep learning
models. The neural models use distributed repre-
sentation (Hinton, 1986; Rumelhart et al., 1986;
Bengio et al., 2003) to automatically learn fea-
tures for target-dependent sentiment classification.
RNN utilizes the recursive structure of text, and it
has achieved state-of-the-art sentiment analysis re-
sults for movie review dataset (Socher et al., 2012;
Socher et al., 2013). The recursive neural mod-
els employ the semantic composition functions,
which enables them to handle the complex com-
positionalities in sentiment analysis.
Specifically, we propose a framework which
learns to propagate the sentiments of words to-
wards the target depending on context and syn-
tactic structure. We employ a novel adaptive
multi-compositionality layer in recursive neural
network, which is named as AdaRNN (Dong et
al., 2014). It consists of more than one compo-
sition functions, and we model the adaptive sen-
timent propagations as learning distributions over
these composition functions. We automatically
learn the composition functions and how to select
them from supervisions, instead of choosing them
heuristically or by hand-crafted rules. AdaRNN
</bodyText>
<page confidence="0.992789">
49
</page>
<bodyText confidence="0.8703121">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49–54,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
determines how to propagate the sentiments to-
wards the target and handles the negation or in-
tensification phenomena (Taboada et al., 2011) in
sentiment analysis. In addition, we introduce a
manually annotated dataset, and conduct extensive
experiments on it. The experimental results sug-
gest that our approach yields better performances
than the baseline methods.
</bodyText>
<sectionHeader confidence="0.923657" genericHeader="method">
2 RNN: Recursive Neural Network
</sectionHeader>
<bodyText confidence="0.56026775">
RNN (Socher et al., 2011) represents the phrases
and words as D-dimensional vectors. It performs
compositions based on the binary trees, and obtain
the vector representations in a bottom-up way.
</bodyText>
<figureCaption confidence="0.9734605">
Figure 1: The composition process for “not very
good” in Recursive Neural Network.
</figureCaption>
<bodyText confidence="0.9999265">
As illustrated in Figure 1, we obtain the repre-
sentation of “very good” by the composition of
“very” and “good”, and the representation of tri-
gram “not very good” is recursively obtained by
the vectors of “not” and “very good”. The di-
mensions of parent node are calculated by linear
combination of the child vectors’ dimensions. The
vector representation v is obtained via:
</bodyText>
<equation confidence="0.964239666666667">
( [vl] )
v = f (g (vl, vr)) = f W + b (1)
vr
</equation>
<bodyText confidence="0.999036416666667">
where vl, vr are the vectors of its left and right
child, g is the composition function, f is the non-
linearity function (such as tanh, sigmoid, softsign,
etc.), W ∈ RDx2D is the composition matrix, and
b is the bias vector. The dimension of v is the
same as its child vectors, and it is recursively used
in the next step. Notably, the word vectors in the
leaf nodes are regarded as the parameters, and will
be updated according to the supervisions.
The vector representation of root node is then
fed into a softmax classifier to predict the label.
The k-th element of softmax(x) is exp{xk} For
</bodyText>
<subsectionHeader confidence="0.260566">
Ej exp{xj} .
</subsectionHeader>
<bodyText confidence="0.7717154">
a vector, the softmax obtains the distribution over
K classes. Specifically, the predicted distribution
is y = softmax (Uv), where y is the predicted
distribution, U ∈ RKxD is the classification ma-
trix, and v is the vector representation of node.
</bodyText>
<sectionHeader confidence="0.951906" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.994379125">
We use the dependency parsing results to find the
words syntactically connected with the interested
target. Adaptive Recursive Neural Network is pro-
posed to propagate the sentiments of words to the
target node. We model the adaptive sentiment
propagations as semantic compositions. The com-
putation process is conducted in a bottom-up man-
ner, and the vector representations are computed
recursively. After we obtain the representation of
target node, a classifier is used to predict the sen-
timent label according to the vector.
In Section 3.1, we show how to build recur-
sive structure for target using the dependency pars-
ing results. In Section 3.2, we propose Adaptive
Recursive Neural Network and use it for target-
dependent sentiment analysis.
</bodyText>
<subsectionHeader confidence="0.998603">
3.1 Build Recursive Structure
</subsectionHeader>
<bodyText confidence="0.9613234">
The dependency tree indicates the dependency re-
lations between words. As described above, we
propagate the sentiments of words to the target.
Hence the target is placed at the root node to com-
bine with its connected words recursively. The de-
pendency relation types are remained to guide the
sentiment propagations in our model.
Algorithm 1 Convert Dependency Tree
Input: Target node, Dependency tree
Output: Converted tree
</bodyText>
<listItem confidence="0.996979545454545">
1: function CONV(r)
2: Er ← SORT(dep edges connected with r)
3: v ← r
4: for (r →−t u/u →−t r) in Er do
5: if r is head of u then
6: w ← node with CONV(u), v as children
7: else
8: w ← node with v, CONV(u) as children
9: v ← w
10: return v
11: Call CONV(target node) to get converted tree
</listItem>
<bodyText confidence="0.999084111111111">
As illustrated in the Algorithm 1, we recursively
convert the dependency tree starting from the tar-
get node. We find all the words connected to the
target, and these words are combined with target
node by certain order. Every combination is con-
sidered as once propagation of sentiments. If the
target is head of the connected words, the target
vector is combined as the right node; if otherwise,
it is combined as the left node. This ensures the
</bodyText>
<figure confidence="0.9785084">
Negative
Softmax
not very good
very good
not very good
</figure>
<page confidence="0.985472">
50
</page>
<bodyText confidence="0.999908909090909">
child nodes in a certain order. We use two rules
to determine the order of combinations: (1) the
words whose head is the target in dependency tree
are first combined, and then the rest of connected
words are combined; (2) if the first rule cannot de-
termine the order, the connected words are sorted
by their positions in sentence from right to left.
Notably, the conversion is performed recursively
for the connected words and the dependency rela-
tion types are remained. Figure 2 shows the con-
verted results for different targets in one sentence.
</bodyText>
<subsectionHeader confidence="0.8865075">
3.2 AdaRNN: Adaptive Recursive Neural
Network
</subsectionHeader>
<bodyText confidence="0.999950318181818">
RNN employs one global matrix to linearly com-
bine the elements of vectors. Sometimes it is
challenging to obtain a single powerful function
to model the semantic composition, which moti-
vates us to propose AdaRNN. The basic idea of
AdaRNN is to use more than one composition
functions and adaptively select them depending on
the linguistic tags and the combined vectors. The
model learns to propagate the sentiments of words
by using the different composition functions.
Figure 2 shows the computation process for the
example sentence “windows is better than ios”,
where the user expresses positive sentiment to-
wards windows and negative sentiment to ios. For
the targets, the order of compositions and the de-
pendency types are different. AdaRNN adap-
tively selects the composition functions g1 ... gC
depending on the child vectors and the linguistic
types. Thus it is able to determine how to propa-
gate the sentiments of words towards the target.
Based on RNN described in Section 2, we de-
fine the composition result v in AdaRNN as:
</bodyText>
<equation confidence="0.984562">
�P (gh|vl, vr, e) gh (vl, vr) (2)
</equation>
<bodyText confidence="0.99986725">
where g1, ... , gC are the composition functions,
P (gh|vl, vr, e) is the probability of employing gh
given the child vectors vl, vr and external feature
vector e, and f is the nonlinearity function. For
the composition functions, we use the same forms
as in Equation (1), i.e., we have C composition
matrices W1 ... WC. We define the distribution
over these composition functions as:
</bodyText>
<equation confidence="0.9907506">
⎡P (g1|vl, vr, e)
⎢ .
⎣ ..
P (gC|vl, vr, e)
(3)
</equation>
<bodyText confidence="0.999906866666667">
where β is the hyper-parameter, S E RCx(2D+|e|)
is the matrix used to determine which composition
function we use, vl, vr are the left and right child
vectors, and e are external feature vector. In this
work, e is a one-hot binary feature vector which
indicates what the dependency type is. If relation
is the k-th type, we set ek to 1 and the others to 0.
Adding β in softmax function is a widely used
parametrization method in statistical mechanics,
which is known as Boltzmann distribution and
Gibbs measure (Georgii, 2011). When β = 0, this
function produces a uniform distribution; when
β = 1, it is the same as softmax function; when
β oc, it only activates the dimension with max-
imum weight, and sets its probability to 1.
</bodyText>
<subsectionHeader confidence="0.99897">
3.3 Model Training
</subsectionHeader>
<bodyText confidence="0.999987333333333">
We use the representation of root node as the fea-
tures, and feed them into the softmax classifier to
predict the distribution over classes. We define the
ground truth vector t as a binary vector. If the k-th
class is the label, only tk is 1 and the others are
0. Our goal is to minimize the cross-entropy error
between the predicted distribution y and ground
truth distribution t. For each training instance, we
define the objective function as:
</bodyText>
<equation confidence="0.990193">
λθllθll22 (4)
</equation>
<bodyText confidence="0.99970375">
where O represents the parameters, and the L2-
regularization penalty is used.
Based on the converted tree, we employ back-
propagation algorithm (Rumelhart et al., 1986) to
propagate the errors from root node to the leaf
nodes. We calculate the derivatives to update the
parameters. The AdaGrad (Duchi et al., 2011) is
employed to solve this optimization problem.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99999025">
As people tend to post comments for the celebri-
ties, products, and companies, we use these key-
words (such as “bill gates”, “taylor swift”, “xbox”,
“windows T”, “google”) to query the Twitter API.
After obtaining the tweets, we manually anno-
tate the sentiment labels (negative, neutral, posi-
tive) for these targets. In order to eliminate the
effects of data imbalance problem, we randomly
sample the tweets and make the data balanced.
The negative, neutral, positive classes account for
25%, 50%, 25%, respectively. Training data con-
sists of 6,248 tweets, and testing data has 692
</bodyText>
<equation confidence="0.639683571428571">
C
v = f
h=1
⎛ ⎡vl
= softmax βS vr
e I )
⎤
</equation>
<page confidence="0.336073">
⎦⎥
</page>
<figure confidence="0.9489935">
� �
min − tj log yj +
O θEO
j
</figure>
<page confidence="0.886425">
51
</page>
<figureCaption confidence="0.8909624">
Figure 2: For the sentence “windows is better than ios”, we convert its dependency tree for the different
targets (windows and ios). AdaRNN performs semantic compositions in bottom-up manner and forward
propagates sentiment information to the target node. The g1, ... , gc are different composition functions,
and the combined vectors and dependency types are used to select them adaptively. These composition
functions decide how to propagate the sentiments to the target.
</figureCaption>
<figure confidence="0.928323638888889">
g1 ... gC
cop
pobj
ios than
cop
is better
prep pobj
Softmax
Positve
Negative
Softmax
windows is target: ios is target:
Dependency tree:
ROOT
g1 ... gC
g1 ... gC
nsubj
windows
nsubj
ios
pobj
g1 ... gC
cop
prep
is
than
windows is better than ios
(target) (target)
g1 ... gC
g1 ... gC
prep
better
windows
nsubj
g1 ... gC
g1 ... gC
</figure>
<bodyText confidence="0.996748419354839">
tweets. We randomly sample some tweets, and
they are assigned with sentiment labels by two an-
notators. About 82.5% of them have the same la-
bels. The agreement percentage of polarity clas-
sification is higher than subjectivity classification.
To the best of our knowledge, this is the largest
target-dependent Twitter sentiment classification
dataset which is annotated manually. We make the
dataset publicly available 1 for research purposes.
We preprocess the tweets by replacing the tar-
gets with $T$ and setting their POS tags to NN.
Liblinear (Fan et al., 2008) is used for baselines.
A tweet-specific tokenizer (Gimpel et al., 2011)
is employed, and the dependency parsing results
are computed by Stanford Parser (Klein and Man-
ning, 2003). The hyper-parameters are chosen by
cross-validation on the training split, and the test
accuracy and macro-average F1-score score are re-
ported. For recursive neural models, the dimen-
sion of word vector is set to 25, and f = tanh
is used as the nonlinearity function. We employ
10 composition matrices in AdaRNN. The param-
eters are randomly initialized. Notably, the word
vectors will also be updated.
SVM-indep: It uses the uni-gram, bi-gram,
punctuations, emoticons, and #hashtags as the
content features, and the numbers of positive or
negative words in General Inquirer as lexicon fea-
tures. These features are all target-independent.
SVM-dep: We re-implement the method pro-
posed by Jiang et al. (2011). It combines both
</bodyText>
<footnote confidence="0.73507">
1http://goo.gl/5Enpu7
</footnote>
<bodyText confidence="0.997916">
the target-independent (SVM-indep) and target-
dependent features and uses SVM as the classifier.
There are seven rules to extract target-sensitive
features. We do not implement the social graph
optimization and target expansion tricks in it.
SVM-conn: The words, punctuations, emoti-
cons, and #hashtags included in the converted de-
pendency tree are used as the features for SVM.
RNN: It is performed on the converted depen-
dency tree without adaptive composition selection.
AdaRNN-w/oE: Our approach without using
the dependency types as features in adaptive se-
lection for the composition functions.
AdaRNN-w/E: Our approach with employing
the dependency types as features in adaptive se-
lection for the composition functions.
AdaRNN-comb: We combine the root vectors
obtained by AdaRNN-w/E with the uni/bi-gram
features, and they are fed into a SVM classifier.
</bodyText>
<table confidence="0.999263">
Method Accuracy Macro-F1
SVM-indep 62.7 60.2
SVM-dep 63.4 63.3
SVM-conn 60.0 59.6
RNN 63.0 62.8
AdaRNN-w/oE 64.9 64.4
AdaRNN-w/E 65.8 65.5
AdaRNN-comb 66.3 65.9
</table>
<tableCaption confidence="0.979986333333333">
Table 1: Evaluation results on target-dependent
Twitter sentiment classification dataset. Our ap-
proach outperforms the baseline methods.
</tableCaption>
<page confidence="0.998273">
52
</page>
<bodyText confidence="0.999955810810811">
As shown in the Table 1, AdaRNN achieves bet-
ter results than the baselines. Specifically, we find
that the performances of SVM-dep increase than
SVM-indep. It indicates that target-dependent fea-
tures help improve the results. However, the accu-
racy and F1-score do not gain significantly. This
is caused by mismatch of the rules (Jiang et al.,
2011) used to extract the target-dependent fea-
tures. The POS tagging and dependency parsing
results are not precise enough for the Twitter data,
so these hand-crafted rules are rarely matched.
Further, the results of SVM-conn illustrate that us-
ing the words which have paths to target as bag-of-
words features does not perform well.
RNN is also based on the converted depen-
dency tree. It outperforms SVM-indep, and is
comparable with SVM-dep. The performances
of AdaRNN-w/oE are better than the above base-
lines. It shows that multiple composition functions
and adaptive selection help improve the results.
AdaRNN provides more powerful composition
ability, so that it achieves better semantic compo-
sition for recursive neural models. AdaRNN-w/E
obtains best performances among the above meth-
ods. Its macro-average F1-score rises by 5.3%
than the target-independent method SVM-indep.
It employs dependency types as binary features to
select the composition functions adaptively. The
results illustrate that the syntactic tags are helpful
to guide the model propagate sentiments of words
towards target. Although the dependency results
are also not precise enough, the composition se-
lection is automatically learned from data. Hence
AdaRNN is more robust for the imprecision of
parsing results than the hand-crafted rules. The
performances become better after adding the uni-
gram and bi-gram features (target-independent).
</bodyText>
<subsectionHeader confidence="0.99661">
4.1 Effects of Q
</subsectionHeader>
<bodyText confidence="0.99928925">
We compare different Q for AdaRNN defined in
Equation (3) in this section. Different parameter Q
leads to different composition selection schemes.
As illustrated in Figure 3, the AdaRNN-w/oE
and AdaRNN-w/E achieve the best accuracies at
Q = 2, and they have a similar trend. Specifi-
cally, Q = 0 obtains a uniform distribution over
the composition functions which does not help im-
prove performances. Q → ∞ results in a max-
imum probability selection algorithm, i.e., only
the composition function which has the maximum
probability is used. This selection scheme makes
</bodyText>
<figureCaption confidence="0.862786">
Figure 3: The curve shows the accuracy as the
</figureCaption>
<bodyText confidence="0.953005333333333">
hyper-parameter Q = 0, 20, 21,... , 26 increases.
AdaRNN achieves the best results at Q = 21.
the optimization instable. The performances of
Q = 1, 2 are similar and they are better than
other settings. It indicates that adaptive selection
method is useful to model the compositions. The
hyper-parameter Q makes trade-offs between uni-
form selection and maximum selection. It adjusts
the effects of these two perspectives.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999957">
We propose Adaptive Recursive Neural Network
(AdaRNN) for the target-dependent Twitter senti-
ment classification. AdaRNN employs more than
one composition functions and adaptively chooses
them depending on the context and linguistic tags.
For a given tweet, we first convert its dependency
tree for the interested target. Next, the AdaRNN
learns how to adaptively propagate the sentiments
of words to the target node. AdaRNN enables
the sentiment propagations to be sensitive to both
linguistic and semantic categories by using differ-
ent compositions. The experimental results illus-
trate that AdaRNN improves the baselines without
hand-crafted rules.
</bodyText>
<sectionHeader confidence="0.996981" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999573333333333">
This research was partly supported by the National
863 Program of China (No. 2012AA011005), the
fund of SKLSDE (Grant No. SKLSDE-2013ZX-
06), and Research Fund for the Doctoral Pro-
gram of Higher Education of China (Grant No.
20111102110019).
</bodyText>
<sectionHeader confidence="0.997669" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.964012">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137–1155, March.
</reference>
<figure confidence="0.997430545454545">
Accuracy 66
65
64
63
62
61
RNN
AdaRNN-w/oE
AdaRNN-w/E
0 20 21 22 23 24 25 26
β
</figure>
<page confidence="0.988546">
53
</page>
<reference confidence="0.99526187654321">
Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014.
Adaptive multi-compositionality for recursive neu-
ral models with applications to sentiment analysis.
In Twenty-Eighth AAAI Conference on Artificial In-
telligence (AAAI). AAAI.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. JMLR, 12:2121–2159,
July.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871–1874, June.
H.O. Georgii. 2011. Gibbs Measures and Phase
Transitions. De Gruyter studies in mathematics. De
Gruyter.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: Short Papers - Volume 2,
HLT ’11, pages 42–47, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Geoffrey E. Hinton. 1986. Learning distributed repre-
sentations of concepts. In Proceedings of the Eighth
Annual Conference of the Cognitive Science Society,
pages 1–12. Hillsdale, NJ: Erlbaum.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA. ACM.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ’11, pages 151–160, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 786–794.
Association for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.
D.E. Rumelhart, G.E. Hinton, and R.J. Williams. 1986.
Learning representations by back-propagating er-
rors. Nature, 323(6088):533–536.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011. Parsing Natural
Scenes and Natural Language with Recursive Neural
Networks. In ICML.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
EMNLP-CoNLL, pages 1201–1211.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. In EMNLP, pages 1631–1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Comput. Lin-
guist., 37(2):267–307, June.
</reference>
<page confidence="0.999025">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.309427">
<title confidence="0.8951245">Adaptive Recursive Neural for Target-dependent Twitter Sentiment Classification</title>
<affiliation confidence="0.921105">University, Beijing,</affiliation>
<address confidence="0.772491">Research, Beijing,</address>
<affiliation confidence="0.933374">HHarbin Institute of Technology, Harbin,</affiliation>
<email confidence="0.8170895">fuwei@microsoft.commingzhou@microsoft.comkexu@nlsde.buaa.edu.cn</email>
<abstract confidence="0.998064">We propose Adaptive Recursive Neural Network (AdaRNN) for target-dependent Twitter sentiment classification. AdaRNN adaptively propagates the sentiments of words to target depending on the context and syntactic relationships between them. It consists of more than one composition functions, and we model the adaptive sentiment propagations as distributions over these composition functions. The experimental studies illustrate that AdaRNN improves the baseline methods. Furthermore, we introduce a manually annotated dataset for target-dependent Twitter sentiment analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>JMLR,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2988" citStr="Bengio et al., 2003" startWordPosition="428" endWordPosition="431"> the features of products as targets, and sentiments for them are heuristically determined by the dominant opinion words. Jiang et al. (2011) combine the target-independent features (content and lexicon) and target-dependent features (rules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets. In this paper, we mainly focus on integrating target information with Recursive Neural Network (RNN) to leverage the ability of deep learning models. The neural models use distributed representation (Hinton, 1986; Rumelhart et al., 1986; Bengio et al., 2003) to automatically learn features for target-dependent sentiment classification. RNN utilizes the recursive structure of text, and it has achieved state-of-the-art sentiment analysis results for movie review dataset (Socher et al., 2012; Socher et al., 2013). The recursive neural models employ the semantic composition functions, which enables them to handle the complex compositionalities in sentiment analysis. Specifically, we propose a framework which learns to propagate the sentiments of words towards the target depending on context and syntactic structure. We employ a novel adaptive multi-co</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. JMLR, 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Dong</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
<author>Ke Xu</author>
</authors>
<title>Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis.</title>
<date>2014</date>
<booktitle>In Twenty-Eighth AAAI Conference on Artificial Intelligence (AAAI).</booktitle>
<publisher>AAAI.</publisher>
<contexts>
<context position="3682" citStr="Dong et al., 2014" startWordPosition="531" endWordPosition="534">on. RNN utilizes the recursive structure of text, and it has achieved state-of-the-art sentiment analysis results for movie review dataset (Socher et al., 2012; Socher et al., 2013). The recursive neural models employ the semantic composition functions, which enables them to handle the complex compositionalities in sentiment analysis. Specifically, we propose a framework which learns to propagate the sentiments of words towards the target depending on context and syntactic structure. We employ a novel adaptive multi-compositionality layer in recursive neural network, which is named as AdaRNN (Dong et al., 2014). It consists of more than one composition functions, and we model the adaptive sentiment propagations as learning distributions over these composition functions. We automatically learn the composition functions and how to select them from supervisions, instead of choosing them heuristically or by hand-crafted rules. AdaRNN 49 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49–54, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics determines how to propagate the sentiments towards the target </context>
</contexts>
<marker>Dong, Wei, Zhou, Xu, 2014</marker>
<rawString>Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014. Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis. In Twenty-Eighth AAAI Conference on Artificial Intelligence (AAAI). AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>JMLR,</journal>
<pages>12--2121</pages>
<contexts>
<context position="11789" citStr="Duchi et al., 2011" startWordPosition="1914" endWordPosition="1917">round truth vector t as a binary vector. If the k-th class is the label, only tk is 1 and the others are 0. Our goal is to minimize the cross-entropy error between the predicted distribution y and ground truth distribution t. For each training instance, we define the objective function as: λθllθll22 (4) where O represents the parameters, and the L2- regularization penalty is used. Based on the converted tree, we employ backpropagation algorithm (Rumelhart et al., 1986) to propagate the errors from root node to the leaf nodes. We calculate the derivatives to update the parameters. The AdaGrad (Duchi et al., 2011) is employed to solve this optimization problem. 4 Experiments As people tend to post comments for the celebrities, products, and companies, we use these keywords (such as “bill gates”, “taylor swift”, “xbox”, “windows T”, “google”) to query the Twitter API. After obtaining the tweets, we manually annotate the sentiment labels (negative, neutral, positive) for these targets. In order to eliminate the effects of data imbalance problem, we randomly sample the tweets and make the data balanced. The negative, neutral, positive classes account for 25%, 50%, 25%, respectively. Training data consists</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121–2159, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>9--1871</pages>
<contexts>
<context position="13875" citStr="Fan et al., 2008" startWordPosition="2273" endWordPosition="2276"> ... gC prep better windows nsubj g1 ... gC g1 ... gC tweets. We randomly sample some tweets, and they are assigned with sentiment labels by two annotators. About 82.5% of them have the same labels. The agreement percentage of polarity classification is higher than subjectivity classification. To the best of our knowledge, this is the largest target-dependent Twitter sentiment classification dataset which is annotated manually. We make the dataset publicly available 1 for research purposes. We preprocess the tweets by replacing the targets with $T$ and setting their POS tags to NN. Liblinear (Fan et al., 2008) is used for baselines. A tweet-specific tokenizer (Gimpel et al., 2011) is employed, and the dependency parsing results are computed by Stanford Parser (Klein and Manning, 2003). The hyper-parameters are chosen by cross-validation on the training split, and the test accuracy and macro-average F1-score score are reported. For recursive neural models, the dimension of word vector is set to 25, and f = tanh is used as the nonlinearity function. We employ 10 composition matrices in AdaRNN. The parameters are randomly initialized. Notably, the word vectors will also be updated. SVM-indep: It uses </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. J. Mach. Learn. Res., 9:1871–1874, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H O Georgii</author>
</authors>
<title>Gibbs Measures and Phase Transitions. De Gruyter studies in mathematics. De Gruyter.</title>
<date>2011</date>
<contexts>
<context position="10792" citStr="Georgii, 2011" startWordPosition="1742" endWordPosition="1743">on over these composition functions as: ⎡P (g1|vl, vr, e) ⎢ . ⎣ .. P (gC|vl, vr, e) (3) where β is the hyper-parameter, S E RCx(2D+|e|) is the matrix used to determine which composition function we use, vl, vr are the left and right child vectors, and e are external feature vector. In this work, e is a one-hot binary feature vector which indicates what the dependency type is. If relation is the k-th type, we set ek to 1 and the others to 0. Adding β in softmax function is a widely used parametrization method in statistical mechanics, which is known as Boltzmann distribution and Gibbs measure (Georgii, 2011). When β = 0, this function produces a uniform distribution; when β = 1, it is the same as softmax function; when β oc, it only activates the dimension with maximum weight, and sets its probability to 1. 3.3 Model Training We use the representation of root node as the features, and feed them into the softmax classifier to predict the distribution over classes. We define the ground truth vector t as a binary vector. If the k-th class is the label, only tk is 1 and the others are 0. Our goal is to minimize the cross-entropy error between the predicted distribution y and ground truth distribution</context>
</contexts>
<marker>Georgii, 2011</marker>
<rawString>H.O. Georgii. 2011. Gibbs Measures and Phase Transitions. De Gruyter studies in mathematics. De Gruyter.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11,</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 42–47, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Learning distributed representations of concepts.</title>
<date>1986</date>
<booktitle>In Proceedings of the Eighth Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1--12</pages>
<publisher>Erlbaum.</publisher>
<location>Hillsdale, NJ:</location>
<contexts>
<context position="2942" citStr="Hinton, 1986" startWordPosition="422" endWordPosition="423"> targets are. Hu and Liu (2004) regard the features of products as targets, and sentiments for them are heuristically determined by the dominant opinion words. Jiang et al. (2011) combine the target-independent features (content and lexicon) and target-dependent features (rules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets. In this paper, we mainly focus on integrating target information with Recursive Neural Network (RNN) to leverage the ability of deep learning models. The neural models use distributed representation (Hinton, 1986; Rumelhart et al., 1986; Bengio et al., 2003) to automatically learn features for target-dependent sentiment classification. RNN utilizes the recursive structure of text, and it has achieved state-of-the-art sentiment analysis results for movie review dataset (Socher et al., 2012; Socher et al., 2013). The recursive neural models employ the semantic composition functions, which enables them to handle the complex compositionalities in sentiment analysis. Specifically, we propose a framework which learns to propagate the sentiments of words towards the target depending on context and syntactic </context>
</contexts>
<marker>Hinton, 1986</marker>
<rawString>Geoffrey E. Hinton. 1986. Learning distributed representations of concepts. In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, pages 1–12. Hillsdale, NJ: Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2361" citStr="Hu and Liu (2004)" startWordPosition="336" endWordPosition="339">tral, positive, and negative sentiments for them, respectively. If target information is ignored, it is difficult to obtain the correct sentiment for a specified target. For target-dependent sentiment classification, the manual evaluation of Jiang et al. (2011) ∗Contribution during internship at Microsoft Research. show that about 40% of errors are caused by not considering the targets in classification. The features used in traditional learning-based methods (Pang et al., 2002; Nakagawa et al., 2010) are independent to the targets, hence the results are computed despite what the targets are. Hu and Liu (2004) regard the features of products as targets, and sentiments for them are heuristically determined by the dominant opinion words. Jiang et al. (2011) combine the target-independent features (content and lexicon) and target-dependent features (rules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets. In this paper, we mainly focus on integrating target information with Recursive Neural Network (RNN) to leverage the ability of deep learning models. The neural models use distributed representation (Hinton, 1986; Rumelhart et al.,</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent twitter sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>151--160</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2005" citStr="Jiang et al. (2011)" startWordPosition="281" endWordPosition="284">is to classify their sentiments for a given target as positive, negative, and neutral. People may mention several entities (or targets) in one tweet, which affects the availabilities for most of existing methods. For example, the tweet “@ballmer: windows phone is better than ios!” has three targets (@ballmer, windows phone, and ios). The user expresses neutral, positive, and negative sentiments for them, respectively. If target information is ignored, it is difficult to obtain the correct sentiment for a specified target. For target-dependent sentiment classification, the manual evaluation of Jiang et al. (2011) ∗Contribution during internship at Microsoft Research. show that about 40% of errors are caused by not considering the targets in classification. The features used in traditional learning-based methods (Pang et al., 2002; Nakagawa et al., 2010) are independent to the targets, hence the results are computed despite what the targets are. Hu and Liu (2004) regard the features of products as targets, and sentiments for them are heuristically determined by the dominant opinion words. Jiang et al. (2011) combine the target-independent features (content and lexicon) and target-dependent features (ru</context>
<context position="14759" citStr="Jiang et al. (2011)" startWordPosition="2413" endWordPosition="2416">st accuracy and macro-average F1-score score are reported. For recursive neural models, the dimension of word vector is set to 25, and f = tanh is used as the nonlinearity function. We employ 10 composition matrices in AdaRNN. The parameters are randomly initialized. Notably, the word vectors will also be updated. SVM-indep: It uses the uni-gram, bi-gram, punctuations, emoticons, and #hashtags as the content features, and the numbers of positive or negative words in General Inquirer as lexicon features. These features are all target-independent. SVM-dep: We re-implement the method proposed by Jiang et al. (2011). It combines both 1http://goo.gl/5Enpu7 the target-independent (SVM-indep) and targetdependent features and uses SVM as the classifier. There are seven rules to extract target-sensitive features. We do not implement the social graph optimization and target expansion tricks in it. SVM-conn: The words, punctuations, emoticons, and #hashtags included in the converted dependency tree are used as the features for SVM. RNN: It is performed on the converted dependency tree without adaptive composition selection. AdaRNN-w/oE: Our approach without using the dependency types as features in adaptive sel</context>
<context position="16307" citStr="Jiang et al., 2011" startWordPosition="2644" endWordPosition="2647">ndep 62.7 60.2 SVM-dep 63.4 63.3 SVM-conn 60.0 59.6 RNN 63.0 62.8 AdaRNN-w/oE 64.9 64.4 AdaRNN-w/E 65.8 65.5 AdaRNN-comb 66.3 65.9 Table 1: Evaluation results on target-dependent Twitter sentiment classification dataset. Our approach outperforms the baseline methods. 52 As shown in the Table 1, AdaRNN achieves better results than the baselines. Specifically, we find that the performances of SVM-dep increase than SVM-indep. It indicates that target-dependent features help improve the results. However, the accuracy and F1-score do not gain significantly. This is caused by mismatch of the rules (Jiang et al., 2011) used to extract the target-dependent features. The POS tagging and dependency parsing results are not precise enough for the Twitter data, so these hand-crafted rules are rarely matched. Further, the results of SVM-conn illustrate that using the words which have paths to target as bag-ofwords features does not perform well. RNN is also based on the converted dependency tree. It outperforms SVM-indep, and is comparable with SVM-dep. The performances of AdaRNN-w/oE are better than the above baselines. It shows that multiple composition functions and adaptive selection help improve the results. </context>
</contexts>
<marker>Jiang, Yu, Zhou, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 151–160, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14053" citStr="Klein and Manning, 2003" startWordPosition="2300" endWordPosition="2304"> them have the same labels. The agreement percentage of polarity classification is higher than subjectivity classification. To the best of our knowledge, this is the largest target-dependent Twitter sentiment classification dataset which is annotated manually. We make the dataset publicly available 1 for research purposes. We preprocess the tweets by replacing the targets with $T$ and setting their POS tags to NN. Liblinear (Fan et al., 2008) is used for baselines. A tweet-specific tokenizer (Gimpel et al., 2011) is employed, and the dependency parsing results are computed by Stanford Parser (Klein and Manning, 2003). The hyper-parameters are chosen by cross-validation on the training split, and the test accuracy and macro-average F1-score score are reported. For recursive neural models, the dimension of word vector is set to 25, and f = tanh is used as the nonlinearity function. We employ 10 composition matrices in AdaRNN. The parameters are randomly initialized. Notably, the word vectors will also be updated. SVM-indep: It uses the uni-gram, bi-gram, punctuations, emoticons, and #hashtags as the content features, and the numbers of positive or negative words in General Inquirer as lexicon features. Thes</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 423– 430, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using crfs with hidden variables.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>786--794</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2250" citStr="Nakagawa et al., 2010" startWordPosition="317" endWordPosition="320">mer: windows phone is better than ios!” has three targets (@ballmer, windows phone, and ios). The user expresses neutral, positive, and negative sentiments for them, respectively. If target information is ignored, it is difficult to obtain the correct sentiment for a specified target. For target-dependent sentiment classification, the manual evaluation of Jiang et al. (2011) ∗Contribution during internship at Microsoft Research. show that about 40% of errors are caused by not considering the targets in classification. The features used in traditional learning-based methods (Pang et al., 2002; Nakagawa et al., 2010) are independent to the targets, hence the results are computed despite what the targets are. Hu and Liu (2004) regard the features of products as targets, and sentiments for them are heuristically determined by the dominant opinion words. Jiang et al. (2011) combine the target-independent features (content and lexicon) and target-dependent features (rules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets. In this paper, we mainly focus on integrating target information with Recursive Neural Network (RNN) to leverage the abi</context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classification using crfs with hidden variables. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 786–794. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2226" citStr="Pang et al., 2002" startWordPosition="313" endWordPosition="316">e, the tweet “@ballmer: windows phone is better than ios!” has three targets (@ballmer, windows phone, and ios). The user expresses neutral, positive, and negative sentiments for them, respectively. If target information is ignored, it is difficult to obtain the correct sentiment for a specified target. For target-dependent sentiment classification, the manual evaluation of Jiang et al. (2011) ∗Contribution during internship at Microsoft Research. show that about 40% of errors are caused by not considering the targets in classification. The features used in traditional learning-based methods (Pang et al., 2002; Nakagawa et al., 2010) are independent to the targets, hence the results are computed despite what the targets are. Hu and Liu (2004) regard the features of products as targets, and sentiments for them are heuristically determined by the dominant opinion words. Jiang et al. (2011) combine the target-independent features (content and lexicon) and target-dependent features (rules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets. In this paper, we mainly focus on integrating target information with Recursive Neural Network (</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rumelhart</author>
<author>G E Hinton</author>
<author>R J Williams</author>
</authors>
<title>Learning representations by back-propagating errors.</title>
<date>1986</date>
<journal>Nature,</journal>
<volume>323</volume>
<issue>6088</issue>
<contexts>
<context position="2966" citStr="Rumelhart et al., 1986" startWordPosition="424" endWordPosition="427">Hu and Liu (2004) regard the features of products as targets, and sentiments for them are heuristically determined by the dominant opinion words. Jiang et al. (2011) combine the target-independent features (content and lexicon) and target-dependent features (rules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets. In this paper, we mainly focus on integrating target information with Recursive Neural Network (RNN) to leverage the ability of deep learning models. The neural models use distributed representation (Hinton, 1986; Rumelhart et al., 1986; Bengio et al., 2003) to automatically learn features for target-dependent sentiment classification. RNN utilizes the recursive structure of text, and it has achieved state-of-the-art sentiment analysis results for movie review dataset (Socher et al., 2012; Socher et al., 2013). The recursive neural models employ the semantic composition functions, which enables them to handle the complex compositionalities in sentiment analysis. Specifically, we propose a framework which learns to propagate the sentiments of words towards the target depending on context and syntactic structure. We employ a n</context>
<context position="11643" citStr="Rumelhart et al., 1986" startWordPosition="1889" endWordPosition="1892">e the representation of root node as the features, and feed them into the softmax classifier to predict the distribution over classes. We define the ground truth vector t as a binary vector. If the k-th class is the label, only tk is 1 and the others are 0. Our goal is to minimize the cross-entropy error between the predicted distribution y and ground truth distribution t. For each training instance, we define the objective function as: λθllθll22 (4) where O represents the parameters, and the L2- regularization penalty is used. Based on the converted tree, we employ backpropagation algorithm (Rumelhart et al., 1986) to propagate the errors from root node to the leaf nodes. We calculate the derivatives to update the parameters. The AdaGrad (Duchi et al., 2011) is employed to solve this optimization problem. 4 Experiments As people tend to post comments for the celebrities, products, and companies, we use these keywords (such as “bill gates”, “taylor swift”, “xbox”, “windows T”, “google”) to query the Twitter API. After obtaining the tweets, we manually annotate the sentiment labels (negative, neutral, positive) for these targets. In order to eliminate the effects of data imbalance problem, we randomly sam</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>D.E. Rumelhart, G.E. Hinton, and R.J. Williams. 1986. Learning representations by back-propagating errors. Nature, 323(6088):533–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Natural Scenes and Natural Language with Recursive Neural Networks.</title>
<date>2011</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="4641" citStr="Socher et al., 2011" startWordPosition="670" endWordPosition="673">s of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49–54, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics determines how to propagate the sentiments towards the target and handles the negation or intensification phenomena (Taboada et al., 2011) in sentiment analysis. In addition, we introduce a manually annotated dataset, and conduct extensive experiments on it. The experimental results suggest that our approach yields better performances than the baseline methods. 2 RNN: Recursive Neural Network RNN (Socher et al., 2011) represents the phrases and words as D-dimensional vectors. It performs compositions based on the binary trees, and obtain the vector representations in a bottom-up way. Figure 1: The composition process for “not very good” in Recursive Neural Network. As illustrated in Figure 1, we obtain the representation of “very good” by the composition of “very” and “good”, and the representation of trigram “not very good” is recursively obtained by the vectors of “not” and “very good”. The dimensions of parent node are calculated by linear combination of the child vectors’ dimensions. The vector represe</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. 2011. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces. In EMNLP-CoNLL,</title>
<date>2012</date>
<pages>1201--1211</pages>
<contexts>
<context position="3223" citStr="Socher et al., 2012" startWordPosition="461" endWordPosition="464">ules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets. In this paper, we mainly focus on integrating target information with Recursive Neural Network (RNN) to leverage the ability of deep learning models. The neural models use distributed representation (Hinton, 1986; Rumelhart et al., 1986; Bengio et al., 2003) to automatically learn features for target-dependent sentiment classification. RNN utilizes the recursive structure of text, and it has achieved state-of-the-art sentiment analysis results for movie review dataset (Socher et al., 2012; Socher et al., 2013). The recursive neural models employ the semantic composition functions, which enables them to handle the complex compositionalities in sentiment analysis. Specifically, we propose a framework which learns to propagate the sentiments of words towards the target depending on context and syntactic structure. We employ a novel adaptive multi-compositionality layer in recursive neural network, which is named as AdaRNN (Dong et al., 2014). It consists of more than one composition functions, and we model the adaptive sentiment propagations as learning distributions over these c</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In EMNLP-CoNLL, pages 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In</title>
<date>2013</date>
<booktitle>EMNLP,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="3245" citStr="Socher et al., 2013" startWordPosition="465" endWordPosition="468">endency parsing results) together in subjectivity classification and polarity classification for tweets. In this paper, we mainly focus on integrating target information with Recursive Neural Network (RNN) to leverage the ability of deep learning models. The neural models use distributed representation (Hinton, 1986; Rumelhart et al., 1986; Bengio et al., 2003) to automatically learn features for target-dependent sentiment classification. RNN utilizes the recursive structure of text, and it has achieved state-of-the-art sentiment analysis results for movie review dataset (Socher et al., 2012; Socher et al., 2013). The recursive neural models employ the semantic composition functions, which enables them to handle the complex compositionalities in sentiment analysis. Specifically, we propose a framework which learns to propagate the sentiments of words towards the target depending on context and syntactic structure. We employ a novel adaptive multi-compositionality layer in recursive neural network, which is named as AdaRNN (Dong et al., 2014). It consists of more than one composition functions, and we model the adaptive sentiment propagations as learning distributions over these composition functions. </context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In EMNLP, pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexiconbased methods for sentiment analysis.</title>
<date>2011</date>
<journal>Comput. Linguist.,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="4358" citStr="Taboada et al., 2011" startWordPosition="628" endWordPosition="631">nd we model the adaptive sentiment propagations as learning distributions over these composition functions. We automatically learn the composition functions and how to select them from supervisions, instead of choosing them heuristically or by hand-crafted rules. AdaRNN 49 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49–54, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics determines how to propagate the sentiments towards the target and handles the negation or intensification phenomena (Taboada et al., 2011) in sentiment analysis. In addition, we introduce a manually annotated dataset, and conduct extensive experiments on it. The experimental results suggest that our approach yields better performances than the baseline methods. 2 RNN: Recursive Neural Network RNN (Socher et al., 2011) represents the phrases and words as D-dimensional vectors. It performs compositions based on the binary trees, and obtain the vector representations in a bottom-up way. Figure 1: The composition process for “not very good” in Recursive Neural Network. As illustrated in Figure 1, we obtain the representation of “ver</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexiconbased methods for sentiment analysis. Comput. Linguist., 37(2):267–307, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>