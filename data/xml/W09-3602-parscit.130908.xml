<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.995413">
Anchor Text Extraction for Academic Search
</title>
<author confidence="0.999215">
Shuming Shi1 Fei Xing2* Mingjie Zhu3* Zaiqing Nie1 Ji-Rong Wen1
</author>
<affiliation confidence="0.973152666666667">
1Microsoft Research Asia
2Alibaba Group, China
3University of Science and Technology of China
</affiliation>
<email confidence="0.9312965">
{shumings, znie, jrwen}@microsoft.com
fei_c_xing@yahoo.com; mjzhu@ustc.edu
</email>
<sectionHeader confidence="0.420126" genericHeader="abstract">
Abstract*
</sectionHeader>
<bodyText confidence="0.998984956521739">
Anchor text plays a special important role in
improving the performance of general Web
search, due to the fact that it is relatively ob-
jective description for a Web page by poten-
tially a large number of other Web pages.
Academic Search provides indexing and
search functionality for academic articles. It
may be desirable to utilize anchor text in aca-
demic search as well to improve the search re-
sults quality. The main challenge here is that
no explicit URLs and anchor text is available
for academic articles. In this paper we define
and automatically assign a pseudo-URL for
each academic article. And a machine learning
approach is adopted to extract pseudo-anchor
text for academic articles, by exploiting the ci-
tation relationship between them. The ex-
tracted pseudo-anchor text is then indexed and
involved in the relevance score computation of
academic articles. Experiments conducted on
0.9 million research papers show that our ap-
proach is able to dramatically improve search
performance.
</bodyText>
<sectionHeader confidence="0.995117" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994335">
Anchor text is a piece of clickable text that links
to a target Web page. In general Web search,
anchor text plays an extremely important role in
improving the search quality. The main reason
for this is that anchor text actually aggregates the
opinion (which is more comprehensive, accurate,
and objective) of a potentially large number of
people for a Web page.
</bodyText>
<note confidence="0.558545">
* This work was performed when Fei Xing and Mingjie Zhu
were interns at Microsoft Research Asia.
</note>
<bodyText confidence="0.976714466666667">
In recent years, academic search (Giles et al.,
1998; Lawrence et al., 1999; Nie et al., 2005;
Chakrabarti et al., 2006) has become an impor-
tant supplement to general web search for re-
trieving research articles. Several academic
search systems (including Google Scholar†, Cite-
seer‡, DBLP§, Libra**, ArnetMiner††, etc.) have
been deployed. In order to improve the results
quality of an academic search system, we may
consider exploiting the techniques which are
demonstrated to be quite useful and critical in
general Web search. In this paper, we study the
possibility of extracting anchor text for research
papers and using them to improve the search per-
formance of an academic search system.
</bodyText>
<figureCaption confidence="0.986734">
Figure 1. An example of one paper citing other papers
</figureCaption>
<bodyText confidence="0.999973727272727">
The basic search unit in most academic search
systems is a research paper. Borrowing the con-
cepts of URL and anchor-text in general Web
search, we may need to assign a pseudo-URL for
one research paper as its identifier and to define
the pseudo-anchor text for it by the contextual
description when this paper is referenced (or
mentioned). The pseudo-URL of a research pa-
per could be the combination of its title, authors
and publication information. Figure-1 shows an
excerpt where one paper cites a couple of other
</bodyText>
<footnote confidence="0.701401">
† http://scholar.google.com/
‡ http://citeseerx.ist.psu.edu/
§ http://www.informatik.uni-trier.de/~ley/db/
** http://libra.msra.cn/
†† http://www.arnetminer.org/
</footnote>
<note confidence="0.797473">
10
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 10–18,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999186111111112">
papers. The grayed text can be treated as the
pseudo-anchor text of the papers being refe-
renced. Once the pseudo-anchor text of research
papers is acquired, it can be indexed and utilized
to help ranking, just as in general web search.
However it remains a challenging task to cor-
rectly identify and extract these pseudo-URLs
and pseudo-anchor texts. First, unlike the situa-
tion in general web search where one unique
URL is assigned to each web page as a natural
identifier, the information of research papers
need to be extracted from web pages or PDF files.
As a result, in constructing pseudo-URLs for
research papers, we may face the problem of ex-
traction errors, typos, and the case of one re-
search paper having different expressions in dif-
ferent places. Second, in general Web search,
anchor text is always explicitly specified by
HTML tags (&lt;a&gt; and &lt;/a&gt;). It is however much
harder to perform anchor text extraction for re-
search papers. For example, human knowledge
may be required in Figure-1 to accurately identi-
fy the description of every cited paper.
To address the above challenges, we propose
an approach for extracting and utilizing pseudo-
anchor text information in academic search to
improve the search results quality. Our approach
is composed of three phases. In the first phase,
each time a paper is cited in another paper, we
construct a tentative pseudo-URL for the cited
paper and extract a candidate anchor block for it.
The tentative pseudo-URL and the candidate
anchor block are allowed to be inaccurate. In the
second phase, we merge the tentative pseudo-
URLs that should represent the same paper. All
candidate anchor blocks belong to the same pa-
per are grouped accordingly in this phase. In the
third phase, the final pseudo-anchor text of each
paper is generated from all its candidate blocks,
by adopting a SVM-based machine learning me-
thodology. We conduct experiments upon a data-
set containing 0.9 million research papers. The
experimental results show that lots of useful anc-
hor text can be successfully extracted and accu-
mulated using our approach, and the ultimate
search performance is dramatically improved
when anchor information is indexed and used for
paper ranking.
The remaining part of this paper is organized
as follows. In Section 2, we describe in detail our
approach for pseudo-anchor text extraction and
accumulation. Experimental results are reported
in Section 3. We discuss related work in Section
4 and finally conclude the paper in Section 5.
</bodyText>
<sectionHeader confidence="0.886788" genericHeader="introduction">
2 Our Approach
</sectionHeader>
<subsectionHeader confidence="0.92136">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.999900071428572">
Before describing our approach in detail, we first
recall how anchor text is processed in general
Web search. Assume that there have been a col-
lection of documents being crawled and stored
on local disk. In the first step, each web page is
parsed and the out links (or forward links) within
the page are extracted. Each link is comprised of
a URL and its corresponding anchor text. In the
second step, all links are accumulated according
to their destination URLs (i.e. the anchor texts of
all links pointed to the same URL are merged).
Thus, we can get all anchor text corresponding to
each web page. Figure-2 (a) demonstrates this
process.
</bodyText>
<figureCaption confidence="0.70371">
Figure 2. The main process of extracting (a) anchor
</figureCaption>
<bodyText confidence="0.964653285714286">
text in general web search and (b) pseudo-anchor text
in academic search
For academic search, we need to extract and
parse the text content of papers. When a paper A
mentions another paper B, it either explicitly or
implicitly displays the key information of B to let
the users know that it is referencing B instead of
other papers. Such information can be extracted
to construct the tentative pseudo-URL of B. The
pseudo-URLs constructed in this phase are tenta-
tive because different tentative pseudo-URLs
may be merged to generate the same final pseu-
do-URL. All information related to paper B in
different papers can be accumulated and treated
</bodyText>
<figure confidence="0.9994889375">
HTML parsing Paper parsing
Group by link
destination
Anchor text
for pages
Links
Anchor block accumulation
Tentative pseudo-URLs
Candidate anchor blocks
Papers with their
candidate anchor blocks
Anchor-text learning
Papers with their
pseudo-anchor text
Web pages
Papers
</figure>
<page confidence="0.631442">
11
</page>
<bodyText confidence="0.998076693333334">
as the potential anchor text of B. Our goal is to
get the anchor text related to each paper.
Our approach for pseudo-anchor text extrac-
tion is shown in Figure-2 (b). The key process is
similar to that in general Web search for accumu-
lating and utilizing page anchor text. One prima-
ry difference between Figure-2 (a) and (b) is the
latter accumulates candidate anchor blocks rather
than pieces of anchor text. A candidate anchor
block is a piece of text that contains the descrip-
tion of one paper. The basic idea is: Instead of
extracting the anchor text for a paper directly (a
difficult task because of the lack of enough in-
formation), we first construct a candidate anchor
block to contain the &amp;quot;possible&amp;quot; or &amp;quot;potential&amp;quot; de-
scription of the paper. After we accumulate all
candidate anchor blocks, we have more informa-
tion to provide a better estimation about which
pieces of texts are anchor texts. Following this
idea, our proposed approach adopts a three-phase
methodology to extract pseudo-anchor text. In
the first phase, each time a paper B appearing in
another paper A, a candidate anchor block is ex-
tracted for B. All candidate anchor blocks belong
to the same paper are grouped in the second
phase. In the third phase, the final pseudo-anchor
text of each paper is selected among all candidate
blocks.
Extracting tentative pseudo-URLs and can-
didate anchor blocks: When one paper cites
another paper, a piece of short text (e.g. &amp;quot;[1]&amp;quot; or
“(xxx et al., 2008)”) is commonly inserted to
represent the paper to be cited, and the detail in-
formation (key attributes) of it are typically put
at the end of the document (in the references sec-
tion). We call each paper listed in the references
section a reference item. The references section
can be located by searching for the last occur-
rence of term &apos;reference&apos; or &apos;references&apos; in larger
fonts. Then, we adopt a rule-based approach to
divide the text in the references section into ref-
erence items. Another rule-based approach is
used to extract paper attributes (title, authors,
year, etc) from a reference item. We observed
some errors in our resulting pseudo-URLs caused
by the quality of HTML files converted from
PDF format, reference item extraction errors,
paper attribute extraction errors, and other fac-
tors. We also observed different reference item
formats for the same paper. The pseudo-URL for
a paper is defined according to its title, authors,
publisher, and publication year, because these
four kinds of information can readily be used to
identify a paper.
For each citation of a paper, we treat the sen-
tence containing the reference point (or citation
point) as one candidate anchor block. When mul-
tiple papers are cited in one sentence, we treat
the sentence as the candidate anchor block of
every destination paper.
Candidate Anchor Block Accumulation:
This phase is in charge of merging all candidate
blocks of the same pseudo-URL. As has been
discussed, tentative pseudo-URLs are often inac-
curate; and different tentative pseudo-URLs may
correspond to the same paper. The primary chal-
lenge here is perform the task in an efficient way
and with high accuracy. We will address this
problem in Subsection 2.2.
Pseudo-Anchor Generation: In the previous
phase, all candidate blocks of each paper have
been accumulated. This phase is to generate the
final anchor text for each paper from all its can-
didate blocks. Please refer to Subsection 2.3 for
details.
</bodyText>
<subsectionHeader confidence="0.99066">
2.2 Candidate Anchor Block Accumulation
via Multiple Feature-String Hashing
</subsectionHeader>
<bodyText confidence="0.99969875">
Consider this problem: Given a potentially huge
number of tentative pseudo-URLs for papers, we
need to identify and merge the tentative pseudo-
URLs that represent the same paper. This is like
the problems in the record linkage (Fellegi and
Sunter, 1969), entity matching, and data integra-
tion which have been extensively studied in da-
tabase, AI, and other areas. In this sub-section,
we will first show the major challenges and the
previous similar work on this kind of problem.
Then a possible approach is described to achieve
a trade-off between accuracy and efficiency.
</bodyText>
<figureCaption confidence="0.95553">
Figure 3. Two tentative pseudo-URLs representing
the same paper
</figureCaption>
<subsubsectionHeader confidence="0.996128">
2.2.1 Challenges and candidate techniques
</subsubsectionHeader>
<bodyText confidence="0.9999906">
Two issues should be addressed for this problem:
similarity measurement, and the efficiency of the
algorithm. On one hand, a proper similarity func-
tion is needed to identify two tentative pseudo-
URLs representing the same paper. Second, the
</bodyText>
<page confidence="0.66816">
12
</page>
<bodyText confidence="0.999982754716982">
integration process has to be accomplished effi-
ciently.
We choose to compute the similarity between
two papers to be a linear combination of the si-
milarities on the following fields: title, authors,
venue (conference/journal name), and year. The
similarity function on each field is carefully de-
signed. For paper title, we adopt a term-level edit
distance to compute similarity. And for paper
authors, person name abbreviation is considered.
The similarity function we adopted is fairly well
in accuracy (e.g., the similarity between the two
pseudo-URLs in Figure-3 is high according to
our function); but it is quite time-consuming to
compute the similarity for each pair of papers
(roughly 1012 similarity computation operations
are needed for 1 million different tentative pseu-
do-URLs).
Some existing methods are available for de-
creasing the times of similarity calculation opera-
tions. McCallum et al. (2000) addresses this high
dimensional data clustering problem by dividing
data into overlapping subsets called canopies
according to a cheap, approximate distance mea-
surement. Then the clustering process is per-
formed by measuring the exact distances only
between objects from the same canopy. There are
also other subspace methods (Parsons et al., 2004)
in data clustering areas, where data are divided
into subspaces of high dimensional spaces first
and then processing is done in these subspaces.
Also there are fast blocking approaches for
record linkage in Baxter et al. (2003). Though
they may have different names, they hold similar
ideas of dividing data into subsets to reduce the
candidate comparison records. The size of data-
set used in the above papers is typically quite
small (about thousands of data items). For effi-
ciency issue, Broder et al. (1997) proposed a
shingling approach to detect similar Web pages.
They noticed that it is infeasible to compare
sketches (which are generated by shingling) of
all pairs of documents. So they built an inverted
index that contains a list of shingle values and
the documents they appearing in. With the in-
verted index, they can effectively generate a list
of all the pairs of documents that share any shin-
gles, along with the number of shingles they
have in common. They did experiments on a da-
taset containing 30 million documents.
By adopting the main ideas of the above tech-
niques to our pseudo-URL matching problem, a
possible approach can be as follows.
</bodyText>
<figure confidence="0.984816181818182">
Algorithm Multiple Feature-String Hashing for candidate anchor
block accumulation
Input: A list of papers (with their tentative pseudo-URLs
and candidate anchor blocks)
Output: Papers with all candidate anchor blocks of the
same paper aggregated
Initial: An empty hashtable h (each slot of h is a list of pa-
pers)
For each paper A in the input list {
For each feature-string of A {
Lookup by the feature-string in h to get a slot s;
Add A into s;
}
}
For each slot s with size smaller than a threshold {
For any two papers A1, A2 in s {
float fSim = Similarity(A1, A2);
if(fSim &gt; the specified threshold) {
Merge A1 and A2;
}
}
}
</figure>
<figureCaption confidence="0.9374555">
Figure 4. The Multiple Feature-String Hashing algo-
rithm for candidate anchor block accumulation
</figureCaption>
<subsubsectionHeader confidence="0.989541">
2.2.2 Method adopted
</subsubsectionHeader>
<bodyText confidence="0.999992">
The method utilized here for candidate anchor
block accumulation is shown in Figure 4. The
main idea is to construct a certain number of fea-
ture strings for a tentative pseudo-URL (abbre-
viated as TP-URL) and do hash for the feature
strings. A feature string of a paper is a small
piece of text which records a part of the paper’s
key information, satisfying the following condi-
tions: First, multiple feature strings can typically
be built from a TP-URL. Second, if two TP-
URLs are different representations of the same
paper, then the probability that they have at least
one common feature string is extremely high. We
can choose the term-level n-grams of paper titles
(referring to Section 3.4) as feature strings.
The algorithm maintains an in-memory hash-
table which contains a lot of slots each of which
is a list of TP-URLs belonging to this slot. For
each TP-URL, feature strings are generated and
hashed by a specified hash function. The TP-
URL is then added into some slots according to
the hash values of its feature strings. Any two
TP-URLs belonging to the same slot are further
compared by utilizing our similarity function. If
their similarity is larger than a threshold, the two
TP-URLs are treated as being the same and
therefore their corresponding candidate anchor
blocks are merged.
The above algorithm tries to achieve good bal-
ance between accuracy and performance. On one
hand, compared with the naïve algorithm of per-
forming one-one comparison between all pairs of
TP-URLs, the algorithm needs only to compute
</bodyText>
<page confidence="0.779836">
13
</page>
<bodyText confidence="0.999985181818182">
the similarity for the TP-URLs that share a
common slot. On the other hand, because of the
special property of feature strings, most TP-
URLs representing the same paper can be de-
tected and merged.
The basic idea of dividing data into over-
lapped subsets is inherited from McCallum et al.
(2000), Broder et al. (1997), and some subspace
clustering approaches. Slightly different, we do
not count the number of common feature strings
between TP-URLs. Common bins (or inverted
indices) between data points are calculated in
McCallum et al. (2000) as a “cheap distance” for
creating canopies. The number of common Shin-
gles between two Web documents is calculated
(efficiently via inverted indices), such that Jac-
card similarity could be used to measure the si-
milarity between them. In our case, we simply
compare any two TP-URLs in the same slot by
using our similarity function directly.
The effective and efficiency of this algorithm
depend on the selection of feature strings. For a
fixed feature string generation method, the per-
formance of this algorithm is affected by the size
of each slot, especially the number and size of
big slots (slots with size larger than a threshold).
Big slots will be discarded in the algorithm to
improve performance, just like removing com-
mon Shingles in Broder et al. (1997). In Section
4, we conduct experiments to test the perfor-
mance of the above algorithm with different fea-
ture string functions and different slot size thre-
sholds.
</bodyText>
<subsectionHeader confidence="0.996976">
2.3 Pseudo-Anchor Text Learning
</subsectionHeader>
<bodyText confidence="0.99970275">
In this subsection, we address the problem of
extracting the final pseudo-anchor text for a pa-
per, given all its candidate anchor blocks (see
Figure 5 for an example).
</bodyText>
<subsubsectionHeader confidence="0.997868">
2.3.1 Problem definition
</subsubsectionHeader>
<bodyText confidence="0.999868375">
A candidate anchor block is a piece of text with
one or some reference points (a reference point is
one occurrence of citation in a paper) specified,
where a reference point is denoted by a
&lt;start_pos, end_pos&gt; pair (means start position
and end position respectively): ref = &lt;start_pos,
end_pos&gt;. We represent a candidate anchor
block to be the following format,
</bodyText>
<equation confidence="0.672917">
AnchorBlock = (Text, ref1, ref2, ...)
</equation>
<bodyText confidence="0.999016">
We define a block set to be a set of candidate
anchor blocks for a paper,
</bodyText>
<equation confidence="0.918985">
BlockSet = {AnchorBlock1, AnchorBlock2, ...}
</equation>
<bodyText confidence="0.999857">
Now the problem is: Given a block set con-
taining N elements, extract some text excerpts
from them as the anchor text of the paper.
</bodyText>
<subsubsectionHeader confidence="0.961165">
2.3.2 Learn term weights
</subsubsectionHeader>
<bodyText confidence="0.999646769230769">
We adopt a machine-learning approach to assign,
for each term in the anchor blocks, a discrete de-
gree of being anchor text. The main reasons for
taking such an approach is twofold: First, we
believe that assigning each term a fuzzy degree
of being anchor text is more appropriate than a
binary judgment as either an anchor-term or non-
anchor-term. Second, since the importance of a
term for a “link” may be determined by many
factors in paper search, a machine-learning could
be more flexible and general than the approaches
that compute term degrees by a specially de-
signed formula.
</bodyText>
<figureCaption confidence="0.9563215">
Figure 5. The candidate pseudo-anchor blocks of a
paper
</figureCaption>
<bodyText confidence="0.996560285714286">
The features used for learning are listed in Ta-
ble-1.
We observed that it would be more effective if
some of the above features are normalized before
being used for learning. For a term in candidate
anchor block B, its TF are normalized by the
BM25 formula (Robertson et al., 1999),
</bodyText>
<equation confidence="0.9608086">
 ||
)
N
k1  (b(1 b)

</equation>
<bodyText confidence="0.935819714285714">
where L is average length of the candidate blocks,
|B |is the length of B, and k1, b are parameters.
DF is normalized by the following formula,
IDFlog(1
where N is the number of elements in the block
set (i.e. total number of candidate anchor blocks
for the current paper).
</bodyText>
<figure confidence="0.738766125">
Features RefPos and Dist are normalized as,
RefPosnorm = RefPos / |B|
Distnorm = (Dist-RefPos) / |B|
And the feature BlockLen is normalized as,
 TF
(k1  1)
TFnorm

B
TF

L
DF
)
14
BlockLennorm = log(1+BlockLen)
</figure>
<table confidence="0.9917503">
Features Description
DF Document frequency: Number of candidate blocks in
which the term appears, counted among all candidate
blocks of all papers. It is used to indicate whether the
term is a stop word or not.
BF Block frequency: Number of candidate blocks in
which the term appears, counted among all candidate
blocks of this paper.
CTF Collection term frequency: Total number of times the
term appearing in the blocks. For multiple times of
occurrences in one block, all of them are counted.
IsInURL Specify whether the term appears in the pseudo-URL
of the paper.
TF Term frequency: Number of times the terms appearing
in the candidate block.
Dist Directed distance from the nearest reference point to
the term location
RefPos Position of the nearest reference point in the candidate
pseudo-anchor block.
BlockLen Length of the candidate pseudo-anchor block
</table>
<tableCaption confidence="0.97807">
Table 1. Features for learning
</tableCaption>
<bodyText confidence="0.999972">
We set four term importance levels, from 1
(unrelated terms or stop words) to 4 (words par-
ticipating in describing the main ideas of the pa-
per).
We choose support vector machine (SVM) for
learning term weights here, because of its power-
ful classification ability and well generalization
ability (Burges, 1998). We believe some other
machine learning techniques should also work
here. The input of the classifier is a feature vec-
tor of a term and the output is the importance
level of the term. Given a set of training data
{feature,, level; , a decision function f(x) can be
acquired after training. Using the decision func-
tion, we can assign an importance level for each
term automatically.
</bodyText>
<sectionHeader confidence="0.999713" genericHeader="background">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999105">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999898266666667">
Our experimental dataset contains 0.9 million
papers crawled from the web. All the papers are
processed according to the process in Figure-2
(b). We randomly select 300 queries from the
query log of Libra (libra.msra.cn) and retrieve
the results in our indexing and ranking system
with/without the pseudo-anchors generated by
our approach. Then the volunteer researchers and
students in our group are involved to judge the
search results. The top 30 results of different
ranking algorithms for each query are labeled
and assigned a relevance value from 1 (meaning
&apos;poor match&apos;) to 5 (meaning &apos;perfect match&apos;). The
search results quality is measured by NDCG
(Jarvelin and Kekalainen, 2000).
</bodyText>
<subsectionHeader confidence="0.997118">
3.2 Overall Effect of our Approach
</subsectionHeader>
<bodyText confidence="0.9873865">
Figure 6 shows the performance comparison be-
tween the results of two baseline paper ranking
algorithms and the results of including pseudo-
anchor text in ranking.
</bodyText>
<figureCaption confidence="0.6241165">
Figure 6. Comparison between the baseline approach
and our approach (measure: nDCG)
</figureCaption>
<equation confidence="0.612347">
0.619
0.627
</equation>
<bodyText confidence="0.999823777777778">
The “Base” algorithm considers the title, ab-
stract, full-text and static-rank (which is a func-
tion of the citation count) of a paper. In a bit
more detail, for each paper, we adopt the BM25
formula (Robertson et al., 1999) over its title,
abstract, and full-text respectively. And then the
resulting score is linearly combined with the stat-
ic-rank to get its final score. The static-rank is
computed as follows,
</bodyText>
<equation confidence="0.808523">
StaticRank = log(1+CitationCount) (3.1)
</equation>
<bodyText confidence="0.986503105263158">
To test the performance of including pseudo-
anchor text in ranking, we compute an anchor
score for each paper and linearly combine it with
its baseline score (i.e. the score computed by the
baseline algorithm).
We tried two kinds of ways for anchor score
computation. The first is to merge all pieces of
anchor excerpts (extracted in the previous section)
into a larger piece of anchor text, and use BM25
to compute its relevance score. In another ap-
proach called homogeneous evidence combina-
tion (Shi et al., 2006), a relevance score is com-
puted for each anchor excerpt (still using BM25),
and all the scores for the excerpts are sorted des-
cending and then combined by the following
formula,
i  1
where si (i=1, ..., m) are scores for the m anchor
excerpts, and c is a parameter. The primary idea
</bodyText>
<figure confidence="0.992544263157895">
Base(Without CitationCount)
Base
Pseudo-Anchor Included
0.689 0.673 0.672
0.597
0.466
0.426 0.388
NDCG@1 NDCG@3 NDCG@10
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<equation confidence="0.6696808">
Sanchor (1  c
M 1
(i 1))2 S
i (3.2)
15
</equation>
<bodyText confidence="0.999662666666667">
here is to let larger scores to have relative greater
weights. Please refer to Shi et al. (2006) for a
justification of this approach. As we get slightly
better results with the latter way, we use it as our
final choice for computing anchor scores.
From Figure 6, we can see that the overall per-
formance is greatly improved by including pseu-
do-anchor information. Table 2 shows the t-test
results, where a “&gt;” indicates that the algorithm
in the row outperforms that in the column with a
p-value of 0.05 or less, and a “&gt;&gt;” means a p-
value of 0.01 or less.
</bodyText>
<table confidence="0.997879">
Base Base (without
CitationCount)
Our approach &gt; &gt;&gt;
Base &gt;&gt;
Base (without Cita-
tionCount)
</table>
<tableCaption confidence="0.9619765">
Table 2. Statistical significance tests (t-test over
nDCG@3)
</tableCaption>
<bodyText confidence="0.9935158">
Table 3 shows the performance comparison by
using some traditional IR measures based on bi-
nary judgments. Since the results of not includ-
ing CitationCount are much worse than the other
two, we omit it in the table.
</bodyText>
<table confidence="0.9996966">
Measure MAP MRR P@1 P@10
Approach
Base (including 0.364 0.727 0.613 0.501
CitationCount)
Our Approach 0.381 0.734 0.625 0.531
</table>
<tableCaption confidence="0.997285">
Table 3. Performance compassion using binary judg-
ment measures
</tableCaption>
<subsectionHeader confidence="0.998061">
3.3 Sample Query Analysis
</subsectionHeader>
<bodyText confidence="0.999933117647059">
Here we analyze some sample queries to get
some insights about why and how pseudo-anchor
improves search performance. Figure-7 and Fig-
ure-8 show the top-3 results of two sample que-
ries: {TF-IDF} and {Page Rank}.
For query &amp;quot;TF-IDF&amp;quot;, the top results of the
baseline approach have keyword &amp;quot;TF-IDF&amp;quot; ap-
peared in the title as well as in other places of the
papers. Although the returned papers are relevant
to the query, they are not excellent because typi-
cally users may want to get the first TF-IDF pa-
per or some papers introducing TF-IDF. When
pseudo-anchor information is involved, some
excellent results (B1, B2, B3) are generated. The
main reason for getting the improved results is
that these papers (or books) are described with
&amp;quot;TF-IDF&amp;quot; when lots of other papers cite them.
</bodyText>
<note confidence="0.574174">
A1. K Sugiyama, K Hatano, M Yoshikawa, S Uemura. Refinement of TF-
IDF schemes for web pages using their hyperlinked neighboring pages.
Hypertext’03
</note>
<reference confidence="0.987798181818182">
A2. A Aizawa. An information-theoretic perspective of tf-idf measures.
IPM’03.
A3. N Oren. Reexamining tf.idf based information retrieval with Genet-
ic Programming. SAICSIT’02.
(a) Without anchor
B1. G Salton, MJ McGill. Introduction to Modern Information Retriev-
al. McGraw-Hill, 1983.
B2. G Salton and C Buckley. Term weighting approaches in automatic
text retrieval. IPM’98.
B3. R Baeza-Yates, B Ribeiro-Neto. Modern Information Retrieval.
Addison-Wesley, 1999
</reference>
<figure confidence="0.783182">
(b) With anchor
</figure>
<figureCaption confidence="0.964088">
Figure 7. Top-3 results for query TF-IDF
</figureCaption>
<reference confidence="0.970254142857143">
A1. V Safronov, M Parashar, Y Wang et al. Optimizing Web servers
using Page rank prefetching for clustered accesses. Information
Sciences. 2003.
A2. AO Mendelzon, D Rafiei. An autonomous page ranking method for
metasearch engines. WWW, 2002.
A3. FB Kalhoff. On formally real Division Algebras and Quasifields of
Rank two.
(a) Without anchor
B1. S Brin, L Page. The Anatomy of a Large-Scale Hypertextual Web
Search Engine. WWW, 1998
B2. L Page, S Brin, R Motwani, T Winograd. The pagerank citation
ranking: Bringing order to the web. 1998.
B3. JM Kleinberg. Authoritative sources in a hyperlinked environment.
Journal of the ACM, 1999.
</reference>
<figure confidence="0.81412">
(b) With anchor
</figure>
<figureCaption confidence="0.99942">
Figure 8. Top-3 results for query Page Rank
</figureCaption>
<bodyText confidence="0.9992888">
Figure-8 shows another example about how
pseudo-anchor helps to improve search results
quality. For query &amp;quot;Page Rank&amp;quot; (note that there is
a space in between), the results returned by the
baseline approach are not satisfactory. In the pa-
pers returned by our approach, at least B1 and B2
are very good results. Although they did not la-
bel themselves &amp;quot;Page Rank&amp;quot;, other papers do so
in citing them. Interestingly, although the result
B3 is not about the &amp;quot;PageRank&amp;quot; algorithm, it de-
scribes another popular &amp;quot;Page Rank&amp;quot; algorithm
in addition to PageRank.
Another interesting observation from the two
figures is that our approach retrieves older papers
than the baseline method, because old papers
tend to have more anchor text (due to more cita-
tions). So our approach may not be suitable for
retrieve newer papers. To overcome this problem,
maybe publication year should be considered in
our ranking functions.
</bodyText>
<subsectionHeader confidence="0.938092">
3.4 Anchor Accumulation Experiments
</subsectionHeader>
<bodyText confidence="0.9999438">
We conduct experiments to test the effectiveness
and efficiency of the multiple-feature-string-
hashing algorithm presented in Section 2.2. The
duplication detection quality of this algorithm is
determined by the appropriate selection of fea-
</bodyText>
<page confidence="0.62625">
16
</page>
<bodyText confidence="0.995266666666667">
ture strings. When feature strings are fixed, the
slot size threshold can be used to tune the tra-
deoff between accuracy and performance.
</bodyText>
<table confidence="0.999212222222222">
Feature Strings Ungram Bigram Trigram 4-gram
Slot Distr.
# of Slots 1.4*105 1.2*106 2.8*106 3.4*106
# of Slots with 5240 6806 1541 253
size &gt; 100
# of Slots with 998 363 50 5
size &gt; 1000
# of Slots with 59 11 0 0
size &gt; 10000
</table>
<tableCaption confidence="0.999796">
Table 4. Slot distribution with different feature strings
</tableCaption>
<bodyText confidence="0.999962352941177">
We take all the papers extracted from PDF
files as input to run the algorithm. Identical TP-
URLs are first eliminated (therefore their candi-
date anchor blocks are merged) by utilizing a
hash table. This pre-process step results in about
1.46 million distinct TP-URLs. The number is
larger than our collection size (0.9 million), be-
cause some cited papers are not in our paper col-
lection. We tested four kinds of feature strings all
of which are generated from paper title: uni-
grams, bigrams, trigrams, and 4-grams. Table-4
shows the slot size distribution corresponding to
each kind of feature strings. The performance
comparison among different feature strings and
slot size thresholds is shown in Table 5. It seems
that bigrams achieve a good trade-off between
accuracy and performance.
</bodyText>
<table confidence="0.9993025">
Feature Slot Size Dup. papers Processing
Strings Threshold Detected Time (sec)
Unigram 5000 529,717 119,739.0
500 327,357 7,552.7
Bigram 500 528,981 8,229.6
Trigram Infinite 518,564 8,420.4
500 516,369 2,654.9
4-gram 500 482,299 1,138.2
</table>
<tableCaption confidence="0.9713975">
Table 5. Performance comparison between different
feature strings and slot size thresholds
</tableCaption>
<sectionHeader confidence="0.999619" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.99999075">
There has been some work which uses anchor
text or their surrounding text for various Web
information retrieval tasks. It was known at the
very beginning era of internet that anchor text
was useful to Web search (McBryan, 1994).
Most Web search engines now use anchor text as
primary and power evidence for improving
search performance. The idea of using contextual
text in a certain vicinity of the anchor text was
proposed in Chakrabarti et al. (1998) to automat-
ically compile some lists of authoritative Web
resources on a range of topics. An anchor win-
dow approach is proposed in Chakrabarti et al
(1998) to extract implicit anchor text. Following
this work, anchor windows were considered in
some other tasks (Amitay et al., 1998; Haveli-
wala et al., 2002; Davison, 2002; Attardi et al.,
1999). Although we are inspired by these ideas,
our work is different because research papers
have many different properties from Web pages.
From the viewpoint of implicit anchor extraction
techniques, our approach is different from the
anchor window approach. The anchor window
approach is somewhat simpler and easy to im-
plement than ours. However, our method is more
general and flexible. In our approach, the anchor
text is not necessarily to be in a window.
Citeseer (Giles et al., 1998; Lawrence et al.,
1999) has been doing a lot of valuable work on
citation recognition, reference matching, and pa-
per indexing. It has been displaying contextual
information for cited papers. This feature has
been shown to be helpful and useful for re-
searchers. Differently, we are using context de-
scription for improving ranking rather than dis-
play purpose. In addition to Citeseer, some other
work (McCallum et al., 1999; Nanba and Oku-
mura, 1999; Nanba et al., 2004; Shi et al., 2006)
is also available for extracting and accumulating
reference information for research papers.
</bodyText>
<sectionHeader confidence="0.995158" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999721">
In this paper, we propose to improve academic
search by utilizing pseudo-anchor information.
As pseudo-URL and pseudo-anchor text are not
as explicit as in general web search, more efforts
are needed for pseudo-anchor extraction. Our
machine-learning approach has proven success-
ful in automatically extracting implicit anchor
text. By using the pseudo-anchors in our academ-
ic search system, we see a significant perfor-
mance improvement over the basic approach.
</bodyText>
<sectionHeader confidence="0.995203" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999976166666667">
We would like to thank Yunxiao Ma and Pu
Wang for converting paper full-text from PDF to
HTML format. Jian Shen has been helping us do
some reference extraction and matching work.
Special thanks are given to the researchers and
students taking part in data labeling.
</bodyText>
<page confidence="0.922011">
17
</page>
<sectionHeader confidence="0.995108" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99996984">
E. Amitay. 1998. Using common hypertext links to
identify the best phrasal description of target web
documents. In Proc. of the SIGIR&apos;98 Post Confe-
rence Workshop on Hypertext Information Re-
trieval for the Web, Melbourne, Australia.
G. Attardi, A. Gulli, and F. Sebastiani. 1999. Theseus:
categorization by context. In Proceedings of the 8th
International World Wide Web Conference.
A. Baxter, P. Christen, T. Churches. 2003. A compar-
ison of fast blocking methods for record linkage. In
ACM SIGKDD&apos;03 Workshop on Data Cleaning,
Record Linkage and Object consolidation. Wash-
ington DC.
A. Broder, S. Glassman, M. Manasse, and G. Zweig.
1997. Syntactic clustering of the Web. In Proceed-
ings of the Sixth International World Wide Web
Conference, pp. 391-404.
C.J.C. Burges. 1998. A tutorial on support vector ma-
chines for pattern recognition. Data Mining and
Knowledge Discovery, 2, 121-167.
S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg, P.
Raghavan, and S. Rajagopalan. 1998. Automatic
resource list compilation by analyzing hyperlink
structure and associated text. In Proceedings of the
7th International World Wide Web Conference.
K. Chakrabarti, V. Ganti, J. Han, and D. Xin. 2006.
Ranking objects based on relationships. In SIG-
MOD ’06: Proceedings of the 2006 ACM SIG-
MOD international conference on Management of
data, pages 371–382, New York, NY, USA. ACM.
B. Davison. 2000. Topical locality in the web. In SI-
GIR&apos;00: Proceedings of the 23rd annual interna-
tional ACM SIGIR conference on Research and
development in information retrieval, pages 272-
279, New York, NY, USA. ACM.
I.P. Fellegi, and A.B. Sunter. A Theory for Record
Linkage, Journal of the American Statistical Asso-
ciation, 64, (1969), 1183-1210.
C. L. Giles, K. Bollacker, and S. Lawrence. 1998.
CiteSeer: An automatic citation indexing system.
In IanWitten, Rob Akscyn, and Frank M. Shipman
III, editors, Digital Libraries 98 - The Third ACM
Conference on Digital Libraries, pages 89–98,
Pittsburgh, PA, June 23–26. ACM Press.
T.H. Haveliwala, A. Gionis, D. Klein, and P. Indyk.
2002. Evaluating strategies for similarity search on
the web. In WWW ’02: Proceedings of the 11th in-
ternational conference on World Wide Web, pages
432–442, New York, NY, USA. ACM.
K. Jarvelin, and J. Kekalainen. 2000. IR Evaluation
Methods for Retrieving Highly Relevant Docu-
ments. In Proceedings of the 23rd Annual Interna-
tional ACM SIGIR Conference on Research and
Development in Information Retrieval (SI-
GIR2000).
S. Lawrence, C.L. Giles, and K. Bollacker. 1999. Dig-
ital libraries and Autonomous Citation Indexing.
IEEE Computer, 32(6):67–71.
A. McCallum, K. Nigam, J. Rennie, and K. Seymore.
1999. Building Domain-specific Search Engines
with Machine Learning Techniques. In Proceed-
ings of the AAAI-99 Spring Symposium on Intelli-
gent Agents in Cyberspace.
A. McCallum, K. Nigam, and L. Ungar. 2000. Effi-
cient clustering of high-dimensional data sets with
application to reference matching. In Proc. 6th
ACM SIGKDD Int. Conf. on Knowledge Discov-
ery and Data Mining.
O.A. McBryan. 1994. Genvl and wwww: Tools for
taming the web. In In Proceedings of the First In-
ternational World Wide Web Conference, pages
79-90.
H. Nanba, M. Okumura. 1999. Towards Multi-paper
Summarization Using Reference Information. In
Proc. of the 16th International Joint Conference on
Artificial Intelligence, pp.926-931.
H. Nanba, T. Abekawa, M. Okumura, and S. Saito.
2004. Bilingual PRESRI: Integration of Multiple
Research Paper Databases. In Proc. of RIAO 2004,
195-211.
L. Parsons, E. Haque, H. Liu. 2004. Subspace cluster-
ing for high dimensional data: a review. SIGKDD
Explorations 6(1): 90-105.
S.E. Robertson, S. Walker, and M. Beaulieu. 1999.
Okapi at TREC-7: automatic ad hoc, filtering, VLC
and filtering tracks. In Proceedings of TREC’99.
S. Shi, R. Song, and J-R Wen. 2006. Latent Additivity:
Combining Homogeneous Evidence. Technique
report, MSR-TR-2006-110, Microsoft Research,
August 2006.
S. Shi, F. Xing, M. Zhu, Z.Nie, and J.-R. Wen. 2006.
Pseudo-Anchor Extraction for Search Vertical Ob-
jects. In Proc. of the 2006 ACM 15th Conference
on Information and Knowledge Management. Ar-
lington, USA.
Z. Nie, Y. Zhang, J.-R. Wen, and W.-Y. Ma. 2005.
Object-level ranking: bringing order to web objects.
InWWW’05: Proceedings of the 14th international
conference on World Wide Web, pages 567–574,
New York, NY, USA. ACM.
</reference>
<page confidence="0.949194">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.693517">
<title confidence="0.995675">Anchor Text Extraction for Academic Search</title>
<author confidence="0.927087">Fei Mingjie Zaiqing Ji-Rong</author>
<affiliation confidence="0.931646333333333">Research Group, of Science and Technology of China</affiliation>
<email confidence="0.9630475">{shumings,znie,fei_c_xing@yahoo.com;mjzhu@ustc.edu</email>
<abstract confidence="0.996166869565217">Anchor text plays a special important role in improving the performance of general Web search, due to the fact that it is relatively objective description for a Web page by potentially a large number of other Web pages. Academic Search provides indexing and search functionality for academic articles. It may be desirable to utilize anchor text in academic search as well to improve the search results quality. The main challenge here is that no explicit URLs and anchor text is available for academic articles. In this paper we define automatically assign a each academic article. And a machine learning is adopted to extract text for academic articles, by exploiting the citation relationship between them. The extracted pseudo-anchor text is then indexed and involved in the relevance score computation of academic articles. Experiments conducted on 0.9 million research papers show that our approach is able to dramatically improve search performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Without anchor B1 G Salton</author>
<author>MJ McGill</author>
</authors>
<title>Introduction to Modern Information Retriev-</title>
<date>1983</date>
<journal>B2. G Salton</journal>
<booktitle>IPM’98. B3. R Baeza-Yates, B Ribeiro-Neto. Modern Information Retrieval.</booktitle>
<publisher>Addison-Wesley,</publisher>
<note>(b) With anchor</note>
<marker>Salton, McGill, 1983</marker>
<rawString>(a) Without anchor B1. G Salton, MJ McGill. Introduction to Modern Information Retriev- al. McGraw-Hill, 1983. B2. G Salton and C Buckley. Term weighting approaches in automatic text retrieval. IPM’98. B3. R Baeza-Yates, B Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley, 1999 (b) With anchor</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>