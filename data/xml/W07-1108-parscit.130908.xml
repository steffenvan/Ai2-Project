<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000976">
<title confidence="0.988363">
Co-occurrence Contexts for Noun Compound Interpretation
</title>
<author confidence="0.906523">
Diarmuid O´ S´eaghdha
</author>
<affiliation confidence="0.957784">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.976563666666667">
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
</address>
<email confidence="0.998444">
do242@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993866" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999738461538462">
Contextual information extracted from cor-
pora is frequently used to model seman-
tic similarity. We discuss distinct classes
of context types and compare their effec-
tiveness for compound noun interpretation.
Contexts corresponding to word-word sim-
ilarity perform better than contexts corre-
sponding to relation similarity, even when
relational co-occurrences are extracted from
a much larger corpus. Combining word-
similarity and relation-similarity kernels fur-
ther improves SVM classification perfor-
mance.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951789473684">
The compound interpretation task is frequently cast
as the problem of classifying an unseen compound
noun with one of a closed set of relation categories.
These categories may consist of lexical paraphrases,
such as the prepositions of Lauer (1995), or deeper
semantic relations, such as the relations of Girju et
al. (2005) and those used here. The challenge lies in
the fact that by their very nature compounds do not
give any surface realisation to the relation that holds
between their constituents. To identify the differ-
ence between bread knife and steel knife it is not suf-
ficient to assign correct word-senses to bread, steel
and knife; it is also necessary to reason about how
the entities referred to interact in the world. A com-
mon assumption in data-driven approaches to the
problem is that compounds with semantically sim-
ilar constituents will encode similar relations. If a
hearer knows that a fish knife is a knife used to eat
fish, he/she might conclude that the novel compound
</bodyText>
<author confidence="0.43295">
Ann Copestake
</author>
<affiliation confidence="0.717034">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.787891666666667">
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
</address>
<email confidence="0.985947">
aac10@cl.cam.ac.uk
</email>
<bodyText confidence="0.999803333333333">
pigeon fork is a fork used to eat pigeon given that
pigeon is similar to fish and knife is similar to fork.
A second useful intuition is that word pairs which
co-occur in similar contexts are likely to enter into
similar relations.
In this paper, we apply these insights to identify
different kinds of contextual information that cap-
ture different kinds of similarity and compare their
applicability using medium- to large-sized corpora.
In keeping with most other research on the prob-
lem,1 we take a supervised learning approach to
compound interpretation.
</bodyText>
<sectionHeader confidence="0.997301" genericHeader="introduction">
2 Defining Contexts for Compound
Interpretation
</sectionHeader>
<bodyText confidence="0.999637">
When extracting corpus information to interpret a
compound such as bread knife, there are a number
of context types that might plausibly be of interest:
</bodyText>
<listItem confidence="0.920949538461538">
1. The contexts in which instances of the com-
pound type appear (type similarity); e.g., all
sentences in the corpus that contain the com-
pound bread knife.
2. The contexts in which instances of each con-
stituent appear (word similarity); e.g., all sen-
tences containing the word bread or the word
knife.
3. The contexts in which both constituents appear
together (relation similarity); e.g., all sentences
containing both bread and knife.
4. The context in which the particular compound
token was found (token similarity).
</listItem>
<footnote confidence="0.732071333333333">
1Such as Girju et al. (2005), Girju (2006), Turney (2006).
Lapata and Keller’s (2004) unsupervised approach is a notable
exception.
</footnote>
<page confidence="0.9808">
57
</page>
<note confidence="0.786429">
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 57–64,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999769310344827">
A simple but effective method for exploiting these
contexts is to count features that co-occur with the
target items in those contexts. Co-occurrence may
be defined in terms of proximity in the text, lexi-
cal patterns, or syntactic patterns in a parse graph.
We can parameterise our notion of context further,
for example by enforcing a constraint that the co-
occurrence correspond to a particular type of gram-
matical relation or that co-occurrence features be-
long to a particular word class.2
Research in NLP frequently makes use of one or
more of these similarity types. For example, Culotta
and Sorensen (2004) combine word similarity and
relation similarity for relation extraction; Gliozzo et
al. (2005) combine word similarity and token simi-
larity for word sense disambiguation. Turney (2006)
discusses word similarity (which he calls ”attribu-
tional similarity”) and relation similarity, but fo-
cusses on the latter and does not perform a compar-
ative study of the kind presented here.
The experiments described here investigate type,
word and relation similarity. However, token simi-
larity clearly has a role to play in the interpretation
task, as a given compound type can have a differ-
ent meaning in different contexts – for example, a
school book can be a book used in school, a book
belonging to a school or a book about a school. As
our data have been annotated in context, we intend
to model this dynamic in future work.
</bodyText>
<sectionHeader confidence="0.99613" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.993036">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999924076923077">
We used the dataset of 1443 compounds whose
development is described in O´ S´eaghdha (2007).
These compounds have been annotated in their sen-
tential contexts using the six deep semantic rela-
tions listed in Table 1. On the basis of a dual-
annotator study, O´ S´eaghdha reports agreement of
66.2% (h = 0.62) on a more general task of an-
notating a noisy corpus and estimated agreement of
73.6% (h = 0.68) on annotating the six relations
used here. These figures are superior to previously
reported results on annotating compounds extracted
from corpora. Always choosing the most frequent
class (IN) would give accuracy of 21.34%, and we
</bodyText>
<footnote confidence="0.995816">
2A flexible framework for this kind of context definition is
presented by Pad´o and Lapata (2003).
</footnote>
<table confidence="0.999393285714286">
Relation Distribution Example
BE 191 (13.24 %) steel knife, elm tree
HAVE 199 (13.79 %) street name, car door
IN 308 (21.34 %) forest hut, lunch time
INST 266 (18.43 %) rice cooker, bread knife
ACTOR 236 (16.35 %) honey bee, bus driver
ABOUT 243 (16.84 %) fairy tale, history book
</table>
<tableCaption confidence="0.9577965">
Table 1: The 6 relation classes and their distribution
in the dataset
</tableCaption>
<bodyText confidence="0.816895">
use this as a baseline for our experiments.
</bodyText>
<subsectionHeader confidence="0.997022">
3.2 Corpus
</subsectionHeader>
<bodyText confidence="0.999984857142857">
The written section of the British National Corpus,3
consisting of around 90 million words, was used in
all our experiments. This corpus is not large com-
pared to other corpora used in NLP, but it has been
manually compiled with a view to a balance of genre
and should be more representative of the language in
general than corpora containing only newswire text.
Furthermore, the compound dataset was also ex-
tracted from the BNC and information derived from
it will arguably describe the data items more accu-
rately than information from other sources. How-
ever, this information may be very sparse given the
corpus’ size. For comparison we also use a 187
million word subset of the English Gigaword Cor-
pus (Graff, 2003) to derive relational information
in Section 6. This subset consists of every para-
graph in the Gigaword Corpus belonging to articles
tagged as ‘story’ and containing both constituents of
a compound in the dataset, whether or not they are
compounded there. Both corpora were lemmatised,
tagged and parsed with RASP (Briscoe et al., 2006).
</bodyText>
<subsectionHeader confidence="0.999863">
3.3 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.99986225">
In all our experiments we use a one-against-all im-
plementation of the Support Vector Machine.4 Ex-
cept for the work described in Section 6.2 we used
the linear kernel K(x, y) = x·y to compute similar-
ity between vector representations of the data items.
The linear kernel consistently achieved superior per-
formance to the more flexible Gaussian kernel in
a range tests, presumably due to the sensitivity of
</bodyText>
<footnote confidence="0.9978955">
3http://www.natcorp.ox.ac.uk/
4The software used was LIBSVM (Chang and Lin, 2001).
</footnote>
<page confidence="0.999389">
58
</page>
<bodyText confidence="0.999985461538461">
the Gaussian kernel to its parameter settings.5 One-
against-all classification (training one classifier per
class) performed better than one-against-one (train-
ing one classifier for each pair of classes). We es-
timate test accuracy by 5-fold cross-validation and
within each fold we perform further 5-fold cross-
validation on the training set to optimise the single
SVM parameter C. An advantage of the linear kernel
is that learning is very efficient. The optimisation,
training and testing steps for each fold take from less
than a minute on a single processor for the sparsest
feature vectors to a few hours for the most dense, and
the folds can easily be distributed across machines.
</bodyText>
<sectionHeader confidence="0.97591" genericHeader="method">
4 Word Similarity
</sectionHeader>
<bodyText confidence="0.998976428571429">
O´ S´eaghdha (2007) investigates the effectiveness of
word-level co-occurrences for compound interpre-
tation, and the results presented in this section are
taken from that paper. Co-occurrences were identi-
fied in the BNC for each compound constituent in
the dataset, using the following context definitions:
win5, win10: Each word within a window of 5 or
10 words on either side of the item is a feature.
Rbasic, Rmod, Rverb, Rconj: These feature sets
use the grammatical relation output of the
RASP parser run over the written BNC. The
Rbasic feature set conflates information about
25 grammatical relations; Rmod counts only
prepositional, nominal and adjectival noun
modification; Rverb counts only relations
among subjects, objects and verbs; Rconj
counts only conjunctions of nouns.
The feature vector for each target constituent counts
its co-occurrences with the 10,000 words that most
frequently appear in the co-occurrence relations of
interest over the entire corpus. A feature vector for
each compound was created by appending the vec-
tors for its modifier and head, and these compound
vectors were used for SVM learning. To model as-
pects of co-occurrence association that might be ob-
scured by raw frequency, the log-likelihood ratio G2
(Dunning, 1993) was also used to transform the fea-
ture space.
</bodyText>
<footnote confidence="0.823143333333333">
5Keerthi and Lin (2003) prove that the Gaussian kernel will
always do as well as or better than the linear kernel for binary
classification. For multiclass classification we use multiple bi-
</footnote>
<table confidence="0.998938625">
Raw G2
Accuracy Macro Accuracy Macro
w5 52.60% 51.07% 51.35% 49.93%
w10 51.84% 50.32% 50.10% 48.60%
Rbasic 51.28% 49.92% 51.83% 50.26%
Rmod 51.35% 50.06% 48.51% 47.03%
Rverb 48.79% 47.13% 48.58% 47.07%
Rconj 54.12% 52.44% 54.95% 53.42%
</table>
<tableCaption confidence="0.999277">
Table 2: Classification results for word similarity
</tableCaption>
<bodyText confidence="0.998912375">
Micro- and macro-averaged performance figures
are given in Table 2. The micro-averaged figure
is calculated as the overall proportion of items that
were classified correctly, whereas the macro-average
is calculated as the average of the accuracy on each
class and thus balances out any skew in the class
distribution. In all cases macro-accuracy is lower
than micro-accuracy; this is due to much better per-
formance on the relations IN, INST, ACTOR and
ABOUT than on BE and HAVE. This may be be-
cause those two relations are slightly rarer and hence
provide less training data, or it may reflect a dif-
ference in the suitability of co-occurrence data for
their classification. It is interesting that features de-
rived only from conjunctions give the best perfor-
mance; these features are the most sparse but ap-
pear to be of high quality. The information con-
tained in conjunctions is conceptually very close to
the WordNet-derived information frequently used in
word-similarity based approaches to compound se-
mantics, and the performance of these features is not
far off the 56.76% accuracy (54.6% macro-average)
reported for WordNet-based classification for the
same dataset by O´ S´eaghdha (2007).
</bodyText>
<sectionHeader confidence="0.968632" genericHeader="method">
5 Type Similarity
</sectionHeader>
<bodyText confidence="0.980532454545455">
Type similarity is measured by identifying co-
occurrences with each instance of the compound
type in the corpus. In effect, we are treating com-
pounds as single words and calculating their word
similarity with each other. The same feature extrac-
tion methods were used as in the previous section.
Classification results are given in Table 3.
This method performs very poorly. Sparsity is un-
doubtedly a factor: 513 of the 1,443 compounds oc-
nary classifiers with a shared set of parameters which may not
be optimal for any single classifier.
</bodyText>
<page confidence="0.995859">
59
</page>
<table confidence="0.999473285714286">
Accuracy Macro
win5 28.62% 27.71%
win10 30.01% 28.69%
Rbasic 29.31% 28.22%
Rmod 26.54% 25.30%
Rverb 25.02% 23.96%
Rconj 24.60% 24.48%
</table>
<tableCaption confidence="0.999768">
Table 3: Classification results for type similarity
</tableCaption>
<bodyText confidence="0.998875">
cur 5 times or fewer in the BNC and 186 occur just
once. The sparser feature sets (Rmod, Rverb and
Rconj) are all outperformed by the more dense ones.
However, there is also a conceptual problem with
type similarity, in that the context of a compound
may contain information about the referent of the
compound but is less likely to contain information
about the implicit semantic relation. For example,
the following compounds all encode different mean-
ings but are likely to appear in similar contexts:
</bodyText>
<listItem confidence="0.999936666666667">
• John cut the bread with the kitchen knife.
• John cut the bread with the steel knife.
• John cut the bread with the bread knife.
</listItem>
<sectionHeader confidence="0.981651" genericHeader="method">
6 Relation Similarity
</sectionHeader>
<subsectionHeader confidence="0.970767">
6.1 Vector Space Kernels
</subsectionHeader>
<bodyText confidence="0.992539673913043">
The intuition underlying the use of relation similar-
ity is that while the relation between the constituents
of a compound may not be made explicit in the con-
text of that compound, it may be described in other
contexts where both constituents appear. For ex-
ample, sentences containing both bread and knife
may contain information about the typical interac-
tions between their referents. To extract feature vec-
tors for each constituent pair, we took the maximal
context unit to be each sentence in which both con-
stituents appear, and experimented with a range of
refinements to that context definition. The result-
ing definitions are given below in order of intuitive
richness, from measures based on word-counting to
measures making use of the structure of the sen-
tence’s dependency parse graph.
allwords All words in the sentence are co-
occurrence features. This context may be pa-
rameterised by specifying a limit on the win-
dow size to the left of the leftmost constituent
and to the right of the rightmost constituent i.e.,
the words between the two constituents are al-
ways counted.
midwords All words between the constituents are
counted.
allGRs All words in the sentence entering into a
grammatical relation (with any other word) are
counted. This context may be parameterised by
specifying a limit on the length of the shortest
path in the dependency graph from either of the
target constituents to the feature word.
shortest path All words on the shortest depen-
dency path between the two constituents are
features. If there is no such path, no features
are extracted.
path triples The shortest dependency path is de-
composed into a set of triples and these triples
are used as features. Each triple consists of a
node on the shortest path (the triple’s centre
node) and two edges connecting that node with
other nodes in the parse graph (not necessarily
nodes on the path). To generate further triple
features, one or both of the off-centre nodes is
replaced by part(s) of speech. For example, the
RASP dependency parse of The knife cut the
fresh bread is:
</bodyText>
<equation confidence="0.9999134">
(|ncsubj ||cut:3_VVD ||knife:2_NN1 |_)
(|dobj ||cut:3_VVD ||bread:6_NN1|)
(|det ||bread:6_NN1 ||the:4_AT|)
(|ncmod |_ |bread:6_NN1 ||fresh:5_JJ|)
(|det ||knife:2_NN1 ||The:1_AT|)
</equation>
<bodyText confidence="0.969724636363636">
The derived set of features includes the triples
{the:A:det+—knife:N+—cut:V:ncsubj,
A:det+—knife:N+—cut:V:ncsubj,
the:A:det+—knife:N+—V:ncsubj,
A:det+—knife:N+—V:ncsubj,
knife:N:ncsubj+—cut:V—*bread:N:dobj,
N:ncsubj+—cut:V—*bread:N:dobj,
knife:N:ncsubj+—cut:V—*N:dobj,
N:ncsubj+—cut:V—*N:dobj,... }
(The +— and —* arrows indicate the direction of
the head-modifier dependency)
</bodyText>
<page confidence="0.872103">
60
</page>
<figure confidence="0.998483">
aw5
trip
GW
BNC
0 500 1000 1500
Size
Accuracy
30 35 40 45 50 55 60
0 100 200 300 400 500
Threshold
0 100 200 300 400 500
Threshold
</figure>
<figureCaption confidence="0.999921">
Figure 1: Effect of BNC frequency on test item ac-
</figureCaption>
<bodyText confidence="0.965120961538461">
curacy for the allwords5 and triples contexts
Table 4 presents results for these contexts; in
the case of parameterisable contexts the best-
performing parameter setting is presented. We are
currently unable to present results for the path-based
contexts using the Gigaword corpus. It is clear
from the accuracy figures that we have not matched
the performance of the word similarity approach.
The best-performing single context definition is all-
words with a window parameter of 5, which yields
accuracy of 38.74% (36.78% macro-average). We
can combine the contributions of two contexts by
generating a new kernel that is the sum of the lin-
ear kernels for the individual contexts;6 the sum of
allwords5 and triples achieves the best performance
with 42.34% (40.20% macro-average).
It might be expected that the richer context def-
initions provide sparser but more precise informa-
tion, and that their relative performance might im-
prove when only frequently observed word pairs are
to be classified. However, thresholding inclusion
in the test set on corpus frequency belies that ex-
pectation; as the threshold increases and the test-
6The summed kernel function value for a pair of items is
simply the sum of the two kernel functions’ values for the pair,
i.e.:
</bodyText>
<equation confidence="0.909118">
K...(x, y) = K1(01(x), 01(y)) + K2(02(x), 02(y))
</equation>
<bodyText confidence="0.581001">
where 01, 02 are the context representations used by the two
kernels. A detailed study of kernel combination is presented by
Joachims et al. (2001).
</bodyText>
<figureCaption confidence="0.813515375">
Figure 2: Effect of corpus frequency on dataset size
for the BNC and Gigaword-derived corpus
ing data contains only more frequent pairs, all con-
texts show improved performance but the effect is
strongest for the allwords and midwords contexts.
Figure 1 shows threshold-accuracy curves for two
representative contexts (the macro-accuracy curves
are similar).
</figureCaption>
<bodyText confidence="0.96493025">
For all frequency thresholds above 6, the number
of noun pairs with above-threshold corpus frequency
is greater for the Gigaword corpus than for the BNC,
and this effect is amplified with increasing threshold
(see Figure 2). However, this difference in sparsity
does not always induce an improvement in perfor-
mance, but nor does the difference in corpus type
consistently favour the BNC.
</bodyText>
<table confidence="0.997830818181818">
BNC Gigaword
Accuracy Macro Accuracy Macro
aw 35.97% 33.39% 34.58% 32.62%
aw5 38.74% 36.78% 37.28% 35.25%
mw 32.29% 30.38% 36.24% 34.25%
agr 35.34% 33.40% 35.34% 33.34%
agr2 36.73% 34.81% 37.28% 35.59%
sp 33.54% 31.51%
trip 35.62% 34.39%
aw5+ 42.34% 40.20%
trip
</table>
<tableCaption confidence="0.999647">
Table 4: Classification results for relation similarity
</tableCaption>
<page confidence="0.998745">
61
</page>
<subsectionHeader confidence="0.999167">
6.2 String Kernels
</subsectionHeader>
<bodyText confidence="0.991614458333334">
The classification techniques described in the pre-
vious subsection represent the relational context for
each word pair as a co-occurrence vector in an in-
ner product space and compute the similarity be-
tween two pairs as a function of their vector repre-
sentations. A different kind of similarity measure is
provided by string kernels, which count the num-
ber of subsequences shared by two strings. This
class of kernel function implicitly calculates an in-
ner product in a feature space indexed by all pos-
sible subsequences (possibly restricted by length or
contiguity), but the feature vectors are not explic-
itly represented. This approach affords our notion of
context an increase in richness (features can be se-
quences of length ≥ 1) without incurring the com-
putational cost of the exponential growth in the di-
mension of our feature space. A particularly flexible
string kernel is the gap-weighted kernel described by
Lodhi et al. (2002), which allows the subsequences
to be non-contiguous but penalises the contribution
of each subsequence to the kernel value according to
the number of items occurring between the start and
end of the subsequence, including those that do not
belong to the subsequence (the “gaps”).
The kernel is defined as follows. Let s and t
be two strings of words belonging to a vocabulary
E. A subsequence u of s is defined by a sequence
of indices i = (i1, ... , i|u|) such that 1 ≤ i1 &lt;
... &lt; i|u |≤ |s|, where s is the length of s. Let
l(i) = i|u |− i1 + 1 be the length of the subsequence
in s. For example, if s is the string “cut the bread
with the knife” and u is the subsequence “cut with”
indexed by i then l(i) = 4. A is a decay parameter
between 0 and 1. The gap-weighted kernel value for
subsequences of length n of strings s and t is given
by
is generally more suitable for NLP applications to
use sequences of words (Cancedda et al., 2003).
This kernel calculates a similarity score for a pair
of strings, but for context-based compound classi-
fication we are interested in the similarity between
two sets of strings. We therefore define a context
kernel, which sums the kernel scores for each pair
of strings from the two context sets C1, C2 and nor-
malises them by the number of pairs contributing to
the sum:
s∈C1,t∈C2
That this is a valid kernel (i.e., defines an inner prod-
uct in some induced vector space) can be proven us-
ing the definition of the derived subsets kernel in
Shawe-Taylor and Cristianini (2004, p. 317). In our
experiments we further normalise the kernel to en-
sure that KCn(C1, C2) = 1 if and only if C1 = C2.
To generate the context set for a given word pair,
we extract a string from every sentence in the BNC
where the pair of words occurs no more than eight
words apart. On the hypothesis that the context
between the target words was most important and
to avoid the computational cost incurred by long
strings, we only use this middle context. To facilitate
generalisations over subsequences, the compound
head is replaced by a marker HEAD and the modifier
is replaced by a marker MOD. Word pairs for which
no context strings were extracted (i.e., pairs which
only occur as compounds in the corpus) are repre-
sented by a dummy string that matches no other. The
value of A is set to 0.5 as in Cancedda et al. (2003).
Table 5 presents results for the context kernels with
subsequence lengths 1,2,3 as well as the kernel sum
of these three kernels. These kernels perform better
than the relational vector space kernels, with the ex-
ception of the summed allwords5 + triples kernel.
</bodyText>
<equation confidence="0.978381666666667">
1 �
KCn(C1,C2) = KSn(s, t)
|C1||C2|
� Al(i)+l(j)
KSn(s, t) =
u∈En i,j:s[i]=u=t[j]
</equation>
<bodyText confidence="0.999873571428571">
Directly computing this function would be in-
tractable, as the sum is over all |E|n possible sub-
sequences of length n; however, Lodhi et al. (2002)
present an efficient dynamic programming algo-
rithm that can evaluate the kernel in O(n|s||t|) time.
Those authors’ application of string kernels to text
categorisation counts sequences of characters, but it
</bodyText>
<sectionHeader confidence="0.973238" genericHeader="method">
7 Combining Contexts
</sectionHeader>
<bodyText confidence="0.999836">
We can use the method of kernel summation to com-
bine information from different context types. If our
intuition is correct that type and relation similarity
provide different “views” of the same semantic rela-
tion, we would expect their combination to give bet-
ter results than either taken alone. This is also sug-
gested by the observation that the different context
</bodyText>
<page confidence="0.996858">
62
</page>
<table confidence="0.9975096">
Accuracy Macro
n = 1 15.94% 19.88%
n = 2 39.09% 37.23%
n = 3 39.29% 39.29%
E1�2�3 40.61% 38.53%
</table>
<tableCaption confidence="0.682851333333333">
Table 5: Classification results for gap-weighted
string kernels with subsequence lengths 1,2,3 and
the kernel sum of these kernels
</tableCaption>
<table confidence="0.9997785">
Accuracy Macro
Rconj-G2 + aw5 54.95% 53.50%
Rconj-G2 + triples 56.20% 54.54%
Rconj-G2 + aw5 + triples 55.86% 54.13%
Rconj-G2 + Kc2 56.48% 54.89%
Rconj-G2 + Kc. 56.55% 54.96%
</table>
<tableCaption confidence="0.923322">
Table 6: Classification results for context combina-
tions
</tableCaption>
<bodyText confidence="0.999827166666667">
types favour different relations: the summed string
kernel is the best at identifying IN relations (70.45%
precision, 46.67% recall), but Rconj-G2 is best at
identifying all others. This intuition is confirmed by
our experiments, the results of which appear in Ta-
ble 6. The best performance of 56.55% accuracy
(54.96% macro-average) is attained by the com-
bination of the G2-transformed Rconj word simi-
larity kernel and the summed string kernel Kc..
We note that this result, using only information ex-
tracted from the BNC, compares favourably with the
56.76% accuracy (54.60% macro-average) results
described by O´ S´eaghdha (2007) for a WordNet-
based method. The combination of Rconj-G2 and
triples is also competitive, demonstrating that a less
flexible learning algorithm (the linear kernel) can
perform well if it has access to a richer source of
information (dependency paths).
</bodyText>
<sectionHeader confidence="0.885184" genericHeader="method">
8 Comparison with Prior Work
</sectionHeader>
<bodyText confidence="0.999946088888889">
Previous work on compound semantics has tended
to concentrate on either word or relation similarity.
Approaches based on word similarity generally use
information extracted from WordNet. For example,
Girju et al. (2005) train SVM classifiers on hyper-
nymy features for each constituent. Their best re-
ported accuracy with an equivalent level of supervi-
sion to our work is 54.2%; they then improve perfor-
mance by adding a significant amount of manually-
annotated semantic information to the data, as does
Girju (2006) in a multilingual context. It is difficult
to make any conclusive comparison with these re-
sults due to fundamental differences in datasets and
classification schemes.
Approaches based on relational similarity of-
ten use relative frequencies of fixed lexical se-
quences estimated from massive corpora. Lap-
ata and Keller (2004) use Web counts for phrases
Noun P Noun where P belongs to a predefined set
of prepositions. This unsupervised approach gives
state-of-the-art results on the assignment of prepo-
sitional paraphrases, but cannot be applied to deep
semantic relations which cannot be directly identi-
fied in text. Turney and Littman (2005) search for
phrases Noun R Noun where R is one of 64 “join-
ing words”. Turney (2006) presents a more flexible
framework in which automatically identified n-gram
features replace fixed unigrams and additional word
pairs are generated by considering synonyms, but
this method still requires a Web-magnitude corpus
and a very large amount of computational time and
storage space. The latter paper reports accuracy of
58.0% (55.9% macro-average), which remains the
highest reported figure for corpus-based approaches
and demonstrates that relational similarity can per-
form well given sufficient resources.
We are not aware of previous work that compares
the effectiveness of different classes of context for
compound interpretation, nor of work that investi-
gates the utility of different corpora. We have also
described the first application of string kernels to the
compound task, though gap-weighted kernels have
been used successfully for related tasks such as word
sense disambiguation (Gliozzo et al., 2005) and re-
lation extraction (Bunescu and Mooney, 2005).
</bodyText>
<sectionHeader confidence="0.98407" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999692125">
We have defined four kinds of co-occurrence con-
texts for compound interpretation and demonstrated
that word similarity outperforms a range of relation
contexts using information derived from the British
National Corpus. Our experiments with the English
Gigaword Corpus indicate that more data is not al-
ways better, and that large newswire corpora may
not be ideally suited to general relation-based tasks.
</bodyText>
<page confidence="0.998156">
63
</page>
<bodyText confidence="0.999928592592593">
On the other hand it might be expected to be very
useful for disambiguating relations more typical of
news stories (such as tax cut, rail strike).
Future research directions include developing
more sophisticated context kernels. Cancedda et
al. (2003) present a number of potentially useful re-
finements of the gap-weighted string kernel, includ-
ing “soft matching” and differential values of A for
different words or word classes. We intend to com-
bine the benefits of string kernels with the linguis-
tic richness of syntactic parses by computing subse-
quence kernels on dependency paths. We have also
begun to experiment with the tree kernels of Mos-
chitti (2006), but are not yet in a position to report
results. As mentioned in Section 2, we also intend
to investigate the potential contribution of the sen-
tential contexts that contain the compound tokens to
be classified (token similarity).
While the BNC has many desirable properties,
it may also be fruitful to investigate the utility of
a large encyclopaedic corpus such as Wikipedia,
which may be more explicit in its description of re-
lations between real-world entities than typical text
corpora. Wikipedia has shown promise as a re-
source for measuring word similarity (Strube and
Ponzetto, 2006) and relation similarity (Suchanek et
al. (2006)).
</bodyText>
<sectionHeader confidence="0.998562" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997395164383562">
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the ACL-06 Interactive Presentation Sessions.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In Pro-
ceedings of the 19th Conference on Neural Informa-
tion Processing Systems.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean-
Michel Renders. 2003. Word-sequence kernels. Jour-
nal of Machine Learning Research, 3:1059–1082.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/˜cjlin/libsvm.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL-04.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61–74.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19(4):479–496.
Roxana Girju. 2006. Out-of-context noun phrase seman-
tic interpretation with cross-linguistic evidence. In
Proceedings of CIKM-06.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL-05.
David Graff, 2003. English Gigaword. Linguistic Data
Consortium, Philadelphia.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cate-
gorisation. In Proceedings of ICML-01.
S. Sathiya Keerthi and Chih-Jen Lin. 2003. Asymptotic
behaviors of support vector machines with Gaussian
kernel. Neural Computation, 15:1667–1689.
Mirella Lapata and Frank Keller. 2004. The Web as a
baseline: Evaluating the performance of unsupervised
Web-based models for a range of NLP tasks. In Pro-
ceedings of HLT-NAACL-04.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal ofMachine Learning
Research, 2:419–444.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML-06.
Sebastian Pad´o and Mirella Lapata. 2003. Constructing
semantic space models from parsed corpora. In Pro-
ceedings of ACL-03.
Diarmuid O´ S´eaghdha. 2007. Annotating and learn-
ing compound noun semantics. In Proceedings of the
ACL-07 Student Research Workshop.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, Cambridge.
Michael Strube and Simone Paolo Ponzetto. 2006.
WikiRelate! computing semantic relatedness using
Wikipedia. In Proceedings of AAAI-06.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. LEILA: Learning to extract infor-
mation by linguistic analysis. In Proceedings of the
ACL-06 Workshop on Ontology Learning and Popula-
tion.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1–3):251–278.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
</reference>
<page confidence="0.999419">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.232565">
<title confidence="0.999427">Co-occurrence Contexts for Noun Compound Interpretation</title>
<author confidence="0.954016">Diarmuid O´</author>
<affiliation confidence="0.9856875">Computer University of</affiliation>
<address confidence="0.796336">15 JJ Thomson Cambridge CB3</address>
<note confidence="0.6052455">United do242@cl.cam.ac.uk</note>
<abstract confidence="0.996324714285714">Contextual information extracted from corpora is frequently used to model semantic similarity. We discuss distinct classes of context types and compare their effectiveness for compound noun interpretation. Contexts corresponding to word-word similarity perform better than contexts corresponding to relation similarity, even when relational co-occurrences are extracted from a much larger corpus. Combining wordsimilarity and relation-similarity kernels further improves SVM classification performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL-06 Interactive Presentation Sessions.</booktitle>
<contexts>
<context position="7072" citStr="Briscoe et al., 2006" startWordPosition="1140" endWordPosition="1143">information derived from it will arguably describe the data items more accurately than information from other sources. However, this information may be very sparse given the corpus’ size. For comparison we also use a 187 million word subset of the English Gigaword Corpus (Graff, 2003) to derive relational information in Section 6. This subset consists of every paragraph in the Gigaword Corpus belonging to articles tagged as ‘story’ and containing both constituents of a compound in the dataset, whether or not they are compounded there. Both corpora were lemmatised, tagged and parsed with RASP (Briscoe et al., 2006). 3.3 Learning Algorithm In all our experiments we use a one-against-all implementation of the Support Vector Machine.4 Except for the work described in Section 6.2 we used the linear kernel K(x, y) = x·y to compute similarity between vector representations of the data items. The linear kernel consistently achieved superior performance to the more flexible Gaussian kernel in a range tests, presumably due to the sensitivity of 3http://www.natcorp.ox.ac.uk/ 4The software used was LIBSVM (Chang and Lin, 2001). 58 the Gaussian kernel to its parameter settings.5 Oneagainst-all classification (train</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the ACL-06 Interactive Presentation Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th Conference on Neural Information Processing Systems.</booktitle>
<contexts>
<context position="25976" citStr="Bunescu and Mooney, 2005" startWordPosition="4210" endWordPosition="4213">verage), which remains the highest reported figure for corpus-based approaches and demonstrates that relational similarity can perform well given sufficient resources. We are not aware of previous work that compares the effectiveness of different classes of context for compound interpretation, nor of work that investigates the utility of different corpora. We have also described the first application of string kernels to the compound task, though gap-weighted kernels have been used successfully for related tasks such as word sense disambiguation (Gliozzo et al., 2005) and relation extraction (Bunescu and Mooney, 2005). 9 Conclusion and Future Work We have defined four kinds of co-occurrence contexts for compound interpretation and demonstrated that word similarity outperforms a range of relation contexts using information derived from the British National Corpus. Our experiments with the English Gigaword Corpus indicate that more data is not always better, and that large newswire corpora may not be ideally suited to general relation-based tasks. 63 On the other hand it might be expected to be very useful for disambiguating relations more typical of news stories (such as tax cut, rail strike). Future resear</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. Subsequence kernels for relation extraction. In Proceedings of the 19th Conference on Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Cancedda</author>
</authors>
<title>Eric Gaussier, Cyril Goutte, and JeanMichel Renders.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1059</pages>
<marker>Cancedda, 2003</marker>
<rawString>Nicola Cancedda, Eric Gaussier, Cyril Goutte, and JeanMichel Renders. 2003. Word-sequence kernels. Journal of Machine Learning Research, 3:1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.</title>
<date>2001</date>
<tech>tw/˜cjlin/libsvm.</tech>
<contexts>
<context position="7583" citStr="Chang and Lin, 2001" startWordPosition="1221" endWordPosition="1224">t they are compounded there. Both corpora were lemmatised, tagged and parsed with RASP (Briscoe et al., 2006). 3.3 Learning Algorithm In all our experiments we use a one-against-all implementation of the Support Vector Machine.4 Except for the work described in Section 6.2 we used the linear kernel K(x, y) = x·y to compute similarity between vector representations of the data items. The linear kernel consistently achieved superior performance to the more flexible Gaussian kernel in a range tests, presumably due to the sensitivity of 3http://www.natcorp.ox.ac.uk/ 4The software used was LIBSVM (Chang and Lin, 2001). 58 the Gaussian kernel to its parameter settings.5 Oneagainst-all classification (training one classifier per class) performed better than one-against-one (training one classifier for each pair of classes). We estimate test accuracy by 5-fold cross-validation and within each fold we perform further 5-fold crossvalidation on the training set to optimise the single SVM parameter C. An advantage of the linear kernel is that learning is very efficient. The optimisation, training and testing steps for each fold take from less than a minute on a single processor for the sparsest feature vectors to</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu. tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04.</booktitle>
<contexts>
<context position="4032" citStr="Culotta and Sorensen (2004)" startWordPosition="625" endWordPosition="628">nal Linguistics A simple but effective method for exploiting these contexts is to count features that co-occur with the target items in those contexts. Co-occurrence may be defined in terms of proximity in the text, lexical patterns, or syntactic patterns in a parse graph. We can parameterise our notion of context further, for example by enforcing a constraint that the cooccurrence correspond to a particular type of grammatical relation or that co-occurrence features belong to a particular word class.2 Research in NLP frequently makes use of one or more of these similarity types. For example, Culotta and Sorensen (2004) combine word similarity and relation similarity for relation extraction; Gliozzo et al. (2005) combine word similarity and token similarity for word sense disambiguation. Turney (2006) discusses word similarity (which he calls ”attributional similarity”) and relation similarity, but focusses on the latter and does not perform a comparative study of the kind presented here. The experiments described here investigate type, word and relation similarity. However, token similarity clearly has a role to play in the interpretation task, as a given compound type can have a different meaning in differ</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="9552" citStr="Dunning, 1993" startWordPosition="1533" endWordPosition="1534">l and adjectival noun modification; Rverb counts only relations among subjects, objects and verbs; Rconj counts only conjunctions of nouns. The feature vector for each target constituent counts its co-occurrences with the 10,000 words that most frequently appear in the co-occurrence relations of interest over the entire corpus. A feature vector for each compound was created by appending the vectors for its modifier and head, and these compound vectors were used for SVM learning. To model aspects of co-occurrence association that might be obscured by raw frequency, the log-likelihood ratio G2 (Dunning, 1993) was also used to transform the feature space. 5Keerthi and Lin (2003) prove that the Gaussian kernel will always do as well as or better than the linear kernel for binary classification. For multiclass classification we use multiple biRaw G2 Accuracy Macro Accuracy Macro w5 52.60% 51.07% 51.35% 49.93% w10 51.84% 50.32% 50.10% 48.60% Rbasic 51.28% 49.92% 51.83% 50.26% Rmod 51.35% 50.06% 48.51% 47.03% Rverb 48.79% 47.13% 48.58% 47.07% Rconj 54.12% 52.44% 54.95% 53.42% Table 2: Classification results for word similarity Micro- and macro-averaged performance figures are given in Table 2. The micr</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Dan Moldovan</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
</authors>
<title>On the semantics of noun compounds.</title>
<date>2005</date>
<journal>Computer Speech and Language,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="1046" citStr="Girju et al. (2005)" startWordPosition="144" endWordPosition="147">nding to word-word similarity perform better than contexts corresponding to relation similarity, even when relational co-occurrences are extracted from a much larger corpus. Combining wordsimilarity and relation-similarity kernels further improves SVM classification performance. 1 Introduction The compound interpretation task is frequently cast as the problem of classifying an unseen compound noun with one of a closed set of relation categories. These categories may consist of lexical paraphrases, such as the prepositions of Lauer (1995), or deeper semantic relations, such as the relations of Girju et al. (2005) and those used here. The challenge lies in the fact that by their very nature compounds do not give any surface realisation to the relation that holds between their constituents. To identify the difference between bread knife and steel knife it is not sufficient to assign correct word-senses to bread, steel and knife; it is also necessary to reason about how the entities referred to interact in the world. A common assumption in data-driven approaches to the problem is that compounds with semantically similar constituents will encode similar relations. If a hearer knows that a fish knife is a </context>
<context position="3154" citStr="Girju et al. (2005)" startWordPosition="488" endWordPosition="491">ere are a number of context types that might plausibly be of interest: 1. The contexts in which instances of the compound type appear (type similarity); e.g., all sentences in the corpus that contain the compound bread knife. 2. The contexts in which instances of each constituent appear (word similarity); e.g., all sentences containing the word bread or the word knife. 3. The contexts in which both constituents appear together (relation similarity); e.g., all sentences containing both bread and knife. 4. The context in which the particular compound token was found (token similarity). 1Such as Girju et al. (2005), Girju (2006), Turney (2006). Lapata and Keller’s (2004) unsupervised approach is a notable exception. 57 Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 57–64, Prague, June 2007. c�2007 Association for Computational Linguistics A simple but effective method for exploiting these contexts is to count features that co-occur with the target items in those contexts. Co-occurrence may be defined in terms of proximity in the text, lexical patterns, or syntactic patterns in a parse graph. We can parameterise our notion of context further, for example by enforcing</context>
<context position="23982" citStr="Girju et al. (2005)" startWordPosition="3902" endWordPosition="3905">om the BNC, compares favourably with the 56.76% accuracy (54.60% macro-average) results described by O´ S´eaghdha (2007) for a WordNetbased method. The combination of Rconj-G2 and triples is also competitive, demonstrating that a less flexible learning algorithm (the linear kernel) can perform well if it has access to a richer source of information (dependency paths). 8 Comparison with Prior Work Previous work on compound semantics has tended to concentrate on either word or relation similarity. Approaches based on word similarity generally use information extracted from WordNet. For example, Girju et al. (2005) train SVM classifiers on hypernymy features for each constituent. Their best reported accuracy with an equivalent level of supervision to our work is 54.2%; they then improve performance by adding a significant amount of manuallyannotated semantic information to the data, as does Girju (2006) in a multilingual context. It is difficult to make any conclusive comparison with these results due to fundamental differences in datasets and classification schemes. Approaches based on relational similarity often use relative frequencies of fixed lexical sequences estimated from massive corpora. Lapata</context>
</contexts>
<marker>Girju, Moldovan, Tatu, Antohe, 2005</marker>
<rawString>Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe. 2005. On the semantics of noun compounds. Computer Speech and Language, 19(4):479–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Out-of-context noun phrase semantic interpretation with cross-linguistic evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of CIKM-06.</booktitle>
<contexts>
<context position="3168" citStr="Girju (2006)" startWordPosition="492" endWordPosition="493">ontext types that might plausibly be of interest: 1. The contexts in which instances of the compound type appear (type similarity); e.g., all sentences in the corpus that contain the compound bread knife. 2. The contexts in which instances of each constituent appear (word similarity); e.g., all sentences containing the word bread or the word knife. 3. The contexts in which both constituents appear together (relation similarity); e.g., all sentences containing both bread and knife. 4. The context in which the particular compound token was found (token similarity). 1Such as Girju et al. (2005), Girju (2006), Turney (2006). Lapata and Keller’s (2004) unsupervised approach is a notable exception. 57 Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 57–64, Prague, June 2007. c�2007 Association for Computational Linguistics A simple but effective method for exploiting these contexts is to count features that co-occur with the target items in those contexts. Co-occurrence may be defined in terms of proximity in the text, lexical patterns, or syntactic patterns in a parse graph. We can parameterise our notion of context further, for example by enforcing a constraint </context>
<context position="24276" citStr="Girju (2006)" startWordPosition="3953" endWordPosition="3954">f it has access to a richer source of information (dependency paths). 8 Comparison with Prior Work Previous work on compound semantics has tended to concentrate on either word or relation similarity. Approaches based on word similarity generally use information extracted from WordNet. For example, Girju et al. (2005) train SVM classifiers on hypernymy features for each constituent. Their best reported accuracy with an equivalent level of supervision to our work is 54.2%; they then improve performance by adding a significant amount of manuallyannotated semantic information to the data, as does Girju (2006) in a multilingual context. It is difficult to make any conclusive comparison with these results due to fundamental differences in datasets and classification schemes. Approaches based on relational similarity often use relative frequencies of fixed lexical sequences estimated from massive corpora. Lapata and Keller (2004) use Web counts for phrases Noun P Noun where P belongs to a predefined set of prepositions. This unsupervised approach gives state-of-the-art results on the assignment of prepositional paraphrases, but cannot be applied to deep semantic relations which cannot be directly ide</context>
</contexts>
<marker>Girju, 2006</marker>
<rawString>Roxana Girju. 2006. Out-of-context noun phrase semantic interpretation with cross-linguistic evidence. In Proceedings of CIKM-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Claudio Giuliano</author>
<author>Carlo Strapparava</author>
</authors>
<title>Domain kernels for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05.</booktitle>
<contexts>
<context position="4127" citStr="Gliozzo et al. (2005)" startWordPosition="638" endWordPosition="641"> co-occur with the target items in those contexts. Co-occurrence may be defined in terms of proximity in the text, lexical patterns, or syntactic patterns in a parse graph. We can parameterise our notion of context further, for example by enforcing a constraint that the cooccurrence correspond to a particular type of grammatical relation or that co-occurrence features belong to a particular word class.2 Research in NLP frequently makes use of one or more of these similarity types. For example, Culotta and Sorensen (2004) combine word similarity and relation similarity for relation extraction; Gliozzo et al. (2005) combine word similarity and token similarity for word sense disambiguation. Turney (2006) discusses word similarity (which he calls ”attributional similarity”) and relation similarity, but focusses on the latter and does not perform a comparative study of the kind presented here. The experiments described here investigate type, word and relation similarity. However, token similarity clearly has a role to play in the interpretation task, as a given compound type can have a different meaning in different contexts – for example, a school book can be a book used in school, a book belonging to a s</context>
<context position="25925" citStr="Gliozzo et al., 2005" startWordPosition="4202" endWordPosition="4205"> paper reports accuracy of 58.0% (55.9% macro-average), which remains the highest reported figure for corpus-based approaches and demonstrates that relational similarity can perform well given sufficient resources. We are not aware of previous work that compares the effectiveness of different classes of context for compound interpretation, nor of work that investigates the utility of different corpora. We have also described the first application of string kernels to the compound task, though gap-weighted kernels have been used successfully for related tasks such as word sense disambiguation (Gliozzo et al., 2005) and relation extraction (Bunescu and Mooney, 2005). 9 Conclusion and Future Work We have defined four kinds of co-occurrence contexts for compound interpretation and demonstrated that word similarity outperforms a range of relation contexts using information derived from the British National Corpus. Our experiments with the English Gigaword Corpus indicate that more data is not always better, and that large newswire corpora may not be ideally suited to general relation-based tasks. 63 On the other hand it might be expected to be very useful for disambiguating relations more typical of news st</context>
</contexts>
<marker>Gliozzo, Giuliano, Strapparava, 2005</marker>
<rawString>Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava. 2005. Domain kernels for word sense disambiguation. In Proceedings of ACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia.</location>
<contexts>
<context position="6736" citStr="Graff, 2003" startWordPosition="1088" endWordPosition="1089">ll our experiments. This corpus is not large compared to other corpora used in NLP, but it has been manually compiled with a view to a balance of genre and should be more representative of the language in general than corpora containing only newswire text. Furthermore, the compound dataset was also extracted from the BNC and information derived from it will arguably describe the data items more accurately than information from other sources. However, this information may be very sparse given the corpus’ size. For comparison we also use a 187 million word subset of the English Gigaword Corpus (Graff, 2003) to derive relational information in Section 6. This subset consists of every paragraph in the Gigaword Corpus belonging to articles tagged as ‘story’ and containing both constituents of a compound in the dataset, whether or not they are compounded there. Both corpora were lemmatised, tagged and parsed with RASP (Briscoe et al., 2006). 3.3 Learning Algorithm In all our experiments we use a one-against-all implementation of the Support Vector Machine.4 Except for the work described in Section 6.2 we used the linear kernel K(x, y) = x·y to compute similarity between vector representations of the</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>David Graff, 2003. English Gigaword. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>Nello Cristianini</author>
<author>John ShaweTaylor</author>
</authors>
<title>Composite kernels for hypertext categorisation.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML-01.</booktitle>
<contexts>
<context position="16943" citStr="Joachims et al. (2001)" startWordPosition="2716" endWordPosition="2719">itions provide sparser but more precise information, and that their relative performance might improve when only frequently observed word pairs are to be classified. However, thresholding inclusion in the test set on corpus frequency belies that expectation; as the threshold increases and the test6The summed kernel function value for a pair of items is simply the sum of the two kernel functions’ values for the pair, i.e.: K...(x, y) = K1(01(x), 01(y)) + K2(02(x), 02(y)) where 01, 02 are the context representations used by the two kernels. A detailed study of kernel combination is presented by Joachims et al. (2001). Figure 2: Effect of corpus frequency on dataset size for the BNC and Gigaword-derived corpus ing data contains only more frequent pairs, all contexts show improved performance but the effect is strongest for the allwords and midwords contexts. Figure 1 shows threshold-accuracy curves for two representative contexts (the macro-accuracy curves are similar). For all frequency thresholds above 6, the number of noun pairs with above-threshold corpus frequency is greater for the Gigaword corpus than for the BNC, and this effect is amplified with increasing threshold (see Figure 2). However, this d</context>
</contexts>
<marker>Joachims, Cristianini, ShaweTaylor, 2001</marker>
<rawString>Thorsten Joachims, Nello Cristianini, and John ShaweTaylor. 2001. Composite kernels for hypertext categorisation. In Proceedings of ICML-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sathiya Keerthi</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Asymptotic behaviors of support vector machines with Gaussian kernel.</title>
<date>2003</date>
<journal>Neural Computation,</journal>
<pages>15--1667</pages>
<contexts>
<context position="9622" citStr="Keerthi and Lin (2003)" startWordPosition="1544" endWordPosition="1547">s among subjects, objects and verbs; Rconj counts only conjunctions of nouns. The feature vector for each target constituent counts its co-occurrences with the 10,000 words that most frequently appear in the co-occurrence relations of interest over the entire corpus. A feature vector for each compound was created by appending the vectors for its modifier and head, and these compound vectors were used for SVM learning. To model aspects of co-occurrence association that might be obscured by raw frequency, the log-likelihood ratio G2 (Dunning, 1993) was also used to transform the feature space. 5Keerthi and Lin (2003) prove that the Gaussian kernel will always do as well as or better than the linear kernel for binary classification. For multiclass classification we use multiple biRaw G2 Accuracy Macro Accuracy Macro w5 52.60% 51.07% 51.35% 49.93% w10 51.84% 50.32% 50.10% 48.60% Rbasic 51.28% 49.92% 51.83% 50.26% Rmod 51.35% 50.06% 48.51% 47.03% Rverb 48.79% 47.13% 48.58% 47.07% Rconj 54.12% 52.44% 54.95% 53.42% Table 2: Classification results for word similarity Micro- and macro-averaged performance figures are given in Table 2. The micro-averaged figure is calculated as the overall proportion of items tha</context>
</contexts>
<marker>Keerthi, Lin, 2003</marker>
<rawString>S. Sathiya Keerthi and Chih-Jen Lin. 2003. Asymptotic behaviors of support vector machines with Gaussian kernel. Neural Computation, 15:1667–1689.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>The Web as a baseline: Evaluating the performance of unsupervised Web-based models for a range of NLP tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL-04.</booktitle>
<contexts>
<context position="24600" citStr="Lapata and Keller (2004)" startWordPosition="3999" endWordPosition="4003">(2005) train SVM classifiers on hypernymy features for each constituent. Their best reported accuracy with an equivalent level of supervision to our work is 54.2%; they then improve performance by adding a significant amount of manuallyannotated semantic information to the data, as does Girju (2006) in a multilingual context. It is difficult to make any conclusive comparison with these results due to fundamental differences in datasets and classification schemes. Approaches based on relational similarity often use relative frequencies of fixed lexical sequences estimated from massive corpora. Lapata and Keller (2004) use Web counts for phrases Noun P Noun where P belongs to a predefined set of prepositions. This unsupervised approach gives state-of-the-art results on the assignment of prepositional paraphrases, but cannot be applied to deep semantic relations which cannot be directly identified in text. Turney and Littman (2005) search for phrases Noun R Noun where R is one of 64 “joining words”. Turney (2006) presents a more flexible framework in which automatically identified n-gram features replace fixed unigrams and additional word pairs are generated by considering synonyms, but this method still req</context>
</contexts>
<marker>Lapata, Keller, 2004</marker>
<rawString>Mirella Lapata and Frank Keller. 2004. The Web as a baseline: Evaluating the performance of unsupervised Web-based models for a range of NLP tasks. In Proceedings of HLT-NAACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Designing Statistical Language Learners: Experiments on Compound Nouns.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Macquarie University.</institution>
<contexts>
<context position="970" citStr="Lauer (1995)" startWordPosition="133" endWordPosition="134">eir effectiveness for compound noun interpretation. Contexts corresponding to word-word similarity perform better than contexts corresponding to relation similarity, even when relational co-occurrences are extracted from a much larger corpus. Combining wordsimilarity and relation-similarity kernels further improves SVM classification performance. 1 Introduction The compound interpretation task is frequently cast as the problem of classifying an unseen compound noun with one of a closed set of relation categories. These categories may consist of lexical paraphrases, such as the prepositions of Lauer (1995), or deeper semantic relations, such as the relations of Girju et al. (2005) and those used here. The challenge lies in the fact that by their very nature compounds do not give any surface realisation to the relation that holds between their constituents. To identify the difference between bread knife and steel knife it is not sufficient to assign correct word-senses to bread, steel and knife; it is also necessary to reason about how the entities referred to interact in the world. A common assumption in data-driven approaches to the problem is that compounds with semantically similar constitue</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Designing Statistical Language Learners: Experiments on Compound Nouns. Ph.D. thesis, Macquarie University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="18967" citStr="Lodhi et al. (2002)" startWordPosition="3037" endWordPosition="3040">string kernels, which count the number of subsequences shared by two strings. This class of kernel function implicitly calculates an inner product in a feature space indexed by all possible subsequences (possibly restricted by length or contiguity), but the feature vectors are not explicitly represented. This approach affords our notion of context an increase in richness (features can be sequences of length ≥ 1) without incurring the computational cost of the exponential growth in the dimension of our feature space. A particularly flexible string kernel is the gap-weighted kernel described by Lodhi et al. (2002), which allows the subsequences to be non-contiguous but penalises the contribution of each subsequence to the kernel value according to the number of items occurring between the start and end of the subsequence, including those that do not belong to the subsequence (the “gaps”). The kernel is defined as follows. Let s and t be two strings of words belonging to a vocabulary E. A subsequence u of s is defined by a sequence of indices i = (i1, ... , i|u|) such that 1 ≤ i1 &lt; ... &lt; i|u |≤ |s|, where s is the length of s. Let l(i) = i|u |− i1 + 1 be the length of the subsequence in s. For example, </context>
<context position="21798" citStr="Lodhi et al. (2002)" startWordPosition="3557" endWordPosition="3560"> the corpus) are represented by a dummy string that matches no other. The value of A is set to 0.5 as in Cancedda et al. (2003). Table 5 presents results for the context kernels with subsequence lengths 1,2,3 as well as the kernel sum of these three kernels. These kernels perform better than the relational vector space kernels, with the exception of the summed allwords5 + triples kernel. 1 � KCn(C1,C2) = KSn(s, t) |C1||C2| � Al(i)+l(j) KSn(s, t) = u∈En i,j:s[i]=u=t[j] Directly computing this function would be intractable, as the sum is over all |E|n possible subsequences of length n; however, Lodhi et al. (2002) present an efficient dynamic programming algorithm that can evaluate the kernel in O(n|s||t|) time. Those authors’ application of string kernels to text categorisation counts sequences of characters, but it 7 Combining Contexts We can use the method of kernel summation to combine information from different context types. If our intuition is correct that type and relation similarity provide different “views” of the same semantic relation, we would expect their combination to give better results than either taken alone. This is also suggested by the observation that the different context 62 Acc</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. Journal ofMachine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Proceedings of ECML-06.</booktitle>
<contexts>
<context position="27075" citStr="Moschitti (2006)" startWordPosition="4385" endWordPosition="4387">e very useful for disambiguating relations more typical of news stories (such as tax cut, rail strike). Future research directions include developing more sophisticated context kernels. Cancedda et al. (2003) present a number of potentially useful refinements of the gap-weighted string kernel, including “soft matching” and differential values of A for different words or word classes. We intend to combine the benefits of string kernels with the linguistic richness of syntactic parses by computing subsequence kernels on dependency paths. We have also begun to experiment with the tree kernels of Moschitti (2006), but are not yet in a position to report results. As mentioned in Section 2, we also intend to investigate the potential contribution of the sentential contexts that contain the compound tokens to be classified (token similarity). While the BNC has many desirable properties, it may also be fruitful to investigate the utility of a large encyclopaedic corpus such as Wikipedia, which may be more explicit in its description of relations between real-world entities than typical text corpora. Wikipedia has shown promise as a resource for measuring word similarity (Strube and Ponzetto, 2006) and rel</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings of ECML-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Constructing semantic space models from parsed corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-03.</booktitle>
<marker>Pad´o, Lapata, 2003</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2003. Constructing semantic space models from parsed corpora. In Proceedings of ACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Annotating and learning compound noun semantics.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-07 Student Research Workshop.</booktitle>
<marker>S´eaghdha, 2007</marker>
<rawString>Diarmuid O´ S´eaghdha. 2007. Annotating and learning compound noun semantics. In Proceedings of the ACL-07 Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="20481" citStr="Shawe-Taylor and Cristianini (2004" startWordPosition="3321" endWordPosition="3324">pplications to use sequences of words (Cancedda et al., 2003). This kernel calculates a similarity score for a pair of strings, but for context-based compound classification we are interested in the similarity between two sets of strings. We therefore define a context kernel, which sums the kernel scores for each pair of strings from the two context sets C1, C2 and normalises them by the number of pairs contributing to the sum: s∈C1,t∈C2 That this is a valid kernel (i.e., defines an inner product in some induced vector space) can be proven using the definition of the derived subsets kernel in Shawe-Taylor and Cristianini (2004, p. 317). In our experiments we further normalise the kernel to ensure that KCn(C1, C2) = 1 if and only if C1 = C2. To generate the context set for a given word pair, we extract a string from every sentence in the BNC where the pair of words occurs no more than eight words apart. On the hypothesis that the context between the target words was most important and to avoid the computational cost incurred by long strings, we only use this middle context. To facilitate generalisations over subsequences, the compound head is replaced by a marker HEAD and the modifier is replaced by a marker MOD. Wo</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>WikiRelate! computing semantic relatedness using Wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI-06.</booktitle>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>Michael Strube and Simone Paolo Ponzetto. 2006. WikiRelate! computing semantic relatedness using Wikipedia. In Proceedings of AAAI-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Georgiana Ifrim</author>
<author>Gerhard Weikum</author>
</authors>
<title>LEILA: Learning to extract information by linguistic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL-06 Workshop on Ontology Learning and Population.</booktitle>
<marker>Suchanek, Ifrim, Weikum, 2006</marker>
<rawString>Fabian M. Suchanek, Georgiana Ifrim, and Gerhard Weikum. 2006. LEILA: Learning to extract information by linguistic analysis. In Proceedings of the ACL-06 Workshop on Ontology Learning and Population.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Corpusbased learning of analogies and semantic relations.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="24918" citStr="Turney and Littman (2005)" startWordPosition="4050" endWordPosition="4053">l context. It is difficult to make any conclusive comparison with these results due to fundamental differences in datasets and classification schemes. Approaches based on relational similarity often use relative frequencies of fixed lexical sequences estimated from massive corpora. Lapata and Keller (2004) use Web counts for phrases Noun P Noun where P belongs to a predefined set of prepositions. This unsupervised approach gives state-of-the-art results on the assignment of prepositional paraphrases, but cannot be applied to deep semantic relations which cannot be directly identified in text. Turney and Littman (2005) search for phrases Noun R Noun where R is one of 64 “joining words”. Turney (2006) presents a more flexible framework in which automatically identified n-gram features replace fixed unigrams and additional word pairs are generated by considering synonyms, but this method still requires a Web-magnitude corpus and a very large amount of computational time and storage space. The latter paper reports accuracy of 58.0% (55.9% macro-average), which remains the highest reported figure for corpus-based approaches and demonstrates that relational similarity can perform well given sufficient resources.</context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2005. Corpusbased learning of analogies and semantic relations. Machine Learning, 60(1–3):251–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="3183" citStr="Turney (2006)" startWordPosition="494" endWordPosition="495">hat might plausibly be of interest: 1. The contexts in which instances of the compound type appear (type similarity); e.g., all sentences in the corpus that contain the compound bread knife. 2. The contexts in which instances of each constituent appear (word similarity); e.g., all sentences containing the word bread or the word knife. 3. The contexts in which both constituents appear together (relation similarity); e.g., all sentences containing both bread and knife. 4. The context in which the particular compound token was found (token similarity). 1Such as Girju et al. (2005), Girju (2006), Turney (2006). Lapata and Keller’s (2004) unsupervised approach is a notable exception. 57 Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 57–64, Prague, June 2007. c�2007 Association for Computational Linguistics A simple but effective method for exploiting these contexts is to count features that co-occur with the target items in those contexts. Co-occurrence may be defined in terms of proximity in the text, lexical patterns, or syntactic patterns in a parse graph. We can parameterise our notion of context further, for example by enforcing a constraint that the cooccu</context>
<context position="25001" citStr="Turney (2006)" startWordPosition="4069" endWordPosition="4070">tal differences in datasets and classification schemes. Approaches based on relational similarity often use relative frequencies of fixed lexical sequences estimated from massive corpora. Lapata and Keller (2004) use Web counts for phrases Noun P Noun where P belongs to a predefined set of prepositions. This unsupervised approach gives state-of-the-art results on the assignment of prepositional paraphrases, but cannot be applied to deep semantic relations which cannot be directly identified in text. Turney and Littman (2005) search for phrases Noun R Noun where R is one of 64 “joining words”. Turney (2006) presents a more flexible framework in which automatically identified n-gram features replace fixed unigrams and additional word pairs are generated by considering synonyms, but this method still requires a Web-magnitude corpus and a very large amount of computational time and storage space. The latter paper reports accuracy of 58.0% (55.9% macro-average), which remains the highest reported figure for corpus-based approaches and demonstrates that relational similarity can perform well given sufficient resources. We are not aware of previous work that compares the effectiveness of different cla</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>