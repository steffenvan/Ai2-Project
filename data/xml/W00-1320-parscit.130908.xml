<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.995939">
A Statistical Model for Parsing and Word-Sense Disambiguation
</title>
<author confidence="0.999345">
Daniel M. Bikel
</author>
<affiliation confidence="0.9975255">
Dept. of Computer &amp; Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.802766">
200 South 33rd Street, Philadelphia, PA 19104-6389, U.S.A.
</address>
<email confidence="0.678967">
dbikel@c is .upenn.edu
</email>
<sectionHeader confidence="0.987544" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970272727273">
This paper describes a first attempt at a sta-
tistical model for simultaneous syntactic pars-
ing and generalized word-sense disambigua-
tion. On a new data set we have constructed
for the task, while we were disappointed not
to find parsing improvement over a traditional
parsing model, our model achieves a recall of
84.0% and a precision of 67.3% of exact synset
matches on our test corpus, where the gold
standard has a reported inter-annotator agree-
ment of 78.6%.
</bodyText>
<sectionHeader confidence="0.997911" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999228888888889">
In this paper we describe a generative, statis-
tical model for simultaneously producing syn-
tactic parses and word senses in sentences.
We begin by motivating this new approach to
these two, previously-separate problems, then,
after reviewing previous work in these areas,
we describe our model in detail. Finally, we
will present the promising results of this, our
first attempt, and the direction of future work.
</bodyText>
<sectionHeader confidence="0.701905" genericHeader="introduction">
2 Motivation for the Approach
2.1 Motivation from examples
</sectionHeader>
<bodyText confidence="0.814053">
Consider the following examples:
</bodyText>
<listItem confidence="0.985374857142857">
1. IBM bought Lotus for $200 million.
2. Sony widened its product line with per-
sonal computers.
3. The bank issued a check for $100,000.
4. Apple is expecting [Ni, strong results].
5. IBM expected [SBAR each employee to
wear a shirt and tie].
</listItem>
<bodyText confidence="0.999842346153846">
With Example 1, the reading [IBM bought
[Lotus for $200 million]] is nearly impossi-
ble, for the simple reason that a monetary
amount is a likely instrument for buying and
not for describing a company. Similarly, there
is a reasonably strong preference in Example
2 for [pp with personal computers] to attach
to widened, because personal computers are
products with which a product line could be
widened. As pointed out by (Stetina and Na-
gao, 1997), word sense information can be a
proxy for the semantic- and world-knowledge
we as humans bring to bear on attachment
decisions such as these. This proxy effect is
due to the &amp;quot;lightweight semantics&amp;quot; that word
senses—in particular WordNet word senses—
convey.
Conversely, both the syntactic and semantic
context in Example 3 let us know that bank
is not a river bank and that check is not a
restaurant bill. In Examples 4 and 5, knowing
that the complement of expect is an NP or an
SBAR provides information as to whether the
sense is &amp;quot;await&amp;quot; or &amp;quot;require&amp;quot;. Thus, Examples
3-5 illustrate how the syntactic context of a
word can help determine its meaning.
</bodyText>
<subsectionHeader confidence="0.9342955">
2.2 Motivation from previous work
2.2.1 Parsing
</subsectionHeader>
<bodyText confidence="0.99854745">
In recent years, the success of statistical pars-
ing techniques can be attributed to several fac-
tors, such as the increasing size of comput-
ing machinery to accommodate larger models,
the availability of resources such as the Penn
Treebank (Marcus et al., 1993) and the suc-
cess of machine learning techniques for lower-
level NLP problems, such as part-of-speech
tagging (Church, 1988; Brill, 1995), and PP-
attachment (Brill and Resnik, 1994; Collins
and Brooks, 1995). However, perhaps even
more significant has been the lexicalization
of the grammar formalisms being probabilis-
tically modeled: crucially, all the recent, suc-
cessful statistical parsers have in some way
made use of bilexical dependencies. This in-
cludes both the parsers that attach probabili-
ties to parser moves (Magerman, 1995; Ratna-
parkhi, 1997), but also those of the lexicalized
PCFG variety (Collins, 1997; Charniak, 1997).
</bodyText>
<page confidence="0.998343">
155
</page>
<bodyText confidence="0.9999867">
Even more crucially, the bilexical dependen-
cies involve head-modifier relations (hereafter
referred to simply as &amp;quot;head relations&amp;quot;). The in-
tuition behind the lexicalization of a grammar
formalism is to capture lexical items&apos; idiosyn-
cratic parsing preferences. The intuition be-
hind using heads as the members of the bilex-
ical relations is twofold. First, many linguis-
tic theories tell us that the head of a phrase
projects the skeleton of that phrase, to be filled
in by specifiers, complements and adjuncts;
such a notion is captured quite directly by
a formalism such as LTAG (Joshi and Sch-
abes, 1997). Second, the head of a phrase usu-
ally conveys some large component of the se-
mantics of that phrase.&apos; In this way, using
head-relation statistics encodes a bit of the
predicate-argument structure in the syntac-
tic model. While there are cases such as John
was believed to have been shot by Bill where
structural preference virtually eliminates one
of the two semantically plausible analyses, it
is quite clear that semantics—and, in particu-
lar, lexical head semantics—play a very im-
portant role in reducing parsing ambiguity.
(See (Collins, 1999), pp. 207ff., for an excel-
lent discussion of structural vs. semantic pars-
ing preferences, including the above John was
believed... example.)
Another motivation for incorporating word
senses into a statistical parsing model has
been to ameliorate the sparse data prob-
lem. Inspired by the PP-attachment work of
(Stetina and Nagao, 1997), we use Word-
Net v1.6 (Miller et al., 1990) as our seman-
tic dictionary, where the hypernym structure
provides the basis for semantically-motivated
soft clusters.2 We discuss this benefit of word
senses and the details of our implementation
further in Section 4.
</bodyText>
<subsubsectionHeader confidence="0.727363">
2.2.2 Word-sense disambiguation
</subsubsectionHeader>
<bodyText confidence="0.9702205">
While there has been much work in this area,
let us examine the features used in recent
&apos;Heads originated this way, but it has become nec-
essary to distinguish &amp;quot;semantic&amp;quot; heads, such as nouns
and verbs, that correspond roughly to predicates and
arguments, from &amp;quot;functional&amp;quot; heads, such as deter-
miners, INFL&apos;s and complementizers, that correspond
roughly to logical operators or are purely syntactic el-
ements. In this paper, we almost always intend &amp;quot;head&amp;quot;
to mean &amp;quot;semantic head&amp;quot;.
</bodyText>
<footnote confidence="0.760844">
2Soft clusters are sets where the elements have
weights indicating the strength of their membership
in the set, which in this case allows for a probability
distribution to be defined over a word&apos;s membership
in all the clusters.
</footnote>
<bodyText confidence="0.999867285714286">
statistical approaches. (Yarowsky, 1992) uses
wide &amp;quot;bag-of-words&amp;quot; contexts with a naive
Bayes classifier. (Yarowsky, 1995) also uses
wide context, but incorporates the one-sense-
per-discourse and one-sense-per-collocation
constraints, using an unsupervised learn-
ing technique. The supervised technique in
(Yarowsky, 1994) has a more specific notion
of context, employing not just words that can
appear within a window of ±k, but crucially
words that abut and fall in the ±2 window
of the target word. More recently, (Lin, 1997)
has shown how syntactic context, and depen-
dency structures in particular, can be suc-
cessfully employed for word sense disambigua-
tion. (Stetina and Nagao, 1997) have shown
that by employing a fairly simple and some-
what ad-hoc unsupervised method of WSD us-
ing a WordNet-based similarity heuristic, they
could enhance PP-attachment performance to
a significantly higher level than systems that
made no use of lexical semantics (88.1% accu-
racy). Most recently, in (Stetina et al., 1998),
the authors made use of head-driven bilexi-
cal dependencies with syntactic relations to
attack the problem of generalized word-sense
disambiguation, precisely one of the two prob-
lems we are dealing with here.
</bodyText>
<sectionHeader confidence="0.997725" genericHeader="method">
3 The Model
</sectionHeader>
<subsectionHeader confidence="0.528607">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999976181818182">
The parsing model we started with was ex-
tracted from BBN&apos;s SIFT system (Miller et
al., 1998), which we briefly present again here,
using examples from Figure 1 to illustrate the
model&apos;s paxameters.3
The model generates the head of a con-
stituent first, then each of the left- and right-
modifiers, generating from the head outward,
using a bigram model of node labels. Here are
the first few elements generated by the model
for the tree of Figure 1:
</bodyText>
<listItem confidence="0.9946488">
1. S and its head word and part of speech,
caught-VBD.
2. The head constituent of S, VP.
3. The head word of the VP, caught-VBD.
4. The premodifier constituent ADVP.
</listItem>
<footnote confidence="0.99828075">
3We began with the BBN parser because its authors
were kind enough to allow us to extend it, and because
its design allowed easy integration with our existing
WordNet code.
</footnote>
<page confidence="0.997632">
156
</page>
<figure confidence="0.975040857142857">
S(caught-VBD)
NP(boy-NN) ADVP(also-RB) VP(caught-VBD)
......-----...„ RB VBD NP(ball-NN)
DET NN also I__.-------.,
I I caught DET NN
The boy I I
the ball
</figure>
<figureCaption confidence="0.998732">
Figure 1: A sample sentence with parse tree.
</figureCaption>
<listItem confidence="0.889535428571429">
5. The head word of the premodifier ADVP,
also-RB.
6. The premodifier constituent NP.
7. The head word of the premodifier NP,
boy-NN.
8. The +END+ (null) postmodifier con-
stituent of the VP.
</listItem>
<bodyText confidence="0.9998582">
This process recurses on each of the modifier
constituents (in this case, the subject NP and
the VP) until all words have been generated.
(Note that many words effectively get gener-
ated high up in the tree; in this example sen-
tence, the last words to get generated are the
two the&apos;s)
More formally, the lexicalized PCFG that
sits behind the parsing model has rules of the
form
</bodyText>
<equation confidence="0.993895">
P -- L7,1,7,_i - - • LiHRi - • - R„_11?„, (1)
</equation>
<bodyText confidence="0.9999933">
where P, H, L, and Ri are all lexicalized non-
terminals, i.e., of the form X(w, t, f), where X
is a traditional CFG nonterminal and (w, t, f)
is the word-part-of-speech-word-feature triple
that is the head of the phrase denoted by X.4
The lexicalized nonterminal H is so named be-
cause it is the head constituent, where P inher-
its its head triple from this head constituent.
The constituents labeled L, and R, are left-
and right-modifier constituents, respectively.
</bodyText>
<subsectionHeader confidence="0.943145">
3.2 Probability structure of the
original model
</subsectionHeader>
<bodyText confidence="0.987612647058823">
We use p to denote the unlexicalized nontermi-
nal corresponding to P, and similarly for li, ri
and h. We now present the top-level genera-
tion probabilities, along with examples from
4The inclusion of the word feature in the BBN
model was due to the work described in (Weischedel
et al., 1993), where word features helped reduce part
of speech ambiguity for unknown words.
Figure 1. For brevity, we omit the smooth-
ing details of BBN&apos;s model (see (Miller et al.,
1998) for a complete description); we note that
all smoothing weights are computed via the
technique described in (Bikel et al., 1997).
The probability of generating p as the
root label is predicted conditioning on only
+TOP+, which is the hidden root of all parse
trees:
</bodyText>
<equation confidence="0.997016">
P (pl + TOP+) , e.g., P(S I + TOP+). (2)
</equation>
<bodyText confidence="0.9997955">
The probability of generating a head node h
with a parent p is
</bodyText>
<equation confidence="0.958345">
P(hIp), e.g., P(VP IS). (3)
</equation>
<bodyText confidence="0.990243">
The probability of generating a left-modifier 1,
is
</bodyText>
<equation confidence="0.864148">
PL(lillt-1,11,11,wh)„ e.g., (4)
PL, (NP I ADVP, S, VP, caught)
</equation>
<bodyText confidence="0.997976666666667">
when generating the NP for NP(boy-NN), and
the probability of generating a right modifier
r, is
</bodyText>
<equation confidence="0.997078">
PR(r, I ri_i,p, h,wh), e.g., (5)
PR(NP I + BEGIN+, VP, VBD, caught)
</equation>
<bodyText confidence="0.92939175">
when generating the NP for NP(ball-NN).5
The probabilities for generating lexical el-
ements (past-of-speech tags, words and word
features) are as follows. The part of speech
tag of the head of the entire sentence, th, is
5The hidden nonterminal +BEGIN+ is used to pro-
vide a convenient mechanism for determining the ini-
tial probability of the underlying Markov process gen-
erating the modifying nonterminals; the hidden non-
terminal +END+ is used to provide consistency to the
underlying Markov process, i.e., so that the probabil-
ities of all possible nonterminal sequences sum to 1.
</bodyText>
<page confidence="0.94845">
157
</page>
<bodyText confidence="0.542313">
computed conditioning only on the top-most
symbol p:6
</bodyText>
<equation confidence="0.97259">
P(th I P)- (6)
</equation>
<bodyText confidence="0.9955558">
Part of speech tags of modifier constituents,
and tr„ are predicted conditioning on the
modifier constituent 1, or ri, the tag of the
head constituent, th, and the word of the head
constituent, wh:
</bodyText>
<equation confidence="0.972228">
P(ti,I /i, th, wh ) and P(trtir,, th, wh). (7)
</equation>
<bodyText confidence="0.999860333333333">
The head word of the entire sentence, wh, is
predicted conditioning only on the top-most
symbol p and th-
</bodyText>
<equation confidence="0.95984">
P(Whith,p). (8)
</equation>
<bodyText confidence="0.99997975">
Head words of modifier constituents, wiz and
wr„ are predicted conditioning on all the con-
text used for predicting parts of speech in (7),
as well as the parts of speech themsleves
</bodyText>
<equation confidence="0.83592">
P(wt, tt, th, wh)
</equation>
<bodyText confidence="0.9387332">
and P(wrz tr„ ri, th, wh)- (9)
The word feature of the head of the entire sen-
tence, fh, is predicted conditioning on the top-
most symbol p, its head word, wh, and its head
tag, th:
</bodyText>
<equation confidence="0.929151">
P(fh Wit7th,P)- (10)
</equation>
<bodyText confidence="0.9999522">
Finally, the word features for the head words
of modifier constituents, A and ft.,, are pre-
dicted conditioning on all the context used to
predict modifier head words in (9), as well as
the modifier head words themselves:
</bodyText>
<equation confidence="0.83266">
P(f4 I known(wiz ), tt„ti,th,wh) (11)
</equation>
<bodyText confidence="0.900973111111111">
and P(fr, I known(wrz ), ri, th, wh )
where known(x) is a predicate returning true
if the word x was observed more than 4 times
in the training data.
The probability of an entire parse tree is the
product of the probabilities of generating all of
the elements of that parse tree, where an el-
ement is either a constituent label, a part of
speech tag, a word or a word feature. We ob-
tain maximum-likelihood estimates of the pa-
rameters of this model using frequencies gath-
ered from the training data.
6This is the one place where we have altered the
original model, as the lexical components of the head
of the entire sentence were all being estimated incor-
rectly, causing an inconsistency in the model. We have
corrected the estimation of th, wh and fh in our im-
plementation.
</bodyText>
<sectionHeader confidence="0.66246" genericHeader="method">
4 Word-sense Extensions to the
</sectionHeader>
<subsectionHeader confidence="0.547635">
Lexical Model
</subsectionHeader>
<bodyText confidence="0.999989444444445">
The desired output structure of our com-
bined parser/word-sense disambiguator is a
standard, Treebank-style parse tree, where the
words not only have parts of speech, but also
WordNet synsets. Incorporating synsets into
the lexical part of the model is fairly straight-
forward: a synset is yet another element to be
generated. The question is when to generate it.
The lexical model has decomposed the genera-
tion of the (w, t, f) triple into three steps, each
conditioning on all the history of the previ-
ous step. While it is probabilistically identical
to predict synsets at any of the four possible
points if we continue to condition on all the
history at each step, we would like to pick the
point that is most well-founded both in terms
of the underlying linguistic structure and in
terms of what can be well-estimated. In Sec-
tion 2.2.1 we mentioned the soft-clustering as-
pect of synsets; in fact, they have a duality.
On the one hand, they serve to add specificity
to what might otherwise be an ambiguous lexi-
cal item; on the other, they are sets, clustering
lexical items that have similar meanings. Even
further, noun and verb synsets form a con-
cept taxonomy, the hypernym relation forming
a partial ordering on the lemmas contained
in WordNet. The former aspect corresponds
roughly to what we as human listeners or read-
ers do: we hear or see a sequence of words in
context, and determine incrementally the par-
ticular meaning of each of those words. The
latter aspect corresponds more closely to a
mental model of generation: we have a desire
or intention to convey, we choose the appropri-
ate concepts with which to convey it, and we
realize that desire or intention with the most
felicitous syntactic structure and lexical real-
izations of those concepts. As this is a genera-
tive model, we generate a word&apos;s synset after
generating the part of speech tag but before
generating the word itself.7
The synset of the head of the entire sen-
tence, sh is predicted conditioning only on the
top-most symbol p and the head tag, th:
</bodyText>
<equation confidence="0.662891">
P(sh th,p). (12)
</equation>
<bodyText confidence="0.977956">
We accordingly changed the probability of
</bodyText>
<footnote confidence="0.97229625">
7We believe that synsets and parts of speech are
largely orthogonal with respect to their lexical infor-
mation, and thus their relative order of prediction was
not a concern.
</footnote>
<page confidence="0.991581">
158
</page>
<bodyText confidence="0.93183175">
generating the head word of the entire sen-
tence to be
P(Wh I Sh, th,P). (13)
The probability estimates for (12) and (13) are
not smoothed.
The probability model for generating
synsets of modifier constituents mi, complete
with smoothing components, is as follows:
</bodyText>
<table confidence="0.825669857142857">
P(sm, I tini, m„ wh, sh ) =
A015( smi trrh, 7nz wh, sh)
• P(sm, itm,, mi, sh
+ A2P(srrzz I tmt, niz,©1(sh))
+ An-Fi.P(sm I trn,,Mi, On (sh))
▪ An+2P(sm, I tm2 mi)
▪ An+3P(Smi I tm2 )
</table>
<bodyText confidence="0.999965833333333">
where ©2(sh ) is the ith hypernym of sh. The
WordNet hypernym relations, however, do not
form a tree, but a DAG, so whenever there are
multiple hypernyms, the uniformly-weighted
mean is taken of the probabilities condition-
ing on each of the hypernyms. That is,
</bodyText>
<equation confidence="0.9723686">
P(sm, I trh,, mi, ©3 (sh )) = (15)
1 n
— E P(s,„ tm.,, mi, (sh))
n k=1
when ©3 (sh ) = {41 (sh ), . . . ,
</equation>
<bodyText confidence="0.999656363636364">
Note that in the first level of back-off, we no
longer condition on the head word, but strictly
on its synset, and thereafter on hypernyms of
that synset; these models, then, get at the
heart of our approach, which is to abstract
away from lexical head relations, and move
to the more general lexi co-semantic relations,
here represented by synset relations.
Now that we generate synsets for words us-
ing (14), we can also change the word genera-
tion model to have synsets in its history:
</bodyText>
<figure confidence="0.279367">
/5(turn„ I sm,, tmt, ntz, Wh, Sh) = (16)
AoP(wm, I s 77-k tm, mi wh )
± Al P ( Wm I Smi trni Mi7 Sh)
• A2P(W777, I Sin, tin° Mil ©1 (Sh ))
</figure>
<table confidence="0.9921224">
▪ Art.+1/5(Wm, I Smi trrz, niz, On (sh))
n+j)(.w rrz,. Srn,,trnz mi)
+
+ An+3P(wm, Sm,, trn,.)
+ Art+4-t(w,ni I sm, )
</table>
<bodyText confidence="0.999930181818182">
where once again, Oi(sh) is the zth hypernym
of sh. For both the word and synset prediction
models, by backing off up the hypernym chain,
there is an appropriate conflation of similar
head relations. For example, if in training the
verb phrase [strike the target] had been seen, if
the unseen verb phrase [attack the target] ap-
peared during testing, then the training from
the semantically-similar training phrase could
be used, since this sense of attack is the hy-
pernym of this sense of strike.
Finally, we note that both of these synset-
and word-prediction probability estimates
contain an enormous number of back-off lev-
els for nouns and verbs, corresponding to the
head word&apos;s depth in the synset hierarchy. A
valid concern would be that the model might
be backing off using histories that are far too
general, so we experimented with limiting the
hypernym back-off to only two, three and four
levels. This change produced a negligible dif-
ference in parsing performance.8
</bodyText>
<sectionHeader confidence="0.9466795" genericHeader="method">
5 A New Approach, A New Data
Set
</sectionHeader>
<bodyText confidence="0.999987181818182">
Ideally, the well-established gold standard for
syntax, the Penn Treebank, would have a
parallel word-sense—annotated corpus; unfor-
tunately, no such word-sense corpus exists.
However, we do have SemCor (Miller et al.,
1994), where every noun, verb, adjective and
adverb from a 455k word portion of the Brown
Corpus has been assigned a WordNet synset.
While all of the Brown Corpus was anno-
tated in the style of Treebank I, a great deal
was also more recently annotated in Tree-
bank II format, and this corpus has recently
been released by the Linguistic Data Con-
sortium.9 As it happens, the intersection be-
tween the Treebank-II-annotated Brown and
SemCor comprises some 220k words, most of
which is fiction, with some nonfiction and hu-
mor writing as well.
We went through all 220k words of the cor-
pora, synchronizing them. That is, we made
sure that the corpora were identical up to
the spelling of individual tokens, correcting all
</bodyText>
<footnote confidence="0.9509934">
9We aim to investigate the precise effects of our
back-off strategy in the next version of our combined
parsing/WSD model.
9We were given permission to use a pre-release ver-
sion of this Treebank II—style corpus.
</footnote>
<equation confidence="0.467387">
(14)
</equation>
<page confidence="0.992768">
159
</page>
<bodyText confidence="0.999957941176471">
tokenization and sentence-breaking discrepan-
cies. This correcton task ranged from the sim-
ple, such as connecting two sentences in one
corpus that were erroneously broken, to the
middling, such as joining two tokens in Sem-
Cor that comprised a hyphenate in Brown, to
the difficult, such as correcting egregious parse
annotation errors, or annotating entire sen-
tences that were omitted from SemCor. In par-
ticular, the case of hyphenates was quite fre-
quent, as it was the default in SemCor to split
up all such words and assign them their indi-
vidual word senses (synsets). In general, we at-
tempted to make SemCor look as much as pos-
sible like the Treebank II-annotated Brown,
and we used the following guidelines for as-
signing word senses to hyphenates:
</bodyText>
<listItem confidence="0.985341615384615">
1. Assign the word sense of the head of
the hyphenate. E.g., both twelve-foot and
ten-foot get the word sense of foot_l (the
unit of measure equal to 12 inches).
2. If there is no clear head, then attempt
to annotate with the word sense of the
hypernym of the senses of the hyphenate
components. E.g., U.S.-Soviet gets the
word sense of country_2 (a state or na-
tion).
3. If options 1 and 2 are not possible, the
hyphenate is split in the Treebank II file.
4. If the hyphenate has the prefix non- or
</listItem>
<bodyText confidence="0.949662322580645">
anti-, annotate with the word sense of
that which follows, with the understand-
ing that a post-processing step could re-
cover the antonymous word sense, if nec-
essary.
After three passes through the corpora, they
were perfectly synchronized. We are seeking
permission to make this data set available to
any who already have access to both SemCor
and the Treebank II version of Brown.
After this synchronization process, we
merged the word-sense annotations of our cor-
rected SemCor with the tokens of our cor-
rected version of the Treebank II Brown data.
Here we were forced to make two decisions.
First, SemCor allows multiple synsets to be as-
signed to a particular word; in these cases, we
simply discard all but the first assigned synset.
Second, WordNet has collocations, whereas
Treebank does not. To deal with this dis-
parity, we re-analyze annotated collocations
as a sequence of separate words that have
all been assigned the same synset as was as-
signed the collocation as a whole. This is not
as unreasonable as it may sound; for exam-
ple, vice_president is a lemma in WordNet
and appears in SemCor, so the merged corpus
has instances where the word president has
the synset vice_president_1, but only when
preceded by the word vice. The cost of this
decision is an increase in average polysemy.
</bodyText>
<sectionHeader confidence="0.907964" genericHeader="method">
6 Training and Decoding
</sectionHeader>
<bodyText confidence="0.999949">
Using this merged corpus, actual training of
our model proceeds in an identical fashion
to training the non-WordNet--extended model,
except that for each lexical relation, the hy-
pernyrn chain of the parent head is followed
to derive counts for the various back-off levels
described in Section 4. We also developed a
&amp;quot;plug-&apos;n&apos;-play&amp;quot; lexical model system to facili-
tate experimentation with various word- and
synset-prediction models and back-off strate-
gies.
Even though the model is a top-down, gen-
erative one, parsing proceeds bottom-up. The
model is searched via a modified version of
CKY, where candidate parse trees that cover
the same span of words are ranked against
each other. In the unextended parsing model,
the cells corresponding to spans of length one
are seeded with (w, t, f) triples, with every
possible tag t for a given word w (the word-
feature f is computed deterministically for w);
this step introduces the first degree of ambi-
guity in the decoding process. Our WordNet-
extended model adds to this initial ambiguity,
for each cell is seeded with (w, t, f, s) quadru-
ples, with every possible synset s for a given
word-tag pair.
During decoding, two forms of pruning are
employed: a beam is applied to each cell in the
chart, pruning away all parses whose ranking
score is not within a factor of e-ic of the top-
ranked parse, and only the top-ranked n sub-
trees are maintained, and the rest are pruned
away. The &amp;quot;out-of-the-box&amp;quot; BBN program uses
values of -5 and 25 for k and n, respectively.
We changed these to default to -9 and 50, be-
cause generating additional unseen items (in
our case, synsets) will necessarily lower inter-
mediate ranking scores.
</bodyText>
<sectionHeader confidence="0.978408" genericHeader="evaluation">
7 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.987526">
7.1 Parsing
</subsectionHeader>
<bodyText confidence="0.9999015">
Initially, we created a small test set, blindly
choosing the last 117 sentences, or 1%, of
</bodyText>
<page confidence="0.991875">
160
</page>
<bodyText confidence="0.999314183673469">
our 220k word corpus, sentences which were,
as it happens, from section &amp;quot;r&amp;quot; of the Brown
Corpus. After some disappointing parsing
results using both the regular parser and
our WordNet-extended version, we peeked in
(Francis and Kueera, 1979) and discovered
this was the humor writing section; our ini-
tial test corpus was literally a joke. To cre-
ate a more representative test set, we sam-
pled every 100th sentence to create a new 117-
sentence test set that spanned the entire range
of styles in the 220k words; we put all other
sentences in the training set.10 For the sake of
comparison, we present results for both test
sets (from section &amp;quot;r&amp;quot; and the balanced test
set) and both the standard model (Norm) and
our WN-extended model (WN-ext) in Table
1.11 We note that after we switched to the bal-
anced test set, we did not use the &amp;quot;out-of-the-
box&amp;quot; version of the BBN parser, as its default
settings for pruning away low-count items and
the threshold at which to count a word as &amp;quot;un-
known&amp;quot; were too high to yield decent results.
Instead, we used precisely the same settings as
for our WordNet-extended version, complete
with the larger beam width discussed in the
previous section.12
The reader will note that our extended
model performs at roughly the same level
as the unextended version with respect to
parsing-a shave better with the &amp;quot;r&amp;quot; test set,
and slightly worse on the balanced test set.
Recall, however, that this is in spite of adding
more intermediate ambiguity during the de-
coding process, and yet using the same beam
width. Furthermore, our extensions have oc-
curred strictly within the framework of the
original model, but we believe that for the
true advantages of synsets to become appar-
ent, we must use trilexical or even tetralex-
1°We realize these are very small test sets, but we
presume they are large enough to at least gitre a good
indicator of performance on the tasks evaluated. They
were kept small to allow for a rapid train-test-analyze
cycle, i.e., they were actually used as development test
sets. With the completion of these initial experiments,
we are going to designate a proper three-way divsion
of training, devtest and test set of this new merged
corpus.
</bodyText>
<footnote confidence="0.978024875">
&amp;quot;The scores in the rows labeled Norm, &amp;quot;r&amp;quot;, indicat-
ing the performance of the standard BBN model on
the &amp;quot;r&amp;quot; test set, are actually scores based on 116 of the
117 sentences, as one sentence did not get parsed due
to a timeout in the program.
12This is partly an unfair comparison, then, since
ours is a larger model, but we wanted to give the stan-
dard model every conceivable advantage.
</footnote>
<table confidence="0.999532538461538">
Model, &lt;40 words
test set
LR LP CB OCB &lt;2CB
Norm, &amp;quot;r&amp;quot;* 69.7 72.6 2.93 31.9 55.0
WN-ext, &amp;quot;r&amp;quot; 69.7 72.7 2.86 30.8 56.0
Norm, bal 83.1 85.0 0.82 75.9 85.7
WN-ext, bal 82.9 84.0 1.02, 70.5 _ 81.3
All sentences
LR LP CB OCB &lt;2CB
Norm, &amp;quot;r&amp;quot;* 68.6 71.2 3.83 25.9 44.8
WN-ext, &amp;quot;r&amp;quot; 69.7 71.5 3.77 25.0 45.7
Norm, bal 82.0 84.4 1.00 73.5 83.8
WN-ext, bal 80.5 82.2 1.43 _ 68.4 78.6
</table>
<tableCaption confidence="0.989807333333333">
Table 1: Results for both parsing models on
both test sets. All results are percentages, ex-
cept for those in the CB column. *See footnote
</tableCaption>
<figure confidence="0.9415105">
11.
S(will)
NP (Jane) VP(will)
Jane will VP(kill)
kill NP(Bob)
Bob
</figure>
<figureCaption confidence="0.9804235">
Figure 2: Head rules are tuned for syntax, not
semantics.
</figureCaption>
<bodyText confidence="0.999938208333333">
ical dependencies. Whereas such long-range
dependencies might cripple a standard gen-
erative model, the soft-clustering aspects of
synsets should offset the sparse data problem.
As an example of the lack of such dependen-
cies, in the current model when predicting the
attachment of [bought company [for million]],
there is no current dependence between the
verb bought and the object of the preposition
million-a dependence shown to be useful in
virtually all the PP attachment work, and par-
ticularly in (Stetina and Nagao, 1997). Re-
lated to this issue, we note that the head rules,
which were nearly identical to those used in
(Collins, 1997), have not been tuned at all to
this task. For example, in the sentence in Fig-
ure 2, the subject Jane is predicted condition-
ing on the head of the VP, which is the modal
will, as opposed to the more semantically-
content-rich kill. So, while the head relations
provide a very useful structure for many syn-
tactic decisions the parser needs to make, it is
quite possible that the synset relations of this
model would require additional or different de.
</bodyText>
<page confidence="0.994819">
161
</page>
<table confidence="0.991096">
Recall Precision
Noun 86.5% 70.9%
Verb 84.0% 59.5%
Adj 80.2% 70.4%
Adv 78.5% 75.8%
Total 84.0% 67.3%
</table>
<tableCaption confidence="0.9339965">
Table 2: Word sense disambiguation results for
balanced test set.
</tableCaption>
<bodyText confidence="0.995360125">
pendencies that would help in the prediction
of correct synsets, and in turn help further re-
duce certain syntactic ambiguities, such as PP
attachment. This is because the &amp;quot;lightweight
semantics&amp;quot; offered by synset relations can pro-
vide selectional and world-knowledge restric-
tions that simple lexicalized nonterminal rela-
tions cannot.
</bodyText>
<subsectionHeader confidence="0.828318">
7.2 Word-sense disambiguation
</subsectionHeader>
<bodyText confidence="0.990350653061225">
The WSD results on the balanced test set are
shown in Table 2. A few important points must
be made when evaluating these results. First,
almost all other WSD approaches are aimed at
distinguishing homonyms, as opposed to the
type of fine-grained distinctions that can be
made by WordNet. Second, almost all other
WSD approaches attempt to disambiguate a
small set of such homonymous terms, whereas
here we are attacking the generalized word-
sense disambiguation problem. Third, we call
attention to the fact that SemCor has a re-
ported inter-annotator agreement of 78.6%
overall, and as low as 70% for words with pol-
ysemy of 8 or above (Fellbaum et al., 1998), so
it is with this upper bound in mind that one
must consider the precision of any generalized
WSD system. Finally, we note that the scores
in Table 2 are for exact synset matches; that
is, if our program delivers a synset that is, say,
the hypernym or sibling of the correct answer,
no credit is given.
While it is tempting to compare these re-
sults to those of (Stetina et al., 1998), who re-
ported 79.4% overall accuracy on a different,
larger test set using their non-discourse model,
we note that that was more of an upper-
bound study, examining how well a WSD al-
gorithm could perform if it had access to gold-
standard—perfect parse trees.13 By way of fur-
ther comparison, that algorithm has a feature
space similar to the synset-prediction compo-
131t is not clear how or why the results of (Stetina et
al., 1998) exceeded the reported inter-annotator agree-
ment of the entire corpus.
nents of our model, but the steps used to rank
possible answers are based largely on heuris-
tics; in contrast, our model is based entirely
on maximum-likelihood probability estimates.
A final note on the scores of Table 2: given
the fact that there is not a deterministic
mapping between the 50-odd Treebank and
4 WordNet parts of speech, when our pro-
gram delivers a synset for a WordNet part of
speech that is different from our gold file, we
have called this a recall error, as this is con-
sistent with all other WSD work, where part
of speech ambiguity is not a component of an
algorithm&apos;s precision.
</bodyText>
<sectionHeader confidence="0.998382" genericHeader="discussions">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999974740740741">
This paper represents a first attempt at a
combined parsing/word sense disambiguation
model. Although it has been very useful to
work with the BBN model, we are currently
implementing and hope to augment a more
state-of-the-art model, viz., Models 2 and 3 of
(Collins, 1997). We would also like to explore
the use of a more radical model, where nonter-
mina&apos;s only have synsets as their heads, and
words are generated strictly at the leaves. We
would also like to incorporate long-distance
context in the model as an aid to WSD, a
demonstrably effective feature in virtually all
the recent, statistical WSD work. Also, as
mentioned earlier, we believe there are several
features that would allow significant parsing
improvement. Finally, we would like to inves-
tigate the incorporation of unsupervised meth-
ods for WSD, such as the heuristically-based
methods of (Stetina and Nagao, 1997) and
(Stetina et al., 1998), and the theoretically
purer bootstrapping method of (Yarowsky,
1995). Bolstered by the success of (Stetina
and Nagao, 1997), (Lin, 1997) and especially
(Stetina et al., 1998), we believe there is great
promise the incorporation of word-sense into
a probabilistic parsing model.
</bodyText>
<sectionHeader confidence="0.995081" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99994">
I would like to greatly acknowledge the re-
searchers at BBN who allowed me to use and
abuse their parser and who fostered the begin-
ning of this research effort: Scott Miller, Lance
Ramshaw, Heidi Fox, Sean Boisen and Ralph
Weischedel. Thanks to Michelle Engel, who
helped enormously with the task of prepar-
ing the new data set. Finally, I would like to
thank my advisor Mitch Marcus for his invalu-
able technical advice and support.
</bodyText>
<page confidence="0.996235">
162
</page>
<sectionHeader confidence="0.991902" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999741584745763">
Daniel M. Bikel, Richard Schwartz, Ralph
Weischedel, and Scott Miller. 1997. Nymble:
A high-performance learning name-finder. In
Fifth Conference on Applied Natural Language
Processing, pages 194-201„ Washington, D.C.
E. Brill and P. Resnik. 1994. A rule-
based approach to prepositional phrase attach-
ment disambiguation. In Fifteenth Interna-
tional Conference on Computational Linguistics
(COLING-1994).
Eric Brill. 1995. Transformation-based error-
driven learning and natural language process-
ing: A case study in part-of-speech tagging.
Computational Linguistics, 21(4) :543-565.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proceedings of the Fourteenth National Con-
ference on Artificial Intelligence, Menlo Park.
AAAI Press/MIT Press.
Kenneth Church. 1988. A stochastic parts pro-
gram and noun phrase parser for unrestricted
text. In Second Conference on Applied Natu-
ral Language Processing, pages 136-143, Austin,
Texas.
M. Collins and J. Brooks. 1995. Prepositional
phrase attachment through a backed-off model.
In Third Workshop on Very Large Corpora,
pages 27-38.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of ACL-EACL &apos;97, pages 16-23.
Michael John Collins. 1999. Head-Driven Statisti-
cal Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Christiane Fellbaum, Jaochim Grabowski, and
Shari Landes. 1998. Performance and confi-
dence in a semantic annotation task. In Chris-
tiane Fellbaum, editor, WordNet: An Electronic
Lexical Database, chapter 9. MIT Press, Cam-
bridge, Massachusetts.
W. N. Francis and H. K&amp;era. 1979. Manual of
Information to accompany A Standard Corpus
of Present-Day Edited American English, for
use with Digital Computers. Department of Lin-
guistics, Brown University, Providence, Rhode
Island.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In A. Salomma and
G. Rosenberg, editors, Handbook of Formal Lan-
guages and Automata, volume 3, pages 69-124.
Springer-Verlag, Heidelberg.
Dekang Lin. 1997. Using syntactic dependency as
local context to resolve word sense ambiguity.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
Madrid, Spain.
D. Magerman. 1995. Statistical decision tree
models for parsing. In 33rd Annual Meeting
of the Association for Computational Linguis-
tics, pages 276-283, Cambridge, Massachusetts.
Morgan Kaufmann Publishers.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313-
330.
George A. Miller, Richard T. Beckwith, Chris-
tiane D. Fellbaum, Derek Gross, and Kather-
ine J. Miller. 1990. WordNet: An on-line lexi-
cal database. International Journal of Lexicog-
raphy, 3(4):235-244.
George A. Miller, Martin Chodorow, Shari Lan-
des, Claudia Leacock, and Robert G. Thomas.
1994. Using a semantic concordance for sense
identification. In Proceedings of the ARPA Hu-
man Language Technology Workshop.
Scott Miller, Heidi Fox, Lance Ramshaw, and
Ralph Weischedel. 1998. SIFT - Statistically-
derived Information From Text. In Seventh
Message Understanding Conference (MUG- 7),
Washington, D.C.
Adwait Ratnaparkhi. 1997. A linear observed
time statistical parser based on maximum en-
tropy models. In Proceedings of the Second
Conference on Empirical Methods in Natural
Language Processing, Brown University, Prov-
idence, Rhode Island.
Jiri Stetina and Makoto Nagao. 1997. Corpus
based PP attachment ambiguity resolution with
a semantic dictionary. In Fifth Workshop on
Very Large Corpora, pages 66-80, Beijing.
Jiri Stetina, Sadao Kurohashi, and Makoto Na-
ga°. 1998. General word sense disambiguation
method based on a full sentential context. In
COLING-ACL &apos;98 Workshop: Usage of Word-
Net in Natural Language Processing Systems,
Montreal, Canada, August.
R. Weischedel, M. Meteer, R. Schwartz,
L. Ramshaw, and J. Palmucci. 1993. Coping
with ambiguity and unknown words through
probabilistic methods. Computational Linguis-
tics, 19(2):359-382.
David Yarowsky. 1992. Word-sense disambigua-
tion using statistical models of roget&apos;s categories
trained on large corpora. In Fourteenth Interna-
tional Conference on Computational Linguistics
(COLING), pages 454-460.
David Yarowsky. 1994. Decision lists for lexi-
cal ambiguity resolution: Application to accent
restoration in Spanish and French. In Proceed-
ings of the 32nd Annual Meeting of the Assoca-
tion for Computational Linguistics, pages 88-
95.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics,
pages 189-196.
</reference>
<page confidence="0.999101">
163
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.891023">
<title confidence="0.999987">A Statistical Model for Parsing and Word-Sense Disambiguation</title>
<author confidence="0.999953">M Daniel</author>
<affiliation confidence="0.999052">of Computer University of</affiliation>
<address confidence="0.99861">200 South 33rd Street, Philadelphia, PA 19104-6389,</address>
<email confidence="0.951607">dbikel@cis.upenn.edu</email>
<abstract confidence="0.994653083333333">This paper describes a first attempt at a statistical model for simultaneous syntactic parsing and generalized word-sense disambiguation. On a new data set we have constructed for the task, while we were disappointed not to find parsing improvement over a traditional parsing model, our model achieves a recall of and a precision of exact synset matches on our test corpus, where the gold standard has a reported inter-annotator agreement of 78.6%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
<author>Scott Miller</author>
</authors>
<title>Nymble: A high-performance learning name-finder.</title>
<date>1997</date>
<booktitle>In Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>194--201</pages>
<location>Washington, D.C.</location>
<contexts>
<context position="9942" citStr="Bikel et al., 1997" startWordPosition="1628" endWordPosition="1631">re of the original model We use p to denote the unlexicalized nonterminal corresponding to P, and similarly for li, ri and h. We now present the top-level generation probabilities, along with examples from 4The inclusion of the word feature in the BBN model was due to the work described in (Weischedel et al., 1993), where word features helped reduce part of speech ambiguity for unknown words. Figure 1. For brevity, we omit the smoothing details of BBN&apos;s model (see (Miller et al., 1998) for a complete description); we note that all smoothing weights are computed via the technique described in (Bikel et al., 1997). The probability of generating p as the root label is predicted conditioning on only +TOP+, which is the hidden root of all parse trees: P (pl + TOP+) , e.g., P(S I + TOP+). (2) The probability of generating a head node h with a parent p is P(hIp), e.g., P(VP IS). (3) The probability of generating a left-modifier 1, is PL(lillt-1,11,11,wh)„ e.g., (4) PL, (NP I ADVP, S, VP, caught) when generating the NP for NP(boy-NN), and the probability of generating a right modifier r, is PR(r, I ri_i,p, h,wh), e.g., (5) PR(NP I + BEGIN+, VP, VBD, caught) when generating the NP for NP(ball-NN).5 The probab</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, Miller, 1997</marker>
<rawString>Daniel M. Bikel, Richard Schwartz, Ralph Weischedel, and Scott Miller. 1997. Nymble: A high-performance learning name-finder. In Fifth Conference on Applied Natural Language Processing, pages 194-201„ Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>P Resnik</author>
</authors>
<title>A rulebased approach to prepositional phrase attachment disambiguation.</title>
<date>1994</date>
<booktitle>In Fifteenth International Conference on Computational Linguistics (COLING-1994).</booktitle>
<contexts>
<context position="3049" citStr="Brill and Resnik, 1994" startWordPosition="497" endWordPosition="500">ther the sense is &amp;quot;await&amp;quot; or &amp;quot;require&amp;quot;. Thus, Examples 3-5 illustrate how the syntactic context of a word can help determine its meaning. 2.2 Motivation from previous work 2.2.1 Parsing In recent years, the success of statistical parsing techniques can be attributed to several factors, such as the increasing size of computing machinery to accommodate larger models, the availability of resources such as the Penn Treebank (Marcus et al., 1993) and the success of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging (Church, 1988; Brill, 1995), and PPattachment (Brill and Resnik, 1994; Collins and Brooks, 1995). However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies. This includes both the parsers that attach probabilities to parser moves (Magerman, 1995; Ratnaparkhi, 1997), but also those of the lexicalized PCFG variety (Collins, 1997; Charniak, 1997). 155 Even more crucially, the bilexical dependencies involve head-modifier relations (hereafter referred to simply as &amp;quot;head relations&amp;quot;). The intu</context>
</contexts>
<marker>Brill, Resnik, 1994</marker>
<rawString>E. Brill and P. Resnik. 1994. A rulebased approach to prepositional phrase attachment disambiguation. In Fifteenth International Conference on Computational Linguistics (COLING-1994).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based errordriven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>543--565</pages>
<contexts>
<context position="3007" citStr="Brill, 1995" startWordPosition="492" endWordPosition="493">R provides information as to whether the sense is &amp;quot;await&amp;quot; or &amp;quot;require&amp;quot;. Thus, Examples 3-5 illustrate how the syntactic context of a word can help determine its meaning. 2.2 Motivation from previous work 2.2.1 Parsing In recent years, the success of statistical parsing techniques can be attributed to several factors, such as the increasing size of computing machinery to accommodate larger models, the availability of resources such as the Penn Treebank (Marcus et al., 1993) and the success of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging (Church, 1988; Brill, 1995), and PPattachment (Brill and Resnik, 1994; Collins and Brooks, 1995). However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies. This includes both the parsers that attach probabilities to parser moves (Magerman, 1995; Ratnaparkhi, 1997), but also those of the lexicalized PCFG variety (Collins, 1997; Charniak, 1997). 155 Even more crucially, the bilexical dependencies involve head-modifier relations (hereafter referre</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based errordriven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4) :543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence, Menlo Park.</booktitle>
<publisher>AAAI Press/MIT Press.</publisher>
<contexts>
<context position="3503" citStr="Charniak, 1997" startWordPosition="567" endWordPosition="568"> of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging (Church, 1988; Brill, 1995), and PPattachment (Brill and Resnik, 1994; Collins and Brooks, 1995). However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies. This includes both the parsers that attach probabilities to parser moves (Magerman, 1995; Ratnaparkhi, 1997), but also those of the lexicalized PCFG variety (Collins, 1997; Charniak, 1997). 155 Even more crucially, the bilexical dependencies involve head-modifier relations (hereafter referred to simply as &amp;quot;head relations&amp;quot;). The intuition behind the lexicalization of a grammar formalism is to capture lexical items&apos; idiosyncratic parsing preferences. The intuition behind using heads as the members of the bilexical relations is twofold. First, many linguistic theories tell us that the head of a phrase projects the skeleton of that phrase, to be filled in by specifiers, complements and adjuncts; such a notion is captured quite directly by a formalism such as LTAG (Joshi and Schabes</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, Menlo Park. AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<location>Austin, Texas.</location>
<contexts>
<context position="2993" citStr="Church, 1988" startWordPosition="490" endWordPosition="491">n NP or an SBAR provides information as to whether the sense is &amp;quot;await&amp;quot; or &amp;quot;require&amp;quot;. Thus, Examples 3-5 illustrate how the syntactic context of a word can help determine its meaning. 2.2 Motivation from previous work 2.2.1 Parsing In recent years, the success of statistical parsing techniques can be attributed to several factors, such as the increasing size of computing machinery to accommodate larger models, the availability of resources such as the Penn Treebank (Marcus et al., 1993) and the success of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging (Church, 1988; Brill, 1995), and PPattachment (Brill and Resnik, 1994; Collins and Brooks, 1995). However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies. This includes both the parsers that attach probabilities to parser moves (Magerman, 1995; Ratnaparkhi, 1997), but also those of the lexicalized PCFG variety (Collins, 1997; Charniak, 1997). 155 Even more crucially, the bilexical dependencies involve head-modifier relations (her</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Kenneth Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Second Conference on Applied Natural Language Processing, pages 136-143, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Brooks</author>
</authors>
<title>Prepositional phrase attachment through a backed-off model.</title>
<date>1995</date>
<booktitle>In Third Workshop on Very Large Corpora,</booktitle>
<pages>27--38</pages>
<contexts>
<context position="3076" citStr="Collins and Brooks, 1995" startWordPosition="501" endWordPosition="504">&amp;quot; or &amp;quot;require&amp;quot;. Thus, Examples 3-5 illustrate how the syntactic context of a word can help determine its meaning. 2.2 Motivation from previous work 2.2.1 Parsing In recent years, the success of statistical parsing techniques can be attributed to several factors, such as the increasing size of computing machinery to accommodate larger models, the availability of resources such as the Penn Treebank (Marcus et al., 1993) and the success of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging (Church, 1988; Brill, 1995), and PPattachment (Brill and Resnik, 1994; Collins and Brooks, 1995). However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies. This includes both the parsers that attach probabilities to parser moves (Magerman, 1995; Ratnaparkhi, 1997), but also those of the lexicalized PCFG variety (Collins, 1997; Charniak, 1997). 155 Even more crucially, the bilexical dependencies involve head-modifier relations (hereafter referred to simply as &amp;quot;head relations&amp;quot;). The intuition behind the lexicaliza</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>M. Collins and J. Brooks. 1995. Prepositional phrase attachment through a backed-off model. In Third Workshop on Very Large Corpora, pages 27-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL-EACL &apos;97,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="3486" citStr="Collins, 1997" startWordPosition="565" endWordPosition="566">and the success of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging (Church, 1988; Brill, 1995), and PPattachment (Brill and Resnik, 1994; Collins and Brooks, 1995). However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies. This includes both the parsers that attach probabilities to parser moves (Magerman, 1995; Ratnaparkhi, 1997), but also those of the lexicalized PCFG variety (Collins, 1997; Charniak, 1997). 155 Even more crucially, the bilexical dependencies involve head-modifier relations (hereafter referred to simply as &amp;quot;head relations&amp;quot;). The intuition behind the lexicalization of a grammar formalism is to capture lexical items&apos; idiosyncratic parsing preferences. The intuition behind using heads as the members of the bilexical relations is twofold. First, many linguistic theories tell us that the head of a phrase projects the skeleton of that phrase, to be filled in by specifiers, complements and adjuncts; such a notion is captured quite directly by a formalism such as LTAG (</context>
<context position="27190" citStr="Collins, 1997" startWordPosition="4655" endWordPosition="4656">such long-range dependencies might cripple a standard generative model, the soft-clustering aspects of synsets should offset the sparse data problem. As an example of the lack of such dependencies, in the current model when predicting the attachment of [bought company [for million]], there is no current dependence between the verb bought and the object of the preposition million-a dependence shown to be useful in virtually all the PP attachment work, and particularly in (Stetina and Nagao, 1997). Related to this issue, we note that the head rules, which were nearly identical to those used in (Collins, 1997), have not been tuned at all to this task. For example, in the sentence in Figure 2, the subject Jane is predicted conditioning on the head of the VP, which is the modal will, as opposed to the more semanticallycontent-rich kill. So, while the head relations provide a very useful structure for many syntactic decisions the parser needs to make, it is quite possible that the synset relations of this model would require additional or different de. 161 Recall Precision Noun 86.5% 70.9% Verb 84.0% 59.5% Adj 80.2% 70.4% Adv 78.5% 75.8% Total 84.0% 67.3% Table 2: Word sense disambiguation results for</context>
<context position="30606" citStr="Collins, 1997" startWordPosition="5243" endWordPosition="5244">ng between the 50-odd Treebank and 4 WordNet parts of speech, when our program delivers a synset for a WordNet part of speech that is different from our gold file, we have called this a recall error, as this is consistent with all other WSD work, where part of speech ambiguity is not a component of an algorithm&apos;s precision. 8 Future Work This paper represents a first attempt at a combined parsing/word sense disambiguation model. Although it has been very useful to work with the BBN model, we are currently implementing and hope to augment a more state-of-the-art model, viz., Models 2 and 3 of (Collins, 1997). We would also like to explore the use of a more radical model, where nontermina&apos;s only have synsets as their heads, and words are generated strictly at the leaves. We would also like to incorporate long-distance context in the model as an aid to WSD, a demonstrably effective feature in virtually all the recent, statistical WSD work. Also, as mentioned earlier, we believe there are several features that would allow significant parsing improvement. Finally, we would like to investigate the incorporation of unsupervised methods for WSD, such as the heuristically-based methods of (Stetina and Na</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of ACL-EACL &apos;97, pages 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4655" citStr="Collins, 1999" startWordPosition="754" endWordPosition="755"> quite directly by a formalism such as LTAG (Joshi and Schabes, 1997). Second, the head of a phrase usually conveys some large component of the semantics of that phrase.&apos; In this way, using head-relation statistics encodes a bit of the predicate-argument structure in the syntactic model. While there are cases such as John was believed to have been shot by Bill where structural preference virtually eliminates one of the two semantically plausible analyses, it is quite clear that semantics—and, in particular, lexical head semantics—play a very important role in reducing parsing ambiguity. (See (Collins, 1999), pp. 207ff., for an excellent discussion of structural vs. semantic parsing preferences, including the above John was believed... example.) Another motivation for incorporating word senses into a statistical parsing model has been to ameliorate the sparse data problem. Inspired by the PP-attachment work of (Stetina and Nagao, 1997), we use WordNet v1.6 (Miller et al., 1990) as our semantic dictionary, where the hypernym structure provides the basis for semantically-motivated soft clusters.2 We discuss this benefit of word senses and the details of our implementation further in Section 4. 2.2.</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael John Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
<author>Jaochim Grabowski</author>
<author>Shari Landes</author>
</authors>
<title>Performance and confidence in a semantic annotation task.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database, chapter 9.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="28831" citStr="Fellbaum et al., 1998" startWordPosition="4926" endWordPosition="4929">t set are shown in Table 2. A few important points must be made when evaluating these results. First, almost all other WSD approaches are aimed at distinguishing homonyms, as opposed to the type of fine-grained distinctions that can be made by WordNet. Second, almost all other WSD approaches attempt to disambiguate a small set of such homonymous terms, whereas here we are attacking the generalized wordsense disambiguation problem. Third, we call attention to the fact that SemCor has a reported inter-annotator agreement of 78.6% overall, and as low as 70% for words with polysemy of 8 or above (Fellbaum et al., 1998), so it is with this upper bound in mind that one must consider the precision of any generalized WSD system. Finally, we note that the scores in Table 2 are for exact synset matches; that is, if our program delivers a synset that is, say, the hypernym or sibling of the correct answer, no credit is given. While it is tempting to compare these results to those of (Stetina et al., 1998), who reported 79.4% overall accuracy on a different, larger test set using their non-discourse model, we note that that was more of an upperbound study, examining how well a WSD algorithm could perform if it had a</context>
</contexts>
<marker>Fellbaum, Grabowski, Landes, 1998</marker>
<rawString>Christiane Fellbaum, Jaochim Grabowski, and Shari Landes. 1998. Performance and confidence in a semantic annotation task. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database, chapter 9. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H K&amp;era</author>
</authors>
<title>Manual of Information to accompany A Standard Corpus of Present-Day Edited American English, for use with Digital Computers.</title>
<date>1979</date>
<institution>Department of Linguistics, Brown University,</institution>
<location>Providence, Rhode Island.</location>
<marker>Francis, K&amp;era, 1979</marker>
<rawString>W. N. Francis and H. K&amp;era. 1979. Manual of Information to accompany A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. Department of Linguistics, Brown University, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Treeadjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages and Automata,</booktitle>
<volume>3</volume>
<pages>69--124</pages>
<editor>In A. Salomma and G. Rosenberg, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Heidelberg.</location>
<contexts>
<context position="4110" citStr="Joshi and Schabes, 1997" startWordPosition="663" endWordPosition="667">; Charniak, 1997). 155 Even more crucially, the bilexical dependencies involve head-modifier relations (hereafter referred to simply as &amp;quot;head relations&amp;quot;). The intuition behind the lexicalization of a grammar formalism is to capture lexical items&apos; idiosyncratic parsing preferences. The intuition behind using heads as the members of the bilexical relations is twofold. First, many linguistic theories tell us that the head of a phrase projects the skeleton of that phrase, to be filled in by specifiers, complements and adjuncts; such a notion is captured quite directly by a formalism such as LTAG (Joshi and Schabes, 1997). Second, the head of a phrase usually conveys some large component of the semantics of that phrase.&apos; In this way, using head-relation statistics encodes a bit of the predicate-argument structure in the syntactic model. While there are cases such as John was believed to have been shot by Bill where structural preference virtually eliminates one of the two semantically plausible analyses, it is quite clear that semantics—and, in particular, lexical head semantics—play a very important role in reducing parsing ambiguity. (See (Collins, 1999), pp. 207ff., for an excellent discussion of structural</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1997. Treeadjoining grammars. In A. Salomma and G. Rosenberg, editors, Handbook of Formal Languages and Automata, volume 3, pages 69-124. Springer-Verlag, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Using syntactic dependency as local context to resolve word sense ambiguity.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="6517" citStr="Lin, 1997" startWordPosition="1042" endWordPosition="1043">robability distribution to be defined over a word&apos;s membership in all the clusters. statistical approaches. (Yarowsky, 1992) uses wide &amp;quot;bag-of-words&amp;quot; contexts with a naive Bayes classifier. (Yarowsky, 1995) also uses wide context, but incorporates the one-senseper-discourse and one-sense-per-collocation constraints, using an unsupervised learning technique. The supervised technique in (Yarowsky, 1994) has a more specific notion of context, employing not just words that can appear within a window of ±k, but crucially words that abut and fall in the ±2 window of the target word. More recently, (Lin, 1997) has shown how syntactic context, and dependency structures in particular, can be successfully employed for word sense disambiguation. (Stetina and Nagao, 1997) have shown that by employing a fairly simple and somewhat ad-hoc unsupervised method of WSD using a WordNet-based similarity heuristic, they could enhance PP-attachment performance to a significantly higher level than systems that made no use of lexical semantics (88.1% accuracy). Most recently, in (Stetina et al., 1998), the authors made use of head-driven bilexical dependencies with syntactic relations to attack the problem of genera</context>
<context position="31381" citStr="Lin, 1997" startWordPosition="5367" endWordPosition="5368"> We would also like to incorporate long-distance context in the model as an aid to WSD, a demonstrably effective feature in virtually all the recent, statistical WSD work. Also, as mentioned earlier, we believe there are several features that would allow significant parsing improvement. Finally, we would like to investigate the incorporation of unsupervised methods for WSD, such as the heuristically-based methods of (Stetina and Nagao, 1997) and (Stetina et al., 1998), and the theoretically purer bootstrapping method of (Yarowsky, 1995). Bolstered by the success of (Stetina and Nagao, 1997), (Lin, 1997) and especially (Stetina et al., 1998), we believe there is great promise the incorporation of word-sense into a probabilistic parsing model. 9 Acknowledgements I would like to greatly acknowledge the researchers at BBN who allowed me to use and abuse their parser and who fostered the beginning of this research effort: Scott Miller, Lance Ramshaw, Heidi Fox, Sean Boisen and Ralph Weischedel. Thanks to Michelle Engel, who helped enormously with the task of preparing the new data set. Finally, I would like to thank my advisor Mitch Marcus for his invaluable technical advice and support. 162 Refe</context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>Dekang Lin. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
</authors>
<title>Statistical decision tree models for parsing.</title>
<date>1995</date>
<booktitle>In 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="3403" citStr="Magerman, 1995" startWordPosition="552" endWordPosition="553">dels, the availability of resources such as the Penn Treebank (Marcus et al., 1993) and the success of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging (Church, 1988; Brill, 1995), and PPattachment (Brill and Resnik, 1994; Collins and Brooks, 1995). However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies. This includes both the parsers that attach probabilities to parser moves (Magerman, 1995; Ratnaparkhi, 1997), but also those of the lexicalized PCFG variety (Collins, 1997; Charniak, 1997). 155 Even more crucially, the bilexical dependencies involve head-modifier relations (hereafter referred to simply as &amp;quot;head relations&amp;quot;). The intuition behind the lexicalization of a grammar formalism is to capture lexical items&apos; idiosyncratic parsing preferences. The intuition behind using heads as the members of the bilexical relations is twofold. First, many linguistic theories tell us that the head of a phrase projects the skeleton of that phrase, to be filled in by specifiers, complements a</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>D. Magerman. 1995. Statistical decision tree models for parsing. In 33rd Annual Meeting of the Association for Computational Linguistics, pages 276-283, Cambridge, Massachusetts. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<pages>19--313</pages>
<contexts>
<context position="2872" citStr="Marcus et al., 1993" startWordPosition="469" endWordPosition="472">bank is not a river bank and that check is not a restaurant bill. In Examples 4 and 5, knowing that the complement of expect is an NP or an SBAR provides information as to whether the sense is &amp;quot;await&amp;quot; or &amp;quot;require&amp;quot;. Thus, Examples 3-5 illustrate how the syntactic context of a word can help determine its meaning. 2.2 Motivation from previous work 2.2.1 Parsing In recent years, the success of statistical parsing techniques can be attributed to several factors, such as the increasing size of computing machinery to accommodate larger models, the availability of resources such as the Penn Treebank (Marcus et al., 1993) and the success of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging (Church, 1988; Brill, 1995), and PPattachment (Brill and Resnik, 1994; Collins and Brooks, 1995). However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies. This includes both the parsers that attach probabilities to parser moves (Magerman, 1995; Ratnaparkhi, 1997), but also those of the lexicalized PCFG variety </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard T Beckwith</author>
<author>Christiane D Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="5032" citStr="Miller et al., 1990" startWordPosition="812" endWordPosition="815">ructural preference virtually eliminates one of the two semantically plausible analyses, it is quite clear that semantics—and, in particular, lexical head semantics—play a very important role in reducing parsing ambiguity. (See (Collins, 1999), pp. 207ff., for an excellent discussion of structural vs. semantic parsing preferences, including the above John was believed... example.) Another motivation for incorporating word senses into a statistical parsing model has been to ameliorate the sparse data problem. Inspired by the PP-attachment work of (Stetina and Nagao, 1997), we use WordNet v1.6 (Miller et al., 1990) as our semantic dictionary, where the hypernym structure provides the basis for semantically-motivated soft clusters.2 We discuss this benefit of word senses and the details of our implementation further in Section 4. 2.2.2 Word-sense disambiguation While there has been much work in this area, let us examine the features used in recent &apos;Heads originated this way, but it has become necessary to distinguish &amp;quot;semantic&amp;quot; heads, such as nouns and verbs, that correspond roughly to predicates and arguments, from &amp;quot;functional&amp;quot; heads, such as determiners, INFL&apos;s and complementizers, that correspond roug</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard T. Beckwith, Christiane D. Fellbaum, Derek Gross, and Katherine J. Miller. 1990. WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Martin Chodorow</author>
<author>Shari Landes</author>
<author>Claudia Leacock</author>
<author>Robert G Thomas</author>
</authors>
<title>Using a semantic concordance for sense identification.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="18041" citStr="Miller et al., 1994" startWordPosition="3065" endWordPosition="3068">ack-off levels for nouns and verbs, corresponding to the head word&apos;s depth in the synset hierarchy. A valid concern would be that the model might be backing off using histories that are far too general, so we experimented with limiting the hypernym back-off to only two, three and four levels. This change produced a negligible difference in parsing performance.8 5 A New Approach, A New Data Set Ideally, the well-established gold standard for syntax, the Penn Treebank, would have a parallel word-sense—annotated corpus; unfortunately, no such word-sense corpus exists. However, we do have SemCor (Miller et al., 1994), where every noun, verb, adjective and adverb from a 455k word portion of the Brown Corpus has been assigned a WordNet synset. While all of the Brown Corpus was annotated in the style of Treebank I, a great deal was also more recently annotated in Treebank II format, and this corpus has recently been released by the Linguistic Data Consortium.9 As it happens, the intersection between the Treebank-II-annotated Brown and SemCor comprises some 220k words, most of which is fiction, with some nonfiction and humor writing as well. We went through all 220k words of the corpora, synchronizing them. T</context>
</contexts>
<marker>Miller, Chodorow, Landes, Leacock, Thomas, 1994</marker>
<rawString>George A. Miller, Martin Chodorow, Shari Landes, Claudia Leacock, and Robert G. Thomas. 1994. Using a semantic concordance for sense identification. In Proceedings of the ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>SIFT - Statisticallyderived Information From Text.</title>
<date>1998</date>
<booktitle>In Seventh Message Understanding Conference (MUG- 7),</booktitle>
<location>Washington, D.C.</location>
<contexts>
<context position="7327" citStr="Miller et al., 1998" startWordPosition="1170" endWordPosition="1173">a fairly simple and somewhat ad-hoc unsupervised method of WSD using a WordNet-based similarity heuristic, they could enhance PP-attachment performance to a significantly higher level than systems that made no use of lexical semantics (88.1% accuracy). Most recently, in (Stetina et al., 1998), the authors made use of head-driven bilexical dependencies with syntactic relations to attack the problem of generalized word-sense disambiguation, precisely one of the two problems we are dealing with here. 3 The Model 3.1 Overview The parsing model we started with was extracted from BBN&apos;s SIFT system (Miller et al., 1998), which we briefly present again here, using examples from Figure 1 to illustrate the model&apos;s paxameters.3 The model generates the head of a constituent first, then each of the left- and rightmodifiers, generating from the head outward, using a bigram model of node labels. Here are the first few elements generated by the model for the tree of Figure 1: 1. S and its head word and part of speech, caught-VBD. 2. The head constituent of S, VP. 3. The head word of the VP, caught-VBD. 4. The premodifier constituent ADVP. 3We began with the BBN parser because its authors were kind enough to allow us </context>
<context position="9813" citStr="Miller et al., 1998" startWordPosition="1607" endWordPosition="1610">ead constituent. The constituents labeled L, and R, are leftand right-modifier constituents, respectively. 3.2 Probability structure of the original model We use p to denote the unlexicalized nonterminal corresponding to P, and similarly for li, ri and h. We now present the top-level generation probabilities, along with examples from 4The inclusion of the word feature in the BBN model was due to the work described in (Weischedel et al., 1993), where word features helped reduce part of speech ambiguity for unknown words. Figure 1. For brevity, we omit the smoothing details of BBN&apos;s model (see (Miller et al., 1998) for a complete description); we note that all smoothing weights are computed via the technique described in (Bikel et al., 1997). The probability of generating p as the root label is predicted conditioning on only +TOP+, which is the hidden root of all parse trees: P (pl + TOP+) , e.g., P(S I + TOP+). (2) The probability of generating a head node h with a parent p is P(hIp), e.g., P(VP IS). (3) The probability of generating a left-modifier 1, is PL(lillt-1,11,11,wh)„ e.g., (4) PL, (NP I ADVP, S, VP, caught) when generating the NP for NP(boy-NN), and the probability of generating a right modif</context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 1998</marker>
<rawString>Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph Weischedel. 1998. SIFT - Statisticallyderived Information From Text. In Seventh Message Understanding Conference (MUG- 7), Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<institution>Brown University,</institution>
<location>Providence, Rhode Island.</location>
<contexts>
<context position="3423" citStr="Ratnaparkhi, 1997" startWordPosition="554" endWordPosition="556">bility of resources such as the Penn Treebank (Marcus et al., 1993) and the success of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging (Church, 1988; Brill, 1995), and PPattachment (Brill and Resnik, 1994; Collins and Brooks, 1995). However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies. This includes both the parsers that attach probabilities to parser moves (Magerman, 1995; Ratnaparkhi, 1997), but also those of the lexicalized PCFG variety (Collins, 1997; Charniak, 1997). 155 Even more crucially, the bilexical dependencies involve head-modifier relations (hereafter referred to simply as &amp;quot;head relations&amp;quot;). The intuition behind the lexicalization of a grammar formalism is to capture lexical items&apos; idiosyncratic parsing preferences. The intuition behind using heads as the members of the bilexical relations is twofold. First, many linguistic theories tell us that the head of a phrase projects the skeleton of that phrase, to be filled in by specifiers, complements and adjuncts; such a </context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, Brown University, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Stetina</author>
<author>Makoto Nagao</author>
</authors>
<title>Corpus based PP attachment ambiguity resolution with a semantic dictionary.</title>
<date>1997</date>
<booktitle>In Fifth Workshop on Very Large Corpora,</booktitle>
<pages>66--80</pages>
<location>Beijing.</location>
<contexts>
<context position="1910" citStr="Stetina and Nagao, 1997" startWordPosition="305" endWordPosition="309">line with personal computers. 3. The bank issued a check for $100,000. 4. Apple is expecting [Ni, strong results]. 5. IBM expected [SBAR each employee to wear a shirt and tie]. With Example 1, the reading [IBM bought [Lotus for $200 million]] is nearly impossible, for the simple reason that a monetary amount is a likely instrument for buying and not for describing a company. Similarly, there is a reasonably strong preference in Example 2 for [pp with personal computers] to attach to widened, because personal computers are products with which a product line could be widened. As pointed out by (Stetina and Nagao, 1997), word sense information can be a proxy for the semantic- and world-knowledge we as humans bring to bear on attachment decisions such as these. This proxy effect is due to the &amp;quot;lightweight semantics&amp;quot; that word senses—in particular WordNet word senses— convey. Conversely, both the syntactic and semantic context in Example 3 let us know that bank is not a river bank and that check is not a restaurant bill. In Examples 4 and 5, knowing that the complement of expect is an NP or an SBAR provides information as to whether the sense is &amp;quot;await&amp;quot; or &amp;quot;require&amp;quot;. Thus, Examples 3-5 illustrate how the synta</context>
<context position="4989" citStr="Stetina and Nagao, 1997" startWordPosition="803" endWordPosition="806">was believed to have been shot by Bill where structural preference virtually eliminates one of the two semantically plausible analyses, it is quite clear that semantics—and, in particular, lexical head semantics—play a very important role in reducing parsing ambiguity. (See (Collins, 1999), pp. 207ff., for an excellent discussion of structural vs. semantic parsing preferences, including the above John was believed... example.) Another motivation for incorporating word senses into a statistical parsing model has been to ameliorate the sparse data problem. Inspired by the PP-attachment work of (Stetina and Nagao, 1997), we use WordNet v1.6 (Miller et al., 1990) as our semantic dictionary, where the hypernym structure provides the basis for semantically-motivated soft clusters.2 We discuss this benefit of word senses and the details of our implementation further in Section 4. 2.2.2 Word-sense disambiguation While there has been much work in this area, let us examine the features used in recent &apos;Heads originated this way, but it has become necessary to distinguish &amp;quot;semantic&amp;quot; heads, such as nouns and verbs, that correspond roughly to predicates and arguments, from &amp;quot;functional&amp;quot; heads, such as determiners, INFL&apos;</context>
<context position="6677" citStr="Stetina and Nagao, 1997" startWordPosition="1065" endWordPosition="1068">ds&amp;quot; contexts with a naive Bayes classifier. (Yarowsky, 1995) also uses wide context, but incorporates the one-senseper-discourse and one-sense-per-collocation constraints, using an unsupervised learning technique. The supervised technique in (Yarowsky, 1994) has a more specific notion of context, employing not just words that can appear within a window of ±k, but crucially words that abut and fall in the ±2 window of the target word. More recently, (Lin, 1997) has shown how syntactic context, and dependency structures in particular, can be successfully employed for word sense disambiguation. (Stetina and Nagao, 1997) have shown that by employing a fairly simple and somewhat ad-hoc unsupervised method of WSD using a WordNet-based similarity heuristic, they could enhance PP-attachment performance to a significantly higher level than systems that made no use of lexical semantics (88.1% accuracy). Most recently, in (Stetina et al., 1998), the authors made use of head-driven bilexical dependencies with syntactic relations to attack the problem of generalized word-sense disambiguation, precisely one of the two problems we are dealing with here. 3 The Model 3.1 Overview The parsing model we started with was extr</context>
<context position="27076" citStr="Stetina and Nagao, 1997" startWordPosition="4632" endWordPosition="4635">l) Jane will VP(kill) kill NP(Bob) Bob Figure 2: Head rules are tuned for syntax, not semantics. ical dependencies. Whereas such long-range dependencies might cripple a standard generative model, the soft-clustering aspects of synsets should offset the sparse data problem. As an example of the lack of such dependencies, in the current model when predicting the attachment of [bought company [for million]], there is no current dependence between the verb bought and the object of the preposition million-a dependence shown to be useful in virtually all the PP attachment work, and particularly in (Stetina and Nagao, 1997). Related to this issue, we note that the head rules, which were nearly identical to those used in (Collins, 1997), have not been tuned at all to this task. For example, in the sentence in Figure 2, the subject Jane is predicted conditioning on the head of the VP, which is the modal will, as opposed to the more semanticallycontent-rich kill. So, while the head relations provide a very useful structure for many syntactic decisions the parser needs to make, it is quite possible that the synset relations of this model would require additional or different de. 161 Recall Precision Noun 86.5% 70.9%</context>
<context position="31216" citStr="Stetina and Nagao, 1997" startWordPosition="5340" endWordPosition="5343">Collins, 1997). We would also like to explore the use of a more radical model, where nontermina&apos;s only have synsets as their heads, and words are generated strictly at the leaves. We would also like to incorporate long-distance context in the model as an aid to WSD, a demonstrably effective feature in virtually all the recent, statistical WSD work. Also, as mentioned earlier, we believe there are several features that would allow significant parsing improvement. Finally, we would like to investigate the incorporation of unsupervised methods for WSD, such as the heuristically-based methods of (Stetina and Nagao, 1997) and (Stetina et al., 1998), and the theoretically purer bootstrapping method of (Yarowsky, 1995). Bolstered by the success of (Stetina and Nagao, 1997), (Lin, 1997) and especially (Stetina et al., 1998), we believe there is great promise the incorporation of word-sense into a probabilistic parsing model. 9 Acknowledgements I would like to greatly acknowledge the researchers at BBN who allowed me to use and abuse their parser and who fostered the beginning of this research effort: Scott Miller, Lance Ramshaw, Heidi Fox, Sean Boisen and Ralph Weischedel. Thanks to Michelle Engel, who helped eno</context>
</contexts>
<marker>Stetina, Nagao, 1997</marker>
<rawString>Jiri Stetina and Makoto Nagao. 1997. Corpus based PP attachment ambiguity resolution with a semantic dictionary. In Fifth Workshop on Very Large Corpora, pages 66-80, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Stetina</author>
<author>Sadao Kurohashi</author>
<author>Makoto Naga°</author>
</authors>
<title>General word sense disambiguation method based on a full sentential context.</title>
<date>1998</date>
<booktitle>In COLING-ACL &apos;98 Workshop: Usage of WordNet in Natural Language Processing Systems,</booktitle>
<location>Montreal, Canada,</location>
<marker>Stetina, Kurohashi, Naga°, 1998</marker>
<rawString>Jiri Stetina, Sadao Kurohashi, and Makoto Naga°. 1998. General word sense disambiguation method based on a full sentential context. In COLING-ACL &apos;98 Workshop: Usage of WordNet in Natural Language Processing Systems, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>M Meteer</author>
<author>R Schwartz</author>
<author>L Ramshaw</author>
<author>J Palmucci</author>
</authors>
<title>Coping with ambiguity and unknown words through probabilistic methods.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="9639" citStr="Weischedel et al., 1993" startWordPosition="1577" endWordPosition="1580">re triple that is the head of the phrase denoted by X.4 The lexicalized nonterminal H is so named because it is the head constituent, where P inherits its head triple from this head constituent. The constituents labeled L, and R, are leftand right-modifier constituents, respectively. 3.2 Probability structure of the original model We use p to denote the unlexicalized nonterminal corresponding to P, and similarly for li, ri and h. We now present the top-level generation probabilities, along with examples from 4The inclusion of the word feature in the BBN model was due to the work described in (Weischedel et al., 1993), where word features helped reduce part of speech ambiguity for unknown words. Figure 1. For brevity, we omit the smoothing details of BBN&apos;s model (see (Miller et al., 1998) for a complete description); we note that all smoothing weights are computed via the technique described in (Bikel et al., 1997). The probability of generating p as the root label is predicted conditioning on only +TOP+, which is the hidden root of all parse trees: P (pl + TOP+) , e.g., P(S I + TOP+). (2) The probability of generating a head node h with a parent p is P(hIp), e.g., P(VP IS). (3) The probability of generati</context>
</contexts>
<marker>Weischedel, Meteer, Schwartz, Ramshaw, Palmucci, 1993</marker>
<rawString>R. Weischedel, M. Meteer, R. Schwartz, L. Ramshaw, and J. Palmucci. 1993. Coping with ambiguity and unknown words through probabilistic methods. Computational Linguistics, 19(2):359-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of roget&apos;s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Fourteenth International Conference on Computational Linguistics (COLING),</booktitle>
<pages>454--460</pages>
<contexts>
<context position="6031" citStr="Yarowsky, 1992" startWordPosition="969" endWordPosition="970"> necessary to distinguish &amp;quot;semantic&amp;quot; heads, such as nouns and verbs, that correspond roughly to predicates and arguments, from &amp;quot;functional&amp;quot; heads, such as determiners, INFL&apos;s and complementizers, that correspond roughly to logical operators or are purely syntactic elements. In this paper, we almost always intend &amp;quot;head&amp;quot; to mean &amp;quot;semantic head&amp;quot;. 2Soft clusters are sets where the elements have weights indicating the strength of their membership in the set, which in this case allows for a probability distribution to be defined over a word&apos;s membership in all the clusters. statistical approaches. (Yarowsky, 1992) uses wide &amp;quot;bag-of-words&amp;quot; contexts with a naive Bayes classifier. (Yarowsky, 1995) also uses wide context, but incorporates the one-senseper-discourse and one-sense-per-collocation constraints, using an unsupervised learning technique. The supervised technique in (Yarowsky, 1994) has a more specific notion of context, employing not just words that can appear within a window of ±k, but crucially words that abut and fall in the ±2 window of the target word. More recently, (Lin, 1997) has shown how syntactic context, and dependency structures in particular, can be successfully employed for word s</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>David Yarowsky. 1992. Word-sense disambiguation using statistical models of roget&apos;s categories trained on large corpora. In Fourteenth International Conference on Computational Linguistics (COLING), pages 454-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Assocation for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="6311" citStr="Yarowsky, 1994" startWordPosition="1004" endWordPosition="1005">is paper, we almost always intend &amp;quot;head&amp;quot; to mean &amp;quot;semantic head&amp;quot;. 2Soft clusters are sets where the elements have weights indicating the strength of their membership in the set, which in this case allows for a probability distribution to be defined over a word&apos;s membership in all the clusters. statistical approaches. (Yarowsky, 1992) uses wide &amp;quot;bag-of-words&amp;quot; contexts with a naive Bayes classifier. (Yarowsky, 1995) also uses wide context, but incorporates the one-senseper-discourse and one-sense-per-collocation constraints, using an unsupervised learning technique. The supervised technique in (Yarowsky, 1994) has a more specific notion of context, employing not just words that can appear within a window of ±k, but crucially words that abut and fall in the ±2 window of the target word. More recently, (Lin, 1997) has shown how syntactic context, and dependency structures in particular, can be successfully employed for word sense disambiguation. (Stetina and Nagao, 1997) have shown that by employing a fairly simple and somewhat ad-hoc unsupervised method of WSD using a WordNet-based similarity heuristic, they could enhance PP-attachment performance to a significantly higher level than systems that ma</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>David Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the Assocation for Computational Linguistics, pages 88-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="6113" citStr="Yarowsky, 1995" startWordPosition="980" endWordPosition="981">nd roughly to predicates and arguments, from &amp;quot;functional&amp;quot; heads, such as determiners, INFL&apos;s and complementizers, that correspond roughly to logical operators or are purely syntactic elements. In this paper, we almost always intend &amp;quot;head&amp;quot; to mean &amp;quot;semantic head&amp;quot;. 2Soft clusters are sets where the elements have weights indicating the strength of their membership in the set, which in this case allows for a probability distribution to be defined over a word&apos;s membership in all the clusters. statistical approaches. (Yarowsky, 1992) uses wide &amp;quot;bag-of-words&amp;quot; contexts with a naive Bayes classifier. (Yarowsky, 1995) also uses wide context, but incorporates the one-senseper-discourse and one-sense-per-collocation constraints, using an unsupervised learning technique. The supervised technique in (Yarowsky, 1994) has a more specific notion of context, employing not just words that can appear within a window of ±k, but crucially words that abut and fall in the ±2 window of the target word. More recently, (Lin, 1997) has shown how syntactic context, and dependency structures in particular, can be successfully employed for word sense disambiguation. (Stetina and Nagao, 1997) have shown that by employing a fair</context>
<context position="31313" citStr="Yarowsky, 1995" startWordPosition="5356" endWordPosition="5357">e synsets as their heads, and words are generated strictly at the leaves. We would also like to incorporate long-distance context in the model as an aid to WSD, a demonstrably effective feature in virtually all the recent, statistical WSD work. Also, as mentioned earlier, we believe there are several features that would allow significant parsing improvement. Finally, we would like to investigate the incorporation of unsupervised methods for WSD, such as the heuristically-based methods of (Stetina and Nagao, 1997) and (Stetina et al., 1998), and the theoretically purer bootstrapping method of (Yarowsky, 1995). Bolstered by the success of (Stetina and Nagao, 1997), (Lin, 1997) and especially (Stetina et al., 1998), we believe there is great promise the incorporation of word-sense into a probabilistic parsing model. 9 Acknowledgements I would like to greatly acknowledge the researchers at BBN who allowed me to use and abuse their parser and who fostered the beginning of this research effort: Scott Miller, Lance Ramshaw, Heidi Fox, Sean Boisen and Ralph Weischedel. Thanks to Michelle Engel, who helped enormously with the task of preparing the new data set. Finally, I would like to thank my advisor Mi</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189-196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>