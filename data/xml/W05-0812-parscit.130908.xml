<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004120">
<title confidence="0.982439">
Improved HMM Alignment Models for Languages with Scarce Resources
</title>
<author confidence="0.99671">
Adam Lopez
</author>
<affiliation confidence="0.998429">
Institute for Advanced Computer Studies
Department of Computer Science
University of Maryland
</affiliation>
<address confidence="0.944963">
College Park, MD 20742
</address>
<email confidence="0.998503">
alopez@cs.umd.edu
</email>
<author confidence="0.99413">
Philip Resnik
</author>
<affiliation confidence="0.998338">
Institute for Advanced Computer Studies
Department of Linguistics
University of Maryland
</affiliation>
<address confidence="0.945172">
College Park, MD 20742
</address>
<email confidence="0.9991">
resnik@umiacs.umd.edu
</email>
<sectionHeader confidence="0.997391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969125">
We introduce improvements to statistical word
alignment based on the Hidden Markov
Model. One improvement incorporates syntac-
tic knowledge. Results on the workshop data
show that alignment performance exceeds that
of a state-of-the art system based on more com-
plex models, resulting in over a 5.5% absolute
reduction in error on Romanian-English.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980214285714">
The most widely used alignment model is IBM Model 4
(Brown et al., 1993). In empirical evaluations it has out-
performed the other IBM Models and a Hidden Markov
Model (HMM) (Och and Ney, 2003). It was the basis
for a system that performed very well in a comparison
of several alignment systems (Dejean et al., 2003; Mihal-
cea and Pedersen, 2003). Implementations are also freely
available (Al-Onaizan et al., 1999; Och and Ney, 2003).
The IBM Model 4 search space cannot be efficiently
enumerated; therefore it cannot be trained directly using
Expectation Maximization (EM). In practice, a sequence
of simpler models such as IBM Model 1 and an HMM
Model are used to generate initial parameter estimates
and to enumerate a partial search space which can be ex-
panded using hill-climbing heuristics. IBM Model 4 pa-
rameters are then estimated over this partial search space
as an approximation to EM (Brown et al., 1993; Och and
Ney, 2003). This approach yields good results, but it has
been observed that the IBM Model 4 performance is only
slightly better than that of the underlying HMM Model
used in this bootstrapping process (Och and Ney, 2003).
This is illustrated in Figure 1.
Based on this observation, we hypothesize that imple-
mentations of IBM Model 4 derive most of their per-
formance benefits from the underlying HMM Model.
Furthermore, owing to the simplicity of HMM Models,
we believe that they are more conducive to study and
improvement than more complex models such as IBM
</bodyText>
<page confidence="0.991496">
83
</page>
<figure confidence="0.726766">
Training Iterations
</figure>
<figureCaption confidence="0.8553254">
Figure 1: The improvement in Alignment Error Rate
(AER) is shown for both P(f|e) and P(e|f) alignments on
the Romanian-English development set over several iter-
ations of the IBM Model 1 —&gt; HMM —&gt; IBM Model 4
training sequence.
</figureCaption>
<sectionHeader confidence="0.8561" genericHeader="introduction">
2 HMMs and Word Alignment
</sectionHeader>
<bodyText confidence="0.982335578947369">
The objective of word alignment is to discover the word-
to-word translational correspondences in a bilingual cor-
pus of S sentence pairs, which we denote {(f(s),e(s)) : s E
[1,S]}. Each sentence pair (f,e) = (f M 1 ,eN 1 ) consists of
a sentence f in one language and its translation e in the
other, with lengths M and N, respectively. By convention
we refer to e as the English sentence and f as the French
sentence. Correspondences in a sentence are represented
by a set of links between words. A link (fj,ei) denotes a
correspondence between the ith word ei of e and the jth
word fj of f.
Many alignment models arise from the conditional dis-
tribution P(f|e). We can decompose this by introducing
the hidden alignment variable a = aM1 . Each element of
a takes on a value in the range [1,N]. The value of ai
determines a link between the ith French word fi and
the aith English word ea,. This representation introduces
Model 4. We illustrate this point by introducing modifi-
cations to the HMM model which improve performance.
</bodyText>
<figure confidence="0.983044611111111">
.7
.65
◦◦
.6
.55
.5
.45
.4
AER
◦
◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦
◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦
Model 1 HMM Model 4
.35
.3
◦
◦◦
◦ ◦ ◦
</figure>
<note confidence="0.4894705">
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 83–86,
Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<bodyText confidence="0.999993111111111">
an asymmetry into the model because it constrains each
French word to correspond to exactly one English word,
while each English word is permitted to correspond to an
arbitrary number of French words. Although the result-
ing set of links may still be relatively accurate, we can
symmetrize by combining it with the set produced by ap-
plying the complementary model P(e|f) to the same data
(Och and Ney, 2000b). Making a few independence as-
sumptions we arrive at the decomposition in Equation 1. 1
</bodyText>
<equation confidence="0.998696">
M
P(f,a|e) = ∏ d(ai|ai−1) ·t(fi|eai) (1)
i=1
</equation>
<bodyText confidence="0.999979857142857">
We refer to d(ai|ai−1) as the distortion model and t(fi|eai)
as the translation model. Conveniently, Equation 1 is in
the form of an HMM, so we can apply standard algo-
rithms for HMM parameter estimation and maximization.
This approach was proposed in Vogel et al. (1996) and
subsequently improved (Och and Ney, 2000a; Toutanova
et al., 2002).
</bodyText>
<subsectionHeader confidence="0.989202">
2.1 The Tree Distortion Model
</subsectionHeader>
<bodyText confidence="0.998083">
Equation 1 is adequate in practice, but we can improve
it. Numerous parameterizations have been proposed for
the distortion model. In our surface distortion model, it
depends only on the distance ai − ai−1 and an automati-
cally determined word class C(eai−1) as shown in Equa-
tion 2. It is similar to (Och and Ney, 2000a). The word
class C(eai−1) is assigned using an unsupervised approach
(Och, 1999).
</bodyText>
<equation confidence="0.99797">
d(ai|ai−1) = p(ai|ai −ai−1,C(eai−1)) (2)
</equation>
<bodyText confidence="0.999972142857143">
The surface distortion model can capture local move-
ment but it cannot capture movement of structures or the
behavior of long-distance dependencies across transla-
tions. The intuitive appeal of capturing richer informa-
tion has inspired numerous alignment models (Wu, 1995;
Yamada and Knight, 2001; Cherry and Lin, 2003). How-
ever, we would like to retain the simplicity and good per-
formance of the HMM Model.
We introduce a distortion model which depends on the
tree distance τ(ei,ek) = (w,x,y) between each pair of En-
glish words ei and ek. Given a dependency parse of eM1 ,
w and x represent the respective number of dependency
links separating ei and ek from their closest common an-
cestor node in the parse tree. 2 The final element y = {1
</bodyText>
<footnote confidence="0.9981425">
1We ignore the sentence length probability p(MIN), which
is not relevant to word alignment. We also omit discussion
of HMM start and stop probabilities, and normalization of
t(fi ea;), although we find in practice that attention to these de-
tails can be beneficial.
2The tree distance could easily be adapted to work with
phrase-structure parses or tree-adjoining parses instead of de-
pendency parses.
</footnote>
<equation confidence="0.9999035">
τ(I1,very2) = (1,2,0)
τ(very2,I1) = (2,1,1)
τ(I1,doubt4) = (1,0,0)
τ(that5,I1) = (1,1,1)
</equation>
<figureCaption confidence="0.975413">
Figure 2: Example of tree distances in a sentence from
the Romanian-English development set.
</figureCaption>
<bodyText confidence="0.846025">
if i &gt; k; 0 otherwise} is simply a binary indicator of the
linear relationship of the words within the surface string.
Tree distance is illustrated in Figure 2.
In our tree distortion model, we condition on the tree
distance and the part of speech T(ei−1), giving us Equa-
tion 3.
</bodyText>
<equation confidence="0.999806">
d(ai|ai−1) = p(ai,|τ(eai,eai−1),T(eai−1)) (3)
</equation>
<bodyText confidence="0.974058333333333">
Since both the surface distortion and tree distortion
models represent p(ai|ai−1), we can combine them using
linear interpolation as in Equation 4.
</bodyText>
<equation confidence="0.999849333333333">
d(ai|ai−1) =
λC(eai−1),T(eai−1)p(ai|τ(eai,eai−1),T(eai−1)) + (4)
(1 − λC(eai−1),T (eai−1))p(ai|ai − ai−1,C(eai−1))
</equation>
<bodyText confidence="0.9998735">
The λC,T parameters can be initialized from a uniform
distribution and trained with the other parameters using
EM. In principle, any number of alternative distortion
models could be combined with this framework.
</bodyText>
<subsectionHeader confidence="0.995275">
2.2 Improving Initialization
</subsectionHeader>
<bodyText confidence="0.99488125">
Our HMM produces reasonable results if we draw our
initial parameter estimates from a uniform distribution.
However, we can do better. We estimate the initial
translation probability t(fj|ei) from the smoothed log-
likelihood ratio LLR(ei, fj)01 computed over sentence
cooccurrences. Since this method works well, we apply
LLR(ei, fj) in a single reestimation step shown in Equa-
tion 5.
</bodyText>
<equation confidence="0.996985666666667">
LLR(f|e)02 +n
t(f|e) = (5)
∑e0 LLR(f|e&apos;)02 +n·|V|
</equation>
<bodyText confidence="0.999947363636364">
In reestimation LLR(f |e) is computed from the expected
counts of f and e produced by the EM algorithm. This is
similar to Moore (2004); as in that work, |V |= 100,000,
and φ1, φ2, and n are estimated on development data.
We can also use an improved initial estimate for distor-
tion. Consider a simple distortion model p(ai|ai − ai−1).
We expect this distribution to have a maximum near
P(ai|0) because we know that words tend to retain their
locality across translation. Rather than wait for this to
occur, we use an initial estimate for the distortion model
given in Equation 6.
</bodyText>
<equation confidence="0.737097">
I1 very2 much3 doubt4 that5
</equation>
<page confidence="0.953902">
84
</page>
<table confidence="0.9988566">
corpus n φ1 φ2 α symmetrization n−1 φ−1 φ−1 α−1
1 2
English-Inuktitut 1−4 1.0 1.75 -1.5 n 5−4 1.0 1.75 -1.5
Romanian-English 5−4 1.5 1.0 -2.5 refined (Och and Ney, 2000b) 5−4 1.5 1.0 -2.5
English-Hindi 1−4 1.5 3.0 -2.5 U 1−2 1.0 1.0 -1.0
</table>
<tableCaption confidence="0.996396">
Table 1: Training parameters for the workshop data (see Section 2.2). Parameters n, φ1, φ2, and α were used in the
</tableCaption>
<table confidence="0.910773866666667">
initialization of P(f|e) model, while n−1, φ−1
1 , φ−1
2 , and α−1 were used in the initialization of the P(e|f) model.
corpus type HMM limited (Eq. 2) HMM unlimited (Eq. 4) IBM Model 4
P R AER P R AER P R AER
P(f|e) .4962 .6894 .4513 – – – .4211 .6519 .5162
English-Inuktitut P(e|f) .5789 .8635 .3856 – – – .5971 .8089 .3749
n .8916 .6280 .2251 – – – .8682 .5700 .2801
P(f|e) .5079 .4769 .5081 .5057 .4748 .5102 .5219 .4223 .5332
English-Hindi P(e|f) .5566 .4429 .5067 .5566 .4429 .5067 .5652 .3939 .5358
U .4408 .5649 .5084 .4365 .5614 .5088
.4543 .5401 .5065
P(f|e) .6876 .6233 .3461 .6876 .6233 .3461 .6828 .5414 .3961
Romanian-English P(e|f) .7168 .6217 .3341 .7155 .6205 .3354 .7520 .5496 .3649
refined .7377 .6169 .3281 .7241 .6215 .3311 .7620 .5134 .3865
</table>
<tableCaption confidence="0.994851">
Table 2: Results on the workshop data. The systems highlighted in bold are the ones that were used in the shared task.
</tableCaption>
<bodyText confidence="0.986929416666667">
For each corpus, the last row shown represents the results that were actually submitted. Note that for English-Hindi,
our self-reported results in the unlimited task are slightly lower than the original results submitted for the workshop,
which contained an error.
(6)
We choose Z to normalize the distribution. We must
optimize α on a development set. This distribution has
a maximum when |ai − ai−1 |E {−1,0,1}. Although we
could reasonably choose any of these three values as the
maximum for the initial estimate, we found in develop-
ment that the maximum of the surface distortion distribu-
tion varied with C(eai−1), although it was always in the
range [−1,2].
</bodyText>
<subsectionHeader confidence="0.989369">
2.3 Does NULL Matter in Asymmetric Alignment?
</subsectionHeader>
<bodyText confidence="0.9999853">
Och and Ney (2000a) introduce a NULL-alignment ca-
pability to the HMM alignment model. This allows any
word fj to link to a special NULL word – by conven-
tion denoted e0 – instead of one of the words eN1 . A link
(fj,e0) indicates that fj does not correspond to any word
in e. This improved alignment performance in the ab-
sence of symmetrization, presumably because it allows
the model to be conservative when evidence for an align-
ment is lacking.
We hypothesize that NULL alignment is unnecessary
for asymmetric alignment models when we symmetrize
using intersection-based methods (Och and Ney, 2000b).
The intuition is simple: if we don’t permit NULL align-
ments, then we expect to produce a high-recall, low-
precision alignment; the intersection of two such align-
ments should mainly improve precision, resulting in a
high-recall, high-precision alignment. If we allow NULL
alignments, we may be able produce a high-precision,
low-recall asymmetric alignment, but symmetrization by
intersection will not improve recall.
</bodyText>
<sectionHeader confidence="0.992394" genericHeader="method">
3 Results with the Workshop Data
</sectionHeader>
<bodyText confidence="0.999990625">
In our experiments, the dependency parse and parts of
speech are produced by minipar (Lin, 1998). This parser
has been used in a much different alignment model
(Cherry and Lin, 2003). Since we only had parses for
English, we did not use tree distortion in the application
of P(e|f), needed for symmetrization.
The parameter settings that we used in aligning the
workshop data are presented in Table 1. Although our
prior work with English and French indicated that in-
tersection was the best method for symmetrization, we
found in development that this varied depending on the
characteristics of the corpus and the type of annotation
(in particular, whether the annotation set included proba-
ble alignments). The results are summarized in Table 2.
It shows results with our HMM model using both Equa-
tions 2 and 4 as our distortion model, which represent
</bodyText>
<equation confidence="0.783270333333333">
� |ai − ai−1|α/Z,α &lt; 0 if ai =� ai−1.
d(ai|ai−1) = 1/Z
ai
</equation>
<bodyText confidence="0.650430666666667">
ai−1.
if
=
</bodyText>
<page confidence="0.998709">
85
</page>
<bodyText confidence="0.999983730769231">
the unlimited and limited resource tracks, respectively.
It also includes a comparison with IBM Model 4, for
which we use a training sequence of IBM Model 1 (5
iterations), HMM (6 iterations), and IBM Model 4 (5 it-
erations). This sequence performed well in an evaluation
of the IBM Models (Och and Ney, 2003).
For comparative purposes, we show results of apply-
ing both P(fle) and P(elf) prior to symmetrization, along
with results of symmetrization. Comparison of the asym-
metric and symmetric results largely supports the hypoth-
esis presented in Section 2.3, as our system generally pro-
duces much better recall than IBM Model 4, while of-
fering a competitive precision. Our symmetrized results
usually produced higher recall and precision, and lower
alignment error rate.
We found that the largest gain in performance came
from the improved initialization. The combined distor-
tion model (Equation 4), which provided a small benefit
over the surface distortion model (Equation 2) on the de-
velopment set, performed slightly worse on the test set.
We found that the dependencies on C(eai_,) and
T (eai_,) were harmful to the P(f|e) alignment for Inukti-
tut, and did not submit results for the unlimited resources
configuration. However, we found that alignment was
generally difficult for all models on this particular task,
perhaps due to the agglutinative nature of Inuktitut.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999901">
We have proposed improvements to the largely over-
looked HMM word alignment model. Our improvements
yield good results on the workshop data. We have addi-
tionally shown that syntactic information can be incorpo-
rated into such a model; although the results are not su-
perior, they are competitive with surface distortion. In fu-
ture work we expect to explore additional parameteriza-
tions of the HMM model, and to perform extrinsic evalu-
ations of the resulting alignments by using them in the pa-
rameter estimation of a phrase-based translation model.
</bodyText>
<sectionHeader confidence="0.996494" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99878925">
This research was supported in part by ONR MURI Con-
tract FCPO.810548265. The authors would like to thank
Bill Byrne, David Chiang, Okan Kolak, and the anony-
mous reviewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.999319" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999708642857143">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz Josef Och,
David Purdy, Noah A. Smith, and David Yarowsky.
1999. Statistical machine translation: Final report. In
Johns Hopkins University 1999 Summer Workshop on
Language Engineering.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311, Jun.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In ACL Proceed-
ings, Jul.
Herve Dejean, Eric Gaussier, Cyril Goutte, and Kenji
Yamada. 2003. Reducing parameter space for word
alignment. In Proceedings of the Workshop on Build-
ing and Using Parallel Texts: Data Driven Machine
Translation and Beyond, pages 23–26, May.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on the Eval-
uation ofParsing Systems, May.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Proceedings of the
Workshop on Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond, pages 1–10,
May.
Robert C. Moore. 2004. Improving IBM word-
alignment model 1. In ACL Proceedings, pages 519–
526, Jul.
Franz Josef Och and Hermann Ney. 2000a. A compari-
son of alignment models for statistical machine trans-
lation. In COLING Proceedings, pages 1086–1090,
Jul.
Franz Josef Och and Hermann Ney. 2000b. Improved
statistical alignment models. In ACL Proceedings,
pages 440–447, Oct.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison on various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In EACL Proceedings,
pages 71–76, Jun.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to hmm-based statistical
word alignment models. In EMNLP, pages 87–94, Jul.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. Hmm-based word alignment in statistical ma-
chine translation. In COLING Proceedings, pages
836–841, Aug.
Dekai Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation, bracket-
ing, and alignment of parallel corpora. In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 1328–1335, Aug.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL Proceedings.
</reference>
<page confidence="0.99856">
86
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.789707">
<title confidence="0.978128">Improved HMM Alignment Models for Languages with Scarce Resources</title>
<author confidence="0.952502">Adam</author>
<affiliation confidence="0.992175666666667">Institute for Advanced Computer Department of Computer University of</affiliation>
<address confidence="0.982796">College Park, MD</address>
<email confidence="0.998819">alopez@cs.umd.edu</email>
<author confidence="0.935894">Philip</author>
<affiliation confidence="0.996582">Institute for Advanced Computer Department of University of</affiliation>
<address confidence="0.985618">College Park, MD</address>
<email confidence="0.999467">resnik@umiacs.umd.edu</email>
<abstract confidence="0.991826777777778">We introduce improvements to statistical word alignment based on the Hidden Markov Model. One improvement incorporates syntactic knowledge. Results on the workshop data show that alignment performance exceeds that of a state-of-the art system based on more complex models, resulting in over a 5.5% absolute reduction in error on Romanian-English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation: Final report.</title>
<date>1999</date>
<booktitle>In Johns Hopkins University 1999 Summer Workshop on Language Engineering.</booktitle>
<contexts>
<context position="1142" citStr="Al-Onaizan et al., 1999" startWordPosition="168" endWordPosition="171"> the workshop data show that alignment performance exceeds that of a state-of-the art system based on more complex models, resulting in over a 5.5% absolute reduction in error on Romanian-English. 1 Introduction The most widely used alignment model is IBM Model 4 (Brown et al., 1993). In empirical evaluations it has outperformed the other IBM Models and a Hidden Markov Model (HMM) (Och and Ney, 2003). It was the basis for a system that performed very well in a comparison of several alignment systems (Dejean et al., 2003; Mihalcea and Pedersen, 2003). Implementations are also freely available (Al-Onaizan et al., 1999; Och and Ney, 2003). The IBM Model 4 search space cannot be efficiently enumerated; therefore it cannot be trained directly using Expectation Maximization (EM). In practice, a sequence of simpler models such as IBM Model 1 and an HMM Model are used to generate initial parameter estimates and to enumerate a partial search space which can be expanded using hill-climbing heuristics. IBM Model 4 parameters are then estimated over this partial search space as an approximation to EM (Brown et al., 1993; Och and Ney, 2003). This approach yields good results, but it has been observed that the IBM Mod</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation: Final report. In Johns Hopkins University 1999 Summer Workshop on Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="803" citStr="Brown et al., 1993" startWordPosition="111" endWordPosition="114"> MD 20742 alopez@cs.umd.edu Philip Resnik Institute for Advanced Computer Studies Department of Linguistics University of Maryland College Park, MD 20742 resnik@umiacs.umd.edu Abstract We introduce improvements to statistical word alignment based on the Hidden Markov Model. One improvement incorporates syntactic knowledge. Results on the workshop data show that alignment performance exceeds that of a state-of-the art system based on more complex models, resulting in over a 5.5% absolute reduction in error on Romanian-English. 1 Introduction The most widely used alignment model is IBM Model 4 (Brown et al., 1993). In empirical evaluations it has outperformed the other IBM Models and a Hidden Markov Model (HMM) (Och and Ney, 2003). It was the basis for a system that performed very well in a comparison of several alignment systems (Dejean et al., 2003; Mihalcea and Pedersen, 2003). Implementations are also freely available (Al-Onaizan et al., 1999; Och and Ney, 2003). The IBM Model 4 search space cannot be efficiently enumerated; therefore it cannot be trained directly using Expectation Maximization (EM). In practice, a sequence of simpler models such as IBM Model 1 and an HMM Model are used to generate</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A probability model to improve word alignment.</title>
<date>2003</date>
<booktitle>In ACL Proceedings,</booktitle>
<contexts>
<context position="5455" citStr="Cherry and Lin, 2003" startWordPosition="922" endWordPosition="925">surface distortion model, it depends only on the distance ai − ai−1 and an automatically determined word class C(eai−1) as shown in Equation 2. It is similar to (Och and Ney, 2000a). The word class C(eai−1) is assigned using an unsupervised approach (Och, 1999). d(ai|ai−1) = p(ai|ai −ai−1,C(eai−1)) (2) The surface distortion model can capture local movement but it cannot capture movement of structures or the behavior of long-distance dependencies across translations. The intuitive appeal of capturing richer information has inspired numerous alignment models (Wu, 1995; Yamada and Knight, 2001; Cherry and Lin, 2003). However, we would like to retain the simplicity and good performance of the HMM Model. We introduce a distortion model which depends on the tree distance τ(ei,ek) = (w,x,y) between each pair of English words ei and ek. Given a dependency parse of eM1 , w and x represent the respective number of dependency links separating ei and ek from their closest common ancestor node in the parse tree. 2 The final element y = {1 1We ignore the sentence length probability p(MIN), which is not relevant to word alignment. We also omit discussion of HMM start and stop probabilities, and normalization of t(fi</context>
<context position="11510" citStr="Cherry and Lin, 2003" startWordPosition="1934" endWordPosition="1937">ion is simple: if we don’t permit NULL alignments, then we expect to produce a high-recall, lowprecision alignment; the intersection of two such alignments should mainly improve precision, resulting in a high-recall, high-precision alignment. If we allow NULL alignments, we may be able produce a high-precision, low-recall asymmetric alignment, but symmetrization by intersection will not improve recall. 3 Results with the Workshop Data In our experiments, the dependency parse and parts of speech are produced by minipar (Lin, 1998). This parser has been used in a much different alignment model (Cherry and Lin, 2003). Since we only had parses for English, we did not use tree distortion in the application of P(e|f), needed for symmetrization. The parameter settings that we used in aligning the workshop data are presented in Table 1. Although our prior work with English and French indicated that intersection was the best method for symmetrization, we found in development that this varied depending on the characteristics of the corpus and the type of annotation (in particular, whether the annotation set included probable alignments). The results are summarized in Table 2. It shows results with our HMM model </context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Colin Cherry and Dekang Lin. 2003. A probability model to improve word alignment. In ACL Proceedings, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herve Dejean</author>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
<author>Kenji Yamada</author>
</authors>
<title>Reducing parameter space for word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,</booktitle>
<pages>23--26</pages>
<contexts>
<context position="1044" citStr="Dejean et al., 2003" startWordPosition="154" endWordPosition="157">based on the Hidden Markov Model. One improvement incorporates syntactic knowledge. Results on the workshop data show that alignment performance exceeds that of a state-of-the art system based on more complex models, resulting in over a 5.5% absolute reduction in error on Romanian-English. 1 Introduction The most widely used alignment model is IBM Model 4 (Brown et al., 1993). In empirical evaluations it has outperformed the other IBM Models and a Hidden Markov Model (HMM) (Och and Ney, 2003). It was the basis for a system that performed very well in a comparison of several alignment systems (Dejean et al., 2003; Mihalcea and Pedersen, 2003). Implementations are also freely available (Al-Onaizan et al., 1999; Och and Ney, 2003). The IBM Model 4 search space cannot be efficiently enumerated; therefore it cannot be trained directly using Expectation Maximization (EM). In practice, a sequence of simpler models such as IBM Model 1 and an HMM Model are used to generate initial parameter estimates and to enumerate a partial search space which can be expanded using hill-climbing heuristics. IBM Model 4 parameters are then estimated over this partial search space as an approximation to EM (Brown et al., 1993</context>
</contexts>
<marker>Dejean, Gaussier, Goutte, Yamada, 2003</marker>
<rawString>Herve Dejean, Eric Gaussier, Cyril Goutte, and Kenji Yamada. 2003. Reducing parameter space for word alignment. In Proceedings of the Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, pages 23–26, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of minipar.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on the Evaluation ofParsing Systems,</booktitle>
<contexts>
<context position="11424" citStr="Lin, 1998" startWordPosition="1921" endWordPosition="1922">ymmetrize using intersection-based methods (Och and Ney, 2000b). The intuition is simple: if we don’t permit NULL alignments, then we expect to produce a high-recall, lowprecision alignment; the intersection of two such alignments should mainly improve precision, resulting in a high-recall, high-precision alignment. If we allow NULL alignments, we may be able produce a high-precision, low-recall asymmetric alignment, but symmetrization by intersection will not improve recall. 3 Results with the Workshop Data In our experiments, the dependency parse and parts of speech are produced by minipar (Lin, 1998). This parser has been used in a much different alignment model (Cherry and Lin, 2003). Since we only had parses for English, we did not use tree distortion in the application of P(e|f), needed for symmetrization. The parameter settings that we used in aligning the workshop data are presented in Table 1. Although our prior work with English and French indicated that intersection was the best method for symmetrization, we found in development that this varied depending on the characteristics of the corpus and the type of annotation (in particular, whether the annotation set included probable al</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of minipar. In Proceedings of the Workshop on the Evaluation ofParsing Systems, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="1074" citStr="Mihalcea and Pedersen, 2003" startWordPosition="158" endWordPosition="162">arkov Model. One improvement incorporates syntactic knowledge. Results on the workshop data show that alignment performance exceeds that of a state-of-the art system based on more complex models, resulting in over a 5.5% absolute reduction in error on Romanian-English. 1 Introduction The most widely used alignment model is IBM Model 4 (Brown et al., 1993). In empirical evaluations it has outperformed the other IBM Models and a Hidden Markov Model (HMM) (Och and Ney, 2003). It was the basis for a system that performed very well in a comparison of several alignment systems (Dejean et al., 2003; Mihalcea and Pedersen, 2003). Implementations are also freely available (Al-Onaizan et al., 1999; Och and Ney, 2003). The IBM Model 4 search space cannot be efficiently enumerated; therefore it cannot be trained directly using Expectation Maximization (EM). In practice, a sequence of simpler models such as IBM Model 1 and an HMM Model are used to generate initial parameter estimates and to enumerate a partial search space which can be expanded using hill-climbing heuristics. IBM Model 4 parameters are then estimated over this partial search space as an approximation to EM (Brown et al., 1993; Och and Ney, 2003). This app</context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>Rada Mihalcea and Ted Pedersen. 2003. An evaluation exercise for word alignment. In Proceedings of the Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, pages 1–10, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Improving IBM wordalignment model 1. In</title>
<date>2004</date>
<booktitle>ACL Proceedings,</booktitle>
<pages>519--526</pages>
<contexts>
<context position="7857" citStr="Moore (2004)" startWordPosition="1308" endWordPosition="1309">is framework. 2.2 Improving Initialization Our HMM produces reasonable results if we draw our initial parameter estimates from a uniform distribution. However, we can do better. We estimate the initial translation probability t(fj|ei) from the smoothed loglikelihood ratio LLR(ei, fj)01 computed over sentence cooccurrences. Since this method works well, we apply LLR(ei, fj) in a single reestimation step shown in Equation 5. LLR(f|e)02 +n t(f|e) = (5) ∑e0 LLR(f|e&apos;)02 +n·|V| In reestimation LLR(f |e) is computed from the expected counts of f and e produced by the EM algorithm. This is similar to Moore (2004); as in that work, |V |= 100,000, and φ1, φ2, and n are estimated on development data. We can also use an improved initial estimate for distortion. Consider a simple distortion model p(ai|ai − ai−1). We expect this distribution to have a maximum near P(ai|0) because we know that words tend to retain their locality across translation. Rather than wait for this to occur, we use an initial estimate for the distortion model given in Equation 6. I1 very2 much3 doubt4 that5 84 corpus n φ1 φ2 α symmetrization n−1 φ−1 φ−1 α−1 1 2 English-Inuktitut 1−4 1.0 1.75 -1.5 n 5−4 1.0 1.75 -1.5 Romanian-English</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. Improving IBM wordalignment model 1. In ACL Proceedings, pages 519– 526, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In COLING Proceedings,</booktitle>
<pages>1086--1090</pages>
<contexts>
<context position="4188" citStr="Och and Ney, 2000" startWordPosition="716" endWordPosition="719"> ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ Model 1 HMM Model 4 .35 .3 ◦ ◦◦ ◦ ◦ ◦ Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 83–86, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005 an asymmetry into the model because it constrains each French word to correspond to exactly one English word, while each English word is permitted to correspond to an arbitrary number of French words. Although the resulting set of links may still be relatively accurate, we can symmetrize by combining it with the set produced by applying the complementary model P(e|f) to the same data (Och and Ney, 2000b). Making a few independence assumptions we arrive at the decomposition in Equation 1. 1 M P(f,a|e) = ∏ d(ai|ai−1) ·t(fi|eai) (1) i=1 We refer to d(ai|ai−1) as the distortion model and t(fi|eai) as the translation model. Conveniently, Equation 1 is in the form of an HMM, so we can apply standard algorithms for HMM parameter estimation and maximization. This approach was proposed in Vogel et al. (1996) and subsequently improved (Och and Ney, 2000a; Toutanova et al., 2002). 2.1 The Tree Distortion Model Equation 1 is adequate in practice, but we can improve it. Numerous parameterizations have b</context>
<context position="8501" citStr="Och and Ney, 2000" startWordPosition="1422" endWordPosition="1425">0,000, and φ1, φ2, and n are estimated on development data. We can also use an improved initial estimate for distortion. Consider a simple distortion model p(ai|ai − ai−1). We expect this distribution to have a maximum near P(ai|0) because we know that words tend to retain their locality across translation. Rather than wait for this to occur, we use an initial estimate for the distortion model given in Equation 6. I1 very2 much3 doubt4 that5 84 corpus n φ1 φ2 α symmetrization n−1 φ−1 φ−1 α−1 1 2 English-Inuktitut 1−4 1.0 1.75 -1.5 n 5−4 1.0 1.75 -1.5 Romanian-English 5−4 1.5 1.0 -2.5 refined (Och and Ney, 2000b) 5−4 1.5 1.0 -2.5 English-Hindi 1−4 1.5 3.0 -2.5 U 1−2 1.0 1.0 -1.0 Table 1: Training parameters for the workshop data (see Section 2.2). Parameters n, φ1, φ2, and α were used in the initialization of P(f|e) model, while n−1, φ−1 1 , φ−1 2 , and α−1 were used in the initialization of the P(e|f) model. corpus type HMM limited (Eq. 2) HMM unlimited (Eq. 4) IBM Model 4 P R AER P R AER P R AER P(f|e) .4962 .6894 .4513 – – – .4211 .6519 .5162 English-Inuktitut P(e|f) .5789 .8635 .3856 – – – .5971 .8089 .3749 n .8916 .6280 .2251 – – – .8682 .5700 .2801 P(f|e) .5079 .4769 .5081 .5057 .4748 .5102 .5</context>
<context position="10294" citStr="Och and Ney (2000" startWordPosition="1737" endWordPosition="1740">h-Hindi, our self-reported results in the unlimited task are slightly lower than the original results submitted for the workshop, which contained an error. (6) We choose Z to normalize the distribution. We must optimize α on a development set. This distribution has a maximum when |ai − ai−1 |E {−1,0,1}. Although we could reasonably choose any of these three values as the maximum for the initial estimate, we found in development that the maximum of the surface distortion distribution varied with C(eai−1), although it was always in the range [−1,2]. 2.3 Does NULL Matter in Asymmetric Alignment? Och and Ney (2000a) introduce a NULL-alignment capability to the HMM alignment model. This allows any word fj to link to a special NULL word – by convention denoted e0 – instead of one of the words eN1 . A link (fj,e0) indicates that fj does not correspond to any word in e. This improved alignment performance in the absence of symmetrization, presumably because it allows the model to be conservative when evidence for an alignment is lacking. We hypothesize that NULL alignment is unnecessary for asymmetric alignment models when we symmetrize using intersection-based methods (Och and Ney, 2000b). The intuition i</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000a. A comparison of alignment models for statistical machine translation. In COLING Proceedings, pages 1086–1090, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In ACL Proceedings,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="4188" citStr="Och and Ney, 2000" startWordPosition="716" endWordPosition="719"> ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ Model 1 HMM Model 4 .35 .3 ◦ ◦◦ ◦ ◦ ◦ Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 83–86, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005 an asymmetry into the model because it constrains each French word to correspond to exactly one English word, while each English word is permitted to correspond to an arbitrary number of French words. Although the resulting set of links may still be relatively accurate, we can symmetrize by combining it with the set produced by applying the complementary model P(e|f) to the same data (Och and Ney, 2000b). Making a few independence assumptions we arrive at the decomposition in Equation 1. 1 M P(f,a|e) = ∏ d(ai|ai−1) ·t(fi|eai) (1) i=1 We refer to d(ai|ai−1) as the distortion model and t(fi|eai) as the translation model. Conveniently, Equation 1 is in the form of an HMM, so we can apply standard algorithms for HMM parameter estimation and maximization. This approach was proposed in Vogel et al. (1996) and subsequently improved (Och and Ney, 2000a; Toutanova et al., 2002). 2.1 The Tree Distortion Model Equation 1 is adequate in practice, but we can improve it. Numerous parameterizations have b</context>
<context position="8501" citStr="Och and Ney, 2000" startWordPosition="1422" endWordPosition="1425">0,000, and φ1, φ2, and n are estimated on development data. We can also use an improved initial estimate for distortion. Consider a simple distortion model p(ai|ai − ai−1). We expect this distribution to have a maximum near P(ai|0) because we know that words tend to retain their locality across translation. Rather than wait for this to occur, we use an initial estimate for the distortion model given in Equation 6. I1 very2 much3 doubt4 that5 84 corpus n φ1 φ2 α symmetrization n−1 φ−1 φ−1 α−1 1 2 English-Inuktitut 1−4 1.0 1.75 -1.5 n 5−4 1.0 1.75 -1.5 Romanian-English 5−4 1.5 1.0 -2.5 refined (Och and Ney, 2000b) 5−4 1.5 1.0 -2.5 English-Hindi 1−4 1.5 3.0 -2.5 U 1−2 1.0 1.0 -1.0 Table 1: Training parameters for the workshop data (see Section 2.2). Parameters n, φ1, φ2, and α were used in the initialization of P(f|e) model, while n−1, φ−1 1 , φ−1 2 , and α−1 were used in the initialization of the P(e|f) model. corpus type HMM limited (Eq. 2) HMM unlimited (Eq. 4) IBM Model 4 P R AER P R AER P R AER P(f|e) .4962 .6894 .4513 – – – .4211 .6519 .5162 English-Inuktitut P(e|f) .5789 .8635 .3856 – – – .5971 .8089 .3749 n .8916 .6280 .2251 – – – .8682 .5700 .2801 P(f|e) .5079 .4769 .5081 .5057 .4748 .5102 .5</context>
<context position="10294" citStr="Och and Ney (2000" startWordPosition="1737" endWordPosition="1740">h-Hindi, our self-reported results in the unlimited task are slightly lower than the original results submitted for the workshop, which contained an error. (6) We choose Z to normalize the distribution. We must optimize α on a development set. This distribution has a maximum when |ai − ai−1 |E {−1,0,1}. Although we could reasonably choose any of these three values as the maximum for the initial estimate, we found in development that the maximum of the surface distortion distribution varied with C(eai−1), although it was always in the range [−1,2]. 2.3 Does NULL Matter in Asymmetric Alignment? Och and Ney (2000a) introduce a NULL-alignment capability to the HMM alignment model. This allows any word fj to link to a special NULL word – by convention denoted e0 – instead of one of the words eN1 . A link (fj,e0) indicates that fj does not correspond to any word in e. This improved alignment performance in the absence of symmetrization, presumably because it allows the model to be conservative when evidence for an alignment is lacking. We hypothesize that NULL alignment is unnecessary for asymmetric alignment models when we symmetrize using intersection-based methods (Och and Ney, 2000b). The intuition i</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000b. Improved statistical alignment models. In ACL Proceedings, pages 440–447, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison on various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="922" citStr="Och and Ney, 2003" startWordPosition="132" endWordPosition="135">f Maryland College Park, MD 20742 resnik@umiacs.umd.edu Abstract We introduce improvements to statistical word alignment based on the Hidden Markov Model. One improvement incorporates syntactic knowledge. Results on the workshop data show that alignment performance exceeds that of a state-of-the art system based on more complex models, resulting in over a 5.5% absolute reduction in error on Romanian-English. 1 Introduction The most widely used alignment model is IBM Model 4 (Brown et al., 1993). In empirical evaluations it has outperformed the other IBM Models and a Hidden Markov Model (HMM) (Och and Ney, 2003). It was the basis for a system that performed very well in a comparison of several alignment systems (Dejean et al., 2003; Mihalcea and Pedersen, 2003). Implementations are also freely available (Al-Onaizan et al., 1999; Och and Ney, 2003). The IBM Model 4 search space cannot be efficiently enumerated; therefore it cannot be trained directly using Expectation Maximization (EM). In practice, a sequence of simpler models such as IBM Model 1 and an HMM Model are used to generate initial parameter estimates and to enumerate a partial search space which can be expanded using hill-climbing heuristi</context>
<context position="12560" citStr="Och and Ney, 2003" startWordPosition="2117" endWordPosition="2120">type of annotation (in particular, whether the annotation set included probable alignments). The results are summarized in Table 2. It shows results with our HMM model using both Equations 2 and 4 as our distortion model, which represent � |ai − ai−1|α/Z,α &lt; 0 if ai =� ai−1. d(ai|ai−1) = 1/Z ai ai−1. if = 85 the unlimited and limited resource tracks, respectively. It also includes a comparison with IBM Model 4, for which we use a training sequence of IBM Model 1 (5 iterations), HMM (6 iterations), and IBM Model 4 (5 iterations). This sequence performed well in an evaluation of the IBM Models (Och and Ney, 2003). For comparative purposes, we show results of applying both P(fle) and P(elf) prior to symmetrization, along with results of symmetrization. Comparison of the asymmetric and symmetric results largely supports the hypothesis presented in Section 2.3, as our system generally produces much better recall than IBM Model 4, while offering a competitive precision. Our symmetrized results usually produced higher recall and precision, and lower alignment error rate. We found that the largest gain in performance came from the improved initialization. The combined distortion model (Equation 4), which pr</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison on various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In EACL Proceedings,</booktitle>
<pages>71--76</pages>
<contexts>
<context position="5095" citStr="Och, 1999" startWordPosition="871" endWordPosition="872"> algorithms for HMM parameter estimation and maximization. This approach was proposed in Vogel et al. (1996) and subsequently improved (Och and Ney, 2000a; Toutanova et al., 2002). 2.1 The Tree Distortion Model Equation 1 is adequate in practice, but we can improve it. Numerous parameterizations have been proposed for the distortion model. In our surface distortion model, it depends only on the distance ai − ai−1 and an automatically determined word class C(eai−1) as shown in Equation 2. It is similar to (Och and Ney, 2000a). The word class C(eai−1) is assigned using an unsupervised approach (Och, 1999). d(ai|ai−1) = p(ai|ai −ai−1,C(eai−1)) (2) The surface distortion model can capture local movement but it cannot capture movement of structures or the behavior of long-distance dependencies across translations. The intuitive appeal of capturing richer information has inspired numerous alignment models (Wu, 1995; Yamada and Knight, 2001; Cherry and Lin, 2003). However, we would like to retain the simplicity and good performance of the HMM Model. We introduce a distortion model which depends on the tree distance τ(ei,ek) = (w,x,y) between each pair of English words ei and ek. Given a dependency </context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz Josef Och. 1999. An efficient method for determining bilingual word classes. In EACL Proceedings, pages 71–76, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>H Tolga Ilhan</author>
<author>Christopher D Manning</author>
</authors>
<title>Extensions to hmm-based statistical word alignment models.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>87--94</pages>
<contexts>
<context position="4664" citStr="Toutanova et al., 2002" startWordPosition="796" endWordPosition="799">ly accurate, we can symmetrize by combining it with the set produced by applying the complementary model P(e|f) to the same data (Och and Ney, 2000b). Making a few independence assumptions we arrive at the decomposition in Equation 1. 1 M P(f,a|e) = ∏ d(ai|ai−1) ·t(fi|eai) (1) i=1 We refer to d(ai|ai−1) as the distortion model and t(fi|eai) as the translation model. Conveniently, Equation 1 is in the form of an HMM, so we can apply standard algorithms for HMM parameter estimation and maximization. This approach was proposed in Vogel et al. (1996) and subsequently improved (Och and Ney, 2000a; Toutanova et al., 2002). 2.1 The Tree Distortion Model Equation 1 is adequate in practice, but we can improve it. Numerous parameterizations have been proposed for the distortion model. In our surface distortion model, it depends only on the distance ai − ai−1 and an automatically determined word class C(eai−1) as shown in Equation 2. It is similar to (Och and Ney, 2000a). The word class C(eai−1) is assigned using an unsupervised approach (Och, 1999). d(ai|ai−1) = p(ai|ai −ai−1,C(eai−1)) (2) The surface distortion model can capture local movement but it cannot capture movement of structures or the behavior of long-d</context>
</contexts>
<marker>Toutanova, Ilhan, Manning, 2002</marker>
<rawString>Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Manning. 2002. Extensions to hmm-based statistical word alignment models. In EMNLP, pages 87–94, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillman</author>
</authors>
<title>Hmm-based word alignment in statistical machine translation.</title>
<date>1996</date>
<booktitle>In COLING Proceedings,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="4593" citStr="Vogel et al. (1996)" startWordPosition="785" endWordPosition="788">ch words. Although the resulting set of links may still be relatively accurate, we can symmetrize by combining it with the set produced by applying the complementary model P(e|f) to the same data (Och and Ney, 2000b). Making a few independence assumptions we arrive at the decomposition in Equation 1. 1 M P(f,a|e) = ∏ d(ai|ai−1) ·t(fi|eai) (1) i=1 We refer to d(ai|ai−1) as the distortion model and t(fi|eai) as the translation model. Conveniently, Equation 1 is in the form of an HMM, so we can apply standard algorithms for HMM parameter estimation and maximization. This approach was proposed in Vogel et al. (1996) and subsequently improved (Och and Ney, 2000a; Toutanova et al., 2002). 2.1 The Tree Distortion Model Equation 1 is adequate in practice, but we can improve it. Numerous parameterizations have been proposed for the distortion model. In our surface distortion model, it depends only on the distance ai − ai−1 and an automatically determined word class C(eai−1) as shown in Equation 2. It is similar to (Och and Ney, 2000a). The word class C(eai−1) is assigned using an unsupervised approach (Och, 1999). d(ai|ai−1) = p(ai|ai −ai−1,C(eai−1)) (2) The surface distortion model can capture local movement</context>
</contexts>
<marker>Vogel, Ney, Tillman, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillman. 1996. Hmm-based word alignment in statistical machine translation. In COLING Proceedings, pages 836–841, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1328--1335</pages>
<contexts>
<context position="5407" citStr="Wu, 1995" startWordPosition="916" endWordPosition="917">d for the distortion model. In our surface distortion model, it depends only on the distance ai − ai−1 and an automatically determined word class C(eai−1) as shown in Equation 2. It is similar to (Och and Ney, 2000a). The word class C(eai−1) is assigned using an unsupervised approach (Och, 1999). d(ai|ai−1) = p(ai|ai −ai−1,C(eai−1)) (2) The surface distortion model can capture local movement but it cannot capture movement of structures or the behavior of long-distance dependencies across translations. The intuitive appeal of capturing richer information has inspired numerous alignment models (Wu, 1995; Yamada and Knight, 2001; Cherry and Lin, 2003). However, we would like to retain the simplicity and good performance of the HMM Model. We introduce a distortion model which depends on the tree distance τ(ei,ek) = (w,x,y) between each pair of English words ei and ek. Given a dependency parse of eM1 , w and x represent the respective number of dependency links separating ei and ek from their closest common ancestor node in the parse tree. 2 The final element y = {1 1We ignore the sentence length probability p(MIN), which is not relevant to word alignment. We also omit discussion of HMM start a</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. 1995. Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 1328–1335, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In ACL Proceedings.</booktitle>
<contexts>
<context position="5432" citStr="Yamada and Knight, 2001" startWordPosition="918" endWordPosition="921">distortion model. In our surface distortion model, it depends only on the distance ai − ai−1 and an automatically determined word class C(eai−1) as shown in Equation 2. It is similar to (Och and Ney, 2000a). The word class C(eai−1) is assigned using an unsupervised approach (Och, 1999). d(ai|ai−1) = p(ai|ai −ai−1,C(eai−1)) (2) The surface distortion model can capture local movement but it cannot capture movement of structures or the behavior of long-distance dependencies across translations. The intuitive appeal of capturing richer information has inspired numerous alignment models (Wu, 1995; Yamada and Knight, 2001; Cherry and Lin, 2003). However, we would like to retain the simplicity and good performance of the HMM Model. We introduce a distortion model which depends on the tree distance τ(ei,ek) = (w,x,y) between each pair of English words ei and ek. Given a dependency parse of eM1 , w and x represent the respective number of dependency links separating ei and ek from their closest common ancestor node in the parse tree. 2 The final element y = {1 1We ignore the sentence length probability p(MIN), which is not relevant to word alignment. We also omit discussion of HMM start and stop probabilities, an</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In ACL Proceedings.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>