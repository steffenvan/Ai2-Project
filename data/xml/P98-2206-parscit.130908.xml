<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.777171285714286">
Chinese Word Segmentation
without Using Lexicon and Hand-crafted Training Data
Sun Maosong, Shen Dayang*, Benjamin K Tsou**
State Key Laboratory of Intelligent Technology and Systems, Tsinghua University, Beijing, China
Email: lkc-dcs@mail tsinghu a. edu. cn
* Computer Science Institute, Shantou University, Guangdong, China
** Language Information Sciences Research Centre, City University of Hong Kong, Hong Kong
</note>
<sectionHeader confidence="0.977339" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885411764706">
Chinese word segmentation is the first step in any
Chinese NLP system. This paper presents a new
algorithm for segmenting Chinese texts without
making use of any lexicon and hand-crafted
linguistic resource. The statistical data required by
the algorithm, that is, mutual information and the
difference of t-score between characters, is
derived automatically from raw Chinese corpora.
The preliminary experiment shows that the
segmentation accuracy of our algorithm is
acceptable. We hope the gaining of this approach
will be beneficial to improving the
performance(especially in ability to cope with
unknown words and ability to adapt to various
domains) of the existing segmenters, though the
algorithm itself can also be utilized as a stand-alone
segmenter in some NLP applications.
</bodyText>
<sectionHeader confidence="0.997607" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999802019230769">
Any Chinese word is composed of either single
or multiple characters. Chinese texts are explicitly
concatenations of characters, words are not
delimited by spaces as that in English. Chinese
word segmentation is therefore the first step for any
Chinese information processing system[1].
Almost all methods for Chinese word
segmentation developed so far, both statistical and
rule-based, exploited two kinds of important
resources, i.e., lexicon and hand-crafted linguistic
resources(manually segmented and tagged corpus,
knowledge for unknown words, and linguistic
This work was supported in part by the National
Natural Science Foundation of China under grant
No. 69433010.
rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as
the means for finding segmentation candidates for
input sentences, while linguistic resources for
solving segmentation ambiguities. Preparation of
these resources (well-defined lexicon, widely
accepted tag set, consistent annotated corpus etc.)
is very hard due to particularity of Chinese, and
time consuming. Furthermore, even the lexicon is
large enough, and the corpus annotated is balanced
and huge in size, the word segmenter will still face
the problem of data incompleteness, sparseness and
bias as it is utilized in different domains.
An important issue in designing Chinese
segmenters is thus how to reduce the effort of
human supervision as much as possible.
Palmer(1997) conducted a Chinese segmenter
which merely made use of a manually segmented
corpus(without referring to any lexicon). A
transformation-based algorithm was then explored
to learn segmentation rules automatically from the
segmented corpus. Sproat and Shih(1993) further
proposed a method using neither lexicon nor
segmented corpus: for input texts, simply grouping
character pairs with high value of mutual
information into words. Although this strategy is
very simple and has many limitations(e.g., it can
only treat bi-character words) , the characteristic of
it is that it is fully automatic -- the mutual
information between characters can be trained from
raw Chinese corpus directly.
Following the line of Sproat and Shih, here we
present a new algorithm for segmenting Chinese
texts which depends upon neither lexicon nor any
hand-crafted resource. All data necessary for our
system is derived from the raw corpus. The system
may be viewed as a stand-alone segmenter in some
applications (preliminary experiments show that its
</bodyText>
<page confidence="0.982169">
1265
</page>
<bodyText confidence="0.9999746">
accuracy is acceptable); nevertheless, our main
purpose is to study how and how well the work can
be done by machine at the extreme conditions, say,
without any assistance of human. We believe the
performance of the existing Chinese segmenters,
that is, the ability to deal with segmentation
ambiguities and unknown words as well as the
ability to adapt to new domains, will be improved
in some degree if the gaining of this approach is
incorporated into systems properly.
</bodyText>
<sectionHeader confidence="0.994519" genericHeader="introduction">
2. Principle
</sectionHeader>
<subsectionHeader confidence="0.9937725">
2.1. Mutual information and difference of
t-score between characters
</subsectionHeader>
<bodyText confidence="0.999944833333333">
Mutual information and t-score, two
important concepts in information theory and
statistics, have been exploited to measure the
degree of association between two words in an
English corpus[4]. We adopt these measures
almost completely here, with one major
modification: the variables in two relevant formulae
are no longer words but Chinese characters.
Definition 1 Given a Chinese character string txy&apos;,
the mutual information between characters x and
y(or equally, the mutual information of the
location between x and y) is defined as:
</bodyText>
<equation confidence="0.877021">
mi(x: y) = log, P(x)P(Y)
</equation>
<bodyText confidence="0.999810333333333">
where p(x,y) is the co-occurrence probability of x
and y, and p(x), p(y) are the independent
probabilities of x and y respectively.
As claimed by Church(1991), the larger the
mutual information between x and y, the higher the
possibility of x and y being combined together. For
</bodyText>
<equation confidence="0.783883">
example: Atklef..o.gfavm
h (I)
</equation>
<bodyText confidence="0.999827">
The distribution of mi(x:y) for sentence (1) is
illustrated in Fig. 1(where &amp;quot;*&amp;quot; denotes x, y should
be combined and &amp;quot;IV be separated in terms of
human judgment. This convention will be effective
throughout the paper). The correct segmentation
for (1) can be achieved when we decide that every
location between x and y in the sentence be treated
as &apos;combined&apos; or &apos;separated&apos; accordingly if its mi
value is greater than or below a threshold(suppose
the threshold is 3.0 for this example):
</bodyText>
<equation confidence="0.526027">
I *11g I 14 I
</equation>
<bodyText confidence="0.940728818181818">
economy cooperation will be
Xst I 1=1 I I Ioo I a* I
for current world economy trend
of an appropriate answer
(Economic cooperation will be an
appropriate answer to the trend of economics
in current world)
It is evident that x and y are to be strongly
combined together if mi(x:y)&gt;&gt;0 and to be
separated if mi(x:y)&lt;&lt;0. But if mi(x:y) 0, the
association of x and y becomes uncertain.
</bodyText>
<equation confidence="0.65554975">
Observe the mi distribution for sentence (2) in
Fig. 2:
&amp;QM( 4/4;3=M4-EfkffiAllA9f
a*. (2)
</equation>
<bodyText confidence="0.992379">
In the region of 2.0 mi &lt;4.0, there exist
some confusions: we have mi( .0: ff)&gt;
mi(ff: 0), mi(7T:14&gt; mi(0 4)&gt; mi(a•
and mi(&apos; 71-: E)&gt;if), however, &amp;quot;3/1_;.&apos;&amp;quot;:
E&amp;quot;should be separated and &amp;quot;/z..;.:
be combined by human
judgment -- the power of mi is somewhat weak in
</bodyText>
<figure confidence="0.818746">
. 10
4
• connect
a break
</figure>
<page confidence="0.405088">
2
</page>
<bodyText confidence="0.508990333333333">
ft- gf fr i&apos;,1 gf — h
gf fi 44 A RI 1=1 n t 57- .t1 gf — 1:1
Fig.I The distribution of mi(sentence I) Character pairs in sentence
</bodyText>
<page confidence="0.829101">
1266
</page>
<figure confidence="0.931197">
• connect
II break
4
LE MA M4 39f 44&apos;7AE E EV5 50i MA tff 3:Bit AIM
</figure>
<figureCaption confidence="0.787712">
Fig.2 The distribution of mi(sentence 2) Character pairs in sentence
</figureCaption>
<bodyText confidence="0.9996108">
the &apos;intermediate&apos; range of its value. To solve this
problem, we need to seek other ways additionally.
Definition 2 Given a Chinese character string
ixyzi. the t-score of the character y relevant to
characters x and z is defined as:
</bodyText>
<equation confidence="0.98986">
PO 190114
-jvar(p(zjy)) + var(p(yj x))
</equation>
<bodyText confidence="0.88142756">
where p(yjx) is the conditional probability of y
given x, and p(zly), of z given y, and var(p(y1x)),
var(p(zly)) are variances of p(ylx) and of p(zly)
respectively.
Also as pointed out by Church(1991), tsx,, (y)
indicates the binding tendency of y in the context of
x and z:
if p(zly)&gt; p(y1x), or tsx,x(y) &gt; 0
then y tends to be bound with z rather
than with x
if p(y1x)&gt; p(zly), or (y) &lt; 0
then y tends to be bound with x rather
than with z
A distinct feature of ts is that it is context-
dependent (a relative measure), along with certain
degree of flexibility to the context, whereas mi is
context-independent (an absolute measure). Its
drawback is it attaches to a character rather than to
the location between two adjacent characters. This
may cause some inconvenience if we want to unify
it with mi. We initially introduce a new measure dts
instead of ts:
Definition 3 Given a Chinese character string
ivxywi, the difference oft-score between characters
x and y is defined as:
</bodyText>
<equation confidence="0.900789">
dts(x: y) = tsv.y(x)— tsx.„,(y)
Now dts(x: y) is allocated to the location
</equation>
<bodyText confidence="0.990332666666667">
between x and y, just like mi (x: y) . And the
context of dts(x: y) becomes 4 characters, 1
character larger than that of tsx,,(y) .
The value of dts(x: y) reflects the
competition results among four adjacent characters
v, x, y and w:
</bodyText>
<equation confidence="0.879649">
(1) ts (x) &gt; 0 ts x,.(y) &lt; 0
v,y
</equation>
<bodyText confidence="0.944603">
(x tends to combine with y, and y tends to
combine with x) ==&gt; dts(x:y)&gt; 0
In this case, x and y attract each other. The
location between x and y should be bound.
</bodyText>
<listItem confidence="0.852679">
(2) ts, (x) &lt; 0 ts, (y)&gt; 0
</listItem>
<bodyText confidence="0.8089088">
(x tends to combine with v, and y tends to
combine with w) ==&gt; dts(x:y)&lt; 0
0 ® --3-*
In this case, x and y repel each other. The
location between x and y should be separated.
</bodyText>
<equation confidence="0.813046">
(3a) ts„,), (x) &gt; 0 tsx,.(y) &gt; 0
(x tends to combine with y, whereas y tends
to combine with w)
0 ® *
(3b) tsv,), (x) &lt; 0 ts(y) &lt; 0
</equation>
<bodyText confidence="0.920276166666667">
(x tends to combine with v, whereas y tends
to combine with x)
0
In cases of (3a) and (3b), the status of the
location between x and y is determined by the
competition of tsv,), (x) and ts(y) :
</bodyText>
<construct confidence="0.821961">
if dts(x: y) &gt; 0 then it tends to be bound
if dts(x:y)&lt; 0 then it tends to be separated
</construct>
<page confidence="0.950464">
1267
</page>
<figure confidence="0.983312153846154">
200
dts
150
100
50
•
• connect
break
• •:
-50
St •
-100 •
&amp;IN 111K1 R43 3Tft 1€•-&lt;7`. ZEE E.V V5 5311 3fR At*
</figure>
<figureCaption confidence="0.560471">
Fig.3 The distribution of dts(sentence 2) Character pairs in sentence
</figureCaption>
<bodyText confidence="0.996850333333333">
The general rule governing dts is similar as
that governing mi: the higher the difference of t-
score between x and y, the stronger the
combination strength between them, and vice versa.
But the role of dts is somewhat different from that
of mi: it is capable of complementing the &apos;blind
area of mi on some occasions.
Consider sentence (2) again. The distribution
of (its for it is shown in Fig. 3. Return to the
character pairs whose mi values fall into the region
of 2.0 mi &lt; 4.0 in Fig. 2, compare their dts
values accordingly: dts( A: ,Y)&gt; dts(: A)&gt;
</bodyText>
<listItem confidence="0.7573605">
dts(.2TF: cits(g: &gt; dts(X:7)&gt; dts(0.: 4‘) ,
and dts(Ij•: JD&gt; E) -- the conclusion
</listItem>
<bodyText confidence="0.919686">
drawn from these comparisons is very close to the
human judgment.
</bodyText>
<subsectionHeader confidence="0.987874">
2.2. Local maximum and local minimum
of dts
</subsectionHeader>
<bodyText confidence="0.998420857142857">
Most of the character pairs in sentence (2)
have got satisfactory explanations by their mi and
dts so far. &amp;quot; g : &amp;quot;F:&amp;quot;are two of few
exceptions. We have mi 9&gt; Ini(3f:4) and
dts(g: A9&gt; dts( JT: 0), however, the human
judgment is the former should be separated and the
latter be bound. Aiming at this, we further
proposed two new concepts, that is, local maximum
and local minimum of dts.
Definition 4 Given &apos;vxyw&apos; a Chinese character
string, dts(x:y) is said to be a local maximum if
dts(x.y)&gt; dts(v:x) and dts(x:y)&gt; dts(y:w). And,
the height of the local maximum dts(x:y) is defined
as:
</bodyText>
<equation confidence="0.995369">
h(dts(x:y)) = min ( dts(x:y)— dts(v:x),
dts(x:y)— dts(y:w) }
</equation>
<bodyText confidence="0.9897728">
Definition 5 Given &apos;vxyw&apos; a Chinese character
string, dts(x:y) is said to be a local minimum if
dts(x:y)&lt; dts(v:x) and dts(x:y) &lt; dts(y:w). And,
the depth of the local minimum dts(x:y) is defined
as:
</bodyText>
<equation confidence="0.9605835">
d(dts(x:y)) = min ( dts(v:x)— dts(x:y),
dts(y:w)— dts(x:y)}
</equation>
<bodyText confidence="0.991552285714286">
Two basic hypotheses can be easily made as
the consequence of context-dependability of
dts(note: mi has not such property):
Hypothesis 1 x and y tends to be bound if dts(x:y)
is a local maximum, regardless of the value of
dts(x:y)(even it is low).
Hypothesis 2 x and y tends to be separated if
dts(x:y) is a local minimum, regardless of the value
of dts(x:y) (even it is high).
In Fig. 3, dts(g: A) is a local minimum
whereas dts(fi: 02 isn&apos;t. At least we can say that
&amp;quot;4: &amp;quot; is likely to be separated, as suggested by
the hypothesis 2(though we still can say nothing
more about &amp;quot;TF:4&amp;quot;).
</bodyText>
<subsectionHeader confidence="0.9866185">
2.3. The second local maximum and the
second local minimum of dts
</subsectionHeader>
<bodyText confidence="0.980157777777778">
We continue to define other four related
concepts:
Definition 6 Suppose &apos;vxyzw&apos; is a Chinese
character string, and dts(x:y) is a local maximum.
Then dts(y:z) is said to be the right second local
maximum of dts(x:y) if dts(y:z)&gt; dts(v:x) and
dts(y:z)&gt; dts(z:w).And, the distance between the
local maximum and the second local maximum is
defined as:
</bodyText>
<equation confidence="0.5589065">
dis(locmax, y:z) = dts(x:y)— dts(y:z)
Definition 7 Suppose &apos;vxyzw&apos; is a Chinese
</equation>
<page confidence="0.954897">
1268
</page>
<bodyText confidence="0.997980333333333">
character string, and dts(x:y) is a local minimum.
Then dts(y:z) is said to be the right second local
minimum of dts(x:y) if dts(y:z)&lt; dts(v:x) and
dts(y:z)&lt; dts(z:w). And, the distance between the
local minimum and the second local minimum is
defined as:
</bodyText>
<equation confidence="0.80229">
dis(locmin, y:z) = dts(y:z)— dts(x:y)
</equation>
<bodyText confidence="0.980584">
The left second local maximum and the left
second local minimum of dts(x:y) can be defined
similarly.
Refer to Fig. 3. By definition, clis(if: A is the
left second local minimum of dts(: 4), and
dts(X: A) is the right second local maximum of
dts( : ) meanwhile the left second local
minimum of dts(A? E).
These four measures are designed to deal with
two common construction types in Chinese word
formation: &amp;quot;2 characters + 1 character&amp;quot; and
&amp;quot;1 character + 2 characters&amp;quot;. We will skip the
discussion about this due to the limited volume of
the paper.
</bodyText>
<sectionHeader confidence="0.969342" genericHeader="method">
3. Algorithm
</sectionHeader>
<bodyText confidence="0.908894066666667">
The basic idea is to try to integrate all of the
measures introduced in section 2 together into an
algorithm, making best use of the advantages and
bypassing the disadvantages of them under
different conditions.
Given an input sentence S, let
pm,: the mean of mi of all locations in S;
: the standard deviation of mi of all
locations in S;
P dr.,: the mean of dts of all locations in S;
(in fact, iid„. 0)
o-dis : the standard deviation of dts of all
locations in S
we divide the distribution graphs of mi and dts
of S into several regions(4 regions for each graph)
</bodyText>
<equation confidence="0.840471">
by / C r Mi Pd tli and Cr dts:
region A dts(x:y)&gt;
dr.,
region B 0 &lt; dts(x:
ars
region C - Gras. &lt; 0
region D dts(x:y) -%C.- cr eh;
region a mi(x:y)&gt; pm, + cr mi
region b pm, &lt; pm, + 0m1
region c pm; — 0mi &lt; pm,
region d pm, — m,
</equation>
<bodyText confidence="0.83891">
The algorithm scans the input sentence S from
left to right two times:
The first round for S
</bodyText>
<figure confidence="0.700894714285714">
For any location (x:y) in S, do
1. in cases that &lt;dts(x:y), mi(x:y)&gt; falls into:
1.1 Aa or Ba or Ca or Da or Ab
mark (x:y) &apos;bound&apos;
1.2 Ad or Bd or Cd or Dd or Dc
mark (x:y) &apos;separated&apos;
1.3 Ac or Cb
</figure>
<construct confidence="0.910949333333333">
if dts(x:y) is local maximum then
if h(dts(x:y))&gt; 81
then mark (x:y) &apos;bound&apos; else &amp;quot;?&apos;
if dts(x:y) is local minimum then
if d(dts(x:y))&gt;
then mark (x:y) &apos;separated&apos; else &amp;quot;?&apos;
1.4 Bc or Db
if dts(x:y) is local maximum then
if h(dts(x:y))&gt; 82
then mark (x:y) &apos;bound&apos; else &amp;quot;?&apos;
if dts(x:y) is local minimum then
if d(dts(x:y))&gt;
</construct>
<figure confidence="0.977519461538462">
then mark (x:y) &apos;separated&apos; else &amp;quot;?&apos;
1.5 Cc
if (dts(x:y) is local maximum) and
(h(dts(x:y))&gt; 83)
then mark (x:y) &apos;bound&apos; else &amp;quot;?&apos;
if dts(x:y) is local minimum
then mark (x:y) &apos;separated&apos; else &amp;quot;?&apos;
1.6 Bb
if dts(x:y) is local maximum
then mark (x:y) &apos;bound&apos; else &amp;quot;?&apos;
if (dts(x:y) is local minimum) and
(d(dts(x:y))&gt; )
then mark (x:y) &apos;separated&apos; else &amp;quot;?&apos;
</figure>
<listItem confidence="0.900512">
2. For (x:y) unmarked so far, mark it as
except that:
</listItem>
<equation confidence="0.645964941176471">
if dts(x:y) is the second local maximum
then if dis(locmax, x:y) &lt;
0.5 X Irmin(loc,x:y)
/* Refer to the notations in definition 6&amp;7.
Irmin(loc,x:y) =min {Idts(x:y)— dts(v:x)I,
Idts(x:y)— dts(z:w)1} *I
1269
then mark (x:y) if
(x:y) is the right second local max
or if
(x:y) is the left second local max
if dts(x:y) is the second local minimum
then if dis(locmin, x:y) &lt;
0.5 X lrmin(loc,x:y)
then mark (x:y) —&apos; if
(x:y) is the right second local min
or if
</equation>
<bodyText confidence="0.851688333333333">
(x:y) is the left second local min
The second round for S
if (x:y) is marked &amp;quot;?&apos;
then if mi(x:y)...?- 0
then mark (x:y) &apos;bound&apos; else &apos;separated&apos;
if (x:y) is marked
then the status of (x:y) follows that of
the adjacent location on the left side
if (x:y) is marked
then the status of (x:y) follows that of
the adjacent location on the right side
(The constants 81,are
</bodyText>
<equation confidence="0.564349333333333">
2 3 1 ; 2 3
determined by experiments, satisfying:
81 &lt; 82 &lt; 83 ;1&lt;2&lt;3
</equation>
<bodyText confidence="0.997124620689655">
and 0 =2.5)
Generally speaking, the lower the &lt;dts(x:y),
mi(x:y)&gt; in distribution graphs, the more restrictive
the constraints. Take &apos;bound&apos; operation as example:
there is not any additional condition in case 1.1; in
case 1.6 however, the existence of a local
maximum is needed; in case 1.3, a requirement for
the height of local maximum is added; in case 1.4,
the height required becomes even higher; and in
case 1.5, which is the worst case for &apos;bound&apos;
operation, the height must be high enough.
Case 2 says if the second local maximum is
pretty near to the local maximum corresponded,
then its status (&apos;bound&apos; or &apos;separated&apos;) would be
likely to be consistent with that of the local
maximum. So does the second local minimum.
Finally, for locations marked &amp;quot;?&apos; with which
we have no more means to cope, simply make
decisions by the value of mi(1,ve set it to 2.5, same
as that in the system of Sproat and Shih(1993)).
Recall sentence (2). The character pair &amp;quot;-R:
.{.±&amp;quot; is regarded as &apos;separated&apos; successfully by
following E.&amp;quot;(local minimum) with the rule in
case 2 although its mi value is rather high(3.4). &amp;quot;}f:
is marked &apos;T in the first round and treated
properly by 0 in the second round.
The algorithm outputs the correct
segmentation for sentence (2) at last:
France tennis competition today
</bodyText>
<sectionHeader confidence="0.727081" genericHeader="method">
EV IN1113
</sectionHeader>
<bodyText confidence="0.883334">
in Paris the western suburbs
</bodyText>
<figure confidence="0.3986175">
TF d4
open curtain
</figure>
<figureCaption confidence="0.414127">
(The Tennis Competition of France opened in
the western suburbs of Paris today.)
</figureCaption>
<bodyText confidence="0.9993544">
Note that there exist two ambiguous fragments
or &amp;quot;z )f&amp;quot;) and &amp;quot;fATFR
or &amp;quot;k ffa I 4&amp;quot;), as well
as two proper nouns &amp;quot;France&amp;quot; and &amp;quot;Paris&amp;quot; in
sentence (2).
</bodyText>
<sectionHeader confidence="0.993972" genericHeader="evaluation">
4. Experimental results
</sectionHeader>
<bodyText confidence="0.995121458333333">
We select 100 Chinese sentences, consisting of
1588 characters (or 1587 locations between
character pairs) randomly as testing texts. The
statistical data required by calculating mi and dts,
in fact it is character bigram, is automatically
derived from a news corpus of about 20M Chinese
characters. The testing texts and training corpus
are mutually excluded.
Out of 1587 locations in the testing texts,
1456 are correctly marked by our algorithm.
We define the accuracy of segmentation as:
of locations being correctly marked
# of locations in texts
Then, the accuracy for testing texts is
1456/1587 = 91.75%.
The distribution of local maximum, local
minimum and other types of dts value(involving the
second local maximum and the second local
minimum) of the testing texts over &lt;dts, mi&gt;
regions is summarized in Fig. 4 (Fig. 5 is the same
distribution in percentage representation). This
would be helpful for readers to understand our
algorithm.
Future work includes: (1) enlarging the size of
</bodyText>
<page confidence="0.962044">
1270
</page>
<bodyText confidence="0.999966285714286">
experiments; (2) refining the algorithm by studying
the relationship between mi and dts in depth; and (3)
integrating it as a module with the existing Chinese
segmenters so as to improve their performance
(especially in ability to cope with unknown words
and ability to adapt to various domains). -- it is
indeed the ultimate goal of our research here.
</bodyText>
<sectionHeader confidence="0.997785" genericHeader="conclusions">
5. Acknowledgments
</sectionHeader>
<bodyText confidence="0.9744806">
This work benefited a lot from discussions
with Professor Huang Changning of Tsinghua
University, Bejing, China. We would also like to
thank anonymous COLING-ACL&apos;98 reviewers for
their helpful comments.
</bodyText>
<figure confidence="0.977269444444445">
SUO9R001 Jo # 250 0 Others
200 • LocMin
150 LocMax
100
50
0
Aa Ab Ac Ad Ba Bb Be Bd Ca Cb Cc Cd Da Db Dc Dd
Fig.4 The distribution of dts types in testing texts Region
100%
&amp;quot;cr.! 60%
40%
20%
0%
0 Others
• LocMin
LocMax
Aa Ab Ac Ad Ba Bb Be Bd Ca Cb Cc Cd Da Db Dc Dd
Fig.5 The distribution of dts types in testing texts Region
</figure>
<sectionHeader confidence="0.975537" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999664658536585">
[1] Liang N.Y.. &amp;quot;CDWS: An Automatic Word
Segmentation System for Written Chinese Texts&amp;quot;,
Journal of Chinese Information Processing, Vol.1,
No.2, 1987 (in Chinese)
[2] Fan C.K.,Tsai W.H.. &amp;quot;Automatic Word
Identification in Chinese Sentences by the
Relaxation Technique&amp;quot;, Computer Processing of
Chinese &amp; Oriental Languages, Vol.4, No.!, 1988
[3] Yao T.S., Zhang G.P., Wu Y.M., &amp;quot;A Rule-
based Chinese Word Segmentation System&amp;quot;,
Journal of Chinese Information Processing, Vol.4,
No.1, 1990 (in Chinese)
[4] Church K.W., Hanks P., Hindle D., &amp;quot;Using
Statistics in Lexical Analysis&amp;quot;, In Lexical
Acquisition: Exploiting On-line Resources to
Build a Lexicon, edited by U. Zernik, Hillsdale,
N.J.:Erlbaum, 1991
[5] Chan K.J., Liu S.H., &amp;quot;Word Identification for
Mandarin Chinese Sentences&amp;quot;, Proc. of COLING-
92. Nantes, 1992
[6] Sun M.S., Lai B.Y., Lun S., Sun C.F., &amp;quot;Some
Issues on Statistical Approach to Chinese Word
Identification&amp;quot;, Proc. of the 3rd International
Conference on Chinese Information Processing,
Beijing, 1992
[7] Sproat R., Shih C.L., &amp;quot;A Statistical Method
for Finding Word Boundaries in Chinese Text&amp;quot;,
Computer Processing of Chinese and Oriental
Languages, No.4, 1993
[8] Sproat R. et al, &amp;quot;A Stochastic Finite-State
Word Segmentation Algorithm for Chinese&amp;quot;, Proc.
of the 32nd Annual Meeting of ACL, New Mexico,
1994
[9] Palmer D.D., &amp;quot;A Trainable Rule-based
Algorithm for Word Segmentation&amp;quot;, Proc. of the
35th Annual Meeting of ACL and 8th Conftrence
of the European Chapter of ACL, Madrid, 1997
[10] Sun M.S., Shen D.Y., Huang C.N.,
&amp;quot;CSeg&amp;Tag1.0: A Practical Word Segmenter and
POS Tagger for Chinese Texts&amp;quot;, Proc. of the 6th
ANLP, Washington D.C., 1997
</reference>
<page confidence="0.991294">
1271
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.128754">
<title confidence="0.9569945">Chinese Word Segmentation without Using Lexicon and Hand-crafted Training Data</title>
<author confidence="0.457004">Sun Maosong</author>
<author confidence="0.457004">Shen Dayang</author>
<author confidence="0.457004">Benjamin K Tsou</author>
<address confidence="0.96291">State Key Laboratory of Intelligent Technology and Systems, Tsinghua University, Beijing, China</address>
<email confidence="0.980339">lkc-dcs@mailtsinghua.edu.cn</email>
<note confidence="0.3645235">Computer Science Institute, Shantou University, Guangdong, China ** Language Information Sciences Research Centre, City University of Hong Kong, Hong Kong</note>
<abstract confidence="0.998078611111111">Chinese word segmentation is the first step in any Chinese NLP system. This paper presents a new algorithm for segmenting Chinese texts without making use of any lexicon and hand-crafted linguistic resource. The statistical data required by the algorithm, that is, mutual information and the difference of t-score between characters, is derived automatically from raw Chinese corpora. The preliminary experiment shows that the segmentation accuracy of our algorithm is acceptable. We hope the gaining of this approach will be beneficial to improving the performance(especially in ability to cope with unknown words and ability to adapt to various domains) of the existing segmenters, though the algorithm itself can also be utilized as a stand-alone segmenter in some NLP applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Y Liang</author>
</authors>
<title>CDWS: An Automatic Word Segmentation System for Written Chinese Texts&amp;quot;,</title>
<date>1987</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>1</volume>
<note>(in Chinese)</note>
<contexts>
<context position="1512" citStr="[1]" startWordPosition="218" endWordPosition="218">hope the gaining of this approach will be beneficial to improving the performance(especially in ability to cope with unknown words and ability to adapt to various domains) of the existing segmenters, though the algorithm itself can also be utilized as a stand-alone segmenter in some NLP applications. 1. Introduction Any Chinese word is composed of either single or multiple characters. Chinese texts are explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these reso</context>
</contexts>
<marker>[1]</marker>
<rawString>Liang N.Y.. &amp;quot;CDWS: An Automatic Word Segmentation System for Written Chinese Texts&amp;quot;, Journal of Chinese Information Processing, Vol.1, No.2, 1987 (in Chinese)</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Fan</author>
<author>W H Tsai</author>
</authors>
<title>Automatic Word Identification in Chinese Sentences by the Relaxation Technique&amp;quot;,</title>
<date>1988</date>
<booktitle>Computer Processing of Chinese &amp; Oriental Languages,</booktitle>
<location>Vol.4, No.!,</location>
<contexts>
<context position="1926" citStr="[1,2,3,5,6,8,9,10]" startWordPosition="273" endWordPosition="273">ts are explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desig</context>
</contexts>
<marker>[2]</marker>
<rawString>Fan C.K.,Tsai W.H.. &amp;quot;Automatic Word Identification in Chinese Sentences by the Relaxation Technique&amp;quot;, Computer Processing of Chinese &amp; Oriental Languages, Vol.4, No.!, 1988</rawString>
</citation>
<citation valid="true">
<authors>
<author>T S Yao</author>
<author>G P Zhang</author>
<author>Y M Wu</author>
</authors>
<title>A Rulebased Chinese Word Segmentation System&amp;quot;,</title>
<date>1990</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>4</volume>
<note>(in Chinese)</note>
<contexts>
<context position="1926" citStr="[1,2,3,5,6,8,9,10]" startWordPosition="273" endWordPosition="273">ts are explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desig</context>
</contexts>
<marker>[3]</marker>
<rawString>Yao T.S., Zhang G.P., Wu Y.M., &amp;quot;A Rulebased Chinese Word Segmentation System&amp;quot;, Journal of Chinese Information Processing, Vol.4, No.1, 1990 (in Chinese)</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
<author>D Hindle</author>
</authors>
<title>Using Statistics in Lexical Analysis&amp;quot;, In Lexical Acquisition: Exploiting On-line Resources to Build a Lexicon, edited by U. Zernik,</title>
<date>1991</date>
<location>Hillsdale, N.J.:Erlbaum,</location>
<contexts>
<context position="4420" citStr="[4]" startWordPosition="654" endWordPosition="654"> say, without any assistance of human. We believe the performance of the existing Chinese segmenters, that is, the ability to deal with segmentation ambiguities and unknown words as well as the ability to adapt to new domains, will be improved in some degree if the gaining of this approach is incorporated into systems properly. 2. Principle 2.1. Mutual information and difference of t-score between characters Mutual information and t-score, two important concepts in information theory and statistics, have been exploited to measure the degree of association between two words in an English corpus[4]. We adopt these measures almost completely here, with one major modification: the variables in two relevant formulae are no longer words but Chinese characters. Definition 1 Given a Chinese character string txy&apos;, the mutual information between characters x and y(or equally, the mutual information of the location between x and y) is defined as: mi(x: y) = log, P(x)P(Y) where p(x,y) is the co-occurrence probability of x and y, and p(x), p(y) are the independent probabilities of x and y respectively. As claimed by Church(1991), the larger the mutual information between x and y, the higher the po</context>
</contexts>
<marker>[4]</marker>
<rawString>Church K.W., Hanks P., Hindle D., &amp;quot;Using Statistics in Lexical Analysis&amp;quot;, In Lexical Acquisition: Exploiting On-line Resources to Build a Lexicon, edited by U. Zernik, Hillsdale, N.J.:Erlbaum, 1991</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Chan</author>
<author>S H Liu</author>
</authors>
<title>Word Identification for Mandarin Chinese Sentences&amp;quot;,</title>
<date>1992</date>
<booktitle>Proc. of COLING92.</booktitle>
<location>Nantes,</location>
<contexts>
<context position="1926" citStr="[1,2,3,5,6,8,9,10]" startWordPosition="273" endWordPosition="273">ts are explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desig</context>
</contexts>
<marker>[5]</marker>
<rawString>Chan K.J., Liu S.H., &amp;quot;Word Identification for Mandarin Chinese Sentences&amp;quot;, Proc. of COLING92. Nantes, 1992</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Sun</author>
<author>B Y Lai</author>
<author>S Lun</author>
<author>C F Sun</author>
</authors>
<title>Some Issues on Statistical Approach to Chinese Word Identification&amp;quot;,</title>
<date>1992</date>
<booktitle>Proc. of the 3rd International Conference on Chinese Information Processing,</booktitle>
<location>Beijing,</location>
<contexts>
<context position="1926" citStr="[1,2,3,5,6,8,9,10]" startWordPosition="273" endWordPosition="273">ts are explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desig</context>
</contexts>
<marker>[6]</marker>
<rawString>Sun M.S., Lai B.Y., Lun S., Sun C.F., &amp;quot;Some Issues on Statistical Approach to Chinese Word Identification&amp;quot;, Proc. of the 3rd International Conference on Chinese Information Processing, Beijing, 1992</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>C L Shih</author>
</authors>
<title>A Statistical Method for Finding Word Boundaries in Chinese Text&amp;quot;,</title>
<date>1993</date>
<booktitle>Computer Processing of Chinese and Oriental Languages, No.4,</booktitle>
<marker>[7]</marker>
<rawString>Sproat R., Shih C.L., &amp;quot;A Statistical Method for Finding Word Boundaries in Chinese Text&amp;quot;, Computer Processing of Chinese and Oriental Languages, No.4, 1993</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Sproat</author>
</authors>
<title>A Stochastic Finite-State Word Segmentation Algorithm for Chinese&amp;quot;,</title>
<booktitle>Proc. of the 32nd Annual Meeting of ACL,</booktitle>
<location>New Mexico,</location>
<contexts>
<context position="1926" citStr="[1,2,3,5,6,8,9,10]" startWordPosition="273" endWordPosition="273">ts are explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desig</context>
</contexts>
<marker>[8]</marker>
<rawString>Sproat R. et al, &amp;quot;A Stochastic Finite-State Word Segmentation Algorithm for Chinese&amp;quot;, Proc. of the 32nd Annual Meeting of ACL, New Mexico,</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Palmer</author>
</authors>
<title>A Trainable Rule-based Algorithm for Word Segmentation&amp;quot;,</title>
<date>1997</date>
<booktitle>Proc. of the 35th Annual Meeting of ACL and 8th Conftrence of the European Chapter of ACL,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="1926" citStr="[1,2,3,5,6,8,9,10]" startWordPosition="273" endWordPosition="273">ts are explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desig</context>
</contexts>
<marker>[9]</marker>
<rawString>Palmer D.D., &amp;quot;A Trainable Rule-based Algorithm for Word Segmentation&amp;quot;, Proc. of the 35th Annual Meeting of ACL and 8th Conftrence of the European Chapter of ACL, Madrid, 1997</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Sun</author>
<author>D Y Shen</author>
<author>C N Huang</author>
</authors>
<title>CSeg&amp;Tag1.0: A Practical Word Segmenter and POS Tagger for Chinese Texts&amp;quot;,</title>
<date>1997</date>
<booktitle>Proc. of the 6th ANLP,</booktitle>
<location>Washington D.C.,</location>
<contexts>
<context position="1926" citStr="[1,2,3,5,6,8,9,10]" startWordPosition="273" endWordPosition="273">ts are explicitly concatenations of characters, words are not delimited by spaces as that in English. Chinese word segmentation is therefore the first step for any Chinese information processing system[1]. Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. rules)[1,2,3,5,6,8,9,10]. Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segmentation ambiguities. Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming. Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains. An important issue in desig</context>
</contexts>
<marker>[10]</marker>
<rawString>Sun M.S., Shen D.Y., Huang C.N., &amp;quot;CSeg&amp;Tag1.0: A Practical Word Segmenter and POS Tagger for Chinese Texts&amp;quot;, Proc. of the 6th ANLP, Washington D.C., 1997</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>