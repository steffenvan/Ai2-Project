<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999478">
A Scalable Decoder for Parsing-based Machine Translation
with Equivalent Language Model State Maintenance
</title>
<author confidence="0.998425">
Zhifei Li and Sanjeev Khudanpur
</author>
<affiliation confidence="0.9124815">
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
</affiliation>
<email confidence="0.991503">
zhifei.work@gmail.com and khudanpur@jhu.edu
</email>
<sectionHeader confidence="0.994218" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999759761904762">
We describe a scalable decoder for parsing-
based machine translation. The decoder is
written in JAVA and implements all the es-
sential algorithms described in Chiang (2007):
chart-parsing, m-gram language model inte-
gration, beam- and cube-pruning, and unique
k-best extraction. Additionally, parallel
and distributed computing techniques are ex-
ploited to make it scalable. We also propose
an algorithm to maintain equivalent language
model states that exploits the back-off prop-
erty of m-gram language models: instead of
maintaining a separate state for each distin-
guished sequence of “state” words, we merge
multiple states that can be made equivalent for
language model probability calculations due
to back-off. We demonstrate experimentally
that our decoder is more than 30 times faster
than a baseline decoder written in PYTHON.
We propose to release our decoder as an open-
source toolkit.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99900975">
Large-scale parsing-based statistical machine trans-
lation (MT) has made remarkable progress in the
last few years. The systems being developed differ
in whether they use source- or target-language syn-
tax. For instance, the hierarchical translation sys-
tem of Chiang (2007) extracts a synchronous gram-
mar from pairs of strings, Quirk et al. (2005), Liu et
al. (2006) and Huang et al. (2006) perform syntac-
tic analyses in the source-language, and Galley et al.
(2006) use target-language syntax.
A critical component in parsing-based MT sys-
tems is the decoder, which is complex to imple-
</bodyText>
<page confidence="0.96884">
10
</page>
<bodyText confidence="0.999933514285714">
ment and scale up. Most of the systems described
above employ tailor-made, dedicated decoders that
are not open-source, which results in a high barrier
to entry for other researchers in the field. How-
ever, with the algorithms proposed in (Huang and
Chiang, 2005; Chiang, 2007; Huang and Chiang,
2007), it is possible to develop a general-purpose de-
coder that can be used by all the parsing-based sys-
tems. In this paper, we describe an important first-
step towards an extensible, general-purpose, scal-
able, and open-source parsing-based MT decoder.
Our decoder is written in JAVA and implements all
the essential algorithms described in Chiang (2007):
chart-parsing, m-gram language model integration,
beam- and cube-pruning, and unique k-best extrac-
tion. Additionally, parallel and distributed comput-
ing techniques are exploited to make it scalable.
Straightforward integration of an m-gram lan-
guage model (LM) into a parsing-based decoder
substantially increases its computational complex-
ity. Therefore, it is important to develop efficient
methods for LM integration. We propose an algo-
rithm to maintain equivalent LM states by exploit-
ing the back-off property of m-gram LMs. Specifi-
cally, instead of maintaining a separate state for each
distinguished sequence of “state” words, we merge
multiple states that can be made equivalent for LM
calculations by anticipating such back-off.
We demonstrate experimentally that our decoder
is 38 times faster than a previous decoder written in
PYTHON. Furthermore, the distributed computing
permits improving translation quality via large-scale
LMs. We have successfully use our decoder to trans-
late about a million sentences in a parallel corpus for
large-scale discriminative training experiments.
</bodyText>
<note confidence="0.9782975">
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 10–18,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.930315" genericHeader="method">
2 Parsing-based MT Decoder
</sectionHeader>
<bodyText confidence="0.999924">
In this section, we discuss the core algorithms imple-
mented in our decoder. These algorithms have been
discussed by Chiang (2007) in detail, and we reca-
pitulate the essential parts here for completeness.
</bodyText>
<subsectionHeader confidence="0.965163">
2.1 Grammar Formalism
</subsectionHeader>
<bodyText confidence="0.999917428571429">
Our decoder assumes a probabilistic synchronous
context-free grammar (SCFG). Following the nota-
tion in Venugopal et al. (2007), a probabilistic SCFG
comprises a set of source-language terminal sym-
bols TS, a set of target-language terminal symbols
TT, a shared set of nonterminal symbols N, and a
set of rules of the form
</bodyText>
<equation confidence="0.993561">
X —* (γ, α, —, w) , (1)
</equation>
<bodyText confidence="0.999528714285714">
where X E N, γ E [NUTS]* is a (mixed) sequence
of nonterminals and source terminals, α E [NUTT]*
is a sequence of nonterminals and target terminals,
— is a one-to-one correspondence or alignment be-
tween the nonterminal elements of γ and α, and
w &gt; 0 is a weight assigned to the rule. An illus-
trative rule for Chinese-to-English translation is
</bodyText>
<equation confidence="0.79578">
NP —* ( NP0 n, NP1 , NP1 of NP0 ) ,
</equation>
<bodyText confidence="0.999944625">
where the Chinese word n, (pronounced de or di)
means of, and the alignment, encoded via subscripts
on the nonterminals, causes the two noun phrases
around n, to be reordered around of in the transla-
tion. The rule weight is omitted in this example.
A bilingual SCFG derivation is analogous to a
monolingual CFG derivation. It begins with a pair
of aligned start symbols. At each step, an aligned
pair of nonterminals is rewritten as the two corre-
sponding components of a single rule. In this sense,
the derivations are generated synchronously.
Our decoder presently handles SCFGs of the kind
extracted by Heiro (Chiang, 2007), but is easily ex-
tensible to more general SCFGs and closely related
formalisms such as synchronous tree substitution
grammars (Eisner, 2003; Chiang, 2006).
</bodyText>
<subsectionHeader confidence="0.992493">
2.2 MT Decoding as Chart Parsing
</subsectionHeader>
<bodyText confidence="0.7884595">
Given a source-language sentence f*, the decoder
must find the target-language yield e(D) of the best
derivation D among all derivations with source-
language yield f(D) = f*, i.e.
</bodyText>
<equation confidence="0.982852">
e* = e Carg max w(D)) , (2)
</equation>
<bodyText confidence="0.999088166666667">
where w(D) is the composite weight of D.
The parser may be treated as a deductive proof
system (Shieber et al., 1995). Formally (cf. (Chiang,
2007)), a parser defines a space of weighted items,
with some items designated as axioms and some as
goals, and a set of inference rules of the form
</bodyText>
<equation confidence="0.990853">
I1 : w1 ··· Ik : wk
</equation>
<bodyText confidence="0.999579083333333">
which states that if all the antecedent items Ii are
provable, respectively with weight wi, then the con-
sequent item I is provable with weight w, provided
the side condition φ holds. For a grammar with a
maximum of two (pairs of) nonterminals per rule1,
Figure 1 illustrates the resulting chart parsing proce-
dure, including the integration of an m-gram LM.
The actual decoding algorithm maintains a chart,
which contains an array of cells. Each cell in turn
maintains a list of proved items. The parsing process
starts with the axioms, and proceeds by applying the
inference rules to prove more and more items until
a goal item is proved. Whenever the parser proves a
new item, it adds the item to the appropriate chart
cell. It also maintains backpointers to antecedent
items, which are used for k-best extraction, as dis-
cussed in Section 2.4 below.
In a SCFG-based decoder, an item is identi-
fied by its source-language span, left-side non-
terminal label, and left- and right-context for the
target-language m-gram LM. Therefore, in a given
cell, the maximum possible number of items is
O(|N||TT|2(m−1)), and the worst case decoding
complexity is
</bodyText>
<equation confidence="0.97242">
�|N|K |TT|2K(m−1)n3�
O ,(3)
</equation>
<bodyText confidence="0.990888">
where K is the maximum number of nonterminal
pairs per rule and n is the source-language sentence
length (Venugopal et al., 2007).
</bodyText>
<footnote confidence="0.983239">
1For more general grammars with K &gt; 2 pairs of non-
terminals per rule, see Venugopal et al. (2007).
</footnote>
<equation confidence="0.854649857142857">
φ ,
I : w
11
X→(γ,α):w (X (&apos;y, α, w)) E G
X→(fji+1, α) : w
[X, i, j; q(α)] : wp(α)
Z→(fi1 0 = α[e1/X]
i+1Xfj j1+1, α) : w [X,i1,j1;e1] : w1
α
[Z, i, j; q(α0)] : ww1p(α0)
Z→(fi+1X1f�i+1Y2fjj2+1, α) : w [X,i1,j1;e1] : w1 [Y,i2,j2;e2] : w2 0 = α[e1/X1, e2/Y2]
α
[Z, i, j; q(α0)] : ww1w2p(α0)
Goal item: [S, 0, n; (s)m−1 * e(/s)]
</equation>
<figureCaption confidence="0.537967">
Figure 1: Inference rules from Chiang (2007) for a parser with an m-gram LM. G denotes the translation grammar.
w[x/X] denotes substitution of the string x for the symbol X in the string w. The function p(·) provides the LM
probability for all complete m-grams in a string, while the function q(·) elides symbols whose m-grams have been
accounted for by p(·). Details about the functions p(·) and q(·) are provided in Section 4.
</figureCaption>
<subsectionHeader confidence="0.997784">
2.3 Pruning in a Decoder
</subsectionHeader>
<bodyText confidence="0.983462375">
Severe pruning is needed in order to make the decod-
ing computationally feasible for SCFGs with large
vocabularies TT and detailed nonterminal sets. In
our decoder, we incorporate two pruning techniques
described by (Chiang, 2007; Huang and Chiang,
2007). For beam pruning, in each cell, we discard
all items whose weight is worse, by a relative thresh-
old Q, than the weight of the best item in the same
cell. If too many items pass the threshold, a cell only
retains the top-b items by weight. When combining
smaller items to obtain a larger item by applying an
inference rule, we use cube-pruning to simulate k-
best extraction in each destination cell, and discard
combinations that lead to an item whose weight is
worse than the best item in that cell by a margin of
E.
</bodyText>
<subsectionHeader confidence="0.999575">
2.4 k-best Extraction Over Hyper-graphs
</subsectionHeader>
<bodyText confidence="0.9998901">
For each source-language sentence f*, the output
of the chart-parsing algorithm may be treated as a
hyper-graph representing a set of likely hypotheses
D in (2). Briefly, a hyper-graph is a set of vertices
and hyper-edges, with each hyper-edge connecting
a set of antecedent vertices to a consequent vertex,
and a special vertex designated as the target vertex.
In parsing parlance, a vertex corresponds to an item
in the chart, a hyper-edge corresponds to a SCFG
rule with the nonterminals on the right-side replaced
by back-pointers to antecedent items, and the target
vertex corresponds to the goal item2.
Given a hyper-graph for a source-language sen-
tence f*, we use the k-best extraction algorithm of
Huang and Chiang (2005) to extract its k most likely
translations. Moreover, since many different deriva-
tions D in (2) may lead to the same target-language
yield e(D), we adopt the modification described in
Huang et al. (2006) to efficiently generate the unique
k best translations of f*.
</bodyText>
<sectionHeader confidence="0.979969" genericHeader="method">
3 Parallel and Distributed Computing
</sectionHeader>
<bodyText confidence="0.97608752631579">
Many applications of parsing-based MT entail the
use of SCFGs extracted from millions of bilin-
gual sentence pairs and LMs extracted from bil-
lions of words of target-language text. This requires
the decoder to make use of distributed computing
to spread the memory required to load large-scale
SCFGs and LMs onto multiple processors. Further-
more, techniques such as iterative minimum error-
rate training (Och et al., 2003) as well as web-based
MT services require the decoder to translate a large
number of source-language sentences per unit time.
This requires the decoder to make use of parallel
computing to utilize each individual multi-core pro-
cessor more effectively. We have incorporated two
such performance enhancements in our decoder.
2In a decoder integrating an m-gram LM, there may be mul-
tiple goal items due to different LM contexts. However, one can
image a single goal item identified by the span [0, n] and the
goal nonterminal S, but not by the LM contexts.
</bodyText>
<page confidence="0.992413">
12
</page>
<subsectionHeader confidence="0.998252">
3.1 Parallel Decoding
</subsectionHeader>
<bodyText confidence="0.999995076923077">
We have enhanced our decoder to translate multiple
source-language sentences in parallel by exploiting
the ability of a multi-core processor to concurrently
run several threads that share memory. Specifi-
cally, given one (or more) document(s) containing
multiple source-language sentences, the decoder au-
tomatically splits the set of sentences into several
subsets, and initiates concurrent decoding threads;
once all the threads finish, the main thread merges
back the translations. Since all the threads naturally
share memory, the decoder needs to load the (large)
SCFG and LM into memory only once. This multi-
threading provides a very significant speed-up.
</bodyText>
<subsectionHeader confidence="0.997538">
3.2 Distributed Language Models
</subsectionHeader>
<bodyText confidence="0.999312647058823">
It is not possible in some cases to load a very large
LM into memory on a single machine, particularly
if the SCFG is also very large. In other cases, load-
ing the LM each time the decoder runs may be too
time-consuming relative to the time required for de-
coding itself, such as in iterative decoding with up-
dated combination weights during minimum error-
rate training. It is therefore desirable to have dedi-
cated servers to load parts of the LM3 — an idea that
has been exploited by (Zhang et al., 2006; Emami et
al., 2007; Brants et al., 2007).
Our implementation can load a (partitioned) LM
on different servers before initiating decoding. The
decoder remotely calls the servers to obtain individ-
ual LM probabilities, and linearly interpolates them
on the fly using a given set of interpolation weights.
With this architecture, one can deal with a very large
target-language text corpus by splitting it into many
parts and training separate LMs from each. The run-
time interpolation capability may also be used for
LM adaptation, e.g. for building document-specific
language models.
To mitigate potential network communication de-
lays inherent to a distributed LM, we implement a
simple cache mechanism in the decoder. The cache
saves the outcomes of the most recent LM calls,
including interpolated LM probabilities; the cache
is reset whenever its size exceeds a threshold. We
could have maintained a cache at each LM server
as well; however, the resultant saving is not signif-
3Similarly, distributing the SCFG is also possible.
icant because the trie data-structures used to imple-
ment m-gram LMs are quite fast relative to the cache
lookup overhead.
</bodyText>
<sectionHeader confidence="0.81321" genericHeader="method">
4 Equivalent LM-state Maintenance
</sectionHeader>
<bodyText confidence="0.999942125">
It is clear from the complexity (3) of the inference
rules (Figure 1) that a straightforward integration
of an m-gram LM adds a multiplicative factor of
|TT |2K(m−1) to the computational complexity of the
decoder, where TT is the set of target-language ter-
minal symbols. We illustrate in this section how this
potentially very large multiplier can be dramatically
reduced by exploiting the structure of the LM.
</bodyText>
<subsectionHeader confidence="0.999197">
4.1 Applying an m-gram LM in the Decoder
</subsectionHeader>
<bodyText confidence="0.999986428571429">
Integrating an LM into chart parsing requires two
functions p(·) and q(·) (see Figure 1) that oper-
ate on strings over TT U {*}, where * is a special
“placeholder” symbol for an elided part of a target-
language string.
The function p(e) calculates the LM probability
of the complete m-grams in e - e1 ... el, i.e.
</bodyText>
<equation confidence="0.995979">
PLM(ei  |hi) , (4)
m &lt; i &lt; l &amp; *Oea_(M_1)
</equation>
<bodyText confidence="0.9997185">
where hi = ei−(m−1) ... ei−1 is the m−1-word
“LM history” of the target-language word ei.
Since the p-probability of e does not include the
LM probability for the partial m-grams (i.e., the first
(m − 1) words) of e, the exact weights of two items
[X, i, j; e] and [X, i, j; e&apos;] in the chart are not avail-
able during the bottom-up pruning of Section 2.3.
Therefore, as an approximation, we also compute
</bodyText>
<equation confidence="0.974200333333333">
min{m−1, jell
p(e) = H PLM(ek  |e1 ... ek−1), (5)
k=1
</equation>
<bodyText confidence="0.987020571428571">
an estimate of the LM probability of the m−1-gram
prefix of e. This estimated probability is taken into
account for pruning purposes (only).
The function q(e1 ... el) determines the left and
right LM states that must be maintained for future
computation of the exact LM probability, respec-
tively, of e1 ... em−1 and el+1 ... el+m−1.
</bodyText>
<equation confidence="0.974663555555556">
q(e1 ... el) (6)
�e1 ... el if &lt; m − 1,
...em−1
el−(m−2)
e1
*
... el otherwise.
p(e1 ... el) =
=
</equation>
<page confidence="0.992252">
13
</page>
<subsectionHeader confidence="0.770188">
4.2 Back-off Parameterization of m-gram LMs
</subsectionHeader>
<bodyText confidence="0.9996685">
While many different methods are popular for esti-
mating m-gram LMs, most store the estimated LM
parameters in the ARPA back-off file format; using
the notation eji to denote a target-language word se-
quence ei ei+1 ... ej, the LM probability calcula-
tion is carried out as
</bodyText>
<equation confidence="0.831666777777778">
PBO(em  |em−1
1 ) (7)
π(em1 ) if em1 ∈ LM
β(em−1
1 )×
|em−1
2 )
PBO(em
otherwise,
</equation>
<bodyText confidence="0.8736635">
where the lower order probability
is recursively defined in the same way, and
is the back-off weight of the history. The LM file
contains the parameter
</bodyText>
<figure confidence="0.453455352941176">
for each listed m-gram,
and the parameters
and
for each listed
1
&lt; m; for unlisted
= 1
by definition.
Observe from (7) that if
is not listed in the LM,
the back-off weight
is the same for all words
em, and the backed-off probability PBO(em
is the
same for all words
Furthermore, as m grows, the
fraction of possible m-grams actuall
</figure>
<equation confidence="0.9366958125">
PBO(em|em−1
2 )
β(em−1
1 )
π(·)
π(·)
β(·)
-m-
gram,
≤m-
-m-grams,β(·)
em1
β(·)
|·)
e1.
y observed in a
</equation>
<bodyText confidence="0.813805444444444">
training corpus diminishes rapidly.
We merge multiple LM states (6) that already
have—or back-off to—the same
in
the calculation (7) of LM probabilities, e.g. due
to different unlisted m-grams that back-off to the
same
For simplicity, we only consider
LM state merging by the function
</bodyText>
<equation confidence="0.2353055">
of (6) when
l
</equation>
<bodyText confidence="0.622246571428572">
Though the equivalent LM state maintenance
technique is discussed here in the context of a
parsing-based MT decoder, it is also applicable to
standard left-to-right phrase-based decoders. In par-
ticular, the right-side equivalent LM state mainte-
nance proposed in Section 4.3.1 may be used.
4.3.1 Obtaining the Equivalent Right LM State
Recall that the right LM state
of
serves as the
for calculating the ex-
act LM probabilities of the yet-to-be-determined
word
Recall further the computation (7) of
</bodyText>
<listItem confidence="0.806827">
• If the m-gram
</listItem>
<bodyText confidence="0.4787355">
is not listed in the LM
for any word
</bodyText>
<equation confidence="0.97742">
then the LM will back-off to
“LMhistory”
m−1-gram.
q(·)
≥m−1.
ell−(m−2)
el1
“LMhistory”
el+1.
PBO(el+1  |el l−(m−2)).
el+1
l−(m−2)
el+1,
PBO(el+1|ell−(m−3)),
el−(m−2).
m−1-gramell−(m−2)
β(ell−(m−2))=1.
q(·)
el−(m−2)
el1.
−
−
ell−(m−2)+i, where
i ∈ [0, m − 2], leading to the equivalent right state
</equation>
<bodyText confidence="0.987946571428571">
computation procedure of Figure 2. The procedure
=
which does not depend
on the word
IS-A-PREFIX(e�m1 ) checks if its argument e
m-grams (due to reordering and
translation combinations) that do not appear in natu-
ral texts, leading to frequent back-off during the LM
probability calculation (7). In this subsection, we
propose a method to collapse equivalent LM states
so that the decoder effectively considers many more
items in each cell
“unseen”
without increasing beam size.
</bodyText>
<subsectionHeader confidence="0.992367">
4.3 The Equivalent LM State of an Item
</subsectionHeader>
<bodyText confidence="0.999786076923077">
The maximum possible number of items in a cell in-
creases exponentially with the LM order m, as dis-
cussed in Section 2.2. With pruning (cf. Section
2.3), we restrict the maximum number of items in
each cell to some threshold b. Intuitively, therefore,
if we increase the LM order m, we should also in-
crease the beam size b to reduce search errors. This
could slow down the decoder significantly.
Recall from the previous subsection, however,
that when m increases, the fraction of m-grams
that will need to back-off also increases. Moreover,
even for modest values of m, the decoder consid-
ers many
</bodyText>
<page confidence="0.990776">
14
</page>
<figure confidence="0.616826636363637">
• If the
also is not listed in
the LM, then
If these two conditions hold true,
may safely
elide the word
in (6) no matter what words
follow
The right LM state is thus reduced from
m
1 words tom
</figure>
<footnote confidence="0.276412">
2 words.
</footnote>
<bodyText confidence="0.841798214285714">
The argument above can be applied recursively
to the resulting right LM state
i` is a pre-
fix of any k-gram listed in the LM, k
m].
4.3.2 Obtaining the Equivalent Left LM State
Recall that the left LM state
of
is
the prefix whose exact LM probability is unknown
during bottom-up parsing, and is replaced by the
estimated probability
of (5) for pruning
purposes. Recall further the computation (7) of
</bodyText>
<listItem confidence="0.951965">
• If the m-gram
</listItem>
<bodyText confidence="0.675798333333333">
is not listed in the LM
for any word
then it will
</bodyText>
<equation confidence="0.9736810625">
∈[-m,
em−1
1
el1
�p(em−1
1 )
PBO(em−1|em−2
0 ).
em−1
0
e0,
back-off to
EQ-R-STATE (ell−(m−2))
1 ers ← ell−(m−2)
2 for i ← 0 to m − 2 ✄ left to right
3 if IS-A-PREFIX (ell−(m−2)+i)
</equation>
<figure confidence="0.969299">
4 break ✄ stop reducing ers
5 else
6 ers ← ell−(m−2)+i+1 ✄ reduce state
7 return ers
</figure>
<figureCaption confidence="0.985247">
Figure 2: Equivalent Right LM State Computation.
</figureCaption>
<equation confidence="0.400757">
PBO(em−1  |em−2
1 ), which can be computed
right away based on em−1
1 without waiting for
</equation>
<bodyText confidence="0.8490286">
the unknown e0. Moreover, the back-off weight
�(em−2
0 ) does not depend on the word em−1.
Therefore, q(·) may safely elide the word em−1, and
reduce the left LM state in (6) from em−1
</bodyText>
<equation confidence="0.8994345">
1 to em−2
1 .
</equation>
<bodyText confidence="0.968276230769231">
Also, p(·) should also co-opt PBO(em−1  |em−2
1 ) into
the complete m-gram probability of (4) and p(·)
should exclude em−1 in (5).
The argument above can again be applied recur-
sively to the resulting left LM state ei1, i ∈ [1, m−1],
leading to the equivalent left state procedure of Fig-
ure 3. The procedure IS-A-SUFFIX(e�m1 ) checks if
T is a suffix of any listed k-gram in the LM, k ∈
[m, m]. In Figure 3, fin refers to the probability
that can be computed right away based on the state
itself, for co-opting into the complete m-gram prob-
ability of (4) as mentioned above.
</bodyText>
<subsectionHeader confidence="0.696785">
4.3.3 Modified Cost Functions for Parsing
</subsectionHeader>
<bodyText confidence="0.9991616">
When carrying out the reduction of the left and
right LM states to their shortest equivalents, the for-
mula (4) for calculating the probability of the com-
plete m-grams in an item [X, i, j; e], where e = el 1,
is modified as
</bodyText>
<equation confidence="0.9089105">
p(el1)
= EQ-L-STATE(em−1
1 ).fin × � PLM(ei  |hi)
m &lt; i &lt; l &amp; ? Oea_(M_1)
</equation>
<bodyText confidence="0.9559734">
with the further qualification that some care must be
taken later to incorporate the back-off weights of the
“LM histories” of the suffix of em−1
1 that went miss-
ing due to left LM state reduction.
</bodyText>
<figure confidence="0.899662285714286">
EQ-L-STATE (em−1
1 )
1 els ← em−1
1
2 fin ← 1 ✄ update to final probability p
3 for i ← m − 1 to 1 ✄ right to left
4 if IS-A-SUFFIX(ei 1)
5 break ✄ stop reducing els
6 else
7 fin ← PBO(ei  |ei−1
1 ) × fin
8 els ← ei−1 ✄ reduce state
1
9 return els, fin
</figure>
<figureCaption confidence="0.999808">
Figure 3: Equivalent Left LM State Computation.
</figureCaption>
<bodyText confidence="0.924555">
The estimated probability of the left LM state is
modified as
</bodyText>
<equation confidence="0.921971166666667">
p(e) = �p
�p(EQ-L-STATE(em−1
e
1 ).els) otherwise,
if |e |&lt; m − 1
with p� as defined in (5).
Finally, the LM state function is
q(el1)
= { e1 ... el if &lt; m − 1
EQ-L-STATE(em−1
EQ-R-STATE(ell−(m−2)).ers otherwise.
1 ).els �
</equation>
<subsectionHeader confidence="0.623854">
4.3.4 Suffix and Prefix Look-Up
</subsectionHeader>
<bodyText confidence="0.999977375">
As done in the SRILM toolkit (Stolcke, 2002), a
back-off m-gram LM is stored using a reverse trie
data structure. We store the suffix and prefix infor-
mation in the same data structure without incurring
much additional memory cost. Specifically, the pre-
fix information is stored at the back-off state, while
the suffix information is stored as one bit alongside
the regular m-gram probability.
</bodyText>
<sectionHeader confidence="0.988866" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.9980445">
In this section, we evaluate the performance of our
decoder on a Chinese to English translation task.
</bodyText>
<subsectionHeader confidence="0.9962">
5.1 System Training
</subsectionHeader>
<bodyText confidence="0.981909">
We use various parallel text corpora distributed by
the Linguistic Data Consortium (LDC) for the NIST
MT evaluation. The parallel data we select contains
about 570K Chinese-English sentence pairs, adding
</bodyText>
<page confidence="0.994958">
15
</page>
<bodyText confidence="0.999904">
up to about 19M words on each side. To train the
English language models, we use the English side
of the parallel text and a subset of the English Giga-
word corpus, for a total of about 130M words.
We use the GIZA toolkit (Och and Ney, 2000),
a suffix-array architecture (Lopez, 2007), the
SRILM toolkit (Stolcke, 2002), and minimum er-
ror rate training (Och et al., 2003) to obtain word-
alignments, a translation model, language models,
and the optimal weights for combining these mod-
els, respectively.
</bodyText>
<subsectionHeader confidence="0.993416">
5.2 Improvements in Decoding Speed
</subsectionHeader>
<bodyText confidence="0.999782285714286">
We use a PYTHON implementation of a state-of-
the-art decoder as our baseline4 for decoder compar-
isons. For a direct comparison, we use exactly the
same models and pruning parameters. The SCFG
contains about 3M rules, the 5-gram LM explicitly
lists about 49M k-grams, k = 1, 2, ... , 5, and the
pruning uses Q = 10, b = 30 and 2 = 0.1.
</bodyText>
<table confidence="0.999553833333333">
Decoder Speed BLEU-4
(sec/sent)
MT ’03 MT ’05
Python 26.5 34.4% 32.7%
Java 1.2 34.5% 32.9%
Java (parallel) 0.7
</table>
<tableCaption confidence="0.983783">
Table 1: Decoder Comparison: Translation speed and
quality on the 2003 and 2005 NIST MT benchmark tests.
</tableCaption>
<bodyText confidence="0.9999495">
As shown in Table 1, the JAVA decoder (without
explicit parallelization) is 22 times faster than the
PYTHON decoder, while achieving slightly better
translation quality as measured by BLEU-4 (Pap-
ineni et al., 2002). The parallelization further speeds
it up by a factor of 1.7, making the parallel JAVA de-
coder is 38 times faster than the PYTHON decoder.
We have used the decoder to successfully decode
about one million sentences for a large-scale dis-
criminative training experiment.
</bodyText>
<subsectionHeader confidence="0.999321">
5.3 Impact of a Distributed Language Model
</subsectionHeader>
<bodyText confidence="0.9995395">
We use the SRILM toolkit to build eight 7-gram lan-
guage models, and load and call the LMs using a
</bodyText>
<footnote confidence="0.61725775">
4We are extremely thankful to Philip Resnik at University of
Maryland for allowing us the use of their PYTHON decoder as
the baseline. Thanks also go to David Chiang who originally
implement the decoder.
</footnote>
<bodyText confidence="0.996388285714286">
distributed LM architecture5 as discussed in Section
3.2. As shown in Table 2, the 7-gram distributed lan-
guage model (DLM) significantly improves trans-
lation performance over the 5-gram LM. However,
decoding is significantly slower (12.2 sec/sent when
using the non-parallel decoder) due to the added net-
work communication overhead.
</bodyText>
<table confidence="0.949278333333333">
LM type # k-grams MT ’03 MT ’05
5-gram LM 49 M 34.5% 32.9%
7-gram DLM 310M 35.5% 33.9%
</table>
<tableCaption confidence="0.94193425">
Table 2: Distributed language model: the 7-gram LM
cannot be loaded alongside the SCFG on a single ma-
chine; via distributed computing, it yields significant im-
provement in BLEU-4 over a 5-gram.
</tableCaption>
<subsectionHeader confidence="0.99444">
5.4 Utility of Equivalent LM States
</subsectionHeader>
<bodyText confidence="0.999954769230769">
To reduce the number of search errors, one may ei-
ther increase the beam size, or employ techniques
such as the equivalent LM state maintenance de-
scribed in Section 4. In this subsection, we compare
the tradeoff between the search effort (measured by
decoding time per sentence) and the search qual-
ity (measured by the average model cost of the best
translation found).
Intuitively, collapsing equivalent LM states is use-
ful only when the language model is very sparse, i.e.,
most of the evaluated m-grams will need to back-
off. A sparse LM is obtained in practice by using
a large order m relative to the amount of training
data. To test this intuition, we train a 7-gram LM
using only the English side of the parallel text (-
19M words). Figure 4 compares maintenance of
the full LM state v/s the equivalent LM state. The
beam size b for decoding with equivalent LM states
is fixed at 30; it is increased considerably—30, 50,
70, 90, 120, and 150—with the full LM state in
an effort to reduce search errors. It is clear from
the figure that collapsing items that differ due only
to equivalent LM states improves the search quality
considerably while actually reducing search effort.
This shows the effectiveness of equivalent LM state
maintenance.
</bodyText>
<footnote confidence="0.99008">
5Since our distributed LM architecture dynamically interpo-
lates multiple LM scores, it cannot yet exploit the equivalent
LM state maintenance of Section 4, for different LMs will have
different reduced LM states. We will address this in the future.
</footnote>
<page confidence="0.996247">
16
</page>
<figureCaption confidence="0.983696333333333">
Figure 4: Search quality with equivalent 7-gram LM state
maintenance (EquivLM) and without it (Baseline) as a
function of search effort as controlled by the beam size.
</figureCaption>
<bodyText confidence="0.999919777777778">
We also train a 3-gram LM using an English cor-
pus of about 130M words, and repeat the above ex-
periments. In this case, maintaining equivalent LM
states costs more decoding time than using the full
LM state to achieve the same search quality. This
is due partly to our inefficient implementation of the
prefix- and suffix-lookup required to determine the
equivalent LM state, and partly to the fact that with
130M words, a 3-gram LM backs off less frequently.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999995684210526">
We have described a scalable decoder for parsing-
based machine translation. It is written in JAVA and
implements all the essential algorithms described
in Chiang (2007): chart-parsing, m-gram language
model integration, beam- and cube-pruning, and
unique k-best extraction. Additionally, parallel and
distributed computing techniques are exploited to
make it scalable. We demonstrate that our decoder
is 38 times faster than a baseline decoder written in
PYTHON, and that the distributed language model
is very useful to improve translation quality in a
large-scale task. We also describe an algorithm that
exploits the back-off property of an m-gram model
to maintain equivalent LM states, and show that bet-
ter search quality is obtained with less search effort
when the search space is organized to exploit this
equivalence. We plan to incorporate some additional
syntax-based components into the decoder and re-
lease it as an open-source toolkit.
</bodyText>
<sectionHeader confidence="0.997137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999887571428572">
We thank Philip Resnik, Chris Dyer, Smaranda
Muresan and Adam Lopez for very helpful discus-
sions, and the anonymous reviewers for their con-
structive comments. This research was partially sup-
ported by the Defense Advanced Research Projects
Agency’s GALE program via Contract No¯ HR0011-
06-2-0001.
</bodyText>
<sectionHeader confidence="0.998895" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999678461538462">
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2006. Large Language Models in
Machine Translation. In Proceedings of EMNLP 2007.
David Chiang. 2006. An Introduction to
Synchronous Grammars. Available at
http://www.isi.edu/∼chiang/papers/synchtut.pdf.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201-228.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL
2003.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of ICASSP 2007.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL 2006.
Liang Huang and David Chiang. 2005. Better k-best pars-
ing. In Proceedings of IWPT 2005.
Liang Huang and David Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models. In
Proceedings of the ACL 2007.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA 2006.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL 2006.
Adam Lopez. 2007. Hierarchical Phrase-Based Transla-
tion with Suffix Arrays. In Proceedings of EMNLP
2007.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000.
</reference>
<figure confidence="0.987764384615385">
Search Effort vs Search Quality
beam size = 30
Baseline
EquivLM
0 2 4 6 8 10
Number of Seconds per Sentence
20.07
20.05
20.03
20.01
19.99
19.97
19.95
</figure>
<page confidence="0.978827">
17
</page>
<reference confidence="0.999770142857143">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proceedings of ACL 2005.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3-15.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, volume
2, pages 901-904.
Ashish Venugopal, Andreas Zollmann, Stephan Vo-
gel. 2007. An Efficient Two-Pass Approach to
Synchronous-CFG Driven Statistical MT. In Proceed-
ings of NAACL 2007.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list re-
ranking. In Proceedings of EMNLP 2006.
</reference>
<page confidence="0.999292">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872627">
<title confidence="0.9896675">A Scalable Decoder for Parsing-based Machine with Equivalent Language Model State Maintenance</title>
<author confidence="0.996086">Li</author>
<affiliation confidence="0.998932">Department of Computer Science and Center for Language and Speech</affiliation>
<address confidence="0.945487">Johns Hopkins University, Baltimore, MD 21218,</address>
<abstract confidence="0.9975025">We describe a scalable decoder for parsingbased machine translation. The decoder is written in JAVA and implements all the essential algorithms described in Chiang (2007): language model integration, beamand cube-pruning, and unique extraction. Additionally, and distributed computing techniques are exploited to make it scalable. We also propose an algorithm to maintain equivalent language states that exploits the propof language models: instead of maintaining a separate state for each distinguished sequence of “state” words, we merge states that can be made language model probability calculations due to back-off. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in PYTHON. We propose to release our decoder as an opensource toolkit.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large Language Models in Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Brants, Popat, Xu, Och, Dean, 2006</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2006. Large Language Models in Machine Translation. In Proceedings of EMNLP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>An Introduction to Synchronous Grammars. Available at http://www.isi.edu/∼chiang/papers/synchtut.pdf.</title>
<date>2006</date>
<contexts>
<context position="5517" citStr="Chiang, 2006" startWordPosition="859" endWordPosition="860">ordered around of in the translation. The rule weight is omitted in this example. A bilingual SCFG derivation is analogous to a monolingual CFG derivation. It begins with a pair of aligned start symbols. At each step, an aligned pair of nonterminals is rewritten as the two corresponding components of a single rule. In this sense, the derivations are generated synchronously. Our decoder presently handles SCFGs of the kind extracted by Heiro (Chiang, 2007), but is easily extensible to more general SCFGs and closely related formalisms such as synchronous tree substitution grammars (Eisner, 2003; Chiang, 2006). 2.2 MT Decoding as Chart Parsing Given a source-language sentence f*, the decoder must find the target-language yield e(D) of the best derivation D among all derivations with sourcelanguage yield f(D) = f*, i.e. e* = e Carg max w(D)) , (2) where w(D) is the composite weight of D. The parser may be treated as a deductive proof system (Shieber et al., 1995). Formally (cf. (Chiang, 2007)), a parser defines a space of weighted items, with some items designated as axioms and some as goals, and a set of inference rules of the form I1 : w1 ··· Ik : wk which states that if all the antecedent items I</context>
</contexts>
<marker>Chiang, 2006</marker>
<rawString>David Chiang. 2006. An Introduction to Synchronous Grammars. Available at http://www.isi.edu/∼chiang/papers/synchtut.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--2</pages>
<contexts>
<context position="1496" citStr="Chiang (2007)" startWordPosition="215" endWordPosition="216">uished sequence of “state” words, we merge multiple states that can be made equivalent for language model probability calculations due to back-off. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in PYTHON. We propose to release our decoder as an opensource toolkit. 1 Introduction Large-scale parsing-based statistical machine translation (MT) has made remarkable progress in the last few years. The systems being developed differ in whether they use source- or target-language syntax. For instance, the hierarchical translation system of Chiang (2007) extracts a synchronous grammar from pairs of strings, Quirk et al. (2005), Liu et al. (2006) and Huang et al. (2006) perform syntactic analyses in the source-language, and Galley et al. (2006) use target-language syntax. A critical component in parsing-based MT systems is the decoder, which is complex to imple10 ment and scale up. Most of the systems described above employ tailor-made, dedicated decoders that are not open-source, which results in a high barrier to entry for other researchers in the field. However, with the algorithms proposed in (Huang and Chiang, 2005; Chiang, 2007; Huang an</context>
<context position="3916" citStr="Chiang (2007)" startWordPosition="582" endWordPosition="583">. Furthermore, the distributed computing permits improving translation quality via large-scale LMs. We have successfully use our decoder to translate about a million sentences in a parallel corpus for large-scale discriminative training experiments. Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 10–18, ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 2 Parsing-based MT Decoder In this section, we discuss the core algorithms implemented in our decoder. These algorithms have been discussed by Chiang (2007) in detail, and we recapitulate the essential parts here for completeness. 2.1 Grammar Formalism Our decoder assumes a probabilistic synchronous context-free grammar (SCFG). Following the notation in Venugopal et al. (2007), a probabilistic SCFG comprises a set of source-language terminal symbols TS, a set of target-language terminal symbols TT, a shared set of nonterminal symbols N, and a set of rules of the form X —* (γ, α, —, w) , (1) where X E N, γ E [NUTS]* is a (mixed) sequence of nonterminals and source terminals, α E [NUTT]* is a sequence of nonterminals and target terminals, — is a on</context>
<context position="5362" citStr="Chiang, 2007" startWordPosition="836" endWordPosition="837">inese word n, (pronounced de or di) means of, and the alignment, encoded via subscripts on the nonterminals, causes the two noun phrases around n, to be reordered around of in the translation. The rule weight is omitted in this example. A bilingual SCFG derivation is analogous to a monolingual CFG derivation. It begins with a pair of aligned start symbols. At each step, an aligned pair of nonterminals is rewritten as the two corresponding components of a single rule. In this sense, the derivations are generated synchronously. Our decoder presently handles SCFGs of the kind extracted by Heiro (Chiang, 2007), but is easily extensible to more general SCFGs and closely related formalisms such as synchronous tree substitution grammars (Eisner, 2003; Chiang, 2006). 2.2 MT Decoding as Chart Parsing Given a source-language sentence f*, the decoder must find the target-language yield e(D) of the best derivation D among all derivations with sourcelanguage yield f(D) = f*, i.e. e* = e Carg max w(D)) , (2) where w(D) is the composite weight of D. The parser may be treated as a deductive proof system (Shieber et al., 1995). Formally (cf. (Chiang, 2007)), a parser defines a space of weighted items, with some</context>
<context position="7845" citStr="Chiang (2007)" startWordPosition="1277" endWordPosition="1278">1)n3� O ,(3) where K is the maximum number of nonterminal pairs per rule and n is the source-language sentence length (Venugopal et al., 2007). 1For more general grammars with K &gt; 2 pairs of nonterminals per rule, see Venugopal et al. (2007). φ , I : w 11 X→(γ,α):w (X (&apos;y, α, w)) E G X→(fji+1, α) : w [X, i, j; q(α)] : wp(α) Z→(fi1 0 = α[e1/X] i+1Xfj j1+1, α) : w [X,i1,j1;e1] : w1 α [Z, i, j; q(α0)] : ww1p(α0) Z→(fi+1X1f�i+1Y2fjj2+1, α) : w [X,i1,j1;e1] : w1 [Y,i2,j2;e2] : w2 0 = α[e1/X1, e2/Y2] α [Z, i, j; q(α0)] : ww1w2p(α0) Goal item: [S, 0, n; (s)m−1 * e(/s)] Figure 1: Inference rules from Chiang (2007) for a parser with an m-gram LM. G denotes the translation grammar. w[x/X] denotes substitution of the string x for the symbol X in the string w. The function p(·) provides the LM probability for all complete m-grams in a string, while the function q(·) elides symbols whose m-grams have been accounted for by p(·). Details about the functions p(·) and q(·) are provided in Section 4. 2.3 Pruning in a Decoder Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large vocabularies TT and detailed nonterminal sets. In our decoder, we incorporate two pruning</context>
<context position="27024" citStr="Chiang (2007)" startWordPosition="4606" endWordPosition="4607">sing an English corpus of about 130M words, and repeat the above experiments. In this case, maintaining equivalent LM states costs more decoding time than using the full LM state to achieve the same search quality. This is due partly to our inefficient implementation of the prefix- and suffix-lookup required to determine the equivalent LM state, and partly to the fact that with 130M words, a 3-gram LM backs off less frequently. 6 Conclusions We have described a scalable decoder for parsingbased machine translation. It is written in JAVA and implements all the essential algorithms described in Chiang (2007): chart-parsing, m-gram language model integration, beam- and cube-pruning, and unique k-best extraction. Additionally, parallel and distributed computing techniques are exploited to make it scalable. We demonstrate that our decoder is 38 times faster than a baseline decoder written in PYTHON, and that the distributed language model is very useful to improve translation quality in a large-scale task. We also describe an algorithm that exploits the back-off property of an m-gram model to maintain equivalent LM states, and show that better search quality is obtained with less search effort when </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="5502" citStr="Eisner, 2003" startWordPosition="857" endWordPosition="858">nd n, to be reordered around of in the translation. The rule weight is omitted in this example. A bilingual SCFG derivation is analogous to a monolingual CFG derivation. It begins with a pair of aligned start symbols. At each step, an aligned pair of nonterminals is rewritten as the two corresponding components of a single rule. In this sense, the derivations are generated synchronously. Our decoder presently handles SCFGs of the kind extracted by Heiro (Chiang, 2007), but is easily extensible to more general SCFGs and closely related formalisms such as synchronous tree substitution grammars (Eisner, 2003; Chiang, 2006). 2.2 MT Decoding as Chart Parsing Given a source-language sentence f*, the decoder must find the target-language yield e(D) of the best derivation D among all derivations with sourcelanguage yield f(D) = f*, i.e. e* = e Carg max w(D)) , (2) where w(D) is the composite weight of D. The parser may be treated as a deductive proof system (Shieber et al., 1995). Formally (cf. (Chiang, 2007)), a parser defines a space of weighted items, with some items designated as axioms and some as goals, and a set of inference rules of the form I1 : w1 ··· Ik : wk which states that if all the ant</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings of ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad Emami</author>
<author>Kishore Papineni</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Large-scale distributed language modeling.</title>
<date>2007</date>
<booktitle>In Proceedings of ICASSP</booktitle>
<contexts>
<context position="12310" citStr="Emami et al., 2007" startWordPosition="2018" endWordPosition="2021"> once. This multithreading provides a very significant speed-up. 3.2 Distributed Language Models It is not possible in some cases to load a very large LM into memory on a single machine, particularly if the SCFG is also very large. In other cases, loading the LM each time the decoder runs may be too time-consuming relative to the time required for decoding itself, such as in iterative decoding with updated combination weights during minimum errorrate training. It is therefore desirable to have dedicated servers to load parts of the LM3 — an idea that has been exploited by (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007). Our implementation can load a (partitioned) LM on different servers before initiating decoding. The decoder remotely calls the servers to obtain individual LM probabilities, and linearly interpolates them on the fly using a given set of interpolation weights. With this architecture, one can deal with a very large target-language text corpus by splitting it into many parts and training separate LMs from each. The runtime interpolation capability may also be used for LM adaptation, e.g. for building document-specific language models. To mitigate potential network communic</context>
</contexts>
<marker>Emami, Papineni, Sorensen, 2007</marker>
<rawString>Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen. 2007. Large-scale distributed language modeling. In Proceedings of ICASSP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL</booktitle>
<contexts>
<context position="1689" citStr="Galley et al. (2006)" startWordPosition="247" endWordPosition="250"> decoder is more than 30 times faster than a baseline decoder written in PYTHON. We propose to release our decoder as an opensource toolkit. 1 Introduction Large-scale parsing-based statistical machine translation (MT) has made remarkable progress in the last few years. The systems being developed differ in whether they use source- or target-language syntax. For instance, the hierarchical translation system of Chiang (2007) extracts a synchronous grammar from pairs of strings, Quirk et al. (2005), Liu et al. (2006) and Huang et al. (2006) perform syntactic analyses in the source-language, and Galley et al. (2006) use target-language syntax. A critical component in parsing-based MT systems is the decoder, which is complex to imple10 ment and scale up. Most of the systems described above employ tailor-made, dedicated decoders that are not open-source, which results in a high barrier to entry for other researchers in the field. However, with the algorithms proposed in (Huang and Chiang, 2005; Chiang, 2007; Huang and Chiang, 2007), it is possible to develop a general-purpose decoder that can be used by all the parsing-based systems. In this paper, we describe an important firststep towards an extensible, </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING/ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT</booktitle>
<contexts>
<context position="2072" citStr="Huang and Chiang, 2005" startWordPosition="310" endWordPosition="313">erarchical translation system of Chiang (2007) extracts a synchronous grammar from pairs of strings, Quirk et al. (2005), Liu et al. (2006) and Huang et al. (2006) perform syntactic analyses in the source-language, and Galley et al. (2006) use target-language syntax. A critical component in parsing-based MT systems is the decoder, which is complex to imple10 ment and scale up. Most of the systems described above employ tailor-made, dedicated decoders that are not open-source, which results in a high barrier to entry for other researchers in the field. However, with the algorithms proposed in (Huang and Chiang, 2005; Chiang, 2007; Huang and Chiang, 2007), it is possible to develop a general-purpose decoder that can be used by all the parsing-based systems. In this paper, we describe an important firststep towards an extensible, general-purpose, scalable, and open-source parsing-based MT decoder. Our decoder is written in JAVA and implements all the essential algorithms described in Chiang (2007): chart-parsing, m-gram language model integration, beam- and cube-pruning, and unique k-best extraction. Additionally, parallel and distributed computing techniques are exploited to make it scalable. Straightforw</context>
<context position="9795" citStr="Huang and Chiang (2005)" startWordPosition="1606" endWordPosition="1609">graph representing a set of likely hypotheses D in (2). Briefly, a hyper-graph is a set of vertices and hyper-edges, with each hyper-edge connecting a set of antecedent vertices to a consequent vertex, and a special vertex designated as the target vertex. In parsing parlance, a vertex corresponds to an item in the chart, a hyper-edge corresponds to a SCFG rule with the nonterminals on the right-side replaced by back-pointers to antecedent items, and the target vertex corresponds to the goal item2. Given a hyper-graph for a source-language sentence f*, we use the k-best extraction algorithm of Huang and Chiang (2005) to extract its k most likely translations. Moreover, since many different derivations D in (2) may lead to the same target-language yield e(D), we adopt the modification described in Huang et al. (2006) to efficiently generate the unique k best translations of f*. 3 Parallel and Distributed Computing Many applications of parsing-based MT entail the use of SCFGs extracted from millions of bilingual sentence pairs and LMs extracted from billions of words of target-language text. This requires the decoder to make use of distributed computing to spread the memory required to load large-scale SCFG</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of IWPT 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest Rescoring: Faster Decoding with Integrated Language Models.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL</booktitle>
<contexts>
<context position="2111" citStr="Huang and Chiang, 2007" startWordPosition="316" endWordPosition="319">g (2007) extracts a synchronous grammar from pairs of strings, Quirk et al. (2005), Liu et al. (2006) and Huang et al. (2006) perform syntactic analyses in the source-language, and Galley et al. (2006) use target-language syntax. A critical component in parsing-based MT systems is the decoder, which is complex to imple10 ment and scale up. Most of the systems described above employ tailor-made, dedicated decoders that are not open-source, which results in a high barrier to entry for other researchers in the field. However, with the algorithms proposed in (Huang and Chiang, 2005; Chiang, 2007; Huang and Chiang, 2007), it is possible to develop a general-purpose decoder that can be used by all the parsing-based systems. In this paper, we describe an important firststep towards an extensible, general-purpose, scalable, and open-source parsing-based MT decoder. Our decoder is written in JAVA and implements all the essential algorithms described in Chiang (2007): chart-parsing, m-gram language model integration, beam- and cube-pruning, and unique k-best extraction. Additionally, parallel and distributed computing techniques are exploited to make it scalable. Straightforward integration of an m-gram language m</context>
<context position="8508" citStr="Huang and Chiang, 2007" startWordPosition="1386" endWordPosition="1389">es the translation grammar. w[x/X] denotes substitution of the string x for the symbol X in the string w. The function p(·) provides the LM probability for all complete m-grams in a string, while the function q(·) elides symbols whose m-grams have been accounted for by p(·). Details about the functions p(·) and q(·) are provided in Section 4. 2.3 Pruning in a Decoder Severe pruning is needed in order to make the decoding computationally feasible for SCFGs with large vocabularies TT and detailed nonterminal sets. In our decoder, we incorporate two pruning techniques described by (Chiang, 2007; Huang and Chiang, 2007). For beam pruning, in each cell, we discard all items whose weight is worse, by a relative threshold Q, than the weight of the best item in the same cell. If too many items pass the threshold, a cell only retains the top-b items by weight. When combining smaller items to obtain a larger item by applying an inference rule, we use cube-pruning to simulate kbest extraction in each destination cell, and discard combinations that lead to an item whose weight is worse than the best item in that cell by a margin of E. 2.4 k-best Extraction Over Hyper-graphs For each source-language sentence f*, the </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. In Proceedings of the ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA</booktitle>
<contexts>
<context position="1613" citStr="Huang et al. (2006)" startWordPosition="235" endWordPosition="238">bility calculations due to back-off. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in PYTHON. We propose to release our decoder as an opensource toolkit. 1 Introduction Large-scale parsing-based statistical machine translation (MT) has made remarkable progress in the last few years. The systems being developed differ in whether they use source- or target-language syntax. For instance, the hierarchical translation system of Chiang (2007) extracts a synchronous grammar from pairs of strings, Quirk et al. (2005), Liu et al. (2006) and Huang et al. (2006) perform syntactic analyses in the source-language, and Galley et al. (2006) use target-language syntax. A critical component in parsing-based MT systems is the decoder, which is complex to imple10 ment and scale up. Most of the systems described above employ tailor-made, dedicated decoders that are not open-source, which results in a high barrier to entry for other researchers in the field. However, with the algorithms proposed in (Huang and Chiang, 2005; Chiang, 2007; Huang and Chiang, 2007), it is possible to develop a general-purpose decoder that can be used by all the parsing-based system</context>
<context position="9998" citStr="Huang et al. (2006)" startWordPosition="1640" endWordPosition="1643"> special vertex designated as the target vertex. In parsing parlance, a vertex corresponds to an item in the chart, a hyper-edge corresponds to a SCFG rule with the nonterminals on the right-side replaced by back-pointers to antecedent items, and the target vertex corresponds to the goal item2. Given a hyper-graph for a source-language sentence f*, we use the k-best extraction algorithm of Huang and Chiang (2005) to extract its k most likely translations. Moreover, since many different derivations D in (2) may lead to the same target-language yield e(D), we adopt the modification described in Huang et al. (2006) to efficiently generate the unique k best translations of f*. 3 Parallel and Distributed Computing Many applications of parsing-based MT entail the use of SCFGs extracted from millions of bilingual sentence pairs and LMs extracted from billions of words of target-language text. This requires the decoder to make use of distributed computing to spread the memory required to load large-scale SCFGs and LMs onto multiple processors. Furthermore, techniques such as iterative minimum errorrate training (Och et al., 2003) as well as web-based MT services require the decoder to translate a large numbe</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of AMTA 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<contexts>
<context position="1589" citStr="Liu et al. (2006)" startWordPosition="230" endWordPosition="233">r language model probability calculations due to back-off. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in PYTHON. We propose to release our decoder as an opensource toolkit. 1 Introduction Large-scale parsing-based statistical machine translation (MT) has made remarkable progress in the last few years. The systems being developed differ in whether they use source- or target-language syntax. For instance, the hierarchical translation system of Chiang (2007) extracts a synchronous grammar from pairs of strings, Quirk et al. (2005), Liu et al. (2006) and Huang et al. (2006) perform syntactic analyses in the source-language, and Galley et al. (2006) use target-language syntax. A critical component in parsing-based MT systems is the decoder, which is complex to imple10 ment and scale up. Most of the systems described above employ tailor-made, dedicated decoders that are not open-source, which results in a high barrier to entry for other researchers in the field. However, with the algorithms proposed in (Huang and Chiang, 2005; Chiang, 2007; Huang and Chiang, 2007), it is possible to develop a general-purpose decoder that can be used by all </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of COLING-ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical Phrase-Based Translation with Suffix Arrays.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="22434" citStr="Lopez, 2007" startWordPosition="3830" endWordPosition="3831">In this section, we evaluate the performance of our decoder on a Chinese to English translation task. 5.1 System Training We use various parallel text corpora distributed by the Linguistic Data Consortium (LDC) for the NIST MT evaluation. The parallel data we select contains about 570K Chinese-English sentence pairs, adding 15 up to about 19M words on each side. To train the English language models, we use the English side of the parallel text and a subset of the English Gigaword corpus, for a total of about 130M words. We use the GIZA toolkit (Och and Ney, 2000), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och et al., 2003) to obtain wordalignments, a translation model, language models, and the optimal weights for combining these models, respectively. 5.2 Improvements in Decoding Speed We use a PYTHON implementation of a state-ofthe-art decoder as our baseline4 for decoder comparisons. For a direct comparison, we use exactly the same models and pruning parameters. The SCFG contains about 3M rules, the 5-gram LM explicitly lists about 49M k-grams, k = 1, 2, ... , 5, and the pruning uses Q = 10, b = 30 and 2 = 0.1. Decoder Speed</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical Phrase-Based Translation with Suffix Arrays. In Proceedings of EMNLP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="22391" citStr="Och and Ney, 2000" startWordPosition="3823" endWordPosition="3826">gular m-gram probability. 5 Experimental Results In this section, we evaluate the performance of our decoder on a Chinese to English translation task. 5.1 System Training We use various parallel text corpora distributed by the Linguistic Data Consortium (LDC) for the NIST MT evaluation. The parallel data we select contains about 570K Chinese-English sentence pairs, adding 15 up to about 19M words on each side. To train the English language models, we use the English side of the parallel text and a subset of the English Gigaword corpus, for a total of about 130M words. We use the GIZA toolkit (Och and Ney, 2000), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och et al., 2003) to obtain wordalignments, a translation model, language models, and the optimal weights for combining these models, respectively. 5.2 Improvements in Decoding Speed We use a PYTHON implementation of a state-ofthe-art decoder as our baseline4 for decoder comparisons. For a direct comparison, we use exactly the same models and pruning parameters. The SCFG contains about 3M rules, the 5-gram LM explicitly lists about 49M k-grams, k = 1, 2, ... , 5, and the pruning use</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="23451" citStr="Papineni et al., 2002" startWordPosition="4001" endWordPosition="4005">the same models and pruning parameters. The SCFG contains about 3M rules, the 5-gram LM explicitly lists about 49M k-grams, k = 1, 2, ... , 5, and the pruning uses Q = 10, b = 30 and 2 = 0.1. Decoder Speed BLEU-4 (sec/sent) MT ’03 MT ’05 Python 26.5 34.4% 32.7% Java 1.2 34.5% 32.9% Java (parallel) 0.7 Table 1: Decoder Comparison: Translation speed and quality on the 2003 and 2005 NIST MT benchmark tests. As shown in Table 1, the JAVA decoder (without explicit parallelization) is 22 times faster than the PYTHON decoder, while achieving slightly better translation quality as measured by BLEU-4 (Papineni et al., 2002). The parallelization further speeds it up by a factor of 1.7, making the parallel JAVA decoder is 38 times faster than the PYTHON decoder. We have used the decoder to successfully decode about one million sentences for a large-scale discriminative training experiment. 5.3 Impact of a Distributed Language Model We use the SRILM toolkit to build eight 7-gram language models, and load and call the LMs using a 4We are extremely thankful to Philip Resnik at University of Maryland for allowing us the use of their PYTHON decoder as the baseline. Thanks also go to David Chiang who originally implemen</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1570" citStr="Quirk et al. (2005)" startWordPosition="226" endWordPosition="229">be made equivalent for language model probability calculations due to back-off. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in PYTHON. We propose to release our decoder as an opensource toolkit. 1 Introduction Large-scale parsing-based statistical machine translation (MT) has made remarkable progress in the last few years. The systems being developed differ in whether they use source- or target-language syntax. For instance, the hierarchical translation system of Chiang (2007) extracts a synchronous grammar from pairs of strings, Quirk et al. (2005), Liu et al. (2006) and Huang et al. (2006) perform syntactic analyses in the source-language, and Galley et al. (2006) use target-language syntax. A critical component in parsing-based MT systems is the decoder, which is complex to imple10 ment and scale up. Most of the systems described above employ tailor-made, dedicated decoders that are not open-source, which results in a high barrier to entry for other researchers in the field. However, with the algorithms proposed in (Huang and Chiang, 2005; Chiang, 2007; Huang and Chiang, 2007), it is possible to develop a general-purpose decoder that </context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Yves Schabes</author>
<author>Fernando Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--3</pages>
<contexts>
<context position="5876" citStr="Shieber et al., 1995" startWordPosition="922" endWordPosition="925"> generated synchronously. Our decoder presently handles SCFGs of the kind extracted by Heiro (Chiang, 2007), but is easily extensible to more general SCFGs and closely related formalisms such as synchronous tree substitution grammars (Eisner, 2003; Chiang, 2006). 2.2 MT Decoding as Chart Parsing Given a source-language sentence f*, the decoder must find the target-language yield e(D) of the best derivation D among all derivations with sourcelanguage yield f(D) = f*, i.e. e* = e Carg max w(D)) , (2) where w(D) is the composite weight of D. The parser may be treated as a deductive proof system (Shieber et al., 1995). Formally (cf. (Chiang, 2007)), a parser defines a space of weighted items, with some items designated as axioms and some as goals, and a set of inference rules of the form I1 : w1 ··· Ik : wk which states that if all the antecedent items Ii are provable, respectively with weight wi, then the consequent item I is provable with weight w, provided the side condition φ holds. For a grammar with a maximum of two (pairs of) nonterminals per rule1, Figure 1 illustrates the resulting chart parsing procedure, including the integration of an m-gram LM. The actual decoding algorithm maintains a chart, </context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24:3-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="21450" citStr="Stolcke, 2002" startWordPosition="3667" endWordPosition="3668">✄ update to final probability p 3 for i ← m − 1 to 1 ✄ right to left 4 if IS-A-SUFFIX(ei 1) 5 break ✄ stop reducing els 6 else 7 fin ← PBO(ei |ei−1 1 ) × fin 8 els ← ei−1 ✄ reduce state 1 9 return els, fin Figure 3: Equivalent Left LM State Computation. The estimated probability of the left LM state is modified as p(e) = �p �p(EQ-L-STATE(em−1 e 1 ).els) otherwise, if |e |&lt; m − 1 with p� as defined in (5). Finally, the LM state function is q(el1) = { e1 ... el if &lt; m − 1 EQ-L-STATE(em−1 EQ-R-STATE(ell−(m−2)).ers otherwise. 1 ).els � 4.3.4 Suffix and Prefix Look-Up As done in the SRILM toolkit (Stolcke, 2002), a back-off m-gram LM is stored using a reverse trie data structure. We store the suffix and prefix information in the same data structure without incurring much additional memory cost. Specifically, the prefix information is stored at the back-off state, while the suffix information is stored as one bit alongside the regular m-gram probability. 5 Experimental Results In this section, we evaluate the performance of our decoder on a Chinese to English translation task. 5.1 System Training We use various parallel text corpora distributed by the Linguistic Data Consortium (LDC) for the NIST MT e</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, volume 2, pages 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Stephan Vogel</author>
</authors>
<title>An Efficient Two-Pass Approach to Synchronous-CFG Driven Statistical MT.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="4139" citStr="Venugopal et al. (2007)" startWordPosition="613" endWordPosition="616">discriminative training experiments. Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 10–18, ACL-08: HLT, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 2 Parsing-based MT Decoder In this section, we discuss the core algorithms implemented in our decoder. These algorithms have been discussed by Chiang (2007) in detail, and we recapitulate the essential parts here for completeness. 2.1 Grammar Formalism Our decoder assumes a probabilistic synchronous context-free grammar (SCFG). Following the notation in Venugopal et al. (2007), a probabilistic SCFG comprises a set of source-language terminal symbols TS, a set of target-language terminal symbols TT, a shared set of nonterminal symbols N, and a set of rules of the form X —* (γ, α, —, w) , (1) where X E N, γ E [NUTS]* is a (mixed) sequence of nonterminals and source terminals, α E [NUTT]* is a sequence of nonterminals and target terminals, — is a one-to-one correspondence or alignment between the nonterminal elements of γ and α, and w &gt; 0 is a weight assigned to the rule. An illustrative rule for Chinese-to-English translation is NP —* ( NP0 n, NP1 , NP1 of NP0 ) , wh</context>
<context position="7374" citStr="Venugopal et al., 2007" startWordPosition="1178" endWordPosition="1181">adds the item to the appropriate chart cell. It also maintains backpointers to antecedent items, which are used for k-best extraction, as discussed in Section 2.4 below. In a SCFG-based decoder, an item is identified by its source-language span, left-side nonterminal label, and left- and right-context for the target-language m-gram LM. Therefore, in a given cell, the maximum possible number of items is O(|N||TT|2(m−1)), and the worst case decoding complexity is �|N|K |TT|2K(m−1)n3� O ,(3) where K is the maximum number of nonterminal pairs per rule and n is the source-language sentence length (Venugopal et al., 2007). 1For more general grammars with K &gt; 2 pairs of nonterminals per rule, see Venugopal et al. (2007). φ , I : w 11 X→(γ,α):w (X (&apos;y, α, w)) E G X→(fji+1, α) : w [X, i, j; q(α)] : wp(α) Z→(fi1 0 = α[e1/X] i+1Xfj j1+1, α) : w [X,i1,j1;e1] : w1 α [Z, i, j; q(α0)] : ww1p(α0) Z→(fi+1X1f�i+1Y2fjj2+1, α) : w [X,i1,j1;e1] : w1 [Y,i2,j2;e2] : w2 0 = α[e1/X1, e2/Y2] α [Z, i, j; q(α0)] : ww1w2p(α0) Goal item: [S, 0, n; (s)m−1 * e(/s)] Figure 1: Inference rules from Chiang (2007) for a parser with an m-gram LM. G denotes the translation grammar. w[x/X] denotes substitution of the string x for the symbol X </context>
</contexts>
<marker>Venugopal, Zollmann, Vogel, 2007</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Stephan Vogel. 2007. An Efficient Two-Pass Approach to Synchronous-CFG Driven Statistical MT. In Proceedings of NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Distributed language modeling for n-best list reranking.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="12290" citStr="Zhang et al., 2006" startWordPosition="2014" endWordPosition="2017"> LM into memory only once. This multithreading provides a very significant speed-up. 3.2 Distributed Language Models It is not possible in some cases to load a very large LM into memory on a single machine, particularly if the SCFG is also very large. In other cases, loading the LM each time the decoder runs may be too time-consuming relative to the time required for decoding itself, such as in iterative decoding with updated combination weights during minimum errorrate training. It is therefore desirable to have dedicated servers to load parts of the LM3 — an idea that has been exploited by (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007). Our implementation can load a (partitioned) LM on different servers before initiating decoding. The decoder remotely calls the servers to obtain individual LM probabilities, and linearly interpolates them on the fly using a given set of interpolation weights. With this architecture, one can deal with a very large target-language text corpus by splitting it into many parts and training separate LMs from each. The runtime interpolation capability may also be used for LM adaptation, e.g. for building document-specific language models. To mitigate potent</context>
</contexts>
<marker>Zhang, Hildebrand, Vogel, 2006</marker>
<rawString>Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel. 2006. Distributed language modeling for n-best list reranking. In Proceedings of EMNLP 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>