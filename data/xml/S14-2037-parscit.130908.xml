<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001585">
<title confidence="0.837506">
DIT: Summarisation and Semantic Expansion in Evaluating Semantic
Similarity
</title>
<author confidence="0.979716">
Magdalena Kacmajor
</author>
<affiliation confidence="0.824159666666667">
IBM Technology Campus
Dublin Software Lab
Ireland
</affiliation>
<email confidence="0.995206">
magdalena.kacmajor@ie.ibm.com
</email>
<author confidence="0.994687">
John D. Kelleher
</author>
<affiliation confidence="0.886270333333333">
Applied Intelligence Research Centre
Dublin Institute of Technology
Ireland
</affiliation>
<email confidence="0.991886">
john.d.kelleher@dit.ie
</email>
<sectionHeader confidence="0.993696" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998382">
This paper describes an approach to im-
plementing a tool for evaluating seman-
tic similarity. We investigated the poten-
tial benefits of (1) using text summarisa-
tion to narrow down the comparison to the
most important concepts in both texts, and
(2) leveraging WordNet information to in-
crease usefulness of cosine comparisons
of short texts. In our experiments, text
summarisation using a graph-based algo-
rithm did not prove to be helpful. Se-
mantic and lexical expansion based upon
word relationships defined in WordNet in-
creased the agreement of cosine similarity
values with human similarity judgements.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975210526316">
This paper describes a system that addresses the
problem of assessing semantic similarity between
two different-sized texts. The system has been ap-
plied to SemEval-2014 Task 3, Cross-Level Se-
mantic Similarity (Jurgens et al, 2014). The appli-
cation is limited to a single comparison type, that
is, paragraph to sentence.
The general approach taken can be charac-
terised as text summarisation followed by a pro-
cess of semantic expansion and finally similarity
computation using cosine similarity.
The rationale for applying summarisation is to
focus the comparison on the most important ele-
ments of the text by selecting key words to be used
in the similarity comparison. This summarisation
approach is based on the assumption that if sum-
mary of a paragraph is similar to the summary sen-
tence paired with the paragraph in the task dataset,
then the original paragraph and sentence pair must
</bodyText>
<footnote confidence="0.97053325">
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.99840212">
have been similar and so should receive a high
similarity rating.
The subsequent semantic expansion is intended
to counteract the problem arising from the small
size of both compared text units. The similarity
metric used by the system is essentially a func-
tion of word overlap. However, because both the
paragraphs and sentences being compared are rel-
atively short, the probability of a word overlap -
even between semantically similar texts - is quite
small. Therefore prior to estimating the similarity
between the texts we extend the word vectors cre-
ated by the summarisation process with the syn-
onyms and other words semantically and lexically
related to the words occurring in the text.
By using cosine similarity measure, we nor-
malize the lengths of word vectors representing
different-sized documents (paragraphs and sen-
tences).
The rest of the paper is organized as follows:
section 2 describes the components of the sys-
tem in more detail; section 3 describes parame-
ters used in the experiments we conducted and
presents our results; and section 4 provides con-
cluding remarks.
</bodyText>
<sectionHeader confidence="0.961578" genericHeader="method">
2 System Description
</sectionHeader>
<subsectionHeader confidence="0.937009">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.9993438">
There are four main stages in the system process-
ing pipeline: (1) text pre-processing; (2) summari-
sation; (3) semantic expansion; (4) computing the
similarity scores. In the following sections we de-
scribe each of these stages in turn.
</bodyText>
<subsectionHeader confidence="0.99991">
2.2 Pre-processing
</subsectionHeader>
<bodyText confidence="0.9688495">
Paragraphs and sentences are tokenized and anno-
tated using Stanford CoreNLP1. The annotations
include part-of-speech (POS), lemmatisation, de-
pendency parse and coreference resolution. Then
</bodyText>
<footnote confidence="0.990402">
1http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<page confidence="0.879868">
230
</page>
<note confidence="0.7389945">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 230–234,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999929875">
the following processes are applied: (a) Token se-
lection, the system ignores tokens with POS other
than nouns, adjectives and verbs (in our exper-
iments we tested various combinations of these
three categories); (b) Token merging, the criteria
for merging can be more restrictive (same word
form) or less restrictive – based on same lemma,
or even same lemma ignoring POS; (c) Stopword
removal, we apply a customized stopword list to
exclude verbs that have very general meaning.
In this way, each text unit is processed to pro-
duce a filtered set of tokens which at the next step
can be directly transformed into nodes in the graph
representation of the text. Dependency and coref-
erence annotations will be used for defining edges
in the graph.
</bodyText>
<subsectionHeader confidence="0.999759">
2.3 Summarisation system
</subsectionHeader>
<bodyText confidence="0.99996428">
Summarisation has been implemented using Tex-
tRank (Mihalcea and Tarau, 2004), an itera-
tive graph-based ranking algorithm derived from
PageRank (Brin and Page, 1998).
The ranking is based on the following principle:
when node i links to node j, a vote is cast that in-
creases the rank of node j. The strength of the vote
depends on the importance (rank) of the casting
node, thus the algorithm is run iteratively until the
ranks stop changing beyond a given threshold, or
until a specified limit of iterations is reached.
To apply this algorithm to paragraphs and sen-
tences, our system builds a graph representation
for each of these text units, with nodes represent-
ing the tokens selected and merged at the pre-
ceding stage. The nodes are connected by co-
occurrence (Mihalcea and Tarau, 2004), depen-
dency and/or coreference relations. Next, a un-
weighted or weighted version of the ranking algo-
rithm is iterated until convergence.
For each test unit, the output of the summariser
is a list of words sorted by rank. Depending on
the experimental setup, the summariser forwards
on all processed words, or only a subset of top-
ranked words.
</bodyText>
<subsectionHeader confidence="0.984677">
2.4 Lexico-semantic expansion
</subsectionHeader>
<bodyText confidence="0.998988941176471">
For each word returned from the summariser, we
retrieve all (or a predetermined number of) synsets
that have this word as a member. For each re-
trieved synset, we also identify synsets related
through semantic and lexical relations. Finally, us-
ing all these synsets we create the synonym group
for a word that includes all the members of these
synsets.
If a word has many different senses, then the
synonym group grows large, and the chances that
the sense of a given member of this large group
will match the sense of the original word are
shrinking. To account for this fact, each member
of the synonym group is assigned a weight using
Equation 1. This weight is simply 1 divided by
the count of the number of words in the synonym
group.
</bodyText>
<equation confidence="0.890696">
1
synweight = (1)
#SynonymGroup
</equation>
<bodyText confidence="0.999804">
At the end of this process for each document
we have the set of words that occurred in the doc-
ument, and each of these words has a synonym
group associated with it. All of the members of
the synonym groups have a weight value.
</bodyText>
<subsectionHeader confidence="0.987739">
2.5 Similarity comparison
</subsectionHeader>
<bodyText confidence="0.999963684210526">
Cosine similarity is used to compute the similar-
ity for each paragraph-sentence pair. For this cal-
culation each text (paragraph or sentence) is rep-
resented by a bag-of-words vector containing all
the words derived from the text together with their
synonym groups.
The bag-of-words can be binary or frequency
based, with the counts optionally modified by the
word ranks. The counts for words retrieved from
WordNet are weighted with synweights, which
means that they are usually represented by very
small numbers. However, if a match is found be-
tween a WordNet word and a word observed in
the document, the weight of both is adjusted ac-
cording to semantic match rules. These rules have
been established empirically, and are presented in
section 3.3.1.
The cosine values for each paragraph-sentence
pair are not subject to any further processing.
</bodyText>
<sectionHeader confidence="0.999622" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.944808">
3.1 Dataset
</subsectionHeader>
<bodyText confidence="0.99993">
All experiments were carried out on training
dataset provided by SemEval-2014 Task 3 for
paragraph to sentence comparisons.
</bodyText>
<subsectionHeader confidence="0.996709">
3.2 Parameters
</subsectionHeader>
<bodyText confidence="0.994900666666667">
For each stage in the pipeline, there is a set of pa-
rameters whose values influence the final results.
Each set of parameters will be discussed next.
</bodyText>
<page confidence="0.986227">
231
</page>
<subsectionHeader confidence="0.79087">
3.2.1 Pre-processing parameters
</subsectionHeader>
<bodyText confidence="0.994506">
The parameters used for pre-processing determine
the type and number of nodes included in the
graph:
</bodyText>
<listItem confidence="0.99954275">
• POS: Parts-of-speech that are allowed into
the graph, e.g. only nouns and verbs, or
nouns, verbs and adjectives.
• Merging criteria: The principle by which
we decide whether two tokens should be rep-
resented by the same node in the graph.
• Excluded verbs: The contents of the stop-
word list.
</listItem>
<subsectionHeader confidence="0.694096">
3.2.2 Summarisation parameters
</subsectionHeader>
<bodyText confidence="0.999736">
These parameters control the structure of the graph
and the results yielded by TextRank algorithm.
The types of nodes in the graph are already de-
cided at the pre-processing stage.
</bodyText>
<listItem confidence="0.75190744">
• Relation type: In order to link the nodes
(words) in the graph representation of a docu-
ment, we use co-occurrence relations (Mihal-
cea and Tarau, 2004), dependency relations
and coreference relations. The two latter are
defined based on the Stanford CoreNLP an-
notations, whereas a co-occurrence edge is
created when two words appear in the text
within a word span of a specified length. The
co-occurrence relation comes with two addi-
tional parameters:
– Window size: Maximum number of
words constituting the span.
– Window application: The window can
be applied before or after filtering away
tokens of unwanted POS, i.e. we can re-
quire either the co-occurrence within the
original text or in the filtered text.
• Graph type: A document can be represented
as an unweighted or weighted graph. In
the second case we use a weighted version
of TextRank algorithm (Mihalcea and Tarau,
2004) in which the strength of a vote depends
both on the rank of the casting node and on
the weight of the link producing the vote.
</listItem>
<bodyText confidence="0.888836833333333">
– Edge weights: In general, the weight
of an edge between any two nodes de-
pends on the number of identified rela-
tions, but we also experimented with as-
signing different weights depending on
the relation type.
</bodyText>
<listItem confidence="0.9906055">
• Normalisation: This parameter refers to nor-
malising word ranks computed for the longer
and the shorter text unit.
• Word limit: The maximum number of top-
</listItem>
<bodyText confidence="0.8486076">
ranked words included in vector representa-
tion of the longer text. May be equal to the
number of words in the shorter of the two
compared texts, or fixed at some arbitrary
value.
</bodyText>
<subsectionHeader confidence="0.893051">
3.2.3 Semantic extension parameters
</subsectionHeader>
<bodyText confidence="0.9998685">
The following factors regulate the impact of addi-
tional words retrieved from WordNet:
</bodyText>
<listItem confidence="0.984435764705882">
• Synset limit: The maximum number of
synsets (word senses) retrieved from Word-
Net per each word. Can be controlled by
word ranks returned from the summariser.
• Synonym limit: The maximum number of
synonyms (per synset) added to vector repre-
sentation of the document. Can be controlled
by word ranks.
• WordNet relations: The types of semantic
and lexical relations used to acquire addi-
tional synsets.
3.2.4 Similarity comparison parameters
• Bag-of-words model: The type of bag-of-
word used for cosine comparisons.
• Semantic match weights: The rules for ad-
justing weights of WordNet words that match
observed words from the other vector.
</listItem>
<subsectionHeader confidence="0.959586">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.999966625">
The above parameters in various combinations
were applied in an extensive series of experiments.
Contrary to our expectations, the results indicate
that the summariser has either no impact or has a
negative effect. Table 1 presents the set of param-
eters that seem to have impact, and the values that
resulted in best scores, as calculated by SemEval
Task 3 evaluation tool against the training dataset.
</bodyText>
<subsectionHeader confidence="0.712434">
3.3.1 Discussion
</subsectionHeader>
<bodyText confidence="0.9995355">
In the course of experiments we consistently ob-
served higher performance when all words from
both compared documents were included, as op-
posed to selecting top-ranked words from the
longer document. Furthermore, less restrictive cri-
teria for merging tended to give better results.
</bodyText>
<page confidence="0.983472">
232
</page>
<table confidence="0.9998116">
Parameter Value
Word limit no limit
POS JJ, NN, V
Merging criteria lemma, ignore POS
Custom stopword list yes
Synset limit 15
Synonym limit no limit
WordNet relations similar to, pertainym,
hypernym
Bag-of-words model binary
</table>
<tableCaption confidence="0.999895">
Table 1: Parameter values yielding the best scores.
</tableCaption>
<bodyText confidence="0.997644222222222">
We noticed clear improvement after extend-
ing word vectors with synonyms and related
words. WordNet relations that contributed most
are similar to, hypernym (ISA relation), pertainym
(relational adjective) and derivationally related
form. The results obtained before and after apply-
ing summarisation and lexico-semantic expansion
(while keeping other parameters fixed at values re-
ported in Table 1) are shown in Table 2.
</bodyText>
<table confidence="0.997644166666667">
Expansion Yes
Word ranks❵❵❵❵No
Ignored 0.728 0.755
Used to select top-rank words 0.690 0.716
Used to control synset limit N/A 0.752
Used to weight vector counts 0.694 N/A
</table>
<tableCaption confidence="0.9158535">
Table 2: The effects of applying text summarisa-
tion and lexico-semantic expansion.
</tableCaption>
<bodyText confidence="0.975695666666667">
Table 3 summarises the most efficient rules for
adjusting weights in word vectors when a match
has been found between an observed word from
one vector and a WordNet word in the other vec-
tor. The rules are as follows: (1) If the match is be-
tween an observed word from the paragraph vector
and a WordNet word from the sentence vector, the
weight of both is set to 0.25; (2) If the match is be-
tween an observed word from the sentence vector
and the WordNet word from the paragraph vector,
the weight of both is set to 0.75; (3) If the match is
between two WordNet words, one from the para-
graph and one from the sentence, the weight of
both is set to whichever synweight is higher; (4)
If the match is between two observed words, the
weight of both is set to 1.
We received slightly better results after setting a
limit on the number of included word senses, and
</bodyText>
<table confidence="0.994419">
Sent. Obs. word WordNet word
��������
Paragr.
Observed word 1.0 0.25
WordNet word 0.75 max(synweight)
</table>
<tableCaption confidence="0.999694">
Table 3: Optimal weights for semantic match.
</tableCaption>
<bodyText confidence="0.9946335">
after ignoring a few verbs with particularly broad
meaning.
</bodyText>
<subsectionHeader confidence="0.969735">
3.3.2 Break-down into categories
</subsectionHeader>
<bodyText confidence="0.918504166666666">
Pearson correlation between gold standard and the
submitted results was 0.785. Table 4 shows the
correlations within each category, both for the test
set and the train set. The results are very con-
sistent across datasets, except for Reviews which
scored much lower with the test data. The over-
all result was lower with the training data because
of higher number of examples in Metaphoric cat-
egory, where the performance of our system was
extremely poor.
Category Test data Train data
newswire 0.907 0.926
cqa 0.778 0.779
metaphoric 0.099 -0.16
scientific 0.856 -
travel 0.880 0.887
review 0.752 0.884
overall 0.785 0.755
</bodyText>
<tableCaption confidence="0.954952">
Table 4: Break-down of the results.
</tableCaption>
<sectionHeader confidence="0.997482" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99996725">
We described our approach, parameters used in the
system, and the results of experiments. Text sum-
marisation didn’t prove to be helpful. One possi-
ble explanation of the neutral or negative effect of
summarisation is the small size of the texts units:
with the limited number of words available for
comparison, any procedure reducing this already
scarce set may be disadvantageous.
The results benefited from adding synonyms
and semantically and lexically related words.
Lemmatisation and merging same-lemma words
regardless the POS, as well as ignoring very gen-
eral verbs seem to be helpful.
The best performance has been observed in
Newswire category. Finally, given that the simi-
larity metric used by the system is essentially a
</bodyText>
<page confidence="0.992789">
233
</page>
<bodyText confidence="0.997359666666667">
function of word overlap between the two texts,
it is not surprising that the system struggled with
metaphorically related texts.
</bodyText>
<sectionHeader confidence="0.995946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999895777777778">
Sergey Brin and Lawrence Page. 1998. The Anatomy
of Large-Scale Hypertextual Web Search Engine.
In Computer Networks and ISDN Systems, 30(1-
7):107–117.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014). August 23-24, 2014, Dublin,
Ireland.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Texts. In Proceedings of
EMNLP 2004:404–411, Barcelona, Spain.
George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
Vol. 38, No. 11:39–41.
</reference>
<page confidence="0.998379">
234
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.816348">
<title confidence="0.9771735">DIT: Summarisation and Semantic Expansion in Evaluating Semantic Similarity</title>
<author confidence="0.994227">Magdalena</author>
<affiliation confidence="0.9989195">IBM Technology Dublin Software</affiliation>
<email confidence="0.998162">magdalena.kacmajor@ie.ibm.com</email>
<author confidence="0.994237">D John</author>
<affiliation confidence="0.994983">Applied Intelligence Research Dublin Institute of</affiliation>
<email confidence="0.892699">john.d.kelleher@dit.ie</email>
<abstract confidence="0.998640125">This paper describes an approach to implementing a tool for evaluating semantic similarity. We investigated the potential benefits of (1) using text summarisation to narrow down the comparison to the most important concepts in both texts, and (2) leveraging WordNet information to increase usefulness of cosine comparisons of short texts. In our experiments, text summarisation using a graph-based algorithm did not prove to be helpful. Semantic and lexical expansion based upon word relationships defined in WordNet increased the agreement of cosine similarity values with human similarity judgements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The Anatomy of Large-Scale Hypertextual Web Search Engine.</title>
<date>1998</date>
<booktitle>In Computer Networks and ISDN Systems,</booktitle>
<pages>30--1</pages>
<contexts>
<context position="4704" citStr="Brin and Page, 1998" startWordPosition="714" endWordPosition="717">ve – based on same lemma, or even same lemma ignoring POS; (c) Stopword removal, we apply a customized stopword list to exclude verbs that have very general meaning. In this way, each text unit is processed to produce a filtered set of tokens which at the next step can be directly transformed into nodes in the graph representation of the text. Dependency and coreference annotations will be used for defining edges in the graph. 2.3 Summarisation system Summarisation has been implemented using TextRank (Mihalcea and Tarau, 2004), an iterative graph-based ranking algorithm derived from PageRank (Brin and Page, 1998). The ranking is based on the following principle: when node i links to node j, a vote is cast that increases the rank of node j. The strength of the vote depends on the importance (rank) of the casting node, thus the algorithm is run iteratively until the ranks stop changing beyond a given threshold, or until a specified limit of iterations is reached. To apply this algorithm to paragraphs and sentences, our system builds a graph representation for each of these text units, with nodes representing the tokens selected and merged at the preceding stage. The nodes are connected by cooccurrence (</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The Anatomy of Large-Scale Hypertextual Web Search Engine. In Computer Networks and ISDN Systems, 30(1-7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval-2014 Task 3: Cross-Level Semantic Similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1148" citStr="Jurgens et al, 2014" startWordPosition="161" endWordPosition="164">oth texts, and (2) leveraging WordNet information to increase usefulness of cosine comparisons of short texts. In our experiments, text summarisation using a graph-based algorithm did not prove to be helpful. Semantic and lexical expansion based upon word relationships defined in WordNet increased the agreement of cosine similarity values with human similarity judgements. 1 Introduction This paper describes a system that addresses the problem of assessing semantic similarity between two different-sized texts. The system has been applied to SemEval-2014 Task 3, Cross-Level Semantic Similarity (Jurgens et al, 2014). The application is limited to a single comparison type, that is, paragraph to sentence. The general approach taken can be characterised as text summarisation followed by a process of semantic expansion and finally similarity computation using cosine similarity. The rationale for applying summarisation is to focus the comparison on the most important elements of the text by selecting key words to be used in the similarity comparison. This summarisation approach is based on the assumption that if summary of a paragraph is similar to the summary sentence paired with the paragraph in the task da</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. SemEval-2014 Task 3: Cross-Level Semantic Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014). August 23-24, 2014, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing Order into Texts.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004:404–411,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="4616" citStr="Mihalcea and Tarau, 2004" startWordPosition="701" endWordPosition="704"> merging, the criteria for merging can be more restrictive (same word form) or less restrictive – based on same lemma, or even same lemma ignoring POS; (c) Stopword removal, we apply a customized stopword list to exclude verbs that have very general meaning. In this way, each text unit is processed to produce a filtered set of tokens which at the next step can be directly transformed into nodes in the graph representation of the text. Dependency and coreference annotations will be used for defining edges in the graph. 2.3 Summarisation system Summarisation has been implemented using TextRank (Mihalcea and Tarau, 2004), an iterative graph-based ranking algorithm derived from PageRank (Brin and Page, 1998). The ranking is based on the following principle: when node i links to node j, a vote is cast that increases the rank of node j. The strength of the vote depends on the importance (rank) of the casting node, thus the algorithm is run iteratively until the ranks stop changing beyond a given threshold, or until a specified limit of iterations is reached. To apply this algorithm to paragraphs and sentences, our system builds a graph representation for each of these text units, with nodes representing the toke</context>
<context position="8680" citStr="Mihalcea and Tarau, 2004" startWordPosition="1391" endWordPosition="1395"> allowed into the graph, e.g. only nouns and verbs, or nouns, verbs and adjectives. • Merging criteria: The principle by which we decide whether two tokens should be represented by the same node in the graph. • Excluded verbs: The contents of the stopword list. 3.2.2 Summarisation parameters These parameters control the structure of the graph and the results yielded by TextRank algorithm. The types of nodes in the graph are already decided at the pre-processing stage. • Relation type: In order to link the nodes (words) in the graph representation of a document, we use co-occurrence relations (Mihalcea and Tarau, 2004), dependency relations and coreference relations. The two latter are defined based on the Stanford CoreNLP annotations, whereas a co-occurrence edge is created when two words appear in the text within a word span of a specified length. The co-occurrence relation comes with two additional parameters: – Window size: Maximum number of words constituting the span. – Window application: The window can be applied before or after filtering away tokens of unwanted POS, i.e. we can require either the co-occurrence within the original text or in the filtered text. • Graph type: A document can be represe</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing Order into Texts. In Proceedings of EMNLP 2004:404–411, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, Vol. 38, No. 11:39–41.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>