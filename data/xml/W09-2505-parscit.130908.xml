<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000039">
<title confidence="0.9967325">
Optimizing Textual Entailment Recognition Using Particle Swarm
Optimization
</title>
<author confidence="0.997201">
Yashar Mehdad
</author>
<affiliation confidence="0.999083">
University of Trento and FBK - Irst
</affiliation>
<address confidence="0.606128">
Trento, Italy
</address>
<email confidence="0.998302">
mehdad@fbk.eu
</email>
<sectionHeader confidence="0.993882" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980117647059">
This paper introduces a new method to im-
prove tree edit distance approach to tex-
tual entailment recognition, using particle
swarm optimization. Currently, one of the
main constraints of recognizing textual en-
tailment using tree edit distance is to tune
the cost of edit operations, which is a dif-
ficult and challenging task in dealing with
the entailment problem and datasets. We
tried to estimate the cost of edit operations
in tree edit distance algorithm automati-
cally, in order to improve the results for
textual entailment. Automatically estimat-
ing the optimal values of the cost opera-
tions over all RTE development datasets,
we proved a significant enhancement in
accuracy obtained on the test sets.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999570666666667">
One of the main aspects of natural languages is to
express the same meaning in many possible ways,
which directly increase the language variability
and emerges the complex structure in dealing with
human languages. Almost all computational lin-
guistics tasks such as Information Retrieval (IR),
Question Answering (QA), Information Extrac-
tion (IE), text summarization and Machine Trans-
lation (MT) have to cope with this notion. Textual
Entailment Recognition was proposed by (Dagan
and Glickman, 2004), as a generic task in order to
conquer the problem of lexical, syntactic and se-
mantic variabilities in languages.
Textual Entailment can be explained as an as-
sociation between a coherent text (T) and a lan-
guage expression, called hypothesis (H) such that
entailment function for the pair T-H returns the
true value when the meaning of H can be inferred
from the meaning of T and false, otherwise.
Amongst the approaches to the problem of tex-
tual entailment, some methods utilize the no-
</bodyText>
<note confidence="0.832056666666667">
Bernardo Magnini
FBK - Irst
Trento, Italy
</note>
<email confidence="0.956371">
magnini@fbk.eu
</email>
<bodyText confidence="0.99990856097561">
tion of distance between the pair of T and H as
the main feature which separates the entailment
classes (positive and negative). One of the suc-
cessful algorithms implemented Tree Edit Dis-
tance (TED), based on the syntactic features that
are represented in the structured parse tree of each
string (Kouylekov and Magnini, 2005). In this
method the distance is computed as the cost of
the edit operations (insertion, deletion and substi-
tution) that transform the text T into the hypothesis
H. Each edit operation has an associated cost and
the entailment score is calculated such that the set
of operations would lead to the minimum cost.
Generally, the initial cost is assigned to each
edit operation empirically, or based on the ex-
pert knowledge and experience. These methods
emerge a critical problem when the domain, field
or application is new and the level of expertise and
empirical knowledge is very limited. In dealing
with textual entailment, (Kouylekov and Magnini,
2006) tried to experiment different cost values
based on various linguistics knowledge and prob-
abilistics estimations. For instance, they defined
the substitution cost as a function of similarity
between two nodes, or, for insertion cost, they
employed Inverse Document Frequency (IDF) of
the inserted node. However, the results could not
proven to be optimal.
Other approaches towards estimating the cost
of operations in TED tried to learn a generic or
discriminative probabilistic model (Bernard et al.,
2008; Neuhaus and Bunke, 2004) from the data,
without concerning the optimal value of each op-
eration. One of the drawbacks of those approaches
is that the cost values of edit operations are hidden
behind the probabilistic model. Additionally, the
cost can not be weighted or varied according to
the tree context and node location (Bernard et al.,
2008).
In order to overcome these drawbacks, we are
proposing a stochastic method based on Particle
</bodyText>
<page confidence="0.98849">
36
</page>
<note confidence="0.9851215">
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 36–43,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9998837">
Swarm Optimization (PSO), to estimate the cost
of each edit operation for textual entailment prob-
lem. Implementing PSO, we try to learn the op-
timal cost for each operation in order to improve
the prior textual entailment model. In this paper,
the goal is to automatically estimate the best possi-
ble operation costs on the development set. A fur-
ther advantage of such method, besides automatic
learning of the operation costs, is being able to in-
vestigate the cost values to better understand how
TED approaches the data in textual entailment.
The rest of the paper is organized as follows:
After describing the TED approach to textual en-
tailment in the next section, PSO optimization al-
gorithm and our method in applying it to the prob-
lem are explained in sections 4 and 5. Then we
present our experimental setup as well as the re-
sults, in detail. Finally, in the conclusion, the main
advantages of our approach are reviewed and fur-
ther developments are proposed accordingly.
</bodyText>
<sectionHeader confidence="0.8272825" genericHeader="introduction">
2 Tree Edit Distance and Textual
Entailment
</sectionHeader>
<bodyText confidence="0.9999734375">
One of the approaches to textual entailment is
based on the Tree Edit Distance (TED) between
T and H. The tree edit distance measure is a simi-
larity metric for rooted ordered trees. This metric
was initiated by (Tai, 1979) as a generalization of
the string edit distance problem and was improved
by (Zhang and Shasha, 1989) and (Klein, 1998).
The distance is computed as the cost of editing
operations (i.e. insertion, deletion and substitu-
tion), which are required to transform the text T
into the hypothesis H, while each edit operation on
two text fragments A and B (denoted as A —* B)
has an associated cost (denoted as γ (A —* B)). In
textual entailment context, the edit operations are
defined in the following way based on the depen-
dency parse tree of T and H:
</bodyText>
<listItem confidence="0.99848305882353">
• Insertion (λ —* A): insert a node A from
the dependency tree of H into the depen-
dency tree of T. When a node is inserted it
is attached to the dependency relation of the
source label.
• Deletion (A —* λ): delete a node A from
the dependency tree of T. When A is deleted
all its children are attached to the parent of
A. It is not required to explicitly delete the
children of A, as they are going to be either
deleted or substituted in a following step.
• Substitution (A —* B): change the label of
a node A in the source tree into a label of a
node B of the target tree. In the case of substi-
tution, the relation attached to the substituted
node is changed with the relation of the new
node.
</listItem>
<bodyText confidence="0.9966425625">
According to (Zhang and Shasha, 1989), the min-
imum cost mappings of all the descendants of
each node has to be computed before the node
is encountered, so the least-cost mapping can be
selected right away. To accomplish this the al-
gorithm keeps track of the keyroots of the tree,
which are defined as a set that contains the root
of the tree plus all nodes which have a left sibling.
This problem can be easily solved using recursive
methods (Selkow, 1977), or as it was suggested in
(Zhang and Shasha, 1989) by dynamic program-
ming. (Zhang and Shasha, 1989) defined the rel-
evant subproblems of tree T as the prefixes of all
special subforests rooted in the keyroots. This ap-
proach computes the TED (δ) by the following
equations:
</bodyText>
<equation confidence="0.999535727272727">
δ(FT, θ) _
δ(FT − rFT, θ) + γ(rFT —* λ) (1)
δ(θ, FH) _
δ(θ, FH − rFH) + γ(λ —* rFH) (2)
δ(FT, FH) _
{ δ(FT − rFT, FH) + γ(rFT —* λ)
δ(FT, FH − rFH) + γ(λ —* rFH)
δ(FT (rFT ), FH(rFH ))+
δ(FT − T (rFT ), FH − H(rFH))+
γ(rFT —* rFH )
(3)
</equation>
<bodyText confidence="0.99992525">
where FT and FH are forests of T and H, while
rFT and rFH are the rightmost roots of the trees
in FT and FH respectively. θ is an empty forest.
Moreover, FT (rFT ) and FH(rFH) are the forests
rooted in rFT and rFH respectively.
Estimating δ as the bottom line of the compu-
tation is directly related to the cost of each oper-
ation. Moreover, the cost of edit operations can
simply change the way that a tree is transformed
to another. As Figure 11 shows (Demaine et al.,
2007), there could exist more than one edit script
for transforming each tree to another. Based on the
</bodyText>
<footnote confidence="0.680076">
1The example adapted from (Demaine et al., 2007)
</footnote>
<bodyText confidence="0.535757">
min
</bodyText>
<page confidence="0.993633">
37
</page>
<figureCaption confidence="0.999782">
Figure 1: Two possible edit scripts to transform one tree to another.
</figureCaption>
<bodyText confidence="0.999925791666667">
main definition of this approach, TED is the cost
of minimum cost edit script between two trees.
The entailment score for a pair is calculated on
the minimal set of edit operations that transform
the dependency parse tree of T into H. An entail-
ment relation is assigned to a T-H pair where the
overall cost of the transformations is below a cer-
tain threshold. The threshold, which corresponds
to tree edit distace, is empirically estimated over
the dataset. This method was implemented by
(Kouylekov and Magnini, 2005), based on the al-
gorithm by (Zhang and Shasha, 1989).
In this method, a cost value is assigned to each
operation initially, and the distance is computed
based on the initial cost values. Considering that
the distance can vary in different datasets, con-
verging to an optimal set of values for operations
is almost empirically impossible. In the follow-
ing sections, we propose a method for estimat-
ing the optimum set of values for operation costs
in TED algorithm dealing with textual entailment
problem. Our method is built on adapting PSO
optimization approach as a search process to auto-
mate the procedure of the cost estimation.
</bodyText>
<sectionHeader confidence="0.981017" genericHeader="method">
3 Particle Swarm Optimization
</sectionHeader>
<bodyText confidence="0.999984068181818">
PSO is a stochastic optimization technique which
was introduced based on the social behaviour of
bird flocking and fish schooling (Eberhart et al.,
2001). It is one of the population-based search
methods which takes advantage of the concept of
social sharing of information. The main struc-
ture of this algorithm is not very different from
other evolutionary techniques such as Genetic Al-
gorithms (GA); however, the easy implementation
and less complexity of PSO, as two main charac-
teristics, are good motivations to apply this opti-
mization approach in many areas.
In this algorithm each particle can learn from
the experience of other particles in the same pop-
ulation (called swarm). In other words, each parti-
cle in the iterative search process, would adjust its
flying velocity as well as position not only based
on its own acquaintance, but also other particles’
flying experience in the swarm. This algorithm has
found efficient in solving a number of engineering
problems. In the following, we briefly explain the
main concepts of PSO.
To be concise, for each particle at each itera-
tion, the position Xi (Equation 4) and velocity Vi
(Equation 5) is updated. Xbi is the best position
of the particle during its past routes and Xgi is
the best global position over all routes travelled by
the particles of the swarm. r1 and r2 are random
variables drawn from a uniform distribution in the
range [0,1], while c1 and c2 are two acceleration
constants regulating the relative velocities with re-
spect to the best local and global positions. The
weight w is used as a tradeoff between the global
and local best positions and its value is usually
selected slightly less than 1 for better global ex-
ploration (Melgani and Bazi, 2008). The optimal
position is computed based on the fitness func-
tion defined in association with the related prob-
lem. Both position and velocity are updated dur-
ing the iterations until convergence is reached or
iterations attain the maximum number defined by
the user. This search process returns the best fit-
ness function over the particles, which is defined
as the optimized solution.
</bodyText>
<equation confidence="0.998745333333333">
Xi = Xi + Vi (4)
Vi = wVi + c1r1(Xbi − Xi)
+ c2r2(Xgi − Xi) (5)
</equation>
<bodyText confidence="0.99439875">
Algorithm 1 shows a simple pseudo code of
how this optimization algorithm works. In the rest
of the paper, we describe our method to integrate
this algorithm with TED.
</bodyText>
<page confidence="0.995448">
38
</page>
<bodyText confidence="0.42226">
Algorithm 1 PSO algorithm
</bodyText>
<sectionHeader confidence="0.991751" genericHeader="method">
4 Automatic Cost Estimation
</sectionHeader>
<bodyText confidence="0.999982636363636">
One of the challenges in applying TED for rec-
ognizing textual entailment is estimating the cost
of each edit operation which transforms the text T
into the hypothesis H in an entailment pair. Since
the cost of edit operations can directly affect the
distance, which is the main criteria to measure the
entailment, it is not trivial to estimate the cost of
each operation. Moreover, considering that imply-
ing different costs for edit operations can affect the
results in different data sets and approaches, it mo-
tivates the idea of optimizing the cost values.
</bodyText>
<subsectionHeader confidence="0.959491">
4.1 PSO Setup
</subsectionHeader>
<bodyText confidence="0.999939181818182">
One of the most important steps in applying PSO
is to define a fitness function which could lead the
swarm to the optimized particles based on the ap-
plication and data. The choice of this function
is very crucial, since PSO evaluates the quality
of each candidate particle for driving the solution
space to optimization, on the basis of the fitness
function. Moreover, this function should possibly
improve the textual entailment recognition model.
In order to attain these goals, we tried to define
two main fitness functions as follows.
</bodyText>
<listItem confidence="0.554296">
1. Bhattacharyya Distance: This measure was
proposed by (Bhattacharyya, 1943) as a sta-
tistical measure to determine the similarity
or distance between two discrete probabil-
ity distributions. In binary classification, this
</listItem>
<bodyText confidence="0.999563428571429">
method is widely used to measure the dis-
tance between two different classes. In the
studies by (Fukunaga, 1990), Bhattacharyya
distance was occluded to be one of the most
effective measure specifically for estimating
the separability of two classes. Figure 2
shows the intuition behind this measure.
</bodyText>
<figureCaption confidence="0.9975645">
Figure 2: Bhattacharyya distance between two
classes with similar variances.
</figureCaption>
<bodyText confidence="0.996499571428571">
Bhattacharyya distance is calculated based on
the covariance (Q) and mean (µ) of each dis-
tribution based on its simplest formulation in
Equation 6 (Reyes-Aldasoro and Bhalerao,
2006). Maximizing the distance between
the classes would result a better separability
which aims to a better classification results.
Furthermore, estimating the costs using this
function would indirectly improve the perfor-
mance specially in classification problems. It
could be stated that, maximizing the Bhat-
tacharyya distance would increase the separa-
bility of two entailment classes which result
in a better performance.
</bodyText>
<equation confidence="0.991473142857143">
2 2
BD(c1, c2) = 1ln{ 1(Qc1 + Qc2 + 2)}
2
4 4 Q2 U2�c1
2
22
4 Qc1 + Qc2
</equation>
<listItem confidence="0.941495">
2. Accuracy: Accuracy or any performance
</listItem>
<bodyText confidence="0.994021">
measure obtained from a TED based system,
can define a good fitness function in optimiz-
ing the cost values. Since maximizing the
accuracy would directly increase the perfor-
mance of the system or enhance the model
to solve the problem, this measure is a pos-
sible choice to adapt in order to achieve our
aim. In this method, trying to maximize the
fitness function will compute the best model
based on the optimal cost values in the parti-
cle space of PSO algorithm.
In other words, by defining the accuracy ob-
tained from 10 fold cross-validation over the
</bodyText>
<figure confidence="0.984518166666667">
for all particles do
Initialize particle
end for
while Convergence or maximum iteration
do
for all particles do
Calculate fitness function
if fitness function value &gt; Xbi then
Xbi ⇐ fitness function value
end if
end for
choose the best particle amongst all in Xgi
for all particles do
calculate Vi
update Xi
end for
end while
return best particle
</figure>
<page confidence="0.998284">
39
</page>
<bodyText confidence="0.999959666666667">
development set, as the fitness function, we
could estimate the optimized cost of the edit
operations. Maximizing the accuracy gained
in this way, would lead to find the set of edit
operation costs which directly increases our
accuracy, and consequently guides us to the
main goal of optimization.
In the following section, the procedure of esti-
mating the optimal costs are described in detail.
</bodyText>
<subsectionHeader confidence="0.9859865">
4.2 Integrating TED with PSO for Textual
Entailment Problem
</subsectionHeader>
<bodyText confidence="0.954729566666667">
The procedure describing the proposed system to
optimize and estimate the cost of edit operations
in TED applying PSO algorithm is as follows.
a) Initialization
Step 1) Generate a random swarm of particles
(in a simple case each particle is de-
fined by the cost of three operations).
Step 2) For each position of the particle from
the swarm, obtain the fitness function
value (Bhattacharyya distance or accu-
racy) over the training data.
Step 3) Set the best position of each particle
with its initial position (Xbi).
b) Search
Step 4) Detect the best global position (Xgi)
in the swarm based on maximum value
of the fitness function over all explored
routes.
Step 5) Update the velocity of each particle
(Vi).
Step 6) Update the position of each particle
(Xi). In this step, by defining the
boundaries, we could stop the particle
to exit the allowed search space.
Step 7) For each candidate particle calculate
the fitness function (Bhattacharyya
distance or accuracy).
Step 8) Update the best position of each parti-
cle if the current position has a larger
value.
</bodyText>
<listItem confidence="0.455597">
c) Convergence
</listItem>
<bodyText confidence="0.909434">
Step 9) Run till the maximum number of iter-
ation (in our case set to 10) is reached
or start the search process.
</bodyText>
<sectionHeader confidence="0.224984" genericHeader="method">
d) Results
</sectionHeader>
<bodyText confidence="0.994487333333333">
Step 10) Return the best fitness function value
and the best particle. In this step the
optimum costs are returned.
Following the steps above, in contrary to de-
termine the entailment relation applying tree edit
distance, the operation costs can be automatically
estimated and optimized. In this process, both fit-
ness functions could be easily compared and the
cost values leading to the better model would be
selected. In the following section, the experimen-
tal procedure for obtaining the optimal costs by
exploiting the PSO approach to TE is described.
</bodyText>
<sectionHeader confidence="0.998298" genericHeader="method">
5 Experimental Design
</sectionHeader>
<bodyText confidence="0.9999902">
In our experiments we show an increase in the per-
formance of TED based approach to textual en-
tailment, by optimizing the cost of edit operations.
In the following subsections, the framework and
dataset of our experiments are elaborated.
</bodyText>
<subsectionHeader confidence="0.965124">
5.1 Dataset Description
</subsectionHeader>
<bodyText confidence="0.999527888888889">
Our experiments were conducted on the basis
of the Recognizing Textual Entailment (RTE)
datasets2, which were developed under PASCAL
RTE challenge. Each RTE dataset includes its own
development and test set, however, RTE-4 was re-
leased only as a test set and the data from RTE-1
to RTE-3 were used as development set. More de-
tails about the RTE datasets are illustrated in Table
5.1.
</bodyText>
<table confidence="0.997942857142857">
Number of pairs
Development Test
Datasets YES NO YES NO
RTE-1 283 284 400 400
RTE-2 400 400 400 400
RTE-3 412 388 410 390
RTE-4 — — 500 500
</table>
<tableCaption confidence="0.999666">
Table 1: RTE-1 to RTE-4 datasets.
</tableCaption>
<subsectionHeader confidence="0.991331">
5.2 Experimental Framework
</subsectionHeader>
<bodyText confidence="0.994506">
In our experiments, in order to deal with TED
approach to textual entailment, we used EDITS3
package (Edit Distance Textual Entailment Suite)
</bodyText>
<footnote confidence="0.99797075">
2http://www.pascal-network.org/Challenges/RTE1-4
3The EDITS system has been supported by the EU-
funded project QALL-ME (FP6 IST-033860). Available at
http://edits.fbk.eu/
</footnote>
<page confidence="0.997928">
40
</page>
<bodyText confidence="0.994164823529412">
(Magnini et al., 2009). This system is an open
source software based on edit distance algorithms,
and computes the T-H distance as the cost of the
edit operations (i.e. insertion, deletion and substi-
tution) that are necessary to transform T into H.
By defining the edit distance algorithm and a cost
scheme (assigning a cost to the edit operations),
this package is able to learn a TED threshold, over
a set of string pairs, to decide if the entailment ex-
ists in a pair.
In addition, we partially exploit the JSwarm-
PSO4 (Cingolani, 2005) package, with some adap-
tations, as an implementation of PSO algorithm.
Each pair in the datasets is converted to two syn-
tactic dependency parse trees using the Stanford
statistical parser5, developed in the Stanford uni-
versity NLP group by (Klein and Manning, 2003).
</bodyText>
<figureCaption confidence="0.744434">
Figure 3: Five main steps of the experimental
framework.
</figureCaption>
<bodyText confidence="0.999921272727273">
In order to take advantage of PSO optimization
approach, we integrated EDITS and JSwarm-PSO
to provide a flexible framework for the experi-
ments (Figure 5.3). In this way, we applied the
defined fitness functions in the integrated system.
The Bhattacharyya distance between two classes
(YES and NO), in each experiment, could be com-
puted based on the TED score of each pair in the
dataset. Moreover, the accuracy, by default, is
computed by EDITS over the training set based
on 10-fold cross-validation.
</bodyText>
<subsectionHeader confidence="0.998151">
5.3 Experimental Scheme
</subsectionHeader>
<bodyText confidence="0.99934225">
We conducted six different experiments in two sets
on each RTE dataset. The costs were estimated on
the training set and the results obtained based on
the estimated costs over the test set. In the first
</bodyText>
<footnote confidence="0.998454">
4http://jswarm-pso.sourceforge.net/
5http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<bodyText confidence="0.999947930232558">
set of experiments, we set a simple cost scheme
based on three operations. Implementing this cost
scheme, we expect to optimize the cost of each
edit operation without considering that the opera-
tion costs may vary based on different character-
istics of a node, such as size, location or content.
The results were obtained considering three dif-
ferent settings: 1) the random cost assignment; 2)
assigning the cost based on the human expertise
knowledge and intuition (called Intuitive), and 3)
automatic estimated and optimized cost for each
operation. In the second case, we used the same
scheme which was used in EDITS by its develop-
ers (Magnini et al., 2009).
In the second set of experiments, we tried to
compose an advanced cost scheme with more
fine-grained operations to assign a weight to the
edit operations based on the characteristics of the
nodes. For example if a node is in the list of stop-
words, the deletion cost is set to zero. Otherwise,
the cost of deletion would be equal to the number
of words in H multiplied by word’s length (num-
ber of characters). Similarly, the cost of inserting
a word w in H is set to 0 if w is a stop word,
and to the number of words in T multiplied by
words length otherwise. The cost of substituting
two words is the Levenshtein distance (i.e. the edit
distance calculated at the level of characters) be-
tween their lemmas, multiplied by the number of
words in T, plus number of words in H. By this in-
tuition, we tried to optimize nine specialized costs
for edit operations (i.e. each particle is defined by
9 parameters to be optimized). We conducted the
experiments using all three cases mentioned in the
simple cost scheme.
In each experiment, we applied both fitness
functions in the optimization; however, at the final
phase, the costs which led to the maximum results
were chosen as the estimated operation costs. In
order to save breath and time, we set the number
of iterations to 10, in addition, the weight ω was
set to 0.95 for better global exploration (Melgani
and Bazi, 2008).
</bodyText>
<sectionHeader confidence="0.999981" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999909333333333">
Our results are summarized in Table 2. We show
the accuracy gained by a distance-based (word-
overlap) baseline for textual entailment (Mehdad
and Magnini, 2009) to be compared with the re-
sults achieved by the random, intuitive and op-
timized cost schemes using EDITS system. For
</bodyText>
<page confidence="0.998668">
41
</page>
<table confidence="0.9907188">
Data set
Model RTE-4 RTE-3 RTE-2 RTE-1
Random 49.6 53.62 50.37 50.5
Simple Intuitive 51.3 59.6 56.5 49.8
Optimized 56.5 61.62 58 58.12
Random 53.60 52.0 54.62 53.5
Advanced Intuitive 57.6 59.37 57.75 55.5
Optimized 59.5 62.4 59.87 58.62
Baseline 55.2 60.9 54.8 51.4
RTE-4 Challenge 57.0
</table>
<tableCaption confidence="0.999886">
Table 2: Comparison of accuracy on all RTE datasets based on optimized and unoptimized cost schemes.
</tableCaption>
<bodyText confidence="0.999716023809524">
the better comparison, we also present the results
of the EDITS system in RTE-4 challenge using a
combination of different distances as features for
classification (Cabrio et al., 2008).
In the first experiment, we estimated the cost of
each operation using the simple cost scheme. Ta-
ble 2 shows that in all datasets, accuracy improved
up to 9% by optimizing the cost of each edit opera-
tion. Results prove that the optimized cost scheme
enhances the quality of the system performance,
even more than the cost scheme used by experts
(Intuitive cost scheme) (Magnini et al., 2009).
Furthermore, in the second set of experiments,
using the fine-grained and weighted cost scheme
for edit operations we could achieve the highest re-
sults in accuracy. The chart in Figure 4, illustares
that all optimized results outperform the word-
overlap baseline for textual entailment as well as
the accuracy obtained in RTE-4 challenge using
combination of different distances as features for
classification (Cabrio et al., 2008).
By exploring the estimated optimal cost of each
operation, another interesting point was discov-
ered. The estimated cost of deletion in the first
set of experiments was 0, which means that delet-
ing a node from the dependency tree of T does not
effect the quality of results. This proves that by
setting different cost schemes, we could explore
even some linguistics phenomena which exists in
the entailment dataset. Studying the dataset from
this point of view might be interesting to find some
hidden information which can not be explored eas-
ily.
In addition, the optimized model can reflect
more consistency and stability (from 58 to 62 in
accuracy) than other models, while in unoptimized
models the result varies more, on different datasets
(from 50 in RTE-1 to 59 in RTE-3). Moreover, we
believe that by changing some parameters such as
maximum number of iterations, or by defining a
better cost scheme, there could be still a room for
improvement.
</bodyText>
<figureCaption confidence="0.942121">
Figure 4: Accuracy obtained by different experi-
mental setups.
</figureCaption>
<sectionHeader confidence="0.99742" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999248">
In this paper, we proposed a novel approach for es-
timating the cost of edit operations for the tree edit
distance approach to textual entailment. With this
work we illustrated another step forward in im-
proving the foundation of working with distance-
based algorithms for textual entailment. The ex-
perimental results confirm our working hypothe-
sis that by improving the results in applying tree
edit distance for textual entailment, besides out-
performing the distance-based baseline for recog-
</bodyText>
<page confidence="0.996996">
42
</page>
<bodyText confidence="0.999666625">
nizing textual entailment.
We believe that for further development, ex-
tending the cost scheme to find weighted and
specialized cost operations to deal with different
cases, can lead to more interesting results. Besides
that, exploring and studying the estimated cost of
operations, could be interesting from a linguistics
point of view.
</bodyText>
<sectionHeader confidence="0.998282" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999336142857143">
Besides my special thanks to Farid Melgani
for his helpful ideas, I acknowledge Milen
Kouylekov for his academic and technical sup-
ports. This work has been partially sup-
ported by the three-year project LiveMemories
(http://www.livememories.org/), funded by the
Provincia Autonoma di Trento.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99991612987013">
Marc Bernard, Laurent Boyer, Amaury Habrard, and
Marc Sebban. 2008. Learning probabilistic models
of tree edit distance. Pattern Recogn., 41(8):2611–
2629.
A. Bhattacharyya. 1943. On a measure of diver-
gence between two statistical populations defined by
probability distributions. Bull. Calcutta Math. Soc.,
35:99109.
Elena Cabrio, Milen Kouylekovand, and Bernardo
Magnini. 2008. Combining specialized entailment
engines for rte-4. In Proceedings of TAC08, 4th
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment.
Pablo Cingolani. 2005. Jswarm-pso: Particle swarm
optimization package. Available at http://jswarm-
pso.sourceforge.net/.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic textual entailment: Generic applied modeling of
language variability. In Proceedings of the PAS-
CAL Workshop of Learning Methods for Text Under-
standing and Mining.
E. Demaine, S. Mozes, B. Rossman, and O. Weimann.
2007. An optimal decomposition algorithm for tree
edit distance. In Proceedings of the 34th Inter-
national Colloquium on Automata, Languages and
Programming (ICALP), pages 146–157.
Russell C. Eberhart, Yuhui Shi, and James Kennedy.
2001. Swarm Intelligence. The Morgan Kaufmann
Series in Artificial Intelligence. Morgan Kaufmann.
Keinosuke Fukunaga. 1990. Introduction to statisti-
cal pattern recognition (2nd ed.). Academic Press
Professional, Inc., San Diego, CA, USA.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15, pages 3–10, Cambridge,
MA. MIT Press.
Philip N. Klein. 1998. Computing the edit-distance
between unrooted ordered trees. In ESA ’98: Pro-
ceedings of the 6th Annual European Symposium on
Algorithms, pages 91–102, London, UK. Springer-
Verlag.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance
algorithms. In PASCAL Challenges on RTE, pages
17–20.
Milen Kouylekov and Bernardo Magnini. 2006. Tree
edit distance for recognizing textual entailment: Es-
timating the cost of insertion. In PASCAL RTE-2
Challenge.
Bernardo Magnini, Milen Kouylekov, and Elena
Cabrio. 2009. Edits - edit distance tex-
tual entailment suite user manual. Available at
http://edits.fbk.eu/.
Yashar Mehdad and Bernardo Magnini. 2009. A word
overlap baseline for the recognizing textual entail-
ment task. Available at http://edits.fbk.eu/.
Farid Melgani and Yakoub Bazi. 2008. Classi-
fication of electrocardiogram signals with support
vector machines and particle swarm optimization.
IEEE Transactions on Information Technology in
Biomedicine, 12(5):667–677.
Michel Neuhaus and Horst Bunke. 2004. A proba-
bilistic approach to learning costs for graph edit dis-
tance. In ICPR ’04, pages 389–393, Washington,
DC, USA. IEEE Computer Society.
C. C. Reyes-Aldasoro and A. Bhalerao. 2006. The
bhattacharyya space for feature selection and its ap-
plication to texture segmentation. Pattern Recogn.,
39(5):812–826.
Stanley M. Selkow. 1977. The tree-to-tree editing
problem. Inf. Process. Lett., 6(6):184–186.
Kuo-Chung Tai. 1979. The tree-to-tree correction
problem. J. ACM, 26(3):422–433.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAMJ. Comput., 18(6):1245–1262.
</reference>
<page confidence="0.999833">
43
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.739153">
<title confidence="0.9998495">Optimizing Textual Entailment Recognition Using Particle Swarm Optimization</title>
<author confidence="0.965595">Yashar</author>
<affiliation confidence="0.8875305">University of Trento and FBK - Trento,</affiliation>
<email confidence="0.99765">mehdad@fbk.eu</email>
<abstract confidence="0.999452555555556">This paper introduces a new method to improve tree edit distance approach to textual entailment recognition, using particle swarm optimization. Currently, one of the main constraints of recognizing textual entailment using tree edit distance is to tune the cost of edit operations, which is a difficult and challenging task in dealing with the entailment problem and datasets. We tried to estimate the cost of edit operations in tree edit distance algorithm automatically, in order to improve the results for textual entailment. Automatically estimating the optimal values of the cost operations over all RTE development datasets, we proved a significant enhancement in accuracy obtained on the test sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marc Bernard</author>
<author>Laurent Boyer</author>
<author>Amaury Habrard</author>
<author>Marc Sebban</author>
</authors>
<title>Learning probabilistic models of tree edit distance.</title>
<date>2008</date>
<journal>Pattern Recogn.,</journal>
<volume>41</volume>
<issue>8</issue>
<pages>2629</pages>
<contexts>
<context position="3413" citStr="Bernard et al., 2008" startWordPosition="537" endWordPosition="540">pertise and empirical knowledge is very limited. In dealing with textual entailment, (Kouylekov and Magnini, 2006) tried to experiment different cost values based on various linguistics knowledge and probabilistics estimations. For instance, they defined the substitution cost as a function of similarity between two nodes, or, for insertion cost, they employed Inverse Document Frequency (IDF) of the inserted node. However, the results could not proven to be optimal. Other approaches towards estimating the cost of operations in TED tried to learn a generic or discriminative probabilistic model (Bernard et al., 2008; Neuhaus and Bunke, 2004) from the data, without concerning the optimal value of each operation. One of the drawbacks of those approaches is that the cost values of edit operations are hidden behind the probabilistic model. Additionally, the cost can not be weighted or varied according to the tree context and node location (Bernard et al., 2008). In order to overcome these drawbacks, we are proposing a stochastic method based on Particle 36 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 36–43, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP Swarm </context>
</contexts>
<marker>Bernard, Boyer, Habrard, Sebban, 2008</marker>
<rawString>Marc Bernard, Laurent Boyer, Amaury Habrard, and Marc Sebban. 2008. Learning probabilistic models of tree edit distance. Pattern Recogn., 41(8):2611– 2629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bhattacharyya</author>
</authors>
<title>On a measure of divergence between two statistical populations defined by probability distributions.</title>
<date>1943</date>
<journal>Bull. Calcutta Math. Soc.,</journal>
<pages>35--99109</pages>
<contexts>
<context position="12922" citStr="Bhattacharyya, 1943" startWordPosition="2214" endWordPosition="2215">tup One of the most important steps in applying PSO is to define a fitness function which could lead the swarm to the optimized particles based on the application and data. The choice of this function is very crucial, since PSO evaluates the quality of each candidate particle for driving the solution space to optimization, on the basis of the fitness function. Moreover, this function should possibly improve the textual entailment recognition model. In order to attain these goals, we tried to define two main fitness functions as follows. 1. Bhattacharyya Distance: This measure was proposed by (Bhattacharyya, 1943) as a statistical measure to determine the similarity or distance between two discrete probability distributions. In binary classification, this method is widely used to measure the distance between two different classes. In the studies by (Fukunaga, 1990), Bhattacharyya distance was occluded to be one of the most effective measure specifically for estimating the separability of two classes. Figure 2 shows the intuition behind this measure. Figure 2: Bhattacharyya distance between two classes with similar variances. Bhattacharyya distance is calculated based on the covariance (Q) and mean (µ) </context>
</contexts>
<marker>Bhattacharyya, 1943</marker>
<rawString>A. Bhattacharyya. 1943. On a measure of divergence between two statistical populations defined by probability distributions. Bull. Calcutta Math. Soc., 35:99109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Cabrio</author>
<author>Milen Kouylekovand</author>
<author>Bernardo Magnini</author>
</authors>
<title>Combining specialized entailment engines for rte-4.</title>
<date>2008</date>
<booktitle>In Proceedings of TAC08, 4th PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="23033" citStr="Cabrio et al., 2008" startWordPosition="3893" endWordPosition="3896">imized cost schemes using EDITS system. For 41 Data set Model RTE-4 RTE-3 RTE-2 RTE-1 Random 49.6 53.62 50.37 50.5 Simple Intuitive 51.3 59.6 56.5 49.8 Optimized 56.5 61.62 58 58.12 Random 53.60 52.0 54.62 53.5 Advanced Intuitive 57.6 59.37 57.75 55.5 Optimized 59.5 62.4 59.87 58.62 Baseline 55.2 60.9 54.8 51.4 RTE-4 Challenge 57.0 Table 2: Comparison of accuracy on all RTE datasets based on optimized and unoptimized cost schemes. the better comparison, we also present the results of the EDITS system in RTE-4 challenge using a combination of different distances as features for classification (Cabrio et al., 2008). In the first experiment, we estimated the cost of each operation using the simple cost scheme. Table 2 shows that in all datasets, accuracy improved up to 9% by optimizing the cost of each edit operation. Results prove that the optimized cost scheme enhances the quality of the system performance, even more than the cost scheme used by experts (Intuitive cost scheme) (Magnini et al., 2009). Furthermore, in the second set of experiments, using the fine-grained and weighted cost scheme for edit operations we could achieve the highest results in accuracy. The chart in Figure 4, illustares that a</context>
</contexts>
<marker>Cabrio, Kouylekovand, Magnini, 2008</marker>
<rawString>Elena Cabrio, Milen Kouylekovand, and Bernardo Magnini. 2008. Combining specialized entailment engines for rte-4. In Proceedings of TAC08, 4th PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Cingolani</author>
</authors>
<title>Jswarm-pso: Particle swarm optimization package. Available at http://jswarmpso.sourceforge.net/.</title>
<date>2005</date>
<contexts>
<context position="18993" citStr="Cingolani, 2005" startWordPosition="3221" endWordPosition="3222">Ufunded project QALL-ME (FP6 IST-033860). Available at http://edits.fbk.eu/ 40 (Magnini et al., 2009). This system is an open source software based on edit distance algorithms, and computes the T-H distance as the cost of the edit operations (i.e. insertion, deletion and substitution) that are necessary to transform T into H. By defining the edit distance algorithm and a cost scheme (assigning a cost to the edit operations), this package is able to learn a TED threshold, over a set of string pairs, to decide if the entailment exists in a pair. In addition, we partially exploit the JSwarmPSO4 (Cingolani, 2005) package, with some adaptations, as an implementation of PSO algorithm. Each pair in the datasets is converted to two syntactic dependency parse trees using the Stanford statistical parser5, developed in the Stanford university NLP group by (Klein and Manning, 2003). Figure 3: Five main steps of the experimental framework. In order to take advantage of PSO optimization approach, we integrated EDITS and JSwarm-PSO to provide a flexible framework for the experiments (Figure 5.3). In this way, we applied the defined fitness functions in the integrated system. The Bhattacharyya distance between tw</context>
</contexts>
<marker>Cingolani, 2005</marker>
<rawString>Pablo Cingolani. 2005. Jswarm-pso: Particle swarm optimization package. Available at http://jswarmpso.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
</authors>
<title>Probabilistic textual entailment: Generic applied modeling of language variability.</title>
<date>2004</date>
<booktitle>In Proceedings of the PASCAL Workshop of Learning Methods for Text Understanding and Mining.</booktitle>
<contexts>
<context position="1384" citStr="Dagan and Glickman, 2004" startWordPosition="208" endWordPosition="211">over all RTE development datasets, we proved a significant enhancement in accuracy obtained on the test sets. 1 Introduction One of the main aspects of natural languages is to express the same meaning in many possible ways, which directly increase the language variability and emerges the complex structure in dealing with human languages. Almost all computational linguistics tasks such as Information Retrieval (IR), Question Answering (QA), Information Extraction (IE), text summarization and Machine Translation (MT) have to cope with this notion. Textual Entailment Recognition was proposed by (Dagan and Glickman, 2004), as a generic task in order to conquer the problem of lexical, syntactic and semantic variabilities in languages. Textual Entailment can be explained as an association between a coherent text (T) and a language expression, called hypothesis (H) such that entailment function for the pair T-H returns the true value when the meaning of H can be inferred from the meaning of T and false, otherwise. Amongst the approaches to the problem of textual entailment, some methods utilize the noBernardo Magnini FBK - Irst Trento, Italy magnini@fbk.eu tion of distance between the pair of T and H as the main </context>
</contexts>
<marker>Dagan, Glickman, 2004</marker>
<rawString>Ido Dagan and Oren Glickman. 2004. Probabilistic textual entailment: Generic applied modeling of language variability. In Proceedings of the PASCAL Workshop of Learning Methods for Text Understanding and Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Demaine</author>
<author>S Mozes</author>
<author>B Rossman</author>
<author>O Weimann</author>
</authors>
<title>An optimal decomposition algorithm for tree edit distance.</title>
<date>2007</date>
<booktitle>In Proceedings of the 34th International Colloquium on Automata, Languages and Programming (ICALP),</booktitle>
<pages>146--157</pages>
<contexts>
<context position="7930" citStr="Demaine et al., 2007" startWordPosition="1370" endWordPosition="1373">(FT, FH) _ { δ(FT − rFT, FH) + γ(rFT —* λ) δ(FT, FH − rFH) + γ(λ —* rFH) δ(FT (rFT ), FH(rFH ))+ δ(FT − T (rFT ), FH − H(rFH))+ γ(rFT —* rFH ) (3) where FT and FH are forests of T and H, while rFT and rFH are the rightmost roots of the trees in FT and FH respectively. θ is an empty forest. Moreover, FT (rFT ) and FH(rFH) are the forests rooted in rFT and rFH respectively. Estimating δ as the bottom line of the computation is directly related to the cost of each operation. Moreover, the cost of edit operations can simply change the way that a tree is transformed to another. As Figure 11 shows (Demaine et al., 2007), there could exist more than one edit script for transforming each tree to another. Based on the 1The example adapted from (Demaine et al., 2007) min 37 Figure 1: Two possible edit scripts to transform one tree to another. main definition of this approach, TED is the cost of minimum cost edit script between two trees. The entailment score for a pair is calculated on the minimal set of edit operations that transform the dependency parse tree of T into H. An entailment relation is assigned to a T-H pair where the overall cost of the transformations is below a certain threshold. The threshold, w</context>
</contexts>
<marker>Demaine, Mozes, Rossman, Weimann, 2007</marker>
<rawString>E. Demaine, S. Mozes, B. Rossman, and O. Weimann. 2007. An optimal decomposition algorithm for tree edit distance. In Proceedings of the 34th International Colloquium on Automata, Languages and Programming (ICALP), pages 146–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Russell C Eberhart</author>
<author>Yuhui Shi</author>
<author>James Kennedy</author>
</authors>
<title>Swarm Intelligence.</title>
<date>2001</date>
<booktitle>Series in Artificial Intelligence.</booktitle>
<publisher>The Morgan Kaufmann</publisher>
<contexts>
<context position="9486" citStr="Eberhart et al., 2001" startWordPosition="1631" endWordPosition="1634">dering that the distance can vary in different datasets, converging to an optimal set of values for operations is almost empirically impossible. In the following sections, we propose a method for estimating the optimum set of values for operation costs in TED algorithm dealing with textual entailment problem. Our method is built on adapting PSO optimization approach as a search process to automate the procedure of the cost estimation. 3 Particle Swarm Optimization PSO is a stochastic optimization technique which was introduced based on the social behaviour of bird flocking and fish schooling (Eberhart et al., 2001). It is one of the population-based search methods which takes advantage of the concept of social sharing of information. The main structure of this algorithm is not very different from other evolutionary techniques such as Genetic Algorithms (GA); however, the easy implementation and less complexity of PSO, as two main characteristics, are good motivations to apply this optimization approach in many areas. In this algorithm each particle can learn from the experience of other particles in the same population (called swarm). In other words, each particle in the iterative search process, would </context>
</contexts>
<marker>Eberhart, Shi, Kennedy, 2001</marker>
<rawString>Russell C. Eberhart, Yuhui Shi, and James Kennedy. 2001. Swarm Intelligence. The Morgan Kaufmann Series in Artificial Intelligence. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keinosuke Fukunaga</author>
</authors>
<title>Introduction to statistical pattern recognition (2nd ed.).</title>
<date>1990</date>
<publisher>Academic Press Professional, Inc.,</publisher>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="13178" citStr="Fukunaga, 1990" startWordPosition="2254" endWordPosition="2255">didate particle for driving the solution space to optimization, on the basis of the fitness function. Moreover, this function should possibly improve the textual entailment recognition model. In order to attain these goals, we tried to define two main fitness functions as follows. 1. Bhattacharyya Distance: This measure was proposed by (Bhattacharyya, 1943) as a statistical measure to determine the similarity or distance between two discrete probability distributions. In binary classification, this method is widely used to measure the distance between two different classes. In the studies by (Fukunaga, 1990), Bhattacharyya distance was occluded to be one of the most effective measure specifically for estimating the separability of two classes. Figure 2 shows the intuition behind this measure. Figure 2: Bhattacharyya distance between two classes with similar variances. Bhattacharyya distance is calculated based on the covariance (Q) and mean (µ) of each distribution based on its simplest formulation in Equation 6 (Reyes-Aldasoro and Bhalerao, 2006). Maximizing the distance between the classes would result a better separability which aims to a better classification results. Furthermore, estimating </context>
</contexts>
<marker>Fukunaga, 1990</marker>
<rawString>Keinosuke Fukunaga. 1990. Introduction to statistical pattern recognition (2nd ed.). Academic Press Professional, Inc., San Diego, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15,</booktitle>
<pages>3--10</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="19259" citStr="Klein and Manning, 2003" startWordPosition="3262" endWordPosition="3265"> deletion and substitution) that are necessary to transform T into H. By defining the edit distance algorithm and a cost scheme (assigning a cost to the edit operations), this package is able to learn a TED threshold, over a set of string pairs, to decide if the entailment exists in a pair. In addition, we partially exploit the JSwarmPSO4 (Cingolani, 2005) package, with some adaptations, as an implementation of PSO algorithm. Each pair in the datasets is converted to two syntactic dependency parse trees using the Stanford statistical parser5, developed in the Stanford university NLP group by (Klein and Manning, 2003). Figure 3: Five main steps of the experimental framework. In order to take advantage of PSO optimization approach, we integrated EDITS and JSwarm-PSO to provide a flexible framework for the experiments (Figure 5.3). In this way, we applied the defined fitness functions in the integrated system. The Bhattacharyya distance between two classes (YES and NO), in each experiment, could be computed based on the TED score of each pair in the dataset. Moreover, the accuracy, by default, is computed by EDITS over the training set based on 10-fold cross-validation. 5.3 Experimental Scheme We conducted s</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15, pages 3–10, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip N Klein</author>
</authors>
<title>Computing the edit-distance between unrooted ordered trees.</title>
<date>1998</date>
<booktitle>In ESA ’98: Proceedings of the 6th Annual European Symposium on Algorithms,</booktitle>
<pages>91--102</pages>
<publisher>SpringerVerlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="5368" citStr="Klein, 1998" startWordPosition="869" endWordPosition="870">ned in sections 4 and 5. Then we present our experimental setup as well as the results, in detail. Finally, in the conclusion, the main advantages of our approach are reviewed and further developments are proposed accordingly. 2 Tree Edit Distance and Textual Entailment One of the approaches to textual entailment is based on the Tree Edit Distance (TED) between T and H. The tree edit distance measure is a similarity metric for rooted ordered trees. This metric was initiated by (Tai, 1979) as a generalization of the string edit distance problem and was improved by (Zhang and Shasha, 1989) and (Klein, 1998). The distance is computed as the cost of editing operations (i.e. insertion, deletion and substitution), which are required to transform the text T into the hypothesis H, while each edit operation on two text fragments A and B (denoted as A —* B) has an associated cost (denoted as γ (A —* B)). In textual entailment context, the edit operations are defined in the following way based on the dependency parse tree of T and H: • Insertion (λ —* A): insert a node A from the dependency tree of H into the dependency tree of T. When a node is inserted it is attached to the dependency relation of the s</context>
</contexts>
<marker>Klein, 1998</marker>
<rawString>Philip N. Klein. 1998. Computing the edit-distance between unrooted ordered trees. In ESA ’98: Proceedings of the 6th Annual European Symposium on Algorithms, pages 91–102, London, UK. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milen Kouylekov</author>
<author>Bernardo Magnini</author>
</authors>
<title>Recognizing textual entailment with tree edit distance algorithms.</title>
<date>2005</date>
<booktitle>In PASCAL Challenges on RTE,</booktitle>
<pages>17--20</pages>
<contexts>
<context position="2253" citStr="Kouylekov and Magnini, 2005" startWordPosition="354" endWordPosition="357"> (H) such that entailment function for the pair T-H returns the true value when the meaning of H can be inferred from the meaning of T and false, otherwise. Amongst the approaches to the problem of textual entailment, some methods utilize the noBernardo Magnini FBK - Irst Trento, Italy magnini@fbk.eu tion of distance between the pair of T and H as the main feature which separates the entailment classes (positive and negative). One of the successful algorithms implemented Tree Edit Distance (TED), based on the syntactic features that are represented in the structured parse tree of each string (Kouylekov and Magnini, 2005). In this method the distance is computed as the cost of the edit operations (insertion, deletion and substitution) that transform the text T into the hypothesis H. Each edit operation has an associated cost and the entailment score is calculated such that the set of operations would lead to the minimum cost. Generally, the initial cost is assigned to each edit operation empirically, or based on the expert knowledge and experience. These methods emerge a critical problem when the domain, field or application is new and the level of expertise and empirical knowledge is very limited. In dealing </context>
<context position="8672" citStr="Kouylekov and Magnini, 2005" startWordPosition="1497" endWordPosition="1500">pted from (Demaine et al., 2007) min 37 Figure 1: Two possible edit scripts to transform one tree to another. main definition of this approach, TED is the cost of minimum cost edit script between two trees. The entailment score for a pair is calculated on the minimal set of edit operations that transform the dependency parse tree of T into H. An entailment relation is assigned to a T-H pair where the overall cost of the transformations is below a certain threshold. The threshold, which corresponds to tree edit distace, is empirically estimated over the dataset. This method was implemented by (Kouylekov and Magnini, 2005), based on the algorithm by (Zhang and Shasha, 1989). In this method, a cost value is assigned to each operation initially, and the distance is computed based on the initial cost values. Considering that the distance can vary in different datasets, converging to an optimal set of values for operations is almost empirically impossible. In the following sections, we propose a method for estimating the optimum set of values for operation costs in TED algorithm dealing with textual entailment problem. Our method is built on adapting PSO optimization approach as a search process to automate the pro</context>
</contexts>
<marker>Kouylekov, Magnini, 2005</marker>
<rawString>Milen Kouylekov and Bernardo Magnini. 2005. Recognizing textual entailment with tree edit distance algorithms. In PASCAL Challenges on RTE, pages 17–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milen Kouylekov</author>
<author>Bernardo Magnini</author>
</authors>
<title>Tree edit distance for recognizing textual entailment: Estimating the cost of insertion.</title>
<date>2006</date>
<booktitle>In PASCAL RTE-2 Challenge.</booktitle>
<contexts>
<context position="2907" citStr="Kouylekov and Magnini, 2006" startWordPosition="461" endWordPosition="464">nce is computed as the cost of the edit operations (insertion, deletion and substitution) that transform the text T into the hypothesis H. Each edit operation has an associated cost and the entailment score is calculated such that the set of operations would lead to the minimum cost. Generally, the initial cost is assigned to each edit operation empirically, or based on the expert knowledge and experience. These methods emerge a critical problem when the domain, field or application is new and the level of expertise and empirical knowledge is very limited. In dealing with textual entailment, (Kouylekov and Magnini, 2006) tried to experiment different cost values based on various linguistics knowledge and probabilistics estimations. For instance, they defined the substitution cost as a function of similarity between two nodes, or, for insertion cost, they employed Inverse Document Frequency (IDF) of the inserted node. However, the results could not proven to be optimal. Other approaches towards estimating the cost of operations in TED tried to learn a generic or discriminative probabilistic model (Bernard et al., 2008; Neuhaus and Bunke, 2004) from the data, without concerning the optimal value of each operati</context>
</contexts>
<marker>Kouylekov, Magnini, 2006</marker>
<rawString>Milen Kouylekov and Bernardo Magnini. 2006. Tree edit distance for recognizing textual entailment: Estimating the cost of insertion. In PASCAL RTE-2 Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Milen Kouylekov</author>
<author>Elena Cabrio</author>
</authors>
<title>Edits - edit distance textual entailment suite user manual. Available at http://edits.fbk.eu/.</title>
<date>2009</date>
<contexts>
<context position="18478" citStr="Magnini et al., 2009" startWordPosition="3129" endWordPosition="3132">development set. More details about the RTE datasets are illustrated in Table 5.1. Number of pairs Development Test Datasets YES NO YES NO RTE-1 283 284 400 400 RTE-2 400 400 400 400 RTE-3 412 388 410 390 RTE-4 — — 500 500 Table 1: RTE-1 to RTE-4 datasets. 5.2 Experimental Framework In our experiments, in order to deal with TED approach to textual entailment, we used EDITS3 package (Edit Distance Textual Entailment Suite) 2http://www.pascal-network.org/Challenges/RTE1-4 3The EDITS system has been supported by the EUfunded project QALL-ME (FP6 IST-033860). Available at http://edits.fbk.eu/ 40 (Magnini et al., 2009). This system is an open source software based on edit distance algorithms, and computes the T-H distance as the cost of the edit operations (i.e. insertion, deletion and substitution) that are necessary to transform T into H. By defining the edit distance algorithm and a cost scheme (assigning a cost to the edit operations), this package is able to learn a TED threshold, over a set of string pairs, to decide if the entailment exists in a pair. In addition, we partially exploit the JSwarmPSO4 (Cingolani, 2005) package, with some adaptations, as an implementation of PSO algorithm. Each pair in </context>
<context position="20794" citStr="Magnini et al., 2009" startWordPosition="3504" endWordPosition="3507">st scheme based on three operations. Implementing this cost scheme, we expect to optimize the cost of each edit operation without considering that the operation costs may vary based on different characteristics of a node, such as size, location or content. The results were obtained considering three different settings: 1) the random cost assignment; 2) assigning the cost based on the human expertise knowledge and intuition (called Intuitive), and 3) automatic estimated and optimized cost for each operation. In the second case, we used the same scheme which was used in EDITS by its developers (Magnini et al., 2009). In the second set of experiments, we tried to compose an advanced cost scheme with more fine-grained operations to assign a weight to the edit operations based on the characteristics of the nodes. For example if a node is in the list of stopwords, the deletion cost is set to zero. Otherwise, the cost of deletion would be equal to the number of words in H multiplied by word’s length (number of characters). Similarly, the cost of inserting a word w in H is set to 0 if w is a stop word, and to the number of words in T multiplied by words length otherwise. The cost of substituting two words is t</context>
<context position="23426" citStr="Magnini et al., 2009" startWordPosition="3961" endWordPosition="3964">on optimized and unoptimized cost schemes. the better comparison, we also present the results of the EDITS system in RTE-4 challenge using a combination of different distances as features for classification (Cabrio et al., 2008). In the first experiment, we estimated the cost of each operation using the simple cost scheme. Table 2 shows that in all datasets, accuracy improved up to 9% by optimizing the cost of each edit operation. Results prove that the optimized cost scheme enhances the quality of the system performance, even more than the cost scheme used by experts (Intuitive cost scheme) (Magnini et al., 2009). Furthermore, in the second set of experiments, using the fine-grained and weighted cost scheme for edit operations we could achieve the highest results in accuracy. The chart in Figure 4, illustares that all optimized results outperform the wordoverlap baseline for textual entailment as well as the accuracy obtained in RTE-4 challenge using combination of different distances as features for classification (Cabrio et al., 2008). By exploring the estimated optimal cost of each operation, another interesting point was discovered. The estimated cost of deletion in the first set of experiments wa</context>
</contexts>
<marker>Magnini, Kouylekov, Cabrio, 2009</marker>
<rawString>Bernardo Magnini, Milen Kouylekov, and Elena Cabrio. 2009. Edits - edit distance textual entailment suite user manual. Available at http://edits.fbk.eu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Bernardo Magnini</author>
</authors>
<title>A word overlap baseline for the recognizing textual entailment task. Available at http://edits.fbk.eu/.</title>
<date>2009</date>
<contexts>
<context position="22339" citStr="Mehdad and Magnini, 2009" startWordPosition="3779" endWordPosition="3782">d). We conducted the experiments using all three cases mentioned in the simple cost scheme. In each experiment, we applied both fitness functions in the optimization; however, at the final phase, the costs which led to the maximum results were chosen as the estimated operation costs. In order to save breath and time, we set the number of iterations to 10, in addition, the weight ω was set to 0.95 for better global exploration (Melgani and Bazi, 2008). 6 Results Our results are summarized in Table 2. We show the accuracy gained by a distance-based (wordoverlap) baseline for textual entailment (Mehdad and Magnini, 2009) to be compared with the results achieved by the random, intuitive and optimized cost schemes using EDITS system. For 41 Data set Model RTE-4 RTE-3 RTE-2 RTE-1 Random 49.6 53.62 50.37 50.5 Simple Intuitive 51.3 59.6 56.5 49.8 Optimized 56.5 61.62 58 58.12 Random 53.60 52.0 54.62 53.5 Advanced Intuitive 57.6 59.37 57.75 55.5 Optimized 59.5 62.4 59.87 58.62 Baseline 55.2 60.9 54.8 51.4 RTE-4 Challenge 57.0 Table 2: Comparison of accuracy on all RTE datasets based on optimized and unoptimized cost schemes. the better comparison, we also present the results of the EDITS system in RTE-4 challenge u</context>
</contexts>
<marker>Mehdad, Magnini, 2009</marker>
<rawString>Yashar Mehdad and Bernardo Magnini. 2009. A word overlap baseline for the recognizing textual entailment task. Available at http://edits.fbk.eu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Farid Melgani</author>
<author>Yakoub Bazi</author>
</authors>
<title>Classification of electrocardiogram signals with support vector machines and particle swarm optimization.</title>
<date>2008</date>
<journal>IEEE Transactions on Information Technology in Biomedicine,</journal>
<volume>12</volume>
<issue>5</issue>
<contexts>
<context position="11061" citStr="Melgani and Bazi, 2008" startWordPosition="1898" endWordPosition="1901"> Xi (Equation 4) and velocity Vi (Equation 5) is updated. Xbi is the best position of the particle during its past routes and Xgi is the best global position over all routes travelled by the particles of the swarm. r1 and r2 are random variables drawn from a uniform distribution in the range [0,1], while c1 and c2 are two acceleration constants regulating the relative velocities with respect to the best local and global positions. The weight w is used as a tradeoff between the global and local best positions and its value is usually selected slightly less than 1 for better global exploration (Melgani and Bazi, 2008). The optimal position is computed based on the fitness function defined in association with the related problem. Both position and velocity are updated during the iterations until convergence is reached or iterations attain the maximum number defined by the user. This search process returns the best fitness function over the particles, which is defined as the optimized solution. Xi = Xi + Vi (4) Vi = wVi + c1r1(Xbi − Xi) + c2r2(Xgi − Xi) (5) Algorithm 1 shows a simple pseudo code of how this optimization algorithm works. In the rest of the paper, we describe our method to integrate this algor</context>
<context position="22168" citStr="Melgani and Bazi, 2008" startWordPosition="3752" endWordPosition="3755">us number of words in H. By this intuition, we tried to optimize nine specialized costs for edit operations (i.e. each particle is defined by 9 parameters to be optimized). We conducted the experiments using all three cases mentioned in the simple cost scheme. In each experiment, we applied both fitness functions in the optimization; however, at the final phase, the costs which led to the maximum results were chosen as the estimated operation costs. In order to save breath and time, we set the number of iterations to 10, in addition, the weight ω was set to 0.95 for better global exploration (Melgani and Bazi, 2008). 6 Results Our results are summarized in Table 2. We show the accuracy gained by a distance-based (wordoverlap) baseline for textual entailment (Mehdad and Magnini, 2009) to be compared with the results achieved by the random, intuitive and optimized cost schemes using EDITS system. For 41 Data set Model RTE-4 RTE-3 RTE-2 RTE-1 Random 49.6 53.62 50.37 50.5 Simple Intuitive 51.3 59.6 56.5 49.8 Optimized 56.5 61.62 58 58.12 Random 53.60 52.0 54.62 53.5 Advanced Intuitive 57.6 59.37 57.75 55.5 Optimized 59.5 62.4 59.87 58.62 Baseline 55.2 60.9 54.8 51.4 RTE-4 Challenge 57.0 Table 2: Comparison o</context>
</contexts>
<marker>Melgani, Bazi, 2008</marker>
<rawString>Farid Melgani and Yakoub Bazi. 2008. Classification of electrocardiogram signals with support vector machines and particle swarm optimization. IEEE Transactions on Information Technology in Biomedicine, 12(5):667–677.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Neuhaus</author>
<author>Horst Bunke</author>
</authors>
<title>A probabilistic approach to learning costs for graph edit distance.</title>
<date>2004</date>
<booktitle>In ICPR ’04,</booktitle>
<pages>389--393</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="3439" citStr="Neuhaus and Bunke, 2004" startWordPosition="541" endWordPosition="544">knowledge is very limited. In dealing with textual entailment, (Kouylekov and Magnini, 2006) tried to experiment different cost values based on various linguistics knowledge and probabilistics estimations. For instance, they defined the substitution cost as a function of similarity between two nodes, or, for insertion cost, they employed Inverse Document Frequency (IDF) of the inserted node. However, the results could not proven to be optimal. Other approaches towards estimating the cost of operations in TED tried to learn a generic or discriminative probabilistic model (Bernard et al., 2008; Neuhaus and Bunke, 2004) from the data, without concerning the optimal value of each operation. One of the drawbacks of those approaches is that the cost values of edit operations are hidden behind the probabilistic model. Additionally, the cost can not be weighted or varied according to the tree context and node location (Bernard et al., 2008). In order to overcome these drawbacks, we are proposing a stochastic method based on Particle 36 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 36–43, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP Swarm Optimization (PSO), to est</context>
</contexts>
<marker>Neuhaus, Bunke, 2004</marker>
<rawString>Michel Neuhaus and Horst Bunke. 2004. A probabilistic approach to learning costs for graph edit distance. In ICPR ’04, pages 389–393, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Reyes-Aldasoro</author>
<author>A Bhalerao</author>
</authors>
<title>The bhattacharyya space for feature selection and its application to texture segmentation.</title>
<date>2006</date>
<journal>Pattern Recogn.,</journal>
<volume>39</volume>
<issue>5</issue>
<contexts>
<context position="13626" citStr="Reyes-Aldasoro and Bhalerao, 2006" startWordPosition="2318" endWordPosition="2321">een two discrete probability distributions. In binary classification, this method is widely used to measure the distance between two different classes. In the studies by (Fukunaga, 1990), Bhattacharyya distance was occluded to be one of the most effective measure specifically for estimating the separability of two classes. Figure 2 shows the intuition behind this measure. Figure 2: Bhattacharyya distance between two classes with similar variances. Bhattacharyya distance is calculated based on the covariance (Q) and mean (µ) of each distribution based on its simplest formulation in Equation 6 (Reyes-Aldasoro and Bhalerao, 2006). Maximizing the distance between the classes would result a better separability which aims to a better classification results. Furthermore, estimating the costs using this function would indirectly improve the performance specially in classification problems. It could be stated that, maximizing the Bhattacharyya distance would increase the separability of two entailment classes which result in a better performance. 2 2 BD(c1, c2) = 1ln{ 1(Qc1 + Qc2 + 2)} 2 4 4 Q2 U2�c1 2 22 4 Qc1 + Qc2 2. Accuracy: Accuracy or any performance measure obtained from a TED based system, can define a good fitness</context>
</contexts>
<marker>Reyes-Aldasoro, Bhalerao, 2006</marker>
<rawString>C. C. Reyes-Aldasoro and A. Bhalerao. 2006. The bhattacharyya space for feature selection and its application to texture segmentation. Pattern Recogn., 39(5):812–826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley M Selkow</author>
</authors>
<title>The tree-to-tree editing problem.</title>
<date>1977</date>
<journal>Inf. Process. Lett.,</journal>
<volume>6</volume>
<issue>6</issue>
<contexts>
<context position="6946" citStr="Selkow, 1977" startWordPosition="1170" endWordPosition="1171">bel of a node B of the target tree. In the case of substitution, the relation attached to the substituted node is changed with the relation of the new node. According to (Zhang and Shasha, 1989), the minimum cost mappings of all the descendants of each node has to be computed before the node is encountered, so the least-cost mapping can be selected right away. To accomplish this the algorithm keeps track of the keyroots of the tree, which are defined as a set that contains the root of the tree plus all nodes which have a left sibling. This problem can be easily solved using recursive methods (Selkow, 1977), or as it was suggested in (Zhang and Shasha, 1989) by dynamic programming. (Zhang and Shasha, 1989) defined the relevant subproblems of tree T as the prefixes of all special subforests rooted in the keyroots. This approach computes the TED (δ) by the following equations: δ(FT, θ) _ δ(FT − rFT, θ) + γ(rFT —* λ) (1) δ(θ, FH) _ δ(θ, FH − rFH) + γ(λ —* rFH) (2) δ(FT, FH) _ { δ(FT − rFT, FH) + γ(rFT —* λ) δ(FT, FH − rFH) + γ(λ —* rFH) δ(FT (rFT ), FH(rFH ))+ δ(FT − T (rFT ), FH − H(rFH))+ γ(rFT —* rFH ) (3) where FT and FH are forests of T and H, while rFT and rFH are the rightmost roots of the t</context>
</contexts>
<marker>Selkow, 1977</marker>
<rawString>Stanley M. Selkow. 1977. The tree-to-tree editing problem. Inf. Process. Lett., 6(6):184–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuo-Chung Tai</author>
</authors>
<title>The tree-to-tree correction problem.</title>
<date>1979</date>
<journal>J. ACM,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="5249" citStr="Tai, 1979" startWordPosition="849" endWordPosition="850">al entailment in the next section, PSO optimization algorithm and our method in applying it to the problem are explained in sections 4 and 5. Then we present our experimental setup as well as the results, in detail. Finally, in the conclusion, the main advantages of our approach are reviewed and further developments are proposed accordingly. 2 Tree Edit Distance and Textual Entailment One of the approaches to textual entailment is based on the Tree Edit Distance (TED) between T and H. The tree edit distance measure is a similarity metric for rooted ordered trees. This metric was initiated by (Tai, 1979) as a generalization of the string edit distance problem and was improved by (Zhang and Shasha, 1989) and (Klein, 1998). The distance is computed as the cost of editing operations (i.e. insertion, deletion and substitution), which are required to transform the text T into the hypothesis H, while each edit operation on two text fragments A and B (denoted as A —* B) has an associated cost (denoted as γ (A —* B)). In textual entailment context, the edit operations are defined in the following way based on the dependency parse tree of T and H: • Insertion (λ —* A): insert a node A from the depende</context>
</contexts>
<marker>Tai, 1979</marker>
<rawString>Kuo-Chung Tai. 1979. The tree-to-tree correction problem. J. ACM, 26(3):422–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zhang</author>
<author>D Shasha</author>
</authors>
<title>Simple fast algorithms for the editing distance between trees and related problems.</title>
<date>1989</date>
<journal>SIAMJ. Comput.,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="5350" citStr="Zhang and Shasha, 1989" startWordPosition="864" endWordPosition="867"> it to the problem are explained in sections 4 and 5. Then we present our experimental setup as well as the results, in detail. Finally, in the conclusion, the main advantages of our approach are reviewed and further developments are proposed accordingly. 2 Tree Edit Distance and Textual Entailment One of the approaches to textual entailment is based on the Tree Edit Distance (TED) between T and H. The tree edit distance measure is a similarity metric for rooted ordered trees. This metric was initiated by (Tai, 1979) as a generalization of the string edit distance problem and was improved by (Zhang and Shasha, 1989) and (Klein, 1998). The distance is computed as the cost of editing operations (i.e. insertion, deletion and substitution), which are required to transform the text T into the hypothesis H, while each edit operation on two text fragments A and B (denoted as A —* B) has an associated cost (denoted as γ (A —* B)). In textual entailment context, the edit operations are defined in the following way based on the dependency parse tree of T and H: • Insertion (λ —* A): insert a node A from the dependency tree of H into the dependency tree of T. When a node is inserted it is attached to the dependency</context>
<context position="6998" citStr="Zhang and Shasha, 1989" startWordPosition="1178" endWordPosition="1181"> case of substitution, the relation attached to the substituted node is changed with the relation of the new node. According to (Zhang and Shasha, 1989), the minimum cost mappings of all the descendants of each node has to be computed before the node is encountered, so the least-cost mapping can be selected right away. To accomplish this the algorithm keeps track of the keyroots of the tree, which are defined as a set that contains the root of the tree plus all nodes which have a left sibling. This problem can be easily solved using recursive methods (Selkow, 1977), or as it was suggested in (Zhang and Shasha, 1989) by dynamic programming. (Zhang and Shasha, 1989) defined the relevant subproblems of tree T as the prefixes of all special subforests rooted in the keyroots. This approach computes the TED (δ) by the following equations: δ(FT, θ) _ δ(FT − rFT, θ) + γ(rFT —* λ) (1) δ(θ, FH) _ δ(θ, FH − rFH) + γ(λ —* rFH) (2) δ(FT, FH) _ { δ(FT − rFT, FH) + γ(rFT —* λ) δ(FT, FH − rFH) + γ(λ —* rFH) δ(FT (rFT ), FH(rFH ))+ δ(FT − T (rFT ), FH − H(rFH))+ γ(rFT —* rFH ) (3) where FT and FH are forests of T and H, while rFT and rFH are the rightmost roots of the trees in FT and FH respectively. θ is an empty forest</context>
<context position="8724" citStr="Zhang and Shasha, 1989" startWordPosition="1507" endWordPosition="1510">sible edit scripts to transform one tree to another. main definition of this approach, TED is the cost of minimum cost edit script between two trees. The entailment score for a pair is calculated on the minimal set of edit operations that transform the dependency parse tree of T into H. An entailment relation is assigned to a T-H pair where the overall cost of the transformations is below a certain threshold. The threshold, which corresponds to tree edit distace, is empirically estimated over the dataset. This method was implemented by (Kouylekov and Magnini, 2005), based on the algorithm by (Zhang and Shasha, 1989). In this method, a cost value is assigned to each operation initially, and the distance is computed based on the initial cost values. Considering that the distance can vary in different datasets, converging to an optimal set of values for operations is almost empirically impossible. In the following sections, we propose a method for estimating the optimum set of values for operation costs in TED algorithm dealing with textual entailment problem. Our method is built on adapting PSO optimization approach as a search process to automate the procedure of the cost estimation. 3 Particle Swarm Opti</context>
</contexts>
<marker>Zhang, Shasha, 1989</marker>
<rawString>K. Zhang and D. Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. SIAMJ. Comput., 18(6):1245–1262.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>