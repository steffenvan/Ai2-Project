<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001239">
<title confidence="0.998575">
Generalized Inference with Multiple Semantic Role Labeling Systems
</title>
<author confidence="0.994246">
Peter Koomen Vasin Punyakanok Dan Roth Wen-tau Yih
</author>
<affiliation confidence="0.906789666666667">
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
</affiliation>
<email confidence="0.998569">
{pkoomen2,punyakan,danr,yih}@uiuc.edu
</email>
<sectionHeader confidence="0.993891" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947866666667">
We present an approach to semantic role
labeling (SRL) that takes the output of
multiple argument classifiers and com-
bines them into a coherent predicate-
argument output by solving an optimiza-
tion problem. The optimization stage,
which is solved via integer linear pro-
gramming, takes into account both the rec-
ommendation of the classifiers and a set
of problem specific constraints, and is thus
used both to clean the classification results
and to ensure structural integrity of the fi-
nal role labeling. We illustrate a signifi-
cant improvement in overall SRL perfor-
mance through this inference.
</bodyText>
<sectionHeader confidence="0.970327" genericHeader="method">
1 SRL System Architecture
</sectionHeader>
<bodyText confidence="0.996571">
Our SRL system consists of four stages: prun-
ing, argument identification, argument classifica-
tion, and inference. In particular, the goal of pruning
and argument identification is to identify argument
candidates for a given verb predicate. The system
only classifies the argument candidates into their
types during the argument classification stage. Lin-
guistic and structural constraints are incorporated
in the inference stage to resolve inconsistent global
predictions. The inference stage can take as its input
the output of the argument classification of a single
system or of multiple systems. We explain the infer-
ence for multiple systems in Sec. 2.
</bodyText>
<subsectionHeader confidence="0.984134">
1.1 Pruning
</subsectionHeader>
<bodyText confidence="0.999877631578947">
Only the constituents in the parse tree are considered
as argument candidates. In addition, our system ex-
ploits the heuristic introduced by (Xue and Palmer,
2004) to filter out very unlikely constituents. The
heuristic is a recursive process starting from the verb
whose arguments are to be identified. It first returns
the siblings of the verb; then it moves to the parent of
the verb, and collects the siblings again. The process
goes on until it reaches the root. In addition, if a con-
stituent is a PP (propositional phrase), its children
are also collected. Candidates consisting of only a
single punctuation mark are not considered.
This heuristic works well with the correct parse
trees. However, one of the errors by automatic
parsers is due to incorrect PP attachment leading to
missing arguments. To attempt to fix this, we con-
sider as arguments the combination of any consec-
utive NP and PP, and the split of NP and PP inside
the NP that was chosen by the previous heuristics.
</bodyText>
<subsectionHeader confidence="0.99079">
1.2 Argument Identification
</subsectionHeader>
<bodyText confidence="0.9993565">
The argument identification stage utilizes binary
classification to identify whether a candidate is an
argument or not. We train and apply the binary clas-
sifiers on the constituents supplied by the pruning
stage. Most of the features used in our system are
standard features, which include
</bodyText>
<listItem confidence="0.994394090909091">
• Predicate and POS tag of predicate indicate the lemma
of the predicate and its POS tag.
• Voice indicates tbe voice of the predicate.
• Phrase type of the constituent.
• Head word and POS tag of the head word include head
word and its POS tag of the constituent. We use rules
introduced by (Collins, 1999) to extract this feature.
• First and last words and POS tags of the constituent.
• Two POS tags before and after the constituent.
• Position feature describes if the constituent is before or
after the predicate relative to the position in the sentence.
</listItem>
<page confidence="0.850979">
181
</page>
<note confidence="0.297673">
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 181–184, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<listItem confidence="0.979637652173913">
• Path records the traversal path in the parse tree from the
predicate to the constituent.
• Subcategorization feature describes the phrase structure
around the predicate’s parent. It records the immediate
structure in the parse tree that expands to its parent.
• Verb class feature is the class of the active predicate de-
scribed in PropBank Frames.
• Lengths of the target constituent, in the numbers of words
and chunks separately.
• Chunk tells if the target argument is, embeds, overlaps,
or is embedded in a chunk with its type.
• Chunk pattern length feature counts the number of
chunks from the predicate to the argument.
• Clause relative position is the position of the target word
relative to the predicate in the pseudo-parse tree con-
structed only from clause constituent. There are four
configurations—target constituent and predicate share the
same parent, target constituent parent is an ancestor of
predicate, predicate parent is an ancestor of target word,
or otherwise.
• Clause coverage describes how much of the local clause
(from the predicate) is covered by the argument. It is
round to the multiples of 1/4.
</listItem>
<subsectionHeader confidence="0.987412">
1.3 Argument Classification
</subsectionHeader>
<bodyText confidence="0.998899727272727">
This stage assigns the final argument labels to the ar-
gument candidates supplied from the previous stage.
A multi-class classifier is trained to classify the
types of the arguments supplied by the argument
identification stage. To reduce the excessive candi-
dates mistakenly output by the previous stage, the
classifier can also classify the argument as NULL
(“not an argument”) to discard the argument.
The features used here are the same as those used
in the argument identification stage with the follow-
ing additional features.
</bodyText>
<listItem confidence="0.996519166666667">
• Syntactic frame describes the sequential pattern of the
noun phrases and the predicate in the sentence. This is
the feature introduced by (Xue and Palmer, 2004).
• Propositional phrase head is the head of the first phrase
after the preposition inside PP.
• NEG and MOD feature indicate if the argument is a
baseline for AM-NEG or AM-MOD. The rules of the
NEG and MOD features are used in a baseline SRL sys-
tem developed by Erik Tjong Kim Sang (Carreras and
M`arquez, 2004).
• NE indicates if the target argument is, embeds, overlaps,
or is embedded in a named-entity along with its type.
</listItem>
<subsectionHeader confidence="0.764223">
1.4 Inference
</subsectionHeader>
<bodyText confidence="0.999989653846154">
The purpose of this stage is to incorporate some
prior linguistic and structural knowledge, such as
“arguments do not overlap” or “each verb takes at
most one argument of each type.” This knowledge is
used to resolve any inconsistencies of argument clas-
sification in order to generate final legitimate pre-
dictions. We use the inference process introduced
by (Punyakanok et al., 2004). The process is formu-
lated as an integer linear programming (ILP) prob-
lem that takes as inputs the confidences over each
type of the arguments supplied by the argument clas-
sifier. The output is the optimal solution that maxi-
mizes the linear sum of the confidence scores (e.g.,
the conditional probabilities estimated by the argu-
ment classifier), subject to the constraints that en-
code the domain knowledge.
Formally speaking, the argument classifier at-
tempts to assign labels to a set of arguments, S1:M,
indexed from 1 to M. Each argument Si can take
any label from a set of argument labels, P, and the
indexed set of arguments can take a set of labels,
c1:M E PM. If we assume that the argument classi-
fier returns an estimated conditional probability dis-
tribution, Prob(Si = ci), then, given a sentence, the
inference procedure seeks an global assignment that
maximizes the following objective function,
</bodyText>
<equation confidence="0.991783">
M
�c1:M =argmax Prob(Si = ci),
c1:MEPM i=1
</equation>
<bodyText confidence="0.997179">
subject to linguistic and structural constraints. In
other words, this objective function reflects the ex-
pected number of correct argument predictions, sub-
ject to the constraints. The constraints are encoded
as the followings.
</bodyText>
<listItem confidence="0.97055625">
• No overlapping or embedding arguments.
• No duplicate argument classes for A0-A5.
• Exactly one V argument per predicate considered.
• If there is C-V, then there has to be a V-A1-CV pattern.
• If there is an R-arg argument, then there has to be an arg
argument.
• If there is a C-arg argument, there must be an arg argu-
ment; moreover, the C-arg argument must occur after arg.
• Given the predicate, some argument types are illegal (e.g.
predicate ‘stalk’ can take only A0 or A1). The illegal
types may consist of A0-A5 and their corresponding C-
arg and R-arg arguments. For each predicate, we look
for the minimum value of i such that the class Ai is men-
tioned in its frame file as well as its maximum value j.
All argument types Ak such that k &lt; i or k &gt; j are
considered illegal.
</listItem>
<page confidence="0.995797">
182
</page>
<sectionHeader confidence="0.891124" genericHeader="method">
2 Inference with Multiple SRL Systems
</sectionHeader>
<bodyText confidence="0.955719142857143">
The inference process allows a natural way to com-
bine the outputs from multiple argument classi-
fiers. Specifically, given k argument classifiers
which perform classification on k argument sets,
{S1, ... , Sk}. The inference process aims to opti-
mize the objective function:
..., traders say, unable to cool the selling panic in both stocks and futures.
</bodyText>
<figure confidence="0.911288166666667">
a1 a4
b1 b b
2 3
b4
a2
a3
</figure>
<equation confidence="0.730584">
C1:N = argmax N Prob(Si = ci), Figure 1: Two SRL systems’ output (a1, a4, b1, b2,
c1:NE,pN i=1 and b3), and phantom candidates (a2, a3, and b4).
where S1:N = Uk and
=1 Si,
1 k
Prob(Si = ci) = k
j=1
Probj(Si = ci),
</equation>
<bodyText confidence="0.999838137931034">
where Probj is the probability output by system j.
Note that all systems may not output with the
same set of argument candidates due to the pruning
and argument identification. For the systems that do
not output for any candidate, we assign the proba-
bility with a prior to this phantom candidate. In par-
ticular, the probability of the NULL class is set to be
0.6 based on empirical tests, and the probabilities of
the other classes are set proportionally to their oc-
currence frequencies in the training data.
For example, Figure 1 shows the two candidate
sets for a fragment of a sentence, “..., traders say,
unable to cool the selling panic in both stocks and
futures.” In this example, system A has two argu-
ment candidates, a1 = “traders” and a4 = “the sell-
ing panic in both stocks and futures”; system B has
three argument candidates, b1 = “traders”, b2 = “the
selling panic”, and b3 = “in both stocks and fu-
tures”. The phantom candidates are created for a2,
a3, and b4 of which probability is set to the prior.
Specifically for this implementation, we first train
two SRL systems that use Collins’ parser and Char-
niak’s parser respectively. In fact, these two parsers
have noticeably different output. In evaluation, we
run the system that was trained with Charniak’s
parser 5 times with the top-5 parse trees output by
Charniak’s parser1. Together we have six different
outputs per predicate. Per each parse tree output, we
ran the first three stages, namely pruning, argument
</bodyText>
<footnote confidence="0.9445965">
1The top parse tree were from the official output by CoNLL.
The 2nd-5th parse trees were output by Charniak’s parser.
</footnote>
<bodyText confidence="0.99958">
identification, and argument classification. Then a
joint inference stage is used to resolve the incon-
sistency of the output of argument classification in
these systems.
</bodyText>
<sectionHeader confidence="0.949363" genericHeader="method">
3 Learning and Evaluation
</sectionHeader>
<bodyText confidence="0.973065684210526">
The learning algorithm used is a variation of the
Winnow update rule incorporated in SNoW (Roth,
1998; Roth and Yih, 2002), a multi-class classi-
fier that is tailored for large scale learning tasks.
SNoW learns a sparse network of linear functions,
in which the targets (argument border predictions
or argument type predictions, in this case) are rep-
resented as linear functions over a common feature
space. It improves the basic Winnow multiplicative
update rule with a regularization term, which has the
effect of trying to separate the data with a large mar-
gin separator (Grove and Roth, 2001; Hang et al.,
2002) and voted (averaged) weight vector (Freund
and Schapire, 1999).
Softmax function (Bishop, 1995) is used to con-
vert raw activation to conditional probabilities. If
there are n classes and the raw activation of class i
is acti, the posterior estimation for class i is
each
</bodyText>
<equation confidence="0.894891">
Prob(i) = E1&lt; each
j ct
.
</equation>
<bodyText confidence="0.998533125">
In summary, training used both full and partial
syntactic information as described in Section 1. In
training, SNoW’s default parameters were used with
the exception of the separator thickness 1.5, the use
of average weight vector, and 5 training cycles. The
parameters are optimized on the development set.
Training for each system took about 6 hours. The
evaluation on both test sets which included running
</bodyText>
<page confidence="0.996472">
183
</page>
<table confidence="0.999996657894737">
Precision Recall Fp=1
Development 80.05% 74.83% 77.35
Test WSJ 82.28% 76.78% 79.44
Test Brown 73.38% 62.93% 67.75
Test WSJ+Brown 81.18% 74.92% 77.92
Test WSJ Precision Recall Fp=1
Overall 82.28% 76.78% 79.44
A0 88.22% 87.88% 88.05
A1 82.25% 77.69% 79.91
A2 78.27% 60.36% 68.16
A3 82.73% 52.60% 64.31
A4 83.91% 71.57% 77.25
A5 0.00% 0.00% 0.00
AM-ADV 63.82% 56.13% 59.73
AM-CAU 64.15% 46.58% 53.97
AM-DIR 57.89% 38.82% 46.48
AM-DIS 75.44% 80.62% 77.95
AM-EXT 68.18% 46.88% 55.56
AM-LOC 66.67% 55.10% 60.33
AM-MNR 66.79% 53.20% 59.22
AM-MOD 96.11% 98.73% 97.40
AM-NEG 97.40% 97.83% 97.61
AM-PNC 60.00% 36.52% 45.41
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 78.16% 76.72% 77.44
R-A0 89.72% 85.71% 87.67
R-A1 70.00% 76.28% 73.01
R-A2 85.71% 37.50% 52.17
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 85.71% 57.14% 68.57
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 72.34% 65.38% 68.69
V 98.92% 97.10% 98.00
</table>
<tableCaption confidence="0.949165">
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
with all six different parse trees (assumed already
given) and the joint inference took about 4.5 hours.
</tableCaption>
<table confidence="0.9999755">
Precision Recall Fp=1
Charniak-1 75.40% 74.13% 74.76
Charniak-2 74.21% 73.06% 73.63
Charniak-3 73.52% 72.31% 72.91
Charniak-4 74.29% 72.92% 73.60
Charniak-5 72.57% 71.40% 71.98
Collins 73.89% 70.11% 71.95
Joint inference 80.05% 74.83% 77.35
</table>
<tableCaption confidence="0.9008385">
Table 2: The results of individual systems and the
result with joint inference on the development set.
</tableCaption>
<bodyText confidence="0.9728095">
Overall results on the development and test sets
are shown in Table 1. Table 2 shows the results of
individual systems and the improvement gained by
the joint inference on the development set.
</bodyText>
<sectionHeader confidence="0.999654" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999949">
We present an implementation of SRL system which
composed of four stages—1) pruning, 2) argument
identification, 3) argument classification, and 4) in-
ference. The inference provides a natural way to
take the output of multiple argument classifiers and
combines them into a coherent predicate-argument
output. Significant improvement in overall SRL per-
formance through this inference is illustrated.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999793">
We are grateful to Dash Optimization for the free
academic use of Xpress-MP. This research is sup-
ported by ARDA’s AQUAINT Program, DOI’s Re-
flex program, and an ONR MURI Award.
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99993875">
C. Bishop, 1995. Neural Networks for Pattern Recognition,
chapter 6.4: Modelling conditional distributions, page 215.
Oxford University Press.
X. Carreras and L. M`arquez. 2004. Introduction to the conll-
2004 shared tasks: Semantic role labeling. In Proc. of
CoNLL-2004.
M. Collins. 1999. Head-driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, Computer Science Depart-
ment, University of Pennsylvenia, Philadelphia.
Y. Freund and R. Schapire. 1999. Large margin classifica-
tion using the perceptron algorithm. Machine Learning,
37(3):277–296.
A. Grove and D. Roth. 2001. Linear concepts and hidden vari-
ables. Machine Learning, 42(1/2):123–141.
T. Hang, F. Damerau, and D. Johnson. 2002. Text chunking
based on a generalization of winnow. Journal of Machine
Learning Research, 2:615–637.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. Seman-
tic role labeling via integer linear programming inference. In
Proc. of COLING-2004.
D. Roth and W. Yih. 2002. Probabilistic reasoning for entity &amp;
relation recognition. In Proc. of COLING-2002, pages 835–
841.
D. Roth. 1998. Learning to resolve natural language ambigui-
ties: A unified approach. In Proc. ofAAAI, pages 806–813.
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proc. of the EMNLP-2004, pages 88–94,
Barcelona, Spain.
</reference>
<page confidence="0.998706">
184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.030475">
<title confidence="0.999984">Generalized Inference with Multiple Semantic Role Labeling Systems</title>
<author confidence="0.999184">Peter Koomen Vasin Punyakanok Dan Roth Wen-tau Yih</author>
<affiliation confidence="0.998354">Department of Computer University of Illinois at</affiliation>
<address confidence="0.965216">Urbana, IL 61801,</address>
<abstract confidence="0.996220133064516">We present an approach to semantic role labeling (SRL) that takes the output of multiple argument classifiers and combines them into a coherent predicateargument output by solving an optimization problem. The optimization stage, which is solved via integer linear programming, takes into account both the recommendation of the classifiers and a set of problem specific constraints, and is thus used both to clean the classification results and to ensure structural integrity of the final role labeling. We illustrate a significant improvement in overall SRL performance through this inference. 1 SRL System Architecture SRL system consists of four stages: prunclassificaand In particular, the goal of pruning and argument identification is to identify argument candidates for a given verb predicate. The system only classifies the argument candidates into their types during the argument classification stage. Linguistic and structural constraints are incorporated in the inference stage to resolve inconsistent global predictions. The inference stage can take as its input the output of the argument classification of a single system or of multiple systems. We explain the inference for multiple systems in Sec. 2. 1.1 Pruning Only the constituents in the parse tree are considered argument candidates. In addition, our system exploits the heuristic introduced by (Xue and Palmer, 2004) to filter out very unlikely constituents. The heuristic is a recursive process starting from the verb whose arguments are to be identified. It first returns the siblings of the verb; then it moves to the parent of the verb, and collects the siblings again. The process goes on until it reaches the root. In addition, if a conis a phrase), its children are also collected. Candidates consisting of only a single punctuation mark are not considered. This heuristic works well with the correct parse trees. However, one of the errors by automatic is due to incorrect leading to missing arguments. To attempt to fix this, we consider as arguments the combination of any consecand the split of was chosen by the previous heuristics. 1.2 Argument Identification The argument identification stage utilizes binary classification to identify whether a candidate is an argument or not. We train and apply the binary classifiers on the constituents supplied by the pruning stage. Most of the features used in our system are standard features, which include Predicate and POS tag of predicate the lemma of the predicate and its POS tag. Voice tbe voice of the predicate. Phrase type the constituent. Head word and POS tag of the head word head word and its POS tag of the constituent. We use rules introduced by (Collins, 1999) to extract this feature. First and last words and POS tags the constituent. Two POS tags before and after constituent. Position describes if the constituent is before or after the predicate relative to the position in the sentence. 181 of the 9th Conference on Computational Natural Language Learning 181–184, Ann Arbor, June 2005. Association for Computational Linguistics Path the traversal path in the parse tree from the predicate to the constituent. Subcategorization describes the phrase structure around the predicate’s parent. It records the immediate structure in the parse tree that expands to its parent. Verb class is the class of the active predicate described in PropBank Frames. Lengths the target constituent, in the numbers of words and chunks separately. Chunk if the target argument is, embeds, overlaps, or is embedded in a chunk with its type. Chunk pattern length counts the number of chunks from the predicate to the argument. Clause relative position the position of the target word relative to the predicate in the pseudo-parse tree constructed only from clause constituent. There are four configurations—target constituent and predicate share the same parent, target constituent parent is an ancestor of predicate, predicate parent is an ancestor of target word, or otherwise. Clause coverage how much of the local clause (from the predicate) is covered by the argument. It is to the multiples of 1.3 Argument Classification This stage assigns the final argument labels to the argument candidates supplied from the previous stage. A multi-class classifier is trained to classify the types of the arguments supplied by the argument identification stage. To reduce the excessive candidates mistakenly output by the previous stage, the can also classify the argument as (“not an argument”) to discard the argument. The features used here are the same as those used in the argument identification stage with the following additional features. Syntactic frame the sequential pattern of the noun phrases and the predicate in the sentence. This is the feature introduced by (Xue and Palmer, 2004). Propositional phrase head the head of the first phrase the preposition inside NEG and MOD indicate if the argument is a baseline for AM-NEG or AM-MOD. The rules of the are used in a baseline SRL system developed by Erik Tjong Kim Sang (Carreras and M`arquez, 2004). NE if the target argument is, embeds, overlaps, or is embedded in a named-entity along with its type. 1.4 Inference The purpose of this stage is to incorporate some prior linguistic and structural knowledge, such as “arguments do not overlap” or “each verb takes at most one argument of each type.” This knowledge is used to resolve any inconsistencies of argument classification in order to generate final legitimate predictions. We use the inference process introduced by (Punyakanok et al., 2004). The process is formulated as an integer linear programming (ILP) problem that takes as inputs the confidences over each type of the arguments supplied by the argument classifier. The output is the optimal solution that maximizes the linear sum of the confidence scores (e.g., the conditional probabilities estimated by the argument classifier), subject to the constraints that encode the domain knowledge. Formally speaking, the argument classifier atto assign labels to a set of arguments, from 1 to Each argument can take label from a set of argument labels, and the indexed set of arguments can take a set of labels, If we assume that the argument classifier returns an estimated conditional probability dis- = then, given a sentence, the inference procedure seeks an global assignment that maximizes the following objective function, M =argmax = subject to linguistic and structural constraints. In other words, this objective function reflects the expected number of correct argument predictions, subject to the constraints. The constraints are encoded as the followings. • No overlapping or embedding arguments. • No duplicate argument classes for A0-A5. • Exactly one V argument per predicate considered. • If there is C-V, then there has to be a V-A1-CV pattern. If there is an then there has to be an argument. If there is a there must be an argumoreover, the must occur after • Given the predicate, some argument types are illegal (e.g. predicate ‘stalk’ can take only A0 or A1). The illegal types may consist of A0-A5 and their corresponding C- For each predicate, we look the minimum value of that the class menin its frame file as well as its maximum value argument types that &lt; i &gt; j considered illegal. 182 2 Inference with Multiple SRL Systems The inference process allows a natural way to combine the outputs from multiple argument classi- Specifically, given classifiers perform classification on sets, ... , The inference process aims to optimize the objective function: traders say, unable tocoolthe selling panic in both stocks and futures. b b 2 3 argmax N = 1: Two SRL systems’ output and phantom candidates and = = = = the probability output by system Note that all systems may not output with the same set of argument candidates due to the pruning and argument identification. For the systems that do not output for any candidate, we assign the probawith a prior to this In parthe probability of the is set to be 0.6 based on empirical tests, and the probabilities of the other classes are set proportionally to their occurrence frequencies in the training data. For example, Figure 1 shows the two candidate for a fragment of a sentence, traders say, to selling panic in both stocks and In this example, system A has two argucandidates, and selling panic in both stocks and futures”; system B has argument candidates, panic”, and both stocks and fu- The phantom candidates are created for and which probability is set to the prior. Specifically for this implementation, we first train two SRL systems that use Collins’ parser and Charniak’s parser respectively. In fact, these two parsers have noticeably different output. In evaluation, we run the system that was trained with Charniak’s parser 5 times with the top-5 parse trees output by Together we have six different outputs per predicate. Per each parse tree output, we ran the first three stages, namely pruning, argument top parse tree were from the official output by CoNLL. The 2nd-5th parse trees were output by Charniak’s parser. identification, and argument classification. Then a joint inference stage is used to resolve the inconsistency of the output of argument classification in these systems. 3 Learning and Evaluation The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It improves the basic Winnow multiplicative update rule with a regularization term, which has the effect of trying to separate the data with a large margin separator (Grove and Roth, 2001; Hang et al., 2002) and voted (averaged) weight vector (Freund and Schapire, 1999). Softmax function (Bishop, 1995) is used to convert raw activation to conditional probabilities. If are and the raw activation of class the posterior estimation for class = . In summary, training used both full and partial syntactic information as described in Section 1. In training, SNoW’s default parameters were used with the exception of the separator thickness 1.5, the use of average weight vector, and 5 training cycles. The parameters are optimized on the development set. Training for each system took about 6 hours. The evaluation on both test sets which included running 183</abstract>
<title confidence="0.463426">Precision Recall</title>
<note confidence="0.95103756">Development 80.05% 74.83% 77.35 Test WSJ 82.28% 76.78% 79.44 Test Brown 73.38% 62.93% 67.75 Test WSJ+Brown 81.18% 74.92% 77.92 Test WSJ Precision Recall Overall 82.28% 76.78% 79.44 A0 88.22% 87.88% 88.05 A1 82.25% 77.69% 79.91 A2 78.27% 60.36% 68.16 A3 82.73% 52.60% 64.31 A4 83.91% 71.57% 77.25 A5 0.00% 0.00% 0.00 AM-ADV 63.82% 56.13% 59.73 AM-CAU 64.15% 46.58% 53.97 AM-DIR 57.89% 38.82% 46.48 AM-DIS 75.44% 80.62% 77.95 AM-EXT 68.18% 46.88% 55.56 AM-LOC 66.67% 55.10% 60.33 AM-MNR 66.79% 53.20% 59.22 AM-MOD 96.11% 98.73% 97.40 AM-NEG 97.40% 97.83% 97.61 AM-PNC 60.00% 36.52% 45.41 AM-PRD 0.00% 0.00% 0.00 AM-REC 0.00% 0.00% 0.00 AM-TMP 78.16% 76.72% 77.44 R-A0 89.72% 85.71% 87.67 R-A1 70.00% 76.28% 73.01 R-A2 85.71% 37.50% 52.17 R-A3 0.00% 0.00% 0.00 R-A4 0.00% 0.00% 0.00 R-AM-ADV 0.00% 0.00% 0.00 R-AM-CAU 0.00% 0.00% 0.00 R-AM-EXT 0.00% 0.00% 0.00 R-AM-LOC 85.71% 57.14% 68.57 R-AM-MNR 0.00% 0.00% 0.00 R-AM-TMP 72.34% 65.38% 68.69 V 98.92% 97.10% 98.00 Table 1: Overall results (top) and detailed results on the WSJ test (bottom). with all six different parse trees (assumed already given) and the joint inference took about 4.5 hours. Precision Recall Charniak-1 75.40% 74.13% 74.76 Charniak-2 74.21% 73.06% 73.63 Charniak-3 73.52% 72.31% 72.91 Charniak-4 74.29% 72.92% 73.60 Charniak-5 72.57% 71.40% 71.98 Collins 73.89% 70.11% 71.95 Joint inference 80.05% 74.83% 77.35 Table 2: The results of individual systems and the</note>
<abstract confidence="0.998314235294118">result with joint inference on the development set. Overall results on the development and test sets are shown in Table 1. Table 2 shows the results of individual systems and the improvement gained by the joint inference on the development set. 4 Conclusions We present an implementation of SRL system which composed of four stages—1) pruning, 2) argument identification, 3) argument classification, and 4) inference. The inference provides a natural way to take the output of multiple argument classifiers and combines them into a coherent predicate-argument output. Significant improvement in overall SRL performance through this inference is illustrated. Acknowledgments We are grateful to Dash Optimization for the free academic use of Xpress-MP. This research is sup-</abstract>
<note confidence="0.8241058">ported by ARDA’s AQUAINT Program, DOI’s Reflex program, and an ONR MURI Award. References Bishop, 1995. Networks for Pattern chapter 6.4: Modelling conditional distributions, page 215.</note>
<affiliation confidence="0.670367">Oxford University Press.</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Bishop</author>
</authors>
<title>Neural Networks for Pattern Recognition, chapter 6.4: Modelling conditional distributions,</title>
<date>1995</date>
<pages>215</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="11366" citStr="Bishop, 1995" startWordPosition="1899" endWordPosition="1900">1998; Roth and Yih, 2002), a multi-class classifier that is tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It improves the basic Winnow multiplicative update rule with a regularization term, which has the effect of trying to separate the data with a large margin separator (Grove and Roth, 2001; Hang et al., 2002) and voted (averaged) weight vector (Freund and Schapire, 1999). Softmax function (Bishop, 1995) is used to convert raw activation to conditional probabilities. If there are n classes and the raw activation of class i is acti, the posterior estimation for class i is each Prob(i) = E1&lt; each j ct . In summary, training used both full and partial syntactic information as described in Section 1. In training, SNoW’s default parameters were used with the exception of the separator thickness 1.5, the use of average weight vector, and 5 training cycles. The parameters are optimized on the development set. Training for each system took about 6 hours. The evaluation on both test sets which include</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>C. Bishop, 1995. Neural Networks for Pattern Recognition, chapter 6.4: Modelling conditional distributions, page 215. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the conll2004 shared tasks: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proc. of CoNLL-2004.</booktitle>
<marker>Carreras, M`arquez, 2004</marker>
<rawString>X. Carreras and L. M`arquez. 2004. Introduction to the conll2004 shared tasks: Semantic role labeling. In Proc. of CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department, University of Pennsylvenia,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="3160" citStr="Collins, 1999" startWordPosition="506" endWordPosition="507">t Identification The argument identification stage utilizes binary classification to identify whether a candidate is an argument or not. We train and apply the binary classifiers on the constituents supplied by the pruning stage. Most of the features used in our system are standard features, which include • Predicate and POS tag of predicate indicate the lemma of the predicate and its POS tag. • Voice indicates tbe voice of the predicate. • Phrase type of the constituent. • Head word and POS tag of the head word include head word and its POS tag of the constituent. We use rules introduced by (Collins, 1999) to extract this feature. • First and last words and POS tags of the constituent. • Two POS tags before and after the constituent. • Position feature describes if the constituent is before or after the predicate relative to the position in the sentence. 181 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 181–184, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics • Path records the traversal path in the parse tree from the predicate to the constituent. • Subcategorization feature describes the phrase structure around the predicate</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, Computer Science Department, University of Pennsylvenia, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="11333" citStr="Freund and Schapire, 1999" startWordPosition="1893" endWordPosition="1896">innow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It improves the basic Winnow multiplicative update rule with a regularization term, which has the effect of trying to separate the data with a large margin separator (Grove and Roth, 2001; Hang et al., 2002) and voted (averaged) weight vector (Freund and Schapire, 1999). Softmax function (Bishop, 1995) is used to convert raw activation to conditional probabilities. If there are n classes and the raw activation of class i is acti, the posterior estimation for class i is each Prob(i) = E1&lt; each j ct . In summary, training used both full and partial syntactic information as described in Section 1. In training, SNoW’s default parameters were used with the exception of the separator thickness 1.5, the use of average weight vector, and 5 training cycles. The parameters are optimized on the development set. Training for each system took about 6 hours. The evaluatio</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Grove</author>
<author>D Roth</author>
</authors>
<title>Linear concepts and hidden variables.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>42--1</pages>
<contexts>
<context position="11250" citStr="Grove and Roth, 2001" startWordPosition="1880" endWordPosition="1883">3 Learning and Evaluation The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It improves the basic Winnow multiplicative update rule with a regularization term, which has the effect of trying to separate the data with a large margin separator (Grove and Roth, 2001; Hang et al., 2002) and voted (averaged) weight vector (Freund and Schapire, 1999). Softmax function (Bishop, 1995) is used to convert raw activation to conditional probabilities. If there are n classes and the raw activation of class i is acti, the posterior estimation for class i is each Prob(i) = E1&lt; each j ct . In summary, training used both full and partial syntactic information as described in Section 1. In training, SNoW’s default parameters were used with the exception of the separator thickness 1.5, the use of average weight vector, and 5 training cycles. The parameters are optimized</context>
</contexts>
<marker>Grove, Roth, 2001</marker>
<rawString>A. Grove and D. Roth. 2001. Linear concepts and hidden variables. Machine Learning, 42(1/2):123–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hang</author>
<author>F Damerau</author>
<author>D Johnson</author>
</authors>
<title>Text chunking based on a generalization of winnow.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--615</pages>
<contexts>
<context position="11270" citStr="Hang et al., 2002" startWordPosition="1884" endWordPosition="1887">ion The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It improves the basic Winnow multiplicative update rule with a regularization term, which has the effect of trying to separate the data with a large margin separator (Grove and Roth, 2001; Hang et al., 2002) and voted (averaged) weight vector (Freund and Schapire, 1999). Softmax function (Bishop, 1995) is used to convert raw activation to conditional probabilities. If there are n classes and the raw activation of class i is acti, the posterior estimation for class i is each Prob(i) = E1&lt; each j ct . In summary, training used both full and partial syntactic information as described in Section 1. In training, SNoW’s default parameters were used with the exception of the separator thickness 1.5, the use of average weight vector, and 5 training cycles. The parameters are optimized on the development </context>
</contexts>
<marker>Hang, Damerau, Johnson, 2002</marker>
<rawString>T. Hang, F. Damerau, and D. Johnson. 2002. Text chunking based on a generalization of winnow. Journal of Machine Learning Research, 2:615–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>In Proc. of COLING-2004.</booktitle>
<contexts>
<context position="6265" citStr="Punyakanok et al., 2004" startWordPosition="1013" endWordPosition="1016">d MOD features are used in a baseline SRL system developed by Erik Tjong Kim Sang (Carreras and M`arquez, 2004). • NE indicates if the target argument is, embeds, overlaps, or is embedded in a named-entity along with its type. 1.4 Inference The purpose of this stage is to incorporate some prior linguistic and structural knowledge, such as “arguments do not overlap” or “each verb takes at most one argument of each type.” This knowledge is used to resolve any inconsistencies of argument classification in order to generate final legitimate predictions. We use the inference process introduced by (Punyakanok et al., 2004). The process is formulated as an integer linear programming (ILP) problem that takes as inputs the confidences over each type of the arguments supplied by the argument classifier. The output is the optimal solution that maximizes the linear sum of the confidence scores (e.g., the conditional probabilities estimated by the argument classifier), subject to the constraints that encode the domain knowledge. Formally speaking, the argument classifier attempts to assign labels to a set of arguments, S1:M, indexed from 1 to M. Each argument Si can take any label from a set of argument labels, P, and</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. Semantic role labeling via integer linear programming inference. In Proc. of COLING-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Probabilistic reasoning for entity &amp; relation recognition.</title>
<date>2002</date>
<booktitle>In Proc. of COLING-2002,</booktitle>
<pages>835--841</pages>
<contexts>
<context position="10778" citStr="Roth and Yih, 2002" startWordPosition="1803" endWordPosition="1806">-5 parse trees output by Charniak’s parser1. Together we have six different outputs per predicate. Per each parse tree output, we ran the first three stages, namely pruning, argument 1The top parse tree were from the official output by CoNLL. The 2nd-5th parse trees were output by Charniak’s parser. identification, and argument classification. Then a joint inference stage is used to resolve the inconsistency of the output of argument classification in these systems. 3 Learning and Evaluation The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It improves the basic Winnow multiplicative update rule with a regularization term, which has the effect of trying to separate the data with a large margin separator (Grove and Roth, 2001; Hang et al., 2002) and voted (averaged) weight vector (Freund and Schapire, 1999). Softmax function (Bishop, 1995) is used to </context>
</contexts>
<marker>Roth, Yih, 2002</marker>
<rawString>D. Roth and W. Yih. 2002. Probabilistic reasoning for entity &amp; relation recognition. In Proc. of COLING-2002, pages 835– 841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: A unified approach.</title>
<date>1998</date>
<booktitle>In Proc. ofAAAI,</booktitle>
<pages>806--813</pages>
<contexts>
<context position="10757" citStr="Roth, 1998" startWordPosition="1801" endWordPosition="1802">with the top-5 parse trees output by Charniak’s parser1. Together we have six different outputs per predicate. Per each parse tree output, we ran the first three stages, namely pruning, argument 1The top parse tree were from the official output by CoNLL. The 2nd-5th parse trees were output by Charniak’s parser. identification, and argument classification. Then a joint inference stage is used to resolve the inconsistency of the output of argument classification in these systems. 3 Learning and Evaluation The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It improves the basic Winnow multiplicative update rule with a regularization term, which has the effect of trying to separate the data with a large margin separator (Grove and Roth, 2001; Hang et al., 2002) and voted (averaged) weight vector (Freund and Schapire, 1999). Softmax function (Bish</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In Proc. ofAAAI, pages 806–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>M Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proc. of the EMNLP-2004,</booktitle>
<pages>88--94</pages>
<location>Barcelona,</location>
<contexts>
<context position="1711" citStr="Xue and Palmer, 2004" startWordPosition="254" endWordPosition="257">es for a given verb predicate. The system only classifies the argument candidates into their types during the argument classification stage. Linguistic and structural constraints are incorporated in the inference stage to resolve inconsistent global predictions. The inference stage can take as its input the output of the argument classification of a single system or of multiple systems. We explain the inference for multiple systems in Sec. 2. 1.1 Pruning Only the constituents in the parse tree are considered as argument candidates. In addition, our system exploits the heuristic introduced by (Xue and Palmer, 2004) to filter out very unlikely constituents. The heuristic is a recursive process starting from the verb whose arguments are to be identified. It first returns the siblings of the verb; then it moves to the parent of the verb, and collects the siblings again. The process goes on until it reaches the root. In addition, if a constituent is a PP (propositional phrase), its children are also collected. Candidates consisting of only a single punctuation mark are not considered. This heuristic works well with the correct parse trees. However, one of the errors by automatic parsers is due to incorrect </context>
<context position="5440" citStr="Xue and Palmer, 2004" startWordPosition="871" endWordPosition="874"> supplied from the previous stage. A multi-class classifier is trained to classify the types of the arguments supplied by the argument identification stage. To reduce the excessive candidates mistakenly output by the previous stage, the classifier can also classify the argument as NULL (“not an argument”) to discard the argument. The features used here are the same as those used in the argument identification stage with the following additional features. • Syntactic frame describes the sequential pattern of the noun phrases and the predicate in the sentence. This is the feature introduced by (Xue and Palmer, 2004). • Propositional phrase head is the head of the first phrase after the preposition inside PP. • NEG and MOD feature indicate if the argument is a baseline for AM-NEG or AM-MOD. The rules of the NEG and MOD features are used in a baseline SRL system developed by Erik Tjong Kim Sang (Carreras and M`arquez, 2004). • NE indicates if the target argument is, embeds, overlaps, or is embedded in a named-entity along with its type. 1.4 Inference The purpose of this stage is to incorporate some prior linguistic and structural knowledge, such as “arguments do not overlap” or “each verb takes at most one</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>N. Xue and M. Palmer. 2004. Calibrating features for semantic role labeling. In Proc. of the EMNLP-2004, pages 88–94, Barcelona, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>