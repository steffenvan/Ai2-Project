<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.989545">
Thai Sentence-Breaking for Large-Scale SMT
</title>
<author confidence="0.98583">
Glenn Slayden Mei-Yuh Hwang Lee Schwartz
</author>
<affiliation confidence="0.94817">
thai-language.com Microsoft Research Microsoft Research
</affiliation>
<email confidence="0.980598">
glenn@thai-language.com mehwang@microsoft.com leesc@microsoft.com
</email>
<sectionHeader confidence="0.982285" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998394375">
Thai language text presents challenges
for integration into large-scale multi-
language statistical machine translation
(SMT) systems, largely stemming from
the nominal lack of punctuation and in-
ter-word space. For Thai sentence break-
ing, we describe a monolingual maxi-
mum entropy classifier with features that
may be applicable to other languages
such as Arabic, Khmer and Lao. We ap-
ply this sentence breaker to our large-
vocabulary, general-purpose, bidirec-
tional Thai-English SMT system, and
achieve BLEU scores of around 0.20,
reaching our threshold of releasing it as a
free online service.
</bodyText>
<sectionHeader confidence="0.995037" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950412698413">
NLP research has consolidated around the notion
of the sentence as the fundamental unit of trans-
lation, a consensus which has fostered the devel-
opment powerful statistical and analytical ap-
proaches which incorporate an assumption of
deterministic sentence delineation. As such sys-
tems become more sophisticated, languages for
which this assumption is challenged receive in-
creased attention. Thai is one such language,
since it uses space neither to distinguish syl-
lables from words or affixes, nor to unambi-
guously signal sentence boundaries.
Written Thai has no sentence-end punctuation,
but a space character is always present between
sentences. There is generally no space between
words, but a space character may appear within a
sentence according to linguistic or prescriptive
orthographic motivation (Wathabunditkul 2003),
and these characteristics disqualify sentence-
breaking (SB) methods used for other languages,
such as Palmer and Hearst (1997). Thai SB has
therefore been regarded as the task of classifying
each space that appears in a Thai source text as
either sentence-breaking (sb) or non-sentence-
breaking (nsb).
Several researchers have investigated Thai
SB. Along with a discussion of Thai word break-
ing (WB), Aroonmanakun (2007) examines the
issue. With a human study, he establishes that
sentence breaks elicited from Thai informants
exhibit varying degrees of consensus. Mittra-
piyanuruk and Sornlertlamvanich (2000) define
part-of-speech (POS) tags for sb and nsb and
train a trigram model over a POS-annotated cor-
pus. At runtime, they use the Viterbi algorithm
to select the POS sequence with the highest
probability, from which the corresponding space
type is read back. Charoenpornsawat and Sornler-
tlamvanich (2001) apply Winnow, a multiplica-
tive trigger threshold classifier, to the problem.
Their model has ten features: the number of
words to the left and right, and the left-two and
right-two POS tags and words.
We present a monolingual Thai SB based on a
maximum entropy (ME) classifier (Ratnaparkhi
1996; Reynar and Ratnaparkhi, 1997) which is
suitable for sentence-breaking SMT training data
and runtime inputs. Our model uses a four token
window of Thai lemmas, plus categorical fea-
tures, to describe the proximal environment of
the space token under consideration, allowing
runtime classification of space tokens with pos-
sibly unseen contexts.
As our SB model relies on Thai WB, we re-
view our approach to this problem, plus related
preprocessing, in the next section. Section 2 also
discusses the complementary operation to WB,
namely, the re-spacing of Thai text generated by
SMT output. Section 3 details our SB model and
evaluates its performance. We describe the inte-
gration of this work with our large-scale SMT
system in Section 4. We draw conclusions in
Section 5.
</bodyText>
<page confidence="0.524869">
8
</page>
<note confidence="0.972177">
Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), pages 8‚Äì16,
the 23rd International Conference on Computational Linguistics (COLING), Beijing, August 2010
</note>
<sectionHeader confidence="0.301087" genericHeader="introduction">
2 Pre- and Post-processing
</sectionHeader>
<bodyText confidence="0.9999709">
As will be shown in Section 3, our sentence
breaker relies on Thai WB. In turn, with the aim
of minimizing WB errors, we perform Unicode
character sequence normalization prior to WB.
As output byproducts, our WB analysis readily
identifies certain types of named entities which
we propagate into our THA-ENG SMT; in this
section, we briefly summarize these preliminary
processing steps, and we conclude the section
with a discussion of Thai text re-spacing.
</bodyText>
<subsectionHeader confidence="0.968323">
2.1 Character Sequence Normalization
</subsectionHeader>
<bodyText confidence="0.9953228">
Thai orthography uses an alphabet of 44 conso-
nants and a number of vowel glyphs and tone
marks. The four Thai tone marks and some Thai
vowel characters are super- and/or sub-scripted
with respect to a base character. For example,
‡πâ
the ‡∏≠‡∏¥ sequence consists of three code points:
‡∏≠ ‚óå‡∏¥ ‚óå‡πâ. When two or more of these combining
marks are present on the same base character, the
ordering of these code points in memory should
be consistent so that orthographically identical
entities are recognized as equivalent by comput-
er systems. However, some computer word pro-
cessors do not enforce the correct sequence or do
not properly indicate incorrect sequences to the
user visually. This often results in documents
with invalid byte sequences.
Correcting these errors is desirable for SMT
inputs. In order to normalize Thai input character
sequences to a canonical Unicode form, we de-
veloped a finite state transducer (FST) which
detects and repairs a number of sequencing er-
rors which render Thai text either orthographi-
cally invalid, or not in a correct Unicode se-
quence.
For example, a superscripted Thai tone mark
should follow a super- or sub-scripted Thai vo-
wel when they both apply to the same consonant.
When the input has the tone mark and the vowel
glyph swapped, the input can be fully repaired:
</bodyText>
<equation confidence="0.740432">
‡∏≠ ‡∏≤ ‚óå‡πà ‡∏ô ‚Üí ‡∏≠ ‚óå‡πà ‡∏≤ ‡∏ô ‚Üí ‡∏≠‡πà‡∏≤‡∏ô
</equation>
<figureCaption confidence="0.985519">
Figure 1. Two unambiguous repairs
</figureCaption>
<bodyText confidence="0.966703375">
Other cases are ambiguous. The occurrence of
multiple adjacent vowel glyphs is an error where
the intention may not be clear. We retain the
first-appearing glyph, unless it is a pre-posed
vowel, in which case we retain the last-appearing
instance. These two treatments are contrasted in
Figure 2. Miscoding (Figure 3) is another variety
of input error that is readily repaired.
</bodyText>
<equation confidence="0.8410145">
‡∏à‡∏∞‡∏∞‡∏≤ ‚Üí ‡∏à‡∏∞
‡πÉ‡πÄ‡πÑ‡∏õ ‚Üí ‡πÑ‡∏õ
</equation>
<figureCaption confidence="0.991896">
Figure 2. Two ambiguous repairs
</figureCaption>
<bodyText confidence="0.998935090909091">
Within the Infoquest Thai newswire corpus, a
low-noise corpus, about 0.05% of the lines exhi-
bit at least one of the problems mentioned here.
For some chunks of broad-range web scraped
data, we observe rates as high as 4.1%. This
measure is expected to under-represent the utility
of the filter to WB, since Thai text streams, lack-
ing intra-word spacing and permitting two un-
written vowels, have few re-alignment check-
points, allowing tokenization state machines to
linger in misaligned states.
</bodyText>
<equation confidence="0.9852305">
‡∏≠ ‚óå‡πç ‡∏≤ ‚Üí ‡∏≠ ‚óå‡πç‡∏≤ ‚Üí ‡∏≠‡πç‡∏≤
‡πÄ ‡πÄ ‡∏≠ ‚Üí ‡πÅ ‡∏≠ ‚Üí ‡πÅ‡∏≠
</equation>
<figureCaption confidence="0.979975">
Figure 3. Two common mis-codings
</figureCaption>
<subsectionHeader confidence="0.998737">
2.2 Uniscribe Thai Tokenization
</subsectionHeader>
<bodyText confidence="0.999923464285714">
Thai text does not normally use the space cha-
racter to separate words, except in certain specif-
ic contexts. Although Unicode offers the Zero-
Width Space (ZWSP) as one solution for indicat-
ing word breaks in Thai, it is infrequently used.
Programmatic tokenization has become a staple
of Thai computational linguistics. The problem
has been well studied, with precision and recall
near 95% (Haruechaiyasak et al. 2008).
In our SMT application, both the sentence
breaker and the SMT system itself require Thai
WB, and we use the same word breaker for these
tasks (although the system design currently pro-
hibits directly passing tokens between these two
components). Our method is to apply post-
processing heuristics to the output of Uniscribe
(Bishop et al. 2003), which is provided as part of
the Microsoft¬Æ WindowsTM operating system
interface. Our heuristics fall into two categories:
‚Äúre-gluing‚Äù words that Uniscribe broke too ag-
gressively, and a smaller class of cases of further
breaking of words that Uniscribe did not break.
Re-gluing is achieved by comparing Uniscribe
output against a Thai lexicon in which desired
breaks within a word are tagged. Underbreaking
by Uniscribe is less common and is restricted to
a number of common patterns which are repaired
explicitly.
</bodyText>
<equation confidence="0.9693974">
‡πâ
‡πâ
‡∏¥
‚óå
‚óå
‡∏ô
‡∏ô ‚Üí ‡∏≠ ‚óå‡∏¥
‡∏ô ‚Üí ‡∏≠‡∏¥
‡∏≠ ‚óå ‡πâ
9
</equation>
<subsectionHeader confidence="0.99869">
2.3 Person Name Entities
</subsectionHeader>
<bodyText confidence="0.99847752631579">
In written Thai, certain types of entities employ
prescriptive whitespace patterns. By removing
these recognized patterns from consideration, SB
precision can be improved. Furthermore, be-
cause our re-gluing procedure requires a lookup
of every syllable proposed by Uniscribe, it is
efficient to consider, during WB, additional
processing that can be informed by the same
lookup. Accordingly, we briefly mention some
of the entity types that our WB identifies, focus-
ing on those that incorporate distinctive spacing
patterns.
Person names in Thai adhere to a convention
for the use of space characters. This helps Thai
readers to identify the boundaries of multi-
syllable surnames that they may not have seen
before. The following grammar summarizes the
prescriptive conventions for names appearing in
Thai text:
</bodyText>
<equation confidence="0.786561666666667">
&lt;name-entity&gt; ::= &lt;honorific&gt; &lt;full-name&gt;
&lt;full-name&gt; ::= &lt;first-name&gt; [&lt;last-name&gt;]
&lt;first-name&gt; ::= &lt;name-text&gt; space
&lt;last-name&gt; ::= &lt;name-text&gt; space
&lt;name-text&gt; ::= &lt;thai-alphabetic-char&gt;+
&lt;thai-alphabetic-char&gt; ::= ‡∏Å  |‡∏Ç  |‡∏É  |‡∏Ñ  |...
</equation>
<figureCaption confidence="0.980287">
Figure 4. Name entity recognition grammar
</figureCaption>
<bodyText confidence="0.999940428571429">
The re-glue lookup also determines if a sylla-
ble matches one of the following predefined spe-
cial categories: name-introducing honorific (h),
Thai or foreign given name (g), token which is
likely to form part of a surname (s), or token
which aborts the gathering of a name (i.e. is un-
likely to form part of a name).
</bodyText>
<table confidence="0.5917194">
.../‡∏ß‡πà‡∏≤/‡∏ô‡∏≤‡∏¢/‡∏à‡∏¥/‡∏£‡∏∞/‡∏ô‡∏∏‡∏ä/ /‡∏ß‡∏¥/‡∏ô‡∏¥‡∏à/‡∏à‡∏Å‡∏π/‡∏•/ /‡∏ß‡πà‡∏≤/...
‡∏ß‡πà‡∏≤ ‡∏ô‡∏≤‡∏¢ ‡∏à‡∏¥ ‡∏£‡∏∞ ‡∏ô‡∏∏‡∏ä ‡∏¥‡∏ß ‡∏ô‡∏¥‡∏à ‡∏à‡∏Å‡∏π ‡∏• ‡∏ß‡πà‡∏≤
h g0 g1 g2 sp0 s0 s1 s2 s3 sp1
that Mr. &lt;oov&gt; hit beloved &lt;oov&gt; stable &lt;oov&gt; &lt;oov&gt; said
...that Mr. Chiranut Winichotkun said...
</table>
<figureCaption confidence="0.9894">
Figure 5. Thai person-name entity recognition
</figureCaption>
<bodyText confidence="0.999972043478261">
Figure 5 shows a Thai name appearing within
a text fragment, with Uniscribe detected token
boundaries indicated by slashes. In the third row
we have identified the special category, if any,
for each token. The fourth line shows the Eng-
lish translation gloss, or &lt;oov&gt; if none. The bot-
tom row is the desired translation output.
Our name identifier first notes the presence of
an honorific {h} ‡∏ô‡∏≤‡∏¢ followed by a pattern of
tokens {g0-gn}, {s0-sn} and spaces {sp0, sp1}
that is compatible with a person name and sur-
name of sensible length.
Next, we determine which of those tokens in
the ranges {g} and {s} following the honorific
do not have a gloss translation (i.e., are not
found in the lexicon). These tokens are indicated
by &lt;oov&gt; in the gloss above. When the number
of unknown tokens exceeds a threshold, we hy-
pothesize that these tokens form a name. The
lack of lexical morphology in Thai facilitates
this method because token (or syllable) lookup
generally equates with the lookup of a stemmed
lemma.
</bodyText>
<subsectionHeader confidence="0.994806">
2.4 Calendar Date Entities
</subsectionHeader>
<bodyText confidence="0.997096090909091">
Our WB also identifies Thai calendar dates, as
these also exhibit a pattern which incorporates
spaces. As a prerequisite to identifying dates, we
map Thai orthographic digits {‡πê ‡πë ‡πí ‡πì ‡πî ‡πï ‡πñ
‡πó ‡πò ‡πô} to Arabic digits 0 through 9, respec-
tively. For example, our system would interpret
the input text ‡πí‡πï‡πî‡πê as equivalent to ‚Äú2540.‚Äù
.../‡πÉ‡∏ô/‡∏ß‡∏±‡∏ô/‡∏ó‡∏µ‡πà/ /14/ ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°/ /‡πí‡πï‡πî‡πê/ /‡πÅ‡∏•‡∏∞/...
‡πÉ‡∏ô ‡∏ß‡∏±‡∏ô ‡∏ó‡∏µ‡πà sp 14 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° sp ‡πí‡πï‡πî‡πê sp ‡πÅ‡∏•‡∏∞
on day which 14 March 2540 and
...on March 14th, 1997 and...
</bodyText>
<figureCaption confidence="0.935487">
Figure 6. Date entity recognition
</figureCaption>
<bodyText confidence="0.999695625">
Figure 6 shows a fragment of Thai text which
contains a calendar date for which our system
will emit a single token. As shown in the exam-
ple, our system detects and adjusts for the use of
Thai Buddhist year dates when necessary. Ga-
thering of disparate and optional parts of the
Thai date is summarized by the grammar in Fig-
ure 7.
</bodyText>
<equation confidence="0.999464615384615">
&lt;date-entity&gt; ::= [&lt;cardinal-words&gt;] [space] &lt;date&gt;
&lt;cardinal-words&gt; ::= ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà  |‡∏ó‡∏µ‡πà
&lt;date&gt; ::= month-date [space] year
&lt;year&gt; ::= &lt;tha-digit&gt; &lt;tha-digit&gt; &lt;tha-digit&gt; &lt;tha-digit&gt;
&lt;year&gt; ::= &lt;ara-digit&gt; &lt;ara-digit&gt; &lt;ara-digit&gt; &lt;ara-digit&gt;
&lt;month-date&gt; ::= &lt;day&gt; [space] &lt;month&gt;
&lt;day&gt; ::= &lt;thai-digit&gt;+
&lt;day&gt; ::= &lt;ara-digit&gt;+
&lt;month&gt; ::= &lt;month-full&gt;  |&lt;month-abbr&gt;
&lt;month-full&gt; ::= ‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°  |‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå  |‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°  |...
&lt;month-abbr&gt; ::= ‡∏°.‡∏Ñ.  |‡∏Å.‡∏û.  |‡∏°‡∏µ.‡∏Ñ.  |...
&lt;tha-digit&gt; ::= ‡πê  |‡πë  |‡πí  |‡πì  |‡πî  |‡πï  |‡πñ  |‡πó  |‡πò  |‡πô
&lt;ara-digit&gt; ::= 0  |1  |2  |3  |4  |5  |6  |7  |8  |9
</equation>
<figureCaption confidence="0.996598">
Figure 7. Date recognition grammar
</figureCaption>
<figure confidence="0.649946">
10
</figure>
<subsectionHeader confidence="0.967252">
2.5 Thai Text Re-spacing
</subsectionHeader>
<bodyText confidence="0.999972785714286">
To conclude this section, we mention an opera-
tion complementary to Thai WB, whereby Thai
words output by an SMT system must be re-
spaced in accordance with Thai prescriptive
convention. As will be mentioned in Section 4.2,
for each input sentence, our English-Thai system
has access to an English dependency parse tree,
as well as links between this tree and a Thai
transfer dependency tree. After using these links
to transfer syntactic information to the Thai tree,
we are able to apply prescriptive spacing rules
(Wathabunditkul 2003) as closely as possible.
Human evaluation showed satisfactory results
for this process.
</bodyText>
<sectionHeader confidence="0.964362" genericHeader="method">
3 Maximum Entropy Sentence-Breaking
</sectionHeader>
<bodyText confidence="0.999981">
We now turn to a description of our statistical
sentence-breaking model. We train an ME clas-
sifier on features which describe the proximal
environment of the space token under considera-
tion and use this model at runtime to classify
space tokens with possibly unseen contexts.
</bodyText>
<subsectionHeader confidence="0.998121">
3.1 Modeling
</subsectionHeader>
<bodyText confidence="0.972294571428571">
Under the ME framework, let B={sb, nsb}
represent the set of possible classes we are inter-
ested in predicting for each space token in the
input stream. Let C={linguistic contexts}
represent the set of possible contexts that we can
observe, which must be encoded by binary fea-
tures, ùëì‡Øù(b, c),1 &lt; ùëó &lt; ùëò, such as:
</bodyText>
<equation confidence="0.902681">
J1 if the previous word is English ùëéùëõùëë b = ùêßùê¨ùêõ.
ùëì‡¨µ (b, c) = 1 0 otherwise.
</equation>
<bodyText confidence="0.855052">
This feature helps us learn that the space after an
English word is usually not a sentence boundary.
</bodyText>
<equation confidence="0.955245">
(1 if the distance to the previous honorific
ùëìz (b, c) = j is less than 15 tokens ùëéùëõùëë b = ùêßùê¨ùêõ
( 0 otherwise.
</equation>
<bodyText confidence="0.99156975">
This feature enables us to learn that spaces
which follow an honorific are less likely to mark
sentence boundaries. Assume the joint probabili-
ty p(b,c) is modeled by
</bodyText>
<equation confidence="0.990183333333333">
‡Øû ‡Øô‡≥ï(‡Øï,‡Øñ)
ùëù(b, c) = Z ‡∑ëùú∂‡Øù
‡Øù‡≠Ä‡¨µ
</equation>
<bodyText confidence="0.9931304">
where we have k free parameters {ùú∂‡Øù} to esti-
mate and Z is a normalization factor to make
‚àë‡Øï,‡Øñ ùëù(b, c) = 1. The ME learning algorithm
finds a solution {ùú∂‡Øù} representing the most un-
certain commitment
</bodyText>
<equation confidence="0.531797">
max H(ùëù) = ‚àí‡∑ç ùëù(b, c) logùëù(b, c)
</equation>
<bodyText confidence="0.9970735">
that satisfies the observed distribution ùëùÃÇ(b, c) of
the training data
</bodyText>
<equation confidence="0.581534">
‚àë ùëù(b, c)ùëì‡Øù(b, c) = ‚àë ùëùÃÇ(b, c)ùëì‡Øù(b, c), 1 &lt; ùëó &lt; ùëò .
</equation>
<bodyText confidence="0.999807">
This is solved via the Generalized Iterative Scal-
ing algorithm (Darroch and Ratcliff 1972). At
run-time, a space token is considered an sb, if
and only if p(sb|c) &gt; 0.5, where
</bodyText>
<equation confidence="0.9956825">
ùëù(SùíÉ, c)
ùëù(SùíÉ|c) =
</equation>
<subsectionHeader confidence="0.996288">
3.2 Feature Selection
</subsectionHeader>
<bodyText confidence="0.9970594">
The core context of our model, {w, x, y, z}, is a
window spanning two tokens to the left (posi-
tions w and x) and two tokens to the right (posi-
tions y and z) of a classification candidate space
token.
</bodyText>
<table confidence="0.9996008">
C token characteristic
yk Yamok (syllable reduplication) symbol ‡πÜ
sp space
‡πê‡πô Thai numeric digits
num Arabic numeric digits
ABC Sequence of all capital ASCII characters
cnn single character (derived from hex)
ckkmmnn single character (derived from UTF8 hex)
ascii any amount of non-Thai text
(Thai text) Thai word (derived from lemma)
</table>
<tableCaption confidence="0.999773">
Table 1. Categorical and derived feature names
</tableCaption>
<bodyText confidence="0.999941210526316">
The possible values of each of the window
positions {w, x, y, z} are shown in Table 1,
where the first match to the token at the desig-
nated position is assigned as the feature value for
that position. Foreign-text tokens plus any inter-
vening space are merged, so a single ‚Äúascii‚Äù fea-
ture may represent an arbitrary amount of non-
Thai script with interior space.
Figure 8 shows an example sentence that has
been tokenized. Token boundaries are indicated
by slashes. Although there are three space tokens
in the original input, we extract four contexts.
The shaded boxes in the source text‚Äîand the
shaded line in the figure‚Äîindicate the single sb
context that is synthesized by wrapping, to be
described in Section 3.4.
For each context, in addition to the {w, x, y, z}
features, we extract two more features indicated
by {l ,r} in Figure 8. They are the number of
</bodyText>
<equation confidence="0.7274805">
ùëù(SùíÉ, c) + ùëù(ùíèSùíÉ, c) .
11
</equation>
<bodyText confidence="0.999702636363637">
tokens between the previous space token (wrap-
ping as necessary) and the current one, and the
number of tokens between the current space to-
ken and the next space token (wrapping as ne-
cessary). These features do not distinguish
whether the bounding space token is sb or nsb.
This is because, processing left-to-right, it is
permissible to use a feature such as ‚Äúnumber of
tokens since last sb,‚Äù but not ‚Äúnumber of tokens
until next sb,‚Äù which would be available during
training but not at runtime.
</bodyText>
<table confidence="0.8556441">
‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÅ‡∏ö‡∏ö R1C1 ‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Å‡∏≤‡∏£
‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÅ‡∏ö‡∏ö A1
‚ÄúR1C1 reference style was converted to A1 reference style.‚Äù
__/‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞/‡∏Å‡∏≤‡∏£/‡∏≠‡πâ‡∏≤‡∏á/‡∏≠‡∏¥‡∏á/‡πÅ‡∏ö‡∏ö/ /R1C1/ /‡∏ñ‡∏π‡∏Å/‡πÅ‡∏õ‡∏•‡∏á/‡πÑ‡∏õ/
‡πÄ‡∏õ‡πá‡∏ô/‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞/‡∏Å‡∏≤‡∏£/‡∏≠‡πâ‡∏≤‡∏á/‡∏≠‡∏¥‡∏á/‡πÅ‡∏ö‡∏ö/ /A1/__
b c=w c=x c=y c=z c=l c=r
nsb ‡∏≠‡∏¥‡∏á ‡πÅ‡∏ö‡∏ö ABC sp 5 1
nsb sp ABC ‡∏ñ‡∏π‡∏Å ‡πÅ‡∏õ‡∏•‡∏á 1 9
nsb ‡∏≠‡∏¥‡∏á ‡πÅ‡∏ö‡∏ö ABC sp 9 1
sb sp ABC ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ ‡∏Å‡∏≤‡∏£ 1 5
</table>
<figureCaption confidence="0.98813">
Figure 8. A Thai sentence and the training contexts extracted. Hig-
hlighting shows the context for sb.
</figureCaption>
<bodyText confidence="0.996238">
In addition to the above core features, our
model emits certain extra features only if they
appear:
</bodyText>
<listItem confidence="0.9930219">
‚Ä¢ An individual feature for each English punc-
tuation mark, since these are sometimes used
in Thai. For example, there is one feature for
the sentence end period (i.e. full-stop);
‚Ä¢ The current nest depth for paired glyphs with
directional variation, such as brackets, braces,
and parentheses;
‚Ä¢ The current parity value for paired glyphs
without directional distinction such as
‚Äústraight‚Äù quotation marks.
</listItem>
<bodyText confidence="0.9984495">
The following example illustrates paired direc-
tional glyphs (in this case, parentheses):
</bodyText>
<figure confidence="0.895423">
.../‡∏¢‡∏π‡∏ô‡∏¥‡∏•‡∏¥‡πÄ‡∏ß‡∏≠‡∏£‡πå/ /(/‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®/__/‡πÑ‡∏ó‡∏¢/)/ /‡∏à‡πç‡∏≤‡∏Å‡∏±‡∏î/ /‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ú‡∏¢/‡∏ß‡πà‡∏≤/...
...Unilever (Thailand) Ltd. disclosed that...
b c=w c=x c=y c=z c=pn
nsb ( ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡πÑ‡∏ó‡∏¢ ) 1
</figure>
<figureCaption confidence="0.978673">
Figure 9. Text fragment illustrating paired directional glyphs and
the context for the highlighted space
</figureCaption>
<bodyText confidence="0.999970428571429">
In Figure 9, the space between ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®
‚Äúcountry‚Äù and ‡πÑ‡∏ó‡∏¢ ‚ÄúThai,‚Äù generates an nsb
context which includes the features shown,
where ‚Äúpn‚Äù is an extra feature which indicates
the parenthesis nesting level. This feature helps
the model learn that spaces which occur within
parentheses are likely to be nsb.
Parity features for the non-directional paired
glyphs, which do nest, are true binary features.
Since these features have only two possible val-
ues (inside or outside), they are only emitted
when their value is ‚Äúinside,‚Äù that is, when the
space under consideration occurs between such a
pair.
</bodyText>
<subsectionHeader confidence="0.999236">
3.3 Sentence Breaker Training Corpus
</subsectionHeader>
<bodyText confidence="0.99993825">
Thai corpora which are marked with sentence
breaks are required for training. We assembled a
corpus of 361,802 probable sentences. This cor-
pus includes purchased, publicly available, and
web-crawled content. In total it contains 911,075
spaces, a figure which includes one inter-
sentence space per sentence, generated as de-
scribed below.
</bodyText>
<subsectionHeader confidence="0.828162">
3.4 Out-of-context Sentences
</subsectionHeader>
<bodyText confidence="0.993555678571428">
For SB training, paragraphs are first tokenized
into words as described in Section 2.2. This
process does not introduce new spaces between
tokens; only original spaces in the text are classi-
fied as sb/nsb and used for the context features
described below. To keep this distinction clear,
token boundaries are indicated by a slash rather
than space in the examples shown in this paper.
For 91% of our training sentences, the para-
graphs from which they originate are inaccessi-
ble. In feature extraction for each of these sen-
tences, we wrap the sentence‚Äôs head around to its
tail to obtain its sb context. In other words, for a
sentence of tokens t0-tn-1, the context of sb (the
last space) is given by
{ w=tn-2, x=tn-1, y=t0, z=t1 }.
This process was illustrated in Figure 8. Al-
though not an ideal substitute for sentences in
context, this ensures that we extract at least one
sb context per sentence. The number of nsb con-
texts extracted per sentence is equal to the num-
ber of interior space tokens in the original sen-
tence. Sentence wrapping is not needed when
training with sentence-delimited paragraph
sources. Contexts sb and nsb are extracted from
the token stream of the entire paragraph and
wrapping is used only to generate one additional
sb for the entire paragraph.
</bodyText>
<page confidence="0.796015">
12
</page>
<subsectionHeader confidence="0.965664">
3.5 Sentence Breaker Evaluation
</subsectionHeader>
<bodyText confidence="0.9926814">
Although evaluation against a single-domain
corpus does not measure important design re-
quirements of our system, namely resilience to
broad-domain input texts, we evaluated against
the ORCHID corpus (Charoenporn et al. 1997)
for the purpose of comparison with the existing
literature. Following the methodology of the stu-
dies cited below, we use 10-fold √ó10% averaged
testing against the ORCHID corpus.
Our results are consistent with recent work us-
ing the Winnow algorithm, which itself com-
pares favorably with the probabilistic POS tri-
gram approach. Both of these studies use evalua-
tion metrics, attributed to Black and Taylor
(1997), which aim to more usefully measure sen-
tence-breaker utility. Accordingly, the following
definitions are used in Table 2:
#sb false positives
total # of space tokens
It was generally possible to reconstruct preci-
sion and recall figures from these published re-
sults1 and we present a comprehensive table of
results. Reconstructed values are marked with a
dagger and the optimal result in each category is
marked in boldface.
</bodyText>
<table confidence="0.992023636363636">
Mittrapiyanuruk Charoenpornsawat Our result
et al. et al.
method Tr igOram Winnow MaxEnt
#sb in reference 10528 1086‚Ä† 2133
#space tokens 33141 3801 7227
nsb-precision 90.27‚Ä† 91.48‚Ä† 93.18
nsb-recall 87.18‚Ä† 97.56‚Ä† 94.41
sb-precision 74.35‚Ä† 92.69‚Ä† 86.21
sb-recall 79.82 77.27 83.50
‚Äúspace-correct‚Äù 85.26 89.13 91.19
‚Äúfalse-break‚Äù 8.75 1.74 3.94
</table>
<tableCaption confidence="0.999913">
Table 2. Evaluation of Thai Sentence Breakers against
</tableCaption>
<note confidence="0.347577">
ORCHID
</note>
<bodyText confidence="0.994106454545455">
Finally, we would be remiss in not acknowl-
edging the general hazard of assigning sentence
breaks in a language such as Thai, where source
1 Full results for Charoenpornsawat et al. are reconstructed based
on remarks in their text, including that ‚Äúthe ratio of the number of
[nsb to sb] is about 5:2.‚Äù
text authors may intentionally include or omit
spaces in order to create syntactic or semantic
ambiguity. We defer to Mittrapiyanuruk and
Sornlertlamvanich (2000) and Aroonmanakun
(2007) for informed commentary on this topic.
</bodyText>
<sectionHeader confidence="0.693276" genericHeader="method">
4 SMT System and Integration
</sectionHeader>
<bodyText confidence="0.999946">
The primary application for which we developed
the Thai sentence breaker described in this work
is the Microsoft¬Æ BINGTM general-domain ma-
chine translation service. In this section, we pro-
vide a brief overview of this large-scale SMT
system, focusing on Thai-specific integration
issues.
</bodyText>
<subsectionHeader confidence="0.989583">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.9999106">
Like many multilingual SMT systems, our sys-
tem is based on hybrid generative/discriminative
models. Given a sequence of foreign words, f, its
best translation is the sequence of target words,
e, that maximizes
</bodyText>
<equation confidence="0.860257">
e* = argmaxe p(e|ùíá) = argmaxe p(ùíá |e)p(e)
= argmaxe { log p(ùíá |e) + log p(e)}
</equation>
<bodyText confidence="0.999865">
where the translation model p (ùíá |e) is computed
on dozens to hundreds of features. The target
language model (LM), p(e), is represented by a
smoothed n-grams (Chen 1996) and sometimes
more than one LM is adopted in practice. To
achieve the best performance, the log likelihoods
evaluated by these features/models are linearly
combined. After p (ùíá |e) and p (e) are trained, the
combination weights A‡Øú are tuned on a held-out
dataset to optimize an objective function, which
we set to be the BLEU score (Papineni et al.
2002):
</bodyText>
<equation confidence="0.990889666666667">
{A‡Øú*} = max{‡∞í‡Øú} BLEU({ùëí*}, {ùëü})
e* = argmaxe {‡∑ç A‡Øúlog p‡Øú(ùíá |e) + ‡∑çA‡Øùlog p‡Øù(e)}
‡Øú ‡≠®
</equation>
<bodyText confidence="0.999626">
where {r} is the set of gold translations for the
given input source sentences. To learn A‡Øú we use
the algorithm described by Och (2003), where
the decoder output at any point is approximated
using n-best lists, allowing an optimal line
search to be employed.
</bodyText>
<subsectionHeader confidence="0.987251">
4.2 Phrasal and Treelet Translation
</subsectionHeader>
<bodyText confidence="0.997352">
Since we have a high-quality real-time rule-
based English parser available, we base our Eng-
</bodyText>
<figure confidence="0.98423325">
space-correct =
total # of space tokens
(#correct sb+#correct nsb)
false break=
</figure>
<page confidence="0.636155">
13
</page>
<bodyText confidence="0.998762294117647">
lish-to-Thai translation (ENG-THA) on the
‚Äútreelet‚Äù concept suggested in Menezes and
Quirk (2008). This approach parses the source
language into a dependency tree which includes
part-of-speech labels.
Lacking a Thai parser, we use a purely statis-
tical phrasal translator after Pharaoh (Koehn
2004) for THA-ENG translation, where we
adopt the name and date translation described in
Sections 2.3 and 2.4.
We also experimented with phrasal ENG-
THA translation. Though we actually achieved a
slightly better BLEU score than treelet for this
translation direction, qualitative human evalua-
tion by native speaker informants was mixed.
We adopted the treelet ENG-THA in the final
system, for its better re-spacing (Section 2.5).
</bodyText>
<subsectionHeader confidence="0.990826">
4.3 Training, Development and Test Data
</subsectionHeader>
<bodyText confidence="0.999845956521739">
Naturally, our system relies on parallel text cor-
pora to learn the mapping between two languag-
es. The parallel corpus contains sentence pairs,
corresponding to translations of each other. For
Thai, quality corpora are generally not available
in sufficient quality for training a general-
domain SMT system. For the ENG-THA pair,
we resort to Internet crawls as a source of text.
We first identify paired documents, break each
document into sentences, and align sentences in
one document against those in its parallel docu-
ment. Bad alignments are discarded. Only sen-
tence pairs with high alignment confidence are
kept in our parallel corpus. Our sentence align-
ment algorithm is based on Moore (2002).
For our ENG-THA translation system, we as-
sembled three resources: a parallel training cor-
pus, a development bitext (also called the lamb-
da set) for training the feature combination
weights {Ail, and a test corpus for BLEU and
human evaluation. Both the lambda and the test
sets have single reference translations per sen-
tence.
</bodyText>
<table confidence="0.999587833333333">
Data Set #Sentences
(ENG||THA) training 725K
(ENG,THA) lambda 2K
(ENG,THA) test 5K
THA LM text 10.3M
ENG LM text 45.6M
</table>
<tableCaption confidence="0.999781">
Table 3. Corpus size of parallel and monolingual data
</tableCaption>
<bodyText confidence="0.999902791666667">
Although it is well known that language trans-
lation pairs are not symmetric, we use these
same resources to build our THA-ENG transla-
tion system due to the lack of additional corpora.
Our parallel MT corpus consists of approx-
imately 725,000 English-Thai sentence pairs
from various sources. Additionally we have 9.6
million Thai sentences, which are used to train a
Thai 4-gram LM for ENG-THA translation, to-
gether with the Thai sentences in the parallel
corpus. Trigrams and 4-grams that occur only
once are pruned, and n-gram backoff weights are
re-normalized after pruning, with the surviving
KN smoothed probabilities intact (Kneser and
Ney 1995). Similarly, a 4-gram ENG LM is
trained for THA-ENG translation, on a total of
45.6M English sentences.
For both the lambda and test sets, THA LM
incurs higher out-of-vocabulary (OOV) rates
(1.6%) than ENG LM (0.7%), due to its smaller
training set and thus smaller lexicon. Both trans-
lation directions define the maximum
phrase/treelet length to be 4 and the maximum
re-ordering jump to be 4 as well.
</bodyText>
<subsectionHeader confidence="0.987657">
4.4 BLEU Scores
</subsectionHeader>
<bodyText confidence="0.99987425">
To evaluate our end-to-end performance, we
compute case insensitive 4-gram BLEU scores.
Translation outputs are WB first according to the
Thai/English tokenizer, before BLEU scores are
computed. The BLEU scores on the test sets are
shown in Table 4. We are not aware of any pre-
viously published BLEU results for either direc-
tion of this language pair.
</bodyText>
<table confidence="0.899028666666667">
BLEU
THA-ENG 0.233
ENG-THA 0.194
</table>
<tableCaption confidence="0.996871">
Table 4. Four-gram case-insensitive BLEU scores.
</tableCaption>
<bodyText confidence="0.867305333333333">
Figures 10 and 11 illustrate sample outputs for
the each translation direction, with reference
translations.
</bodyText>
<table confidence="0.697908428571429">
INPUT: ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡∏Å‡∏•‡πâ‡∏ß‡∏¢‡πÑ‡∏°‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì ‡πë‡πó‡πï ‡∏ä‡∏ô‡∏¥‡∏î ‡∏ñ‡πâ‡∏≤
‡∏™‡∏π‡∏ç‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡πÑ‡∏õ‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡∏Å‡πá‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏™‡∏π‡∏ç‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡πÑ‡∏õ‡∏à‡∏≤‡∏Å‡πÇ‡∏•‡∏Å
OUTPUT: In Thailand a Orchid approximately 175 type if
extinct from Thailand. It means extinct from the world.
REF: In Thailand, there are about 175 species of Orchid. If
they disappear from Thailand, they will be gone from the
world.
</table>
<figureCaption confidence="0.97421">
Figure 10. THA-ENG Sample Translation Output
</figureCaption>
<figure confidence="0.8902973">
14
INPUT: In our nation the problems and barriers we face are
just problems and barriers of law not selection or develop-
ment.
OUTPUT: ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ä‡∏≤‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤
‡πÄ‡∏ú‡∏ä‡∏¥‡∏ç‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏Ç‡∏≠‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÑ‡∏°‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏£‡∏∑‡∏≠
‡∏û‡∏±‡∏í‡∏ô‡∏≤
REF: ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ ‡∏Å‡πá‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤
‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏ó‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£
‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå
</figure>
<figureCaption confidence="0.99973">
Figure 11. ENG-THA Sample Translation Output
</figureCaption>
<bodyText confidence="0.999257">
Although the translation quality is far from being
perfect, SMT is making good process on build-
ing useful applications.
</bodyText>
<sectionHeader confidence="0.976845" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999908">
Our maximum entropy model for Thai sentence-
breaking achieves results which are consistent
with contemporary work in this task, allowing us
to overcome this obstacle to Thai SMT integra-
tion. This general approach can be applied to
other South-East Asian languages in which space
does not deterministically delimit sentence
boundaries.
In Arabic writing, commas are often used to
separate sentences until the end of a paragraph
when a period is finally used. In this case, the
comma character is similar to the space token in
Thai where its usage is ambiguous. We can use
the same approach (perhaps with different lin-
guistic features) to identify which commas are
sentence-breaking and which are not.
Our overall system incorporates a range of in-
dependent solutions to problems in Thai text
processing, including character sequence norma-
lization, tokenization, name and date identifica-
tion, sentence-breaking, and Thai text re-
spacing. We successfully integrated each solu-
tion into an existing large-scale SMT frame-
work, obtaining sufficient quality to release the
Thai-English language pair in a high-volume,
general-domain, free public online service.
There remains much room for improvement.
We need to find or create true Thai-English di-
rectional corpora to train the lambdas and to test
our models. The size of our parallel corpus for
Thai should increase by at least an order of mag-
nitude, without loss of bitext quality. With a
larger corpus, we can consider longer phrase
length, higher-order n-grams, and longer re-
ordering distance.
</bodyText>
<sectionHeader confidence="0.996348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994774184782609">
W. Aroonmanakun. 2007. Thoughts on Word
and Sentence Segmentation in Thai. In Pro-
ceedings of the Seventh International Sympo-
sium on Natural Language Processing, Pat-
taya, Thailand, 85-90.
F. Avery Bishop, David C. Brown and David M.
Meltzer. 2003. Supporting Multilanguage
Text Layout and Complex Scripts with Win-
dows 2000. http://www.microsoft.com/typo-
graphy/developers/uniscribe/intro.htm
A. W. Black and P. Taylor. 1997. Assigning
Phrase Breaks from Part-of-Speech Se-
quences. Computer Speech and Language,
12:99-117.
Thatsanee Charoenporn, Virach Sornlertlamva-
nich, and Hitoshi Isahara. 1997. Building A
Thai Part-Of-Speech Tagged Corpus (ORC-
HID).
Paisarn Charoenpornsawat and Virach Sornler-
tlamvanich. 2001. Automatic sentence break
disambiguation for Thai. In International
Conference on Computer Processing of
Oriental Languages (ICCPOL), 231-235.
S. F. Chen and J. Goodman. 1996. An empirical
study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual
Meeting on Association for Computational
Linguistics, 310-318. Morristown, NJ: ACL.
J. N. Darroch and D. Ratcliff. 1972. Generalized
Iterative Scaling for Log-Linear Models. The
Annals of Mathematical Statistics, 43(5):
1470-1480.
Choochart Haruechaiyasak, Sarawoot Kon-
gyoung, and Matthew N. Dailey. 2008. A
Comparative Study on Thai Word Segmenta-
tion Approaches. In Proceedings of ECTI-
CON 2008. Pathumthani, Thailand: ECTI.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved Backing-Off for M-Gram Language
Modeling. In Proceedings of International
Conference on Acoustics, Speech and Signal
Procesing (ICASSP), 1:181-184.
Philipp Koehn. 2004. Pharaoh: a Beam Search
Decoder for Phrase-Based Statistical Machine
Translation Models. In Proceedings of the As-
15
sociation of Machine Translation in the Amer-
icas (AMTA-2004).
Arul Menezes, and Chris Quirk. 2008. Syntactic
Models for Structural Word Insertion and De-
letion during Translation. In Proceedings of
the 2008 Conference on Empirical Methods in
Natural Language Processing.
P. Mittrapiyanuruk and V. Sornlertlamvanich.
2000. The Automatic Thai Sentence Extrac-
tion. In Proceedings of the Fourth Symposium
on Natural Language Processing, 23-28.
Robert C. Moore. 2002. Fast and Accurate Sen-
tence Alignment of Bilingual Corpora. In Ma-
chine Translation: From Research to Real
Users (Proceedings, 5th Conference of the As-
sociation for Machine Translation in the
Americas, Tiburon, California), Springer-
Verlag, Heidelberg, Germany, 135-244
Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In
Proceedings of the 41th Annual Meeting of the
Association for Computational Linguistics.
Stroudsburg, PA: ACL.
David D. Palmer and Marti A. Hearst. 1997.
Adaptive Multilingual Sentence Boundary
Disambiguation. Computational Linguistics,
23:241-267.
Kishore Papineni, Salim Roukos, Todd Ward,
and Wei-jing Zhu. 2002. BLEU: a method for
automatic evaluation of machine translation.
In Proceedings of the 40th Annual meeting of
the Association for Computational Linguistics,
311‚Äì318. Stroudsburg, PA: ACL.
Adwait Ratnaparkhi, 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Pro-
ceedings of the Conference on Empirical Me-
thods in Natural Language Processing, 133-
142.
Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997. A Maximum Entropy Approach to Iden-
tifying Sentence Boundaries, In Proceedings
of the Fifth Conference on Applied Natural
Language Processing, 16-19.
Suphawut Wathabunditkul. 2003. Spacing in the
Thai Language. http://www.thailanguage.com/
ref/spacing
</reference>
<page confidence="0.884801">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.899918">
<title confidence="0.998076">Thai Sentence-Breaking for Large-Scale SMT</title>
<author confidence="0.999922">Glenn Slayden Mei-Yuh Hwang Lee Schwartz</author>
<affiliation confidence="0.909641">thai-language.com Microsoft Research Microsoft</affiliation>
<email confidence="0.994402">glenn@thai-language.commehwang@microsoft.comleesc@microsoft.com</email>
<abstract confidence="0.999768">Thai language text presents challenges for integration into large-scale multilanguage statistical machine translation (SMT) systems, largely stemming from the nominal lack of punctuation and inter-word space. For Thai sentence breaking, we describe a monolingual maximum entropy classifier with features that may be applicable to other languages such as Arabic, Khmer and Lao. We apply this sentence breaker to our largevocabulary, general-purpose, bidirectional Thai-English SMT system, and achieve BLEU scores of around 0.20, reaching our threshold of releasing it as a free online service.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Aroonmanakun</author>
</authors>
<title>Thoughts on Word and Sentence Segmentation in Thai.</title>
<date>2007</date>
<booktitle>In Proceedings of the Seventh International Symposium on Natural Language Processing,</booktitle>
<pages>85--90</pages>
<location>Pattaya, Thailand,</location>
<contexts>
<context position="2070" citStr="Aroonmanakun (2007)" startWordPosition="296" endWordPosition="297"> between sentences. There is generally no space between words, but a space character may appear within a sentence according to linguistic or prescriptive orthographic motivation (Wathabunditkul 2003), and these characteristics disqualify sentencebreaking (SB) methods used for other languages, such as Palmer and Hearst (1997). Thai SB has therefore been regarded as the task of classifying each space that appears in a Thai source text as either sentence-breaking (sb) or non-sentencebreaking (nsb). Several researchers have investigated Thai SB. Along with a discussion of Thai word breaking (WB), Aroonmanakun (2007) examines the issue. With a human study, he establishes that sentence breaks elicited from Thai informants exhibit varying degrees of consensus. Mittrapiyanuruk and Sornlertlamvanich (2000) define part-of-speech (POS) tags for sb and nsb and train a trigram model over a POS-annotated corpus. At runtime, they use the Viterbi algorithm to select the POS sequence with the highest probability, from which the corresponding space type is read back. Charoenpornsawat and Sornlertlamvanich (2001) apply Winnow, a multiplicative trigger threshold classifier, to the problem. Their model has ten features: </context>
<context position="22036" citStr="Aroonmanakun (2007)" startWordPosition="3608" endWordPosition="3609">7 83.50 ‚Äúspace-correct‚Äù 85.26 89.13 91.19 ‚Äúfalse-break‚Äù 8.75 1.74 3.94 Table 2. Evaluation of Thai Sentence Breakers against ORCHID Finally, we would be remiss in not acknowledging the general hazard of assigning sentence breaks in a language such as Thai, where source 1 Full results for Charoenpornsawat et al. are reconstructed based on remarks in their text, including that ‚Äúthe ratio of the number of [nsb to sb] is about 5:2.‚Äù text authors may intentionally include or omit spaces in order to create syntactic or semantic ambiguity. We defer to Mittrapiyanuruk and Sornlertlamvanich (2000) and Aroonmanakun (2007) for informed commentary on this topic. 4 SMT System and Integration The primary application for which we developed the Thai sentence breaker described in this work is the Microsoft¬Æ BINGTM general-domain machine translation service. In this section, we provide a brief overview of this large-scale SMT system, focusing on Thai-specific integration issues. 4.1 Overview Like many multilingual SMT systems, our system is based on hybrid generative/discriminative models. Given a sequence of foreign words, f, its best translation is the sequence of target words, e, that maximizes e* = argmaxe p(e|ùíá) </context>
</contexts>
<marker>Aroonmanakun, 2007</marker>
<rawString>W. Aroonmanakun. 2007. Thoughts on Word and Sentence Segmentation in Thai. In Proceedings of the Seventh International Symposium on Natural Language Processing, Pattaya, Thailand, 85-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Avery Bishop</author>
<author>David C Brown</author>
<author>David M Meltzer</author>
</authors>
<date>2003</date>
<booktitle>Supporting Multilanguage Text Layout and Complex Scripts with Windows</booktitle>
<note>http://www.microsoft.com/typography/developers/uniscribe/intro.htm</note>
<contexts>
<context position="7503" citStr="Bishop et al. 2003" startWordPosition="1189" endWordPosition="1192">oWidth Space (ZWSP) as one solution for indicating word breaks in Thai, it is infrequently used. Programmatic tokenization has become a staple of Thai computational linguistics. The problem has been well studied, with precision and recall near 95% (Haruechaiyasak et al. 2008). In our SMT application, both the sentence breaker and the SMT system itself require Thai WB, and we use the same word breaker for these tasks (although the system design currently prohibits directly passing tokens between these two components). Our method is to apply postprocessing heuristics to the output of Uniscribe (Bishop et al. 2003), which is provided as part of the Microsoft¬Æ WindowsTM operating system interface. Our heuristics fall into two categories: ‚Äúre-gluing‚Äù words that Uniscribe broke too aggressively, and a smaller class of cases of further breaking of words that Uniscribe did not break. Re-gluing is achieved by comparing Uniscribe output against a Thai lexicon in which desired breaks within a word are tagged. Underbreaking by Uniscribe is less common and is restricted to a number of common patterns which are repaired explicitly. ‡πâ ‡πâ ‡∏¥ ‚óå ‚óå ‡∏ô ‡∏ô ‚Üí ‡∏≠ ‚óå‡∏¥ ‡∏ô ‚Üí ‡∏≠‡∏¥ ‡∏≠ ‚óå ‡πâ 9 2.3 Person Name Entities In written Thai, certa</context>
</contexts>
<marker>Bishop, Brown, Meltzer, 2003</marker>
<rawString>F. Avery Bishop, David C. Brown and David M. Meltzer. 2003. Supporting Multilanguage Text Layout and Complex Scripts with Windows 2000. http://www.microsoft.com/typography/developers/uniscribe/intro.htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>A W Black</author>
<author>P Taylor</author>
</authors>
<title>Assigning Phrase Breaks from Part-of-Speech Sequences. Computer Speech and Language,</title>
<date>1997</date>
<pages>12--99</pages>
<contexts>
<context position="20721" citStr="Black and Taylor (1997)" startWordPosition="3402" endWordPosition="3405">luation against a single-domain corpus does not measure important design requirements of our system, namely resilience to broad-domain input texts, we evaluated against the ORCHID corpus (Charoenporn et al. 1997) for the purpose of comparison with the existing literature. Following the methodology of the studies cited below, we use 10-fold √ó10% averaged testing against the ORCHID corpus. Our results are consistent with recent work using the Winnow algorithm, which itself compares favorably with the probabilistic POS trigram approach. Both of these studies use evaluation metrics, attributed to Black and Taylor (1997), which aim to more usefully measure sentence-breaker utility. Accordingly, the following definitions are used in Table 2: #sb false positives total # of space tokens It was generally possible to reconstruct precision and recall figures from these published results1 and we present a comprehensive table of results. Reconstructed values are marked with a dagger and the optimal result in each category is marked in boldface. Mittrapiyanuruk Charoenpornsawat Our result et al. et al. method Tr igOram Winnow MaxEnt #sb in reference 10528 1086‚Ä† 2133 #space tokens 33141 3801 7227 nsb-precision 90.27‚Ä† 9</context>
</contexts>
<marker>Black, Taylor, 1997</marker>
<rawString>A. W. Black and P. Taylor. 1997. Assigning Phrase Breaks from Part-of-Speech Sequences. Computer Speech and Language, 12:99-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thatsanee Charoenporn</author>
<author>Virach Sornlertlamvanich</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Building A Thai Part-Of-Speech Tagged Corpus (ORCHID).</title>
<date>1997</date>
<contexts>
<context position="20310" citStr="Charoenporn et al. 1997" startWordPosition="3336" endWordPosition="3339"> of nsb contexts extracted per sentence is equal to the number of interior space tokens in the original sentence. Sentence wrapping is not needed when training with sentence-delimited paragraph sources. Contexts sb and nsb are extracted from the token stream of the entire paragraph and wrapping is used only to generate one additional sb for the entire paragraph. 12 3.5 Sentence Breaker Evaluation Although evaluation against a single-domain corpus does not measure important design requirements of our system, namely resilience to broad-domain input texts, we evaluated against the ORCHID corpus (Charoenporn et al. 1997) for the purpose of comparison with the existing literature. Following the methodology of the studies cited below, we use 10-fold √ó10% averaged testing against the ORCHID corpus. Our results are consistent with recent work using the Winnow algorithm, which itself compares favorably with the probabilistic POS trigram approach. Both of these studies use evaluation metrics, attributed to Black and Taylor (1997), which aim to more usefully measure sentence-breaker utility. Accordingly, the following definitions are used in Table 2: #sb false positives total # of space tokens It was generally possi</context>
</contexts>
<marker>Charoenporn, Sornlertlamvanich, Isahara, 1997</marker>
<rawString>Thatsanee Charoenporn, Virach Sornlertlamvanich, and Hitoshi Isahara. 1997. Building A Thai Part-Of-Speech Tagged Corpus (ORCHID).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paisarn Charoenpornsawat</author>
<author>Virach Sornlertlamvanich</author>
</authors>
<title>Automatic sentence break disambiguation for Thai.</title>
<date>2001</date>
<booktitle>In International Conference on Computer Processing of Oriental Languages (ICCPOL),</booktitle>
<pages>231--235</pages>
<contexts>
<context position="2562" citStr="Charoenpornsawat and Sornlertlamvanich (2001)" startWordPosition="367" endWordPosition="371">sb) or non-sentencebreaking (nsb). Several researchers have investigated Thai SB. Along with a discussion of Thai word breaking (WB), Aroonmanakun (2007) examines the issue. With a human study, he establishes that sentence breaks elicited from Thai informants exhibit varying degrees of consensus. Mittrapiyanuruk and Sornlertlamvanich (2000) define part-of-speech (POS) tags for sb and nsb and train a trigram model over a POS-annotated corpus. At runtime, they use the Viterbi algorithm to select the POS sequence with the highest probability, from which the corresponding space type is read back. Charoenpornsawat and Sornlertlamvanich (2001) apply Winnow, a multiplicative trigger threshold classifier, to the problem. Their model has ten features: the number of words to the left and right, and the left-two and right-two POS tags and words. We present a monolingual Thai SB based on a maximum entropy (ME) classifier (Ratnaparkhi 1996; Reynar and Ratnaparkhi, 1997) which is suitable for sentence-breaking SMT training data and runtime inputs. Our model uses a four token window of Thai lemmas, plus categorical features, to describe the proximal environment of the space token under consideration, allowing runtime classification of space</context>
</contexts>
<marker>Charoenpornsawat, Sornlertlamvanich, 2001</marker>
<rawString>Paisarn Charoenpornsawat and Virach Sornlertlamvanich. 2001. Automatic sentence break disambiguation for Thai. In International Conference on Computer Processing of Oriental Languages (ICCPOL), 231-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ:</location>
<marker>Chen, Goodman, 1996</marker>
<rawString>S. F. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, 310-318. Morristown, NJ: ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized Iterative Scaling for Log-Linear Models.</title>
<date>1972</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>43</volume>
<issue>5</issue>
<pages>1470--1480</pages>
<contexts>
<context position="14402" citStr="Darroch and Ratcliff 1972" startWordPosition="2355" endWordPosition="2358">nables us to learn that spaces which follow an honorific are less likely to mark sentence boundaries. Assume the joint probability p(b,c) is modeled by  ‡≥ï(,) ùëù(b, c) = Z ‡∑ëùú∂ ‡≠Ä‡¨µ where we have k free parameters {ùú∂} to estimate and Z is a normalization factor to make ‚àë, ùëù(b, c) = 1. The ME learning algorithm finds a solution {ùú∂} representing the most uncertain commitment max H(ùëù) = ‚àí ùëù(b, c) logùëù(b, c) that satisfies the observed distribution ùëùÃÇ(b, c) of the training data ‚àë ùëù(b, c)ùëì(b, c) = ‚àë ùëùÃÇ(b, c)ùëì(b, c), 1 &lt; ùëó &lt; ùëò . This is solved via the Generalized Iterative Scaling algorithm (Darroch and Ratcliff 1972). At run-time, a space token is considered an sb, if and only if p(sb|c) &gt; 0.5, where ùëù(SùíÉ, c) ùëù(SùíÉ|c) = 3.2 Feature Selection The core context of our model, {w, x, y, z}, is a window spanning two tokens to the left (positions w and x) and two tokens to the right (positions y and z) of a classification candidate space token. C token characteristic yk Yamok (syllable reduplication) symbol ‡πÜ sp space ‡πê‡πô Thai numeric digits num Arabic numeric digits ABC Sequence of all capital ASCII characters cnn single character (derived from hex) ckkmmnn single character (derived from UTF8 hex) ascii any amoun</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. 1972. Generalized Iterative Scaling for Log-Linear Models. The Annals of Mathematical Statistics, 43(5): 1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Choochart Haruechaiyasak</author>
<author>Sarawoot Kongyoung</author>
<author>Matthew N Dailey</author>
</authors>
<title>A Comparative Study on Thai Word Segmentation Approaches.</title>
<date>2008</date>
<booktitle>In Proceedings of ECTICON</booktitle>
<publisher>ECTI.</publisher>
<location>Pathumthani, Thailand:</location>
<contexts>
<context position="7160" citStr="Haruechaiyasak et al. 2008" startWordPosition="1132" endWordPosition="1135">els, have few re-alignment checkpoints, allowing tokenization state machines to linger in misaligned states. ‡∏≠ ‚óå‡πç ‡∏≤ ‚Üí ‡∏≠ ‚óå‡πç‡∏≤ ‚Üí ‡∏≠‡πç‡∏≤ ‡πÄ ‡πÄ ‡∏≠ ‚Üí ‡πÅ ‡∏≠ ‚Üí ‡πÅ‡∏≠ Figure 3. Two common mis-codings 2.2 Uniscribe Thai Tokenization Thai text does not normally use the space character to separate words, except in certain specific contexts. Although Unicode offers the ZeroWidth Space (ZWSP) as one solution for indicating word breaks in Thai, it is infrequently used. Programmatic tokenization has become a staple of Thai computational linguistics. The problem has been well studied, with precision and recall near 95% (Haruechaiyasak et al. 2008). In our SMT application, both the sentence breaker and the SMT system itself require Thai WB, and we use the same word breaker for these tasks (although the system design currently prohibits directly passing tokens between these two components). Our method is to apply postprocessing heuristics to the output of Uniscribe (Bishop et al. 2003), which is provided as part of the Microsoft¬Æ WindowsTM operating system interface. Our heuristics fall into two categories: ‚Äúre-gluing‚Äù words that Uniscribe broke too aggressively, and a smaller class of cases of further breaking of words that Uniscribe di</context>
</contexts>
<marker>Haruechaiyasak, Kongyoung, Dailey, 2008</marker>
<rawString>Choochart Haruechaiyasak, Sarawoot Kongyoung, and Matthew N. Dailey. 2008. A Comparative Study on Thai Word Segmentation Approaches. In Proceedings of ECTICON 2008. Pathumthani, Thailand: ECTI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Backing-Off for M-Gram Language Modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of International Conference on Acoustics, Speech and Signal Procesing (ICASSP),</booktitle>
<pages>1--181</pages>
<contexts>
<context position="26382" citStr="Kneser and Ney 1995" startWordPosition="4310" endWordPosition="4313"> language translation pairs are not symmetric, we use these same resources to build our THA-ENG translation system due to the lack of additional corpora. Our parallel MT corpus consists of approximately 725,000 English-Thai sentence pairs from various sources. Additionally we have 9.6 million Thai sentences, which are used to train a Thai 4-gram LM for ENG-THA translation, together with the Thai sentences in the parallel corpus. Trigrams and 4-grams that occur only once are pruned, and n-gram backoff weights are re-normalized after pruning, with the surviving KN smoothed probabilities intact (Kneser and Ney 1995). Similarly, a 4-gram ENG LM is trained for THA-ENG translation, on a total of 45.6M English sentences. For both the lambda and test sets, THA LM incurs higher out-of-vocabulary (OOV) rates (1.6%) than ENG LM (0.7%), due to its smaller training set and thus smaller lexicon. Both translation directions define the maximum phrase/treelet length to be 4 and the maximum re-ordering jump to be 4 as well. 4.4 BLEU Scores To evaluate our end-to-end performance, we compute case insensitive 4-gram BLEU scores. Translation outputs are WB first according to the Thai/English tokenizer, before BLEU scores a</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved Backing-Off for M-Gram Language Modeling. In Proceedings of International Conference on Acoustics, Speech and Signal Procesing (ICASSP), 1:181-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models.</title>
<date>2004</date>
<booktitle>In Proceedings of the Association of Machine Translation in the Americas (AMTA-2004).</booktitle>
<contexts>
<context position="24070" citStr="Koehn 2004" startWordPosition="3942" endWordPosition="3943"> output at any point is approximated using n-best lists, allowing an optimal line search to be employed. 4.2 Phrasal and Treelet Translation Since we have a high-quality real-time rulebased English parser available, we base our Engspace-correct = total # of space tokens (#correct sb+#correct nsb) false break= 13 lish-to-Thai translation (ENG-THA) on the ‚Äútreelet‚Äù concept suggested in Menezes and Quirk (2008). This approach parses the source language into a dependency tree which includes part-of-speech labels. Lacking a Thai parser, we use a purely statistical phrasal translator after Pharaoh (Koehn 2004) for THA-ENG translation, where we adopt the name and date translation described in Sections 2.3 and 2.4. We also experimented with phrasal ENGTHA translation. Though we actually achieved a slightly better BLEU score than treelet for this translation direction, qualitative human evaluation by native speaker informants was mixed. We adopted the treelet ENG-THA in the final system, for its better re-spacing (Section 2.5). 4.3 Training, Development and Test Data Naturally, our system relies on parallel text corpora to learn the mapping between two languages. The parallel corpus contains sentence </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models. In Proceedings of the Association of Machine Translation in the Americas (AMTA-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arul Menezes</author>
<author>Chris Quirk</author>
</authors>
<title>Syntactic Models for Structural Word Insertion and Deletion during Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="23870" citStr="Menezes and Quirk (2008)" startWordPosition="3910" endWordPosition="3913">, {ùëü}) e* = argmaxe { Alog p(ùíá |e) + Alog p(e)}  ‡≠® where {r} is the set of gold translations for the given input source sentences. To learn A we use the algorithm described by Och (2003), where the decoder output at any point is approximated using n-best lists, allowing an optimal line search to be employed. 4.2 Phrasal and Treelet Translation Since we have a high-quality real-time rulebased English parser available, we base our Engspace-correct = total # of space tokens (#correct sb+#correct nsb) false break= 13 lish-to-Thai translation (ENG-THA) on the ‚Äútreelet‚Äù concept suggested in Menezes and Quirk (2008). This approach parses the source language into a dependency tree which includes part-of-speech labels. Lacking a Thai parser, we use a purely statistical phrasal translator after Pharaoh (Koehn 2004) for THA-ENG translation, where we adopt the name and date translation described in Sections 2.3 and 2.4. We also experimented with phrasal ENGTHA translation. Though we actually achieved a slightly better BLEU score than treelet for this translation direction, qualitative human evaluation by native speaker informants was mixed. We adopted the treelet ENG-THA in the final system, for its better re</context>
</contexts>
<marker>Menezes, Quirk, 2008</marker>
<rawString>Arul Menezes, and Chris Quirk. 2008. Syntactic Models for Structural Word Insertion and Deletion during Translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mittrapiyanuruk</author>
<author>V Sornlertlamvanich</author>
</authors>
<title>The Automatic Thai Sentence Extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fourth Symposium on Natural Language Processing,</booktitle>
<pages>23--28</pages>
<contexts>
<context position="2259" citStr="Mittrapiyanuruk and Sornlertlamvanich (2000)" startWordPosition="319" endWordPosition="323">ic motivation (Wathabunditkul 2003), and these characteristics disqualify sentencebreaking (SB) methods used for other languages, such as Palmer and Hearst (1997). Thai SB has therefore been regarded as the task of classifying each space that appears in a Thai source text as either sentence-breaking (sb) or non-sentencebreaking (nsb). Several researchers have investigated Thai SB. Along with a discussion of Thai word breaking (WB), Aroonmanakun (2007) examines the issue. With a human study, he establishes that sentence breaks elicited from Thai informants exhibit varying degrees of consensus. Mittrapiyanuruk and Sornlertlamvanich (2000) define part-of-speech (POS) tags for sb and nsb and train a trigram model over a POS-annotated corpus. At runtime, they use the Viterbi algorithm to select the POS sequence with the highest probability, from which the corresponding space type is read back. Charoenpornsawat and Sornlertlamvanich (2001) apply Winnow, a multiplicative trigger threshold classifier, to the problem. Their model has ten features: the number of words to the left and right, and the left-two and right-two POS tags and words. We present a monolingual Thai SB based on a maximum entropy (ME) classifier (Ratnaparkhi 1996; </context>
<context position="22012" citStr="Mittrapiyanuruk and Sornlertlamvanich (2000)" startWordPosition="3603" endWordPosition="3606">recision 74.35‚Ä† 92.69‚Ä† 86.21 sb-recall 79.82 77.27 83.50 ‚Äúspace-correct‚Äù 85.26 89.13 91.19 ‚Äúfalse-break‚Äù 8.75 1.74 3.94 Table 2. Evaluation of Thai Sentence Breakers against ORCHID Finally, we would be remiss in not acknowledging the general hazard of assigning sentence breaks in a language such as Thai, where source 1 Full results for Charoenpornsawat et al. are reconstructed based on remarks in their text, including that ‚Äúthe ratio of the number of [nsb to sb] is about 5:2.‚Äù text authors may intentionally include or omit spaces in order to create syntactic or semantic ambiguity. We defer to Mittrapiyanuruk and Sornlertlamvanich (2000) and Aroonmanakun (2007) for informed commentary on this topic. 4 SMT System and Integration The primary application for which we developed the Thai sentence breaker described in this work is the Microsoft¬Æ BINGTM general-domain machine translation service. In this section, we provide a brief overview of this large-scale SMT system, focusing on Thai-specific integration issues. 4.1 Overview Like many multilingual SMT systems, our system is based on hybrid generative/discriminative models. Given a sequence of foreign words, f, its best translation is the sequence of target words, e, that maximi</context>
</contexts>
<marker>Mittrapiyanuruk, Sornlertlamvanich, 2000</marker>
<rawString>P. Mittrapiyanuruk and V. Sornlertlamvanich. 2000. The Automatic Thai Sentence Extraction. In Proceedings of the Fourth Symposium on Natural Language Processing, 23-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Fast and Accurate Sentence Alignment of Bilingual Corpora.</title>
<date>2002</date>
<booktitle>In Machine Translation: From Research to Real Users (Proceedings, 5th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>135--244</pages>
<location>Tiburon, California), SpringerVerlag, Heidelberg, Germany,</location>
<contexts>
<context position="25230" citStr="Moore (2002)" startWordPosition="4125" endWordPosition="4126">wo languages. The parallel corpus contains sentence pairs, corresponding to translations of each other. For Thai, quality corpora are generally not available in sufficient quality for training a generaldomain SMT system. For the ENG-THA pair, we resort to Internet crawls as a source of text. We first identify paired documents, break each document into sentences, and align sentences in one document against those in its parallel document. Bad alignments are discarded. Only sentence pairs with high alignment confidence are kept in our parallel corpus. Our sentence alignment algorithm is based on Moore (2002). For our ENG-THA translation system, we assembled three resources: a parallel training corpus, a development bitext (also called the lambda set) for training the feature combination weights {Ail, and a test corpus for BLEU and human evaluation. Both the lambda and the test sets have single reference translations per sentence. Data Set #Sentences (ENG||THA) training 725K (ENG,THA) lambda 2K (ENG,THA) test 5K THA LM text 10.3M ENG LM text 45.6M Table 3. Corpus size of parallel and monolingual data Although it is well known that language translation pairs are not symmetric, we use these same res</context>
</contexts>
<marker>Moore, 2002</marker>
<rawString>Robert C. Moore. 2002. Fast and Accurate Sentence Alignment of Bilingual Corpora. In Machine Translation: From Research to Real Users (Proceedings, 5th Conference of the Association for Machine Translation in the Americas, Tiburon, California), SpringerVerlag, Heidelberg, Germany, 135-244</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA:</location>
<contexts>
<context position="23440" citStr="Och (2003)" startWordPosition="3846" endWordPosition="3847"> by a smoothed n-grams (Chen 1996) and sometimes more than one LM is adopted in practice. To achieve the best performance, the log likelihoods evaluated by these features/models are linearly combined. After p (ùíá |e) and p (e) are trained, the combination weights A are tuned on a held-out dataset to optimize an objective function, which we set to be the BLEU score (Papineni et al. 2002): {A*} = max{‡∞í} BLEU({ùëí*}, {ùëü}) e* = argmaxe { Alog p(ùíá |e) + Alog p(e)}  ‡≠® where {r} is the set of gold translations for the given input source sentences. To learn A we use the algorithm described by Och (2003), where the decoder output at any point is approximated using n-best lists, allowing an optimal line search to be employed. 4.2 Phrasal and Treelet Translation Since we have a high-quality real-time rulebased English parser available, we base our Engspace-correct = total # of space tokens (#correct sb+#correct nsb) false break= 13 lish-to-Thai translation (ENG-THA) on the ‚Äútreelet‚Äù concept suggested in Menezes and Quirk (2008). This approach parses the source language into a dependency tree which includes part-of-speech labels. Lacking a Thai parser, we use a purely statistical phrasal transla</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41th Annual Meeting of the Association for Computational Linguistics. Stroudsburg, PA: ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
<author>Marti A Hearst</author>
</authors>
<date>1997</date>
<booktitle>Adaptive Multilingual Sentence Boundary Disambiguation. Computational Linguistics,</booktitle>
<pages>23--241</pages>
<contexts>
<context position="1777" citStr="Palmer and Hearst (1997)" startWordPosition="248" endWordPosition="251">h this assumption is challenged receive increased attention. Thai is one such language, since it uses space neither to distinguish syllables from words or affixes, nor to unambiguously signal sentence boundaries. Written Thai has no sentence-end punctuation, but a space character is always present between sentences. There is generally no space between words, but a space character may appear within a sentence according to linguistic or prescriptive orthographic motivation (Wathabunditkul 2003), and these characteristics disqualify sentencebreaking (SB) methods used for other languages, such as Palmer and Hearst (1997). Thai SB has therefore been regarded as the task of classifying each space that appears in a Thai source text as either sentence-breaking (sb) or non-sentencebreaking (nsb). Several researchers have investigated Thai SB. Along with a discussion of Thai word breaking (WB), Aroonmanakun (2007) examines the issue. With a human study, he establishes that sentence breaks elicited from Thai informants exhibit varying degrees of consensus. Mittrapiyanuruk and Sornlertlamvanich (2000) define part-of-speech (POS) tags for sb and nsb and train a trigram model over a POS-annotated corpus. At runtime, th</context>
</contexts>
<marker>Palmer, Hearst, 1997</marker>
<rawString>David D. Palmer and Marti A. Hearst. 1997. Adaptive Multilingual Sentence Boundary Disambiguation. Computational Linguistics, 23:241-267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA:</location>
<contexts>
<context position="23219" citStr="Papineni et al. 2002" startWordPosition="3802" endWordPosition="3805">e, that maximizes e* = argmaxe p(e|ùíá) = argmaxe p(ùíá |e)p(e) = argmaxe { log p(ùíá |e) + log p(e)} where the translation model p (ùíá |e) is computed on dozens to hundreds of features. The target language model (LM), p(e), is represented by a smoothed n-grams (Chen 1996) and sometimes more than one LM is adopted in practice. To achieve the best performance, the log likelihoods evaluated by these features/models are linearly combined. After p (ùíá |e) and p (e) are trained, the combination weights A are tuned on a held-out dataset to optimize an objective function, which we set to be the BLEU score (Papineni et al. 2002): {A*} = max{‡∞í} BLEU({ùëí*}, {ùëü}) e* = argmaxe { Alog p(ùíá |e) + Alog p(e)}  ‡≠® where {r} is the set of gold translations for the given input source sentences. To learn A we use the algorithm described by Och (2003), where the decoder output at any point is approximated using n-best lists, allowing an optimal line search to be employed. 4.2 Phrasal and Treelet Translation Since we have a high-quality real-time rulebased English parser available, we base our Engspace-correct = total # of space tokens (#correct sb+#correct nsb) false break= 13 lish-to-Thai translation (ENG-THA) on the ‚Äútre</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual meeting of the Association for Computational Linguistics, 311‚Äì318. Stroudsburg, PA: ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="2857" citStr="Ratnaparkhi 1996" startWordPosition="419" endWordPosition="420">rtlamvanich (2000) define part-of-speech (POS) tags for sb and nsb and train a trigram model over a POS-annotated corpus. At runtime, they use the Viterbi algorithm to select the POS sequence with the highest probability, from which the corresponding space type is read back. Charoenpornsawat and Sornlertlamvanich (2001) apply Winnow, a multiplicative trigger threshold classifier, to the problem. Their model has ten features: the number of words to the left and right, and the left-two and right-two POS tags and words. We present a monolingual Thai SB based on a maximum entropy (ME) classifier (Ratnaparkhi 1996; Reynar and Ratnaparkhi, 1997) which is suitable for sentence-breaking SMT training data and runtime inputs. Our model uses a four token window of Thai lemmas, plus categorical features, to describe the proximal environment of the space token under consideration, allowing runtime classification of space tokens with possibly unseen contexts. As our SB model relies on Thai WB, we review our approach to this problem, plus related preprocessing, in the next section. Section 2 also discusses the complementary operation to WB, namely, the re-spacing of Thai text generated by SMT output. Section 3 d</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi, 1996. A Maximum Entropy Model for Part-of-Speech Tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Approach to Identifying Sentence Boundaries,</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>16--19</pages>
<contexts>
<context position="2888" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="421" endWordPosition="424"> define part-of-speech (POS) tags for sb and nsb and train a trigram model over a POS-annotated corpus. At runtime, they use the Viterbi algorithm to select the POS sequence with the highest probability, from which the corresponding space type is read back. Charoenpornsawat and Sornlertlamvanich (2001) apply Winnow, a multiplicative trigger threshold classifier, to the problem. Their model has ten features: the number of words to the left and right, and the left-two and right-two POS tags and words. We present a monolingual Thai SB based on a maximum entropy (ME) classifier (Ratnaparkhi 1996; Reynar and Ratnaparkhi, 1997) which is suitable for sentence-breaking SMT training data and runtime inputs. Our model uses a four token window of Thai lemmas, plus categorical features, to describe the proximal environment of the space token under consideration, allowing runtime classification of space tokens with possibly unseen contexts. As our SB model relies on Thai WB, we review our approach to this problem, plus related preprocessing, in the next section. Section 2 also discusses the complementary operation to WB, namely, the re-spacing of Thai text generated by SMT output. Section 3 details our SB model and evaluat</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A Maximum Entropy Approach to Identifying Sentence Boundaries, In Proceedings of the Fifth Conference on Applied Natural Language Processing, 16-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suphawut Wathabunditkul</author>
</authors>
<title>Spacing in the Thai Language.</title>
<date>2003</date>
<note>http://www.thailanguage.com/ ref/spacing</note>
<contexts>
<context position="1650" citStr="Wathabunditkul 2003" startWordPosition="232" endWordPosition="233">orporate an assumption of deterministic sentence delineation. As such systems become more sophisticated, languages for which this assumption is challenged receive increased attention. Thai is one such language, since it uses space neither to distinguish syllables from words or affixes, nor to unambiguously signal sentence boundaries. Written Thai has no sentence-end punctuation, but a space character is always present between sentences. There is generally no space between words, but a space character may appear within a sentence according to linguistic or prescriptive orthographic motivation (Wathabunditkul 2003), and these characteristics disqualify sentencebreaking (SB) methods used for other languages, such as Palmer and Hearst (1997). Thai SB has therefore been regarded as the task of classifying each space that appears in a Thai source text as either sentence-breaking (sb) or non-sentencebreaking (nsb). Several researchers have investigated Thai SB. Along with a discussion of Thai word breaking (WB), Aroonmanakun (2007) examines the issue. With a human study, he establishes that sentence breaks elicited from Thai informants exhibit varying degrees of consensus. Mittrapiyanuruk and Sornlertlamvani</context>
<context position="12753" citStr="Wathabunditkul 2003" startWordPosition="2056" endWordPosition="2057">|3 |4 |5 |6 |7 |8 |9 Figure 7. Date recognition grammar 10 2.5 Thai Text Re-spacing To conclude this section, we mention an operation complementary to Thai WB, whereby Thai words output by an SMT system must be respaced in accordance with Thai prescriptive convention. As will be mentioned in Section 4.2, for each input sentence, our English-Thai system has access to an English dependency parse tree, as well as links between this tree and a Thai transfer dependency tree. After using these links to transfer syntactic information to the Thai tree, we are able to apply prescriptive spacing rules (Wathabunditkul 2003) as closely as possible. Human evaluation showed satisfactory results for this process. 3 Maximum Entropy Sentence-Breaking We now turn to a description of our statistical sentence-breaking model. We train an ME classifier on features which describe the proximal environment of the space token under consideration and use this model at runtime to classify space tokens with possibly unseen contexts. 3.1 Modeling Under the ME framework, let B={sb, nsb} represent the set of possible classes we are interested in predicting for each space token in the input stream. Let C={linguistic contexts} represe</context>
</contexts>
<marker>Wathabunditkul, 2003</marker>
<rawString>Suphawut Wathabunditkul. 2003. Spacing in the Thai Language. http://www.thailanguage.com/ ref/spacing</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>