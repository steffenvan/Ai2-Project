<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000360">
<title confidence="0.7513736">
Using Scenario Knowledge in Automatic Question Answering
Sanda Harabagiu and Andrew Hickl
Language Computer Corporation
1701 North Collins Boulevard
Richardson, Texas 75080 USA
</title>
<email confidence="0.995988">
sanda@languagecomputer.com
</email>
<sectionHeader confidence="0.997352" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938416666667">
This paper describes a novel framework
for using scenario knowledge in open-
domain Question Answering (Q/A) appli-
cations that uses a state-of-the-art textual
entailment system (Hickl et al., 2006b) in
order to discover textual information rele-
vant to the set of topics associated with a
scenario description. An intrinsic and an
extrinsic evaluation of this method is pre-
sented in the context of an automatic Q/A
system and results from several user sce-
narios are discussed.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999454363636364">
Users of today’s automatic question-answering
(Q/A) systems generally have complex informa-
tion needs that cannot be satisfied by asking single
questions in isolation. When users interact with
Q/A systems, they often formulate sets of queries
that they believe will help them gather the infor-
mation that needed to perform one or more spe-
cific tasks. While human users are generally able
to identify their information needs independently,
the information needs of organizations are often
presented in the form of short prose descriptions
– known as scenarios – which outline the range
of knowledge sought by a customer in order to
achieve a specific outcome or to accomplish a par-
ticular task. (An example of one scenario is pre-
sented in Figure 1.)
Recent work in Q/A has sought to use in-
formation derived from these kinds of scenar-
ios in order to retrieve sets of answers that are
more relevant – and responsive – to a customer’s
information needs. While (Harabagiu et al.,
2005) used topic signatures (Lin and Hovy, 2000;
</bodyText>
<subsectionHeader confidence="0.995104">
Scenario Description
</subsectionHeader>
<bodyText confidence="0.9941977">
The customer has commissioned a research project looking at the
impact of the outsourcing of American jobs on the United States’
relationship with India. After conducting research on U.S.
companies currently doing business in India, the customer wants
to know why American corporations have sought to outsource jobs
to India, the types of economic advantages that American companies
could gain from relocating to India, and the kinds of economic or
political inducements that India has offered to American companies
looking to outsource jobs there. The customer is not interested
in demographic information on Indian employees of American firms.
</bodyText>
<tableCaption confidence="0.99508">
Table 1: Example of a User Scenario.
</tableCaption>
<bodyText confidence="0.999645266666667">
Harabagiu, 2004) computed automatically from
collections of documents relevant to a scenario in
order to approximate the semantic content of a
scenario, (Narayanan and Harabagiu, 2004) em-
ployed formal models of the interrelated events,
actions, states, and relations implicit to a sce-
nario in order to produce fine-grained, context-
sensitive inferences that could be used to answer
questions. Scenario knowledge was also included
in the form of axiomatic logic transformation de-
veloped in (Moldovan et al., 2003). Under this
approach, information extracted from the scenario
narrative is converted to logical axioms that can
used in conjunction with a logic prover in order
justify answers returned for questions.
In this paper, we propose that scenario-relevant
passages in natural language texts can be identified
by recognizing a semantic relation, known as con-
textual entailment (CE), that exists between a text
passage and one of a set of subquestions that are
conventionally implied by a scenario. Under this
model, we expect that a scenario S can be consid-
ered to contextually entail a passage t, when there
exists at least one subquestion q derived from S
that textually entails the passage t. We show that
by using a state-of-the-art textual entailment sys-
tem (Hickl et al., 2006b), we can provide Q/A sys-
tems with another mechanism for approximating
the inference between questions and relevant an-
swers. We show how each of these cases of con-
</bodyText>
<page confidence="0.975876">
32
</page>
<note confidence="0.6992905">
Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 32–39,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998335642857143">
textual entailment can be computed and how it can
be used in the intrinsic and extrinsic evaluation of
a Q/A system.
The remainder of the paper is organized in the
following way. Section 2 introduces our notion of
contextual entailment and provides a framework
for recognizing instances of CE between scenar-
ios and both questions and answers. Section 3 de-
scribes the textual entailment system used at the
core of our CE system. Sections 4 and 5 describe
separate frameworks for intrinsically and extrinsi-
cally evaluating the impact of CE on current Q/A
systems. Section 6 presents results from our evalu-
ations, and Section 7 summarizes our conclusions
</bodyText>
<sectionHeader confidence="0.997405" genericHeader="introduction">
2 Recognizing Contextual Entailment
</sectionHeader>
<bodyText confidence="0.999927647058824">
We define contextual entailment (CE) as a direc-
tional relation that exists between a text passage t
and one of a set of implicit subquestions q that can
be derived from a user’s interpretation of a sce-
nario. Informally, we consider that a scenario S
contextually entails a passage t when there exists
at least one subquestion q implied by S that can be
considered to entail t.
We expect that the meaning of an information-
seeking scenario S can be represented as a ques-
tion under discussion (QUD) Qs, which denotes a
partially-ordered set of subquestions (q E Qs) that
represent the entire set of questions that could po-
tentially be asked in order to gather information
relevant to S. Taken together, we expect these
subquestions to represent the widest possible con-
strual of a user’s information need given S.
</bodyText>
<table confidence="0.909807428571429">
CASE 1 CASE 2 CASE 3
User Scenario Contextual ?ilEttmenna User Scenario Contextual ?ilEttmenna User Scenario
Contextual Answer Contextual
Entailment? Entailment? Entailment?
Answer Question Answer
Entailment? Entailment?
Question Question
</table>
<figureCaption confidence="0.995219">
Figure 1: Three types of Contextual Entailment
</figureCaption>
<bodyText confidence="0.979496333333333">
We believe the set of subquestions implied by
Qs can be used to test whether a text passage is
relevant to S. Since the formal answerhood re-
lation between a question and its answer(s) can
be cast in terms of (logical) entailment (Groe-
nendijk, 1999; Lewis, 1988), we believe that sys-
tems for recognizing textual entailment (Dagan et
al., 2005) could be used in order to identify those
text passages that should be considered when gath-
ering information related to a scenario. Based on
these assumptions, we expect that the set of text
passages that are textually entailed by subques-
tions derived from a scenario represent informa-
tion that is more likely to be relevant to the overall
topic of the scenario as a whole.
We expect that there are three types of contex-
tual entailment relationships that could prove use-
ful for automatic Q/A systems. First, as illustrated
in Case 1 in 1, CE could exist between a scenario
and one of the set of answers returned by a Q/A
system in response to a user’s question. Second,
as in Case 2, CE could be established directly be-
tween a scenario and the question asked by the
user. Finally, as in Case 3, CE could be established
both between a scenario and a user’s question as
well as between a scenario and one of the answers
returned by the Q/A system for that question.
Figure 2 provides examples of each of these
three types of contextual entailment using the sce-
nario presented in Figure 1.
</bodyText>
<figure confidence="0.942809">
CASE 2: S does not entail A, Q entails A, S entails Q
Textual Entailment Contextual Entailment
</figure>
<figureCaption confidence="0.999574">
Figure 2: Examples of Contextual Entailment.
</figureCaption>
<bodyText confidence="0.999935636363636">
In Case 1, the scenario textually entails the
meaning of the answer passage, as earnings
growth from outsourcing necessarily represents
one of the types of economic advantages that can
be derived from outsourcing. However, the sce-
nario cannot be seen as entailing the user’s ques-
tion, as the user’s interest in job outsourcing in
Indonesia cannot be interpreted as being part of
the topics that are associated with the scenario.
In this case, recognition of contextual entailment
would allow systems to be sensitive to the types of
</bodyText>
<figure confidence="0.644002">
CASE 1: S entails A, Q entails A, S does not entail Q
Scenario: the types of economic advantages that American
companies could gain from relocating to India
Answer: GE and Dell have reported earnings growth after
outsourcing jobs to both Indonesia and India
Question: What U.S. companies are outsourcing jobs to
Indonesia?
</figure>
<figureCaption confidence="0.756981923076923">
Scenario: the types of economic advantages that American
companies could gain from relocating to India
Answer: Despite public opposition to outsourcing jobs to
India, political support has never been higher.
Question: How could U.S. companies benefit by moving jobs
to India?
CASE 3: S entails A, Q entails A, S entails Q
Scenario: the types of economic advantages that American
companies could gain from relocating to India
Answer: Outsourcing jobs to India saved the carrier $25
million, enabling it to turn a profit for the first time.
Question: How could U.S. companies profit from moving
certain types of jobs to India?
</figureCaption>
<page confidence="0.998227">
33
</page>
<bodyText confidence="0.999944470588235">
scenario-relevant information that is encountered
– even when the user asks questions that are not
entailed by the scenario itself. We expect that this
type of contextual entailment would allow systems
to identify scenario-relevant knowledge through-
out a user’s interaction with a system, regardless
of topic of a user’s last query.
In Case 2, the user’s question is entailed by
the scenario, but no corresponding entailment re-
lationship can be established between the scenario
and the answer passage identified by the Q/A sys-
tem as an answer to the question. While political
support may be interpretable as one of the benefits
realized by companies that outsource, it cannot be
understood as one of the economic advantages of
outsourcing. Here, recognizing that contextual en-
tailment could not be established between the sce-
nario and the answer – but could be established
between the scenario and the question – could be
used to signal the Q/A system to consider addi-
tional answers before moving on to the user’s next
question. By identifying contextual entailment
relationships between answers and elements in a
scenario, systems could perform valuable forms of
answer validation that could be used to select only
the most relevant answers for a user’s considera-
tion.
Finally, in Case 3, entailment relationships exist
between the scenario and both the user’s question
and the returned answer, as saving $25 million can
be considered to be both an economic advantage
and one of the ways that companies profit from
outsourcing. In this case, the establishment of con-
textual entailment could be used to inform topic
models that could be used to identify and extract
other similarly relevant passages for the user.
In order to capture these three types of CE re-
lationships, we developed the architecture for rec-
ognizing contextual entailment illustrated in Fig-
ure 3.
This architecture includes three basic types of
modules: (1) a Context Discovery module, which
identifies passages relevant to the concepts men-
tioned in a scenario, (2) a Textual Entailment mod-
ule, which recognizes implicational relationships
between passages, and (3) a Entailment Merg-
ing module, which ranks relevant passages ac-
cording to their relevance to the scenario itself.
In Context Discovery, document retrieval queries
are first extracted from each sentence found in a
scenario. Once a set of documents has been as-
</bodyText>
<figure confidence="0.597744">
Merging Textual Entailment Results
Contextual Entailment Decision / Confidence
</figure>
<figureCaption confidence="0.999394">
Figure 3: Contextual Entailment Architecture.
</figureCaption>
<bodyText confidence="0.998647952380952">
sembled, topic signatures (Lin and Hovy, 2000;
Harabagiu 2004) are computed which identify the
set of topic-relevant concepts – and relations be-
tween concepts – that are found in the relevant set
of documents. Weights associated with each set of
topic signatures are then used to extract a set of
relevant sentences – referred to as topic answers –
from each relevant document. Once a set of topic
answers have been identified, each topic answer is
paired with a question submitted by a user and sent
to the Textual Entailment system described in Sec-
tion 2. Topic answers that are deemed to be pos-
itive entailments of the user question are assigned
a confidence value by the TE system and are then
sent to a Entailment Merging module, which uses
logistic regression in order to rank passages ac-
cording to their expected relevance to the user sce-
nario. Here, logistic regression is used to find a set
of coefficients bj (where 0 &lt; j &lt; p) in order to fit
a variable x to a logistic transformation of a prob-
ability q.
</bodyText>
<equation confidence="0.959680666666667">
p
logit (q) = log 1 q q = b0 + E bjxj + e
j=1
</equation>
<bodyText confidence="0.99992075">
We believe that since logistic regression uses a
maximum likelihood method, it is a suitable tech-
nique for normalizing across range of confidence
values output by the TE system.
</bodyText>
<figure confidence="0.999294454545455">
User
Scenario
Query Extraction
Signature
Answer
Relevant
Documents
Signature
Answer
Question/Answer
Textual
Entailment
Topic Signatures
Signature
Answer
Signature
Answer
Irelevant
Documents
Scenario Context
Signature
Answer
</figure>
<page confidence="0.812772">
34
</page>
<figureCaption confidence="0.998639">
Figure 4: Textual Entailment Architecture.
</figureCaption>
<sectionHeader confidence="0.966336" genericHeader="method">
3 Recognizing Textual Entailment
</sectionHeader>
<bodyText confidence="0.979433293103448">
Recent work in computational seman-
tics (Haghighi et al., 2005; Hickl et al., 2006b;
MacCartney et al., 2006) has demonstrated the
viability of supervised machine learning-based
approaches to the recognition of textual en-
tailment (TE). While these approaches have
not incorporated the forms of structured world
knowledge featured in many logic-based TE sys-
tems, classification-based approaches have been
consistently among the top-performing systems
in both the 2005 and 2006 PASCAL Recognizing
Textual Entailment (RTE) Challenges (Dagan et
al., 2005), with the best systems (such as (Hickl
et al., 2006b)) correctly identifying instances of
textual entailment more than 75% of the time.
The architecture of our TE system is presented
in Figure 4.1 Pairs of texts are initially sent to a
Preprocessing Module, which performs syntactic
and semantic parsing of each sentence, resolves
coreference, and annotates entities and predicates
with a wide range of lexico-semantic and prag-
&apos;For more information on the TE system described in this
section, please see (Hickl et al., 2006b) and (Harabagiu and
Hickl, 2006).
matic information, including named entity infor-
mation, synonymy and antonymy information, and
polarity and modality information.
Once preprocessing is complete, texts are then
sent to an Alignment Module, which uses lexi-
cal alignment module in conjunction with a para-
phrase acquisition module in order to determine
the likelihood that pairs of elements selected from
each sentence contain corresponding information
that could be used in recognizing textual entail-
ment. Lexical Alignment is performed using a
Maximum Entropy-based classifier which com-
putes an alignment probability p(a) equal to the
likelihood that a term selected from one text cor-
responds to an element selected from another text.
Once these pairs of corresponding elements are
identified, alignment information is then used in
order to extract portions of texts that could be
related via one or more phrase-level alternations
or “paraphrases”. In order to acquire these al-
ternations, the most likely pairs of aligned ele-
ments were then sent to a Paraphrase Acquisition
module, which extracts sentences that contain in-
stances of both aligned elements from the World
Wide Web.
Output from these two modules are then com-
bined in a final Classification Module, which uses
features derived from (1) lexico-semantic prop-
erties, (2) semantic dependencies, (3) predicate-
based features (including polarity and modality),
(4) lexical alignment, and (5) paraphrase acquisi-
tion in order learn a decision tree classifier capable
of determining whether an entailment relationship
exists for a pair of texts.
</bodyText>
<sectionHeader confidence="0.949272" genericHeader="method">
4 Intrinsic Evaluation of Contextual
Entailment
</sectionHeader>
<bodyText confidence="0.999553142857143">
Since we believe CE is intrinsic to the Q/A task,
we have evaluated the impact of contextual en-
tailment on our Q/A system in two ways. First,
we compared the quality of the answers produced,
with and without contextual entailment. Second,
we evaluated the quality of the ranked list of para-
graphs against the list of entailed paragraphs iden-
tified by the CE system and the set of relevant an-
swers identified by the Q/A system. This compar-
ison was performed for each of the three cases of
entailment presented in Figure 2.
We have explored the impact of knowledge
derived from the user scenario through different
forms of contextual entailment by comparing the
</bodyText>
<table confidence="0.992382909090909">
Textual Textual
Input 1 Input 2
Preprocessing
Lexico−Semantic Parsing Coreference
PoS/ NER Syntactic Concept
Synonyms/ Semantic Coreference
Antonyms Temporal NE
Normalization Aliasing
Pragmatics
Modality Detection Speech Act Recognition
Factivity Detection Belief Recognition
</table>
<figure confidence="0.979527933333333">
Paraphrase Acquisition
Feature Extraction
Dependency
Features
Alignment Module
Alignment
Features
Lexical Alignment
Semantic/
Pragmatic
Features
Paraphrase
Features
Classification Module
Classifier
Training
Corpora
YES
WWW
NO
35
Question
User
Scenario
Answer
Set1
Answer
Set2
Topic Signatures Relevant Answers
Documents
AUTO−QUAB
Generation
Question
Similarity
List of Questions
Passage
Retrieval
Module
(PR)
Ranked List of Paragraphs
CONTEXTUAL
ENTAILMENT
List of Entailed Paragraphs
Question
Processing
Module
(QP)
Answer
Processing
Module
(AP)
Keywords
Answer
Processing
Module
(AP)
CONTEXTUAL
ENTAILMENT
Answer
Set3
</figure>
<figureCaption confidence="0.999881">
Figure 5: Framework for Intrinsic Evaluation of Contextual Entailment in Q/A.
</figureCaption>
<bodyText confidence="0.999904552631579">
results of such knowledge integration in a Q/A
system against the usage of scenario knowledge
reported in (Harabagiu et al., 2005).
Topic signatures, derived from the user scenario
and from documents are used to establish text pas-
sages that are relevant to the scenario, and thus
constitute relevant answers. For each such an-
swer, one or multiple questions were built auto-
matically with the method reported in (Harabagiu
et al., 2005). When a new question was asked, its
similarity to any of the questions generated based
on the knowledge of the scenario is computed, and
its corresponding answer is provided as an answer
for the current question as well. Since the ques-
tions are ranked by similarity to the current ques-
tion, the answers are also ranked and produce the
Answer Seti illustrated in Figure 5.
When a Q/A system is used for answering the
question, the scenario knowledge can be used in
two ways. First, the keywords extracted by the
Question Processing module can be enhanced with
concepts from the topic signatures to produce a
ranked list of paragraphs, resulting from the Pas-
sage Retrieval Module. These passages together
with the question and the user scenario are used
in one of the contextual entailment configurations
to derive a list of entailed paragraphs from which
the Answer Processing module can extract the an-
swer set 2 illustrated in Figure 5. In another way,
the ranked list of paragraphs is passed to the An-
swer Processing module, which provides a set of
ranked answers to the contextual entailment con-
figurations to derive a list of entailed answers, rep-
resented as answer set 3 in Figure 5. We evalu-
ate the quality of each set of answers, and for the
answer set 2 and 3, we produce separate evalua-
tion for each configuration for the contextual en-
tailment.
</bodyText>
<sectionHeader confidence="0.9363185" genericHeader="method">
5 Extrinsic Evaluation of Contextual
Entailment
</sectionHeader>
<bodyText confidence="0.99993425">
Questions asked in response to a user scenario
tend to be complex. Following work in (Hickl
et al., 2004), we believe complex questions can
be answered in one of two ways: either by
(1) using techniques (similar to the ones pro-
posed in (Harabagiu et al., 2006)) for automati-
cally decomposing complex questions into sets of
informationally-simpler questions, or by (2) us-
ing a multi-document summarization (MDS) sys-
tem (such as the one described in (Lacatusu et al.,
2006)) in order to assemble a ranked list of pas-
sages which contain information that is potentially
relevant to the user’s question.
First, we expect that contextual entailment can
be used to select the decompositions of a complex
question that are most closely related to a scenario.
By assigning more confidence to the decomposi-
tions that are contextually entailed by a scenario,
systems can select a set of answers that are rel-
evant to both the user scenario – and the user’s
question. In contrast, contextual entailment can be
used in conjunction with the output of a MDS sys-
tem: once a summary has been constructed from
the passages retrieved for a query, contextual en-
</bodyText>
<page confidence="0.988672">
36
</page>
<figure confidence="0.999856">
Question
User
Scenario
Contextual
Entailment
Keyword Extraction
Question Decomposition
Sub−Questions
Query
Question Answering
System
Multi−Document
Summarization
System
Relevant
Documents
Summary
Candidate
Candidate Answers
Documents
Contextual
Entailment
Answers
Ranked
</figure>
<figureCaption confidence="0.999798">
Figure 6: Framework for Extrinsic Evaluation of Contextual Entailment in Q/A.
</figureCaption>
<bodyText confidence="0.993671821428571">
tailment can be used to select the most relevant
sentences from the summary.
The architecture of this proposed system is il-
lustrated in Figure 6.
When using contextual entailment for selecting
question decompositions, we rely on the method
reported in (Harabagiu et al., 2006) which gener-
ates questions by using a random walk on a bipar-
tite graph of salient relations and answers. In this
case, the recognition of entailment between ques-
tions operates as a filter, forcing questions that are
not entailed by any of the signature answers de-
rived from the scenario context (see Figure 3) to
be dropped from consideration.
When entailment information is used for re-
ranking candidate answers, the summary is added
to the scenario context, each summary sentence
being treated akin to a signature answer. We be-
lieve that the summary contains the most informa-
tive information from both the question and the
scenario, since the queries that produced it orig-
inated both in the question and in the scenario. By
adding summary sentences to the scenario context,
we have introduced a new dimension to the pro-
cessing of the scenario. The contextual entailment
is based on the textual entailments between any of
the texts from the scenario context and any of the
candidate answers.
</bodyText>
<sectionHeader confidence="0.989298" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.998585952380953">
In this section, we present preliminary results from
four sets of experiments which show how forms of
textual – and contextual – entailment can enhance
the quality of answers returned by an automatic
Q/A system.
Questions used in these experiments were gath-
ered from human interactions with the interactive
Q/A system described in (Hickl et al., 2006a). A
total of 6 users were asked to spend approximately
90 minutes gathering information related to three
different information-gathering scenarios similar
to the one in Table 1. Each user researched two
different scenarios, resulting in a total of 12 to-
tal research sessions. Once all research sessions
were completed, linguistically well-formed ques-
tions were extracted from the system logs for each
session for use in our experiments; ungrammatical
questions or keyword-style queries were not used
in our experiments. Table 2 presents a breakdown
of the total number of questions collected for each
of the 6 scenarios.
</bodyText>
<table confidence="0.9991266">
Scenario Name Users Total Qs Avg. Q/Session v2
S1. India Outsourcing 4 45 11.25 2.50
S2. Chinese WMD Proliferation 4 38 9.50 6.45
S3. Libyan Bioweapons Programs 4 63 15.75 2.22
Total 12 146 12.17 1.23
</table>
<tableCaption confidence="0.9110815">
Table 2: Questions Collected from User Experi-
ments.
</tableCaption>
<bodyText confidence="0.999648230769231">
In order to evaluate the performance of our Q/A
system under each of the experimental conditions
described below, questions were re-submitted to
the Q/A system and the top 10 answers were re-
trieved. Two annotators were then tasked with
judging the correctness – or “relevance” – of each
returned answer to the original question. If the an-
swer could be considered to provide either a com-
plete or partial answer to the original question, it
was marked as correct; if the answer contained in-
formation that could not be construed as an answer
to the original question, it was marked as incor-
rect.
</bodyText>
<subsectionHeader confidence="0.999324">
6.1 Textual Entailment
</subsectionHeader>
<bodyText confidence="0.999710090909091">
Following (Harabagiu and Hickl, 2006), we used
TE information in order to filter answers identified
by the Q/A system that were not entailed by the
user’s original question. After filtering, the top-
ranked entailed answer (as determined by the Q/A
system) was returned as the system’s answer to the
question. Results from both a baseline version and
a TE-enhanced version of our Q/A system are pre-
sented in Table 4.
Although no information from the scenario was
used in this experiment, performance of the Q/A
</bodyText>
<page confidence="0.998776">
37
</page>
<table confidence="0.998278166666667">
S1 S2 S3 Total
# of Questions 45 38 63 146
baseline top 1 8 (17.78%) 6 (15.79%) 11 (17.46%) 25 (17.12%)
TE top 1 10 (22.22%) 8 (21.05%) 16 (25.40%) 34 (23.29%)
baseline top 5 17 (37.78%) 16 (42.11%) 27 (42.86%) 60 (41.10%)
TE top 5 20 (44.44%) 17 (44.74%) 32 (50.79%) 69 (47.26%)
</table>
<tableCaption confidence="0.99984">
Table 3: Impact of Textual Entailment on Q/A.
</tableCaption>
<bodyText confidence="0.999908">
system increased by more than 6% over the base-
line system for each of the three scenarios. These
results suggest that TE can be used effectively in
order to boost the percentage of relevant answers
found in the top answers returned by a system:
by focusing only on answers that are entailed by
a user’s question, we feel that systems can better
identify passages that might contain information
relevant to a user’s information need.
</bodyText>
<subsectionHeader confidence="0.997156">
6.2 Contextual Entailment
</subsectionHeader>
<bodyText confidence="0.99977892">
In order to evaluate the performance of our con-
textual entailment system directly, two annota-
tors were tasked with identifying instances of CE
amongst the passages and answers returned by
our Q/A system. Annotators were instructed to
mark a passage as being contextually entailed by
a scenario only when the passage could be rea-
sonably expected to be associated with one of the
subtopics they believed to be entailed by the com-
plex scenario. If the passage could not be associ-
ated with the extension of any subtopic they be-
lieved to be entailed by the scenario, annotators
were instructed to mark the passage as not being
contextually entailed by the scenario. For evalua-
tion purposes, only examples that were marked by
both annotators were considered as valid examples
of CE.
Annotators were tasked with evaluating three
types of output from our Q/A system: (1) the
ranked list of passages retrieved by our system’s
Passage Retrieval module, (2) the list of passages
identified as being CE by the scenario, and (3) the
set of answers marked as being CE by the scenario
(AnsSet3). Results from the annotation of these
passages are presented in Table 4.
</bodyText>
<table confidence="0.9978238">
S1 S2 S3 Total
# %Rel # %Rel # %Rel # %Rel
Ranked Paragraphs 450 40.4% 380 31.3% 630 42.5% 1460 39.3%
Entailed Paragraphs 112 46.5% 87 44.8% 149 52.4% 348 48.6%
Answer Set 3 304 44.4% 188 39.9% 322 49.1% 814 45.2%
</table>
<tableCaption confidence="0.998596">
Table 4: Distribution of CE.
</tableCaption>
<bodyText confidence="0.999528714285714">
Annotators marked 39.3% of retrieved passages
as being CE by one of the three scenarios. This
number increased substantially when only pas-
sages identified by the CE system were consid-
ered, as annotators judged 48.6% of CE passages
and 45.2% of CE-filtered answers to be valid in-
stances of contextual entailment.
</bodyText>
<subsectionHeader confidence="0.98028">
6.3 Intrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.916271111111111">
In order to evaluate the impact of CE on a Q/A sys-
tem, we compared the quality of answers produced
(1) when no CE information was used (AnsSet1),
(2) when CE information was used to select a
list of entailed paragraphs that were submitted to
an Answer Processing module (AnsSet2), and (3)
when CE information was used directly to select
answers (AnsSet3). Results from these three ex-
periments are presented in Table 5.
</bodyText>
<table confidence="0.99903575">
S1 S2 S3 Total
# of Questions 45 38 63 146
AnsSet1 top 1 12 (26.67%) 9 (23.68%) 19 (30.16%) 40 (27.39%)
AnsSet2 top 1 16 (35.56%) 11 (28.95%) 26 (41.27%) 53 (36.30%)
AnsSet3 top 1 14 (31.11%) 15 (39.47%) 31 (49.21%) 60 (41.09%)
AnsSet1 top 5 21 (46.67%) 17 (44.74%) 30 (47.62%) 68 (46.58%)
AnsSet2 top 5 24 (53.33%) 18 (47.37%) 35 (55.55%) 77 (52.74%)
AnsSet3 top 5 29 (64.44%) 20 (52.63%) 39 (61.90%) 88 (60.27%)
</table>
<tableCaption confidence="0.8948815">
Table 5: Intrinsic Evaluation of CE on Q/A Per-
formance.
</tableCaption>
<bodyText confidence="0.999666181818182">
As with the TE-based experiments described in
Section 7.1, we found that the Q/A system was
more likely to return at least one relevant an-
swer among the top-ranked answers when con-
textual entailment information was used to either
rank or select answers. When CE was used to
rank passages for Answer Processing (AnsSet2),
accuracy increased by nearly 9% over the base-
line (AnsSet1), while accuracy increased by al-
most 14% overall when CE was used to select an-
swers directly (AnsSet3).
</bodyText>
<subsectionHeader confidence="0.982545">
6.4 Extrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.999793263157895">
In order to evaluate the performance of the frame-
work illustrated in Figure 6, we compared the per-
formance of a question-focused MDS system (first
described in (Lacatusu et al., 2006)) that did not
use CE with a similar system that used CE to rank
passages for a summary answer.
When CE was not used, sentences identified by
the system’s Q/A and MDS system for each ques-
tion were combined and ranked based on number
of question keywords found in each sentence. In
the CE-enabled system (analogous to the system
depicted in Figure 6), only the sentences that were
contextually entailed by the scenario were consid-
ered; sentences were then ranked using the real-
valued entailment confidence computed by the CE
system for each sentence. Results from this sys-
tem are presented in Table 6.
Although the CE-enabled system was more
likely to return a scenario-relevant sentence in top
</bodyText>
<page confidence="0.997382">
38
</page>
<table confidence="0.993551333333333">
S1 S2 S3 Total
# of Questions 45 38 63 146
Without CE top 1 14 (31.11%) 15 (39.47%) 31 (49.21%) 60 (41.09%)
With CE top 1 20 (44.44%) 16 (42.11%) 32 (50.79%) 68 (48.23%)
Without CE top 5 29 (64.44%) 20 (52.63%) 39 (61.90%) 88 (60.27%)
With CE top 5 29 (64.44%) 21 (55.26%) 40 (63.49%) 90 (61.64%)
</table>
<tableCaption confidence="0.998902">
Table 6: Extrinsic Evaluation.
</tableCaption>
<bodyText confidence="0.99983475">
position (48.23%) than the system that did not
use CE (41.09%), differences between the systems
were much less apparent when the top 5 answers
returned by each system were compared.
</bodyText>
<sectionHeader confidence="0.999565" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999966076923077">
This paper introduced a new form of textual entail-
ment, known as contextual entailment, which can
be used to recognize scenario-relevant information
in both the questions users ask and in the answers
that automatic Q/A systems return. In addition
to outlining a framework for recognizing contex-
tual entailment in texts, we showed that contextual
entailment information can significantly enhance
the quality of answers returned by a Q/A system
in response to users’ questions about a particular
scenario. In our evaluations, we found that using
contextual entailment allowed Q/A systems to im-
prove their accuracy by more than 10% overall.
</bodyText>
<sectionHeader confidence="0.9993" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9991255">
This material is based upon work funded in whole
or in part by the U.S. Government and any opin-
ions, findings, conclusions, or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the views of the U.S.
Government.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999720309859155">
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The PASCAL Recognizing Textual Entailment Challenge.
In Proceedings of the PASCAL Challenges Workshop.
Jeroen Groenendijk. 1999. The logic of interrogation: Clas-
sical version. In Proceedings of the Ninth Semantics and
Linguistics Theory Conference (SALT IX), Ithaca, NY.
Aria Haghighi, Andrew Ng, and Christopher Manning. 2005.
Robust textual inference via graph matching. In Pro-
ceedings of Human Language Technology Conference and
Conference on Empirical Methods in Natural Language
Processing, Vancouver, British Columbia, Canada, Octo-
ber.
Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain question-
answering. In Proceedings of the Joint International
Conference on Computational Linguistics and Annual
Meeting of the Association for Computational Linguistics
(COLING-ACL 2006), Sydney, Australia.
Sanda Harabagiu, Andrew Hickl, John Lehmann, and Dan
Moldovan. 2005. Experiments with Interactive Question-
Answering. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL’05).
Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl. 2006.
Answering complex questions with random walk models.
In 2006 ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR), Seattle, WA.
Sanda Harabagiu. 2004. Incremental Topic Representations.
In Proceedings of the 20th COLING Conference, Geneva,
Switzerland.
Andrew Hickl, John Lehmann, John Williams, and Sanda
Harabagiu. 2004. Experiments with Interactive Question-
Answering in Complex Scenarios. In Proceedings of the
Workshop on the Pragmatics of Question Answering at
HLT-NAACL 2004, Boston, MA.
Andrew Hickl, Patrick Wang, John Lehmann, and Sanda
Harabagiu. 2006a. FERRET: Interactive Question-
Answering for Real-World Environments. In Proceed-
ings of the Joint International Conference on Computa-
tional Linguistics and Annual Meeting of the Association
for Computational Linguistics (COLING-ACL 2006) In-
teractive Presentations Program, Sydney, Australia.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006b. Recognizing Textual
Entailment with LCC’s Groundhog System. In Proceed-
ings of the Second PASCAL Challenges Workshop, Syd-
ney, Australia.
Finley Lacatusu, Andrew Hickl, Kirk Roberts, Ying Shi,
Jeremy Bensley, Bryan Rink, Patrick Wang, and Lara Tay-
lor. 2006. LCC’s GISTexter at DUC 2006: Multi-Strategy
Multi-Document Summarization. In Proceedings of the
2006 Document Understanding Conference (DUC 2006),
New York, New York.
David Lewis. 1988. Relevant Implication. Theoria,
54(3):161–174.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text summariza-
tion. In Proceedings of the 18th COLING Conference,
Saarbr¨ucken, Germany.
Bill MacCartney, Trond Grenager, Marie-Catherine de Marn-
effe, Daniel Cer, and Christopher D. Manning. 2006.
Learning to recognize features of valid textual entail-
ments. In Proceedings of the Joint Human Language
Technology Conference and Annual Meeting of the North
American Chapter of the Association for Computational
Linguistics (HLT-NAACL 2006), New York, New York.
Dan Moldovan, Christine Clark, Sanda Harabagiu, and Steve
Maiorano. 2003. COGEX: A Logic Prover for Question
Answering. In Proceedings of HLT/NAACL-2003.
Srini Narayanan and Sanda Harabagiu. 2004. Question An-
swering based on Semantic Structures. In Proceedings of
COLING-2004.
</reference>
<page confidence="0.999532">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955367">
<title confidence="0.999608">Using Scenario Knowledge in Automatic Question Answering</title>
<author confidence="0.982165">Sanda Harabagiu</author>
<author confidence="0.982165">Andrew</author>
<affiliation confidence="0.995802">Language Computer</affiliation>
<address confidence="0.997562">1701 North Collins Richardson, Texas 75080</address>
<email confidence="0.999707">sanda@languagecomputer.com</email>
<abstract confidence="0.998471076923077">This paper describes a novel framework for using scenario knowledge in opendomain Question Answering (Q/A) applications that uses a state-of-the-art textual entailment system (Hickl et al., 2006b) in order to discover textual information relevant to the set of topics associated with a scenario description. An intrinsic and an extrinsic evaluation of this method is presented in the context of an automatic Q/A system and results from several user scenarios are discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop.</booktitle>
<contexts>
<context position="6171" citStr="Dagan et al., 2005" startWordPosition="979" endWordPosition="982">SE 2 CASE 3 User Scenario Contextual ?ilEttmenna User Scenario Contextual ?ilEttmenna User Scenario Contextual Answer Contextual Entailment? Entailment? Entailment? Answer Question Answer Entailment? Entailment? Question Question Figure 1: Three types of Contextual Entailment We believe the set of subquestions implied by Qs can be used to test whether a text passage is relevant to S. Since the formal answerhood relation between a question and its answer(s) can be cast in terms of (logical) entailment (Groenendijk, 1999; Lewis, 1988), we believe that systems for recognizing textual entailment (Dagan et al., 2005) could be used in order to identify those text passages that should be considered when gathering information related to a scenario. Based on these assumptions, we expect that the set of text passages that are textually entailed by subquestions derived from a scenario represent information that is more likely to be relevant to the overall topic of the scenario as a whole. We expect that there are three types of contextual entailment relationships that could prove useful for automatic Q/A systems. First, as illustrated in Case 1 in 1, CE could exist between a scenario and one of the set of answe</context>
<context position="13489" citStr="Dagan et al., 2005" startWordPosition="2178" endWordPosition="2181">4: Textual Entailment Architecture. 3 Recognizing Textual Entailment Recent work in computational semantics (Haghighi et al., 2005; Hickl et al., 2006b; MacCartney et al., 2006) has demonstrated the viability of supervised machine learning-based approaches to the recognition of textual entailment (TE). While these approaches have not incorporated the forms of structured world knowledge featured in many logic-based TE systems, classification-based approaches have been consistently among the top-performing systems in both the 2005 and 2006 PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005), with the best systems (such as (Hickl et al., 2006b)) correctly identifying instances of textual entailment more than 75% of the time. The architecture of our TE system is presented in Figure 4.1 Pairs of texts are initially sent to a Preprocessing Module, which performs syntactic and semantic parsing of each sentence, resolves coreference, and annotates entities and predicates with a wide range of lexico-semantic and prag&apos;For more information on the TE system described in this section, please see (Hickl et al., 2006b) and (Harabagiu and Hickl, 2006). matic information, including named entit</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeroen Groenendijk</author>
</authors>
<title>The logic of interrogation: Classical version.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Semantics and Linguistics Theory Conference (SALT IX),</booktitle>
<location>Ithaca, NY.</location>
<contexts>
<context position="6076" citStr="Groenendijk, 1999" startWordPosition="965" endWordPosition="967">ns to represent the widest possible construal of a user’s information need given S. CASE 1 CASE 2 CASE 3 User Scenario Contextual ?ilEttmenna User Scenario Contextual ?ilEttmenna User Scenario Contextual Answer Contextual Entailment? Entailment? Entailment? Answer Question Answer Entailment? Entailment? Question Question Figure 1: Three types of Contextual Entailment We believe the set of subquestions implied by Qs can be used to test whether a text passage is relevant to S. Since the formal answerhood relation between a question and its answer(s) can be cast in terms of (logical) entailment (Groenendijk, 1999; Lewis, 1988), we believe that systems for recognizing textual entailment (Dagan et al., 2005) could be used in order to identify those text passages that should be considered when gathering information related to a scenario. Based on these assumptions, we expect that the set of text passages that are textually entailed by subquestions derived from a scenario represent information that is more likely to be relevant to the overall topic of the scenario as a whole. We expect that there are three types of contextual entailment relationships that could prove useful for automatic Q/A systems. Firs</context>
</contexts>
<marker>Groenendijk, 1999</marker>
<rawString>Jeroen Groenendijk. 1999. The logic of interrogation: Classical version. In Proceedings of the Ninth Semantics and Linguistics Theory Conference (SALT IX), Ithaca, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="13000" citStr="Haghighi et al., 2005" startWordPosition="2109" endWordPosition="2112">ansformation of a probability q. p logit (q) = log 1 q q = b0 + E bjxj + e j=1 We believe that since logistic regression uses a maximum likelihood method, it is a suitable technique for normalizing across range of confidence values output by the TE system. User Scenario Query Extraction Signature Answer Relevant Documents Signature Answer Question/Answer Textual Entailment Topic Signatures Signature Answer Signature Answer Irelevant Documents Scenario Context Signature Answer 34 Figure 4: Textual Entailment Architecture. 3 Recognizing Textual Entailment Recent work in computational semantics (Haghighi et al., 2005; Hickl et al., 2006b; MacCartney et al., 2006) has demonstrated the viability of supervised machine learning-based approaches to the recognition of textual entailment (TE). While these approaches have not incorporated the forms of structured world knowledge featured in many logic-based TE systems, classification-based approaches have been consistently among the top-performing systems in both the 2005 and 2006 PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005), with the best systems (such as (Hickl et al., 2006b)) correctly identifying instances of textual entailment m</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>Aria Haghighi, Andrew Ng, and Christopher Manning. 2005. Robust textual inference via graph matching. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
</authors>
<title>Methods for using textual entailment in open-domain questionanswering.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics (COLING-ACL</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="14047" citStr="Harabagiu and Hickl, 2006" startWordPosition="2267" endWordPosition="2270">Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005), with the best systems (such as (Hickl et al., 2006b)) correctly identifying instances of textual entailment more than 75% of the time. The architecture of our TE system is presented in Figure 4.1 Pairs of texts are initially sent to a Preprocessing Module, which performs syntactic and semantic parsing of each sentence, resolves coreference, and annotates entities and predicates with a wide range of lexico-semantic and prag&apos;For more information on the TE system described in this section, please see (Hickl et al., 2006b) and (Harabagiu and Hickl, 2006). matic information, including named entity information, synonymy and antonymy information, and polarity and modality information. Once preprocessing is complete, texts are then sent to an Alignment Module, which uses lexical alignment module in conjunction with a paraphrase acquisition module in order to determine the likelihood that pairs of elements selected from each sentence contain corresponding information that could be used in recognizing textual entailment. Lexical Alignment is performed using a Maximum Entropy-based classifier which computes an alignment probability p(a) equal to the</context>
<context position="23792" citStr="Harabagiu and Hickl, 2006" startWordPosition="3810" endWordPosition="3813">rformance of our Q/A system under each of the experimental conditions described below, questions were re-submitted to the Q/A system and the top 10 answers were retrieved. Two annotators were then tasked with judging the correctness – or “relevance” – of each returned answer to the original question. If the answer could be considered to provide either a complete or partial answer to the original question, it was marked as correct; if the answer contained information that could not be construed as an answer to the original question, it was marked as incorrect. 6.1 Textual Entailment Following (Harabagiu and Hickl, 2006), we used TE information in order to filter answers identified by the Q/A system that were not entailed by the user’s original question. After filtering, the topranked entailed answer (as determined by the Q/A system) was returned as the system’s answer to the question. Results from both a baseline version and a TE-enhanced version of our Q/A system are presented in Table 4. Although no information from the scenario was used in this experiment, performance of the Q/A 37 S1 S2 S3 Total # of Questions 45 38 63 146 baseline top 1 8 (17.78%) 6 (15.79%) 11 (17.46%) 25 (17.12%) TE top 1 10 (22.22%) </context>
</contexts>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>Sanda Harabagiu and Andrew Hickl. 2006. Methods for using textual entailment in open-domain questionanswering. In Proceedings of the Joint International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
<author>John Lehmann</author>
<author>Dan Moldovan</author>
</authors>
<title>Experiments with Interactive QuestionAnswering.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).</booktitle>
<contexts>
<context position="1680" citStr="Harabagiu et al., 2005" startWordPosition="263" endWordPosition="266">human users are generally able to identify their information needs independently, the information needs of organizations are often presented in the form of short prose descriptions – known as scenarios – which outline the range of knowledge sought by a customer in order to achieve a specific outcome or to accomplish a particular task. (An example of one scenario is presented in Figure 1.) Recent work in Q/A has sought to use information derived from these kinds of scenarios in order to retrieve sets of answers that are more relevant – and responsive – to a customer’s information needs. While (Harabagiu et al., 2005) used topic signatures (Lin and Hovy, 2000; Scenario Description The customer has commissioned a research project looking at the impact of the outsourcing of American jobs on the United States’ relationship with India. After conducting research on U.S. companies currently doing business in India, the customer wants to know why American corporations have sought to outsource jobs to India, the types of economic advantages that American companies could gain from relocating to India, and the kinds of economic or political inducements that India has offered to American companies looking to outsourc</context>
<context position="17424" citStr="Harabagiu et al., 2005" startWordPosition="2764" endWordPosition="2767">g Corpora YES WWW NO 35 Question User Scenario Answer Set1 Answer Set2 Topic Signatures Relevant Answers Documents AUTO−QUAB Generation Question Similarity List of Questions Passage Retrieval Module (PR) Ranked List of Paragraphs CONTEXTUAL ENTAILMENT List of Entailed Paragraphs Question Processing Module (QP) Answer Processing Module (AP) Keywords Answer Processing Module (AP) CONTEXTUAL ENTAILMENT Answer Set3 Figure 5: Framework for Intrinsic Evaluation of Contextual Entailment in Q/A. results of such knowledge integration in a Q/A system against the usage of scenario knowledge reported in (Harabagiu et al., 2005). Topic signatures, derived from the user scenario and from documents are used to establish text passages that are relevant to the scenario, and thus constitute relevant answers. For each such answer, one or multiple questions were built automatically with the method reported in (Harabagiu et al., 2005). When a new question was asked, its similarity to any of the questions generated based on the knowledge of the scenario is computed, and its corresponding answer is provided as an answer for the current question as well. Since the questions are ranked by similarity to the current question, the </context>
</contexts>
<marker>Harabagiu, Hickl, Lehmann, Moldovan, 2005</marker>
<rawString>Sanda Harabagiu, Andrew Hickl, John Lehmann, and Dan Moldovan. 2005. Experiments with Interactive QuestionAnswering. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Finley Lacatusu</author>
<author>Andrew Hickl</author>
</authors>
<title>Answering complex questions with random walk models.</title>
<date>2006</date>
<booktitle>In 2006 ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="19390" citStr="Harabagiu et al., 2006" startWordPosition="3102" endWordPosition="3105">es a set of ranked answers to the contextual entailment configurations to derive a list of entailed answers, represented as answer set 3 in Figure 5. We evaluate the quality of each set of answers, and for the answer set 2 and 3, we produce separate evaluation for each configuration for the contextual entailment. 5 Extrinsic Evaluation of Contextual Entailment Questions asked in response to a user scenario tend to be complex. Following work in (Hickl et al., 2004), we believe complex questions can be answered in one of two ways: either by (1) using techniques (similar to the ones proposed in (Harabagiu et al., 2006)) for automatically decomposing complex questions into sets of informationally-simpler questions, or by (2) using a multi-document summarization (MDS) system (such as the one described in (Lacatusu et al., 2006)) in order to assemble a ranked list of passages which contain information that is potentially relevant to the user’s question. First, we expect that contextual entailment can be used to select the decompositions of a complex question that are most closely related to a scenario. By assigning more confidence to the decompositions that are contextually entailed by a scenario, systems can </context>
<context position="20898" citStr="Harabagiu et al., 2006" startWordPosition="3332" endWordPosition="3335">estion User Scenario Contextual Entailment Keyword Extraction Question Decomposition Sub−Questions Query Question Answering System Multi−Document Summarization System Relevant Documents Summary Candidate Candidate Answers Documents Contextual Entailment Answers Ranked Figure 6: Framework for Extrinsic Evaluation of Contextual Entailment in Q/A. tailment can be used to select the most relevant sentences from the summary. The architecture of this proposed system is illustrated in Figure 6. When using contextual entailment for selecting question decompositions, we rely on the method reported in (Harabagiu et al., 2006) which generates questions by using a random walk on a bipartite graph of salient relations and answers. In this case, the recognition of entailment between questions operates as a filter, forcing questions that are not entailed by any of the signature answers derived from the scenario context (see Figure 3) to be dropped from consideration. When entailment information is used for reranking candidate answers, the summary is added to the scenario context, each summary sentence being treated akin to a signature answer. We believe that the summary contains the most informative information from bo</context>
</contexts>
<marker>Harabagiu, Lacatusu, Hickl, 2006</marker>
<rawString>Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl. 2006. Answering complex questions with random walk models. In 2006 ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
</authors>
<title>Incremental Topic Representations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th COLING Conference,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2444" citStr="Harabagiu, 2004" startWordPosition="381" endWordPosition="382">urcing of American jobs on the United States’ relationship with India. After conducting research on U.S. companies currently doing business in India, the customer wants to know why American corporations have sought to outsource jobs to India, the types of economic advantages that American companies could gain from relocating to India, and the kinds of economic or political inducements that India has offered to American companies looking to outsource jobs there. The customer is not interested in demographic information on Indian employees of American firms. Table 1: Example of a User Scenario. Harabagiu, 2004) computed automatically from collections of documents relevant to a scenario in order to approximate the semantic content of a scenario, (Narayanan and Harabagiu, 2004) employed formal models of the interrelated events, actions, states, and relations implicit to a scenario in order to produce fine-grained, contextsensitive inferences that could be used to answer questions. Scenario knowledge was also included in the form of axiomatic logic transformation developed in (Moldovan et al., 2003). Under this approach, information extracted from the scenario narrative is converted to logical axioms t</context>
<context position="11459" citStr="Harabagiu 2004" startWordPosition="1848" endWordPosition="1849">entifies passages relevant to the concepts mentioned in a scenario, (2) a Textual Entailment module, which recognizes implicational relationships between passages, and (3) a Entailment Merging module, which ranks relevant passages according to their relevance to the scenario itself. In Context Discovery, document retrieval queries are first extracted from each sentence found in a scenario. Once a set of documents has been asMerging Textual Entailment Results Contextual Entailment Decision / Confidence Figure 3: Contextual Entailment Architecture. sembled, topic signatures (Lin and Hovy, 2000; Harabagiu 2004) are computed which identify the set of topic-relevant concepts – and relations between concepts – that are found in the relevant set of documents. Weights associated with each set of topic signatures are then used to extract a set of relevant sentences – referred to as topic answers – from each relevant document. Once a set of topic answers have been identified, each topic answer is paired with a question submitted by a user and sent to the Textual Entailment system described in Section 2. Topic answers that are deemed to be positive entailments of the user question are assigned a confidence </context>
</contexts>
<marker>Harabagiu, 2004</marker>
<rawString>Sanda Harabagiu. 2004. Incremental Topic Representations. In Proceedings of the 20th COLING Conference, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Lehmann</author>
<author>John Williams</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Experiments with Interactive QuestionAnswering in Complex Scenarios.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on the Pragmatics of Question Answering at HLT-NAACL 2004,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="19235" citStr="Hickl et al., 2004" startWordPosition="3074" endWordPosition="3077">extract the answer set 2 illustrated in Figure 5. In another way, the ranked list of paragraphs is passed to the Answer Processing module, which provides a set of ranked answers to the contextual entailment configurations to derive a list of entailed answers, represented as answer set 3 in Figure 5. We evaluate the quality of each set of answers, and for the answer set 2 and 3, we produce separate evaluation for each configuration for the contextual entailment. 5 Extrinsic Evaluation of Contextual Entailment Questions asked in response to a user scenario tend to be complex. Following work in (Hickl et al., 2004), we believe complex questions can be answered in one of two ways: either by (1) using techniques (similar to the ones proposed in (Harabagiu et al., 2006)) for automatically decomposing complex questions into sets of informationally-simpler questions, or by (2) using a multi-document summarization (MDS) system (such as the one described in (Lacatusu et al., 2006)) in order to assemble a ranked list of passages which contain information that is potentially relevant to the user’s question. First, we expect that contextual entailment can be used to select the decompositions of a complex question</context>
</contexts>
<marker>Hickl, Lehmann, Williams, Harabagiu, 2004</marker>
<rawString>Andrew Hickl, John Lehmann, John Williams, and Sanda Harabagiu. 2004. Experiments with Interactive QuestionAnswering in Complex Scenarios. In Proceedings of the Workshop on the Pragmatics of Question Answering at HLT-NAACL 2004, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>Patrick Wang</author>
<author>John Lehmann</author>
<author>Sanda Harabagiu</author>
</authors>
<title>FERRET: Interactive QuestionAnswering for Real-World Environments.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006) Interactive Presentations Program,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="3715" citStr="Hickl et al., 2006" startWordPosition="581" endWordPosition="584">der justify answers returned for questions. In this paper, we propose that scenario-relevant passages in natural language texts can be identified by recognizing a semantic relation, known as contextual entailment (CE), that exists between a text passage and one of a set of subquestions that are conventionally implied by a scenario. Under this model, we expect that a scenario S can be considered to contextually entail a passage t, when there exists at least one subquestion q derived from S that textually entails the passage t. We show that by using a state-of-the-art textual entailment system (Hickl et al., 2006b), we can provide Q/A systems with another mechanism for approximating the inference between questions and relevant answers. We show how each of these cases of con32 Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 32–39, Sydney, July 2006. c�2006 Association for Computational Linguistics textual entailment can be computed and how it can be used in the intrinsic and extrinsic evaluation of a Q/A system. The remainder of the paper is organized in the following way. Section 2 introduces our notion of contextual entailment and provides a framework for recog</context>
<context position="13020" citStr="Hickl et al., 2006" startWordPosition="2113" endWordPosition="2116">bility q. p logit (q) = log 1 q q = b0 + E bjxj + e j=1 We believe that since logistic regression uses a maximum likelihood method, it is a suitable technique for normalizing across range of confidence values output by the TE system. User Scenario Query Extraction Signature Answer Relevant Documents Signature Answer Question/Answer Textual Entailment Topic Signatures Signature Answer Signature Answer Irelevant Documents Scenario Context Signature Answer 34 Figure 4: Textual Entailment Architecture. 3 Recognizing Textual Entailment Recent work in computational semantics (Haghighi et al., 2005; Hickl et al., 2006b; MacCartney et al., 2006) has demonstrated the viability of supervised machine learning-based approaches to the recognition of textual entailment (TE). While these approaches have not incorporated the forms of structured world knowledge featured in many logic-based TE systems, classification-based approaches have been consistently among the top-performing systems in both the 2005 and 2006 PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005), with the best systems (such as (Hickl et al., 2006b)) correctly identifying instances of textual entailment more than 75% of the </context>
<context position="22263" citStr="Hickl et al., 2006" startWordPosition="3559" endWordPosition="3562">es to the scenario context, we have introduced a new dimension to the processing of the scenario. The contextual entailment is based on the textual entailments between any of the texts from the scenario context and any of the candidate answers. 6 Experimental Results In this section, we present preliminary results from four sets of experiments which show how forms of textual – and contextual – entailment can enhance the quality of answers returned by an automatic Q/A system. Questions used in these experiments were gathered from human interactions with the interactive Q/A system described in (Hickl et al., 2006a). A total of 6 users were asked to spend approximately 90 minutes gathering information related to three different information-gathering scenarios similar to the one in Table 1. Each user researched two different scenarios, resulting in a total of 12 total research sessions. Once all research sessions were completed, linguistically well-formed questions were extracted from the system logs for each session for use in our experiments; ungrammatical questions or keyword-style queries were not used in our experiments. Table 2 presents a breakdown of the total number of questions collected for ea</context>
</contexts>
<marker>Hickl, Wang, Lehmann, Harabagiu, 2006</marker>
<rawString>Andrew Hickl, Patrick Wang, John Lehmann, and Sanda Harabagiu. 2006a. FERRET: Interactive QuestionAnswering for Real-World Environments. In Proceedings of the Joint International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006) Interactive Presentations Program, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing Textual Entailment with LCC’s Groundhog System.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="3715" citStr="Hickl et al., 2006" startWordPosition="581" endWordPosition="584">der justify answers returned for questions. In this paper, we propose that scenario-relevant passages in natural language texts can be identified by recognizing a semantic relation, known as contextual entailment (CE), that exists between a text passage and one of a set of subquestions that are conventionally implied by a scenario. Under this model, we expect that a scenario S can be considered to contextually entail a passage t, when there exists at least one subquestion q derived from S that textually entails the passage t. We show that by using a state-of-the-art textual entailment system (Hickl et al., 2006b), we can provide Q/A systems with another mechanism for approximating the inference between questions and relevant answers. We show how each of these cases of con32 Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 32–39, Sydney, July 2006. c�2006 Association for Computational Linguistics textual entailment can be computed and how it can be used in the intrinsic and extrinsic evaluation of a Q/A system. The remainder of the paper is organized in the following way. Section 2 introduces our notion of contextual entailment and provides a framework for recog</context>
<context position="13020" citStr="Hickl et al., 2006" startWordPosition="2113" endWordPosition="2116">bility q. p logit (q) = log 1 q q = b0 + E bjxj + e j=1 We believe that since logistic regression uses a maximum likelihood method, it is a suitable technique for normalizing across range of confidence values output by the TE system. User Scenario Query Extraction Signature Answer Relevant Documents Signature Answer Question/Answer Textual Entailment Topic Signatures Signature Answer Signature Answer Irelevant Documents Scenario Context Signature Answer 34 Figure 4: Textual Entailment Architecture. 3 Recognizing Textual Entailment Recent work in computational semantics (Haghighi et al., 2005; Hickl et al., 2006b; MacCartney et al., 2006) has demonstrated the viability of supervised machine learning-based approaches to the recognition of textual entailment (TE). While these approaches have not incorporated the forms of structured world knowledge featured in many logic-based TE systems, classification-based approaches have been consistently among the top-performing systems in both the 2005 and 2006 PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2005), with the best systems (such as (Hickl et al., 2006b)) correctly identifying instances of textual entailment more than 75% of the </context>
<context position="22263" citStr="Hickl et al., 2006" startWordPosition="3559" endWordPosition="3562">es to the scenario context, we have introduced a new dimension to the processing of the scenario. The contextual entailment is based on the textual entailments between any of the texts from the scenario context and any of the candidate answers. 6 Experimental Results In this section, we present preliminary results from four sets of experiments which show how forms of textual – and contextual – entailment can enhance the quality of answers returned by an automatic Q/A system. Questions used in these experiments were gathered from human interactions with the interactive Q/A system described in (Hickl et al., 2006a). A total of 6 users were asked to spend approximately 90 minutes gathering information related to three different information-gathering scenarios similar to the one in Table 1. Each user researched two different scenarios, resulting in a total of 12 total research sessions. Once all research sessions were completed, linguistically well-formed questions were extracted from the system logs for each session for use in our experiments; ungrammatical questions or keyword-style queries were not used in our experiments. Table 2 presents a breakdown of the total number of questions collected for ea</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006b. Recognizing Textual Entailment with LCC’s Groundhog System. In Proceedings of the Second PASCAL Challenges Workshop, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finley Lacatusu</author>
<author>Andrew Hickl</author>
<author>Kirk Roberts</author>
<author>Ying Shi</author>
<author>Jeremy Bensley</author>
<author>Bryan Rink</author>
<author>Patrick Wang</author>
<author>Lara Taylor</author>
</authors>
<title>LCC’s GISTexter at DUC 2006: Multi-Strategy Multi-Document Summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Document Understanding Conference (DUC</booktitle>
<location>New York, New York.</location>
<contexts>
<context position="19601" citStr="Lacatusu et al., 2006" startWordPosition="3134" endWordPosition="3137">swer set 2 and 3, we produce separate evaluation for each configuration for the contextual entailment. 5 Extrinsic Evaluation of Contextual Entailment Questions asked in response to a user scenario tend to be complex. Following work in (Hickl et al., 2004), we believe complex questions can be answered in one of two ways: either by (1) using techniques (similar to the ones proposed in (Harabagiu et al., 2006)) for automatically decomposing complex questions into sets of informationally-simpler questions, or by (2) using a multi-document summarization (MDS) system (such as the one described in (Lacatusu et al., 2006)) in order to assemble a ranked list of passages which contain information that is potentially relevant to the user’s question. First, we expect that contextual entailment can be used to select the decompositions of a complex question that are most closely related to a scenario. By assigning more confidence to the decompositions that are contextually entailed by a scenario, systems can select a set of answers that are relevant to both the user scenario – and the user’s question. In contrast, contextual entailment can be used in conjunction with the output of a MDS system: once a summary has be</context>
<context position="28366" citStr="Lacatusu et al., 2006" startWordPosition="4604" endWordPosition="4607">at the Q/A system was more likely to return at least one relevant answer among the top-ranked answers when contextual entailment information was used to either rank or select answers. When CE was used to rank passages for Answer Processing (AnsSet2), accuracy increased by nearly 9% over the baseline (AnsSet1), while accuracy increased by almost 14% overall when CE was used to select answers directly (AnsSet3). 6.4 Extrinsic Evaluation In order to evaluate the performance of the framework illustrated in Figure 6, we compared the performance of a question-focused MDS system (first described in (Lacatusu et al., 2006)) that did not use CE with a similar system that used CE to rank passages for a summary answer. When CE was not used, sentences identified by the system’s Q/A and MDS system for each question were combined and ranked based on number of question keywords found in each sentence. In the CE-enabled system (analogous to the system depicted in Figure 6), only the sentences that were contextually entailed by the scenario were considered; sentences were then ranked using the realvalued entailment confidence computed by the CE system for each sentence. Results from this system are presented in Table 6.</context>
</contexts>
<marker>Lacatusu, Hickl, Roberts, Shi, Bensley, Rink, Wang, Taylor, 2006</marker>
<rawString>Finley Lacatusu, Andrew Hickl, Kirk Roberts, Ying Shi, Jeremy Bensley, Bryan Rink, Patrick Wang, and Lara Taylor. 2006. LCC’s GISTexter at DUC 2006: Multi-Strategy Multi-Document Summarization. In Proceedings of the 2006 Document Understanding Conference (DUC 2006), New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lewis</author>
</authors>
<title>Relevant Implication.</title>
<date>1988</date>
<journal>Theoria,</journal>
<volume>54</volume>
<issue>3</issue>
<contexts>
<context position="6090" citStr="Lewis, 1988" startWordPosition="968" endWordPosition="969"> widest possible construal of a user’s information need given S. CASE 1 CASE 2 CASE 3 User Scenario Contextual ?ilEttmenna User Scenario Contextual ?ilEttmenna User Scenario Contextual Answer Contextual Entailment? Entailment? Entailment? Answer Question Answer Entailment? Entailment? Question Question Figure 1: Three types of Contextual Entailment We believe the set of subquestions implied by Qs can be used to test whether a text passage is relevant to S. Since the formal answerhood relation between a question and its answer(s) can be cast in terms of (logical) entailment (Groenendijk, 1999; Lewis, 1988), we believe that systems for recognizing textual entailment (Dagan et al., 2005) could be used in order to identify those text passages that should be considered when gathering information related to a scenario. Based on these assumptions, we expect that the set of text passages that are textually entailed by subquestions derived from a scenario represent information that is more likely to be relevant to the overall topic of the scenario as a whole. We expect that there are three types of contextual entailment relationships that could prove useful for automatic Q/A systems. First, as illustra</context>
</contexts>
<marker>Lewis, 1988</marker>
<rawString>David Lewis. 1988. Relevant Implication. Theoria, 54(3):161–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th COLING Conference,</booktitle>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="1722" citStr="Lin and Hovy, 2000" startWordPosition="270" endWordPosition="273">r information needs independently, the information needs of organizations are often presented in the form of short prose descriptions – known as scenarios – which outline the range of knowledge sought by a customer in order to achieve a specific outcome or to accomplish a particular task. (An example of one scenario is presented in Figure 1.) Recent work in Q/A has sought to use information derived from these kinds of scenarios in order to retrieve sets of answers that are more relevant – and responsive – to a customer’s information needs. While (Harabagiu et al., 2005) used topic signatures (Lin and Hovy, 2000; Scenario Description The customer has commissioned a research project looking at the impact of the outsourcing of American jobs on the United States’ relationship with India. After conducting research on U.S. companies currently doing business in India, the customer wants to know why American corporations have sought to outsource jobs to India, the types of economic advantages that American companies could gain from relocating to India, and the kinds of economic or political inducements that India has offered to American companies looking to outsource jobs there. The customer is not interest</context>
<context position="11442" citStr="Lin and Hovy, 2000" startWordPosition="1844" endWordPosition="1847">ery module, which identifies passages relevant to the concepts mentioned in a scenario, (2) a Textual Entailment module, which recognizes implicational relationships between passages, and (3) a Entailment Merging module, which ranks relevant passages according to their relevance to the scenario itself. In Context Discovery, document retrieval queries are first extracted from each sentence found in a scenario. Once a set of documents has been asMerging Textual Entailment Results Contextual Entailment Decision / Confidence Figure 3: Contextual Entailment Architecture. sembled, topic signatures (Lin and Hovy, 2000; Harabagiu 2004) are computed which identify the set of topic-relevant concepts – and relations between concepts – that are found in the relevant set of documents. Weights associated with each set of topic signatures are then used to extract a set of relevant sentences – referred to as topic answers – from each relevant document. Once a set of topic answers have been identified, each topic answer is paired with a question submitted by a user and sent to the Textual Entailment system described in Section 2. Topic answers that are deemed to be positive entailments of the user question are assig</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th COLING Conference, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<location>New York, New York.</location>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of the Joint Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2006), New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Christine Clark</author>
<author>Sanda Harabagiu</author>
<author>Steve Maiorano</author>
</authors>
<title>COGEX: A Logic Prover for Question Answering.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL-2003.</booktitle>
<contexts>
<context position="2939" citStr="Moldovan et al., 2003" startWordPosition="454" endWordPosition="457">erested in demographic information on Indian employees of American firms. Table 1: Example of a User Scenario. Harabagiu, 2004) computed automatically from collections of documents relevant to a scenario in order to approximate the semantic content of a scenario, (Narayanan and Harabagiu, 2004) employed formal models of the interrelated events, actions, states, and relations implicit to a scenario in order to produce fine-grained, contextsensitive inferences that could be used to answer questions. Scenario knowledge was also included in the form of axiomatic logic transformation developed in (Moldovan et al., 2003). Under this approach, information extracted from the scenario narrative is converted to logical axioms that can used in conjunction with a logic prover in order justify answers returned for questions. In this paper, we propose that scenario-relevant passages in natural language texts can be identified by recognizing a semantic relation, known as contextual entailment (CE), that exists between a text passage and one of a set of subquestions that are conventionally implied by a scenario. Under this model, we expect that a scenario S can be considered to contextually entail a passage t, when the</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>Dan Moldovan, Christine Clark, Sanda Harabagiu, and Steve Maiorano. 2003. COGEX: A Logic Prover for Question Answering. In Proceedings of HLT/NAACL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Question Answering based on Semantic Structures.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-2004.</booktitle>
<contexts>
<context position="2612" citStr="Narayanan and Harabagiu, 2004" startWordPosition="403" endWordPosition="406">customer wants to know why American corporations have sought to outsource jobs to India, the types of economic advantages that American companies could gain from relocating to India, and the kinds of economic or political inducements that India has offered to American companies looking to outsource jobs there. The customer is not interested in demographic information on Indian employees of American firms. Table 1: Example of a User Scenario. Harabagiu, 2004) computed automatically from collections of documents relevant to a scenario in order to approximate the semantic content of a scenario, (Narayanan and Harabagiu, 2004) employed formal models of the interrelated events, actions, states, and relations implicit to a scenario in order to produce fine-grained, contextsensitive inferences that could be used to answer questions. Scenario knowledge was also included in the form of axiomatic logic transformation developed in (Moldovan et al., 2003). Under this approach, information extracted from the scenario narrative is converted to logical axioms that can used in conjunction with a logic prover in order justify answers returned for questions. In this paper, we propose that scenario-relevant passages in natural la</context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>Srini Narayanan and Sanda Harabagiu. 2004. Question Answering based on Semantic Structures. In Proceedings of COLING-2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>