<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.995143">
Learning Relational Features with Backward Random Walks
</title>
<author confidence="0.993236">
Ni Lao Einat Minkov William W. Cohen
</author>
<affiliation confidence="0.995524">
Google Inc. University of Haifa Carnegie Mellon University
</affiliation>
<email confidence="0.99182">
nlao@google.com einatm@is.haifa.ac.il wcohen@cs.cmu.edu
</email>
<sectionHeader confidence="0.99373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999493636363636">
The path ranking algorithm (PRA)
has been recently proposed to address
relational classification and retrieval tasks
at large scale. We describe Cor-PRA,
an enhanced system that can model a
larger space of relational rules, including
longer relational rules and a class of
first order rules with constants, while
maintaining scalability. We describe
and test faster algorithms for searching
for these features. A key contribution
is to leverage backward random walks
to efficiently discover these types of
rules. An empirical study is conducted
on the tasks of graph-based knowledge
base inference, and person named entity
extraction from parsed text. Our results
show that learning paths with constants
improves performance on both tasks, and
that modeling longer paths dramatically
improves performance for the named
entity extraction task.
</bodyText>
<sectionHeader confidence="0.999139" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986912477272727">
Structured knowledge about entities and the
relationships between them can be represented
as an edge-typed graph, and relational learning
methods often base predictions on connectivity
patterns in this graph. One such method is the
Path Ranking Algorithm (PRA), a random-walk
based relational learning and inference framework
due to Lao and Cohen (2010b). PRA is highly
scalable compared with other statistical relational
learning approaches, and can therefore be applied
to perform inference in large knowledge bases
(KBs). Several recent works have applied PRA
to link prediction in semantic KBs, as well as
to learning syntactic relational patterns used in
information extraction from the Web (Lao et al.,
2012; Gardner et al., 2013; Gardner et al., 2014;
Dong et al., 2014).
A typical relational inference problem is
illustrated in Figure 1. Having relational
knowledge represented as a graph, it is desired
to infer additional relations of interest between
entity pairs. For example, one may wish to
infer whether an AthletePlaysInLeague relation
holds between nodes HinesWard and NFL. More
generally, link prediction involves queries of the
form: which entities are linked to a source node s
(HinesWard) over a relation of interest r (e.g., r is
AlthletePlaysInLeague)?
PRA gauges the relevance of a target node t
with respect to the source node s and relation r
based on a set of relation paths (i.e., sequences
of edge labels) that connect the node pair. Each
path Tri is considered as feature, and the value of
feature Tri for an instance (s, t) is the probability of
reaching t from s following path Tri. A classifier
is learned in this feature space, using logistic
regression.
PRA’s candidate paths correspond
closely to a certain class of Horn
clauses: for instance, the path Tr =
(AthletePlaysForTeam, TeamPlaysInLeague),
when used as a feature for the relation
r = AthletePlaysForLeague, corresponds to
the Horn clause
</bodyText>
<construct confidence="0.752536">
AthletePlaysForTeam(s, z) ∧ TeamPlaysInLeague(z, t)
→ AthletePlaysForLeague(s, t)
</construct>
<bodyText confidence="0.999493">
One difference between PRA’s features and
more traditional logical inference is that
random-walk weighting means that not all
inferences instantiated by a clause will be given
the same weight. Another difference is that
PRA is very limited in terms of expressiveness.
In particular, inductive logic programming
</bodyText>
<page confidence="0.976064">
666
</page>
<note confidence="0.989214">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 666–675,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.998388">
Figure 1: An example knowledge graph
</figureCaption>
<bodyText confidence="0.9996425">
(ILP) methods such as FOIL (Quinlan and
Cameron-Jones, 1993) learn first-order Horn
rules that may involve constants. Consider the
following rules as motivating examples.
</bodyText>
<equation confidence="0.980224">
EmployeedByAgent(s, t) n IsA(t, SportsTeam)
—* AthletePlaysForTeam(s, t)
t = NFL —* AthletePlaysForTeam(s, t)
</equation>
<bodyText confidence="0.998964467532467">
The first rule includes SportsTeam as a constant,
corresponding to a particular graph node, which
is a the semantic class (hypernym) of the target
node t. The second rule simply assigns NFL
as the target node for the AthletePlaysForTeam
relation; if used probabilistically, this rule can
serve as a prior. Neither feature can be expressed
in PRA, as PRA features are restricted to edge type
sequences.
We are interested in extending the range of
relational rules that can be represented within the
PRA framework, including rules with constants.
A key challenge is that this greatly increases
the space of candidate rules. Knowledge
bases such as Freebase (Bollacker et al., 2008),
YAGO (Suchanek et al., 2007), or NELL (Carlson
et al., 2010a), may contain thousands of predicates
and millions of concepts. The number of features
involving concepts as constants (even if limited
to simple structures such as the example rules
above) will thus be prohibitively large. Therefore,
it is necessary to search the space of candidate
paths π very efficiently. More efficient candidate
generation is also necessary if one attempts to use
a looser bound on the length of candidate paths.
To achieve this, we propose using backward
random walks. Given target nodes that are
known to be relevant for relation r, we perform
backward random walks (up to finite length E)
originating at these target nodes, where every
graph node c reachable in this random walk
process is considered as a potentially useful
constant. Consequently, the relational paths
that connect nodes c and t are evaluated as
possible random walk features. As we will show,
such paths provide informative class priors for
relational classification tasks.
Concretely, this paper makes the following
contributions. First, we outline and discuss a
new and larger family of relational features that
may be represented in terms of random walks
within the PRA framework. These features
represent paths with constants, expanding the
expressiveness of PRA. In addition, we propose to
encode bi-directional random walk probabilities as
features; we will show that accounting for this sort
of directionality provides useful information about
graph structure.
Second, we describe the learning of this
extended set of paths by means of backward walks
from relevant target nodes. Importantly, the search
and computation of the extended set of features is
performed efficiently, maintaining high scalability
of the framework. Concretely, using backward
walks, one can compute random walk probabilities
in a bi-directional fashion; this means that for
paths of length 2M, the time complexity of path
finding is reduced from O(|V |&apos;M) to O(|V |M),
where |V  |is the number of edge types in graph.
Finally, we report experimental results for
relational inference tasks in two different domains,
including knowledge base link prediction and
person named entity extraction from parsed
text (Minkov and Cohen, 2008). It is shown
that the proposed extensions allow one to
effectively explore a larger feature space,
significantly improving model quality over
previously published results in both domains. In
particular, incorporating paths with constants
significantly improves model quality on
both tasks. Bi-directional walk probability
computation also enables the learning of longer
predicate chains, and the modeling of long paths
is shown to substantially improve performance
on the person name extraction task. Importantly,
learning and inference remain highly efficient in
both these settings.
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9988058">
ILP complexity stems from two main
sources—the complexity of searching for
clauses, and of evaluating them. First-order
learning systems (e.g. FOIL, FOCL (Pazzani et
al., 1991)) mostly rely on hill-climbing search,
</bodyText>
<figure confidence="0.999081214285714">
TeamPlays
InLeague
TeamPlays
InLeague
Giants
MLB
AthletePlays
ForTeam
AthletePlays
ForTeam InLeague NFL
TeamPlays
HinesWard
Steelers
Eli Manning
</figure>
<page confidence="0.987385">
667
</page>
<bodyText confidence="0.9980415">
i.e., incrementally expanding existing patterns
to explore the combinatorial model space, and
are thus often vulnerable to local maxima. PRA
takes another approach, generating features using
efficient random graph walks, and selecting a
subset of those features which pass precision and
frequency thresholds. In this respect, it resembles
a stochastic approach to ILP used in earlier
work (Sebag and Rouveirol, 1997).The idea of
sampling-based inference and induction has been
further explored by later systems (Kuˇzelka and
ˇZelezn´y, 2008; Kuˇzelka and ˇZelezn´y, 2009).
Compared with conventional ILP or relational
learning systems, PRA is limited to learning
from binary predicates, and applies random-walk
semantics to its clauses. Using sampling
strategies (Lao and Cohen, 2010a), the
computation of clause probabilities can be
done in time that is independent of the knowledge
base size, with bounded error rate (Wang et al.,
2013). Unlike in FORTE and similar systems, in
PRA, sampling is also applied to the induction
path-finding stage. The relational feature
construction problem (or propositionalization)
has previously been addressed in the ILP
community—e.g., the RSD system (ˇZelezn´y and
Lavraˇc, 2006) performs explicit first-order feature
construction guided by an precision heuristic
function. In comparison, PRA uses precision and
recall measures, which can be readily read off
from random walk results.
Bi-directional search is a popular strategy
in AI, and in the ILP literature. The
Aleph algorithm (Srinivasan, 2001) combines
top-down with bottom-up search of the refinement
graph, an approach inherited from Progol.
FORTE (Richards and Mooney, 1991) was another
early ILP system which enumerated paths via
a bi-directional seach. Computing backward
random walks for PRA can be seen as a particular
way of bi-directional search, which is also
assigned a random walk probability semantics.
Unlike in prior work, we will use this probability
semantics directly for feature selection.
</bodyText>
<sectionHeader confidence="0.996528" genericHeader="method">
3 Background
</sectionHeader>
<bodyText confidence="0.99936075">
We first review the Path Ranking Algorithm
(PRA) as introduced by (Lao and Cohen, 2010b),
paying special attention to its random walk feature
estimation and selection components.
</bodyText>
<subsectionHeader confidence="0.99981">
3.1 Path Ranking Algorithm
</subsectionHeader>
<bodyText confidence="0.842917631578947">
Given a directed graph G, with nodes N, edges E
and edge types R, we assume that all edges can be
traversed in both directions, and use r−1 to denote
the reverse of edge type r ∈ R. A path type 7r
is defined as a sequence of edge types r1 ... rt.
Such path types may be indicative of an extended
relational meaning between graph nodes that are
linked over these paths; for example, the path
hAtheletePlaysForTeam, TeamPlaysInLeaguei
implies the relationship “the league a certain
player plays for”. PRA encodes P(s → t; 7rj), the
probability of reaching target node t starting from
source node s and following path 7rj, as a feature
that describes the semantic relation between s and
t. Specifically, provided with a set of selected
path types up to length E, Pt = {7r1, ... , 7rm},
the relevancy of target nodes t with respect to the
query node s and the relationship of interest is
evaluated using the following scoring function
</bodyText>
<equation confidence="0.9958415">
�score(s, t) = 0jP(s → t; 7rj), (1)
πj∈P¢
</equation>
<bodyText confidence="0.99998715">
where 0 are appropriate weights for the features,
estimated in the following fashion.
Given a relation of interest r and a set of
annotated node pairs {(s, t)}, for which it is
known whether r(s, t) holds or not, a training
data set D = {(x, y)} is constructed, where
x is a vector of all the path features for the
pair (s, t)—i.e., the j-th component of x is
P(s → t; 7rj), and y is a boolean variable
indicating whether r(s, t) is true. We adopt
the closed-world assumption—a set of relevant
target nodes Gz is specified for every example
source node sz and relation r, and all other nodes
are treated as negative target nodes. A biased
sampling procedure selects only a small subset of
negative samples to be included in the objective
function (Lao and Cohen, 2010b). The parameters
0 are estimated from both positive and negative
examples using a regularized logistic regression
model.
</bodyText>
<subsectionHeader confidence="0.999749">
3.2 PRA Features–Generation and Selection
</subsectionHeader>
<bodyText confidence="0.9998965">
PRA features are of the form P(s → t; 7rj),
denoting the probability of reaching target node t,
originating random walk at node s and following
edge type sequence 7rj. These path probabilities
need to be estimated for every node pair, as part
of both training and inference. High scalability
</bodyText>
<page confidence="0.992164">
668
</page>
<bodyText confidence="0.998533714285714">
is achieved due to efficient path probability
estimation. In addition, feature selection is
applied so as to allow efficient learning and avoid
overfitting.
Concretely, the probability of reaching t from s
following path type 7r can be recursively defined
as
</bodyText>
<equation confidence="0.7900595">
1: P(s —* t; 7r) = P(s —* z; 7r&apos;)P(z —* t; r),
z
</equation>
<bodyText confidence="0.996002727272727">
(2)
where r is the last edge type in path 7r, and 7r&apos;
is its prefix, such that adding r to 7r’ gives 7r.
In the terminal case that 7r’ is the empty path 0,
P(s —* z; 0) is defined to be 1 if s = z, and 0
otherwise. The probability P(z —* t; r) is defined
as 1/|r(z) |if r(z, t), and 0 otherwise, where r(z)
is the set of nodes linked to node z over edge
type r. It has been shown that P(s —* t; 7r)
can be effectively estimated using random walk
sampling techniques, with bounded complexity
and bounded error, for all graph nodes that can be
reached from s over path type 7r (Lao and Cohen,
2010a).
Due to the exponentially large feature space
in relational domains, candidate path features are
first generated using a dedicated particle filtering
path-finding procedure (Lao et al., 2011), which
is informed by training signals. Meaningful
features are then selected using the following
goodness measures, considering path precision
and coverage:
</bodyText>
<equation confidence="0.9996615">
1 1:precision(7r) =
n Z P(sZ —* GZ; 7r), (3)
1: coverage(7r) = I(P(sZ —* GZ; 7r) &gt; 0). (4)
Z
</equation>
<bodyText confidence="0.9998305">
where P(sZ —* GZ; 7r) = EtEGi P(sZ —* t; 7r).
The first measure prefers paths that lead to correct
nodes with high average probability. The second
measure reflects the number of queries for which
some correct node is reached over path 7r. In
order for a path type 7r to be included in the
PRA model, it is required that the respective
scores pass thresholds, precision(7r) &gt; a and
coverage(7r) &gt; h, where the thresholds a and h
are tuned empirically using training data.
</bodyText>
<sectionHeader confidence="0.987109" genericHeader="method">
4 Cor-PRA
</sectionHeader>
<bodyText confidence="0.999801833333333">
We will now describe the enhanced system, which
we call Cor-PRA, for the Constant and Reversed
Path Ranking Algorithm. Our goal is to enrich the
space of relational rules that can be represented
using PRA, while maintaining the scalability of
this framework.
</bodyText>
<subsectionHeader confidence="0.99832">
4.1 Backward random walks
</subsectionHeader>
<bodyText confidence="0.9996759">
We first introduce backward random walks, which
are useful for generating and evaluating the set
of proposed relational path types, including paths
with constants. As discussed in Sec.4.4, the use of
backward random walks also enables the modeling
of long relational paths within Cor-PRA.
A key observation is that the path probability
P(s —* t; 7r) may be computed using forward
random walks (Eq. (2)), or alternatively, it can be
recursively defined in a backward fashion:
</bodyText>
<equation confidence="0.9362785">
1: P (t +— s; 7r) = P(t +— z; 7r&apos;−1)P(z +— s; r−1)
z
</equation>
<bodyText confidence="0.990353583333333">
(5)
where 7r&apos;−1 is the path that results from removing
the last edge type r in 7r&apos;. Here, in the terminal
condition that 7r&apos;−1 = 0, P(t +— z; 7r&apos;−1) is
defined to be 1 for z = t, and 0 otherwise. In
what follows, the starting point of the random
walk calculation is indicated at the left side of
the arrow symbol; i.e., P(s —* t; 7r) denotes the
probability of reaching t from s computed using
forward random walks, and P(t +— s; 7r) denotes
the same probability, computed in a backward
fashion.
</bodyText>
<subsectionHeader confidence="0.966912">
4.2 Relational paths with constants
</subsectionHeader>
<bodyText confidence="0.999992">
As stated before, we wish to model relational
rules that may include constants, denoting related
entities or concepts. Main questions are, how
can relational rules with constants be represented
as path probability features? and, how can
meaningful rules with constants be generated and
selected efficiently?
In order to address the first question, let
us assume that a set of constant nodes {c},
which are known to be useful with respect to
relation r, has been already identified. The
relationship between each constant c and target
node t may be represented in terms of path
probability features, P(c —* t; 7r). For example,
the rule IsA(t, SportsTeam) corresponds to a path
originating at constant SportsTeam, and reaching
target node t over a direct edge typed IsA−1. Such
paths, which are independent of the source node
s, readily represent the semantic type, or other
</bodyText>
<page confidence="0.994333">
669
</page>
<bodyText confidence="0.998751090909091">
characteristic attributes of relevant target nodes.
Similarly, a feature (c, φ), designating a constant
and an empty path, forms a prior for the target
node identity.
The remaining question is how to identify
meaningful constant features. Apriori, candidate
constants range over all of the graph nodes,
and searching for useful paths that originate
at arbitrary constants is generally intractable.
Provided with labeled examples, we apply the
path-finding procedure for this purpose, where
rather than search for high-probability paths from
source node s to target t, paths are explored in
a backward fashion, initiating path search at the
known relevant target nodes t E Gi per each
labeled query. This process identifies candidate
(c, 7r) tuples, which give high P(c +— t; 7r−1)
values, at bounded computation cost. As a second
step, P(c —* t; 7r) feature values are calculated,
where useful path features are selected using the
precision and coverage criteria. Further details are
discussed in Section 4.4.
</bodyText>
<subsectionHeader confidence="0.998954">
4.3 Bi-directional Random Walk Features
</subsectionHeader>
<bodyText confidence="0.967303034482759">
The PRA algorithm only uses features of the form
P(s —* t; 7r). In this study we also consider
graph walk features in the inverse direction of
the form P(s +— t; 7r−1). Similarly, we
consider both P(c —* t; 7r) and P(c +— t; 7r−1).
While these path feature pairs represent the same
logical expressions, the directional random walk
probabilities may greatly differ. For example,
it may be highly likely for a random walker to
reach a target node representing a sports team
t from node s denoting a player over a path 7r
that describes the functional AthletePlaysForTeam
relation, but unlikely to reach a particular player
node s from the multiplayer team t via the reversed
path 7r−1.
In general, there are six types of random walk
probabilities that may be modeled as relational
features following the introduction of constant
paths and inverse path probabilities. The random
walk probabilities between s and constant nodes
c, P(s —* c; 7r) and P(s +— c; 7r), do not directly
affect the ranking of candidate target nodes, so
we do not use them in this study. It is possible,
however, to generate random walk features that
combine these probabilities with random walks
starting or ending with t through conjunction.
Algorithm 1 Cor-PRA Feature Induction1
Input training queries {(si, Gi)}, i = 1...n
for each query (s, G) do
</bodyText>
<sectionHeader confidence="0.445568" genericHeader="method">
1. Path exploration
</sectionHeader>
<bodyText confidence="0.959248">
(i). Apply path-finding to generate paths Ps up to length
f that originate at si.
(ii). Apply path-finding to generate paths Pt up to
length f that originate at every ti ∈ Gi.
</bodyText>
<figure confidence="0.4682524">
2. Calculate random walk probabilities:
for each irs ∈ Ps: do
compute P(s → x; irs) and P(s ← x; ir�1
s )
end for
for each irt ∈ Pt: do
compute P(G → x; irt) and P(G ← x; ir�1
t )
end for
3. Generate constant paths candidates:
for each (x ∈ N, ir ∈ Pt) with P(G → x|irt) &gt; 0 do
propose path feature P(c ← t; ir�1
t ) setting c = x,
and update its statistics by coverage += 1.
end for
</figure>
<equation confidence="0.932512421052632">
for each (x ∈ N, ir ∈ Pt) with P(G ← x|ir�1
t ) &gt; 0
do
propose P(c → t; irt) setting c = x and update its
statistics by coverage += 1
end for
4. Generate long (concatenated) path candidates:
for each (x ∈ N, irs ∈ Ps, irt ∈ Pt) with P(s →
x|irs) &gt; 0 and P(G ← x|ir�1
t ) &gt; 0 do
propose long path P(s → t; irs.ir�1
t ) and update its
statistics by coverage += 1, and precision +=
P(s → x|irs)P(G ← x|ir�1
t )/n.
end for
for each (x ∈ N, irs ∈ Ps, irt ∈ Pt) with P(s ←
x|ir�1
s ) &gt; 0 and P(G → x|irt) &gt; 0 do
</equation>
<table confidence="0.52914">
propose long path P(s ← t; irt.ir�1
s ) and update its
statistics by coverage += 1, and precision +=
P(s ← x|ir�1
s )P(G → x|irt)/n.
</table>
<tableCaption confidence="0.6878865">
end for
end for
</tableCaption>
<subsectionHeader confidence="0.985407">
4.4 Cor-PRA feature induction and selection
</subsectionHeader>
<bodyText confidence="0.99997047368421">
The proposed feature induction procedure is
outlined in Alg. 1. Given labeled node pairs,
the particle-filtering path-finding procedure is first
applied to identify edge type sequences up to
length E that originate at either source nodes si
or relevant target nodes ti (step 1). Bi-directional
path probabilities are then calculated over these
paths, recording the terminal graph nodes x (step
2). Note that since the set of nodes x may be
large, path probabilities are all computed with
respect to s or t as starting points. As a result
of the induction process, candidate relational
paths involving constants are identified, and are
associated with precision and coverage statistics
(step 3). Further, long paths up to length 2E are
formed between the source and target nodes as the
combination of paths 7rs from the source side and
path 7rt from the target side, updating accuracy and
coverage statistics for the concatenated paths 7rs7rt
</bodyText>
<page confidence="0.989725">
670
</page>
<bodyText confidence="0.999228588235294">
(step 4).
Following feature induction, feature selection is
applied. First, random walks are performed for
all the training queries, so as to obtain complete
(rather than sampled) precision and coverage
statistics per path. Then relational paths, which
pass respective tuned thresholds are added to the
model. We found, however, that applying this
strategy for paths with constants often leads to
over-fitting. We therefore select only the top K
constant features in terms of F12, where K is
tuned using training examples.
Finally, at test time, random walk probabilities
are calculated for the selected paths, starting from
either s or c nodes per query–since the identity of
relevant targets t is unknown, but rather has to be
revealed.
</bodyText>
<sectionHeader confidence="0.999601" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.991454172413793">
In this section, we report the results of applying
Cor-PRA to the tasks of knowledge base inference
and person named entity extraction from parsed
text.
We performed 3-fold cross validation
experiments, given datasets of labeled queries.
For each query node in the evaluation set, a list of
graph nodes ranked by their estimated relevancy
to the query node s and relation r is generated.
Ideally, relevant nodes should be ranked at the
top of these lists. Since the number of correct
answers is large for some queries, we report
results in terms of mean average precision (MAP),
a measure that reflects both precision and recall
(Turpin and Scholer, 2006).
The coverage and precision thresholds of
Cor-PRA were set to h = 2 and a = 0.001
in all of the experiments, following empirical
tuning using a small subset of the training data.
The particle filtering path-finding algorithm was
applied using the parameter setting wg = 106, so
as to find useful paths with high probability and
yet constrain the computational cost.
Our results are compared against the FOIL
algorithm3, which learns first-order horn clauses.
In order to evaluate FOIL using MAP, its candidate
beliefs are first ranked by the number of FOIL
rules they match. We further report results
using Random Walks with Restart (RWR), also
</bodyText>
<footnote confidence="0.9684216">
2F1 is the harmonic mean of precision and recall, where
coverage
the latter is defined as
total number targets in training queries
3http://www.rulequest.com/Personal/
</footnote>
<tableCaption confidence="0.990688666666667">
Table 1: MAP and training time [sec] on KB
inference and NE extraction tasks. consti denotes
constant paths up to length i.
</tableCaption>
<table confidence="0.99979125">
KB inference NE extraction
Time MAP Time MAP
RWR 25.6 0.429 7,375 0.017
FOIL 18918.1 0.358 366,558 0.167
PRA 10.2 0.477 277 0.107
CoR-PRA-no-const 16.7 0.479 449 0.167
CoR-PRA-const2 23.3 0.524 556 0.186
CoR-PRA-const3 27.1 0.530 643 0.316
</table>
<bodyText confidence="0.999327466666667">
known as personalized PageRank (Haveliwala,
2002), a popular random walk based graph
similarity measure, that has been shown to be
fairly successful for many types of tasks (e.g.,
(Agirre and Soroa, 2009; Moro et al., 2014)).
Finally, we compare against PRA, which models
relational paths in the form of edge-sequences
(no constants), using only uni-directional path
probabilities, P(s → t; π).
All experiments were run on a machine with a
16 core Intel Xeon 2.33GHz CPU and 24Gb of
memory. All methods are trained and tested with
the same data splits. We report the total training
time of each method, measuring the efficiency of
inference and induction as a whole.
</bodyText>
<subsectionHeader confidence="0.988634">
5.1 Knowledge Base Inference
</subsectionHeader>
<bodyText confidence="0.999886739130435">
We first consider relational inference in the
context of NELL, a semantic knowledge base
constructed by continually extracting facts from
the Web (Carlson et al., 2010b). This work uses
a snapshot of the NELL knowledge base graph,
which consists of —1.6M edges comprised of
353 edge types, and —750K nodes. Following
Lao et al. (2011), we test our approach on 16
link prediction tasks, targeting relations such
as Athlete-plays-in-league, Team-plays-in-league
and Competes-with.
Table 1 reports MAP results and training times
for all of the evaluated methods. The maximum
path length of RWR, PRA, and CoR-PRA are set
to 3 since longer path lengths do not result in better
MAPs. As shown, RWR performance is inferior to
PRA; unlike the other approaches, RWR is merely
associative and does not involve path learning.
PRA is significantly faster than FOIL due to its
particle filtering approach in feature induction
and inference. It also results in a better MAP
performance due to its ability to combine random
walk features in a discriminative model.
</bodyText>
<page confidence="0.988172">
671
</page>
<figure confidence="0.998607526315789">
2 3 4 5
Max Path Length
MAP
0.5
0.4
0.3
0.2
2F
1F+1B
3F
2 F+ 1 B
4F
3F+1
B
2F+2B
3F+2B
2 3 4 5
Max Path Length
(c) (d)
</figure>
<figureCaption confidence="0.918383">
Figure 2: Path finding time (a) and MAP (b)
for the KB inference (top) and name extraction
(bottom) tasks. A marker iF + jB indicates the
maximum path exploration depth i from query
</figureCaption>
<bodyText confidence="0.944213074074074">
node s and j from target node t–so that the
combined path length is up to i + j. No paths with
constants were used.
Table 1 further displays the evaluation results of
several variants of CoR-PRA. As shown, modeling
features that encode random walk probabilities
in both directions (CoR-PRA-no-const), yet no
paths with constants, requires longer training
times, but results in slightly better performance
compared with PRA. Note that for a fixed path
length, CoR-PRA has “forward” features of the
form P(s → t; 7r), the probability of reaching
target node t from source node s over path 7r
(similarly to PRA), as well as backward features
of the form P(s ← t; 7r−1), the probability of
reaching s from t over the backward path 7r−1.
As mentioned earlier these probabilities are not the
same; for example, a player usually plays for one
team, whereas a team is linked to many players.
Performance improves significantly, however,
when paths with constants are further added. The
table includes our results using constant paths
up to length E = 2 and E = 3 (denoted as
CoR-PRA-constt). Based on tuning experiments
on one fold of the data, K = 20 top-rated constant
paths were included in the models.4 We found
that these paths provide informative class priors;
</bodyText>
<footnote confidence="0.8294085">
4MAP performance peaked at roughly K = 20, and
gradually decayed as K increased.
</footnote>
<tableCaption confidence="0.74188675">
Table 2: Example paths with constants learnt for
the knowledge base inference tasks. (φ denotes
empty paths.)
Constant path Interpretation
</tableCaption>
<equation confidence="0.924546846153846">
r=athletePlaysInLeague
P(mlb --+ t; 0) Bias toward MLB.
P(boston braves --+ t; The leagues played by
(athletePlaysForTeam−1, Boston Braves university
athletePlaysInLeague)) team members.
r=competesWith
P(google --+ t; 0) Bias toward Google.
P(google --+ t; Companies which compete
(competesWith, competesWith))with Google’s competitors.
r=teamPlaysInLeague
P(ncaa --+ t; 0) Bias toward NCAA.
P(boise state --+ t; The leagues played by Boise
(teamPlaysInLeague)) State university teams.
</equation>
<bodyText confidence="0.99938052">
example paths and their interpretation are included
in Table 2.
Figure 2(a) shows the effect of increasing the
maximal path length on path finding and selection
time. The leftmost (blue) bars show baseline
performance of PRA, where only forward random
walks are applied. It is clearly demonstrated that
the time spent on path finding grows exponentially
with E. Due to memory limitations, we were
able to execute forward-walk models only up to
4 steps. The bars denoted by iF + jB show
the results of combining forward walks up to
length i with backward walks of up to j = 1 or
j = 2 steps. Time complexity using bidirectional
random walks is dominated by the longest path
segment (either forward or backward)—e.g., the
settings 3F, 3F + 1B, 3F + 2B have similar time
complexity. Using bidirectional search, we were
able to consider relational paths up to length 5.
Figure 2(b) presents MAP performance, where it
is shown that extending the maximal explored path
length did not improve performance in this case.
This result indicates that meaningful paths in this
domain are mostly short. Accordingly, path length
was set to 3 in the respective main experiments.
</bodyText>
<subsectionHeader confidence="0.998982">
5.2 Named Entity Extraction
</subsectionHeader>
<bodyText confidence="0.99977">
We further consider the task of named entity
extraction from a corpus of parsed texts, following
previous work by Minkov and Cohen (2008).
In this case, an entity-relation graph schema is
used to represent a corpus of parsed sentences,
as illustrated in Figure 3. Graph nodes denoting
word mentions (in round edged boxes) are linked
over edges typed with dependency relations. The
</bodyText>
<figure confidence="0.998899">
4 F
3 F
3 F+2 B
2 F+2 B
2 F
2F+1B
3 F+1 B
1 F + 1 B
3 4 5 6
Max Path Length
3 4 5 6
Max Path Length
(a) (b)
0.20
3 F
5 F
4F+2B
3F+3B
4 F
4 F+ 1 B
4 F+2 B
0.15
5F
4F+1B
3F+2B
2 F+ 1 B
3 F+ 1 B
2 F+ 2 B
3 F+2 B
3 F+3 B
MAP
0.10
0.05
3F
2F+1B
4F
3F+1B
2F+2B
0.00
Path Finding Time (s ) 1000
Path Discovery Time (s) 100
10
1
1000
100
10
1
0.1
</figure>
<page confidence="0.994564">
672
</page>
<bodyText confidence="0.998127941176471">
parsed sentence structures are connected via nodes
that denote word lemmas, where every word
lemma is linked to all of its mentions in the
corpus via the special edge type W. We represent
part-of-speech tags as another set of graph nodes,
where word mentions are connected to the relevant
tag over POS edge type.
In this graph, task-specific word similarity
measures can be derived based on the
lexico-syntactic paths that connect word
types (Minkov and Cohen, 2014). The task
defined in the experiments is to retrieve a ranked
list of person names given a small set of seeds.
This task is implemented in the graph as a query,
where we let the query distribution be uniform
over the given seeds (and zero elsewhere). That
is, our goal is to find target nodes that are related
to the query nodes over the relation r =similar-to,
or, coordinate-term. We apply link prediction in
this case with the expected result of generating
a ranked list of graph nodes, which is populated
with many additional person names. The named
entity extraction task we consider is somewhat
similar to the one adopted by FIGER (Ling
and Weld, 2012), in that a finer-grain category
is being assigned to proposed named entities.
Our approach follows however set expansion
settings (Wang and Cohen, 2007), where the goal
is to find new instances of the specified type from
parsed text.
In the experiments, we use the training set
portion of the MUC-6 data set (MUC, 1995),
represented as a graph of 153k nodes and 748K
edges. We generated 30 labeled queries, each
comprised of 4 person names selected randomly
from the person names mentioned in the data
set. The MUC corpus is fully annotated with
entity names, so that relevant target nodes (other
person names) were readily sampled. Extraction
performance was evaluated considering the tagged
person names, which were not included in the
query, as the correct answer set. The maximum
path length of RWR, PRA, and CoR-PRA are set
to 6 due to memory limitation.
Table 1 shows that PRA is much faster
than RWR or FOIL on this data set, giving
competitive MAP performance to FOIL. RWR
is generally ineffective on this task, because
similarity in this domain is represented by a
relatively small set of long paths, whereas
RWR express local node associations in the
</bodyText>
<figureCaption confidence="0.9259695">
Figure 3: Part of a typed graph representing a
corpus of parsed sentences.
</figureCaption>
<tableCaption confidence="0.766603333333333">
Table 3: Highly weighted paths with constants
learnt for the person name extraction task.
Constant path Interpretation
</tableCaption>
<equation confidence="0.990681">
P(said ← t; W−1, nsubj, W) The subjects of ‘said’ or ‘say’
P(says ← t; W−1, nsubj, W) are likely to be a person name.
P(vbg ← t; POS−1, nsubj, W) Subjects, proper nouns, and
P(nnp ← t; POS−1, W) nouns with apposition or
P(nn ← t; POS−1, appos−1, W) possessive constructions, are
P(nn ← t; POS−1, poss, W) likely to be person names.
</equation>
<bodyText confidence="0.999885761904762">
graph (Minkov and Cohen, 2008). Modeling
inverse path probabilities improves performance
substantially, and adding relational features with
constants boosts performance further. The
constant paths learned encode lexical features, as
well as provide useful priors, mainly over different
part-of-speech tags. Example constant paths that
were highly weighted in the learned models and
their interpretation are given in Table 3.
Figure 2(c) shows the effect of modeling long
relational paths using bidirectional random walks
in the language domain. Here, forward path
finding was applied to paths up to length 5 due
to memory limitation. The figure displays the
results of exploring paths up to a total length of
6 edges, performing backward search from the
target nodes of up to j = 1, 2, 3 steps. MAP
performance (Figure 2(d)) using paths of varying
lengths shows significant improvements as the
path length increases. Top weighted long features
include:
</bodyText>
<equation confidence="0.966400333333333">
P(s → t; W−1, conj and−1, W, W−1, conj and, W)
P(s → t; W−1, nn, W, W−1, appos−1, W)
P(s → t; W−1, appos, W, W−1, appos−1, W)
</equation>
<bodyText confidence="0.999921833333333">
These paths are similar to the top ranked paths
found in previous work (Minkov and Cohen,
2008). In comparison, their results on this dataset
using paths of up to 6 steps measured 0.09 in
MAP. Our results reach roughly 0.16 in MAP due
to modeling of inverse paths; and, when constant
</bodyText>
<figure confidence="0.998011272727273">
nnp
POS
POS
W
CEO
CEO
W
nnp
eppos
eppos
POS
POS
SteveJobs
SteveJobs
BillGates
W
W
nsubj
nsubj
founded
founded
W
W
POS
POS
vbd
Tokens
Words/
POSs
Tokens
CEO
BillGates
founded
</figure>
<page confidence="0.995888">
673
</page>
<bodyText confidence="0.991239230769231">
paths are incorporated, MAP reaches 0.32.
Interestingly, in this domain, FOIL generates
fewer yet more complex rules, which are
characterised with low recall and high precision,
such as: W (B, A) n POS(B, nnp) n nsubj(D, B) n
W(D, said) n appos(B, F) → person(A). Note
that subsets of these rules, namely, POS(B, nnp),
nsubj(D, B) ∧ W (D, said) and appos(B, F)
have been discovered by PRA as individual
features assigned with high weights (Table 3).
This indicates an interesting future work, where
products of random walk features can be used to
express their conjunctions.
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999961090909091">
We have introduced CoR-PRA, extending an
existing random walk based relational learning
paradigm to consider relational paths with
constants, bi-directional path features, as well
as long paths. Our experiments on knowledge
base inference and person name extraction tasks
show significant improvements over previously
published results, while maintaining efficiency.
An interesting future direction is to use products
of these random walk features to express their
conjunctions.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999191">
We thank the reviewers for their helpful feedback.
This work was supported in part by BSF grant No.
2010090 and a grant from Google Research.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99826364">
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational
Linguistics.
Kurt Bollacker, Colin Evans, Praveen Paritosh,
Tim Sturge, and Jamie Taylor. 2008. Freebase:
a collaboratively created graph database for
structuring human knowledge. In Proceedings of
the 2008 ACM SIGMOD international conference
on Management of data, pages 1247–1250. ACM.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles,
E. Hruschka Jr., and T. Mitchell. 2010a. Toward
an architecture for never-ending language learning.
In AAAI.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010b. Toward an Architecture for
Never-Ending Language Learning. In AAAI.
Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowledge
vault: a web-scale approach to probabilistic
knowledge fusion. In The 20th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, KDD ’14, New York, NY, USA -
August 24 - 27, 2014, pages 601–610.
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
and Tom Mitchell. 2013. Improving learning and
inference in a large knowledge-base using latent
syntactic cues. In EMNLP.
Matt Gardner, Partha Talukdar, Jayant Krishnamurthy,
and Tom Mitchell. 2014. Incorporating Vector
Space Similarity in Random Walk Inference over
Knowledge Bases. In EMNLP.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank.
In WWW, pages 517–526.
Ond&amp;quot;rej Kuz&amp;quot;elka and Filip &amp;quot;Zelezn´y. 2008. A restarted
strategy for efficient subsumption testing. Fundam.
Inf., 89(1):95–109, January.
Ond&amp;quot;rej Ku&amp;quot;zelka and Filip &amp;quot;Zelezn´y. 2009. Block-wise
construction of acyclic relational features with
monotone irreducibility and relevancy properties.
In Proceedings of the 26th Annual International
Conference on Machine Learning, ICML ’09, pages
569–576, New York, NY, USA. ACM.
Ni Lao and William W. Cohen. 2010a. Fast
query execution for retrieval models based on
path-constrained random walks. In Proceedings of
the 16th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’10,
pages 881–888, New York, NY, USA. ACM.
Ni Lao and William W. Cohen. 2010b. Relational
retrieval using a combination of path-constrained
random walks. In Machine Learning, volume 81,
pages 53–67, July.
Ni Lao, Tom M. Mitchell, and William W. Cohen.
2011. Random Walk Inference and Learning in A
Large Scale Knowledge Base. In EMNLP, pages
529–539.
Ni Lao, Amarnag Subramanya, Fernando Pereira,
and William W. Cohen. 2012. Reading
the web with learned syntactic-semantic inference
rules. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL ’12, pages
1017–1026, Stroudsburg, PA, USA. Association for
Computational Linguistics.
X. Ling and D.S. Weld. 2012. Fine-grained entity
recognition. In Proceedings of the 26th Conference
on Artificial Intelligence (AAAI).
Einat Minkov and William W Cohen. 2008. Learning
Graph Walk Based Similarity Measures for Parsed
Text. EMNLP.
</reference>
<page confidence="0.988484">
674
</page>
<reference confidence="0.999283615384615">
Einat Minkov and William W. Cohen. 2014. Adaptive
graph walk-based similarity measures for parsed
text. Natural Language Engineering, 20(3).
Andrea Moro, Alessandro Raganato, and Roberto
Navigli. 2014. Entity Linking meets Word Sense
Disambiguation: a Unified Approach. Transactions
of the Association for Computational Linguistics
(TACL), 2.
1995. MUC6 ’95: Proceedings of the 6th Conference
on Message Understanding, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael Pazzani, Cliff Brunk, and Glenn Silverstein.
1991. A Knowledge-Intensive Approach to
Learning Relational Concepts. In Proceedings
of the Eighth International Workshop on Machine
Learning, pages 432–436. Morgan Kaufmann.
J. Ross Quinlan and R. Mike Cameron-Jones. 1993.
FOIL: A Midterm Report. In ECML, pages 3–20.
B L Richards and R J Mooney. 1991. First-Order
Theory Revision. In Proceedings of the 8th
International Workshop on Machine Learning,
pages 447–451. Morgan Kaufmann.
Michele Sebag and Celine Rouveirol. 1997. Tractable
induction and classification in first order logic
via stochastic matching. In Proceedings of the
Fifteenth International Joint Conference on Artifical
Intelligence - Volume 2, IJCAI’97, pages 888–893,
San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Ashwin Srinivasan. 2001. The Aleph Manual. In
http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/.
F. Suchanek, G. Kasneci, and G. Weikum. 2007.
YAGO - A Core of Semantic Knowledge. In WWW.
Andrew Turpin and Falk Scholer. 2006. User
performance versus precision measures for
simple search tasks. In PProceedings of the
international ACM SIGIR conference on Research
and development in information retrieval (SIGIR).
Filip ˇZelezn´y and Nada Lavraˇc. 2006.
Propositionalization-based relational subgroup
discovery with rsd. Mach. Learn., 62(1-2):33–63,
February.
Richard C Wang and William W Cohen. 2007.
Language-independent set expansion of named
entities using the web. In Proceedings of the IEEE
International Conference on Data Mining (ICDM).
William Yang Wang, Kathryn Mazaitis, and William W
Cohen. 2013. Programming with personalized
pagerank: A locally groundable first-order
probabilistic logic. Proceedings of the 22nd
ACM International Conference on Information and
Knowledge Management (CIKM 2013).
</reference>
<page confidence="0.998949">
675
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.879837">
<title confidence="0.999959">Learning Relational Features with Backward Random Walks</title>
<author confidence="0.999914">Ni Lao Einat Minkov William W Cohen</author>
<affiliation confidence="0.897977">Google Inc. University of Haifa Carnegie Mellon</affiliation>
<email confidence="0.991524">nlao@google.comeinatm@is.haifa.ac.ilwcohen@cs.cmu.edu</email>
<abstract confidence="0.999452130434782">The path ranking algorithm (PRA) has been recently proposed to address relational classification and retrieval tasks at large scale. We describe Cor-PRA, an enhanced system that can model a larger space of relational rules, including longer relational rules and a class of first order rules with constants, while maintaining scalability. We and test faster algorithms for searching for these features. A key contribution is to leverage backward random walks to efficiently discover these types of rules. An empirical study is conducted on the tasks of graph-based knowledge base inference, and person named entity extraction from parsed text. Our results show that learning paths with constants improves performance on both tasks, and that modeling longer paths dramatically improves performance for the named entity extraction task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing pagerank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23766" citStr="Agirre and Soroa, 2009" startWordPosition="3892" endWordPosition="3895">n training queries 3http://www.rulequest.com/Personal/ Table 1: MAP and training time [sec] on KB inference and NE extraction tasks. consti denotes constant paths up to length i. KB inference NE extraction Time MAP Time MAP RWR 25.6 0.429 7,375 0.017 FOIL 18918.1 0.358 366,558 0.167 PRA 10.2 0.477 277 0.107 CoR-PRA-no-const 16.7 0.479 449 0.167 CoR-PRA-const2 23.3 0.524 556 0.186 CoR-PRA-const3 27.1 0.530 643 0.316 known as personalized PageRank (Haveliwala, 2002), a popular random walk based graph similarity measure, that has been shown to be fairly successful for many types of tasks (e.g., (Agirre and Soroa, 2009; Moro et al., 2014)). Finally, we compare against PRA, which models relational paths in the form of edge-sequences (no constants), using only uni-directional path probabilities, P(s → t; π). All experiments were run on a machine with a 16 core Intel Xeon 2.33GHz CPU and 24Gb of memory. All methods are trained and tested with the same data splits. We report the total training time of each method, measuring the efficiency of inference and induction as a whole. 5.1 Knowledge Base Inference We first consider relational inference in the context of NELL, a semantic knowledge base constructed by con</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,</booktitle>
<pages>1247--1250</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4664" citStr="Bollacker et al., 2008" startWordPosition="698" endWordPosition="701">ponding to a particular graph node, which is a the semantic class (hypernym) of the target node t. The second rule simply assigns NFL as the target node for the AthletePlaysForTeam relation; if used probabilistically, this rule can serve as a prior. Neither feature can be expressed in PRA, as PRA features are restricted to edge type sequences. We are interested in extending the range of relational rules that can be represented within the PRA framework, including rules with constants. A key challenge is that this greatly increases the space of candidate rules. Knowledge bases such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), or NELL (Carlson et al., 2010a), may contain thousands of predicates and millions of concepts. The number of features involving concepts as constants (even if limited to simple structures such as the example rules above) will thus be prohibitively large. Therefore, it is necessary to search the space of candidate paths π very efficiently. More efficient candidate generation is also necessary if one attempts to use a looser bound on the length of candidate paths. To achieve this, we propose using backward random walks. Given target nodes that are known to be rele</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247–1250. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
<author>B Kisiel</author>
<author>B Settles</author>
<author>E Hruschka Jr</author>
<author>T Mitchell</author>
</authors>
<title>Toward an architecture for never-ending language learning.</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="4725" citStr="Carlson et al., 2010" startWordPosition="709" endWordPosition="712">s (hypernym) of the target node t. The second rule simply assigns NFL as the target node for the AthletePlaysForTeam relation; if used probabilistically, this rule can serve as a prior. Neither feature can be expressed in PRA, as PRA features are restricted to edge type sequences. We are interested in extending the range of relational rules that can be represented within the PRA framework, including rules with constants. A key challenge is that this greatly increases the space of candidate rules. Knowledge bases such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), or NELL (Carlson et al., 2010a), may contain thousands of predicates and millions of concepts. The number of features involving concepts as constants (even if limited to simple structures such as the example rules above) will thus be prohibitively large. Therefore, it is necessary to search the space of candidate paths π very efficiently. More efficient candidate generation is also necessary if one attempts to use a looser bound on the length of candidate paths. To achieve this, we propose using backward random walks. Given target nodes that are known to be relevant for relation r, we perform backward random walks (up to </context>
<context position="24426" citStr="Carlson et al., 2010" startWordPosition="4000" endWordPosition="4003">re against PRA, which models relational paths in the form of edge-sequences (no constants), using only uni-directional path probabilities, P(s → t; π). All experiments were run on a machine with a 16 core Intel Xeon 2.33GHz CPU and 24Gb of memory. All methods are trained and tested with the same data splits. We report the total training time of each method, measuring the efficiency of inference and induction as a whole. 5.1 Knowledge Base Inference We first consider relational inference in the context of NELL, a semantic knowledge base constructed by continually extracting facts from the Web (Carlson et al., 2010b). This work uses a snapshot of the NELL knowledge base graph, which consists of —1.6M edges comprised of 353 edge types, and —750K nodes. Following Lao et al. (2011), we test our approach on 16 link prediction tasks, targeting relations such as Athlete-plays-in-league, Team-plays-in-league and Competes-with. Table 1 reports MAP results and training times for all of the evaluated methods. The maximum path length of RWR, PRA, and CoR-PRA are set to 3 since longer path lengths do not result in better MAPs. As shown, RWR performance is inferior to PRA; unlike the other approaches, RWR is merely </context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. Hruschka Jr., and T. Mitchell. 2010a. Toward an architecture for never-ending language learning. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an Architecture for Never-Ending Language Learning.</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="4725" citStr="Carlson et al., 2010" startWordPosition="709" endWordPosition="712">s (hypernym) of the target node t. The second rule simply assigns NFL as the target node for the AthletePlaysForTeam relation; if used probabilistically, this rule can serve as a prior. Neither feature can be expressed in PRA, as PRA features are restricted to edge type sequences. We are interested in extending the range of relational rules that can be represented within the PRA framework, including rules with constants. A key challenge is that this greatly increases the space of candidate rules. Knowledge bases such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), or NELL (Carlson et al., 2010a), may contain thousands of predicates and millions of concepts. The number of features involving concepts as constants (even if limited to simple structures such as the example rules above) will thus be prohibitively large. Therefore, it is necessary to search the space of candidate paths π very efficiently. More efficient candidate generation is also necessary if one attempts to use a looser bound on the length of candidate paths. To achieve this, we propose using backward random walks. Given target nodes that are known to be relevant for relation r, we perform backward random walks (up to </context>
<context position="24426" citStr="Carlson et al., 2010" startWordPosition="4000" endWordPosition="4003">re against PRA, which models relational paths in the form of edge-sequences (no constants), using only uni-directional path probabilities, P(s → t; π). All experiments were run on a machine with a 16 core Intel Xeon 2.33GHz CPU and 24Gb of memory. All methods are trained and tested with the same data splits. We report the total training time of each method, measuring the efficiency of inference and induction as a whole. 5.1 Knowledge Base Inference We first consider relational inference in the context of NELL, a semantic knowledge base constructed by continually extracting facts from the Web (Carlson et al., 2010b). This work uses a snapshot of the NELL knowledge base graph, which consists of —1.6M edges comprised of 353 edge types, and —750K nodes. Following Lao et al. (2011), we test our approach on 16 link prediction tasks, targeting relations such as Athlete-plays-in-league, Team-plays-in-league and Competes-with. Table 1 reports MAP results and training times for all of the evaluated methods. The maximum path length of RWR, PRA, and CoR-PRA are set to 3 since longer path lengths do not result in better MAPs. As shown, RWR performance is inferior to PRA; unlike the other approaches, RWR is merely </context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010b. Toward an Architecture for Never-Ending Language Learning. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Dong</author>
<author>Evgeniy Gabrilovich</author>
<author>Geremy Heitz</author>
<author>Wilko Horn</author>
<author>Ni Lao</author>
<author>Kevin Murphy</author>
<author>Thomas Strohmann</author>
<author>Shaohua Sun</author>
<author>Wei Zhang</author>
</authors>
<title>Knowledge vault: a web-scale approach to probabilistic knowledge fusion.</title>
<date>2014</date>
<booktitle>In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14,</booktitle>
<volume>24</volume>
<pages>601--610</pages>
<location>New York, NY, USA</location>
<contexts>
<context position="1852" citStr="Dong et al., 2014" startWordPosition="268" endWordPosition="271">edictions on connectivity patterns in this graph. One such method is the Path Ranking Algorithm (PRA), a random-walk based relational learning and inference framework due to Lao and Cohen (2010b). PRA is highly scalable compared with other statistical relational learning approaches, and can therefore be applied to perform inference in large knowledge bases (KBs). Several recent works have applied PRA to link prediction in semantic KBs, as well as to learning syntactic relational patterns used in information extraction from the Web (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014; Dong et al., 2014). A typical relational inference problem is illustrated in Figure 1. Having relational knowledge represented as a graph, it is desired to infer additional relations of interest between entity pairs. For example, one may wish to infer whether an AthletePlaysInLeague relation holds between nodes HinesWard and NFL. More generally, link prediction involves queries of the form: which entities are linked to a source node s (HinesWard) over a relation of interest r (e.g., r is AlthletePlaysInLeague)? PRA gauges the relevance of a target node t with respect to the source node s and relation r based on</context>
</contexts>
<marker>Dong, Gabrilovich, Heitz, Horn, Lao, Murphy, Strohmann, Sun, Zhang, 2014</marker>
<rawString>Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge vault: a web-scale approach to probabilistic knowledge fusion. In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, New York, NY, USA -August 24 - 27, 2014, pages 601–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Gardner</author>
<author>Partha Pratim Talukdar</author>
<author>Bryan Kisiel</author>
<author>Tom Mitchell</author>
</authors>
<title>Improving learning and inference in a large knowledge-base using latent syntactic cues.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1810" citStr="Gardner et al., 2013" startWordPosition="260" endWordPosition="263">nd relational learning methods often base predictions on connectivity patterns in this graph. One such method is the Path Ranking Algorithm (PRA), a random-walk based relational learning and inference framework due to Lao and Cohen (2010b). PRA is highly scalable compared with other statistical relational learning approaches, and can therefore be applied to perform inference in large knowledge bases (KBs). Several recent works have applied PRA to link prediction in semantic KBs, as well as to learning syntactic relational patterns used in information extraction from the Web (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014; Dong et al., 2014). A typical relational inference problem is illustrated in Figure 1. Having relational knowledge represented as a graph, it is desired to infer additional relations of interest between entity pairs. For example, one may wish to infer whether an AthletePlaysInLeague relation holds between nodes HinesWard and NFL. More generally, link prediction involves queries of the form: which entities are linked to a source node s (HinesWard) over a relation of interest r (e.g., r is AlthletePlaysInLeague)? PRA gauges the relevance of a target node t with respect to</context>
</contexts>
<marker>Gardner, Talukdar, Kisiel, Mitchell, 2013</marker>
<rawString>Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and Tom Mitchell. 2013. Improving learning and inference in a large knowledge-base using latent syntactic cues. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Gardner</author>
<author>Partha Talukdar</author>
<author>Jayant Krishnamurthy</author>
<author>Tom Mitchell</author>
</authors>
<title>Incorporating Vector Space Similarity in Random Walk Inference over Knowledge Bases. In EMNLP.</title>
<date>2014</date>
<contexts>
<context position="1832" citStr="Gardner et al., 2014" startWordPosition="264" endWordPosition="267"> methods often base predictions on connectivity patterns in this graph. One such method is the Path Ranking Algorithm (PRA), a random-walk based relational learning and inference framework due to Lao and Cohen (2010b). PRA is highly scalable compared with other statistical relational learning approaches, and can therefore be applied to perform inference in large knowledge bases (KBs). Several recent works have applied PRA to link prediction in semantic KBs, as well as to learning syntactic relational patterns used in information extraction from the Web (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014; Dong et al., 2014). A typical relational inference problem is illustrated in Figure 1. Having relational knowledge represented as a graph, it is desired to infer additional relations of interest between entity pairs. For example, one may wish to infer whether an AthletePlaysInLeague relation holds between nodes HinesWard and NFL. More generally, link prediction involves queries of the form: which entities are linked to a source node s (HinesWard) over a relation of interest r (e.g., r is AlthletePlaysInLeague)? PRA gauges the relevance of a target node t with respect to the source node s and</context>
</contexts>
<marker>Gardner, Talukdar, Krishnamurthy, Mitchell, 2014</marker>
<rawString>Matt Gardner, Partha Talukdar, Jayant Krishnamurthy, and Tom Mitchell. 2014. Incorporating Vector Space Similarity in Random Walk Inference over Knowledge Bases. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher H Haveliwala</author>
</authors>
<title>Topic-sensitive pagerank.</title>
<date>2002</date>
<booktitle>In WWW,</booktitle>
<pages>517--526</pages>
<contexts>
<context position="23612" citStr="Haveliwala, 2002" startWordPosition="3868" endWordPosition="3869">andom Walks with Restart (RWR), also 2F1 is the harmonic mean of precision and recall, where coverage the latter is defined as total number targets in training queries 3http://www.rulequest.com/Personal/ Table 1: MAP and training time [sec] on KB inference and NE extraction tasks. consti denotes constant paths up to length i. KB inference NE extraction Time MAP Time MAP RWR 25.6 0.429 7,375 0.017 FOIL 18918.1 0.358 366,558 0.167 PRA 10.2 0.477 277 0.107 CoR-PRA-no-const 16.7 0.479 449 0.167 CoR-PRA-const2 23.3 0.524 556 0.186 CoR-PRA-const3 27.1 0.530 643 0.316 known as personalized PageRank (Haveliwala, 2002), a popular random walk based graph similarity measure, that has been shown to be fairly successful for many types of tasks (e.g., (Agirre and Soroa, 2009; Moro et al., 2014)). Finally, we compare against PRA, which models relational paths in the form of edge-sequences (no constants), using only uni-directional path probabilities, P(s → t; π). All experiments were run on a machine with a 16 core Intel Xeon 2.33GHz CPU and 24Gb of memory. All methods are trained and tested with the same data splits. We report the total training time of each method, measuring the efficiency of inference and indu</context>
</contexts>
<marker>Haveliwala, 2002</marker>
<rawString>Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW, pages 517–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Kuzelka</author>
<author>Filip Zelezn´y</author>
</authors>
<title>A restarted strategy for efficient subsumption testing.</title>
<date>2008</date>
<journal>Fundam. Inf.,</journal>
<volume>89</volume>
<issue>1</issue>
<marker>Kuzelka, Zelezn´y, 2008</marker>
<rawString>Ond&amp;quot;rej Kuz&amp;quot;elka and Filip &amp;quot;Zelezn´y. 2008. A restarted strategy for efficient subsumption testing. Fundam. Inf., 89(1):95–109, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Kuzelka</author>
<author>Filip Zelezn´y</author>
</authors>
<title>Block-wise construction of acyclic relational features with monotone irreducibility and relevancy properties.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09,</booktitle>
<pages>569--576</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Kuzelka, Zelezn´y, 2009</marker>
<rawString>Ond&amp;quot;rej Ku&amp;quot;zelka and Filip &amp;quot;Zelezn´y. 2009. Block-wise construction of acyclic relational features with monotone irreducibility and relevancy properties. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 569–576, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>William W Cohen</author>
</authors>
<title>Fast query execution for retrieval models based on path-constrained random walks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’10,</booktitle>
<pages>881--888</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1427" citStr="Lao and Cohen (2010" startWordPosition="201" endWordPosition="204">erence, and person named entity extraction from parsed text. Our results show that learning paths with constants improves performance on both tasks, and that modeling longer paths dramatically improves performance for the named entity extraction task. 1 Introduction Structured knowledge about entities and the relationships between them can be represented as an edge-typed graph, and relational learning methods often base predictions on connectivity patterns in this graph. One such method is the Path Ranking Algorithm (PRA), a random-walk based relational learning and inference framework due to Lao and Cohen (2010b). PRA is highly scalable compared with other statistical relational learning approaches, and can therefore be applied to perform inference in large knowledge bases (KBs). Several recent works have applied PRA to link prediction in semantic KBs, as well as to learning syntactic relational patterns used in information extraction from the Web (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014; Dong et al., 2014). A typical relational inference problem is illustrated in Figure 1. Having relational knowledge represented as a graph, it is desired to infer additional relations of interes</context>
<context position="8683" citStr="Lao and Cohen, 2010" startWordPosition="1296" endWordPosition="1299"> generating features using efficient random graph walks, and selecting a subset of those features which pass precision and frequency thresholds. In this respect, it resembles a stochastic approach to ILP used in earlier work (Sebag and Rouveirol, 1997).The idea of sampling-based inference and induction has been further explored by later systems (Kuˇzelka and ˇZelezn´y, 2008; Kuˇzelka and ˇZelezn´y, 2009). Compared with conventional ILP or relational learning systems, PRA is limited to learning from binary predicates, and applies random-walk semantics to its clauses. Using sampling strategies (Lao and Cohen, 2010a), the computation of clause probabilities can be done in time that is independent of the knowledge base size, with bounded error rate (Wang et al., 2013). Unlike in FORTE and similar systems, in PRA, sampling is also applied to the induction path-finding stage. The relational feature construction problem (or propositionalization) has previously been addressed in the ILP community—e.g., the RSD system (ˇZelezn´y and Lavraˇc, 2006) performs explicit first-order feature construction guided by an precision heuristic function. In comparison, PRA uses precision and recall measures, which can be re</context>
<context position="10005" citStr="Lao and Cohen, 2010" startWordPosition="1495" endWordPosition="1498">ILP literature. The Aleph algorithm (Srinivasan, 2001) combines top-down with bottom-up search of the refinement graph, an approach inherited from Progol. FORTE (Richards and Mooney, 1991) was another early ILP system which enumerated paths via a bi-directional seach. Computing backward random walks for PRA can be seen as a particular way of bi-directional search, which is also assigned a random walk probability semantics. Unlike in prior work, we will use this probability semantics directly for feature selection. 3 Background We first review the Path Ranking Algorithm (PRA) as introduced by (Lao and Cohen, 2010b), paying special attention to its random walk feature estimation and selection components. 3.1 Path Ranking Algorithm Given a directed graph G, with nodes N, edges E and edge types R, we assume that all edges can be traversed in both directions, and use r−1 to denote the reverse of edge type r ∈ R. A path type 7r is defined as a sequence of edge types r1 ... rt. Such path types may be indicative of an extended relational meaning between graph nodes that are linked over these paths; for example, the path hAtheletePlaysForTeam, TeamPlaysInLeaguei implies the relationship “the league a certain </context>
<context position="11865" citStr="Lao and Cohen, 2010" startWordPosition="1828" endWordPosition="1831">rs {(s, t)}, for which it is known whether r(s, t) holds or not, a training data set D = {(x, y)} is constructed, where x is a vector of all the path features for the pair (s, t)—i.e., the j-th component of x is P(s → t; 7rj), and y is a boolean variable indicating whether r(s, t) is true. We adopt the closed-world assumption—a set of relevant target nodes Gz is specified for every example source node sz and relation r, and all other nodes are treated as negative target nodes. A biased sampling procedure selects only a small subset of negative samples to be included in the objective function (Lao and Cohen, 2010b). The parameters 0 are estimated from both positive and negative examples using a regularized logistic regression model. 3.2 PRA Features–Generation and Selection PRA features are of the form P(s → t; 7rj), denoting the probability of reaching target node t, originating random walk at node s and following edge type sequence 7rj. These path probabilities need to be estimated for every node pair, as part of both training and inference. High scalability 668 is achieved due to efficient path probability estimation. In addition, feature selection is applied so as to allow efficient learning and a</context>
<context position="13229" citStr="Lao and Cohen, 2010" startWordPosition="2079" endWordPosition="2082"> z; 7r&apos;)P(z —* t; r), z (2) where r is the last edge type in path 7r, and 7r&apos; is its prefix, such that adding r to 7r’ gives 7r. In the terminal case that 7r’ is the empty path 0, P(s —* z; 0) is defined to be 1 if s = z, and 0 otherwise. The probability P(z —* t; r) is defined as 1/|r(z) |if r(z, t), and 0 otherwise, where r(z) is the set of nodes linked to node z over edge type r. It has been shown that P(s —* t; 7r) can be effectively estimated using random walk sampling techniques, with bounded complexity and bounded error, for all graph nodes that can be reached from s over path type 7r (Lao and Cohen, 2010a). Due to the exponentially large feature space in relational domains, candidate path features are first generated using a dedicated particle filtering path-finding procedure (Lao et al., 2011), which is informed by training signals. Meaningful features are then selected using the following goodness measures, considering path precision and coverage: 1 1:precision(7r) = n Z P(sZ —* GZ; 7r), (3) 1: coverage(7r) = I(P(sZ —* GZ; 7r) &gt; 0). (4) Z where P(sZ —* GZ; 7r) = EtEGi P(sZ —* t; 7r). The first measure prefers paths that lead to correct nodes with high average probability. The second measure</context>
</contexts>
<marker>Lao, Cohen, 2010</marker>
<rawString>Ni Lao and William W. Cohen. 2010a. Fast query execution for retrieval models based on path-constrained random walks. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’10, pages 881–888, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>William W Cohen</author>
</authors>
<title>Relational retrieval using a combination of path-constrained random walks.</title>
<date>2010</date>
<booktitle>In Machine Learning,</booktitle>
<volume>81</volume>
<pages>53--67</pages>
<contexts>
<context position="1427" citStr="Lao and Cohen (2010" startWordPosition="201" endWordPosition="204">erence, and person named entity extraction from parsed text. Our results show that learning paths with constants improves performance on both tasks, and that modeling longer paths dramatically improves performance for the named entity extraction task. 1 Introduction Structured knowledge about entities and the relationships between them can be represented as an edge-typed graph, and relational learning methods often base predictions on connectivity patterns in this graph. One such method is the Path Ranking Algorithm (PRA), a random-walk based relational learning and inference framework due to Lao and Cohen (2010b). PRA is highly scalable compared with other statistical relational learning approaches, and can therefore be applied to perform inference in large knowledge bases (KBs). Several recent works have applied PRA to link prediction in semantic KBs, as well as to learning syntactic relational patterns used in information extraction from the Web (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014; Dong et al., 2014). A typical relational inference problem is illustrated in Figure 1. Having relational knowledge represented as a graph, it is desired to infer additional relations of interes</context>
<context position="8683" citStr="Lao and Cohen, 2010" startWordPosition="1296" endWordPosition="1299"> generating features using efficient random graph walks, and selecting a subset of those features which pass precision and frequency thresholds. In this respect, it resembles a stochastic approach to ILP used in earlier work (Sebag and Rouveirol, 1997).The idea of sampling-based inference and induction has been further explored by later systems (Kuˇzelka and ˇZelezn´y, 2008; Kuˇzelka and ˇZelezn´y, 2009). Compared with conventional ILP or relational learning systems, PRA is limited to learning from binary predicates, and applies random-walk semantics to its clauses. Using sampling strategies (Lao and Cohen, 2010a), the computation of clause probabilities can be done in time that is independent of the knowledge base size, with bounded error rate (Wang et al., 2013). Unlike in FORTE and similar systems, in PRA, sampling is also applied to the induction path-finding stage. The relational feature construction problem (or propositionalization) has previously been addressed in the ILP community—e.g., the RSD system (ˇZelezn´y and Lavraˇc, 2006) performs explicit first-order feature construction guided by an precision heuristic function. In comparison, PRA uses precision and recall measures, which can be re</context>
<context position="10005" citStr="Lao and Cohen, 2010" startWordPosition="1495" endWordPosition="1498">ILP literature. The Aleph algorithm (Srinivasan, 2001) combines top-down with bottom-up search of the refinement graph, an approach inherited from Progol. FORTE (Richards and Mooney, 1991) was another early ILP system which enumerated paths via a bi-directional seach. Computing backward random walks for PRA can be seen as a particular way of bi-directional search, which is also assigned a random walk probability semantics. Unlike in prior work, we will use this probability semantics directly for feature selection. 3 Background We first review the Path Ranking Algorithm (PRA) as introduced by (Lao and Cohen, 2010b), paying special attention to its random walk feature estimation and selection components. 3.1 Path Ranking Algorithm Given a directed graph G, with nodes N, edges E and edge types R, we assume that all edges can be traversed in both directions, and use r−1 to denote the reverse of edge type r ∈ R. A path type 7r is defined as a sequence of edge types r1 ... rt. Such path types may be indicative of an extended relational meaning between graph nodes that are linked over these paths; for example, the path hAtheletePlaysForTeam, TeamPlaysInLeaguei implies the relationship “the league a certain </context>
<context position="11865" citStr="Lao and Cohen, 2010" startWordPosition="1828" endWordPosition="1831">rs {(s, t)}, for which it is known whether r(s, t) holds or not, a training data set D = {(x, y)} is constructed, where x is a vector of all the path features for the pair (s, t)—i.e., the j-th component of x is P(s → t; 7rj), and y is a boolean variable indicating whether r(s, t) is true. We adopt the closed-world assumption—a set of relevant target nodes Gz is specified for every example source node sz and relation r, and all other nodes are treated as negative target nodes. A biased sampling procedure selects only a small subset of negative samples to be included in the objective function (Lao and Cohen, 2010b). The parameters 0 are estimated from both positive and negative examples using a regularized logistic regression model. 3.2 PRA Features–Generation and Selection PRA features are of the form P(s → t; 7rj), denoting the probability of reaching target node t, originating random walk at node s and following edge type sequence 7rj. These path probabilities need to be estimated for every node pair, as part of both training and inference. High scalability 668 is achieved due to efficient path probability estimation. In addition, feature selection is applied so as to allow efficient learning and a</context>
<context position="13229" citStr="Lao and Cohen, 2010" startWordPosition="2079" endWordPosition="2082"> z; 7r&apos;)P(z —* t; r), z (2) where r is the last edge type in path 7r, and 7r&apos; is its prefix, such that adding r to 7r’ gives 7r. In the terminal case that 7r’ is the empty path 0, P(s —* z; 0) is defined to be 1 if s = z, and 0 otherwise. The probability P(z —* t; r) is defined as 1/|r(z) |if r(z, t), and 0 otherwise, where r(z) is the set of nodes linked to node z over edge type r. It has been shown that P(s —* t; 7r) can be effectively estimated using random walk sampling techniques, with bounded complexity and bounded error, for all graph nodes that can be reached from s over path type 7r (Lao and Cohen, 2010a). Due to the exponentially large feature space in relational domains, candidate path features are first generated using a dedicated particle filtering path-finding procedure (Lao et al., 2011), which is informed by training signals. Meaningful features are then selected using the following goodness measures, considering path precision and coverage: 1 1:precision(7r) = n Z P(sZ —* GZ; 7r), (3) 1: coverage(7r) = I(P(sZ —* GZ; 7r) &gt; 0). (4) Z where P(sZ —* GZ; 7r) = EtEGi P(sZ —* t; 7r). The first measure prefers paths that lead to correct nodes with high average probability. The second measure</context>
</contexts>
<marker>Lao, Cohen, 2010</marker>
<rawString>Ni Lao and William W. Cohen. 2010b. Relational retrieval using a combination of path-constrained random walks. In Machine Learning, volume 81, pages 53–67, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Tom M Mitchell</author>
<author>William W Cohen</author>
</authors>
<title>Random Walk Inference and Learning in A Large Scale Knowledge Base. In</title>
<date>2011</date>
<booktitle>EMNLP,</booktitle>
<pages>529--539</pages>
<contexts>
<context position="13423" citStr="Lao et al., 2011" startWordPosition="2106" endWordPosition="2109">defined to be 1 if s = z, and 0 otherwise. The probability P(z —* t; r) is defined as 1/|r(z) |if r(z, t), and 0 otherwise, where r(z) is the set of nodes linked to node z over edge type r. It has been shown that P(s —* t; 7r) can be effectively estimated using random walk sampling techniques, with bounded complexity and bounded error, for all graph nodes that can be reached from s over path type 7r (Lao and Cohen, 2010a). Due to the exponentially large feature space in relational domains, candidate path features are first generated using a dedicated particle filtering path-finding procedure (Lao et al., 2011), which is informed by training signals. Meaningful features are then selected using the following goodness measures, considering path precision and coverage: 1 1:precision(7r) = n Z P(sZ —* GZ; 7r), (3) 1: coverage(7r) = I(P(sZ —* GZ; 7r) &gt; 0). (4) Z where P(sZ —* GZ; 7r) = EtEGi P(sZ —* t; 7r). The first measure prefers paths that lead to correct nodes with high average probability. The second measure reflects the number of queries for which some correct node is reached over path 7r. In order for a path type 7r to be included in the PRA model, it is required that the respective scores pass t</context>
<context position="24593" citStr="Lao et al. (2011)" startWordPosition="4029" endWordPosition="4032">re run on a machine with a 16 core Intel Xeon 2.33GHz CPU and 24Gb of memory. All methods are trained and tested with the same data splits. We report the total training time of each method, measuring the efficiency of inference and induction as a whole. 5.1 Knowledge Base Inference We first consider relational inference in the context of NELL, a semantic knowledge base constructed by continually extracting facts from the Web (Carlson et al., 2010b). This work uses a snapshot of the NELL knowledge base graph, which consists of —1.6M edges comprised of 353 edge types, and —750K nodes. Following Lao et al. (2011), we test our approach on 16 link prediction tasks, targeting relations such as Athlete-plays-in-league, Team-plays-in-league and Competes-with. Table 1 reports MAP results and training times for all of the evaluated methods. The maximum path length of RWR, PRA, and CoR-PRA are set to 3 since longer path lengths do not result in better MAPs. As shown, RWR performance is inferior to PRA; unlike the other approaches, RWR is merely associative and does not involve path learning. PRA is significantly faster than FOIL due to its particle filtering approach in feature induction and inference. It als</context>
</contexts>
<marker>Lao, Mitchell, Cohen, 2011</marker>
<rawString>Ni Lao, Tom M. Mitchell, and William W. Cohen. 2011. Random Walk Inference and Learning in A Large Scale Knowledge Base. In EMNLP, pages 529–539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Amarnag Subramanya</author>
<author>Fernando Pereira</author>
<author>William W Cohen</author>
</authors>
<title>Reading the web with learned syntactic-semantic inference rules.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>1017--1026</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1788" citStr="Lao et al., 2012" startWordPosition="256" endWordPosition="259">dge-typed graph, and relational learning methods often base predictions on connectivity patterns in this graph. One such method is the Path Ranking Algorithm (PRA), a random-walk based relational learning and inference framework due to Lao and Cohen (2010b). PRA is highly scalable compared with other statistical relational learning approaches, and can therefore be applied to perform inference in large knowledge bases (KBs). Several recent works have applied PRA to link prediction in semantic KBs, as well as to learning syntactic relational patterns used in information extraction from the Web (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014; Dong et al., 2014). A typical relational inference problem is illustrated in Figure 1. Having relational knowledge represented as a graph, it is desired to infer additional relations of interest between entity pairs. For example, one may wish to infer whether an AthletePlaysInLeague relation holds between nodes HinesWard and NFL. More generally, link prediction involves queries of the form: which entities are linked to a source node s (HinesWard) over a relation of interest r (e.g., r is AlthletePlaysInLeague)? PRA gauges the relevance of a target </context>
</contexts>
<marker>Lao, Subramanya, Pereira, Cohen, 2012</marker>
<rawString>Ni Lao, Amarnag Subramanya, Fernando Pereira, and William W. Cohen. 2012. Reading the web with learned syntactic-semantic inference rules. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 1017–1026, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Ling</author>
<author>D S Weld</author>
</authors>
<title>Fine-grained entity recognition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="30618" citStr="Ling and Weld, 2012" startWordPosition="5080" endWordPosition="5083">is to retrieve a ranked list of person names given a small set of seeds. This task is implemented in the graph as a query, where we let the query distribution be uniform over the given seeds (and zero elsewhere). That is, our goal is to find target nodes that are related to the query nodes over the relation r =similar-to, or, coordinate-term. We apply link prediction in this case with the expected result of generating a ranked list of graph nodes, which is populated with many additional person names. The named entity extraction task we consider is somewhat similar to the one adopted by FIGER (Ling and Weld, 2012), in that a finer-grain category is being assigned to proposed named entities. Our approach follows however set expansion settings (Wang and Cohen, 2007), where the goal is to find new instances of the specified type from parsed text. In the experiments, we use the training set portion of the MUC-6 data set (MUC, 1995), represented as a graph of 153k nodes and 748K edges. We generated 30 labeled queries, each comprised of 4 person names selected randomly from the person names mentioned in the data set. The MUC corpus is fully annotated with entity names, so that relevant target nodes (other pe</context>
</contexts>
<marker>Ling, Weld, 2012</marker>
<rawString>X. Ling and D.S. Weld. 2012. Fine-grained entity recognition. In Proceedings of the 26th Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>William W Cohen</author>
</authors>
<title>Learning Graph Walk Based Similarity Measures for Parsed Text.</title>
<date>2008</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="6935" citStr="Minkov and Cohen, 2008" startWordPosition="1051" endWordPosition="1054">rtantly, the search and computation of the extended set of features is performed efficiently, maintaining high scalability of the framework. Concretely, using backward walks, one can compute random walk probabilities in a bi-directional fashion; this means that for paths of length 2M, the time complexity of path finding is reduced from O(|V |&apos;M) to O(|V |M), where |V |is the number of edge types in graph. Finally, we report experimental results for relational inference tasks in two different domains, including knowledge base link prediction and person named entity extraction from parsed text (Minkov and Cohen, 2008). It is shown that the proposed extensions allow one to effectively explore a larger feature space, significantly improving model quality over previously published results in both domains. In particular, incorporating paths with constants significantly improves model quality on both tasks. Bi-directional walk probability computation also enables the learning of longer predicate chains, and the modeling of long paths is shown to substantially improve performance on the person name extraction task. Importantly, learning and inference remain highly efficient in both these settings. 2 Related Work</context>
<context position="28907" citStr="Minkov and Cohen (2008)" startWordPosition="4745" endWordPosition="4748">ard)—e.g., the settings 3F, 3F + 1B, 3F + 2B have similar time complexity. Using bidirectional search, we were able to consider relational paths up to length 5. Figure 2(b) presents MAP performance, where it is shown that extending the maximal explored path length did not improve performance in this case. This result indicates that meaningful paths in this domain are mostly short. Accordingly, path length was set to 3 in the respective main experiments. 5.2 Named Entity Extraction We further consider the task of named entity extraction from a corpus of parsed texts, following previous work by Minkov and Cohen (2008). In this case, an entity-relation graph schema is used to represent a corpus of parsed sentences, as illustrated in Figure 3. Graph nodes denoting word mentions (in round edged boxes) are linked over edges typed with dependency relations. The 4 F 3 F 3 F+2 B 2 F+2 B 2 F 2F+1B 3 F+1 B 1 F + 1 B 3 4 5 6 Max Path Length 3 4 5 6 Max Path Length (a) (b) 0.20 3 F 5 F 4F+2B 3F+3B 4 F 4 F+ 1 B 4 F+2 B 0.15 5F 4F+1B 3F+2B 2 F+ 1 B 3 F+ 1 B 2 F+ 2 B 3 F+2 B 3 F+3 B MAP 0.10 0.05 3F 2F+1B 4F 3F+1B 2F+2B 0.00 Path Finding Time (s ) 1000 Path Discovery Time (s) 100 10 1 1000 100 10 1 0.1 672 parsed senten</context>
<context position="32336" citStr="Minkov and Cohen, 2008" startWordPosition="5375" endWordPosition="5378"> of long paths, whereas RWR express local node associations in the Figure 3: Part of a typed graph representing a corpus of parsed sentences. Table 3: Highly weighted paths with constants learnt for the person name extraction task. Constant path Interpretation P(said ← t; W−1, nsubj, W) The subjects of ‘said’ or ‘say’ P(says ← t; W−1, nsubj, W) are likely to be a person name. P(vbg ← t; POS−1, nsubj, W) Subjects, proper nouns, and P(nnp ← t; POS−1, W) nouns with apposition or P(nn ← t; POS−1, appos−1, W) possessive constructions, are P(nn ← t; POS−1, poss, W) likely to be person names. graph (Minkov and Cohen, 2008). Modeling inverse path probabilities improves performance substantially, and adding relational features with constants boosts performance further. The constant paths learned encode lexical features, as well as provide useful priors, mainly over different part-of-speech tags. Example constant paths that were highly weighted in the learned models and their interpretation are given in Table 3. Figure 2(c) shows the effect of modeling long relational paths using bidirectional random walks in the language domain. Here, forward path finding was applied to paths up to length 5 due to memory limitati</context>
</contexts>
<marker>Minkov, Cohen, 2008</marker>
<rawString>Einat Minkov and William W Cohen. 2008. Learning Graph Walk Based Similarity Measures for Parsed Text. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>William W Cohen</author>
</authors>
<title>Adaptive graph walk-based similarity measures for parsed text.</title>
<date>2014</date>
<journal>Natural Language Engineering,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="29960" citStr="Minkov and Cohen, 2014" startWordPosition="4964" endWordPosition="4967">+ 1 B 2 F+ 2 B 3 F+2 B 3 F+3 B MAP 0.10 0.05 3F 2F+1B 4F 3F+1B 2F+2B 0.00 Path Finding Time (s ) 1000 Path Discovery Time (s) 100 10 1 1000 100 10 1 0.1 672 parsed sentence structures are connected via nodes that denote word lemmas, where every word lemma is linked to all of its mentions in the corpus via the special edge type W. We represent part-of-speech tags as another set of graph nodes, where word mentions are connected to the relevant tag over POS edge type. In this graph, task-specific word similarity measures can be derived based on the lexico-syntactic paths that connect word types (Minkov and Cohen, 2014). The task defined in the experiments is to retrieve a ranked list of person names given a small set of seeds. This task is implemented in the graph as a query, where we let the query distribution be uniform over the given seeds (and zero elsewhere). That is, our goal is to find target nodes that are related to the query nodes over the relation r =similar-to, or, coordinate-term. We apply link prediction in this case with the expected result of generating a ranked list of graph nodes, which is populated with many additional person names. The named entity extraction task we consider is somewhat</context>
</contexts>
<marker>Minkov, Cohen, 2014</marker>
<rawString>Einat Minkov and William W. Cohen. 2014. Adaptive graph walk-based similarity measures for parsed text. Natural Language Engineering, 20(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Alessandro Raganato</author>
<author>Roberto Navigli</author>
</authors>
<title>Entity Linking meets Word Sense Disambiguation: a Unified Approach.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<volume>2</volume>
<contexts>
<context position="23786" citStr="Moro et al., 2014" startWordPosition="3896" endWordPosition="3899">://www.rulequest.com/Personal/ Table 1: MAP and training time [sec] on KB inference and NE extraction tasks. consti denotes constant paths up to length i. KB inference NE extraction Time MAP Time MAP RWR 25.6 0.429 7,375 0.017 FOIL 18918.1 0.358 366,558 0.167 PRA 10.2 0.477 277 0.107 CoR-PRA-no-const 16.7 0.479 449 0.167 CoR-PRA-const2 23.3 0.524 556 0.186 CoR-PRA-const3 27.1 0.530 643 0.316 known as personalized PageRank (Haveliwala, 2002), a popular random walk based graph similarity measure, that has been shown to be fairly successful for many types of tasks (e.g., (Agirre and Soroa, 2009; Moro et al., 2014)). Finally, we compare against PRA, which models relational paths in the form of edge-sequences (no constants), using only uni-directional path probabilities, P(s → t; π). All experiments were run on a machine with a 16 core Intel Xeon 2.33GHz CPU and 24Gb of memory. All methods are trained and tested with the same data splits. We report the total training time of each method, measuring the efficiency of inference and induction as a whole. 5.1 Knowledge Base Inference We first consider relational inference in the context of NELL, a semantic knowledge base constructed by continually extracting </context>
</contexts>
<marker>Moro, Raganato, Navigli, 2014</marker>
<rawString>Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity Linking meets Word Sense Disambiguation: a Unified Approach. Transactions of the Association for Computational Linguistics (TACL), 2.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>MUC6 ’95: Proceedings of the 6th Conference on Message Understanding,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>1995</marker>
<rawString>1995. MUC6 ’95: Proceedings of the 6th Conference on Message Understanding, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Pazzani</author>
<author>Cliff Brunk</author>
<author>Glenn Silverstein</author>
</authors>
<title>A Knowledge-Intensive Approach to Learning Relational Concepts.</title>
<date>1991</date>
<booktitle>In Proceedings of the Eighth International Workshop on Machine Learning,</booktitle>
<pages>432--436</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="7712" citStr="Pazzani et al., 1991" startWordPosition="1161" endWordPosition="1164">blished results in both domains. In particular, incorporating paths with constants significantly improves model quality on both tasks. Bi-directional walk probability computation also enables the learning of longer predicate chains, and the modeling of long paths is shown to substantially improve performance on the person name extraction task. Importantly, learning and inference remain highly efficient in both these settings. 2 Related Work ILP complexity stems from two main sources—the complexity of searching for clauses, and of evaluating them. First-order learning systems (e.g. FOIL, FOCL (Pazzani et al., 1991)) mostly rely on hill-climbing search, TeamPlays InLeague TeamPlays InLeague Giants MLB AthletePlays ForTeam AthletePlays ForTeam InLeague NFL TeamPlays HinesWard Steelers Eli Manning 667 i.e., incrementally expanding existing patterns to explore the combinatorial model space, and are thus often vulnerable to local maxima. PRA takes another approach, generating features using efficient random graph walks, and selecting a subset of those features which pass precision and frequency thresholds. In this respect, it resembles a stochastic approach to ILP used in earlier work (Sebag and Rouveirol, 1</context>
</contexts>
<marker>Pazzani, Brunk, Silverstein, 1991</marker>
<rawString>Michael Pazzani, Cliff Brunk, and Glenn Silverstein. 1991. A Knowledge-Intensive Approach to Learning Relational Concepts. In Proceedings of the Eighth International Workshop on Machine Learning, pages 432–436. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
<author>R Mike Cameron-Jones</author>
</authors>
<title>FOIL: A Midterm Report.</title>
<date>1993</date>
<booktitle>In ECML,</booktitle>
<pages>3--20</pages>
<contexts>
<context position="3764" citStr="Quinlan and Cameron-Jones, 1993" startWordPosition="559" endWordPosition="562"> more traditional logical inference is that random-walk weighting means that not all inferences instantiated by a clause will be given the same weight. Another difference is that PRA is very limited in terms of expressiveness. In particular, inductive logic programming 666 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 666–675, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: An example knowledge graph (ILP) methods such as FOIL (Quinlan and Cameron-Jones, 1993) learn first-order Horn rules that may involve constants. Consider the following rules as motivating examples. EmployeedByAgent(s, t) n IsA(t, SportsTeam) —* AthletePlaysForTeam(s, t) t = NFL —* AthletePlaysForTeam(s, t) The first rule includes SportsTeam as a constant, corresponding to a particular graph node, which is a the semantic class (hypernym) of the target node t. The second rule simply assigns NFL as the target node for the AthletePlaysForTeam relation; if used probabilistically, this rule can serve as a prior. Neither feature can be expressed in PRA, as PRA features are restricted t</context>
</contexts>
<marker>Quinlan, Cameron-Jones, 1993</marker>
<rawString>J. Ross Quinlan and R. Mike Cameron-Jones. 1993. FOIL: A Midterm Report. In ECML, pages 3–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B L Richards</author>
<author>R J Mooney</author>
</authors>
<title>First-Order Theory Revision.</title>
<date>1991</date>
<booktitle>In Proceedings of the 8th International Workshop on Machine Learning,</booktitle>
<pages>447--451</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="9574" citStr="Richards and Mooney, 1991" startWordPosition="1427" endWordPosition="1430">he relational feature construction problem (or propositionalization) has previously been addressed in the ILP community—e.g., the RSD system (ˇZelezn´y and Lavraˇc, 2006) performs explicit first-order feature construction guided by an precision heuristic function. In comparison, PRA uses precision and recall measures, which can be readily read off from random walk results. Bi-directional search is a popular strategy in AI, and in the ILP literature. The Aleph algorithm (Srinivasan, 2001) combines top-down with bottom-up search of the refinement graph, an approach inherited from Progol. FORTE (Richards and Mooney, 1991) was another early ILP system which enumerated paths via a bi-directional seach. Computing backward random walks for PRA can be seen as a particular way of bi-directional search, which is also assigned a random walk probability semantics. Unlike in prior work, we will use this probability semantics directly for feature selection. 3 Background We first review the Path Ranking Algorithm (PRA) as introduced by (Lao and Cohen, 2010b), paying special attention to its random walk feature estimation and selection components. 3.1 Path Ranking Algorithm Given a directed graph G, with nodes N, edges E a</context>
</contexts>
<marker>Richards, Mooney, 1991</marker>
<rawString>B L Richards and R J Mooney. 1991. First-Order Theory Revision. In Proceedings of the 8th International Workshop on Machine Learning, pages 447–451. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Sebag</author>
<author>Celine Rouveirol</author>
</authors>
<title>Tractable induction and classification in first order logic via stochastic matching.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifteenth International Joint Conference on Artifical Intelligence - Volume 2, IJCAI’97,</booktitle>
<pages>888--893</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="8316" citStr="Sebag and Rouveirol, 1997" startWordPosition="1245" endWordPosition="1248">(Pazzani et al., 1991)) mostly rely on hill-climbing search, TeamPlays InLeague TeamPlays InLeague Giants MLB AthletePlays ForTeam AthletePlays ForTeam InLeague NFL TeamPlays HinesWard Steelers Eli Manning 667 i.e., incrementally expanding existing patterns to explore the combinatorial model space, and are thus often vulnerable to local maxima. PRA takes another approach, generating features using efficient random graph walks, and selecting a subset of those features which pass precision and frequency thresholds. In this respect, it resembles a stochastic approach to ILP used in earlier work (Sebag and Rouveirol, 1997).The idea of sampling-based inference and induction has been further explored by later systems (Kuˇzelka and ˇZelezn´y, 2008; Kuˇzelka and ˇZelezn´y, 2009). Compared with conventional ILP or relational learning systems, PRA is limited to learning from binary predicates, and applies random-walk semantics to its clauses. Using sampling strategies (Lao and Cohen, 2010a), the computation of clause probabilities can be done in time that is independent of the knowledge base size, with bounded error rate (Wang et al., 2013). Unlike in FORTE and similar systems, in PRA, sampling is also applied to the</context>
</contexts>
<marker>Sebag, Rouveirol, 1997</marker>
<rawString>Michele Sebag and Celine Rouveirol. 1997. Tractable induction and classification in first order logic via stochastic matching. In Proceedings of the Fifteenth International Joint Conference on Artifical Intelligence - Volume 2, IJCAI’97, pages 888–893, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashwin Srinivasan</author>
</authors>
<title>The Aleph Manual.</title>
<date>2001</date>
<note>In http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/.</note>
<contexts>
<context position="9440" citStr="Srinivasan, 2001" startWordPosition="1410" endWordPosition="1411">g et al., 2013). Unlike in FORTE and similar systems, in PRA, sampling is also applied to the induction path-finding stage. The relational feature construction problem (or propositionalization) has previously been addressed in the ILP community—e.g., the RSD system (ˇZelezn´y and Lavraˇc, 2006) performs explicit first-order feature construction guided by an precision heuristic function. In comparison, PRA uses precision and recall measures, which can be readily read off from random walk results. Bi-directional search is a popular strategy in AI, and in the ILP literature. The Aleph algorithm (Srinivasan, 2001) combines top-down with bottom-up search of the refinement graph, an approach inherited from Progol. FORTE (Richards and Mooney, 1991) was another early ILP system which enumerated paths via a bi-directional seach. Computing backward random walks for PRA can be seen as a particular way of bi-directional search, which is also assigned a random walk probability semantics. Unlike in prior work, we will use this probability semantics directly for feature selection. 3 Background We first review the Path Ranking Algorithm (PRA) as introduced by (Lao and Cohen, 2010b), paying special attention to its</context>
</contexts>
<marker>Srinivasan, 2001</marker>
<rawString>Ashwin Srinivasan. 2001. The Aleph Manual. In http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Suchanek</author>
<author>G Kasneci</author>
<author>G Weikum</author>
</authors>
<title>YAGO - A Core of Semantic Knowledge.</title>
<date>2007</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="4694" citStr="Suchanek et al., 2007" startWordPosition="703" endWordPosition="706">ode, which is a the semantic class (hypernym) of the target node t. The second rule simply assigns NFL as the target node for the AthletePlaysForTeam relation; if used probabilistically, this rule can serve as a prior. Neither feature can be expressed in PRA, as PRA features are restricted to edge type sequences. We are interested in extending the range of relational rules that can be represented within the PRA framework, including rules with constants. A key challenge is that this greatly increases the space of candidate rules. Knowledge bases such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), or NELL (Carlson et al., 2010a), may contain thousands of predicates and millions of concepts. The number of features involving concepts as constants (even if limited to simple structures such as the example rules above) will thus be prohibitively large. Therefore, it is necessary to search the space of candidate paths π very efficiently. More efficient candidate generation is also necessary if one attempts to use a looser bound on the length of candidate paths. To achieve this, we propose using backward random walks. Given target nodes that are known to be relevant for relation r, we perfor</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>F. Suchanek, G. Kasneci, and G. Weikum. 2007. YAGO - A Core of Semantic Knowledge. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Turpin</author>
<author>Falk Scholer</author>
</authors>
<title>User performance versus precision measures for simple search tasks.</title>
<date>2006</date>
<booktitle>In PProceedings of the international ACM SIGIR conference on Research and development in information retrieval (SIGIR).</booktitle>
<contexts>
<context position="22386" citStr="Turpin and Scholer, 2006" startWordPosition="3670" endWordPosition="3673"> results of applying Cor-PRA to the tasks of knowledge base inference and person named entity extraction from parsed text. We performed 3-fold cross validation experiments, given datasets of labeled queries. For each query node in the evaluation set, a list of graph nodes ranked by their estimated relevancy to the query node s and relation r is generated. Ideally, relevant nodes should be ranked at the top of these lists. Since the number of correct answers is large for some queries, we report results in terms of mean average precision (MAP), a measure that reflects both precision and recall (Turpin and Scholer, 2006). The coverage and precision thresholds of Cor-PRA were set to h = 2 and a = 0.001 in all of the experiments, following empirical tuning using a small subset of the training data. The particle filtering path-finding algorithm was applied using the parameter setting wg = 106, so as to find useful paths with high probability and yet constrain the computational cost. Our results are compared against the FOIL algorithm3, which learns first-order horn clauses. In order to evaluate FOIL using MAP, its candidate beliefs are first ranked by the number of FOIL rules they match. We further report result</context>
</contexts>
<marker>Turpin, Scholer, 2006</marker>
<rawString>Andrew Turpin and Falk Scholer. 2006. User performance versus precision measures for simple search tasks. In PProceedings of the international ACM SIGIR conference on Research and development in information retrieval (SIGIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip ˇZelezn´y</author>
<author>Nada Lavraˇc</author>
</authors>
<title>Propositionalization-based relational subgroup discovery with rsd.</title>
<date>2006</date>
<journal>Mach. Learn.,</journal>
<pages>62--1</pages>
<marker>ˇZelezn´y, Lavraˇc, 2006</marker>
<rawString>Filip ˇZelezn´y and Nada Lavraˇc. 2006. Propositionalization-based relational subgroup discovery with rsd. Mach. Learn., 62(1-2):33–63, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Wang</author>
<author>William W Cohen</author>
</authors>
<title>Language-independent set expansion of named entities using the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Conference on Data Mining (ICDM).</booktitle>
<contexts>
<context position="30771" citStr="Wang and Cohen, 2007" startWordPosition="5103" endWordPosition="5106">bution be uniform over the given seeds (and zero elsewhere). That is, our goal is to find target nodes that are related to the query nodes over the relation r =similar-to, or, coordinate-term. We apply link prediction in this case with the expected result of generating a ranked list of graph nodes, which is populated with many additional person names. The named entity extraction task we consider is somewhat similar to the one adopted by FIGER (Ling and Weld, 2012), in that a finer-grain category is being assigned to proposed named entities. Our approach follows however set expansion settings (Wang and Cohen, 2007), where the goal is to find new instances of the specified type from parsed text. In the experiments, we use the training set portion of the MUC-6 data set (MUC, 1995), represented as a graph of 153k nodes and 748K edges. We generated 30 labeled queries, each comprised of 4 person names selected randomly from the person names mentioned in the data set. The MUC corpus is fully annotated with entity names, so that relevant target nodes (other person names) were readily sampled. Extraction performance was evaluated considering the tagged person names, which were not included in the query, as the </context>
</contexts>
<marker>Wang, Cohen, 2007</marker>
<rawString>Richard C Wang and William W Cohen. 2007. Language-independent set expansion of named entities using the web. In Proceedings of the IEEE International Conference on Data Mining (ICDM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Yang Wang</author>
<author>Kathryn Mazaitis</author>
<author>William W Cohen</author>
</authors>
<title>Programming with personalized pagerank: A locally groundable first-order probabilistic logic.</title>
<date>2013</date>
<booktitle>Proceedings of the 22nd ACM International Conference on Information and Knowledge Management (CIKM</booktitle>
<contexts>
<context position="8838" citStr="Wang et al., 2013" startWordPosition="1322" endWordPosition="1325">pect, it resembles a stochastic approach to ILP used in earlier work (Sebag and Rouveirol, 1997).The idea of sampling-based inference and induction has been further explored by later systems (Kuˇzelka and ˇZelezn´y, 2008; Kuˇzelka and ˇZelezn´y, 2009). Compared with conventional ILP or relational learning systems, PRA is limited to learning from binary predicates, and applies random-walk semantics to its clauses. Using sampling strategies (Lao and Cohen, 2010a), the computation of clause probabilities can be done in time that is independent of the knowledge base size, with bounded error rate (Wang et al., 2013). Unlike in FORTE and similar systems, in PRA, sampling is also applied to the induction path-finding stage. The relational feature construction problem (or propositionalization) has previously been addressed in the ILP community—e.g., the RSD system (ˇZelezn´y and Lavraˇc, 2006) performs explicit first-order feature construction guided by an precision heuristic function. In comparison, PRA uses precision and recall measures, which can be readily read off from random walk results. Bi-directional search is a popular strategy in AI, and in the ILP literature. The Aleph algorithm (Srinivasan, 200</context>
</contexts>
<marker>Wang, Mazaitis, Cohen, 2013</marker>
<rawString>William Yang Wang, Kathryn Mazaitis, and William W Cohen. 2013. Programming with personalized pagerank: A locally groundable first-order probabilistic logic. Proceedings of the 22nd ACM International Conference on Information and Knowledge Management (CIKM 2013).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>