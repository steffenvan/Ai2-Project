<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008737">
<note confidence="0.816212">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 91-94, Lisbon, Portugal, 2000.
</note>
<title confidence="0.995306">
Inducing Syntactic Categories by Context Distribution Clustering
</title>
<author confidence="0.991562">
Alexander Clark
</author>
<affiliation confidence="0.9945865">
School of Cognitive and Computing Sciences
University of Sussex
</affiliation>
<email confidence="0.993864">
alexc@cogs.susx.ac.uk
</email>
<sectionHeader confidence="0.998574" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940142857143">
This paper addresses the issue of the automatic
induction of syntactic categories from unanno-
tated corpora. Previous techniques give good
results, but fail to cope well with ambiguity or
rare words. An algorithm, context distribution
clustering (CDC), is presented which can be
naturally extended to handle these problems.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999848434782609">
In this paper I present a novel program that in-
duces syntactic categories from comparatively
small corpora of unlabelled text, using only dis-
tributional information. There are various mo-
tivations for this task, which affect the algo-
rithms employed. Many NLP systems use a
set of tags, largely syntactic in motivation, that
have been selected according to various criteria.
In many circumstances it would be desirable for
engineering reasons to generate a larger set of
tags, or a set of domain-specific tags for a par-
ticular corpus. Furthermore, the construction
of cognitive models of language acquisition —
that will almost certainly involve some notion
of syntactic category — requires an explanation
of the acquisition of that set of syntactic cate-
gories. The amount of data used in this study
is 12 million words, which is consistent with a
pessimistic lower bound on the linguistic experi-
ence of the infant language learner in the period
from 2 to 5 years of age, and has had capitalisa-
tion removed as being information not available
in that circumstance.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999669578947369">
Previous work falls into two categories. A num-
ber of researchers have obtained good results
using pattern recognition techniques. Finch
and Chater (1992), (1995) and Schiitze (1993),
(1997) use a set of features derived from the
co-occurrence statistics of common words to-
gether with standard clustering and information
extraction techniques. For sufficiently frequent
words this method produces satisfactory results.
Brown et al. (1992) use a very large amount
of data, and a well-founded information theo-
retic model to induce large numbers of plausi-
ble semantic and syntactic clusters. Both ap-
proaches have two flaws: they cannot deal well
with ambiguity, though Schiitze addresses this
issue partially, and they do not cope well with
rare words. Since rare and ambiguous words are
very common in natural language, these limita-
tions are serious.
</bodyText>
<sectionHeader confidence="0.992803" genericHeader="method">
3 Context Distributions
</sectionHeader>
<bodyText confidence="0.999864590909091">
Whereas earlier methods all share the same ba-
sic intuition, i.e. that similar words occur in
similar contexts, I formalise this in a slightly
different way: each word defines a probability
distribution over all contexts, namely the prob-
ability of the context given the word. If the
context is restricted to the word on either side,
I can define the context distribution to be a dis-
tribution over all ordered pairs of words: the
word before and the word after. The context
distribution of a word can be estimated from
the observed contexts in a corpus. We can then
measure the similarity of words by the simi-
larity of their context distributions, using the
Kullback-Leibler (KL) divergence as a distance
function.
Unfortunately it is not possible to cluster
based directly on the context distributions for
two reasons: first the data is too sparse to es-
timate the context distributions adequately for
any but the most frequent words, and secondly
some words which intuitively are very similar
</bodyText>
<page confidence="0.997992">
91
</page>
<bodyText confidence="0.95018075">
(Schi_itze&apos;s example is &apos;a&apos; and &apos;an&apos;) have rad-
ically different context distributions. Both of
these problems can be overcome in the normal
way by using clusters: approximate the context
distribution as being a probability distribution
over ordered pairs of clusters multiplied by the
conditional distributions of the words given the
clusters :
</bodyText>
<equation confidence="0.993951">
P(&lt; w1, w2 &gt;) =p(&lt; Cl, C2 &gt;)P(wi ici)P(w21c2)
</equation>
<bodyText confidence="0.99973796969697">
I use an iterative algorithm, starting with a
trivial clustering, with each of the K clusters
filled with the kth most frequent word in the
corpus. At each iteration, I calculate the con-
text distribution of each cluster, which is the
weighted average of the context distributions of
each word in the cluster. The distribution is cal-
culated with respect to the K current clusters
and a further ground cluster of all unclassified
words: each distribution therefore has (K + 1)2
parameters. For every word that occurs more
than 50 times in the corpus, I calculate the con-
text distribution, and then find the cluster with
the lowest KL divergence from that distribution.
I then sort the words by the divergence from
the cluster that is closest to them, and select
the best as being the members of the cluster
for the next iteration. This is repeated, grad-
ually increasing the number of words included
at each iteration, until a high enough propor-
tion has been clustered, for example 80%. Af-
ter each iteration, if the distance between two
clusters falls below a threshhold value, the clus-
ters are merged, and a new cluster is formed
from the most frequent unclustered word. Since
there will be zeroes in the context distributions,
they are smoothed using Good-Turing smooth-
ing(Good, 1953) to avoid singularities in the KL
divergence. At this point we have a preliminary
clustering — no very rare words will be included,
and some common words will also not be as-
signed, because they are ambiguous or have id-
iosyncratic distributional properties.
</bodyText>
<sectionHeader confidence="0.939198" genericHeader="method">
4 Ambiguity and Sparseness
</sectionHeader>
<bodyText confidence="0.9996155">
Ambiguity can be handled naturally within
this framework. The context distribution p(w)
of a particular ambiguous word w can be
modelled as a linear combination of the con-
text distributions of the various clusters. We
can find the mixing coefficients by minimising
</bodyText>
<equation confidence="0.937294">
D (p(w) E qz) where the cc(w) are some co-
</equation>
<bodyText confidence="0.99998980952381">
efficients that sum to unity and the qi are the
context distributions of the clusters. A mini-
mum of this function can be found using the
EM algorithm(Dempster et al., 1977). There
are often several local minima — in practice this
does not seem to be a major problem.
Note that with rare words, the KL divergence
reduces to the log likelihood of the word&apos;s con-
text distribution plus a constant factor. How-
ever, the observed context distributions of rare
words may be insufficient to make a definite de-
termination of its cluster membership. In this
case, under the assumption that the word is
unambiguous, which is only valid for compar-
atively rare words, we can use Bayes&apos;s rule to
calculate the posterior probability that it is in
each class, using as a prior probability the dis-
tribution of rare words in each class. This in-
corporates the fact that rare words are much
more likely to be adjectives or nouns than, for
example, pronouns.
</bodyText>
<sectionHeader confidence="0.999731" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999924592592593">
I used 12 million words of the British Na-
tional Corpus as training data, and ran this al-
gorithm with various numbers of clusters (77,
100 and 150). All of the results in this paper
are produced with 77 clusters corresponding to
the number of tags in the CLAWS tagset used
to tag the BNC, plus a distinguished sentence
boundary token. In each case, the clusters in-
duced contained accurate classes corresponding
to the major syntactic categories, and various
subgroups of them such as prepositional verbs,
first names, last names and so on. Appendix A
shows the five most frequent words in a cluster-
ing with 77 clusters. In general, as can be seen,
the clusters correspond to traditional syntactic
classes. There are a few errors — notably, the
right bracket is classified with adverbial parti-
cles like &amp;quot;UP&amp;quot;.
For each word w, I then calculated the opti-
mal coefficents crtv). Table 1 shows some sam-
ple ambiguous words, together with the clusters
with largest values of ai. Each cluster is repre-
sented by the most frequent member of the clus-
ter. Note that &amp;quot;US&amp;quot; is a proper noun cluster.
As there is more than one common noun clus-
ter, for many unambiguous nouns the optimum
is a mixture of the various classes.
</bodyText>
<page confidence="0.974596">
92
</page>
<table confidence="0.999315428571429">
Word Clusters
ROSE CAME CHARLES GROUP
VAN JOHN TIME GROUP
MAY WILL US JOHN
US YOU US NEW
HER THE YOU LAST
THIS THE IT
</table>
<tableCaption confidence="0.996574333333333">
Table 1: Ambiguous words. For each word, the
clusters that have the highest a are shown, if
a &gt; 0.01.
</tableCaption>
<table confidence="0.999875625">
Model CDC Brown CDC Brown
Freq NN1 NN1 AJO AJO
1 0.66 0.21 0.77 0.41
2 0.64 0.27 0.77 0.58
3 0.68 0.36 0.82 0.73
5 0.69 0.40 0.83 0.81
10 0.72 0.50 0.92 0.94
20 0.73 0.61 0.91 0.94
</table>
<tableCaption confidence="0.996183">
Table 2: Accuracy of classification of rare words
</tableCaption>
<bodyText confidence="0.990754551724138">
with tags NN1 (common noun) and AJO (adjec-
tive).
Table 2 shows the accuracy of cluster assign-
ment for rare words. For two CLAWS tags, AJO
(adjective) and NN1(singular common noun)
that occur frequently among rare words in the
corpus, I selected all of the words that oc-
curred n times in the corpus, and at least half
the time had that CLAWS tag. I then tested
the accuracy of my assignment algorithm by
marking it as correct if it assigned the word
to a &apos;plausible&apos; cluster — for AJO, either of the
clusters &amp;quot;NEW&amp;quot; or &amp;quot;IMPORTANT&amp;quot;, and for
NN1, one of the clusters &amp;quot;TIME&amp;quot;, &amp;quot;PEOPLE&amp;quot;,
&amp;quot;WORLD&amp;quot;, &amp;quot;GROUP&amp;quot; or &amp;quot;FACT&amp;quot;. I did this
for n in {1, 2, 3, 5, 10, 20}. I proceeded similarly
for the Brown clustering algorithm, selecting
two clusters for NN1 and four for AJO. This can
only be approximate, since the choice of accept-
able clusters is rather arbitrary, and the BNC
tags are not perfectly accurate, but the results
are quite clear; for words that occur 5 times or
less the CDC algorithm is clearly more accurate.
Evaluation is in general difficult with unsu-
pervised learning algorithms. Previous authors
have relied on both informal evaluations of the
plausibility of the classes produced, and more
formal statistical methods. Comparison against
existing tag-sets is not meaningful — one set of
</bodyText>
<table confidence="0.988554">
Test set 1 2 3 4 Mean
CLAWS 411 301 478 413 395
Brown et al. 380 252 444 369 354
CDC 372 255 427 354 346
</table>
<tableCaption confidence="0.969297666666667">
Table 3: Perplexities of class tri-gram models
on 4 test sets of 100,000 words, together with
geometric mean.
</tableCaption>
<bodyText confidence="0.999797607142857">
tags chosen by linguists would score very badly
against another without this implying any fault
as there is no &apos;gold standard&apos;. I therefore chose
to use an objective statistical measure, the per-
plexity of a very simple finite state model, to
compare the tags generated with this cluster-
ing technique against the BNC tags, which uses
the CLAWS-4 tag set (Leech et al., 1994) which
had 76 tags. I tagged 12 million words of BNC
text with the 77 tags, assigning each word to
the cluster with the highest a posteriori proba-
bility given its prior cluster distribution and its
context.
I then trained 2nd-order Markov models
(equivalently class trigram models) on the orig-
inal BNC tags, on the outputs from my algo-
rithm (CDC), and for comparision on the out-
put from the Brown algorithm. The perplexities
on held-out data are shown in table 3. As can
be seen, the perplexity is lower with the model
trained on data tagged with the new algorithm.
This does not imply that the new tagset is bet-
ter; it merely shows that it is capturing statisti-
cal significant generalisations. In absolute terms
the perplexities are rather high; I deliberately
chose a rather crude model without backing off
and only the minimum amount of smoothing,
which I felt might sharpen the contrast.
</bodyText>
<sectionHeader confidence="0.993233" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99907475">
The work of Chater and Finch can be seen as
similar to the work presented here given an in-
dependence assumption. We can model the con-
text distribution as being the product of inde-
pendent distributions for each relative position;
in this case the KL divergence is the sum of
the divergences for each independent distribu-
tion. This independence assumption is most
clearly false when the word is ambiguous; this
perhaps explains the poor performance of these
algorithms with ambiguous words. The new
algorithm currently does not use information
</bodyText>
<page confidence="0.997227">
93
</page>
<bodyText confidence="0.999950909090909">
about the orthography of the word, an impor-
tant source of information. In future work, I will
integrate this with a morphology-learning pro-
gram. I am currently applying this approach
to the induction of phrase structure rules, and
preliminary experiments have shown encourag-
ing results.
In summary, the new method avoids the limi-
tations of other approaches, and is better suited
to integration into a complete unsupervised lan-
guage acquisition system.
</bodyText>
<sectionHeader confidence="0.994562" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.99875440625">
Peter F. Brown, Vincent J. Della Pietra, Peter V.
de Souza, Jenifer C. Lai, and Robert Mercer.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467-479.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical
Society Series B, 39:1-38.
S. Finch and N. Chater. 1992. Bootstrapping syn-
tactic categories. In Proceedings of the .14th An-
nual Meeting of the Cognitive Science Society,
pages 820-825.
S. Finch, N. Chater, and Redington M. 1995. Ac-
quiring syntactic information from distributional
statistics. In Joseph P. Levy, Dimitrios Bairak-
taris, John A. Bullinaria, and Paul Cairns, edi-
tors, Connectionist Models of Memory and Lan-
guage. UCL Press.
I. J. Good. 1953. The population frequencies of
species and the estimation of population parame-
ters. Biometrika, 40:237-264.
G. Leech, R. Garside, and M Bryant. 1994.
CLAWS4: the tagging of the British National
Corpus. In Proceedings of the 15th International
Conference on Computational Linguistics, pages
622-628.
Hinrich Schfitze. 1993. Part of speech induction
from scratch. In Proceedings of the 31st an-
nual meeting of the Association for Computa-
tional Linguistics, pages 251-258.
Hinrich Schiitze. 1997. Ambiguity Resolution in
Language Learning. CSLI Publications.
</reference>
<sectionHeader confidence="0.654713" genericHeader="method">
A Clusters
</sectionHeader>
<bodyText confidence="0.822605333333333">
Here are the five most frequent words in each of the
77 clusters, one cluster per line except where indi-
cated with a double slash \\
</bodyText>
<table confidence="0.868597846153846">
THE A HIS THIS AN
PEOPLE WORK LIFE RIGHT END
OF IN FOR ON WITH \\ , &amp;MDASM (
NEW OTHER FIRST OWN GOOD
&amp;SENTENCE \\ . ? !
AND AS OR UNTIL SUCHuAS
NOT BEEN N&apos;T SO ONLY
IS WAS HAD HAS DID
MADE USED FOUND LEFT PUT
ONE ALL MORE SOME TWO
TIME WAY YEAR DAY MAN \\ TO
WORLD GOVERNMENT PARTY FAMILY WEST
BE HAVE DO MAKE GET
</table>
<figure confidence="0.90370818367347">
HE I THEY SHE WE
US BRITAIN LONDON GOD LABOUR
BUT WHEN IF WHERE BECAUSE
) UP OUT BACK DOWN
WILL WOULD CAN COULD MAY
USE HELP FORM CHANGE SUPPORT
THAT BEFORE ABOVE OUTSIDE BELOW
IT EVERYBODY GINA
GROUP NUMBER SYSTEM OFFICE CENTRE
YOU THEM HIM ME THEMSELVES
&amp;BQUO \ \ &amp;EQUO \ \ ARE WERE \ \ &apos;S &apos;
CHARLES MARK PHILIP HENRY MARY
WHAT HOW WHY HAVING MAKING
IMPORTANT POSSIBLE CLEAR HARD CLOSE
WHICH WHO
CAME WENT LOOKED SEEMED BEGAN
JOHN SIR DAVID ST DE
YEARS PERuCENT DAYS TIMES MONTHS
GOING ABLE LOOKING TRYING COMING
THOUGHT FELT KNEW DECIDED HOPE
SEE SAY FEEL MEAN REMEMBER
SAID SAYS WROTE EXPLAINED REPLIED
GO COME TRY CONTINUE APPEAR \\ THERE
LOOK RUN LIVE MOVE TALK
SUCH USING PROVIDING DEVELOPING WINNING
TOOK TOLD SAW GAVE MAKES
HOWEVER OFuCOURSE FORuEXAMPLE INDEED
PART SORT THINKING LACK NONE
SOMETHING ANYTHING SOMEONE EVERYTHING
MR MRS DR HONG MR.
NEED NEEDS SEEM ATTEMPT OPPORTUNITY
WANT WANTED TRIED WISH WANTS
BASED RESPONSIBLE COMPARED INTERESTED
THAN \\ LAST NEXT GOLDEN FT-SE \\ THOSE
THINK BELIEVE SUPPOSE INSIST RECKON
KNOW UNDERSTAND REALISE
LATER AGO EARLIER THEREAFTER
BETTER WORSE LONGER BIGGER STRONGER
&amp;HELLIP ..
ASKED LIKED WATCHED SMILED INVITED
&apos;M AM \\ &apos;D
FACT IMPRESSION ASSUMPTION IMPLICATION
NOTHING NOWHERE RISEN
BECOME \\ ENOUGH \\ FAR INFINITELY
&apos;LL \\ &apos;RE \\ &apos;VE \\ CA WO Al
COPE DEPEND CONCENTRATE SUCCEED COMPETE
RO HVK AMEN
KLERK CLOWES HOWE COLI GAULLE
NEZ KHMER
</figure>
<page confidence="0.990552">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.949991">
<note confidence="0.979531">of CoNLL-2000 and LLL-2000, 91-94, Lisbon, Portugal, 2000.</note>
<title confidence="0.993262">Inducing Syntactic Categories by Context Distribution Clustering</title>
<author confidence="0.995447">Alexander</author>
<affiliation confidence="0.998791">School of Cognitive and Computing University of</affiliation>
<email confidence="0.985417">alexc@cogs.susx.ac.uk</email>
<abstract confidence="0.99947875">This paper addresses the issue of the automatic induction of syntactic categories from unannotated corpora. Previous techniques give good results, but fail to cope well with ambiguity or rare words. An algorithm, context distribution clustering (CDC), is presented which can be naturally extended to handle these problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V de Souza</author>
<author>Jenifer C Lai</author>
<author>Robert Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<marker>Brown, Pietra, de Souza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza, Jenifer C. Lai, and Robert Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society Series B,</journal>
<pages>39--1</pages>
<contexts>
<context position="5975" citStr="Dempster et al., 1977" startWordPosition="966" endWordPosition="969"> common words will also not be assigned, because they are ambiguous or have idiosyncratic distributional properties. 4 Ambiguity and Sparseness Ambiguity can be handled naturally within this framework. The context distribution p(w) of a particular ambiguous word w can be modelled as a linear combination of the context distributions of the various clusters. We can find the mixing coefficients by minimising D (p(w) E qz) where the cc(w) are some coefficients that sum to unity and the qi are the context distributions of the clusters. A minimum of this function can be found using the EM algorithm(Dempster et al., 1977). There are often several local minima — in practice this does not seem to be a major problem. Note that with rare words, the KL divergence reduces to the log likelihood of the word&apos;s context distribution plus a constant factor. However, the observed context distributions of rare words may be insufficient to make a definite determination of its cluster membership. In this case, under the assumption that the word is unambiguous, which is only valid for comparatively rare words, we can use Bayes&apos;s rule to calculate the posterior probability that it is in each class, using as a prior probability </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society Series B, 39:1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Finch</author>
<author>N Chater</author>
</authors>
<title>Bootstrapping syntactic categories.</title>
<date>1992</date>
<booktitle>In Proceedings of the .14th Annual Meeting of the Cognitive Science Society,</booktitle>
<pages>820--825</pages>
<contexts>
<context position="1826" citStr="Finch and Chater (1992)" startWordPosition="278" endWordPosition="281"> that will almost certainly involve some notion of syntactic category — requires an explanation of the acquisition of that set of syntactic categories. The amount of data used in this study is 12 million words, which is consistent with a pessimistic lower bound on the linguistic experience of the infant language learner in the period from 2 to 5 years of age, and has had capitalisation removed as being information not available in that circumstance. 2 Previous Work Previous work falls into two categories. A number of researchers have obtained good results using pattern recognition techniques. Finch and Chater (1992), (1995) and Schiitze (1993), (1997) use a set of features derived from the co-occurrence statistics of common words together with standard clustering and information extraction techniques. For sufficiently frequent words this method produces satisfactory results. Brown et al. (1992) use a very large amount of data, and a well-founded information theoretic model to induce large numbers of plausible semantic and syntactic clusters. Both approaches have two flaws: they cannot deal well with ambiguity, though Schiitze addresses this issue partially, and they do not cope well with rare words. Sinc</context>
</contexts>
<marker>Finch, Chater, 1992</marker>
<rawString>S. Finch and N. Chater. 1992. Bootstrapping syntactic categories. In Proceedings of the .14th Annual Meeting of the Cognitive Science Society, pages 820-825.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Finch</author>
<author>N Chater</author>
<author>M Redington</author>
</authors>
<title>Acquiring syntactic information from distributional statistics.</title>
<date>1995</date>
<booktitle>Connectionist Models of Memory and Language.</booktitle>
<editor>In Joseph P. Levy, Dimitrios Bairaktaris, John A. Bullinaria, and Paul Cairns, editors,</editor>
<publisher>UCL Press.</publisher>
<marker>Finch, Chater, Redington, 1995</marker>
<rawString>S. Finch, N. Chater, and Redington M. 1995. Acquiring syntactic information from distributional statistics. In Joseph P. Levy, Dimitrios Bairaktaris, John A. Bullinaria, and Paul Cairns, editors, Connectionist Models of Memory and Language. UCL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika,</journal>
<pages>40--237</pages>
<contexts>
<context position="5213" citStr="Good, 1953" startWordPosition="837" endWordPosition="839">n sort the words by the divergence from the cluster that is closest to them, and select the best as being the members of the cluster for the next iteration. This is repeated, gradually increasing the number of words included at each iteration, until a high enough proportion has been clustered, for example 80%. After each iteration, if the distance between two clusters falls below a threshhold value, the clusters are merged, and a new cluster is formed from the most frequent unclustered word. Since there will be zeroes in the context distributions, they are smoothed using Good-Turing smoothing(Good, 1953) to avoid singularities in the KL divergence. At this point we have a preliminary clustering — no very rare words will be included, and some common words will also not be assigned, because they are ambiguous or have idiosyncratic distributional properties. 4 Ambiguity and Sparseness Ambiguity can be handled naturally within this framework. The context distribution p(w) of a particular ambiguous word w can be modelled as a linear combination of the context distributions of the various clusters. We can find the mixing coefficients by minimising D (p(w) E qz) where the cc(w) are some coefficients</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>I. J. Good. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40:237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
<author>R Garside</author>
<author>M Bryant</author>
</authors>
<title>CLAWS4: the tagging of the British National Corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>622--628</pages>
<contexts>
<context position="10288" citStr="Leech et al., 1994" startWordPosition="1742" endWordPosition="1745">is not meaningful — one set of Test set 1 2 3 4 Mean CLAWS 411 301 478 413 395 Brown et al. 380 252 444 369 354 CDC 372 255 427 354 346 Table 3: Perplexities of class tri-gram models on 4 test sets of 100,000 words, together with geometric mean. tags chosen by linguists would score very badly against another without this implying any fault as there is no &apos;gold standard&apos;. I therefore chose to use an objective statistical measure, the perplexity of a very simple finite state model, to compare the tags generated with this clustering technique against the BNC tags, which uses the CLAWS-4 tag set (Leech et al., 1994) which had 76 tags. I tagged 12 million words of BNC text with the 77 tags, assigning each word to the cluster with the highest a posteriori probability given its prior cluster distribution and its context. I then trained 2nd-order Markov models (equivalently class trigram models) on the original BNC tags, on the outputs from my algorithm (CDC), and for comparision on the output from the Brown algorithm. The perplexities on held-out data are shown in table 3. As can be seen, the perplexity is lower with the model trained on data tagged with the new algorithm. This does not imply that the new t</context>
</contexts>
<marker>Leech, Garside, Bryant, 1994</marker>
<rawString>G. Leech, R. Garside, and M Bryant. 1994. CLAWS4: the tagging of the British National Corpus. In Proceedings of the 15th International Conference on Computational Linguistics, pages 622-628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schfitze</author>
</authors>
<title>Part of speech induction from scratch.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>251--258</pages>
<marker>Schfitze, 1993</marker>
<rawString>Hinrich Schfitze. 1993. Part of speech induction from scratch. In Proceedings of the 31st annual meeting of the Association for Computational Linguistics, pages 251-258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schiitze</author>
</authors>
<title>Ambiguity Resolution in Language Learning.</title>
<date>1997</date>
<publisher>CSLI Publications.</publisher>
<marker>Schiitze, 1997</marker>
<rawString>Hinrich Schiitze. 1997. Ambiguity Resolution in Language Learning. CSLI Publications.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>