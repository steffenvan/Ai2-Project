<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017358">
<title confidence="0.992218">
Improving the Efficiency of a Wide-Coverage CCG Parser
</title>
<author confidence="0.994052">
Bojan Djordjevic and James R. Curran
</author>
<affiliation confidence="0.9983415">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.667104">
NSW 2006, Australia
</address>
<email confidence="0.998977">
{bojan,james}@it.usyd.edu.au
</email>
<author confidence="0.976782">
Stephen Clark
</author>
<affiliation confidence="0.9599545">
Computing Laboratory
Oxford University
</affiliation>
<address confidence="0.9917235">
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
</address>
<email confidence="0.999443">
stephen.clark@comlab.ox.ac.uk
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993805">
The C&amp;C CCG parser is a highly efficient
linguistically motivated parser. The effi-
ciency is achieved using a tightly-integrated
supertagger, which assigns CCG lexical cat-
egories to words in a sentence. The integra-
tion allows the parser to request more cat-
egories if it cannot find a spanning anal-
ysis. We present several enhancements to
the CKY chart parsing algorithm used by the
parser. The first proposal is chart repair,
which allows the chart to be efficiently up-
dated by adding lexical categories individu-
ally, and we evaluate several strategies for
adding these categories. The second pro-
posal is to add constraints to the chart which
require certain spans to be constituents. Fi-
nally, we propose partial beam search to fur-
ther reduce the search space. Overall, the
parsing speed is improved by over 35% with
negligible loss of accuracy or coverage.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969162790698">
A recent theme in parsing research has been the
application of statistical methods to linguistically
motivated grammars, for example LFG (Kaplan et
al., 2004; Cahill et al., 2004), HPSG (Toutanova
et al., 2002; Malouf and van Noord, 2004), TAG
(Sarkar and Joshi, 2003) and CCG (Hockenmaier
and Steedman, 2002; Clark and Curran, 2004b). The
attraction of linguistically motivated parsers is the
potential to produce rich output, in particular the
predicate-argument structure representing the under-
lying meaning of a sentence. The disadvantage of
such parsers is that they are typically not very effi-
cient, parsing a few sentences per second on com-
modity hardware (Kaplan et al., 2004). The C&amp;C
CCG parser (Clark and Curran, 2004b) is an order
of magnitude faster, but is still limited to around 25
sentences per second.
The key to efficient CCG parsing is a finite-state
supertagger which performs much of the parsing
work (Bangalore and Joshi, 1999). CCG is a lex-
icalised grammar formalism, in which elementary
syntactic structures — in CCG’s case lexical cate-
gories expressing subcategorisation information —
are assigned to the words in a sentence. CCG su-
pertagging can be performed accurately and effi-
ciently by a Maximum Entropy tagger (Clark and
Curran, 2004a). Since the lexical categories contain
so much grammatical information, assigning them
with low average ambiguity leaves the parser, which
combines them together, with much less work to do
at parse time. Hence Bangalore and Joshi (1999), in
the context of LTAG parsing, refer to supertagging as
almost parsing.
Clark and Curran (2004a) presents a novel
method of integrating the supertagger and parser:
initially only a small number of categories, on av-
erage, is assigned to each word, and the parser at-
tempts to find a spanning analysis using the CKY
chart-parsing algorithm. If one cannot be found, the
parser requests more categories from the supertagger
and builds the chart again from scratch. This process
repeats until the parser is able to build a chart con-
taining a spanning analysis.1
</bodyText>
<footnote confidence="0.6487845">
1Tsuruoka and Tsujii (2004) investigate a similar idea in the
context of the CKY algorithm for a PCFG.
</footnote>
<page confidence="0.989072">
39
</page>
<note confidence="0.923377">
Proceedings of the 10th Conference on Parsing Technologies, pages 39–47,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999842894736842">
The supertagging accuracy is high enough that
the parser fails to find a spanning analysis using the
initial category assignment in approximately 4% of
Wall Street Journal sentences (?). However, parsing
this 4%, which largely consists of the longer sen-
tences, is disproportionately expensive.
This paper describes several modifications to the
C&amp;C parser which improve parsing efficiency with-
out reducing accuracy or coverage by reducing the
impact of the longer sentences. The first involves
chart repair, where the CKY chart is repaired when
extra lexical categories are added (according to the
scheme described above), instead of being rebuilt
from scratch. This allows an even tighter integra-
tion of the supertagger, in that the parser is able to
request individual categories. We explore methods
for choosing which individual categories to add, re-
sulting in an 11% speed improvement.
The next modification involves parsing with con-
straints, so that certain spans are required to be con-
stituents. This reduces the search space consider-
ably by eliminating a large number of constituents
which cross the boundaries of these spans. The best
set of constraints results in a 10% speed improve-
ment over the original parser. These constraints are
general enough that they could be applied to any
constituency-based parser. Finally, we experiment
with several beam strategies to reduce the search
space, finding that a partial beam which operates on
part of the chart is most effective, giving a further
6.1% efficiency improvement.
The chart repair and constraints interact in an in-
teresting, and unexpected, manner when combined,
giving a 35.7% speed improvement overall without
any loss in accuracy or coverage. This speed im-
provement is particularly impressive because it in-
volves techniques which only apply to 4% of Wall
Street Journal sentences.
</bodyText>
<sectionHeader confidence="0.988041" genericHeader="method">
2 The CCG Parser
</sectionHeader>
<bodyText confidence="0.999947111111111">
Clark and Curran (2004b) describes the CCG parser.
The grammar used by the parser is extracted from
CCGbank, a CCG version of the Penn Treebank
(Hockenmaier, 2003). The grammar consists of 425
lexical categories plus a small number of combi-
natory rules which combine the categories (Steed-
man, 2000). A Maximum Entropy supertagger first
assigns lexical categories to the words in a sen-
tence, which are then combined by the parser using
the combinatory rules. A log-linear model scores
the alternative parses. We use the normal-form
model, which assigns probabilities to single deriva-
tions based on the normal-form derivations in CCG-
bank. The features in the model are defined over
local parts of the derivation and include word-word
dependencies. A packed chart representation allows
efficient decoding, with the Viterbi algorithm find-
ing the most probable derivation.
The supertagger uses a log-linear model to de-
fine a distribution over the lexical category set for
each word and the previous two categories (Ratna-
parkhi, 1996) and the forward backward algorithm
efficiently sums over all histories to give a distribu-
tion for each word. These distributions are then used
to assign a set of lexical categories to each word (?).
The number of categories in each set is determined
by a parameter Q: all categories are assigned whose
forward-backward probabilities are within Q of the
highest probability category (?). If the parser can-
not then find a spanning analysis, the value of Q is
reduced — so that more lexical categories are as-
signed — and the parser tries again. This process re-
peats until an analysis spanning the whole sentence
is found.
In our previous work, when the parser was unable
to find a spanning analysis, the chart was destroyed
and then rebuilt from scratch with more lexical cate-
gories assigned to each word. However, this rebuild-
ing process is wasteful because the new chart is al-
ways a superset of the old one and could be created
by just updating the previous chart. We describe the
chart repair process in Section 3 which allows addi-
tional categories to be assigned to an existing chart
and the CKY algorithm run over just those parts of
the chart which require modification.
</bodyText>
<subsectionHeader confidence="0.996068">
2.1 Chart Parsing
</subsectionHeader>
<bodyText confidence="0.999929375">
The parser uses the CKY chart parsing algorithm
(Kasami, 1965; Younger, 1967) described in Steed-
man (2000). The CKY algorithm applies naturally to
CCG since the grammar is binary. It builds the chart
bottom-up, starting with the lexical categories span-
ning single words, incrementally increasing the span
until the whole sentence is covered. Since the con-
stituents are built in order of span size, at every stage
</bodyText>
<page confidence="0.996846">
40
</page>
<bodyText confidence="0.956238205128205">
all the sub-constituents which could be used to cre-
ate a particular new constituent are already present
in the chart.
The charts are packed by grouping together equiv-
alent chart entries, which allows a large number of
derivations to be represented efficiently. Entries are
equivalent when they interact in the same manner
with both the generation of subsequent parse struc-
ture and the statistical parse selection. In practice,
this means that equivalent entries have the same
span; form the same structures, i.e. the remain-
ing derivation plus dependencies, in any subsequent
parsing; and generate the same features in any sub-
sequent parsing.
The Viterbi algorithm is used to find the most
probable derivation from a packed chart. For each
equivalence class of individual entries, we record the
entry at the root of the subderivation which has the
highest score for the class. The equivalence classes
are defined so that any other individual entry can-
not be part of the highest scoring derivation for the
sentence. The highest-scoring subderivations can
be calculated recursively using the highest-scoring
equivalence classes that were combined to create the
individual entry.
Given a sentence of n words, we define pos E
10, ... , n − 1} to be the starting position of an en-
try in the chart (represented by a CCG category) and
span E 11, ... , n} its length. Let cell(pos, span)
be the set of categories which span the sentence from
pos to pos + span. These will be combinations of
categories in cell(pos, k) and cell(pos+k, span−k)
for all k E 11, ... , span −1}. The chart is a two di-
mensional array indexed by pos and span. The valid
(pos, span) pairs correspond to pos + span &lt; n,
that is, to spans that do not extend beyond the end
of the sentence. The squares represent valid cells in
Figure 1. The span from position 3 with length 4,
i.e. cell(3, 4), is marked with a diamond in Figure 2.
</bodyText>
<sectionHeader confidence="0.973109" genericHeader="method">
3 Chart Repair
</sectionHeader>
<bodyText confidence="0.999951666666667">
The parser interacts with the supertagger by decreas-
ing the value of the Q parameter when a spanning
analysis cannot be found for a sentence. This has
the effect of adding more lexical categories to the
chart. Instead of rebuilding the chart from scratch
when new categories are added, it can be repaired
</bodyText>
<figure confidence="0.98249">
span
10
9
8
7
6
5
4
3
2
1
0 1 2 3 4 5 6 7 8 9
pos
</figure>
<figureCaption confidence="0.99998">
Figure 1: Cells affected by chart repair.
</figureCaption>
<bodyText confidence="0.940472121212121">
by modifying cells that are affected by the new cat-
egories. Considering the case where a single lexical
category is added to the ith word in an n word sen-
tence, the new category can only affect the cells that
satisfy pos &lt; i and pos + span &gt; i. These cells are
shown in Figure 1 for the word at position 3.
The number of affected cells is (n−pos)(pos+1),
and so the average over the sentence is approxi-
mately n f0n−1(n − p)(p + 1) dp pz� n2
6cells. The
total number of cells in the chart is n(n�1)
2 . The chart
can therefore be repaired bottom up, in CKY order,
by updating a third of the cells on average.
Additional lexical categories for a word are in-
serted into the corresponding cell in the bottom row,
with the additional categories being marked as new.
For each cell C in the second row, each pair of cells
A and B is considered whose spans combine to cre-
ate the span of C. In the original CKY, all categories
from A are combined with all categories from B. In
chart repair, categories are only combined if at least
one of them is new, because otherwise the result is
already in C. The categories added to C are marked,
and the process is repeated for all affected cells in
CKY order.
Chart repair speeds up parsing for two reasons.
First, it reuses previous computations and eliminates
wasteful rebuilding of the chart. Second, it allows
lexical categories to be added to the chart one at a
cell with a new
category added
affected cells
</bodyText>
<page confidence="0.732869">
41
</page>
<figure confidence="0.766291">
pos
</figure>
<figureCaption confidence="0.998618">
Figure 2: Cells affected by adding a constraint.
</figureCaption>
<bodyText confidence="0.999976266666667">
time until a spanning derivation is found. In the orig-
inal approach extra categories were added in bulk by
changing the Q level, which significantly increased
the average ambiguity. Chart repair allows the min-
imum amount of ambiguity to be added for a span-
ning derivation to be found.
The C&amp;C parser has a predefined limit on the num-
ber of categories in the chart. If this is exceeded
before a spanning analysis is found then the parser
fails on the sentence. Our new strategy allows a
chart containing a spanning analysis to be built with
the minimum number of categories possible. This
means that some sentences can now be parsed that
would have previously exceeded the limit, slightly
increasing coverage.
</bodyText>
<subsectionHeader confidence="0.998944">
3.1 Category selection
</subsectionHeader>
<bodyText confidence="0.981670538461538">
The order in which lexical categories are added to
the chart will impact on parsing speed and accu-
racy, and so we evaluate several alternatives. The
first ordering (Q VALUE) is by decreasing Q value,
where the Q value is the ratio between the probabil-
ity of the most likely category and the probability of
the given category for that word.2 The second or-
dering (PROB) is by decreasing category probability
2We are overloading the use of Q for convenience. Here, Q
refers to the variable ratio dependent on the particular category,
whereas the 3 value used in supertagging is a cutoff applied to
the variable ratio.
as assigned by the supertagger using the forward-
backward algorithm.
We also investigated ordering categories using in-
formation from the chart. Examining the sentences
which required chart repair showed that, when a
word is missing the correct category, the cells af-
fected (as defined in Section 3) by the cell are often
empty. The CHART ordering uses this observation to
select the next lexical category to assign. It selects
the word corresponding to the cell with the high-
est number of empty affected cells, and then adds
the highest probability category not in the chart for
that word. Finally, we included a RANDOM ordering
baseline for comparison purposes.
</bodyText>
<sectionHeader confidence="0.999524" genericHeader="method">
4 Constraints
</sectionHeader>
<bodyText confidence="0.997275555555556">
The set of possible derivations can be constrained
if we know in advance that a particular span is re-
quired to be the yield of a single constituent in the
correct parse. A constraint on span p reduces the
search space because p must be the yield of a single
cell. This means that cells with yields that cross the
boundary of p cannot be part of a correct derivation,
and do not need to be considered (the grey cells in
Figure 2). In addition, if a cell yields p as a prefix or
suffix (the hashed cells in Figure 2) then it also has
constraints on how it can be created.
Figure 2 shows an example constraint requiring
words 3–6 to be a constituent, which corresponds to
p = cell(3, 4). Consider cell(3, 7): it yields words
3–9 and so contains p as the prefix. Normally it can
be created by combining cell(3, 1) with cell(4, 6),
or cell(3, 2) with cell(5, 5), and so on up to cell(3, 6)
with cell(9,1). However the first three combinations
are not allowed because the second child crosses the
boundary of p. This gives a lower limit for the span
of the left child. Similarly, if p is the suffix of the
span of a cell then there is a lower limit on the span
of the right child.
As the example demonstrates, a single constraint
can eliminate many combinations, reducing the
search space significantly, and thus improving pars-
ing efficiency.
</bodyText>
<subsectionHeader confidence="0.997878">
4.1 Creating Constraints
</subsectionHeader>
<bodyText confidence="0.999064333333333">
How can we know in advance that the correct deriva-
tion must yield specific spans, since this appears to
require knowledge of the parse itself? We have ex-
</bodyText>
<figure confidence="0.9973382">
span
0 1 2 3 4 5 6 7 8 9
10
9
8
7
6
5
4
3
2
1
required cell
invalid cells
affected cells
</figure>
<page confidence="0.994549">
42
</page>
<bodyText confidence="0.9999695">
plored constraints derived from shallow parsing and
from the raw sentence. Our results demonstrate that
simple constraints can reduce parsing time signifi-
cantly without loss of coverage or accuracy.
Chunk tags were used to create constraints. We
experimented with both gold standard chunks from
the Penn Treebank and also chunker output from the
C&amp;C chunk tagger. The tagger is very similar to the
Maximum Entropy POS tagger described in Curran
and Clark (2003). Only NP chunks were used be-
cause the accuracy of the tagger for other chunks is
lower. The Penn Treebank chunks required modi-
fication because CCGbank analyses some construc-
tions differently. We also created longer NPs by con-
catenating adjacent base NPs, for example in the case
of possessives.
A number of punctuation constraints were used
and had a significant impact especially for longer
sentences. There are a number of punctuation rules
in CCGbank which absorb a punctuation mark by
combining it with a category and returning a cate-
gory of the same type. These rules are very produc-
tive, combining with many constituent types. How-
ever, in CCGbank the sentence final punctuation is
always attached at the root. A constraint on the first
n − 1 words was added to force the parser to only
attach the sentence final punctuation once the rest of
the sentence has been parsed.
Constraints are placed around parenthesised and
quoted phrases that usually form constituents be-
fore attaching elsewhere. Constraints are also placed
around phrases bound by colons, semicolons, or hy-
phens. These constraints are especially effective
for long sentences with many clauses separated by
semicolons, reducing the sentence to a number of
smaller units which significantly improves parsing
efficiency.
In some instances, adding constraints can be
harmful to parsing efficiency and/or accuracy. Lack
of precision in the constraints can come from noisy
output from NLP components, e.g. the chunker, or
from rules which are not always applicable, e.g.
punctuation constraints. We find that the punctua-
tion constraints are particularly effective while the
gold standard chunks are required to gain any ben-
efit for the NP constraints. Adding constraints also
has the potential to increase coverage because the re-
duced search space means that longer sentences can
be parsed without exceeding the pre-defined limits
on chart size.
</bodyText>
<sectionHeader confidence="0.977951" genericHeader="method">
5 Selective Beam Search
</sectionHeader>
<bodyText confidence="0.999940909090909">
Beam search involves greedy elimination of low
probability partial derivations before they can form
complete derivations. It is used in many parsers to
reduce the search space, for example Collins (2003).
We use a variable width beam where all categories
c in a particular cell C that satisfy score(c) &lt;
max{score(x)|x E C} − B, for some beam cut-
off B, are removed. The category scores score(c)
are log probabilities.
In the C&amp;C parser, the entire packed chart is con-
structed first and then the spanning derivations are
marked. Only the partial derivations that form part
of spanning derivations are scored to select the best
parse, which is a small fraction of the categories in
the chart. Because the categories are scored with
a complex statistical model with a large number of
features, the time spent calculating scores is signif-
icant. We found that applying a beam to every cell
during the construction of the chart was more expen-
sive than not using the beam at all. When the beam
was made harsh enough to be worthwhile, it reduced
accuracy and coverage significantly.
We propose selective beam search where the
beam is only applied to spans of particular lengths.
The shorter spans are most important to cull because
there are many more of them and removing them has
the largest impact in terms of reducing the search
space. However, the supertagger already acts like
a beam at the lexical category level and the parser
model has fewer features at this level, so the beam
may be more accurate for longer spans. We there-
fore expect the beam to be most effective for spans
of intermediate length.
</bodyText>
<sectionHeader confidence="0.999772" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999595">
The parser was trained on CCGbank sections 02-21
and section 00 was used for development. The per-
formance is measured in terms of coverage, F-score
and parsing time. The F-score is for labelled depen-
dencies compared against the predicate-argument
dependencies in CCGbank. The time reported in-
cludes loading the grammar and statistical model,
which takes around 5 seconds, and parsing the 1913
</bodyText>
<page confidence="0.99951">
43
</page>
<bodyText confidence="0.998680086956522">
sentences in section 00.
The failure rate (opposite of coverage) is broken
down into sentences with length G 40 and &gt; 40
because longer sentences are more difficult to parse
and the C&amp;C parser already has very high coverage
on shorter sentences. There are 1784 1-40 word sen-
tences and 129 41+ word sentences. The average
length and standard deviation in the 41+ set are 50.8
and 31.5 respectively.
All experiments used gold standard POS tags.
Original and REPAIR do not use constraints. The
NP(GOLD) experiments use Penn Treebank gold
standard NP chunks to determine an upper bound
on the utility of chunk constraints. The times re-
ported for NP(C&amp;C) using the C&amp;C chunker include
the time to load the chunker model and run the chun-
ker (around 1.3 seconds). PUNCT adds all of the
punctuation constraints.
Finally the best system was compared against the
original parser on section 23, which has 2257 sen-
tences of length 1-40 and 153 of length 41+. The
maximum length is only 65, which explains the high
coverage for the 41+ section.
</bodyText>
<subsectionHeader confidence="0.999768">
6.1 Chart Repair Results
</subsectionHeader>
<bodyText confidence="0.999824086956522">
The results in Table 1 show that chart repair gives
an immediate 11.1% improvement in speed and a
small 0.21% improvement in accuracy. 96.1% of
sentences do not require chart repair because they
are successfully parsed using the initial set of lexi-
cal categories supplied by the supertagger. Hence,
11% is a significant improvement for less than 4%
of the sentences.
We believe the accuracy was improved (on top of
the efficiency) because of the way the repair process
adds new categories. Adding categories individually
allows the parser to be influenced by the probabil-
ities which the supertagger assigns, which are not
directly modelled in the parser. If we were to add
this information from the supertagger into the parser
statistical model directly we would expect almost
no accuracy difference between the original method
and chart repair.
Table 2 shows the impact of different category
ordering approaches for chart repair (with PUNCT
constraints). The most effective approach is to use
the information from the chart about the proportion
of empty cells, which adds half as many categories
</bodyText>
<table confidence="0.9996116">
METHOD secs % F-SCORE CATS
RANDOM 70.2 -16.2 86.57 23.1
Q VALUE 60.4 — 86.66 15.7
PROB 60.1 0.5 86.65 14.3
CHART 57.2 5.3 86.61 7.0
</table>
<tableCaption confidence="0.999277">
Table 2: Category ordering for chart repair.
</tableCaption>
<bodyText confidence="0.9996586">
on average as the Q value and probability based ap-
proaches. All of our approaches significantly out-
perform randomly selecting extra categories. The
CHART category ordering is used for the remaining
experiments.
</bodyText>
<subsectionHeader confidence="0.99979">
6.2 Constraints Results
</subsectionHeader>
<bodyText confidence="0.999989484848485">
The results in Table 1 show that, without chart re-
pair, using gold standard noun phrases does not im-
prove efficiency, while using noun phrases identi-
fied by the C&amp;C chunker decreases speed by 10.8%.
They both also slightly reduce parsing accuracy.
The number of times the parsing process had to be
restarted with the constraints removed, was more
costly than the reduction of the search space. This
is unsurprising because the chunk data was not ob-
tained from CCGbank and the chunker is not ac-
curate enough for the constraints to improve pars-
ing efficiency. The most frequent inconsistencies
between CCGbank and chunks extracted from the
Penn Treebank were fixed in a preprocessing step as
explained in Section 4.1, but the less frequent con-
structions are still problematic.
The best results for parsing with constraints (with-
out repair) were with both punctuation and gold
standard noun phrase constraints, with 20.5% im-
provement in speed and 0.42% in coverage, but an
F-score penalty of 0.3%. This demonstrates the pos-
sible efficiency gain with a perfect chunker – the
corresponding results with the C&amp;C chunker are still
worse than without constraints. The best results
without a decrease in accuracy use only punctuation
constraints, with 10.4% increase in speed and 0.37%
in coverage. The punctuation constraints also have
the advantage of being simple to implement.
The best overall efficiency gain was obtained
when punctuation and gold standard noun phrases
were used with chart repair, with a 45.4% improve-
ment in speed and 0.63% in coverage, and a 0.4%
drop in accuracy. The best results without a drop in
</bodyText>
<page confidence="0.996904">
44
</page>
<table confidence="0.999788846153846">
METHOD secs % F-SCORE COVER n &lt; 40 n &gt; 40
Original 88.3 — 86.54 98.85 0.392 11.63
REPAIR 78.5 11.1 86.75 99.01 0.336 10.08
NP(GOLD) 88.4 -0.1 86.27 99.06 0.224 10.85
NP(C&amp;C) 97.8 -10.8 86.31 99.16 0.224 9.30
PUNCT 79.1 10.4 86.56 99.22 0.168 9.30
NP(GOLD) + PUNCT 69.8 20.5 86.24 99.27 0.168 8.53
NP(C&amp;C) + PUNCT 97.0 -9.9 86.31 99.16 0.168 10.08
NP(GOLD) + REPAIR 65.0 26.4 86.04 99.37 0.224 6.20
NP(C&amp;C) + REPAIR 77.5 12.2 86.35 99.37 0.224 6.20
PUNCT + REPAIR 57.2 35.2 86.61 99.48 0.168 5.43
NP(GOLD) + PUNCT + REPAIR 48.2 45.4 86.14 99.48 0.168 5.43
NP(C&amp;C) + PUNCT + REPAIR 63.2 28.4 86.43 99.53 0.163 3.88
</table>
<tableCaption confidence="0.999499">
Table 1: Parsing performance on section 00 with constraints and chart repair
</tableCaption>
<table confidence="0.999813333333333">
METHOD secs % F-SCORE COVER n &lt; 40 n &gt; 40
Original 88.3 — 86.54 98.85 0.392 11.63
PUNCT 79.1 10.4 86.56 99.22 0.168 9.30
REPAIR 78.5 11.1 86.75 99.01 0.336 10.08
PUNCT + REPAIR 57.2 35.2 86.61 99.48 0.168 5.43
PUNCT + REPAIR + BEAM 52.4 40.7 86.56 99.48 0.168 5.43
</table>
<tableCaption confidence="0.999664">
Table 3: Best performance on Section 00
</tableCaption>
<bodyText confidence="0.999966333333333">
accuracy were with only punctuation constraints and
chart repair, with improvements of 35.2% speed and
0.63% coverage. Coverage on both short and long
sentences is improved – the best results show a 43%
and 67% decrease in failure rate for sentence lengths
in the ranges 1-40 and 41+ respectively.
</bodyText>
<subsectionHeader confidence="0.997307">
6.3 Partial Beam Results
</subsectionHeader>
<bodyText confidence="0.9999879">
We found that using the selective beam on 1–2 word
spans had negligible impact on speed and accuracy.
Using the beam on 3–4 word spans had the most im-
pact without accuracy penalty, improving efficiency
by another -5%. Experiments with the selective
beam on longer spans continued to improve effi-
ciency, but with a much greater penalty in F-score,
e.g. a further -5% at a cost of 0.5% F-score for 3–6
word spans. However, we are interested in efficiency
improvements with negligible cost to accuracy.
</bodyText>
<subsectionHeader confidence="0.993768">
6.4 Overall Results
</subsectionHeader>
<bodyText confidence="0.99999125">
Table 3 summarises the results for section 00. The
chart repair and punctuation constraints individually
increase parsing efficiency by around 10%. How-
ever, the most interesting result is that in combina-
tion they increase efficiency by over 35%. This is
because the cost of rebuilding the chart when the
constraints are incorrect has been significantly re-
duced by chart repair. Finally, the use of the selec-
tive beam gives modest improvement of 5.5%. The
overall efficiency gain on section 00 is 40.7% with
an additional 0.5% coverage, halving both the num-
ber of short and long sentences that fail to be parsed.
Table 4 shows the performance of the punctuation
constraints, chart repair and selective beam system
on section 23. The results are consistent with sec-
tion 00, showing a 30.9% improvement in speed and
0.29% in coverage, with accuracy staying at roughly
the same level. The results show a consistent 35-
40% reduction in parsing time and a 40-65% reduc-
tion in parse failure rate.
</bodyText>
<sectionHeader confidence="0.999286" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999065666666667">
We have introduced several modifications to CKY
parsing for CCG that significantly increase parsing
efficiency without an accuracy or coverage penalty.
</bodyText>
<page confidence="0.996162">
45
</page>
<table confidence="0.996622333333333">
METHOD secs % F-SCORE COVER n &lt; 40 n &gt; 40
Original 91.3 — 86.92 99.29 0.621 1.961
PUNCT + REPAIR + BEAM 58.7 35.7 86.82 99.58 0.399 0.654
</table>
<tableCaption confidence="0.999425">
Table 4: Best performance on Section 23
</tableCaption>
<bodyText confidence="0.999882323529412">
Chart repair improves efficiency by reusing the
chart from the previous parse attempts. This allows
us to further tighten the parser-supertagger integra-
tion by adding one lexical category at a time until a
spanning derivation is found. We have also explored
several approaches to selecting which category to
add next. We intend to further explore strategies
for determining which category to add next when a
parse fails. This includes combining chart and prob-
ability based orderings. Chart repair alone gives an
11.1% efficiency improvement.
Constraints improve efficiency by avoiding the
construction of sub-derivations that will not be used.
They have a significant impact on parsing speed and
coverage without reducing the accuracy, provided
the constraints are identified with sufficient preci-
sion. Punctuation constraints give a 10.4% improve-
ment, but NP constraints require higher precision NP
chunking than is currently available for CCGbank.
Constraints and chart repair both manipulate the
chart for more efficient parsing. Adding categories
one at a time using chart repair is almost a form of
agenda-based parsing. We intend to explore other
methods for pruning the space and agenda-based
parsing, in particular A* parsing (Klein and Man-
ning, 2003), which will allow only the most proba-
ble parts of the chart to be built, improving efficiency
while still ensuring the optimal derivation is found.
When all of our modifications are used parsing
speed increases by 35-40% and the failure rate de-
creases by 40-65%, both for sentences of length 1-40
and 41+, with a negligible accuracy penalty. The re-
sult is an even faster state-of-the-art wide-coverage
CCG parser.
</bodyText>
<sectionHeader confidence="0.996921" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998884333333333">
We would like to thank the anonymous reviewers
for their feedback. James Curran was funded under
ARC Discovery grants DP0453131 and DP0665973.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999621357142857">
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237–265.
A. Cahill, M. Burke, R. O’Donovan, J. van Genabith,
and A. Way. 2004. Long-distance dependency resolu-
tion in automatically acquired wide-coverage PCFG-
based LFG approximations. In Proceedings of the
42nd Meeting of the ACL, pages 320–327, Barcelona,
Spain.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING-04, pages 282–288,
Geneva, Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings ofACL-04, pages 104–111, Barcelona, Spain.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589–637.
James R. Curran and Stephen Clark. 2003. Investigating
GIS and smoothing for maximum entropy taggers. In
Proceedings of the 10th Meeting of the EACL, pages
91–98, Budapest, Hungary.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar parsing.
In Proceedings of COLING/ACL-06, pages 697–704,
Sydney, Austrailia.
Julia Hockenmaier and Mark Steedman. 2002. Gener-
ative models for statistical parsing with Combinatory
Categorial Grammar. In Proceedings of the 40th Meet-
ing of the ACL, pages 335–342, Philadelphia, PA.
Julia Hockenmaier. 2003. Data and Models for Statis-
tical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Ron Kaplan, Stefan Riezler, Tracy H. King, John
T. Maxwell III, Alexander Vasserman, and Richard
Crouch. 2004. Speed and accuracy in shallow and
deep stochastic parsing. In Proceedings of the Human
Language Technology Conference and the 4th Meeting
of the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL’04), Boston,
MA.
</reference>
<page confidence="0.989603">
46
</page>
<reference confidence="0.999795054054054">
J. Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context-free languages. Techni-
cal Report AFCRL-65-758, Air Force Cambridge Re-
search Laboratory, Bedford, MA.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact Viterbi parse selection. In Proceed-
ings of Human Language Technology and the North
American Chapter of the Association for Computa-
tional Linguistics Conference, pages 119–126, Ed-
mond, Canada.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value gram-
mars. In Proceedings of the IJCNLP-04 Workshop:
Beyond shallow analyses - Formalisms and statistical
modeling for deep analyses, Hainan Island, China.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the EMNLP Con-
ference, pages 133–142, Philadelphia, PA.
Anoop Sarkar and Aravind Joshi. 2003. Tree-adjoining
grammars and its application to statistical parsing. In
Rens Bod, Remko Scha, and Khalil Sima’an, editors,
Data-oriented parsing. CSLI.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Kristina Toutanova, Christopher Manning, Stuart
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich HPSG grammar. In
Proceedings of the First Workshop on Treebanks
and Linguistic Theories, pages 253–263, Sozopol,
Bulgaria.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2004. Iterative
cky parsing for probabilistic context-free grammars.
In Proceedings of the IJCNLP conference, pages 52–
60, Hainan Island, China.
D. Younger. 1967. Recognition and parsing of context-
free languages in time n3. Information and Control,
10(2):189–208.
</reference>
<page confidence="0.99949">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.427605">
<title confidence="0.999205">Improving the Efficiency of a Wide-Coverage CCG Parser</title>
<author confidence="0.983099">R Djordjevic</author>
<affiliation confidence="0.9953465">School of Information University of</affiliation>
<address confidence="0.623612">NSW 2006,</address>
<author confidence="0.922617">Stephen</author>
<affiliation confidence="0.9672055">Computing Oxford</affiliation>
<address confidence="0.9058335">Wolfson Building, Parks Oxford, OX1 3QD, UK</address>
<email confidence="0.998055">stephen.clark@comlab.ox.ac.uk</email>
<abstract confidence="0.998999476190476">CCG is a highly efficient linguistically motivated parser. The efficiency is achieved using a tightly-integrated which assigns categories to words in a sentence. The integration allows the parser to request more categories if it cannot find a spanning analysis. We present several enhancements to parsing algorithm used by the The first proposal is which allows the chart to be efficiently updated by adding lexical categories individually, and we evaluate several strategies for adding these categories. The second prois to add the chart which require certain spans to be constituents. Fiwe propose search to further reduce the search space. Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="2160" citStr="Bangalore and Joshi, 1999" startWordPosition="333" endWordPosition="336">an, 2004b). The attraction of linguistically motivated parsers is the potential to produce rich output, in particular the predicate-argument structure representing the underlying meaning of a sentence. The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al., 2004). The C&amp;C CCG parser (Clark and Curran, 2004b) is an order of magnitude faster, but is still limited to around 25 sentences per second. The key to efficient CCG parsing is a finite-state supertagger which performs much of the parsing work (Bangalore and Joshi, 1999). CCG is a lexicalised grammar formalism, in which elementary syntactic structures — in CCG’s case lexical categories expressing subcategorisation information — are assigned to the words in a sentence. CCG supertagging can be performed accurately and efficiently by a Maximum Entropy tagger (Clark and Curran, 2004a). Since the lexical categories contain so much grammatical information, assigning them with low average ambiguity leaves the parser, which combines them together, with much less work to do at parse time. Hence Bangalore and Joshi (1999), in the context of LTAG parsing, refer to super</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>M Burke</author>
<author>R O’Donovan</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<title>Long-distance dependency resolution in automatically acquired wide-coverage PCFGbased LFG approximations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the ACL,</booktitle>
<pages>320--327</pages>
<location>Barcelona,</location>
<marker>Cahill, Burke, O’Donovan, van Genabith, Way, 2004</marker>
<rawString>A. Cahill, M. Burke, R. O’Donovan, J. van Genabith, and A. Way. 2004. Long-distance dependency resolution in automatically acquired wide-coverage PCFGbased LFG approximations. In Proceedings of the 42nd Meeting of the ACL, pages 320–327, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>282--288</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1542" citStr="Clark and Curran, 2004" startWordPosition="234" endWordPosition="237">he second proposal is to add constraints to the chart which require certain spans to be constituents. Finally, we propose partial beam search to further reduce the search space. Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage. 1 Introduction A recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars, for example LFG (Kaplan et al., 2004; Cahill et al., 2004), HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), TAG (Sarkar and Joshi, 2003) and CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b). The attraction of linguistically motivated parsers is the potential to produce rich output, in particular the predicate-argument structure representing the underlying meaning of a sentence. The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al., 2004). The C&amp;C CCG parser (Clark and Curran, 2004b) is an order of magnitude faster, but is still limited to around 25 sentences per second. The key to efficient CCG parsing is a finite-state supertagger which performs much of the parsing work (Bangalor</context>
<context position="2809" citStr="Clark and Curran (2004" startWordPosition="435" endWordPosition="438">mar formalism, in which elementary syntactic structures — in CCG’s case lexical categories expressing subcategorisation information — are assigned to the words in a sentence. CCG supertagging can be performed accurately and efficiently by a Maximum Entropy tagger (Clark and Curran, 2004a). Since the lexical categories contain so much grammatical information, assigning them with low average ambiguity leaves the parser, which combines them together, with much less work to do at parse time. Hence Bangalore and Joshi (1999), in the context of LTAG parsing, refer to supertagging as almost parsing. Clark and Curran (2004a) presents a novel method of integrating the supertagger and parser: initially only a small number of categories, on average, is assigned to each word, and the parser attempts to find a spanning analysis using the CKY chart-parsing algorithm. If one cannot be found, the parser requests more categories from the supertagger and builds the chart again from scratch. This process repeats until the parser is able to build a chart containing a spanning analysis.1 1Tsuruoka and Tsujii (2004) investigate a similar idea in the context of the CKY algorithm for a PCFG. 39 Proceedings of the 10th Conferen</context>
<context position="5422" citStr="Clark and Curran (2004" startWordPosition="848" endWordPosition="851">ey could be applied to any constituency-based parser. Finally, we experiment with several beam strategies to reduce the search space, finding that a partial beam which operates on part of the chart is most effective, giving a further 6.1% efficiency improvement. The chart repair and constraints interact in an interesting, and unexpected, manner when combined, giving a 35.7% speed improvement overall without any loss in accuracy or coverage. This speed improvement is particularly impressive because it involves techniques which only apply to 4% of Wall Street Journal sentences. 2 The CCG Parser Clark and Curran (2004b) describes the CCG parser. The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence, which are then combined by the parser using the combinatory rules. A log-linear model scores the alternative parses. We use the normal-form model, which assigns probabilities to single derivations based on the normal-form derivatio</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004a. The importance of supertagging for wide-coverage CCG parsing. In Proceedings of COLING-04, pages 282–288, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL-04,</booktitle>
<pages>104--111</pages>
<location>Barcelona,</location>
<contexts>
<context position="1542" citStr="Clark and Curran, 2004" startWordPosition="234" endWordPosition="237">he second proposal is to add constraints to the chart which require certain spans to be constituents. Finally, we propose partial beam search to further reduce the search space. Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage. 1 Introduction A recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars, for example LFG (Kaplan et al., 2004; Cahill et al., 2004), HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), TAG (Sarkar and Joshi, 2003) and CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b). The attraction of linguistically motivated parsers is the potential to produce rich output, in particular the predicate-argument structure representing the underlying meaning of a sentence. The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al., 2004). The C&amp;C CCG parser (Clark and Curran, 2004b) is an order of magnitude faster, but is still limited to around 25 sentences per second. The key to efficient CCG parsing is a finite-state supertagger which performs much of the parsing work (Bangalor</context>
<context position="2809" citStr="Clark and Curran (2004" startWordPosition="435" endWordPosition="438">mar formalism, in which elementary syntactic structures — in CCG’s case lexical categories expressing subcategorisation information — are assigned to the words in a sentence. CCG supertagging can be performed accurately and efficiently by a Maximum Entropy tagger (Clark and Curran, 2004a). Since the lexical categories contain so much grammatical information, assigning them with low average ambiguity leaves the parser, which combines them together, with much less work to do at parse time. Hence Bangalore and Joshi (1999), in the context of LTAG parsing, refer to supertagging as almost parsing. Clark and Curran (2004a) presents a novel method of integrating the supertagger and parser: initially only a small number of categories, on average, is assigned to each word, and the parser attempts to find a spanning analysis using the CKY chart-parsing algorithm. If one cannot be found, the parser requests more categories from the supertagger and builds the chart again from scratch. This process repeats until the parser is able to build a chart containing a spanning analysis.1 1Tsuruoka and Tsujii (2004) investigate a similar idea in the context of the CKY algorithm for a PCFG. 39 Proceedings of the 10th Conferen</context>
<context position="5422" citStr="Clark and Curran (2004" startWordPosition="848" endWordPosition="851">ey could be applied to any constituency-based parser. Finally, we experiment with several beam strategies to reduce the search space, finding that a partial beam which operates on part of the chart is most effective, giving a further 6.1% efficiency improvement. The chart repair and constraints interact in an interesting, and unexpected, manner when combined, giving a 35.7% speed improvement overall without any loss in accuracy or coverage. This speed improvement is particularly impressive because it involves techniques which only apply to 4% of Wall Street Journal sentences. 2 The CCG Parser Clark and Curran (2004b) describes the CCG parser. The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence, which are then combined by the parser using the combinatory rules. A log-linear model scores the alternative parses. We use the normal-form model, which assigns probabilities to single derivations based on the normal-form derivatio</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004b. Parsing the WSJ using CCG and log-linear models. In Proceedings ofACL-04, pages 104–111, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="18076" citStr="Collins (2003)" startWordPosition="3046" endWordPosition="3047">cable, e.g. punctuation constraints. We find that the punctuation constraints are particularly effective while the gold standard chunks are required to gain any benefit for the NP constraints. Adding constraints also has the potential to increase coverage because the reduced search space means that longer sentences can be parsed without exceeding the pre-defined limits on chart size. 5 Selective Beam Search Beam search involves greedy elimination of low probability partial derivations before they can form complete derivations. It is used in many parsers to reduce the search space, for example Collins (2003). We use a variable width beam where all categories c in a particular cell C that satisfy score(c) &lt; max{score(x)|x E C} − B, for some beam cutoff B, are removed. The category scores score(c) are log probabilities. In the C&amp;C parser, the entire packed chart is constructed first and then the spanning derivations are marked. Only the partial derivations that form part of spanning derivations are scored to select the best parse, which is a small fraction of the categories in the chart. Because the categories are scored with a complex statistical model with a large number of features, the time spe</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Investigating GIS and smoothing for maximum entropy taggers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Meeting of the EACL,</booktitle>
<pages>91--98</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="15938" citStr="Curran and Clark (2003)" startWordPosition="2705" endWordPosition="2708">ince this appears to require knowledge of the parse itself? We have exspan 0 1 2 3 4 5 6 7 8 9 10 9 8 7 6 5 4 3 2 1 required cell invalid cells affected cells 42 plored constraints derived from shallow parsing and from the raw sentence. Our results demonstrate that simple constraints can reduce parsing time significantly without loss of coverage or accuracy. Chunk tags were used to create constraints. We experimented with both gold standard chunks from the Penn Treebank and also chunker output from the C&amp;C chunk tagger. The tagger is very similar to the Maximum Entropy POS tagger described in Curran and Clark (2003). Only NP chunks were used because the accuracy of the tagger for other chunks is lower. The Penn Treebank chunks required modification because CCGbank analyses some constructions differently. We also created longer NPs by concatenating adjacent base NPs, for example in the case of possessives. A number of punctuation constraints were used and had a significant impact especially for longer sentences. There are a number of punctuation rules in CCGbank which absorb a punctuation mark by combining it with a category and returning a category of the same type. These rules are very productive, combi</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Investigating GIS and smoothing for maximum entropy taggers. In Proceedings of the 10th Meeting of the EACL, pages 91–98, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
<author>David Vadas</author>
</authors>
<title>Multi-tagging for lexicalized-grammar parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-06,</booktitle>
<pages>697--704</pages>
<location>Sydney, Austrailia.</location>
<marker>Curran, Clark, Vadas, 2006</marker>
<rawString>James R. Curran, Stephen Clark, and David Vadas. 2006. Multi-tagging for lexicalized-grammar parsing. In Proceedings of COLING/ACL-06, pages 697–704, Sydney, Austrailia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>335--342</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1518" citStr="Hockenmaier and Steedman, 2002" startWordPosition="230" endWordPosition="233">s for adding these categories. The second proposal is to add constraints to the chart which require certain spans to be constituents. Finally, we propose partial beam search to further reduce the search space. Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage. 1 Introduction A recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars, for example LFG (Kaplan et al., 2004; Cahill et al., 2004), HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), TAG (Sarkar and Joshi, 2003) and CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b). The attraction of linguistically motivated parsers is the potential to produce rich output, in particular the predicate-argument structure representing the underlying meaning of a sentence. The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al., 2004). The C&amp;C CCG parser (Clark and Curran, 2004b) is an order of magnitude faster, but is still limited to around 25 sentences per second. The key to efficient CCG parsing is a finite-state supertagger which performs much of th</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical parsing with Combinatory Categorial Grammar. In Proceedings of the 40th Meeting of the ACL, pages 335–342, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="5563" citStr="Hockenmaier, 2003" startWordPosition="873" endWordPosition="874">t a partial beam which operates on part of the chart is most effective, giving a further 6.1% efficiency improvement. The chart repair and constraints interact in an interesting, and unexpected, manner when combined, giving a 35.7% speed improvement overall without any loss in accuracy or coverage. This speed improvement is particularly impressive because it involves techniques which only apply to 4% of Wall Street Journal sentences. 2 The CCG Parser Clark and Curran (2004b) describes the CCG parser. The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence, which are then combined by the parser using the combinatory rules. A log-linear model scores the alternative parses. We use the normal-form model, which assigns probabilities to single derivations based on the normal-form derivations in CCGbank. The features in the model are defined over local parts of the derivation and include word-word dependencies. A packed chart re</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kaplan</author>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>John T Maxwell Alexander Vasserman</author>
<author>Richard Crouch</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the 4th Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL’04),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1367" citStr="Kaplan et al., 2004" startWordPosition="205" endWordPosition="208"> chart repair, which allows the chart to be efficiently updated by adding lexical categories individually, and we evaluate several strategies for adding these categories. The second proposal is to add constraints to the chart which require certain spans to be constituents. Finally, we propose partial beam search to further reduce the search space. Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage. 1 Introduction A recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars, for example LFG (Kaplan et al., 2004; Cahill et al., 2004), HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), TAG (Sarkar and Joshi, 2003) and CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b). The attraction of linguistically motivated parsers is the potential to produce rich output, in particular the predicate-argument structure representing the underlying meaning of a sentence. The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al., 2004). The C&amp;C CCG parser (Clark and Curran, 2004b) is an order of magnitude f</context>
</contexts>
<marker>Kaplan, Riezler, King, Vasserman, Crouch, 2004</marker>
<rawString>Ron Kaplan, Stefan Riezler, Tracy H. King, John T. Maxwell III, Alexander Vasserman, and Richard Crouch. 2004. Speed and accuracy in shallow and deep stochastic parsing. In Proceedings of the Human Language Technology Conference and the 4th Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL’04), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kasami</author>
</authors>
<title>An efficient recognition and syntax analysis algorithm for context-free languages.</title>
<date>1965</date>
<tech>Technical Report AFCRL-65-758,</tech>
<institution>Air Force Cambridge Research Laboratory,</institution>
<location>Bedford, MA.</location>
<contexts>
<context position="7682" citStr="Kasami, 1965" startWordPosition="1227" endWordPosition="1228">when the parser was unable to find a spanning analysis, the chart was destroyed and then rebuilt from scratch with more lexical categories assigned to each word. However, this rebuilding process is wasteful because the new chart is always a superset of the old one and could be created by just updating the previous chart. We describe the chart repair process in Section 3 which allows additional categories to be assigned to an existing chart and the CKY algorithm run over just those parts of the chart which require modification. 2.1 Chart Parsing The parser uses the CKY chart parsing algorithm (Kasami, 1965; Younger, 1967) described in Steedman (2000). The CKY algorithm applies naturally to CCG since the grammar is binary. It builds the chart bottom-up, starting with the lexical categories spanning single words, incrementally increasing the span until the whole sentence is covered. Since the constituents are built in order of span size, at every stage 40 all the sub-constituents which could be used to create a particular new constituent are already present in the chart. The charts are packed by grouping together equivalent chart entries, which allows a large number of derivations to be represent</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>J. Kasami. 1965. An efficient recognition and syntax analysis algorithm for context-free languages. Technical Report AFCRL-65-758, Air Force Cambridge Research Laboratory, Bedford, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* parsing: Fast exact Viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology and the North American Chapter of the Association for Computational Linguistics Conference,</booktitle>
<pages>119--126</pages>
<location>Edmond, Canada.</location>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. A* parsing: Fast exact Viterbi parse selection. In Proceedings of Human Language Technology and the North American Chapter of the Association for Computational Linguistics Conference, pages 119–126, Edmond, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP-04 Workshop: Beyond</booktitle>
<location>Hainan Island, China.</location>
<marker>Malouf, van Noord, 2004</marker>
<rawString>Robert Malouf and Gertjan van Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In Proceedings of the IJCNLP-04 Workshop: Beyond shallow analyses - Formalisms and statistical modeling for deep analyses, Hainan Island, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>133--142</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="6428" citStr="Ratnaparkhi, 1996" startWordPosition="1008" endWordPosition="1010">ined by the parser using the combinatory rules. A log-linear model scores the alternative parses. We use the normal-form model, which assigns probabilities to single derivations based on the normal-form derivations in CCGbank. The features in the model are defined over local parts of the derivation and include word-word dependencies. A packed chart representation allows efficient decoding, with the Viterbi algorithm finding the most probable derivation. The supertagger uses a log-linear model to define a distribution over the lexical category set for each word and the previous two categories (Ratnaparkhi, 1996) and the forward backward algorithm efficiently sums over all histories to give a distribution for each word. These distributions are then used to assign a set of lexical categories to each word (?). The number of categories in each set is determined by a parameter Q: all categories are assigned whose forward-backward probabilities are within Q of the highest probability category (?). If the parser cannot then find a spanning analysis, the value of Q is reduced — so that more lexical categories are assigned — and the parser tries again. This process repeats until an analysis spanning the whole</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy partof-speech tagger. In Proceedings of the EMNLP Conference, pages 133–142, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
<author>Aravind Joshi</author>
</authors>
<title>Tree-adjoining grammars and its application to statistical parsing.</title>
<date>2003</date>
<editor>In Rens Bod, Remko Scha, and Khalil Sima’an, editors, Data-oriented parsing.</editor>
<publisher>CSLI.</publisher>
<contexts>
<context position="1478" citStr="Sarkar and Joshi, 2003" startWordPosition="224" endWordPosition="227">and we evaluate several strategies for adding these categories. The second proposal is to add constraints to the chart which require certain spans to be constituents. Finally, we propose partial beam search to further reduce the search space. Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage. 1 Introduction A recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars, for example LFG (Kaplan et al., 2004; Cahill et al., 2004), HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), TAG (Sarkar and Joshi, 2003) and CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b). The attraction of linguistically motivated parsers is the potential to produce rich output, in particular the predicate-argument structure representing the underlying meaning of a sentence. The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al., 2004). The C&amp;C CCG parser (Clark and Curran, 2004b) is an order of magnitude faster, but is still limited to around 25 sentences per second. The key to efficient CCG parsing is a finite-sta</context>
</contexts>
<marker>Sarkar, Joshi, 2003</marker>
<rawString>Anoop Sarkar and Aravind Joshi. 2003. Tree-adjoining grammars and its application to statistical parsing. In Rens Bod, Remko Scha, and Khalil Sima’an, editors, Data-oriented parsing. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5698" citStr="Steedman, 2000" startWordPosition="894" endWordPosition="896"> constraints interact in an interesting, and unexpected, manner when combined, giving a 35.7% speed improvement overall without any loss in accuracy or coverage. This speed improvement is particularly impressive because it involves techniques which only apply to 4% of Wall Street Journal sentences. 2 The CCG Parser Clark and Curran (2004b) describes the CCG parser. The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence, which are then combined by the parser using the combinatory rules. A log-linear model scores the alternative parses. We use the normal-form model, which assigns probabilities to single derivations based on the normal-form derivations in CCGbank. The features in the model are defined over local parts of the derivation and include word-word dependencies. A packed chart representation allows efficient decoding, with the Viterbi algorithm finding the most probable derivation. The supertagger uses a log-lin</context>
<context position="7727" citStr="Steedman (2000)" startWordPosition="1233" endWordPosition="1235">ing analysis, the chart was destroyed and then rebuilt from scratch with more lexical categories assigned to each word. However, this rebuilding process is wasteful because the new chart is always a superset of the old one and could be created by just updating the previous chart. We describe the chart repair process in Section 3 which allows additional categories to be assigned to an existing chart and the CKY algorithm run over just those parts of the chart which require modification. 2.1 Chart Parsing The parser uses the CKY chart parsing algorithm (Kasami, 1965; Younger, 1967) described in Steedman (2000). The CKY algorithm applies naturally to CCG since the grammar is binary. It builds the chart bottom-up, starting with the lexical categories spanning single words, incrementally increasing the span until the whole sentence is covered. Since the constituents are built in order of span size, at every stage 40 all the sub-constituents which could be used to create a particular new constituent are already present in the chart. The charts are packed by grouping together equivalent chart entries, which allows a large number of derivations to be represented efficiently. Entries are equivalent when t</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher Manning</author>
<author>Stuart Shieber</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Parse disambiguation for a rich HPSG grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the First Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>253--263</pages>
<location>Sozopol, Bulgaria.</location>
<contexts>
<context position="1419" citStr="Toutanova et al., 2002" startWordPosition="214" endWordPosition="217">iently updated by adding lexical categories individually, and we evaluate several strategies for adding these categories. The second proposal is to add constraints to the chart which require certain spans to be constituents. Finally, we propose partial beam search to further reduce the search space. Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage. 1 Introduction A recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars, for example LFG (Kaplan et al., 2004; Cahill et al., 2004), HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), TAG (Sarkar and Joshi, 2003) and CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b). The attraction of linguistically motivated parsers is the potential to produce rich output, in particular the predicate-argument structure representing the underlying meaning of a sentence. The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al., 2004). The C&amp;C CCG parser (Clark and Curran, 2004b) is an order of magnitude faster, but is still limited to around 25 sentences p</context>
</contexts>
<marker>Toutanova, Manning, Shieber, Flickinger, Oepen, 2002</marker>
<rawString>Kristina Toutanova, Christopher Manning, Stuart Shieber, Dan Flickinger, and Stephan Oepen. 2002. Parse disambiguation for a rich HPSG grammar. In Proceedings of the First Workshop on Treebanks and Linguistic Theories, pages 253–263, Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Iterative cky parsing for probabilistic context-free grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP conference,</booktitle>
<pages>52--60</pages>
<location>Hainan Island, China.</location>
<contexts>
<context position="3298" citStr="Tsuruoka and Tsujii (2004)" startWordPosition="516" endWordPosition="519">e time. Hence Bangalore and Joshi (1999), in the context of LTAG parsing, refer to supertagging as almost parsing. Clark and Curran (2004a) presents a novel method of integrating the supertagger and parser: initially only a small number of categories, on average, is assigned to each word, and the parser attempts to find a spanning analysis using the CKY chart-parsing algorithm. If one cannot be found, the parser requests more categories from the supertagger and builds the chart again from scratch. This process repeats until the parser is able to build a chart containing a spanning analysis.1 1Tsuruoka and Tsujii (2004) investigate a similar idea in the context of the CKY algorithm for a PCFG. 39 Proceedings of the 10th Conference on Parsing Technologies, pages 39–47, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics The supertagging accuracy is high enough that the parser fails to find a spanning analysis using the initial category assignment in approximately 4% of Wall Street Journal sentences (?). However, parsing this 4%, which largely consists of the longer sentences, is disproportionately expensive. This paper describes several modifications to the C&amp;C parser which imp</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2004</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2004. Iterative cky parsing for probabilistic context-free grammars. In Proceedings of the IJCNLP conference, pages 52– 60, Hainan Island, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Younger</author>
</authors>
<title>Recognition and parsing of contextfree languages in time n3.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="7698" citStr="Younger, 1967" startWordPosition="1229" endWordPosition="1230">r was unable to find a spanning analysis, the chart was destroyed and then rebuilt from scratch with more lexical categories assigned to each word. However, this rebuilding process is wasteful because the new chart is always a superset of the old one and could be created by just updating the previous chart. We describe the chart repair process in Section 3 which allows additional categories to be assigned to an existing chart and the CKY algorithm run over just those parts of the chart which require modification. 2.1 Chart Parsing The parser uses the CKY chart parsing algorithm (Kasami, 1965; Younger, 1967) described in Steedman (2000). The CKY algorithm applies naturally to CCG since the grammar is binary. It builds the chart bottom-up, starting with the lexical categories spanning single words, incrementally increasing the span until the whole sentence is covered. Since the constituents are built in order of span size, at every stage 40 all the sub-constituents which could be used to create a particular new constituent are already present in the chart. The charts are packed by grouping together equivalent chart entries, which allows a large number of derivations to be represented efficiently. </context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>D. Younger. 1967. Recognition and parsing of contextfree languages in time n3. Information and Control, 10(2):189–208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>