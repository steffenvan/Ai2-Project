<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99885">
Improving Context Vector Models by Feature Clustering for Auto-
matic Thesaurus Construction
</title>
<author confidence="0.998103">
Jia-Ming You Keh-Jiann Chen
</author>
<affiliation confidence="0.835914">
Institute of Information Science Institute of Information Science
Academia Sinica Academia Sinica
</affiliation>
<email confidence="0.99284">
swimming@hp.iis.sinica.edu.tw kchen@iis.sinica.edu.tw
</email>
<sectionHeader confidence="0.994627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999315">
Thesauruses are useful resources for NLP;
however, manual construction of thesau-
rus is time consuming and suffers low
coverage. Automatic thesaurus construc-
tion is developed to solve the problem.
Conventional way to automatically con-
struct thesaurus is by finding similar
words based on context vector models
and then organizing similar words into
thesaurus structure. But the context vec-
tor methods suffer from the problems of
vast feature dimensions and data sparse-
ness. Latent Semantic Index (LSI) was
commonly used to overcome the prob-
lems. In this paper, we propose a feature
clustering method to overcome the same
problems. The experimental results show
that it performs better than the LSI mod-
els and do enhance contextual informa-
tion for infrequent words.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913923076923">
Thesaurus is one of the most useful linguistic
resources. It provides information more than just
synonyms. For example, in WordNet (Fellbaum,
1998), it also builds up relations between syno-
nym sets, such as hyponym, hypernym. There are
two Chinese thesauruses Cilin(1983) and
Hownet1. Cilin provides synonym sets with sim-
ple hierarchical structure. Hownet uses some
primitive senses to describe word meanings. The
common primitive senses provide additional re-
lations between words implicitly. However,
many words occurred in contemporary news cor-
pora are not covered by Chinese thesauruses.
</bodyText>
<footnote confidence="0.8523875">
1 http://www.HowNet.com(Dong Zhendong, Dong
Qiang:HowNet)
</footnote>
<bodyText confidence="0.999846238095238">
Therefore, we intend to create a thesaurus
based on contemporary news corpora. The com-
mon steps to automatically construct a thesaurus
include a) contextual information extraction, b)
finding synonym words and c) organizing syno-
nym words into a thesaurus. The approach is
based upon the fact that word meaning lays on its
contextual behavior. If words act similarly in
context, they may share the same meaning.
However, the method can only handle frequent
words rather than infrequent ones. In fact most of
vocabularies occur infrequently, one has to dis-
cover extend information to overcome the data
sparseness problem. We will introduce the con-
ventional approaches for automatic thesaurus
construction in section 2. Follow a discussion
about the problems and solutions of context vec-
tor models in section 3. In section 4, we use two
performance evaluation metrics, i.e. discrimina-
tion and nonlinear interpolated precision, to
evaluate our proposed method.
</bodyText>
<sectionHeader confidence="0.9050435" genericHeader="introduction">
2 Conventional approaches for auto-
matic thesaurus construction
</sectionHeader>
<bodyText confidence="0.994182">
The conventional approaches for automatic the-
saurus construction include three steps: (1) Ac-
quire contextual behaviors of words from cor-
pora. (2) Calculate the similarity between words.
(3) Finding similar words and then organizing
into a thesaurus structure.
</bodyText>
<subsectionHeader confidence="0.956272">
2.1 Acquire word sense knowledge
</subsectionHeader>
<bodyText confidence="0.999834111111111">
One can model word meanings by their co-
occurrence context. The common ways to extract
co-occurrence contextual words include simple
window based and syntactic dependent based
(You, 2004). Obviously, syntactic dependent
relations carry more accurate information than
window based. Also, it can bring additional in-
formation, such as POS (part of speech) and se-
mantic roles etc. To extract the syntactic de-
</bodyText>
<page confidence="0.828075">
1
</page>
<note confidence="0.667433">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 1–8,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.995013769230769">
pended relation, a raw text has to be segmented,
POS tagged, and parsed. Then the relation ex-
tractor identifies the head-modifier relations
and/or head-argument relations. Each relation
could be defined as a triple (w, r, c), where w is
the thesaurus term, c is the co-occurred context
word and r is the relation between w and c.
Then context vector of a word is represented
differently by different models, such as: tf,
weight-tf, Latent Semantic Indexing (LSI)
(Deerwester, S.,et al., 1990) and Probabilistic
LSI (Hofmann, 1999). The context vectors of
word x can be express by:
</bodyText>
<equation confidence="0.998974">
x x x
a) tf model: word x = {tf ,... , tf },where x
, tf tf is
1 2 n i
</equation>
<bodyText confidence="0.98063325">
the term frequency of the ith context word when
given word x.
b) weight-tf model: assume there are n contex-
tual words and m target words. word x=
</bodyText>
<equation confidence="0.99933">
x x x
{tf × weight 1, tf × weight 2.. .tf weight }
n × n
1 2
</equation>
<bodyText confidence="0.9984885">
,where weighti, we used here, is defined as
[logm-entropy(wordi)]/logm
Then calculate the distance between probabil-
istic vectors by sums up the all probabilistic dif-
ference among each context word so called cross
entropy.
</bodyText>
<equation confidence="0.689235">
KL Distance : KL(p, q) =∑p(i) • log2 pi qi
1
</equation>
<bodyText confidence="0.9943936">
Due to the original KL distance is asymmetric
and is not defined when zero frequency occurs.
Some enhanced KL models were developed to
prevent these problems such as Jensen-Shannon
(Jianhua, 1991), which introducing a probabilis-
tic variable m, or α -Skew Divergence (Lee,
1999), by adopting adjustable variable α. Re-
search shows that Skew Divergence achieves
better performance than other measures. (Lee,
2001)
</bodyText>
<equation confidence="0.999406538461539">
y
D(SkewDivergence) S
= a = KL x ax + − a y
x (  ||(1 )
x
1
x
p2
x
n} ,
x
pi
x
tf i
n
∑k=
x
1tfk
=
=
,
,
x
1
x
q2
x
qi
tfiy
n
∑k=
y
1tfk
=
q
=
=
wordx
p
{p
,...p
wordy
{q
,...q
x
n } ,
i=
n
)
m i i ; p (wordi ) D(Jensen -Shannon) = JS ( x, y) { (   ||) (  ||) } / 2,
entropy(wordi) = ∑ 1− p(wordk)log p(wordk) k = KL x m KL y m
k= +
</equation>
<bodyText confidence="0.9903842">
is the co-occurrence probability of wordk when m= (x + y) / 2
given wordi.
c) LSI or PLSI models: using tf or weighted-tf
co-occurrence matrix and by adopting LSI or
PLSI to reduce the dimension of the matrix.
</bodyText>
<subsectionHeader confidence="0.999193">
2.2 Similarity between words
</subsectionHeader>
<bodyText confidence="0.999274">
The common similarity functions include
</bodyText>
<listItem confidence="0.801671">
a) Adopting simple frequency feature, such as
cosine, which computes the angle between two
context vectors;
b) Represent words by the probabilistic distribu-
tion among contexts, such as Kull-Leiber diver-
gence (Cover and Thomas, 1991).
</listItem>
<bodyText confidence="0.9997025">
The first step is to convert the co-occurrence
matrix into a probabilistic matrix by simple for-
mula.
To convert distance to similarity value, we
adopt the formula inspired by Mochihashi, and
Matsumoto 2002.
similarity(wordx, wordy) exp{ distance( , ) }
= − X ⋅ x y
</bodyText>
<subsectionHeader confidence="0.999096">
2.3 Organize similar words into thesaurus
</subsectionHeader>
<bodyText confidence="0.999924666666667">
There are several clustering methods can be used
to cluster similar words. For example, by select-
ing N target words as the entries of a thesaurus,
then extract top-n similar words for each entry;
adopting HAC(Hierarchical agglomerative clus-
tering, E.M. Voorhees,1986) method to cluster
the most similar word pairs in each clustering
loop. Eventually, these similar words will be
formed into synonyms sets.
</bodyText>
<sectionHeader confidence="0.987749" genericHeader="method">
3 Difficulties and Solutions
</sectionHeader>
<bodyText confidence="0.997854">
There are two difficulties of using context vector
models. One is the enormous dimensions of con-
</bodyText>
<equation confidence="0.913832666666667">
•
x
y
=
y)
cos(x,
×
x
y
</equation>
<page confidence="0.85732">
2
</page>
<bodyText confidence="0.9999437">
textual words, and the other is data sparseness
problem. Conventionally LSI or PLSI methods
are used to reduce feature dimensions by map-
ping literal words into latent semantic classes.
The researches show that it’s a promising
method (April Kontostathis, 2003). However the
latent semantic classes also smooth the informa-
tion content of feature vectors. Here we proposed
a different approach to cope with the feature re-
duction and data sparseness problems.
</bodyText>
<subsectionHeader confidence="0.999109">
3.1 Feature Clustering
</subsectionHeader>
<bodyText confidence="0.954889566666666">
Reduced feature dimensions and data sparseness
cause the problem of inaccurate contextual in-
formation. In general, one has to reduce the fea-
ture dimensions for computational feasibility and
also to extend the contextual word information to
overcome the problem of insufficient context
information.
In our experiments, we took the clustered-
feature approaches instead of LSI to cope with
these two problems and showed better perform-
ances. The idea of clustered-feature approaches
is by adopting the classes of clustering result of
the frequent words as the new set of features
which has less feature dimensions and context
words are naturally extend to their class mem-
bers. We followed the steps described in section
2 to develop the synonyms sets. First, the syntac-
tic dependent relations were extracted to create
the context vectors for each word. We adopted
the skew divergence as the similarity function,
which is reported to be the suitable similarity
function (Masato, 2005), to measure the distance
between words.
We used HAC algorithm to develop the syno-
nyms classes, which is a greedy method, simply
to cluster the most similar word pairs at each
clustering iteration.
The HAC clustering process:
While the similarity of the most similar word pair
(wordx, wordy) is greater than a threshold ε
</bodyText>
<figure confidence="0.46934525">
then cluster wordx, wordy together and replace it with
the centroid between wordx and wordy
Recalculate the similarity between other words and
the centroid
</figure>
<subsectionHeader confidence="0.993115">
3.2 Clustered-Feature Vectors
</subsectionHeader>
<bodyText confidence="0.99999225">
We obtain the synonyms sets S from above HAC
method. Let the extracted synonyms sets S = { S1,
S2,...SR} which contains R synonym classes;
Sj stands for the jth element of the ith synonym
</bodyText>
<equation confidence="0.767081833333333">
i
class; the ith synonym class Si contains Qi ele-
ments.
S S
R R
1 2
</equation>
<bodyText confidence="0.997667181818182">
The feature extension processing transforms
the coordination from literal words to synonyms
sets. Assume there are N contextual words
{C1,C2,...CN}, and the first step is to transform
the context vector of of Ci to the distribution vec-
tor among S. Then the new feature vector is the
summation of the distribution vectors among S
of its all contextual words.
,where tfi is the term f
pS , requency of the context
word Ci occurs with wor
</bodyText>
<equation confidence="0.9058396">
.
{
dj
bution Vector among_S( Ci )= &apos;P2 S P
i i i
</equation>
<bodyText confidence="0.955731666666667">
context words of at the synonyms S j .
Ci jth
simple cosine function to measure the similari
ty
between these transformed clustered-feature vec-
tors.
</bodyText>
<sectionHeader confidence="0.998949" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.998159">
spectively. We wan
t to see the effects of feature
reduction and feature extension for both frequent
and infrequent words.
</bodyText>
<figure confidence="0.988853666666667">
2 2
S ... S
2 Q
... ... ... ...
S
2
S2
1
Qj
,
Ci
j
q
freq S
(
1
∑
=
q
=
,
</figure>
<figureCaption confidence="0.493942">
means the distributi
</figureCaption>
<bodyText confidence="0.888618625">
where
Distri
Due to the transformed coordination no longer
stands for either frequency or probability, we use
To evaluate the performance of the feature clus-
tering method, we had prepared two sets of test-
ing data with high and low frequency words re-
on of
</bodyText>
<figure confidence="0.990636">
)
,
PSj
i
(Ci)
freq
1 1
S ... S
2 Q
S1
1
...
S R
QR
⎤
1 ⎥
⎥
⎥
⎥
⎦⎥
The new feature vector of wordj =
N j Distribution_Vector_among_S( Ci )
∑ tf ×
i
i =1
</figure>
<page confidence="0.962184">
3
</page>
<subsectionHeader confidence="0.978208">
4.1 Discrimination Rates
</subsectionHeader>
<bodyText confidence="0.947653285714286">
The discrimination rate is used to examine the
capability of distinguishing the correlation be-
tween words. Given a word pair (wordi,wordj),
one has to decide whether the word pair is simi-
lar or not. Therefore, we will arrange two differ-
ent word pair sets, related and unrelated, to esti-
mate the discrimination. By given the formula
below
Discrimination rate = 1 æ na + nb
2 Na Nb
,where Na and Nb are respectively the numbers
of synonym word pairs and unrelated word pairs.
As well as, na and nb are the numbers of correct
labeled pairs in synonyms and unrelated words.
</bodyText>
<subsectionHeader confidence="0.792384">
4.2 Nonlinear interpolated precision
</subsectionHeader>
<bodyText confidence="0.982781111111111">
The Nap evaluation is used to measure the per-
formance of restoring words to taxonomy, a
similar task of restoring words in WordNet
(Dominic Widdows, 2003).
The way we adopted Nap evaluation is to re-
construct a partial Chinese synonym set, and
measure the structure resemblance between
original synonyms and the reconstructed one. By
doing so, one has to prepare certain number of
synonyms sets from Chinese taxonomy, and try
to reclassify these words.
Assume there are n testing words distributed
in R synonyms sets. Let i
R 1 stands for the repre-
sented word of the ith synonyms set. Then we
will compute the similarity ranking between each
represented word and the rest n-1 testing words.
By given formula
</bodyText>
<equation confidence="0.724299">
NAP 1 R n−1 Zi j−1
å å jç1+åZk
R i=1 j=1 j èk=1 ø
</equation>
<bodyText confidence="0.6182">
Sj represents the jth similar word of i
i R 1 among
the rest n-1 words
</bodyText>
<equation confidence="0.9304485">
r 1, if S� and R, are synonym
= jl 0 þ
</equation>
<bodyText confidence="0.999981">
The NAP value means how many percent
synonyms can be identified. The maximum value
of NAP is 1, means the extracted similar words
are exactly match to the synonyms.
</bodyText>
<sectionHeader confidence="0.998635" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999957882352941">
The context vectors were derived from a 10
year news corpus from The Central News
Agency. It contains nearly 33 million sentences,
234 million word tokens, and we extracted 186
million syntactic relations from this corpus. Due
to the low reliability of infrequent data, only the
relation triples (w, r, c), which occurs more than
3 times and POS of w and c must be noun or
verb, are used. It results that nearly 30,000 high
frequent nouns and verbs are used as the contex-
tual features. And with feature clustering2, the
contextual dimensions were reduced from 30,988
literal words to 12,032 semantic classes.
In selecting testing data, we consider the
words that occur more than 200 times as high
frequent words and the frequencies range from
40 to 200 as low frequent words.
</bodyText>
<sectionHeader confidence="0.841704" genericHeader="method">
Discrimination
</sectionHeader>
<bodyText confidence="0.968384222222222">
For the discrimination experiments, we randomly
extract high frequent word pairs which include
500 synonym pairs and 500 unrelated word pairs
from Cilin (Mei et. al, 1983). At the mean time,
we also prepare equivalent low frequency data.
We use a mathematical technique Singular
Value Decomposition (SVD) to derive principal
components and to implement LSI models with
respect to different feature dimensions from 100
to 1000. We compare the performances of differ-
ent models. The results are shown in the follow-
ing figures.
Figure1. Discrimination for high frequent words
The result shows that for the high frequent
data, although the feature clustering method did
not achieve the best performance, it perform-
ances better at related data and a balanced per-
formance at unrelated data. The tradeoffs be-
</bodyText>
<footnote confidence="0.823382666666667">
2 Some feature clustering results are listed in the Ap-
pendix
Zji
</footnote>
<page confidence="0.995578">
4
</page>
<bodyText confidence="0.999932333333333">
tween related recalls and unrelated recalls are
clearly shown. Another observation is that no
matter of using LSI or literal word features (tf or
weight_tf), the performances are comparable.
Therefore, we could simply use any method to
handle the high frequent words.
</bodyText>
<subsubsectionHeader confidence="0.883301">
Figure2 Discrimination for low frequent word
</subsubsectionHeader>
<bodyText confidence="0.999964">
For the infrequent words experiments, neither
LSI nor weighted-tf performs well due to insuffi-
cient contextual information. But by introducing
feature clustering method, one can gain more 6%
accuracy for the related data. It shows feature
clustering method could help gather more infor-
mation for the infrequent words.
</bodyText>
<subsectionHeader confidence="0.609474">
Nonlinear interpolated precision
</subsectionHeader>
<bodyText confidence="0.9997488">
For the Nap evaluation, we prepared two testing
data from Cilin and Hownet. In the high frequent
words experiments, we extract 1311 words
within 352 synonyms sets from Cilin and 2981
words within 570 synonyms sets from Hownet.
</bodyText>
<figureCaption confidence="0.97677">
Figure 3. Nap performance for high frequent words
</figureCaption>
<bodyText confidence="0.999836">
In high frequent experiments, the results show
that the models retaining literal form perform
better than dimension reduction methods. It
means in the task of measuring similarity of high
frequent words using literal contextual feature
vectors is more precise than using dimension
reduction feature vectors.
In the infrequent words experiments, we can
only extract 202 words distributed in 62 syno-
nyms sets from Cilin and 1089 words within 222
synonyms sets. Due to fewer testing words, LSI
was not applied in this experiment.
</bodyText>
<figureCaption confidence="0.977803">
Figure 4. Nap performance for low frequent words
</figureCaption>
<bodyText confidence="0.999867">
It shows with insufficient contextual informa-
tion, the feature clustering method could not help
in recalling synonyms because of dimensional
reduction.
</bodyText>
<sectionHeader confidence="0.934308" genericHeader="method">
6. Error Analysis and Conclusion
</sectionHeader>
<bodyText confidence="0.999210448275862">
Using context vector models to construct thesau-
rus suffers from the problems of large feature
dimensions and data sparseness. We propose a
feature clustering method to overcome the prob-
lems. The experimental results show that it per-
forms better than the LSI models in distinguish-
ing related/unrelated pairs for the infrequent data,
and also achieve relevant scores on other evalua-
tions.
Feature clustering method could raise the abil-
ity of discrimination, but not robust enough to
improve the performance in extracting synonyms.
It also reveals the truth that it’s easy to distin-
guish whether a pair is related or unrelated once
the word pair shares the same sense in their
senses. However, it’s not the case when seeking
synonyms. One has to discriminate each sense
for each word first and then compute the similar-
ity between these senses to achieve synonyms.
Because feature clustering method lacks the abil-
ity of senses discrimination of a word, the
method can handle the task of distinguishing cor-
relation pairs rather than synonyms identification.
Also, after analyzing discrimination errors
made by context vector models, we found that
some errors are not due to insufficient contextual
information. Certain synonyms have dissimilar
contextual contents for different reasons. We
observed some phenomenon of these cases:
</bodyText>
<page confidence="0.983635">
5
</page>
<bodyText confidence="0.963942930232558">
a) Some senses of synonyms in testing data are
not their dominant senses.
Take guang1hua2 (光華) for example, it has a
sense of “splendid” which is similar to the sense
of guang1mang2 ( 光 芒 ). Guang1hua2 and
guang1mang2 are certainly mutually changeable
in a certain degree, guang1hua2jin4shi4 (光華盡
失) and guang1mang2jin4shi4 (光芒盡失), or
xi2ri4guang1hua2 ( 昔 日 光 華 ) and
xi2ri4guang1mang2 (昔日光芒). However, the
dominated contextual sense of guang1hua2 is
more likely to be a place name, like
guang1hua2shi4chang3( 光 華 市 場 ) or
hua1lian2guang1hua2 (花蓮光華) etc3.
b) Some synonyms are different in usages for
pragmatic reasons.
Synonyms with different contextual vectors
could be result from different perspective views.
For example, we may view wai4jie4 (外界) as a
container image with viewer inside, but on the
other hand, yi3wai4 (以外) is an omnipotence
perspective. This similar meaning but different
perspective makes distinct grammatical usage
and different collocations.
Similarly, zhong1shen1 (終身) and sheng1ping2
( 生 平 ) both refer to “life-long time”.
zhong1shen1 explicates things after a time point,
which differs from sheng1ping2, showing mat-
ters before a time point.
to kind of illness. Then the corpus reinterpret
wa1wa1 (娃娃) as a sick people, due to it occurs
with medical term. But the synonym of wa1wa1
( 娃娃 ), xiao3peng2you3( 小 朋 友 ) stands for
money in some finance news. Therefore, the
meanings of words change from time to time. It’s
hard to decide whether meaning is the right an-
swer when finding synonyms.
With above observations, our future researches
will be how to distinguish different word senses
from its context features. Once we could distin-
guish the corresponding features for different
senses, it will help us to extract more accurate
synonyms for both frequent and infrequent words.
</bodyText>
<sectionHeader confidence="0.996434" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.945222777777778">
April Kontostathis, William M. Pottenger 2003. , A
Framework for Understanding LSI Performance, In
the Proceedings of the ACM SIGIR Workshop on
Mathematical/Formal Methods in Information Re-
trieval, Annual International SIGIR Conference, 2003.
Christiance Fellbaum, editor 1998,WordNet: An alec-
tronic lwxical database. MIT press, Cambrige MA.
Deerwester, S.,et al. 1990 Indexing by Latent Seman-
tic Analysis. Jorunal of the American Society for In-
formation Science, 41(6):391-407
Dominic Widdows. 2003. Unsupervised methods for
developing taxonomies by combining syntactic and
statistical information. In Proceeding of HLT-NAACL
2003 Main papers, pp, 197-204.
E.M. Voorhees, “Implement agglomerative hierarchi-
cal clustering algorithm for use in document re-
trieval”, Information Processing &amp; Management. , no.
22 pp.46-476,1986
</reference>
<figure confidence="0.996379272727273">
Wai4jie4
外界
viewer
Omnipotence viewer
Yi3wai4 以外
•
•
Zhong1shen1
終身
sheng1ping2
生平
</figure>
<figureCaption confidence="0.484427">
Hofmann, T.1999. Probabilistic Latent Semantic
Indexing. Proc.of the 22nd International conference on
Research and Development in Information Retrieval
(SIGIR’99),50-57
</figureCaption>
<bodyText confidence="0.734185666666667">
c) Domain specific usages.
For example, in medical domain news ,wa1wa1
(娃娃) occurs frequently with bo1li2 (玻璃) refer
</bodyText>
<footnote confidence="0.758634666666667">
3 This may due to different genres. In newspapers the
proper noun usage of guang1hua2 is more common
than in a literature text.
James R.Curran and Marc Moens. 2002. Improve-
ments in Automatic Thesaurus Extraction. Proceed-
ings of the Workshop of the ACL Special Interest
Group on the Lexicon (SIGLEX), pp. 59-66
Jia-Ming You and Keh-Jiann Chen, 2004 Automatic
Semantic Role Assignment for a Tree Structure, Pro-
</footnote>
<page confidence="0.995857">
6
</page>
<reference confidence="0.997148434782609">
ceedings of 3rd ACL SIGHAN Workshop
Jiahua Lin. 1991. Divergence measures based on the
Shannon Entropy. IEEE transactions on Information
Theory, 37(1): 145-151
Lillian Lee. 2001. On the effectiveness of the skew
divergence for statistical language analysis. In Artifi-
cial Intelligence and Statistics 2001, page 65-72.
Lillian Lee. 1999. Measure of distributional similarity.
In Proceeding of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-1999),
page 23-32.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2005. PLSI Utilization for Automatic The-
saurus Construction. IJCNLP 2005, LNAI 651, pp.
334-345.
Mei,Jiaju,Yiming Lan, Yunqi Gao, Yongxian Ying
(1983) ! &amp;quot;&amp;quot; # [ A Dictionary of Syno-
nyms],Shanghai Cishu Chubanshe.
Mochihashi, D., Matsumoto, Y.2002. Probabilistic
Representation of Meanings. IPSJ SIG Notes Natural
Language, 2002-NL-147:77-84.
T.Cover and J.Thomas, 1991. Element of Information
Theory. Wiley &amp; sons, New York
</reference>
<page confidence="0.999734">
7
</page>
<sectionHeader confidence="0.934101" genericHeader="method">
Appendix:
</sectionHeader>
<bodyText confidence="0.779716">
Some feature clustering results
</bodyText>
<equation confidence="0.999453486111111">
一下 一陣子
一千多 四百多 一百多 二百多
一切 災難性
一月份 二月份 類型
一生 畢生 一輩子 生平
一年級 國一 大學部
一成 三成 兩成 二成
一百多萬 三百多萬 一千多萬
一段 大半 較多 多一點 空檔 美東 間隔 尖峰 睡眠
美西 需要
一家人 家人 親人
一席之地 優勢
一氧化碳 沼氣 食物 河豚 粉塵
一級 黨務 行庫 原由 二級
一般性 計畫型
一號機 二號機
一銀 二銀 五金
一審 原審 陪審團 審法院
一樓 大會堂 中庭
一舉一動 動向 事項 言行 舉止
一體 中西文
乙級 技術士 廚師 中餐
丁等 甲等 乙等 丙等 中醫師
優等
七人 九人 六人 九人決策
七夕 西洋
七月號 月刊 八月號 雜誌 消息報 週報 二月號
七成 六成 八成 五成 四成 九成
七股 鰲鼓
七美 東引
九孔 草蝦 鰻魚 石斑魚 虱目魚 文蛤 吳郭魚 牡蠣
甲魚 箱網 蝦子 魚蝦 蚵仔 黑鯛 魚群 蝸牛
九份 草嶺 國姓鄉 瑞芳鎮 竹北市
二仁溪 大漢溪 淡水河 漢江 新店溪 游泳池 泳池 鴨
綠江 朴子溪 後龍溪 農漁局
二月 十二月 九月 十一月 十月 七月 元月
二年制 四年制
二年級 五年級 大二
二次大戰 第二次世界大戰 大戰 韓戰
二兵 上士 准將 新聞官
二者 兩者 三者
二金 三金 一金
二段 三段
二胡 琵琶 古箏 吉他
二重 疏洪道
二氧化碳 廢氣 污染物 廢水 二氧化硫 氣體 柴油車
氧化物 臭氣 污染源
二專 四技二專 學校院 校院
二組 三組 五組 八組 標準組 組別 梯隊
二號 三號 四號 五號 一號 太原 風雲 型號
二路 三路
二線 三線 四線
二壘 三壘 隊友
二讀會 三讀會 讀會 提案
人人 舉世 舉國 兩性
人力 物力 頻寬
人口 人數 人口數 救濟金 大軍 週數 戶數 家數
次
數 隊數 保險金 頻率 斷面
人才 人材 師資 運動選手 增長點 搖籃 英才 專才
人文 美學 藝能 科展
人文組 數理組
人犯 罪犯 煙毒犯
人生 寶島 山m
人生IN W值IN IN念
人次 車次
人行道 V樓
人身 信仰 言論 性行為
人事費 醫療費f 利息 保費 保險費
人fa 黨代表 會in S親 股東 社in 締約國
人命 性命 生命
人物 學府 旅遊點 傑作 寶庫 勁旅
</equation>
<page confidence="0.985676">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.360739">
<title confidence="0.998249">Context Vector Models by Feature Clustering for matic Thesaurus Construction</title>
<author confidence="0.996891">Jia-Ming You Keh-Jiann Chen</author>
<affiliation confidence="0.999183">Institute of Information Science Institute of Information Science</affiliation>
<author confidence="0.465899">Academia Sinica Academia Sinica</author>
<email confidence="0.735003">swimming@hp.iis.sinica.edu.twkchen@iis.sinica.edu.tw</email>
<abstract confidence="0.997667952380953">Thesauruses are useful resources for NLP; however, manual construction of thesaurus is time consuming and suffers low coverage. Automatic thesaurus construction is developed to solve the problem. Conventional way to automatically construct thesaurus is by finding similar words based on context vector models and then organizing similar words into thesaurus structure. But the context vector methods suffer from the problems of vast feature dimensions and data sparseness. Latent Semantic Index (LSI) was commonly used to overcome the problems. In this paper, we propose a feature clustering method to overcome the same problems. The experimental results show that it performs better than the LSI models and do enhance contextual information for infrequent words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>April Kontostathis</author>
<author>William M Pottenger</author>
</authors>
<title>A Framework for Understanding LSI Performance,</title>
<date>2003</date>
<booktitle>In the Proceedings of the ACM SIGIR Workshop on Mathematical/Formal Methods in Information Retrieval, Annual International SIGIR Conference,</booktitle>
<marker>Kontostathis, Pottenger, 2003</marker>
<rawString>April Kontostathis, William M. Pottenger 2003. , A Framework for Understanding LSI Performance, In the Proceedings of the ACM SIGIR Workshop on Mathematical/Formal Methods in Information Retrieval, Annual International SIGIR Conference, 2003.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christiance Fellbaum</author>
</authors>
<title>editor 1998,WordNet: An alectronic lwxical database.</title>
<publisher>MIT press,</publisher>
<location>Cambrige MA.</location>
<marker>Fellbaum, </marker>
<rawString>Christiance Fellbaum, editor 1998,WordNet: An alectronic lwxical database. MIT press, Cambrige MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Jorunal of the American Society for Information Science,</journal>
<pages>41--6</pages>
<marker>Deerwester, 1990</marker>
<rawString>Deerwester, S.,et al. 1990 Indexing by Latent Semantic Analysis. Jorunal of the American Society for Information Science, 41(6):391-407</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Unsupervised methods for developing taxonomies by combining syntactic and statistical information.</title>
<date>2003</date>
<booktitle>In Proceeding of HLT-NAACL 2003 Main papers,</booktitle>
<pages>197--204</pages>
<contexts>
<context position="11105" citStr="Widdows, 2003" startWordPosition="1886" endWordPosition="1887"> to decide whether the word pair is similar or not. Therefore, we will arrange two different word pair sets, related and unrelated, to estimate the discrimination. By given the formula below Discrimination rate = 1 æ na + nb 2 Na Nb ,where Na and Nb are respectively the numbers of synonym word pairs and unrelated word pairs. As well as, na and nb are the numbers of correct labeled pairs in synonyms and unrelated words. 4.2 Nonlinear interpolated precision The Nap evaluation is used to measure the performance of restoring words to taxonomy, a similar task of restoring words in WordNet (Dominic Widdows, 2003). The way we adopted Nap evaluation is to reconstruct a partial Chinese synonym set, and measure the structure resemblance between original synonyms and the reconstructed one. By doing so, one has to prepare certain number of synonyms sets from Chinese taxonomy, and try to reclassify these words. Assume there are n testing words distributed in R synonyms sets. Let i R 1 stands for the represented word of the ith synonyms set. Then we will compute the similarity ranking between each represented word and the rest n-1 testing words. By given formula NAP 1 R n−1 Zi j−1 å å jç1+åZk R i=1 j=1 j èk=1</context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>Dominic Widdows. 2003. Unsupervised methods for developing taxonomies by combining syntactic and statistical information. In Proceeding of HLT-NAACL 2003 Main papers, pp, 197-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Implement agglomerative hierarchical clustering algorithm for use in document retrieval”,</title>
<date>1991</date>
<journal>IEEE transactions on Information Theory,</journal>
<booktitle>Information Processing &amp; Management. , no. 22 pp.46-476,1986 ceedings of 3rd ACL SIGHAN Workshop Jiahua Lin.</booktitle>
<volume>37</volume>
<issue>1</issue>
<pages>145--151</pages>
<marker>Voorhees, 1991</marker>
<rawString>E.M. Voorhees, “Implement agglomerative hierarchical clustering algorithm for use in document retrieval”, Information Processing &amp; Management. , no. 22 pp.46-476,1986 ceedings of 3rd ACL SIGHAN Workshop Jiahua Lin. 1991. Divergence measures based on the Shannon Entropy. IEEE transactions on Information Theory, 37(1): 145-151</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>On the effectiveness of the skew divergence for statistical language analysis.</title>
<date>2001</date>
<booktitle>In Artificial Intelligence and Statistics</booktitle>
<pages>65--72</pages>
<contexts>
<context position="5114" citStr="Lee, 2001" startWordPosition="799" endWordPosition="800">ogm Then calculate the distance between probabilistic vectors by sums up the all probabilistic difference among each context word so called cross entropy. KL Distance : KL(p, q) =∑p(i) • log2 pi qi 1 Due to the original KL distance is asymmetric and is not defined when zero frequency occurs. Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991), which introducing a probabilistic variable m, or α -Skew Divergence (Lee, 1999), by adopting adjustable variable α. Research shows that Skew Divergence achieves better performance than other measures. (Lee, 2001) y D(SkewDivergence) S = a = KL x ax + − a y x ( ||(1 ) x 1 x p2 x n} , x pi x tf i n ∑k= x 1tfk = = , , x 1 x q2 x qi tfiy n ∑k= y 1tfk = q = = wordx p {p ,...p wordy {q ,...q x n } , i= n ) m i i ; p (wordi ) D(Jensen -Shannon) = JS ( x, y) { ( ||) ( ||) } / 2, entropy(wordi) = ∑ 1− p(wordk)log p(wordk) k = KL x m KL y m k= + is the co-occurrence probability of wordk when m= (x + y) / 2 given wordi. c) LSI or PLSI models: using tf or weighted-tf co-occurrence matrix and by adopting LSI or PLSI to reduce the dimension of the matrix. 2.2 Similarity between words The common similarity functions</context>
</contexts>
<marker>Lee, 2001</marker>
<rawString>Lillian Lee. 2001. On the effectiveness of the skew divergence for statistical language analysis. In Artificial Intelligence and Statistics 2001, page 65-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measure of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceeding of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-1999),</booktitle>
<pages>23--32</pages>
<contexts>
<context position="4981" citStr="Lee, 1999" startWordPosition="780" endWordPosition="781">d x= x x x {tf × weight 1, tf × weight 2.. .tf weight } n × n 1 2 ,where weighti, we used here, is defined as [logm-entropy(wordi)]/logm Then calculate the distance between probabilistic vectors by sums up the all probabilistic difference among each context word so called cross entropy. KL Distance : KL(p, q) =∑p(i) • log2 pi qi 1 Due to the original KL distance is asymmetric and is not defined when zero frequency occurs. Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991), which introducing a probabilistic variable m, or α -Skew Divergence (Lee, 1999), by adopting adjustable variable α. Research shows that Skew Divergence achieves better performance than other measures. (Lee, 2001) y D(SkewDivergence) S = a = KL x ax + − a y x ( ||(1 ) x 1 x p2 x n} , x pi x tf i n ∑k= x 1tfk = = , , x 1 x q2 x qi tfiy n ∑k= y 1tfk = q = = wordx p {p ,...p wordy {q ,...q x n } , i= n ) m i i ; p (wordi ) D(Jensen -Shannon) = JS ( x, y) { ( ||) ( ||) } / 2, entropy(wordi) = ∑ 1− p(wordk)log p(wordk) k = KL x m KL y m k= + is the co-occurrence probability of wordk when m= (x + y) / 2 given wordi. c) LSI or PLSI models: using tf or weighted-tf co-occurrence m</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measure of distributional similarity. In Proceeding of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-1999), page 23-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masato Hagiwara</author>
<author>Yasuhiro Ogawa</author>
<author>Katsuhiko Toyama</author>
</authors>
<title>PLSI Utilization for Automatic Thesaurus Construction.</title>
<date>2005</date>
<booktitle>IJCNLP 2005, LNAI 651,</booktitle>
<pages>334--345</pages>
<marker>Hagiwara, Ogawa, Toyama, 2005</marker>
<rawString>Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama. 2005. PLSI Utilization for Automatic Thesaurus Construction. IJCNLP 2005, LNAI 651, pp. 334-345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiaju Mei</author>
<author>Yiming Lan</author>
</authors>
<title>Yunqi Gao, Yongxian Ying</title>
<date>1983</date>
<journal>[ A Dictionary of Synonyms],Shanghai Cishu Chubanshe.</journal>
<marker>Mei, Lan, 1983</marker>
<rawString>Mei,Jiaju,Yiming Lan, Yunqi Gao, Yongxian Ying (1983) ! &amp;quot;&amp;quot; # [ A Dictionary of Synonyms],Shanghai Cishu Chubanshe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mochihashi</author>
<author>Matsumoto</author>
</authors>
<title>Probabilistic Representation of Meanings.</title>
<date>2002</date>
<journal>IPSJ SIG Notes Natural Language,</journal>
<pages>2002--147</pages>
<marker>Mochihashi, Matsumoto, 2002</marker>
<rawString>Mochihashi, D., Matsumoto, Y.2002. Probabilistic Representation of Meanings. IPSJ SIG Notes Natural Language, 2002-NL-147:77-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>J Thomas</author>
</authors>
<title>Element of Information Theory.</title>
<date>1991</date>
<publisher>Wiley &amp; sons,</publisher>
<location>New York</location>
<contexts>
<context position="5955" citStr="Cover and Thomas, 1991" startWordPosition="984" endWordPosition="987">n -Shannon) = JS ( x, y) { ( ||) ( ||) } / 2, entropy(wordi) = ∑ 1− p(wordk)log p(wordk) k = KL x m KL y m k= + is the co-occurrence probability of wordk when m= (x + y) / 2 given wordi. c) LSI or PLSI models: using tf or weighted-tf co-occurrence matrix and by adopting LSI or PLSI to reduce the dimension of the matrix. 2.2 Similarity between words The common similarity functions include a) Adopting simple frequency feature, such as cosine, which computes the angle between two context vectors; b) Represent words by the probabilistic distribution among contexts, such as Kull-Leiber divergence (Cover and Thomas, 1991). The first step is to convert the co-occurrence matrix into a probabilistic matrix by simple formula. To convert distance to similarity value, we adopt the formula inspired by Mochihashi, and Matsumoto 2002. similarity(wordx, wordy) exp{ distance( , ) } = − X ⋅ x y 2.3 Organize similar words into thesaurus There are several clustering methods can be used to cluster similar words. For example, by selecting N target words as the entries of a thesaurus, then extract top-n similar words for each entry; adopting HAC(Hierarchical agglomerative clustering, E.M. Voorhees,1986) method to cluster the m</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>T.Cover and J.Thomas, 1991. Element of Information Theory. Wiley &amp; sons, New York</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>