<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.99434">
Semantic Lexicon Construction:
Learning from Unlabeled Data via Spectral Analysis
</title>
<author confidence="0.800676">
Rie Kubota Ando
</author>
<affiliation confidence="0.633581">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.85826">
19 Skyline Dr., Hawthorne, NY 10532
</address>
<email confidence="0.997886">
rie1@us.ibm.com
</email>
<sectionHeader confidence="0.993854" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999859615384615">
This paper considers the task of automatically
collecting words with their entity class labels,
starting from a small number of labeled ex-
amples (‘seed’ words). We show that spec-
tral analysis is useful for compensating for the
paucity of labeled examples by learning from
unlabeled data. The proposed method signif-
icantly outperforms a number of methods that
employ techniques such as EM and co-training.
Furthermore, when trained with 300 labeled
examples and unlabeled data, it rivals Naive
Bayes classifiers trained with 7500 labeled ex-
amples.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996776098765432">
Entity detection plays an important role in information
extraction systems. Whether entity recognizers employ
machine learning techniques or rule-based approaches, it
is useful to have a gazetteer of words&apos; that reliably sug-
gest target entity class membership. This paper considers
the task of generating such gazetteers from a large unan-
notated corpus with minimal manual effort. Starting from
a small number of labeled examples (seeds), e.g., “car”,
“plane”, “ship” labeled as vehicles, we seek to automat-
ically collect more of these.
This task is sometimes called the semi-automatic con-
struction of semantic lexicons, e.g. (Riloff and Shepherd,
1997; Roark and Charniak, 1998; Thelen and Riloff,
2002; Phillips and Riloff, 2002). A common trend in
prior studies is bootstrapping, which is an iterative pro-
cess to collect new words and regard the words newly
collected with high confidence as additional labeled ex-
amples for the next iteration. The aim of bootstrapping
is to compensate for the paucity of labeled examples.
However, its potential danger is label ‘contamination’ —
namely, wrongly (automatically) labeled examples may
&apos;Our argument in this paper holds for relatively small lin-
guistic objects including words, phrases, collocations, and so
forth. For simplicity, we refer to words.
misdirect the succeeding iterations. Also, low frequency
words are known to be problematic. They do not pro-
vide sufficient corpus statistics (e.g., how frequently the
word occurs as the subject of “said”), for adequate label
prediction.
By contrast, we focus on improving feature vector rep-
resentation for use in standard linear classifiers. To coun-
teract data sparseness, we employ subspace projection
where subspaces are derived by singular value decompo-
sition (SVD). In this paper, we generally call such SVD-
based subspace construction spectral analysis.
Latent Semantic Indexing (LSI) (Deerwester et al.,
1990) is a well-known application of spectral analysis
to word-by-document matrices. Formal analyses of LSI
were published relatively recently, e.g., (Papadimitriou
et al., 2000; Azar et al., 2001). Ando and Lee (2001)
show the factors that may affect LSI’s performance by
analyzing the conditions under which the LSI subspace
approximates an optimum subspace. Our theoretical ba-
sis is partly derived from this analysis. In particular, we
replace the abstract notion of ‘optimum subspace’ with a
precise definition of a subspace useful for our task.
The essence of spectral analysis is to capture the most
prominently observed vector directions (or sub-vectors)
into a subspace. Hence, we should apply spectral analysis
only to ‘good’ feature vectors so that useful portions are
captured into the subspace, and then factor out ‘harmful’
portions of all the vectors via subspace projection. We
first formalize the notion of harmful portions of the com-
monly used feature vector representation. Experimental
results show that this new strategy significantly improves
label prediction performance. For instance, when trained
with 300 labeled examples and unlabeled data, the pro-
posed method rivals Naive Bayes classifiers trained with
7500 labeled examples.
In general, generation of labeled training data involves
expensive manual effort, while unlabeled data can be eas-
ily obtained in large amounts. This fact has motivated su-
pervised learning with unlabeled data, such as co-training
(e.g., Blum and Mitchell (1998)). The method we pro-
pose (called Spectral) can also be regarded as exploiting
unlabeled data for supervised learning. The main differ-
ence from co-training or popular EM-based approaches
is that the process of learning from unlabeled data (via
spectral analysis) does not use any class information. It
encodes learned information into feature vectors – which
essentially serves as prediction of unseen feature occur-
rences – for use in supervised classification. The ab-
sence of class information during the learning process
may seem to be disadvantageous. On the contrary, our
experiments show that Spectral consistently outperforms
all the tested methods that employ techniques such as EM
and co-training.
We formalize the problem in Section 2, and propose
the method in Section 3. We discuss related work in Sec-
tion 4. Experiments are reported in Section 5, and we
conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.959108" genericHeader="method">
2 Word Classification Problem
</sectionHeader>
<bodyText confidence="0.991402272727273">
The problem is to classify words (as lexical items) into
the entity classes that are most likely referred to by their
occurrences, where the notion of ‘most likely’ is with re-
spect to the domain of the text2.
More formally, consider all the possible instances of
word occurrences (including their context) in the world,
which we call set , and assume that each word occur-
rence in refers to one of the entity classes in set (e.g.,
‘Person’, ‘Location’, ‘Others’ ). Further assume
that observed word occurrences (i.e., corpora) are inde-
pendently drawn from according to some probability
distribution . An example of might be the distribu-
tion observed in all the newspaper articles in 1980’s, or
the distribution observed in biomedical articles. That is,
represents the assumed domain of text.
We define to be the entity class most
likely referred to by word ’s occurrences in
the assumed domain of text, i.e.,
refers to is an occurrence of
given that is arbitrarily drawn from according to
. Then, our word classification problem is to predict
-labels of all the words (as lexical items) in a given
word set , when the following resources are available:
An unannotated corpus of the domain of interest –
which we regard as unlabeled word occurrences ar-
bitrarily drawn from according to . We assume
that all the words in appear in this corpus.
Feature extractors. We assume that some feature
extractors are available, which we can apply to
word occurrences in the above unannotated corpus.
Feature might be, for instance, the set of head
nouns that participate in list construction with the
focus word of .
2E.g., “plant” might be most likely to be a living thing if it
occurred in gardening books, but it might be most likely to be a
facility in newspaper articles.
Seed words and their labels. We assume that the
-labels of several words in are revealed as la-
beled examples.
Note that in this task configuration, test data is known
at the time of training (as in the transductive setting). Al-
though we do not pursue transductive learning techniques
(e.g., Vapnik (1998)) in this work, we will set up the ex-
perimental framework accordingly.
</bodyText>
<sectionHeader confidence="0.823792" genericHeader="method">
3 Using Vector Similarity
</sectionHeader>
<subsectionHeader confidence="0.977781">
3.1 Error Factors
</subsectionHeader>
<bodyText confidence="0.974646866666667">
Consider a straightforward feature vector representation
using normalized joint counts of features and the word,
which we call count vector . More formally, the -
th element of is where denotes
the count of events observed in the unannotated corpus.
One way to classify words would be to compare count
vectors for seeds and words and to choose the most simi-
lar seeds, using inner products as the similarity measure.
Let us investigate the factors that may affect the perfor-
mance of such inner product-based label prediction. Let
(for word ) and (for class ) be the vectors of fea-
ture occurrence probabilities, so that their -th elements
are 3 and , respectively. Now we set vec-
tors and so that they satisfy:
That is, is a vector of the difference between true (but
unknown) feature occurrence probabilities and their max-
imum likelihood estimations. We call estimation er-
ror.
If occurrences of word and features are conditionally
independent given labels, then is zero4. Therefore, we
call , dependency. It would be ideal (even if unrealis-
tic) if the dependency were zero so that features convey
class information rather than information specific to .
Now consider the conditions under which a word pair
with the same label has a larger inner product than the
pair with different labels. It is easy to show that, with fea-
ture extractors fixed to reasonable ones, smaller estima-
tion errors and smaller dependency ensure better perfor-
mance of label prediction, in terms of lower-bound analy-
sis. More precise descriptions are found in the Appendix.
</bodyText>
<footnote confidence="0.788588">
3 denotes the probability that feature is in
given that is an occurrence of word , where is randomly
drawn from according to .
4Because their conditional independence implies
.
</footnote>
<subsectionHeader confidence="0.999894">
3.2 Spectral analysis for classifying words
</subsectionHeader>
<bodyText confidence="0.999849184210527">
We seek to remove the above harmful portions and
from count vectors — which correspond to estimation
error and feature dependency — by employing spectral
analysis and succeeding subspace projection.
Background A brief review of spectral analysis is
found in the Appendix. Ando and Lee (2001) analyze the
conditions under which the application of spectral analy-
sis to a term-document matrix (as in LSI) approximates
an optimum subspace. The notion of ‘optimum’ is with
respect to the accuracy of topic-based document similari-
ties. The proofs rely on the mathematical findings known
as the invariant subspace perturbation theorems proved
by Davis and Kahan (1970).
Approximation of the span of ’s By adapting Ando
and Lee’s analysis to our problem, it can be shown that
spectral analysis will approximate the span of ’s, essen-
tially,
if the count vectors (chosen as input to spectral anal-
ysis) well-represent all the classes, and
if these input vectors have sufficiently small estima-
tion errors and dependency.
This is because, intuitively, ’s are the most promi-
nently observed sub-vectors among the input vectors in
that case. (Recall that the essence of spectral analysis is
to capture the most prominently observed vector direc-
tions into a subspace.) Then, the error portions can be
mostly removed from any count vectors by orthogonally
projecting the vectors onto the subspace, assuming error
portions are mostly orthogonal to the span of ’s.
Choice of count vectors As indicated by the above two
conditions, the choice of input vectors is important when
applying spectral analysis. The tightness of subspace ap-
proximation depends on the degree to which those condi-
tions are met. In fact, it is easy to choose vectors with
small estimation errors so that the second condition is
likely to be met. Vectors for high frequency words are
expected to have small estimation errors. Hence, we pro-
pose the following procedure.
</bodyText>
<listItem confidence="0.943451">
1. From the unlabeled word set , choose the most
</listItem>
<bodyText confidence="0.798202">
frequent words. is a sufficiently large constant.
Frequency is counted in the given unannotated cor-
pus.
</bodyText>
<listItem confidence="0.9694975">
2. Generate count vectors for all the words by ap-
plying a feature extractor to word occurrences in the
given unannotated corpus.
3. Compute the -dimensional subspace by applying
</listItem>
<bodyText confidence="0.947288722222222">
spectral analysis to the count vectors generated in
Step 2 5.
4. Generate count vectors (as in Step 2) for all the
words (including seeds) in . Generate new fea-
ture vectors by orthogonally projecting them onto
the subspace6.
When we have multiple feature extractors, we perform
the above procedure independently for each of the feature
extractors, and concatenate the vectors in the end. Here-
after, we call this procedure and the vectors obtained in
this manner Spectral and spectral vectors, respectively.
Spectral vectors serve as feature vectors for a linear clas-
sifier for classifying words.
Note that we do not claim that the above conditions
for subspace approximation are always satisfied. Rather,
we consider them as insight into spectral analysis on this
task, and design the method so that the conditions are
likely to be met.
</bodyText>
<subsectionHeader confidence="0.875838">
3.3 The number of input vectors and the subspace
dimensionality
</subsectionHeader>
<bodyText confidence="0.99977405">
There are two parameters: , the number of count vectors
used as input to spectral analysis, and , the dimension-
ality of the subspace.
should be sufficiently large so that all the classes are
represented by the chosen vectors. However, an exces-
sively large would result in including low frequency
words, which might degrade the subspace approximation.
In principle, the dimensionality of the subspace
should be set to the number of classes , since we seek
to approximate the span of ’s for all . How-
ever, for the typical practice of semantic lexicon con-
struction, should be greater than because at least
one class tends to have very broad coverage – ‘Others’
as in Person, Organization, Others . It is reasonable
to assume that features correlate to its (unknown) inher-
ent subclasses rather than to such a broadly defined class
itself. The dimensionality should take account of the
number of such subclasses.
In practice, and need be determined empirically.
We will return to this issue in Section 5.2.
</bodyText>
<footnote confidence="0.982275571428571">
5We generate a matrix so that its columns are the length-
normalized count vectors. We compute left singular vectors of
this matrix corresponding to the largest singular values. The
computed left singular vectors are the basis vectors of the de-
sired subspace.
6We compute where is the left singular
vector computed in the previous step. Alternatively, one can
</footnote>
<bodyText confidence="0.554863666666667">
generate the vector whose -th entry is , as it produces the
same inner products, due to the orthonormality of left singular
vectors.
</bodyText>
<sectionHeader confidence="0.998147" genericHeader="method">
4 Related Work and Discussion
</sectionHeader>
<subsectionHeader confidence="0.953191">
4.1 Spectral analysis for word similarity
measurement
</subsectionHeader>
<bodyText confidence="0.999982947368421">
Spectral analysis has been used in traditional factor anal-
ysis techniques (such as Principal Component Analysis)
to summarize high-dimensional data. LSI uses spectral
analysis for measuring document or word similarities.
From our perspective, the LSI word similarity measure-
ment is similar to the special case where we have a single
feature extractor that returns the document membership
of word occurrence .
Among numerous empirical studies of LSI,
Landauer and Dumais (1997) report that using the
LSI word similarity measure, 64.4% of the synonym sec-
tion of TOEFL (multi-choice) were answered correctly,
which rivals college students from non-English speaking
countries. We conjecture that if more effective feature
extractors were used, performance might be better.
Sch¨uetze (1992)’s word sense disambiguation method
uses spectral analysis for vector dimensionality reduc-
tion. He reports that use of spectral analysis does not af-
fect the task performance, either positively or negatively.
</bodyText>
<subsectionHeader confidence="0.722782">
4.2 Bootstrapping methods for constructing
semantic lexicons
</subsectionHeader>
<bodyText confidence="0.9999735">
A common trend for the semantic lexicon construction
task is that of bootstrapping, exploiting strong syntactic
cues — such as a bootstrapping method that iteratively
grows seeds by using cooccurrences in lists, conjunc-
tions, and appositives (Roark and Charniak, 1998); meta-
bootstrapping which repeatedly finds extraction patterns
and extracts words from the found patterns (Riloff and
Jones, 1999); a co-training combination of three boot-
strapping processes each of which exploits appositives,
compound nouns, and ISA-clauses (Phillips and Riloff,
2002). Thelen and Riloff (2002)’s bootstrapping method
iteratively performs feature selection and word selection
for each class. It outperformed the best-performing boot-
strapping method for this task at the time. We also note
that there are a number of bootstrapping methods suc-
cessfully applied to text – e.g., word sense disambigua-
tion (Yarowsky, 1995), named entity instance classifi-
cation (Collins and Singer, 1999), and the extraction of
‘parts’ word given the ‘whole’ word (Berland and Char-
niak, 1999).
In Section 5, we report experiments using syntactic
features shown to be useful by the above studies, and
compare performance with Thelen and Riloff (2002)’s
bootstrapping method.
</bodyText>
<subsectionHeader confidence="0.864146">
4.3 Techniques for learning from unlabeled data
</subsectionHeader>
<bodyText confidence="0.99987562264151">
While most of the above bootstrapping methods are tar-
geted to NLP tasks, techniques such as EM and co-
training are generally applicable when equipped with ap-
propriate models or classifiers. We will present high-
level and empirical comparisons (Sections 4.4 and 5, re-
spectively) of Spectral with representative techniques for
learning from unlabeled data, described below.
Expectation Maximization (EM) is an iterative algo-
rithm for model parameter estimation (Dempster et al.,
1977). Starting from some initial model parameters, the
E-step estimates the expectation of the hidden class vari-
ables. Then, the M-step recomputes the model parame-
ters so that the likelihood is maximized, and the process
repeats. EM is guaranteed to converge to some local max-
imum. It is very popular and useful, but also known to be
sensitive to the initialization of parameters.
The co-training paradigm proposed by
Blum and Mitchell (1998) involves two classifiers
employing two distinct views of the feature space, e.g.,
‘textual content’ and ‘hyperlink’ of web documents. The
two classifiers are first trained with labeled data. Each of
the classifiers adds to the labeled data pool the examples
whose labels are predicted with the highest confidence.
The classifiers are trained with the new augmented
labeled data, and the process repeats. Its theoretical
foundations are based on the assumptions that two views
are redundantly sufficient and conditionally independent
given classes. Abney (2002) presents an analysis to relax
the (fairly strong) conditional independence assumption
to weak rule dependence.
Nigam and Ghani (2000) study the effectiveness of co-
training through experiments on the text categorization
task. Pierce and Cardie (2001) investigate the scalability
of co-training on the base noun phrase bracketing task,
which typically requires a larger number of labeled exam-
ples than text categorization. They propose to manually
correct labels to counteract the degradation of automati-
cally assigned labels on large data sets. We use these two
empirical studies as references for the implementation of
co-training in our experiments.
Co-EM (Nigam and Ghani, 2000) combines the
essence of co-training and EM in an elegant way. Classi-
fier A is initially trained with the labeled data, and com-
putes probabilistically-weighted labels for all the unla-
beled data (as in E-step). Then classifier B is trained
with the labeled data plus the probabilistic labels com-
puted by classifier A. It computes probabilistic labels
for A, and the process repeats. Co-EM differs from
co-training in that all the unlabeled data points are
re-assigned probabilistic labels in every iteration. In
Nigam and Ghani (2000)’s experiments, co-EM outper-
formed EM, and rivaled co-training. Based on the results,
they argued for the benefit of exploiting distinct views.
</bodyText>
<subsectionHeader confidence="0.954492">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999556517241379">
We observe two major differences between spectral anal-
ysis and the above techniques for learning from unlabeled
data.
Feature prediction (Spectral) vs. label prediction
First, the learning processes of the above techniques are
driven by the prediction of class labels on the unlabeled
data. As their iterations proceed, for instance, the estima-
tions of class-related probabilities such as ,
may be improved. On the other hand, a spectral vector
can be regarded as an approximation of (a vector of
) when the dependency is sufficiently small.
In that sense, spectral analysis predicts unseen feature
occurrences which might be observed with word if
had more occurrences in the corpus.
Global optimization (Spectral) vs. local optimization
Secondly, starting from the status initialized by labeled
data, EM performs local maximization, and co-training
and other bootstrapping methods proceed greedily. Con-
sequently, they are sensitive to the given labeled data.
In contrast, spectral analysis performs global optimiza-
tion (eigenvector computation) independently from the
labeled data. Whether or not the performed global op-
timization is meaningful for classification depends on the
‘usefulness’ of the given feature extractors. We say fea-
tures are useful if dependency and feature mingling (de-
fined in the Appendix) are small.
It is interesting to see how these differences affect the
performance on the word classification task. We will re-
port experimental results in the next section.
</bodyText>
<sectionHeader confidence="0.999421" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999532">
We study Spectral’s performance in comparison with the
algorithms discussed in the previous sections.
</bodyText>
<subsectionHeader confidence="0.990636">
5.1 Baseline algorithms
</subsectionHeader>
<bodyText confidence="0.967912487804878">
We use the following algorithms as baseline: EM, co-
training, and co-EM, as established techniques for learn-
ing from unlabeled data in general; the bootstrapping
method proposed by Thelen and Riloff (2002) (hereafter,
TRB and TR) as a state-of-the-art bootstrapping method
designed for semantic lexicon construction.
5.1.1 Implementation of EM, co-training, and
co-EM
Naive Bayes classifier To instantiate EM, co-training,
and co-EM, we use a standard Naive Bayes classifier,
as it is often used for co-training experiments, e.g.,
(Nigam and Ghani, 2000; Pierce and Cardie, 2001). As
in Nigam and Ghani (2000)’s experiments, we estimate
with Laplace smoothing, and for label prediction,
we compute for every :
The underlying naive Bayes assumption is that occur-
rences of features are conditionally independent of each
other, given class labels. The generative interpretation in
this case is analogous to that of text categorization, when
we regard features (or contexts) of all the occurrences of
word as a pseudo document.
We initialize model parameters ( and ) using
labeled examples. The test data is labeled after itera-
tions. We explore for EM and co-EM,
and for co-training7. Analogous to the
choice of input vectors for spectral analysis, we hypothe-
size that using all the unlabeled data for EM and co-EM
may rather degrade performance. We feed EM and co-
EM with the most frequent unlabeled words8. As for
co-training, we let each of the classifiers predict labels of
all the unlabeled data, and choose words labeled with
the highest confidence9.
Co-training and co-EM require two redundantly suf-
ficient and conditionally independent views of features.
We split features randomly, as in one of the settings in
Nigam and Ghani (2000). We also tested left context vs.
right context (not reported in this paper), and found that
random split performs slightly better.
To study the potential best performance of the baseline
methods, we explore the parameters described above and
report the best results.
</bodyText>
<subsectionHeader confidence="0.999677">
5.2 Implementation of Spectral
</subsectionHeader>
<bodyText confidence="0.998832454545455">
In principle, spectral vectors can be used with any linear
classifier. In our experiments, we use a standard centroid-
based classifier using cosine as the similarity measure.
For comparison, we also test count vectors (with and
without tf-idf weighting) with the same centroid-based
classifier.
Spectral has two parameters: the number of input vec-
tors , and the subspace dimensionality . We set
and based on the observation on a corpus
disjoint from the test corpora, and use these settings for
all the experiments.
</bodyText>
<footnote confidence="0.998939">
7The maximum numbers of iterations were chosen so that
the best performance of the baseline algorithms can be ob-
served.
8Indeed, it turned out that setting to an appropriate value
(2500 on the particular data described below) produces signifi-
cantly better results than using all the unlabeled data.
9We also tested Pierce and Cardie (2001)’s modification to
choose examples according to the label distribution, but it did
not make any significant difference.
</footnote>
<table confidence="0.895186375">
Spectral TRB co-TR co-EM EM NB Tf-idf Count
100 seeds 60.2 51.7 50.4 49.4 50.7 43.3 40.7 32.6
300 seeds 62.9 47.1 57.8 55.8 53.2 50.8 46.7 35.2
500 seeds 63.8 42.7 56.7 56.6 54.3 53.2 49.9 36.0
Exploiting unlabeled data? Yes No
Classification model Centroid - Naive Bayes Centroid
Figure 1: F-measure results (%) on high frequency seeds. AP-corpus.
Cf. Naive Bayes classifiers (NB) trained with 7500 seeds produce 62.9% on average over five runs with random training/test splits.
</table>
<subsectionHeader confidence="0.997845">
5.3 Target Classes and Data
</subsectionHeader>
<bodyText confidence="0.99995985">
Following previous semantic lexicon studies, we evalu-
ate on the classification of lemma-form nouns. As noted
by several authors, accurate evaluation on a large num-
ber of proper nouns (without context) is extremely hard
since the judgment requires real-world knowledge. We
choose to focus on non-proper head nouns. To gener-
ate the training/test data, we extracted all the non-proper
nouns which appeared at least twice as the head word of
a noun phrase in the AP newswire articles (25K docu-
ments), using a statistical syntactic chunker and a lem-
matizer. This resulted in approx. 10K words. These 10K
words were manually annotated with six classes: five tar-
get classes – persons, organizations, geo-political entities
(GPE), locational entities, and facilities —, and ‘others’.
The assumed distribution ( ) was that of general news-
paper articles. The definitions of the classes follow the
annotation guidelines for ACE (Automatic Content Ex-
traction)10. Our motivation for choosing these classes
is the availability of such independent guidelines. The
breakdown of the 10K words is as follows.
</bodyText>
<table confidence="0.71985">
Per. 1347 13.8% Fac. 238 2.4%
Loc. 145 1.5% GPE 17 0.2%
Org. 136 1.4% Others 7871 80.7%
</table>
<bodyText confidence="0.994925416666666">
The majority (80.7%) are labeled as Others. The
most populous target class is Person (13.8%). The rea-
son for GPE’s small population is that geo-political enti-
ties are typically referred to by their names or pronouns
rather than common nominal. We measure precision
( target-class match proposed as target-class ) and re-
call ( target-class match target-class members ), and
combine them into the F-measure with equal weight.
The chance performance is extremely low since target
classes are very sparse. Random choice would result in
F-measure=6.3%. Always proposing Person would pro-
duce F=23.1%.
</bodyText>
<subsectionHeader confidence="0.783682">
5.4 Features
</subsectionHeader>
<bodyText confidence="0.964872583333333">
Types of feature extractors used in our experiments are
essentially the same as those used in TR’s experiments,
lohttp://www.nist.gov/speech/index.htm
which exploit the syntactic constructions such as subject-
verb, verb-object, NP-pp-NP (pp is preposition), and
subject-verb-object. In addition, we exploit syntactic
constructions shown to be useful by other studies — lists
and conjunctions (Roark and Charniak, 1998), and adja-
cent words (Riloff and Shepherd, 1997).
We count feature occurrences ( ) in the unan-
notated corpus. All the tested methods are given exactly
the same data points.
</bodyText>
<subsectionHeader confidence="0.997315">
5.5 High-frequency seed experiments
</subsectionHeader>
<bodyText confidence="0.9999616">
Prior semantic lexicon studies (e.g., TR) note that the
choice of seeds is critical – i.e., seeds should be high-
frequency words so that methods are provided with plenty
of feature information to bootstrap with. In practice,
this can be achieved by first extracting the most frequent
words from the target corpus and manually labeling them
for use as seeds.
To simulate this practical situation, we split the above
10K words into a labeled set and an unlabeled set11, by
choosing the most frequent words as the labeled set,
where , and . Note that approximately
80% of the seeds are negative examples (‘Others’). As
we assume that test data is known at the time of training,
we use the unlabeled set as both unlabeled data and test
data.
</bodyText>
<subsectionHeader confidence="0.984215">
5.5.1 AP-corpus high-frequency seed results
</subsectionHeader>
<bodyText confidence="0.951288615384615">
Overall F-measure results on the AP corpus are shown
in Figure 1. The columns of the figure are roughly sorted
in the descending order of performance. Spectral signif-
icantly outperforms the others. The algorithms that ex-
ploit unlabeled data outperform those which do not. Tf-
idf and Count perform poorly on this task. Although
TRB’s performance was better on a smaller number of
seeds in this particular setting, it showed different trends
in other settings.
Spectral trained with 300 or 500 labeled examples
(and 1000 unlabeled examples via spectral analysis) ri-
vals Naive Bayes classifiers trained with 7500 labeled ex-
amples (which produce on average over five runs
</bodyText>
<footnote confidence="0.8735">
&amp;quot;The labels of the ‘unlabeled set’ are hidden from the meth-
ods.
</footnote>
<table confidence="0.990374923076923">
Spectral baseline ceiling
100 seeds 59.2 52.3 (co-EM)
300 seeds 62.3 55.8 (co-TR)
500 seeds 61.4 56.6 (co-TR)
Figure 2: F-measure results (%) on high frequency seeds.
WSJ-corpus. Results of Spectral and the best-performing base-
line are shown.
Cf. Naive Bayes classifiers trained with 7500 seeds achieve
62.6% on average over five runs with random training/test splits.
Spectral baseline ceiling
100 seeds 61.3 (+1.1) 38.9 (-11.5) co-TR
300 seeds 64.5 (+1.6) 47.9 ( -9.9) co-TR
500 seeds 64.9 (+1.1) 53.4 ( -3.2) co-EM
</table>
<figureCaption confidence="0.99753125">
Figure 3: Results on randomly chosen seeds. AP-corpus. Av-
erage over five runs with different seeds. Numbers in parenthe-
ses are ‘random-seed performance’ minus ‘high-frequency-seed
performance’ (in Figure 1).
</figureCaption>
<bodyText confidence="0.993337142857143">
with random training/test splits).
Also note that the reported numbers for TRB, co-
training, co-EM, and EM are the best performance among
the explored parameter settings (described in Section
5.1.1), whereas Spectral’s parameters were determined
on a corpus disjoint from the test corpora once and used
for all the experiments (Section 5.2).
</bodyText>
<subsectionHeader confidence="0.828397">
5.5.2 WSJ-corpus high-frequency seed results
</subsectionHeader>
<bodyText confidence="0.998678">
Figure 2 shows the results of Spectral and the best-
performing baseline algorithms when features are ex-
tracted from a different corpus (Wall Street Journal 36K
documents). We use the same 10K words as the la-
beled/unlabeled word set while discarding 501 words
which do not occur in this corpus. Spectral outperforms
the others. Furthermore, Spectral trained with 300 or
500 seeds rivals Naive Bayes classifiers trained with 7500
seeds on this corpus (which achieve on average
over five runs with random training/test splits).
</bodyText>
<subsectionHeader confidence="0.776914">
5.6 Random-seed experiments
</subsectionHeader>
<bodyText confidence="0.99911">
To study performance dependency on the choice of seeds,
we made labeled/unlabeled splits randomly. Figure 3
shows results of Spectral and the best-performing base-
line algorithms. The average results over five runs using
different seeds are shown.
All the methods (except Spectral) exhibit the same ten-
dency. That is, performance on random seeds is lower
than that on high-frequency seeds, and the degradation
is larger when the number of seeds is small. This is
not surprising since a small number of randomly chosen
seeds provide much less information (corpus statistics)
than high frequency seeds. However, Spectral’s perfor-
</bodyText>
<table confidence="0.9809556">
High Medium Low All
(Spectral)
100 seeds 61.3 47.8 30.6 48.1
300 seeds 64.5 57.6 37.9 53.9
500 seeds 64.9 57.0 37.9 54.3
</table>
<figureCaption confidence="0.913247">
Figure 4: F-measure results (%) in relation to the choice of
input vectors for spectral analysis. AP-corpus. Random seeds.
Using count vectors from high-, medium, low-frequency words
(1000 each), and all the 10K words.
</figureCaption>
<bodyText confidence="0.997548">
mance does not degrade on randomly chosen seeds. We
presume that this is because it learns from unlabeled data
independently from seeds.
</bodyText>
<subsectionHeader confidence="0.995063">
5.7 Choice of input vectors for spectral analysis
</subsectionHeader>
<bodyText confidence="0.999989538461538">
Recall that our basic idea is to use vectors with small es-
timation errors to achieve better subspace approximation.
This idea led to applying spectral analysis to the most fre-
quent words. We confirm the effectiveness of this strategy
in Figure 4. ‘Medium’ and ‘Low’ in the figure compute
the subspaces from 1000 words with medium frequency
(68 to 197) and with low frequency (2 on average), re-
spectively. Clearly, standard Spectral (‘High’: computing
subspace from the most frequent 1000 words; frequency
) outperforms the others. When all the vectors
are used (as LSI does), performance degrades to below
Medium. ‘Low’ gains almost no benefits from spectral
analysis. The results are in line with our prediction.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999967769230769">
We show that spectral analysis is useful for overcoming
data sparseness on the task of classifying words into their
entity classes. In a series of experiments, the proposed
method compares favorably with a number of methods
that employ techniques such as EM and co-training.
We formalize the notion of harmful portions of the
commonly used feature vectors for linear classifiers, and
seek to factor out them via spectral analysis of unlabeled
data. This process does not use any class information.
By contrast, the process of bootstrapping is generally
driven by class label prediction. As future work, we are
interested in combining these somewhat orthogonal ap-
proaches.
</bodyText>
<sectionHeader confidence="0.99651" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997854833333333">
I would like to thank colleagues at IBM Research for
helpful discussions, and anonymous reviewers for useful
comments. This work was supported by the Advanced
Research and Development Activity under the Novel In-
telligence and Massive Data (NIMD) program PNWD-
SW-6059.
</bodyText>
<sectionHeader confidence="0.444733" genericHeader="conclusions">
References
</sectionHeader>
<bodyText confidence="0.478067">
Steven Abney. 2002. Bootstrapping. In Proceedings of
ACL’02.
Brian Roark and Eugene Charniak. 1998. Noun-phrase
co-occurrence statistics for semi-automatic semantic
lexicon construction. In Proceedings of COLING-
ACL’98.
</bodyText>
<reference confidence="0.999017016949153">
Rie Kubota Ando and Lillian Lee. 2001. Iterative Resid-
ual Rescaling: An analysis and generalization of LSI.
In Proceedings of SIGIR’01, pages 154–162.
Yossi Azar, Amos Fiat, Anna Karlin, Frank McSherry,
and Jared Saia. 2001. Spectral analysis of data. In
Proceedings of STOC 2001.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proceedings ofACL’99.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unalbeled data with co-training. In Proceed-
ings of COLT-98.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of EMNLP/VLC’99.
Chandler Davis and W. M. Kahan. 1970. The rotation of
eigenvectors by a perturbation. III. SIAM Journal on
Numerical Analysis, 7(1):1–46, March.
Scott Deerwester, Susan T. Dumais, Geroge W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of the
Society for Information Science, 41:391–407.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum
likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, 39(1):1–38.
Gene H. Golub and Charles F. Van Loan. 1996. Matrix
computations third edition.
Thomas K. Landauer and Susan T. Dumais. 1997. A
solution to Plato’s problem. Psychological Review,
104:211–240.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the
effectiveness and applicability of co-training. In Pro-
ceedings ofInformation and Knowledge Management.
Christos H. Papadimitriou, Prabhakar Raghavan, Hisao
Tamaki, and Santosh Vempala. 2000. Latent Semantic
Indexing: A probabilistic analysis. Journal of Com-
puter and System Sciences, 61(2):217–235.
William Phillips and Ellen Riloff. 2002. Exploiting
strong syntactic heuristics and co-training to learn se-
mantic lexicons. In Proceedings of EMNLP’02.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings ofEMNLP’01.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In Proceedings of the Sixteenth National Confer-
ence on Artificial Intelligence.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of EMNLP’97.
Hinrich Sch¨uetze. 1992. Dimensions of meaning. In
Proceedings of Supercomputing’92, pages 787–796.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extracting
pattern contexts. In Proceedings of EMNLP’02.
Vladimir Vapnik. 1998. Statistical Learning Theory.
Wiley Interscience, New York.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings
ofACL’95, pages 189–196.
</reference>
<sectionHeader confidence="0.985622" genericHeader="references">
Appendix
</sectionHeader>
<bodyText confidence="0.990763941176471">
Estimation error, dependency, and feature mingling
Using the notation in Section 3.1, for simplicity, as-
sume that all the seeds and words are non-polysemous.
Suppose that label prediction is done by choosing the
most similar seed where similarity is measured by inner
products of corresponding count vectors. Set
where is a matrix whose -element is if
; otherwise. Intuitively, quantifies ‘fea-
ture mingling’; it is larger when feature distributions over
classes are uniform (i.e., useless for label prediction). Let
be a set of given seeds. Set
Using properties of the matrix norm, it is easy to show
that for arbitrary , if
then, ’s label is predicted correctly.
Since the condition is sufficient but not necessary, the
proportion of the words that satisfy this condition gives
the lower bound of the label prediction accuracy.
</bodyText>
<subsectionHeader confidence="0.701944">
Background: spectral analysis
</subsectionHeader>
<bodyText confidence="0.998989466666667">
Singular value decomposition (SVD) factors a matrix
into the product: , such that and
are orthonormal and is diagonal. Columns of are
called left singular vectors, and diagonal entries of are
called singular values. Also note that left singular vec-
tors of are eigenvectors of . Let be the sub-
space spanned by left singular vectors corresponding to
the largest singular values of matrix . In this paper,
we call this process of computing spectral analysis.
Among all possible -dimensional subspaces, is the
subspace that maximizes orthogonal projections of ’s
column vectors in terms of the sum of squares of vector
lengths. In that sense, we say that spectral analysis cap-
tures the most prominent vector directions. More details
are found in e.g., (Golub and Loan, 1996).
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.728614">
<title confidence="0.999562">Semantic Lexicon Construction: Learning from Unlabeled Data via Spectral Analysis</title>
<author confidence="0.99656">Rie Kubota</author>
<affiliation confidence="0.998578">IBM T.J. Watson Research</affiliation>
<address confidence="0.919706">19 Skyline Dr., Hawthorne, NY</address>
<email confidence="0.997728">rie1@us.ibm.com</email>
<abstract confidence="0.979031">This paper considers the task of automatically collecting words with their entity class labels, starting from a small number of labeled examples (‘seed’ words). We show that spectral analysis is useful for compensating for the paucity of labeled examples by learning from unlabeled data. The proposed method significantly outperforms a number of methods that employ techniques such as EM and co-training. Furthermore, when trained with 300 labeled examples and unlabeled data, it rivals Naive Bayes classifiers trained with 7500 labeled examples.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Lillian Lee</author>
</authors>
<title>Iterative Residual Rescaling: An analysis and generalization of LSI.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR’01,</booktitle>
<pages>154--162</pages>
<contexts>
<context position="2879" citStr="Ando and Lee (2001)" startWordPosition="430" endWordPosition="433">r adequate label prediction. By contrast, we focus on improving feature vector representation for use in standard linear classifiers. To counteract data sparseness, we employ subspace projection where subspaces are derived by singular value decomposition (SVD). In this paper, we generally call such SVDbased subspace construction spectral analysis. Latent Semantic Indexing (LSI) (Deerwester et al., 1990) is a well-known application of spectral analysis to word-by-document matrices. Formal analyses of LSI were published relatively recently, e.g., (Papadimitriou et al., 2000; Azar et al., 2001). Ando and Lee (2001) show the factors that may affect LSI’s performance by analyzing the conditions under which the LSI subspace approximates an optimum subspace. Our theoretical basis is partly derived from this analysis. In particular, we replace the abstract notion of ‘optimum subspace’ with a precise definition of a subspace useful for our task. The essence of spectral analysis is to capture the most prominently observed vector directions (or sub-vectors) into a subspace. Hence, we should apply spectral analysis only to ‘good’ feature vectors so that useful portions are captured into the subspace, and then fa</context>
<context position="9333" citStr="Ando and Lee (2001)" startWordPosition="1486" endWordPosition="1489">f label prediction, in terms of lower-bound analysis. More precise descriptions are found in the Appendix. 3 denotes the probability that feature is in given that is an occurrence of word , where is randomly drawn from according to . 4Because their conditional independence implies . 3.2 Spectral analysis for classifying words We seek to remove the above harmful portions and from count vectors — which correspond to estimation error and feature dependency — by employing spectral analysis and succeeding subspace projection. Background A brief review of spectral analysis is found in the Appendix. Ando and Lee (2001) analyze the conditions under which the application of spectral analysis to a term-document matrix (as in LSI) approximates an optimum subspace. The notion of ‘optimum’ is with respect to the accuracy of topic-based document similarities. The proofs rely on the mathematical findings known as the invariant subspace perturbation theorems proved by Davis and Kahan (1970). Approximation of the span of ’s By adapting Ando and Lee’s analysis to our problem, it can be shown that spectral analysis will approximate the span of ’s, essentially, if the count vectors (chosen as input to spectral analysis)</context>
</contexts>
<marker>Ando, Lee, 2001</marker>
<rawString>Rie Kubota Ando and Lillian Lee. 2001. Iterative Residual Rescaling: An analysis and generalization of LSI. In Proceedings of SIGIR’01, pages 154–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yossi Azar</author>
<author>Amos Fiat</author>
<author>Anna Karlin</author>
<author>Frank McSherry</author>
<author>Jared Saia</author>
</authors>
<title>Spectral analysis of data.</title>
<date>2001</date>
<booktitle>In Proceedings of STOC</booktitle>
<contexts>
<context position="2858" citStr="Azar et al., 2001" startWordPosition="426" endWordPosition="429">bject of “said”), for adequate label prediction. By contrast, we focus on improving feature vector representation for use in standard linear classifiers. To counteract data sparseness, we employ subspace projection where subspaces are derived by singular value decomposition (SVD). In this paper, we generally call such SVDbased subspace construction spectral analysis. Latent Semantic Indexing (LSI) (Deerwester et al., 1990) is a well-known application of spectral analysis to word-by-document matrices. Formal analyses of LSI were published relatively recently, e.g., (Papadimitriou et al., 2000; Azar et al., 2001). Ando and Lee (2001) show the factors that may affect LSI’s performance by analyzing the conditions under which the LSI subspace approximates an optimum subspace. Our theoretical basis is partly derived from this analysis. In particular, we replace the abstract notion of ‘optimum subspace’ with a precise definition of a subspace useful for our task. The essence of spectral analysis is to capture the most prominently observed vector directions (or sub-vectors) into a subspace. Hence, we should apply spectral analysis only to ‘good’ feature vectors so that useful portions are captured into the </context>
</contexts>
<marker>Azar, Fiat, Karlin, McSherry, Saia, 2001</marker>
<rawString>Yossi Azar, Amos Fiat, Anna Karlin, Frank McSherry, and Jared Saia. 2001. Spectral analysis of data. In Proceedings of STOC 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL’99.</booktitle>
<contexts>
<context position="15882" citStr="Berland and Charniak, 1999" startWordPosition="2522" endWordPosition="2526">three bootstrapping processes each of which exploits appositives, compound nouns, and ISA-clauses (Phillips and Riloff, 2002). Thelen and Riloff (2002)’s bootstrapping method iteratively performs feature selection and word selection for each class. It outperformed the best-performing bootstrapping method for this task at the time. We also note that there are a number of bootstrapping methods successfully applied to text – e.g., word sense disambiguation (Yarowsky, 1995), named entity instance classification (Collins and Singer, 1999), and the extraction of ‘parts’ word given the ‘whole’ word (Berland and Charniak, 1999). In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)’s bootstrapping method. 4.3 Techniques for learning from unlabeled data While most of the above bootstrapping methods are targeted to NLP tasks, techniques such as EM and cotraining are generally applicable when equipped with appropriate models or classifiers. We will present highlevel and empirical comparisons (Sections 4.4 and 5, respectively) of Spectral with representative techniques for learning from unlabeled data, described below. </context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proceedings ofACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unalbeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of COLT-98.</booktitle>
<contexts>
<context position="4175" citStr="Blum and Mitchell (1998)" startWordPosition="626" endWordPosition="629">We first formalize the notion of harmful portions of the commonly used feature vector representation. Experimental results show that this new strategy significantly improves label prediction performance. For instance, when trained with 300 labeled examples and unlabeled data, the proposed method rivals Naive Bayes classifiers trained with 7500 labeled examples. In general, generation of labeled training data involves expensive manual effort, while unlabeled data can be easily obtained in large amounts. This fact has motivated supervised learning with unlabeled data, such as co-training (e.g., Blum and Mitchell (1998)). The method we propose (called Spectral) can also be regarded as exploiting unlabeled data for supervised learning. The main difference from co-training or popular EM-based approaches is that the process of learning from unlabeled data (via spectral analysis) does not use any class information. It encodes learned information into feature vectors – which essentially serves as prediction of unseen feature occurrences – for use in supervised classification. The absence of class information during the learning process may seem to be disadvantageous. On the contrary, our experiments show that Spe</context>
<context position="17030" citStr="Blum and Mitchell (1998)" startWordPosition="2701" endWordPosition="2704">epresentative techniques for learning from unlabeled data, described below. Expectation Maximization (EM) is an iterative algorithm for model parameter estimation (Dempster et al., 1977). Starting from some initial model parameters, the E-step estimates the expectation of the hidden class variables. Then, the M-step recomputes the model parameters so that the likelihood is maximized, and the process repeats. EM is guaranteed to converge to some local maximum. It is very popular and useful, but also known to be sensitive to the initialization of parameters. The co-training paradigm proposed by Blum and Mitchell (1998) involves two classifiers employing two distinct views of the feature space, e.g., ‘textual content’ and ‘hyperlink’ of web documents. The two classifiers are first trained with labeled data. Each of the classifiers adds to the labeled data pool the examples whose labels are predicted with the highest confidence. The classifiers are trained with the new augmented labeled data, and the process repeats. Its theoretical foundations are based on the assumptions that two views are redundantly sufficient and conditionally independent given classes. Abney (2002) presents an analysis to relax the (fai</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unalbeled data with co-training. In Proceedings of COLT-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC’99.</booktitle>
<contexts>
<context position="15794" citStr="Collins and Singer, 1999" startWordPosition="2508" endWordPosition="2511"> words from the found patterns (Riloff and Jones, 1999); a co-training combination of three bootstrapping processes each of which exploits appositives, compound nouns, and ISA-clauses (Phillips and Riloff, 2002). Thelen and Riloff (2002)’s bootstrapping method iteratively performs feature selection and word selection for each class. It outperformed the best-performing bootstrapping method for this task at the time. We also note that there are a number of bootstrapping methods successfully applied to text – e.g., word sense disambiguation (Yarowsky, 1995), named entity instance classification (Collins and Singer, 1999), and the extraction of ‘parts’ word given the ‘whole’ word (Berland and Charniak, 1999). In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)’s bootstrapping method. 4.3 Techniques for learning from unlabeled data While most of the above bootstrapping methods are targeted to NLP tasks, techniques such as EM and cotraining are generally applicable when equipped with appropriate models or classifiers. We will present highlevel and empirical comparisons (Sections 4.4 and 5, respectively) of Spe</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of EMNLP/VLC’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chandler Davis</author>
<author>W M Kahan</author>
</authors>
<title>The rotation of eigenvectors by a perturbation.</title>
<date>1970</date>
<journal>III. SIAM Journal on Numerical Analysis,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="9703" citStr="Davis and Kahan (1970)" startWordPosition="1543" endWordPosition="1546">tions and from count vectors — which correspond to estimation error and feature dependency — by employing spectral analysis and succeeding subspace projection. Background A brief review of spectral analysis is found in the Appendix. Ando and Lee (2001) analyze the conditions under which the application of spectral analysis to a term-document matrix (as in LSI) approximates an optimum subspace. The notion of ‘optimum’ is with respect to the accuracy of topic-based document similarities. The proofs rely on the mathematical findings known as the invariant subspace perturbation theorems proved by Davis and Kahan (1970). Approximation of the span of ’s By adapting Ando and Lee’s analysis to our problem, it can be shown that spectral analysis will approximate the span of ’s, essentially, if the count vectors (chosen as input to spectral analysis) well-represent all the classes, and if these input vectors have sufficiently small estimation errors and dependency. This is because, intuitively, ’s are the most prominently observed sub-vectors among the input vectors in that case. (Recall that the essence of spectral analysis is to capture the most prominently observed vector directions into a subspace.) Then, the</context>
</contexts>
<marker>Davis, Kahan, 1970</marker>
<rawString>Chandler Davis and W. M. Kahan. 1970. The rotation of eigenvectors by a perturbation. III. SIAM Journal on Numerical Analysis, 7(1):1–46, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>Geroge W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the Society for Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="2666" citStr="Deerwester et al., 1990" startWordPosition="399" endWordPosition="402">to words. misdirect the succeeding iterations. Also, low frequency words are known to be problematic. They do not provide sufficient corpus statistics (e.g., how frequently the word occurs as the subject of “said”), for adequate label prediction. By contrast, we focus on improving feature vector representation for use in standard linear classifiers. To counteract data sparseness, we employ subspace projection where subspaces are derived by singular value decomposition (SVD). In this paper, we generally call such SVDbased subspace construction spectral analysis. Latent Semantic Indexing (LSI) (Deerwester et al., 1990) is a well-known application of spectral analysis to word-by-document matrices. Formal analyses of LSI were published relatively recently, e.g., (Papadimitriou et al., 2000; Azar et al., 2001). Ando and Lee (2001) show the factors that may affect LSI’s performance by analyzing the conditions under which the LSI subspace approximates an optimum subspace. Our theoretical basis is partly derived from this analysis. In particular, we replace the abstract notion of ‘optimum subspace’ with a precise definition of a subspace useful for our task. The essence of spectral analysis is to capture the most</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, Geroge W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the Society for Information Science, 41:391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="16592" citStr="Dempster et al., 1977" startWordPosition="2630" endWordPosition="2633"> above studies, and compare performance with Thelen and Riloff (2002)’s bootstrapping method. 4.3 Techniques for learning from unlabeled data While most of the above bootstrapping methods are targeted to NLP tasks, techniques such as EM and cotraining are generally applicable when equipped with appropriate models or classifiers. We will present highlevel and empirical comparisons (Sections 4.4 and 5, respectively) of Spectral with representative techniques for learning from unlabeled data, described below. Expectation Maximization (EM) is an iterative algorithm for model parameter estimation (Dempster et al., 1977). Starting from some initial model parameters, the E-step estimates the expectation of the hidden class variables. Then, the M-step recomputes the model parameters so that the likelihood is maximized, and the process repeats. EM is guaranteed to converge to some local maximum. It is very popular and useful, but also known to be sensitive to the initialization of parameters. The co-training paradigm proposed by Blum and Mitchell (1998) involves two classifiers employing two distinct views of the feature space, e.g., ‘textual content’ and ‘hyperlink’ of web documents. The two classifiers are fir</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene H Golub</author>
<author>Charles F Van Loan</author>
</authors>
<date>1996</date>
<note>Matrix computations third edition.</note>
<marker>Golub, Van Loan, 1996</marker>
<rawString>Gene H. Golub and Charles F. Van Loan. 1996. Matrix computations third edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<pages>104--211</pages>
<contexts>
<context position="14248" citStr="Landauer and Dumais (1997)" startWordPosition="2286" endWordPosition="2289">ner products, due to the orthonormality of left singular vectors. 4 Related Work and Discussion 4.1 Spectral analysis for word similarity measurement Spectral analysis has been used in traditional factor analysis techniques (such as Principal Component Analysis) to summarize high-dimensional data. LSI uses spectral analysis for measuring document or word similarities. From our perspective, the LSI word similarity measurement is similar to the special case where we have a single feature extractor that returns the document membership of word occurrence . Among numerous empirical studies of LSI, Landauer and Dumais (1997) report that using the LSI word similarity measure, 64.4% of the synonym section of TOEFL (multi-choice) were answered correctly, which rivals college students from non-English speaking countries. We conjecture that if more effective feature extractors were used, performance might be better. Sch¨uetze (1992)’s word sense disambiguation method uses spectral analysis for vector dimensionality reduction. He reports that use of spectral analysis does not affect the task performance, either positively or negatively. 4.2 Bootstrapping methods for constructing semantic lexicons A common trend for the</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to Plato’s problem. Psychological Review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Rayid Ghani</author>
</authors>
<title>Analyzing the effectiveness and applicability of co-training.</title>
<date>2000</date>
<booktitle>In Proceedings ofInformation and Knowledge Management.</booktitle>
<contexts>
<context position="17725" citStr="Nigam and Ghani (2000)" startWordPosition="2803" endWordPosition="2806">ce, e.g., ‘textual content’ and ‘hyperlink’ of web documents. The two classifiers are first trained with labeled data. Each of the classifiers adds to the labeled data pool the examples whose labels are predicted with the highest confidence. The classifiers are trained with the new augmented labeled data, and the process repeats. Its theoretical foundations are based on the assumptions that two views are redundantly sufficient and conditionally independent given classes. Abney (2002) presents an analysis to relax the (fairly strong) conditional independence assumption to weak rule dependence. Nigam and Ghani (2000) study the effectiveness of cotraining through experiments on the text categorization task. Pierce and Cardie (2001) investigate the scalability of co-training on the base noun phrase bracketing task, which typically requires a larger number of labeled examples than text categorization. They propose to manually correct labels to counteract the degradation of automatically assigned labels on large data sets. We use these two empirical studies as references for the implementation of co-training in our experiments. Co-EM (Nigam and Ghani, 2000) combines the essence of co-training and EM in an ele</context>
<context position="21141" citStr="Nigam and Ghani, 2000" startWordPosition="3320" endWordPosition="3323">gorithms discussed in the previous sections. 5.1 Baseline algorithms We use the following algorithms as baseline: EM, cotraining, and co-EM, as established techniques for learning from unlabeled data in general; the bootstrapping method proposed by Thelen and Riloff (2002) (hereafter, TRB and TR) as a state-of-the-art bootstrapping method designed for semantic lexicon construction. 5.1.1 Implementation of EM, co-training, and co-EM Naive Bayes classifier To instantiate EM, co-training, and co-EM, we use a standard Naive Bayes classifier, as it is often used for co-training experiments, e.g., (Nigam and Ghani, 2000; Pierce and Cardie, 2001). As in Nigam and Ghani (2000)’s experiments, we estimate with Laplace smoothing, and for label prediction, we compute for every : The underlying naive Bayes assumption is that occurrences of features are conditionally independent of each other, given class labels. The generative interpretation in this case is analogous to that of text categorization, when we regard features (or contexts) of all the occurrences of word as a pseudo document. We initialize model parameters ( and ) using labeled examples. The test data is labeled after iterations. We explore for EM and c</context>
</contexts>
<marker>Nigam, Ghani, 2000</marker>
<rawString>Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In Proceedings ofInformation and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos H Papadimitriou</author>
<author>Prabhakar Raghavan</author>
<author>Hisao Tamaki</author>
<author>Santosh Vempala</author>
</authors>
<title>Latent Semantic Indexing: A probabilistic analysis.</title>
<date>2000</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>61</volume>
<issue>2</issue>
<contexts>
<context position="2838" citStr="Papadimitriou et al., 2000" startWordPosition="422" endWordPosition="425">ly the word occurs as the subject of “said”), for adequate label prediction. By contrast, we focus on improving feature vector representation for use in standard linear classifiers. To counteract data sparseness, we employ subspace projection where subspaces are derived by singular value decomposition (SVD). In this paper, we generally call such SVDbased subspace construction spectral analysis. Latent Semantic Indexing (LSI) (Deerwester et al., 1990) is a well-known application of spectral analysis to word-by-document matrices. Formal analyses of LSI were published relatively recently, e.g., (Papadimitriou et al., 2000; Azar et al., 2001). Ando and Lee (2001) show the factors that may affect LSI’s performance by analyzing the conditions under which the LSI subspace approximates an optimum subspace. Our theoretical basis is partly derived from this analysis. In particular, we replace the abstract notion of ‘optimum subspace’ with a precise definition of a subspace useful for our task. The essence of spectral analysis is to capture the most prominently observed vector directions (or sub-vectors) into a subspace. Hence, we should apply spectral analysis only to ‘good’ feature vectors so that useful portions ar</context>
</contexts>
<marker>Papadimitriou, Raghavan, Tamaki, Vempala, 2000</marker>
<rawString>Christos H. Papadimitriou, Prabhakar Raghavan, Hisao Tamaki, and Santosh Vempala. 2000. Latent Semantic Indexing: A probabilistic analysis. Journal of Computer and System Sciences, 61(2):217–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Phillips</author>
<author>Ellen Riloff</author>
</authors>
<title>Exploiting strong syntactic heuristics and co-training to learn semantic lexicons.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP’02.</booktitle>
<contexts>
<context position="1484" citStr="Phillips and Riloff, 2002" startWordPosition="219" endWordPosition="222">ploy machine learning techniques or rule-based approaches, it is useful to have a gazetteer of words&apos; that reliably suggest target entity class membership. This paper considers the task of generating such gazetteers from a large unannotated corpus with minimal manual effort. Starting from a small number of labeled examples (seeds), e.g., “car”, “plane”, “ship” labeled as vehicles, we seek to automatically collect more of these. This task is sometimes called the semi-automatic construction of semantic lexicons, e.g. (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Phillips and Riloff, 2002). A common trend in prior studies is bootstrapping, which is an iterative process to collect new words and regard the words newly collected with high confidence as additional labeled examples for the next iteration. The aim of bootstrapping is to compensate for the paucity of labeled examples. However, its potential danger is label ‘contamination’ — namely, wrongly (automatically) labeled examples may &apos;Our argument in this paper holds for relatively small linguistic objects including words, phrases, collocations, and so forth. For simplicity, we refer to words. misdirect the succeeding iterati</context>
<context position="15380" citStr="Phillips and Riloff, 2002" startWordPosition="2446" endWordPosition="2449">egatively. 4.2 Bootstrapping methods for constructing semantic lexicons A common trend for the semantic lexicon construction task is that of bootstrapping, exploiting strong syntactic cues — such as a bootstrapping method that iteratively grows seeds by using cooccurrences in lists, conjunctions, and appositives (Roark and Charniak, 1998); metabootstrapping which repeatedly finds extraction patterns and extracts words from the found patterns (Riloff and Jones, 1999); a co-training combination of three bootstrapping processes each of which exploits appositives, compound nouns, and ISA-clauses (Phillips and Riloff, 2002). Thelen and Riloff (2002)’s bootstrapping method iteratively performs feature selection and word selection for each class. It outperformed the best-performing bootstrapping method for this task at the time. We also note that there are a number of bootstrapping methods successfully applied to text – e.g., word sense disambiguation (Yarowsky, 1995), named entity instance classification (Collins and Singer, 1999), and the extraction of ‘parts’ word given the ‘whole’ word (Berland and Charniak, 1999). In Section 5, we report experiments using syntactic features shown to be useful by the above stu</context>
</contexts>
<marker>Phillips, Riloff, 2002</marker>
<rawString>William Phillips and Ellen Riloff. 2002. Exploiting strong syntactic heuristics and co-training to learn semantic lexicons. In Proceedings of EMNLP’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pierce</author>
<author>Claire Cardie</author>
</authors>
<title>Limitations of co-training for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In Proceedings ofEMNLP’01.</booktitle>
<contexts>
<context position="17841" citStr="Pierce and Cardie (2001)" startWordPosition="2820" endWordPosition="2823">ata. Each of the classifiers adds to the labeled data pool the examples whose labels are predicted with the highest confidence. The classifiers are trained with the new augmented labeled data, and the process repeats. Its theoretical foundations are based on the assumptions that two views are redundantly sufficient and conditionally independent given classes. Abney (2002) presents an analysis to relax the (fairly strong) conditional independence assumption to weak rule dependence. Nigam and Ghani (2000) study the effectiveness of cotraining through experiments on the text categorization task. Pierce and Cardie (2001) investigate the scalability of co-training on the base noun phrase bracketing task, which typically requires a larger number of labeled examples than text categorization. They propose to manually correct labels to counteract the degradation of automatically assigned labels on large data sets. We use these two empirical studies as references for the implementation of co-training in our experiments. Co-EM (Nigam and Ghani, 2000) combines the essence of co-training and EM in an elegant way. Classifier A is initially trained with the labeled data, and computes probabilistically-weighted labels fo</context>
<context position="21167" citStr="Pierce and Cardie, 2001" startWordPosition="3324" endWordPosition="3327">he previous sections. 5.1 Baseline algorithms We use the following algorithms as baseline: EM, cotraining, and co-EM, as established techniques for learning from unlabeled data in general; the bootstrapping method proposed by Thelen and Riloff (2002) (hereafter, TRB and TR) as a state-of-the-art bootstrapping method designed for semantic lexicon construction. 5.1.1 Implementation of EM, co-training, and co-EM Naive Bayes classifier To instantiate EM, co-training, and co-EM, we use a standard Naive Bayes classifier, as it is often used for co-training experiments, e.g., (Nigam and Ghani, 2000; Pierce and Cardie, 2001). As in Nigam and Ghani (2000)’s experiments, we estimate with Laplace smoothing, and for label prediction, we compute for every : The underlying naive Bayes assumption is that occurrences of features are conditionally independent of each other, given class labels. The generative interpretation in this case is analogous to that of text categorization, when we regard features (or contexts) of all the occurrences of word as a pseudo document. We initialize model parameters ( and ) using labeled examples. The test data is labeled after iterations. We explore for EM and co-EM, and for co-training7</context>
<context position="23478" citStr="Pierce and Cardie (2001)" startWordPosition="3697" endWordPosition="3700">hout tf-idf weighting) with the same centroid-based classifier. Spectral has two parameters: the number of input vectors , and the subspace dimensionality . We set and based on the observation on a corpus disjoint from the test corpora, and use these settings for all the experiments. 7The maximum numbers of iterations were chosen so that the best performance of the baseline algorithms can be observed. 8Indeed, it turned out that setting to an appropriate value (2500 on the particular data described below) produces significantly better results than using all the unlabeled data. 9We also tested Pierce and Cardie (2001)’s modification to choose examples according to the label distribution, but it did not make any significant difference. Spectral TRB co-TR co-EM EM NB Tf-idf Count 100 seeds 60.2 51.7 50.4 49.4 50.7 43.3 40.7 32.6 300 seeds 62.9 47.1 57.8 55.8 53.2 50.8 46.7 35.2 500 seeds 63.8 42.7 56.7 56.6 54.3 53.2 49.9 36.0 Exploiting unlabeled data? Yes No Classification model Centroid - Naive Bayes Centroid Figure 1: F-measure results (%) on high frequency seeds. AP-corpus. Cf. Naive Bayes classifiers (NB) trained with 7500 seeds produce 62.9% on average over five runs with random training/test splits. </context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In Proceedings ofEMNLP’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="15224" citStr="Riloff and Jones, 1999" startWordPosition="2425" endWordPosition="2428">ctral analysis for vector dimensionality reduction. He reports that use of spectral analysis does not affect the task performance, either positively or negatively. 4.2 Bootstrapping methods for constructing semantic lexicons A common trend for the semantic lexicon construction task is that of bootstrapping, exploiting strong syntactic cues — such as a bootstrapping method that iteratively grows seeds by using cooccurrences in lists, conjunctions, and appositives (Roark and Charniak, 1998); metabootstrapping which repeatedly finds extraction patterns and extracts words from the found patterns (Riloff and Jones, 1999); a co-training combination of three bootstrapping processes each of which exploits appositives, compound nouns, and ISA-clauses (Phillips and Riloff, 2002). Thelen and Riloff (2002)’s bootstrapping method iteratively performs feature selection and word selection for each class. It outperformed the best-performing bootstrapping method for this task at the time. We also note that there are a number of bootstrapping methods successfully applied to text – e.g., word sense disambiguation (Yarowsky, 1995), named entity instance classification (Collins and Singer, 1999), and the extraction of ‘parts</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Jessica Shepherd</author>
</authors>
<title>A corpus-based approach for building semantic lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP’97.</booktitle>
<contexts>
<context position="1405" citStr="Riloff and Shepherd, 1997" startWordPosition="207" endWordPosition="210">mportant role in information extraction systems. Whether entity recognizers employ machine learning techniques or rule-based approaches, it is useful to have a gazetteer of words&apos; that reliably suggest target entity class membership. This paper considers the task of generating such gazetteers from a large unannotated corpus with minimal manual effort. Starting from a small number of labeled examples (seeds), e.g., “car”, “plane”, “ship” labeled as vehicles, we seek to automatically collect more of these. This task is sometimes called the semi-automatic construction of semantic lexicons, e.g. (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Phillips and Riloff, 2002). A common trend in prior studies is bootstrapping, which is an iterative process to collect new words and regard the words newly collected with high confidence as additional labeled examples for the next iteration. The aim of bootstrapping is to compensate for the paucity of labeled examples. However, its potential danger is label ‘contamination’ — namely, wrongly (automatically) labeled examples may &apos;Our argument in this paper holds for relatively small linguistic objects including words, phrases, collocations, an</context>
<context position="26358" citStr="Riloff and Shepherd, 1997" startWordPosition="4147" endWordPosition="4150">is extremely low since target classes are very sparse. Random choice would result in F-measure=6.3%. Always proposing Person would produce F=23.1%. 5.4 Features Types of feature extractors used in our experiments are essentially the same as those used in TR’s experiments, lohttp://www.nist.gov/speech/index.htm which exploit the syntactic constructions such as subjectverb, verb-object, NP-pp-NP (pp is preposition), and subject-verb-object. In addition, we exploit syntactic constructions shown to be useful by other studies — lists and conjunctions (Roark and Charniak, 1998), and adjacent words (Riloff and Shepherd, 1997). We count feature occurrences ( ) in the unannotated corpus. All the tested methods are given exactly the same data points. 5.5 High-frequency seed experiments Prior semantic lexicon studies (e.g., TR) note that the choice of seeds is critical – i.e., seeds should be highfrequency words so that methods are provided with plenty of feature information to bootstrap with. In practice, this can be achieved by first extracting the most frequent words from the target corpus and manually labeling them for use as seeds. To simulate this practical situation, we split the above 10K words into a labeled </context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>Ellen Riloff and Jessica Shepherd. 1997. A corpus-based approach for building semantic lexicons. In Proceedings of EMNLP’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨uetze</author>
</authors>
<title>Dimensions of meaning.</title>
<date>1992</date>
<booktitle>In Proceedings of Supercomputing’92,</booktitle>
<pages>787--796</pages>
<marker>Sch¨uetze, 1992</marker>
<rawString>Hinrich Sch¨uetze. 1992. Dimensions of meaning. In Proceedings of Supercomputing’92, pages 787–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Thelen</author>
<author>Ellen Riloff</author>
</authors>
<title>A bootstrapping method for learning semantic lexicons using extracting pattern contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP’02.</booktitle>
<contexts>
<context position="1456" citStr="Thelen and Riloff, 2002" startWordPosition="215" endWordPosition="218">her entity recognizers employ machine learning techniques or rule-based approaches, it is useful to have a gazetteer of words&apos; that reliably suggest target entity class membership. This paper considers the task of generating such gazetteers from a large unannotated corpus with minimal manual effort. Starting from a small number of labeled examples (seeds), e.g., “car”, “plane”, “ship” labeled as vehicles, we seek to automatically collect more of these. This task is sometimes called the semi-automatic construction of semantic lexicons, e.g. (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Phillips and Riloff, 2002). A common trend in prior studies is bootstrapping, which is an iterative process to collect new words and regard the words newly collected with high confidence as additional labeled examples for the next iteration. The aim of bootstrapping is to compensate for the paucity of labeled examples. However, its potential danger is label ‘contamination’ — namely, wrongly (automatically) labeled examples may &apos;Our argument in this paper holds for relatively small linguistic objects including words, phrases, collocations, and so forth. For simplicity, we refer to words. misd</context>
<context position="15406" citStr="Thelen and Riloff (2002)" startWordPosition="2450" endWordPosition="2453"> methods for constructing semantic lexicons A common trend for the semantic lexicon construction task is that of bootstrapping, exploiting strong syntactic cues — such as a bootstrapping method that iteratively grows seeds by using cooccurrences in lists, conjunctions, and appositives (Roark and Charniak, 1998); metabootstrapping which repeatedly finds extraction patterns and extracts words from the found patterns (Riloff and Jones, 1999); a co-training combination of three bootstrapping processes each of which exploits appositives, compound nouns, and ISA-clauses (Phillips and Riloff, 2002). Thelen and Riloff (2002)’s bootstrapping method iteratively performs feature selection and word selection for each class. It outperformed the best-performing bootstrapping method for this task at the time. We also note that there are a number of bootstrapping methods successfully applied to text – e.g., word sense disambiguation (Yarowsky, 1995), named entity instance classification (Collins and Singer, 1999), and the extraction of ‘parts’ word given the ‘whole’ word (Berland and Charniak, 1999). In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performa</context>
<context position="20793" citStr="Thelen and Riloff (2002)" startWordPosition="3270" endWordPosition="3273"> feature extractors. We say features are useful if dependency and feature mingling (defined in the Appendix) are small. It is interesting to see how these differences affect the performance on the word classification task. We will report experimental results in the next section. 5 Experiments We study Spectral’s performance in comparison with the algorithms discussed in the previous sections. 5.1 Baseline algorithms We use the following algorithms as baseline: EM, cotraining, and co-EM, as established techniques for learning from unlabeled data in general; the bootstrapping method proposed by Thelen and Riloff (2002) (hereafter, TRB and TR) as a state-of-the-art bootstrapping method designed for semantic lexicon construction. 5.1.1 Implementation of EM, co-training, and co-EM Naive Bayes classifier To instantiate EM, co-training, and co-EM, we use a standard Naive Bayes classifier, as it is often used for co-training experiments, e.g., (Nigam and Ghani, 2000; Pierce and Cardie, 2001). As in Nigam and Ghani (2000)’s experiments, we estimate with Laplace smoothing, and for label prediction, we compute for every : The underlying naive Bayes assumption is that occurrences of features are conditionally indepen</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>Michael Thelen and Ellen Riloff. 2002. A bootstrapping method for learning semantic lexicons using extracting pattern contexts. In Proceedings of EMNLP’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley Interscience,</publisher>
<location>New York.</location>
<contexts>
<context position="7161" citStr="Vapnik (1998)" startWordPosition="1128" endWordPosition="1129">n the above unannotated corpus. Feature might be, for instance, the set of head nouns that participate in list construction with the focus word of . 2E.g., “plant” might be most likely to be a living thing if it occurred in gardening books, but it might be most likely to be a facility in newspaper articles. Seed words and their labels. We assume that the -labels of several words in are revealed as labeled examples. Note that in this task configuration, test data is known at the time of training (as in the transductive setting). Although we do not pursue transductive learning techniques (e.g., Vapnik (1998)) in this work, we will set up the experimental framework accordingly. 3 Using Vector Similarity 3.1 Error Factors Consider a straightforward feature vector representation using normalized joint counts of features and the word, which we call count vector . More formally, the - th element of is where denotes the count of events observed in the unannotated corpus. One way to classify words would be to compare count vectors for seeds and words and to choose the most similar seeds, using inner products as the similarity measure. Let us investigate the factors that may affect the performance of suc</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir Vapnik. 1998. Statistical Learning Theory. Wiley Interscience, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings ofACL’95,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="15729" citStr="Yarowsky, 1995" startWordPosition="2501" endWordPosition="2502">which repeatedly finds extraction patterns and extracts words from the found patterns (Riloff and Jones, 1999); a co-training combination of three bootstrapping processes each of which exploits appositives, compound nouns, and ISA-clauses (Phillips and Riloff, 2002). Thelen and Riloff (2002)’s bootstrapping method iteratively performs feature selection and word selection for each class. It outperformed the best-performing bootstrapping method for this task at the time. We also note that there are a number of bootstrapping methods successfully applied to text – e.g., word sense disambiguation (Yarowsky, 1995), named entity instance classification (Collins and Singer, 1999), and the extraction of ‘parts’ word given the ‘whole’ word (Berland and Charniak, 1999). In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)’s bootstrapping method. 4.3 Techniques for learning from unlabeled data While most of the above bootstrapping methods are targeted to NLP tasks, techniques such as EM and cotraining are generally applicable when equipped with appropriate models or classifiers. We will present highlevel an</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings ofACL’95, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>