<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000053">
<title confidence="0.9972705">
Thread-Level Information for Comment Classification
in Community Question Answering
</title>
<author confidence="0.997368">
Alberto Barr´on-Cede˜no, Simone Filice, Giovanni Da San Martino,
Shafiq Joty, Lluis M`arquez, Preslav Nakov, Alessandro Moschitti
</author>
<affiliation confidence="0.994292">
Qatar Computing Research Institute, Hamad Bin Khalifa University
</affiliation>
<address confidence="0.420044">
{albarron,sfilice,gmartino, sjoty,lmarquez,
</address>
<email confidence="0.990996">
pnakov,amoschitti}@qf.org.qa
</email>
<sectionHeader confidence="0.993658" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998692739130435">
Community Question Answering (cQA) is
a new application of QA in social contexts
(e.g., fora). It presents new interesting
challenges and research directions, e.g.,
exploiting the dependencies between the
different comments of a thread to select
the best answer for a given question. In
this paper, we explored two ways of mod-
eling such dependencies: (i) by designing
specific features looking globally at the
thread; and (ii) by applying structure pre-
diction models. We trained and evaluated
our models on data from SemEval-2015
Task 3 on Answer Selection in cQA. Our
experiments show that: (i) the thread-level
features consistently improve the perfor-
mance for a variety of machine learning
models, yielding state-of-the-art results;
and (ii) sequential dependencies between
the answer labels captured by structured
prediction models are not enough to im-
prove the results, indicating that more in-
formation is needed in the joint model.
</bodyText>
<sectionHeader confidence="0.999112" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994052696428572">
Community Question Answering (cQA) is an evo-
lution of a typical QA setting put in a Web forum
context, where user interaction is enabled, without
much restrictions on who can post and who can
answer a question. This is a powerful mechanism,
which allows users to freely ask questions and ex-
pect some good, honest answers.
Unfortunately, a user has to go through all pos-
sible answers and to make sense of them. It is of-
ten the case that many answers are only loosely re-
lated to the actual question, and some even change
the topic. This is especially common for long
threads where, as the thread progresses, users start
talking to each other, instead of trying to answer
the initial question.
This is a real problem, as a question can have
hundreds of answers, the vast majority of which
would not satisfy the users’ information needs.
Thus, finding the desired information in a long list
of answers might be very time-consuming.
The problem of selecting the relevant text pas-
sages (i.e., those containing good answers) has
been tackled in QA research, either for non-factoid
QA or for passage reranking. Usually, automatic
classifiers are applied to the answer passages re-
trieved by a search engine to derive a relative or-
der; see (Radlinski and Joachims, 2005; Jeon et
al., 2005; Shen and Lapata, 2007; Moschitti et
al., 2007; Surdeanu et al., 2008; Heilman and
Smith, 2010; Wang and Manning, 2010; Severyn
and Moschitti, 2012; Yao et al., 2013; Severyn et
al., 2013; Severyn and Moschitti, 2013) for detail.
To the best of our knowledge, there is no
QA work that effectively identifies good answers
based on the selection of the other answers re-
trieved for a question. This is mainly due to the
loose dependencies between the different answer
passages in standard QA. In contrast, we postulate
that in a cQA setting, the answers from different
users in a common thread are strongly intercon-
nected and, thus, a joint answer selection model
should be adopted to achieve higher accuracy.
To test our hypothesis about the usefulness of
thread-level information, we used a publicly avail-
able dataset, recently developed for the SemEval-
2015 Task 3 (Nakov et al., 2015). Subtask A in
that challenge asks to identify the posts in the an-
swer thread that answer the question well vs. those
that can be potentially useful to the user vs. those
that are just bad or useless.
We model the thread-level dependencies in two
different ways: (i) by designing specific features
that are able to capture the dependencies between
the answers in the same thread; and (ii) by exploit-
ing the sequential organization of the output labels
for the complete thread.
</bodyText>
<page confidence="0.808148">
687
</page>
<footnote confidence="0.769267142857143">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 687–693,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
Q: Can I obtain Driving License my QID is written Em-
ployee
A1 the word employee is a general term that refers to all the
staff in your company either the manager, secretary up
to the lowest position or whatever positions they have.
you are all considered employees of your company.
A2 your qid should specify what is the actual profession you
have. i think for me, your chances to have a drivers
license is low.
A3 dear richard, his asking if he can obtain. means he have
the driver license
</footnote>
<note confidence="0.488766">
A4 Slim chance ...
</note>
<figureCaption confidence="0.9928515">
Figure 1: Simplified example from SemEval-2015
Task 3, English subtask A.
</figureCaption>
<bodyText confidence="0.962203266666667">
For the latter, we used the usual extensions
of Logistic Regression and SVM to linear-chain
models such as CRF and SVMhmm.
The results clearly show that the thread-level
features are important, providing consistent im-
provement for all our learning models. In contrast,
the linear-chain models fail to exploit the sequen-
tial dependencies between nearby answer labels to
improve the results significantly: although the la-
bels from the neighboring answers can affect the
label of the current answer, this dependency is too
loose to have impact on the selection accuracy. In
other words, labels should be used together with
answers’ content to account for stronger and more
effective dependencies.
</bodyText>
<sectionHeader confidence="0.95419" genericHeader="method">
2 The Task
</sectionHeader>
<bodyText confidence="0.993878117647059">
We use the CQA-QL corpus, which was used for
Subtask A of SemEval-2015 Task 3 on Answer Se-
lection in cQA. The corpus contains data from
the Qatar Living forum,1 and is publicly avail-
able on the task’s website.2 The dataset consists of
questions and a list of the answers for each ques-
tion, i.e., the question-answer thread. Each ques-
tion, and also each answer, consists of a short title
and a more detailed description. Moreover, there
is some meta information associated with both,
e.g., ID of the user asking/answering the question,
timestamp, question category, etc.
The task asks to determine for each answer in
the thread whether it is good, bad, or potentially
useful. A simplified example is shown in Fig-
ure 1,3 where answers 2 and 4 are good, answer
1 is potentially useful, and answer 3 is bad.
</bodyText>
<footnote confidence="0.92557725">
1http://www.qatarliving.com/forum
2http://alt.qcri.org/semeval2015/task3/
3http://www.qatarliving.com/moving-qatar/posts/can-i-
obtain-driving-license-my-qid-written-employee
</footnote>
<bodyText confidence="0.9998858">
Below, we start with the original definition of
Subtask A, as described above. Then, we switch
to a binary classification setting (i.e., identifying
good vs. bad answers), which is much closer to a
real cQA application (see Section 4.3).
</bodyText>
<sectionHeader confidence="0.966582" genericHeader="method">
3 Basic and Thread-Level Features
</sectionHeader>
<bodyText confidence="0.999916142857143">
Subsection 3.1 summarizes the basic features we
used to implement the baseline systems. More im-
portantly, Section 3.2 describes the set of thread-
level features we designed in order to test our
working hypothesis. Below we use the following
notation: q is a question posted by user uQ, c is a
comment, and C is the comment thread.
</bodyText>
<subsectionHeader confidence="0.997838">
3.1 Baseline Features
</subsectionHeader>
<bodyText confidence="0.936517592592593">
We measure lexical and syntactic similarity be-
tween q and c. We compute the similarity between
word n-grams (n = [1, ... , 4]), after stopword
removal, using greedy string tiling (Wise, 1996),
longest common subsequences (Allison and Dix,
1986), Jaccard coefficient (Jaccard, 1901), word
containment (Lyon et al., 2001), and cosine sim-
ilarity. We also apply partial tree kernels (Mos-
chitti, 2006) on shallow syntactic trees.
We designed a set of heuristic features that
might suggest whether c is good or not. Forty-four
Boolean features express whether c (i) includes
URLs or emails (2 feats.); (ii) contains the word
“yes”, “sure”, “no”, “can”, “neither”, “okay”, and
“sorry”, as well as symbols ‘?’ and ‘@’ (9 feats.);
(iii) starts with “yes” (1 feat.); (iv) includes a se-
quence of three or more repeated characters or
a word longer than fifteen characters (2 feats.);
(v) belongs to one of the categories of the forum
(Socialising, Life in Qatar, etc.) (26 feats.); and
(vi) has been posted by the same uQ, such a com-
ment can include a question (i.e., contain a ques-
tion mark), and acknowledgement (e.g., contain
thank*, acknowl*), or none of them (4 feats.). An
extra feature captures the length of c (as longer —
good— comments usually contain detailed infor-
mation to answer a question).
</bodyText>
<subsectionHeader confidence="0.99992">
3.2 Thread-Level Global Features
</subsectionHeader>
<bodyText confidence="0.999901">
Comments are organized sequentially according to
the time line of the comment thread.4 Our first
four features indicate whether c appears in the
proximity of a comment by uQ.
</bodyText>
<footnote confidence="0.805875">
4The task organizers report that some comments in the
threads were discarded due to disagreement in the annotation
process. The extent of discarded comments is unknown.
</footnote>
<page confidence="0.9899">
688
</page>
<table confidence="0.999729636363636">
Pca Rca Fl,ca A
Baseline Features
SVM 52.96 53.14 52.87 67.61
OrdReg 53.33 51.54 51.87 65.38
Baseline+Thread-level Features
SVM 56.31 56.46 56.33 72.27
OrdReg 57.68 57.04 57.20 72.47
SemEval top three
JAIST 57.31 57.20 57.19 72.52
HITSZ 57.83 56.82 56.41 68.67
QCRI 54.34 53.57 53.74 70.50
</table>
<tableCaption confidence="0.999607">
Table 1: Macro-averaged precision, recall, Fi-
</tableCaption>
<bodyText confidence="0.98200565">
measure, and accuracy on the multi-class (good,
bad, potential) setting on the official SemEval-
2015 Task 3 test set. The top-2 systems are in-
cluded for comparison. QCRI refers to our official
results, using an older version of our system.
The assumption is that an acknowledgment or
further questions by uq in the thread could sig-
nal a good answer. More specifically, they test
if among the comments following c there is one
by uq (i) containing an acknowledgment, (ii) not
containing an acknowledgment, (iii) containing a
question, and, (iv) if among the comments preced-
ing c there is one by uq containing a question. The
value of these four features —a propagation of the
information captured by some of the heuristics de-
scribed in Section 3.1— depends on the distance
k, in terms of the number of comments, between c
and the closest comment by uq:
� _ max (0, 1.1 − (k · 0.1)) 1
AC) O 0 if no comments by uq exist, ( )
that is, the closer the comment to cq, the higher the
value assigned to this feature.
We try to model potential dialogues, which at
the end represent bad comments, by identifying
interlacing comments between two users. Our dia-
logue features are identifying conversation chains:
ui -* ... -* uj -* ... -* ui -* ... -* [uj].
Comments by other users can appear in between
the nodes of this “pseudo-conversation” chain. We
consider three features: whether a comment is at
the beginning, in the middle, or at the end of such
a chain. Three more features exist in those cases
in which uq is one of the participants of these
pseudo-conversations.
Another Boolean feature for cui is set to true if
ui wrote more than one comment in the current
thread. Three more features identify the first, the
middle and the last comments by ui. One extra
feature counts the total number of comments writ-
ten by ui in the thread up to that moment.
Table 2: Performance of the binary (good vs. bad)
classifiers on the official SemEval-2015 Task 3 test
dataset. Precision, recall, Fi-measure and accu-
racy are calculated at the comment level, while
F1,ta and Ata are averaged at the thread level.
Moreover, we empirically observed that the
likelihood of some comment being good decreases
with its position in the thread. Therefore,
we also included another real-valued feature:
max(20, i)/20, where i represents the position of
the comment in the thread.
Finally, we perform a pseudo-ranking of the
comments. The relevance of c is computed as its
similarity to q (using word n-grams), normalized
by the maximum similarity among all the com-
ments in the thread. The resulting relative scores
are mapped into three binary features depending
on the range they fall at: [0, 0.2], (0.2, 0.8), or
[0.8, 1] (intervals resemble the three-class setting
and were empirically set on the training data).
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99999">
Below we first describe the data we used, then we
introduce the experimental setup, and finally we
present and discuss the results of our experiments.
</bodyText>
<subsectionHeader confidence="0.97792">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999767454545454">
The original CQA-QL corpus (Nakov et al., 2015)
consists of 3,229 questions: 2,600 for training,
300 for development, and 329 for testing. The
total number of comments is 20,162, with an
average of 6.24 comments per question. The
class labels for the comments are distributed
as follows: 9,941 good (49.31%), 2,013 poten-
tial (9.98%), and 8,208 bad (40.71%) comments.
Since a typical answer selection setting only
considers correct and incorrect answers, we also
experiment with potential labelled as bad.
</bodyText>
<table confidence="0.769475657894737">
P R Fl A
Fl,ta Ata
Baseline Features
SVM 70.58 84.45 76.89 74.39
SVMhmm 72.57 85.46 78.49 76.37
LogReg 65.05 91.27 75.96 70.85
CRFmap 72.48 86.66 78.94 76.67
CRFmpm 71.55 84.25 77.38 75.15
Baseline+Thread-level Features
SVM 75.29 85.26 79.96 78.44
SVMhmm 74.84 83.25 78.82 77.43
LogReg 73.32 86.56 79.39 77.33
CRFmap 73.77 85.76 79.31 77.43
CRFmpm 74.35 85.46 79.51 77.78
66.52 76.13
68.55 77.58
68.84 74.79
67.17 76.55
66.54 75.42
67.65 76.02
66.61 77.06
68.10 75.57
66.37 76.08
67.36 76.63
689
P R Fl A Fl,ta Ata
Baseline Features 82.34±1.04 74.98±0.73 72.90±1.00 64.56±0.97 75.32±0.40
SVM 68.86±1.42 81.00±1.98 75.28±1.05 73.75±1.56 65.25±1.16 74.68±1.05
SVMhmm 70.34±1.57 88.54±0.81 74.42±0.80 69.99±0.94 66.00±1.33 73.04±0.96
LogReg 64.20±1.33 80.63±1.76 74.42±1.29 72.66±1.75 63.90±1.71 73.51±0.73
CRFmap 69.11±1.41 81.17±1.28 74.93±1.19 73.20±1.77 64.53±1.37 74.32±0.92
CRFmpm 69.60±1.65
Baseline+Thread-level Features
SVM 72.55±0.96 83.39±1.36 77.59±0.95 76.23±1.37 66.41±1.30 76.23±0.45
SVMhmm 73.24±1.66 81.66±1.21 77.21±1.18 76.20±1.81 65.33±1.12 76.43±0.92
LogReg 71.15±0.96 84.44±1.50 77.22±1.07 75.43±1.47 66.57±1.49 75.05±0.70
CRFmap 71.27±1.20 83.15±1.81 76.75±1.28 75.14±1.72 65.36±1.45 75.61±0.63
CRFmpm 71.56±1.31 83.34±1.84 77.00±1.35 75.43±1.84 65.57±1.54 75.71±0.71
</table>
<tableCaption confidence="0.756911333333333">
Table 3: Precision, Recall, F1, Accuracy computed at the comment level; F1,ta and Ata are averaged at
the thread level. Precision, Recall, F1, F1,ta are computed with respect to the good classifier on 5-fold
cross-validation (mean±stand. dev.).
</tableCaption>
<subsectionHeader confidence="0.914956">
4.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999900961538462">
Our local classifiers are support vector machines
(SVM) with C = 1 (Joachims, 1999), logistic
regression with a Gaussian prior with variance 10,
and logistic ordinal regression (McCullagh, 1980).
In order to capture long-range sequential depen-
dencies, we use a second-order SVMhmm (Yu
and Joachims, 2008) (with C = 500 and
epsilon = 0.01) and a second-order linear-chain
CRF, which considers dependencies between
three neighboring labels in a sequence (Lafferty et
al., 2001; Cuong et al., 2014). In CRF, we perform
two kinds of inference to find the most probable
labels for the comments in a sequence. (i) We
compute the maximum a posterior (MAP) or the
(jointly) most probable sequence of labels using
the Viterbi algorithm. Specifically, it computes
y∗ = argmaxy1:T P(y1:T|x1:T), where T is the
number of comments in the thread. (ii) We use
the forward–backward algorithm to find the labels
by maximizing (individual) posterior marginals
(MPM). More formally, we compute y� =
(argmaxy1P(y1|x1:T), ··· , argmaxyTP(yT|x1:T)).
While MAP yields a globally consistent sequence
of labels, MPM can be more robust in many cases;
see (Murphy, 2012, p. 613) for details. CRF also
uses a Gaussian prior with variance 10.5
</bodyText>
<subsectionHeader confidence="0.999021">
4.3 Experiment results
</subsectionHeader>
<bodyText confidence="0.992132318181818">
In order to compare the quality of our features to
the existing state of the art, we performed a first
experiment aligned to the multi-class setting of the
SemEval 2015 Task 3 competition. Table 1 shows
our results on the official test dataset.
5Varying regularization strength (variance of the prior)
did not make much difference.
As in the competition, the results are macro-
averaged at class level. The results of the top 3
systems are reported for comparison: JAIST (Tran
et al., 2015), HITSZ (Hou et al., 2015) and
QCRI (Nicosia et al., 2015), where the latter refers
to our old system that we used for the competition.
The two main observations are (i) using thread-
level features helps significantly; and (ii) the ordi-
nal regression model, which captures the idea that
potential lies between good and bad, achieves at
least as good results as the top system at SemEval,
namely JAIST.
For the remaining experiments, we reduce the
multi-class problem to a binary one (cf. Section 2).
Table 2 shows the results obtained on the official
test dataset. Note that ordinal regression is not ap-
plicable in this binary setting. The F1 values for
the baseline features suggest that using the labels
in the thread sequence yields better performance
with SVMhmm and CRF. When thread-level fea-
tures are used, the models using sequence labels
do not outperform SVM and logistic regression
anymore. Regarding the two variations of CRF,
the posterior marginals maximization is slightly
better: maximizing on each comment pays more
than on the entire thread.
Since the task consists in identifying good an-
swers for a given question, further figures at the
question level are necessary, i.e., we compute the
target performance measure for all comments of
each question and then we average the results over
all threads (ta). Table 2 shows such the result using
two measures: F1 and accuracy, i.e., F1,ta and Ata,
for which long threads have less impact on the fi-
nal outcome. The impact of the thread features is
not-so-high in terms of these measures, sometimes
even negatively affecting some of the models.
</bodyText>
<page confidence="0.995536">
690
</page>
<table confidence="0.930586476190476">
Qu1: Gymnastic world cup.
Does anyone know what time the competition
starts today?
Thanks
c1,u2: sorry - is this being held here in Bad--+Bad
Doha? If so, I’d love to go.
Expat Sueo
P.S. Is that a labradoodle in your
avatar?
c2,u1: No actually a Cockapoo! Bad--+Bad
Yes the comp. runs from today until
Wednesday at Aspire.
c3,u2: Thanks for the info - maybe I’ll turn Good--+Bad
up after the TableTop Sale is done
and dusted!
ES
P.S. Cute pup!
Qu4: Good Scissor.
Dears, anyone have an idea where to find a good
scissor for hair and beard trimming please???
c1,u5: Visit Family food center Bad--+Good
</table>
<tableCaption confidence="0.512735666666667">
c2,u6: Al rawnaq airport road...U’ll find all Bad--+Good
types of scissors there...
c3,u4: Thank you all ... I will try that. Bad--+Bad
</tableCaption>
<figureCaption confidence="0.8038865">
Figure 2: Two real question–comments threads
(simplified; ID in CQA-QL: Q770 and Q752). The
</figureCaption>
<bodyText confidence="0.965034104166667">
sub-indexes stand for the position in the thread and
the author of the comment. The class label corre-
sponds to the prediction before and after consider-
ing thread-level information. The right-hand label
matches with the gold one in all the cases.
Cross validation. In order to better understand
the mixed results obtained on the single official
test set, we performed 5-fold cross validation over
the entire dataset. The results are shown in Ta-
ble 3. When looking at the performance of the
different models with the same set of features, no
statistically significant differences are observed on
F1 or F1,ta (t-test with confidence level 95%). The
sequence of predicted labels in CRF or SVMhmm
does not impact the final result. In contrast, an im-
portant difference is observed when thread-level
features come into play: the performance of all the
models improves by approximately two F1 points
absolute, and statistically significant differences
are observed for SVM and logistic regression (t-
test, 95%). Moreover, while on the test dataset the
thread-level features do not always improve F1,ta
and Ata, on the 5-fold cross-validation using them
is always beneficial: for F1,ta statistically signifi-
cant difference is observed for SVM only (t-test,
90%).
Qualitative results. In order to get an intuition
about the effect of the thread-level features, we
show two example comment threads in Figure 2.
These comments are classified correctly when
thread features are used in the classifier, and in-
correctly when only basic features are used.
In the first case (Qu1), the third comment is clas-
sified as good by models that only use basic fea-
tures. In contrast, thanks to the thread-level fea-
tures, the classifier can consider that there is a di-
alogue between u1 and u2, causing all the com-
ments to be assigned to the correct class: bad.
In the second example (Qu4), the first two com-
ments are classified as bad when using the basic
features. However, the third comment —written
by the same user who asked Qu4— includes an
acknowledgment. The latter is propagated to the
previous comments in terms of a thread feature,
which indicates that such comments are more
likely to be good answers. This feature provides
the classifier with enough information to properly
label the first two comments as good.
</bodyText>
<sectionHeader confidence="0.996564" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999961055555556">
We presented a study on using dependencies be-
tween the different answers in the same question
thread in the context of answer selection in cQA.
Our experiments with different classifiers, fea-
tures, and experimental conditions, reveal that an-
swer dependencies are helpful to improve results
on the task. Such dependencies are best exploited
by means of carefully designed thread-level fea-
tures, whereas sequence label information alone,
e.g., used in CRF or SVMhmm, is not effective.
In future work, we plan to (i) experiment with
more sophisticated thread-level features, as well
as with other features that model context in gen-
eral; (ii) try data from other cQA websites, e.g.,
where dialogue between users is marked explic-
itly; and finally, (iii) integrate sequence, prece-
dence, dependency information with global —
thread-level— features in a unified framework.
</bodyText>
<sectionHeader confidence="0.989776" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996245">
This research is developed by the Arabic Lan-
guage Technologies (ALT) group at the Qatar
Computing Research Institute (QCRI), Hamad
Bin Khalifa University, within Qatar Foundation
in collaboration with MIT. It is part of the Interac-
tive sYstems for Answer Search (Iyas) project.
</bodyText>
<page confidence="0.998065">
691
</page>
<sectionHeader confidence="0.98878" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988503009009009">
Lloyd Allison and Trevor Dix. 1986. A bit-string
longest-common-subsequence algorithm. Inf. Pro-
cess. Lett., 23(6):305–310, December.
Nguyen Viet Cuong, Nan Ye, Wee Sun Lee, and
Hai Leong Chieu. 2014. Conditional random field
with high-order dependencies for sequence labeling
and segmentation. The Journal of Machine Learn-
ing Research, 15(1):981–1009.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceed-
ings of Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ’10, pages 1011–1019, Los Angeles, Califor-
nia, USA.
Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun
Zhang, Jun Xu, and Qingcai Chen. 2015. HITSZ-
ICRC: Exploiting classification approach for answer
selection in community question answering. In Pro-
ceedings of the 9th International Workshop on Se-
mantic Evaluation, SemEval ’15, pages 196–202,
Denver, Colorado, USA.
Paul Jaccard. 1901. ´Etude comparative de la distribu-
tion florale dans une portion des Alpes et des Jura.
Bulletin del la Soci´et´e Vaudoise des Sciences Na-
turelles, 37:547–579.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM In-
ternational Conference on Information and Knowl-
edge Management, CIKM ’05, pages 84–90, Bre-
men, Germany.
Thorsten Joachims. 1999. Making Large-scale Sup-
port Vector Machine Learning Practical. In Bern-
hard Sch¨olkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Advances in Kernel
Methods, pages 169–184. MIT Press, Cambridge,
Massachusetts, USA.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth
International Conference on Machine Learning,
ICML ’01, pages 282–289, San Francisco, Califor-
nia, USA.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in
large document collections. In Proceedings of the
2001 Conference on Empirical Methods in Natural
Language Processing, EMNLP ’01, pages 118–125,
Pittsburgh, Pennsylvania, USA.
Peter McCullagh. 1980. Regression models for ordinal
data. J. Roy. Statist. Soc. B, 42:109–142.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion answer classification. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, ACL ’07, pages 776–783, Prague,
Czech Republic.
Alessandro Moschitti. 2006. Efficient Convolution
Kernels for Dependency and Constituent Syntac-
tic Trees. In Johannes F¨urnkranz, Tobias Scheffer,
and Myra Spiliopoulou, editors, Machine Learning:
ECML 2006, volume 4212 of Lecture Notes in Com-
puter Science, pages 318–329. Springer Berlin Hei-
delberg.
Kevin Murphy. 2012. Machine Learning A Probabilis-
tic Perspective. The MIT Press.
Preslav Nakov, Llu´ıs M`arquez, Walid Magdy, Alessan-
dro Moschitti, Jim Glass, and Bilal Randeree. 2015.
SemEval-2015 task 3: Answer selection in com-
munity question answering. In Proceedings of the
9th International Workshop on Semantic Evaluation,
SemEval ’15, pages 269–281, Denver, Colorado,
USA.
Massimo Nicosia, Simone Filice, Alberto Barr´on-
Cede˜no, Iman Saleh, Hamdy Mubarak, Wei Gao,
Preslav Nakov, Giovanni Da San Martino, Alessan-
dro Moschitti, Kareem Darwish, Llu´ıs M`arquez,
Shafiq Joty, and Walid Magdy. 2015. QCRI: An-
swer selection for community question answering -
experiments for Arabic and English. In Proceedings
of the 9th International Workshop on Semantic Eval-
uation, SemEval ’15, pages 203–209, Denver, Col-
orado, USA.
Filip Radlinski and Thorsten Joachims. 2005. Query
chains: Learning to rank from implicit feedback. In
Proceedings of the Eleventh ACM SIGKDD Interna-
tional Conference on Knowledge Discovery in Data
Mining, KDD ’05, pages 239–248, Chicago, Illinois,
USA.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’12,
pages 741–750, Portland, Oregon, USA.
Aliaksei Severyn and Alessandro Moschitti. 2013.
Automatic feature engineering for answer selection
and extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’13, pages 458–467, Seattle,
Washington, USA.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ’13, pages 75–83, Sofia,
Bulgaria.
</reference>
<page confidence="0.976404">
692
</page>
<reference confidence="0.997814904761905">
Dan Shen and Mirella Lapata. 2007. Using seman-
tic roles to improve question answering. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ’07, pages 12–21, Prague, Czech Republic.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large
online QA collections. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics and the Human Language Tech-
nology Conference, ACL-HLT ’08, pages 719–727,
Columbus, Ohio, USA.
Quan Hung Tran, Vu Tran, Tu Vu, Minh Nguyen, and
Son Bao Pham. 2015. JAIST: Combining multiple
features for answer selection in community question
answering. In Proceedings of the 9th International
Workshop on Semantic Evaluation, SemEval ’15,
pages 215–219, Denver, Colorado, USA.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question an-
swering. In Proceedings of the 23rd International
Conference on Computational Linguistics, COL-
ING ’10, pages 1164–1172, Beijing, China.
Michael Wise. 1996. Yap3: Improved detection of
similarities in computer program and other texts. In
Proceedings of the Twenty-seventh SIGCSE Tech-
nical Symposium on Computer Science Education,
SIGCSE ’96, pages 130–134, New York, New York,
USA.
Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013. Answer extrac-
tion as sequence tagging with tree edit distance. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT ’13, pages 858–867.
Chun-Nam Yu and T. Joachims. 2008. Training struc-
tural SVMs with kernels using sampled cuts. In
ACM SIGKDD Conference on Knowledge Discov-
ery and Data Mining, KDD ’08, pages 794–802.
</reference>
<page confidence="0.99912">
693
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738453">
<title confidence="0.999211">Thread-Level Information for Comment in Community Question Answering</title>
<author confidence="0.876691">Simone Filice</author>
<author confidence="0.876691">Giovanni Da San Qatar Computing Research Institute</author>
<author confidence="0.876691">Hamad Bin Khalifa</author>
<abstract confidence="0.9992115">Community Question Answering (cQA) is a new application of QA in social contexts (e.g., fora). It presents new interesting challenges and research directions, e.g., exploiting the dependencies between the different comments of a thread to select the best answer for a given question. In this paper, we explored two ways of modsuch dependencies: by designing specific features looking globally at the and by applying structure prediction models. We trained and evaluated our models on data from SemEval-2015 Task 3 on Answer Selection in cQA. Our show that: the thread-level features consistently improve the performance for a variety of machine learning models, yielding state-of-the-art results; sequential dependencies between the answer labels captured by structured prediction models are not enough to improve the results, indicating that more information is needed in the joint model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lloyd Allison</author>
<author>Trevor Dix</author>
</authors>
<title>A bit-string longest-common-subsequence algorithm.</title>
<date>1986</date>
<journal>Inf. Process. Lett.,</journal>
<volume>23</volume>
<issue>6</issue>
<contexts>
<context position="7368" citStr="Allison and Dix, 1986" startWordPosition="1175" endWordPosition="1178">and Thread-Level Features Subsection 3.1 summarizes the basic features we used to implement the baseline systems. More importantly, Section 3.2 describes the set of threadlevel features we designed in order to test our working hypothesis. Below we use the following notation: q is a question posted by user uQ, c is a comment, and C is the comment thread. 3.1 Baseline Features We measure lexical and syntactic similarity between q and c. We compute the similarity between word n-grams (n = [1, ... , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also apply partial tree kernels (Moschitti, 2006) on shallow syntactic trees. We designed a set of heuristic features that might suggest whether c is good or not. Forty-four Boolean features express whether c (i) includes URLs or emails (2 feats.); (ii) contains the word “yes”, “sure”, “no”, “can”, “neither”, “okay”, and “sorry”, as well as symbols ‘?’ and ‘@’ (9 feats.); (iii) starts with “yes” (1 feat.); (iv) includes a sequence of three or more repeated characters or a word longer than fif</context>
</contexts>
<marker>Allison, Dix, 1986</marker>
<rawString>Lloyd Allison and Trevor Dix. 1986. A bit-string longest-common-subsequence algorithm. Inf. Process. Lett., 23(6):305–310, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Viet Cuong</author>
<author>Nan Ye</author>
<author>Wee Sun Lee</author>
<author>Hai Leong Chieu</author>
</authors>
<title>Conditional random field with high-order dependencies for sequence labeling and segmentation.</title>
<date>2014</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="14651" citStr="Cuong et al., 2014" startWordPosition="2334" endWordPosition="2337">1,ta are computed with respect to the good classifier on 5-fold cross-validation (mean±stand. dev.). 4.2 Experimental Setup Our local classifiers are support vector machines (SVM) with C = 1 (Joachims, 1999), logistic regression with a Gaussian prior with variance 10, and logistic ordinal regression (McCullagh, 1980). In order to capture long-range sequential dependencies, we use a second-order SVMhmm (Yu and Joachims, 2008) (with C = 500 and epsilon = 0.01) and a second-order linear-chain CRF, which considers dependencies between three neighboring labels in a sequence (Lafferty et al., 2001; Cuong et al., 2014). In CRF, we perform two kinds of inference to find the most probable labels for the comments in a sequence. (i) We compute the maximum a posterior (MAP) or the (jointly) most probable sequence of labels using the Viterbi algorithm. Specifically, it computes y∗ = argmaxy1:T P(y1:T|x1:T), where T is the number of comments in the thread. (ii) We use the forward–backward algorithm to find the labels by maximizing (individual) posterior marginals (MPM). More formally, we compute y� = (argmaxy1P(y1|x1:T), ··· , argmaxyTP(yT|x1:T)). While MAP yields a globally consistent sequence of labels, MPM can </context>
</contexts>
<marker>Cuong, Ye, Lee, Chieu, 2014</marker>
<rawString>Nguyen Viet Cuong, Nan Ye, Wee Sun Lee, and Hai Leong Chieu. 2014. Conditional random field with high-order dependencies for sequence labeling and segmentation. The Journal of Machine Learning Research, 15(1):981–1009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>1011--1019</pages>
<location>Los Angeles, California, USA.</location>
<contexts>
<context position="2687" citStr="Heilman and Smith, 2010" startWordPosition="417" endWordPosition="420">answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve high</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 1011–1019, Los Angeles, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongshuai Hou</author>
<author>Cong Tan</author>
<author>Xiaolong Wang</author>
<author>Yaoyun Zhang</author>
<author>Jun Xu</author>
<author>Qingcai Chen</author>
</authors>
<title>HITSZICRC: Exploiting classification approach for answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<pages>196--202</pages>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="15908" citStr="Hou et al., 2015" startWordPosition="2541" endWordPosition="2544">y, 2012, p. 613) for details. CRF also uses a Gaussian prior with variance 10.5 4.3 Experiment results In order to compare the quality of our features to the existing state of the art, we performed a first experiment aligned to the multi-class setting of the SemEval 2015 Task 3 competition. Table 1 shows our results on the official test dataset. 5Varying regularization strength (variance of the prior) did not make much difference. As in the competition, the results are macroaveraged at class level. The results of the top 3 systems are reported for comparison: JAIST (Tran et al., 2015), HITSZ (Hou et al., 2015) and QCRI (Nicosia et al., 2015), where the latter refers to our old system that we used for the competition. The two main observations are (i) using threadlevel features helps significantly; and (ii) the ordinal regression model, which captures the idea that potential lies between good and bad, achieves at least as good results as the top system at SemEval, namely JAIST. For the remaining experiments, we reduce the multi-class problem to a binary one (cf. Section 2). Table 2 shows the results obtained on the official test dataset. Note that ordinal regression is not applicable in this binary </context>
</contexts>
<marker>Hou, Tan, Wang, Zhang, Xu, Chen, 2015</marker>
<rawString>Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun Zhang, Jun Xu, and Qingcai Chen. 2015. HITSZICRC: Exploiting classification approach for answer selection in community question answering. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, pages 196–202, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Jaccard</author>
</authors>
<title>Etude comparative de la distribution florale dans une portion des Alpes et des Jura.</title>
<date>1901</date>
<booktitle>Bulletin del la Soci´et´e Vaudoise des Sciences Naturelles,</booktitle>
<pages>37--547</pages>
<contexts>
<context position="7405" citStr="Jaccard, 1901" startWordPosition="1181" endWordPosition="1182">arizes the basic features we used to implement the baseline systems. More importantly, Section 3.2 describes the set of threadlevel features we designed in order to test our working hypothesis. Below we use the following notation: q is a question posted by user uQ, c is a comment, and C is the comment thread. 3.1 Baseline Features We measure lexical and syntactic similarity between q and c. We compute the similarity between word n-grams (n = [1, ... , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also apply partial tree kernels (Moschitti, 2006) on shallow syntactic trees. We designed a set of heuristic features that might suggest whether c is good or not. Forty-four Boolean features express whether c (i) includes URLs or emails (2 feats.); (ii) contains the word “yes”, “sure”, “no”, “can”, “neither”, “okay”, and “sorry”, as well as symbols ‘?’ and ‘@’ (9 feats.); (iii) starts with “yes” (1 feat.); (iv) includes a sequence of three or more repeated characters or a word longer than fifteen characters (2 feats.); (v) belon</context>
</contexts>
<marker>Jaccard, 1901</marker>
<rawString>Paul Jaccard. 1901. ´Etude comparative de la distribution florale dans une portion des Alpes et des Jura. Bulletin del la Soci´et´e Vaudoise des Sciences Naturelles, 37:547–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM ’05,</booktitle>
<pages>84--90</pages>
<location>Bremen, Germany.</location>
<contexts>
<context position="2592" citStr="Jeon et al., 2005" startWordPosition="401" endWordPosition="404"> answer the initial question. This is a real problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are stro</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005. Finding similar questions in large question and answer archives. In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM ’05, pages 84–90, Bremen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large-scale Support Vector Machine Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods,</booktitle>
<pages>169--184</pages>
<editor>In Bernhard Sch¨olkopf, Christopher J. C. Burges, and Alexander J. Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="14239" citStr="Joachims, 1999" startWordPosition="2273" endWordPosition="2274"> 77.21±1.18 76.20±1.81 65.33±1.12 76.43±0.92 LogReg 71.15±0.96 84.44±1.50 77.22±1.07 75.43±1.47 66.57±1.49 75.05±0.70 CRFmap 71.27±1.20 83.15±1.81 76.75±1.28 75.14±1.72 65.36±1.45 75.61±0.63 CRFmpm 71.56±1.31 83.34±1.84 77.00±1.35 75.43±1.84 65.57±1.54 75.71±0.71 Table 3: Precision, Recall, F1, Accuracy computed at the comment level; F1,ta and Ata are averaged at the thread level. Precision, Recall, F1, F1,ta are computed with respect to the good classifier on 5-fold cross-validation (mean±stand. dev.). 4.2 Experimental Setup Our local classifiers are support vector machines (SVM) with C = 1 (Joachims, 1999), logistic regression with a Gaussian prior with variance 10, and logistic ordinal regression (McCullagh, 1980). In order to capture long-range sequential dependencies, we use a second-order SVMhmm (Yu and Joachims, 2008) (with C = 500 and epsilon = 0.01) and a second-order linear-chain CRF, which considers dependencies between three neighboring labels in a sequence (Lafferty et al., 2001; Cuong et al., 2014). In CRF, we perform two kinds of inference to find the most probable labels for the comments in a sequence. (i) We compute the maximum a posterior (MAP) or the (jointly) most probable seq</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making Large-scale Support Vector Machine Learning Practical. In Bernhard Sch¨olkopf, Christopher J. C. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods, pages 169–184. MIT Press, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<location>San Francisco, California, USA.</location>
<contexts>
<context position="14630" citStr="Lafferty et al., 2001" startWordPosition="2330" endWordPosition="2333">recision, Recall, F1, F1,ta are computed with respect to the good classifier on 5-fold cross-validation (mean±stand. dev.). 4.2 Experimental Setup Our local classifiers are support vector machines (SVM) with C = 1 (Joachims, 1999), logistic regression with a Gaussian prior with variance 10, and logistic ordinal regression (McCullagh, 1980). In order to capture long-range sequential dependencies, we use a second-order SVMhmm (Yu and Joachims, 2008) (with C = 500 and epsilon = 0.01) and a second-order linear-chain CRF, which considers dependencies between three neighboring labels in a sequence (Lafferty et al., 2001; Cuong et al., 2014). In CRF, we perform two kinds of inference to find the most probable labels for the comments in a sequence. (i) We compute the maximum a posterior (MAP) or the (jointly) most probable sequence of labels using the Viterbi algorithm. Specifically, it computes y∗ = argmaxy1:T P(y1:T|x1:T), where T is the number of comments in the thread. (ii) We use the forward–backward algorithm to find the labels by maximizing (individual) posterior marginals (MPM). More formally, we compute y� = (argmaxy1P(y1|x1:T), ··· , argmaxyTP(yT|x1:T)). While MAP yields a globally consistent sequenc</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Lyon</author>
<author>James Malcolm</author>
<author>Bob Dickerson</author>
</authors>
<title>Detecting short passages of similar text in large document collections.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, EMNLP ’01,</booktitle>
<pages>118--125</pages>
<location>Pittsburgh, Pennsylvania, USA.</location>
<contexts>
<context position="7443" citStr="Lyon et al., 2001" startWordPosition="1185" endWordPosition="1188">to implement the baseline systems. More importantly, Section 3.2 describes the set of threadlevel features we designed in order to test our working hypothesis. Below we use the following notation: q is a question posted by user uQ, c is a comment, and C is the comment thread. 3.1 Baseline Features We measure lexical and syntactic similarity between q and c. We compute the similarity between word n-grams (n = [1, ... , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also apply partial tree kernels (Moschitti, 2006) on shallow syntactic trees. We designed a set of heuristic features that might suggest whether c is good or not. Forty-four Boolean features express whether c (i) includes URLs or emails (2 feats.); (ii) contains the word “yes”, “sure”, “no”, “can”, “neither”, “okay”, and “sorry”, as well as symbols ‘?’ and ‘@’ (9 feats.); (iii) starts with “yes” (1 feat.); (iv) includes a sequence of three or more repeated characters or a word longer than fifteen characters (2 feats.); (v) belongs to one of the categories of the for</context>
</contexts>
<marker>Lyon, Malcolm, Dickerson, 2001</marker>
<rawString>Caroline Lyon, James Malcolm, and Bob Dickerson. 2001. Detecting short passages of similar text in large document collections. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, EMNLP ’01, pages 118–125, Pittsburgh, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter McCullagh</author>
</authors>
<title>Regression models for ordinal data.</title>
<date>1980</date>
<journal>J. Roy. Statist. Soc. B,</journal>
<pages>42--109</pages>
<contexts>
<context position="14350" citStr="McCullagh, 1980" startWordPosition="2288" endWordPosition="2289">05±0.70 CRFmap 71.27±1.20 83.15±1.81 76.75±1.28 75.14±1.72 65.36±1.45 75.61±0.63 CRFmpm 71.56±1.31 83.34±1.84 77.00±1.35 75.43±1.84 65.57±1.54 75.71±0.71 Table 3: Precision, Recall, F1, Accuracy computed at the comment level; F1,ta and Ata are averaged at the thread level. Precision, Recall, F1, F1,ta are computed with respect to the good classifier on 5-fold cross-validation (mean±stand. dev.). 4.2 Experimental Setup Our local classifiers are support vector machines (SVM) with C = 1 (Joachims, 1999), logistic regression with a Gaussian prior with variance 10, and logistic ordinal regression (McCullagh, 1980). In order to capture long-range sequential dependencies, we use a second-order SVMhmm (Yu and Joachims, 2008) (with C = 500 and epsilon = 0.01) and a second-order linear-chain CRF, which considers dependencies between three neighboring labels in a sequence (Lafferty et al., 2001; Cuong et al., 2014). In CRF, we perform two kinds of inference to find the most probable labels for the comments in a sequence. (i) We compute the maximum a posterior (MAP) or the (jointly) most probable sequence of labels using the Viterbi algorithm. Specifically, it computes y∗ = argmaxy1:T P(y1:T|x1:T), where T is</context>
</contexts>
<marker>McCullagh, 1980</marker>
<rawString>Peter McCullagh. 1980. Regression models for ordinal data. J. Roy. Statist. Soc. B, 42:109–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question answer classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07,</booktitle>
<pages>776--783</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2639" citStr="Moschitti et al., 2007" startWordPosition="409" endWordPosition="412">al problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer s</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question answer classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07, pages 776–783, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.</title>
<date>2006</date>
<booktitle>Machine Learning: ECML 2006,</booktitle>
<volume>4212</volume>
<pages>318--329</pages>
<editor>In Johannes F¨urnkranz, Tobias Scheffer, and Myra Spiliopoulou, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="7520" citStr="Moschitti, 2006" startWordPosition="1199" endWordPosition="1201">set of threadlevel features we designed in order to test our working hypothesis. Below we use the following notation: q is a question posted by user uQ, c is a comment, and C is the comment thread. 3.1 Baseline Features We measure lexical and syntactic similarity between q and c. We compute the similarity between word n-grams (n = [1, ... , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also apply partial tree kernels (Moschitti, 2006) on shallow syntactic trees. We designed a set of heuristic features that might suggest whether c is good or not. Forty-four Boolean features express whether c (i) includes URLs or emails (2 feats.); (ii) contains the word “yes”, “sure”, “no”, “can”, “neither”, “okay”, and “sorry”, as well as symbols ‘?’ and ‘@’ (9 feats.); (iii) starts with “yes” (1 feat.); (iv) includes a sequence of three or more repeated characters or a word longer than fifteen characters (2 feats.); (v) belongs to one of the categories of the forum (Socialising, Life in Qatar, etc.) (26 feats.); and (vi) has been posted b</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Johannes F¨urnkranz, Tobias Scheffer, and Myra Spiliopoulou, editors, Machine Learning: ECML 2006, volume 4212 of Lecture Notes in Computer Science, pages 318–329. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Murphy</author>
</authors>
<title>Machine Learning A Probabilistic Perspective.</title>
<date>2012</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="15298" citStr="Murphy, 2012" startWordPosition="2439" endWordPosition="2440">nference to find the most probable labels for the comments in a sequence. (i) We compute the maximum a posterior (MAP) or the (jointly) most probable sequence of labels using the Viterbi algorithm. Specifically, it computes y∗ = argmaxy1:T P(y1:T|x1:T), where T is the number of comments in the thread. (ii) We use the forward–backward algorithm to find the labels by maximizing (individual) posterior marginals (MPM). More formally, we compute y� = (argmaxy1P(y1|x1:T), ··· , argmaxyTP(yT|x1:T)). While MAP yields a globally consistent sequence of labels, MPM can be more robust in many cases; see (Murphy, 2012, p. 613) for details. CRF also uses a Gaussian prior with variance 10.5 4.3 Experiment results In order to compare the quality of our features to the existing state of the art, we performed a first experiment aligned to the multi-class setting of the SemEval 2015 Task 3 competition. Table 1 shows our results on the official test dataset. 5Varying regularization strength (variance of the prior) did not make much difference. As in the competition, the results are macroaveraged at class level. The results of the top 3 systems are reported for comparison: JAIST (Tran et al., 2015), HITSZ (Hou et </context>
</contexts>
<marker>Murphy, 2012</marker>
<rawString>Kevin Murphy. 2012. Machine Learning A Probabilistic Perspective. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Llu´ıs M`arquez</author>
<author>Walid Magdy</author>
<author>Alessandro Moschitti</author>
<author>Jim Glass</author>
<author>Bilal Randeree</author>
</authors>
<title>SemEval-2015 task 3: Answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<pages>269--281</pages>
<location>Denver, Colorado, USA.</location>
<marker>Nakov, M`arquez, Magdy, Moschitti, Glass, Randeree, 2015</marker>
<rawString>Preslav Nakov, Llu´ıs M`arquez, Walid Magdy, Alessandro Moschitti, Jim Glass, and Bilal Randeree. 2015. SemEval-2015 task 3: Answer selection in community question answering. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, pages 269–281, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Massimo Nicosia</author>
<author>Simone Filice</author>
<author>Alberto Barr´onCede˜no</author>
<author>Iman Saleh</author>
<author>Hamdy Mubarak</author>
<author>Wei Gao</author>
<author>Preslav Nakov</author>
<author>Giovanni Da San Martino</author>
<author>Alessandro Moschitti</author>
<author>Kareem Darwish</author>
<author>Llu´ıs M`arquez</author>
<author>Shafiq Joty</author>
<author>Walid Magdy</author>
</authors>
<title>QCRI: Answer selection for community question answering -experiments for Arabic and English.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<pages>203--209</pages>
<location>Denver, Colorado, USA.</location>
<marker>Nicosia, Filice, Barr´onCede˜no, Saleh, Mubarak, Gao, Nakov, Martino, Moschitti, Darwish, M`arquez, Joty, Magdy, 2015</marker>
<rawString>Massimo Nicosia, Simone Filice, Alberto Barr´onCede˜no, Iman Saleh, Hamdy Mubarak, Wei Gao, Preslav Nakov, Giovanni Da San Martino, Alessandro Moschitti, Kareem Darwish, Llu´ıs M`arquez, Shafiq Joty, and Walid Magdy. 2015. QCRI: Answer selection for community question answering -experiments for Arabic and English. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, pages 203–209, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Radlinski</author>
<author>Thorsten Joachims</author>
</authors>
<title>Query chains: Learning to rank from implicit feedback.</title>
<date>2005</date>
<booktitle>In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, KDD ’05,</booktitle>
<pages>239--248</pages>
<location>Chicago, Illinois, USA.</location>
<contexts>
<context position="2573" citStr="Radlinski and Joachims, 2005" startWordPosition="397" endWordPosition="400">ch other, instead of trying to answer the initial question. This is a real problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a com</context>
</contexts>
<marker>Radlinski, Joachims, 2005</marker>
<rawString>Filip Radlinski and Thorsten Joachims. 2005. Query chains: Learning to rank from implicit feedback. In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, KDD ’05, pages 239–248, Chicago, Illinois, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structural relationships for large-scale learning of answer re-ranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’12,</booktitle>
<pages>741--750</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="2740" citStr="Severyn and Moschitti, 2012" startWordPosition="425" endWordPosition="428">isfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve higher accuracy. To test our hypothesis about the usefuln</context>
</contexts>
<marker>Severyn, Moschitti, 2012</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2012. Structural relationships for large-scale learning of answer re-ranking. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’12, pages 741–750, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic feature engineering for answer selection and extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP ’13,</booktitle>
<pages>458--467</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="2810" citStr="Severyn and Moschitti, 2013" startWordPosition="437" endWordPosition="440">ion in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve higher accuracy. To test our hypothesis about the usefulness of thread-level information, we used a publicly available dataset,</context>
</contexts>
<marker>Severyn, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2013. Automatic feature engineering for answer selection and extraction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP ’13, pages 458–467, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Learning adaptable patterns for passage reranking.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, CoNLL ’13,</booktitle>
<pages>75--83</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="2780" citStr="Severyn et al., 2013" startWordPosition="433" endWordPosition="436">g the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve higher accuracy. To test our hypothesis about the usefulness of thread-level information, we used</context>
</contexts>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013. Learning adaptable patterns for passage reranking. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, CoNLL ’13, pages 75–83, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Mirella Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’07,</booktitle>
<pages>12--21</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2615" citStr="Shen and Lapata, 2007" startWordPosition="405" endWordPosition="408"> question. This is a real problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>Dan Shen and Mirella Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’07, pages 12–21, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Learning to rank answers on large online QA collections.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics and the Human Language Technology Conference, ACL-HLT ’08,</booktitle>
<pages>719--727</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="2662" citStr="Surdeanu et al., 2008" startWordPosition="413" endWordPosition="416">n can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should b</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2008</marker>
<rawString>Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2008. Learning to rank answers on large online QA collections. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics and the Human Language Technology Conference, ACL-HLT ’08, pages 719–727, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quan Hung Tran</author>
<author>Vu Tran</author>
<author>Tu Vu</author>
<author>Minh Nguyen</author>
<author>Son Bao Pham</author>
</authors>
<title>JAIST: Combining multiple features for answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<pages>215--219</pages>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="15882" citStr="Tran et al., 2015" startWordPosition="2536" endWordPosition="2539">t in many cases; see (Murphy, 2012, p. 613) for details. CRF also uses a Gaussian prior with variance 10.5 4.3 Experiment results In order to compare the quality of our features to the existing state of the art, we performed a first experiment aligned to the multi-class setting of the SemEval 2015 Task 3 competition. Table 1 shows our results on the official test dataset. 5Varying regularization strength (variance of the prior) did not make much difference. As in the competition, the results are macroaveraged at class level. The results of the top 3 systems are reported for comparison: JAIST (Tran et al., 2015), HITSZ (Hou et al., 2015) and QCRI (Nicosia et al., 2015), where the latter refers to our old system that we used for the competition. The two main observations are (i) using threadlevel features helps significantly; and (ii) the ordinal regression model, which captures the idea that potential lies between good and bad, achieves at least as good results as the top system at SemEval, namely JAIST. For the remaining experiments, we reduce the multi-class problem to a binary one (cf. Section 2). Table 2 shows the results obtained on the official test dataset. Note that ordinal regression is not </context>
</contexts>
<marker>Tran, Tran, Vu, Nguyen, Pham, 2015</marker>
<rawString>Quan Hung Tran, Vu Tran, Tu Vu, Minh Nguyen, and Son Bao Pham. 2015. JAIST: Combining multiple features for answer selection in community question answering. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, pages 215–219, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Probabilistic tree-edit models with structured latent variables for textual entailment and question answering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>1164--1172</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2711" citStr="Wang and Manning, 2010" startWordPosition="421" endWordPosition="424">y of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve higher accuracy. To test our</context>
</contexts>
<marker>Wang, Manning, 2010</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. 2010. Probabilistic tree-edit models with structured latent variables for textual entailment and question answering. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 1164–1172, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wise</author>
</authors>
<title>Yap3: Improved detection of similarities in computer program and other texts.</title>
<date>1996</date>
<booktitle>In Proceedings of the Twenty-seventh SIGCSE Technical Symposium on Computer Science Education, SIGCSE ’96,</booktitle>
<pages>130--134</pages>
<location>New York, New York, USA.</location>
<contexts>
<context position="7315" citStr="Wise, 1996" startWordPosition="1170" endWordPosition="1171">QA application (see Section 4.3). 3 Basic and Thread-Level Features Subsection 3.1 summarizes the basic features we used to implement the baseline systems. More importantly, Section 3.2 describes the set of threadlevel features we designed in order to test our working hypothesis. Below we use the following notation: q is a question posted by user uQ, c is a comment, and C is the comment thread. 3.1 Baseline Features We measure lexical and syntactic similarity between q and c. We compute the similarity between word n-grams (n = [1, ... , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also apply partial tree kernels (Moschitti, 2006) on shallow syntactic trees. We designed a set of heuristic features that might suggest whether c is good or not. Forty-four Boolean features express whether c (i) includes URLs or emails (2 feats.); (ii) contains the word “yes”, “sure”, “no”, “can”, “neither”, “okay”, and “sorry”, as well as symbols ‘?’ and ‘@’ (9 feats.); (iii) starts with “yes” (1 feat.); (iv) includes a sequence of three </context>
</contexts>
<marker>Wise, 1996</marker>
<rawString>Michael Wise. 1996. Yap3: Improved detection of similarities in computer program and other texts. In Proceedings of the Twenty-seventh SIGCSE Technical Symposium on Computer Science Education, SIGCSE ’96, pages 130–134, New York, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
<author>Chris CallisonBurch</author>
<author>Peter Clark</author>
</authors>
<title>Answer extraction as sequence tagging with tree edit distance.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’13,</booktitle>
<pages>858--867</pages>
<marker>Yao, Van Durme, CallisonBurch, Clark, 2013</marker>
<rawString>Xuchen Yao, Benjamin Van Durme, Chris CallisonBurch, and Peter Clark. 2013. Answer extraction as sequence tagging with tree edit distance. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’13, pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam Yu</author>
<author>T Joachims</author>
</authors>
<title>Training structural SVMs with kernels using sampled cuts.</title>
<date>2008</date>
<booktitle>In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD ’08,</booktitle>
<pages>794--802</pages>
<contexts>
<context position="14460" citStr="Yu and Joachims, 2008" startWordPosition="2303" endWordPosition="2306">±1.84 77.00±1.35 75.43±1.84 65.57±1.54 75.71±0.71 Table 3: Precision, Recall, F1, Accuracy computed at the comment level; F1,ta and Ata are averaged at the thread level. Precision, Recall, F1, F1,ta are computed with respect to the good classifier on 5-fold cross-validation (mean±stand. dev.). 4.2 Experimental Setup Our local classifiers are support vector machines (SVM) with C = 1 (Joachims, 1999), logistic regression with a Gaussian prior with variance 10, and logistic ordinal regression (McCullagh, 1980). In order to capture long-range sequential dependencies, we use a second-order SVMhmm (Yu and Joachims, 2008) (with C = 500 and epsilon = 0.01) and a second-order linear-chain CRF, which considers dependencies between three neighboring labels in a sequence (Lafferty et al., 2001; Cuong et al., 2014). In CRF, we perform two kinds of inference to find the most probable labels for the comments in a sequence. (i) We compute the maximum a posterior (MAP) or the (jointly) most probable sequence of labels using the Viterbi algorithm. Specifically, it computes y∗ = argmaxy1:T P(y1:T|x1:T), where T is the number of comments in the thread. (ii) We use the forward–backward algorithm to find the labels by maximi</context>
</contexts>
<marker>Yu, Joachims, 2008</marker>
<rawString>Chun-Nam Yu and T. Joachims. 2008. Training structural SVMs with kernels using sampled cuts. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD ’08, pages 794–802.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>