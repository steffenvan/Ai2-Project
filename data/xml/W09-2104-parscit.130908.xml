<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000231">
<title confidence="0.990425">
A New Yardstick and Tool for Personalized Vocabulary Building
</title>
<author confidence="0.705155">
Thomas K Landauer Kirill Kireyev Charles Panaccione
</author>
<affiliation confidence="0.407408">
Pearson Education,
Knowledge Technologies
</affiliation>
<email confidence="0.986625">
{tom.landauer,kirill.kireyev,charles.panaccione}@pearson.com
</email>
<sectionHeader confidence="0.993612" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999853">
The goal of this research is to increase the
value of each individual student&apos;s vocabulary
by finding words that the student doesn’t
know, needs to, and is ready to learn. To help
identify such words, a better model of how
well any given word is expected to be known
was created. This is accomplished by using a
semantic language model, LSA, to track how
every word changes with the addition of more
and more text from an appropriate corpus. We
define the “maturity” of a word as the degree
to which it has become similar to that after
training on the entire corpus.
An individual student’s average vocabu-
lary level can then be placed on the word-
maturity scale by an adaptive test. Finally, the
words that the student did or did not know on
the test can be used to predict what other
words the same student knows by using mul-
tiple maturity models trained on random sam-
ples of typical educational readings. This
detailed information can be used to generate
highly customized vocabulary teaching and
testing exercises, such as Cloze tests.
</bodyText>
<sectionHeader confidence="0.99975" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.999145">
1.1 Why “Vocabulary First”
</subsectionHeader>
<bodyText confidence="0.984597333333333">
There are many arguments for the importance
of more effective teaching of vocabulary. Here are
some examples:
</bodyText>
<listItem confidence="0.558689714285714">
(1) Baker, Simmons, &amp; Kame&apos;enui (1997)
found that children who enter school with limited
vocabulary knowledge grow much more discrepant
over time from their peers who have rich vocabu-
lary knowledge.
(2.) Anderson &amp; Freebody (1981) found that
the number of words in student’s meaning vocabu-
</listItem>
<page confidence="0.975953">
27
</page>
<bodyText confidence="0.9952465">
laries was the best predictor of how well they
comprehend text.
</bodyText>
<listItem confidence="0.908720444444445">
(3) An unpublished 1966 study of the correla-
tion between entering scores of Stanford Students
on the SAT found the vocabulary component to be
the best predictor of grades in every subject, in-
cluding science.
(4) The number of words students learn varies
greatly, from 0.2 to 8 words per day and from 50 to
over 3,000 per year. (Anderson &amp; Freebody,1981)
(5) Printed materials in grades 3 to 9 on average
contain almost 90,000, distinct word families and
nearly 500,000 word forms (including proper
names.) (Nagy &amp; Anderson, 1984).
(6) Nagy and Anderson (1984) found that on
average not knowing more than one word in a sen-
tence prevented its tested understanding, and that
the probability of learning the meaning of a new
word by one encounter on average was less than
one in ten.
(7) John B. Carroll’s (1993) meta-analysis of
factor analyses of measured cognitive ability found
the best predictor to be tests of vocabulary.
(8) Hart and Risley’s large randomized obser-
vational study of the language used in households
with young children found that the number of
words spoken within hearing of a child was associ-
ated with a three-fold difference in vocabulary by
school entry.
</listItem>
<subsectionHeader confidence="0.964106">
1.2 The Challenge
</subsectionHeader>
<bodyText confidence="0.99962125">
Several published sources and inspection of the
number of words taught in recent literacy text-
books and online tools suggest that less than 400
words per year are directly tutored in American
schools. Thus, the vast majority of vocabulary
must be acquired from language exposure, espe-
cially from print because the oral vocabulary of
daily living is usually estimated to be about 20,000
</bodyText>
<note confidence="0.670161">
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 27–33,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998035">
words, of which most are known by early school
years. But it would obviously be of great value to
find a way to make the explicit teaching of vocabu-
lary more effective, and to make it multiply the
effects of reading. These are the goals of the new
methodologies reported here.
It is also clear that words are not learned in iso-
lation: learning the meaning of a new word re-
quires prior knowledge of many other words, and
by most estimates it takes a (widely variable) aver-
age of ten encounters in different and separated
contexts. (This, by the way, is what is required to
match human adult competence in the computa-
tional language model used here. Given a text cor-
pus highly similar to that experienced by a
language learner, the model learns at very close to
the same rate as an average child, and it learns new
words as much as four times faster the more old
words it knows (Landauer &amp; Dumais, 1997).)
An important aside here concerns a widely cir-
culated inference from the Nagy and Anderson
(1984) result that teaching words by presenting
them in context doesn’t produce enough vocabu-
lary growth to be the answer. The problem is that
the experiments actually show only that the in-
serted target word itself is usually not learned well
enough to pass a test. But in the simulations, words
are learned a little at a time; exposure to a sentence
increases the knowledge of many other words, both
ones in the sentence and not. Every encounter with
any word in context percolates meaning through
the whole current and future vocabulary. Indeed, in
the simulator, indirect learning is three to five
times as much as direct, and is what accounts for
its ability to match human vocabulary growth and
passage similarity. Put differently, the helpful thing
that happens on encountering an unknown word is
not guessing its meaning but its contribution to
underlying understanding of language.
However, a vicious negative feedback loop
lurks in this process. Learning from reading re-
quires vocabulary knowledge. So the vocabulary-
rich get richer and the vocabulary-poor get rela-
tively poorer. Fortunately, however, in absolute
terms there is a positive feedback loop: the more
words you know, the faster you can learn new
ones, generating exponential positive growth. Thus
the problem and solution may boil down to in-
creasing the growth parameter for a given student
enough to make natural reading do its magic better.
Nonetheless, importantly, it is patently obvious
that it matters greatly what words are taught how,
when and to which students.
The hypothesis, then, is that a set of tools that
could determine what particular words an individ-
ual student knows and doesn’t, and which ones
learned (and sentences understood) would most
help other words to be learned by that student
might have a large multiplying effect. It is such a
toolbox that we are endeavoring to create by using
a computational language model with demon-
strated ability to simulate human vocabulary
growth to a reasonably close approximation. The
principal foci are better selection and “personaliza-
tion” of what is taught and teaching more quickly
and with more permanence by application of opti-
mal spacing of tests and practice—into which we
will not go here.
</bodyText>
<subsectionHeader confidence="0.996116">
1.3 Measuring vocabulary knowledge
</subsectionHeader>
<bodyText confidence="0.984959">
Currently there are three main methods for
measuring learner vocabulary, all of which are in-
adequate for the goal. They are:
</bodyText>
<listItem confidence="0.965529117647059">
1. Corpus Frequency. Collect a large sample
of words used in the domain of interest, for exam-
ple a collection of textbooks and readers used in
classrooms, text from popular newspapers, a large
dictionary or the Internet. Rank the words by fre-
quency of occurrence. Test students on a random
subset of, say, the 1,000, 2,000 and 5,000 most
frequent words, compute the proportion known at
each “level” and interpolate and extrapolate. This
is a reasonable method, because frequently en-
countered words are the ones most frequently
needed to be understood.
2. Educational Materials. Sample vocabulary
lessons and readings over classrooms at different
school grades.
3. Expert Judgments. Obtain informed expert
opinions about what words are important to know
</listItem>
<bodyText confidence="0.971870181818182">
by what age for what purposes.
Some estimates combine two or more of these
approaches, and they vary in psychometric sophis-
tication. For example, one of the most sophisti-
cated, the Lexile Framework, uses Rasch scaling
(Rasch, 1980) of a large sample of student vocabu-
lary test scores (probability right on a test, holding
student ability constant) to create a difficulty
measure for sentences and then infers the difficulty
of words, in essence, from the average difficulty of
the sentences in which they appear.
</bodyText>
<page confidence="0.991348">
28
</page>
<bodyText confidence="0.986575035714286">
The problem addressed in the present project
goal is that all of these methods measure only the
proportion of tested words known at one or more
frequency ranges, in chosen school grades or for
particular subsets of vocabulary (e.g. “academic”
words), and for a very small subset—those tested -
some of the words that the majority of a class
knows. What they don’t measure is exactly which
words in the whole corpus a given student knows
and to what extent, or which words would be most
important for that student to learn.
A lovely analog of the problem comes from
Ernst Rothkopf’s (1970) metaphor that everyone
passes through highly different “word swarms”
each day on their way to their (still highly differen-
tiated) adult literacy.
2 A new metric: Word Maturity
The new metric first applies Latent Semantic
Analysis (LSA) to model how representation of
individual words changes and grows toward their
adult meaning as more and more language is en-
countered. Once the simulation has been created,
an adaptive testing method can be applied to place
individual words on separate growth curves - char-
acteristic functions in psychometric terminology.
Finally, correlations between growth curves at
given levels can be used to estimate the achieved
growth of other words.
</bodyText>
<subsectionHeader confidence="0.992132">
2.1 How it works in more detail: LSA.
</subsectionHeader>
<bodyText confidence="0.983381835820896">
A short review of how LSA works will be use-
ful here because it is often misunderstood and a
correct interpretation is important in what follows.
LSA models how words combine into meaningful
passages, the aspect of verbal meaning we take to
be most critical to the role of words in literacy. It
does this by assuming that the “meaning” (please
bear with the nickname) of a meaningful passage is
the sum of the meanings of its words:
Meaning of passage =
{meaning of first wd} +
{meaning of second word} + ... +
{meaning of last word}
A very large and representative corpus of the
language to be modeled is first collected and repre-
sented as a term-by-document matrix. A powerful
matrix algebra method called Singular Value De-
composition is then used to make every paragraph
in the corpus conform to the above objective func-
tion—word representations sum to passage repre-
sentations - up to a best least-squares
approximation. A dimensionality-reduction step is
performed, resulting in each word and passage
meanings represented as a (typically) 300 element
real number vector. Note that the property of a vec-
tor standing for a word form in this representation
is the effect that it has on the vector standing for
the passage. (In particular, it is only indirectly a
reflection of how similar two words are to each
other or how frequently they have occurred in the
same passages.) In the result, the vector for a word
is the average of the vectors for all the passages in
which it occurs, and the vector for a passage is, of
course, the average all of its words.
In many previous applications to education, in-
cluding automatic scoring of essays, the model’s
similarity to human judgments (e.g. by mutual in-
formation measures) has been found to be 80 to
90% as high as that between two expert humans,
and, as mentioned earlier, the rate at which it
learns the meaning of words as assessed by various
standardized and textbook-based tests has been
found to closely match that of students. For more
details, evaluations and previous educational appli-
cations, see (Landauer et al., 2007).
2.2 How it works in more detail: Word Ma-
turity.
Taking LSA to be a sufficiently good approxi-
mation of human learning of the meanings con-
veyed by printed word forms, we can use it to track
their gradual acquisition as a function of increasing
exposure to text representative in size and content
of that which students at successive grade levels
read.
Thus, to model the growth of meaning of indi-
vidual words, a series of sequentially accumulated
LSA “semantic spaces” (the collection of vectors
for all of the words and passages) are created. Cu-
mulative portions of the corpus thus emulate the
growing total amount of text that has been read by
a student. At each step, a new LSA semantic space
is created from a cumulatively larger subset of the
full adult corpus.
Several different ways of choosing the succes-
sive sets of passages to be added to the training set
have been tried, ranging from ones based on read-
ability metrics (such as Lexiles or DRPs) to en-
</bodyText>
<page confidence="0.993543">
29
</page>
<bodyText confidence="0.982532038461538">
tirely randomly selected subsets. Here, the steps
are based on Lexiles to emulate their order of en-
counter in typical school reading.
This process results in a separate LSA model of
word meanings corresponding to each stage of lan-
guage learning. To determine how well a word or
passage is known at a given stage of learning—a
given number or proportion of passages from the
corpus—its vector in the LSA model correspond-
ing to a particular stage is compared with the vec-
tor of the full adult model (one that has been
trained on a corpus corresponding to a typical
adult’s amount of language exposure). This is done
using a linear transformation technique known as
Procrustes Alignment to align the two spaces—
those after a given step to those based on the full
corpus, which we call its “adult” meaning.
Word maturity is defined as the similarity of a
word’s vector at a given stage of training and that
at its adult stage as measured by cosine. It is scaled
as values ranging between 0 (least mature) and 1
(most mature).
Figure 1 shows growth curves for an illustrative
set of words. In this example, 17 successive cumu-
lative steps were created, each containing ~5000
additional passages.
</bodyText>
<figureCaption confidence="0.8340305">
Figure 1. An illustration of meaning maturity growth of sev-
eral words as a function of language exposure.
</figureCaption>
<bodyText confidence="0.99998993442623">
Some words (e.g. “dog”) are almost at their
adult meaning very early. Others hardly get started
until later. Some grow quickly, some slowly. Some
grow smoothly, some in spurts. Some, like “tur-
key,” grow rapidly, plateau, then resume growing
again, presumably due to multiple senses
(“Thanksgiving bird” vs. “country”) learned at dif-
ferent periods (in LSA, multiple “senses” are com-
bined in a word representation approximately in
proportion to their frequency.)
The maturity metric has several conceptual ad-
vantages over existing measures of the status of
a word’s meaning, and in particular should be kept
conceptually distinct from the ambiguous and often
poorly defined term “difficulty” and from whether
or not students in general or at some developmen-
tal stage can properly use, define or understand its
meaning. It is a mathematical property of a word
that may or may not be related to what particular
people can do with it.
What it does is provide a detailed view of the
course of development of a word’s changing repre-
sentation—its “meaning”, reciprocally defined as
its effect on the “meaning” of passages in which it
occurs,—as a function of the amount and nature of
the attestedly meaningful passages in which it has
been encountered. Its relation to “difficulty” as
commonly used would depend, among other
things, on whether a human could use it for some
purpose at some stage of development of the word.
Thus, its relation to a student’s use of a word re-
quires a second step of aligning the student’s word
knowledge with the metric scaling. This is analo-
gous to describing a runner’s “performance” by
aligning it with well-defined metrics for time and
distance.
It is nevertheless worth noting that the word
maturity metric is not based directly on corpus fre-
quency as some other measures of word status are
(although its average level over all maturities is
moderately highly correlated with total corpus fre-
quency as it should be) or on other heuristics, such
as grade of first use or expert opinions of suitabil-
ity.
What is especially apparent in the graph above
is that after a given amount of language exposure,
analogous to age or school grade, there are large
differences in the maturity of different words. In
fact the correlation between frequency of occur-
rence in a particular one of the 17 intermediate cor-
pora and word maturity is only 0.1, measured over
20,000 random words. According to the model--
and surely common sense--words of the same fre-
quency of encounter (or occurrence in a corpus)
are far from equally well known. Thus, all methods
for “leveling” text and vocabulary instruction
based on word frequency must hide a great range
of differences.
To illustrate this in more detail, Table 1, shows
computed word maturities for a set of words that
have nearly the same frequency in the full corpus
</bodyText>
<figure confidence="0.9950346">
Similarity
0.8
0.6
0.4
0.2
0.0
1.0
1 3 5 7 9 11 13 15 17
Word Meaning Maturity
Model Level
dog
electoral
primate
productivity
turkey
</figure>
<page confidence="0.990594">
30
</page>
<bodyText confidence="0.9978005">
(column four) when they have been added only
505 times (column two). The differences are so
large as to suggest the choice of words to teach
students in a given school grade would profit much
from being based on something more discrimina-
tive than either average word frequency or word
frequency as found in the texts being read or in the
small sample that can be humanly judged. Even
better, it would appear, should be to base what is
taught to a given student on what that student does
and doesn’t know but needs to locally and would
most profit from generally.
</bodyText>
<table confidence="0.999725545454545">
Word Occurrences Occurrences Word
in intermedi- in adult maturity
ate corpus corpus (at level
(level 5) 5)
marble 54 485 0.21
sunshine 49 508 0.31
drugs 53 532 0.42
carpet 48 539 0.59
twin 48 458 0.61
earn 53 489 0.70
beam 47 452 0.76
</table>
<tableCaption confidence="0.958709">
Table 1 A sample of words with roughly the same number of
occurrences in both intermediate (~50) and adult (~500) cor-
pus
</tableCaption>
<bodyText confidence="0.999940571428572">
The word maturity metric appears to perform
well when validated by some external methods.
For example, it reliably discriminates between
words that were assigned to be taught in different
school grades by (Biemiller, 2008), based on a
combination of expert judgments and comprehen-
sion tests (p &lt; 0.03), as shown in Table 2.
</bodyText>
<table confidence="0.9817062">
grade 2, grade 2, grade 6, grade 6,
known known by known by known
by &gt; 80% 40-80% 40-80% by &lt; 40%
n=1034 n=606 n=1125 n=1411
4.4 6.5 8.8 9.5
</table>
<tableCaption confidence="0.965458666666667">
Table 2 Average level for each word to reach a 0.5 maturity
threshold, for words that are known at different levels by stu-
dents of different grades (Biemiller, 2008).
</tableCaption>
<bodyText confidence="0.955498909090909">
Median word maturity also tracks the differ-
ences (p &lt; 0.01) between essays written by stu-
dents in different grades as shown in Figure 2.
Figure 2 Percentage of “adult” words used in essays written
by students of different grade levels. “Adult” words are de-
fined as words that reach a 0.5 word maturity threshold at or
later than the point where half of the words in the language
have reached 0.5 threshold.
2.3 Finding words to teach individual stu-
dents
Using the computed word maturity values, a
sigmoid characteristic curve is generated to ap-
proximate the growth curve of every word in the
corpus. A model similar to one used in item re-
sponse theory (Rasch, 1980) can be constructed
from the growth curve due to its similarity in shape
and function to an IRT characteristic curve; both
curves represent the ability of a student. The char-
acteristic curve for the IRT is needed to properly
administer adaptive testing, which greatly in-
creases the precision and generalizeability of the
exam. Words to be tested are chosen from the cor-
pus beginning at the average maturity of words at
the approximate grade level of the student. Thirty
to fifty word tests are used to home in on the stu-
dent’s average word maturity level. In initial trials,
a combination of yes/no and Cloze tests are being
used. Because our model does not treat all words
of a given frequency as equivalent, this alone sup-
ports a more precise and personalized measure of a
student’s vocabulary. In plan, the student level will
be updated by the results of additional tests admin-
istered in school or by Internet delivery.
The final step is to generalize from the assessed
knowledge of words a particular student (let’s call
her Alice) is tested on to other words in the corpus.
This is accomplished by first generating a large
number of simulated students (and their word ma-
turity curves) using the method described above.
Each simulated student is trained on one of many ~
12 million word corpora, size and content ap-
proximating the lifelong reading of a typical col-
lege student, that have been randomly sampled
from a representative corpus of more than half a
</bodyText>
<figure confidence="0.992955444444444">
4%
2%
5%
3%
0%
1%
4 6 8 10 12
Student grade level
Percent of &amp;quot;adult&amp;quot; words in essay
</figure>
<page confidence="0.999871">
31
</page>
<bodyText confidence="0.999975785714286">
billion words. Some of these simulated students’
knowledge of the words being tested will be more
similar to Alice than others. We can then estimate
Alice’s knowledge of any other word w in the cor-
pus by averaging the levels of knowledge of w by
simulated students whose patterns of tested word
knowledge are most similar hers. The method rests
on the assumption that there are sufficiently strong
correlations between the words that a given student
has learned at a given stage (e.g. resulting from
Rothkopf’s personal “swarms’.) While simulations
are promising, empirical evidence as to the power
of the approach with non-simulated students is yet
to be determined.
</bodyText>
<sectionHeader confidence="0.99766" genericHeader="method">
3 Applying the method
</sectionHeader>
<bodyText confidence="0.999891807692308">
On the assumption that learning words by their
effects on passage meanings as LSA does is good,
initial applications use Cloze items to simultane-
ously test and teach word meanings by presenting
them in a natural linguistic context. Using the
simulator, the context words in an item are pre-
dicted to be ones that the individual student already
knows at a chosen level. The target words, where
the wider pedagogy permits, are ones that are re-
lated and important to the meaning of the sentence
or passage, as measured by LSA cosine similarity
metric, and, ipso facto, the context tends to contex-
tually teach their meaning. They can also be cho-
sen to be those that are computationally estimated
to be the most important for a student to know in
order to comprehend assigned or student-chosen
readings—because their lack has the most effect on
passage meanings—and/or in the language in gen-
eral. Using a set of natural language processing
algorithms (such as n-gram models, POS-tagging,
WordNet relations and LSA) the distracter items
for each Cloze are chosen in such a way that they
are appropriate grammatically, but not semanti-
cally, as illustrated in the example below.
In summary, Cloze-test generation involves the
following steps:
</bodyText>
<listItem confidence="0.673794142857143">
1. Determine the student’s overall knowledge
level and individual word knowledge predictions
based on previous interactions.
2. Find important words in a reading that are
appropriate for a particular student (using metrics
that include word maturity).
3. For each word, find a sentence in a large
</listItem>
<bodyText confidence="0.971757842105263">
collection of natural text, such that the rest of the
sentence semantically implies (is related to) the
target word and is appropriate for student’s knowl-
edge level.
4.Find distracter words that are (a) level-
appropriate, (b) are sufficiently related and (c) fit
grammatically, but (d) not semantically, into the
sentence.
All the living and nonliving things around an ___
is its environment.
A. organism B. oxygen C. algae
Freshwater habitats can be classified according to
the characteristic species of fish found in them,
indicating the strong ecological relationship be-
tween an ___ and its environment.
A. adaptation B. energy C. organism
Table 3 Examples of auto-generated Cloze tests for the same
word (organism) and two students of lower and higher ability,
respectively.
</bodyText>
<sectionHeader confidence="0.991889" genericHeader="method">
4 Summary and present status
</sectionHeader>
<bodyText confidence="0.999983111111111">
A method based on computational model-
ing of language, in particular one that makes the
representation of the meaning of a word its effect
on the meaning of a passage its objective, LSA,
has been developed and used to simulate the
growth of meaning of individual word representa-
tions towards those of literate adults. Based
thereon, a new metric for word meaning growth
called “Word Maturity” is proposed. The measure
is then applied to adaptively measuring the average
level of an individual student’s vocabulary, pre-
sumably with greater breadth and precision than
offered by other methods, especially those based
on knowledge of words at different corpus fre-
quency. There are many other things the metric
may support, for example better personalized
measurement of text comprehensibility.
However, it must be emphasized that the
method is very new and essentially untried except
in simulation. And it is worth noting that while the
proposed method is based on LSA, many or all of
its functionalities could be obtained with some
other computational language models, for example
the Topics model. Comparisons with other meth-
ods will be of interest, and more and more rigorous
evaluations are needed, as are trials with more
various applications to assure robustness.
</bodyText>
<page confidence="0.997227">
32
</page>
<bodyText confidence="0.973971">
and afterword by B.D. Wright. Chicago: The
</bodyText>
<sectionHeader confidence="0.757583" genericHeader="method">
5 References University of Chicago Press.
</sectionHeader>
<reference confidence="0.996866808510638">
Richard C. Anderson, Peter Freebody. 1981. Vo-
cabulary Knowledge. In J. T. Guthrie (Ed.),
Comprehension and teaching: Research reviews
(pp. 77-117). International Reading Association,
Newark DE.
Scott K. Baker, Deborah C. Simmons, Edward J.
Kameenui. 1997. Vocabulary acquisition: Re-
search bases. In Simmons, D. C. &amp; Kameenui,
E. J. (Eds.), What reading research tells us
about children with diverse learning needs:
Bases and basics. Erlbaum, Mahwah, NJ.
Andrew Biemiller (2008). Words Worth Teaching.
Co-lumbus, OH: SRA/McGraw-Hill.
John B Carroll. 1993. Cognitive Abilities: A survey
of factor-analytic studies. Cambridge: Cam-
bridge University Press, 1993.
Betty Hart, Todd R. Risley. 1995. Meaningful dif-
ferences in the everyday experience of young
American children. Brookes Publishing, 1995.
Melanie R. Kuhn, Steven A. Stahl. 1998. Teaching
children to learn word meanings from context:
A synthesis and some questions. Journal of Lit-
eracy Research, 30(1) 119-138.
Thomas K Landauer, Susan Dumais. 1997. A solu-
tion to Plato&apos;s problem: The Latent Semantic
Analysis theory of the Acquisition, Induction,
and Representation of Knowledge. Psychologi-
cal Review, 104, pp 211-240.
Thomas K Landauer, Danielle S. McNamara,
Simon Dennis, and Walter Kintsch. 2007. Hand-
book of Latent Semantic Analysis. Lawrence
Erlbaum.
Cleborne D. Maddux (1999). Peabody Picture Vo-
cabulary Test III (PPVT-III). Diagnostique, v24
n1-4, p221-28, 1998-1999
William E. Nagy, Richard C. Anderson. 1984.
How many words are there in printed school
English? Reading Research Quarterly, 19, 304-
330.
Ernst Z. Rothkopf, Ronald D. Thurner. 1970. Ef-
fects of written instructional material on the sta-
tistical structure of test essays. Journal of
Educacational Psychology, 61, 83-89.
George Rasch. (1980). Probabilistic models for
some intelligence and attainment tests. (Copen-
hagen, Danish Institute for Educational Re-
search), expanded edition (1980) with foreword
</reference>
<page confidence="0.999357">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.725522">
<title confidence="0.999978">A New Yardstick and Tool for Personalized Vocabulary Building</title>
<author confidence="0.997967">Thomas K Landauer Kirill Kireyev Charles Panaccione</author>
<affiliation confidence="0.872443">Pearson Education, Knowledge Technologies</affiliation>
<email confidence="0.998812">tom.landauer@pearson.com</email>
<email confidence="0.998812">kirill.kireyev@pearson.com</email>
<email confidence="0.998812">charles.panaccione@pearson.com</email>
<abstract confidence="0.99776004">The goal of this research is to increase the value of each individual student&apos;s vocabulary by finding words that the student doesn’t know, needs to, and is ready to learn. To help identify such words, a better model of how well any given word is expected to be known was created. This is accomplished by using a semantic language model, LSA, to track how every word changes with the addition of more and more text from an appropriate corpus. We define the “maturity” of a word as the degree to which it has become similar to that after training on the entire corpus. An individual student’s average vocabulary level can then be placed on the wordmaturity scale by an adaptive test. Finally, the words that the student did or did not know on the test can be used to predict what other words the same student knows by using multiple maturity models trained on random samples of typical educational readings. This detailed information can be used to generate highly customized vocabulary teaching and testing exercises, such as Cloze tests.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richard C Anderson</author>
<author>Peter Freebody</author>
</authors>
<title>Vocabulary Knowledge. In</title>
<date>1981</date>
<location>Newark DE.</location>
<contexts>
<context position="1646" citStr="Anderson &amp; Freebody (1981)" startWordPosition="260" endWordPosition="263">same student knows by using multiple maturity models trained on random samples of typical educational readings. This detailed information can be used to generate highly customized vocabulary teaching and testing exercises, such as Cloze tests. 1 Introduction 1.1 Why “Vocabulary First” There are many arguments for the importance of more effective teaching of vocabulary. Here are some examples: (1) Baker, Simmons, &amp; Kame&apos;enui (1997) found that children who enter school with limited vocabulary knowledge grow much more discrepant over time from their peers who have rich vocabulary knowledge. (2.) Anderson &amp; Freebody (1981) found that the number of words in student’s meaning vocabu27 laries was the best predictor of how well they comprehend text. (3) An unpublished 1966 study of the correlation between entering scores of Stanford Students on the SAT found the vocabulary component to be the best predictor of grades in every subject, including science. (4) The number of words students learn varies greatly, from 0.2 to 8 words per day and from 50 to over 3,000 per year. (Anderson &amp; Freebody,1981) (5) Printed materials in grades 3 to 9 on average contain almost 90,000, distinct word families and nearly 500,000 word </context>
</contexts>
<marker>Anderson, Freebody, 1981</marker>
<rawString>Richard C. Anderson, Peter Freebody. 1981. Vocabulary Knowledge. In J. T. Guthrie (Ed.), Comprehension and teaching: Research reviews (pp. 77-117). International Reading Association, Newark DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott K Baker</author>
<author>Deborah C Simmons</author>
<author>Edward J Kameenui</author>
</authors>
<title>Vocabulary acquisition: Research bases. In</title>
<date>1997</date>
<location>Mahwah, NJ.</location>
<marker>Baker, Simmons, Kameenui, 1997</marker>
<rawString>Scott K. Baker, Deborah C. Simmons, Edward J. Kameenui. 1997. Vocabulary acquisition: Research bases. In Simmons, D. C. &amp; Kameenui, E. J. (Eds.), What reading research tells us about children with diverse learning needs: Bases and basics. Erlbaum, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Biemiller</author>
</authors>
<title>Words Worth Teaching.</title>
<date>2008</date>
<publisher>SRA/McGraw-Hill.</publisher>
<location>Co-lumbus, OH:</location>
<contexts>
<context position="17955" citStr="Biemiller, 2008" startWordPosition="3030" endWordPosition="3031">and would most profit from generally. Word Occurrences Occurrences Word in intermedi- in adult maturity ate corpus corpus (at level (level 5) 5) marble 54 485 0.21 sunshine 49 508 0.31 drugs 53 532 0.42 carpet 48 539 0.59 twin 48 458 0.61 earn 53 489 0.70 beam 47 452 0.76 Table 1 A sample of words with roughly the same number of occurrences in both intermediate (~50) and adult (~500) corpus The word maturity metric appears to perform well when validated by some external methods. For example, it reliably discriminates between words that were assigned to be taught in different school grades by (Biemiller, 2008), based on a combination of expert judgments and comprehension tests (p &lt; 0.03), as shown in Table 2. grade 2, grade 2, grade 6, grade 6, known known by known by known by &gt; 80% 40-80% 40-80% by &lt; 40% n=1034 n=606 n=1125 n=1411 4.4 6.5 8.8 9.5 Table 2 Average level for each word to reach a 0.5 maturity threshold, for words that are known at different levels by students of different grades (Biemiller, 2008). Median word maturity also tracks the differences (p &lt; 0.01) between essays written by students in different grades as shown in Figure 2. Figure 2 Percentage of “adult” words used in essays w</context>
</contexts>
<marker>Biemiller, 2008</marker>
<rawString>Andrew Biemiller (2008). Words Worth Teaching. Co-lumbus, OH: SRA/McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John B Carroll</author>
</authors>
<title>Cognitive Abilities: A survey of factor-analytic studies. Cambridge:</title>
<date>1993</date>
<publisher>Cambridge University Press,</publisher>
<marker>Carroll, 1993</marker>
<rawString>John B Carroll. 1993. Cognitive Abilities: A survey of factor-analytic studies. Cambridge: Cambridge University Press, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Betty Hart</author>
<author>Todd R Risley</author>
</authors>
<title>Meaningful differences in the everyday experience of young American children.</title>
<date>1995</date>
<publisher>Brookes Publishing,</publisher>
<marker>Hart, Risley, 1995</marker>
<rawString>Betty Hart, Todd R. Risley. 1995. Meaningful differences in the everyday experience of young American children. Brookes Publishing, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie R Kuhn</author>
<author>Steven A Stahl</author>
</authors>
<title>Teaching children to learn word meanings from context: A synthesis and some questions.</title>
<date>1998</date>
<journal>Journal of Literacy Research,</journal>
<volume>30</volume>
<issue>1</issue>
<pages>119--138</pages>
<marker>Kuhn, Stahl, 1998</marker>
<rawString>Melanie R. Kuhn, Steven A. Stahl. 1998. Teaching children to learn word meanings from context: A synthesis and some questions. Journal of Literacy Research, 30(1) 119-138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of the Acquisition, Induction, and Representation of Knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<pages>211--240</pages>
<contexts>
<context position="4448" citStr="Landauer &amp; Dumais, 1997" startWordPosition="741" endWordPosition="744">It is also clear that words are not learned in isolation: learning the meaning of a new word requires prior knowledge of many other words, and by most estimates it takes a (widely variable) average of ten encounters in different and separated contexts. (This, by the way, is what is required to match human adult competence in the computational language model used here. Given a text corpus highly similar to that experienced by a language learner, the model learns at very close to the same rate as an average child, and it learns new words as much as four times faster the more old words it knows (Landauer &amp; Dumais, 1997).) An important aside here concerns a widely circulated inference from the Nagy and Anderson (1984) result that teaching words by presenting them in context doesn’t produce enough vocabulary growth to be the answer. The problem is that the experiments actually show only that the inserted target word itself is usually not learned well enough to pass a test. But in the simulations, words are learned a little at a time; exposure to a sentence increases the knowledge of many other words, both ones in the sentence and not. Every encounter with any word in context percolates meaning through the whol</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer, Susan Dumais. 1997. A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of the Acquisition, Induction, and Representation of Knowledge. Psychological Review, 104, pp 211-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Danielle S McNamara</author>
<author>Simon Dennis</author>
<author>Walter Kintsch</author>
</authors>
<title>Handbook of Latent Semantic Analysis. Lawrence Erlbaum.</title>
<date>2007</date>
<contexts>
<context position="11586" citStr="Landauer et al., 2007" startWordPosition="1926" endWordPosition="1929">assages in which it occurs, and the vector for a passage is, of course, the average all of its words. In many previous applications to education, including automatic scoring of essays, the model’s similarity to human judgments (e.g. by mutual information measures) has been found to be 80 to 90% as high as that between two expert humans, and, as mentioned earlier, the rate at which it learns the meaning of words as assessed by various standardized and textbook-based tests has been found to closely match that of students. For more details, evaluations and previous educational applications, see (Landauer et al., 2007). 2.2 How it works in more detail: Word Maturity. Taking LSA to be a sufficiently good approximation of human learning of the meanings conveyed by printed word forms, we can use it to track their gradual acquisition as a function of increasing exposure to text representative in size and content of that which students at successive grade levels read. Thus, to model the growth of meaning of individual words, a series of sequentially accumulated LSA “semantic spaces” (the collection of vectors for all of the words and passages) are created. Cumulative portions of the corpus thus emulate the growi</context>
</contexts>
<marker>Landauer, McNamara, Dennis, Kintsch, 2007</marker>
<rawString>Thomas K Landauer, Danielle S. McNamara, Simon Dennis, and Walter Kintsch. 2007. Handbook of Latent Semantic Analysis. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cleborne D Maddux</author>
</authors>
<date>1999</date>
<journal>Peabody Picture Vocabulary Test III (PPVT-III). Diagnostique,</journal>
<volume>24</volume>
<pages>1--4</pages>
<marker>Maddux, 1999</marker>
<rawString>Cleborne D. Maddux (1999). Peabody Picture Vocabulary Test III (PPVT-III). Diagnostique, v24 n1-4, p221-28, 1998-1999</rawString>
</citation>
<citation valid="true">
<authors>
<author>William E Nagy</author>
<author>Richard C Anderson</author>
</authors>
<title>How many words are there in printed school English?</title>
<date>1984</date>
<journal>Reading Research Quarterly,</journal>
<volume>19</volume>
<pages>304--330</pages>
<contexts>
<context position="2331" citStr="Nagy and Anderson (1984)" startWordPosition="377" endWordPosition="380"> laries was the best predictor of how well they comprehend text. (3) An unpublished 1966 study of the correlation between entering scores of Stanford Students on the SAT found the vocabulary component to be the best predictor of grades in every subject, including science. (4) The number of words students learn varies greatly, from 0.2 to 8 words per day and from 50 to over 3,000 per year. (Anderson &amp; Freebody,1981) (5) Printed materials in grades 3 to 9 on average contain almost 90,000, distinct word families and nearly 500,000 word forms (including proper names.) (Nagy &amp; Anderson, 1984). (6) Nagy and Anderson (1984) found that on average not knowing more than one word in a sentence prevented its tested understanding, and that the probability of learning the meaning of a new word by one encounter on average was less than one in ten. (7) John B. Carroll’s (1993) meta-analysis of factor analyses of measured cognitive ability found the best predictor to be tests of vocabulary. (8) Hart and Risley’s large randomized observational study of the language used in households with young children found that the number of words spoken within hearing of a child was associated with a three-fold difference in vocabulary</context>
<context position="4547" citStr="Nagy and Anderson (1984)" startWordPosition="757" endWordPosition="760">es prior knowledge of many other words, and by most estimates it takes a (widely variable) average of ten encounters in different and separated contexts. (This, by the way, is what is required to match human adult competence in the computational language model used here. Given a text corpus highly similar to that experienced by a language learner, the model learns at very close to the same rate as an average child, and it learns new words as much as four times faster the more old words it knows (Landauer &amp; Dumais, 1997).) An important aside here concerns a widely circulated inference from the Nagy and Anderson (1984) result that teaching words by presenting them in context doesn’t produce enough vocabulary growth to be the answer. The problem is that the experiments actually show only that the inserted target word itself is usually not learned well enough to pass a test. But in the simulations, words are learned a little at a time; exposure to a sentence increases the knowledge of many other words, both ones in the sentence and not. Every encounter with any word in context percolates meaning through the whole current and future vocabulary. Indeed, in the simulator, indirect learning is three to five times</context>
<context position="2301" citStr="Nagy &amp; Anderson, 1984" startWordPosition="372" endWordPosition="375">n student’s meaning vocabu27 laries was the best predictor of how well they comprehend text. (3) An unpublished 1966 study of the correlation between entering scores of Stanford Students on the SAT found the vocabulary component to be the best predictor of grades in every subject, including science. (4) The number of words students learn varies greatly, from 0.2 to 8 words per day and from 50 to over 3,000 per year. (Anderson &amp; Freebody,1981) (5) Printed materials in grades 3 to 9 on average contain almost 90,000, distinct word families and nearly 500,000 word forms (including proper names.) (Nagy &amp; Anderson, 1984). (6) Nagy and Anderson (1984) found that on average not knowing more than one word in a sentence prevented its tested understanding, and that the probability of learning the meaning of a new word by one encounter on average was less than one in ten. (7) John B. Carroll’s (1993) meta-analysis of factor analyses of measured cognitive ability found the best predictor to be tests of vocabulary. (8) Hart and Risley’s large randomized observational study of the language used in households with young children found that the number of words spoken within hearing of a child was associated with a three</context>
</contexts>
<marker>Nagy, Anderson, 1984</marker>
<rawString>William E. Nagy, Richard C. Anderson. 1984. How many words are there in printed school English? Reading Research Quarterly, 19, 304-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ernst Z Rothkopf</author>
<author>Ronald D Thurner</author>
</authors>
<title>Effects of written instructional material on the statistical structure of test essays.</title>
<date>1970</date>
<journal>Journal of Educacational Psychology,</journal>
<volume>61</volume>
<pages>83--89</pages>
<marker>Rothkopf, Thurner, 1970</marker>
<rawString>Ernst Z. Rothkopf, Ronald D. Thurner. 1970. Effects of written instructional material on the statistical structure of test essays. Journal of Educacational Psychology, 61, 83-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Rasch</author>
</authors>
<title>Probabilistic models for some intelligence and attainment tests.</title>
<date>1980</date>
<institution>Danish Institute for Educational</institution>
<location>Copenhagen,</location>
<note>Research), expanded edition</note>
<contexts>
<context position="7916" citStr="Rasch, 1980" startWordPosition="1305" endWordPosition="1306">tion known at each “level” and interpolate and extrapolate. This is a reasonable method, because frequently encountered words are the ones most frequently needed to be understood. 2. Educational Materials. Sample vocabulary lessons and readings over classrooms at different school grades. 3. Expert Judgments. Obtain informed expert opinions about what words are important to know by what age for what purposes. Some estimates combine two or more of these approaches, and they vary in psychometric sophistication. For example, one of the most sophisticated, the Lexile Framework, uses Rasch scaling (Rasch, 1980) of a large sample of student vocabulary test scores (probability right on a test, holding student ability constant) to create a difficulty measure for sentences and then infers the difficulty of words, in essence, from the average difficulty of the sentences in which they appear. 28 The problem addressed in the present project goal is that all of these methods measure only the proportion of tested words known at one or more frequency ranges, in chosen school grades or for particular subsets of vocabulary (e.g. “academic” words), and for a very small subset—those tested - some of the words tha</context>
<context position="19030" citStr="Rasch, 1980" startWordPosition="3226" endWordPosition="3227"> 0.01) between essays written by students in different grades as shown in Figure 2. Figure 2 Percentage of “adult” words used in essays written by students of different grade levels. “Adult” words are defined as words that reach a 0.5 word maturity threshold at or later than the point where half of the words in the language have reached 0.5 threshold. 2.3 Finding words to teach individual students Using the computed word maturity values, a sigmoid characteristic curve is generated to approximate the growth curve of every word in the corpus. A model similar to one used in item response theory (Rasch, 1980) can be constructed from the growth curve due to its similarity in shape and function to an IRT characteristic curve; both curves represent the ability of a student. The characteristic curve for the IRT is needed to properly administer adaptive testing, which greatly increases the precision and generalizeability of the exam. Words to be tested are chosen from the corpus beginning at the average maturity of words at the approximate grade level of the student. Thirty to fifty word tests are used to home in on the student’s average word maturity level. In initial trials, a combination of yes/no a</context>
</contexts>
<marker>Rasch, 1980</marker>
<rawString>George Rasch. (1980). Probabilistic models for some intelligence and attainment tests. (Copenhagen, Danish Institute for Educational Research), expanded edition (1980) with foreword</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>