<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<note confidence="0.444885333333333">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 55-62.
Association for Computational Linguistics.
</note>
<title confidence="0.9963305">
Combining Sample Selection and Error-Driven Pruning for
Machine Learning of Coreference Rules
</title>
<author confidence="0.999586">
Vincent Ng and Claire Cardie
</author>
<affiliation confidence="0.996472">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.577028">
Ithaca, NY 14853-7501
</address>
<email confidence="0.996708">
yung,cardie @cs.cornell.edu
</email>
<sectionHeader confidence="0.995584" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999846105263158">
Most machine learning solutions to noun
phrase coreference resolution recast the
problem as a classification task. We ex-
amine three potential problems with this
reformulation, namely, skewed class dis-
tributions, the inclusion of “hard” training
instances, and the loss of transitivity in-
herent in the original coreference relation.
We show how these problems can be han-
dled via intelligent sample selection and
error-driven pruning of classification rule-
sets. The resulting system achieves an F-
measure of 69.5 and 63.4 on the MUC-
6 and MUC-7 coreference resolution data
sets, respectively, surpassing the perfor-
mance of the best MUC-6 and MUC-7
coreference systems. In particular, the
system outperforms the best-performing
learning-based coreference system to date.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990640628571429">
Noun phrase coreference resolution refers to the
problem of determining which noun phrases (NPs)
refer to each real-world entity mentioned in a doc-
ument. Machine learning approaches to this prob-
lem have been reasonably successful, operating pri-
marily by recasting the problem as a classi�cation
task (e.g. Aone and Bennett (1995), McCarthy and
Lehnert (1995), Soon et al. (2001)). Specifically, an
inductive learning algorithm is used to train a classi-
fier that decides whether or not two NPs in a docu-
ment are coreferent. Training data are typically cre-
ated by relying on coreference chains from the train-
ing documents: training instances are generated by
pairing each NP with each of its preceding NPs; in-
stances are labeled as positive if the two NPs are in
the same coreference chain, and labeled as negative
otherwise.1
A separate clustering mechanism then coordinates
the possibly contradictory pairwise coreference clas-
sification decisions and constructs a partition on the
set of NPs with one cluster for each set of corefer-
ent NPs. Although, in principle, any clustering algo-
rithm can be used, most previous work uses a single-
link clustering algorithm to impose coreference par-
titions.2 An implicit assumption in the choice of the
single-link clustering algorithm is that coreference
resolution is viewed as anaphora resolution, i.e. the
goal during clustering is to find an antecedent for
each anaphoric NP in a document.3
Three intrinsic properties of coreference4, how-
ever, make the formulation of the problem as a
classification-based single-link clustering task po-
tentially undesirable:
Coreference is a rare relation. That is, most
NP pairs in a document are not coreferent. Con-
</bodyText>
<footnote confidence="0.945804545454545">
1Two NPs are in the same coreference chain if and only if
they are coreferent.
2One exception is Kehler’s work on probabilistic corefer-
ence (Kehler, 1997), in which he applies Dempster’s Rule of
Combination (Dempster, 1968) to combine all pairwise proba-
bilities of coreference to form a partition.
3In this paper, we consider an NP anaphoric if it is part of a
coreference chain but is not the head of the chain.
4Here, we use the term coreference loosely to refer to either
the problem or the binary relation defined on a set of NPs. The
particular choice should be clear from the context.
</footnote>
<bodyText confidence="0.993134316455697">
sequently, generating training instances by pairing
each NP with each of its preceding NPs creates
highly skewed class distributions, in which the num-
ber of positive instances is overwhelmed by the
number of negative instances. For example, the stan-
dard MUC-6 and MUC-7 (1995; 1998) coreference
data sets contain only 2% positive instances. Un-
fortunately, learning in the presence of such skewed
class distributions remains an open area of research
in the machine learning community (e.g. Pazzani et
al. (1994), Fawcett (1996), Cardie and Howe (1997),
Kubat and Matwin (1997)).
Coreference is a discourse-level problem with dif-
ferent solutions for different types of NPs. The
interpretation of a pronoun, for example, may be de-
pendent only on its closest antecedent and not on the
rest of the members of the same coreference chain.
Proper name resolution, on the other hand, may be
better served by ignoring locality constraints alto-
gether and relying on string-matching or more so-
phisticated aliasing techniques. Consequently, gen-
erating positive instances from all pairs of NPs from
the same coreference chain can potentially make the
learning task harder: all but a few coreference links
derived from any chain might be hard to identify
based on the available contextual cues.
Coreference is an equivalence relation. Recast-
ing the problem as a classification task precludes en-
forcement of the transitivity constraint. After train-
ing, for example, the classifier might determine that
A is coreferent with B, and B with C, but that A and
C are not coreferent. Hence, the clustering mecha-
nism is needed to coordinate these possibly contra-
dictory pairwise classifications. In addition, because
the coreference classifiers are trained independent of
the clustering algorithm to be used, improvements in
classification accuracy do not guarantee correspond-
ing improvements in clustering-level accuracy, i.e.
overall performance on the coreference resolution
task might not improve.
This paper examines each of the above issues.
First, to address the problem of skewed class dis-
tributions, we apply a technique for negative in-
stance selection similar to that proposed in Soon et
al. (2001). In contrast to results reported there, how-
ever, we show empirically that system performance
increases noticeably in response to negative example
selection, with increases in F-measure of 3-5%.
Second, in an attempt to avoid the inclusion of
“hard” training instances, we present a corpus-based
method for implicit selection of positive instances.
The approach is a fully automated variant of the ex-
ample selection algorithm introduced in Harabagiu
et al. (2001). With positive example selection, sys-
tem performance (F-measure) again increases, by
12-14%.
Finally, to more tightly tie the classification- and
clustering-level coreference decisions, we propose
an error-driven rule pruning algorithm that opti-
mizes the coreference classifier ruleset with respect
to the clustering-level coreference scoring function.
Overall, the use of pruning boosts system perfor-
mance from an F-measure of 69.3 to 69.5, and from
57.2 to 63.4 for the MUC-6 and MUC-7 data sets,
respectively, enabling the system to achieve perfor-
mance that surpasses that of the best MUC corefer-
ence systems by 4.6% and 1.6%. In particular, the
system outperforms the best-performing learning-
based coreference system (Soon et al., 2001) by
6.9% and 3.0%.
The remainder of the paper is organized as fol-
lows. In sections 2 and 3, we present the machine
learning framework underlying the baseline corefer-
ence system and examine the effect of negative sam-
ple selection. Section 4 presents our corpus-based
algorithm for selection of positive instances. Section
5 describes and evaluates the error-driven pruning
algorithm. We conclude with future work in section
6.
</bodyText>
<sectionHeader confidence="0.8129215" genericHeader="method">
2 The Machine Learning Framework for
Coreference Resolution
</sectionHeader>
<bodyText confidence="0.6334762">
Our machine learning framework for coreference
resolution is a standard combination of classification
and clustering, as described above.
Creating an instance. An instance in our machine
learning framework is a description of two NPs in a
document. More formally, let NP✂ be the th NP in
document . An instance formed from NP✞ and NP✟
is denoted by . A valid instance is an
instance such that NP✞ precedes NP✟ .5
Following previous work (Aone and Bennett (1995),
</bodyText>
<footnote confidence="0.804875">
5By definition, exactly valid instances can be created
from NPs in a given document.
</footnote>
<bodyText confidence="0.998679166666667">
Soon et al. (2001)), we assume throughout the paper
that only valid instances will be generated and used
for training and testing. Each instance consists of 25
features, which are described in Table 1.6 The clas-
sification associated with a training instance is one
of COREFERENT or NOT COREFERENT depending
on whether the NPs co-refer in the associated train-
ing text.7
Building an NP coreference classifier. We use
RIPPER (Cohen, 1995), an information gain-based
propositional rule learning system, to train a classi-
fier that, given a test instance , decides
whether or not NP✞ and NP✟ are coreferent. Specifi-
cally, RIPPER sequentially covers the positive train-
ing instances and induces a ruleset that determines
when two NPs are coreferent. When none of the
rules in the ruleset is applicable to a given NP pair, a
default rule that classifies the pair as not coreferent
is automatically invoked. The output of the classifier
is either COREFERENT or NOT COREFERENT along
with a number between 0 and 1 that indicates the
confidence of the classification.
Applying the classifier to create coreference
chains. After training, the resulting ruleset is used
by a best-first clustering algorithm to impose a par-
titioning on all NPs in the test texts, creating one
cluster for each set of coreferent NPs. Texts are
processed from left to right. Each NP encountered,
NP✟ , is compared in turn to each preceding NP, NP✞ ,
from right to left. For each pair, a test instance is
created as during training and is presented to the
coreference classifier. The NP with the highest con-
fidence value among the preceding NPs that are clas-
sified as being coreferent with NP✟ is selected as the
antecedent of NP✟ ; otherwise, no antecedent is se-
lected for NP✟ .
</bodyText>
<sectionHeader confidence="0.994204" genericHeader="method">
3 Negative Sample Selection
</sectionHeader>
<bodyText confidence="0.999892">
As noted above, skewed class distributions arise
when generating all valid instances from the train-
ing texts. A number of methods for handling skewed
distributions have been proposed in the machine
learning literature, most of which modify the learn-
</bodyText>
<footnote confidence="0.98944475">
6See Ng and Cardie (2002) for a detailed description of the
features.
7In all of the work presented here, NPs are identified, and
feature values computed entirely automatically.
</footnote>
<construct confidence="0.4251528">
Algorithm NEG-SELECT(NEG: set of all possible
negative instances)
for NEG do
if NP✟ is anaphoric then
if NP✞ precedes (NP✟ ) then
</construct>
<figure confidence="0.606933">
NEG := NEG
else
NEG := NEG
return NEG
</figure>
<figureCaption confidence="0.999926">
Figure 1: The NEG-SELECT algorithm
</figureCaption>
<bodyText confidence="0.999594">
ing algorithm to incorporate a loss function with a
much larger penalty for minority class errors than for
instances from the majority classes (e.g. Gordon and
Perlis (1989), Pazzani et al. (1994)). We investigate
here a different approach to handling skewed class
distributions — negative sample selection, i.e. the
selection of a smaller subset of negative instances
from the set of available negative instances. In the
case of NP coreference, we hypothesize that reduc-
ing the number of negative instances will improve
recall but potentially reduce precision: intuitively,
the existence of fewer negative instances should al-
low RIPPER to more liberally induce positive rules.
We propose a method for negative sample selection
that, for each anaphoric NP, NP✟ , retains only those
negative instances for non-coreferent NPs that lie
between NP✟ and its farthest preceding antecedent,
(NP✟ ). The algorithm for negative sample selec-
tion, NEG-SELECT, is shown in Figure 1. NEG-
SELECT takes as input the set of all possible neg-
ative instances in the training texts, i.e. the set of
valid instances such that NP✞ and NP✟
are not in the same coreference chain.
The intuition behind this approach is very simple.
Let (NP✟ ) be the set of preceding antecedents of
NP✟ , and (NP✞ ,NP✟ ) be the set consisting of NPs
NP✞ , NP✡ , , NP✟ . Recall that the goal dur-
ing clustering is to compute, for each NP NP✟ , the
set (NP✟ ) from which the element with the high-
est confidence is selected as the antecedent of NP✟ .
Since (1) (NP✟ ) is a subset of ( (NP✟ ),NP✟ )8 and
</bodyText>
<footnote confidence="0.666251">
8We define ( (NP ),NP ) to be the empty set if (NP )
does not exist (i.e. NP is not anaphoric).
</footnote>
<table confidence="0.999969567567567">
Feature Type Feature Description
Lexical PRO STR C if both NPs are pronominal and are the same string; else I.
PN STR C if both NPs are proper names and are the same string; else I.
SOON STR NONPRO C if both NPs are non-pronominal and the string of NP matches that of NP✗ ;
else I.
Grammatical PRONOUN 1 Y if NP is a pronoun; else N.
PRONOUN 2 Y if NP is a pronoun; else N.
DEMONSTRATIVE 2 Y if NP starts with a demonstrative such as “this,” “that,” “these,” or “those;”
else N.
BOTH PROPER NOUNS C if both NPs are proper names; NA if exactly one NP is a proper name; else I.
NUMBER C if the NP pair agree in number; I if they disagree; NA if number information
for one or both NPs cannot be determined.
GENDER C if the NP pair agree in gender; I if they disagree; NA if gender information
for one or both NPs cannot be determined.
ANIMACY C if the NPs match in animacy; else I.
APPOSITIVE C if the NPs are in an appositive relationship; else I.
PREDNOM C if the NPs form a predicate nominal construction; else I.
BINDING I if the NPs violate conditions B or C of the Binding Theory; else C.
CONTRAINDICES I if the NPs cannot be co-indexed based on simple heuristics; else C. For
instance, two non-pronominal NPs separated by a preposition cannot be co-
indexed.
SPAN I if one NP spans the other; else C.
MAXIMALNP I if both NPs have the same maximal NP projection; else C.
SYNTAX I if the NPs have incompatible values for the BINDING, CONTRAINDICES, SPAN
or MAXIMALNP constraints; else C.
INDEFINITE I if NP is an indefinite and not appositive; else C.
I if NP✏ is a pronoun and NP is not; else C.
PRONOUN
EMBEDDED 1 Y if NP is an embedded noun; else N.
TITLE I if one or both of the NPs is a title; else C.
Semantic WNCLASS C if the NPs have the same WordNet semantic class; I if they don’t; NA if the
semantic class information for one or both NPs cannot be determined.
ALIAS C if one NP is an alias of the other; else I.
Positional SENTNUM Distance between the NPs in terms of the number of sentences.
Others PRO RESOLVE
C if NP✗ is a pronoun and NP✏ is its antecedent according to a naive pronoun
resolution algorithm; else I.
</table>
<tableCaption confidence="0.912573">
Table 1: Feature Set for the Coreference System. The feature set contains relational and non-relational features. Non-
relational features test some property P of one of the NPs under consideration and take on a value of YES or NO depending on
whether P holds. Relational features test whether some property P holds for the NP pair under consideration and indicate whether
the NPs are COMPATIBLE or INCOMPATIBLE w.r.t. P; a value of NOT APPLICABLE is used when property P does not apply.
</tableCaption>
<bodyText confidence="0.999690564102564">
(2) NP✟ is compared to each preceding NP from
right to left by the clustering algorithm, it follows
that the set of negative instances whose classifica-
tions the classifier needs to determine in order to
compute (NP✟ ) is a superset of the set of instances
(NP✟ ) formed by pairing NP✟ with each of its non-
coreferent preceding NPs in ( (NP✟ ),NP✟ ). Con-
sequently, (NP✟ ) is the minimal set of (nega-
tive) instances whose classifications will be required
during clustering. In principle, to perform the classi-
fications accurately, the classifier needs to be trained
on the corresponding set of negative instances from
the training set, which is (NP✟ ), where NP✟
is now the th NP in training document . NEG-
SELECT is designed essentially to compute this set.
Next, we examine the effects of this minimalist ap-
proach to negative sample selection.
Evaluation. We evaluate the coreference system
with negative sample selection on the MUC-6 and
MUC-7 coreference data sets in each case, train-
ing the coreference classifier on the 30 “dry run”
texts, and applying the coreference resolution algo-
rithm on the 20–30 “formal evaluation” texts. Re-
sults are shown in rows 1 and 2 of Table 2 where
performance is reported in terms of recall, precision,
and F-measure using the model-theoretic MUC scor-
ing program (Vilain et al., 1995). The Baseline sys-
tem employs no sample selection, i.e. all available
training examples are used. Row 2 shows the per-
formance of the Baseline after incorporating NEG-
SELECT. With negative sample selection, the per-
centage of positive instances rises from 2% to 8%
for the MUC-6 data set and from 2% to 7% for the
MUC-7 data set. For both data sets, we see statis-
tically significant increases in recall and statistically
significant, but much larger drops in precision.9 The
resulting F-measure scores, however, increase non-
trivially from 52.4 to 55.2 (for MUC-6), and from
41.3 to 46.0 (for MUC-7).10
</bodyText>
<sectionHeader confidence="0.992933" genericHeader="method">
4 Positive Sample Selection
</sectionHeader>
<bodyText confidence="0.9998366">
Since not all of the coreference relationships de-
rived from coreference chains are equally easy to
identify, training a classifier using all possible coref-
erence relationships can potentially lead to the in-
duction of inaccurate rules. Given the observa-
tion that one antecedent is sufficient to resolve an
anaphor, it may be desirable to learn only from easy
positive instances. Similar observations are made
by Harabagiu et al. (2001), who point out that in-
telligent selection of positive instances can poten-
tially minimize the amount of knowledge required
to perform coreference resolution accurately. They
assume that the easiest types of coreference rela-
tionships to resolve are those that occur with high
frequencies in the data. Consequently, they mine
by hand three sets of coreference rules for cov-
ering positive instances from the training data by
finding the coreference knowledge satisfied by the
largest number of anaphor-antecedent pairs. While
the Harabagiu et al. algorithm attempts to mine
easy coreference rules from the data by hand, nei-
ther the rule creation process nor stopping condi-
tions are precisely defined. In addition, a lot of
human intervention is required to derive the rules.
In this section, we describe an automatic positive
sample selection algorithm that coarsely mimics the
Harabagiu et al. algorithm by finding a confident an-
tecedent for each anaphor. Overall, our goal is to
avoid the inclusion of hard training instances by au-
tomating the process of deriving easy coreference
rules from the data.
The Algorithm. The positive sample selection al-
gorithm, POS-SELECT, is shown in Figure 2. It as-
sumes the existence of a rule learner, L, that pro-
duces an ordered set ofpositive rules. POS-SELECT
</bodyText>
<footnote confidence="0.998184666666667">
9Chi-square statistical significance tests are applied to
changes in recall and precision throughout the paper. Unless
otherwise noted, reported differences are at the 0.05 level or
higher. The chi-square test is not applicable to F-measure.
10The F-measure score computed by the MUC scoring pro-
gram is the harmonic mean of recall and precision.
</footnote>
<figure confidence="0.783629363636364">
Algorithm POS-SELECT(L: positive rule learner,
T: set of training instances)
FinalRuleSet := ;
AnaphorSet := ;
BestRule := NIL;
repeat
BestRule := best rule among the ranked set
of positive rules induced on T using L
FinalRuleSet := FinalRuleSet BestRule
//collect anaphors from instances that
// are correctly covered by BestRule
for T do
if is covered by BestRule and
class( ) COREFERENT then
AnaphorSet := AnaphorSet NP✟
//remove instances associated with the
//anaphors covered by BestRule
for T do
if NP✟ AnaphorSet then
T T
until L cannot induce any rule for the positives.
return FinalRuleSet
</figure>
<figureCaption confidence="0.999994">
Figure 2: The POS-SELECT algorithm
</figureCaption>
<bodyText confidence="0.999173733333333">
first uses L to induce a ruleset on the training in-
stances and picks the first rule from the ruleset. For
any training instance correctly covered
by this rule, an antecedent NP✞ has been identi-
fied for the anaphor NP✟ . As a result, all (positive
and negative) training instances formed with NP✟
as the anaphor are no longer needed and are sub-
sequently removed from the training data.11 The
process is repeated until L cannot induce a rule to
cover the remaining positive instances. The output
of POS-SELECT is a set of positive rules selected
during each iteration of the algorithm. Hence, posi-
tive sample selection in POS-SELECT is implicit in
the sense that it is embedded within the rule induc-
tion process.
</bodyText>
<tableCaption confidence="0.637784333333333">
Evaluation. Results are shown in rows 3 and 4 of
Table 2. As in the previous experiments, the rule
learner is RIPPER. We run the system twice, first
</tableCaption>
<footnote confidence="0.982964">
11We speculate that retaining the negative instances would
hurt performance, but this remains to be verified.
</footnote>
<table confidence="0.9994675">
Experiments Algorithms used R MUC-6 F R MUC-7 F
P P
Baseline — 40.7 73.5 52.4 27.2 86.3 41.3
Neg-Only NEG-SELECT 46.5 67.8 55.2 37.4 59.7 46.0
Pos-Only POS-SELECT 53.1 80.8 64.1 41.1 78.0 53.8
Combined NEG-SELECT+POS-SELECT 63.4 76.3 69.3 59.5 55.1 57.2
Pruning NEG-SELECT+POS-SELECT+RULE-SELECT 63.3 76.9 69.5 54.2 76.3 63.4
More Training NEG-SELECT+POS-SELECT 64.8 70.6 67.6 60.0 55.7 57.8
</table>
<tableCaption confidence="0.999605">
Table 2: Effects of sample selection and error-driven pruning.
</tableCaption>
<bodyText confidence="0.9999120625">
with POS-SELECT only and then with both POS-
SELECT and NEG-SELECT. With POS-SELECT
only, the system achieves an F-measure of 64.1
(for MUC-6) and 53.8 (for MUC-7). When POS-
SELECT and NEG-SELECT are used in combina-
tion, however, the system achieves an F-measure of
69.3 (for MUC-6) and 57.2 (for MUC-7).
Discussion. The experimental results are largely
consistent with our hypothesis. System performance
improves dramatically with positive sample selec-
tion using POS-SELECT both in the absence and
presence of negative sample selection. Without neg-
ative sample selection, F-measure increases from
52.4 to 64.1 (for MUC-6), and from 41.3 to 53.8 (for
MUC-7). Similarly, with negative sample selection,
F-measure increases from 55.2 to 69.3 (for MUC-6),
and from 46.0 to 57.2 (for MUC-7). In addition, our
results indicate that applying both negative and pos-
itive sample selection leads to better performance
than applying positive sample selection alone: F-
measure increases from 64.1 to 69.3, and from 53.8
to 57.2 for the MUC-6 and MUC-7 data sets, respec-
tively. Nevertheless, reducing the number of neg-
ative instances (via negative sample selection) im-
proves recall but damages precision: we see sta-
tistically significant gains in recall and statistically
significant drops in precision for both data sets. In
particular, precision drops precipitously from 78.0
to 55.1 for the MUC-7 data set. We hypothesize
that POS-SELECT does not guarantee that hard pos-
itive instances will be avoided and that the inclu-
sion of these hard instances is responsible for the
poorer precision of the system. Anaphors that do not
have easy antecedents can never be removed auto-
matically via the induction of new rules using POS-
SELECT. In fact, RIPPER will possibly induce rules
to handle these hard instances as long as such kind of
anaphors occur sufficiently frequently in the data set
relative to the number of negative instances.12 Al-
though it might be beneficial to acquire these rules
at the classification level (according to the learning
algorithm), they can be detrimental to system per-
formance at the clustering level, especially if the
rules cover a large number of examples with a lot
of exceptions. Consequently, it is necessary to know
which rules are worthy of keeping at the clustering
level and not the classification level. We will address
this issue in the next section.
</bodyText>
<sectionHeader confidence="0.887528" genericHeader="method">
5 Pruning the Coreference Ruleset
</sectionHeader>
<bodyText confidence="0.999829043478261">
As noted in the introduction, machine learning ap-
proaches to coreference resolution that rely only on
pairwise NP coreference classifiers will not neces-
sarily enforce the transitivity constraint inherent in
the coreference relation. Although approaches to
coreference resolution that rely only on clustering
could easily enforce transitivity (as in Cardie and
Wagstaff (1999)), they have not performed as well
as state-of-the-art approaches to coreference. In this
section, we propose a method for resolving this con-
flict: we introduce an error-driven rule pruning al-
gorithm that considers rules induced by the coref-
erence classifier and discards those that cause the
ruleset to perform poorly with respect to the global,
clustering-level coreference scoring function.
The Algorithm. The error-driven pruning algo-
rithm is inspired by the backward elimination al-
gorithm commonly used for feature selection (see
Blum and Langley (1997)) and is shown in Figure
3. The algorithm, RULE-SELECT, takes as input a
ruleset learned from a training corpus for perform-
ing coreference resolution, a pruning corpus (dis-
joint from the training corpus), and a clustering-level
</bodyText>
<footnote confidence="0.99488925">
12More precisely, RIPPER will induce a new rule if the rule
is more than 50% accurate and the resulting description length
is fewer than 64 bits larger than the smallest description length
obtained so far.
</footnote>
<figure confidence="0.561094375">
Algorithm RULE-SELECT(R: ruleset,
P: pruning corpus,
S: scoring function)
BestScore score of the coreference system
using R on P w.r.t. S;
r NIL;
repeat
r := the rule in R whose removal yields a
ruleset with which the coreference system
achieves the best score b on P w.r.t. S.
if b BestScore then
BestScore b;
R R r
else break
while true
return R
</figure>
<figureCaption confidence="0.99999">
Figure 3: The RULE-SELECT algorithm
</figureCaption>
<bodyText confidence="0.998953456140351">
coreference scoring function that is the same as the
one being used for evaluating the final output of the
system.13 At each iteration, RULE-SELECT greed-
ily discards the rule whose removal yields a rule-
set with which the coreference system performs the
best (with respect to the coreference scoring func-
tion) on the pruning corpus. As a hill-climbing pro-
cedure, the algorithm terminates when removal of
any of the rules in the ruleset fails to improve per-
formance. In contrast to most existing algorithms for
coreference resolution, RULE-SELECT establishes
a tighter connection between the classification- and
clustering-level decisions for coreference resolution
and ensures that system performance is optimized
with respect to the coreference scoring function. We
hypothesize that this optimization of the coreference
classifier will improve performance of the resulting
coreference system, in particular by increasing its
precision.
Evaluation and Discussion. Results are shown in
row 5 of Table 2. In the Pruning experiment, the
MUC-7 formal evaluation corpus is the pruning cor-
pus for the MUC-6 run; the MUC-6 formal evalu-
ation corpus is the pruning corpus for the MUC-7
13Importantly, RULE-SELECT assumes no knowledge of the
inner workings of the scoring function.
run. In addition, the quantity that RULE-SELECT
optimizes for a given ruleset is the F-measure re-
turned by the MUC scoring function.14 In compar-
ison to the Combined results, we see an improve-
ment of 0.2% (for MUC-6) and 6.2% (for MUC-7)
in F-measure. In particular, we see statistically sig-
nificant gains in precision (from 55.1 to 73.6) and
statistically significant, but much smaller, drops in
recall (from 59.5 to 54.2) for the MUC-7 data set.
In general, our results support the hypothesis that
rule pruning can be used to improve system perfor-
mance; moreover, the technique is especially effec-
tive at enhancing the precision of the system. How-
ever, performance gains may be negligible when
pruning is used in systems with high precision, as
can be seen from the results for the MUC-6 data set.
To determine whether performance improvements
are instead attributable to the availability of addi-
tional “training” data provided by the pruning cor-
pus, we train a classifier (using the same setting as
the Combined experiments) on both the training and
the pruning corpora. The performance of the system
using this unpruned ruleset is shown in the last row
of Table 2. In comparison to the Combined results,
F-measure drops from 69.3 to 67.6 (for MUC-6),
and rises from 57.2 and 57.8 (for MUC-7). These re-
sults indicate that the RULE-SELECT algorithm has
made a more effective use of the additional data than
the learning algorithm without rule pruning by ex-
ploiting the feedback provided by the scoring func-
tion.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.969964">
We have examined three problems with recasting
noun phrase coreference resolution as a classifica-
tion task. To handle these problems, we presented
a minimalist negative sample selection algorithm to
reduce the skewness of the class distributions, and
an automatic positive sample selection algorithm to
select easy positive instances. In addition, our ex-
periments indicate that the positive sample selection
algorithm does not guarantee that hard instances can
be entirely excluded. As a result, we proposed an
error-driven rule pruning algorithm that can effec-
tively enhance the precision of the system by dis-
14RULE-SELECT can be used in conjunction with any coref-
erence scoring function. The MUC scorer is chosen here to fa-
cilitate comparison with previous results.
carding rules that cause the ruleset to perform poorly
with respect to the coreference scoring function.
The resulting system outperformed the best MUC-
6 and MUC-7 coreference systems as well as the
best-performing learning-based system on the cor-
responding MUC data sets. Nevertheless, there is
substantial room for improvement. For example, it
is important to know how sensitive system perfor-
mance is with respect to the size of the pruning cor-
pus. In addition, although we use RIPPER as the un-
derlying learning algorithm in our coreference sys-
tem, we expect that the techniques described in this
paper can be used in conjunction with other learn-
ing algorithms. We plan to explore this possibility
in future work.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999675">
We thank two anonymous reviewers for their help-
ful comments and Hwee Tou Ng for explaining the
method of training instance selection employed by
his coreference system. This work was supported
in part by DARPA TIDES contract N66001-00-C-
8009, and NSF Grants 0081334 and 0074896.
</bodyText>
<sectionHeader confidence="0.999255" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999950086956522">
C. Aone and S. W. Bennett. 1995. Evaluating Auto-
mated and Manual Acquisition of Anaphora Resolu-
tion Strategies. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 122–129.
A. Blum and P. Langley. 1997. Selection of relevant
features and examples in machine learning. Artificial
Intelligence, pages 245–271.
C. Cardie and N. Howe. 1997. Improving minority class
prediction using case-specific feature weights. In Pro-
ceedings of the Fourteenth International Conference
on Machine Learning, pages 57–65.
C. Cardie and K. Wagstaff. 1999. Noun Phrase Coref-
erence as Clustering. In Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora (EMNLP/VLC-99), pages 82–
89.
W. Cohen. 1995. Fast Effective Rule Induction. In Pro-
ceedings of the Twelfth International Conference on
Machine Learning, San Francisco, CA.
A. Dempster. 1968. A generalization of Bayesian infer-
ence. Journal of the Royal Statistical Society, 30:205–
247.
T. Fawcett. 1996. Learning with skewed class distri-
butions — summary of responses. Machine Learning
List: Vol. 8, No. 20.
D. F. Gordon and D. Perlis. 1989. Explicitly biased gen-
eralization. Computational Intelligence, 5:67–81.
S. Harabagiu, R. Bunescu, and S. Maiorano. 2001. Text
and Knowledge Mining for Coreference Resolution.
In Proceedings of the Second Meeting of the North
America Chapter of the Associationfor Computational
Linguistics (NAACL-2001), pages 55–62.
A. Kehler. 1997. Probabilistic Coreference in Informa-
tion Extraction. In Proceedings of the Second Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 163–173.
M. Kubat and S. Matwin. 1997. Addressing the Curse
of Imbalanced Training Sets: One-Sided Selection. In
Proceedings of the 14th International Conference on
Machine Learning (ICML-97), pages 179–186.
J. McCarthy and W. Lehnert. 1995. Using Decision
Trees for Coreference Resolution. In Proceedings of
the Fourteenth International Conference on Artificial
Intelligence, pages 1050–1055.
MUC-6. 1995. Proceedings of the Sixth Message Under-
standing Conference (MUC-6). Morgan Kaufmann,
San Francisco, CA.
MUC-7. 1998. Proceedings of the Seventh Message
Understanding Conference (MUC-7). Morgan Kauf-
mann, San Francisco, CA.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics. Association for Computa-
tional Linguistics.
M. Pazzani, C. Merz, P. Murphy, K. Ali, T. Hume, and
C. Brunk. 1994. Reducing Misclassification Costs. In
Proceedings of the Eleventh International Conference
on Machine Learning, pages 217–225.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A
Machine Learning Approach to Coreference Resolu-
tion of Noun Phrases. Computational Linguistics,
27(4):521–544.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6), pages 45–
52. Morgan Kaufmann.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.682424">
<note confidence="0.953261333333333">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 55-62. Association for Computational Linguistics.</note>
<title confidence="0.991284">Combining Sample Selection and Error-Driven Pruning for Machine Learning of Coreference Rules</title>
<author confidence="0.982146">Ng</author>
<affiliation confidence="0.960196">Department of Computer Cornell</affiliation>
<address confidence="0.970674">Ithaca, NY</address>
<email confidence="0.995842">yung,cardie@cs.cornell.edu</email>
<abstract confidence="0.9961578">Most machine learning solutions to noun phrase coreference resolution recast the problem as a classification task. We examine three potential problems with this reformulation, namely, skewed class distributions, the inclusion of “hard” training instances, and the loss of transitivity inherent in the original coreference relation. We show how these problems can be handled via intelligent sample selection and error-driven pruning of classification rulesets. The resulting system achieves an Fmeasure of 69.5 and 63.4 on the MUC- 6 and MUC-7 coreference resolution data sets, respectively, surpassing the performance of the best MUC-6 and MUC-7 coreference systems. In particular, the system outperforms the best-performing learning-based coreference system to date.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>S W Bennett</author>
</authors>
<title>Evaluating Automated and Manual Acquisition of Anaphora Resolution Strategies.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>122--129</pages>
<contexts>
<context position="1512" citStr="Aone and Bennett (1995)" startWordPosition="213" endWordPosition="216">es an Fmeasure of 69.5 and 63.4 on the MUC6 and MUC-7 coreference resolution data sets, respectively, surpassing the performance of the best MUC-6 and MUC-7 coreference systems. In particular, the system outperforms the best-performing learning-based coreference system to date. 1 Introduction Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document. Machine learning approaches to this problem have been reasonably successful, operating primarily by recasting the problem as a classi�cation task (e.g. Aone and Bennett (1995), McCarthy and Lehnert (1995), Soon et al. (2001)). Specifically, an inductive learning algorithm is used to train a classifier that decides whether or not two NPs in a document are coreferent. Training data are typically created by relying on coreference chains from the training documents: training instances are generated by pairing each NP with each of its preceding NPs; instances are labeled as positive if the two NPs are in the same coreference chain, and labeled as negative otherwise.1 A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference classif</context>
<context position="7802" citStr="Aone and Bennett (1995)" startWordPosition="1209" endWordPosition="1212">escribes and evaluates the error-driven pruning algorithm. We conclude with future work in section 6. 2 The Machine Learning Framework for Coreference Resolution Our machine learning framework for coreference resolution is a standard combination of classification and clustering, as described above. Creating an instance. An instance in our machine learning framework is a description of two NPs in a document. More formally, let NP✂ be the th NP in document . An instance formed from NP✞ and NP✟ is denoted by . A valid instance is an instance such that NP✞ precedes NP✟ .5 Following previous work (Aone and Bennett (1995), 5By definition, exactly valid instances can be created from NPs in a given document. Soon et al. (2001)), we assume throughout the paper that only valid instances will be generated and used for training and testing. Each instance consists of 25 features, which are described in Table 1.6 The classification associated with a training instance is one of COREFERENT or NOT COREFERENT depending on whether the NPs co-refer in the associated training text.7 Building an NP coreference classifier. We use RIPPER (Cohen, 1995), an information gain-based propositional rule learning system, to train a cla</context>
</contexts>
<marker>Aone, Bennett, 1995</marker>
<rawString>C. Aone and S. W. Bennett. 1995. Evaluating Automated and Manual Acquisition of Anaphora Resolution Strategies. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>P Langley</author>
</authors>
<title>Selection of relevant features and examples in machine learning.</title>
<date>1997</date>
<journal>Artificial Intelligence,</journal>
<pages>245--271</pages>
<contexts>
<context position="23979" citStr="Blum and Langley (1997)" startWordPosition="3936" endWordPosition="3939">uld easily enforce transitivity (as in Cardie and Wagstaff (1999)), they have not performed as well as state-of-the-art approaches to coreference. In this section, we propose a method for resolving this conflict: we introduce an error-driven rule pruning algorithm that considers rules induced by the coreference classifier and discards those that cause the ruleset to perform poorly with respect to the global, clustering-level coreference scoring function. The Algorithm. The error-driven pruning algorithm is inspired by the backward elimination algorithm commonly used for feature selection (see Blum and Langley (1997)) and is shown in Figure 3. The algorithm, RULE-SELECT, takes as input a ruleset learned from a training corpus for performing coreference resolution, a pruning corpus (disjoint from the training corpus), and a clustering-level 12More precisely, RIPPER will induce a new rule if the rule is more than 50% accurate and the resulting description length is fewer than 64 bits larger than the smallest description length obtained so far. Algorithm RULE-SELECT(R: ruleset, P: pruning corpus, S: scoring function) BestScore score of the coreference system using R on P w.r.t. S; r NIL; repeat r := the rule</context>
</contexts>
<marker>Blum, Langley, 1997</marker>
<rawString>A. Blum and P. Langley. 1997. Selection of relevant features and examples in machine learning. Artificial Intelligence, pages 245–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>N Howe</author>
</authors>
<title>Improving minority class prediction using case-specific feature weights.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning,</booktitle>
<pages>57--65</pages>
<contexts>
<context position="4017" citStr="Cardie and Howe (1997)" startWordPosition="620" endWordPosition="623">n a set of NPs. The particular choice should be clear from the context. sequently, generating training instances by pairing each NP with each of its preceding NPs creates highly skewed class distributions, in which the number of positive instances is overwhelmed by the number of negative instances. For example, the standard MUC-6 and MUC-7 (1995; 1998) coreference data sets contain only 2% positive instances. Unfortunately, learning in the presence of such skewed class distributions remains an open area of research in the machine learning community (e.g. Pazzani et al. (1994), Fawcett (1996), Cardie and Howe (1997), Kubat and Matwin (1997)). Coreference is a discourse-level problem with different solutions for different types of NPs. The interpretation of a pronoun, for example, may be dependent only on its closest antecedent and not on the rest of the members of the same coreference chain. Proper name resolution, on the other hand, may be better served by ignoring locality constraints altogether and relying on string-matching or more sophisticated aliasing techniques. Consequently, generating positive instances from all pairs of NPs from the same coreference chain can potentially make the learning task</context>
</contexts>
<marker>Cardie, Howe, 1997</marker>
<rawString>C. Cardie and N. Howe. 1997. Improving minority class prediction using case-specific feature weights. In Proceedings of the Fourteenth International Conference on Machine Learning, pages 57–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>K Wagstaff</author>
</authors>
<title>Noun Phrase Coreference as Clustering.</title>
<date>1999</date>
<booktitle>In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99),</booktitle>
<pages>82--89</pages>
<contexts>
<context position="23421" citStr="Cardie and Wagstaff (1999)" startWordPosition="3852" endWordPosition="3855"> of examples with a lot of exceptions. Consequently, it is necessary to know which rules are worthy of keeping at the clustering level and not the classification level. We will address this issue in the next section. 5 Pruning the Coreference Ruleset As noted in the introduction, machine learning approaches to coreference resolution that rely only on pairwise NP coreference classifiers will not necessarily enforce the transitivity constraint inherent in the coreference relation. Although approaches to coreference resolution that rely only on clustering could easily enforce transitivity (as in Cardie and Wagstaff (1999)), they have not performed as well as state-of-the-art approaches to coreference. In this section, we propose a method for resolving this conflict: we introduce an error-driven rule pruning algorithm that considers rules induced by the coreference classifier and discards those that cause the ruleset to perform poorly with respect to the global, clustering-level coreference scoring function. The Algorithm. The error-driven pruning algorithm is inspired by the backward elimination algorithm commonly used for feature selection (see Blum and Langley (1997)) and is shown in Figure 3. The algorithm,</context>
</contexts>
<marker>Cardie, Wagstaff, 1999</marker>
<rawString>C. Cardie and K. Wagstaff. 1999. Noun Phrase Coreference as Clustering. In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99), pages 82– 89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
</authors>
<title>Fast Effective Rule Induction.</title>
<date>1995</date>
<booktitle>In Proceedings of the Twelfth International Conference on Machine Learning,</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="8324" citStr="Cohen, 1995" startWordPosition="1296" endWordPosition="1297">an instance such that NP✞ precedes NP✟ .5 Following previous work (Aone and Bennett (1995), 5By definition, exactly valid instances can be created from NPs in a given document. Soon et al. (2001)), we assume throughout the paper that only valid instances will be generated and used for training and testing. Each instance consists of 25 features, which are described in Table 1.6 The classification associated with a training instance is one of COREFERENT or NOT COREFERENT depending on whether the NPs co-refer in the associated training text.7 Building an NP coreference classifier. We use RIPPER (Cohen, 1995), an information gain-based propositional rule learning system, to train a classifier that, given a test instance , decides whether or not NP✞ and NP✟ are coreferent. Specifically, RIPPER sequentially covers the positive training instances and induces a ruleset that determines when two NPs are coreferent. When none of the rules in the ruleset is applicable to a given NP pair, a default rule that classifies the pair as not coreferent is automatically invoked. The output of the classifier is either COREFERENT or NOT COREFERENT along with a number between 0 and 1 that indicates the confidence of </context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>W. Cohen. 1995. Fast Effective Rule Induction. In Proceedings of the Twelfth International Conference on Machine Learning, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
</authors>
<title>A generalization of Bayesian inference.</title>
<date>1968</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>30</volume>
<pages>247</pages>
<contexts>
<context position="3099" citStr="Dempster, 1968" startWordPosition="468" endWordPosition="469">on is viewed as anaphora resolution, i.e. the goal during clustering is to find an antecedent for each anaphoric NP in a document.3 Three intrinsic properties of coreference4, however, make the formulation of the problem as a classification-based single-link clustering task potentially undesirable: Coreference is a rare relation. That is, most NP pairs in a document are not coreferent. Con1Two NPs are in the same coreference chain if and only if they are coreferent. 2One exception is Kehler’s work on probabilistic coreference (Kehler, 1997), in which he applies Dempster’s Rule of Combination (Dempster, 1968) to combine all pairwise probabilities of coreference to form a partition. 3In this paper, we consider an NP anaphoric if it is part of a coreference chain but is not the head of the chain. 4Here, we use the term coreference loosely to refer to either the problem or the binary relation defined on a set of NPs. The particular choice should be clear from the context. sequently, generating training instances by pairing each NP with each of its preceding NPs creates highly skewed class distributions, in which the number of positive instances is overwhelmed by the number of negative instances. For </context>
</contexts>
<marker>Dempster, 1968</marker>
<rawString>A. Dempster. 1968. A generalization of Bayesian inference. Journal of the Royal Statistical Society, 30:205– 247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fawcett</author>
</authors>
<title>Learning with skewed class distributions — summary of responses.</title>
<date>1996</date>
<journal>Machine Learning List:</journal>
<volume>8</volume>
<contexts>
<context position="3993" citStr="Fawcett (1996)" startWordPosition="618" endWordPosition="619">lation defined on a set of NPs. The particular choice should be clear from the context. sequently, generating training instances by pairing each NP with each of its preceding NPs creates highly skewed class distributions, in which the number of positive instances is overwhelmed by the number of negative instances. For example, the standard MUC-6 and MUC-7 (1995; 1998) coreference data sets contain only 2% positive instances. Unfortunately, learning in the presence of such skewed class distributions remains an open area of research in the machine learning community (e.g. Pazzani et al. (1994), Fawcett (1996), Cardie and Howe (1997), Kubat and Matwin (1997)). Coreference is a discourse-level problem with different solutions for different types of NPs. The interpretation of a pronoun, for example, may be dependent only on its closest antecedent and not on the rest of the members of the same coreference chain. Proper name resolution, on the other hand, may be better served by ignoring locality constraints altogether and relying on string-matching or more sophisticated aliasing techniques. Consequently, generating positive instances from all pairs of NPs from the same coreference chain can potentiall</context>
</contexts>
<marker>Fawcett, 1996</marker>
<rawString>T. Fawcett. 1996. Learning with skewed class distributions — summary of responses. Machine Learning List: Vol. 8, No. 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D F Gordon</author>
<author>D Perlis</author>
</authors>
<title>Explicitly biased generalization.</title>
<date>1989</date>
<journal>Computational Intelligence,</journal>
<pages>5--67</pages>
<contexts>
<context position="10464" citStr="Gordon and Perlis (1989)" startWordPosition="1653" endWordPosition="1656">sed in the machine learning literature, most of which modify the learn6See Ng and Cardie (2002) for a detailed description of the features. 7In all of the work presented here, NPs are identified, and feature values computed entirely automatically. Algorithm NEG-SELECT(NEG: set of all possible negative instances) for NEG do if NP✟ is anaphoric then if NP✞ precedes (NP✟ ) then NEG := NEG else NEG := NEG return NEG Figure 1: The NEG-SELECT algorithm ing algorithm to incorporate a loss function with a much larger penalty for minority class errors than for instances from the majority classes (e.g. Gordon and Perlis (1989), Pazzani et al. (1994)). We investigate here a different approach to handling skewed class distributions — negative sample selection, i.e. the selection of a smaller subset of negative instances from the set of available negative instances. In the case of NP coreference, we hypothesize that reducing the number of negative instances will improve recall but potentially reduce precision: intuitively, the existence of fewer negative instances should allow RIPPER to more liberally induce positive rules. We propose a method for negative sample selection that, for each anaphoric NP, NP✟ , retains on</context>
</contexts>
<marker>Gordon, Perlis, 1989</marker>
<rawString>D. F. Gordon and D. Perlis. 1989. Explicitly biased generalization. Computational Intelligence, 5:67–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>R Bunescu</author>
<author>S Maiorano</author>
</authors>
<title>Text and Knowledge Mining for Coreference Resolution.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North America Chapter of the Associationfor Computational Linguistics (NAACL-2001),</booktitle>
<pages>55--62</pages>
<contexts>
<context position="6114" citStr="Harabagiu et al. (2001)" startWordPosition="945" endWordPosition="948">First, to address the problem of skewed class distributions, we apply a technique for negative instance selection similar to that proposed in Soon et al. (2001). In contrast to results reported there, however, we show empirically that system performance increases noticeably in response to negative example selection, with increases in F-measure of 3-5%. Second, in an attempt to avoid the inclusion of “hard” training instances, we present a corpus-based method for implicit selection of positive instances. The approach is a fully automated variant of the example selection algorithm introduced in Harabagiu et al. (2001). With positive example selection, system performance (F-measure) again increases, by 12-14%. Finally, to more tightly tie the classification- and clustering-level coreference decisions, we propose an error-driven rule pruning algorithm that optimizes the coreference classifier ruleset with respect to the clustering-level coreference scoring function. Overall, the use of pruning boosts system performance from an F-measure of 69.3 to 69.5, and from 57.2 to 63.4 for the MUC-6 and MUC-7 data sets, respectively, enabling the system to achieve performance that surpasses that of the best MUC corefer</context>
<context position="16939" citStr="Harabagiu et al. (2001)" startWordPosition="2814" endWordPosition="2817">h larger drops in precision.9 The resulting F-measure scores, however, increase nontrivially from 52.4 to 55.2 (for MUC-6), and from 41.3 to 46.0 (for MUC-7).10 4 Positive Sample Selection Since not all of the coreference relationships derived from coreference chains are equally easy to identify, training a classifier using all possible coreference relationships can potentially lead to the induction of inaccurate rules. Given the observation that one antecedent is sufficient to resolve an anaphor, it may be desirable to learn only from easy positive instances. Similar observations are made by Harabagiu et al. (2001), who point out that intelligent selection of positive instances can potentially minimize the amount of knowledge required to perform coreference resolution accurately. They assume that the easiest types of coreference relationships to resolve are those that occur with high frequencies in the data. Consequently, they mine by hand three sets of coreference rules for covering positive instances from the training data by finding the coreference knowledge satisfied by the largest number of anaphor-antecedent pairs. While the Harabagiu et al. algorithm attempts to mine easy coreference rules from t</context>
</contexts>
<marker>Harabagiu, Bunescu, Maiorano, 2001</marker>
<rawString>S. Harabagiu, R. Bunescu, and S. Maiorano. 2001. Text and Knowledge Mining for Coreference Resolution. In Proceedings of the Second Meeting of the North America Chapter of the Associationfor Computational Linguistics (NAACL-2001), pages 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
</authors>
<title>Probabilistic Coreference in Information Extraction.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>163--173</pages>
<contexts>
<context position="3030" citStr="Kehler, 1997" startWordPosition="458" endWordPosition="459">f the single-link clustering algorithm is that coreference resolution is viewed as anaphora resolution, i.e. the goal during clustering is to find an antecedent for each anaphoric NP in a document.3 Three intrinsic properties of coreference4, however, make the formulation of the problem as a classification-based single-link clustering task potentially undesirable: Coreference is a rare relation. That is, most NP pairs in a document are not coreferent. Con1Two NPs are in the same coreference chain if and only if they are coreferent. 2One exception is Kehler’s work on probabilistic coreference (Kehler, 1997), in which he applies Dempster’s Rule of Combination (Dempster, 1968) to combine all pairwise probabilities of coreference to form a partition. 3In this paper, we consider an NP anaphoric if it is part of a coreference chain but is not the head of the chain. 4Here, we use the term coreference loosely to refer to either the problem or the binary relation defined on a set of NPs. The particular choice should be clear from the context. sequently, generating training instances by pairing each NP with each of its preceding NPs creates highly skewed class distributions, in which the number of positi</context>
</contexts>
<marker>Kehler, 1997</marker>
<rawString>A. Kehler. 1997. Probabilistic Coreference in Information Extraction. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 163–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kubat</author>
<author>S Matwin</author>
</authors>
<title>Addressing the Curse of Imbalanced Training Sets: One-Sided Selection.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th International Conference on Machine Learning (ICML-97),</booktitle>
<pages>179--186</pages>
<contexts>
<context position="4042" citStr="Kubat and Matwin (1997)" startWordPosition="624" endWordPosition="627">icular choice should be clear from the context. sequently, generating training instances by pairing each NP with each of its preceding NPs creates highly skewed class distributions, in which the number of positive instances is overwhelmed by the number of negative instances. For example, the standard MUC-6 and MUC-7 (1995; 1998) coreference data sets contain only 2% positive instances. Unfortunately, learning in the presence of such skewed class distributions remains an open area of research in the machine learning community (e.g. Pazzani et al. (1994), Fawcett (1996), Cardie and Howe (1997), Kubat and Matwin (1997)). Coreference is a discourse-level problem with different solutions for different types of NPs. The interpretation of a pronoun, for example, may be dependent only on its closest antecedent and not on the rest of the members of the same coreference chain. Proper name resolution, on the other hand, may be better served by ignoring locality constraints altogether and relying on string-matching or more sophisticated aliasing techniques. Consequently, generating positive instances from all pairs of NPs from the same coreference chain can potentially make the learning task harder: all but a few co</context>
</contexts>
<marker>Kubat, Matwin, 1997</marker>
<rawString>M. Kubat and S. Matwin. 1997. Addressing the Curse of Imbalanced Training Sets: One-Sided Selection. In Proceedings of the 14th International Conference on Machine Learning (ICML-97), pages 179–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCarthy</author>
<author>W Lehnert</author>
</authors>
<title>Using Decision Trees for Coreference Resolution.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Artificial Intelligence,</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="1541" citStr="McCarthy and Lehnert (1995)" startWordPosition="217" endWordPosition="220">d 63.4 on the MUC6 and MUC-7 coreference resolution data sets, respectively, surpassing the performance of the best MUC-6 and MUC-7 coreference systems. In particular, the system outperforms the best-performing learning-based coreference system to date. 1 Introduction Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document. Machine learning approaches to this problem have been reasonably successful, operating primarily by recasting the problem as a classi�cation task (e.g. Aone and Bennett (1995), McCarthy and Lehnert (1995), Soon et al. (2001)). Specifically, an inductive learning algorithm is used to train a classifier that decides whether or not two NPs in a document are coreferent. Training data are typically created by relying on coreference chains from the training documents: training instances are generated by pairing each NP with each of its preceding NPs; instances are labeled as positive if the two NPs are in the same coreference chain, and labeled as negative otherwise.1 A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference classification decisions and constru</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>J. McCarthy and W. Lehnert. 1995. Using Decision Trees for Coreference Resolution. In Proceedings of the Fourteenth International Conference on Artificial Intelligence, pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference (MUC-6).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="1512" citStr="(1995)" startWordPosition="216" endWordPosition="216"> 69.5 and 63.4 on the MUC6 and MUC-7 coreference resolution data sets, respectively, surpassing the performance of the best MUC-6 and MUC-7 coreference systems. In particular, the system outperforms the best-performing learning-based coreference system to date. 1 Introduction Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document. Machine learning approaches to this problem have been reasonably successful, operating primarily by recasting the problem as a classi�cation task (e.g. Aone and Bennett (1995), McCarthy and Lehnert (1995), Soon et al. (2001)). Specifically, an inductive learning algorithm is used to train a classifier that decides whether or not two NPs in a document are coreferent. Training data are typically created by relying on coreference chains from the training documents: training instances are generated by pairing each NP with each of its preceding NPs; instances are labeled as positive if the two NPs are in the same coreference chain, and labeled as negative otherwise.1 A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference classif</context>
<context position="3749" citStr="(1995; 1998)" startWordPosition="581" endWordPosition="582"> of coreference to form a partition. 3In this paper, we consider an NP anaphoric if it is part of a coreference chain but is not the head of the chain. 4Here, we use the term coreference loosely to refer to either the problem or the binary relation defined on a set of NPs. The particular choice should be clear from the context. sequently, generating training instances by pairing each NP with each of its preceding NPs creates highly skewed class distributions, in which the number of positive instances is overwhelmed by the number of negative instances. For example, the standard MUC-6 and MUC-7 (1995; 1998) coreference data sets contain only 2% positive instances. Unfortunately, learning in the presence of such skewed class distributions remains an open area of research in the machine learning community (e.g. Pazzani et al. (1994), Fawcett (1996), Cardie and Howe (1997), Kubat and Matwin (1997)). Coreference is a discourse-level problem with different solutions for different types of NPs. The interpretation of a pronoun, for example, may be dependent only on its closest antecedent and not on the rest of the members of the same coreference chain. Proper name resolution, on the other hand, may be </context>
<context position="7802" citStr="(1995)" startWordPosition="1212" endWordPosition="1212">uates the error-driven pruning algorithm. We conclude with future work in section 6. 2 The Machine Learning Framework for Coreference Resolution Our machine learning framework for coreference resolution is a standard combination of classification and clustering, as described above. Creating an instance. An instance in our machine learning framework is a description of two NPs in a document. More formally, let NP✂ be the th NP in document . An instance formed from NP✞ and NP✟ is denoted by . A valid instance is an instance such that NP✞ precedes NP✟ .5 Following previous work (Aone and Bennett (1995), 5By definition, exactly valid instances can be created from NPs in a given document. Soon et al. (2001)), we assume throughout the paper that only valid instances will be generated and used for training and testing. Each instance consists of 25 features, which are described in Table 1.6 The classification associated with a training instance is one of COREFERENT or NOT COREFERENT depending on whether the NPs co-refer in the associated training text.7 Building an NP coreference classifier. We use RIPPER (Cohen, 1995), an information gain-based propositional rule learning system, to train a cla</context>
</contexts>
<marker>1995</marker>
<rawString>MUC-6. 1995. Proceedings of the Sixth Message Understanding Conference (MUC-6). Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-7</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<marker>MUC-7, 1998</marker>
<rawString>MUC-7. 1998. Proceedings of the Seventh Message Understanding Conference (MUC-7). Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9935" citStr="Ng and Cardie (2002)" startWordPosition="1566" endWordPosition="1569">to left. For each pair, a test instance is created as during training and is presented to the coreference classifier. The NP with the highest confidence value among the preceding NPs that are classified as being coreferent with NP✟ is selected as the antecedent of NP✟ ; otherwise, no antecedent is selected for NP✟ . 3 Negative Sample Selection As noted above, skewed class distributions arise when generating all valid instances from the training texts. A number of methods for handling skewed distributions have been proposed in the machine learning literature, most of which modify the learn6See Ng and Cardie (2002) for a detailed description of the features. 7In all of the work presented here, NPs are identified, and feature values computed entirely automatically. Algorithm NEG-SELECT(NEG: set of all possible negative instances) for NEG do if NP✟ is anaphoric then if NP✞ precedes (NP✟ ) then NEG := NEG else NEG := NEG return NEG Figure 1: The NEG-SELECT algorithm ing algorithm to incorporate a loss function with a much larger penalty for minority class errors than for instances from the majority classes (e.g. Gordon and Perlis (1989), Pazzani et al. (1994)). We investigate here a different approach to h</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pazzani</author>
<author>C Merz</author>
<author>P Murphy</author>
<author>K Ali</author>
<author>T Hume</author>
<author>C Brunk</author>
</authors>
<title>Reducing Misclassification Costs.</title>
<date>1994</date>
<booktitle>In Proceedings of the Eleventh International Conference on Machine Learning,</booktitle>
<pages>217--225</pages>
<contexts>
<context position="3977" citStr="Pazzani et al. (1994)" startWordPosition="614" endWordPosition="617">roblem or the binary relation defined on a set of NPs. The particular choice should be clear from the context. sequently, generating training instances by pairing each NP with each of its preceding NPs creates highly skewed class distributions, in which the number of positive instances is overwhelmed by the number of negative instances. For example, the standard MUC-6 and MUC-7 (1995; 1998) coreference data sets contain only 2% positive instances. Unfortunately, learning in the presence of such skewed class distributions remains an open area of research in the machine learning community (e.g. Pazzani et al. (1994), Fawcett (1996), Cardie and Howe (1997), Kubat and Matwin (1997)). Coreference is a discourse-level problem with different solutions for different types of NPs. The interpretation of a pronoun, for example, may be dependent only on its closest antecedent and not on the rest of the members of the same coreference chain. Proper name resolution, on the other hand, may be better served by ignoring locality constraints altogether and relying on string-matching or more sophisticated aliasing techniques. Consequently, generating positive instances from all pairs of NPs from the same coreference chai</context>
<context position="10487" citStr="Pazzani et al. (1994)" startWordPosition="1657" endWordPosition="1660">g literature, most of which modify the learn6See Ng and Cardie (2002) for a detailed description of the features. 7In all of the work presented here, NPs are identified, and feature values computed entirely automatically. Algorithm NEG-SELECT(NEG: set of all possible negative instances) for NEG do if NP✟ is anaphoric then if NP✞ precedes (NP✟ ) then NEG := NEG else NEG := NEG return NEG Figure 1: The NEG-SELECT algorithm ing algorithm to incorporate a loss function with a much larger penalty for minority class errors than for instances from the majority classes (e.g. Gordon and Perlis (1989), Pazzani et al. (1994)). We investigate here a different approach to handling skewed class distributions — negative sample selection, i.e. the selection of a smaller subset of negative instances from the set of available negative instances. In the case of NP coreference, we hypothesize that reducing the number of negative instances will improve recall but potentially reduce precision: intuitively, the existence of fewer negative instances should allow RIPPER to more liberally induce positive rules. We propose a method for negative sample selection that, for each anaphoric NP, NP✟ , retains only those negative insta</context>
</contexts>
<marker>Pazzani, Merz, Murphy, Ali, Hume, Brunk, 1994</marker>
<rawString>M. Pazzani, C. Merz, P. Murphy, K. Ali, T. Hume, and C. Brunk. 1994. Reducing Misclassification Costs. In Proceedings of the Eleventh International Conference on Machine Learning, pages 217–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D C Y Lim</author>
</authors>
<title>A Machine Learning Approach to Coreference Resolution of Noun Phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1561" citStr="Soon et al. (2001)" startWordPosition="221" endWordPosition="224">coreference resolution data sets, respectively, surpassing the performance of the best MUC-6 and MUC-7 coreference systems. In particular, the system outperforms the best-performing learning-based coreference system to date. 1 Introduction Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document. Machine learning approaches to this problem have been reasonably successful, operating primarily by recasting the problem as a classi�cation task (e.g. Aone and Bennett (1995), McCarthy and Lehnert (1995), Soon et al. (2001)). Specifically, an inductive learning algorithm is used to train a classifier that decides whether or not two NPs in a document are coreferent. Training data are typically created by relying on coreference chains from the training documents: training instances are generated by pairing each NP with each of its preceding NPs; instances are labeled as positive if the two NPs are in the same coreference chain, and labeled as negative otherwise.1 A separate clustering mechanism then coordinates the possibly contradictory pairwise coreference classification decisions and constructs a partition on t</context>
<context position="5651" citStr="Soon et al. (2001)" startWordPosition="875" endWordPosition="878">, the clustering mechanism is needed to coordinate these possibly contradictory pairwise classifications. In addition, because the coreference classifiers are trained independent of the clustering algorithm to be used, improvements in classification accuracy do not guarantee corresponding improvements in clustering-level accuracy, i.e. overall performance on the coreference resolution task might not improve. This paper examines each of the above issues. First, to address the problem of skewed class distributions, we apply a technique for negative instance selection similar to that proposed in Soon et al. (2001). In contrast to results reported there, however, we show empirically that system performance increases noticeably in response to negative example selection, with increases in F-measure of 3-5%. Second, in an attempt to avoid the inclusion of “hard” training instances, we present a corpus-based method for implicit selection of positive instances. The approach is a fully automated variant of the example selection algorithm introduced in Harabagiu et al. (2001). With positive example selection, system performance (F-measure) again increases, by 12-14%. Finally, to more tightly tie the classifica</context>
<context position="7907" citStr="Soon et al. (2001)" startWordPosition="1227" endWordPosition="1230">chine Learning Framework for Coreference Resolution Our machine learning framework for coreference resolution is a standard combination of classification and clustering, as described above. Creating an instance. An instance in our machine learning framework is a description of two NPs in a document. More formally, let NP✂ be the th NP in document . An instance formed from NP✞ and NP✟ is denoted by . A valid instance is an instance such that NP✞ precedes NP✟ .5 Following previous work (Aone and Bennett (1995), 5By definition, exactly valid instances can be created from NPs in a given document. Soon et al. (2001)), we assume throughout the paper that only valid instances will be generated and used for training and testing. Each instance consists of 25 features, which are described in Table 1.6 The classification associated with a training instance is one of COREFERENT or NOT COREFERENT depending on whether the NPs co-refer in the associated training text.7 Building an NP coreference classifier. We use RIPPER (Cohen, 1995), an information gain-based propositional rule learning system, to train a classifier that, given a test instance , decides whether or not NP✞ and NP✟ are coreferent. Specifically, RI</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A Machine Learning Approach to Coreference Resolution of Noun Phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="15878" citStr="Vilain et al., 1995" startWordPosition="2641" endWordPosition="2644"> NEGSELECT is designed essentially to compute this set. Next, we examine the effects of this minimalist approach to negative sample selection. Evaluation. We evaluate the coreference system with negative sample selection on the MUC-6 and MUC-7 coreference data sets in each case, training the coreference classifier on the 30 “dry run” texts, and applying the coreference resolution algorithm on the 20–30 “formal evaluation” texts. Results are shown in rows 1 and 2 of Table 2 where performance is reported in terms of recall, precision, and F-measure using the model-theoretic MUC scoring program (Vilain et al., 1995). The Baseline system employs no sample selection, i.e. all available training examples are used. Row 2 shows the performance of the Baseline after incorporating NEGSELECT. With negative sample selection, the percentage of positive instances rises from 2% to 8% for the MUC-6 data set and from 2% to 7% for the MUC-7 data set. For both data sets, we see statistically significant increases in recall and statistically significant, but much larger drops in precision.9 The resulting F-measure scores, however, increase nontrivially from 52.4 to 55.2 (for MUC-6), and from 41.3 to 46.0 (for MUC-7).10 4</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proceedings of the Sixth Message Understanding Conference (MUC-6), pages 45– 52. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>