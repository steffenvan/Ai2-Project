<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.999501">
A Statistical NLP Approach for
Feature and Sentiment Identification from Chinese Reviews
</title>
<author confidence="0.999887">
Zhen Hai1 Kuiyu Chang1 Qinbao Song2 Jung-jae Kim1
</author>
<affiliation confidence="0.999873">
1School of Computer Engineering, Nanyang Technological University, Singapore 639798
</affiliation>
<email confidence="0.777993">
{haiz0001, askychang, jungjae.kim}@ntu.edu.sg
</email>
<affiliation confidence="0.99672">
2Department of Computer Science and Technology, Xi’an Jiaotong University, Xi’an 710049, China
</affiliation>
<email confidence="0.987616">
qbsong@mail.xjtu.edu.cn
</email>
<sectionHeader confidence="0.997296" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999865590909091">
Existing methods for extracting features
from Chinese reviews only use
simplistic syntactic knowledge, while
those for identifying sentiments rely
heavily on a semantic dictionary. In this
paper, we present a systematic technique
for identifying features and sentiments,
using both syntactic and statistical anal-
ysis. We firstly identify candidate fea-
tures using a proposed set of common
syntactic rules. We then prune irrelevant
candidates with topical relevance scores
below a cut-off point. We also propose
an association analysis method based on
likelihood ratio test to infer the polarity
of opinion word. The sentiment of a fea-
ture is finally adjusted by analyzing the
negative modifiers in the local context
of the opinion word. Experimental re-
sults show that our system performs sig-
nificantly better than a well-known opi-
nion mining system.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9976432">
There were 420 million Internet users in China
by the end of June 2010. As a result, online so-
cial media in China has accumulated massive
amount of valuable peer reviews on almost any-
thing. Mining this pool of Chinese reviews to
detect features (e.g. “�J(” mobile phone) and
identify the corresponding sentiment (e.g. posi-
tive, negative) has recently become a hot re-
search area. However, the vast majority of pre-
vious work on feature detection only uses sim-
plistic syntactic natural language processing
(NLP) approaches, while those on sentiment
identification depend heavily on a semantic dic-
tionary. Syntactic approaches are often prone to
errors due to the informal nature of online re-
views. Dictionary-based approaches are more
robust than syntactic approaches, but must be
constantly updated with new terms and expres-
sions, which are constantly evolving in online
reviews.
To overcome these limitations, we propose a
statistical NLP approach for Chinese feature and
sentiment identification. The technique is in fact
the core of our Chinese review mining system,
called Idea Miner or iMiner. Figure 1 shows the
architectural overview of iMiner, which com-
prises five modules, of which Module III (Opi-
nion Feature Detection) and IV (Contextual Sen-
timent Identification) are the main focus of this
paper.
</bodyText>
<figureCaption confidence="0.995561">
Figure 1: Overview of the iMiner system.
</figureCaption>
<sectionHeader confidence="0.999825" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999945285714286">
Qiu et al. (2007) used syntactic analysis to iden-
tify features1 in Chinese sentences, which is
similar to the methods proposed by Zhuang et al.
(2006) and Xia et al. (2007). However, syntactic
analysis alone tends to extract many invalid fea-
tures due to the colloquial nature of online re-
views, which are often abruptly concise or
</bodyText>
<page confidence="0.522475">
1 A feature refers to the subject of an opinion.
</page>
<figure confidence="0.973724">
0 Segmentation, POS Tagging,
and Syntactic Parsing
0 Candidate
Feature Extraction
® Candidate
Feature Pruning
Sentiment
Feature
@ Review Crawling
07 Feature-Sentiment Summary
05 Polarity Inference
for Opinion Word
© Contextual
Sentiment Identification
</figure>
<bodyText confidence="0.9994206">
grammatically incorrect. To address the issue,
our approach employs an additional step to
prune candidates with low topical relevance,
which is a statistical measure of how frequently
a term appears in one review and across differ-
ent reviews.
Pang et al. (2002) examined the effectiveness
of using supervised learning methods to identify
document level sentiments. But the technique
requires a large amount of training data, and
must be re-trained whenever it is applied to a
new domain. Furthermore, it does not perform
well at the sentence level. Zhou et al. (2008)
and Qiu et al. (2008) proposed dictionary-based
approaches to infer contextual sentiments from
Chinese sentences. However, it is difficult to
maintain an up-to-date dictionary, as new ex-
pressions emerge frequently online. In contrast,
to identify the sentiment expressed in a review
region2, our method first infers the polarity of
an opinion word by using statistical association
analysis, and subsequently analyzes the local
context of the opinion word. Our method is do-
main independent and uses only a small set of
80 polarized words instead of a huge dictionary.
</bodyText>
<subsectionHeader confidence="0.998144">
2.1 Topic Detection and Tracking
</subsectionHeader>
<bodyText confidence="0.9999593">
The task of Topic Detection and Tracking is to
find and follow new events in a stream of news
stories. Fukumoto and Suzuki (2000) proposed
a domain dependence criterion to discriminate a
topic from an event, and find all subsequent
similar news stories. Our idea of topical relev-
ance is related but different; we only focus on
the relevance of a candidate feature with respect
to a review topic, so as to extract the features on
which sentiments are expressed.
</bodyText>
<subsectionHeader confidence="0.999074">
2.2 Polarity Inference for Opinion Word
</subsectionHeader>
<bodyText confidence="0.9992157">
Turney (2002) used point-wise mutual informa-
tion (PMI) to predict the polarity of an opinion
word O, which is calculated as MI1-MI2, where
MI1 is the mutual information between word O
and positive word “excellent”, and MI2 denotes
the mutual information between O and negative
word “poor”. Instead of PMI, our method uses
the likelihood ratio test (LRT) to compute the
semantic association between an opinion word
and each seed word, since LRT leads to better
</bodyText>
<footnote confidence="0.7346915">
2A review region is a sentence or clause which con-
tains one and only feature.
</footnote>
<bodyText confidence="0.99584725">
results in practice. Finally, the polarity is calcu-
lated as the weighted sum of the polarity values
of all seed words, where the weights are deter-
mined by the semantic association.
</bodyText>
<subsectionHeader confidence="0.999476">
2.3 Feature-Sentiment Pair Identification
</subsectionHeader>
<bodyText confidence="0.999886142857143">
Turney (2002) proposed an unsupervised learn-
ing algorithm to identify the overall sentiments
of reviews. However, his method does not
detect features to associate with the sentiments.
Shi and Chang (2006) proposed to build a huge
Chinese semantic lexicon to extract both fea-
tures and sentiments. Other lexicon-based work
for identifying feature-sentiment pair was pro-
posed by Yi et al. (2003) and Xia et al. (2007).
We propose a new statistical NLP approach to
identify feature-sentiment pairs, which uses not
only syntactic analysis but also data-centric sta-
tistical analysis. Most importantly, our approach
requires no semantic lexicon to be maintained.
</bodyText>
<sectionHeader confidence="0.99417" genericHeader="method">
3 Feature Detection
</sectionHeader>
<bodyText confidence="0.99314025">
Module Ⅲ in iMiner aims to detect opinion
features, which are subjects of reviews, such as
the product itself like “手机” (mobile phone) or
specific attributes like “屏幕” (screen).
Example 1: “我喜欢这款手机的颜色” (I like
the color of this mobile phone).
In example 1, the noun “颜色” (color) indi-
cates a feature. Some features are expressed
implicitly in review sentences, as shown below.
Example 2: “太贵了,我买不起” (Too expen-
sive, I cannot afford it).
In example 2, the noun “价格” (price) is the
opinion feature of this sentence, but it does not
occur explicitly. In this paper, we do not deal
with implicit features, but instead focus on the
extraction of explicit features only.
</bodyText>
<subsectionHeader confidence="0.99939">
3.1 Candidate Feature Extraction
</subsectionHeader>
<bodyText confidence="0.9997052">
According to our observation, features are gen-
erally expressed as nouns and occur in certain
patterns in Chinese reviews. Typically, a noun
acting as the object or subject of a verb is a po-
tential feature. In addition, when a clause con-
tains only a noun phrase without any verbs, the
headword of the noun phrase is also a candidate.
Due to the colloquial nature of online reviews, it
is complicated and nearly impossible to collect
all possible syntactic roles of features. Thus, we
</bodyText>
<tableCaption confidence="0.995799">
Table 1: Dependence relations and syntactic rules for candidate feature extraction.
</tableCaption>
<table confidence="0.72001">
Relation Rule Interpretation Example (3-5)
VOB ( N , VOB ) ⇒ ( N , C ) If term is noun (1V) and “我喜欢这款手机” (I like the mobile
</table>
<figureCaption confidence="0.96722495">
depends on another com-
ponent with relation VOB,
extract as candidate (C).
phone). The noun “手机” relies on the
word “喜欢” with relation VOB, thus,
“手机” is extracted as candidate.
SBV ( N , SBV ) ⇒ ( N , C ) If term is noun (1V) and “屏 幕 太小 了 ” (The screen is too
depends on another com-
ponent with relation SBV,
extract as candidate (C).
small). The noun “屏幕” depends on
the word “小” with relation SBV, thus
“屏幕” is extracted as candidate.
HED ( N , HED ) ⇒ ( N , C ) If term is noun (1V) and “漂亮的 外观” (beautiful exterior).
governs another compo-
nent with relation HED,
extract as candidate (C).
The noun “外观” governs the word
“漂亮” with relation HED, thus, “外
观” is extracted as candidate.
</figureCaption>
<bodyText confidence="0.999786588235294">
only use the aforementioned three primary pat-
terns to extract an initial set of candidates.
Dependence Grammar (Tesniere, 1959) expl-
ores asymmetric governor-dependent relation-
ship between words, which are then combined
into the dependency structure of sentences. The
three dependency relations SBV, VOB, and
HED correspond to the three aforementioned
patterns. For each relation, we define a rule with
additional restrictions for candidate feature ex-
traction, as shown in Table 1.
Candidate features are extracted in the fol-
lowing manner: for each word, we first deter-
mine if it is a noun; if so, we apply the VOB,
SBV, and HED rules sequentially. A noun
matching any of the rules is extracted as a can-
didate feature.
</bodyText>
<subsectionHeader confidence="0.999847">
3.2 Candidate Feature Pruning
</subsectionHeader>
<bodyText confidence="0.999969035714286">
Due to the informal nature of online reviews, a
large number of irrelevant candidates are ex-
tracted by the three syntactic rules. Thus, we
need to further prune them by using additional
techniques.
Intuitively, candidates that are found in many
reviews should be more representative com-
pared to candidates that occur in only a few re-
views. This characteristic of candidates can be
captured by the topical relevance (TR) score.
TR can be used to measure how strongly a can-
didate feature is relevant to a review topic. The
TR of a candidate is described by two indica-
tors, i.e., dispersion and deviation. Dispersion
indicates how frequently a candidate occurs
across different reviews, while deviation de-
notes how many times a candidate appears in
one review. The topical relevance score (TRS)
is calculated by combining both dispersion and
deviation. Candidate features with high TRS are
supposed to be highly relevant, while those with
TRS lower than a specified threshold are re-
jected.
Formally, let the i-th candidate feature be de-
noted by Ti, and the j-th review document3 by
Dj. The weight of feature Ti in document Dj is
denoted by Wij, which could be computed based
on TF.IDF (Luhn, 1957) shown in formula (1):
</bodyText>
<equation confidence="0.86500775">
N - -- -
⎧⎪⎨ (1
=
⎪⎩0
</equation>
<bodyText confidence="0.999981166666667">
TFij denotes the term frequency of Ti in Dj, and
DFi denotes the document frequency of Ti; N
indicates the number of documents in the cor-
pus. We compute the standard deviation Si:
where the average weight of Ti across all docu-
ments is calculated as follows:
</bodyText>
<equation confidence="0.892214666666667">
1 N
W = ∑ W j
i i
N j=1
The dispersion Dispi of Ti is then calculated:
Wi
Disp = (3)
i
Si
</equation>
<bodyText confidence="0.585180666666667">
The deviation Deviij of Ti in Dj is computed:
3 A review document refers to a forum review, which
tends to be shorter than full length editorial articles.
</bodyText>
<equation confidence="0.997084909090909">
N
( W − Wi )2 ij(2)
N
Si =
j = 1
1
−
Wij
otherwise
.
Devi ij = W — W (4)
</equation>
<bodyText confidence="0.9988268">
The average scalar weight of all candidate fea-
tures in Dj is calculated as follows:
where M is the vocabulary size of Dj.
We can obtain the topical relevance score
TRSij of Ti in Dj finally as follows:
</bodyText>
<equation confidence="0.885957">
TRSij = Dispi *Deviij (5)
</equation>
<bodyText confidence="0.99986425">
By combining the dispersion and deviation,
the quantity TRSij thus captures the topical re-
levance strength of Ti with respect to the topic of
document Dj.
All candidates of a document are then sorted
in descending order of TRS, and those with
TRS above a pre-specified threshold are ex-
tracted as opinion features. In fact, we can ex-
tract candidates at the document, paragraph, or
sentence resolution. In practice, we observe no
significant performance differences at the vari-
ous resolutions.
</bodyText>
<subsectionHeader confidence="0.991326">
3.3 Experimental Evaluation
</subsectionHeader>
<bodyText confidence="0.999359238095238">
We collected 2,986 real-life review documents
about mobile phones from major online Chinese
forums. Each document corresponds to a forum
topic, where each paragraph in a document
matches a thread under the topic. Of these, we
manually annotated the features and sentiment
orientations expressed in 219 randomly selected
documents, which include 600 review sentences.
To evaluate the performance of our approach,
we first conducted an experiment for extracting
candidate features. We then performed three
other experiments for pruning the candidates at
the document, paragraph, and sentence levels,
respectively. For each experiment, we tried sev-
eral different thresholds, i.e., percentage of TRS
mean (TRSM) of all candidates. The average F-
measure (F), precision (P), and recall (R) of the
results at the three levels are shown in Figure 2.
The highest F-measure results of feature detec-
tion with and without pruning are shown in Ta-
ble 2 for easy comparison.
</bodyText>
<tableCaption confidence="0.981337">
Table 2: Feature detection results.
</tableCaption>
<table confidence="0.996912333333333">
Feature Detection P (%) R (%) F (%)
No Pruning 71.61 90.69 80.03
Pruning (33% TRSM) 81.56 85.22 83.35
</table>
<bodyText confidence="0.998961444444444">
As line 2 of Table 2 shows, feature detection
without pruning achieves 90.69% recall, which
shows that the proposed syntactic rules have
excellent coverage. However, its precision is not
so promising, achieving only 71.61%, which
means that many irrelevant candidates are also
extracted by our rules. Thus, relying on syntac-
tic analysis alone is not good enough, and we
need to take one more step to prune the candi-
date features.
As line 3 of Table 2 shows, after pruning the
candidate set, precision improved remarkably
by 10% to 81.56%, while recall dropped slightly
to 85.22%. For online review mining, precision
is much more important than recall, because
users’ confidence in iMiner rely heavily on the
accuracy of the results they see (precision), and
not on what they don’t see (recall).
</bodyText>
<figureCaption confidence="0.796383">
Figure 2: iMiner feature pruning results.
Figure 2 plots the results of pruning at vari-
ous TRSM thresholds. The best F-measure of
</figureCaption>
<bodyText confidence="0.9840009">
83.35% was achieved with a 33% TRSM. If we
increase the threshold to 43%, the precision in-
creases to 83.19%, while the recall drops to
81.57%. By exploring the distribution of a can-
didate in corpus, its topical relevance with re-
spect to the review topic can be measured statis-
tically, which allows the noisy candidates to be
pruned effectively. From the results in Figure 2,
our idea of topical relevance is shown to be
highly effective in detecting features.
</bodyText>
<tableCaption confidence="0.99844">
Table 3: Characteristics of FBS and iMiner.
</tableCaption>
<table confidence="0.999861454545455">
Aspects FBS iMiner
Candidates Nouns from Nouns from syn-
POS tagger thetic analysis
Pruning Association Topical
Mining Relevance
Opinion word Adjectives Adjectives, verbs
Polarity Dictionary LRT association
inference based based
Sentiment Sentence Sentence, clause
Resolution
Negation Single Single, double
</table>
<equation confidence="0.5757544">
1
M
W&apos;= ∑ Wij
1
M i=
</equation>
<bodyText confidence="0.998055882352941">
We compared our results with that of the as-
sociation mining-based method in Feature-based
Summarization (FBS) (Hu and Liu, 2004) on
the same dataset. Table 3 summarizes the dif-
ferences between FBS and iMiner, parts of
which are elaborated in Section 4. The results of
FBS with various support thresholds are shown
in Figure 3. The support corresponds to the per-
centage of total number of review sentences.
FBS attained the highest F-measure of 76.35%
at a support of 0.4% with 79.6% precision and
73.36% recall. As the support increases, the
precision also increases from 62.99% to 86.92%,
while the recall decreases from 91.61% to
61.86%. Comparing the best results of the two
systems, iMiner beats FBS by 7% in F-Measure,
1.96% in precision, and 11.86% in recall.
</bodyText>
<figureCaption confidence="0.869463">
Figure 3: FBS feature extraction results.
</figureCaption>
<bodyText confidence="0.99959925">
We find that FBS suffers from the following
limitations: (1) FBS extracted an additional
14.11% noisy candidate features due to the lack
of syntactic analysis, which requires more ex-
tensive pruning; and (2) FBS only considers
sentence frequency in computing the support to
identify frequent candidate features, ignoring
the candidate frequency within the sentence.
</bodyText>
<subsectionHeader confidence="0.959482">
3.4 Feature Extraction Error Analysis
</subsectionHeader>
<bodyText confidence="0.969237243902439">
We categorize our feature extraction errors
into 4 main types, FE1 to FE4, as follows.
FE1: When more than one candidate exists in
a review region, our algorithm may pick the
wrong features due to misplaced priorities. Note
that we assume only one (dominant) feature per
region in both our algorithm and the labeled
dataset. A total of 43% errors were due to pick-
ing the wrong dominant candidate.
Example 6: “声音太小,让人听不清楚” (The
sound is too weak, people cannot listen clear-
ly).
In example 6, both “声音” and “人” are ex-
tracted as features. However, the noun “人” is
an incorrect feature detected by our algorithm.
FE2: The proposed set of common syntactic
rules is not comprehensive, missing out 23% of
true features.
Example 7: “对于这个机子我很讨厌” (I am
sick about this phone).
In example 7, the noun “机子” is a missed
feature. This is in fact a POB dependence rela-
tion, which is outside the scope of our three
rules.
FE3: About 22% errors are due to irrelevant
features possessing high TR scores, and there-
fore which are not pruned subsequently.
Example 8: “我好喜欢的哦没有钱买” (I like it
very much, but I have no money to buy it).
In example 8, the noun “钱” is incorrectly
confirmed as a feature due to its high TR score.
FE4: About 9% errors are due to incorrect
POS tags.
Example 9: “听电话时它老卡” (Consistent
interruption during phone calls).
In example 9, the verb “卡” is extracted in-
correctly as a feature, since it is incorrectly
tagged as a noun. The remaining 3% of the er-
rors are due to the system incorrectly extracting
features from sentences that contain no opi-
nions.
</bodyText>
<sectionHeader confidence="0.993668" genericHeader="method">
4 Contextual Sentiment Identification
</sectionHeader>
<bodyText confidence="0.99999125">
The main task of module Ⅳ in iMiner is to iden-
tify the contextual sentiment of a feature. A
two-step approach is proposed: (1) The polarity
of an opinion word within a review region is
inferred via association analysis based on the
likelihood ratio test; and (2) the sentiment is
validated against the contextual information of
the opinion word in the region and finalized.
</bodyText>
<subsectionHeader confidence="0.992514">
4.1 Polarity Inference for Opinion Word
</subsectionHeader>
<bodyText confidence="0.999448615384615">
To infer polarity, an opinion word is first identi-
fied in a review region, as described in Figure 4.
Note that we consider not only adjectives but
also verbs as opinion words. We then measure
the association between the opinion word and
each seed word. We calculate the polarity value
of the opinion word as the association weighted
sum of polarities of all seed words.
Example 10: “这款机子价格很便宜” (The
price of this mobile phone is very cheap).
Example 10 contains an adjective “便 宜”
(cheap) that governs the feature “价格” (price);
thus “便宜” is extracted as an opinion word.
</bodyText>
<listItem confidence="0.99989">
1. feature Ti and word Wj in the same region
2. if (Wj = adjective and depends on Ti)
3. extract Wj as opinion word;
4. else if (Wj = adjective and governs Ti)
5. extract Wj as opinion word;
6. else if (Wj = verb and governs Ti)
7. extract Wj as opinion word;
</listItem>
<figureCaption confidence="0.998231">
Figure 4: Extracting Opinion Word
</figureCaption>
<bodyText confidence="0.999947909090909">
A set of polarized words were collected from
corpus as seed words, including 35 positive
words, 36 negative words, and 9 neutral words.
Each seed word is assigned a polarity weight
from -10 to 10. For example, “漂亮” (lovely)
has a score of 10, “普通” (common) has a score
of 0, and “差劲” (lousy) has a score of -10.
To measure the semantic association Aij be-
tween an opinion word Oi and each seed word Sj,
we propose a formula based on the likelihood
ratio test (Dunning, 1993), as follows:
</bodyText>
<equation confidence="0.999447">
Aij = 2[ log L(p1,k1,n1)+ log L(p2,k2,n2) (6)
− log L(p,k1,n1)− log L(p,k2,n2) ]
</equation>
<bodyText confidence="0.851072">
where
</bodyText>
<equation confidence="0.9777848">
L(p, k, n) = pk (1− p)n−k ;
k k
1 + 2
n1 +n2
n1=k1+k3, n2=k2+k4.
</equation>
<bodyText confidence="0.998794857142857">
The variable k1(O, S) in Table 4 refers to the
count of documents containing⎯ both opinion
word O and seed word S, k2(O, S) indicates the
number of documents containing O but not S,
k3(⎯O, S) counts the number of documents con-
taining S but not O, while k4(⎯O,⎯ S) tallies the
count of documents containing neither O nor S.
</bodyText>
<tableCaption confidence="0.990992">
Table 4: Document counts.
</tableCaption>
<table confidence="0.946944">
S ⎯S
O k1 (O, S) k2 (O,⎯ S)
⎯O k3 (⎯O, S) k4 (⎯O,⎯ S)
</table>
<bodyText confidence="0.9988525">
The higher the quantity Aij, the stronger the
semantic association is between the opinion
word and the seed word.
The polarity value OVi of the opinion word Oi
is computed as the association weighted average
of all seed word polarity values:
</bodyText>
<equation confidence="0.928130545454545">
OV = ∑ ij * SV
i j
j = A
LA
1 i (7)
The sum Ai of all association strength is calcu-
lated as follows:
L
A A
i = ∑ j i
j
</equation>
<bodyText confidence="0.99992725">
where Aij denotes the association between Oi
and Sj, SVj indicates the polarity value of Sj, and
L is the size of the seed word list.
After performing association analysis, we
then classify the polarity value OVi using an
upper bound V+ and lower bound V-, such that
if Vi is larger than V+, then the polarity is in-
ferred as positive; conversely if Vi is smaller
than V-, then the polarity is inferred as negative;
otherwise, it is neutral. Here, the V+ and V-
boundaries refer to thresholds that can be de-
termined experimentally.
</bodyText>
<subsectionHeader confidence="0.997424">
4.2 Contextual Sentiment Identification
</subsectionHeader>
<bodyText confidence="0.999881961538462">
Apart from inferring the polarity of opinion
words, we also examine additional contextual
information around the opinion words. In fact,
the final sentiment is determined by combining
the polarity with the contextual information. In
this work, we focus on negative modifiers, as
shown in the examples below.
Example 11: “我不喜欢这款手机” (I do not
like this mobile phone).
In example 11, the polarity of the opinion
word “喜欢” (like) is inferred as positive, but
the review region expresses a negative orienta-
tion to the feature “手机”, because a negation
word “不” (not) modifies “喜欢”. Thus, it is
important to locate negative modifiers.
Example 12: “手机屏幕不是不漂亮” (The
screen of this mobile phone is not unlovely).
In example 12, the polarity of opinion word
“漂亮” (lovely) is inferred as positive. By ex-
amining its direct modifier, i.e., “不” (un-), we
identify the sentiment of “不漂亮” (unlovely) as
negative. However, the final sentiment about the
feature “屏幕” (screen) is actually positive due
to the earlier negation “不是” (not), which mod-
ifies the latter “不漂亮” (unlovely). This is what
we call a double negation sentence, which is not
</bodyText>
<equation confidence="0.946339666666666">
p
, p1 k1 , p2 = k2
n1 n2
;
1
=
</equation>
<bodyText confidence="0.999954769230769">
uncommon in reviews. Therefore, it is necessary
to take two additional steps to capture the
double negation as follows.
Figure 5 shows the main steps of identifying
contextual sentiment. For an opinion word O; in
the review region, we first determine if there
exists an adverb modifying it. If so, we extract
the adverb as the direct modifier. If the modifier
has a negative meaning, then we reverse the
prior polarity of O;. Similarly, we can take one
additional step to locate the double negation
modifier and finally identify the contextual sen-
timent orientation.
</bodyText>
<listItem confidence="0.9878212">
1. for each opinion word O;
2. if (a word Wj = adverb and depends on O;)
3. extract Wj as direct modifier;
4. if (word Wj = negation word)
5. reverse the prior polarity of O;;
6. if (word Wk = adverb and relies on Wj)
7. extract Wk as indirect modifier;
8. if (word Wk = negation word)
9. reverse the current polarity of O;;
10. output the current polarity of O;;
</listItem>
<figureCaption confidence="0.710243">
Figure 5: Identifying the Contextual Sentiment
</figureCaption>
<subsectionHeader confidence="0.991946">
4.3 Experimental Evaluation
</subsectionHeader>
<bodyText confidence="0.999717">
Since features are detected prior to the senti-
ments, there is a possibility for an erroneous
feature (i.e., a false positive feature) to be asso-
ciated with a sentiment. We thus conducted two
different experiments. In the first case, we enu-
merate all extracted feature-sentiment pairs,
including the wrong features. In the second sce-
nario, we enumerate the feature-sentiment pairs
only for those correctly extracted features. For
each experiment, we further evaluated the result
with (C) and without (N.C.) contextual informa-
tion.
We select the best case of feature detection
and then run our sentiment identification algo-
rithm on the review dataset described in section
3.3; the polarity thresholds V- and V+ are set to
0.45 and 0.5, respectively.
</bodyText>
<tableCaption confidence="0.732606">
Table 5: Results for all features.
</tableCaption>
<table confidence="0.4982596">
Systems P (%) R (%) F (%)
iMiner N.C. 57.07 58.21 57.63
N.C.
70.3 71.72 71
FBS 49.70 45.80 47.67
</table>
<bodyText confidence="0.992224571428572">
Table 5 shows the results for all detected fea-
tures (correct and incorrect). As shown in line 2,
our method achieved an F-measure of 57.63%
without considering contextual information,
while precision and recall are 57% and 58.21%,
respectively. Adding contextual information, as
line 3 shows, boosts the F-measure to 71%, a
remarkable 13.37% improvement.
Table 6 shows the results for just the correct-
ly extracted features. As shown in line 2, in the
case of not considering contextual information,
our method achieved an F-measure of 63.17%,
while precision and recall were 69.05% and
58.21%, respectively. By considering contextual
information, line 3 shows that the F-measure
improved to 77.82% which is 14.65% better,
with precision and recall at 85.06% and 71.72%,
respectively. The above results show that local
contextual analysis of double and single nega-
tion can significantly improve the accuracy of
sentiment orientation identification.
</bodyText>
<tableCaption confidence="0.925454">
Table 6: Results for correctly detected features.
</tableCaption>
<table confidence="0.9501794">
Systems P (%) R (%) F (%)
iMiner N.C. 69.05 58.21 63.17
N.C.
85.06 71.72 77.82
FBS 62.45 45.80 52.84
</table>
<bodyText confidence="0.99994244117647">
By examining the results shown in line 3 (in
bold) of both Tables 5 and 6, the F-measure on
correctly identified features increases from 71%
to 77.82%, while the precision increases drasti-
cally from 70.3% to 85.06%. The results show
that our two-step approach of identifying senti-
ment orientation is reasonable and effective and
that a great many of sentiments can be identified
correctly for related features, especially for
those correctly detected one. However, in prac-
tice there is no way to tell the correctly identi-
fied features from the incorrect ones, thus Table
5 is a more realistic gauge of our approach..
Lastly, we compared our approach to senti-
ment identification with FBS (see Table 3). The
best results are used, as shown in the last rows
of Table 5 and 6. When considering all features
extracted, the F-measure of FBS is only 47.67%,
which is 23.33% lower than that of iMiner,
where both precision and recall are 49.70% and
45.80%, respectively. Considering only the cor-
rectly detected features, iMiner widens its lead
over FBS to 25% in terms of F-measure.
There are several explanations for the poor
results of FBS: (1) The inferior results of feature
detection affect the subsequent task of sentiment
identification; and (2) the polarity inference de-
pends heavily on a semantic dictionary Word-
Net. In our experiments for FBS, we used an
extended version of the “同义AA林” Thesau-
rus containing 77,492 words, and a sentiment
lexicon with 8,856 words that is part of mini
(free) HowNet, and lastly our seed word list
containing 80 words.
</bodyText>
<subsectionHeader confidence="0.98937">
4.4 Sentiment Identification Error Analysis
</subsectionHeader>
<bodyText confidence="0.995107304347826">
We classify our sentiment identification er-
rors into 5 main types, SE1 to SE5, as follows.
SE1: Sentiment identification relies heavily
on feature extraction, which means that if fea-
tures are detected wrongly, it is impossible for
the sentiment identified to be correct. About
49% of false sentiments are due to incorrectly
extracted features.
Even for the correctly extracted features,
there are still several errors as listed below.
SE2: Incorrectly identified opinion words can
lead to mistakes in inferring sentiments, ac-
counting for 14% of the errors.
SE3: Errors in detecting contextual informa-
tion about opinion words led to 12% of the
wrong sentiment identification results.
SE4: Both the quality and quantity of seed
words influence sentiment identification.
SE5: The threshold choices for V+ and V- di-
rectly impact the polarity inference of opinion
words, affecting the sentiment identification.
SE4 and SE5 errors account for the remaining
25% of the erroneous sentiment results.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999981888888889">
The main contribution of this paper is the pro-
posed systematic technique of identifying both
features and sentiments for Chinese reviews.
Our proposed approach compares very favora-
bly against the well-known FBS system on a
small-scale dataset. Our feature detection is 7%
better than FBS in terms of F-measure, with
significantly higher recall. Meanwhile, our ap-
proach of identifying contextual sentiment
achieved around 23% better F-measure than
FBS.
We plan to further explore effective methods
to deal with the various feature and sentiment
errors. In addition, we plan to explore the ex-
traction of implicit features, since a significant
number of reviews express opinion via implicit
features. Lastly, we plan to test out these im-
provements on a large-scale dataset.
</bodyText>
<sectionHeader confidence="0.974855" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.998719">
We thank Harbin Institute of Technology’s Cen-
ter for Information Retrieval in providing their
Language Technology Platform (LTP) software.
This research was supported in part by Singa-
pore Ministry of Education’s Academic Re-
search Fund Tier 1 grant RG 30/09.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999612046511628">
Dunning, T. E. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics
19(1).
Fukumoto, Fumiyo, and Yoshimi Suzuki. 2000. Event
Tracking based on Domain Dependence, SIGIR.
Hu, Minqing, and Bing Liu. 2004. Mining and summariz-
ing customer reviews, SIGKDD, Seattle, WA, USA.
Luhn, Hans Peter. 1957. A statistical approach tomecha-
nized encoding and searching of literary information.
IBM Journal of Research and Development 1 (4):309-
17.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using Ma-
chine Learning Techniques, EMNLP.
Qiu, Guang, Kangmiao Liu, Jiajun Bu, Chun Chen, and
Zhiming Kang. 2007. Extracting opinion topics for
Chinese opinions using dependence grammar,
ADKDD, California, USA.
Qiu, Guang, Can Wang, Jiajun Bu, Kangmiao Liu, and
Chun Chen. 2008. Incorporate the Syntactic Knowledge
in Opinion Mining in User-generated Content, WWW,
Beijing, China.
Shi, Bin, and Kuiyu Chang. 2006. Mining Chinese Re-
views, ICDM Data Mining on Design and Marketing
Workshop.
Tesniere, L. 1959. Elements de Syntaxe Structurale: Li-
brairie C. Klincksieck, Paris.
Turney, Peter D. 2002. Thumbs Up or Thumbs Down?
Semantic Orientation Applied to Unsupervised Classi-
fication of Reviews, ACL, Philadelphia.
Xia, Yunqing, Ruifeng Xu, Kam-Fai Wong, and Fang
Zheng. 2007. The Unified Collocation Framework for
Opinion Mining. International Conference on Machine
Learning and Cybernetics.
Yi, Jeonghee, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment Analyzer: Extracting
Sentiments about a Given Topic using Natural Lan-
guage Processing Techniques, ICDM.
Zhou, Chao, Guang Qiu, Kangmiao Liu, Jiajun Bu, Ming-
cheng Qu, and Chun Chen. 2008. SOPING : a Chinese
customer review mining system, SIGIR, Singapore.
Zhuang, Li, Feng Jing, and Xiaoyan Zhu. 2006. Movie
Review Mining and Summarization, CIKM.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.783595">
<title confidence="0.999884">A Statistical NLP Approach for Feature and Sentiment Identification from Chinese Reviews</title>
<author confidence="0.997257">Kuiyu Qinbao Jung-jae</author>
<affiliation confidence="0.987804">of Computer Engineering, Nanyang Technological University, Singapore</affiliation>
<address confidence="0.88241">haiz0001, askychang, of Computer Science and Technology, Xi’an Jiaotong University, Xi’an 710049,</address>
<email confidence="0.955017">qbsong@mail.xjtu.edu.cn</email>
<abstract confidence="0.999197347826087">Existing methods for extracting features from Chinese reviews only use simplistic syntactic knowledge, while those for identifying sentiments rely heavily on a semantic dictionary. In this paper, we present a systematic technique for identifying features and sentiments, using both syntactic and statistical analysis. We firstly identify candidate features using a proposed set of common syntactic rules. We then prune irrelevant candidates with topical relevance scores below a cut-off point. We also propose an association analysis method based on likelihood ratio test to infer the polarity of opinion word. The sentiment of a feature is finally adjusted by analyzing the negative modifiers in the local context of the opinion word. Experimental results show that our system performs significantly better than a well-known opinion mining system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T E Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="19157" citStr="Dunning, 1993" startWordPosition="3204" endWordPosition="3205">ns Ti) 5. extract Wj as opinion word; 6. else if (Wj = verb and governs Ti) 7. extract Wj as opinion word; Figure 4: Extracting Opinion Word A set of polarized words were collected from corpus as seed words, including 35 positive words, 36 negative words, and 9 neutral words. Each seed word is assigned a polarity weight from -10 to 10. For example, “漂亮” (lovely) has a score of 10, “普通” (common) has a score of 0, and “差劲” (lousy) has a score of -10. To measure the semantic association Aij between an opinion word Oi and each seed word Sj, we propose a formula based on the likelihood ratio test (Dunning, 1993), as follows: Aij = 2[ log L(p1,k1,n1)+ log L(p2,k2,n2) (6) − log L(p,k1,n1)− log L(p,k2,n2) ] where L(p, k, n) = pk (1− p)n−k ; k k 1 + 2 n1 +n2 n1=k1+k3, n2=k2+k4. The variable k1(O, S) in Table 4 refers to the count of documents containing⎯ both opinion word O and seed word S, k2(O, S) indicates the number of documents containing O but not S, k3(⎯O, S) counts the number of documents containing S but not O, while k4(⎯O,⎯ S) tallies the count of documents containing neither O nor S. Table 4: Document counts. S ⎯S O k1 (O, S) k2 (O,⎯ S) ⎯O k3 (⎯O, S) k4 (⎯O,⎯ S) The higher the quantity Aij, th</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, T. E. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics 19(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fumiyo Fukumoto</author>
<author>Yoshimi Suzuki</author>
</authors>
<title>Event Tracking based on Domain Dependence,</title>
<date>2000</date>
<location>SIGIR.</location>
<contexts>
<context position="4553" citStr="Fukumoto and Suzuki (2000)" startWordPosition="700" endWordPosition="703">sentences. However, it is difficult to maintain an up-to-date dictionary, as new expressions emerge frequently online. In contrast, to identify the sentiment expressed in a review region2, our method first infers the polarity of an opinion word by using statistical association analysis, and subsequently analyzes the local context of the opinion word. Our method is domain independent and uses only a small set of 80 polarized words instead of a huge dictionary. 2.1 Topic Detection and Tracking The task of Topic Detection and Tracking is to find and follow new events in a stream of news stories. Fukumoto and Suzuki (2000) proposed a domain dependence criterion to discriminate a topic from an event, and find all subsequent similar news stories. Our idea of topical relevance is related but different; we only focus on the relevance of a candidate feature with respect to a review topic, so as to extract the features on which sentiments are expressed. 2.2 Polarity Inference for Opinion Word Turney (2002) used point-wise mutual information (PMI) to predict the polarity of an opinion word O, which is calculated as MI1-MI2, where MI1 is the mutual information between word O and positive word “excellent”, and MI2 denot</context>
</contexts>
<marker>Fukumoto, Suzuki, 2000</marker>
<rawString>Fukumoto, Fumiyo, and Yoshimi Suzuki. 2000. Event Tracking based on Domain Dependence, SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews, SIGKDD,</title>
<date>2004</date>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="14727" citStr="Hu and Liu, 2004" startWordPosition="2432" endWordPosition="2435">ly. From the results in Figure 2, our idea of topical relevance is shown to be highly effective in detecting features. Table 3: Characteristics of FBS and iMiner. Aspects FBS iMiner Candidates Nouns from Nouns from synPOS tagger thetic analysis Pruning Association Topical Mining Relevance Opinion word Adjectives Adjectives, verbs Polarity Dictionary LRT association inference based based Sentiment Sentence Sentence, clause Resolution Negation Single Single, double 1 M W&apos;= ∑ Wij 1 M i= We compared our results with that of the association mining-based method in Feature-based Summarization (FBS) (Hu and Liu, 2004) on the same dataset. Table 3 summarizes the differences between FBS and iMiner, parts of which are elaborated in Section 4. The results of FBS with various support thresholds are shown in Figure 3. The support corresponds to the percentage of total number of review sentences. FBS attained the highest F-measure of 76.35% at a support of 0.4% with 79.6% precision and 73.36% recall. As the support increases, the precision also increases from 62.99% to 86.92%, while the recall decreases from 91.61% to 61.86%. Comparing the best results of the two systems, iMiner beats FBS by 7% in F-Measure, 1.96</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Hu, Minqing, and Bing Liu. 2004. Mining and summarizing customer reviews, SIGKDD, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Peter Luhn</author>
</authors>
<title>A statistical approach tomechanized encoding and searching of literary information.</title>
<date>1957</date>
<journal>IBM Journal of Research and Development</journal>
<volume>1</volume>
<pages>4--309</pages>
<contexts>
<context position="10378" citStr="Luhn, 1957" startWordPosition="1691" endWordPosition="1692">sion and deviation. Dispersion indicates how frequently a candidate occurs across different reviews, while deviation denotes how many times a candidate appears in one review. The topical relevance score (TRS) is calculated by combining both dispersion and deviation. Candidate features with high TRS are supposed to be highly relevant, while those with TRS lower than a specified threshold are rejected. Formally, let the i-th candidate feature be denoted by Ti, and the j-th review document3 by Dj. The weight of feature Ti in document Dj is denoted by Wij, which could be computed based on TF.IDF (Luhn, 1957) shown in formula (1): N - -- - ⎧⎪⎨ (1 = ⎪⎩0 TFij denotes the term frequency of Ti in Dj, and DFi denotes the document frequency of Ti; N indicates the number of documents in the corpus. We compute the standard deviation Si: where the average weight of Ti across all documents is calculated as follows: 1 N W = ∑ W j i i N j=1 The dispersion Dispi of Ti is then calculated: Wi Disp = (3) i Si The deviation Deviij of Ti in Dj is computed: 3 A review document refers to a forum review, which tends to be shorter than full length editorial articles. N ( W − Wi )2 ij(2) N Si = j = 1 1 − Wij otherwise .</context>
</contexts>
<marker>Luhn, 1957</marker>
<rawString>Luhn, Hans Peter. 1957. A statistical approach tomechanized encoding and searching of literary information. IBM Journal of Research and Development 1 (4):309-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using</title>
<date>2002</date>
<booktitle>Machine Learning Techniques, EMNLP.</booktitle>
<contexts>
<context position="3517" citStr="Pang et al. (2002)" startWordPosition="534" endWordPosition="537"> of online reviews, which are often abruptly concise or 1 A feature refers to the subject of an opinion. 0 Segmentation, POS Tagging, and Syntactic Parsing 0 Candidate Feature Extraction ® Candidate Feature Pruning Sentiment Feature @ Review Crawling 07 Feature-Sentiment Summary 05 Polarity Inference for Opinion Word © Contextual Sentiment Identification grammatically incorrect. To address the issue, our approach employs an additional step to prune candidates with low topical relevance, which is a statistical measure of how frequently a term appears in one review and across different reviews. Pang et al. (2002) examined the effectiveness of using supervised learning methods to identify document level sentiments. But the technique requires a large amount of training data, and must be re-trained whenever it is applied to a new domain. Furthermore, it does not perform well at the sentence level. Zhou et al. (2008) and Qiu et al. (2008) proposed dictionary-based approaches to infer contextual sentiments from Chinese sentences. However, it is difficult to maintain an up-to-date dictionary, as new expressions emerge frequently online. In contrast, to identify the sentiment expressed in a review region2, o</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques, EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Kangmiao Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
<author>Zhiming Kang</author>
</authors>
<title>Extracting opinion topics for Chinese opinions using dependence grammar, ADKDD,</title>
<date>2007</date>
<location>California, USA.</location>
<contexts>
<context position="2641" citStr="Qiu et al. (2007)" startWordPosition="396" endWordPosition="399">s, but must be constantly updated with new terms and expressions, which are constantly evolving in online reviews. To overcome these limitations, we propose a statistical NLP approach for Chinese feature and sentiment identification. The technique is in fact the core of our Chinese review mining system, called Idea Miner or iMiner. Figure 1 shows the architectural overview of iMiner, which comprises five modules, of which Module III (Opinion Feature Detection) and IV (Contextual Sentiment Identification) are the main focus of this paper. Figure 1: Overview of the iMiner system. 2 Related Work Qiu et al. (2007) used syntactic analysis to identify features1 in Chinese sentences, which is similar to the methods proposed by Zhuang et al. (2006) and Xia et al. (2007). However, syntactic analysis alone tends to extract many invalid features due to the colloquial nature of online reviews, which are often abruptly concise or 1 A feature refers to the subject of an opinion. 0 Segmentation, POS Tagging, and Syntactic Parsing 0 Candidate Feature Extraction ® Candidate Feature Pruning Sentiment Feature @ Review Crawling 07 Feature-Sentiment Summary 05 Polarity Inference for Opinion Word © Contextual Sentiment </context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, Kang, 2007</marker>
<rawString>Qiu, Guang, Kangmiao Liu, Jiajun Bu, Chun Chen, and Zhiming Kang. 2007. Extracting opinion topics for Chinese opinions using dependence grammar, ADKDD, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Can Wang</author>
<author>Jiajun Bu</author>
<author>Kangmiao Liu</author>
<author>Chun Chen</author>
</authors>
<date>2008</date>
<booktitle>Incorporate the Syntactic Knowledge in Opinion Mining in User-generated Content,</booktitle>
<location>WWW, Beijing, China.</location>
<contexts>
<context position="3845" citStr="Qiu et al. (2008)" startWordPosition="588" endWordPosition="591">al Sentiment Identification grammatically incorrect. To address the issue, our approach employs an additional step to prune candidates with low topical relevance, which is a statistical measure of how frequently a term appears in one review and across different reviews. Pang et al. (2002) examined the effectiveness of using supervised learning methods to identify document level sentiments. But the technique requires a large amount of training data, and must be re-trained whenever it is applied to a new domain. Furthermore, it does not perform well at the sentence level. Zhou et al. (2008) and Qiu et al. (2008) proposed dictionary-based approaches to infer contextual sentiments from Chinese sentences. However, it is difficult to maintain an up-to-date dictionary, as new expressions emerge frequently online. In contrast, to identify the sentiment expressed in a review region2, our method first infers the polarity of an opinion word by using statistical association analysis, and subsequently analyzes the local context of the opinion word. Our method is domain independent and uses only a small set of 80 polarized words instead of a huge dictionary. 2.1 Topic Detection and Tracking The task of Topic Det</context>
</contexts>
<marker>Qiu, Wang, Bu, Liu, Chen, 2008</marker>
<rawString>Qiu, Guang, Can Wang, Jiajun Bu, Kangmiao Liu, and Chun Chen. 2008. Incorporate the Syntactic Knowledge in Opinion Mining in User-generated Content, WWW, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Shi</author>
<author>Kuiyu Chang</author>
</authors>
<title>Mining Chinese Reviews,</title>
<date>2006</date>
<booktitle>ICDM Data Mining on Design and Marketing Workshop.</booktitle>
<contexts>
<context position="5890" citStr="Shi and Chang (2006)" startWordPosition="919" endWordPosition="922">st (LRT) to compute the semantic association between an opinion word and each seed word, since LRT leads to better 2A review region is a sentence or clause which contains one and only feature. results in practice. Finally, the polarity is calculated as the weighted sum of the polarity values of all seed words, where the weights are determined by the semantic association. 2.3 Feature-Sentiment Pair Identification Turney (2002) proposed an unsupervised learning algorithm to identify the overall sentiments of reviews. However, his method does not detect features to associate with the sentiments. Shi and Chang (2006) proposed to build a huge Chinese semantic lexicon to extract both features and sentiments. Other lexicon-based work for identifying feature-sentiment pair was proposed by Yi et al. (2003) and Xia et al. (2007). We propose a new statistical NLP approach to identify feature-sentiment pairs, which uses not only syntactic analysis but also data-centric statistical analysis. Most importantly, our approach requires no semantic lexicon to be maintained. 3 Feature Detection Module  in iMiner aims to detect opinion features, which are subjects of reviews, such as the product itself like “手机” (mobile </context>
</contexts>
<marker>Shi, Chang, 2006</marker>
<rawString>Shi, Bin, and Kuiyu Chang. 2006. Mining Chinese Reviews, ICDM Data Mining on Design and Marketing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tesniere</author>
</authors>
<title>Elements de Syntaxe Structurale: Librairie C. Klincksieck,</title>
<date>1959</date>
<location>Paris.</location>
<contexts>
<context position="8563" citStr="Tesniere, 1959" startWordPosition="1388" endWordPosition="1389"> , C ) If term is noun (1V) and “屏 幕 太小 了 ” (The screen is too depends on another component with relation SBV, extract as candidate (C). small). The noun “屏幕” depends on the word “小” with relation SBV, thus “屏幕” is extracted as candidate. HED ( N , HED ) ⇒ ( N , C ) If term is noun (1V) and “漂亮的 外观” (beautiful exterior). governs another component with relation HED, extract as candidate (C). The noun “外观” governs the word “漂亮” with relation HED, thus, “外 观” is extracted as candidate. only use the aforementioned three primary patterns to extract an initial set of candidates. Dependence Grammar (Tesniere, 1959) explores asymmetric governor-dependent relationship between words, which are then combined into the dependency structure of sentences. The three dependency relations SBV, VOB, and HED correspond to the three aforementioned patterns. For each relation, we define a rule with additional restrictions for candidate feature extraction, as shown in Table 1. Candidate features are extracted in the following manner: for each word, we first determine if it is a noun; if so, we apply the VOB, SBV, and HED rules sequentially. A noun matching any of the rules is extracted as a candidate feature. 3.2 Candi</context>
</contexts>
<marker>Tesniere, 1959</marker>
<rawString>Tesniere, L. 1959. Elements de Syntaxe Structurale: Librairie C. Klincksieck, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews,</title>
<date>2002</date>
<location>ACL, Philadelphia.</location>
<contexts>
<context position="4938" citStr="Turney (2002)" startWordPosition="766" endWordPosition="767">y a small set of 80 polarized words instead of a huge dictionary. 2.1 Topic Detection and Tracking The task of Topic Detection and Tracking is to find and follow new events in a stream of news stories. Fukumoto and Suzuki (2000) proposed a domain dependence criterion to discriminate a topic from an event, and find all subsequent similar news stories. Our idea of topical relevance is related but different; we only focus on the relevance of a candidate feature with respect to a review topic, so as to extract the features on which sentiments are expressed. 2.2 Polarity Inference for Opinion Word Turney (2002) used point-wise mutual information (PMI) to predict the polarity of an opinion word O, which is calculated as MI1-MI2, where MI1 is the mutual information between word O and positive word “excellent”, and MI2 denotes the mutual information between O and negative word “poor”. Instead of PMI, our method uses the likelihood ratio test (LRT) to compute the semantic association between an opinion word and each seed word, since LRT leads to better 2A review region is a sentence or clause which contains one and only feature. results in practice. Finally, the polarity is calculated as the weighted su</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, Peter D. 2002. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews, ACL, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunqing Xia</author>
<author>Ruifeng Xu</author>
<author>Kam-Fai Wong</author>
<author>Fang Zheng</author>
</authors>
<date>2007</date>
<booktitle>The Unified Collocation Framework for Opinion Mining. International Conference on Machine Learning and Cybernetics.</booktitle>
<contexts>
<context position="2796" citStr="Xia et al. (2007)" startWordPosition="423" endWordPosition="426">e a statistical NLP approach for Chinese feature and sentiment identification. The technique is in fact the core of our Chinese review mining system, called Idea Miner or iMiner. Figure 1 shows the architectural overview of iMiner, which comprises five modules, of which Module III (Opinion Feature Detection) and IV (Contextual Sentiment Identification) are the main focus of this paper. Figure 1: Overview of the iMiner system. 2 Related Work Qiu et al. (2007) used syntactic analysis to identify features1 in Chinese sentences, which is similar to the methods proposed by Zhuang et al. (2006) and Xia et al. (2007). However, syntactic analysis alone tends to extract many invalid features due to the colloquial nature of online reviews, which are often abruptly concise or 1 A feature refers to the subject of an opinion. 0 Segmentation, POS Tagging, and Syntactic Parsing 0 Candidate Feature Extraction ® Candidate Feature Pruning Sentiment Feature @ Review Crawling 07 Feature-Sentiment Summary 05 Polarity Inference for Opinion Word © Contextual Sentiment Identification grammatically incorrect. To address the issue, our approach employs an additional step to prune candidates with low topical relevance, which</context>
<context position="6100" citStr="Xia et al. (2007)" startWordPosition="954" endWordPosition="957"> Finally, the polarity is calculated as the weighted sum of the polarity values of all seed words, where the weights are determined by the semantic association. 2.3 Feature-Sentiment Pair Identification Turney (2002) proposed an unsupervised learning algorithm to identify the overall sentiments of reviews. However, his method does not detect features to associate with the sentiments. Shi and Chang (2006) proposed to build a huge Chinese semantic lexicon to extract both features and sentiments. Other lexicon-based work for identifying feature-sentiment pair was proposed by Yi et al. (2003) and Xia et al. (2007). We propose a new statistical NLP approach to identify feature-sentiment pairs, which uses not only syntactic analysis but also data-centric statistical analysis. Most importantly, our approach requires no semantic lexicon to be maintained. 3 Feature Detection Module  in iMiner aims to detect opinion features, which are subjects of reviews, such as the product itself like “手机” (mobile phone) or specific attributes like “屏幕” (screen). Example 1: “我喜欢这款手机的颜色” (I like the color of this mobile phone). In example 1, the noun “颜色” (color) indicates a feature. Some features are expressed implicitly</context>
</contexts>
<marker>Xia, Xu, Wong, Zheng, 2007</marker>
<rawString>Xia, Yunqing, Ruifeng Xu, Kam-Fai Wong, and Fang Zheng. 2007. The Unified Collocation Framework for Opinion Mining. International Conference on Machine Learning and Cybernetics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeonghee Yi</author>
<author>Tetsuya Nasukawa</author>
<author>Razvan Bunescu</author>
<author>Wayne Niblack</author>
</authors>
<title>Sentiment Analyzer: Extracting Sentiments about a Given Topic using Natural Language Processing Techniques,</title>
<date>2003</date>
<location>ICDM.</location>
<contexts>
<context position="6078" citStr="Yi et al. (2003)" startWordPosition="949" endWordPosition="952"> results in practice. Finally, the polarity is calculated as the weighted sum of the polarity values of all seed words, where the weights are determined by the semantic association. 2.3 Feature-Sentiment Pair Identification Turney (2002) proposed an unsupervised learning algorithm to identify the overall sentiments of reviews. However, his method does not detect features to associate with the sentiments. Shi and Chang (2006) proposed to build a huge Chinese semantic lexicon to extract both features and sentiments. Other lexicon-based work for identifying feature-sentiment pair was proposed by Yi et al. (2003) and Xia et al. (2007). We propose a new statistical NLP approach to identify feature-sentiment pairs, which uses not only syntactic analysis but also data-centric statistical analysis. Most importantly, our approach requires no semantic lexicon to be maintained. 3 Feature Detection Module  in iMiner aims to detect opinion features, which are subjects of reviews, such as the product itself like “手机” (mobile phone) or specific attributes like “屏幕” (screen). Example 1: “我喜欢这款手机的颜色” (I like the color of this mobile phone). In example 1, the noun “颜色” (color) indicates a feature. Some features ar</context>
</contexts>
<marker>Yi, Nasukawa, Bunescu, Niblack, 2003</marker>
<rawString>Yi, Jeonghee, Tetsuya Nasukawa, Razvan Bunescu, and Wayne Niblack. 2003. Sentiment Analyzer: Extracting Sentiments about a Given Topic using Natural Language Processing Techniques, ICDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Zhou</author>
<author>Guang Qiu</author>
<author>Kangmiao Liu</author>
<author>Jiajun Bu</author>
<author>Mingcheng Qu</author>
<author>Chun Chen</author>
</authors>
<title>SOPING : a Chinese customer review mining system, SIGIR,</title>
<date>2008</date>
<contexts>
<context position="3823" citStr="Zhou et al. (2008)" startWordPosition="583" endWordPosition="586">Opinion Word © Contextual Sentiment Identification grammatically incorrect. To address the issue, our approach employs an additional step to prune candidates with low topical relevance, which is a statistical measure of how frequently a term appears in one review and across different reviews. Pang et al. (2002) examined the effectiveness of using supervised learning methods to identify document level sentiments. But the technique requires a large amount of training data, and must be re-trained whenever it is applied to a new domain. Furthermore, it does not perform well at the sentence level. Zhou et al. (2008) and Qiu et al. (2008) proposed dictionary-based approaches to infer contextual sentiments from Chinese sentences. However, it is difficult to maintain an up-to-date dictionary, as new expressions emerge frequently online. In contrast, to identify the sentiment expressed in a review region2, our method first infers the polarity of an opinion word by using statistical association analysis, and subsequently analyzes the local context of the opinion word. Our method is domain independent and uses only a small set of 80 polarized words instead of a huge dictionary. 2.1 Topic Detection and Tracking</context>
</contexts>
<marker>Zhou, Qiu, Liu, Bu, Qu, Chen, 2008</marker>
<rawString>Zhou, Chao, Guang Qiu, Kangmiao Liu, Jiajun Bu, Mingcheng Qu, and Chun Chen. 2008. SOPING : a Chinese customer review mining system, SIGIR, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Zhuang</author>
<author>Feng Jing</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Movie Review Mining and Summarization,</title>
<date>2006</date>
<location>CIKM.</location>
<contexts>
<context position="2774" citStr="Zhuang et al. (2006)" startWordPosition="418" endWordPosition="421">se limitations, we propose a statistical NLP approach for Chinese feature and sentiment identification. The technique is in fact the core of our Chinese review mining system, called Idea Miner or iMiner. Figure 1 shows the architectural overview of iMiner, which comprises five modules, of which Module III (Opinion Feature Detection) and IV (Contextual Sentiment Identification) are the main focus of this paper. Figure 1: Overview of the iMiner system. 2 Related Work Qiu et al. (2007) used syntactic analysis to identify features1 in Chinese sentences, which is similar to the methods proposed by Zhuang et al. (2006) and Xia et al. (2007). However, syntactic analysis alone tends to extract many invalid features due to the colloquial nature of online reviews, which are often abruptly concise or 1 A feature refers to the subject of an opinion. 0 Segmentation, POS Tagging, and Syntactic Parsing 0 Candidate Feature Extraction ® Candidate Feature Pruning Sentiment Feature @ Review Crawling 07 Feature-Sentiment Summary 05 Polarity Inference for Opinion Word © Contextual Sentiment Identification grammatically incorrect. To address the issue, our approach employs an additional step to prune candidates with low to</context>
</contexts>
<marker>Zhuang, Jing, Zhu, 2006</marker>
<rawString>Zhuang, Li, Feng Jing, and Xiaoyan Zhu. 2006. Movie Review Mining and Summarization, CIKM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>