<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9970535">
Partially Supervised Sense Disambiguation by Learning Sense Number
from Tagged and Untagged Corpora
</title>
<author confidence="0.914116">
Zheng-Yu Niu, Dong-Hong Ji
</author>
<affiliation confidence="0.791389">
Institute for Infocomm Research
</affiliation>
<address confidence="0.772775">
21 Heng Mui Keng Terrace
119613 Singapore
</address>
<email confidence="0.995345">
{zniu, dhji}@i2r.a-star.edu.sg
</email>
<author confidence="0.968704">
Chew Lim Tan
</author>
<affiliation confidence="0.9931845">
Department of Computer Science
National University of Singapore
</affiliation>
<sectionHeader confidence="0.7416085" genericHeader="abstract">
3 Science Drive 2
117543 Singapore
</sectionHeader>
<email confidence="0.974862">
tancl@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.997089" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.999359708333334">
Supervised and semi-supervised sense dis-
ambiguation methods will mis-tag the in-
stances of a target word if the senses of
these instances are not defined in sense in-
ventories or there are no tagged instances
for these senses in training data. Here we
used a model order identification method
to avoid the misclassification of the in-
stances with undefined senses by discov-
ering new senses from mixed data (tagged
and untagged corpora). This algorithm
tries to obtain a natural partition of the
mixed data by maximizing a stability cri-
terion defined on the classification result
from an extended label propagation al-
gorithm over all the possible values of
the number of senses (or sense number,
model order). Experimental results on
SENSEVAL-3 data indicate that it outper-
forms SVM, a one-class partially super-
vised classification algorithm, and a clus-
tering based model order identification al-
gorithm when the tagged data is incom-
plete.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98946597826087">
In this paper, we address the problem of partially
supervised word sense disambiguation, which is
to disambiguate the senses of occurrences of a tar-
get word in untagged texts when given incomplete
tagged corpus 1.
Word sense disambiguation can be defined as
associating a target word in a text or discourse
1“incomplete tagged corpus” means that tagged corpus
does not include the instances of some senses for the target
word, while these senses may occur in untagged texts.
with a definition or meaning. Many corpus based
methods have been proposed to deal with the sense
disambiguation problem when given definition for
each possible sense of a target word or a tagged
corpus with the instances of each possible sense,
e.g., supervised sense disambiguation (Leacock et
al., 1998), and semi-supervised sense disambigua-
tion (Yarowsky, 1995).
Supervised methods usually rely on the infor-
mation from previously sense tagged corpora to
determine the senses of words in unseen texts.
Semi-supervised methods for WSD are charac-
terized in terms of exploiting unlabeled data in
the learning procedure with the need of prede-
fined sense inventories for target words. The in-
formation for semi-supervised sense disambigua-
tion is usually obtained from bilingual corpora
(e.g. parallel corpora or untagged monolingual
corpora in two languages) (Brown et al., 1991; Da-
gan and Itai, 1994), or sense-tagged seed examples
(Yarowsky, 1995).
Some observations can be made on the previous
supervised and semi-supervised methods. They
always rely on hand-crafted lexicons (e.g., Word-
Net) as sense inventories. But these resources may
miss domain-specific senses, which leads to in-
complete sense tagged corpus. Therefore, sense
taggers trained on the incomplete tagged corpus
will misclassify some instances if the senses of
these instances are not defined in sense invento-
ries. For example, one performs WSD in informa-
tion technology related texts using WordNet 2 as
sense inventory. When disambiguating the word
“boot” in the phrase “boot sector”, the sense tag-
ger will assign this instance with one of the senses
of “boot” listed in WordNet. But the correct sense
</bodyText>
<note confidence="0.93697">
2Online version of WordNet is available at
http://wordnet.princeton.edu/cgi-bin/webwn2.0
</note>
<page confidence="0.971794">
415
</page>
<note confidence="0.856741">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 415–422,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999899930232558">
“loading operating system into memory” is not in-
cluded in WordNet. Therefore, this instance will
be associated with an incorrect sense.
So, in this work, we would like to study the
problem of partially supervised sense disambigua-
tion with an incomplete sense tagged corpus.
Specifically, given an incomplete sense-tagged
corpus and a large amount of untagged examples
for a target word 3, we are interested in (1) label-
ing the instances in the untagged corpus with sense
tags occurring in the tagged corpus; (2) trying to
find undefined senses (or new senses) of the target
word 4 from the untagged corpus, which will be
represented by instances from the untagged cor-
pus.
We propose an automatic method to estimate
the number of senses (or sense number, model or-
der) of a target word in mixed data (tagged cor-
pus+untagged corpus) by maximizing a stability
criterion defined on classification result over all
the possible values of sense number. At the same
time, we can obtain a classification of the mixed
data with the optimal number of groups. If the es-
timated sense number in the mixed data is equal
to the sense number of the target word in tagged
corpus, then there is no new sense in untagged
corpus. Otherwise new senses will be represented
by groups in which there is no instance from the
tagged corpus.
This partially supervised sense disambiguation
algorithm may help enriching manually compiled
lexicons by inducing new senses from untagged
corpora.
This paper is organized as follows. First, a
model order identification algorithm will be pre-
sented for partially supervised sense disambigua-
tion in section 2. Section 3 will provide experi-
mental results of this algorithm for sense disam-
biguation on SENSEVAL-3 data. Then related
work on partially supervised classification will be
summarized in section 4. Finally we will conclude
our work and suggest possible improvements in
section 5.
</bodyText>
<sectionHeader confidence="0.9961515" genericHeader="method">
2 Partially Supervised Word Sense
Disambiguation
</sectionHeader>
<bodyText confidence="0.853834">
The partially supervised sense disambiguation
problem can be generalized as a model order iden-
</bodyText>
<footnote confidence="0.98505175">
3Untagged data usually includes the occurrences of all the
possible senses of the target word
4“undefined senses” are the senses that do not appear in
tagged corpus.
</footnote>
<bodyText confidence="0.99988552631579">
tification problem. We try to estimate the sense
number of a target word in mixed data (tagged cor-
pus+untagged corpus) by maximizing a stability
criterion defined on classification results over all
the possible values of sense number. If the esti-
mated sense number in the mixed data is equal to
the sense number in the tagged corpus, then there
is no new sense in the untagged corpus. Other-
wise new senses will be represented by clusters in
which there is no instance from the tagged corpus.
The stability criterion assesses the agreement be-
tween classification results on full mixed data and
sampled mixed data. A partially supervised clas-
sification algorithm is used to classify the full or
sampled mixed data into a given number of classes
before the stability assessment, which will be pre-
sented in section 2.1. Then we will provide the
details of the model order identification procedure
in section 2.2.
</bodyText>
<subsectionHeader confidence="0.7603">
2.1 An Extended Label Propagation
Algorithm
</subsectionHeader>
<tableCaption confidence="0.959219">
Table 1: Extended label propagation algorithm.
</tableCaption>
<bodyText confidence="0.5600121">
Function: ELP(DL, DU, k, Y0DL+DU )
Input: labeled examples DL, unlabeled
examples DU, model order k, initial
labeling matrix Y0DL+DU ;
Output: the labeling matrix YDU on DU;
1 If k &lt; kXL then
YDU=NULL;
2 Else if k = kXL then
Run plain label propagation algorithm
on DU with YDU as output;
</bodyText>
<sectionHeader confidence="0.990994" genericHeader="method">
3 Else then
</sectionHeader>
<subsectionHeader confidence="0.8903828">
3.1 Estimate the size of tagged data set
of new classes;
3.2 Generate tagged examples from DU
for (kXL + 1)-th to k-th new classes;
3.3 Run plain label propagation algorithm
</subsectionHeader>
<bodyText confidence="0.9820075">
on DU with augmented tagged dataset
as labeled data;
</bodyText>
<footnote confidence="0.318415">
3.4 YDU is the output from plain label
propagation algorithm;
End if
</footnote>
<sectionHeader confidence="0.978221" genericHeader="method">
4 Return YDU;
</sectionHeader>
<bodyText confidence="0.999719">
Let XL+U = {xi}ni�1 be a set of contexts of
occurrences of an ambiguous word w, where xi
represents the context of the i-th occurrence, and n
is the total number of this word’s occurrences. Let
</bodyText>
<page confidence="0.998427">
416
</page>
<bodyText confidence="0.997711636363636">
SL = {sj}cj=1 denote the sense tag set of w in XL,
where XL denotes the first l examples xg(1 &lt; g &lt;
l) that are labeled as yg (yg E SL). Let XU denote
other u (l + u = n) examples xh(l + 1 &lt; h &lt; n)
that are unlabeled.
Let Y0XL+U E N|XL+U |x|SL |represent initial
soft labels attached to tagged instances, where
Y0XL+U,ij = 1 if yi is sj and 0 otherwise. LetY0XL
be the top l rows of Y0XL+U and Y0XU be the remain-
ing u rows.Y0XL is consistent with the labeling in
labeled data, and the initialization of Y0XU can be
arbitrary.
Let k denote the possible value of the number
of senses in mixed data XL+U, and kXL be the
number of senses in initial tagged data XL. Note
that kXL = |SL|, and k &gt; kXL.
The classification algorithm in the order identi-
fication process should be able to accept labeled
data DL 5, unlabeled data DU 6 and model order k
as input, and assign a class label or a cluster index
to each instance in DU as output. Previous super-
vised or semi-supervised algorithms (e.g. SVM,
label propagation algorithm (Zhu and Ghahra-
mani, 2002)) cannot classify the examples in DU
into k groups if k &gt; kXL. The semi-supervised k-
means clustering algorithm (Wagstaff et al., 2001)
may be used to perform clustering analysis on
mixed data, but its efficiency is a problem for clus-
tering analysis on a very large dataset since multi-
ple restarts are usually required to avoid local op-
tima and multiple iterations will be run in each
clustering process for optimizing a clustering so-
lution.
In this work, we propose an alternative method,
an extended label propagation algorithm (ELP),
which can classify the examples in DU into k
groups. If the value of k is equal to kXL, then
ELP is identical with the plain label propagation
algorithm (LP) (Zhu and Ghahramani, 2002). Oth-
erwise, if the value of k is greater than kXL, we
perform classification by the following steps:
(1) estimate the dataset size of each new class as
sizenew class by identifying the examples of new
classes using the “Spy” technique 7 and assuming
</bodyText>
<footnote confidence="0.796873">
5DL may be the dataset XL or a subset sampled from XL.
6DU may be the dataset XU or a subset sampled from
XU.
7The “Spy” technique was proposed in (Liu et al., 2003).
Our re-implementation of this technique consists of three
</footnote>
<listItem confidence="0.798887869565217">
steps: (1) sample a small subset DsL with the size 15%xADLA
from DL; (2) train a classifier with tagged data DL − DsL;
(3) classify DU and DsL, and then select some examples from
DU as the dataset of new classes, which have the classifica-
that new classes are equally distributed;
(2)DL = DL, D�U = DU;
(3) remove tagged examples of the m-th new
class (kXL + 1 &lt; m &lt; k) from D�L 8 and train a
classifier on this labeled dataset without the m-th
class;
(4) the classifier is then used to classify the ex-
amples in D�U;
(5) the least confidently unlabeled point
xclass m E D�U, together with its label m, is added
to the labeled data D�L = D�L + xclass m, and
DU = DU − xclass m;
(6) steps (3) to (5) are repeated for each new
class till the augmented tagged data set is large
enough (here we try to select sizenew class/4 ex-
amples with their sense tags as tagged data for
each new class);
(7) use plain LP algorithm to classify remaining
unlabeled data D�U with D�L as labeled data.
</listItem>
<bodyText confidence="0.864400444444444">
Table 1 shows this extended label propagation
algorithm.
Next we will provide the details of the plain la-
bel propagation algorithm.
2
Define Wij = exp(− a2) if i 7� j and Wii = 0
(1 &lt; i, j &lt; |DL + DU|), where dij is the distance
(e.g., Euclidean distance) between the example xi
and xj, and Q is used to control the weight Wij.
</bodyText>
<equation confidence="0.7753">
Define |DL + DU |x |DL + DU |probability
transition matrix Tij = P(j —* i) = r , ,
L-�k= W�j
</equation>
<bodyText confidence="0.989143555555556">
where Tij is the probability to jump from example
xj to example xi.
Compute the row-normalized matrix T by
Tij = Tij/ Enk=1 Tik.
The classification solution is obtained by
YDU = (I − Tuu)−1TulY 0 DL. I is |DU |x |DU|
identity matrix. Tuu and Tul are acquired by split-
ting matrix T after the |DL|-th row and the |DL|-th
column into 4 sub-matrices.
</bodyText>
<subsectionHeader confidence="0.999895">
2.2 Model Order Identification Procedure
</subsectionHeader>
<bodyText confidence="0.992199583333333">
For achieving the model order identification (or
sense number estimation) ability, we use a clus-
ter validation based criterion (Levine and Domany,
2001) to infer the optimal number of senses of w
in XL+U.
tion confidence less than the average of that in DsL. Classifi-
cation confidence of the example xi is defined as the absolute
value of the difference between two maximum values from
the i-th row in labeling matrix.
8Initially there are no tagged examples for the m-th class
in D�L. Therefore we do not need to remove tagged examples
for this new class, and then directly train a classifier with D
</bodyText>
<page confidence="0.775664">
L.
417
</page>
<tableCaption confidence="0.468800428571429">
Table 2: Model order evaluation algorithm.
Function: CV(XL+U, k, q, Y0XL+U)
Input: data set XL+U, model order k,
and sampling frequency q;
Output: the score of the merit of k;
1 Run the extended label propagation
algorithm with XL, XU, k and Y0XL+U ;
2 Construct connectivity matrix Ck based
on above classification solution on XU;
3 Use a random predictor Pk to assign
uniformly drawn labels to each vector
in XU;
4 Construct connectivity matrix Cρk using
above classification solution on XU;
</tableCaption>
<figure confidence="0.89084475">
5 For p = 1 to q do
5.1 Randomly sample a subset XµL+U with
the size α|XL+U |from XL+U, 0 &lt; α &lt; 1;
5.2 Run the extended label propagation
algorithm with XµL, XµU, k and Y 0µ;
5.3 Construct connectivity matrix Cµk using
above classification solution on XµU;
5.4 Use Pk to assign uniformly drawn labels
to each vector in XµU;
5.5 Construct connectivity matrix Cµρk using
above classification solution on XµU;
Endfor
</figure>
<sectionHeader confidence="0.519859" genericHeader="method">
6 Evaluate the merit of k using following
formula:
</sectionHeader>
<bodyText confidence="0.755151666666667">
Mk = 1 �µ(M(Cµ k , Ck) − M(Cµ ρk, Cρk)),
q
where M(Cµ, C) is given by equation (2);
</bodyText>
<sectionHeader confidence="0.975857" genericHeader="method">
7 Return Mk;
</sectionHeader>
<bodyText confidence="0.9986705">
Then this model order identification procedure
can be formulated as:
</bodyText>
<equation confidence="0.995679333333333">
ˆkXL+U =argmaxK—:n&lt;k&lt;K—..{CV(XL+U, k, q, Y0
XL+U)}.
(1)
</equation>
<bodyText confidence="0.993018526315789">
�kXL+U is the estimated sense number in XL+U,
Kmin (or Kmax) is the minimum (or maximum)
value of sense number, and k is the possible value
of sense number in XL+U. Note that k &gt; kXL.
Then we set Kmin = kXL. Kmax may be set as a
value greater than the possible ground-truth value.
CV is a cluster validation based evaluation func-
tion. Table 2 shows the details of this function.
We set q, the resampling frequency for estimation
of stability score, as 20. α is set as 0.90. The ran-
dom predictor assigns uniformly distributed class
labels to each instance in a given dataset. We
run this CV procedure for each value of k. The
value of k that maximizes this function will be se-
lected as the estimation of sense number. At the
same time, we can obtain a partition of XL+U with
�kXL+U groups.
The function M(Cµ, C) in Table 2 is given by
(Levine and Domany, 2001):
</bodyText>
<equation confidence="0.901776333333333">
Ei,j 1{Cµi,j = Ci,j = 1, xi, xj E XµU}
/moi,j 1{Ci,j = 1, xi, xj E XµU} ,
(2)
</equation>
<bodyText confidence="0.995083277777778">
where XµU is the untagged data in XµL+U, XµL+U
is a subset with the size α|XL+U |(0 &lt; α &lt; 1)
sampled from XL+U, C or Cµ is |XU |x |XU |or
|XµU |x |XµU |connectivity matrix based on classi-
fication solutions computed on XU or XµU respec-
tively. The connectivity matrix C is defined as:
Ci,j = 1 if xi and xj belong to the same cluster,
otherwise Ci,j = 0. Cµ is calculated in the same
way.
M(Cµ, C) measures the proportion of example
pairs in each group computed on XU that are also
assigned into the same group by the classification
solution on XµU. Clearly, 0 &lt; M &lt; 1. Intu-
itively, if the value of k is identical with the true
value of sense number, then classification results
on the different subsets generated by sampling
should be similar with that on the full dataset. In
the other words, the classification solution with the
true model order as parameter is robust against re-
sampling, which gives rise to a local optimum of
M(Cµ, C).
In this algorithm, we normalize M(Cµk , Ck) by
the equation in step 6 of Table 2, which makes
our objective function different from the figure of
merit (equation ( 2)) proposed in (Levine and Do-
many, 2001). The reason to normalize M(Cµk , Ck)
is that M(Cµk , Ck) tends to decrease when increas-
ing the value of k (Lange et al., 2002). Therefore
for avoiding the bias that the smaller value of k
is to be selected as the model order, we use the
cluster validity of a random predictor to normalize
M(Cµk , Ck).
If �kXL+U is equal to kXL, then there is no new
sense in XU. Otherwise (�kXL+U &gt; kXL) new
senses of w may be represented by the groups in
which there is no instance from XL.
</bodyText>
<sectionHeader confidence="0.998042" genericHeader="evaluation">
3 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.984261">
3.1 Experiment Design
</subsectionHeader>
<bodyText confidence="0.999909333333333">
We evaluated the ELP based model order iden-
tification algorithm on the data in English lexi-
cal sample task of SENSEVAL-3 (including all
</bodyText>
<equation confidence="0.972025">
M(Cµ, C) =
</equation>
<page confidence="0.998793">
418
</page>
<tableCaption confidence="0.97759225">
Table 3: Description of The percentage of official
training data used as tagged data when instances
with different sense sets are removed from official
training data.
</tableCaption>
<table confidence="0.839729875">
The percentage of official
training data used as tagged data
Ssubset = {s1} 42.8%
Ssubset = {s2} 76.7%
Ssubset = {s3} 89.1%
Ssubset = {s1, s2} 19.6%
Ssubset = {s1, s3} 32.0%
Ssubset = {s2, s3} 65.9%
</table>
<bodyText confidence="0.992463911764706">
the 57 English words ) 9, and further empirically
compared it with other state of the art classifi-
cation methods, including SVM 10 (the state of
the art method for supervised word sense disam-
biguation (Mihalcea et al., 2004)), a one-class par-
tially supervised classification algorithm (Liu et
al., 2003) 11, and a semi-supervised k-means clus-
tering based model order identification algorithm.
The data for English lexical samples task in
SENSEVAL-3 consists of 7860 examples as offi-
cial training data, and 3944 examples as official
test data for 57 English words. The number of
senses of each English word varies from 3 to 11.
We evaluated these four algorithms with differ-
ent sizes of incomplete tagged data. Given offi-
cial training data of the word w, we constructed
incomplete tagged data XL by removing the all
the tagged instances from official training data that
have sense tags from Ssubet, where Ssubet is a
subset of the ground-truth sense set S for w, and S
consists of the sense tags in official training set for
w. The removed training data and official test data
of w were used as XU. Note that SL = S−Ssubet.
Then we ran these four algorithm for each target
word w with XL as tagged data and XU as un-
tagged data, and evaluated their performance us-
ing the accuracy on official test data of all the 57
words. We conducted six experiments for each tar-
get word w by setting Ssubet as {s1}, {s2}, {s3},
{s1, s2}, {s1, s3}, or {s2, s3}, where si is the i-th
most frequent sense of w. Ssubet cannot be set as
{s4} since some words have only three senses. Ta-
ble 3 lists the percentage of official training data
used as tagged data (the number of examples in in-
</bodyText>
<footnote confidence="0.9678746">
9Available at http://www.senseval.org/senseval3
10we used a linear SV Mlight, available at
http://svmlight.joachims.org/.
11Available at http://www.cs.uic.edu/∼liub/LPU/LPU-
download.html
</footnote>
<bodyText confidence="0.996762980769231">
complete tagged data divided by the number of ex-
amples in official training data) when we removed
the instances with sense tags from Ssubet for all
the 57 words. If Ssubet = {s3}, then most of
sense tagged examples are still included in tagged
data. If Ssubet = {s1, s2}, then there are very few
tagged examples in tagged data. If no instances are
removed from official training data, then the value
of percentage is 100%.
Given an incomplete tagged corpus for a target
word, SVM does not have the ability to find the
new senses from untagged corpus. Therefore it la-
bels all the instances in the untagged corpus with
sense tags from SL.
Given a set of positive examples for a class and
a set of unlabeled examples, the one-class partially
supervised classification algorithm, LPU (Learn-
ing from Positive and Unlabeled examples) (Liu
et al., 2003), learns a classifier in four steps:
Step 1: Identify a small set of reliable negative
examples from unlabeled examples by the use of a
classifier.
Step 2: Build a classifier using positive ex-
amples and automatically selected negative exam-
ples.
Step 3: Iteratively run previous two steps until
no unlabeled examples are classified as negative
ones or the unlabeled set is null.
Step 4: Select a good classifier from the set of
classifiers constructed above.
For comparison, LPU 12 was run to perform
classification on XU for each class in XL. The
label of each instance in XU was determined by
maximizing the classification score from LPU out-
put for each class. If the maximum score of an
instance is negative, then this instance will be la-
beled as a new class. Note that LPU classifies
XL+U into kX, + 1 groups in most of cases.
The clustering based partially supervised sense
disambiguation algorithm was implemented by re-
placing ELP with a semi-supervised k-means clus-
tering algorithm (Wagstaff et al., 2001) in the
model order identification procedure. The label
information in labeled data was used to guide the
semi-supervised clustering on XL+U. Firstly, the
labeled data may be used to determine initial clus-
ter centroids. If the cluster number is greater
12The three parameters in LPU were set as follows: “-s1
spy -s2 svm -c 1”. It means that we used the spy technique for
step 1 in LPU, the SVM algorithm for step 2, and selected the
first or the last classifier as the final classifier. It is identical
with the algorithm “Spy+SVM IS” in Liu et al. (2003).
</bodyText>
<page confidence="0.998204">
419
</page>
<bodyText confidence="0.999975833333334">
than kXL, the initial centroids of clusters for new
classes will be assigned as randomly selected in-
stances. Secondly, in the clustering process, the
instances with the same class label will stay in
the same cluster, while the instances with different
class labels will belong to different clusters. For
better clustering solution, this clustering process
will be restarted three times. Clustering process
will be terminated when clustering solution con-
verges or the number of iteration steps is more than
30. Kmin = kXL = |SL|, Kmax = Kmin + m. m
is set as 4.
We used Jensen-Shannon (JS) divergence (Lin,
1991) as distance measure for semi-supervised
clustering and ELP, since plain LP with JS diver-
gence achieves better performance than that with
cosine similarity on SENSEVAL-3 data (Niu et al.,
2005).
For the LP process in ELP algorithm, we con-
structed connected graphs as follows: two in-
stances u, v will be connected by an edge if u is
among v’s 10 nearest neighbors, or if v is among
u’s 10 nearest neighbors as measured by cosine or
JS distance measure (following (Zhu and Ghahra-
mani, 2002)).
We used three types of features to capture the
information in all the contextual sentences of tar-
get words in SENSEVAL-3 data for all the four
algorithms: part-of-speech of neighboring words
with position information, words in topical con-
text without position information (after removing
stop words), and local collocations (as same as the
feature set used in (Lee and Ng, 2002) except that
we did not use syntactic relations). We removed
the features with occurrence frequency (counted
in both training set and test set) less than 3 times.
If the estimated sense number is more than the
sense number in the initial tagged corpus XL, then
the results from order identification based meth-
ods will consist of the instances from clusters of
unknown classes. When assessing the agreement
between these classification results and the known
results on official test set, we will encounter the
problem that there is no sense tag for each instance
in unknown classes. Slonim and Tishby (2000)
proposed to assign documents in each cluster with
the most dominant class label in that cluster, and
then conducted evaluation on these labeled docu-
ments. Here we will follow their method for as-
signing sense tags to unknown classes from LPU,
clustering based order identification process, and
ELP based order identification process. We as-
signed the instances from unknown classes with
the dominant sense tag in that cluster. The result
from LPU always includes only one cluster of the
unknown class. We also assigned the instances
from the unknown class with the dominant sense
tag in that cluster. When all instances have their
sense tags, we evaluated the their results using the
accuracy on official test set.
</bodyText>
<subsectionHeader confidence="0.997888">
3.2 Results on Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999985425">
Table 4 summarizes the accuracy of SVM, LPU,
the semi-supervised k-means clustering algorithm
with correct sense number |S |or estimated sense
number �kXL+U as input, and the ELP algorithm
with correct sense number |S |or estimated sense
number kXL+U as input using various incomplete
tagged data. The last row in Table 4 lists the av-
erage accuracy of each algorithm over the six ex-
perimental settings. Using |S |as input means that
we do not perform order identification procedure,
while using �kXL+U as input is to perform order
identification and obtain the classification results
on XU at the same time.
We can see that ELP based method outperforms
clustering based method in terms of average accu-
racy under the same experiment setting, and these
two methods outperforms SVM and LPU. More-
over, using the correct sense number as input helps
to improve the overall performance of both clus-
tering based method and ELP based method.
Comparing the performance of the same sys-
tem with different sizes of tagged data (from the
first experiment to the third experiment, and from
the fourth experiment to the sixth experiment), we
can see that the performance was improved when
given more labeled data. Furthermore, ELP based
method outperforms other methods in terms of ac-
curacy when rare senses (e.g. s3) are missing in
the tagged data. It seems that ELP based method
has the ability to find rare senses with the use of
tagged and untagged corpora.
LPU algorithm can deal with only one-class
classification problem. Therefore the labeled data
of other classes cannot be used when determining
the positive labeled data for current class. ELP
can use the labeled data of all the known classes to
determine the seeds of unknown classes. It may
explain why LPU’s performance is worse than
ELP based sense disambiguation although LPU
can correctly estimate the sense number in XL+U
</bodyText>
<page confidence="0.998568">
420
</page>
<tableCaption confidence="0.93192725">
Table 4: This table summarizes the accuracy of SVM, LPU, the semi-supervised k-means clustering al-
gorithm with correct sense number |S |or estimated sense number kXL+U as input, and the ELP algorithm
with correct sense number |S |or estimated sense number kXL+U as input on the official test data of ELS
task in SENSEVAL-3 when given various incomplete tagged corpora.
</tableCaption>
<table confidence="0.999791416666667">
SVM LPU Clustering algorithm ELP algorithm Clustering algorithm ELP algorithm
with |S |as input with |S |as input with �kXL+U as input with �kXL+U as input
Ssubset = 30.6% 22.3% 43.9% 47.8% 40.0% 38.7%
Ssubset = 59.7% 54.6% 44.0% 62.4% 48.5% 62.6%
Ssubset = 67.0% 53.4% 48.7% 67.2% 52.4% 69.1%
Ssubset = 14.6% 13.1% 44.4% 40.2% 35.6% 33.0%
{s1, s2}
Ssubset = 25.7% 21.1% 48.5% 37.9% 39.8% 31.0%
{s1, s3}
Ssubset = 56.2% 53.1% 47.3% 59.4% 46.6% 58.7%
{s2, s3}
Average accuracy 42.3% 36.3% 46.1% 52.5% 43.8% 48.9%
</table>
<tableCaption confidence="0.983928">
Table 5: These two tables provide the mean and
</tableCaption>
<bodyText confidence="0.90597964">
standard deviation of absolute values of the differ-
ence between ground-truth results |S |and sense
numbers estimated by clustering or ELP based or-
der identification procedure respectively.
Clustering based method ELP based method
Ssubset = 1.3±1.1 2.2±1.1
Ssubset = 2.4±0.9 2.4±0.9
Ssubset = 2.6±0.7 2.6±0.7
Ssubset = 1.2±0.6 1.6±0.5
{s1, s2}
Ssubset = 1.4±0.6 1.8±0.4
{s1, s3}
Ssubset = 1.8±0.5 1.8±0.5
{s2, s3}
when only one sense is missing in XL.
When very few labeled examples are avail-
able, the noise in labeled data makes it difficult
to learn the classification score (each entry in
YDU ). Therefore using the classification confi-
dence criterion may lead to poor performance of
seed selection for unknown classes if the classifi-
cation score is not accurate. It may explain why
ELP based method does not outperform cluster-
ing based method with small labeled data (e.g.,
Ssubset = {s1}).
</bodyText>
<subsectionHeader confidence="0.9454">
3.3 Results on Sense Number Estimation
</subsectionHeader>
<bodyText confidence="0.999903346153846">
Table 5 provides the mean and standard devia-
tion of absolute difference values between ground-
truth results |S |and sense numbers estimated by
clustering or ELP based order identification pro-
cedures respectively. For example, if the ground
truth sense number of the word w is kw, and the es-
timated value is �kw, then the absolute value of the
difference between these two values is |kw − �kw|.
Therefore we can have this value for each word.
Then we calculated the mean and deviation on this
array of absolute values. LPU does not have the
order identification capability since it always as-
sumes that there is at least one new class in un-
labeled data, and does not further differentiate the
instances from these new classes. Therefore we do
not provide the order identification results of LPU.
From the results in Table 5, we can see that esti-
mated sense numbers are closer to ground truth re-
sults when given less labeled data for clustering or
ELP based methods. Moreover, clustering based
method performs better than ELP based method in
terms of order identification when given less la-
beled data (e.g., Ssubset = {s1}). It seems that
ELP is not robust to the noise in small labeled data,
compared with the semi-supervised k-means clus-
tering algorithm.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.985101333333333">
The work closest to ours is partially supervised
classification or building classifiers using positive
examples and unlabeled examples, which has been
studied in machine learning community (Denis et
al., 2002; Liu et al., 2003; Manevitz and Yousef,
2001; Yu et al., 2002). However, they cannot
</bodyText>
<page confidence="0.996046">
421
</page>
<bodyText confidence="0.999985538461538">
group negative examples into meaningful clusters.
In contrast, our algorithm can find the occurrence
of negative examples and further group these neg-
ative examples into a “natural” number of clusters.
Semi-supervised clustering (Wagstaff et al., 2001)
may be used to perform classification by the use
of labeled and unlabeled examples, but it encoun-
ters the same problem of partially supervised clas-
sification that model order cannot be automatically
estimated.
Levine and Domany (2001) and Lange et al.
(2002) proposed cluster validation based criteria
for cluster number estimation. However, they
showed the application of the cluster validation
method only for unsupervised learning. Our work
can be considered as an extension of their methods
in the setting of partially supervised learning.
In natural language processing community, the
work that is closely related to ours is word sense
discrimination which can induce senses by group-
ing occurrences of a word into clusters (Schfitze,
1998). If it is considered as unsupervised meth-
ods to solve sense disambiguation problem, then
our method employs partially supervised learning
technique to deal with sense disambiguation prob-
lem by use of tagged and untagged texts.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999907818181818">
In this paper, we present an order identification
based partially supervised classification algorithm
and investigate its application to partially super-
vised word sense disambiguation problem. Exper-
imental results on SENSEVAL-3 data indicate that
our ELP based model order identification algo-
rithm achieves better performance than other state
of the art classification algorithms, e.g., SVM,
a one-class partially supervised algorithm (LPU),
and a semi-supervised k-means clustering based
model order identification algorithm.
</bodyText>
<sectionHeader confidence="0.999282" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999063148148148">
Brown P., Stephen, D.P., Vincent, D.P., &amp; Robert, Mer-
cer.. 1991. Word Sense Disambiguation Using Sta-
tistical Methods. Proceedings of ACL.
Dagan, I. &amp; Itai A.. 1994. Word Sense Disambigua-
tion Using A Second Language Monolingual Cor-
pus. Computational Linguistics, Vol. 20(4), pp. 563-
596.
Denis, F., Gilleron, R., &amp; Tommasi, M.. 2002. Text
Classification from Positive and Unlabeled Exam-
ples. Proceedings of the 9th International Confer-
ence on Information Processing and Management of
Uncertainty in Knowledge-Based Systems.
Lange, T., Braun, M., Roth, V., &amp; Buhmann, J. M.
2002. Stability-Based Model Selection. NIPS 15.
Leacock, C., Miller, G.A. &amp; Chodorow, M.. 1998.
Using Corpus Statistics and WordNet Relations for
Sense Identification. Computational Linguistics,
24:1, 147–165.
Lee, Y.K. &amp; Ng, H.T.. 2002. An Empirical Eval-
uation of Knowledge Sources and Learning Algo-
rithms for Word Sense Disambiguation. Proceed-
ings of EMNLP, (pp. 41-48).
Levine, E., &amp; Domany, E. 2001. Resampling Method
for Unsupervised Estimation of Cluster Validity.
Neural Computation, Vol. 13, 2573–2593.
Lin, J. 1991. Divergence Measures Based on the
Shannon Entropy. IEEE Transactions on Informa-
tion Theory, 37:1, 145–150.
Liu, B., Dai, Y., Li, X., Lee, W.S., &amp; Yu, P.. 2003.
Building Text Classifiers Using Positive and Unla-
beled Examples. Proceedings of IEEE ICDM.
Manevitz, L.M., &amp; Yousef, M.. 2001. One Class
SVMs for Document Classification. Journal of Ma-
chine Learning, 2, 139-154.
Mihalcea R., Chklovski, T., &amp; Kilgariff, A.. 2004.
The SENSEVAL-3 English Lexical Sample Task.
SENSEVAL-2004.
Niu, Z.Y., Ji, D.H., &amp; Tan, C.L.. 2005. Word Sense
Disambiguation Using Label Propagation Based
Semi-Supervised Learning. Proceedings ofACL.
Sch¨utze, H.. 1998. Automatic Word Sense Discrimi-
nation. Computational Linguistics, 24:1, 97–123.
Wagstaff, K., Cardie, C., Rogers, S., &amp; Schroedl, S..
2001. Constrained K-Means Clustering with Back-
ground Knowledge. Proceedings of ICML.
Yarowsky, D.. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. Pro-
ceedings ofACL.
Yu, H., Han, J., &amp; Chang, K. C.-C.. 2002. PEBL: Pos-
itive example based learning for web page classifi-
cation using SVM. Proceedings of ACM SIGKDD.
Zhu, X. &amp; Ghahramani, Z.. 2002. Learning from La-
beled and Unlabeled Data with Label Propagation.
CMU CALD tech report CMU-CALD-02-107.
</reference>
<page confidence="0.99838">
422
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.630570">
<title confidence="0.980707">Partially Supervised Sense Disambiguation by Learning Sense from Tagged and Untagged Corpora</title>
<author confidence="0.990606">Zheng-Yu Niu</author>
<author confidence="0.990606">Dong-Hong</author>
<affiliation confidence="0.99865">Institute for Infocomm</affiliation>
<address confidence="0.9562835">21 Heng Mui Keng 119613</address>
<author confidence="0.980025">Chew Lim</author>
<affiliation confidence="0.998262">Department of Computer National University of 3 Science Drive</affiliation>
<address confidence="0.962907">117543</address>
<email confidence="0.983195">tancl@comp.nus.edu.sg</email>
<abstract confidence="0.99110732">Supervised and semi-supervised sense disambiguation methods will mis-tag the instances of a target word if the senses of these instances are not defined in sense inventories or there are no tagged instances for these senses in training data. Here we used a model order identification method to avoid the misclassification of the instances with undefined senses by discovering new senses from mixed data (tagged and untagged corpora). This algorithm tries to obtain a natural partition of the mixed data by maximizing a stability criterion defined on the classification result from an extended label propagation algorithm over all the possible values of the number of senses (or sense number, model order). Experimental results on SENSEVAL-3 data indicate that it outperforms SVM, a one-class partially supervised classification algorithm, and a clustering based model order identification algorithm when the tagged data is incomplete.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>D P Stephen</author>
<author>D P Vincent</author>
<author>Mercer Robert</author>
</authors>
<title>Word Sense Disambiguation Using Statistical Methods.</title>
<date>1991</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="2675" citStr="Brown et al., 1991" startWordPosition="411" endWordPosition="414">vised sense disambiguation (Leacock et al., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). Supervised methods usually rely on the information from previously sense tagged corpora to determine the senses of words in unseen texts. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994), or sense-tagged seed examples (Yarowsky, 1995). Some observations can be made on the previous supervised and semi-supervised methods. They always rely on hand-crafted lexicons (e.g., WordNet) as sense inventories. But these resources may miss domain-specific senses, which leads to incomplete sense tagged corpus. Therefore, sense taggers trained on the incomplete tagged corpus will misclassify some instances if the senses of these instances are not defined in sense inventories. For example, one performs WSD in information technology related texts using WordNet 2 as sens</context>
</contexts>
<marker>Brown, Stephen, Vincent, Robert, 1991</marker>
<rawString>Brown P., Stephen, D.P., Vincent, D.P., &amp; Robert, Mercer.. 1991. Word Sense Disambiguation Using Statistical Methods. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>A Itai</author>
</authors>
<title>Word Sense Disambiguation Using A Second Language Monolingual Corpus.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>563--596</pages>
<contexts>
<context position="2698" citStr="Dagan and Itai, 1994" startWordPosition="415" endWordPosition="419">uation (Leacock et al., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). Supervised methods usually rely on the information from previously sense tagged corpora to determine the senses of words in unseen texts. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994), or sense-tagged seed examples (Yarowsky, 1995). Some observations can be made on the previous supervised and semi-supervised methods. They always rely on hand-crafted lexicons (e.g., WordNet) as sense inventories. But these resources may miss domain-specific senses, which leads to incomplete sense tagged corpus. Therefore, sense taggers trained on the incomplete tagged corpus will misclassify some instances if the senses of these instances are not defined in sense inventories. For example, one performs WSD in information technology related texts using WordNet 2 as sense inventory. When disam</context>
</contexts>
<marker>Dagan, Itai, 1994</marker>
<rawString>Dagan, I. &amp; Itai A.. 1994. Word Sense Disambiguation Using A Second Language Monolingual Corpus. Computational Linguistics, Vol. 20(4), pp. 563-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Denis</author>
<author>R Gilleron</author>
<author>M Tommasi</author>
</authors>
<title>Text Classification from Positive and Unlabeled Examples.</title>
<date>2002</date>
<booktitle>Proceedings of the 9th International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems.</booktitle>
<contexts>
<context position="29033" citStr="Denis et al., 2002" startWordPosition="4984" endWordPosition="4987">ers are closer to ground truth results when given less labeled data for clustering or ELP based methods. Moreover, clustering based method performs better than ELP based method in terms of order identification when given less labeled data (e.g., Ssubset = {s1}). It seems that ELP is not robust to the noise in small labeled data, compared with the semi-supervised k-means clustering algorithm. 4 Related Work The work closest to ours is partially supervised classification or building classifiers using positive examples and unlabeled examples, which has been studied in machine learning community (Denis et al., 2002; Liu et al., 2003; Manevitz and Yousef, 2001; Yu et al., 2002). However, they cannot 421 group negative examples into meaningful clusters. In contrast, our algorithm can find the occurrence of negative examples and further group these negative examples into a “natural” number of clusters. Semi-supervised clustering (Wagstaff et al., 2001) may be used to perform classification by the use of labeled and unlabeled examples, but it encounters the same problem of partially supervised classification that model order cannot be automatically estimated. Levine and Domany (2001) and Lange et al. (2002)</context>
</contexts>
<marker>Denis, Gilleron, Tommasi, 2002</marker>
<rawString>Denis, F., Gilleron, R., &amp; Tommasi, M.. 2002. Text Classification from Positive and Unlabeled Examples. Proceedings of the 9th International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lange</author>
<author>M Braun</author>
<author>V Roth</author>
<author>J M Buhmann</author>
</authors>
<title>Stability-Based Model Selection.</title>
<date>2002</date>
<journal>NIPS</journal>
<volume>15</volume>
<contexts>
<context position="15796" citStr="Lange et al., 2002" startWordPosition="2742" endWordPosition="2745">n classification results on the different subsets generated by sampling should be similar with that on the full dataset. In the other words, the classification solution with the true model order as parameter is robust against resampling, which gives rise to a local optimum of M(Cµ, C). In this algorithm, we normalize M(Cµk , Ck) by the equation in step 6 of Table 2, which makes our objective function different from the figure of merit (equation ( 2)) proposed in (Levine and Domany, 2001). The reason to normalize M(Cµk , Ck) is that M(Cµk , Ck) tends to decrease when increasing the value of k (Lange et al., 2002). Therefore for avoiding the bias that the smaller value of k is to be selected as the model order, we use the cluster validity of a random predictor to normalize M(Cµk , Ck). If �kXL+U is equal to kXL, then there is no new sense in XU. Otherwise (�kXL+U &gt; kXL) new senses of w may be represented by the groups in which there is no instance from XL. 3 Experiments and Results 3.1 Experiment Design We evaluated the ELP based model order identification algorithm on the data in English lexical sample task of SENSEVAL-3 (including all M(Cµ, C) = 418 Table 3: Description of The percentage of official </context>
<context position="29633" citStr="Lange et al. (2002)" startWordPosition="5078" endWordPosition="5081">(Denis et al., 2002; Liu et al., 2003; Manevitz and Yousef, 2001; Yu et al., 2002). However, they cannot 421 group negative examples into meaningful clusters. In contrast, our algorithm can find the occurrence of negative examples and further group these negative examples into a “natural” number of clusters. Semi-supervised clustering (Wagstaff et al., 2001) may be used to perform classification by the use of labeled and unlabeled examples, but it encounters the same problem of partially supervised classification that model order cannot be automatically estimated. Levine and Domany (2001) and Lange et al. (2002) proposed cluster validation based criteria for cluster number estimation. However, they showed the application of the cluster validation method only for unsupervised learning. Our work can be considered as an extension of their methods in the setting of partially supervised learning. In natural language processing community, the work that is closely related to ours is word sense discrimination which can induce senses by grouping occurrences of a word into clusters (Schfitze, 1998). If it is considered as unsupervised methods to solve sense disambiguation problem, then our method employs parti</context>
</contexts>
<marker>Lange, Braun, Roth, Buhmann, 2002</marker>
<rawString>Lange, T., Braun, M., Roth, V., &amp; Buhmann, J. M. 2002. Stability-Based Model Selection. NIPS 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>G A Miller</author>
<author>M Chodorow</author>
</authors>
<title>Using Corpus Statistics and WordNet Relations for Sense Identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>147--165</pages>
<contexts>
<context position="2106" citStr="Leacock et al., 1998" startWordPosition="326" endWordPosition="329">untagged texts when given incomplete tagged corpus 1. Word sense disambiguation can be defined as associating a target word in a text or discourse 1“incomplete tagged corpus” means that tagged corpus does not include the instances of some senses for the target word, while these senses may occur in untagged texts. with a definition or meaning. Many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacock et al., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). Supervised methods usually rely on the information from previously sense tagged corpora to determine the senses of words in unseen texts. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994), or sen</context>
</contexts>
<marker>Leacock, Miller, Chodorow, 1998</marker>
<rawString>Leacock, C., Miller, G.A. &amp; Chodorow, M.. 1998. Using Corpus Statistics and WordNet Relations for Sense Identification. Computational Linguistics, 24:1, 147–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y K Lee</author>
<author>H T Ng</author>
</authors>
<title>An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="22462" citStr="Lee and Ng, 2002" startWordPosition="3892" endWordPosition="3895">cted graphs as follows: two instances u, v will be connected by an edge if u is among v’s 10 nearest neighbors, or if v is among u’s 10 nearest neighbors as measured by cosine or JS distance measure (following (Zhu and Ghahramani, 2002)). We used three types of features to capture the information in all the contextual sentences of target words in SENSEVAL-3 data for all the four algorithms: part-of-speech of neighboring words with position information, words in topical context without position information (after removing stop words), and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). We removed the features with occurrence frequency (counted in both training set and test set) less than 3 times. If the estimated sense number is more than the sense number in the initial tagged corpus XL, then the results from order identification based methods will consist of the instances from clusters of unknown classes. When assessing the agreement between these classification results and the known results on official test set, we will encounter the problem that there is no sense tag for each instance in unknown classes. Slonim and Tishby </context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Lee, Y.K. &amp; Ng, H.T.. 2002. An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation. Proceedings of EMNLP, (pp. 41-48).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levine</author>
<author>E Domany</author>
</authors>
<title>Resampling Method for Unsupervised Estimation of Cluster Validity.</title>
<date>2001</date>
<journal>Neural Computation,</journal>
<volume>13</volume>
<pages>2573--2593</pages>
<contexts>
<context position="11949" citStr="Levine and Domany, 2001" startWordPosition="2029" endWordPosition="2032">L + DU |x |DL + DU |probability transition matrix Tij = P(j —* i) = r , , L-�k= W�j where Tij is the probability to jump from example xj to example xi. Compute the row-normalized matrix T by Tij = Tij/ Enk=1 Tik. The classification solution is obtained by YDU = (I − Tuu)−1TulY 0 DL. I is |DU |x |DU| identity matrix. Tuu and Tul are acquired by splitting matrix T after the |DL|-th row and the |DL|-th column into 4 sub-matrices. 2.2 Model Order Identification Procedure For achieving the model order identification (or sense number estimation) ability, we use a cluster validation based criterion (Levine and Domany, 2001) to infer the optimal number of senses of w in XL+U. tion confidence less than the average of that in DsL. Classification confidence of the example xi is defined as the absolute value of the difference between two maximum values from the i-th row in labeling matrix. 8Initially there are no tagged examples for the m-th class in D�L. Therefore we do not need to remove tagged examples for this new class, and then directly train a classifier with D L. 417 Table 2: Model order evaluation algorithm. Function: CV(XL+U, k, q, Y0XL+U) Input: data set XL+U, model order k, and sampling frequency q; Outpu</context>
<context position="14445" citStr="Levine and Domany, 2001" startWordPosition="2481" endWordPosition="2484">greater than the possible ground-truth value. CV is a cluster validation based evaluation function. Table 2 shows the details of this function. We set q, the resampling frequency for estimation of stability score, as 20. α is set as 0.90. The random predictor assigns uniformly distributed class labels to each instance in a given dataset. We run this CV procedure for each value of k. The value of k that maximizes this function will be selected as the estimation of sense number. At the same time, we can obtain a partition of XL+U with �kXL+U groups. The function M(Cµ, C) in Table 2 is given by (Levine and Domany, 2001): Ei,j 1{Cµi,j = Ci,j = 1, xi, xj E XµU} /moi,j 1{Ci,j = 1, xi, xj E XµU} , (2) where XµU is the untagged data in XµL+U, XµL+U is a subset with the size α|XL+U |(0 &lt; α &lt; 1) sampled from XL+U, C or Cµ is |XU |x |XU |or |XµU |x |XµU |connectivity matrix based on classification solutions computed on XU or XµU respectively. The connectivity matrix C is defined as: Ci,j = 1 if xi and xj belong to the same cluster, otherwise Ci,j = 0. Cµ is calculated in the same way. M(Cµ, C) measures the proportion of example pairs in each group computed on XU that are also assigned into the same group by the clas</context>
<context position="15669" citStr="Levine and Domany, 2001" startWordPosition="2715" endWordPosition="2719">sification solution on XµU. Clearly, 0 &lt; M &lt; 1. Intuitively, if the value of k is identical with the true value of sense number, then classification results on the different subsets generated by sampling should be similar with that on the full dataset. In the other words, the classification solution with the true model order as parameter is robust against resampling, which gives rise to a local optimum of M(Cµ, C). In this algorithm, we normalize M(Cµk , Ck) by the equation in step 6 of Table 2, which makes our objective function different from the figure of merit (equation ( 2)) proposed in (Levine and Domany, 2001). The reason to normalize M(Cµk , Ck) is that M(Cµk , Ck) tends to decrease when increasing the value of k (Lange et al., 2002). Therefore for avoiding the bias that the smaller value of k is to be selected as the model order, we use the cluster validity of a random predictor to normalize M(Cµk , Ck). If �kXL+U is equal to kXL, then there is no new sense in XU. Otherwise (�kXL+U &gt; kXL) new senses of w may be represented by the groups in which there is no instance from XL. 3 Experiments and Results 3.1 Experiment Design We evaluated the ELP based model order identification algorithm on the data</context>
<context position="29609" citStr="Levine and Domany (2001)" startWordPosition="5073" endWordPosition="5076">n machine learning community (Denis et al., 2002; Liu et al., 2003; Manevitz and Yousef, 2001; Yu et al., 2002). However, they cannot 421 group negative examples into meaningful clusters. In contrast, our algorithm can find the occurrence of negative examples and further group these negative examples into a “natural” number of clusters. Semi-supervised clustering (Wagstaff et al., 2001) may be used to perform classification by the use of labeled and unlabeled examples, but it encounters the same problem of partially supervised classification that model order cannot be automatically estimated. Levine and Domany (2001) and Lange et al. (2002) proposed cluster validation based criteria for cluster number estimation. However, they showed the application of the cluster validation method only for unsupervised learning. Our work can be considered as an extension of their methods in the setting of partially supervised learning. In natural language processing community, the work that is closely related to ours is word sense discrimination which can induce senses by grouping occurrences of a word into clusters (Schfitze, 1998). If it is considered as unsupervised methods to solve sense disambiguation problem, then </context>
</contexts>
<marker>Levine, Domany, 2001</marker>
<rawString>Levine, E., &amp; Domany, E. 2001. Resampling Method for Unsupervised Estimation of Cluster Validity. Neural Computation, Vol. 13, 2573–2593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
</authors>
<title>Divergence Measures Based on the Shannon Entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<pages>145--150</pages>
<contexts>
<context position="21593" citStr="Lin, 1991" startWordPosition="3747" endWordPosition="3748">han kXL, the initial centroids of clusters for new classes will be assigned as randomly selected instances. Secondly, in the clustering process, the instances with the same class label will stay in the same cluster, while the instances with different class labels will belong to different clusters. For better clustering solution, this clustering process will be restarted three times. Clustering process will be terminated when clustering solution converges or the number of iteration steps is more than 30. Kmin = kXL = |SL|, Kmax = Kmin + m. m is set as 4. We used Jensen-Shannon (JS) divergence (Lin, 1991) as distance measure for semi-supervised clustering and ELP, since plain LP with JS divergence achieves better performance than that with cosine similarity on SENSEVAL-3 data (Niu et al., 2005). For the LP process in ELP algorithm, we constructed connected graphs as follows: two instances u, v will be connected by an edge if u is among v’s 10 nearest neighbors, or if v is among u’s 10 nearest neighbors as measured by cosine or JS distance measure (following (Zhu and Ghahramani, 2002)). We used three types of features to capture the information in all the contextual sentences of target words in</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Lin, J. 1991. Divergence Measures Based on the Shannon Entropy. IEEE Transactions on Information Theory, 37:1, 145–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>Y Dai</author>
<author>X Li</author>
<author>W S Lee</author>
<author>P Yu</author>
</authors>
<title>Building Text Classifiers Using Positive and Unlabeled Examples.</title>
<date>2003</date>
<booktitle>Proceedings of IEEE ICDM.</booktitle>
<contexts>
<context position="9948" citStr="Liu et al., 2003" startWordPosition="1649" endWordPosition="1652">algorithm (ELP), which can classify the examples in DU into k groups. If the value of k is equal to kXL, then ELP is identical with the plain label propagation algorithm (LP) (Zhu and Ghahramani, 2002). Otherwise, if the value of k is greater than kXL, we perform classification by the following steps: (1) estimate the dataset size of each new class as sizenew class by identifying the examples of new classes using the “Spy” technique 7 and assuming 5DL may be the dataset XL or a subset sampled from XL. 6DU may be the dataset XU or a subset sampled from XU. 7The “Spy” technique was proposed in (Liu et al., 2003). Our re-implementation of this technique consists of three steps: (1) sample a small subset DsL with the size 15%xADLA from DL; (2) train a classifier with tagged data DL − DsL; (3) classify DU and DsL, and then select some examples from DU as the dataset of new classes, which have the classificathat new classes are equally distributed; (2)DL = DL, D�U = DU; (3) remove tagged examples of the m-th new class (kXL + 1 &lt; m &lt; k) from D�L 8 and train a classifier on this labeled dataset without the m-th class; (4) the classifier is then used to classify the examples in D�U; (5) the least confidentl</context>
<context position="17014" citStr="Liu et al., 2003" startWordPosition="2958" endWordPosition="2961">raining data used as tagged data when instances with different sense sets are removed from official training data. The percentage of official training data used as tagged data Ssubset = {s1} 42.8% Ssubset = {s2} 76.7% Ssubset = {s3} 89.1% Ssubset = {s1, s2} 19.6% Ssubset = {s1, s3} 32.0% Ssubset = {s2, s3} 65.9% the 57 English words ) 9, and further empirically compared it with other state of the art classification methods, including SVM 10 (the state of the art method for supervised word sense disambiguation (Mihalcea et al., 2004)), a one-class partially supervised classification algorithm (Liu et al., 2003) 11, and a semi-supervised k-means clustering based model order identification algorithm. The data for English lexical samples task in SENSEVAL-3 consists of 7860 examples as official training data, and 3944 examples as official test data for 57 English words. The number of senses of each English word varies from 3 to 11. We evaluated these four algorithms with different sizes of incomplete tagged data. Given official training data of the word w, we constructed incomplete tagged data XL by removing the all the tagged instances from official training data that have sense tags from Ssubet, where</context>
<context position="19409" citStr="Liu et al., 2003" startWordPosition="3372" endWordPosition="3375">uded in tagged data. If Ssubet = {s1, s2}, then there are very few tagged examples in tagged data. If no instances are removed from official training data, then the value of percentage is 100%. Given an incomplete tagged corpus for a target word, SVM does not have the ability to find the new senses from untagged corpus. Therefore it labels all the instances in the untagged corpus with sense tags from SL. Given a set of positive examples for a class and a set of unlabeled examples, the one-class partially supervised classification algorithm, LPU (Learning from Positive and Unlabeled examples) (Liu et al., 2003), learns a classifier in four steps: Step 1: Identify a small set of reliable negative examples from unlabeled examples by the use of a classifier. Step 2: Build a classifier using positive examples and automatically selected negative examples. Step 3: Iteratively run previous two steps until no unlabeled examples are classified as negative ones or the unlabeled set is null. Step 4: Select a good classifier from the set of classifiers constructed above. For comparison, LPU 12 was run to perform classification on XU for each class in XL. The label of each instance in XU was determined by maximi</context>
<context position="20976" citStr="Liu et al. (2003)" startWordPosition="3642" endWordPosition="3645">pervised k-means clustering algorithm (Wagstaff et al., 2001) in the model order identification procedure. The label information in labeled data was used to guide the semi-supervised clustering on XL+U. Firstly, the labeled data may be used to determine initial cluster centroids. If the cluster number is greater 12The three parameters in LPU were set as follows: “-s1 spy -s2 svm -c 1”. It means that we used the spy technique for step 1 in LPU, the SVM algorithm for step 2, and selected the first or the last classifier as the final classifier. It is identical with the algorithm “Spy+SVM IS” in Liu et al. (2003). 419 than kXL, the initial centroids of clusters for new classes will be assigned as randomly selected instances. Secondly, in the clustering process, the instances with the same class label will stay in the same cluster, while the instances with different class labels will belong to different clusters. For better clustering solution, this clustering process will be restarted three times. Clustering process will be terminated when clustering solution converges or the number of iteration steps is more than 30. Kmin = kXL = |SL|, Kmax = Kmin + m. m is set as 4. We used Jensen-Shannon (JS) diver</context>
<context position="29051" citStr="Liu et al., 2003" startWordPosition="4988" endWordPosition="4991">ound truth results when given less labeled data for clustering or ELP based methods. Moreover, clustering based method performs better than ELP based method in terms of order identification when given less labeled data (e.g., Ssubset = {s1}). It seems that ELP is not robust to the noise in small labeled data, compared with the semi-supervised k-means clustering algorithm. 4 Related Work The work closest to ours is partially supervised classification or building classifiers using positive examples and unlabeled examples, which has been studied in machine learning community (Denis et al., 2002; Liu et al., 2003; Manevitz and Yousef, 2001; Yu et al., 2002). However, they cannot 421 group negative examples into meaningful clusters. In contrast, our algorithm can find the occurrence of negative examples and further group these negative examples into a “natural” number of clusters. Semi-supervised clustering (Wagstaff et al., 2001) may be used to perform classification by the use of labeled and unlabeled examples, but it encounters the same problem of partially supervised classification that model order cannot be automatically estimated. Levine and Domany (2001) and Lange et al. (2002) proposed cluster </context>
</contexts>
<marker>Liu, Dai, Li, Lee, Yu, 2003</marker>
<rawString>Liu, B., Dai, Y., Li, X., Lee, W.S., &amp; Yu, P.. 2003. Building Text Classifiers Using Positive and Unlabeled Examples. Proceedings of IEEE ICDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M Manevitz</author>
<author>M Yousef</author>
</authors>
<title>One Class SVMs for Document Classification.</title>
<date>2001</date>
<journal>Journal of Machine Learning,</journal>
<volume>2</volume>
<pages>139--154</pages>
<contexts>
<context position="29078" citStr="Manevitz and Yousef, 2001" startWordPosition="4992" endWordPosition="4995"> when given less labeled data for clustering or ELP based methods. Moreover, clustering based method performs better than ELP based method in terms of order identification when given less labeled data (e.g., Ssubset = {s1}). It seems that ELP is not robust to the noise in small labeled data, compared with the semi-supervised k-means clustering algorithm. 4 Related Work The work closest to ours is partially supervised classification or building classifiers using positive examples and unlabeled examples, which has been studied in machine learning community (Denis et al., 2002; Liu et al., 2003; Manevitz and Yousef, 2001; Yu et al., 2002). However, they cannot 421 group negative examples into meaningful clusters. In contrast, our algorithm can find the occurrence of negative examples and further group these negative examples into a “natural” number of clusters. Semi-supervised clustering (Wagstaff et al., 2001) may be used to perform classification by the use of labeled and unlabeled examples, but it encounters the same problem of partially supervised classification that model order cannot be automatically estimated. Levine and Domany (2001) and Lange et al. (2002) proposed cluster validation based criteria f</context>
</contexts>
<marker>Manevitz, Yousef, 2001</marker>
<rawString>Manevitz, L.M., &amp; Yousef, M.. 2001. One Class SVMs for Document Classification. Journal of Machine Learning, 2, 139-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>T Chklovski</author>
<author>A Kilgariff</author>
</authors>
<date>2004</date>
<booktitle>The SENSEVAL-3 English Lexical Sample Task. SENSEVAL-2004.</booktitle>
<contexts>
<context position="16935" citStr="Mihalcea et al., 2004" startWordPosition="2947" endWordPosition="2950">3 (including all M(Cµ, C) = 418 Table 3: Description of The percentage of official training data used as tagged data when instances with different sense sets are removed from official training data. The percentage of official training data used as tagged data Ssubset = {s1} 42.8% Ssubset = {s2} 76.7% Ssubset = {s3} 89.1% Ssubset = {s1, s2} 19.6% Ssubset = {s1, s3} 32.0% Ssubset = {s2, s3} 65.9% the 57 English words ) 9, and further empirically compared it with other state of the art classification methods, including SVM 10 (the state of the art method for supervised word sense disambiguation (Mihalcea et al., 2004)), a one-class partially supervised classification algorithm (Liu et al., 2003) 11, and a semi-supervised k-means clustering based model order identification algorithm. The data for English lexical samples task in SENSEVAL-3 consists of 7860 examples as official training data, and 3944 examples as official test data for 57 English words. The number of senses of each English word varies from 3 to 11. We evaluated these four algorithms with different sizes of incomplete tagged data. Given official training data of the word w, we constructed incomplete tagged data XL by removing the all the tagge</context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgariff, 2004</marker>
<rawString>Mihalcea R., Chklovski, T., &amp; Kilgariff, A.. 2004. The SENSEVAL-3 English Lexical Sample Task. SENSEVAL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Y Niu</author>
<author>D H Ji</author>
<author>C L Tan</author>
</authors>
<title>Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning.</title>
<date>2005</date>
<booktitle>Proceedings ofACL.</booktitle>
<contexts>
<context position="21786" citStr="Niu et al., 2005" startWordPosition="3775" endWordPosition="3778">ill stay in the same cluster, while the instances with different class labels will belong to different clusters. For better clustering solution, this clustering process will be restarted three times. Clustering process will be terminated when clustering solution converges or the number of iteration steps is more than 30. Kmin = kXL = |SL|, Kmax = Kmin + m. m is set as 4. We used Jensen-Shannon (JS) divergence (Lin, 1991) as distance measure for semi-supervised clustering and ELP, since plain LP with JS divergence achieves better performance than that with cosine similarity on SENSEVAL-3 data (Niu et al., 2005). For the LP process in ELP algorithm, we constructed connected graphs as follows: two instances u, v will be connected by an edge if u is among v’s 10 nearest neighbors, or if v is among u’s 10 nearest neighbors as measured by cosine or JS distance measure (following (Zhu and Ghahramani, 2002)). We used three types of features to capture the information in all the contextual sentences of target words in SENSEVAL-3 data for all the four algorithms: part-of-speech of neighboring words with position information, words in topical context without position information (after removing stop words), a</context>
</contexts>
<marker>Niu, Ji, Tan, 2005</marker>
<rawString>Niu, Z.Y., Ji, D.H., &amp; Tan, C.L.. 2005. Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning. Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic Word Sense Discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>97--123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Sch¨utze, H.. 1998. Automatic Word Sense Discrimination. Computational Linguistics, 24:1, 97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wagstaff</author>
<author>C Cardie</author>
<author>S Rogers</author>
<author>S Schroedl</author>
</authors>
<title>Constrained K-Means Clustering with Background Knowledge.</title>
<date>2001</date>
<booktitle>Proceedings of ICML.</booktitle>
<contexts>
<context position="8946" citStr="Wagstaff et al., 2001" startWordPosition="1470" endWordPosition="1473">of the number of senses in mixed data XL+U, and kXL be the number of senses in initial tagged data XL. Note that kXL = |SL|, and k &gt; kXL. The classification algorithm in the order identification process should be able to accept labeled data DL 5, unlabeled data DU 6 and model order k as input, and assign a class label or a cluster index to each instance in DU as output. Previous supervised or semi-supervised algorithms (e.g. SVM, label propagation algorithm (Zhu and Ghahramani, 2002)) cannot classify the examples in DU into k groups if k &gt; kXL. The semi-supervised kmeans clustering algorithm (Wagstaff et al., 2001) may be used to perform clustering analysis on mixed data, but its efficiency is a problem for clustering analysis on a very large dataset since multiple restarts are usually required to avoid local optima and multiple iterations will be run in each clustering process for optimizing a clustering solution. In this work, we propose an alternative method, an extended label propagation algorithm (ELP), which can classify the examples in DU into k groups. If the value of k is equal to kXL, then ELP is identical with the plain label propagation algorithm (LP) (Zhu and Ghahramani, 2002). Otherwise, i</context>
<context position="20420" citStr="Wagstaff et al., 2001" startWordPosition="3543" endWordPosition="3546">ect a good classifier from the set of classifiers constructed above. For comparison, LPU 12 was run to perform classification on XU for each class in XL. The label of each instance in XU was determined by maximizing the classification score from LPU output for each class. If the maximum score of an instance is negative, then this instance will be labeled as a new class. Note that LPU classifies XL+U into kX, + 1 groups in most of cases. The clustering based partially supervised sense disambiguation algorithm was implemented by replacing ELP with a semi-supervised k-means clustering algorithm (Wagstaff et al., 2001) in the model order identification procedure. The label information in labeled data was used to guide the semi-supervised clustering on XL+U. Firstly, the labeled data may be used to determine initial cluster centroids. If the cluster number is greater 12The three parameters in LPU were set as follows: “-s1 spy -s2 svm -c 1”. It means that we used the spy technique for step 1 in LPU, the SVM algorithm for step 2, and selected the first or the last classifier as the final classifier. It is identical with the algorithm “Spy+SVM IS” in Liu et al. (2003). 419 than kXL, the initial centroids of clu</context>
<context position="29374" citStr="Wagstaff et al., 2001" startWordPosition="5036" endWordPosition="5039">ed with the semi-supervised k-means clustering algorithm. 4 Related Work The work closest to ours is partially supervised classification or building classifiers using positive examples and unlabeled examples, which has been studied in machine learning community (Denis et al., 2002; Liu et al., 2003; Manevitz and Yousef, 2001; Yu et al., 2002). However, they cannot 421 group negative examples into meaningful clusters. In contrast, our algorithm can find the occurrence of negative examples and further group these negative examples into a “natural” number of clusters. Semi-supervised clustering (Wagstaff et al., 2001) may be used to perform classification by the use of labeled and unlabeled examples, but it encounters the same problem of partially supervised classification that model order cannot be automatically estimated. Levine and Domany (2001) and Lange et al. (2002) proposed cluster validation based criteria for cluster number estimation. However, they showed the application of the cluster validation method only for unsupervised learning. Our work can be considered as an extension of their methods in the setting of partially supervised learning. In natural language processing community, the work that</context>
</contexts>
<marker>Wagstaff, Cardie, Rogers, Schroedl, 2001</marker>
<rawString>Wagstaff, K., Cardie, C., Rogers, S., &amp; Schroedl, S.. 2001. Constrained K-Means Clustering with Background Knowledge. Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>Proceedings ofACL.</booktitle>
<contexts>
<context position="2165" citStr="Yarowsky, 1995" startWordPosition="335" endWordPosition="336">disambiguation can be defined as associating a target word in a text or discourse 1“incomplete tagged corpus” means that tagged corpus does not include the instances of some senses for the target word, while these senses may occur in untagged texts. with a definition or meaning. Many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacock et al., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). Supervised methods usually rely on the information from previously sense tagged corpora to determine the senses of words in unseen texts. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994), or sense-tagged seed examples (Yarowsky, 1995). Some observations</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, D.. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>J Han</author>
<author>K C-C Chang</author>
</authors>
<title>PEBL: Positive example based learning for web page classification using SVM.</title>
<date>2002</date>
<booktitle>Proceedings of ACM SIGKDD.</booktitle>
<contexts>
<context position="29096" citStr="Yu et al., 2002" startWordPosition="4996" endWordPosition="4999">ta for clustering or ELP based methods. Moreover, clustering based method performs better than ELP based method in terms of order identification when given less labeled data (e.g., Ssubset = {s1}). It seems that ELP is not robust to the noise in small labeled data, compared with the semi-supervised k-means clustering algorithm. 4 Related Work The work closest to ours is partially supervised classification or building classifiers using positive examples and unlabeled examples, which has been studied in machine learning community (Denis et al., 2002; Liu et al., 2003; Manevitz and Yousef, 2001; Yu et al., 2002). However, they cannot 421 group negative examples into meaningful clusters. In contrast, our algorithm can find the occurrence of negative examples and further group these negative examples into a “natural” number of clusters. Semi-supervised clustering (Wagstaff et al., 2001) may be used to perform classification by the use of labeled and unlabeled examples, but it encounters the same problem of partially supervised classification that model order cannot be automatically estimated. Levine and Domany (2001) and Lange et al. (2002) proposed cluster validation based criteria for cluster number </context>
</contexts>
<marker>Yu, Han, Chang, 2002</marker>
<rawString>Yu, H., Han, J., &amp; Chang, K. C.-C.. 2002. PEBL: Positive example based learning for web page classification using SVM. Proceedings of ACM SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
</authors>
<date>2002</date>
<booktitle>Learning from Labeled and Unlabeled Data with Label Propagation. CMU CALD tech report</booktitle>
<pages>02--107</pages>
<contexts>
<context position="8812" citStr="Zhu and Ghahramani, 2002" startWordPosition="1446" endWordPosition="1450">s.Y0XL is consistent with the labeling in labeled data, and the initialization of Y0XU can be arbitrary. Let k denote the possible value of the number of senses in mixed data XL+U, and kXL be the number of senses in initial tagged data XL. Note that kXL = |SL|, and k &gt; kXL. The classification algorithm in the order identification process should be able to accept labeled data DL 5, unlabeled data DU 6 and model order k as input, and assign a class label or a cluster index to each instance in DU as output. Previous supervised or semi-supervised algorithms (e.g. SVM, label propagation algorithm (Zhu and Ghahramani, 2002)) cannot classify the examples in DU into k groups if k &gt; kXL. The semi-supervised kmeans clustering algorithm (Wagstaff et al., 2001) may be used to perform clustering analysis on mixed data, but its efficiency is a problem for clustering analysis on a very large dataset since multiple restarts are usually required to avoid local optima and multiple iterations will be run in each clustering process for optimizing a clustering solution. In this work, we propose an alternative method, an extended label propagation algorithm (ELP), which can classify the examples in DU into k groups. If the valu</context>
<context position="22081" citStr="Zhu and Ghahramani, 2002" startWordPosition="3830" endWordPosition="3834">of iteration steps is more than 30. Kmin = kXL = |SL|, Kmax = Kmin + m. m is set as 4. We used Jensen-Shannon (JS) divergence (Lin, 1991) as distance measure for semi-supervised clustering and ELP, since plain LP with JS divergence achieves better performance than that with cosine similarity on SENSEVAL-3 data (Niu et al., 2005). For the LP process in ELP algorithm, we constructed connected graphs as follows: two instances u, v will be connected by an edge if u is among v’s 10 nearest neighbors, or if v is among u’s 10 nearest neighbors as measured by cosine or JS distance measure (following (Zhu and Ghahramani, 2002)). We used three types of features to capture the information in all the contextual sentences of target words in SENSEVAL-3 data for all the four algorithms: part-of-speech of neighboring words with position information, words in topical context without position information (after removing stop words), and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). We removed the features with occurrence frequency (counted in both training set and test set) less than 3 times. If the estimated sense number is more than the sense num</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Zhu, X. &amp; Ghahramani, Z.. 2002. Learning from Labeled and Unlabeled Data with Label Propagation. CMU CALD tech report CMU-CALD-02-107.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>