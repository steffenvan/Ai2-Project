<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000274">
<note confidence="0.444606666666667">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 238-247.
Association for Computational Linguistics.
</note>
<title confidence="0.992784">
The SuperARV Language Model: Investigating the Effectiveness
of Tightly Integrating Multiple Knowledge Sources
</title>
<author confidence="0.999586">
Wen Wang and Mary P. Harper
</author>
<affiliation confidence="0.916717666666667">
School of Electrical and Computer Engineering
Purdue University
1285 The Electrical Engineering Building
</affiliation>
<address confidence="0.888046">
West Lafayette, IN 47907-1285
</address>
<email confidence="0.999591">
{wang28,harper}@ecn.purdue.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999788259259259">
A new almost-parsing language model incorporat-
ing multiple knowledge sources that is based upon
the concept of Constraint Dependency Grammars is
presented in this paper. Lexical features and syn-
tactic constraints are tightly integrated into a uni-
form linguistic structure called a SuperARV that is
associated with a word in the lexicon. The Super-
ARV language model reduces perplexity and word er-
ror rate compared to trigram, part-of-speech-based,
and parser-based language models. The relative con-
tributions of the various knowledge sources to the
strength of our model are also investigated by using
constraint relaxation at the level of the knowledge
sources. We have found that although each knowl-
edge source contributes to language model quality,
lexical features are an outstanding contributor when
they are tightly integrated with word identity and
syntactic constraints. Our investigation also suggests
possible reasons for the reported poor performance
of several probabilistic dependency grammar models
in the literature.
these classes to compute n-gram probabilities. Part-
of-Speech (POS) tags were initially used as classes
by Jelinek (1990) in a conditional probabilistic
model (which predicts the tag sequence for a word
sequence first and then uses it to predict the word
sequence):
</bodyText>
<equation confidence="0.9875485">
Pr(ti|ti−1
1 )Pr(wi|ti) (1)
</equation>
<bodyText confidence="0.999769571428571">
However, Jelinek’s POS LM is less effective at pre-
dicting word candidates than an n-gram word-based
LM because it deletes important lexical information
for predicting the next word. Heeman’s (1998) POS
LM achieves a perplexity reduction compared to a
trigram LM by instead redefining the speech recog-
nition problem as determining:
</bodyText>
<equation confidence="0.83878075">
W∗, T∗ = arg max
W,T
P(W, T)P(A|W, T)
�Pr(wN1 ) ≈
t1,t2,...,tN
N
11
P(W,T|A)
= arg max
W,T
1 Introduction ≈ arg max P(W,T)P(A|W)
W,T
</equation>
<bodyText confidence="0.999378185185185">
The purpose of a language model (LM) is to de-
termine the a priori probability of a word sequence
w1, ..., wn, P(w1, ... , wn). Language modeling is es-
sential in a wide variety of applications; we focus on
speech recognition in our research. Although word-
based LMs (with bigram and trigram being the most
common) remain the mainstay in many continuous
speech recognition systems, recent efforts have ex-
plored a variety of ways to improve LM performance
(Niesler and Woodland, 1996; Chelba et al., 1997;
Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosen-
feld, 2000; Goodman, 2001; Roark, 2001; Charniak,
2001).
Class-based LMs attempt to deal with data sparse-
ness and generalize better to unseen word sequences
by first grouping words into classes and then using
where T is the POS sequence tN 1 associated with the
word sequence W = wN1 given the speech utterance
A. The LM P(W, T) is a joint probabilistic model
that accounts for both the sequence of words wN 1
and their tag assignments tN 1 by estimating the joint
probabilities of words and tags:
Johnson (2001) and Lafferty et al. (2001) provide
insight into why a joint model is superior to a con-
ditional model.
Recently, there has been good progress in devel-
oping structured models (Chelba, 2000; Charniak,
</bodyText>
<note confidence="0.883513">
N N N P(wi, ti|wi−1
P(w1 , t1 ) = ri 1 ,ti−1
1 ) (2)
</note>
<bodyText confidence="0.998145419354839">
2001; Roark, 2001) that incorporate syntactic infor-
mation. These LMs capture the hierarchical char-
acteristics of a language rather than specific infor-
mation about words and their lexical features (e.g.,
case, number). In an attempt to incorporate even
more knowledge into a structured LM, Goodman
(1997) has developed a probabilistic feature gram-
mar (PFG) that conditions not only on structure
but also on a small set of grammatical features (e.g.,
number) and has achieved parse accuracy improve-
ment. Goodman’s work suggests that integrating
lexical features with word identity and syntax would
benefit LM predictiveness. PFG uses only a small set
of lexical features because it integrates those features
at the level of the production rules, causing a signif-
icant increase in grammar size and a concomitant
data sparsity problem that preclude the addition of
richer features. This sparseness problem can be ad-
dressed by associating lexical features directly with
words.
We hypothesize that high levels of word predic-
tion capability can be achieved by tightly integrat-
ing structural constraints and lexical features at the
word level. Hence, we develop a new dependency-
grammar almost-parsing LM, SuperARV LM, which
uses enriched tags called SuperARVs. In Section 2,
we introduce our SuperARV LM. Section 3 compares
the performance of the SuperARV LM to other LMs.
Section 4 investigates the knowledge source contribu-
tions by constraint relaxation. Conclusions appear in
Section 5.
</bodyText>
<sectionHeader confidence="0.998101" genericHeader="method">
2 SuperARV Language Model
</sectionHeader>
<bodyText confidence="0.99994580952381">
The SuperARV LM is a highly lexicalized probabilis-
tic LM based on the Constraint Dependency Gram-
mar (CDG) (Harper and Helzerman, 1995). CDG
represents a parse as assignments of dependency re-
lations to functional variables (denoted roles) asso-
ciated with each word in a sentence. Consider the
parse for What did you learn depicted in the white
box of Figure 1. Each word in the parse has a lexi-
cal category and a set of feature values. Also, each
word has a governor role (denoted G) which is as-
signed a role value, comprised of a label as well as a
modifiee, which indicates the position of the word’s
governor or head. For example, the role value as-
signed to the governor role of did is vp-1, where its
label vp indicates its grammatical function and its
modifiee 1 is the position of its head what. The need
roles (denoted N1, N2, and N3) are used to ensure
the grammatical requirements (e.g., subcategoriza-
tion) of a word are met, as in the case of the verb
did, which needs a subject and a base form verb (but
since the word takes no other complements, the mod-
</bodyText>
<table confidence="0.999012866666667">
1 2 3 4
what did you learn
pronoun verbsubcat=base pronoun subcat=obj
case=common verbtype=past case=common vtype=infinitive
behavior=nominal voice=active behavior=nominal voice=active
type=interrogative inverted=yes type=personal inverted=no
semtype=inanimate type=none semtype=human type=none
agr=3s gapp=yes agr=2s gapp=yes
mood=whquestion mood=whquestion
semtype=auxiliary semtype=behavior
agr=all agr=none
G=np-4 G=vp-1 G=subj-2 G=vp-2
Need1=S-3 Need1=S-4
Need2=S-4 Need2=S-1
Need3=S-2 Need3=S-4
</table>
<figureCaption confidence="0.995499857142857">
Figure 1: An example of a CDG parse, an ARV and
ARVP, and the SuperARV of the word did in the sentence
what did you learn. Note: G represents the governor
role; the need roles, Need1, Need2, and Need3, are used
to ensure that the grammatical requirements of the word
are met. PX and MX([R]) represent the position of a
word and its modifiee (for role R), respectively.
</figureCaption>
<bodyText confidence="0.977555296296296">
ifiee of the role value assigned to N3 is set equal to
its own position). Including need roles also provides
a mechanism for using non-headword dependencies
to constrain parse structures, which Bod (2001) has
shown contributes to improved parsing accuracy.
During parsing, the grammaticality of a sentence
in a language defined by a CDG is determined by
applying a set of constraints to the possible role
value assignments (Harper and Helzerman, 1995;
Maruyama, 1990). Originally, the constraints were
comprised of a set of hand-written rules specifying
which role values (unary constraints) and pairs of
role values (binary constraints) were grammatical
(Maruyama, 1990). In order to derive the constraints
directly from CDG annotated sentences, we have de-
veloped an algorithm to extract grammar relations
using information derived directly from annotated
sentences (Harper et al., 2000; Harper and Wang,
2001). Using the relationship between a role value’s
position and its modifiee’s position, unary and bi-
nary constraints can be represented as a finite set of
abstract role values (ARVs) and abstract role value
pairs (ARVPs), respectively. The light gray box of
Figure 1 shows an example of an ARV and an ARVP.
ARV for vp-1 assigned to G for did:
cat1=verb, subcat=base, verbtype=past, voice=active, inverted=yes, type=none,
gapp=yes, mood=whquestion, semtype=auxiliary, agr=all,
</bodyText>
<equation confidence="0.9316462">
role1=G, label1=vp, (PX1&gt;MX1)
ARVP for vp-1 assigned to G for did and subj-2 assigned to G for you:
cat1=verb, subcat=base, verbtype=past, voice=active, inverted=yes, type=none, gapp=yes,
mood=whquestion, semtype=auxiliary, agr=all,
role1=G, label1=vp, (PX1&gt;MX1)
cat2=pronoun, case=common, behavior=nominal, type=personal, semtype=human, agr=2s,
role2=G, label2=subj, (PX2&gt;MX2)
(PX1&lt;PX2), (MX1&lt;MX2),(PX1=MX2),(MX1&lt;PX2)
C
F
</equation>
<figure confidence="0.772936590909091">
(R,L,UC,MC)+
DC
The SuperARV of the word &amp;quot;did&amp;quot;:
Dependent Positional Constraints:
MX[G] &lt; PX = MX[Need3] &lt; MX[Need1]
&lt; MX[Need2]
Role=G, Label=vp, PX&gt;MX,
Role=Need1, Label=S, PX&lt;MX,
Role=Need2, Label=S, PX&lt;MX,
Role=Need3, Label=S, PX=MX,
Features: {verbtype=past, voice=active, inverted=yes, type=none,
gapp=yes,mood=whquestion,agr=all}
Category: Verb
}
(ModifieeCategory=pronoun)
(ModifieeCategory=pronoun)
(ModifieeCategory=verb)
(ModifieeCategory=verb)
MC
}
need role
constraints
</figure>
<bodyText confidence="0.999962949367089">
The ARV for the governor role value of did indicates
its lexical category, lexical features, role, label, and
positional relation information. (PX1 &gt; MX1) in-
dicates that did is governed by a word that precedes
it. Note that the constraints of a CDG can be ex-
tracted from a corpus of parsed sentences.
A super abstract role value (SuperARV) is an ab-
straction of the joint assignment of dependencies for
a word, which provides a mechanism for lexicaliz-
ing CDG parse rules. The dark gray box of Figure 1
presents an example of a SuperARV for the word did.
The SuperARV structure provides an explicit way to
organize information concerning one consistent set of
dependency links for a word that can be directly de-
rived from its parse assignments. SuperARVs encode
lexical information as well as syntactic and semantic
constraints in a uniform representation that is much
more fine-grained than POS. A SuperARV can be
thought of as providing admissibility constraints on
syntactic and lexical environments in which a word
may be used.
A SuperARV is formally defined as a four-tuple
for a word, (C, F, (R, L, UC, MC)+, DC), where C
is the lexical category of the word, F = {Fname1
= Fvalue1, ..., FNamef = FValuefI is a fea-
ture vector (where Fnamei is the name of a feature
and Fvaluei is its corresponding value), (R, L, UC,
MC)+ is a list of one or more four-tuples, each rep-
resenting an abstraction of a role value assignment,
where R is a role variable, L is a functionality la-
bel, UC represents the relative position relation of
a word and its dependent, MC is the lexical cat-
egory of the modifiee for this dependency relation,
and DC represents the relative ordering of the po-
sitions of a word and all of its modifiees. The fol-
lowing features are used in our SuperARV LM: agr,
case, vtype (e.g., progressive), mood, gapp (e.g.,
gap or not), inverted, voice, behavior (e.g., mass,
count), type (e.g., interrogative, relative). These
lexical features constitute a much richer set than the
features used by the parser-based LMs in Section 1.
Since Harper et al. (1999) found that enforcing mod-
ifiee constraints (e.g., the lexical categories of modi-
fiees) in parsing results in efficient pruning, we also
include the modifiee lexical category (MC) in our Su-
perARV structure to impose modifiee constraints.
Words typically have more than one SuperARV to
indicate different types of word usage. The average
number of SuperARVs for words of different lexical
categories vary, with verbs having the greatest Su-
perARV ambiguity. This is mostly due to the vari-
ety of feature combinations and variations on com-
plement types and positions. We have observed in
several experiments that the number of SuperARVs
does not grow significantly as training set size in-
creases; the moderate-sized Resource Management
corpus (Price et al., 1988) with 25,168 words pro-
duces 328 SuperARVs, compared to 538 SuperARVs
for the 1 million word Wall Street Journal (WSJ)
Penn Treebank set (Marcus et al., 1993), and 791 for
the 37 million word training set of the WSJ contin-
uous speech recognition task.
SuperARVs can be accumulated from a corpus an-
notated with CDG relations and stored directly with
words in a lexicon, so we can learn their frequency
of occurrence for the corresponding word. A Super-
ARV can then be selected from the lexicon and used
to generate role values that meet their constraints.
Since there are no large benchmark corpora anno-
tated with CDG information1, we have developed a
methodology to automatically transform constituent
bracketing found in available treebanks into CDG
annotations. In addition to generating dependency
structures by headword percolation (Chelba, 2000),
our transformer also utilizes a rule-based method to
determine lexical features and need role values for
words, as described by Wang et al. (2001).
Our SuperARV LM estimates the joint probability
of words wN1 and their SuperARV tags tN1 :
</bodyText>
<equation confidence="0.998413">
Pr(ti|wi−1
1 ti−1
1 ) · Pr(wi|wi−1
1 ti 1)
Pr(ti|wi−1
i−2ti−1
i−2) · Pr(wi|wi−1
i−2tii−2) (3)
</equation>
<bodyText confidence="0.982758066666667">
Notice we use a joint probabilistic model to enable
the joint prediction of words and their SuperARVs so
that word form information is tightly integrated at
the model level. Our SuperARV LM does not encode
the word identity directly at the data structure level
as was done in (Galescu and Ringger, 1999) since
this could cause serious data sparsity problems.
To estimate the probability distributions in Equa-
tion (3) from training data, we use recursive lin-
ear interpolation among probability estimations of
different orders. Representing each multiplicand
in Equation (3) as the conditional probability
ˆP(x|y1, y2, ..., yam,) where y1, y2, ..., yam, belong to a
mixed set of words and SuperARVs, the recursive
linear interpolation is calculated as follows:
</bodyText>
<footnote confidence="0.91619425">
1We have annotated a moderate-sized corpus,
DARPA Naval Resource Management (Price et al., 1988),
with CDG parse relations as reported in (Harper et al.,
2000; Harper and Wang, 2001).
</footnote>
<equation confidence="0.99625575">
Pr(wN1 tN1 ) = N Pr(witi|wi−1
11 1 ti−1
1 )
=
N
ri
≈
N
ri
ˆPn(x|y1, y2, ... ,yn)
= λ(x, y1, y2, ... , yn) · Pn(x|y1, y2,. . . , yn)
+(1 − λ(x, y1, y2, ..., yn)) · ˆPn−1(x|y1, y2, ... , yn−1)
PPL = 2En (4)
i−1
Pˆ(w)
1
En ≈ − N
N
log2
i=1
r_ti−2,ilog2
Pˆ(witi|wi−1
i−2ti−1
i−2)ˆP(wi−1
i−2ti−1
i−2)
i−1 i−1
Pˆ(wi−2ti−2)
where: ≈ − 1 N
N i=1
r_
ti−2,i−1
</equation>
<listItem confidence="0.975079">
• y1, y2, ... , yn is the context of order n-gram to
predict x;
• Pn(x|y1, y2, ..., yn) is the order n-gram maximum
likelihood estimation.
</listItem>
<bodyText confidence="0.9991394">
Table 1 enumerates the n-grams and their order for
the interpolation smoothing of the two distributions
in Equation (3). The ordering was based on our hy-
pothesis that n-grams with more fine-grained history
information should be ranked higher in the n-gram
list since that information should be more helpful
for discerning word and SuperARVs based on their
history. The SuperARV LM hypothesizes categories
for out-of-vocabulary words using the leave-one-out
technique (Niesler and Woodland, 1996).
</bodyText>
<tableCaption confidence="0.8037245">
Table 1: The enumeration and order of n-grams for
smoothing the distributions in Equation (3).
</tableCaption>
<equation confidence="0.9440688">
n-grams Pˆ(ti|wi−1 Pˆ(wi|wi−1
i−2ti−1 i−2tii−2)
i−2)
highest Pˆ (ti|wi−1 Pˆ (wi|wi−1
lowest i−2ti−1 i−2ti i−2)
i−2) Pˆ (wi|wi−1
Pˆ(ti|wi−1ti−1 i−2ti i−1)
i−2) Pˆ(wi|wi−1tii−2)
Pˆ (ti|wi−1 Pˆ(wi|wi−1tii−1)
i−2ti−1) Pˆ(wi|wi−1ti)
Pˆ(ti|ti−1 Pˆ(wi|tii−1)
i−2) Pˆ(wi|ti)
Pˆ(ti|wi−1ti−1)
Pˆ(ti|ti−1)
Pˆ(ti)
</equation>
<bodyText confidence="0.999880285714286">
In preliminary experiments, we compared several
algorithms for smoothing the probability estima-
tions for our SuperARV LM. The best performance
was achieved by using the modified Kneser-Ney
smoothing algorithm initially introduced in (Chen
and Goodman, 1998) and adapting it by employing
a heldout data set to optimize parameters, includ-
ing cutoffs for rare n-grams, by using Powell’s search
(Press et al., 1988). Parameters are chosen to opti-
mize the perplexity on a heldout set.
In order to compare our SuperARV LM with a
word-based LM, we must use the following equation
to calculate the word perplexity (PPL):
Equation (4) is used by class-based LMs to calculate
word perplexity (Heeman, 1998). Parser-based LMs
use a similar procedure that sums over parses.
The SuperARV LM is most closely related to
the almost-parsing-based LM developed by Srinivas
(1997). Srinivas’ LM, based on the notion of a su-
pertag, the elementary structure of Lexicalized Tree-
Adjoining Grammar, achieved a perplexity reduction
compared to a conditional POS n-gram LM (Niesler
and Woodland, 1996). By comparison, our LM in-
corporates dependencies directly on words instead
of through nonterminals, uses more lexical features
than the supertag LM, uses joint instead of con-
ditional probability estimations, and uses modified
Kneser-Ney rather than Katz smoothing.
</bodyText>
<sectionHeader confidence="0.8218885" genericHeader="method">
3 Evaluating the SuperARV
Language Model
</sectionHeader>
<bodyText confidence="0.999837538461538">
Traditionally, the LM quality in speech recognition is
evaluated on two metrics: perplexity and WER, with
the former commonly selected as a less computation-
ally expensive alternative. We carried out two exper-
iments, one using the Wall Street Journal Penn Tree-
bank (WSJ PTB), a text corpus on which perplexity
can be measured and compared to other LMs, and
the Wall Street Journal Continuous Speech Recog-
nition (WSJ CSR) task, a speech corpus on which
both perplexity and WER can be evaluated after LM
rescoring. These two experiments compare our Su-
perARV LM to a baseline trigram, a POS LM that
was implemented using Equation (3) (where for this
model t represents POS tags instead of SuperARV
tags) and modified Kneser-Ney smoothing (as used
in the SuperARV LM), and one or more parser-based
LMs. Additionally, we evaluate the performance of a
conditional probability SuperARV LM (denoted cSu-
perARV) implemented following Equation (1) rather
than Equation (3) to evaluate the importance of us-
ing joint probability estimations.
For the WSJ PTB task, we compare the Super-
ARV LMs to the parser LMs developed by Chelba
(2000), Roark (2001), and Charniak (2001). Al-
though Srinivas (1997) developed an almost-parsing
supertag-based LM, we cannot compare his LM with
the other LMs because he used a small non-standard
subset of the WSJ PTB2 and a trainable supertag
LM is unavailable. Because none of the parser LMs
has been fully trained for the WSJ CSR task, it is
essential that we retrain them for comparison. The
availability of a trainable version of Chelba’s model
enables us to train and test on the CSR task; how-
ever, because we do not have access to a trainable
version of Charniak’s or Roark’s LMs, they are not
considered in the CSR task. Note that for lattice
rescoring, however, Roark found that Chelba’s model
achieves a greater reduction on WER than his LM
(Roark, 2001).
</bodyText>
<subsectionHeader confidence="0.998714">
3.1 Evaluating on the WSJ PTB
</subsectionHeader>
<bodyText confidence="0.999778324324324">
To evaluate the perplexity of the LMs on the WSJ
PTB task, we adopted the conventions of Chelba
(2000), Roark (2001), and Charniak (2001) for pre-
processing the data. The vocabulary is limited to
the most common 10K words, with all words outside
this vocabulary mapped to (UNK). All punctuation
is removed and no case information is retained. All
symbols and digits are replaced by the symbol N.
Sections 0-20 (929,564 words) are used as the train-
ing set for collecting counts, sections 21-22 (73,760
words) as the development set for tuning parameters,
and sections 23-24 (82,430 words) for testing.
The baseline trigram uses Katz back-off model
with Good-Turing discounting for smoothing. The
POS, cSuperARV, and SuperARV LMs were imple-
mented as described previously. The results for the
parser-based LMs were initially taken from the lit-
erature. The perplexity on the test set using each
LM and their interpolation with the corresponding
trigram (and the interpolation weight) are shown in
the top six rows of Table 2.
As can be seen in Table 2, the SuperARV LM ob-
tains the lowest perplexity of all of the LMs (and
so it is depicted in bold face). The SuperARV LM
achieves the greatest perplexity reduction of 29.19%
compared to the trigram, with Charniak’s interpo-
lated trihead LM a close second at 24.91%. The cSu-
perARV LM is clearly inferior to the SuperARV LM,
even after interpolation. This result highlights the
value of tight coupling of word, lexical feature, and
syntactic knowledge both at the data structure level
(which is the same for the SuperARV and cSuper-
ARV LMs) and at the probability model level (which
is different).
Notice that the cSuperARV, Chelba’s, Roark’s,
and Charniak’s LMs obtain an improvement in per-
formance when interpolated with a trigram; whereas,
</bodyText>
<footnote confidence="0.977745">
2Using the same 180,000 word training and 20,000
word test set as (Srinivas, 1997), our SuperARV LM ob-
tains a perplexity of 92.76, compared to a perplexity of
101 obtained by the supertag LM.
</footnote>
<bodyText confidence="0.9998915">
the POS LM and the SuperARV LM do not benefit
from trigram interpolation3. To gain more insight
into why a trigram is effectively interpolated with
some, but not all, of the LMs, we calculate the cor-
relation of the trigram with each LM. A standard
correlation is calculated between the probabilities as-
signed to each test set sentence by the trigram LM
and the LM in question. This technique has been
used in (Wang et al., 2002) to identify whether two
LMs can be effectively interpolated.
Since we have access to an executable ver-
sion of Charniak’s LM trained on the WSJ PTB
(ftp.cs.brown.edu/pub/nlparser) and a trainable ver-
sion of Chelba’s LM, we are able to calculate their
correlations with our trigram LM. Chelba’s LM was
retrained using more parameter reestimation itera-
tions than in (Chelba, 2000) to optimize the per-
formance. Table 2 shows the correlation between
each of the executable LMs and the trigram LM.
The POS LM has the highest correlation with the
trigram, closely followed by the SuperARV LM. Be-
cause these two LMs tightly integrate the word infor-
mation jointly with the tag distribution, the trigram
information is already represented. In contrast, the
cSuperARV LM and Chelba’s and Charniak’s parser-
based LMs have much lower correlations, indicating
they have much lower overlap with the trigram. Be-
cause the cSuperARV LM only uses weak word dis-
tribution information in probability estimations, it
leaves room for the trigram LM to compensate for
the lack of word knowledge. The correlations for the
parser-based LMs suggest that they capture different
aspects of the words’ distributions in the language
than the words themselves.
</bodyText>
<subsectionHeader confidence="0.999836">
3.2 Evaluating on the WSJ CSR Task
</subsectionHeader>
<bodyText confidence="0.99996475">
Next we compare the effectiveness of using the tri-
gram word-based, POS, cSuperARV, SuperARV,
and Chelba’s LMs in rescoring hypotheses generated
by a speech recognizer. The training set of the WSJ
CSR task is composed of the 1987-1989 files con-
taining 37,243,300 words. The speech data for the
training set is used for building the acoustic model;
whereas, the parse trees for the training set are gen-
erated following the policy that if the context-free
grammar constituent bracketing can be found in the
WSJ PTB, it becomes the parse tree for the training
sentence; otherwise, we use the corresponding tree in
the BLLIP treebank (Charniak et al., 2000). Since
WSJ CSR is a speech corpus, there is no punctua-
tion or case information. All words outside the pro-
vided vocabulary are mapped to (UNK). Note that
</bodyText>
<footnote confidence="0.91168">
3In the remaining experiments, the POS LM and the
SuperARV LM are not interpolated with a trigram.
</footnote>
<table confidence="0.9994013">
Perplexity
LM 3gram Model Intp (Weight) r
POS 167.14 142.55 142.55 (1.0) 0.95
SuperARV 167.14 118.35 118.35 (1.0) 0.92
cSuperARV 167.14 150.01 143.83 (0.65) 0.68
Chelba (2000) 167.14 158.28 148.90 (0.64) N/A
Roark (2001) 167.02 152.26 137.26 (0.64) N/A
Charniak (2001) 167.89 130.20 126.07 (0.64) N/A
Chelba 167.14 153.76 147.70 (0.64) 0.73
Charniak 167.14 130.20 126.03 (0.64) 0.69
</table>
<tableCaption confidence="0.989682333333333">
Table 2: Comparing perplexity results for each LM on the WSJ PTB test set. 3gram represents the word-based
trigram LM, Intp (weight)the LM interpolated with a trigram (and the interpolation weight), and r the correlation
value. N/A means not available.
</tableCaption>
<bodyText confidence="0.998751958333334">
the word-level tokenization of treebank texts differs
from that used in the speech recognition task with
the major differences being: numbers (e.g., “1.2%”
versus “one point two percent”), dates (e.g., “Dec.
20, 2001” versus “December twentieth, two thou-
sand one”) , currencies (e.g., “$10.25” versus “ten
dollars and twenty five cents”), common abbrevia-
tions (e.g., “Inc.” versus “Incorporated”), acronyms
(e.g., “I.B.M.” versus “I. B. M.”), hyphenated and
period-delimited phrases (e.g., “red-carpet” versus
“red carpet”), and contractions and possessives (e.g.,
“do n’t” versus “don’t”). The POS, parser-based,
and SuperARV LMs are all trained using the text-
based tokenization from the treebank. Hence, during
testing, a transformation converts the output of the
recognizer to a form compatible with the text-based
tokenization (Roark, 2001) for rescoring.
For testing the LMs, we use the four available
WSJ CSR evaluation sets: 1992 5K closed vocab-
ulary (denoted 92-5k) with 330 utterances and 5,353
words, 1993 5K closed vocabulary (93-5k) with 215
utterances and 3,849 words, 1992 20K open vocabu-
lary (92-20k) with 333 utterances and 5,643 words,
and 1993 20K (93-20k) with 213 utterances and
3,446 words. We also employ a development set for
each vocabulary size: 93-5k-dt (513 utterances and
8,635 words) and 93-20k-dt (252 utterances and 4,062
words).
The trigram provided by LDC for the CSR task
was used due to its high quality. Before evaluation,
all the other LMs (i.e., the POS LM, the cSuperARV
and SuperARV LMs, and Chelba’s LM) are retrained
on the training set trees for the CSR task. Parameter
tuning for the LMs on each task uses the correspond-
ing development set4.
Perplexity Results Table 3 shows the perplexity
results for each test set with the best result for each
4The interpolation weight for cSuperARV for lattice
rescoring was 0.63 on the 5k tasks and 0.60 on the 20k
tasks, and 0.68 and 0.65 for Chelba’s LM, respectively.
in bold face. The SuperARV LM yields the lowest
perplexity, with Chelba’s LM a close second. The
perplexity reductions for the SuperARV LM over
the trigram across the test sets are 53.19%, 53.63%,
34.33%, and 32.05%, which is even higher than on
the WSJ PTB task. This is probably due to the fact
that more training data was used for the CSR task
(37 million words versus 1 million words).
</bodyText>
<table confidence="0.9995425">
LM 92-5k 93-5k 92-20k 93-20k
3gram 45.61 50.51 106.52 109.22
POS 44.21 30.26 98.79 96.64
cSuperARV 36.53 28.50 86.83 89.12
SuperARV 21.35 23.42 69.95 74.22
Chelba 23.92 25.07 77.16 79.37
</table>
<tableCaption confidence="0.996044">
Table 3: Comparing perplexity results for each LM on
the WSJ CSR test sets.
</tableCaption>
<bodyText confidence="0.999714134615385">
Rescoring Lattices Next using the same LMs, we
rescored the lattices generated by an acoustic recog-
nizer built using HTK (Ent, 1997). For each test
set sentence, we generated a word lattice. We tuned
the parameters of the LMs using the lattices on the
corresponding development sets to minimize WER.
Lattices were rescored using a Viterbi search for each
LM.
Table 4 shows the WER and sentence accuracy
(SAC) after rescoring lattices using each LM, with
the lowest WER and highest SAC for each test set
presented in bold face. We also give the lattice
WER/SAC which defines the best accuracy possible
given perfect knowledge. As can be seen from Table
4, the SuperARV LM produces the best reduction
in WER with Chelba’s LM the second best. When
rescoring lattices on the 92-5k, 93-5k, 92-20k, and
93-20k test sets, the SuperARV LM yields a relative
WER reduction of 13.54%, 9.70%, 8.64%, and 3.12%
compared to the trigram, respectively. SAC results
are similar: the SuperARV LM achieves an absolute
increase on SAC of 4.24%, 6.97%, 2.7%, and 3.75%,
compared to the trigram. Note that Chelba’s LM
tied once with the SuperARV LM on 93-20k SAC,
but always obtained higher WER across the four test
sets. Because Chelba’s LM focuses on developing the
complete parse structure for a word sequence, it en-
forces more strict pruning based on the entire sen-
tence. As can be seen in Table 4, the cSuperARV
LM, even when interpolated with a trigram LM, ob-
tains a lower accuracy than our SuperARV LM. This
result is consistent with the hypothesis that a con-
ditional model suffers from label bias (Lafferty et al.,
2001).
The WER reported by Chelba (2000) on the 93-
20k test set was 13.0%. This WER is lower than
what we obtained for Chelba’s retrained LM on the
same task. This disparity is due to the fact that a
higher quality acoustic decoder was used in (Chelba,
2000), which is not available to us. We further com-
pare the LMs on Dr. Chelba’s 93-20K lattices kindly
provided by him, with the rescoring results shown
in the last column of Table 4. We observe that
Chelba’s retrained LM improves his original result,
but the SuperARV LM still obtains a greater accu-
racy. Sign tests show that the differences between
the accuracies achieved by the SuperARV LM and
the trigram, POS, and cSuperARV LMs are statis-
tically significant. Although there is no significant
difference between the SuperARV LM and Chelba’s
LM, the SuperARV LM has a much lower complexity
than Chelba’s LM.
</bodyText>
<sectionHeader confidence="0.960579" genericHeader="method">
4 Investigating the Knowledge
</sectionHeader>
<subsectionHeader confidence="0.931863">
Source Contributions
</subsectionHeader>
<bodyText confidence="0.999973789473685">
Next, we attempt to explain the contrast between
the encouraging results from our SuperARV LM and
the reported poor performance of several probabilis-
tic dependency grammar models, i.e., the traditional
probabilistic dependency grammar (PDG) LM, the
probabilistic link grammar (PLG) (Lafferty et al.,
1992) LM, and Zeman’s probabilistic dependency
grammar model (ZPDG) (Hajic et al., 1998). ZPDG
was evaluated on the Prague Dependency Treebank
(Hajic, 1998) during the 1998 Johns Hopkins sum-
mer workshop (Hajic et al., 1998) and produced
a much lower parsing accuracy (under 60%) than
Collins’ probabilistic context-free grammar parser
(80%) (Collins, 1996). Fong et al. (1995) evalu-
ated the probabilistic link grammar LM described
in (Lafferty et al., 1992) on small artificial corpora
and found that the LM has a greater perplexity than
a standard bigram. Additionally, only a modest im-
provement on the bigram was achieved after Fong
and Wu (1995) revised the model to make grammar
rule learning feasible.
One possible reason for their poor performance, es-
pecially in the light of our SuperARV LM results, is
that these probabilistic dependency grammar mod-
els do not utilize sufficient knowledge to achieve a
high level of accuracy. The knowledge sources the
SuperARV LM uses, represented as components of
the structure shown in Figure 1, include: lexical
category (denoted c), lexical features (denoted f),
role label or link type information (denoted L), a
governor role dependency relation constraint (R, L,
UC) (denoted g), a set of need role dependency rela-
tion constraints (R, L, UC)+ (denoted n), and mod-
ifiee constraints represented as the lexical category
of the modifiee for each role (denoted m). Table
5 summarizes the knowledge sources that each of
the probabilistic dependency grammar models uses.
To determine whether the poor performance of the
three probabilistic dependency grammar models re-
sults from our hypothesis that they utilize insufficient
knowledge, we will evaluate our SuperARV LM af-
ter eliminating those knowledge sources that are not
used by each of these models. Additionally, we will
evaluate the contribution of each of the knowledge
sources to the predictiveness of our SuperARV LM.
We use the methodology of selectively ignoring dif-
ferent types of knowledge as constraints to evaluate
the knowledge source contributions to our SuperARV
LM, as well as to approximate the performance of
the other probabilistic dependency grammar models.
The framework of CDG, on which our SuperARV LM
is built, allows constraints to be tightened by adding
more knowledge sources or loosened by ignoring cer-
tain knowledge. The SuperARV structure inherits
this capability from CDG; selective constraint re-
laxation is implemented by eliminating one or more
knowledge source in 1C = {c, f, L, g, n, m} from the
SuperARV structure. We have constructed nine dif-
ferent LMs based on reduced SuperARV structures
denoted SARV-k (i.e., a SuperARV structure after
removing k with k C_ 1C), where −k represents the
deletion of a subset of knowledge types (e.g., f, mn,
cgmn). Each model is described next.
Modifiee constraints potentially hamper grammar
generality, and so we consider their impact by delet-
ing them from the LM by using the SARV-m struc-
ture. Need roles are important for capturing the
structural requirements of different types of words
(e.g., subcategorization), and we investigate their ef-
fects by using the SARV-n structure. The model
based on SARV-L is built to investigate the im-
portance of link type information. We can investi-
gate the contribution of the combination of m and
n, fundamental to the enforcement of valency con-
straints, by using the SARV-mn structure. The
model based on SARV-f is used to evaluate whether
</bodyText>
<table confidence="0.999656818181818">
LM Our HTK Lattices Chelba’s
93-20k lattices
92-5k 93-5k 92-20k 93-20k
WER(SAC) WER(SAC) WER(SAC) WER(SAC) WER(SAC)
3gram 4.43(61.52) 6.91(43.26) 11.11(36.94) 14.74(30.52) 13.72(36.18)
POS 3.92(64.85) 6.55(47.91) 10.58(38.14) 14.54(32.39) 13.51(37.96)
cSuperARV 3.89(65.15) 6.42(48.84) 10.51(38.44) 14.45(32.86) 13.32 (38.22)
SuperARV 3.83(65.76) 6.24(50.23) 10.15(39.64) 14.28(34.27) 12.87(42.02)
Chelba 3.85(65.45) 6.26(49.77) 10.19(39.34) 14.36(34.27) 12.93(40.48)
lattice 1.79(79.40) 2.16(73.95) 4.93(59.46) 6.65(52.11) 3.41 (68.86)
accuracy
</table>
<tableCaption confidence="0.99805">
Table 4: Comparing WER and SAC after rescoring lattices using each LM on WSJ CSR 5k- and 20k- test sets.
</tableCaption>
<table confidence="0.999791928571428">
knowledge PDG PLG ZPDG SARV
source
word identity ✓ ✓ lemma ✓
lexical ✓ ✓ ✓
category (c)
lexical morpho- ✓
features (f) logical
features
link type (L) ✓ ✓ ✓
link direction ✓ ✓ ✓ ✓
(UC)
valency (n) ✓
modifiee ✓ ✓
constraints (m)
</table>
<tableCaption confidence="0.94572475">
Table 5: Knowledge sources used by the three prob-
abilistic dependency grammar models compared to our
SuperARV LM. Note link type is defined as L and link
direction is defined as UC in the SuperARV structure.
</tableCaption>
<bodyText confidence="0.9998792">
lexical features improve or degrade LM quality. The
model based on SARV-fmn is very similar to the
standard probabilistic dependency grammar LM, in
which only word, POS, link type, and link di-
rection information is used for probability estima-
tions. The model based on SARV-gmn uses a fea-
ture augmentation of POS, and the model based on
SARV-cgmn uses lexical features only. Addition-
ally, we built the model ZPDG-SARV to approxi-
mate ZPDG. Zeman’s PDG (Hajic et al., 1998) dif-
fers significantly from our original SuperARV LM in
that it ignores label information L and some lexi-
cal feature information (the morphological tags do
not include some lexical features having influence
on syntax, denoted syntactic lexical features, i.e.,
gapp, inverted, mood, type, case, voice), and does
not enforce valency constraints (instead, the model
only counts the number of links associated with a
word without discriminating whether the links repre-
sent governing or linguistic structural requirements).
Also, word identity information is not used, instead,
the model uses a loose integration of a word’s lemma
and its morphological tag. Given this analysis, we
built the model ZPDG-SARV based on a structure
including lexical category, morphological features,
</bodyText>
<table confidence="0.9795185">
SARV-gmn 43.16 27.75 96.69 93.25
SARV-fmn 45.01 27.42 96.23 93.16
SARV-f 42.33 27.06 94.87 90.20
SARV-mn 40.38 26.96 90.23 89.54
SARV-n 35.02 26.08 87.32 88.04
SARV-L 28.76 25.71 82.45 84.82
SARV-m 26.86 25.58 80.24 83.12
SARV 21.35 23.42 69.95 74.22
</table>
<tableCaption confidence="0.940520333333333">
Table 6: Comparing perplexity results for each LM on
the WSJ CSR test sets. The LMs appear in decreasing
order of perplexity.
</tableCaption>
<bodyText confidence="0.982257615384615">
and (G, UC, MC).
Table 6 shows the perplexity results on the WSJ
CSR test sets ordered from highest to lowest for
each test set, with the best result for each in bold
face. The full SuperARV LM yields the lowest per-
plexity. We found that ignoring modifiee constraints
(SARV-m) increases perplexity the least, and ignor-
ing link type information (SARV-L) and need role
constraints (SARV-n) are a little worse than that.
Ignoring both knowledge sources (SARV-mn) should
result in even greater degradation, which is verified
by the results. However, ignoring lexical features
(SARV-f) produces an even greater increase in per-
plexity than relaxing both m and n. The SARV-
fmn, which is closest to the traditional probabilistic
dependency grammar LM, shows fairly poor qual-
ity, not much better than the POS LM. One might
hypothesize that lexical features individually con-
tribute the most to the overall performance of the Su-
perARV LM. However, using this knowledge source
by itself (SARV-cgmn) results in dramatic degrada-
tion on perplexity, in fact even worse than that of the
POS LM, but still slightly better than the baseline
trigram. However, as demonstrated by SARV-gmn,
the constraints from lexical features are strength-
ened by combining them with POS. Given the de-
</bodyText>
<figure confidence="0.985830769230769">
gram
.
.
.
.
POS
44.21
30.26
96.64
98.79
LM 92-5k 93-5k 92-20k 93-20k
SARV-cgmn 45.58 48.37 102.00 104.59
ZPDG-SARV 45.50 47.98 101.89 104.21
</figure>
<bodyText confidence="0.999428666666667">
scriptions in Table 5, we can approximate PLG by
a model based on a SuperARV structure eliminat-
ing f and m (which should have a quality between
SARV-f and SARV-fmn). It is noticeable that with-
out word identity information, syntactic lexical fea-
tures, and valency constraints, the ZPDG-SARV LM
performs worse than the POS-based LM and only
slightly better than the LM based on SARV-cgmn.
This suggests that ZPDG can be strengthened by
incorporating more knowledge.
The same ranking of the performance of the LMs
was obtained for WER/SAC after rescoring the lat-
tices using each LM, as shown in Table 7. Our exper-
iments with relaxed SuperARV LMs suggest likely
methods for improving PDG, PLG, and ZPDG mod-
els. The tight integration of word identity, lexical
category, lexical features, and structural dependency
constraints is likely to improve their performance.
Clearly the investigated knowledge sources are quite
synergistic, and their tight integration achieves the
greatest improvement on both perplexity and WER.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999940904761905">
We have compared our SuperARV LM to a variety of
LMs and found that it achieves both perplexity and
WER reductions compared to a trigram, and despite
the fact that it is an almost-parsing LM, it outper-
forms (or performs comparably to) the more com-
plex parser-based LMs on both perplexity and rescor-
ing accuracy. Additional experiments reveal that se-
lecting a joint instead of a conditional probabilistic
model is an important factor in the performance of
our SuperARV LM. The SuperARV structure pro-
vides a flexible framework that tightly couples a va-
riety of knowledge sources without combinatorial ex-
plosion. We found that although each knowledge
source contributes to the performance of the LM, it
is the tight integration of the word level knowledge
sources (word identity, POS, and lexical features) to-
gether with the structural information of governor
and subcategorization dependencies that produces
the best level of LM performance. We are currently
extending the almost-parsing SuperARV LM to a full
parser-based LM.
</bodyText>
<sectionHeader confidence="0.998845" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.8525974">
This research was supported by Intel, Purdue Re-
search Foundation, and National Science Founda-
tion under Grant No. IRI 97-04358, CDA 96-17388,
and BCS-9980054. We would like to thank the
anonymous reviewers for their comments and sug-
gestions. We would also like to thank Dr. Charniak,
Dr. Chelba, and Dr. Srinivas for their help with this
research effort. Finally, we would like to thank Yang
Liu (Purdue University) for providing us with the
WSJ CSR test set lattices.
</bodyText>
<sectionHeader confidence="0.995077" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999561795918367">
R. Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Pro-
ceedings of ACL’2001.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and
M. Johnson. 2000. BLLIP WSJ Corpus. CD-
ROM. Linguistics Data Consortium.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In Proceedings of ACL’2001.
C. Chelba, F. Jelinek, and S. Khudanpur. 1997.
Structure and performance of a dependency lan-
guage model. In Proceedings of Eurospeech, vol-
ume 5, pages 2775–2778.
C. Chelba. 2000. Exploiting Syntactic Structure for
Natural Language Modeling. Ph.D. thesis, Johns
Hopkins University.
S. F. Chen and J. T. Goodman. 1998. An empir-
ical study of smoothing techniques for language
modeling. Technical report, Harvard University,
Computer Science Group.
M. J. Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proceedings of
ACL’1996, pages 184–191.
Entropic Cambridge Research Laboratory, Ltd.,
1997. HTK: Hidden Markov Model Toolkit V2.1.
E. W. Fong and D. Wu. 1995. Learning restricted
probabilistic link grammars. Technical Report
HKUST-CS95-27, University of Science and Tech-
nology, Clear Water Bay, Hong Kong.
L. Galescu and E. K. Ringger. 1999. Augmenting
words with linguistic information for n-gram lan-
guage models. In Proceedings of Eurospeech.
J. Goodman. 1997. Probabilistic feature grammars.
In Proceedings of the Fourth international work-
shop on parsing technologies.
J. Goodman. 2001. A bit of progress in language
modeling, extended version. Technical Report
MSR-TR-2001-72, Microsoft Research, Redmond,
WA.
J. Hajic, E. Brill, M. Collins, B. Hladka, D. Jones,
C. Kuo, L. Ramshaw, O. Schwartz, C. Tillmann,
and D. Zeman. 1998. Core natural language
processing technology applicable to multiple lan-
guages – Workshop ’98. Technical report, Johns
Hopkins Univ.
J. Hajic. 1998. Building a syntactically anno-
tated corpus: The Prague Dependency Treekbank.
In Issues of Valency and Meaning ( Festschrift
for Jarmila Panevova), pages 106–132. Carolina,
Charles University, Prague.
</reference>
<table confidence="0.999731928571429">
LM 92-5k 93-5k 92-20k 93-20k
WER(SAC) WER(SAC) WER(SAC) WER(SAC)
3gram 4.43(61.52) 6.91(43.26) 11.11(36.94) 14.74(30.52)
SARV-cgmn 4.11(62.12) 6.78(45.12) 10.92(37.24) 14.63(31.46)
ZPDG-SARV 4.11(62.44) 6.71(46.02) 10.92(37.24) 14.63(31.65)
POS 3.92(64.85) 6.55(47.91) 10.58(38.14) 14.54(32.39)
SARV-gmn 3.92(64.85) 6.52(47.91) 10.56(38.14) 14.51(32.39)
SARV-fmn 3.92(64.85) 6.50(48.37) 10.53(38.14) 14.51(32.39)
SARV-f 3.92(64.85) 6.47(48.37) 10.49(38.14) 14.45(32.86)
SARV-mn 3.92(64.85) 6.44(48.37) 10.47(38.44) 14.42(32.86)
SARV-n 3.89(65.15) 6.39(48.37) 10.40(38.74) 14.39(33.33)
SARV-L 3.85(65.15) 6.29(48.92) 10.32(39.04) 14.39(33.33)
SARV-m 3.85(65.15) 6.29(49.77) 10.24(39.34) 14.35(33.80)
SARV 3.83(65.76) 6.24(50.23) 10.15(39.34) 14.28(34.27)
</table>
<tableCaption confidence="0.90972">
Table 7: Comparing WER and SAC after rescoring lattices using each LM on WSJ CSR 5k- and 20k- test sets. The
LMs appear in decreasing order of WER.
</tableCaption>
<reference confidence="0.987228342465754">
M. P. Harper and R. A. Helzerman. 1995. Exten-
sions to constraint dependency parsing for spoken
language processing. Computer Speech and Lan-
guage, 9:187–234.
M. P. Harper and W. Wang. 2001. Approaches for
learning constraint dependency grammar from cor-
pora. In Proceedings of the Grammar and Nat-
ural Language Processing Conference, Montreal,
Canada.
M. P. Harper, S. A. Hockema, and C. M. White.
1999. Enhanced constraint dependency grammar
parsers. In Proceedings of the IASTED Interna-
tional Conference on Artificial Intelligence and
Soft Computing.
M. P. Harper, C. M. White, W. Wang, M. T.
Johnson, and R. A. Helzerman. 2000. Effec-
tiveness of corpus-induced dependency grammars
for post-processing speech. In Proceedings of
NAACL’2000.
P. A. Heeman. 1998. POS tagging versus classes
in language modeling. In Proceedings of the 6th
Workshop on Very Large Corpora, Montreal.
F. Jelinek. 1990. Self-organized language modeling
for speech recognition. In Alex Waibel and Kai-Fu
Lee, editors, Readings in Speech Recognition. Mor-
gan Kaufman Publishers, Inc., San Mateo, CA.
M. Johnson. 2001. Joint and conditional estimation
of tagging and parsing models. In Proceedings of
ACL’2001.
J. D. Lafferty, D. Sleator, and D. Temperley. 1992.
Grammatical trigrams: A probabilistic model of
link grammar. In Proc. of AAAI Fall Symp. Prob-
abilistic Approaches to Natural Language, Cam-
bridge, MA.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic models for
segmenting and labeling sequence data. In Pro-
ceedings of ICML’2001.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313–330.
H. Maruyama. 1990. Structural disambiguation
with constraint propagation. In The Proceedings
of ACL’1990, pages 31–38.
T. R. Niesler and P. C. Woodland. 1996. A variable-
length category-based N-gram language model. In
Proceedings of ICASSP, volume 1, pages 164–167.
W. H. Press, B. P. Flannery, S. A. Teukolsky, and
W. T. Vetterling. 1988. Numerical Recipes in C.
Cambridge University Press, Cambridge.
P. J. Price, W. Fischer, J. Bernstein, and D. Pallett.
1988. A database for continuous speech recogni-
tion in a 1000-word domain. In Proceedings of
ICASSP’1988, pages 651–654.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27(2):249–276.
R. Rosenfeld. 2000. Two decades of statistical lan-
guage modeling: Where do we go from here? Pro-
ceedings of the IEEE, 88:1270–1278.
B. Srinivas. 1997. Complexity of lexical descriptions
and its relevance to partial parsing. Ph.D. thesis,
University of Pennsylvania.
W. Wang and M. P. Harper. 2001. Investigating
probabilistic constraint dependency grammars in
language modeling. Technical Report TR-ECE-
01-4, Purdue University, School of Electrical En-
gineering.
W. Wang, Y. Liu, and M. P. Harper. 2002. Rescor-
ing effectiveness of language models using different
levels of knowledge and their integration. In Pro-
ceedings of ICASSP’2002.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.416197">
<note confidence="0.930468333333333">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 238-247. Association for Computational Linguistics.</note>
<title confidence="0.9208945">The SuperARV Language Model: Investigating the Effectiveness of Tightly Integrating Multiple Knowledge Sources</title>
<author confidence="0.998754">Wen Wang</author>
<author confidence="0.998754">P Mary</author>
<affiliation confidence="0.9732765">School of Electrical and Computer Purdue</affiliation>
<address confidence="0.967104">1285 The Electrical Engineering West Lafayette, IN</address>
<abstract confidence="0.986280463414634">A new almost-parsing language model incorporating multiple knowledge sources that is based upon the concept of Constraint Dependency Grammars is presented in this paper. Lexical features and syntactic constraints are tightly integrated into a unilinguistic structure called a is associated with a word in the lexicon. The Super- ARV language model reduces perplexity and word error rate compared to trigram, part-of-speech-based, and parser-based language models. The relative contributions of the various knowledge sources to the strength of our model are also investigated by using constraint relaxation at the level of the knowledge sources. We have found that although each knowledge source contributes to language model quality, lexical features are an outstanding contributor when they are tightly integrated with word identity and syntactic constraints. Our investigation also suggests possible reasons for the reported poor performance of several probabilistic dependency grammar models in the literature. these classes to compute n-gram probabilities. Partof-Speech (POS) tags were initially used as classes Jelinek (1990) in a probabilistic predicts the tag sequence for a word sequence first and then uses it to predict the word sequence): (1) However, Jelinek’s POS LM is less effective at predicting word candidates than an n-gram word-based LM because it deletes important lexical information for predicting the next word. Heeman’s (1998) POS LM achieves a perplexity reduction compared to a trigram LM by instead redefining the speech recognition problem as determining: = arg max W,T T) N = arg max</abstract>
<intro confidence="0.901363">W,T</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>What is the minimal set of fragments that achieves maximal parse accuracy?</title>
<date>2001</date>
<booktitle>In Proceedings of ACL’2001.</booktitle>
<contexts>
<context position="7235" citStr="Bod (2001)" startWordPosition="1144" endWordPosition="1145">d2=S-4 Need2=S-1 Need3=S-2 Need3=S-4 Figure 1: An example of a CDG parse, an ARV and ARVP, and the SuperARV of the word did in the sentence what did you learn. Note: G represents the governor role; the need roles, Need1, Need2, and Need3, are used to ensure that the grammatical requirements of the word are met. PX and MX([R]) represent the position of a word and its modifiee (for role R), respectively. ifiee of the role value assigned to N3 is set equal to its own position). Including need roles also provides a mechanism for using non-headword dependencies to constrain parse structures, which Bod (2001) has shown contributes to improved parsing accuracy. During parsing, the grammaticality of a sentence in a language defined by a CDG is determined by applying a set of constraints to the possible role value assignments (Harper and Helzerman, 1995; Maruyama, 1990). Originally, the constraints were comprised of a set of hand-written rules specifying which role values (unary constraints) and pairs of role values (binary constraints) were grammatical (Maruyama, 1990). In order to derive the constraints directly from CDG annotated sentences, we have developed an algorithm to extract grammar relatio</context>
</contexts>
<marker>Bod, 2001</marker>
<rawString>R. Bod. 2001. What is the minimal set of fragments that achieves maximal parse accuracy? In Proceedings of ACL’2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>D Blaheta</author>
<author>N Ge</author>
<author>K Hall</author>
<author>M Johnson</author>
</authors>
<date>2000</date>
<journal>BLLIP WSJ Corpus. CDROM. Linguistics Data Consortium.</journal>
<contexts>
<context position="23267" citStr="Charniak et al., 2000" startWordPosition="3715" endWordPosition="3718">ess of using the trigram word-based, POS, cSuperARV, SuperARV, and Chelba’s LMs in rescoring hypotheses generated by a speech recognizer. The training set of the WSJ CSR task is composed of the 1987-1989 files containing 37,243,300 words. The speech data for the training set is used for building the acoustic model; whereas, the parse trees for the training set are generated following the policy that if the context-free grammar constituent bracketing can be found in the WSJ PTB, it becomes the parse tree for the training sentence; otherwise, we use the corresponding tree in the BLLIP treebank (Charniak et al., 2000). Since WSJ CSR is a speech corpus, there is no punctuation or case information. All words outside the provided vocabulary are mapped to (UNK). Note that 3In the remaining experiments, the POS LM and the SuperARV LM are not interpolated with a trigram. Perplexity LM 3gram Model Intp (Weight) r POS 167.14 142.55 142.55 (1.0) 0.95 SuperARV 167.14 118.35 118.35 (1.0) 0.92 cSuperARV 167.14 150.01 143.83 (0.65) 0.68 Chelba (2000) 167.14 158.28 148.90 (0.64) N/A Roark (2001) 167.02 152.26 137.26 (0.64) N/A Charniak (2001) 167.89 130.20 126.07 (0.64) N/A Chelba 167.14 153.76 147.70 (0.64) 0.73 Charni</context>
</contexts>
<marker>Charniak, Blaheta, Ge, Hall, Johnson, 2000</marker>
<rawString>E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson. 2000. BLLIP WSJ Corpus. CDROM. Linguistics Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL’2001.</booktitle>
<contexts>
<context position="2880" citStr="Charniak, 2001" startWordPosition="435" endWordPosition="436">A|W) W,T The purpose of a language model (LM) is to determine the a priori probability of a word sequence w1, ..., wn, P(w1, ... , wn). Language modeling is essential in a wide variety of applications; we focus on speech recognition in our research. Although wordbased LMs (with bigram and trigram being the most common) remain the mainstay in many continuous speech recognition systems, recent efforts have explored a variety of ways to improve LM performance (Niesler and Woodland, 1996; Chelba et al., 1997; Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosenfeld, 2000; Goodman, 2001; Roark, 2001; Charniak, 2001). Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using where T is the POS sequence tN 1 associated with the word sequence W = wN1 given the speech utterance A. The LM P(W, T) is a joint probabilistic model that accounts for both the sequence of words wN 1 and their tag assignments tN 1 by estimating the joint probabilities of words and tags: Johnson (2001) and Lafferty et al. (2001) provide insight into why a joint model is superior to a conditional model. Recently, there has been good progress i</context>
<context position="18193" citStr="Charniak (2001)" startWordPosition="2867" endWordPosition="2868">our SuperARV LM to a baseline trigram, a POS LM that was implemented using Equation (3) (where for this model t represents POS tags instead of SuperARV tags) and modified Kneser-Ney smoothing (as used in the SuperARV LM), and one or more parser-based LMs. Additionally, we evaluate the performance of a conditional probability SuperARV LM (denoted cSuperARV) implemented following Equation (1) rather than Equation (3) to evaluate the importance of using joint probability estimations. For the WSJ PTB task, we compare the SuperARV LMs to the parser LMs developed by Chelba (2000), Roark (2001), and Charniak (2001). Although Srinivas (1997) developed an almost-parsing supertag-based LM, we cannot compare his LM with the other LMs because he used a small non-standard subset of the WSJ PTB2 and a trainable supertag LM is unavailable. Because none of the parser LMs has been fully trained for the WSJ CSR task, it is essential that we retrain them for comparison. The availability of a trainable version of Chelba’s model enables us to train and test on the CSR task; however, because we do not have access to a trainable version of Charniak’s or Roark’s LMs, they are not considered in the CSR task. Note that fo</context>
<context position="23788" citStr="Charniak (2001)" startWordPosition="3803" endWordPosition="3804">ntence; otherwise, we use the corresponding tree in the BLLIP treebank (Charniak et al., 2000). Since WSJ CSR is a speech corpus, there is no punctuation or case information. All words outside the provided vocabulary are mapped to (UNK). Note that 3In the remaining experiments, the POS LM and the SuperARV LM are not interpolated with a trigram. Perplexity LM 3gram Model Intp (Weight) r POS 167.14 142.55 142.55 (1.0) 0.95 SuperARV 167.14 118.35 118.35 (1.0) 0.92 cSuperARV 167.14 150.01 143.83 (0.65) 0.68 Chelba (2000) 167.14 158.28 148.90 (0.64) N/A Roark (2001) 167.02 152.26 137.26 (0.64) N/A Charniak (2001) 167.89 130.20 126.07 (0.64) N/A Chelba 167.14 153.76 147.70 (0.64) 0.73 Charniak 167.14 130.20 126.03 (0.64) 0.69 Table 2: Comparing perplexity results for each LM on the WSJ PTB test set. 3gram represents the word-based trigram LM, Intp (weight)the LM interpolated with a trigram (and the interpolation weight), and r the correlation value. N/A means not available. the word-level tokenization of treebank texts differs from that used in the speech recognition task with the major differences being: numbers (e.g., “1.2%” versus “one point two percent”), dates (e.g., “Dec. 20, 2001” versus “Decemb</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. Immediate-head parsing for language models. In Proceedings of ACL’2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
<author>S Khudanpur</author>
</authors>
<title>Structure and performance of a dependency language model.</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<volume>5</volume>
<pages>2775--2778</pages>
<contexts>
<context position="2774" citStr="Chelba et al., 1997" startWordPosition="418" endWordPosition="421">ax W,T P(W, T)P(A|W, T) �Pr(wN1 ) ≈ t1,t2,...,tN N 11 P(W,T|A) = arg max W,T 1 Introduction ≈ arg max P(W,T)P(A|W) W,T The purpose of a language model (LM) is to determine the a priori probability of a word sequence w1, ..., wn, P(w1, ... , wn). Language modeling is essential in a wide variety of applications; we focus on speech recognition in our research. Although wordbased LMs (with bigram and trigram being the most common) remain the mainstay in many continuous speech recognition systems, recent efforts have explored a variety of ways to improve LM performance (Niesler and Woodland, 1996; Chelba et al., 1997; Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosenfeld, 2000; Goodman, 2001; Roark, 2001; Charniak, 2001). Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using where T is the POS sequence tN 1 associated with the word sequence W = wN1 given the speech utterance A. The LM P(W, T) is a joint probabilistic model that accounts for both the sequence of words wN 1 and their tag assignments tN 1 by estimating the joint probabilities of words and tags: Johnson (2001) and Lafferty et al. (2001) provide i</context>
</contexts>
<marker>Chelba, Jelinek, Khudanpur, 1997</marker>
<rawString>C. Chelba, F. Jelinek, and S. Khudanpur. 1997. Structure and performance of a dependency language model. In Proceedings of Eurospeech, volume 5, pages 2775–2778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
</authors>
<title>Exploiting Syntactic Structure for Natural Language Modeling.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="2818" citStr="Chelba, 2000" startWordPosition="426" endWordPosition="427">11 P(W,T|A) = arg max W,T 1 Introduction ≈ arg max P(W,T)P(A|W) W,T The purpose of a language model (LM) is to determine the a priori probability of a word sequence w1, ..., wn, P(w1, ... , wn). Language modeling is essential in a wide variety of applications; we focus on speech recognition in our research. Although wordbased LMs (with bigram and trigram being the most common) remain the mainstay in many continuous speech recognition systems, recent efforts have explored a variety of ways to improve LM performance (Niesler and Woodland, 1996; Chelba et al., 1997; Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosenfeld, 2000; Goodman, 2001; Roark, 2001; Charniak, 2001). Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using where T is the POS sequence tN 1 associated with the word sequence W = wN1 given the speech utterance A. The LM P(W, T) is a joint probabilistic model that accounts for both the sequence of words wN 1 and their tag assignments tN 1 by estimating the joint probabilities of words and tags: Johnson (2001) and Lafferty et al. (2001) provide insight into why a joint model is superior to</context>
<context position="12993" citStr="Chelba, 2000" startWordPosition="2028" endWordPosition="2029">cognition task. SuperARVs can be accumulated from a corpus annotated with CDG relations and stored directly with words in a lexicon, so we can learn their frequency of occurrence for the corresponding word. A SuperARV can then be selected from the lexicon and used to generate role values that meet their constraints. Since there are no large benchmark corpora annotated with CDG information1, we have developed a methodology to automatically transform constituent bracketing found in available treebanks into CDG annotations. In addition to generating dependency structures by headword percolation (Chelba, 2000), our transformer also utilizes a rule-based method to determine lexical features and need role values for words, as described by Wang et al. (2001). Our SuperARV LM estimates the joint probability of words wN1 and their SuperARV tags tN1 : Pr(ti|wi−1 1 ti−1 1 ) · Pr(wi|wi−1 1 ti 1) Pr(ti|wi−1 i−2ti−1 i−2) · Pr(wi|wi−1 i−2tii−2) (3) Notice we use a joint probabilistic model to enable the joint prediction of words and their SuperARVs so that word form information is tightly integrated at the model level. Our SuperARV LM does not encode the word identity directly at the data structure level as w</context>
<context position="18158" citStr="Chelba (2000)" startWordPosition="2862" endWordPosition="2863">g. These two experiments compare our SuperARV LM to a baseline trigram, a POS LM that was implemented using Equation (3) (where for this model t represents POS tags instead of SuperARV tags) and modified Kneser-Ney smoothing (as used in the SuperARV LM), and one or more parser-based LMs. Additionally, we evaluate the performance of a conditional probability SuperARV LM (denoted cSuperARV) implemented following Equation (1) rather than Equation (3) to evaluate the importance of using joint probability estimations. For the WSJ PTB task, we compare the SuperARV LMs to the parser LMs developed by Chelba (2000), Roark (2001), and Charniak (2001). Although Srinivas (1997) developed an almost-parsing supertag-based LM, we cannot compare his LM with the other LMs because he used a small non-standard subset of the WSJ PTB2 and a trainable supertag LM is unavailable. Because none of the parser LMs has been fully trained for the WSJ CSR task, it is essential that we retrain them for comparison. The availability of a trainable version of Chelba’s model enables us to train and test on the CSR task; however, because we do not have access to a trainable version of Charniak’s or Roark’s LMs, they are not consi</context>
<context position="21730" citStr="Chelba, 2000" startWordPosition="3466" endWordPosition="3467">he correlation of the trigram with each LM. A standard correlation is calculated between the probabilities assigned to each test set sentence by the trigram LM and the LM in question. This technique has been used in (Wang et al., 2002) to identify whether two LMs can be effectively interpolated. Since we have access to an executable version of Charniak’s LM trained on the WSJ PTB (ftp.cs.brown.edu/pub/nlparser) and a trainable version of Chelba’s LM, we are able to calculate their correlations with our trigram LM. Chelba’s LM was retrained using more parameter reestimation iterations than in (Chelba, 2000) to optimize the performance. Table 2 shows the correlation between each of the executable LMs and the trigram LM. The POS LM has the highest correlation with the trigram, closely followed by the SuperARV LM. Because these two LMs tightly integrate the word information jointly with the tag distribution, the trigram information is already represented. In contrast, the cSuperARV LM and Chelba’s and Charniak’s parserbased LMs have much lower correlations, indicating they have much lower overlap with the trigram. Because the cSuperARV LM only uses weak word distribution information in probability </context>
<context position="23695" citStr="Chelba (2000)" startWordPosition="3789" endWordPosition="3790">tuent bracketing can be found in the WSJ PTB, it becomes the parse tree for the training sentence; otherwise, we use the corresponding tree in the BLLIP treebank (Charniak et al., 2000). Since WSJ CSR is a speech corpus, there is no punctuation or case information. All words outside the provided vocabulary are mapped to (UNK). Note that 3In the remaining experiments, the POS LM and the SuperARV LM are not interpolated with a trigram. Perplexity LM 3gram Model Intp (Weight) r POS 167.14 142.55 142.55 (1.0) 0.95 SuperARV 167.14 118.35 118.35 (1.0) 0.92 cSuperARV 167.14 150.01 143.83 (0.65) 0.68 Chelba (2000) 167.14 158.28 148.90 (0.64) N/A Roark (2001) 167.02 152.26 137.26 (0.64) N/A Charniak (2001) 167.89 130.20 126.07 (0.64) N/A Chelba 167.14 153.76 147.70 (0.64) 0.73 Charniak 167.14 130.20 126.03 (0.64) 0.69 Table 2: Comparing perplexity results for each LM on the WSJ PTB test set. 3gram represents the word-based trigram LM, Intp (weight)the LM interpolated with a trigram (and the interpolation weight), and r the correlation value. N/A means not available. the word-level tokenization of treebank texts differs from that used in the speech recognition task with the major differences being: numbe</context>
<context position="28403" citStr="Chelba (2000)" startWordPosition="4566" endWordPosition="4567">7%, 2.7%, and 3.75%, compared to the trigram. Note that Chelba’s LM tied once with the SuperARV LM on 93-20k SAC, but always obtained higher WER across the four test sets. Because Chelba’s LM focuses on developing the complete parse structure for a word sequence, it enforces more strict pruning based on the entire sentence. As can be seen in Table 4, the cSuperARV LM, even when interpolated with a trigram LM, obtains a lower accuracy than our SuperARV LM. This result is consistent with the hypothesis that a conditional model suffers from label bias (Lafferty et al., 2001). The WER reported by Chelba (2000) on the 93- 20k test set was 13.0%. This WER is lower than what we obtained for Chelba’s retrained LM on the same task. This disparity is due to the fact that a higher quality acoustic decoder was used in (Chelba, 2000), which is not available to us. We further compare the LMs on Dr. Chelba’s 93-20K lattices kindly provided by him, with the rescoring results shown in the last column of Table 4. We observe that Chelba’s retrained LM improves his original result, but the SuperARV LM still obtains a greater accuracy. Sign tests show that the differences between the accuracies achieved by the Supe</context>
</contexts>
<marker>Chelba, 2000</marker>
<rawString>C. Chelba. 2000. Exploiting Syntactic Structure for Natural Language Modeling. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J T Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Harvard University, Computer Science Group.</institution>
<contexts>
<context position="15911" citStr="Chen and Goodman, 1998" startWordPosition="2499" endWordPosition="2502">der of n-grams for smoothing the distributions in Equation (3). n-grams Pˆ(ti|wi−1 Pˆ(wi|wi−1 i−2ti−1 i−2tii−2) i−2) highest Pˆ (ti|wi−1 Pˆ (wi|wi−1 lowest i−2ti−1 i−2ti i−2) i−2) Pˆ (wi|wi−1 Pˆ(ti|wi−1ti−1 i−2ti i−1) i−2) Pˆ(wi|wi−1tii−2) Pˆ (ti|wi−1 Pˆ(wi|wi−1tii−1) i−2ti−1) Pˆ(wi|wi−1ti) Pˆ(ti|ti−1 Pˆ(wi|tii−1) i−2) Pˆ(wi|ti) Pˆ(ti|wi−1ti−1) Pˆ(ti|ti−1) Pˆ(ti) In preliminary experiments, we compared several algorithms for smoothing the probability estimations for our SuperARV LM. The best performance was achieved by using the modified Kneser-Ney smoothing algorithm initially introduced in (Chen and Goodman, 1998) and adapting it by employing a heldout data set to optimize parameters, including cutoffs for rare n-grams, by using Powell’s search (Press et al., 1988). Parameters are chosen to optimize the perplexity on a heldout set. In order to compare our SuperARV LM with a word-based LM, we must use the following equation to calculate the word perplexity (PPL): Equation (4) is used by class-based LMs to calculate word perplexity (Heeman, 1998). Parser-based LMs use a similar procedure that sums over parses. The SuperARV LM is most closely related to the almost-parsing-based LM developed by Srinivas (1</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. F. Chen and J. T. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, Harvard University, Computer Science Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL’1996,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="29936" citStr="Collins, 1996" startWordPosition="4812" endWordPosition="4813">etween the encouraging results from our SuperARV LM and the reported poor performance of several probabilistic dependency grammar models, i.e., the traditional probabilistic dependency grammar (PDG) LM, the probabilistic link grammar (PLG) (Lafferty et al., 1992) LM, and Zeman’s probabilistic dependency grammar model (ZPDG) (Hajic et al., 1998). ZPDG was evaluated on the Prague Dependency Treebank (Hajic, 1998) during the 1998 Johns Hopkins summer workshop (Hajic et al., 1998) and produced a much lower parsing accuracy (under 60%) than Collins’ probabilistic context-free grammar parser (80%) (Collins, 1996). Fong et al. (1995) evaluated the probabilistic link grammar LM described in (Lafferty et al., 1992) on small artificial corpora and found that the LM has a greater perplexity than a standard bigram. Additionally, only a modest improvement on the bigram was achieved after Fong and Wu (1995) revised the model to make grammar rule learning feasible. One possible reason for their poor performance, especially in the light of our SuperARV LM results, is that these probabilistic dependency grammar models do not utilize sufficient knowledge to achieve a high level of accuracy. The knowledge sources </context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>M. J. Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of ACL’1996, pages 184–191.</rawString>
</citation>
<citation valid="true">
<date>1997</date>
<booktitle>HTK: Hidden Markov Model Toolkit V2.1.</booktitle>
<institution>Entropic Cambridge Research Laboratory, Ltd.,</institution>
<contexts>
<context position="3892" citStr="(1997)" startWordPosition="613" endWordPosition="613">int probabilities of words and tags: Johnson (2001) and Lafferty et al. (2001) provide insight into why a joint model is superior to a conditional model. Recently, there has been good progress in developing structured models (Chelba, 2000; Charniak, N N N P(wi, ti|wi−1 P(w1 , t1 ) = ri 1 ,ti−1 1 ) (2) 2001; Roark, 2001) that incorporate syntactic information. These LMs capture the hierarchical characteristics of a language rather than specific information about words and their lexical features (e.g., case, number). In an attempt to incorporate even more knowledge into a structured LM, Goodman (1997) has developed a probabilistic feature grammar (PFG) that conditions not only on structure but also on a small set of grammatical features (e.g., number) and has achieved parse accuracy improvement. Goodman’s work suggests that integrating lexical features with word identity and syntax would benefit LM predictiveness. PFG uses only a small set of lexical features because it integrates those features at the level of the production rules, causing a significant increase in grammar size and a concomitant data sparsity problem that preclude the addition of richer features. This sparseness problem c</context>
<context position="16515" citStr="(1997)" startWordPosition="2601" endWordPosition="2601">8) and adapting it by employing a heldout data set to optimize parameters, including cutoffs for rare n-grams, by using Powell’s search (Press et al., 1988). Parameters are chosen to optimize the perplexity on a heldout set. In order to compare our SuperARV LM with a word-based LM, we must use the following equation to calculate the word perplexity (PPL): Equation (4) is used by class-based LMs to calculate word perplexity (Heeman, 1998). Parser-based LMs use a similar procedure that sums over parses. The SuperARV LM is most closely related to the almost-parsing-based LM developed by Srinivas (1997). Srinivas’ LM, based on the notion of a supertag, the elementary structure of Lexicalized TreeAdjoining Grammar, achieved a perplexity reduction compared to a conditional POS n-gram LM (Niesler and Woodland, 1996). By comparison, our LM incorporates dependencies directly on words instead of through nonterminals, uses more lexical features than the supertag LM, uses joint instead of conditional probability estimations, and uses modified Kneser-Ney rather than Katz smoothing. 3 Evaluating the SuperARV Language Model Traditionally, the LM quality in speech recognition is evaluated on two metrics</context>
<context position="18219" citStr="(1997)" startWordPosition="2872" endWordPosition="2872">am, a POS LM that was implemented using Equation (3) (where for this model t represents POS tags instead of SuperARV tags) and modified Kneser-Ney smoothing (as used in the SuperARV LM), and one or more parser-based LMs. Additionally, we evaluate the performance of a conditional probability SuperARV LM (denoted cSuperARV) implemented following Equation (1) rather than Equation (3) to evaluate the importance of using joint probability estimations. For the WSJ PTB task, we compare the SuperARV LMs to the parser LMs developed by Chelba (2000), Roark (2001), and Charniak (2001). Although Srinivas (1997) developed an almost-parsing supertag-based LM, we cannot compare his LM with the other LMs because he used a small non-standard subset of the WSJ PTB2 and a trainable supertag LM is unavailable. Because none of the parser LMs has been fully trained for the WSJ CSR task, it is essential that we retrain them for comparison. The availability of a trainable version of Chelba’s model enables us to train and test on the CSR task; however, because we do not have access to a trainable version of Charniak’s or Roark’s LMs, they are not considered in the CSR task. Note that for lattice rescoring, howev</context>
</contexts>
<marker>1997</marker>
<rawString>Entropic Cambridge Research Laboratory, Ltd., 1997. HTK: Hidden Markov Model Toolkit V2.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W Fong</author>
<author>D Wu</author>
</authors>
<title>Learning restricted probabilistic link grammars.</title>
<date>1995</date>
<tech>Technical Report HKUST-CS95-27,</tech>
<institution>University of Science and Technology, Clear Water Bay, Hong Kong.</institution>
<contexts>
<context position="30228" citStr="Fong and Wu (1995)" startWordPosition="4860" endWordPosition="4863">istic dependency grammar model (ZPDG) (Hajic et al., 1998). ZPDG was evaluated on the Prague Dependency Treebank (Hajic, 1998) during the 1998 Johns Hopkins summer workshop (Hajic et al., 1998) and produced a much lower parsing accuracy (under 60%) than Collins’ probabilistic context-free grammar parser (80%) (Collins, 1996). Fong et al. (1995) evaluated the probabilistic link grammar LM described in (Lafferty et al., 1992) on small artificial corpora and found that the LM has a greater perplexity than a standard bigram. Additionally, only a modest improvement on the bigram was achieved after Fong and Wu (1995) revised the model to make grammar rule learning feasible. One possible reason for their poor performance, especially in the light of our SuperARV LM results, is that these probabilistic dependency grammar models do not utilize sufficient knowledge to achieve a high level of accuracy. The knowledge sources the SuperARV LM uses, represented as components of the structure shown in Figure 1, include: lexical category (denoted c), lexical features (denoted f), role label or link type information (denoted L), a governor role dependency relation constraint (R, L, UC) (denoted g), a set of need role </context>
</contexts>
<marker>Fong, Wu, 1995</marker>
<rawString>E. W. Fong and D. Wu. 1995. Learning restricted probabilistic link grammars. Technical Report HKUST-CS95-27, University of Science and Technology, Clear Water Bay, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Galescu</author>
<author>E K Ringger</author>
</authors>
<title>Augmenting words with linguistic information for n-gram language models.</title>
<date>1999</date>
<booktitle>In Proceedings of Eurospeech.</booktitle>
<contexts>
<context position="13631" citStr="Galescu and Ringger, 1999" startWordPosition="2135" endWordPosition="2138">ormer also utilizes a rule-based method to determine lexical features and need role values for words, as described by Wang et al. (2001). Our SuperARV LM estimates the joint probability of words wN1 and their SuperARV tags tN1 : Pr(ti|wi−1 1 ti−1 1 ) · Pr(wi|wi−1 1 ti 1) Pr(ti|wi−1 i−2ti−1 i−2) · Pr(wi|wi−1 i−2tii−2) (3) Notice we use a joint probabilistic model to enable the joint prediction of words and their SuperARVs so that word form information is tightly integrated at the model level. Our SuperARV LM does not encode the word identity directly at the data structure level as was done in (Galescu and Ringger, 1999) since this could cause serious data sparsity problems. To estimate the probability distributions in Equation (3) from training data, we use recursive linear interpolation among probability estimations of different orders. Representing each multiplicand in Equation (3) as the conditional probability ˆP(x|y1, y2, ..., yam,) where y1, y2, ..., yam, belong to a mixed set of words and SuperARVs, the recursive linear interpolation is calculated as follows: 1We have annotated a moderate-sized corpus, DARPA Naval Resource Management (Price et al., 1988), with CDG parse relations as reported in (Harpe</context>
</contexts>
<marker>Galescu, Ringger, 1999</marker>
<rawString>L. Galescu and E. K. Ringger. 1999. Augmenting words with linguistic information for n-gram language models. In Proceedings of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Probabilistic feature grammars.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourth international workshop on parsing technologies.</booktitle>
<contexts>
<context position="3892" citStr="Goodman (1997)" startWordPosition="612" endWordPosition="613">g the joint probabilities of words and tags: Johnson (2001) and Lafferty et al. (2001) provide insight into why a joint model is superior to a conditional model. Recently, there has been good progress in developing structured models (Chelba, 2000; Charniak, N N N P(wi, ti|wi−1 P(w1 , t1 ) = ri 1 ,ti−1 1 ) (2) 2001; Roark, 2001) that incorporate syntactic information. These LMs capture the hierarchical characteristics of a language rather than specific information about words and their lexical features (e.g., case, number). In an attempt to incorporate even more knowledge into a structured LM, Goodman (1997) has developed a probabilistic feature grammar (PFG) that conditions not only on structure but also on a small set of grammatical features (e.g., number) and has achieved parse accuracy improvement. Goodman’s work suggests that integrating lexical features with word identity and syntax would benefit LM predictiveness. PFG uses only a small set of lexical features because it integrates those features at the level of the production rules, causing a significant increase in grammar size and a concomitant data sparsity problem that preclude the addition of richer features. This sparseness problem c</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>J. Goodman. 1997. Probabilistic feature grammars. In Proceedings of the Fourth international workshop on parsing technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>A bit of progress in language modeling, extended version.</title>
<date>2001</date>
<tech>Technical Report MSR-TR-2001-72,</tech>
<institution>Microsoft Research,</institution>
<location>Redmond, WA.</location>
<contexts>
<context position="2850" citStr="Goodman, 2001" startWordPosition="431" endWordPosition="432">roduction ≈ arg max P(W,T)P(A|W) W,T The purpose of a language model (LM) is to determine the a priori probability of a word sequence w1, ..., wn, P(w1, ... , wn). Language modeling is essential in a wide variety of applications; we focus on speech recognition in our research. Although wordbased LMs (with bigram and trigram being the most common) remain the mainstay in many continuous speech recognition systems, recent efforts have explored a variety of ways to improve LM performance (Niesler and Woodland, 1996; Chelba et al., 1997; Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosenfeld, 2000; Goodman, 2001; Roark, 2001; Charniak, 2001). Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using where T is the POS sequence tN 1 associated with the word sequence W = wN1 given the speech utterance A. The LM P(W, T) is a joint probabilistic model that accounts for both the sequence of words wN 1 and their tag assignments tN 1 by estimating the joint probabilities of words and tags: Johnson (2001) and Lafferty et al. (2001) provide insight into why a joint model is superior to a conditional model. Recently, </context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>J. Goodman. 2001. A bit of progress in language modeling, extended version. Technical Report MSR-TR-2001-72, Microsoft Research, Redmond, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajic</author>
<author>E Brill</author>
<author>M Collins</author>
<author>B Hladka</author>
<author>D Jones</author>
<author>C Kuo</author>
<author>L Ramshaw</author>
<author>O Schwartz</author>
<author>C Tillmann</author>
<author>D Zeman</author>
</authors>
<title>Core natural language processing technology applicable to multiple languages – Workshop ’98.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Johns Hopkins Univ.</institution>
<contexts>
<context position="29668" citStr="Hajic et al., 1998" startWordPosition="4770" endWordPosition="4773">s are statistically significant. Although there is no significant difference between the SuperARV LM and Chelba’s LM, the SuperARV LM has a much lower complexity than Chelba’s LM. 4 Investigating the Knowledge Source Contributions Next, we attempt to explain the contrast between the encouraging results from our SuperARV LM and the reported poor performance of several probabilistic dependency grammar models, i.e., the traditional probabilistic dependency grammar (PDG) LM, the probabilistic link grammar (PLG) (Lafferty et al., 1992) LM, and Zeman’s probabilistic dependency grammar model (ZPDG) (Hajic et al., 1998). ZPDG was evaluated on the Prague Dependency Treebank (Hajic, 1998) during the 1998 Johns Hopkins summer workshop (Hajic et al., 1998) and produced a much lower parsing accuracy (under 60%) than Collins’ probabilistic context-free grammar parser (80%) (Collins, 1996). Fong et al. (1995) evaluated the probabilistic link grammar LM described in (Lafferty et al., 1992) on small artificial corpora and found that the LM has a greater perplexity than a standard bigram. Additionally, only a modest improvement on the bigram was achieved after Fong and Wu (1995) revised the model to make grammar rule </context>
<context position="34585" citStr="Hajic et al., 1998" startWordPosition="5545" endWordPosition="5548">dependency grammar models compared to our SuperARV LM. Note link type is defined as L and link direction is defined as UC in the SuperARV structure. lexical features improve or degrade LM quality. The model based on SARV-fmn is very similar to the standard probabilistic dependency grammar LM, in which only word, POS, link type, and link direction information is used for probability estimations. The model based on SARV-gmn uses a feature augmentation of POS, and the model based on SARV-cgmn uses lexical features only. Additionally, we built the model ZPDG-SARV to approximate ZPDG. Zeman’s PDG (Hajic et al., 1998) differs significantly from our original SuperARV LM in that it ignores label information L and some lexical feature information (the morphological tags do not include some lexical features having influence on syntax, denoted syntactic lexical features, i.e., gapp, inverted, mood, type, case, voice), and does not enforce valency constraints (instead, the model only counts the number of links associated with a word without discriminating whether the links represent governing or linguistic structural requirements). Also, word identity information is not used, instead, the model uses a loose inte</context>
</contexts>
<marker>Hajic, Brill, Collins, Hladka, Jones, Kuo, Ramshaw, Schwartz, Tillmann, Zeman, 1998</marker>
<rawString>J. Hajic, E. Brill, M. Collins, B. Hladka, D. Jones, C. Kuo, L. Ramshaw, O. Schwartz, C. Tillmann, and D. Zeman. 1998. Core natural language processing technology applicable to multiple languages – Workshop ’98. Technical report, Johns Hopkins Univ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajic</author>
</authors>
<title>Building a syntactically annotated corpus: The Prague Dependency Treekbank.</title>
<date>1998</date>
<booktitle>In Issues of Valency and Meaning ( Festschrift for Jarmila Panevova),</booktitle>
<pages>106--132</pages>
<location>Carolina, Charles University, Prague.</location>
<contexts>
<context position="29736" citStr="Hajic, 1998" startWordPosition="4782" endWordPosition="4783">e between the SuperARV LM and Chelba’s LM, the SuperARV LM has a much lower complexity than Chelba’s LM. 4 Investigating the Knowledge Source Contributions Next, we attempt to explain the contrast between the encouraging results from our SuperARV LM and the reported poor performance of several probabilistic dependency grammar models, i.e., the traditional probabilistic dependency grammar (PDG) LM, the probabilistic link grammar (PLG) (Lafferty et al., 1992) LM, and Zeman’s probabilistic dependency grammar model (ZPDG) (Hajic et al., 1998). ZPDG was evaluated on the Prague Dependency Treebank (Hajic, 1998) during the 1998 Johns Hopkins summer workshop (Hajic et al., 1998) and produced a much lower parsing accuracy (under 60%) than Collins’ probabilistic context-free grammar parser (80%) (Collins, 1996). Fong et al. (1995) evaluated the probabilistic link grammar LM described in (Lafferty et al., 1992) on small artificial corpora and found that the LM has a greater perplexity than a standard bigram. Additionally, only a modest improvement on the bigram was achieved after Fong and Wu (1995) revised the model to make grammar rule learning feasible. One possible reason for their poor performance, e</context>
</contexts>
<marker>Hajic, 1998</marker>
<rawString>J. Hajic. 1998. Building a syntactically annotated corpus: The Prague Dependency Treekbank. In Issues of Valency and Meaning ( Festschrift for Jarmila Panevova), pages 106–132. Carolina, Charles University, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Harper</author>
<author>R A Helzerman</author>
</authors>
<title>Extensions to constraint dependency parsing for spoken language processing.</title>
<date>1995</date>
<journal>Computer Speech and Language,</journal>
<pages>9--187</pages>
<contexts>
<context position="5231" citStr="Harper and Helzerman, 1995" startWordPosition="819" endWordPosition="822">diction capability can be achieved by tightly integrating structural constraints and lexical features at the word level. Hence, we develop a new dependencygrammar almost-parsing LM, SuperARV LM, which uses enriched tags called SuperARVs. In Section 2, we introduce our SuperARV LM. Section 3 compares the performance of the SuperARV LM to other LMs. Section 4 investigates the knowledge source contributions by constraint relaxation. Conclusions appear in Section 5. 2 SuperARV Language Model The SuperARV LM is a highly lexicalized probabilistic LM based on the Constraint Dependency Grammar (CDG) (Harper and Helzerman, 1995). CDG represents a parse as assignments of dependency relations to functional variables (denoted roles) associated with each word in a sentence. Consider the parse for What did you learn depicted in the white box of Figure 1. Each word in the parse has a lexical category and a set of feature values. Also, each word has a governor role (denoted G) which is assigned a role value, comprised of a label as well as a modifiee, which indicates the position of the word’s governor or head. For example, the role value assigned to the governor role of did is vp-1, where its label vp indicates its grammat</context>
<context position="7481" citStr="Harper and Helzerman, 1995" startWordPosition="1181" endWordPosition="1184">d Need3, are used to ensure that the grammatical requirements of the word are met. PX and MX([R]) represent the position of a word and its modifiee (for role R), respectively. ifiee of the role value assigned to N3 is set equal to its own position). Including need roles also provides a mechanism for using non-headword dependencies to constrain parse structures, which Bod (2001) has shown contributes to improved parsing accuracy. During parsing, the grammaticality of a sentence in a language defined by a CDG is determined by applying a set of constraints to the possible role value assignments (Harper and Helzerman, 1995; Maruyama, 1990). Originally, the constraints were comprised of a set of hand-written rules specifying which role values (unary constraints) and pairs of role values (binary constraints) were grammatical (Maruyama, 1990). In order to derive the constraints directly from CDG annotated sentences, we have developed an algorithm to extract grammar relations using information derived directly from annotated sentences (Harper et al., 2000; Harper and Wang, 2001). Using the relationship between a role value’s position and its modifiee’s position, unary and binary constraints can be represented as a </context>
</contexts>
<marker>Harper, Helzerman, 1995</marker>
<rawString>M. P. Harper and R. A. Helzerman. 1995. Extensions to constraint dependency parsing for spoken language processing. Computer Speech and Language, 9:187–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Harper</author>
<author>W Wang</author>
</authors>
<title>Approaches for learning constraint dependency grammar from corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the Grammar and Natural Language Processing Conference,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="7942" citStr="Harper and Wang, 2001" startWordPosition="1247" endWordPosition="1250">ty of a sentence in a language defined by a CDG is determined by applying a set of constraints to the possible role value assignments (Harper and Helzerman, 1995; Maruyama, 1990). Originally, the constraints were comprised of a set of hand-written rules specifying which role values (unary constraints) and pairs of role values (binary constraints) were grammatical (Maruyama, 1990). In order to derive the constraints directly from CDG annotated sentences, we have developed an algorithm to extract grammar relations using information derived directly from annotated sentences (Harper et al., 2000; Harper and Wang, 2001). Using the relationship between a role value’s position and its modifiee’s position, unary and binary constraints can be represented as a finite set of abstract role values (ARVs) and abstract role value pairs (ARVPs), respectively. The light gray box of Figure 1 shows an example of an ARV and an ARVP. ARV for vp-1 assigned to G for did: cat1=verb, subcat=base, verbtype=past, voice=active, inverted=yes, type=none, gapp=yes, mood=whquestion, semtype=auxiliary, agr=all, role1=G, label1=vp, (PX1&gt;MX1) ARVP for vp-1 assigned to G for did and subj-2 assigned to G for you: cat1=verb, subcat=base, ve</context>
<context position="14269" citStr="Harper and Wang, 2001" startWordPosition="2232" endWordPosition="2235">d cause serious data sparsity problems. To estimate the probability distributions in Equation (3) from training data, we use recursive linear interpolation among probability estimations of different orders. Representing each multiplicand in Equation (3) as the conditional probability ˆP(x|y1, y2, ..., yam,) where y1, y2, ..., yam, belong to a mixed set of words and SuperARVs, the recursive linear interpolation is calculated as follows: 1We have annotated a moderate-sized corpus, DARPA Naval Resource Management (Price et al., 1988), with CDG parse relations as reported in (Harper et al., 2000; Harper and Wang, 2001). Pr(wN1 tN1 ) = N Pr(witi|wi−1 11 1 ti−1 1 ) = N ri ≈ N ri ˆPn(x|y1, y2, ... ,yn) = λ(x, y1, y2, ... , yn) · Pn(x|y1, y2,. . . , yn) +(1 − λ(x, y1, y2, ..., yn)) · ˆPn−1(x|y1, y2, ... , yn−1) PPL = 2En (4) i−1 Pˆ(w) 1 En ≈ − N N log2 i=1 r_ti−2,ilog2 Pˆ(witi|wi−1 i−2ti−1 i−2)ˆP(wi−1 i−2ti−1 i−2) i−1 i−1 Pˆ(wi−2ti−2) where: ≈ − 1 N N i=1 r_ ti−2,i−1 • y1, y2, ... , yn is the context of order n-gram to predict x; • Pn(x|y1, y2, ..., yn) is the order n-gram maximum likelihood estimation. Table 1 enumerates the n-grams and their order for the interpolation smoothing of the two distributions in Eq</context>
</contexts>
<marker>Harper, Wang, 2001</marker>
<rawString>M. P. Harper and W. Wang. 2001. Approaches for learning constraint dependency grammar from corpora. In Proceedings of the Grammar and Natural Language Processing Conference, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Harper</author>
<author>S A Hockema</author>
<author>C M White</author>
</authors>
<title>Enhanced constraint dependency grammar parsers.</title>
<date>1999</date>
<booktitle>In Proceedings of the IASTED International Conference on Artificial Intelligence and Soft Computing.</booktitle>
<contexts>
<context position="11387" citStr="Harper et al. (1999)" startWordPosition="1771" endWordPosition="1774">iable, L is a functionality label, UC represents the relative position relation of a word and its dependent, MC is the lexical category of the modifiee for this dependency relation, and DC represents the relative ordering of the positions of a word and all of its modifiees. The following features are used in our SuperARV LM: agr, case, vtype (e.g., progressive), mood, gapp (e.g., gap or not), inverted, voice, behavior (e.g., mass, count), type (e.g., interrogative, relative). These lexical features constitute a much richer set than the features used by the parser-based LMs in Section 1. Since Harper et al. (1999) found that enforcing modifiee constraints (e.g., the lexical categories of modifiees) in parsing results in efficient pruning, we also include the modifiee lexical category (MC) in our SuperARV structure to impose modifiee constraints. Words typically have more than one SuperARV to indicate different types of word usage. The average number of SuperARVs for words of different lexical categories vary, with verbs having the greatest SuperARV ambiguity. This is mostly due to the variety of feature combinations and variations on complement types and positions. We have observed in several experimen</context>
</contexts>
<marker>Harper, Hockema, White, 1999</marker>
<rawString>M. P. Harper, S. A. Hockema, and C. M. White. 1999. Enhanced constraint dependency grammar parsers. In Proceedings of the IASTED International Conference on Artificial Intelligence and Soft Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Harper</author>
<author>C M White</author>
<author>W Wang</author>
<author>M T Johnson</author>
<author>R A Helzerman</author>
</authors>
<title>Effectiveness of corpus-induced dependency grammars for post-processing speech.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL’2000.</booktitle>
<contexts>
<context position="7918" citStr="Harper et al., 2000" startWordPosition="1243" endWordPosition="1246">ing, the grammaticality of a sentence in a language defined by a CDG is determined by applying a set of constraints to the possible role value assignments (Harper and Helzerman, 1995; Maruyama, 1990). Originally, the constraints were comprised of a set of hand-written rules specifying which role values (unary constraints) and pairs of role values (binary constraints) were grammatical (Maruyama, 1990). In order to derive the constraints directly from CDG annotated sentences, we have developed an algorithm to extract grammar relations using information derived directly from annotated sentences (Harper et al., 2000; Harper and Wang, 2001). Using the relationship between a role value’s position and its modifiee’s position, unary and binary constraints can be represented as a finite set of abstract role values (ARVs) and abstract role value pairs (ARVPs), respectively. The light gray box of Figure 1 shows an example of an ARV and an ARVP. ARV for vp-1 assigned to G for did: cat1=verb, subcat=base, verbtype=past, voice=active, inverted=yes, type=none, gapp=yes, mood=whquestion, semtype=auxiliary, agr=all, role1=G, label1=vp, (PX1&gt;MX1) ARVP for vp-1 assigned to G for did and subj-2 assigned to G for you: ca</context>
<context position="14245" citStr="Harper et al., 2000" startWordPosition="2228" endWordPosition="2231">1999) since this could cause serious data sparsity problems. To estimate the probability distributions in Equation (3) from training data, we use recursive linear interpolation among probability estimations of different orders. Representing each multiplicand in Equation (3) as the conditional probability ˆP(x|y1, y2, ..., yam,) where y1, y2, ..., yam, belong to a mixed set of words and SuperARVs, the recursive linear interpolation is calculated as follows: 1We have annotated a moderate-sized corpus, DARPA Naval Resource Management (Price et al., 1988), with CDG parse relations as reported in (Harper et al., 2000; Harper and Wang, 2001). Pr(wN1 tN1 ) = N Pr(witi|wi−1 11 1 ti−1 1 ) = N ri ≈ N ri ˆPn(x|y1, y2, ... ,yn) = λ(x, y1, y2, ... , yn) · Pn(x|y1, y2,. . . , yn) +(1 − λ(x, y1, y2, ..., yn)) · ˆPn−1(x|y1, y2, ... , yn−1) PPL = 2En (4) i−1 Pˆ(w) 1 En ≈ − N N log2 i=1 r_ti−2,ilog2 Pˆ(witi|wi−1 i−2ti−1 i−2)ˆP(wi−1 i−2ti−1 i−2) i−1 i−1 Pˆ(wi−2ti−2) where: ≈ − 1 N N i=1 r_ ti−2,i−1 • y1, y2, ... , yn is the context of order n-gram to predict x; • Pn(x|y1, y2, ..., yn) is the order n-gram maximum likelihood estimation. Table 1 enumerates the n-grams and their order for the interpolation smoothing of the</context>
</contexts>
<marker>Harper, White, Wang, Johnson, Helzerman, 2000</marker>
<rawString>M. P. Harper, C. M. White, W. Wang, M. T. Johnson, and R. A. Helzerman. 2000. Effectiveness of corpus-induced dependency grammars for post-processing speech. In Proceedings of NAACL’2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Heeman</author>
</authors>
<title>POS tagging versus classes in language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 6th Workshop on Very Large Corpora,</booktitle>
<location>Montreal.</location>
<contexts>
<context position="2804" citStr="Heeman, 1998" startWordPosition="424" endWordPosition="425">1,t2,...,tN N 11 P(W,T|A) = arg max W,T 1 Introduction ≈ arg max P(W,T)P(A|W) W,T The purpose of a language model (LM) is to determine the a priori probability of a word sequence w1, ..., wn, P(w1, ... , wn). Language modeling is essential in a wide variety of applications; we focus on speech recognition in our research. Although wordbased LMs (with bigram and trigram being the most common) remain the mainstay in many continuous speech recognition systems, recent efforts have explored a variety of ways to improve LM performance (Niesler and Woodland, 1996; Chelba et al., 1997; Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosenfeld, 2000; Goodman, 2001; Roark, 2001; Charniak, 2001). Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using where T is the POS sequence tN 1 associated with the word sequence W = wN1 given the speech utterance A. The LM P(W, T) is a joint probabilistic model that accounts for both the sequence of words wN 1 and their tag assignments tN 1 by estimating the joint probabilities of words and tags: Johnson (2001) and Lafferty et al. (2001) provide insight into why a joint model </context>
<context position="16350" citStr="Heeman, 1998" startWordPosition="2575" endWordPosition="2576">ility estimations for our SuperARV LM. The best performance was achieved by using the modified Kneser-Ney smoothing algorithm initially introduced in (Chen and Goodman, 1998) and adapting it by employing a heldout data set to optimize parameters, including cutoffs for rare n-grams, by using Powell’s search (Press et al., 1988). Parameters are chosen to optimize the perplexity on a heldout set. In order to compare our SuperARV LM with a word-based LM, we must use the following equation to calculate the word perplexity (PPL): Equation (4) is used by class-based LMs to calculate word perplexity (Heeman, 1998). Parser-based LMs use a similar procedure that sums over parses. The SuperARV LM is most closely related to the almost-parsing-based LM developed by Srinivas (1997). Srinivas’ LM, based on the notion of a supertag, the elementary structure of Lexicalized TreeAdjoining Grammar, achieved a perplexity reduction compared to a conditional POS n-gram LM (Niesler and Woodland, 1996). By comparison, our LM incorporates dependencies directly on words instead of through nonterminals, uses more lexical features than the supertag LM, uses joint instead of conditional probability estimations, and uses mod</context>
</contexts>
<marker>Heeman, 1998</marker>
<rawString>P. A. Heeman. 1998. POS tagging versus classes in language modeling. In Proceedings of the 6th Workshop on Very Large Corpora, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition.</title>
<date>1990</date>
<booktitle>In Alex Waibel and Kai-Fu Lee, editors, Readings in Speech Recognition.</booktitle>
<publisher>Morgan Kaufman Publishers, Inc.,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="1637" citStr="Jelinek (1990)" startWordPosition="227" endWordPosition="228">ces to the strength of our model are also investigated by using constraint relaxation at the level of the knowledge sources. We have found that although each knowledge source contributes to language model quality, lexical features are an outstanding contributor when they are tightly integrated with word identity and syntactic constraints. Our investigation also suggests possible reasons for the reported poor performance of several probabilistic dependency grammar models in the literature. these classes to compute n-gram probabilities. Partof-Speech (POS) tags were initially used as classes by Jelinek (1990) in a conditional probabilistic model (which predicts the tag sequence for a word sequence first and then uses it to predict the word sequence): Pr(ti|ti−1 1 )Pr(wi|ti) (1) However, Jelinek’s POS LM is less effective at predicting word candidates than an n-gram word-based LM because it deletes important lexical information for predicting the next word. Heeman’s (1998) POS LM achieves a perplexity reduction compared to a trigram LM by instead redefining the speech recognition problem as determining: W∗, T∗ = arg max W,T P(W, T)P(A|W, T) �Pr(wN1 ) ≈ t1,t2,...,tN N 11 P(W,T|A) = arg max W,T 1 Int</context>
</contexts>
<marker>Jelinek, 1990</marker>
<rawString>F. Jelinek. 1990. Self-organized language modeling for speech recognition. In Alex Waibel and Kai-Fu Lee, editors, Readings in Speech Recognition. Morgan Kaufman Publishers, Inc., San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Joint and conditional estimation of tagging and parsing models.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL’2001.</booktitle>
<contexts>
<context position="3337" citStr="Johnson (2001)" startWordPosition="517" endWordPosition="518">ce (Niesler and Woodland, 1996; Chelba et al., 1997; Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosenfeld, 2000; Goodman, 2001; Roark, 2001; Charniak, 2001). Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using where T is the POS sequence tN 1 associated with the word sequence W = wN1 given the speech utterance A. The LM P(W, T) is a joint probabilistic model that accounts for both the sequence of words wN 1 and their tag assignments tN 1 by estimating the joint probabilities of words and tags: Johnson (2001) and Lafferty et al. (2001) provide insight into why a joint model is superior to a conditional model. Recently, there has been good progress in developing structured models (Chelba, 2000; Charniak, N N N P(wi, ti|wi−1 P(w1 , t1 ) = ri 1 ,ti−1 1 ) (2) 2001; Roark, 2001) that incorporate syntactic information. These LMs capture the hierarchical characteristics of a language rather than specific information about words and their lexical features (e.g., case, number). In an attempt to incorporate even more knowledge into a structured LM, Goodman (1997) has developed a probabilistic feature gramma</context>
</contexts>
<marker>Johnson, 2001</marker>
<rawString>M. Johnson. 2001. Joint and conditional estimation of tagging and parsing models. In Proceedings of ACL’2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Lafferty</author>
<author>D Sleator</author>
<author>D Temperley</author>
</authors>
<title>Grammatical trigrams: A probabilistic model of link grammar.</title>
<date>1992</date>
<booktitle>In Proc. of AAAI Fall Symp. Probabilistic Approaches to Natural Language,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="29585" citStr="Lafferty et al., 1992" startWordPosition="4758" endWordPosition="4761">ween the accuracies achieved by the SuperARV LM and the trigram, POS, and cSuperARV LMs are statistically significant. Although there is no significant difference between the SuperARV LM and Chelba’s LM, the SuperARV LM has a much lower complexity than Chelba’s LM. 4 Investigating the Knowledge Source Contributions Next, we attempt to explain the contrast between the encouraging results from our SuperARV LM and the reported poor performance of several probabilistic dependency grammar models, i.e., the traditional probabilistic dependency grammar (PDG) LM, the probabilistic link grammar (PLG) (Lafferty et al., 1992) LM, and Zeman’s probabilistic dependency grammar model (ZPDG) (Hajic et al., 1998). ZPDG was evaluated on the Prague Dependency Treebank (Hajic, 1998) during the 1998 Johns Hopkins summer workshop (Hajic et al., 1998) and produced a much lower parsing accuracy (under 60%) than Collins’ probabilistic context-free grammar parser (80%) (Collins, 1996). Fong et al. (1995) evaluated the probabilistic link grammar LM described in (Lafferty et al., 1992) on small artificial corpora and found that the LM has a greater perplexity than a standard bigram. Additionally, only a modest improvement on the b</context>
</contexts>
<marker>Lafferty, Sleator, Temperley, 1992</marker>
<rawString>J. D. Lafferty, D. Sleator, and D. Temperley. 1992. Grammatical trigrams: A probabilistic model of link grammar. In Proc. of AAAI Fall Symp. Probabilistic Approaches to Natural Language, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML’2001.</booktitle>
<contexts>
<context position="3364" citStr="Lafferty et al. (2001)" startWordPosition="520" endWordPosition="523">dland, 1996; Chelba et al., 1997; Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosenfeld, 2000; Goodman, 2001; Roark, 2001; Charniak, 2001). Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using where T is the POS sequence tN 1 associated with the word sequence W = wN1 given the speech utterance A. The LM P(W, T) is a joint probabilistic model that accounts for both the sequence of words wN 1 and their tag assignments tN 1 by estimating the joint probabilities of words and tags: Johnson (2001) and Lafferty et al. (2001) provide insight into why a joint model is superior to a conditional model. Recently, there has been good progress in developing structured models (Chelba, 2000; Charniak, N N N P(wi, ti|wi−1 P(w1 , t1 ) = ri 1 ,ti−1 1 ) (2) 2001; Roark, 2001) that incorporate syntactic information. These LMs capture the hierarchical characteristics of a language rather than specific information about words and their lexical features (e.g., case, number). In an attempt to incorporate even more knowledge into a structured LM, Goodman (1997) has developed a probabilistic feature grammar (PFG) that conditions not</context>
<context position="28368" citStr="Lafferty et al., 2001" startWordPosition="4558" endWordPosition="4561">es an absolute increase on SAC of 4.24%, 6.97%, 2.7%, and 3.75%, compared to the trigram. Note that Chelba’s LM tied once with the SuperARV LM on 93-20k SAC, but always obtained higher WER across the four test sets. Because Chelba’s LM focuses on developing the complete parse structure for a word sequence, it enforces more strict pruning based on the entire sentence. As can be seen in Table 4, the cSuperARV LM, even when interpolated with a trigram LM, obtains a lower accuracy than our SuperARV LM. This result is consistent with the hypothesis that a conditional model suffers from label bias (Lafferty et al., 2001). The WER reported by Chelba (2000) on the 93- 20k test set was 13.0%. This WER is lower than what we obtained for Chelba’s retrained LM on the same task. This disparity is due to the fact that a higher quality acoustic decoder was used in (Chelba, 2000), which is not available to us. We further compare the LMs on Dr. Chelba’s 93-20K lattices kindly provided by him, with the rescoring results shown in the last column of Table 4. We observe that Chelba’s retrained LM improves his original result, but the SuperARV LM still obtains a greater accuracy. Sign tests show that the differences between </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML’2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="12302" citStr="Marcus et al., 1993" startWordPosition="1917" endWordPosition="1920">fferent types of word usage. The average number of SuperARVs for words of different lexical categories vary, with verbs having the greatest SuperARV ambiguity. This is mostly due to the variety of feature combinations and variations on complement types and positions. We have observed in several experiments that the number of SuperARVs does not grow significantly as training set size increases; the moderate-sized Resource Management corpus (Price et al., 1988) with 25,168 words produces 328 SuperARVs, compared to 538 SuperARVs for the 1 million word Wall Street Journal (WSJ) Penn Treebank set (Marcus et al., 1993), and 791 for the 37 million word training set of the WSJ continuous speech recognition task. SuperARVs can be accumulated from a corpus annotated with CDG relations and stored directly with words in a lexicon, so we can learn their frequency of occurrence for the corresponding word. A SuperARV can then be selected from the lexicon and used to generate role values that meet their constraints. Since there are no large benchmark corpora annotated with CDG information1, we have developed a methodology to automatically transform constituent bracketing found in available treebanks into CDG annotati</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Maruyama</author>
</authors>
<title>Structural disambiguation with constraint propagation.</title>
<date>1990</date>
<booktitle>In The Proceedings of ACL’1990,</booktitle>
<pages>31--38</pages>
<contexts>
<context position="7498" citStr="Maruyama, 1990" startWordPosition="1185" endWordPosition="1186">that the grammatical requirements of the word are met. PX and MX([R]) represent the position of a word and its modifiee (for role R), respectively. ifiee of the role value assigned to N3 is set equal to its own position). Including need roles also provides a mechanism for using non-headword dependencies to constrain parse structures, which Bod (2001) has shown contributes to improved parsing accuracy. During parsing, the grammaticality of a sentence in a language defined by a CDG is determined by applying a set of constraints to the possible role value assignments (Harper and Helzerman, 1995; Maruyama, 1990). Originally, the constraints were comprised of a set of hand-written rules specifying which role values (unary constraints) and pairs of role values (binary constraints) were grammatical (Maruyama, 1990). In order to derive the constraints directly from CDG annotated sentences, we have developed an algorithm to extract grammar relations using information derived directly from annotated sentences (Harper et al., 2000; Harper and Wang, 2001). Using the relationship between a role value’s position and its modifiee’s position, unary and binary constraints can be represented as a finite set of abs</context>
</contexts>
<marker>Maruyama, 1990</marker>
<rawString>H. Maruyama. 1990. Structural disambiguation with constraint propagation. In The Proceedings of ACL’1990, pages 31–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T R Niesler</author>
<author>P C Woodland</author>
</authors>
<title>A variablelength category-based N-gram language model.</title>
<date>1996</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<volume>1</volume>
<pages>164--167</pages>
<contexts>
<context position="2753" citStr="Niesler and Woodland, 1996" startWordPosition="414" endWordPosition="417"> determining: W∗, T∗ = arg max W,T P(W, T)P(A|W, T) �Pr(wN1 ) ≈ t1,t2,...,tN N 11 P(W,T|A) = arg max W,T 1 Introduction ≈ arg max P(W,T)P(A|W) W,T The purpose of a language model (LM) is to determine the a priori probability of a word sequence w1, ..., wn, P(w1, ... , wn). Language modeling is essential in a wide variety of applications; we focus on speech recognition in our research. Although wordbased LMs (with bigram and trigram being the most common) remain the mainstay in many continuous speech recognition systems, recent efforts have explored a variety of ways to improve LM performance (Niesler and Woodland, 1996; Chelba et al., 1997; Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosenfeld, 2000; Goodman, 2001; Roark, 2001; Charniak, 2001). Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using where T is the POS sequence tN 1 associated with the word sequence W = wN1 given the speech utterance A. The LM P(W, T) is a joint probabilistic model that accounts for both the sequence of words wN 1 and their tag assignments tN 1 by estimating the joint probabilities of words and tags: Johnson (2001) and Lafferty et</context>
<context position="15255" citStr="Niesler and Woodland, 1996" startWordPosition="2413" endWordPosition="2416">2, ... , yn is the context of order n-gram to predict x; • Pn(x|y1, y2, ..., yn) is the order n-gram maximum likelihood estimation. Table 1 enumerates the n-grams and their order for the interpolation smoothing of the two distributions in Equation (3). The ordering was based on our hypothesis that n-grams with more fine-grained history information should be ranked higher in the n-gram list since that information should be more helpful for discerning word and SuperARVs based on their history. The SuperARV LM hypothesizes categories for out-of-vocabulary words using the leave-one-out technique (Niesler and Woodland, 1996). Table 1: The enumeration and order of n-grams for smoothing the distributions in Equation (3). n-grams Pˆ(ti|wi−1 Pˆ(wi|wi−1 i−2ti−1 i−2tii−2) i−2) highest Pˆ (ti|wi−1 Pˆ (wi|wi−1 lowest i−2ti−1 i−2ti i−2) i−2) Pˆ (wi|wi−1 Pˆ(ti|wi−1ti−1 i−2ti i−1) i−2) Pˆ(wi|wi−1tii−2) Pˆ (ti|wi−1 Pˆ(wi|wi−1tii−1) i−2ti−1) Pˆ(wi|wi−1ti) Pˆ(ti|ti−1 Pˆ(wi|tii−1) i−2) Pˆ(wi|ti) Pˆ(ti|wi−1ti−1) Pˆ(ti|ti−1) Pˆ(ti) In preliminary experiments, we compared several algorithms for smoothing the probability estimations for our SuperARV LM. The best performance was achieved by using the modified Kneser-Ney smoothing al</context>
<context position="16729" citStr="Niesler and Woodland, 1996" startWordPosition="2631" endWordPosition="2634">he perplexity on a heldout set. In order to compare our SuperARV LM with a word-based LM, we must use the following equation to calculate the word perplexity (PPL): Equation (4) is used by class-based LMs to calculate word perplexity (Heeman, 1998). Parser-based LMs use a similar procedure that sums over parses. The SuperARV LM is most closely related to the almost-parsing-based LM developed by Srinivas (1997). Srinivas’ LM, based on the notion of a supertag, the elementary structure of Lexicalized TreeAdjoining Grammar, achieved a perplexity reduction compared to a conditional POS n-gram LM (Niesler and Woodland, 1996). By comparison, our LM incorporates dependencies directly on words instead of through nonterminals, uses more lexical features than the supertag LM, uses joint instead of conditional probability estimations, and uses modified Kneser-Ney rather than Katz smoothing. 3 Evaluating the SuperARV Language Model Traditionally, the LM quality in speech recognition is evaluated on two metrics: perplexity and WER, with the former commonly selected as a less computationally expensive alternative. We carried out two experiments, one using the Wall Street Journal Penn Treebank (WSJ PTB), a text corpus on w</context>
</contexts>
<marker>Niesler, Woodland, 1996</marker>
<rawString>T. R. Niesler and P. C. Woodland. 1996. A variablelength category-based N-gram language model. In Proceedings of ICASSP, volume 1, pages 164–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Press</author>
<author>B P Flannery</author>
<author>S A Teukolsky</author>
<author>W T Vetterling</author>
</authors>
<title>Numerical Recipes in C.</title>
<date>1988</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="16065" citStr="Press et al., 1988" startWordPosition="2525" endWordPosition="2528">2ti−1 i−2ti i−2) i−2) Pˆ (wi|wi−1 Pˆ(ti|wi−1ti−1 i−2ti i−1) i−2) Pˆ(wi|wi−1tii−2) Pˆ (ti|wi−1 Pˆ(wi|wi−1tii−1) i−2ti−1) Pˆ(wi|wi−1ti) Pˆ(ti|ti−1 Pˆ(wi|tii−1) i−2) Pˆ(wi|ti) Pˆ(ti|wi−1ti−1) Pˆ(ti|ti−1) Pˆ(ti) In preliminary experiments, we compared several algorithms for smoothing the probability estimations for our SuperARV LM. The best performance was achieved by using the modified Kneser-Ney smoothing algorithm initially introduced in (Chen and Goodman, 1998) and adapting it by employing a heldout data set to optimize parameters, including cutoffs for rare n-grams, by using Powell’s search (Press et al., 1988). Parameters are chosen to optimize the perplexity on a heldout set. In order to compare our SuperARV LM with a word-based LM, we must use the following equation to calculate the word perplexity (PPL): Equation (4) is used by class-based LMs to calculate word perplexity (Heeman, 1998). Parser-based LMs use a similar procedure that sums over parses. The SuperARV LM is most closely related to the almost-parsing-based LM developed by Srinivas (1997). Srinivas’ LM, based on the notion of a supertag, the elementary structure of Lexicalized TreeAdjoining Grammar, achieved a perplexity reduction comp</context>
</contexts>
<marker>Press, Flannery, Teukolsky, Vetterling, 1988</marker>
<rawString>W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. 1988. Numerical Recipes in C. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Price</author>
<author>W Fischer</author>
<author>J Bernstein</author>
<author>D Pallett</author>
</authors>
<title>A database for continuous speech recognition in a 1000-word domain.</title>
<date>1988</date>
<booktitle>In Proceedings of ICASSP’1988,</booktitle>
<pages>651--654</pages>
<contexts>
<context position="12145" citStr="Price et al., 1988" startWordPosition="1890" endWordPosition="1893">lude the modifiee lexical category (MC) in our SuperARV structure to impose modifiee constraints. Words typically have more than one SuperARV to indicate different types of word usage. The average number of SuperARVs for words of different lexical categories vary, with verbs having the greatest SuperARV ambiguity. This is mostly due to the variety of feature combinations and variations on complement types and positions. We have observed in several experiments that the number of SuperARVs does not grow significantly as training set size increases; the moderate-sized Resource Management corpus (Price et al., 1988) with 25,168 words produces 328 SuperARVs, compared to 538 SuperARVs for the 1 million word Wall Street Journal (WSJ) Penn Treebank set (Marcus et al., 1993), and 791 for the 37 million word training set of the WSJ continuous speech recognition task. SuperARVs can be accumulated from a corpus annotated with CDG relations and stored directly with words in a lexicon, so we can learn their frequency of occurrence for the corresponding word. A SuperARV can then be selected from the lexicon and used to generate role values that meet their constraints. Since there are no large benchmark corpora anno</context>
<context position="14183" citStr="Price et al., 1988" startWordPosition="2217" endWordPosition="2220">the data structure level as was done in (Galescu and Ringger, 1999) since this could cause serious data sparsity problems. To estimate the probability distributions in Equation (3) from training data, we use recursive linear interpolation among probability estimations of different orders. Representing each multiplicand in Equation (3) as the conditional probability ˆP(x|y1, y2, ..., yam,) where y1, y2, ..., yam, belong to a mixed set of words and SuperARVs, the recursive linear interpolation is calculated as follows: 1We have annotated a moderate-sized corpus, DARPA Naval Resource Management (Price et al., 1988), with CDG parse relations as reported in (Harper et al., 2000; Harper and Wang, 2001). Pr(wN1 tN1 ) = N Pr(witi|wi−1 11 1 ti−1 1 ) = N ri ≈ N ri ˆPn(x|y1, y2, ... ,yn) = λ(x, y1, y2, ... , yn) · Pn(x|y1, y2,. . . , yn) +(1 − λ(x, y1, y2, ..., yn)) · ˆPn−1(x|y1, y2, ... , yn−1) PPL = 2En (4) i−1 Pˆ(w) 1 En ≈ − N N log2 i=1 r_ti−2,ilog2 Pˆ(witi|wi−1 i−2ti−1 i−2)ˆP(wi−1 i−2ti−1 i−2) i−1 i−1 Pˆ(wi−2ti−2) where: ≈ − 1 N N i=1 r_ ti−2,i−1 • y1, y2, ... , yn is the context of order n-gram to predict x; • Pn(x|y1, y2, ..., yn) is the order n-gram maximum likelihood estimation. Table 1 enumerates the </context>
</contexts>
<marker>Price, Fischer, Bernstein, Pallett, 1988</marker>
<rawString>P. J. Price, W. Fischer, J. Bernstein, and D. Pallett. 1988. A database for continuous speech recognition in a 1000-word domain. In Proceedings of ICASSP’1988, pages 651–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="2863" citStr="Roark, 2001" startWordPosition="433" endWordPosition="434"> max P(W,T)P(A|W) W,T The purpose of a language model (LM) is to determine the a priori probability of a word sequence w1, ..., wn, P(w1, ... , wn). Language modeling is essential in a wide variety of applications; we focus on speech recognition in our research. Although wordbased LMs (with bigram and trigram being the most common) remain the mainstay in many continuous speech recognition systems, recent efforts have explored a variety of ways to improve LM performance (Niesler and Woodland, 1996; Chelba et al., 1997; Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosenfeld, 2000; Goodman, 2001; Roark, 2001; Charniak, 2001). Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using where T is the POS sequence tN 1 associated with the word sequence W = wN1 given the speech utterance A. The LM P(W, T) is a joint probabilistic model that accounts for both the sequence of words wN 1 and their tag assignments tN 1 by estimating the joint probabilities of words and tags: Johnson (2001) and Lafferty et al. (2001) provide insight into why a joint model is superior to a conditional model. Recently, there has bee</context>
<context position="18172" citStr="Roark (2001)" startWordPosition="2864" endWordPosition="2865">periments compare our SuperARV LM to a baseline trigram, a POS LM that was implemented using Equation (3) (where for this model t represents POS tags instead of SuperARV tags) and modified Kneser-Ney smoothing (as used in the SuperARV LM), and one or more parser-based LMs. Additionally, we evaluate the performance of a conditional probability SuperARV LM (denoted cSuperARV) implemented following Equation (1) rather than Equation (3) to evaluate the importance of using joint probability estimations. For the WSJ PTB task, we compare the SuperARV LMs to the parser LMs developed by Chelba (2000), Roark (2001), and Charniak (2001). Although Srinivas (1997) developed an almost-parsing supertag-based LM, we cannot compare his LM with the other LMs because he used a small non-standard subset of the WSJ PTB2 and a trainable supertag LM is unavailable. Because none of the parser LMs has been fully trained for the WSJ CSR task, it is essential that we retrain them for comparison. The availability of a trainable version of Chelba’s model enables us to train and test on the CSR task; however, because we do not have access to a trainable version of Charniak’s or Roark’s LMs, they are not considered in the C</context>
<context position="23740" citStr="Roark (2001)" startWordPosition="3796" endWordPosition="3797">it becomes the parse tree for the training sentence; otherwise, we use the corresponding tree in the BLLIP treebank (Charniak et al., 2000). Since WSJ CSR is a speech corpus, there is no punctuation or case information. All words outside the provided vocabulary are mapped to (UNK). Note that 3In the remaining experiments, the POS LM and the SuperARV LM are not interpolated with a trigram. Perplexity LM 3gram Model Intp (Weight) r POS 167.14 142.55 142.55 (1.0) 0.95 SuperARV 167.14 118.35 118.35 (1.0) 0.92 cSuperARV 167.14 150.01 143.83 (0.65) 0.68 Chelba (2000) 167.14 158.28 148.90 (0.64) N/A Roark (2001) 167.02 152.26 137.26 (0.64) N/A Charniak (2001) 167.89 130.20 126.07 (0.64) N/A Chelba 167.14 153.76 147.70 (0.64) 0.73 Charniak 167.14 130.20 126.03 (0.64) 0.69 Table 2: Comparing perplexity results for each LM on the WSJ PTB test set. 3gram represents the word-based trigram LM, Intp (weight)the LM interpolated with a trigram (and the interpolation weight), and r the correlation value. N/A means not available. the word-level tokenization of treebank texts differs from that used in the speech recognition task with the major differences being: numbers (e.g., “1.2%” versus “one point two percen</context>
<context position="25000" citStr="Roark, 2001" startWordPosition="3981" endWordPosition="3982">er twentieth, two thousand one”) , currencies (e.g., “$10.25” versus “ten dollars and twenty five cents”), common abbreviations (e.g., “Inc.” versus “Incorporated”), acronyms (e.g., “I.B.M.” versus “I. B. M.”), hyphenated and period-delimited phrases (e.g., “red-carpet” versus “red carpet”), and contractions and possessives (e.g., “do n’t” versus “don’t”). The POS, parser-based, and SuperARV LMs are all trained using the textbased tokenization from the treebank. Hence, during testing, a transformation converts the output of the recognizer to a form compatible with the text-based tokenization (Roark, 2001) for rescoring. For testing the LMs, we use the four available WSJ CSR evaluation sets: 1992 5K closed vocabulary (denoted 92-5k) with 330 utterances and 5,353 words, 1993 5K closed vocabulary (93-5k) with 215 utterances and 3,849 words, 1992 20K open vocabulary (92-20k) with 333 utterances and 5,643 words, and 1993 20K (93-20k) with 213 utterances and 3,446 words. We also employ a development set for each vocabulary size: 93-5k-dt (513 utterances and 8,635 words) and 93-20k-dt (252 utterances and 4,062 words). The trigram provided by LDC for the CSR task was used due to its high quality. Befo</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>B. Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>Two decades of statistical language modeling: Where do we go from here?</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>88--1270</pages>
<contexts>
<context position="2835" citStr="Rosenfeld, 2000" startWordPosition="428" endWordPosition="430">arg max W,T 1 Introduction ≈ arg max P(W,T)P(A|W) W,T The purpose of a language model (LM) is to determine the a priori probability of a word sequence w1, ..., wn, P(w1, ... , wn). Language modeling is essential in a wide variety of applications; we focus on speech recognition in our research. Although wordbased LMs (with bigram and trigram being the most common) remain the mainstay in many continuous speech recognition systems, recent efforts have explored a variety of ways to improve LM performance (Niesler and Woodland, 1996; Chelba et al., 1997; Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosenfeld, 2000; Goodman, 2001; Roark, 2001; Charniak, 2001). Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using where T is the POS sequence tN 1 associated with the word sequence W = wN1 given the speech utterance A. The LM P(W, T) is a joint probabilistic model that accounts for both the sequence of words wN 1 and their tag assignments tN 1 by estimating the joint probabilities of words and tags: Johnson (2001) and Lafferty et al. (2001) provide insight into why a joint model is superior to a conditional mo</context>
</contexts>
<marker>Rosenfeld, 2000</marker>
<rawString>R. Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from here? Proceedings of the IEEE, 88:1270–1278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
</authors>
<title>Complexity of lexical descriptions and its relevance to partial parsing.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2790" citStr="Srinivas, 1997" startWordPosition="422" endWordPosition="423">T) �Pr(wN1 ) ≈ t1,t2,...,tN N 11 P(W,T|A) = arg max W,T 1 Introduction ≈ arg max P(W,T)P(A|W) W,T The purpose of a language model (LM) is to determine the a priori probability of a word sequence w1, ..., wn, P(w1, ... , wn). Language modeling is essential in a wide variety of applications; we focus on speech recognition in our research. Although wordbased LMs (with bigram and trigram being the most common) remain the mainstay in many continuous speech recognition systems, recent efforts have explored a variety of ways to improve LM performance (Niesler and Woodland, 1996; Chelba et al., 1997; Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosenfeld, 2000; Goodman, 2001; Roark, 2001; Charniak, 2001). Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using where T is the POS sequence tN 1 associated with the word sequence W = wN1 given the speech utterance A. The LM P(W, T) is a joint probabilistic model that accounts for both the sequence of words wN 1 and their tag assignments tN 1 by estimating the joint probabilities of words and tags: Johnson (2001) and Lafferty et al. (2001) provide insight into why </context>
<context position="16515" citStr="Srinivas (1997)" startWordPosition="2600" endWordPosition="2601">dman, 1998) and adapting it by employing a heldout data set to optimize parameters, including cutoffs for rare n-grams, by using Powell’s search (Press et al., 1988). Parameters are chosen to optimize the perplexity on a heldout set. In order to compare our SuperARV LM with a word-based LM, we must use the following equation to calculate the word perplexity (PPL): Equation (4) is used by class-based LMs to calculate word perplexity (Heeman, 1998). Parser-based LMs use a similar procedure that sums over parses. The SuperARV LM is most closely related to the almost-parsing-based LM developed by Srinivas (1997). Srinivas’ LM, based on the notion of a supertag, the elementary structure of Lexicalized TreeAdjoining Grammar, achieved a perplexity reduction compared to a conditional POS n-gram LM (Niesler and Woodland, 1996). By comparison, our LM incorporates dependencies directly on words instead of through nonterminals, uses more lexical features than the supertag LM, uses joint instead of conditional probability estimations, and uses modified Kneser-Ney rather than Katz smoothing. 3 Evaluating the SuperARV Language Model Traditionally, the LM quality in speech recognition is evaluated on two metrics</context>
<context position="18219" citStr="Srinivas (1997)" startWordPosition="2871" endWordPosition="2872">ine trigram, a POS LM that was implemented using Equation (3) (where for this model t represents POS tags instead of SuperARV tags) and modified Kneser-Ney smoothing (as used in the SuperARV LM), and one or more parser-based LMs. Additionally, we evaluate the performance of a conditional probability SuperARV LM (denoted cSuperARV) implemented following Equation (1) rather than Equation (3) to evaluate the importance of using joint probability estimations. For the WSJ PTB task, we compare the SuperARV LMs to the parser LMs developed by Chelba (2000), Roark (2001), and Charniak (2001). Although Srinivas (1997) developed an almost-parsing supertag-based LM, we cannot compare his LM with the other LMs because he used a small non-standard subset of the WSJ PTB2 and a trainable supertag LM is unavailable. Because none of the parser LMs has been fully trained for the WSJ CSR task, it is essential that we retrain them for comparison. The availability of a trainable version of Chelba’s model enables us to train and test on the CSR task; however, because we do not have access to a trainable version of Charniak’s or Roark’s LMs, they are not considered in the CSR task. Note that for lattice rescoring, howev</context>
<context position="20814" citStr="Srinivas, 1997" startWordPosition="3309" endWordPosition="3310">arniak’s interpolated trihead LM a close second at 24.91%. The cSuperARV LM is clearly inferior to the SuperARV LM, even after interpolation. This result highlights the value of tight coupling of word, lexical feature, and syntactic knowledge both at the data structure level (which is the same for the SuperARV and cSuperARV LMs) and at the probability model level (which is different). Notice that the cSuperARV, Chelba’s, Roark’s, and Charniak’s LMs obtain an improvement in performance when interpolated with a trigram; whereas, 2Using the same 180,000 word training and 20,000 word test set as (Srinivas, 1997), our SuperARV LM obtains a perplexity of 92.76, compared to a perplexity of 101 obtained by the supertag LM. the POS LM and the SuperARV LM do not benefit from trigram interpolation3. To gain more insight into why a trigram is effectively interpolated with some, but not all, of the LMs, we calculate the correlation of the trigram with each LM. A standard correlation is calculated between the probabilities assigned to each test set sentence by the trigram LM and the LM in question. This technique has been used in (Wang et al., 2002) to identify whether two LMs can be effectively interpolated. </context>
</contexts>
<marker>Srinivas, 1997</marker>
<rawString>B. Srinivas. 1997. Complexity of lexical descriptions and its relevance to partial parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>M P Harper</author>
</authors>
<title>Investigating probabilistic constraint dependency grammars in language modeling.</title>
<date>2001</date>
<tech>Technical Report TR-ECE01-4,</tech>
<institution>Purdue University, School of Electrical Engineering.</institution>
<marker>Wang, Harper, 2001</marker>
<rawString>W. Wang and M. P. Harper. 2001. Investigating probabilistic constraint dependency grammars in language modeling. Technical Report TR-ECE01-4, Purdue University, School of Electrical Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>Y Liu</author>
<author>M P Harper</author>
</authors>
<title>Rescoring effectiveness of language models using different levels of knowledge and their integration.</title>
<date>2002</date>
<booktitle>In Proceedings of ICASSP’2002.</booktitle>
<contexts>
<context position="21352" citStr="Wang et al., 2002" startWordPosition="3404" endWordPosition="3407">sing the same 180,000 word training and 20,000 word test set as (Srinivas, 1997), our SuperARV LM obtains a perplexity of 92.76, compared to a perplexity of 101 obtained by the supertag LM. the POS LM and the SuperARV LM do not benefit from trigram interpolation3. To gain more insight into why a trigram is effectively interpolated with some, but not all, of the LMs, we calculate the correlation of the trigram with each LM. A standard correlation is calculated between the probabilities assigned to each test set sentence by the trigram LM and the LM in question. This technique has been used in (Wang et al., 2002) to identify whether two LMs can be effectively interpolated. Since we have access to an executable version of Charniak’s LM trained on the WSJ PTB (ftp.cs.brown.edu/pub/nlparser) and a trainable version of Chelba’s LM, we are able to calculate their correlations with our trigram LM. Chelba’s LM was retrained using more parameter reestimation iterations than in (Chelba, 2000) to optimize the performance. Table 2 shows the correlation between each of the executable LMs and the trigram LM. The POS LM has the highest correlation with the trigram, closely followed by the SuperARV LM. Because these</context>
</contexts>
<marker>Wang, Liu, Harper, 2002</marker>
<rawString>W. Wang, Y. Liu, and M. P. Harper. 2002. Rescoring effectiveness of language models using different levels of knowledge and their integration. In Proceedings of ICASSP’2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>