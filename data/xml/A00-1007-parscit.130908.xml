<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000051">
<title confidence="0.995462">
Distilling dialogues - A method using natural dialogue corpora for
dialogue systems development
</title>
<author confidence="0.992418">
Arne Jonsson and Nils Dahlback
</author>
<affiliation confidence="0.9935005">
Department of Computer and Information Science
Linkoping University
</affiliation>
<address confidence="0.95201">
S-581 83, LINKoPING
SWEDEN
</address>
<email confidence="0.99715">
nilda@ida.liu.se, arnjo@ida.liu.se
</email>
<sectionHeader confidence="0.993869" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999585">
We report on a method for utilising corpora col-
lected in natural settings. It is based on distilling
(re-writing) natural dialogues to elicit the type of
dialogue that would occur if one the dialogue par-
ticipants was a computer instead of a human. The
method is a complement to other means such as Wiz-
ard of Oz-studies and un-distilled natural dialogues.
We present the distilling method and guidelines for
distillation. We also illustrate how the method af-
fects a corpus of dialogues and discuss the pros and
cons of three approaches in different phases of dia-
logue systems development.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999988318181818">
It has been known for quite some time now, that
the language used when interacting with a comput-
er is different from the one used in dialogues between
people, (c.f. Jonsson and Dahlback (1988)). Given
that we know that the language will be different,
but not how it will be different, we need to base
our development of natural language dialogue sys-
tems on a relevant set of dialogue corpora. It is our
belief that we need to clarify a number of different
issues regarding the collection and use of corpora in
the development of speech-only and multimodal dia-
logue systems. Exchanging experiences and develop-
ing guidelines in this area are as important as, and in
some sense a necessary pre-requisite to, the develop-
ment of computational models of speech, language,
and dialogue/discourse. It is interesting to note the
difference in the state of art in the field of natu-
ral language dialogue systems with that of corpus
linguistics, where issues of the usefulness of different
samples, the necessary sampling size, representative-
ness in corpus design and other have been discussed
for quite some time (e.g. (Garside et al., 1997; Atkins
et al., 1992; Crowdy, 1993; Biber, 1993)). Also the
neighboring area of evaluation of NLP systems (for
an overview, see Sparck Jones and Galliers (1996))
seems to have advanced further.
Some work have been done in the area of natu-
ral language dialogue systems, e.g. on the design
of Wizard of Oz-studies (Dahlback et al., 1998),
on measures for inter-rater reliability (Carletta,
1996), on frameworks for evaluating spoken dialogue
agents (Walker et al., 1998) and on the use of differ-
ent corpora in the development of a particular sys-
tem (The Carnegie-Mellon Communicator, Eskenazi
et al. (1999)).
The question we are addressing in this paper is
how to collect and analyse relevant corpora. We be-
gin by describing what we consider to be the main
advantages and disadvantages of the two currently
used methods; studies of human dialogues and Wiz-
ard of Oz-dialogues, especially focusing on the eco-
logical validity of the methods. We then describe a
method called &apos;distilling dialogues&apos;, which can serve
as a supplement to the other two.
</bodyText>
<sectionHeader confidence="0.8053685" genericHeader="introduction">
2 Natural and Wizard of
Oz-Dialogues
</sectionHeader>
<bodyText confidence="0.999310115384615">
The advantage of using real dialogues between peo-
ple is that they will illustrate which tasks and needs
that people actually bring to a particular service
provider. Thus, on the level of the users&apos; general
goals, such dialogues have a high validity. But there
are two drawbacks here. First; it is not self-evident
that users will have the same task expectations from
a computer system as they have with a person. Sec-
ond, the language used will differ from the language
used when interacting with a computer.
These two disadvantages have been the major
force behind the development of Wizard of Oz-
methods. The advantage here is that the setting will
be human-computer interaction. But there are im-
portant disadvantages, too. First, on the practical
side, the task of setting up a high quality simulation
environment and training the operators (&apos;wizards&apos;)
to use this is a resource consuming task (Dahlback et
al., 1998). Second, and probably even more impor-
tant, is that we cannot then observe real users using
a system for real life tasks, where they bring their
own needs, motivations, resources, and constraints
to bear. To some extent this problem can be over-
come using well-designed so called &apos;scenarios&apos;. As
pointed out in Dahlback (1991), on many levels of
analysis the artificiality of the situation will not af-
</bodyText>
<page confidence="0.99843">
44
</page>
<bodyText confidence="0.999975943661972">
fect the language used. An example of this is the
pattern of pronoun-antecedent relations. But since
the tasks given to the users are often pre-described
by the researchers, this means that this is not a good
way of finding out which tasks the users actually
want to perform. Nor does it provide a clear enough
picture on how the users will act to find something
that satisfies their requirements. If e.g. the task is
one of finding a charter holiday trip or buying a TV-
set within a specified set of constraints (economical
and other), it is conceivable that people will stay
with the first item that matches the specification,
whereas in real life they would probably look for
alternatives. In our experience, this is primarily a
concern if the focus is on the users&apos; goals and plans,
but is less a problem when the interest is on lower-
level aspects, such as, syntax or patterns of pronoun-
antecedent relationship (c.f. Dahlback (1991)).
To summarize; real life dialogues will provide a
reasonably correct picture of the way users&apos; ap-
proach their tasks, and what tasks they bring to
the service provider, but the language used will not
give a good approximation of what the system un-
der construction will need to handle. Wizard of Oz-
dialogues, on the other hand, will give a reasonable
approximation of some aspects of the language used,
but in an artificial context.
The usual approach has been to work in three
steps. First analyse real human dialogues, and based
on these, in the second phase, design one or more
Wizard of Oz-studies. The final step is to fine-tune
the system&apos;s performance on real users. A good ex-
ample of this method is presented in Eskenazi et al.
(1999). But there are also possible problems with
this approach (though we are not claiming that this
was the case in their particular project). Eskenazi et
al. (1999) asked a human operator to act &apos;computer-
like&apos; in their Wizard of Oz-phase. The advantage
is of course that the human operator will be able
to perform all the tasks that is usually provided by
this service. The disadvantage is that it puts a heavy
burden on the human operator to act as a comput-
er. Since we know that lay-persons&apos; ideas of what
computers can and cannot do are in many respects
far removed from what is actually the case, we risk
introducing some systematic distortion here. And
since it is difficult to perform consistently in similar
situations, we also risk introducing non-systematic
distortion here, even in those cases when the &apos;wiz-
ard&apos; is an NLP-professional.
Our suggestion is therefore to supplement the
above mentioned methods, and bridge the gap be-
tween them, by post-processing human dialogues to
give them a computer-like quality. The advantage,
compared to having people do the simulation on the
fly, is both that it can be done with more consis-
tency, and also that it can be done by researchers
that actually know what human-computer natural
language dialogues can look like. A possible dis-
advantage with using both Wizard of Oz-and real
computer dialogues, is that users will quickly adapt
to what the system can provide them with, and will
therefore not try to use it for tasks they know it
cannot perform. Consequently, we will not get a full
picture of the different services they would like the
system to provide.
A disadvantage with this method is, of course,
that post-processing takes some time compared to
using the natural dialogues as they are. There is al-
so a concern on the ecological validity of the results,
as discussed later.
</bodyText>
<sectionHeader confidence="0.988351" genericHeader="method">
3 Distilling dialogues
</sectionHeader>
<bodyText confidence="0.999934804878049">
Distilling dialogues, i.e. re-writing human interac-
tions in order to have them reflect what a human-
computer interaction could look like involves a num-
ber of considerations. The main issue is that in cor-
pora of natural dialogues one of the interlocutors is
not a dialogue system. The system&apos;s task is instead
performed by a human and the problem is how to
anticipate the behaviour of a system that does not
exist based on the performance of an agent with dif-
ferent performance characteristics. One important
aspect is how to deal with human features that are
not part of what the system is supposed to be able
to handle, for instance if the user talks about things
outside of the domain, such as discussing an episode
of a recent TV show. It also involves issues on how
to handle situations where one of the interlocuters
discusses with someone else on a different topic, e.g.
discussing the up-coming Friday party with a friend
in the middle of an information providing dialogue
with a customer.
It is important for the distilling process to have at
least an outline of the dialogue system that is under
development: Will it for instance have the capacity
to recognise users&apos; goals, even if not explicitly stat-
ed? Will it be able to reason about the discourse
domain? What services will it provide, and what
will be outside its capacity to handle?
In our case, we assume that the planned dialogue
system has the ability to reason on various aspects
of dialogue and properties of the application. In our
current work, and in the examples used for illustra-
tion in this paper, we assume a dialogue model that
can handle any relevant dialogue phenomenon and
also an interpreter and speech recogniser being able
to understand any user input that is relevant to the
task. There is is also a powerful domain reason-
ing module allowing for more or less any knowledge
reasoning on issues that can be accomplished with-
in the domain (Flycht-Eriksson, 1999). Our current
system does, however, not have an explicit user task
model, as opposed to a system task model (Dahlback
</bodyText>
<page confidence="0.995914">
45
</page>
<bodyText confidence="0.9999747">
and Jonsson, 1999), which is included, and thus, we
can not assume that the &apos;system&apos; remembers utter-
ances where the user explains its task. Furthermore,
as our aim is system development we will not con-
sider interaction outside the systems capabilities as
relevant to include in the distilled dialogues.
The context of our work is the development a
multi-modal dialogue system. However, in our cur-
rent work with distilling dialogues, the abilities of
a multi-modal system were not fully accounted for.
The reason for this is that the dialogues would be
significantly affected, e.g. a telephone conversation
where the user always likes to have the next con-
nection, please will result in a table if multi-modal
output is possible and hence a fair amount of the di-
alogue is removed. We have therefore in this paper
analysed the corpus assuming a speech-only system,
since this is closer to the original telephone conversa-
tions, and hence needs fewer assumptions on system
performance when distilling the dialogues.
</bodyText>
<sectionHeader confidence="0.99341" genericHeader="method">
4 Distillation guidelines
</sectionHeader>
<bodyText confidence="0.999277333333333">
Distilling dialogues requires guidelines for how to
handle various types of utterances. In this section
we will present our guidelines for distilling a corpus
of telephone conversations between a human infor-
mation provider on local buses&apos; to be used for devel-
oping a multimodal dialogue system (Qvarfordt and
Jonsson, 1998; Flycht-Eriksson and JOnsson, 1998;
Dahlback et al., 1999; Qvarfordt, 1998). Similar
guidelines are used within another project on devel-
oping Swedish Dialogue Systems where the domain
is travel bureau information.
We can distinguish three types of contributors:
&apos;System&apos; (i.e. a future systems) utterances, User ut-
terances, and other types, such as moves by other
speakers, and noise.
</bodyText>
<subsectionHeader confidence="0.999943">
4.1 Modifying system utterances
</subsectionHeader>
<bodyText confidence="0.9998636">
The problem of modifying &apos;system&apos; utterances can
be divided into two parts: how to change and when
to change. They are in some respects intertwined,
but as the how-part affects the when-part more we
will take this as a starting point.
</bodyText>
<listItem confidence="0.582738666666667">
• The &apos;system&apos; provides as much relevant infor-
mation as possible at once. This depends on
the capabilities of the systems output modal-
ities. If we have a screen or similar output
device we present as much as possible which
normally is all relevant information. If we, on
the other hand, only have spoken output the
amount of information that the hearer can inter-
pret in one utterance must be considered when
</listItem>
<footnote confidence="0.974314">
1The bus time table dialogues are collected at
Linkoping University and are available (in Swedish) on
http://www.ida.liu.se/—arnjo/kfb/dialoger.html
</footnote>
<bodyText confidence="0.9997816875">
distilling. The system might in such cases pro-
vide less information. The principle of provid-
ing all relevant information is based on the as-
sumption that a computer system often has ac-
cess to all relevant information when querying
the background system and can also present it
more conveniently, especially in a multimodal
system (Ahrenberg et al., 1996). A typical ex-
ample is the dialogue fragment in figure 1. In
this fragment the system provides information
on what train to take and how to change to a
bus. The result of distilling this fragment pro-
vides the revised fragment of figure 2. As seen in
the fragment of figure 2 we also remove a num-
ber of utterances typical for human interaction,
as discussed below.
</bodyText>
<listItem confidence="0.8901605">
• System utterances are made more computer-like
and do not include irrelevant information. The
</listItem>
<bodyText confidence="0.991892705882353">
latter is seen in S9 in the dialogue in figure 3
where the provided information is not relevant.
It could also be possible to remove S5 and re-
spond with S7 at once. This, however, depends
on if the information grounded in S5-U6 is need-
ed for the &apos;system&apos; in order to know the arrival
time or if that could be concluded from U4.
This in turn depends on the system&apos;s capabili-
ties. If we assume that the dialogue system has
a model of user tasks, the information in 55-U6
could have been concluded from that. We will,
in this case, retain S5-U6 as we do not assume a
user task model (Dahlback and Jonsson, 1999)
and in order to stay as close to the original di-
alogue as possible.
The next problem concerns the case when &apos;system&apos;
utterances are changed or removed.
</bodyText>
<listItem confidence="0.727838625">
• Dialogue contributions provided by something or
someone other than the user or the &apos;system&apos; are
removed. These are regarded as not being part
of the interaction. This means that if some-
one interrupts the current interaction, say that
the telephone rings during a face-to-face inter-
action, the interrupting interaction is normally
removed from the corpus.
</listItem>
<bodyText confidence="0.98513925">
Furthermore, &apos;system&apos; interruptions are re-
moved. A human can very well interrupt anoth-
er human interlocuter, but a computer system
will not do that.
However, this guideline could lead to problems,
for instance, when users follow up such interrup-
tions. If no information is provided or the in-
terrupted sequence does not affect the dialogue,
we have no problems removing the interruption.
The problem is what to do when information
from the &apos;system&apos; is used in the continuing dia-
logue. For such cases we have no fixed strategy,
</bodyText>
<page confidence="0.996325">
46
</page>
<figure confidence="0.936764642857143">
U4: yes I wonder if you have any mm buses or (.) like express buses leaving from Linkoping
to Vadstena (.) on sunday
ja ville undra om ni hade nagra Oh bussar eller (.) typ expressbussar som ate frdn Linkoping
till Vadstena (.) pd sonda
S5: no the bus does not run on sundays
nej bussen gar inte pd sandagar
U6: how can you (.) can you take the train and then change some way (.) because (.)
to Mjolby &apos;n&apos; so
hur kan man (.) kan man ta tdg d sen byta pa mitt siitt () for de (.)
till mjalby d sd
S7: that you can do too yes
de kan du gdra ocksa ja
U8: how (.) do you have any such suggestions
hur (.) har du ndra ndgra ulna fOrslag
S9: yes let&apos;s see (4s) a moment (15s) now let us see here (.) was it on the sunday you should travel
ja ska se har (4s) ett ogonblick (15s) nu ska vi se Mr (.) va de pd siindagen du skulle aka pd
U10: yes right afternoon preferably
ja just de eftermidda giirna
S11: afternoon preferable (.) you have train from Linkoping fourteen twenty nine
eftermidda garna (.) du har tag Pan LinkOping fjorton a tjugonie
U12: mm
mm
S13: and then you will change from Mjolby station six hundred sixty
d sd byter du fran MjOlby station sexhundrasexti
U14: sixhundred sixty
sexhundrasexti
S15: fifteen and ten
femton a tie
</figure>
<figureCaption confidence="0.834168555555556">
Figure 1: Dialogue fragment from a real interaction on bus time-table information
U4: I wonder if you have any buses or (.) like express buses going from Linkoping
to Vadstena (.) on sunday
S5: no the bus does not run on sundays
U6: how can you (.) can you take the train and then change some way (.) because (.)
to Mjolby and so
S7: you can take the train from Link8ping fourteen and twenty nine and then you will
change at Mjolby station to bus six hundred sixty at fifteen and ten
Figure 2: A distilled version of the dialogue in figure 1
</figureCaption>
<bodyText confidence="0.999858">
the dialogue needs to be rearranged depending
on how the information is to be used (c.f. the
discussion in the final section of this paper).
</bodyText>
<listItem confidence="0.9906075">
• &apos;System&apos; utterances which are no longer valid
are removed. Typical examples of this are the
utterances S7, 59, S1 1 and S13 in the dialogue
fragment of figure 1.
• Remove sequences of utterances where the &apos;sys-
tem&apos; behaves in a way a computer would not do.
</listItem>
<bodyText confidence="0.952662">
For instance jokes, irony, humor, commenting
on the other dialogue participant, or dropping
the telephone (or whatever is going on in S7
in figure 4). A common case of this is when
the &apos;system&apos; is talking while looking for infor-
mation, S5 in the dialogue fragment of figure 4
is an example of this. Related to this is when
the system provides its own comments. If we
can assume that it has such capabilities they
are included, otherwise we remove them.
</bodyText>
<listItem confidence="0.817756">
• The system does not repeat information that has
already been provided unless explicitly asked to
do so. In human interaction it is not uncommon
to repeat what has been uttered for purposes
other than to provide grounding information or
feedback. This is for instance common during
</listItem>
<page confidence="0.993635">
47
</page>
<figure confidence="0.9105155625">
U4: &apos;n&apos; I must be at Resecentrum before fourteen and thirty five (.) &apos;cause we will going to the
interstate buses
ja ska va pa rececentrum innan fjorton a trettifem (.) fö vi ska till
langfardsbussarna
aha (.) &apos;n&apos; then you must be there around twenty past two something then
jaha (.) a dci behaver du va nere strax efter tjuge over tva ndnting da
yes around that
ja ungefiir
let&apos;s see here (11s) two hundred and fourteen Ryd end station leaves forty six (.) thirteen &apos;n&apos;
forty six then you will be down fourteen oh seven (.)
ska vi se hiir (11s) tvahundrafjorton Ryd andhallplatsen gar fOrtisex (.) tretton
fortisex da dr du nere fjorton noll sju (.)
oho
jaha
&apos;n&apos; (.) the next one takes you there (.) fourteen thirty seven (.) but that is too late
(.) nasta är du nere (.) fjorton d trettisju (.) men de a ju fdr sent
</figure>
<figureCaption confidence="0.999165">
Figure 3: Dialogue fragment from a real interaction on bus time-table information
</figureCaption>
<equation confidence="0.8750419375">
Well, hi (.) I am going to Ugglegatan eighth
ja hej (.) ja ska till Ugglegatan atta
Yes
ja
and (.) I wonder (.) it is somewhere in Tannefors
och (.) jag undrar (.) det ligger nanstans i Tannefors
Yes (.) I will see here one one I will look exactly where it is one moment please
ja (.) jag ska se hiir ett ett jag ska titta exakt var det ligger ett ogonblick bara
Oh Yeah
jara
(operator disconnects) (25s) mm (.) okey (5s) what the hell (2s)
(operator connects again) hello yes
((Telefonisten kopplar ur sig)) (25s) ahh (.) okey (5s) de va som faan (2s)
((Telefonisten kopplar in sig igen)) halla ja
Yes hello
ja hej
</equation>
<bodyText confidence="0.949681">
It is bus two hundred ten which runs on old tannefors road that you have to take and get off at
the bus stop at that bus stop named vetegatan
det a buss tvahundratio som gar gamla tanneforsvagen som du far aka d gd av vid
den hallplatsen vid den hdllplatsen som heter vetegatan.
</bodyText>
<figureCaption confidence="0.997817">
Figure 4: Dialogue fragment from a natural bus timetable interaction
</figureCaption>
<figure confidence="0.836405076923077">
S5:
U6:
S7:
U8:
S9:
U2:
S3:
U4:
S5:
U6:
S7:
U8:
S9:.
</figure>
<bodyText confidence="0.61331">
search procedures as discussed above.
</bodyText>
<listItem confidence="0.999587857142857">
• The system does not ask for information it has
already achieved. For instance asking again if it
is on Sunday as in S9 in figure 1. This is not un-
common in human interaction and such utter-
ances from the user are not removed. However,
we can assume that the dialogue system does
not forget what has been talked about before.
</listItem>
<subsectionHeader confidence="0.98372">
4.2 Modifying user utterances
</subsectionHeader>
<bodyText confidence="0.997931857142857">
The general rule is to change user utterances as lit-
tle as possible. The reason for this is that we do not
want to develop systems where the user needs to
restrict his/her behaviour to the capabilities of the
dialogue system. However, there are certain changes
made to user utterances, in most cases as a conse-
quence of changes of system utterances.
</bodyText>
<listItem confidence="0.781311">
• Utterances that are no longer valid are removed.
</listItem>
<bodyText confidence="0.92793325">
The most common cases are utterances whose
request has already been answered, as seen in
the distilled dialogue in figure 2 of the dialogue
in figure 1.
</bodyText>
<page confidence="0.997628">
48
</page>
<figure confidence="0.942868333333333">
S11: sixteen fifty five
sexton ferntifern
U12: sixteen fifty five (.) aha
sexton ferntifem (.) jaha
S13: bus line four hundred thirty five
linje fyrahundra trettifem
</figure>
<figureCaption confidence="0.9661505">
Figure 5: Dialogue fragment from a natural bus
timetable interaction
</figureCaption>
<listItem confidence="0.8309">
• Utterances are removed where the user discuss-
</listItem>
<bodyText confidence="0.959487666666667">
es things that are in the environment. For
instance commenting the &apos;systems&apos; clothes or
hair. This also includes other types of commu-
nicative signals such as laughter based on things
outside the interaction, for instance, in the en-
vironment of the interlocuters.
</bodyText>
<listItem confidence="0.758456">
• User utterances can also be added in order to
</listItem>
<bodyText confidence="0.962652">
make the dialogue continue. In the dialogue in
figure 5 there is nothing in the dialogue explain-
ing why the system utters S13. In such cases
we need to add a user utterance, e.g. Which
bus is that?. However, it might turn out that
there are cues, such as intonation, found when
listening to the tapes. If such detailed analyses
are carried out, we will, of course, not need to
add utterances. Furthermore, it is sometimes
the case that the telephone operator deliberate-
ly splits the information into chunks that can
be comprehended by the user, which then must
be considered in the distillation.
</bodyText>
<sectionHeader confidence="0.976608" genericHeader="method">
5 Applying the method
</sectionHeader>
<bodyText confidence="0.999990835443038">
To illustrate the method we will in this section try to
characterise the results from our distillations. The
illustration is based on 39 distilled dialogues from
the previously mentioned corpus collected with a
telephone operator having information on local bus
time-tables and persons calling the information ser-
vice.
The distillation took about three hours for all 39
dialogues, i.e. it is reasonably fast. The distilled
dialogues are on the average 27% shorter. However,
this varies between the dialogues, at most 73% was
removed but there were also seven dialogues that
were not changed at all.
At the most 34 utterances where removed from
one single dialogue and that was from a dialogue
with discussions on where to find a parking lot, i.e.
discussions outside the capabilities of the applica-
tion. There was one more dialogue where more than
30 utterances were removed and that dialogue is a
typical example of dialogues where distillation actu-
ally is very useful and also indicates what is normal-
ly removed from the dialogues. This particular dia-
logue begins with the user asking for the telephone
number to &apos;the Lost property office&apos; for a specific bus
operator. However, the operator starts a discussion
on what bus the traveller traveled on before provid-
ing the requested telephone number. The reason for
this discussion is probably that the operator knows
that different bus companies are utilised and would
like to make sure that the user really understands
his/her request. The interaction that follows can,
thus, in that respect be relevant, but for our pur-
pose of developing systems based on an overall goal
of providing information, not to understand human
interaction, our dialogue system will not able to han-
dle such phenomenon (Jonsson, 1996).
The dialogues can roughly be divided into five dif-
ferent categories based on the users task. The dis-
cussion in twenty five dialogues were on bus times
between various places, often one departure and one
arrival but five dialogues involved more places. In
five dialogues the discussion was one price and var-
ious types of discounts. Five users wanted to know
the telephone number to &apos;the Lost property office&apos;,
two discussed only bus stops and two discussed how
they could utilise their season ticket to travel out-
side the trafficking area of the bus company. It is
interesting to note that there is no correspondence
between the task being performed during the inter-
action and the amount of changes made to the dia-
logue. Thus, if we can assume that the amount of
distillation indicates something about a user&apos;s inter-
action style, other factors than the task are impor-
tant when characterising user behaviour.
Looking at what is altered we find that the most
important distilling principle is that the &apos;system&apos;
provides all relevant information at once, c.f. fig-
ures 1 and 2. This in turn removes utterances pro-
vided by both &apos;system&apos; and user.
Most added utterances, both from the user and
the &apos;system&apos;, provide explicit requests for informa-
tion that is later provided in the dialogue, e.g. ut-
terance S3 in figure 6. We have added ten utterances
in all 39 dialogues, five &apos;system&apos; utterances and five
user utterances. Note, however, that we utilised the
transcribed dialogues, without information on into-
nation. We would probably not have needed to add
this many utterances if we had utilised the tapes.
Our reason for not using information on intonation
is that we do not assume that our system&apos;s speech
recogniser can recognise intonation.
Finally, as discussed above, we did not utilise the
full potential of multi-modality when distilling the
dialogues. For instance, some dialogues could be
further distilled if we had assumed that the system
had presented a time-table. One reason for this is
that we wanted to capture as many interesting as-
pects intact as possible. The advantage is, thus, that
we have a better corpus for understanding human-
</bodyText>
<page confidence="0.997961">
49
</page>
<bodyText confidence="0.6253464">
U2: Yees hi Anna Nilsson is my name and I would like to take the bus from Ryd center to Resecentrum
in Linkoping
jaa hej Anna Nilsson heter jag och jag viii dka buss fran Ryds centrum till resecentrum
i Linkoping.
S3: mm When do you want to leave?
mm Nar vill du aka?
U4: &apos;n&apos; I must be at Resecentrum before fourteen and thirty five (.) &apos;cause we will going to the
interstate buses
d ja ska va pa rececentrum innan fjorton a trettifem (.) f5 vi ska till
langfardsbussarna
</bodyText>
<figureCaption confidence="0.999557">
Figure 6: Distilled dialogue fragment with added utterance
</figureCaption>
<bodyText confidence="0.960380333333334">
computer interaction and can from that corpus do
a second distillation where we focus more on multi-
modal interaction.
</bodyText>
<sectionHeader confidence="0.999709" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999996263157894">
We have been presenting a method for distilling hu-
man dialogues to make them resemble human com-
puter interaction, in order to utilise such dialogues
as a knowledge source when developing dialogue sys-
tems. Our own main purpose has been to use them
for developing multimodal systems, however, as dis-
cussed above, we have in this paper rather assumed
a speech-only system. But we believe that the basic
approach can be used also for multi-modal systems
and other kinds of natural language dialogue sys-
tems.
It is important to be aware of the limitations of
the method, and how &apos;realistic&apos; the produced result
will be, compared to a dialogue with the final sys-
tem. Since we are changing the dialogue moves, by
for instance providing all required information in one
move, or never asking to be reminded of what the us-
er has previously requested, it is obvious that what
follows after the changed sequence would probably
be affected one way or another. A consequence of
this is that the resulting dialogue is less accurate as
a model of the entire dialogue. It is therefore not an
ideal candidate for trying out the systems over-all
performance during system development. But for
the smaller sub-segments or sub-dialogues, we be-
lieve that it creates a good approximation of what
will take place once the system is up and running.
Furthermore, we believe distilled dialogues in some
respects to be more realistic than Wizard of Oz-
dialogues collected with a wizard acting as a com-
puter.
Another issue, that has been discussed previously
in the description of the method, is that the distilling
is made based on a particular view of what a dialogue
with a computer will look like. While not necessari-
ly being a detailed and specific model, it is at least
an instance of a class of computer dialogue models.
One example of this is whether the system is meant
to acquire information on the user&apos;s underlying mo-
tivations or goals or not. In the examples presented,
we have not assumed such capabilities, but this as-
sumption is not an absolute necessity. We believe,
however, that the distilling process should be based
on one such model, not the least to ensure a con-
sistent treatment of similar recurring phenomena at
different places in the corpora.
The validity of the results based on analysing dis-
tilled dialogues depends partly on how the distilla-
tion has been carried out. Even when using natural
dialogues we can have situations where the interac-
tion is somewhat mysterious, for instance, if some of
the dialogue participants behaves irrational such as
not providing feedback or being too elliptical. How-
ever, if careful considerations have been made to stay
as close to the original dialogues as possible, we be-
lieve that distilled dialogues will reflect what a hu-
man would consider to be a natural interaction.
</bodyText>
<sectionHeader confidence="0.998586" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999967333333333">
This work results from a number of projects on de-
velopment of natural language interfaces supported
by The Swedish Transport &amp; Communications Re-
search Board (KFB) and the joint Research Program
for Language Technology (HSFR/NUTEK). We are
indebted to the participants of the Swedish Dialogue
Systems project, especially to Staffan Larsson, Lena
Santamarta, and Annika Flycht-Eriksson for inter-
esting discussions on this topic.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99813275">
Lars Ahrenberg, Nils Dahlb6,ck, Arne Jonsson,
and Ake Thuree. 1996. Customizing interac-
tion for natural language interfaces. Link&amp;ping
Electronic articles in Computer and Informa-
tion Science, also in Notes from Workshop on
Pragmatics in Dialogue, The XIV:th Scandi-
navian Conference of Linguistics and the VI-
II:th Conference of Nordic and General Linguis-
</reference>
<page confidence="0.948479">
50
</page>
<reference confidence="0.99965804109589">
tics, Goteborg, Sweden, 1993, 1(1), October, 1.
http://www.ep.liu.se/ea/cis/1996/001/.
Sue Atkins, Jeremy Clear, and Nicholas Ostler.
1992. Corpus design criteria. Literary and Lin-
guistic Computing, 7(1):1-16.
Douglas Biber. 1993. Representativeness in cor-
pus design. Literary and Linguistic Computing,
8(4):244-257.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computation-
al Linguistics, 22(2):249-254.
Steve Crowdy. 1993. Spoken corpus design. Literary
and Linguistic Computing, 8(4):259-265.
Nils Dahlback and Arne Jonsson. 1999. Knowledge
sources in spoken dialogue systems. In Proceed-
ings of Eurospeech&apos;99, Budapest, Hungary.
Nils Dahlback, Arne Jonsson, and Lars Ahrenberg.
1998. Wizard of oz studies — why and how.
In Mark Maybury &amp; Wolfgang Wahlster, editor,
Readings in Intelligent User Interfaces. Morgan
Kaufmann.
Nils Dahlback, Annilca Flycht-Eriksson, Arne
Jonsson, and Pernilla Qvarfordt. 1999. An ar-
chitecture for multi-modal natural dialogue sys-
tems. In Proceedings of ESCA Tutorial and Re-
search Workshop (ETRW) on Interactive Dialogue
in Multi-Modal Systems, Germany.
Nils Dahlback. 1991. Representations of Discourse,
Cognitive and Computational Aspects. Ph.D. the-
sis, Link6ping University.
Maxine Eskenazi, Alexander Rudnicki, Karin Grego-
ry, Paul Constantinides, Robert Brennan, Christi-
na Bennett, and Jwan Allen. 1999. Data collec-
tion and processing in the carnegie mellon com-
municator. In Proceedings of Eurospeech&apos;99, Bu-
dapest, Hungary.
Annilca Flycht-Eriksson and Arne Jonsson. 1998. A
spoken dialogue system utilizing spatial informa-
tion. In Proceedings of ICSLP&apos;98, Sydney, Aus-
tralia.
Annika Flycht-Eriksson. 1999. A survey of knowl-
edge sources in dialogue systems. In Proceedings
of IJCAI-99 Workshop on Knowledge and Reason-
ing in Practical Dialogue Systems, August, Stock-
holm.
Roger Garside, Geoffrey Leech, and Anthony
McEnery. 1997. Corpus Annotation. Longman.
Arne Jonsson and Nils Dahlback. 1988. Talking to a
computer is not like talking to your best friend. In
Proceedings of the First Scandinavian Conference
on Artificial Interlligence, Tromso.
Arne Jonsson. 1996. Natural language generation
without intentions. In Proceedings of ECAI&apos;96
Workshop Gaps and Bridges: New Directions
in Planning and Natural Language Generation,
pages 102-104.
Pernilla Qvarfordt and Arne Jonsson. 1998. Effects
of using speech in timetable information systems
for www. In Proceedings of ICSLP&apos;98, Sydney,
Australia.
Pernilla Qvarfordt. 1998. Usability of multimodal
timetables: Effects of different levels of do-
main knowledge on usability. Master&apos;s thesis,
Linkoping University.
Karen Sparck Jones and Julia R. Galliers. 1996.
Evaluating Natural Language Processing Systems.
Springer Verlag.
Marilyn A. Walker, Diane J. Litman, Candace A.
Kamm, and Alicia Abella. 1998. Paradise: A
framework for evaluating spoken dialogue agents.
In Mark Maybury &amp; Wolfgang Wahlster, editor,
Readings in Intelligent User Interfaces. Morgan
Kaufmann.
</reference>
<page confidence="0.999087">
51
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.552269">
<title confidence="0.9958985">Distilling dialogues - A method using natural dialogue corpora for dialogue systems development</title>
<author confidence="0.999907">Arne Jonsson</author>
<author confidence="0.999907">Nils Dahlback</author>
<affiliation confidence="0.9999795">Department of Computer and Information Science Linkoping University</affiliation>
<address confidence="0.7870585">S-581 83, LINKoPING SWEDEN</address>
<email confidence="0.878143">nilda@ida.liu.se,arnjo@ida.liu.se</email>
<abstract confidence="0.997262615384615">We report on a method for utilising corpora collected in natural settings. It is based on distilling (re-writing) natural dialogues to elicit the type of dialogue that would occur if one the dialogue participants was a computer instead of a human. The method is a complement to other means such as Wizard of Oz-studies and un-distilled natural dialogues. We present the distilling method and guidelines for distillation. We also illustrate how the method affects a corpus of dialogues and discuss the pros and cons of three approaches in different phases of dialogue systems development.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Lars Ahrenberg</author>
<author>Nils Dahlb6</author>
<author>Arne Jonsson ck</author>
<author>Ake Thuree</author>
</authors>
<title>Customizing interaction for natural language interfaces.</title>
<date>1996</date>
<booktitle>Link&amp;ping Electronic articles in Computer and Information Science, also in Notes from Workshop on Pragmatics in Dialogue, The XIV:th Scandinavian Conference of Linguistics and the VIII:th Conference of Nordic and General Linguistics,</booktitle>
<volume>1</volume>
<pages>1996--001</pages>
<location>Goteborg, Sweden,</location>
<marker>Ahrenberg, Dahlb6, ck, Thuree, 1996</marker>
<rawString>Lars Ahrenberg, Nils Dahlb6,ck, Arne Jonsson, and Ake Thuree. 1996. Customizing interaction for natural language interfaces. Link&amp;ping Electronic articles in Computer and Information Science, also in Notes from Workshop on Pragmatics in Dialogue, The XIV:th Scandinavian Conference of Linguistics and the VIII:th Conference of Nordic and General Linguistics, Goteborg, Sweden, 1993, 1(1), October, 1. http://www.ep.liu.se/ea/cis/1996/001/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sue Atkins</author>
<author>Jeremy Clear</author>
<author>Nicholas Ostler</author>
</authors>
<date>1992</date>
<booktitle>Corpus design criteria. Literary and Linguistic Computing,</booktitle>
<pages>7--1</pages>
<contexts>
<context position="2019" citStr="Atkins et al., 1992" startWordPosition="324" endWordPosition="327">eech-only and multimodal dialogue systems. Exchanging experiences and developing guidelines in this area are as important as, and in some sense a necessary pre-requisite to, the development of computational models of speech, language, and dialogue/discourse. It is interesting to note the difference in the state of art in the field of natural language dialogue systems with that of corpus linguistics, where issues of the usefulness of different samples, the necessary sampling size, representativeness in corpus design and other have been discussed for quite some time (e.g. (Garside et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)). Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further. Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlback et al., 1998), on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al. (1999)). The question </context>
</contexts>
<marker>Atkins, Clear, Ostler, 1992</marker>
<rawString>Sue Atkins, Jeremy Clear, and Nicholas Ostler. 1992. Corpus design criteria. Literary and Linguistic Computing, 7(1):1-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<date>1993</date>
<booktitle>Representativeness in corpus design. Literary and Linguistic Computing,</booktitle>
<pages>8--4</pages>
<contexts>
<context position="2047" citStr="Biber, 1993" startWordPosition="330" endWordPosition="331">ystems. Exchanging experiences and developing guidelines in this area are as important as, and in some sense a necessary pre-requisite to, the development of computational models of speech, language, and dialogue/discourse. It is interesting to note the difference in the state of art in the field of natural language dialogue systems with that of corpus linguistics, where issues of the usefulness of different samples, the necessary sampling size, representativeness in corpus design and other have been discussed for quite some time (e.g. (Garside et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)). Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further. Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlback et al., 1998), on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al. (1999)). The question we are addressing in this pa</context>
</contexts>
<marker>Biber, 1993</marker>
<rawString>Douglas Biber. 1993. Representativeness in corpus design. Literary and Linguistic Computing, 8(4):244-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--2</pages>
<contexts>
<context position="2392" citStr="Carletta, 1996" startWordPosition="386" endWordPosition="387">at of corpus linguistics, where issues of the usefulness of different samples, the necessary sampling size, representativeness in corpus design and other have been discussed for quite some time (e.g. (Garside et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)). Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further. Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlback et al., 1998), on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al. (1999)). The question we are addressing in this paper is how to collect and analyse relevant corpora. We begin by describing what we consider to be the main advantages and disadvantages of the two currently used methods; studies of human dialogues and Wizard of Oz-dialogues, especially focusing on the ecological validity of the methods. We then describe a method called &apos;distilling dialogues&apos;,</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Crowdy</author>
</authors>
<date>1993</date>
<booktitle>Spoken corpus design. Literary and Linguistic Computing,</booktitle>
<pages>8--4</pages>
<contexts>
<context position="2033" citStr="Crowdy, 1993" startWordPosition="328" endWordPosition="329">dal dialogue systems. Exchanging experiences and developing guidelines in this area are as important as, and in some sense a necessary pre-requisite to, the development of computational models of speech, language, and dialogue/discourse. It is interesting to note the difference in the state of art in the field of natural language dialogue systems with that of corpus linguistics, where issues of the usefulness of different samples, the necessary sampling size, representativeness in corpus design and other have been discussed for quite some time (e.g. (Garside et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)). Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further. Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlback et al., 1998), on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al. (1999)). The question we are address</context>
</contexts>
<marker>Crowdy, 1993</marker>
<rawString>Steve Crowdy. 1993. Spoken corpus design. Literary and Linguistic Computing, 8(4):259-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils Dahlback</author>
<author>Arne Jonsson</author>
</authors>
<title>Knowledge sources in spoken dialogue systems.</title>
<date>1999</date>
<booktitle>In Proceedings of Eurospeech&apos;99,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="13989" citStr="Dahlback and Jonsson, 1999" startWordPosition="2350" endWordPosition="2353">t information. The latter is seen in S9 in the dialogue in figure 3 where the provided information is not relevant. It could also be possible to remove S5 and respond with S7 at once. This, however, depends on if the information grounded in S5-U6 is needed for the &apos;system&apos; in order to know the arrival time or if that could be concluded from U4. This in turn depends on the system&apos;s capabilities. If we assume that the dialogue system has a model of user tasks, the information in 55-U6 could have been concluded from that. We will, in this case, retain S5-U6 as we do not assume a user task model (Dahlback and Jonsson, 1999) and in order to stay as close to the original dialogue as possible. The next problem concerns the case when &apos;system&apos; utterances are changed or removed. • Dialogue contributions provided by something or someone other than the user or the &apos;system&apos; are removed. These are regarded as not being part of the interaction. This means that if someone interrupts the current interaction, say that the telephone rings during a face-to-face interaction, the interrupting interaction is normally removed from the corpus. Furthermore, &apos;system&apos; interruptions are removed. A human can very well interrupt another h</context>
</contexts>
<marker>Dahlback, Jonsson, 1999</marker>
<rawString>Nils Dahlback and Arne Jonsson. 1999. Knowledge sources in spoken dialogue systems. In Proceedings of Eurospeech&apos;99, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils Dahlback</author>
<author>Arne Jonsson</author>
<author>Lars Ahrenberg</author>
</authors>
<title>Wizard of oz studies — why and how.</title>
<date>1998</date>
<booktitle>Readings in Intelligent User Interfaces.</booktitle>
<editor>In Mark Maybury &amp; Wolfgang Wahlster, editor,</editor>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="2334" citStr="Dahlback et al., 1998" startWordPosition="377" endWordPosition="380"> of art in the field of natural language dialogue systems with that of corpus linguistics, where issues of the usefulness of different samples, the necessary sampling size, representativeness in corpus design and other have been discussed for quite some time (e.g. (Garside et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)). Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further. Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlback et al., 1998), on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al. (1999)). The question we are addressing in this paper is how to collect and analyse relevant corpora. We begin by describing what we consider to be the main advantages and disadvantages of the two currently used methods; studies of human dialogues and Wizard of Oz-dialogues, especially focusing on the ecological validity of the methods</context>
<context position="3998" citStr="Dahlback et al., 1998" startWordPosition="651" endWordPosition="654"> not self-evident that users will have the same task expectations from a computer system as they have with a person. Second, the language used will differ from the language used when interacting with a computer. These two disadvantages have been the major force behind the development of Wizard of Ozmethods. The advantage here is that the setting will be human-computer interaction. But there are important disadvantages, too. First, on the practical side, the task of setting up a high quality simulation environment and training the operators (&apos;wizards&apos;) to use this is a resource consuming task (Dahlback et al., 1998). Second, and probably even more important, is that we cannot then observe real users using a system for real life tasks, where they bring their own needs, motivations, resources, and constraints to bear. To some extent this problem can be overcome using well-designed so called &apos;scenarios&apos;. As pointed out in Dahlback (1991), on many levels of analysis the artificiality of the situation will not af44 fect the language used. An example of this is the pattern of pronoun-antecedent relations. But since the tasks given to the users are often pre-described by the researchers, this means that this is</context>
</contexts>
<marker>Dahlback, Jonsson, Ahrenberg, 1998</marker>
<rawString>Nils Dahlback, Arne Jonsson, and Lars Ahrenberg. 1998. Wizard of oz studies — why and how. In Mark Maybury &amp; Wolfgang Wahlster, editor, Readings in Intelligent User Interfaces. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils Dahlback</author>
<author>Annilca Flycht-Eriksson</author>
<author>Arne Jonsson</author>
<author>Pernilla Qvarfordt</author>
</authors>
<title>An architecture for multi-modal natural dialogue systems.</title>
<date>1999</date>
<booktitle>In Proceedings of ESCA Tutorial and Research Workshop (ETRW) on Interactive Dialogue in Multi-Modal Systems,</booktitle>
<location>Germany.</location>
<contexts>
<context position="11417" citStr="Dahlback et al., 1999" startWordPosition="1912" endWordPosition="1915"> in this paper analysed the corpus assuming a speech-only system, since this is closer to the original telephone conversations, and hence needs fewer assumptions on system performance when distilling the dialogues. 4 Distillation guidelines Distilling dialogues requires guidelines for how to handle various types of utterances. In this section we will present our guidelines for distilling a corpus of telephone conversations between a human information provider on local buses&apos; to be used for developing a multimodal dialogue system (Qvarfordt and Jonsson, 1998; Flycht-Eriksson and JOnsson, 1998; Dahlback et al., 1999; Qvarfordt, 1998). Similar guidelines are used within another project on developing Swedish Dialogue Systems where the domain is travel bureau information. We can distinguish three types of contributors: &apos;System&apos; (i.e. a future systems) utterances, User utterances, and other types, such as moves by other speakers, and noise. 4.1 Modifying system utterances The problem of modifying &apos;system&apos; utterances can be divided into two parts: how to change and when to change. They are in some respects intertwined, but as the how-part affects the when-part more we will take this as a starting point. • The</context>
</contexts>
<marker>Dahlback, Flycht-Eriksson, Jonsson, Qvarfordt, 1999</marker>
<rawString>Nils Dahlback, Annilca Flycht-Eriksson, Arne Jonsson, and Pernilla Qvarfordt. 1999. An architecture for multi-modal natural dialogue systems. In Proceedings of ESCA Tutorial and Research Workshop (ETRW) on Interactive Dialogue in Multi-Modal Systems, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils Dahlback</author>
</authors>
<title>Representations of Discourse, Cognitive and Computational Aspects.</title>
<date>1991</date>
<tech>Ph.D. thesis,</tech>
<institution>Link6ping University.</institution>
<contexts>
<context position="4323" citStr="Dahlback (1991)" startWordPosition="707" endWordPosition="708">hat the setting will be human-computer interaction. But there are important disadvantages, too. First, on the practical side, the task of setting up a high quality simulation environment and training the operators (&apos;wizards&apos;) to use this is a resource consuming task (Dahlback et al., 1998). Second, and probably even more important, is that we cannot then observe real users using a system for real life tasks, where they bring their own needs, motivations, resources, and constraints to bear. To some extent this problem can be overcome using well-designed so called &apos;scenarios&apos;. As pointed out in Dahlback (1991), on many levels of analysis the artificiality of the situation will not af44 fect the language used. An example of this is the pattern of pronoun-antecedent relations. But since the tasks given to the users are often pre-described by the researchers, this means that this is not a good way of finding out which tasks the users actually want to perform. Nor does it provide a clear enough picture on how the users will act to find something that satisfies their requirements. If e.g. the task is one of finding a charter holiday trip or buying a TVset within a specified set of constraints (economica</context>
</contexts>
<marker>Dahlback, 1991</marker>
<rawString>Nils Dahlback. 1991. Representations of Discourse, Cognitive and Computational Aspects. Ph.D. thesis, Link6ping University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxine Eskenazi</author>
<author>Alexander Rudnicki</author>
<author>Karin Gregory</author>
<author>Paul Constantinides</author>
<author>Robert Brennan</author>
<author>Christina Bennett</author>
<author>Jwan Allen</author>
</authors>
<title>Data collection and processing in the carnegie mellon communicator.</title>
<date>1999</date>
<booktitle>In Proceedings of Eurospeech&apos;99,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2603" citStr="Eskenazi et al. (1999)" startWordPosition="418" endWordPosition="421">side et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)). Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further. Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlback et al., 1998), on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al. (1999)). The question we are addressing in this paper is how to collect and analyse relevant corpora. We begin by describing what we consider to be the main advantages and disadvantages of the two currently used methods; studies of human dialogues and Wizard of Oz-dialogues, especially focusing on the ecological validity of the methods. We then describe a method called &apos;distilling dialogues&apos;, which can serve as a supplement to the other two. 2 Natural and Wizard of Oz-Dialogues The advantage of using real dialogues between people is that they will illustrate which tasks and needs that people actuall</context>
<context position="6078" citStr="Eskenazi et al. (1999)" startWordPosition="1012" endWordPosition="1015">nd what tasks they bring to the service provider, but the language used will not give a good approximation of what the system under construction will need to handle. Wizard of Ozdialogues, on the other hand, will give a reasonable approximation of some aspects of the language used, but in an artificial context. The usual approach has been to work in three steps. First analyse real human dialogues, and based on these, in the second phase, design one or more Wizard of Oz-studies. The final step is to fine-tune the system&apos;s performance on real users. A good example of this method is presented in Eskenazi et al. (1999). But there are also possible problems with this approach (though we are not claiming that this was the case in their particular project). Eskenazi et al. (1999) asked a human operator to act &apos;computerlike&apos; in their Wizard of Oz-phase. The advantage is of course that the human operator will be able to perform all the tasks that is usually provided by this service. The disadvantage is that it puts a heavy burden on the human operator to act as a computer. Since we know that lay-persons&apos; ideas of what computers can and cannot do are in many respects far removed from what is actually the case, we</context>
</contexts>
<marker>Eskenazi, Rudnicki, Gregory, Constantinides, Brennan, Bennett, Allen, 1999</marker>
<rawString>Maxine Eskenazi, Alexander Rudnicki, Karin Gregory, Paul Constantinides, Robert Brennan, Christina Bennett, and Jwan Allen. 1999. Data collection and processing in the carnegie mellon communicator. In Proceedings of Eurospeech&apos;99, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annilca Flycht-Eriksson</author>
<author>Arne Jonsson</author>
</authors>
<title>A spoken dialogue system utilizing spatial information.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP&apos;98,</booktitle>
<location>Sydney, Australia.</location>
<marker>Flycht-Eriksson, Jonsson, 1998</marker>
<rawString>Annilca Flycht-Eriksson and Arne Jonsson. 1998. A spoken dialogue system utilizing spatial information. In Proceedings of ICSLP&apos;98, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annika Flycht-Eriksson</author>
</authors>
<title>A survey of knowledge sources in dialogue systems.</title>
<date>1999</date>
<booktitle>In Proceedings of IJCAI-99 Workshop on Knowledge and Reasoning in Practical Dialogue Systems,</booktitle>
<location>August, Stockholm.</location>
<contexts>
<context position="9879" citStr="Flycht-Eriksson, 1999" startWordPosition="1667" endWordPosition="1668">e its capacity to handle? In our case, we assume that the planned dialogue system has the ability to reason on various aspects of dialogue and properties of the application. In our current work, and in the examples used for illustration in this paper, we assume a dialogue model that can handle any relevant dialogue phenomenon and also an interpreter and speech recogniser being able to understand any user input that is relevant to the task. There is is also a powerful domain reasoning module allowing for more or less any knowledge reasoning on issues that can be accomplished within the domain (Flycht-Eriksson, 1999). Our current system does, however, not have an explicit user task model, as opposed to a system task model (Dahlback 45 and Jonsson, 1999), which is included, and thus, we can not assume that the &apos;system&apos; remembers utterances where the user explains its task. Furthermore, as our aim is system development we will not consider interaction outside the systems capabilities as relevant to include in the distilled dialogues. The context of our work is the development a multi-modal dialogue system. However, in our current work with distilling dialogues, the abilities of a multi-modal system were not</context>
</contexts>
<marker>Flycht-Eriksson, 1999</marker>
<rawString>Annika Flycht-Eriksson. 1999. A survey of knowledge sources in dialogue systems. In Proceedings of IJCAI-99 Workshop on Knowledge and Reasoning in Practical Dialogue Systems, August, Stockholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Garside</author>
<author>Geoffrey Leech</author>
<author>Anthony McEnery</author>
</authors>
<title>Corpus Annotation.</title>
<date>1997</date>
<publisher>Longman.</publisher>
<contexts>
<context position="1998" citStr="Garside et al., 1997" startWordPosition="320" endWordPosition="323"> the development of speech-only and multimodal dialogue systems. Exchanging experiences and developing guidelines in this area are as important as, and in some sense a necessary pre-requisite to, the development of computational models of speech, language, and dialogue/discourse. It is interesting to note the difference in the state of art in the field of natural language dialogue systems with that of corpus linguistics, where issues of the usefulness of different samples, the necessary sampling size, representativeness in corpus design and other have been discussed for quite some time (e.g. (Garside et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)). Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further. Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlback et al., 1998), on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al. (</context>
</contexts>
<marker>Garside, Leech, McEnery, 1997</marker>
<rawString>Roger Garside, Geoffrey Leech, and Anthony McEnery. 1997. Corpus Annotation. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Jonsson</author>
<author>Nils Dahlback</author>
</authors>
<title>Talking to a computer is not like talking to your best friend.</title>
<date>1988</date>
<booktitle>In Proceedings of the First Scandinavian Conference on Artificial Interlligence,</booktitle>
<location>Tromso.</location>
<contexts>
<context position="1061" citStr="Jonsson and Dahlback (1988)" startWordPosition="164" endWordPosition="167">would occur if one the dialogue participants was a computer instead of a human. The method is a complement to other means such as Wizard of Oz-studies and un-distilled natural dialogues. We present the distilling method and guidelines for distillation. We also illustrate how the method affects a corpus of dialogues and discuss the pros and cons of three approaches in different phases of dialogue systems development. 1 Introduction It has been known for quite some time now, that the language used when interacting with a computer is different from the one used in dialogues between people, (c.f. Jonsson and Dahlback (1988)). Given that we know that the language will be different, but not how it will be different, we need to base our development of natural language dialogue systems on a relevant set of dialogue corpora. It is our belief that we need to clarify a number of different issues regarding the collection and use of corpora in the development of speech-only and multimodal dialogue systems. Exchanging experiences and developing guidelines in this area are as important as, and in some sense a necessary pre-requisite to, the development of computational models of speech, language, and dialogue/discourse. It</context>
</contexts>
<marker>Jonsson, Dahlback, 1988</marker>
<rawString>Arne Jonsson and Nils Dahlback. 1988. Talking to a computer is not like talking to your best friend. In Proceedings of the First Scandinavian Conference on Artificial Interlligence, Tromso.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Jonsson</author>
</authors>
<title>Natural language generation without intentions.</title>
<date>1996</date>
<booktitle>In Proceedings of ECAI&apos;96 Workshop Gaps and Bridges: New Directions in Planning and Natural Language Generation,</booktitle>
<pages>102--104</pages>
<contexts>
<context position="23786" citStr="Jonsson, 1996" startWordPosition="4099" endWordPosition="4100"> specific bus operator. However, the operator starts a discussion on what bus the traveller traveled on before providing the requested telephone number. The reason for this discussion is probably that the operator knows that different bus companies are utilised and would like to make sure that the user really understands his/her request. The interaction that follows can, thus, in that respect be relevant, but for our purpose of developing systems based on an overall goal of providing information, not to understand human interaction, our dialogue system will not able to handle such phenomenon (Jonsson, 1996). The dialogues can roughly be divided into five different categories based on the users task. The discussion in twenty five dialogues were on bus times between various places, often one departure and one arrival but five dialogues involved more places. In five dialogues the discussion was one price and various types of discounts. Five users wanted to know the telephone number to &apos;the Lost property office&apos;, two discussed only bus stops and two discussed how they could utilise their season ticket to travel outside the trafficking area of the bus company. It is interesting to note that there is </context>
</contexts>
<marker>Jonsson, 1996</marker>
<rawString>Arne Jonsson. 1996. Natural language generation without intentions. In Proceedings of ECAI&apos;96 Workshop Gaps and Bridges: New Directions in Planning and Natural Language Generation, pages 102-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pernilla Qvarfordt</author>
<author>Arne Jonsson</author>
</authors>
<title>Effects of using speech in timetable information systems for www.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP&apos;98,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="11359" citStr="Qvarfordt and Jonsson, 1998" startWordPosition="1904" endWordPosition="1907">ence a fair amount of the dialogue is removed. We have therefore in this paper analysed the corpus assuming a speech-only system, since this is closer to the original telephone conversations, and hence needs fewer assumptions on system performance when distilling the dialogues. 4 Distillation guidelines Distilling dialogues requires guidelines for how to handle various types of utterances. In this section we will present our guidelines for distilling a corpus of telephone conversations between a human information provider on local buses&apos; to be used for developing a multimodal dialogue system (Qvarfordt and Jonsson, 1998; Flycht-Eriksson and JOnsson, 1998; Dahlback et al., 1999; Qvarfordt, 1998). Similar guidelines are used within another project on developing Swedish Dialogue Systems where the domain is travel bureau information. We can distinguish three types of contributors: &apos;System&apos; (i.e. a future systems) utterances, User utterances, and other types, such as moves by other speakers, and noise. 4.1 Modifying system utterances The problem of modifying &apos;system&apos; utterances can be divided into two parts: how to change and when to change. They are in some respects intertwined, but as the how-part affects the w</context>
</contexts>
<marker>Qvarfordt, Jonsson, 1998</marker>
<rawString>Pernilla Qvarfordt and Arne Jonsson. 1998. Effects of using speech in timetable information systems for www. In Proceedings of ICSLP&apos;98, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pernilla Qvarfordt</author>
</authors>
<title>Usability of multimodal timetables: Effects of different levels of domain knowledge on usability. Master&apos;s thesis,</title>
<date>1998</date>
<institution>Linkoping University.</institution>
<contexts>
<context position="11435" citStr="Qvarfordt, 1998" startWordPosition="1916" endWordPosition="1917"> the corpus assuming a speech-only system, since this is closer to the original telephone conversations, and hence needs fewer assumptions on system performance when distilling the dialogues. 4 Distillation guidelines Distilling dialogues requires guidelines for how to handle various types of utterances. In this section we will present our guidelines for distilling a corpus of telephone conversations between a human information provider on local buses&apos; to be used for developing a multimodal dialogue system (Qvarfordt and Jonsson, 1998; Flycht-Eriksson and JOnsson, 1998; Dahlback et al., 1999; Qvarfordt, 1998). Similar guidelines are used within another project on developing Swedish Dialogue Systems where the domain is travel bureau information. We can distinguish three types of contributors: &apos;System&apos; (i.e. a future systems) utterances, User utterances, and other types, such as moves by other speakers, and noise. 4.1 Modifying system utterances The problem of modifying &apos;system&apos; utterances can be divided into two parts: how to change and when to change. They are in some respects intertwined, but as the how-part affects the when-part more we will take this as a starting point. • The &apos;system&apos; provides</context>
</contexts>
<marker>Qvarfordt, 1998</marker>
<rawString>Pernilla Qvarfordt. 1998. Usability of multimodal timetables: Effects of different levels of domain knowledge on usability. Master&apos;s thesis, Linkoping University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
<author>Julia R Galliers</author>
</authors>
<title>Evaluating Natural Language Processing Systems.</title>
<date>1996</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="2159" citStr="Jones and Galliers (1996)" startWordPosition="346" endWordPosition="349">ome sense a necessary pre-requisite to, the development of computational models of speech, language, and dialogue/discourse. It is interesting to note the difference in the state of art in the field of natural language dialogue systems with that of corpus linguistics, where issues of the usefulness of different samples, the necessary sampling size, representativeness in corpus design and other have been discussed for quite some time (e.g. (Garside et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)). Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further. Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlback et al., 1998), on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al. (1999)). The question we are addressing in this paper is how to collect and analyse relevant corpora. We begin by describing what we consider to be the main advan</context>
</contexts>
<marker>Jones, Galliers, 1996</marker>
<rawString>Karen Sparck Jones and Julia R. Galliers. 1996. Evaluating Natural Language Processing Systems. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Diane J Litman</author>
<author>Candace A Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>Paradise: A framework for evaluating spoken dialogue agents.</title>
<date>1998</date>
<booktitle>Readings in Intelligent User Interfaces.</booktitle>
<editor>In Mark Maybury &amp; Wolfgang Wahlster, editor,</editor>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="2467" citStr="Walker et al., 1998" startWordPosition="395" endWordPosition="398">samples, the necessary sampling size, representativeness in corpus design and other have been discussed for quite some time (e.g. (Garside et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)). Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further. Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlback et al., 1998), on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al. (1999)). The question we are addressing in this paper is how to collect and analyse relevant corpora. We begin by describing what we consider to be the main advantages and disadvantages of the two currently used methods; studies of human dialogues and Wizard of Oz-dialogues, especially focusing on the ecological validity of the methods. We then describe a method called &apos;distilling dialogues&apos;, which can serve as a supplement to the other two. 2 Natural and Wizard of </context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1998</marker>
<rawString>Marilyn A. Walker, Diane J. Litman, Candace A. Kamm, and Alicia Abella. 1998. Paradise: A framework for evaluating spoken dialogue agents. In Mark Maybury &amp; Wolfgang Wahlster, editor, Readings in Intelligent User Interfaces. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>