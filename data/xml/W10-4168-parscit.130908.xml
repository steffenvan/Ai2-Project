<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.9931605">
NEUNLPLab Chinese Word Sense Induction System for
SIGHAN Bakeoff 2010
</title>
<author confidence="0.998022">
Hao Zhang Tong Xiao Jingbo Zhu
</author>
<affiliation confidence="0.856863">
1. Key Laboratory of Medical Image Computing (Northeastern University), Ministry
of Education
</affiliation>
<address confidence="0.783837">
2. Natural Language Processing Laboratory, Northeastern University
</address>
<email confidence="0.990516">
zhanghao1216@gmail.com
{xiaotong, zhujingbo}@mail.neu.edu.cn
</email>
<sectionHeader confidence="0.995591" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998669666666667">
This paper describes a character-based
Chinese word sense induction (WSI) sys-
tem for the International Chinese Lan-
guage Processing Bakeoff 2010. By
computing the longest common sub-
strings between any two contexts of the
ambiguous word, our system extracts
collocations as features and does not de-
pend on any extra tools, such as Chinese
word segmenters. We also design a con-
strained clustering algorithm for this task.
Experiemental results show that our sys-
tem could achieve 69.88 scores of
FScore on the development data set of
SIGHAN Bakeoff 2010.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943485714286">
The goal of word sense induction (WSI) is to
group occurrences containing a given ambiguous
word into clusters with respect to sense. Most
researchers take the problem of word sense in-
duction as a clustering problem. Pantel &amp; Lin
(2002) clustered words on the basis of the dis-
tances of their co-occurrence vectors, and used
global clustering as a solution. Neill (2002) used
local clustering, and determined the senses of a
given word by clustering its close associations.
In this paper, we propose a simple but effec-
tive method to extract collocations as features
from texts without pre-segmentations, and de-
sign a constrained clustering algorithm to ad-
dress the issue of Chinese word sense induction.
By using our collocation extraction method, our
Chinese WSI system is independent of any extra
natural language processing tools, such as Chi-
nese word segmenters. On the development set
of SIGHAN 2010 WSI task, the experimental
results show that our system could achieve 69.88
scores of FScore. In addition, the official results
show that the performance of our system is
67.15 scores of FScore on the test set of
SIGHAN Bakeoff 2010.
The rest of this paper is organized as follows.
In Section 2, we present the task description of
Chinese word sense induction. In Section 3, we
first give an overview of our Chinese WSI sys-
tem, and then propose our feature extraction
method and constrained clustering algorithm. In
Section 4, we describe the evaluation method
and show the experimental results on the devel-
opment and test data sets of the Bakeoff 2010. In
Section 5, we conclude our work.
</bodyText>
<sectionHeader confidence="0.974829" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.999940142857143">
Given the number of senses S and occurrences of
the ambiguous word w, a word sense induction
system is supposed to cluster the occurrences
into S clusters, with each cluster representing a
sense of the ambiguous word w. For example,
suppose that there are some sentences containing
the ambiguous word “暗淡” (gloomy), and the
sense number S is 2, the job of WSI system is to
cluster these sentences into 2 clusters, with each
cluster representing a sense of “暗淡”. Based on
this task description, it is obvious to regard the
problem of WSI as a clustering problem.
Figures 1-2 shows example input and output
of our WSI system , where there are 6 sentences
and 2 resulting clusters. In Figure 1, the first
column are the identifiers of sentences contain-
ing the word “暗淡”, and the second column are
part of the sentences. In Figure 2, the first col-
umn represents the identifiers of sentences, and
the second column represents the identifiers of
clusters generated by our Chinese WSI system.
</bodyText>
<figureCaption confidence="0.989551">
Figure 1 Part of input of word “暗淡” for our
</figureCaption>
<figure confidence="0.82009">
WSI system
</figure>
<figureCaption confidence="0.99234">
Figure 2 Output of our WSI system for word
</figureCaption>
<bodyText confidence="0.870217">
“暗淡”
</bodyText>
<sectionHeader confidence="0.954259" genericHeader="method">
3 NEU Chinese WSI System
</sectionHeader>
<subsectionHeader confidence="0.999367">
3.1 System overview
</subsectionHeader>
<bodyText confidence="0.999918428571429">
Our Chinese word sense induction system is
built based on clustering work-frame. There are
four major modules in the system, including
data pre-processing, feature extraction, cluster-
ing and data post-processing modules. The ar-
chitecture of our Chinese WSI system is illus-
trated in Figure 3.
</bodyText>
<subsectionHeader confidence="0.999104">
3.2 Feature extraction
</subsectionHeader>
<bodyText confidence="0.954782423076923">
Since there is no separators in Chinese like
“space” in English to mark word boundaries,
most Chinese natural language processing appli-
cations need to first apply a Chinese word seg-
menter to segment Chinese sentences. In our
Chinese word sense induction system, we extract
collocations from sentences containing the am-
biguous word as features. To extract collocations,
we might first segment the sentences into word
sequences, and then conduct feature extraction
on the word-segmented corpus. However, errors
might be induced in the procedure due to un-
avoidable incorrect segmentation results. Ad-
dressing this issue, we propose a method to di-
rectly extract collocations from sentences with-
out pre-segmentations.
In our method, we extract two kinds of collo-
cations, namely “global collocation” and “local
collocation”. Here global collocations are de-
fined to be the words (or character sequences)
that frequently co-occur with the ambiguous
word, and local collocations are defined to be
the characters adjacent to the ambiguous word1.
data pre-processing
feature extraction
clustering
</bodyText>
<figureCaption confidence="0.831506">
Figure 3 Architecture of our system
</figureCaption>
<bodyText confidence="0.911017230769231">
To extract global collocations, we first com-
pute all the longest common substrings between
any two of the sentences containing the ambigu-
ous word to form the set of candidate global col-
locations. For each candidate global collocation,
we count the number of sentences containing it.
We then reduce the size of the candidate set by
eliminating candidates which contain only one
character or functional words. We also remove
the candidate with other candidates as its sub-
strings. Finally, we eliminate the candidates
whose count of the number of sentences is below
a certain threshold. The threshold equals to two
in our experiments. We regard the candidates
after the above processing as global collocations
for WSI.
To extract local collocations, we simply ex-
tract one character on both left and right sides of
the ambiguous word to form the set of candidate
local collocations. We then refine the candidate
set by eliminating candidates which are func-
tional words or whose frequency is below a cer-
tain threshold. The threshold is set to two in our
experiments.
After extracting global collocations and local
collocations, we put them together to form the
1 Definitions of global collocation and local collocation
might be different from those in other papers.
start
data post-processing
end
final set of collocations and use them as features
of our system. For each collocation (or feature),
we compute the list of indices of sentences that
containing the collocation. Thus, every element
of the set of collocations has the data structure of
pair of “key” and “value”, where “key” is the
collocation itself, and the “value” is the list of
indices.
</bodyText>
<subsectionHeader confidence="0.999689">
3.3 Clustering algorithm
</subsectionHeader>
<bodyText confidence="0.99998772">
We find that the high-confidence collocation is a
very good indicator to distinguish the senses of
an ambiguous word. However, the traditional
clustering methods are based on the vector rep-
resentations of features, which probably de-
creases the effect of dominant features (i.e. high-
confidence collocations). To alleviate the prob-
lem, a nice way is to incorporate collocations
into the clustering process as constraints. Moti-
vated by this idea, we design a constrained clus-
tering algorithm. In this algorithm, we could en-
sure that some occurrences of the ambiguous
word must be in one cluster and some must not
be in one cluster. The input for our constrained
clustering algorithm is the set of collocations
described in the previous section and the process
of our clustering algorithm is shown in Table 1.
Here the notation starting with character “C”
represents a collocation, and the notations of
“Sin” and “Srlt” represent the collocation set and
the result set, respectively.
Every element in the result set Srlt is regarded
as one cluster for a given ambigous word, and
the list of the element records the indices of the
sentences belonging to the cluster.
</bodyText>
<sectionHeader confidence="0.956255" genericHeader="evaluation">
4 Evaluation of Our System
</sectionHeader>
<bodyText confidence="0.997433666666667">
The evaluation method is F-score which is pro-
vided within the Bakeoff 2010 (Zhao and
Karypis, 2005). Suppose Cr is a class of the gold
standard, and Si is a cluster of our system gener-
ated. FScore is computed with the formulas be-
low.
</bodyText>
<equation confidence="0.9700494">
F −score(Cr,Si) = 2*P*R / (P+ R) (1)
FScore Cr =
( ) max(F − score Cr Si (2)
( , ))
Si
</equation>
<bodyText confidence="0.9875865">
We evaluate our Chinese word sense induc-
tion system on the development data set and the
test data set of the Bakeoff 2010. The details of
the development data set and the test data set are
summarized in Table 2.
For comparison, we develop a baseline system
that also uses the collocations as features and
clustering based on the vector representations of
features. On the development data set, we test
our system and compare it with the baseline sys-
tem. The performance of our Chinese WSI sys-
tem and the baseline system are shown in Table
3. From Table 3, we see that using our con-
strained clustering algorithm is better than using
the traditional hierarchical clustering methods by
7.06 scores of FScore for our Chinese WSI sys-
tem. It indicates that our constrained clustering
algorithm could avoid reducing the effect of
Input: collocation set Sin
while there is available collocation Ci in the
input set Sin
for each collocation Ct in the set Sin
if Ct not equals to Ci, and Ct is avail-
able
</bodyText>
<listItem confidence="0.715078583333333">
if list of Ct has intersection with
that of Ci, or Ct and Ci have a
meaningful substring (word or
character), compose list of Ct into
list of Ci, and mark Ct to be un-
available
end if
end if
end for
store Ci and its list into result set Srlt, and
mark Ci to be unavailable
end while
</listItem>
<bodyText confidence="0.9934408">
if there are available collocations in the input
set Sin
if the size of result set Srlt does not sat-
isfy the given cluster number, devide the
rest collocations in Sin evenly into the
rest clusters, and append their lists to
their own clusters’ lists respectively
else add the rest collocations into the last
cluster, and append their list to the list of
the last cluster
</bodyText>
<footnote confidence="0.457842666666667">
end if
end if
return the result set Srlt
</footnote>
<note confidence="0.27025">
Output: result set Srlt
</note>
<tableCaption confidence="0.979591">
Table 1 Constrained clustering algorithm
</tableCaption>
<equation confidence="0.611720375">
nr
FScore = ∑ FScore(Cr
n
) (3)
c
1
=
r
</equation>
<bodyText confidence="0.980681111111111">
high-confidence features (i.e. high-confidence
collocations) and lead to better clustering results.
This conclusion is also ensured by the compari-
son between our constrained clustering algo-
rithm and the traditional K-means clustering al-
gorithm.
In addition, our system achieves 67.15 scores
of FScore on the test data set reported by the
SIGHAN Bakeoff 2010.
</bodyText>
<table confidence="0.971264571428571">
data descriptions
Dev set containing 50 ambiguous words,
about 50 sentences for each am-
biguous word
Test set containing 100 ambiguous words,
about 50 sentences for each am-
biguous word
</table>
<tableCaption confidence="0.932763">
Table 2 Data sets of SIGHAN Bakeoff 2010
</tableCaption>
<table confidence="0.996428714285714">
clustering methods FScore of
our system
(%)
traditional hierarchical cluster- 62.82
ing
traditional K-means clustering 62.48
our constrained clustering 69.88
</table>
<tableCaption confidence="0.986731">
Table 3 System performance on dev set of
Bakeoff 2010 using different clustering methods
</tableCaption>
<sectionHeader confidence="0.99869" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999949125">
In this paper, we propose a collocation extrac-
tion method and a constrained clustering algo-
rithm for Chinese WSI task. By using the collo-
cation extraction method and the clustering algo-
rithm, our Chinese word sense induction system
is independent of any extra tools. When tested
on the test data set of the Bakeoff 2010, our sys-
tem achieves 67.15 scores of FScore.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999720676470588">
Vickrey, David, Luke Biewald, Marc Teyssler, and
Daphne Koller. 2005. Word-sense disambiguation
for machine translation. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
Morristown, NJ, USA, pages 771-778.
Yarowsky, David. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of 33rd Meeting of the Association for
Computational Linguistics, Cambridge, MA, 189-
196.
Schutze, Hinrich. 1998. Automatic word sense dis-
crimination. Computational Linguistics, Montreal,
Canada, 24(1):97–123.
Ng, Hwee Tou, Hian Beng Lee. 1996. Integrating
Multiple Knowledge Sources to Disambiguate
Word Sense: An Exemplar-Based Approach. In
Proceedings of the 34th Meeting of the Association
for Computational Linguistics, California, USA,
pages 40-47.
Daniel, Neill. 2002. Fully Automatic Word Sense
Induction by Semantic Clustering. In Computer
Speech, Cambridge University, Master’s Thesis.
Pantel, Patrick, Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD,
Edmonton, 613-619.
Rapp, Reinhard. 2004. A Practical Solution to the
Problem of Automatic Word Sense Induction. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics, Barcelona, Spain.
Zhao, Ying, George Karypis. 2005. Hierarchical
Clustering Algorithms for Document Datasets.
Data Mining and Knowledge Discovery, 10:141-
168.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.168691">
<title confidence="0.997852">NEUNLPLab Chinese Word Sense Induction System</title>
<author confidence="0.7427305">SIGHAN Bakeoff Hao Zhang Tong Xiao Jingbo Zhu</author>
<affiliation confidence="0.838285">1. Key Laboratory of Medical Image Computing (Northeastern University), Ministry of Education 2. Natural Language Processing Laboratory, Northeastern University</affiliation>
<email confidence="0.871684">xiaotong@mail.neu.edu.cn</email>
<email confidence="0.871684">zhujingbo@mail.neu.edu.cn</email>
<abstract confidence="0.9797919375">This paper describes a character-based Chinese word sense induction (WSI) system for the International Chinese Language Processing Bakeoff 2010. By computing the longest common substrings between any two contexts of the ambiguous word, our system extracts collocations as features and does not depend on any extra tools, such as Chinese word segmenters. We also design a constrained clustering algorithm for this task. Experiemental results show that our system could achieve 69.88 scores of the development data set of SIGHAN Bakeoff 2010.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Luke Biewald</author>
<author>Marc Teyssler</author>
<author>Daphne Koller</author>
</authors>
<title>Word-sense disambiguation for machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>771--778</pages>
<location>Morristown, NJ, USA,</location>
<marker>Vickrey, Biewald, Teyssler, Koller, 2005</marker>
<rawString>Vickrey, David, Luke Biewald, Marc Teyssler, and Daphne Koller. 2005. Word-sense disambiguation for machine translation. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, Morristown, NJ, USA, pages 771-778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of 33rd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA,</location>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, David. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of 33rd Meeting of the Association for Computational Linguistics, Cambridge, MA, 189-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Automatic word sense discrimination. Computational Linguistics,</title>
<date>1998</date>
<location>Montreal, Canada,</location>
<marker>Schutze, 1998</marker>
<rawString>Schutze, Hinrich. 1998. Automatic word sense discrimination. Computational Linguistics, Montreal, Canada, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<location>California, USA,</location>
<marker>Ng, Lee, 1996</marker>
<rawString>Ng, Hwee Tou, Hian Beng Lee. 1996. Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach. In Proceedings of the 34th Meeting of the Association for Computational Linguistics, California, USA, pages 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neill Daniel</author>
</authors>
<title>Fully Automatic Word Sense Induction by Semantic Clustering.</title>
<date>2002</date>
<institution>In Computer Speech, Cambridge University, Master’s Thesis.</institution>
<marker>Daniel, 2002</marker>
<rawString>Daniel, Neill. 2002. Fully Automatic Word Sense Induction by Semantic Clustering. In Computer Speech, Cambridge University, Master’s Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGKDD, Edmonton,</booktitle>
<pages>613--619</pages>
<contexts>
<context position="1134" citStr="Pantel &amp; Lin (2002)" startWordPosition="166" endWordPosition="169">ings between any two contexts of the ambiguous word, our system extracts collocations as features and does not depend on any extra tools, such as Chinese word segmenters. We also design a constrained clustering algorithm for this task. Experiemental results show that our system could achieve 69.88 scores of FScore on the development data set of SIGHAN Bakeoff 2010. 1 Introduction The goal of word sense induction (WSI) is to group occurrences containing a given ambiguous word into clusters with respect to sense. Most researchers take the problem of word sense induction as a clustering problem. Pantel &amp; Lin (2002) clustered words on the basis of the distances of their co-occurrence vectors, and used global clustering as a solution. Neill (2002) used local clustering, and determined the senses of a given word by clustering its close associations. In this paper, we propose a simple but effective method to extract collocations as features from texts without pre-segmentations, and design a constrained clustering algorithm to address the issue of Chinese word sense induction. By using our collocation extraction method, our Chinese WSI system is independent of any extra natural language processing tools, suc</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Pantel, Patrick, Dekang Lin. 2002. Discovering word senses from text. In Proceedings of ACM SIGKDD, Edmonton, 613-619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>A Practical Solution to the Problem of Automatic Word Sense Induction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<marker>Rapp, 2004</marker>
<rawString>Rapp, Reinhard. 2004. A Practical Solution to the Problem of Automatic Word Sense Induction. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhao</author>
<author>George Karypis</author>
</authors>
<title>Hierarchical Clustering Algorithms for Document Datasets. Data Mining and Knowledge Discovery,</title>
<date>2005</date>
<pages>10--141</pages>
<contexts>
<context position="8043" citStr="Zhao and Karypis, 2005" startWordPosition="1300" endWordPosition="1303">ring algorithm is the set of collocations described in the previous section and the process of our clustering algorithm is shown in Table 1. Here the notation starting with character “C” represents a collocation, and the notations of “Sin” and “Srlt” represent the collocation set and the result set, respectively. Every element in the result set Srlt is regarded as one cluster for a given ambigous word, and the list of the element records the indices of the sentences belonging to the cluster. 4 Evaluation of Our System The evaluation method is F-score which is provided within the Bakeoff 2010 (Zhao and Karypis, 2005). Suppose Cr is a class of the gold standard, and Si is a cluster of our system generated. FScore is computed with the formulas below. F −score(Cr,Si) = 2*P*R / (P+ R) (1) FScore Cr = ( ) max(F − score Cr Si (2) ( , )) Si We evaluate our Chinese word sense induction system on the development data set and the test data set of the Bakeoff 2010. The details of the development data set and the test data set are summarized in Table 2. For comparison, we develop a baseline system that also uses the collocations as features and clustering based on the vector representations of features. On the develo</context>
</contexts>
<marker>Zhao, Karypis, 2005</marker>
<rawString>Zhao, Ying, George Karypis. 2005. Hierarchical Clustering Algorithms for Document Datasets. Data Mining and Knowledge Discovery, 10:141-168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>