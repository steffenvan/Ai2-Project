<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002143">
<title confidence="0.9937985">
DLIREC: Aspect Term Extraction and Term Polarity Classification
System
</title>
<author confidence="0.996245">
Zhiqiang Toh
</author>
<affiliation confidence="0.976763">
Institute for Infocomm Research
</affiliation>
<address confidence="0.905006">
1 Fusionopolis Way
Singapore 138632
</address>
<email confidence="0.997147">
ztoh@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.993811" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999605">
This paper describes our system used in
the Aspect Based Sentiment Analysis Task
4 at the SemEval-2014. Our system con-
sists of two components to address two of
the subtasks respectively: a Conditional
Random Field (CRF) based classifier for
Aspect Term Extraction (ATE) and a linear
classifier for Aspect Term Polarity Classi-
fication (ATP). For the ATE subtask, we
implement a variety of lexicon, syntac-
tic and semantic features, as well as clus-
ter features induced from unlabeled data.
Our system achieves state-of-the-art per-
formances in ATE, ranking 1st (among 28
submissions) and 2rd (among 27 submis-
sions) for the restaurant and laptop domain
respectively.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998838">
Sentiment analysis on document and sentence
level no longer fulfills user’s needs of getting more
accurate and precise information. By perform-
ing sentiment analysis at the aspect level, we can
help users gain more insights on the sentiments of
the various aspects of the target entity. Task 4 of
SemEval-2014 provides a good platform for (1)
aspect term extraction and (2) aspect term polar-
ity classification.
For the first subtask, we follow the approach of
Jakob and Gurevych (2010) by modeling term ex-
traction as a sequential labeling task. Specifically,
we leverage on semantic and syntactic resources
to extract a variety of features and use CRF as the
learning algorithm. For the second subtask, we
</bodyText>
<footnote confidence="0.9630168">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails: http://creativecommons.org/licenses/
by/4.0/
</footnote>
<author confidence="0.571887">
Wenting Wang
</author>
<email confidence="0.965048">
magicwwt@gmail.com
</email>
<bodyText confidence="0.99981175">
simply treat it as a multi-class classification prob-
lem where a linear classifier is learned to predict
the polarity class. Our system achieves good per-
formances for the first subtask in both domains,
ranking 1st for the restaurant domain, and 2nd for
the laptop domain.
The remainder of this paper is structured as fol-
lows: In Section 2, we describe our ATE system
in detail, including experiments and result analy-
sis. Section 3 describes the general approach of
our ATP system. Finally, Section 4 summarizes
our work.
</bodyText>
<sectionHeader confidence="0.955951" genericHeader="method">
2 Aspect Term Extraction
</sectionHeader>
<bodyText confidence="0.999972266666667">
This subtask is to identify the aspects of given tar-
get entities in the restaurant and laptop domains.
Many aspect terms in the laptop domain con-
tains digits or special characters such as “17 inch
screen” and “screen/video resolution”; while in
the restaurant domain, aspects in the sentences are
specific for a type of restaurants such as “pizza”
for Italian restaurants and “sushi” for Japanese
restaurants.
We model ATE as a sequential labeling task
and extract features to be used for CRF training.
Besides the common features used in traditional
Named Entity Recognition (NER) systems, we
also utilize extensive external resources to build
various name lists and word clusters.
</bodyText>
<subsectionHeader confidence="0.992926">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.99698975">
Following the traditional BIO scheme used in se-
quential labeling, we assign a label for each word
in the sentence, where “B-TERM” indicates the
start of an aspect term, “I-TERM” indicates the
continuation of an aspect term, and “O” indicates
not an aspect term.
All sentences are tokenized and parsed using the
Stanford Parser1. The parsing information is used
</bodyText>
<footnote confidence="0.986854">
1http://nlp.stanford.edu/software/
lex-parser.shtml
</footnote>
<page confidence="0.934337">
235
</page>
<note confidence="0.7352075">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 235–240,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.996681333333333">
to extract various syntactic features (e.g. POS,
head word, dependency relation) described in the
next section.
</bodyText>
<subsectionHeader confidence="0.995484">
2.2 General (or Closed) Features
</subsectionHeader>
<bodyText confidence="0.9979812">
In this section, we describe the features commonly
used in traditional NER systems. Such features
can easily be extracted from the training set or
with the help of publicly available NLP tools (e.g.
Stanford Parser, NLTK, etc).
</bodyText>
<subsectionHeader confidence="0.936734">
2.2.1 Word
</subsectionHeader>
<bodyText confidence="0.9997498">
The string of the current token and its lowercase
format are used as features. To capture more con-
text information, we also extract the previous and
next word strings (in original format) as additional
word features.
</bodyText>
<subsectionHeader confidence="0.513874">
2.2.2 POS
</subsectionHeader>
<bodyText confidence="0.9999868">
The part-of-speech (POS) tag of the current token
is used as a feature. Since aspect terms are often
nouns, the POS tag provides useful information
about the lexical category of the word, especially
for unseen words in the test sentences.
</bodyText>
<subsectionHeader confidence="0.969481">
2.2.3 Head Word
</subsectionHeader>
<bodyText confidence="0.999938">
This feature represents the head word of the cur-
rent token. If the current token does not have a
head word, the value “null” is used.
</bodyText>
<subsectionHeader confidence="0.925895">
2.2.4 Head Word POS
</subsectionHeader>
<bodyText confidence="0.99996">
This feature represents the POS of the head word
of the current token. If the current token does not
have a head word, the value “null” is used.
</bodyText>
<subsectionHeader confidence="0.889662">
2.2.5 Dependency Relation
</subsectionHeader>
<bodyText confidence="0.99997625">
From the dependency parse, we identify the de-
pendency relations of the current token. We ex-
tract two different sets of strings: one set contains
the relation strings (e.g. “amod”, “nsubj”) where
the current token is the governor (i.e. head) of the
relation, the other set contains the relation strings
where the current token is the dependent of the re-
lation. For each set, we only keep certain rela-
tions: “amod”, “nsubj” and “dep” for the first set
and “nsubj”, “dobj” and “dep” for the second set.
Each set of strings is used as a feature value for the
current token, resulting in two separate features.
</bodyText>
<subsectionHeader confidence="0.790632">
2.2.6 Name List
</subsectionHeader>
<bodyText confidence="0.999718411764706">
Name lists (or gazetteers) have proven to be useful
in the task of NER (Ratinov and Roth, 2009). We
create a name list feature that uses the name lists
for membership testing.
For each domain, we extract two high precision
name lists from the training set. For the first list,
we collect and keep those aspect terms whose fre-
quency counts are greater than c1. Since an aspect
term can be multi-word, we also extract a second
list to consider the counts of individual words. All
words whose frequency counts greater than c2 are
collected. For each word, the probability of it be-
ing annotated as an aspect word in the training set
is calculated. Only those words whose probabil-
ity value is greater than t is kept in the second list.
The specified values of c1, c2 and t for each do-
main are determined using 5-fold cross validation.
</bodyText>
<subsectionHeader confidence="0.992065">
2.3 Open/External Sources Generated
Features
</subsectionHeader>
<bodyText confidence="0.999883333333333">
This section describes additional features we use
that require external resources and/or complex
processings.
</bodyText>
<subsectionHeader confidence="0.926952">
2.3.1 WordNet Taxonomy
</subsectionHeader>
<bodyText confidence="0.9999466">
This feature represents the set of syntactic cate-
gories (e.g “noun.food”) of the current token as
organized in WordNet lexicographer files (Miller,
1995). We only consider noun synsets of the token
when determining the syntactic categories.
</bodyText>
<subsectionHeader confidence="0.967967">
2.3.2 Word Cluster
</subsectionHeader>
<bodyText confidence="0.9998266875">
Turian et al. (2010) used unsupervised word rep-
resentations as extra word features to improve the
accuracy of both NER and chunking. We followed
their approach by inducing Brown clusters and K-
means clusters from in-domain unlabeled data.
We used the review text from two sources
of unlabeled dataset: the Multi-Domain Senti-
ment Dataset that contains Amazon product re-
views (Blitzer et al., 2007)2, and the Yelp Phoenix
Academic Dataset that contains user reviews3.
We induce 1000 Brown clusters for each
dataset4. For each word in the training/testing set,
its corresponding binary (prefix) string is used as
the feature value.
We experiment with different prefix lengths and
use the best settings using 5-fold cross validation.
</bodyText>
<footnote confidence="0.99447925">
2We used the unprocessed.tar.gz archive found
at http://www.cs.jhu.edu/˜mdredze/
datasets/sentiment/
3http://www.yelp.com/dataset_
challenge/
4Brown clustering are induced using the implementa-
tion by Percy Liang found at https://github.com/
percyliang/brown-cluster/.
</footnote>
<page confidence="0.997381">
236
</page>
<bodyText confidence="0.99994952173913">
For the laptop domain, we create a Brown cluster
feature from Amazon Brown clusters, using prefix
length of 5. For the restaurant domain, we cre-
ated three Brown cluster features: two from Yelp
Brown clusters, using prefix lengths of 4 and 8,
and the last one from Amazon Brown clusters, us-
ing prefix length of 10.
K-means clusters are induced using the
word2vec tool (Mikolov et al., 2013)5. Similar
to Brown cluster feature, the cluster id of each
word is used as the feature value.
When running the word2vec tool, we spe-
cially tune the values for word vector size (size),
cluster size (classes) and sub-sampling threshold
(sample) for optimum 5-fold cross validation per-
formances. We create one K-means cluster fea-
ture for the laptop domain from Amazon K-means
clusters (size = 100, classes = 400, sample =
0.0001), and two K-means cluster features for the
restaurant domain, one from Yelp K-means clus-
ters (size = 200, classes = 300, sample = 0.001),
and the other from Amazon K-means clusters
(size = 1000, classes = 300, sample = 0.0001).
</bodyText>
<sectionHeader confidence="0.617924" genericHeader="method">
2.3.3 Name List Generated using Double
Propagation
</sectionHeader>
<bodyText confidence="0.999687636363636">
We implement the Double Propagation (DP) algo-
rithm described in Qiu et al. (2011) to identify pos-
sible aspect terms in a semi-supervised way. The
terms identified are stored in a list which is used
as another name list feature.
Our implementation follow the Logic Program-
ming approach described in Liu et al. (2013)6. We
write our rules in Prolog and use SWI-Prolog7 as
the solver.
We use the seed opinion lexicon provided by Hu
and Liu (2004) for both domain8. In addition, for
the restaurant domain, we augment the opinion
lexicon with addition seed opinion words by us-
ing the 75 restaurant seed words listed in Sauper
and Barzilay (2013). To increase the coverage, we
expand this list of 75 words by including related
words (e.g. antonym, similar to) in WordNet. The
final expanded list contains 551 words.
Besides the seed opinion words, we also use the
last word of each aspect term in the training set as
a seed aspect word.
The propagation rules we use are modifications
</bodyText>
<footnote confidence="0.99421875">
5https://code.google.com/p/word2vec/
6We did not implement incorrect aspect pruning.
7http://www.swi-prolog.org/
8We ignore the polarity of the opinion word.
</footnote>
<bodyText confidence="0.999103">
of the rules presented in Liu et al. (2013). A total
of 11 rules and 13 rules are used for the laptop
and restaurant domain respectively. An example
of a Prolog rule concerning the extraction of aspect
words is stated below:
</bodyText>
<equation confidence="0.939117">
aspect(A) :-
relation(nsubj, O, A),
relation(cop, O, _),
pos(A, P),
is_noun(P),
opinion(O).
</equation>
<bodyText confidence="0.999964555555556">
For example, given the sentence “The rice is
amazing.”, and “amazing” is a known opinion
word, we can extract “rice” as a possible aspect
word using the rule.
All our rules can only identify individual words
as possible aspect terms. To consider a phrase as
a possible aspect term, we extend the left bound-
ary of the identified span to include any consective
noun words right before the identified word.
</bodyText>
<subsectionHeader confidence="0.984614">
2.4 Algorithms and Evaluation
</subsectionHeader>
<bodyText confidence="0.98377575">
We use the CRFsuite tool (Okazaki, 2007) to
train our CRF model. We use the default set-
tings, except for the negative state features (-p
feature.possible states=1).
</bodyText>
<table confidence="0.999564888888889">
Feature F1
Word 0.6641
+ Name List 0.7106
+ POS 0.7237
+ Head Word 0.7280
+ DP Name List 0.7298
+ Word Cluster 0.7430
+ Head Word POS 0.7437
+ Dependency Relation 0.7521
</table>
<tableCaption confidence="0.998003">
Table 1: 5-fold cross-validation performances on
</tableCaption>
<bodyText confidence="0.7917425">
the laptop domain. Each row uses all features
added in the previous rows.
</bodyText>
<subsectionHeader confidence="0.994712">
2.5 Preliminary Results on Training Set
</subsectionHeader>
<bodyText confidence="0.999589375">
Table 1 and Table 2 show the 5-fold cross-
validation performances after adding each feature
group for the laptop and restaurant domain respec-
tively. Most features are included in the optimum
feature set for both domains, except for Word-
Net Taxonomy feature (only used in the restaurant
domain) and Dependency Relation feature (only
used in the laptop domain).
</bodyText>
<page confidence="0.993651">
237
</page>
<table confidence="0.999622">
laptop restaurant
System Precision Recall F1 Precision Recall F1
DLIREC constrained 0.7931 0.6330 0.7041 (C) 0.8404 0.7337 0.7834 (C)
DLIREC unconstrained 0.8190 0.6713 0.7378 (U) 0.8535 0.8272 0.8401 (U)
Baseline 0.4432 0.2982 0.3565 (C) 0.5255 0.4277 0.4716 (C)
Ranked 1st 0.8480 0.6651 0.7455 (C) 0.8535 0.8272 0.8401 (U)
Ranked 2nd 0.8190 0.6713 0.7378 (U) 0.8625 0.8183 0.8398 (C)
Ranked 3rd 0.7931 0.6330 0.7041 (C) 0.8441 0.7637 0.8019 (C)
</table>
<tableCaption confidence="0.985309">
Table 3: Results of the Aspect Term Extraction subtask. We also indicate whether the system is con-
strained (C) or unconstrained (U).
</tableCaption>
<table confidence="0.999871">
Feature F1
Word 0.7541
+ Name List 0.7808
+ POS 0.7951
+ Head Word 0.7962
+ DP Name List 0.8036
+ Word Cluster 0.8224
+ WordNet Taxonomy 0.8252
+ Head Word POS 0.8274
</table>
<tableCaption confidence="0.978021">
Table 2: 5-fold cross-validation performances on
</tableCaption>
<bodyText confidence="0.99538675">
the restaurant domain. Each row uses all features
added in the previous rows.
For each domain, we make submissions in both
constrained and unconstrained settings. The con-
strained submission only uses the Word and Name
List features, while all features listed in Table 1
and Table 2 are used in the unconstrained submis-
sion for the respective domain.
</bodyText>
<subsectionHeader confidence="0.859129">
2.6 Results on Test Set
</subsectionHeader>
<bodyText confidence="0.987347217391304">
Using the optimum feature set described in Sec-
tion 2.5, we train separate models for each domain
and evaluate them against the SemEval-2014 Task
4 test set9. Table 3 presents the official results of
our submissions. We also include the official base-
line results and the results of the top three par-
ticipating systems for comparison (Pontiki et al.,
2014).
As shown from the table, our system performed
well for both domains. For the laptop domain, our
system is ranked 2nd and 3rd (among 27 submis-
sions) for the unconstrained and constrained set-
ting respectively. For the restaurant domain, our
system is ranked 1st and 9th (among 28 submis-
sions) for the unconstrained and constrained set-
9We train each model using only single-domain data.
ting respectively.
Our unconstrained submissions for both do-
mains outperformed our constrained submissions,
due to a significantly better recall. This indicates
the use of additional external resources (e.g. un-
labeled data) can improve the extraction perfor-
mance.
</bodyText>
<subsectionHeader confidence="0.985359">
2.7 Further Analysis of Feature Engineering
</subsectionHeader>
<bodyText confidence="0.99723525">
Table 4 shows the F1 loss on the test set resulting
from training with each group of feature removed.
We also include the F1 loss when all features are
used.
</bodyText>
<table confidence="0.998529909090909">
Feature laptop restaurant
Word 0.0260 0.0241
Name List 0.0090 0.0054
POS -0.0059 -0.0052
Head Word 0.0072 0.0038
DP Name List 0.0049 0.0064
Word Cluster 0.0061 0.0185
WordNet Taxonomy - -0.0018
Head Word POS -0.0040 -0.0011
Dependency Relation -0.0105 -
All features -0.0132 0.0014
</table>
<tableCaption confidence="0.991113">
Table 4: Feature ablation study on the test set. The
</tableCaption>
<bodyText confidence="0.882846181818182">
quantity is the F1 loss resulted from the removal of
a single feature group. The last row indicates the
F1 loss when all features are used.
Our ablation study showed that a few of our fea-
tures are helpful in varying degrees on both do-
mains: Word, Name List, Head Word, DP Name
List and Word Cluster. However, the use of the
rest of the features individually has a negative im-
pact. In particular, we are surprised that the POS
and Dependency Relation features are detrimen-
tal to the performances, even though our 5-fold
</bodyText>
<page confidence="0.993838">
238
</page>
<bodyText confidence="0.999957294117647">
cross validation experiments suggested otherwise.
Another observation we make is that the Word-
Net Taxonomy feature is actually useful for the
laptop test set: including this feature would have
improved our laptop unconstrained performance
from 0.7378 F1 to 0.7510 F1 (+0.0132), which is
better than the top system performance. We also
note that our restaurant performance on the test
set can potentially be improved from 0.8401 F1
to 0.8454 F1 (+0.0052) if we originally omit the
POS feature.
Overall, we see that all the features we pro-
posed are potentially beneficial to the task. How-
ever, more thorough feature selection experiments
should be conducted to prevent overfitting and to
identify the settings (e.g. domain) in which each
feature may be useful.
</bodyText>
<sectionHeader confidence="0.965549" genericHeader="method">
3 Aspect Term Polarity
</sectionHeader>
<bodyText confidence="0.999920727272727">
In this section, we describe a baseline classifier for
ATP, where we treat the problem as a multi-class
classification problem.
To correctly identify the polarity of an aspect
term, it is crucial to know which words within the
sentence indicate its sentiment. A general lexicon
or WordNet is not sufficient. Thus, we attempt to
build the aspect lexicon based on other informa-
tion such as POS (Sauper and Barzilay, 2013). For
example, sentiment words are more likely to be
adjectives.
</bodyText>
<subsectionHeader confidence="0.9706265">
3.1 Features
3.1.1 Aspect Word
</subsectionHeader>
<bodyText confidence="0.999987833333333">
This is to model the idea that certain aspects tend
to have a particular polarity most of the time. We
compute the most frequent polarity of each aspect
in the training set. For each aspect instance, the
feature corresponding to its most frequent polarity
is set to 1.
</bodyText>
<subsectionHeader confidence="0.935686">
3.1.2 General Sentiment Word Lexicon
</subsectionHeader>
<bodyText confidence="0.999420666666667">
One sentence may express opinions on multi-
ple aspect terms. According to our observations,
words surrounding the aspect term tend to be asso-
ciated with it. Based on the best settings obtained
from 5-fold cross validation, we set a window size
of 12 words and consider words with the following
POS: JJ*, RB*, VB*, DT and NN*10.
Some sentiment words are consistent across as-
pects. For example, “great” for positive and “ter-
</bodyText>
<footnote confidence="0.706604">
10NN* is only used in the restaurant domain.
</footnote>
<bodyText confidence="0.999918176470588">
rible” for negative. On the other hand, some senti-
ment words are quite distinct between aspects. In
certain cases, they may have opposite sentiment
meanings for different aspects (Kim et al., 2013).
For example, “fast” is positive when describing
boot up speed but negative when describing bat-
tery life. Therefore, a general sentiment word lex-
icon is created from the training set.
If a general sentiment word occurs in the sur-
rounding context of the aspect instance, the fea-
ture value for the matched sentiment word is 1.
Since the training set does not contain every pos-
sible sentiment expression, we use synonyms and
antonyms in RiTa WordNet11 to expand the gen-
eral sentiment word lexicon. The expanded lex-
icon contains 2419 words for the laptop domain
and 4262 words for the restaurant domain.
</bodyText>
<subsectionHeader confidence="0.978446">
3.1.3 Aspect-Sentiment Word Pair
</subsectionHeader>
<bodyText confidence="0.99990925">
Besides general sentiment word lexicon, we also
build aspect-sentiment word pair lexicon from the
training set. This lexicon contains 9073 word pairs
for the laptop domain and 22171 word pairs for the
restaurant domain. If an aspect-sentiment word
occurs in the surrounding context of the aspect in-
stance, the feature value for the matched aspect-
sentiment word pair is 1.
</bodyText>
<subsectionHeader confidence="0.991465">
3.2 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.9994935">
We use LIBLINEAR12 to train our logistic regres-
sion classifier using default settings.
</bodyText>
<table confidence="0.931727666666667">
laptop restaurant
5-fold cross validation 0.6322 0.6704
DLIREC unconstrained 0.3654 0.4233
</table>
<tableCaption confidence="0.8533135">
Table 5: Accuracy of the Aspect Term Polarity
subtask.
</tableCaption>
<bodyText confidence="0.9915038">
Table 5 shows the classification accuracy of our
baseline system on the training and test set for
each domain. The performance drops a lot in the
test set as we use very simple approaches to gener-
ate the lexicons. This may cause overfitting on the
training set. We also observe that in the test set of
both domains, more than half of the instances are
positive. In the future, we can explore on using
more sophisticated ways to build more effective
features and to better model data skewness.
</bodyText>
<footnote confidence="0.998291333333333">
11http://www.rednoise.org/rita/wordnet/
12http://www.csie.ntu.edu.tw/˜cjlin/
liblinear/
</footnote>
<page confidence="0.997504">
239
</page>
<sectionHeader confidence="0.998959" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999967466666667">
For ATE subtask, we leverage on the vast amount
of external resources to create additional effective
features, which contribute significantly to the im-
provement of our system. For the unconstrained
setting, our system is ranked 1st (among 28 sub-
missions) and 2rd (among 27 submissions) for the
restaurant and laptop domain respectively. For
ATP subtask, we implement a simple baseline sys-
tem.
Our current work focus on implementing a sep-
arate term extraction system for each domain. In
future, we hope to investigate on domain adapta-
tion methods across different domains. In addi-
tion, we will also address the feature sparseness
problem in our ATP baseline system.
</bodyText>
<sectionHeader confidence="0.997057" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9231295">
This research work is supported by a research
project under Baidu-I2R Research Centre.
</bodyText>
<sectionHeader confidence="0.997871" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999909921875">
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes and
Blenders: Domain Adaptation for Sentiment Clas-
sification. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 440–447, Prague, Czech Republic, June.
Minqing Hu and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’04,
pages 168–177, New York, NY, USA.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
Opinion Targets in a Single and Cross-Domain Set-
ting with Conditional Random Fields. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1035–1045,
Cambridge, MA, October.
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and
Shixia Liu. 2013. A Hierarchical Aspect-Sentiment
Model for Online Reviews. In AAAI Conference on
Artificial Intelligence.
Qian Liu, Zhiqiang Gao, Bing Liu, and Yuanlin Zhang.
2013. A Logic Programming Approach to As-
pect Extraction in Opinion Mining. In Web Intelli-
gence (WI) and IntelligentAgent Technologies (IAT),
2013 IEEE/WIC/ACM International Joint Confer-
ences on, volume 1, pages 276–283, Nov.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751, Atlanta,
Georgia, June.
George A. Miller. 1995. WordNet: A Lexical
Database for English. Commun. ACM, 38(11):39–
41, November.
Naoaki Okazaki. 2007. CRFsuite: a fast implementa-
tion of Conditional Random Fields (CRFs).
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect Based Sentiment Analysis. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval 2014), Dublin, Ireland.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion Word Expansion and Target Extrac-
tion through Double Propagation. Computational
Linguistics, 37(1):9–27.
Lev Ratinov and Dan Roth. 2009. Design Chal-
lenges and Misconceptions in Named Entity Recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147–155, Boulder, Colorado,
June.
Christina Sauper and Regina Barzilay. 2013. Auto-
matic Aggregation by Joint Modeling of Aspects and
Values. J. Artif. Int. Res., 46(1):89–127, January.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384–394, Up-
psala, Sweden, July.
</reference>
<page confidence="0.997029">
240
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.354993">
<title confidence="0.9928045">DLIREC: Aspect Term Extraction and Term Polarity Classification System</title>
<author confidence="0.735963">Zhiqiang</author>
<affiliation confidence="0.613774">Institute for Infocomm 1 Fusionopolis</affiliation>
<address confidence="0.617791">Singapore</address>
<email confidence="0.916291">ztoh@i2r.a-star.edu.sg</email>
<abstract confidence="0.992526333333334">This paper describes our system used in the Aspect Based Sentiment Analysis Task 4 at the SemEval-2014. Our system consists of two components to address two of the subtasks respectively: a Conditional Random Field (CRF) based classifier for Aspect Term Extraction (ATE) and a linear classifier for Aspect Term Polarity Classification (ATP). For the ATE subtask, we implement a variety of lexicon, syntactic and semantic features, as well as cluster features induced from unlabeled data. Our system achieves state-of-the-art performances in ATE, ranking 1st (among 28 submissions) and 2rd (among 27 submissions) for the restaurant and laptop domain respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7087" citStr="Blitzer et al., 2007" startWordPosition="1140" endWordPosition="1143"> syntactic categories (e.g “noun.food”) of the current token as organized in WordNet lexicographer files (Miller, 1995). We only consider noun synsets of the token when determining the syntactic categories. 2.3.2 Word Cluster Turian et al. (2010) used unsupervised word representations as extra word features to improve the accuracy of both NER and chunking. We followed their approach by inducing Brown clusters and Kmeans clusters from in-domain unlabeled data. We used the review text from two sources of unlabeled dataset: the Multi-Domain Sentiment Dataset that contains Amazon product reviews (Blitzer et al., 2007)2, and the Yelp Phoenix Academic Dataset that contains user reviews3. We induce 1000 Brown clusters for each dataset4. For each word in the training/testing set, its corresponding binary (prefix) string is used as the feature value. We experiment with different prefix lengths and use the best settings using 5-fold cross validation. 2We used the unprocessed.tar.gz archive found at http://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 3http://www.yelp.com/dataset_ challenge/ 4Brown clustering are induced using the implementation by Percy Liang found at https://github.com/ percyliang/brown-cluster/</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and Summarizing Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="9231" citStr="Hu and Liu (2004)" startWordPosition="1482" endWordPosition="1485"> classes = 300, sample = 0.001), and the other from Amazon K-means clusters (size = 1000, classes = 300, sample = 0.0001). 2.3.3 Name List Generated using Double Propagation We implement the Double Propagation (DP) algorithm described in Qiu et al. (2011) to identify possible aspect terms in a semi-supervised way. The terms identified are stored in a list which is used as another name list feature. Our implementation follow the Logic Programming approach described in Liu et al. (2013)6. We write our rules in Prolog and use SWI-Prolog7 as the solver. We use the seed opinion lexicon provided by Hu and Liu (2004) for both domain8. In addition, for the restaurant domain, we augment the opinion lexicon with addition seed opinion words by using the 75 restaurant seed words listed in Sauper and Barzilay (2013). To increase the coverage, we expand this list of 75 words by including related words (e.g. antonym, similar to) in WordNet. The final expanded list contains 551 words. Besides the seed opinion words, we also use the last word of each aspect term in the training set as a seed aspect word. The propagation rules we use are modifications 5https://code.google.com/p/word2vec/ 6We did not implement incorr</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and Summarizing Customer Reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niklas Jakob</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1035--1045</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="1345" citStr="Jakob and Gurevych (2010)" startWordPosition="204" endWordPosition="207"> in ATE, ranking 1st (among 28 submissions) and 2rd (among 27 submissions) for the restaurant and laptop domain respectively. 1 Introduction Sentiment analysis on document and sentence level no longer fulfills user’s needs of getting more accurate and precise information. By performing sentiment analysis at the aspect level, we can help users gain more insights on the sentiments of the various aspects of the target entity. Task 4 of SemEval-2014 provides a good platform for (1) aspect term extraction and (2) aspect term polarity classification. For the first subtask, we follow the approach of Jakob and Gurevych (2010) by modeling term extraction as a sequential labeling task. Specifically, we leverage on semantic and syntactic resources to extract a variety of features and use CRF as the learning algorithm. For the second subtask, we This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/ by/4.0/ Wenting Wang magicwwt@gmail.com simply treat it as a multi-class classification problem where a linear classifier is learned to predict the polarity class. Our system</context>
</contexts>
<marker>Jakob, Gurevych, 2010</marker>
<rawString>Niklas Jakob and Iryna Gurevych. 2010. Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1035–1045, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suin Kim</author>
<author>Jianwen Zhang</author>
<author>Zheng Chen</author>
<author>Alice Oh</author>
<author>Shixia Liu</author>
</authors>
<title>A Hierarchical Aspect-Sentiment Model for Online Reviews.</title>
<date>2013</date>
<booktitle>In AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="17036" citStr="Kim et al., 2013" startWordPosition="2774" endWordPosition="2777">aspect terms. According to our observations, words surrounding the aspect term tend to be associated with it. Based on the best settings obtained from 5-fold cross validation, we set a window size of 12 words and consider words with the following POS: JJ*, RB*, VB*, DT and NN*10. Some sentiment words are consistent across aspects. For example, “great” for positive and “ter10NN* is only used in the restaurant domain. rible” for negative. On the other hand, some sentiment words are quite distinct between aspects. In certain cases, they may have opposite sentiment meanings for different aspects (Kim et al., 2013). For example, “fast” is positive when describing boot up speed but negative when describing battery life. Therefore, a general sentiment word lexicon is created from the training set. If a general sentiment word occurs in the surrounding context of the aspect instance, the feature value for the matched sentiment word is 1. Since the training set does not contain every possible sentiment expression, we use synonyms and antonyms in RiTa WordNet11 to expand the general sentiment word lexicon. The expanded lexicon contains 2419 words for the laptop domain and 4262 words for the restaurant domain.</context>
</contexts>
<marker>Kim, Zhang, Chen, Oh, Liu, 2013</marker>
<rawString>Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and Shixia Liu. 2013. A Hierarchical Aspect-Sentiment Model for Online Reviews. In AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qian Liu</author>
<author>Zhiqiang Gao</author>
<author>Bing Liu</author>
<author>Yuanlin Zhang</author>
</authors>
<title>A Logic Programming Approach to Aspect Extraction in Opinion Mining.</title>
<date>2013</date>
<booktitle>In Web Intelligence (WI) and IntelligentAgent Technologies (IAT), 2013 IEEE/WIC/ACM International Joint Conferences on,</booktitle>
<volume>1</volume>
<pages>276--283</pages>
<contexts>
<context position="9103" citStr="Liu et al. (2013)" startWordPosition="1458" endWordPosition="1461">= 400, sample = 0.0001), and two K-means cluster features for the restaurant domain, one from Yelp K-means clusters (size = 200, classes = 300, sample = 0.001), and the other from Amazon K-means clusters (size = 1000, classes = 300, sample = 0.0001). 2.3.3 Name List Generated using Double Propagation We implement the Double Propagation (DP) algorithm described in Qiu et al. (2011) to identify possible aspect terms in a semi-supervised way. The terms identified are stored in a list which is used as another name list feature. Our implementation follow the Logic Programming approach described in Liu et al. (2013)6. We write our rules in Prolog and use SWI-Prolog7 as the solver. We use the seed opinion lexicon provided by Hu and Liu (2004) for both domain8. In addition, for the restaurant domain, we augment the opinion lexicon with addition seed opinion words by using the 75 restaurant seed words listed in Sauper and Barzilay (2013). To increase the coverage, we expand this list of 75 words by including related words (e.g. antonym, similar to) in WordNet. The final expanded list contains 551 words. Besides the seed opinion words, we also use the last word of each aspect term in the training set as a se</context>
</contexts>
<marker>Liu, Gao, Liu, Zhang, 2013</marker>
<rawString>Qian Liu, Zhiqiang Gao, Bing Liu, and Yuanlin Zhang. 2013. A Logic Programming Approach to Aspect Extraction in Opinion Mining. In Web Intelligence (WI) and IntelligentAgent Technologies (IAT), 2013 IEEE/WIC/ACM International Joint Conferences on, volume 1, pages 276–283, Nov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous Space Word Representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>746--751</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="8082" citStr="Mikolov et al., 2013" startWordPosition="1286" endWordPosition="1289">found at http://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 3http://www.yelp.com/dataset_ challenge/ 4Brown clustering are induced using the implementation by Percy Liang found at https://github.com/ percyliang/brown-cluster/. 236 For the laptop domain, we create a Brown cluster feature from Amazon Brown clusters, using prefix length of 5. For the restaurant domain, we created three Brown cluster features: two from Yelp Brown clusters, using prefix lengths of 4 and 8, and the last one from Amazon Brown clusters, using prefix length of 10. K-means clusters are induced using the word2vec tool (Mikolov et al., 2013)5. Similar to Brown cluster feature, the cluster id of each word is used as the feature value. When running the word2vec tool, we specially tune the values for word vector size (size), cluster size (classes) and sub-sampling threshold (sample) for optimum 5-fold cross validation performances. We create one K-means cluster feature for the laptop domain from Amazon K-means clusters (size = 100, classes = 400, sample = 0.0001), and two K-means cluster features for the restaurant domain, one from Yelp K-means clusters (size = 200, classes = 300, sample = 0.001), and the other from Amazon K-means c</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>41</pages>
<contexts>
<context position="6585" citStr="Miller, 1995" startWordPosition="1062" endWordPosition="1063">ord, the probability of it being annotated as an aspect word in the training set is calculated. Only those words whose probability value is greater than t is kept in the second list. The specified values of c1, c2 and t for each domain are determined using 5-fold cross validation. 2.3 Open/External Sources Generated Features This section describes additional features we use that require external resources and/or complex processings. 2.3.1 WordNet Taxonomy This feature represents the set of syntactic categories (e.g “noun.food”) of the current token as organized in WordNet lexicographer files (Miller, 1995). We only consider noun synsets of the token when determining the syntactic categories. 2.3.2 Word Cluster Turian et al. (2010) used unsupervised word representations as extra word features to improve the accuracy of both NER and chunking. We followed their approach by inducing Brown clusters and Kmeans clusters from in-domain unlabeled data. We used the review text from two sources of unlabeled dataset: the Multi-Domain Sentiment Dataset that contains Amazon product reviews (Blitzer et al., 2007)2, and the Yelp Phoenix Academic Dataset that contains user reviews3. We induce 1000 Brown cluster</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A Lexical Database for English. Commun. ACM, 38(11):39– 41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of Conditional Random Fields (CRFs).</title>
<date>2007</date>
<contexts>
<context position="10715" citStr="Okazaki, 2007" startWordPosition="1727" endWordPosition="1728">the extraction of aspect words is stated below: aspect(A) :- relation(nsubj, O, A), relation(cop, O, _), pos(A, P), is_noun(P), opinion(O). For example, given the sentence “The rice is amazing.”, and “amazing” is a known opinion word, we can extract “rice” as a possible aspect word using the rule. All our rules can only identify individual words as possible aspect terms. To consider a phrase as a possible aspect term, we extend the left boundary of the identified span to include any consective noun words right before the identified word. 2.4 Algorithms and Evaluation We use the CRFsuite tool (Okazaki, 2007) to train our CRF model. We use the default settings, except for the negative state features (-p feature.possible states=1). Feature F1 Word 0.6641 + Name List 0.7106 + POS 0.7237 + Head Word 0.7280 + DP Name List 0.7298 + Word Cluster 0.7430 + Head Word POS 0.7437 + Dependency Relation 0.7521 Table 1: 5-fold cross-validation performances on the laptop domain. Each row uses all features added in the previous rows. 2.5 Preliminary Results on Training Set Table 1 and Table 2 show the 5-fold crossvalidation performances after adding each feature group for the laptop and restaurant domain respecti</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>Naoaki Okazaki. 2007. CRFsuite: a fast implementation of Conditional Random Fields (CRFs).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitrios Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>SemEval-2014 Task 4: Aspect Based Sentiment Analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="13060" citStr="Pontiki et al., 2014" startWordPosition="2114" endWordPosition="2117">issions in both constrained and unconstrained settings. The constrained submission only uses the Word and Name List features, while all features listed in Table 1 and Table 2 are used in the unconstrained submission for the respective domain. 2.6 Results on Test Set Using the optimum feature set described in Section 2.5, we train separate models for each domain and evaluate them against the SemEval-2014 Task 4 test set9. Table 3 presents the official results of our submissions. We also include the official baseline results and the results of the top three participating systems for comparison (Pontiki et al., 2014). As shown from the table, our system performed well for both domains. For the laptop domain, our system is ranked 2nd and 3rd (among 27 submissions) for the unconstrained and constrained setting respectively. For the restaurant domain, our system is ranked 1st and 9th (among 28 submissions) for the unconstrained and constrained set9We train each model using only single-domain data. ting respectively. Our unconstrained submissions for both domains outperformed our constrained submissions, due to a significantly better recall. This indicates the use of additional external resources (e.g. unlabe</context>
</contexts>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. SemEval-2014 Task 4: Aspect Based Sentiment Analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Opinion Word Expansion and Target Extraction through Double Propagation.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="8869" citStr="Qiu et al. (2011)" startWordPosition="1418" endWordPosition="1421">r size (size), cluster size (classes) and sub-sampling threshold (sample) for optimum 5-fold cross validation performances. We create one K-means cluster feature for the laptop domain from Amazon K-means clusters (size = 100, classes = 400, sample = 0.0001), and two K-means cluster features for the restaurant domain, one from Yelp K-means clusters (size = 200, classes = 300, sample = 0.001), and the other from Amazon K-means clusters (size = 1000, classes = 300, sample = 0.0001). 2.3.3 Name List Generated using Double Propagation We implement the Double Propagation (DP) algorithm described in Qiu et al. (2011) to identify possible aspect terms in a semi-supervised way. The terms identified are stored in a list which is used as another name list feature. Our implementation follow the Logic Programming approach described in Liu et al. (2013)6. We write our rules in Prolog and use SWI-Prolog7 as the solver. We use the seed opinion lexicon provided by Hu and Liu (2004) for both domain8. In addition, for the restaurant domain, we augment the opinion lexicon with addition seed opinion words by using the 75 restaurant seed words listed in Sauper and Barzilay (2013). To increase the coverage, we expand thi</context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2011</marker>
<rawString>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion Word Expansion and Target Extraction through Double Propagation. Computational Linguistics, 37(1):9–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design Challenges and Misconceptions in Named Entity Recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>147--155</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="5519" citStr="Ratinov and Roth, 2009" startWordPosition="883" endWordPosition="886">ct two different sets of strings: one set contains the relation strings (e.g. “amod”, “nsubj”) where the current token is the governor (i.e. head) of the relation, the other set contains the relation strings where the current token is the dependent of the relation. For each set, we only keep certain relations: “amod”, “nsubj” and “dep” for the first set and “nsubj”, “dobj” and “dep” for the second set. Each set of strings is used as a feature value for the current token, resulting in two separate features. 2.2.6 Name List Name lists (or gazetteers) have proven to be useful in the task of NER (Ratinov and Roth, 2009). We create a name list feature that uses the name lists for membership testing. For each domain, we extract two high precision name lists from the training set. For the first list, we collect and keep those aspect terms whose frequency counts are greater than c1. Since an aspect term can be multi-word, we also extract a second list to consider the counts of individual words. All words whose frequency counts greater than c2 are collected. For each word, the probability of it being annotated as an aspect word in the training set is calculated. Only those words whose probability value is greater</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design Challenges and Misconceptions in Named Entity Recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), pages 147–155, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Sauper</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic Aggregation by Joint Modeling of Aspects and Values.</title>
<date>2013</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="9428" citStr="Sauper and Barzilay (2013)" startWordPosition="1515" endWordPosition="1518">he Double Propagation (DP) algorithm described in Qiu et al. (2011) to identify possible aspect terms in a semi-supervised way. The terms identified are stored in a list which is used as another name list feature. Our implementation follow the Logic Programming approach described in Liu et al. (2013)6. We write our rules in Prolog and use SWI-Prolog7 as the solver. We use the seed opinion lexicon provided by Hu and Liu (2004) for both domain8. In addition, for the restaurant domain, we augment the opinion lexicon with addition seed opinion words by using the 75 restaurant seed words listed in Sauper and Barzilay (2013). To increase the coverage, we expand this list of 75 words by including related words (e.g. antonym, similar to) in WordNet. The final expanded list contains 551 words. Besides the seed opinion words, we also use the last word of each aspect term in the training set as a seed aspect word. The propagation rules we use are modifications 5https://code.google.com/p/word2vec/ 6We did not implement incorrect aspect pruning. 7http://www.swi-prolog.org/ 8We ignore the polarity of the opinion word. of the rules presented in Liu et al. (2013). A total of 11 rules and 13 rules are used for the laptop an</context>
<context position="15971" citStr="Sauper and Barzilay, 2013" startWordPosition="2594" endWordPosition="2597">e task. However, more thorough feature selection experiments should be conducted to prevent overfitting and to identify the settings (e.g. domain) in which each feature may be useful. 3 Aspect Term Polarity In this section, we describe a baseline classifier for ATP, where we treat the problem as a multi-class classification problem. To correctly identify the polarity of an aspect term, it is crucial to know which words within the sentence indicate its sentiment. A general lexicon or WordNet is not sufficient. Thus, we attempt to build the aspect lexicon based on other information such as POS (Sauper and Barzilay, 2013). For example, sentiment words are more likely to be adjectives. 3.1 Features 3.1.1 Aspect Word This is to model the idea that certain aspects tend to have a particular polarity most of the time. We compute the most frequent polarity of each aspect in the training set. For each aspect instance, the feature corresponding to its most frequent polarity is set to 1. 3.1.2 General Sentiment Word Lexicon One sentence may express opinions on multiple aspect terms. According to our observations, words surrounding the aspect term tend to be associated with it. Based on the best settings obtained from 5</context>
</contexts>
<marker>Sauper, Barzilay, 2013</marker>
<rawString>Christina Sauper and Regina Barzilay. 2013. Automatic Aggregation by Joint Modeling of Aspects and Values. J. Artif. Int. Res., 46(1):89–127, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word Representations: A Simple and General Method for Semi-Supervised Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6712" citStr="Turian et al. (2010)" startWordPosition="1080" endWordPosition="1083">robability value is greater than t is kept in the second list. The specified values of c1, c2 and t for each domain are determined using 5-fold cross validation. 2.3 Open/External Sources Generated Features This section describes additional features we use that require external resources and/or complex processings. 2.3.1 WordNet Taxonomy This feature represents the set of syntactic categories (e.g “noun.food”) of the current token as organized in WordNet lexicographer files (Miller, 1995). We only consider noun synsets of the token when determining the syntactic categories. 2.3.2 Word Cluster Turian et al. (2010) used unsupervised word representations as extra word features to improve the accuracy of both NER and chunking. We followed their approach by inducing Brown clusters and Kmeans clusters from in-domain unlabeled data. We used the review text from two sources of unlabeled dataset: the Multi-Domain Sentiment Dataset that contains Amazon product reviews (Blitzer et al., 2007)2, and the Yelp Phoenix Academic Dataset that contains user reviews3. We induce 1000 Brown clusters for each dataset4. For each word in the training/testing set, its corresponding binary (prefix) string is used as the feature</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word Representations: A Simple and General Method for Semi-Supervised Learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>