<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005505">
<title confidence="0.980863">
QAST: Question Answering System for Thai Wikipedia
</title>
<author confidence="0.995705">
Wittawat Jitkrittum† Choochart Haruechaiyasak‡ Thanaruk Theeramunkong†
</author>
<affiliation confidence="0.997943">
†School of Information, Computer and Communication Technology (ICT)
Sirindhorn International Institute of Technology (SIIT)
</affiliation>
<address confidence="0.987855">
131 Moo 5 Tiwanont Rd., Bangkadi, Muang, Phathumthani, Thailand, 12000
</address>
<email confidence="0.995656">
wittawatj@gmail.com, thanaruk@siit.tu.ac.th
</email>
<affiliation confidence="0.864316">
‡Human Language Technology Laboratory (HLT)
National Electronics and Computer Technology Center (NECTEC)
Thailand Science Park, Klong Luang, Pathumthani 12120, Thailand
</affiliation>
<email confidence="0.99453">
choochart.haruechaiyasak@nectec.or.th
</email>
<sectionHeader confidence="0.99557" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999560277777778">
We propose an open-domain question an-
swering system using Thai Wikipedia as
the knowledge base. Two types of in-
formation are used for answering a ques-
tion: (1) structured information extracted
and stored in the form of Resource De-
scription Framework (RDF), and (2) un-
structured texts stored as a search index.
For the structured information, SPARQL
transformed query is applied to retrieve a
short answer from the RDF base. For the
unstructured information, keyword-based
query is used to retrieve the shortest text
span containing the questions’s key terms.
From the experimental results, the system
which integrates both approaches could
achieve an average MRR of 0.47 based on
215 test questions.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999179">
Most keyword-based search engines available on-
line do not support the retrieval of precise infor-
mation. They only return a list of URLs, each re-
ferring to a web page, sorted by relevancy to the
user’s query. Users then have to manually scan
those documents for needed information. Due to
this limitation, many techniques for implement-
ing QA systems have been proposed in the past
decades.
From the literature reviews, previous and exist-
ing QA systems can be broadly categorized into
two types:
</bodyText>
<listItem confidence="0.536703">
1. Knowledge Intensive: Knowledge intensive
</listItem>
<bodyText confidence="0.983063515151515">
systems focus on analyzing and understand-
ing the input questions. The system knows
exactly what to be answered, and also what
type the answer should be. The analysis
phase usually depends on an ontology or a
semantic lexicon like WordNet. The an-
swer is retrieved from a predefined organized
knowledge base. Natural Language Process-
ing (NLP) techniques are heavily used in a
knowledge intensive system.
2. Data Intensive: Data intensive systems,
which do not fully analyze the input ques-
tions, rely on the redundancy of huge amount
of data (Dumais et al., 2002). The idea is that
if we have a huge amount of data, a piece of
information is likely to be stated more than
once in different forms. As a result, the data-
intensive QA systems are not required to per-
form many complex NLP techniques.
In this paper, we propose an open-domain QA
system for Thai Wikipedia called QAST. The sys-
tem supports five types of close-ended questions:
person, organization, place, quantity, and date/-
time. Our system can be classified as a data in-
tensive type with an additional support of struc-
tured information. Structured information in Thai
Wikipedia is extracted and represented in the form
of RDF. We use SPARQL to retrieve specific in-
formation from the RDF base. If using SPARQL
cannot answer a given question, the system will re-
trieve answer candidates from the pre-constructed
search index using a technique based on Minimal
Span Weighting (Monz, 2003).
</bodyText>
<sectionHeader confidence="0.945967" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.829063333333333">
Figure 1 shows the system architecture of QAST
which consists of three main sub-systems: Data
Representation, Question Processor, and Answer
</bodyText>
<page confidence="0.993873">
11
</page>
<note confidence="0.9971135">
Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP 2009, pages 11–14,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.999913">
Figure 1: The system architecture of QAST
</figureCaption>
<bodyText confidence="0.517498">
Processor.
</bodyText>
<subsectionHeader confidence="0.992837">
2.1 Data Representation
</subsectionHeader>
<bodyText confidence="0.999912">
The Data Representation part is a storage for all
information contained in Thai Wikipedia. Two
modules constitute this sub-system.
RDF Base: In QAST, RDF triples are generated
from Wikipedia’s infoboxes following similar ap-
proaches described in the works of Isbell and But-
ler (2007) and Auer and Lehmann (2007). To gen-
erate RDF triples from an infobox, we would have
the article title as the subject. The predicates are
the keys in the first column. The objects are the
values in the second column. Altogether, the num-
ber of generated triples corresponds to the number
of rows in the infobox.
In addition to the infoboxes, we also store syn-
onyms in the form of RDF triples. The synonyms
are extracted from redirect pages in Wikipedia.
For example, a request for the Wikipedia article
titled “Car” will result in another article titled “Au-
tomobile” to be shown up. The former page usu-
ally has no content and only acts as a pointer to
another page which contains the full content. The
relationship of these two pages implies that “Car”
and “Automobile” are synonymous. Synonyms
are useful in retrieving the same piece of informa-
tion with different texual expressions.
Search Index: QAST stores the textual content
as a search index. We used the well-known IR li-
brary, Lucene1, for our search backend. We in-
dexed 41,512 articles (as of February 5, 2009)
from a Thai Wikipedia dump with full term posi-
tions. Firstly, all template constructs and the Wiki-
Text markups are removed, leaving with only the
plain texts. A dictionary-based longest-matching
word segmentation is then performed to tokenize
the plain texts into series of terms. Finally, the
resulted list of non-stopwords are passed to the
Lucene indexing engine. The dictionary used for
word segmentation is a combination of word list
from the LEXiTRON2 and all article titles from
Thai Wikipedia. In total, there are 81,345 words
in the dictionary.
</bodyText>
<subsectionHeader confidence="0.998179">
2.2 Question Processor
</subsectionHeader>
<bodyText confidence="0.995936">
Question processor sub-system consists of four
modules as follows.
</bodyText>
<listItem confidence="0.980599869565217">
1. Question Normalizer – This first module is
to change the way the question is formed into
a normal form to ease the processing at lat-
ter stages. This includes correcting mistyped
words or unusual spelling such as f33t for
feet.
2. Word Segmenter – This module performs
tokenizing on the normalized question to ob-
tain a list of non-stopwords.
3. Question Analyzer – The question ana-
lyzer determines the expected type of answer
(i.e., quantity, person, organization, location,
date/time and unknown) and constructs an
appropriate query. Normally, a SPARQL
query is generated and used to retrieve a can-
didate answer from the RDF base. When the
SPARQL fails to find an answer, the system
will switch to the index search. In that case,
the module also defines a set of hint terms to
help in locating candidate answers.
4. Searcher – This module executes the query
and retrieves candidate answers from the data
representation part.
</listItem>
<bodyText confidence="0.999488">
To generate a SPARQL query, the input ques-
tion is compared against a set of predefined regu-
lar expression patterns. Currently, the system has
two types of patterns: pattern for definitional ques-
tions, and pattern for questions asking for a prop-
</bodyText>
<footnote confidence="0.9998925">
1Apache Lucene, http://lucene.apache.org
2LEXiTRON,http://lexitron.nectec.or.th
</footnote>
<page confidence="0.997314">
12
</page>
<bodyText confidence="0.9999186">
erty of an entity. The pattern for definitional ques-
tion is of the form a-rai-kue-X ‘What is X ?’ or
X-kue-a-rai ‘X is what ?’. After X is determined
from a user’s question, the first paragraph of the
article titled X is retrieved and directly returned to
the user. Since the first paragraph in any article is
usually the summary, it is appropriate to use the
first paragraph to answer a definitional question.
Questions asking for a property of an entity are
of the form a-rai-kue-P-kong-X ‘What is P of X ?’
e.g., “When was SIIT established ?” which can be
answered by looking for the right information in
the RDF base. A simplified SPARQL query used
to retrieve an answer for this type of question is as
follows.
</bodyText>
<equation confidence="0.89678225">
✞
SELECT ?o
WHERE {
?tempPage hasInfobox ?tempBox .
?tempPage rdfs : label ”X” .
?tempBox ?P ?o .
}
✡✝ ✆
</equation>
<bodyText confidence="0.9953554">
The query matches an object of a RDF triple with
the predicate P (e.g., “date of establishment”), pro-
vided that the triple is generated from an infobox
titled X (e.g.,“SIIT”) . The object of the year 1992
is then correctly returned as the answer.
When SPARQL fails, i.e., the question does
not match any known pattern or the answer does
not exist in the RDF base, the system switches to
the index search which performs the following the
steps.
</bodyText>
<listItem confidence="0.991487722222222">
1. Word Segmenter tokenizes the question into
a list of keywords q.
2. Question analyzer analyzes q, generates a ba-
sic Lucene’ TermQuery, and defines a set of
hint terms H.
3. Retrieve the most relevant c documents using
Lucene’s default search scoring function3.
Denote D as the set of retrieved documents.
4. For each document d in D where d =
{t1, t2, ... , t|d|} (t is a term),
(a) Find in d the start term index
mms5tart and end term index
mmsEnd of the shortest term span
containing all terms in q (Monz, 2003).
(b) spanLength ← 1 + mmsEnd −
mms5tart
(c) If spanLength &gt; 30, skip current d.
Go to the next document.
</listItem>
<footnote confidence="0.7709525">
3http://lucene.apache.org/java/2_3_0/
scoring.html
</footnote>
<listItem confidence="0.927791">
(d) Find minimal span weighting
</listItem>
<equation confidence="0.9062273">
score msw (Monz, 2003). If
|q ∩ d |= 1 then, msw =
R5Un(q, d). Otherwise, msw =
0.4·R5Un(q,d)+0.6·( |q∩d|
spanLength)1/8·
( |q∩d|) where R5Un(q, d) =
|q|
lucene(q, d)/maxdlucene(q, d)
(e) mms5tart ← max(mms5tart−s,1)
(f) mmsEnd ← min(mmsEnd + s, |d|)
</equation>
<listItem confidence="0.995391142857143">
(g) Find the weighting for hint terms hw
(0 ≤ hw ≤ 1).
(h) Calculate the span score
sp = msw · (1 + hw)
(i) Add the text span to the span set P (Sort
P by sp in descending order).
5. Return the top k spans in P as answers.
</listItem>
<bodyText confidence="0.993383272727273">
In the actual implementation, we set c equal to
500 so that only the top 500 documents are con-
sidered. Although retrieving more texts from the
corpus would likely increase the chance of find-
ing the answer (Moldovan et al., 2002), our trial-
and-error showed that 500 documents seem to be
a good trade-off between speed and content cov-
erage. To look for an occurrence of hint terms,
each span is stretched backward and forward for
10 terms (i.e., s = 10). Finally, we set k equal to
5 to return only the top five spans as the answers.
</bodyText>
<subsectionHeader confidence="0.998708">
2.3 Answer Processor
</subsectionHeader>
<bodyText confidence="0.999858047619048">
This sub-system contains two modules: Answer
Ranker and Answer Generator.
Answer Ranker concerns with how to rank the
retrieved answer candidates. In the case where
SPARQL query is used, this module is not re-
quired since most of the time there will be only
one result returned.
In the case when the search index is used, all
candidate answers are sorted by the heuristic span
score (i.e., sp = msw · (1 + hw)). The func-
tion mostly relies on regular expressions defining
expected answer patterns. If a span has an occur-
rence of one of the defined patterns (i.e., hw &gt; 0),
it is directly proportional to the suitability of the
occurrence with respect to the question, length and
rareness of the pattern occurrence. For example,
the hint terms of questions asking for a person
would be personal titles such as Ms. and Dr.
As for the final step, the Answer Generator
module formats the top five candidate answers into
an HTML table and returns the results to the user.
</bodyText>
<page confidence="0.997228">
13
</page>
<table confidence="0.999873142857143">
Question Type Index &amp; RDF Index
Person 0.47 0.37
Organization 0.56 0.46
Place/Location 0.43 0.36
Quantity 0.51 0.44
Date/Time 0.39 0.34
Average MRR 0.47 0.39
</table>
<tableCaption confidence="0.983849666666667">
Table 1: QAST’s performance comparison be-
tween (1) using both index and RDF and (2) using
only the index.
</tableCaption>
<sectionHeader confidence="0.996066" genericHeader="method">
3 Evaluation Metric
</sectionHeader>
<bodyText confidence="0.999970933333333">
To evaluate the system, 215 test questions (43
questions for each question type) and their cor-
rect answers were constructed based on the con-
tents of random articles in Thai Wikipedia. Mean
Reciprocal Rank (MRR), the official measurement
used for QA systems in TREC (Voorhees and Tice,
2000), is used as the performance measurement.
To evaluate the system, a question is said to be
correctly answered only when at least one of the
produced five ranked candidates contained the true
answer with the right context. Out-of-context can-
didate phrases which happen to contain the true
answers are not counted. If there is no correct an-
swer in any candidate, the score for that question
is equal to zero.
</bodyText>
<sectionHeader confidence="0.990096" genericHeader="method">
4 Experimental Results and Discussion
</sectionHeader>
<bodyText confidence="0.999604571428572">
Table 1 shows a comparison of the MRR values
when using both index and RDF, and using only
the index. The approach of using only the index,
the overall MRR is equal to 0.39 which is fairly
high with respect to the answer retrieval method-
ology. The index search approach simply relies on
the fact that if the question keywords in a ranked
candidate document occur close together and at
least one occurrence of expected answer pattern
exists, then there is a high chance that the term
span contains an answer.
The MRR significantly increases to 0.47 (20.5%
improvement) when RDF (structured information)
is used together with the index. A thorough analy-
sis showed that out of 215 questions, 21 questions
triggered the RDF base. Among these, 18 ques-
tions were correctly answered. Therefore, using
the additional structured information helps answer
the definitional and factoid questions. We expect a
higher improvement when more structured infor-
mation is incorporated into the system.
</bodyText>
<sectionHeader confidence="0.995119" genericHeader="conclusions">
5 Conclusions and Future Works
</sectionHeader>
<bodyText confidence="0.9999835">
We proposed an open-domain QA system called
QAST. The system uses Thai Wikipedia as the cor-
pus and does not rely on any complex NLP tech-
nique in retrieving an answer.
As for future works, some possiblities for im-
proving the current QAST are as follows.
</bodyText>
<listItem confidence="0.9588315">
• An information extraction module may be
added to extract and generate RDF triples
from unstructured text.
• Infoboxes, wikipedia categories and internal
article links may be further explored to con-
struct an ontology which will allow an auto-
matic type inference of entities.
• More question patterns and the correspond-
ing SPARQL queries can be added so that
SPARQL is used more often.
</listItem>
<sectionHeader confidence="0.978792" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.985939">
The financial support from Young Scientist and
Technologist Programme, NSTDA (YSTP : SP-
51-NT-15) is gratefully acknowledged.
</bodyText>
<sectionHeader confidence="0.998307" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999614590909091">
Soren Auer and Jens Lehmann. 2007. What Have
Innsbruck and Leipzig in Common? Extracting Se-
mantics from Wiki Content. In Proc. of the 4th Eu-
ropean conference on The Semantic Web: Research
and Applications, pp. 503-517.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andrew Ng. 2002. Web Question Answering:
Is More Always Better?. In Proc. of the 25th ACM
SIGIR, pp. 291-298.
Jonathan Isbell and Mark H. Butler. 2007. Extracting
and Re-using Structured Data from Wikis. Technical
Report HPL-2007-182, Hewlett-Packard.
Dan Moldovan, Marius Pasca, Sanda Harabagiu, and
Mihai Surdeanu . 2002. Performance Issues and Er-
ror Analysis in an Open-Domain Question Answer-
ing System. Proc. of the 40th ACL, pp. 33-40.
Christof Monz. 2003. From Document Retrieval to
Question Answering. Ph.D. Thesis. University of
Amsterdam.
Ellen M. Voorhees and Dawn Tice. 2000. Building a
Question Answering Test Collection. In 23rd ACM
SIGIR, pp. 200-207.
</reference>
<page confidence="0.999271">
14
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.242820">
<author confidence="0.369977">QAST Question Answering System for Thai Wikipedia</author>
<affiliation confidence="0.8546585">of Information, Computer and Communication Technology Sirindhorn International Institute of Technology</affiliation>
<address confidence="0.997299">131 Moo 5 Tiwanont Rd., Bangkadi, Muang, Phathumthani, Thailand,</address>
<email confidence="0.990981">wittawatj@gmail.com,thanaruk@siit.tu.ac.th</email>
<affiliation confidence="0.926642">Language Technology Laboratory National Electronics and Computer Technology Center Thailand Science Park, Klong Luang, Pathumthani 12120,</affiliation>
<email confidence="0.913669">choochart.haruechaiyasak@nectec.or.th</email>
<abstract confidence="0.997521105263158">We propose an open-domain question answering system using Thai Wikipedia as the knowledge base. Two types of information are used for answering a question: (1) structured information extracted and stored in the form of Resource Description Framework (RDF), and (2) unstructured texts stored as a search index. For the structured information, SPARQL transformed query is applied to retrieve a short answer from the RDF base. For the unstructured information, keyword-based query is used to retrieve the shortest text span containing the questions’s key terms. From the experimental results, the system which integrates both approaches could an average MRR of on 215 test questions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Soren Auer</author>
<author>Jens Lehmann</author>
</authors>
<title>What Have Innsbruck and Leipzig in Common? Extracting Semantics from Wiki Content.</title>
<date>2007</date>
<booktitle>In Proc. of the 4th European conference on The Semantic Web: Research and Applications,</booktitle>
<pages>503--517</pages>
<contexts>
<context position="3999" citStr="Auer and Lehmann (2007)" startWordPosition="610" endWordPosition="613">b-systems: Data Representation, Question Processor, and Answer 11 Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP 2009, pages 11–14, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP Figure 1: The system architecture of QAST Processor. 2.1 Data Representation The Data Representation part is a storage for all information contained in Thai Wikipedia. Two modules constitute this sub-system. RDF Base: In QAST, RDF triples are generated from Wikipedia’s infoboxes following similar approaches described in the works of Isbell and Butler (2007) and Auer and Lehmann (2007). To generate RDF triples from an infobox, we would have the article title as the subject. The predicates are the keys in the first column. The objects are the values in the second column. Altogether, the number of generated triples corresponds to the number of rows in the infobox. In addition to the infoboxes, we also store synonyms in the form of RDF triples. The synonyms are extracted from redirect pages in Wikipedia. For example, a request for the Wikipedia article titled “Car” will result in another article titled “Automobile” to be shown up. The former page usually has no content and onl</context>
</contexts>
<marker>Auer, Lehmann, 2007</marker>
<rawString>Soren Auer and Jens Lehmann. 2007. What Have Innsbruck and Leipzig in Common? Extracting Semantics from Wiki Content. In Proc. of the 4th European conference on The Semantic Web: Research and Applications, pp. 503-517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Dumais</author>
<author>Michele Banko</author>
<author>Eric Brill</author>
<author>Jimmy Lin</author>
<author>Andrew Ng</author>
</authors>
<title>Web Question Answering: Is More Always Better?.</title>
<date>2002</date>
<booktitle>In Proc. of the 25th ACM SIGIR,</booktitle>
<pages>291--298</pages>
<contexts>
<context position="2390" citStr="Dumais et al., 2002" startWordPosition="351" endWordPosition="354">into two types: 1. Knowledge Intensive: Knowledge intensive systems focus on analyzing and understanding the input questions. The system knows exactly what to be answered, and also what type the answer should be. The analysis phase usually depends on an ontology or a semantic lexicon like WordNet. The answer is retrieved from a predefined organized knowledge base. Natural Language Processing (NLP) techniques are heavily used in a knowledge intensive system. 2. Data Intensive: Data intensive systems, which do not fully analyze the input questions, rely on the redundancy of huge amount of data (Dumais et al., 2002). The idea is that if we have a huge amount of data, a piece of information is likely to be stated more than once in different forms. As a result, the dataintensive QA systems are not required to perform many complex NLP techniques. In this paper, we propose an open-domain QA system for Thai Wikipedia called QAST. The system supports five types of close-ended questions: person, organization, place, quantity, and date/- time. Our system can be classified as a data intensive type with an additional support of structured information. Structured information in Thai Wikipedia is extracted and repre</context>
</contexts>
<marker>Dumais, Banko, Brill, Lin, Ng, 2002</marker>
<rawString>Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, and Andrew Ng. 2002. Web Question Answering: Is More Always Better?. In Proc. of the 25th ACM SIGIR, pp. 291-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Isbell</author>
<author>Mark H Butler</author>
</authors>
<title>Extracting and Re-using Structured Data from Wikis.</title>
<date>2007</date>
<tech>Technical Report HPL-2007-182, Hewlett-Packard.</tech>
<contexts>
<context position="3971" citStr="Isbell and Butler (2007)" startWordPosition="604" endWordPosition="608">ich consists of three main sub-systems: Data Representation, Question Processor, and Answer 11 Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP 2009, pages 11–14, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP Figure 1: The system architecture of QAST Processor. 2.1 Data Representation The Data Representation part is a storage for all information contained in Thai Wikipedia. Two modules constitute this sub-system. RDF Base: In QAST, RDF triples are generated from Wikipedia’s infoboxes following similar approaches described in the works of Isbell and Butler (2007) and Auer and Lehmann (2007). To generate RDF triples from an infobox, we would have the article title as the subject. The predicates are the keys in the first column. The objects are the values in the second column. Altogether, the number of generated triples corresponds to the number of rows in the infobox. In addition to the infoboxes, we also store synonyms in the form of RDF triples. The synonyms are extracted from redirect pages in Wikipedia. For example, a request for the Wikipedia article titled “Car” will result in another article titled “Automobile” to be shown up. The former page us</context>
</contexts>
<marker>Isbell, Butler, 2007</marker>
<rawString>Jonathan Isbell and Mark H. Butler. 2007. Extracting and Re-using Structured Data from Wikis. Technical Report HPL-2007-182, Hewlett-Packard.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Marius Pasca</author>
<author>Sanda Harabagiu</author>
<author>Mihai Surdeanu</author>
</authors>
<title>Performance Issues and Error Analysis in an Open-Domain Question Answering System.</title>
<date>2002</date>
<booktitle>Proc. of the 40th ACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="9611" citStr="Moldovan et al., 2002" startWordPosition="1583" endWordPosition="1586">0.4·R5Un(q,d)+0.6·( |q∩d| spanLength)1/8· ( |q∩d|) where R5Un(q, d) = |q| lucene(q, d)/maxdlucene(q, d) (e) mms5tart ← max(mms5tart−s,1) (f) mmsEnd ← min(mmsEnd + s, |d|) (g) Find the weighting for hint terms hw (0 ≤ hw ≤ 1). (h) Calculate the span score sp = msw · (1 + hw) (i) Add the text span to the span set P (Sort P by sp in descending order). 5. Return the top k spans in P as answers. In the actual implementation, we set c equal to 500 so that only the top 500 documents are considered. Although retrieving more texts from the corpus would likely increase the chance of finding the answer (Moldovan et al., 2002), our trialand-error showed that 500 documents seem to be a good trade-off between speed and content coverage. To look for an occurrence of hint terms, each span is stretched backward and forward for 10 terms (i.e., s = 10). Finally, we set k equal to 5 to return only the top five spans as the answers. 2.3 Answer Processor This sub-system contains two modules: Answer Ranker and Answer Generator. Answer Ranker concerns with how to rank the retrieved answer candidates. In the case where SPARQL query is used, this module is not required since most of the time there will be only one result returne</context>
</contexts>
<marker>Moldovan, Pasca, Harabagiu, Surdeanu, 2002</marker>
<rawString>Dan Moldovan, Marius Pasca, Sanda Harabagiu, and Mihai Surdeanu . 2002. Performance Issues and Error Analysis in an Open-Domain Question Answering System. Proc. of the 40th ACL, pp. 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christof Monz</author>
</authors>
<title>From Document Retrieval to Question Answering.</title>
<date>2003</date>
<tech>Ph.D. Thesis.</tech>
<institution>University of Amsterdam.</institution>
<contexts>
<context position="3274" citStr="Monz, 2003" startWordPosition="503" endWordPosition="504"> QA system for Thai Wikipedia called QAST. The system supports five types of close-ended questions: person, organization, place, quantity, and date/- time. Our system can be classified as a data intensive type with an additional support of structured information. Structured information in Thai Wikipedia is extracted and represented in the form of RDF. We use SPARQL to retrieve specific information from the RDF base. If using SPARQL cannot answer a given question, the system will retrieve answer candidates from the pre-constructed search index using a technique based on Minimal Span Weighting (Monz, 2003). 2 System Architecture Figure 1 shows the system architecture of QAST which consists of three main sub-systems: Data Representation, Question Processor, and Answer 11 Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP 2009, pages 11–14, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP Figure 1: The system architecture of QAST Processor. 2.1 Data Representation The Data Representation part is a storage for all information contained in Thai Wikipedia. Two modules constitute this sub-system. RDF Base: In QAST, RDF triples are generated from Wiki</context>
<context position="8720" citStr="Monz, 2003" startWordPosition="1422" endWordPosition="1423"> the RDF base, the system switches to the index search which performs the following the steps. 1. Word Segmenter tokenizes the question into a list of keywords q. 2. Question analyzer analyzes q, generates a basic Lucene’ TermQuery, and defines a set of hint terms H. 3. Retrieve the most relevant c documents using Lucene’s default search scoring function3. Denote D as the set of retrieved documents. 4. For each document d in D where d = {t1, t2, ... , t|d|} (t is a term), (a) Find in d the start term index mms5tart and end term index mmsEnd of the shortest term span containing all terms in q (Monz, 2003). (b) spanLength ← 1 + mmsEnd − mms5tart (c) If spanLength &gt; 30, skip current d. Go to the next document. 3http://lucene.apache.org/java/2_3_0/ scoring.html (d) Find minimal span weighting score msw (Monz, 2003). If |q ∩ d |= 1 then, msw = R5Un(q, d). Otherwise, msw = 0.4·R5Un(q,d)+0.6·( |q∩d| spanLength)1/8· ( |q∩d|) where R5Un(q, d) = |q| lucene(q, d)/maxdlucene(q, d) (e) mms5tart ← max(mms5tart−s,1) (f) mmsEnd ← min(mmsEnd + s, |d|) (g) Find the weighting for hint terms hw (0 ≤ hw ≤ 1). (h) Calculate the span score sp = msw · (1 + hw) (i) Add the text span to the span set P (Sort P by sp in</context>
</contexts>
<marker>Monz, 2003</marker>
<rawString>Christof Monz. 2003. From Document Retrieval to Question Answering. Ph.D. Thesis. University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Dawn Tice</author>
</authors>
<title>Building a Question Answering Test Collection.</title>
<date>2000</date>
<booktitle>In 23rd ACM SIGIR,</booktitle>
<pages>200--207</pages>
<contexts>
<context position="11478" citStr="Voorhees and Tice, 2000" startWordPosition="1911" endWordPosition="1914">TML table and returns the results to the user. 13 Question Type Index &amp; RDF Index Person 0.47 0.37 Organization 0.56 0.46 Place/Location 0.43 0.36 Quantity 0.51 0.44 Date/Time 0.39 0.34 Average MRR 0.47 0.39 Table 1: QAST’s performance comparison between (1) using both index and RDF and (2) using only the index. 3 Evaluation Metric To evaluate the system, 215 test questions (43 questions for each question type) and their correct answers were constructed based on the contents of random articles in Thai Wikipedia. Mean Reciprocal Rank (MRR), the official measurement used for QA systems in TREC (Voorhees and Tice, 2000), is used as the performance measurement. To evaluate the system, a question is said to be correctly answered only when at least one of the produced five ranked candidates contained the true answer with the right context. Out-of-context candidate phrases which happen to contain the true answers are not counted. If there is no correct answer in any candidate, the score for that question is equal to zero. 4 Experimental Results and Discussion Table 1 shows a comparison of the MRR values when using both index and RDF, and using only the index. The approach of using only the index, the overall MRR</context>
</contexts>
<marker>Voorhees, Tice, 2000</marker>
<rawString>Ellen M. Voorhees and Dawn Tice. 2000. Building a Question Answering Test Collection. In 23rd ACM SIGIR, pp. 200-207.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>