<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.864043">
Argumentative Human Computer Dialogue for Automated Persuasion
</title>
<author confidence="0.966357">
Pierre Andrews* and Suresh Manandhar* and Marco De Boni**
</author>
<affiliation confidence="0.9970825">
* Department of Computer Science
University of York
</affiliation>
<address confidence="0.828872">
York YO10 5DD
UK
</address>
<email confidence="0.995909">
1pandrews,sureshl@cs.york.ac.uk
</email>
<sectionHeader confidence="0.998558" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998328">
Argumentation is an emerging topic in the
field of human computer dialogue. In this
paper we describe a novel approach to dia-
logue management that has been developed to
achieve persuasion using a textual argumen-
tation dialogue system. The paper introduces
a layered management architecture that mixes
task-oriented dialogue techniques with chat-
bot techniques to achieve better persuasive-
ness in the dialogue.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999643952380952">
Human computer dialogue is a wide research area
in Artificial Intelligence. Computer dialogue is
now used at production stage for applications such
as tutorial dialogue – that helps teaching students
(Freedman, 2000) – task-oriented dialogue – that
achieves a particular, limited task, such as book-
ing a trip (Allen et al., 2000) – and chatbot dialogue
(Levy et al., 1997) – that is used within entertain-
ment and help systems.
None of these approaches use persuasion as a
mechanism to achieve dialogue goals. However,
research towards the use of persuasion in Hu-
man Computer Interactions has spawned around the
field of natural argumentation (Norman and Reed,
2003). Similarly research on Embodied Con-
versational Agents (ECA) (Bickmore and Picard,
2005) is also attempting to improve the persuasive-
ness of agents with persuasion techniques; how-
ever, it concentrates on the visual representation
of the interlocutor rather than the dialogue man-
agement. Previous research on human computer
</bodyText>
<address confidence="0.561743666666667">
** Unilever Corporate Research
Bedford MK44 1LQ
UK
</address>
<email confidence="0.877388">
Marco.De-Boni@unilever.com
</email>
<bodyText confidence="0.999928941176471">
dialogue has rarely focused on persuasive tech-
niques (Guerini, Stock, and Zancanaro, 2004, initi-
ated some research in that field). Our dialogue man-
agement system applies a novel method, taking ad-
vantage of persuasive and argumentation techniques
to achieve persuasive dialogue.
According to the cognitive dissonance theory
(Festinger, 1957), people will try to minimise the
discrepancy between their behaviour and their be-
liefs by integrating new beliefs or distorting existing
ones. In this paper, we approach persuasion as a pro-
cess shaping user’s beliefs to eventually change their
behaviour.
The presented dialogue management system has
been developed to work on known limitations of cur-
rent dialogue systems:
The impression of lack of control is an issue when
the user is interacting with a purely task-oriented di-
alogue system (Farzanfar et al., 2005). The system
follows a plan to achieve the particular task, and the
user’s dialogue moves are dictated by the planner
and the plan operators.
The lack of empathy of computers is also a
problem in human-computer interaction for applica-
tions such as health-care, where persuasive dialogue
could be applied (Bickmore and Giorgino, 2004).
The system does not respond to the user’s personal
and emotional state, which sometimes lowers the
user’s implication in the dialogue. However, exist-
ing research (Klein, Moon, and Picard, 1999) shows
that a system that gives appropriate response to the
user’s emotion can lower frustration.
In human-human communication, these lim-
itations reduce the effectiveness of persuasion
</bodyText>
<page confidence="0.969073">
138
</page>
<note confidence="0.8784025">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 138–147,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9921525">
(Stiff and Mongeau, 2002). Even if the response to-
wards the computer is not always identical to the
one to humans, it seems sensible to think that per-
suasive dialogue systems can be improved by apply-
ing known findings from human-human communi-
cation.
The dialogue management architecture described
in this paper (see Figure 1) addresses these dialogue
management issues by using a novel layered ap-
proach to dialogue management, allowing the mix-
ing of techniques from task-oriented dialogue man-
agement and chatbot techniques (see Section 4).
</bodyText>
<figureCaption confidence="0.999535">
Figure 1: Layered Management Architecture
</figureCaption>
<bodyText confidence="0.999987642857143">
The use of a planner guarantees the consistency
of the dialogue and the achievement of persuasive
goals (see Section 4.2). Argumentative dialogue can
be seen as a form of task-oriented dialogue where
the system’s task is to persuade the user by present-
ing the arguments. Thus, the dialogue manager first
uses a task-oriented dialogue methodology to cre-
ate a dialogue plan that will determine the content
of the dialogue. The planning component’s role is
to guarantee the consistency of the dialogue and the
achievement of the persuasive goals.
In state-of-the-art task-oriented dialogue manage-
ment systems, the planner provides instructions for
a surface realizer (Green and Lehman, 2002), re-
sponsible of generating the utterance corresponding
to the plan step. Our approach is different to al-
low more reactivity to the user and give a feeling
of control over the dialogue. In this layered ap-
proach, the reactive component provides a direct re-
action to the user input, generating one or more ut-
terances for a given plan step, allowing for reactions
to user’s counter arguments as well as backchannel
and chitchat phases without cluttering the plan.
Experimental results show that this layered ap-
proach allows the user to feel more comfortable in
the dialogue while preserving the dialogue consis-
tency provided by the planner. Eventually, this trans-
lates into a more persuasive dialogue (see Section 6).
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999941045454545">
Persuasion through dialogue is a novel
field of Human Computer Interaction.
Reiter, Robertson, and Osman (2003),Reed (1998)
and Carenini and Moore (2000) apply persuasive
communication principles to natural language
generation, but only focus on monologue.
The 3-tier planner for tutoring dialogue by
Zinn, Moore, and Core (2002) provides a di-
alogue management technique close to our
approach: a top-tier generates a dialogue plan,
the middle-tier generates refinements to the
plan and the bottom-tier generates utterances.
Mazzotta, de Rosis, and Carofiglio (2007) also
propose a planning framework for user-adapted
persuasion where the plan operators are mapped
to natural language (or ECA) generation. How-
ever, these planning approaches do not include a
mechanism to react to user’s counter arguments
that are difficult to plan beforehand. This paper
propose a novel approach that could improve
the user’s comfort in the dialogue as well as its
persuasiveness.
</bodyText>
<sectionHeader confidence="0.913607" genericHeader="method">
3 Case Study
</sectionHeader>
<bodyText confidence="0.998896470588235">
Part of the problem in evaluating persuasive dia-
logue is using an effective evaluation framework.
Moon (1998) uses the Desert Survival Scenario to
evaluate the difference of persuasion and trust in
interaction between humans when face-to-face or
when mediated by a computer system (via an instant
messaging platform).
The Desert Survival Scenario
(Lafferty, Eady, and Elmers, 1974) is a negoti-
ation scenario used in team training. The team is
put in a scenario where they are stranded in the
desert after a plane crash. They have to negotiate a
ranking of the most eligible items (knife, compass,
map, etc.) that they should keep for their survival.
For the evaluation of the dialogue system, a simi-
lar scenario is presented to the participants. The user
has to choose an initial preferred ranking of items
</bodyText>
<page confidence="0.998283">
139
</page>
<bodyText confidence="0.999897">
and then engages in a discussion with the dialogue
system that tries to persuade the user to change the
ranking. At the end of the dialogue, the user has the
opportunity to either change or keep the ranking.
The architecture of the dialogue system is de-
scribed throughout this paper using examples from
the Desert Scenario. The full evaluation protocol is
described in Section 5 and 6.
</bodyText>
<sectionHeader confidence="0.996173" genericHeader="method">
4 Dialogue Management Architecture
</sectionHeader>
<bodyText confidence="0.692155">
The following sections provide a description of
the dialogue management architecture introduced in
Figure 1.
</bodyText>
<subsectionHeader confidence="0.964151">
4.1 Argumentation Model
</subsectionHeader>
<bodyText confidence="0.9952134">
The Argumentation model represents the different
arguments (conclusions and premises) that can be
proposed by the user or by the system. Figure 2
gives a simplified example of the Desert Scenario
model.
</bodyText>
<figureCaption confidence="0.996771">
Figure 2: Argumentation Model Sample
</figureCaption>
<bodyText confidence="0.999838878787879">
This model shows the different facts that are
known by the system and the relations be-
tween them. Arrows represent the support re-
lation between two facts. For example, res-
cue knows where you are is a support to the fact
goal(signal) (the user goal is to signal presence to
the rescue) as well as a support to goal(stay put) (the
user goal is to stay close to the wreckage). This
relational model is comparable to the argumenta-
tion framework proposed by Dung (1995), but stores
more information about each argument for reason-
ing within the planning and reactive component (see
Section 4.2).
Each fact in this model represents a belief to be
introduced to the user. For example, when the dia-
logue tries to achieve the goal reorder(flashlight &gt;
air map): the system wants the user to believe that
the “flashlight” item should be ranked higher than
the “air map” item. The argumentation model de-
scribes the argumentation process that is required
to introduce this new belief: the system first has to
make sure the user believes in rate lower(air map)
and rate higher(flashlight).
Lower level facts (see Figure 2) are the goal facts
of the dialogue, the ones the system chooses as di-
alogue goals, according to known user beliefs and
the system’s goal beliefs (e.g. according to the rank-
ing the system is trying to defend). The facts in the
middle of the hierarchy are intermediate facts that
need to be asserted during the dialogue. The top-
level facts are world knowledge: facts that require
minimum defense and can be easily grounded in the
dialogue.
</bodyText>
<subsectionHeader confidence="0.996968">
4.2 Planning Component
</subsectionHeader>
<bodyText confidence="0.998950620689655">
The planning component’s task is to find a plan us-
ing the argumentation model to introduce the re-
quired facts in the user’s belief to support the per-
suasive goals. The plan is describes a path in the ar-
gumentation model beliefs hierarchy that translates
to argumentation segments in the dialogue.
In our current evaluation method, the goal of the
dialogue is to change the user’s beliefs about the
items so that the user eventually changes the rank-
ing. At the beginning of the dialogue, the ranking of
the system is chosen and persuasive goals are com-
puted for the dialogue. These persuasive goals cor-
respond to the lower level facts in the argumentation
model – like “reorder(flashlight &gt; air map)” in our
previous example. The available planning operators
are:
use world(fact) describes a step in the dialogue
that introduces a simple fact to the user.
ground(fact) describes a step in the dialogue that
grounds a fact in the user beliefs. Grounding a fact
is a different task from the use world operator as it
will need more support during the dialogue.
do support([fact0, fact], ...], fact2) describes a
complex support operation. The system will initiate
a dialogue segment supporting fact2 with the facts
fact] and fact0, etc. that have previously been intro-
duced in the user beliefs.
The planning component can also use two
non-argumentative operators, do greetings and
</bodyText>
<page confidence="0.981319">
140
</page>
<bodyText confidence="0.975512444444445">
do farewells, that are placed respectively at the be-
ginning and the end of the dialogue plan to open and
close the session.
Here is an example plan using the two argu-
ments described in Figure 2 to support the goal re-
order(flashlight &gt; air map):
Step 1 do greetings
Step 2 use world(goal(be found))
ground(rescue knows where you are)
ground(can(helpatnight,
item(flashlight)))
Step 3 do support([can(helpatnight,
item(flashlight))],
rate higher(item(flashlight)))
do support(
[rescue knows where you are,
goal(be found)],
goal(stay put))
Step 4 do support([goal(stay put)],
rate lower(item(air map)))
Step 5 do support(...,
reorder(item(flashlight),
item(air map)))
Step 6 do farewells
The plan is then interpreted by the reactive com-
ponent that is responsible for realizing each step in
a dialogue segment.
</bodyText>
<subsectionHeader confidence="0.998543">
4.3 The Reactive Component
</subsectionHeader>
<bodyText confidence="0.9999968">
The reactive component’s first task is to realize the
operators chosen by the planning component into di-
alogue utterance(s). However, it should not be mis-
taken for a surface language realizer. The reactive
component’s task, when realizing the operator, is to
decide how to present the particular argumentation
operator and its parameters to the user according to
the dialogue context and the user’s reaction to the
argument. This reactive process is described in the
following sections.
</bodyText>
<subsectionHeader confidence="0.53801">
4.3.1 Realization and Reaction Strategies
</subsectionHeader>
<bodyText confidence="0.999716333333333">
Each step of the plan describes the general topic
of a dialogue segment1. A dialogue segment is
a set of utterances from the system and from
</bodyText>
<footnote confidence="0.9122855">
1i.e. it is not directly interpreted as an instruction to generate
one unique utterance.
</footnote>
<bodyText confidence="0.971339891304348">
the user that are related to a particular argument.
For example, in the Desert Scenario, the operator
ground(can(helpatnight, item(flashlight))) may re-
sult in the following set of utterances:
S(ystem) I think the flashlight could
be useful as it could help us at
night,
U(ser) How is that? We are not going
to move during the night.
S well, if we want to collect water,
it will be best to do things at
night and not under the burning
sun.
U I see. It could be useful then.
In this example, the ground operator has been re-
alized by the reactive component in two different ut-
terances to react to the user’s interaction.
The goal of the reactive component is to make the
user feel that the system understands what has been
said. It is also important to avoid replanning as it
tries to defend the arguments chosen in the plan.
As described in Section 4.2, the planner relies on
the argumentation model to create a dialogue plan.
Encoding all possible defenses and reactions to the
user directly in this model will explode the search
space of the planner and require careful authoring
to avoid planning inconsistencies2. In addition, pre-
dicting at the planning level what counter arguments
a user is likely to make requires a prior knowledge
of the user’s beliefs. At the beginning of a one-off
dialogue, it is not possible to make prior assump-
tions on the user’s beliefs; the system has a shal-
low knowledge of the user’s beliefs and will discover
them as the dialogue goes.
Hence, it is more natural to author a reactive di-
alogue that will respond to the user’s counter ar-
guments as they come and extends the user beliefs
model as it goes. In our architecture if the user is
disagreeing with an argument, the plan is not revised
directly; if possible, the reactive component selects
new, contextually appropriate, supporting facts for
the current plan operator. It can do this multiple
consecutive local repairs if the user needs more con-
vincing and the domain model provides enough de-
fenses. This allows for a simpler planning frame-
work.
</bodyText>
<footnote confidence="0.775967">
2a new plan could go against the previously used arguments.
</footnote>
<page confidence="0.995571">
141
</page>
<bodyText confidence="0.992065833333333">
In addition, when available, and even if the user
agrees with the current argument, the reactive com-
ponent can also choose from a set of “dialogue
smoothing” or backchannel utterances to make the
dialogue feel more natural. Here is an example from
the Desert Scenario:
</bodyText>
<figure confidence="0.6458215">
S We don’t have much water, we need to
be rescued as soon as possible.
(from plan step: user world(goal(be found)))
U right
S I am glad we agree.(backchannel)
S There is a good chance that the
rescue team already knows our
whereabouts. We should be
optimistic and plan accordingly,
don’t you think?
(from plan step:
use world(rescue knows where you are))
</figure>
<subsectionHeader confidence="0.833179">
4.3.2 Detecting user reactions
</subsectionHeader>
<bodyText confidence="0.999793846153846">
The reactive component needs to detect if the user
is agreeing to its current argument or resisting the
new fact that is presented. Because the dialogue
management system was developed from the per-
spective of a system that could be easily ported to
different domains, choice was made to use a domain
independent and robust agreement/disagreement de-
tection.
The agreement/disagreement detection is based
on an utterance classifier. The classifier is a cas-
cade of binary Support Vector Machines (SVM)
(Vapnik, 2000) trained on the ICSI Meeting cor-
pus (Janin et al., 2003). The corpus contains 8135
spurts3 annotated with agreement/disagreement in-
formation Hillard, Ostendorf, and Shriberg (2003).
A multi-class SVM classifier is trained on local
features of the spurts such as a) the length of the
spurt, b) the first word of the spurt, c) the bigrams of
the spurts, and d) part of speech tags. The classifica-
tion achieves an accuracy of 83.17% with an N-Fold
4 ways split cross validation. Additional results and
comparison with state-of-the-art are available in Ap-
pendix A.
During the dialogue, the classifier is applied on
each of the user’s utterances, trying to determine if
the user is agreeing or disagreeing with the system.
</bodyText>
<footnote confidence="0.831738">
3speech utterances that have no pauses longer than .5 sec-
onds.
</footnote>
<bodyText confidence="0.982514">
According to this labelling, the strategies described
in section 4.3.1 and 4.3.3 are applied.
</bodyText>
<subsectionHeader confidence="0.949729">
4.3.3 Revising the plan
</subsectionHeader>
<bodyText confidence="0.999979384615385">
The reactive component will attempt local repairs
to the plan by defending the argumentation move
chosen by the planning component. However, there
are cases when the user will still not accept an ar-
gument. In these cases, imposing the belief to the
user is counter-productive and the current goal be-
lief should be dropped from the plan.
For each utterance chosen by the reactive com-
ponent, the belief model of the user is updated to
represent the system knowledge of the user’s be-
liefs. Every time the user agrees to an utterance
from the system, the belief model is extended with
a new belief; in the previous example, when the
user says “I see, it could be useful then.”, the sys-
tem detects an agreement (see the Section 4.3.2)
and extends the user’s beliefs model with the be-
lief: can(helpatnight, item(flashlight)). The agree-
ment is then followed by a local repair, since the
user doesn’t disagree with the statement made, the
system also extends the belief model with beliefs rel-
evant to the content of the local repair, thus learning
more about the user’s belief model.
As a result of this process, when the system de-
cides to revise the plan, the planning component
does not start from the same beliefs state as previ-
ously. In effect, the system is able to learn user’s be-
liefs based on the agreement/disagreement with the
user, it can therefore make a more effective use of
the argumentation hierarchy to find a better plan to
achieve the persuasive goals.
Still, there are some cases when the planning
component will be unable to find a new plan from
the current belief state to the goal belief state – this
can happen when the planner has exhausted all its
argumentative moves for a particular sub-goal. In
these cases, the system has to make concessions and
drop the persuasive goals that it cannot fulfil. By
dropping goals, the system will lower the final per-
suasiveness, but guarantees not coercing the user.
</bodyText>
<sectionHeader confidence="0.467698" genericHeader="method">
4.3.4 Generation
</sectionHeader>
<bodyText confidence="0.960084333333333">
Utterance generation is made at the reactive com-
ponent level. In the current version of the dia-
logue management system, the utterance generation
</bodyText>
<page confidence="0.990095">
142
</page>
<bodyText confidence="0.958619518518519">
is based on an extended version of Alicebot AIML
4.
AIML is an XML language that provides a pat-
tern/template generation model mainly used for
chatbot systems. An AIML bot defines a set of
categories that associate a topic, the context of the
previous bot utterance (called that in the AIML ter-
minology), a matching pattern that will match the
last user utterance and a generation template. The
topic, matching and that field define matching pat-
terns that can contain * wildcards accepting any to-
ken(s) of the user utterance (e.g. HELLO * would
match any utterance starting by “Hello”). They are
linked to a generation template that can reuse the to-
kens matched by the patterns wildcards to generate
an utterance tailored to the user input and the dia-
logue context.
For the purpose of layered dialogue management,
the AIML language has been extended to include
more features: 1) A new pattern slot has been in-
troduced to link a set of categories to a particular ar-
gumentation operator; 2) Utterances generations are
linked to the belief they are trying to introduce to
the user and if an agreement is detected, this belief
is added to the user belief model.
For example, a set of matching categories for the
Desert Scenario could be:
</bodyText>
<equation confidence="0.5039524">
Plan operator: use world(goal(survive))
Category 1 :
Pattern *
Template Surviving is our
priority, do you want
to hear about my desert
survival insights?
Category 2 :
Pattern * insights
That* survival insights
</equation>
<construct confidence="0.3982343">
Template I mean, I had a few
ideas ...common knowledge I
suppose.
Category 3 :
Pattern *
That* survival insights
Template Well, we are in this
together. Let me tell you
of what I think of desert
survival, ok?
</construct>
<footnote confidence="0.854066">
4http://www.alicebot.org/
</footnote>
<bodyText confidence="0.999983322580645">
These three categories can be used to match
the user reaction during the dialogue seg-
ment corresponding to the plan operator:
use world(goal(survive)). Category 1 is used
as the initiative taking generation. It will be the
first one to be used when the system comes from
a previously finished step. Categories 2-3 are all
“defenses” that support Category 1. They will be
used to react to the user if no agreement is detected
from the last utterances. For example, if the user
says “what kind of survival insights??” as a reply
to the generation from Category 1, a disagreement
is detected and the reactive component will have a
contextualised answer as given by category 2 whose
that pattern matches the last utterance from the
system, the pattern pattern matches the user
utterance.
The dialogue management system uses 187 cate-
gories tailored to the Desert Scenario as well as 3737
general categories coming from the Alice chatbot
and used to generate the dialogue smoothing utter-
ances. Developing domain specific reactions is a te-
dious and slow process that was iteratively achieved
with Wizard of OZ experiments with real users. In
these experiments, users were told they were going
to have a dialogue with another human in the Desert
Scenario context. The dialogue system manages the
whole dialogue, except for the generation phase that
is mediated by an expert that can either choose the
reaction of the system from an existing set of utter-
ances, or type a new one.
</bodyText>
<sectionHeader confidence="0.97449" genericHeader="method">
5 Persuasiveness Metric
</sectionHeader>
<bodyText confidence="0.9998975">
Evaluating a behavior change would require a long-
term observation of the behavior that would be de-
pendent to external elements (Bickmore and Picard,
2005). To evaluate our system, an evaluation proto-
col measuring the change in the beliefs underlying
the behavior was chosen. As explained in Section 3,
the Desert Scenario is used as a base for the evalu-
ation. Each participant is told that he is stranded in
the desert. The user gives a preferred initial rank-
ing RZ of the items (knife, compass, map, etc.). The
user then engages in a dialogue with the system. The
system then attempts to change the user’s ranking to
a different ranking R3 through persuasive dialogue.
At the end of the dialogue, the user can change this
</bodyText>
<page confidence="0.997981">
143
</page>
<bodyText confidence="0.963053666666667">
choice to arrive at a final ranking Rf.
The persuasiveness of the dialogue can be mea-
sured as the evolution of the distance between
the user ranking (Ri, Rf) and the system ranking
(Rs). The Kendall T distance (Kendall, 1938) is
used to compute the pairwise disagreement between
two rankings. The change of the Kendall T dis-
tance during the dialogue gives an evaluation of the
persuasiveness of the dialogue: Persuasiveness =
KT(Ri, Rs) − KT(Rf, Rs). In the current evalu-
ation protocol, the Rs is always the reverse of the
Ri, so KT(Ri, Rs) is always the maximum distance
possible: n�(n−1)
2 where n is the number of items to
rank. The minimum Kendall tau distance is 0. If the
system was persuasive enough to make the user in-
vert the initial ranking, Persuasiveness of the system
is maximum and equal to: n�(n−1)
2 . If the system
does not succeed in changing the user ranking, then
Persuasiveness is zero.
</bodyText>
<sectionHeader confidence="0.802618" genericHeader="evaluation">
6 Evaluation Results and Discussion
</sectionHeader>
<bodyText confidence="0.99660725">
16 participants have been recruited from a variety of
ages (from 20 to 59) and background. They were
all told to use a web application that describes the
Desert Scenario (see Section 3) and proposes to un-
dertake two instant messaging chats with two human
users5. However, both discussions are managed by
different versions of the dialogue system, following
a similar protocol:
</bodyText>
<listItem confidence="0.992928428571429">
• one version of the dialogue is managed by a
limited version of the dialogue system, with no
reactive component. This version is similar to
a purely task-oriented system, planning and re-
vising the plan directly on dialogue failures,
• the second version is the full dialogue system
as described in this paper.
</listItem>
<bodyText confidence="0.999866857142857">
Each participant went through one dialogue with
each system, in a random order. This comparison
shows that the dialogue flexibility provided by the
reactive component allows a more persuasive dia-
logue. In addition, when faced with the second dia-
logue, the participant has formed more beliefs about
the scenario and is more able to counter argue.
</bodyText>
<footnote confidence="0.857654">
5The evaluation is available Online at
http://www.cs.york.ac.uk/aig/eden
</footnote>
<figureCaption confidence="0.998068666666667">
Figure 3: Comparative Results. interpret, not coercive,
perceived persuasion are on a scale of [0 − 4] (see Ap-
pendix B). Persuasiveness is on a scale of [−10, 10].
</figureCaption>
<bodyText confidence="0.99655790625">
Figure 3 reports the independent Persuasiveness
metric results as well as interesting answers to a
questionnaire that the participants filled after each
dialogue (see the Appendix B for detailed results
and questionnaire).
Over all the dialogues, the full system is 18%
more persuasive than the limited system. This is
measured by the Persuasiveness metric introduced in
Section 5. With the full system, the participants did
an average of 1.33 swaps of items towards the sys-
tem’s ranking. With the limited system, the partic-
ipants did an average of 0.47 swaps of items away
from the system’s ranking. However, the answers
to the self evaluated perceived persuasion question
show that the participants did not see any significant
difference in the ability to persuade of the limited
and the full systems.
According to the question interpret, the partici-
pants found that the limited system understood bet-
ter what they said. This last result might be ex-
plained by the behavior of the systems: the limited
system drops an argument at every user disagree-
ment, making the user believe that the disagreement
was understood. The full system tries to defend the
argument; if possible with a contextually tailored
support, however, if this is not available, it may use a
generic support, making the user believe he was not
fully understood.
Our interpretation of the fact that the discrepancy
between user self evaluation of the interaction with
the system and the measured persuasion is that, even
if the full system is more argumentative, the user
</bodyText>
<page confidence="0.99589">
144
</page>
<bodyText confidence="0.999899">
didn’t feel coerced6. These results show that a more
persuasive dialogue can be achieved without deteri-
orating the user perception of the interaction.
</bodyText>
<sectionHeader confidence="0.99844" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999992230769231">
Our dialogue management system introduces a
novel approach to dialogue management by using
a layered model mixing the advantages of state-of-
the-art dialogue management approaches. A plan-
ning component tailored to the task of argumenta-
tion and persuasion searches the ideal path in an ar-
gumentation model to persuade the user. To give a
reactive and natural feel to the dialogue, this task-
oriented layer is extended by a reactive component
inspired from the chatbot dialogue management ap-
proach. The Desert Scenario evaluation, providing
a simple and independent metric for the persuasive-
ness of the dialogue system provided a good proto-
col for the evaluation of the dialogue system. This
one showed to be 18% more persuasive than a purely
task-oriented system that was not able to react to the
user interaction as smoothly.
Our current research on the dialogue management
system consists in developing another evaluation do-
main where a more complex utterance generation
can be used. This will allow going further than the
simple template based system, offering more diverse
answers to the user and avoiding repetitions; it will
also allow us to experiment textual persuasion tai-
lored to other parameters of the user representation,
such as the user personality.
</bodyText>
<sectionHeader confidence="0.999001" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.964873263157895">
Allen, J. F., G. Ferguson, B. W. Miller, E. K. Ringger, and
T. Sikorski. 2000. Dialogue Systems: From Theory to
Practice in TRAINS-96, chapter 14.
Bickmore, T. and T. Giorgino. 2004. Some novel aspects
of health communication from a dialogue systems per-
spective. In AAAI Fall Symposium.
Bickmore, T. W. and R. W. Picard. 2005. Establishing and
maintaining long-term human-computer relationships.
ACM Trans. Comput.-Hum. Interact., 12(2):293–327.
Carenini, G. and J. Moore. 2000. A strategy for generat-
ing evaluative arguments. In International Conference
on Natural Language Generation.
Dung, P. M. 1995. On the acceptability of arguments
and its fundamental role in nonmonotonic reasoning,
6The answers to the not coercive question do not show any
significant difference in the perception of coercion of the two
system.
logic programming and n-person games. Artif. Intell.,
77(2):321–357.
Farzanfar, R., S. Frishkopf, J. Migneault, and R. Fried-
man. 2005. Telephone-linked care for physical ac-
tivity: a qualitative evaluation of the use patterns of
an information technology program for patients. J. of
Biomedical Informatics, 38(3):220–228.
Festinger, Leon. 1957. A Theory of Cognitive Disso-
nance. Stanford University Press.
Freedman, R. 2000. Plan-based dialogue management in
a physics tutor. In Proceedings ofANLP ’00.
Galley, M., K. Mckeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: use of bayesian networks to model
pragmatic dependencies. In Proceedings ofACL’04.
Green, N. and J. F. Lehman. 2002. An integrated dis-
course recipe-based model for task-oriented dialogue.
Discourse Processes, 33(2):133–158.
Guerini, M., O. Stock, and M. Zancanaro. 2004. Per-
suasive strategies and rhetorical relation selection. In
Proceedings of ECAI-CMNA.
Hillard, D., M. Ostendorf, and E. Shriberg. 2003. Detec-
tion of agreement vs. disagreement in meetings: train-
ing with unlabeled data. In Proceedings ofNAACL’03.
Janin, A., D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting corpus.
In Proceedings ofICASSP’03.
Kendall, M. G. 1938. A new measure of rank correlation.
Biometrika, 30(1/2):81–93.
Klein, J., Y. Moon, and R. W. Picard. 1999. This com-
puter responds to user frustration. In CHI’99.
Lafferty, J. C., Eady, and J. Elmers. 1974. The desert
survival problem.
Levy, D., R. Catizone, B. Battacharia, A. Krotov, and
Y. Wilks. 1997. Converse:a conversational compan-
ion. In Proceedings of 1st International Workshop on
Human-Computer Conversation.
Mazzotta, I., F. de Rosis, and V. Carofiglio. 2007. Por-
tia: A user-adapted persuasion system in the healthy-
eating domain. Intelligent Systems, IEEE, 22(6).
Moon, Y. 1998. The effects of distance in local versus
remote human-computer interaction. In Proceedings
of SIGCHI’98.
Norman, Timothy J. and Chris Reed. 2003. Argumenta-
tion Machines: New Frontiers in Argument and Com-
putation (Argumentation Library). Springer.
Reed, C. 1998. Generating Arguments in Natural Lan-
guage. Ph.D. thesis, University College London.
Reiter, E., R. Robertson, and L. M. Osman. 2003.
Lessons from a failure: generating tailored smoking
cessation letters. Artif. Intell., 144(1-2):41–58.
Stiff, J. B. and P. A. Mongeau. 2002. Persuasive Commu-
nication, second edition.
Vapnik, V. N. 2000. The Nature of Statistical Learning
Theory.
Zinn, C., J. D. Moore, and M. G. Core. 2002. A 3-tier
planning architecture for managing tutorial dialogue.
In Proceedings ofITS ’02.
</reference>
<page confidence="0.998684">
145
</page>
<table confidence="0.9040075">
A Agreement/Disagreement Classification
Setup 1 Setup 2
Galley et al., global features 86.92% 84.07%
Galley et al., local features 85.62% 83.11%
Hillard et al. 82% NA
SVM 86.47% 83.17%
</table>
<tableCaption confidence="0.9944305">
Table 1: Accuracy of different agreement/disagreement
classification approaches.
</tableCaption>
<subsectionHeader confidence="0.391113">
The accuracy of state-of-the-art techniques
</subsectionHeader>
<bodyText confidence="0.9340512">
(Hillard, Ostendorf, and Shriberg (2003) and
Galley et al. (2004)) are reported in Table 1 and
compared to our SVM classifier. Two experimental
setups were used:
Setup 1 reproduces Hillard, Ostendorf, and Shriberg
(2003) training/testing split between meetings;
Setup 2 reproduces the N-Fold, 4 ways split used by
Galley et al. (2004).
The SVM results are arguably lower than Galley et al.
system with labeled dependencies. However, this is be-
cause our system only relies on local features of each
utterance, while Galley et al. (2004) use global features
(i.e. features describing relations between consecutive ut-
terances) suggest that adding global features would also
improve the SVM classifier.
</bodyText>
<subsectionHeader confidence="0.525037">
B Evaluation Questionnaire
</subsectionHeader>
<bodyText confidence="0.999128857142857">
In the evaluation described in section 6, the participants
were asked to give their level of agreement with each
statement on the scale: Strongly disagree (0), Disagree
(1), Neither agree nor disagree (2), Agree (3), Strongly
Agree(4). Table 2 provides a list of questions with the
average agreement level and the result of a paired t-test
between the two system results.
</bodyText>
<page confidence="0.960728">
146
</page>
<bodyText confidence="0.999599470588235">
label question full system limited system ttest
interpret “In the conversation, the other user inter- 1.73 2.13 0.06
preted correctly what you said”
perceived persuasion “In the conversation, the other user was 2.47 2.53 0.44
persuasive”
not coercive “The other user was not forceful in 2.4 2.73 0.15
changing your opinion”
sluggish “The other user was sluggish and slow to 1.27 1.27 0.5
reply to you in this conversation”
understand “The other user was easy to understand 3.2 3.13 0.4
in the conversation”
pace “The pace of interaction with the other 2.73 3.07 0.1
user was appropriate in this conversa-
tion”
friendliness ”The other user was friendly” 2.93 2.87 0.4
length length of the dialogue 12min 19s 08min 33s 0.07
persuasiveness persuasiveness 1.33 -0.47 0.05
</bodyText>
<tableCaption confidence="0.994875">
Table 2: Results from the evaluation questionnaire.
</tableCaption>
<page confidence="0.996163">
147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.770612">
<title confidence="0.9976">Argumentative Human Computer Dialogue for Automated Persuasion</title>
<author confidence="0.996782">Andrews Manandhar De_Boni</author>
<affiliation confidence="0.998327">Department of Computer University of</affiliation>
<address confidence="0.81447">York YO10</address>
<abstract confidence="0.995602363636364">Argumentation is an emerging topic in the field of human computer dialogue. In this paper we describe a novel approach to dialogue management that has been developed to achieve persuasion using a textual argumentation dialogue system. The paper introduces a layered management architecture that mixes task-oriented dialogue techniques with chatbot techniques to achieve better persuasiveness in the dialogue.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>G Ferguson</author>
<author>B W Miller</author>
<author>E K Ringger</author>
<author>T Sikorski</author>
</authors>
<date>2000</date>
<note>Dialogue Systems: From Theory to Practice in TRAINS-96, chapter 14.</note>
<contexts>
<context position="984" citStr="Allen et al., 2000" startWordPosition="142" endWordPosition="145">agement that has been developed to achieve persuasion using a textual argumentation dialogue system. The paper introduces a layered management architecture that mixes task-oriented dialogue techniques with chatbot techniques to achieve better persuasiveness in the dialogue. 1 Introduction Human computer dialogue is a wide research area in Artificial Intelligence. Computer dialogue is now used at production stage for applications such as tutorial dialogue – that helps teaching students (Freedman, 2000) – task-oriented dialogue – that achieves a particular, limited task, such as booking a trip (Allen et al., 2000) – and chatbot dialogue (Levy et al., 1997) – that is used within entertainment and help systems. None of these approaches use persuasion as a mechanism to achieve dialogue goals. However, research towards the use of persuasion in Human Computer Interactions has spawned around the field of natural argumentation (Norman and Reed, 2003). Similarly research on Embodied Conversational Agents (ECA) (Bickmore and Picard, 2005) is also attempting to improve the persuasiveness of agents with persuasion techniques; however, it concentrates on the visual representation of the interlocutor rather than th</context>
</contexts>
<marker>Allen, Ferguson, Miller, Ringger, Sikorski, 2000</marker>
<rawString>Allen, J. F., G. Ferguson, B. W. Miller, E. K. Ringger, and T. Sikorski. 2000. Dialogue Systems: From Theory to Practice in TRAINS-96, chapter 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bickmore</author>
<author>T Giorgino</author>
</authors>
<title>Some novel aspects of health communication from a dialogue systems perspective.</title>
<date>2004</date>
<booktitle>In AAAI Fall Symposium.</booktitle>
<contexts>
<context position="2911" citStr="Bickmore and Giorgino, 2004" startWordPosition="438" endWordPosition="441">ventually change their behaviour. The presented dialogue management system has been developed to work on known limitations of current dialogue systems: The impression of lack of control is an issue when the user is interacting with a purely task-oriented dialogue system (Farzanfar et al., 2005). The system follows a plan to achieve the particular task, and the user’s dialogue moves are dictated by the planner and the plan operators. The lack of empathy of computers is also a problem in human-computer interaction for applications such as health-care, where persuasive dialogue could be applied (Bickmore and Giorgino, 2004). The system does not respond to the user’s personal and emotional state, which sometimes lowers the user’s implication in the dialogue. However, existing research (Klein, Moon, and Picard, 1999) shows that a system that gives appropriate response to the user’s emotion can lower frustration. In human-human communication, these limitations reduce the effectiveness of persuasion 138 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 138–147, Columbus, June 2008. c�2008 Association for Computational Linguistics (Stiff and Mongeau, 2002). Even if the response towards the comp</context>
</contexts>
<marker>Bickmore, Giorgino, 2004</marker>
<rawString>Bickmore, T. and T. Giorgino. 2004. Some novel aspects of health communication from a dialogue systems perspective. In AAAI Fall Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T W Bickmore</author>
<author>R W Picard</author>
</authors>
<title>Establishing and maintaining long-term human-computer relationships.</title>
<date>2005</date>
<journal>ACM Trans. Comput.-Hum. Interact.,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="1408" citStr="Bickmore and Picard, 2005" startWordPosition="209" endWordPosition="212">or applications such as tutorial dialogue – that helps teaching students (Freedman, 2000) – task-oriented dialogue – that achieves a particular, limited task, such as booking a trip (Allen et al., 2000) – and chatbot dialogue (Levy et al., 1997) – that is used within entertainment and help systems. None of these approaches use persuasion as a mechanism to achieve dialogue goals. However, research towards the use of persuasion in Human Computer Interactions has spawned around the field of natural argumentation (Norman and Reed, 2003). Similarly research on Embodied Conversational Agents (ECA) (Bickmore and Picard, 2005) is also attempting to improve the persuasiveness of agents with persuasion techniques; however, it concentrates on the visual representation of the interlocutor rather than the dialogue management. Previous research on human computer ** Unilever Corporate Research Bedford MK44 1LQ UK Marco.De-Boni@unilever.com dialogue has rarely focused on persuasive techniques (Guerini, Stock, and Zancanaro, 2004, initiated some research in that field). Our dialogue management system applies a novel method, taking advantage of persuasive and argumentation techniques to achieve persuasive dialogue. According</context>
<context position="22154" citStr="Bickmore and Picard, 2005" startWordPosition="3617" endWordPosition="3620">s is a tedious and slow process that was iteratively achieved with Wizard of OZ experiments with real users. In these experiments, users were told they were going to have a dialogue with another human in the Desert Scenario context. The dialogue system manages the whole dialogue, except for the generation phase that is mediated by an expert that can either choose the reaction of the system from an existing set of utterances, or type a new one. 5 Persuasiveness Metric Evaluating a behavior change would require a longterm observation of the behavior that would be dependent to external elements (Bickmore and Picard, 2005). To evaluate our system, an evaluation protocol measuring the change in the beliefs underlying the behavior was chosen. As explained in Section 3, the Desert Scenario is used as a base for the evaluation. Each participant is told that he is stranded in the desert. The user gives a preferred initial ranking RZ of the items (knife, compass, map, etc.). The user then engages in a dialogue with the system. The system then attempts to change the user’s ranking to a different ranking R3 through persuasive dialogue. At the end of the dialogue, the user can change this 143 choice to arrive at a final</context>
</contexts>
<marker>Bickmore, Picard, 2005</marker>
<rawString>Bickmore, T. W. and R. W. Picard. 2005. Establishing and maintaining long-term human-computer relationships. ACM Trans. Comput.-Hum. Interact., 12(2):293–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carenini</author>
<author>J Moore</author>
</authors>
<title>A strategy for generating evaluative arguments.</title>
<date>2000</date>
<booktitle>In International Conference on Natural Language Generation.</booktitle>
<contexts>
<context position="5600" citStr="Carenini and Moore (2000)" startWordPosition="856" endWordPosition="859">ion to the user input, generating one or more utterances for a given plan step, allowing for reactions to user’s counter arguments as well as backchannel and chitchat phases without cluttering the plan. Experimental results show that this layered approach allows the user to feel more comfortable in the dialogue while preserving the dialogue consistency provided by the planner. Eventually, this translates into a more persuasive dialogue (see Section 6). 2 Related Work Persuasion through dialogue is a novel field of Human Computer Interaction. Reiter, Robertson, and Osman (2003),Reed (1998) and Carenini and Moore (2000) apply persuasive communication principles to natural language generation, but only focus on monologue. The 3-tier planner for tutoring dialogue by Zinn, Moore, and Core (2002) provides a dialogue management technique close to our approach: a top-tier generates a dialogue plan, the middle-tier generates refinements to the plan and the bottom-tier generates utterances. Mazzotta, de Rosis, and Carofiglio (2007) also propose a planning framework for user-adapted persuasion where the plan operators are mapped to natural language (or ECA) generation. However, these planning approaches do not includ</context>
</contexts>
<marker>Carenini, Moore, 2000</marker>
<rawString>Carenini, G. and J. Moore. 2000. A strategy for generating evaluative arguments. In International Conference on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Dung</author>
</authors>
<title>On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, 6The answers to the not coercive question do not show any significant difference in the perception of coercion of the two system. logic programming and n-person games.</title>
<date>1995</date>
<journal>Artif. Intell.,</journal>
<volume>77</volume>
<issue>2</issue>
<contexts>
<context position="8493" citStr="Dung (1995)" startWordPosition="1326" endWordPosition="1327">n be proposed by the user or by the system. Figure 2 gives a simplified example of the Desert Scenario model. Figure 2: Argumentation Model Sample This model shows the different facts that are known by the system and the relations between them. Arrows represent the support relation between two facts. For example, rescue knows where you are is a support to the fact goal(signal) (the user goal is to signal presence to the rescue) as well as a support to goal(stay put) (the user goal is to stay close to the wreckage). This relational model is comparable to the argumentation framework proposed by Dung (1995), but stores more information about each argument for reasoning within the planning and reactive component (see Section 4.2). Each fact in this model represents a belief to be introduced to the user. For example, when the dialogue tries to achieve the goal reorder(flashlight &gt; air map): the system wants the user to believe that the “flashlight” item should be ranked higher than the “air map” item. The argumentation model describes the argumentation process that is required to introduce this new belief: the system first has to make sure the user believes in rate lower(air map) and rate higher(f</context>
</contexts>
<marker>Dung, 1995</marker>
<rawString>Dung, P. M. 1995. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, 6The answers to the not coercive question do not show any significant difference in the perception of coercion of the two system. logic programming and n-person games. Artif. Intell., 77(2):321–357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Farzanfar</author>
<author>S Frishkopf</author>
<author>J Migneault</author>
<author>R Friedman</author>
</authors>
<title>Telephone-linked care for physical activity: a qualitative evaluation of the use patterns of an information technology program for patients.</title>
<date>2005</date>
<journal>J. of Biomedical Informatics,</journal>
<volume>38</volume>
<issue>3</issue>
<contexts>
<context position="2578" citStr="Farzanfar et al., 2005" startWordPosition="385" endWordPosition="388"> techniques to achieve persuasive dialogue. According to the cognitive dissonance theory (Festinger, 1957), people will try to minimise the discrepancy between their behaviour and their beliefs by integrating new beliefs or distorting existing ones. In this paper, we approach persuasion as a process shaping user’s beliefs to eventually change their behaviour. The presented dialogue management system has been developed to work on known limitations of current dialogue systems: The impression of lack of control is an issue when the user is interacting with a purely task-oriented dialogue system (Farzanfar et al., 2005). The system follows a plan to achieve the particular task, and the user’s dialogue moves are dictated by the planner and the plan operators. The lack of empathy of computers is also a problem in human-computer interaction for applications such as health-care, where persuasive dialogue could be applied (Bickmore and Giorgino, 2004). The system does not respond to the user’s personal and emotional state, which sometimes lowers the user’s implication in the dialogue. However, existing research (Klein, Moon, and Picard, 1999) shows that a system that gives appropriate response to the user’s emoti</context>
</contexts>
<marker>Farzanfar, Frishkopf, Migneault, Friedman, 2005</marker>
<rawString>Farzanfar, R., S. Frishkopf, J. Migneault, and R. Friedman. 2005. Telephone-linked care for physical activity: a qualitative evaluation of the use patterns of an information technology program for patients. J. of Biomedical Informatics, 38(3):220–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Festinger</author>
</authors>
<title>A Theory of Cognitive Dissonance.</title>
<date>1957</date>
<publisher>Stanford University Press.</publisher>
<contexts>
<context position="2061" citStr="Festinger, 1957" startWordPosition="304" endWordPosition="305">rsuasiveness of agents with persuasion techniques; however, it concentrates on the visual representation of the interlocutor rather than the dialogue management. Previous research on human computer ** Unilever Corporate Research Bedford MK44 1LQ UK Marco.De-Boni@unilever.com dialogue has rarely focused on persuasive techniques (Guerini, Stock, and Zancanaro, 2004, initiated some research in that field). Our dialogue management system applies a novel method, taking advantage of persuasive and argumentation techniques to achieve persuasive dialogue. According to the cognitive dissonance theory (Festinger, 1957), people will try to minimise the discrepancy between their behaviour and their beliefs by integrating new beliefs or distorting existing ones. In this paper, we approach persuasion as a process shaping user’s beliefs to eventually change their behaviour. The presented dialogue management system has been developed to work on known limitations of current dialogue systems: The impression of lack of control is an issue when the user is interacting with a purely task-oriented dialogue system (Farzanfar et al., 2005). The system follows a plan to achieve the particular task, and the user’s dialogue</context>
</contexts>
<marker>Festinger, 1957</marker>
<rawString>Festinger, Leon. 1957. A Theory of Cognitive Dissonance. Stanford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Freedman</author>
</authors>
<title>Plan-based dialogue management in a physics tutor.</title>
<date>2000</date>
<booktitle>In Proceedings ofANLP ’00.</booktitle>
<contexts>
<context position="871" citStr="Freedman, 2000" startWordPosition="124" endWordPosition="125">ing topic in the field of human computer dialogue. In this paper we describe a novel approach to dialogue management that has been developed to achieve persuasion using a textual argumentation dialogue system. The paper introduces a layered management architecture that mixes task-oriented dialogue techniques with chatbot techniques to achieve better persuasiveness in the dialogue. 1 Introduction Human computer dialogue is a wide research area in Artificial Intelligence. Computer dialogue is now used at production stage for applications such as tutorial dialogue – that helps teaching students (Freedman, 2000) – task-oriented dialogue – that achieves a particular, limited task, such as booking a trip (Allen et al., 2000) – and chatbot dialogue (Levy et al., 1997) – that is used within entertainment and help systems. None of these approaches use persuasion as a mechanism to achieve dialogue goals. However, research towards the use of persuasion in Human Computer Interactions has spawned around the field of natural argumentation (Norman and Reed, 2003). Similarly research on Embodied Conversational Agents (ECA) (Bickmore and Picard, 2005) is also attempting to improve the persuasiveness of agents wit</context>
</contexts>
<marker>Freedman, 2000</marker>
<rawString>Freedman, R. 2000. Plan-based dialogue management in a physics tutor. In Proceedings ofANLP ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K Mckeown</author>
<author>J Hirschberg</author>
<author>E Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: use of bayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL’04.</booktitle>
<marker>Galley, Mckeown, Hirschberg, Shriberg, 2004</marker>
<rawString>Galley, M., K. Mckeown, J. Hirschberg, and E. Shriberg. 2004. Identifying agreement and disagreement in conversational speech: use of bayesian networks to model pragmatic dependencies. In Proceedings ofACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Green</author>
<author>J F Lehman</author>
</authors>
<title>An integrated discourse recipe-based model for task-oriented dialogue.</title>
<date>2002</date>
<booktitle>Discourse Processes,</booktitle>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4717" citStr="Green and Lehman, 2002" startWordPosition="713" endWordPosition="716">nd the achievement of persuasive goals (see Section 4.2). Argumentative dialogue can be seen as a form of task-oriented dialogue where the system’s task is to persuade the user by presenting the arguments. Thus, the dialogue manager first uses a task-oriented dialogue methodology to create a dialogue plan that will determine the content of the dialogue. The planning component’s role is to guarantee the consistency of the dialogue and the achievement of the persuasive goals. In state-of-the-art task-oriented dialogue management systems, the planner provides instructions for a surface realizer (Green and Lehman, 2002), responsible of generating the utterance corresponding to the plan step. Our approach is different to allow more reactivity to the user and give a feeling of control over the dialogue. In this layered approach, the reactive component provides a direct reaction to the user input, generating one or more utterances for a given plan step, allowing for reactions to user’s counter arguments as well as backchannel and chitchat phases without cluttering the plan. Experimental results show that this layered approach allows the user to feel more comfortable in the dialogue while preserving the dialogue</context>
</contexts>
<marker>Green, Lehman, 2002</marker>
<rawString>Green, N. and J. F. Lehman. 2002. An integrated discourse recipe-based model for task-oriented dialogue. Discourse Processes, 33(2):133–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Guerini</author>
<author>O Stock</author>
<author>M Zancanaro</author>
</authors>
<title>Persuasive strategies and rhetorical relation selection.</title>
<date>2004</date>
<booktitle>In Proceedings of ECAI-CMNA.</booktitle>
<contexts>
<context position="1810" citStr="Guerini, Stock, and Zancanaro, 2004" startWordPosition="265" endWordPosition="269">search towards the use of persuasion in Human Computer Interactions has spawned around the field of natural argumentation (Norman and Reed, 2003). Similarly research on Embodied Conversational Agents (ECA) (Bickmore and Picard, 2005) is also attempting to improve the persuasiveness of agents with persuasion techniques; however, it concentrates on the visual representation of the interlocutor rather than the dialogue management. Previous research on human computer ** Unilever Corporate Research Bedford MK44 1LQ UK Marco.De-Boni@unilever.com dialogue has rarely focused on persuasive techniques (Guerini, Stock, and Zancanaro, 2004, initiated some research in that field). Our dialogue management system applies a novel method, taking advantage of persuasive and argumentation techniques to achieve persuasive dialogue. According to the cognitive dissonance theory (Festinger, 1957), people will try to minimise the discrepancy between their behaviour and their beliefs by integrating new beliefs or distorting existing ones. In this paper, we approach persuasion as a process shaping user’s beliefs to eventually change their behaviour. The presented dialogue management system has been developed to work on known limitations of c</context>
</contexts>
<marker>Guerini, Stock, Zancanaro, 2004</marker>
<rawString>Guerini, M., O. Stock, and M. Zancanaro. 2004. Persuasive strategies and rhetorical relation selection. In Proceedings of ECAI-CMNA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hillard</author>
<author>M Ostendorf</author>
<author>E Shriberg</author>
</authors>
<title>Detection of agreement vs. disagreement in meetings: training with unlabeled data.</title>
<date>2003</date>
<booktitle>In Proceedings ofNAACL’03.</booktitle>
<marker>Hillard, Ostendorf, Shriberg, 2003</marker>
<rawString>Hillard, D., M. Ostendorf, and E. Shriberg. 2003. Detection of agreement vs. disagreement in meetings: training with unlabeled data. In Proceedings ofNAACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The ICSI meeting corpus.</title>
<date>2003</date>
<booktitle>In Proceedings ofICASSP’03.</booktitle>
<contexts>
<context position="15896" citStr="Janin et al., 2003" startWordPosition="2557" endWordPosition="2560">ws where you are)) 4.3.2 Detecting user reactions The reactive component needs to detect if the user is agreeing to its current argument or resisting the new fact that is presented. Because the dialogue management system was developed from the perspective of a system that could be easily ported to different domains, choice was made to use a domain independent and robust agreement/disagreement detection. The agreement/disagreement detection is based on an utterance classifier. The classifier is a cascade of binary Support Vector Machines (SVM) (Vapnik, 2000) trained on the ICSI Meeting corpus (Janin et al., 2003). The corpus contains 8135 spurts3 annotated with agreement/disagreement information Hillard, Ostendorf, and Shriberg (2003). A multi-class SVM classifier is trained on local features of the spurts such as a) the length of the spurt, b) the first word of the spurt, c) the bigrams of the spurts, and d) part of speech tags. The classification achieves an accuracy of 83.17% with an N-Fold 4 ways split cross validation. Additional results and comparison with state-of-the-art are available in Appendix A. During the dialogue, the classifier is applied on each of the user’s utterances, trying to dete</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>Janin, A., D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The ICSI meeting corpus. In Proceedings ofICASSP’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Kendall</author>
</authors>
<title>A new measure of rank correlation.</title>
<date>1938</date>
<journal>Biometrika,</journal>
<pages>30--1</pages>
<contexts>
<context position="22952" citStr="Kendall, 1938" startWordPosition="3761" endWordPosition="3762">e for the evaluation. Each participant is told that he is stranded in the desert. The user gives a preferred initial ranking RZ of the items (knife, compass, map, etc.). The user then engages in a dialogue with the system. The system then attempts to change the user’s ranking to a different ranking R3 through persuasive dialogue. At the end of the dialogue, the user can change this 143 choice to arrive at a final ranking Rf. The persuasiveness of the dialogue can be measured as the evolution of the distance between the user ranking (Ri, Rf) and the system ranking (Rs). The Kendall T distance (Kendall, 1938) is used to compute the pairwise disagreement between two rankings. The change of the Kendall T distance during the dialogue gives an evaluation of the persuasiveness of the dialogue: Persuasiveness = KT(Ri, Rs) − KT(Rf, Rs). In the current evaluation protocol, the Rs is always the reverse of the Ri, so KT(Ri, Rs) is always the maximum distance possible: n�(n−1) 2 where n is the number of items to rank. The minimum Kendall tau distance is 0. If the system was persuasive enough to make the user invert the initial ranking, Persuasiveness of the system is maximum and equal to: n�(n−1) 2 . If the </context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>Kendall, M. G. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klein</author>
<author>Y Moon</author>
<author>R W Picard</author>
</authors>
<title>This computer responds to user frustration.</title>
<date>1999</date>
<booktitle>In CHI’99.</booktitle>
<contexts>
<context position="3105" citStr="Klein, Moon, and Picard, 1999" startWordPosition="467" endWordPosition="471">ssue when the user is interacting with a purely task-oriented dialogue system (Farzanfar et al., 2005). The system follows a plan to achieve the particular task, and the user’s dialogue moves are dictated by the planner and the plan operators. The lack of empathy of computers is also a problem in human-computer interaction for applications such as health-care, where persuasive dialogue could be applied (Bickmore and Giorgino, 2004). The system does not respond to the user’s personal and emotional state, which sometimes lowers the user’s implication in the dialogue. However, existing research (Klein, Moon, and Picard, 1999) shows that a system that gives appropriate response to the user’s emotion can lower frustration. In human-human communication, these limitations reduce the effectiveness of persuasion 138 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 138–147, Columbus, June 2008. c�2008 Association for Computational Linguistics (Stiff and Mongeau, 2002). Even if the response towards the computer is not always identical to the one to humans, it seems sensible to think that persuasive dialogue systems can be improved by applying known findings from human-human communication. The dial</context>
</contexts>
<marker>Klein, Moon, Picard, 1999</marker>
<rawString>Klein, J., Y. Moon, and R. W. Picard. 1999. This computer responds to user frustration. In CHI’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Lafferty</author>
<author>Eady</author>
<author>J Elmers</author>
</authors>
<title>The desert survival problem.</title>
<date>1974</date>
<contexts>
<context position="6804" citStr="Lafferty, Eady, and Elmers, 1974" startWordPosition="1036" endWordPosition="1040">ing approaches do not include a mechanism to react to user’s counter arguments that are difficult to plan beforehand. This paper propose a novel approach that could improve the user’s comfort in the dialogue as well as its persuasiveness. 3 Case Study Part of the problem in evaluating persuasive dialogue is using an effective evaluation framework. Moon (1998) uses the Desert Survival Scenario to evaluate the difference of persuasion and trust in interaction between humans when face-to-face or when mediated by a computer system (via an instant messaging platform). The Desert Survival Scenario (Lafferty, Eady, and Elmers, 1974) is a negotiation scenario used in team training. The team is put in a scenario where they are stranded in the desert after a plane crash. They have to negotiate a ranking of the most eligible items (knife, compass, map, etc.) that they should keep for their survival. For the evaluation of the dialogue system, a similar scenario is presented to the participants. The user has to choose an initial preferred ranking of items 139 and then engages in a discussion with the dialogue system that tries to persuade the user to change the ranking. At the end of the dialogue, the user has the opportunity</context>
</contexts>
<marker>Lafferty, Eady, Elmers, 1974</marker>
<rawString>Lafferty, J. C., Eady, and J. Elmers. 1974. The desert survival problem.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Levy</author>
<author>R Catizone</author>
<author>B Battacharia</author>
<author>A Krotov</author>
<author>Y Wilks</author>
</authors>
<title>Converse:a conversational companion.</title>
<date>1997</date>
<booktitle>In Proceedings of 1st International Workshop on Human-Computer Conversation.</booktitle>
<contexts>
<context position="1027" citStr="Levy et al., 1997" startWordPosition="150" endWordPosition="153">ersuasion using a textual argumentation dialogue system. The paper introduces a layered management architecture that mixes task-oriented dialogue techniques with chatbot techniques to achieve better persuasiveness in the dialogue. 1 Introduction Human computer dialogue is a wide research area in Artificial Intelligence. Computer dialogue is now used at production stage for applications such as tutorial dialogue – that helps teaching students (Freedman, 2000) – task-oriented dialogue – that achieves a particular, limited task, such as booking a trip (Allen et al., 2000) – and chatbot dialogue (Levy et al., 1997) – that is used within entertainment and help systems. None of these approaches use persuasion as a mechanism to achieve dialogue goals. However, research towards the use of persuasion in Human Computer Interactions has spawned around the field of natural argumentation (Norman and Reed, 2003). Similarly research on Embodied Conversational Agents (ECA) (Bickmore and Picard, 2005) is also attempting to improve the persuasiveness of agents with persuasion techniques; however, it concentrates on the visual representation of the interlocutor rather than the dialogue management. Previous research on</context>
</contexts>
<marker>Levy, Catizone, Battacharia, Krotov, Wilks, 1997</marker>
<rawString>Levy, D., R. Catizone, B. Battacharia, A. Krotov, and Y. Wilks. 1997. Converse:a conversational companion. In Proceedings of 1st International Workshop on Human-Computer Conversation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mazzotta</author>
<author>F de Rosis</author>
<author>V Carofiglio</author>
</authors>
<title>Portia: A user-adapted persuasion system in the healthyeating domain. Intelligent Systems,</title>
<date>2007</date>
<pages>22--6</pages>
<publisher>IEEE,</publisher>
<marker>Mazzotta, de Rosis, Carofiglio, 2007</marker>
<rawString>Mazzotta, I., F. de Rosis, and V. Carofiglio. 2007. Portia: A user-adapted persuasion system in the healthyeating domain. Intelligent Systems, IEEE, 22(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Moon</author>
</authors>
<title>The effects of distance in local versus remote human-computer interaction.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGCHI’98.</booktitle>
<contexts>
<context position="6533" citStr="Moon (1998)" startWordPosition="999" endWordPosition="1000">e plan and the bottom-tier generates utterances. Mazzotta, de Rosis, and Carofiglio (2007) also propose a planning framework for user-adapted persuasion where the plan operators are mapped to natural language (or ECA) generation. However, these planning approaches do not include a mechanism to react to user’s counter arguments that are difficult to plan beforehand. This paper propose a novel approach that could improve the user’s comfort in the dialogue as well as its persuasiveness. 3 Case Study Part of the problem in evaluating persuasive dialogue is using an effective evaluation framework. Moon (1998) uses the Desert Survival Scenario to evaluate the difference of persuasion and trust in interaction between humans when face-to-face or when mediated by a computer system (via an instant messaging platform). The Desert Survival Scenario (Lafferty, Eady, and Elmers, 1974) is a negotiation scenario used in team training. The team is put in a scenario where they are stranded in the desert after a plane crash. They have to negotiate a ranking of the most eligible items (knife, compass, map, etc.) that they should keep for their survival. For the evaluation of the dialogue system, a similar scenar</context>
</contexts>
<marker>Moon, 1998</marker>
<rawString>Moon, Y. 1998. The effects of distance in local versus remote human-computer interaction. In Proceedings of SIGCHI’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy J Norman</author>
<author>Chris Reed</author>
</authors>
<date>2003</date>
<booktitle>Argumentation Machines: New Frontiers in Argument and Computation (Argumentation Library).</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="1320" citStr="Norman and Reed, 2003" startWordPosition="197" endWordPosition="200">area in Artificial Intelligence. Computer dialogue is now used at production stage for applications such as tutorial dialogue – that helps teaching students (Freedman, 2000) – task-oriented dialogue – that achieves a particular, limited task, such as booking a trip (Allen et al., 2000) – and chatbot dialogue (Levy et al., 1997) – that is used within entertainment and help systems. None of these approaches use persuasion as a mechanism to achieve dialogue goals. However, research towards the use of persuasion in Human Computer Interactions has spawned around the field of natural argumentation (Norman and Reed, 2003). Similarly research on Embodied Conversational Agents (ECA) (Bickmore and Picard, 2005) is also attempting to improve the persuasiveness of agents with persuasion techniques; however, it concentrates on the visual representation of the interlocutor rather than the dialogue management. Previous research on human computer ** Unilever Corporate Research Bedford MK44 1LQ UK Marco.De-Boni@unilever.com dialogue has rarely focused on persuasive techniques (Guerini, Stock, and Zancanaro, 2004, initiated some research in that field). Our dialogue management system applies a novel method, taking advant</context>
</contexts>
<marker>Norman, Reed, 2003</marker>
<rawString>Norman, Timothy J. and Chris Reed. 2003. Argumentation Machines: New Frontiers in Argument and Computation (Argumentation Library). Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Reed</author>
</authors>
<title>Generating Arguments in Natural Language.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University College London.</institution>
<contexts>
<context position="5570" citStr="Reed (1998)" startWordPosition="853" endWordPosition="854">s a direct reaction to the user input, generating one or more utterances for a given plan step, allowing for reactions to user’s counter arguments as well as backchannel and chitchat phases without cluttering the plan. Experimental results show that this layered approach allows the user to feel more comfortable in the dialogue while preserving the dialogue consistency provided by the planner. Eventually, this translates into a more persuasive dialogue (see Section 6). 2 Related Work Persuasion through dialogue is a novel field of Human Computer Interaction. Reiter, Robertson, and Osman (2003),Reed (1998) and Carenini and Moore (2000) apply persuasive communication principles to natural language generation, but only focus on monologue. The 3-tier planner for tutoring dialogue by Zinn, Moore, and Core (2002) provides a dialogue management technique close to our approach: a top-tier generates a dialogue plan, the middle-tier generates refinements to the plan and the bottom-tier generates utterances. Mazzotta, de Rosis, and Carofiglio (2007) also propose a planning framework for user-adapted persuasion where the plan operators are mapped to natural language (or ECA) generation. However, these pla</context>
</contexts>
<marker>Reed, 1998</marker>
<rawString>Reed, C. 1998. Generating Arguments in Natural Language. Ph.D. thesis, University College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Robertson</author>
<author>L M Osman</author>
</authors>
<title>Lessons from a failure: generating tailored smoking cessation letters.</title>
<date>2003</date>
<journal>Artif. Intell.,</journal>
<pages>144--1</pages>
<marker>Reiter, Robertson, Osman, 2003</marker>
<rawString>Reiter, E., R. Robertson, and L. M. Osman. 2003. Lessons from a failure: generating tailored smoking cessation letters. Artif. Intell., 144(1-2):41–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Stiff</author>
<author>P A Mongeau</author>
</authors>
<date>2002</date>
<note>Persuasive Communication, second edition.</note>
<contexts>
<context position="3472" citStr="Stiff and Mongeau, 2002" startWordPosition="519" endWordPosition="522">suasive dialogue could be applied (Bickmore and Giorgino, 2004). The system does not respond to the user’s personal and emotional state, which sometimes lowers the user’s implication in the dialogue. However, existing research (Klein, Moon, and Picard, 1999) shows that a system that gives appropriate response to the user’s emotion can lower frustration. In human-human communication, these limitations reduce the effectiveness of persuasion 138 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 138–147, Columbus, June 2008. c�2008 Association for Computational Linguistics (Stiff and Mongeau, 2002). Even if the response towards the computer is not always identical to the one to humans, it seems sensible to think that persuasive dialogue systems can be improved by applying known findings from human-human communication. The dialogue management architecture described in this paper (see Figure 1) addresses these dialogue management issues by using a novel layered approach to dialogue management, allowing the mixing of techniques from task-oriented dialogue management and chatbot techniques (see Section 4). Figure 1: Layered Management Architecture The use of a planner guarantees the consist</context>
</contexts>
<marker>Stiff, Mongeau, 2002</marker>
<rawString>Stiff, J. B. and P. A. Mongeau. 2002. Persuasive Communication, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<date>2000</date>
<journal>The Nature of Statistical Learning Theory.</journal>
<contexts>
<context position="15840" citStr="Vapnik, 2000" startWordPosition="2548" endWordPosition="2549">t you think? (from plan step: use world(rescue knows where you are)) 4.3.2 Detecting user reactions The reactive component needs to detect if the user is agreeing to its current argument or resisting the new fact that is presented. Because the dialogue management system was developed from the perspective of a system that could be easily ported to different domains, choice was made to use a domain independent and robust agreement/disagreement detection. The agreement/disagreement detection is based on an utterance classifier. The classifier is a cascade of binary Support Vector Machines (SVM) (Vapnik, 2000) trained on the ICSI Meeting corpus (Janin et al., 2003). The corpus contains 8135 spurts3 annotated with agreement/disagreement information Hillard, Ostendorf, and Shriberg (2003). A multi-class SVM classifier is trained on local features of the spurts such as a) the length of the spurt, b) the first word of the spurt, c) the bigrams of the spurts, and d) part of speech tags. The classification achieves an accuracy of 83.17% with an N-Fold 4 ways split cross validation. Additional results and comparison with state-of-the-art are available in Appendix A. During the dialogue, the classifier is </context>
</contexts>
<marker>Vapnik, 2000</marker>
<rawString>Vapnik, V. N. 2000. The Nature of Statistical Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zinn</author>
<author>J D Moore</author>
<author>M G Core</author>
</authors>
<title>A 3-tier planning architecture for managing tutorial dialogue.</title>
<date>2002</date>
<booktitle>In Proceedings ofITS ’02.</booktitle>
<marker>Zinn, Moore, Core, 2002</marker>
<rawString>Zinn, C., J. D. Moore, and M. G. Core. 2002. A 3-tier planning architecture for managing tutorial dialogue. In Proceedings ofITS ’02.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>