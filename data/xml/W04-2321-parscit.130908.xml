<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019049">
<title confidence="0.981524">
On the use of confidence for statistical decision in dialogue strategies
</title>
<author confidence="0.999304">
Christian Raymond1 Fr´ed´eric B´echet1 Renato de Mori1 G´eraldine Damnati2
</author>
<affiliation confidence="0.978495">
1 LIA/CNRS, University of Avignon France 2 France Telecom R&amp;D, Lannion, France
</affiliation>
<email confidence="0.988652">
christian.raymond,frederic.bechet,renato.demori@lia.univ-avignon.fr
geraldine.damnati@rd.francetelecom.com
</email>
<sectionHeader confidence="0.995574" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999994105263158">
This paper describes an interpretation and deci-
sion strategy that minimizes interpretation er-
rors and perform dialogue actions which may
not depend on the hypothesized concepts only,
but also on confidence of what has been rec-
ognized. The concepts introduced here are ap-
plied in a system which integrates language
and interpretation models into Stochastic Finite
State Transducers (SFST). Furthermore, acous-
tic, linguistic and semantic confidence mea-
sures on the hypothesized word sequences are
made available to the dialogue strategy. By
evaluating predicates related to these confi-
dence measures, a decision tree automatically
learn a decision strategy for rescoring a n-best
list of candidates representing a user’s utter-
ance. The different actions that can be then per-
formed are chosen according to the confidence
scores given by the tree.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992791913043478">
There is a wide consensus in the scientific community
that human-computer dialogue systems based on spoken
natural language make mistakes because the Automatic
Speech Recognition (ASR) component may not hypothe-
size some of the pronounced words and the various levels
of knowledge used for recognizing and reasoning about
conceptual entities are imprecise and incomplete. In spite
of these problems, it is possible to make useful applica-
tions with dialogue systems using spoken input if suit-
able interpretation and decision strategies are conceived
that minimize interpretation errors and perform dialogue
actions which may not depend on the hypothesized con-
cepts only, but also on confidence of what has been rec-
ognized.
This paper introduces some concepts developed for
telephone applications in the framework of stochastic
models for interpretation and dialogue strategies, a good
overview of which can be found in (Young, 2002).
The concepts introduced here are applied in a system
which integrates language and interpretation models into
Stochastic Finite State Transducers (SFST). Furthermore,
acoustic, linguistic and semantic confidence measures on
the hypothesized word sequences are made available to
the dialogue strategy. A new way of using them in the
dialogue decision process is proposed in this paper.
Most of the Spoken language Understanding Systems
(SLU) use semantic grammars with semantic tags as non-
terminals (He and Young, 2003) with rules for rewriting
them into strings of words.
The SFSTs of the system used for the experiments de-
scribed here, represent knowledge for the basic building
blocks of a frame-based semantic grammar. Each block
represents a property/value relation. Different SFSTs
may share words in the same sentence. Property/value
hypotheses are generated with an approach described in
(Raymond et al., 2003) and are combined into a sentence
interpretation hypothesis in which the same word may
contribute to more than one property/value pair. The di-
alogue strategy has to evaluate the probability that each
component of each pair has been correctly hypothesized
in order to decide to perform an action that minimizes the
risk of user dissatisfaction.
2 Overview of the decoding process
The starting point for decoding is a lattice of word
hypotheses generated with an n-gram language model
(LM). Decoding is a search process which detects com-
binations of specialized SFSTs and the n-gram LM. The
output of the decoding process consists of a n-best list
of conceptual interpretations P. An interpretation P is
a set of property/value pairs sj = (cj, vj) called con-
cepts. cj is the concept tag and vj is the concept value
of sj. Each concept tag cj is represented by a SFST and
can be related either to the dialogue application (phone
number, date, location expression, etc.) or to the dia-
logue management (confirmation, contestation, etc.). To
each string of words recognized by a given SFST cj is
associated a value vj representing a normalized value
for the concept detected. For example, to the word
phrase: on July the fourteenth, detected by a
SFST dedicated to process dates, is associated the value:
????/07/14.
The n-best list of interpretations output by the decod-
ing process is structured according to the different con-
cept tag strings that can be found in the word lattice. To
each concept tag string is attached another n-best list on
the concept values. This whole n-best is called a struc-
tured n-best. After presenting the statistical model used
in this study, we will describe the implementation of this
decoding process.
</bodyText>
<sectionHeader confidence="0.997413" genericHeader="method">
3 Statistical model
</sectionHeader>
<bodyText confidence="0.9854385">
The contribution of a sequence of words W to a concep-
tual structure r is evaluated by the posterior probability
P(r  |Y ), where Y is the description of acoustic features.
Such a probability is computed as follows:
</bodyText>
<equation confidence="0.995062666666667">
EWESW P(Y  |W)P(  |W)P(W)
P(  |Y ) = (1)
EWESW P(Y  |W)P(W)
</equation>
<bodyText confidence="0.962709166666667">
where P(Y  |W) is provided by the acoustic models,
P(W) is computed with the LM. Exponents S and A are
respectively a semantic and a syntactic fudge factor. SW
corresponds to the set of word strings that can be found in
the word lattice. P(r  |W) is computed by considering
that thus:
</bodyText>
<sectionHeader confidence="0.977377" genericHeader="method">
4 Structured N-best list
</sectionHeader>
<bodyText confidence="0.999932115384615">
N-best lists are generally produced by simply enumerat-
ing the n best paths in the word graphs produced by Au-
tomatic Speech Recognition (ASR) engines. The scores
used in such graphs are usually only a combination of
acoustic and language model scores, and no other linguis-
tic levels are involved. When an n-best word hypothesis
list is generated, the differences between the hypothesis
i and the hypothesis i+1 are often very small, made of
only one or a few words. This phenomenon is aggravated
when the ASR word graph contains a low confidence
area, due for example to an Out-Of-Vocabulary word, to
a noisy input or to a speech disfluency.
This is the main weakness of this approach in a Spoken
Dialogue context: not all words are important to the Di-
alogue Manager, and all the n-best word hypotheses that
differ only between each other because of some speech
disfluency effects can be considered as equals. That’s
why it is important to generate not only a n-best list of
word hypotheses but rather a n-best list of interpretations,
each of them corresponding to a different meaning from
the Dialogue Manager point of view.
We propose here a method for directly extracting such
a structured n-best from a word lattice output by an ASR
engine. This method relies on operations between Finite
State Machines and is implemented thanks to the AT&amp;T
FSM toolkit (see (Mohri et al., 2002) for more details).
</bodyText>
<subsectionHeader confidence="0.997289">
4.1 Word-to-Concept transducer
</subsectionHeader>
<bodyText confidence="0.99957975">
Each concept ck of the dialogue application is associated
with an FSM. These FMSs are called acceptors (Ak for
the concept ck). In order to process strings of words that
don’t belong to any concept, a filler model, called AF
is used. Because the same string of words can’t belong
to both a concept model and the background text, all the
paths contained in the acceptors Ak are removed from the
filler model AF in the following way:
</bodyText>
<equation confidence="0.999882363636364">
P(  |W) = P(s1  |W).
Ak
�J
j=2
Um
k=1
P(sj  |sj−1
1 W) (2)
P(sj  |sj−1
1 W)  P(sj  |W)
AF = E *
</equation>
<bodyText confidence="0.999584">
If the conceptual component sj is hypothesized with
a sentence pattern irj(W) recognized in W and irk(W)
triggers a pair sk and there is a training set with which
the probabilities P(irk(W)  |sk) dk, can be estimated,
then the posterior probability can be obtained as follows:
</bodyText>
<equation confidence="0.9989725">
P s W — P(Irj (W)  |sj)P(sj) (3)
(� I ) — k=1P(Irk(W)  |sk)P(sk)
</equation>
<bodyText confidence="0.99751025">
where P(sk) is a unigram probability of conceptual
components.
where E is the word lexicon of the application and m
is the number of concepts used.
All these acceptors are now turned into transducers
that take words as input symbols and start or end con-
cept tags as output symbols. Indeed, all acceptors Ak be-
come transducers Tk where the first transition emits the
symbol &lt;Ck&gt; and the last transition the symbol &lt;/Ck&gt;.
Similarly the filler model becomes the transducer Tbk
which emits the symbols &lt;BCK&gt; and &lt;/BCK&gt;. Except
these start and end tags, no other symbols are emitted: all
words in the concept or background transducers emit an
empty symbol.
Finally all these transducers are linked together in a
single model called Tconcept as presented in figure 1.
</bodyText>
<figureCaption confidence="0.996017">
Figure 1: Word-to-Concept Transducer
</figureCaption>
<subsectionHeader confidence="0.992524">
4.2 Processing the ASR word lattice
</subsectionHeader>
<bodyText confidence="0.9932289375">
The ASR word lattice is coded by an FSM: an acceptor L
where each transition emits a word. The cost function for
a transition corresponds to the acoustic score of the word
emitted.
The first step in the word lattice processing consists
of rescoring each transition of L by means of a 3-gram
Language Model (LM) in order to obtain the probabili-
ties P(W) of equation 1. This is done by composing the
word lattice with a 3-gram LM also coded as an FSM (see
(Allauzen et al., 2003) for more details about statistical
LMs and FSMs).
The resulting FSM is then composed with the trans-
ducer TConcept in order to obtain the word-to-concept
transducer L&apos;. A path in L&apos; corresponds to a word string
if only the input symbols of the transducer are considered
and its score is the one expressed by equation 1; simi-
larly by considering only the output symbols, a path in L&apos;
corresponds to a concept tag string.
The structured n-best list is directly obtained from L&apos;:
by extracting the n-best concept tag strings (output label
paths) we obtain an n-best list on the conceptual interpre-
tations. The score of each conceptual interpretation is the
sum of all the word strings (input label paths) in the word
lattice producing the same interpretation.
Finally, for every conceptual interpretations C kept at
the previous step, a local n-best list on the word strings is
calculated by selecting in L&apos; the best paths outputting the
string C.
The resulting structured n-best is illustrated by the fol-
lowing example. If we keep the 2 best conceptual in-
terpretations C1, C2 of a transducer L&apos; and, for each of
these, the 2 best word strings, we obtain:
</bodyText>
<equation confidence="0.999933666666667">
1 : C1 = &lt;c1_1,c1_2,..,c1_x&gt;
1.1 : W1.1 = &lt;v1.1_1,v1.1_2,..,v1.1_x&gt;
1.2 : W1.2 = &lt;v1.2_1,v1.2_2,..,v1.2_x&gt;
2 : C2 = &lt;c2_1,c2_2,..,c2_y&gt;
2.1 : W2.1 = &lt;v2.1_1,v2.1_2,..,v2.1_y&gt;
2.2 : W2.2 = &lt;v2.2_1,v2.2_2,..,v2.2_y&gt;
</equation>
<bodyText confidence="0.9837698">
where &lt;ci_1,ci_2,..,ci_y&gt; is the conceptual
interpretation at the rank i in the n-best list; Wi.j is the
word string ranked j of interpretation i; and vi.j_k
is the concept value of the kth concept ci_k of the jth
word string of interpretation i.
</bodyText>
<sectionHeader confidence="0.799675" genericHeader="method">
5 Use of correctness probabilities
</sectionHeader>
<bodyText confidence="0.999572375">
In order to select a particular interpretation P (concep-
tual interpretation + concept values) from the structured
n-best list, we are now interested in computing the proba-
bility that P is correct, given a set of confidence measures
M: P(P I M). The choice of the confidence measures
determines the quality of the decision strategy. Those
used in this study are briefly presented in the next sec-
tions.
</bodyText>
<subsectionHeader confidence="0.919616">
5.1 Confidence measures
</subsectionHeader>
<subsubsectionHeader confidence="0.46361">
5.1.1 Acoustic confidence measure (AC)
</subsubsectionHeader>
<bodyText confidence="0.999534222222222">
This confidence measure relies on the comparison of
the acoustic likelihood provided by the speech recogni-
tion model for a given hypothesis to the one that would
be provided by a totally unconstrained phoneme loop
model. In order to be consistent with the general model,
the acoustic units are kept identical and the loop is over
context dependent phonemes. This confidence measure
is used at the utterance level and at the concept level (see
(Raymond et al., 2003) for more details).
</bodyText>
<subsubsectionHeader confidence="0.857799">
5.1.2 Linguistic confidence measure (LC)
</subsubsectionHeader>
<bodyText confidence="0.9999759">
In order to assess the impact of the absence of ob-
served trigrams as a potential cause of recognition errors,
a Language Model consistency measure is introduced.
This measure is simply, for a given word string candi-
date, the ratio between the number of trigrams observed
in the training corpus of the Language Model vs. the total
number of trigrams in the same word string. Its computa-
tion is very fast and the confidence scores obtained from
it give interesting results as presented in (Est`eve et al.,
2003).
</bodyText>
<subsubsectionHeader confidence="0.920294">
5.1.3 Semantic confidence measure (SC)
</subsubsectionHeader>
<bodyText confidence="0.993019166666667">
Several studies have shown that text classification tools
(like Support Vector Machines or Boosting algorithms)
can be an efficient way of labeling an utterance transcrip-
tion with a semantic label such as a call-type (Haffner et
al., 2003) in a Spoken Dialogue context. In our case, the
semantic labels attached to an utterance are the different
</bodyText>
<figure confidence="0.998377222222222">
in=&lt;start&gt;
out=&lt;&gt;
FILLER
in=&lt;&gt; in=&lt;&gt;
out=&lt;/BCK&gt;
in=&lt;&gt;
out=&lt;&gt;
SFST C1
out=&lt;BCK&gt;
in=&lt;&gt;
out=&lt;&gt;
in=&lt;end&gt;
out=&lt;&gt;
in=w1
out=&lt;&gt;
in=&lt;end
out=&lt;&gt;
out=&lt;&gt;
in=&lt;start&gt;
in=&lt;&gt;
out=&lt;C1&gt; out=&lt;/C1&gt;
in=&lt;&gt;
in=w2
out=&lt;&gt;
out=&lt;C2&gt;
in=&lt;&gt;
SFST C2
out=&lt;/C2&gt;
in=&lt;&gt;
in=&lt;&gt;
out=&lt;Cn&gt;
SFST Cn
in=&lt;&gt;
out=&lt;/Cn&gt;
in=&lt;&gt;
out=&lt;&gt;
</figure>
<bodyText confidence="0.999823705882353">
concepts handled by the Dialogue Manager. One classi-
fier is trained for each concept tag in the following way:
Each utterance of a training corpus is labeled with a
tag, manually checked, indicating if a given concept oc-
curs or not in the utterance. In order to let the classi-
fier model the context of occurrence of a concept rather
than its value we removed most of the concept headwords
from the list of criterion used by the classifier.
During the decision process, if the interpretation eval-
uated contains 2 concepts c1 and c2, then the classifiers
corresponding to c1 and c2 are used to give to the utter-
ance a confidence score of containing these two concepts.
The text classifier used in the experimental section
is a decision-tree classifier based on the Semantic-
Classification-Trees introduced for the ATIS task
by (Kuhn and Mori, 1995) and used for semantic disam-
biguation in (B´echet et al., 2000).
</bodyText>
<subsubsectionHeader confidence="0.83742">
5.1.4 Rank confidence measure (R)
</subsubsectionHeader>
<bodyText confidence="0.9999358">
To the previous confidence measures we added the
rank of each candidate in its n-best. This rank contains
two numbers: the rank of the interpretation of the utter-
ance and the rank of the utterance among those having
the same interpretation.
</bodyText>
<subsectionHeader confidence="0.997575">
5.2 Decision Tree based strategy
</subsectionHeader>
<bodyText confidence="0.999991818181818">
As the dependencies of these measures are difficult to es-
tablish, their values are transformed into symbols by vec-
tor quantization (VQ) and conjunctions of these symbols
expressing relevant statistical dependencies are obtained
by a decision tree which is trained with a development
set of examples. At the leaves probabilities P(M|) are
obtained when  represents any correct hypothesis, the
case in which only the properties have been correctly rec-
ognized or both properties and values have errors. With
these probabilities we are now able to estimate P(  |M)
in the following way:
</bodyText>
<equation confidence="0.9975135">
1
P(  |M) = 1 + P�MJIrjP(&apos;) (4)
</equation>
<bodyText confidence="0.999592">
where &apos; indicates that the interpretation in question is
incorrect and P(M|&apos;) = 1 − P(M|).
</bodyText>
<sectionHeader confidence="0.973436" genericHeader="method">
6 From hypotheses to actions
</sectionHeader>
<bodyText confidence="0.999812414634146">
Once concepts have been hypothesized, a dialog system
has to decide what action to perform. Let A = aj be
the set of actions a system can perform. Some of them
can be requests for clarification or repetition. In partic-
ular, the system may request the repetition of the entire
utterance. Performing an action has a certain risk and the
decision about the action to perform has to be the one that
minimizes the risk of user dissatisfaction.
It is thus possible that some or all the hypothesized
components of a conceptual structure  do not corre-
spond to the user intention because the word sequence
W based on which the conceptual hypothesis has been
generated contains some errors. In particular, there are
requests for clarification or repetition which should be
performed right after the interpretation of an utterance in
order to reduce the stress of the user. It is important to
notice that actions consisting in requests for clarification
or repetition mostly depend on the probability that the in-
terpretation of an utterance is correct, rather than on the
utterance interpretation.
The decoding process described in section 2 provides
a number of hypotheses containing a variable number of
pairs sj = (cj, vj) based on the score expressed by equa-
tion 1.
P(  |M) is then computed for these hypotheses. The
results can be used to decide to accept an interpretation
or to formulate a clarification question which may imply
more hypotheses.
For simplification purpose, we are going to consider
here only two actions: accepting the hypothesis with the
higher P(  |M) or rejecting it. The risk associated to the
acceptation decision is called pfa and corresponds to the
cost of a false acceptation of an incorrect interpretation.
Similarly the risk associated to the rejection decision is
called pfT and corresponds to the cost of a false rejection
of a correct interpretation. In a spoken dialogue context,
pfa is supposed to be higher than pfT.
The choice of the action to perform is determined by
a threshold S on P(  |M). This threshold is tuned on
a development corpus by minimizing the total risk R ex-
pressed as follows:
</bodyText>
<equation confidence="0.956548666666667">
Nfa Nfr
R = pfa × + pfr × (5)
Ntota! Ntota!
</equation>
<bodyText confidence="0.998657761904762">
Nfa and NfT are the numbers of false acceptation and
false rejection decisions on the development corpus for a
given value of S. Ntotal is the total number of examples
available for tuning the strategy.
The final goal of the strategy is to make negligible Nfa
and the best set of confidence measures is the one that
minimizes NfT. In fact, the cost of these cases is lower
because the corresponding action has to be a request for
repetition.
Instead of simply discarding an utterance if P(  |M)
is below S, another strategy we are investigating consists
of estimating the probability that the conceptual interpre-
tation alone (without the concept values) is correct. This
probability can be estimated the same way as P(  |M)
and can be used to choose a third kind of actions: accept-
ing the conceptual meaning of an utterance but asking for
clarifications about the values of the concepts.
A final decision about the strategy to be adopted should
be based on statistics on system performance to be col-
lected and updated after deploying the system on the tele-
phone network.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999795">
7.1 Application domain
</subsectionHeader>
<bodyText confidence="0.988789333333333">
The application domain considered in this study is a
restaurant booking application developed at France Tele-
com R&amp;D. At the moment, we only consider in our strat-
egy the most frequent concepts related to the application
domain: PLACE, PRICE and FOOD TYPE. They can be
described as follows:
</bodyText>
<listItem confidence="0.998184333333333">
• PLACE: an expression related to a restaurant loca-
tion (eg. a restaurant near Bastille);
• PRICE: the price range of a restaurant (eg. less than
a hundred euros);
• FOOD TYPE: the kind of food requested by the
caller (eg. an Indian restaurant).
</listItem>
<bodyText confidence="0.999951">
These entities are expressed in the training corpus by
short sequences of words containing three kinds of to-
ken: head-words like Bastille, concept related words like
restaurant and modifier tokens like near.
A single value is associated to each concept entity
simply be adding together the head-words and some
modifier tokens. For example, the values associated to
the three contexts presented above are: Bastille ,
less+hundred+euros and indian.
In the results section a concept detected is considered a
success only if the tag exists in the reference corpus and if
both values are identical. It’s a binary decision process:
a concept can be considered as a false detection even if
the concept tag is correct and if the value is partially cor-
rect. The measure on the errors (insertion, substitution,
deletion) of these concept/value tokens is called in this
paper the Understanding Error Rate, by opposition to the
standard Word Error Rate measure where all words are
considered equals.
</bodyText>
<subsectionHeader confidence="0.965658">
7.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999988818181818">
Experiments were carried out on a dialogue corpus pro-
vided by France Telecom R&amp;D. The task has a vocabu-
lary of 2200 words. The language model used is made
of 44K words. For this study we selected utterances cor-
responding to answers to a prompt asking for the kind
of restaurant the users were looking for. This corpus has
been cut in two: a development corpus containing 511
utterances and a test corpus containing 419 utterances.
This development corpus has been used to train the deci-
sion tree presented in section 5.2. The Word Error Rate
on the test corpus is 22.7%.
</bodyText>
<subsectionHeader confidence="0.958815">
7.3 Evaluation of the rescoring strategy
</subsectionHeader>
<bodyText confidence="0.998322454545455">
Table 1 shows the results obtained with a rescoring strat-
egy that selects, from the structured n-best list, the hy-
pothesis with the highest P(r  |M). The baseline re-
sults are obtained with a standard maximum-likelihood
approach choosing the hypothesis maximizing the proba-
bility P(r  |Y ) of equation 1. No rejection is performed
in this experiment.
The size of the n-best lists was set to 12 items: the first
4 candidates of the first 3 interpretations in the structured
n-best list. The gain obtained after rescoring is very sig-
nificant and justify our 2-step approach that first extract
an n-best list of interpretations thanks to P(r  |Y ) and
then choose the one with the highest confidence accord-
ing to a large set of confidence measures M. This gain
can be compared to the one obtained on the Word Error
Rate measure: the WER drops from 21.6% to 20.7% af-
ter rescoring on the development corpus and from 22.7%
to 22.5% on the test corpus. It is clear here that the
WER measure is not an adequate measure in a Spoken
Dialogue context as a big reduction in the Understanding
Error Rate might have very little effect on the Word Error
Rate.
</bodyText>
<table confidence="0.986163">
Corpus baseline rescoring UER reduction %
Devt. 15.0 12.4 17.3%
Test 17.7 14.5 18%
</table>
<tableCaption confidence="0.906962">
Table 1: Understanding Error Rate results with and with-
out rescoring on structured n-best lists (n=12) (no rejec-
tion)
</tableCaption>
<subsectionHeader confidence="0.988583">
7.4 Evaluation of the decision strategy
</subsectionHeader>
<bodyText confidence="0.999931923076923">
In this experiment we evaluate the decision strategy con-
sisting of accepting or rejecting an hypothesis r thanks to
a threshold on the probability P(r  |M). Figure 2 shows
the curve UER vs. utterance rejection on the development
and test corpora. As we can see very significant improve-
ments can be achieved with very little utterance rejection.
For example, at a 5% utterance rejection operating point,
the UER on the development corpus drops from 15.0% to
8.6% (42.6% relative improvement) and from 17.7% to
11.4% (35.6% relative improvement).
By using equation 5 for finding the operating point
minimizing the risk fonction (with a cost pfa = 1.5 x
pfT) on the development corpus we obtain:
</bodyText>
<listItem confidence="0.988965333333333">
• on the development corpus: UER=6.5 utterance re-
jection=13.1
• on the test corpus: UER=9.6 utterance rejec-
</listItem>
<page confidence="0.884002">
tion=15.9
</page>
<figure confidence="0.999">
0 5 10 15 20
utterance rejection (%)
</figure>
<figureCaption confidence="0.9711645">
Figure 2: Understanding Error Rate vs. utterance rejec-
tion on the development and test corpora
</figureCaption>
<sectionHeader confidence="0.994245" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999965428571429">
This paper describes an interpretation and decision strat-
egy that minimizes interpretation errors and perform dia-
logue actions which may not depend on the hypothesized
concepts only, but also on confidence of what has been
recognized. The first step in the process consists of gen-
erating a structured n-best list of conceptual interpreta-
tions of an utterance. A set of confidence measures is
then used in order to rescore the n-best list thanks to a de-
cision tree approach. Significant gains in Understanding
Error Rate are achieved with this rescoring method (18%
relative improvement). The confidence score given by the
tree can also be used in a decision strategy about the ac-
tion to perform. By using this score, significant improve-
ments in UER can be achieved with very little utterance
rejection. For example, at a 5% utterance rejection op-
erating point, the UER on the development corpus drops
from 15.0% to 8.6% (42.6% relative improvement) and
from 17.7% to 11.4% (35.6% relative improvement). Fi-
nally the operating point for a deployed dialogue system
can be chosen by explicitly minimizing a risk function on
a development corpus.
</bodyText>
<sectionHeader confidence="0.996381" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99909115">
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In 41st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’03), Sapporo,
Japan.
Fr´ed´eric B´echet, Alexis Nasr, and Franck Genet. 2000.
Tagging unknown proper names using decision trees.
In 38th Annual Meeting of the Association for Compu-
tational Linguistics, Hong-Kong, China, pages 77–84.
Yannick Est`eve, Christian Raymond, Renato De Mori,
and David Janiszek. 2003. On the use of linguistic
consistency in systems for human-computer dialogs.
IEEE Transactions on Speech and Audio Processing,
(Accepted for publication, in press).
Patrick Haffner, Gokhan Tur, and Jerry Wright. 2003.
Optimizing SVMs for complex call classification. In
IEEE International Conference on Acoustics, Speech
and Signal Processing, ICASSP’03, Hong-Kong.
Y. He and S. Young. 2003. A data-driven spoken lan-
guage understanding system. In Automatic Speech
Recognition and Understanding workshop - ASRU’03,
St. Thomas, US-Virgin Islands.
R. Kuhn and R. De Mori. 1995. The application of se-
mantic classification trees to natural language under-
standing. IEEE Trans. on Pattern Analysis and Ma-
chine Intelligence, 17(449-460).
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69–88.
Christian Raymond, Yannick Est`eve, Fr´ed´eric B´echet,
Renato De Mori, and G´eraldine Damnati. 2003. Belief
confirmation in spoken dialogue systems using confi-
dence measures. In Automatic Speech Recognition and
Understanding workshop - ASRU’03, St. Thomas, US-
Virgin Islands.
Steve Young. 2002. Talking to machines (statisti-
cally speaking). In International Conference on Spo-
ken Language Processing, ICSLP’02, pages 113–120,
Denver, CO.
</reference>
<figure confidence="0.966460272727273">
18
16
14
12
10
8
6
4
devt
test
understanding error rate
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.906320">
<title confidence="0.999974">On the use of confidence for statistical decision in dialogue strategies</title>
<author confidence="0.999237">Fr´ed´eric Renato de_G´eraldine</author>
<affiliation confidence="0.918332">1LIA/CNRS, University of Avignon France 2France Telecom R&amp;D, Lannion,</affiliation>
<email confidence="0.999844">geraldine.damnati@rd.francetelecom.com</email>
<abstract confidence="0.99934315">This paper describes an interpretation and decision strategy that minimizes interpretation errors and perform dialogue actions which may not depend on the hypothesized concepts only, but also on confidence of what has been recognized. The concepts introduced here are applied in a system which integrates language and interpretation models into Stochastic Finite State Transducers (SFST). Furthermore, acoustic, linguistic and semantic confidence measures on the hypothesized word sequences are made available to the dialogue strategy. By evaluating predicates related to these confidence measures, a decision tree automatically learn a decision strategy for rescoring a n-best list of candidates representing a user’s utterance. The different actions that can be then performed are chosen according to the confidence scores given by the tree.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized algorithms for constructing statistical language models.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Association for Computational Linguistics (ACL’03),</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="8993" citStr="Allauzen et al., 2003" startWordPosition="1474" endWordPosition="1477">ed together in a single model called Tconcept as presented in figure 1. Figure 1: Word-to-Concept Transducer 4.2 Processing the ASR word lattice The ASR word lattice is coded by an FSM: an acceptor L where each transition emits a word. The cost function for a transition corresponds to the acoustic score of the word emitted. The first step in the word lattice processing consists of rescoring each transition of L by means of a 3-gram Language Model (LM) in order to obtain the probabilities P(W) of equation 1. This is done by composing the word lattice with a 3-gram LM also coded as an FSM (see (Allauzen et al., 2003) for more details about statistical LMs and FSMs). The resulting FSM is then composed with the transducer TConcept in order to obtain the word-to-concept transducer L&apos;. A path in L&apos; corresponds to a word string if only the input symbols of the transducer are considered and its score is the one expressed by equation 1; similarly by considering only the output symbols, a path in L&apos; corresponds to a concept tag string. The structured n-best list is directly obtained from L&apos;: by extracting the n-best concept tag strings (output label paths) we obtain an n-best list on the conceptual interpretation</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized algorithms for constructing statistical language models. In 41st Annual Meeting of the Association for Computational Linguistics (ACL’03), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fr´ed´eric B´echet</author>
<author>Alexis Nasr</author>
<author>Franck Genet</author>
</authors>
<title>Tagging unknown proper names using decision trees.</title>
<date>2000</date>
<booktitle>In 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>77--84</pages>
<location>Hong-Kong, China,</location>
<marker>B´echet, Nasr, Genet, 2000</marker>
<rawString>Fr´ed´eric B´echet, Alexis Nasr, and Franck Genet. 2000. Tagging unknown proper names using decision trees. In 38th Annual Meeting of the Association for Computational Linguistics, Hong-Kong, China, pages 77–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Est`eve</author>
<author>Christian Raymond</author>
<author>Renato De Mori</author>
<author>David Janiszek</author>
</authors>
<title>On the use of linguistic consistency in systems for human-computer dialogs.</title>
<date>2003</date>
<journal>IEEE Transactions on Speech</journal>
<note>and Audio Processing, (Accepted for publication, in press).</note>
<marker>Est`eve, Raymond, De Mori, Janiszek, 2003</marker>
<rawString>Yannick Est`eve, Christian Raymond, Renato De Mori, and David Janiszek. 2003. On the use of linguistic consistency in systems for human-computer dialogs. IEEE Transactions on Speech and Audio Processing, (Accepted for publication, in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Haffner</author>
<author>Gokhan Tur</author>
<author>Jerry Wright</author>
</authors>
<title>Optimizing SVMs for complex call classification.</title>
<date>2003</date>
<booktitle>In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP’03, Hong-Kong.</booktitle>
<contexts>
<context position="12421" citStr="Haffner et al., 2003" startWordPosition="2043" endWordPosition="2046">easure is simply, for a given word string candidate, the ratio between the number of trigrams observed in the training corpus of the Language Model vs. the total number of trigrams in the same word string. Its computation is very fast and the confidence scores obtained from it give interesting results as presented in (Est`eve et al., 2003). 5.1.3 Semantic confidence measure (SC) Several studies have shown that text classification tools (like Support Vector Machines or Boosting algorithms) can be an efficient way of labeling an utterance transcription with a semantic label such as a call-type (Haffner et al., 2003) in a Spoken Dialogue context. In our case, the semantic labels attached to an utterance are the different in=&lt;start&gt; out=&lt;&gt; FILLER in=&lt;&gt; in=&lt;&gt; out=&lt;/BCK&gt; in=&lt;&gt; out=&lt;&gt; SFST C1 out=&lt;BCK&gt; in=&lt;&gt; out=&lt;&gt; in=&lt;end&gt; out=&lt;&gt; in=w1 out=&lt;&gt; in=&lt;end out=&lt;&gt; out=&lt;&gt; in=&lt;start&gt; in=&lt;&gt; out=&lt;C1&gt; out=&lt;/C1&gt; in=&lt;&gt; in=w2 out=&lt;&gt; out=&lt;C2&gt; in=&lt;&gt; SFST C2 out=&lt;/C2&gt; in=&lt;&gt; in=&lt;&gt; out=&lt;Cn&gt; SFST Cn in=&lt;&gt; out=&lt;/Cn&gt; in=&lt;&gt; out=&lt;&gt; concepts handled by the Dialogue Manager. One classifier is trained for each concept tag in the following way: Each utterance of a training corpus is labeled with a tag, manually checked, indicating if a </context>
</contexts>
<marker>Haffner, Tur, Wright, 2003</marker>
<rawString>Patrick Haffner, Gokhan Tur, and Jerry Wright. 2003. Optimizing SVMs for complex call classification. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP’03, Hong-Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>S Young</author>
</authors>
<title>A data-driven spoken language understanding system.</title>
<date>2003</date>
<booktitle>In Automatic Speech Recognition and Understanding workshop - ASRU’03, St. Thomas, US-Virgin Islands.</booktitle>
<contexts>
<context position="2652" citStr="He and Young, 2003" startWordPosition="380" endWordPosition="383"> for interpretation and dialogue strategies, a good overview of which can be found in (Young, 2002). The concepts introduced here are applied in a system which integrates language and interpretation models into Stochastic Finite State Transducers (SFST). Furthermore, acoustic, linguistic and semantic confidence measures on the hypothesized word sequences are made available to the dialogue strategy. A new way of using them in the dialogue decision process is proposed in this paper. Most of the Spoken language Understanding Systems (SLU) use semantic grammars with semantic tags as nonterminals (He and Young, 2003) with rules for rewriting them into strings of words. The SFSTs of the system used for the experiments described here, represent knowledge for the basic building blocks of a frame-based semantic grammar. Each block represents a property/value relation. Different SFSTs may share words in the same sentence. Property/value hypotheses are generated with an approach described in (Raymond et al., 2003) and are combined into a sentence interpretation hypothesis in which the same word may contribute to more than one property/value pair. The dialogue strategy has to evaluate the probability that each c</context>
</contexts>
<marker>He, Young, 2003</marker>
<rawString>Y. He and S. Young. 2003. A data-driven spoken language understanding system. In Automatic Speech Recognition and Understanding workshop - ASRU’03, St. Thomas, US-Virgin Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R De Mori</author>
</authors>
<title>The application of semantic classification trees to natural language understanding.</title>
<date>1995</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence,</journal>
<pages>17--449</pages>
<marker>Kuhn, De Mori, 1995</marker>
<rawString>R. Kuhn and R. De Mori. 1995. The application of semantic classification trees to natural language understanding. IEEE Trans. on Pattern Analysis and Machine Intelligence, 17(449-460).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<journal>Computer, Speech and Language,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="6766" citStr="Mohri et al., 2002" startWordPosition="1076" endWordPosition="1079">r, and all the n-best word hypotheses that differ only between each other because of some speech disfluency effects can be considered as equals. That’s why it is important to generate not only a n-best list of word hypotheses but rather a n-best list of interpretations, each of them corresponding to a different meaning from the Dialogue Manager point of view. We propose here a method for directly extracting such a structured n-best from a word lattice output by an ASR engine. This method relies on operations between Finite State Machines and is implemented thanks to the AT&amp;T FSM toolkit (see (Mohri et al., 2002) for more details). 4.1 Word-to-Concept transducer Each concept ck of the dialogue application is associated with an FSM. These FMSs are called acceptors (Ak for the concept ck). In order to process strings of words that don’t belong to any concept, a filler model, called AF is used. Because the same string of words can’t belong to both a concept model and the background text, all the paths contained in the acceptors Ak are removed from the filler model AF in the following way: P( |W) = P(s1 |W). Ak �J j=2 Um k=1 P(sj |sj−1 1 W) (2) P(sj |sj−1 1 W)  P(sj |W) AF = E * If the conceptual compon</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer, Speech and Language, 16(1):69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Raymond</author>
<author>Yannick Est`eve</author>
<author>Fr´ed´eric B´echet</author>
<author>Renato De Mori</author>
<author>G´eraldine Damnati</author>
</authors>
<title>Belief confirmation in spoken dialogue systems using confidence measures.</title>
<date>2003</date>
<booktitle>In Automatic Speech Recognition and Understanding workshop - ASRU’03, St. Thomas, USVirgin Islands.</booktitle>
<marker>Raymond, Est`eve, B´echet, De Mori, Damnati, 2003</marker>
<rawString>Christian Raymond, Yannick Est`eve, Fr´ed´eric B´echet, Renato De Mori, and G´eraldine Damnati. 2003. Belief confirmation in spoken dialogue systems using confidence measures. In Automatic Speech Recognition and Understanding workshop - ASRU’03, St. Thomas, USVirgin Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
</authors>
<title>Talking to machines (statistically speaking).</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing, ICSLP’02,</booktitle>
<pages>113--120</pages>
<location>Denver, CO.</location>
<contexts>
<context position="2132" citStr="Young, 2002" startWordPosition="305" endWordPosition="306">ut conceptual entities are imprecise and incomplete. In spite of these problems, it is possible to make useful applications with dialogue systems using spoken input if suitable interpretation and decision strategies are conceived that minimize interpretation errors and perform dialogue actions which may not depend on the hypothesized concepts only, but also on confidence of what has been recognized. This paper introduces some concepts developed for telephone applications in the framework of stochastic models for interpretation and dialogue strategies, a good overview of which can be found in (Young, 2002). The concepts introduced here are applied in a system which integrates language and interpretation models into Stochastic Finite State Transducers (SFST). Furthermore, acoustic, linguistic and semantic confidence measures on the hypothesized word sequences are made available to the dialogue strategy. A new way of using them in the dialogue decision process is proposed in this paper. Most of the Spoken language Understanding Systems (SLU) use semantic grammars with semantic tags as nonterminals (He and Young, 2003) with rules for rewriting them into strings of words. The SFSTs of the system us</context>
</contexts>
<marker>Young, 2002</marker>
<rawString>Steve Young. 2002. Talking to machines (statistically speaking). In International Conference on Spoken Language Processing, ICSLP’02, pages 113–120, Denver, CO.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>