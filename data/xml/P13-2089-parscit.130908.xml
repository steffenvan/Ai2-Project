<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.8097105">
Generating Recommendation Dialogs by Extracting Information from
User Reviews
</title>
<author confidence="0.995562">
Kevin Reschke, Adam Vogel, and Dan Jurafsky
</author>
<affiliation confidence="0.992836">
Stanford University
</affiliation>
<address confidence="0.911124">
Stanford, CA, USA
</address>
<email confidence="0.999799">
{kreschke,acvogel,jurafsky}@stanford.edu
</email>
<sectionHeader confidence="0.994796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998562363636364">
Recommendation dialog systems help
users navigate e-commerce listings by ask-
ing questions about users’ preferences to-
ward relevant domain attributes. We
present a framework for generating and
ranking fine-grained, highly relevant ques-
tions from user-generated reviews. We
demonstrate our approach on a new dataset
just released by Yelp, and release a new
sentiment lexicon with 1329 adjectives for
the restaurant domain.
</bodyText>
<sectionHeader confidence="0.998425" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99978608">
Recommendation dialog systems have been devel-
oped for a number of tasks ranging from product
search to restaurant recommendation (Chai et al.,
2002; Thompson et al., 2004; Bridge et al., 2005;
Young et al., 2010). These systems learn user re-
quirements through spoken or text-based dialog,
asking questions about particular attributes to fil-
ter the space of relevant documents.
Traditionally, these systems draw questions
from a small, fixed set of attributes, such as cuisine
or price in the restaurant domain. However, these
systems overlook an important element in users’
interactions with online product listings: user-
generated reviews. Huang et al. (2012) show that
information extracted from user reviews greatly
improves user experience in visual search inter-
faces. In this paper, we present a dialog-based in-
terface that takes advantage of review texts. We
demonstrate our system on a new challenge cor-
pus of 11,537 businesses and 229,907 user reviews
released by the popular review website Yelp1, fo-
cusing on the dataset’s 4724 restaurants and bars
(164,106 reviews).
This paper makes two main contributions. First,
we describe and qualitatively evaluate a frame-
</bodyText>
<footnote confidence="0.930461">
1https://www.yelp.com/dataset_challenge/
</footnote>
<bodyText confidence="0.999853428571429">
work for generating new, highly-relevant ques-
tions from user review texts. The framework
makes use of techniques from topic modeling and
sentiment-based aspect extraction to identify fine-
grained attributes for each business. These at-
tributes form the basis of a new set of questions
that the system can ask the user.
Second, we use a method based on information-
gain for dynamically ranking candidate questions
during dialog production. This allows our system
to select the most informative question at each di-
alog step. An evaluation based on simulated di-
alogs shows that both the ranking method and the
automatically generated questions improve recall.
</bodyText>
<sectionHeader confidence="0.95314" genericHeader="method">
2 Generating Questions from Reviews
</sectionHeader>
<subsectionHeader confidence="0.966716">
2.1 Subcategory Questions
</subsectionHeader>
<bodyText confidence="0.999755571428571">
Yelp provides each business with category labels
for top-level cuisine types like Japanese, Coffee
&amp; Tea, and Vegetarian. Many of these top-level
categories have natural subcategories (e.g., ramen
vs. sushi). By identifying these subcategories, we
enable questions which probe one step deeper than
the top-level category label.
To identify these subcategories, we run Latent
Dirichlet Analysis (LDA) (Blei et al., 2003) on
the reviews of each set of businesses in the twenty
most common top-level categories, using 10 top-
ics and concatenating all of a business’s reviews
into one document.2 Several researchers have used
sentence-level documents to model topics in re-
views, but these tend to generate topics about fine-
grained aspects of the sort we discuss in Section
2.2 (Jo and Oh, 2011; Brody and Elhadad, 2010).
We then manually labeled the topics, discarding
junk topics and merging similar topics. Table 1
displays sample extracted subcategories.
Using these topic models, we assign a business
</bodyText>
<footnote confidence="0.989893">
2We use the Topic Modeling Toolkit implementation:
http://nlp.stanford.edu/software/tmt
</footnote>
<page confidence="0.952305">
499
</page>
<note confidence="0.5258835">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 499–504,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<table confidence="0.910666857142857">
Category Topic Label Top Words
Italian pizza crust sauce pizza garlic sausage slice salad
traditional pasta sauce delicious ravioli veal dishes gnocchi
bistro bruschetta patio salad valet delicious brie panini
deli sandwich deli salad pasta delicious grocery meatball
American (New) brew pub beers peaks ale brewery patio ipa brew
grill steak salad delicious sliders ribs tots drinks
bar drinks vig bartender patio uptown dive karaoke
bistro drinks pretzel salad fondue patio sanwich windsor
brunch sandwich brunch salad delicious pancakes patio
burger burger fries sauce beef potato sandwich delicious
mediterranean pita hummus jungle salad delicious mediterranean wrap
Delis italian deli sandwich meats cannoli cheeses authentic sausage
new york deli beef sandwich pastrami corned fries waitress
bagels bagel sandwiches toasted lox delicious donuts yummy
mediterranean pita lemonade falafel hummus delicious salad bakery
sandwiches sandwich subs sauce beef tasty meats delicious
Japanese sushi sushi kyoto zen rolls tuna sashimi spicy
teppanyaki sapporo chef teppanyaki sushi drinks shrimp fried
teriyaki teriyaki sauce beef bowls veggies spicy grill
ramen noodles udon dishes blossom delicious soup ramen
</table>
<tableCaption confidence="0.999509">
Table 1: A sample of subcategory topics with hand-labels and top words.
</tableCaption>
<bodyText confidence="0.999597375">
to a subcategory based on the topic with high-
est probability in that business’s topic distribution.
Finally, we use these subcategory topics to gen-
erate questions for our recommender dialog sys-
tem. Each top-level category corresponds to a sin-
gle question whose potential answers are the set of
subcategories: e.g., “What type of Japanese cui-
sine do you want?”
</bodyText>
<subsectionHeader confidence="0.999338">
2.2 Questions from Fine-Grained Aspects
</subsectionHeader>
<bodyText confidence="0.999981916666667">
Our second source for questions is based on as-
pect extraction in sentiment summarization (Blair-
Goldensohn et al., 2008; Brody and Elhadad,
2010). We define an aspect as any noun-phrase
which is targeted by a sentiment predicate. For
example, from the sentence “The place had great
atmosphere, but the service was slow.” we ex-
tract two aspects: +atmosphere and –service.
Our aspect extraction system has two steps.
First we develop a domain specific sentiment lex-
icon. Second, we apply syntactic patterns to iden-
tify NPs targeted by these sentiment predicates.
</bodyText>
<subsectionHeader confidence="0.667231">
2.2.1 Sentiment Lexicon
</subsectionHeader>
<bodyText confidence="0.972701952380953">
Coordination Graph We generate a list of
domain-specific sentiment adjectives using graph
propagation. We begin with a seed set combin-
ing PARADIGM+ (Jo and Oh, 2011) with ‘strongly
subjective’ adjectives from the OpinionFinder lex-
icon (Wilson et al., 2005), yielding 1342 seeds.
Like Brody and Elhadad (2010), we then construct
a coordination graph that links adjectives modify-
ing the same noun, but to increase precision we
require that the adjectives also be conjoined by
and (Hatzivassiloglou and McKeown, 1997). This
reduces problems like propagating positive sen-
timent to orange in good orange chicken. We
marked adjectives that follow too or lie in the
scope of negation with special prefixes and treated
them as distinct lexical entries.
Sentiment Propagation Negative and positive
seeds are assigned values of 0 and 1 respectively.
All other adjectives begin at 0.5. Then a stan-
dard propagation update is computed iteratively
(see Eq. 3 of Brody and Elhadad (2010)).
In Brody and Elhadad’s implementation of this
propagation method, seed sentiment values are
fixed, and the update step is repeated until the non-
seed values converge. We found that three modifi-
cations significantly improved precision. First, we
omit candidate nodes that don’t link to at least two
positive or two negative seeds. This eliminated
spurious propagation caused by one-off parsing er-
rors. Second, we run the propagation algorithm for
fewer iterations (two iterations for negative terms
and one for positive terms). We found that addi-
tional iterations led to significant error propaga-
tion when neutral (italian) or ambiguous (thick)
terms were assigned sentiment.3 Third, we update
both non-seed and seed adjectives. This allows us
to learn, for example, that the negative seed deca-
dent is positive in the restaurant domain.
Table 2 shows a sample of sentiment adjectives
3Our results are consistent with the recent finding of Whit-
ney and Sarkar (2012) that cautious systems are better when
bootstrapping from seeds.
</bodyText>
<page confidence="0.906569">
500
</page>
<table confidence="0.780357166666667">
Negative Sentiment
institutional, underwhelming, not nice, burn-
tish, unidentifiable, inefficient, not attentive,
grotesque, confused, trashy, insufferable,
grandiose, not pleasant, timid, degrading,
laughable, under-seasoned, dismayed, torn
Positive Sentiment
decadent, satisfied, lovely, stupendous,
sizable, nutritious, intense, peaceful,
not expensive, elegant, rustic, fast, affordable,
efficient, congenial, rich, not too heavy,
wholesome, bustling, lush
</table>
<tableCaption confidence="0.947206">
Table 2: Sample of Learned Sentiment Adjectives
</tableCaption>
<bodyText confidence="0.994121625">
derived by this graph propagation method. The
final lexicon has 1329 adjectives4, including 853
terms not in the original seed set. The lexicon is
available for download.5
Evaluative Verbs In addition to this adjective
lexicon, we take 56 evaluative verbs such as love
and hate from admire-class VerbNet predicates
(Kipper-Schuler, 2005).
</bodyText>
<subsubsectionHeader confidence="0.792759">
2.2.2 Extraction Patterns
</subsubsectionHeader>
<bodyText confidence="0.999541941176471">
To identify noun-phrases which are targeted by
predicates in our sentiment lexicon, we develop
hand-crafted extraction patterns defined over syn-
tactic dependency parses (Blair-Goldensohn et al.,
2008; Somasundaran and Wiebe, 2009) generated
by the Stanford parser (Klein and Manning, 2003).
Table 3 shows a sample of the aspects generated by
these methods.
Adj + NP It is common practice to extract any
NP modified by a sentiment adjective. However,
this simple extraction rule suffers from precision
problems. First, reviews often contain sentiment
toward irrelevant, non-business targets (Wayne is
the target of excellent job in (1)). Second, hypo-
thetical contexts lead to spurious extractions. In
(2), the extraction +service is clearly wrong–in
fact, the opposite sentiment is being expressed.
</bodyText>
<listItem confidence="0.9971115">
(1) Wayne did an excellent job addressing our
needs and giving us our options.
(2) Nice and airy atmosphere, but service could be
more attentive at times.
</listItem>
<footnote confidence="0.98101">
4We manually removed 26 spurious terms which were
caused by parsing errors or propagation to a neutral term.
5http://nlp.stanford.edu/projects/
yelp.shtml
</footnote>
<bodyText confidence="0.99975175">
We address these problems by filtering out sen-
tences in hypothetical contexts cued by if, should,
could, or a question mark, and by adopting the fol-
lowing, more conservative extractions rules:
</bodyText>
<listItem confidence="0.922007375">
i) [BIZ + have + adj. + NP] Sentiment adjec-
tive modifies NP, main verb is have, subject
is business name, it, they, place, or absent.
(E.g., This place has some really great yogurt
and toppings).
ii) [NP + be + adj.] Sentiment adjective linked
to NP by be—e.g., Our pizza was much too
jalapeno-y.
</listItem>
<bodyText confidence="0.92270425">
“Good For” + NP Next, we extract aspects us-
ing the pattern BIZ + positive adj. + for + NP, as in
It’s perfect for a date night. Examples of extracted
aspects include +lunch, +large groups, +drinks,
and +quick lunch.
Verb + NP Finally, we extract NPs that appear
as direct object to one of our evaluative verbs (e.g.,
We loved the fried chicken).
</bodyText>
<subsectionHeader confidence="0.728687">
2.2.3 Aspects as Questions
</subsectionHeader>
<bodyText confidence="0.99871825">
We generate questions from these extracted as-
pects using simple templates. For example, the as-
pect +burritos yields the question: Do you want a
place with good burritos?
</bodyText>
<sectionHeader confidence="0.96669" genericHeader="method">
3 Question Selection for Dialog
</sectionHeader>
<bodyText confidence="0.999963666666667">
To utilize the questions generated from reviews in
recommendation dialogs, we first formalize the di-
alog optimization task and then offer a solution.
</bodyText>
<subsectionHeader confidence="0.999003">
3.1 Problem Statement
</subsectionHeader>
<bodyText confidence="0.999748666666667">
We consider a version of the Information Retrieval
Dialog task introduced by Kopeˇcek (1999). Busi-
nesses b E B have associated attributes, coming
from a set Att. These attributes are a combination
of Yelp categories and our automatically extracted
aspects described in Section 2. Attributes att E Att
take values in a finite domain dom(att). We denote
the subset of businesses with an attribute att tak-
ing value val E dom(att), as B|att=val. Attributes
are functions from businesses to subsets of values:
att : B → P(dom(att)). We model a user in-
formation need I as a set of attribute/value pairs:
I = {(atti, vali), ... , (atte,e, vale,)}.
Given a set of businesses and attributes, a rec-
ommendation agent π selects an attribute to ask
</bodyText>
<page confidence="0.996247">
501
</page>
<table confidence="0.91453575">
Chinese: Mexican:
+beef +egg roll +sour soup +orange chicken +salsa bar +burritos +fish tacos +guacamole
+noodles +crab puff +egg drop soup +enchiladas +hot sauce +carne asade +breakfast burritos
+dim sum +fried rice +honey chicken +horchata +green salsa +tortillas +quesadillas
Japanese: American (New)
+rolls +sushi rolls +wasabi +sushi bar +salmon +environment +drink menu +bar area +cocktails +brunch
+chicken katsu +crunch +green tea +sake selection +hummus +mac and cheese +outdoor patio +seating area
+oysters +drink menu +sushi selection +quality +lighting +brews +sangria +cheese plates
</table>
<tableCaption confidence="0.993182">
Table 3: Sample of the most frequent positive aspects extracted from review texts.
</tableCaption>
<figure confidence="0.5382056">
Input: Information need I
Set of businesses B
Set of attributes Att
Recommendation agent π
Dialog length K
Output: Dialog history H
Recommended businesses B
Initialize dialog history H = 0
for step = 0; step &lt; K; step++ do
Select an attribute: att = π(B, H)
Query user for the answer: val = I(att)
Restrict set of businesses: B = B|att=val
Append answer: H = H U {(att, val)}
end
Return (H, B)
</figure>
<figureCaption confidence="0.4701195">
Algorithm 1: Procedure for evaluating a recom-
mendation agent
</figureCaption>
<bodyText confidence="0.9998917">
the user about, then uses the answer value to nar-
row the set of businesses to those with the de-
sired attribute value, and selects another query.
Algorithm 1 presents this process more formally.
The recommendation agent can use both the set of
businesses B and the history of question and an-
swers H from the user to select the next query.
Thus, formally a recommendation agent is a func-
tion π : B x H -+ Att. The dialog ends after a
fixed number of queries K.
</bodyText>
<subsectionHeader confidence="0.991152">
3.2 Information Gain Agent
</subsectionHeader>
<bodyText confidence="0.999935666666667">
The information gain recommendation agent
chooses questions to ask the user by selecting
question attributes that maximize the entropy of
the resulting document set, in a manner similar to
decision tree learning (Mitchell, 1997). Formally,
we define a function infogain : Att x P(B) -+ R:
</bodyText>
<equation confidence="0.98278125">
infogain(att, B) =
�−
valsEP(dom(att)) |Batt=vals |lo I Batt=vals
|B |g |B|
</equation>
<bodyText confidence="0.8856004">
The agent then selects questions att E Att that
maximize the information gain with respect to the
set of businesses satisfying the dialog history H:
π(B, H) = arg max infogain(att, B|H)
attEAtt
</bodyText>
<sectionHeader confidence="0.997165" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.992568">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9999692">
We follow the standard approach of using the at-
tributes of an individual business as a simulation
of a user’s preferences (Chung, 2004; Young et al.,
2010). For every business b E B we form an in-
formation need composed of all of b’s attributes:
</bodyText>
<equation confidence="0.952069">
UIb = (att, att(b))
{attEAttjatt(b)�0}
</equation>
<bodyText confidence="0.999938428571429">
To evaluate a recommendation agent, we use
the recall metric, which measures how well an in-
formation need is satisfied. For each information
need I, let BI be the set of businesses that satisfy
the questions of an agent. We define the recall of
the set of businesses with respect to the informa-
tion need as
</bodyText>
<equation confidence="0.4653695">
recall(BI, I) = EbEBI E(att,val)EI ✶[val E att(b)]
|BI||I|
</equation>
<bodyText confidence="0.9999855">
We average recall across all information needs,
yielding average recall.
We compare against a random agent baseline
that selects attributes att E Att uniformly at ran-
dom at each time step. Other recommendation di-
alog systems such as Young et al. (2010) select
questions from a small fixed hierarchy, which is
not applicable to our large set of attributes.
</bodyText>
<subsectionHeader confidence="0.689397">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.9999038">
Figure 1 shows the average recall for the ran-
dom agent versus the information gain agent with
varying sets of attributes. ‘Top-level’ repeatedly
queries the user’s top-level category preferences,
‘Subtopic’ additionally uses our topic modeling
subcategories, and ‘All’ uses these plus the as-
pects extracted from reviews. We see that for suf-
ficiently long dialogs, ‘All’ outperforms the other
systems. The ‘Subtopic’ and ‘Top-level’ systems
plateau after a few dialog steps once they’ve asked
</bodyText>
<page confidence="0.989772">
502
</page>
<figure confidence="0.625892">
Average Recall by Agent
</figure>
<figureCaption confidence="0.9998">
Figure 1: Average recall for each agent.
</figureCaption>
<bodyText confidence="0.93032975">
all useful questions. For instance, most businesses
only have one or two top-level categories, so af-
ter the system has identified the top-level cate-
gory that the user is interested in, it has no more
good questions to ask. Note that the information
gain agent starts dialogs with the top-level and ap-
propriate subcategory questions, so it is only for
longer dialogs that the fine-grained aspects boost
performance.
Below we show a few sample output dialogs
from our ‘All’ information gain agent.
Q: What kind of place do you want?
A: American (New)
Q: What kind of American (New) do you want:
bar, bistro, standard, burgers, brew pub, or
brunch?
</bodyText>
<figure confidence="0.8162652">
A: bistro
Q: Do you want a place with a good patio?
A: Yes
Q: What kind of place do you want?
A: Chinese
</figure>
<bodyText confidence="0.906178166666667">
Q: What kind of Chinese place do you want:
buffet, dim sum, noodles, pan Asian, Panda
Express, sit down, or veggie?
A: sit down
Q: Do you want a place with a good lunch
special?
</bodyText>
<subsectionHeader confidence="0.421002">
A: Yes
</subsectionHeader>
<bodyText confidence="0.758802">
Q: What kind of place do you want?
</bodyText>
<subsectionHeader confidence="0.43787">
A: Mexican
</subsectionHeader>
<bodyText confidence="0.7810304">
Q: What kind of Mexican place do you want:
dinner, taqueria, margarita bar, or tortas?
A: Margarita bar
Q: Do you want a place with a good patio?
A: Yes
</bodyText>
<sectionHeader confidence="0.985208" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999987105263158">
We presented a system for extracting large sets
of attributes from user reviews and selecting rel-
evant attributes to ask questions about. Using
topic models to discover subtypes of businesses, a
domain-specific sentiment lexicon, and a number
of new techniques for increasing precision in sen-
timent aspect extraction yields attributes that give
a rich representation of the restaurant domain. We
have made this 1329-term sentiment lexicon for
the restaurant domain available as useful resource
to the community. Our information gain recom-
mendation agent gives a principled way to dynam-
ically combine these diverse attributes to ask rele-
vant questions in a coherent dialog. Our approach
thus offers a new way to integrate the advantages
of the curated hand-build attributes used in statisti-
cal slot and filler dialog systems, and the distribu-
tionally induced, highly relevant categories built
by sentiment aspect extraction systems.
</bodyText>
<sectionHeader confidence="0.999058" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999867727272727">
Thanks to the anonymous reviewers and the Stan-
ford NLP group for helpful suggestions. The au-
thors also gratefully acknowledge the support of
the Nuance Foundation, the Defense Advanced
Research Projects Agency (DARPA) Deep Explo-
ration and Filtering of Text (DEFT) Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-13-2-0040, ONR grants
N00014-10-1-0109 and N00014-13-1-0287 and
ARO grant W911NF-07-1-0216, and the Center
for Advanced Study in the Behavioral Sciences.
</bodyText>
<sectionHeader confidence="0.997892" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987152071428571">
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George A Reis, and Jeff Reynar.
2008. Building a sentiment summarizer for local
service reviews. In WWW Workshop on NLP in the
Information Explosion Era.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. The Journal of
Machine Learning Research, 3:993–1022.
Derek Bridge, Mehmet H. G¨oker, Lorraine McGinty,
and Barry Smyth. 2005. Case-based recom-
mender systems. Knowledge Engineering Review,
20(3):315–320.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
</reference>
<figure confidence="0.999493">
Average Recall
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1 2 3 4 5 6 7 8 9 10
Dialog Length
1
Random
Top-level
Subtopic
All
</figure>
<page confidence="0.989989">
503
</page>
<reference confidence="0.999867666666667">
In Proceedings of HLT NAACL 2010, pages 804–
812.
Joyce Chai, Veronika Horvath, Nicolas Nicolov, Margo
Stys, A Kambhatla, Wlodek Zadrozny, and Prem
Melville. 2002. Natural language assistant - a di-
alog system for online product recommendation. AI
Magazine, 23:63–75.
Grace Chung. 2004. Developing a flexible spoken dia-
log system using simulation. In Proceedings of ACL
2004, pages 63–70.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of EACL 1997, pages 174–
181.
Jeff Huang, Oren Etzioni, Luke Zettlemoyer, Kevin
Clark, and Christian Lee. 2012. Revminer: An ex-
tractive interface for navigating reviews on a smart-
phone. In Proceedings of UIST 2012.
Yohan Jo and Alice H Oh. 2011. Aspect and sentiment
unification model for online review analysis. In Pro-
ceedings of the Fourth ACM International Confer-
ence on Web Search and Data Mining, pages 815–
824.
Karin Kipper-Schuler. 2005. Verbnet: A broad-
coverage, comprehensive verb lexicon.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings ACL
2003, pages 423–430.
I. Kopeˇcek. 1999. Modeling of the information re-
trieval dialogue systems. In Proceedings of the
Workshop on Text, Speech and Dialogue-TSD 99,
Lectures Notes in Artificial Intelligence 1692, pages
302–307. Springer-Verlag.
Tom M. Mitchell. 1997. Machine Learning. McGraw-
Hill, New York.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of ACL 2009, pages 226–234.
Cynthia A. Thompson, Mehmet H. Goeker, and Pat
Langley. 2004. A personalized system for conver-
sational recommendations. Journal of Artificial In-
telligence Research (JAIR), 21:393–428.
Max Whitney and Anoop Sarkar. 2012. Bootstrapping
via graph propagation. In Proceedings of the ACL
2012, pages 620–628, Jeju Island, Korea.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005. Opinionfinder: A system for subjectivity
analysis. In Proceedings of HLT/EMNLP 2005 on
Interactive Demonstrations, pages 34–35.
Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150–174, April.
</reference>
<page confidence="0.998346">
504
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.449181">
<title confidence="0.989178">Generating Recommendation Dialogs by Extracting Information from User Reviews</title>
<author confidence="0.907893">Adam Vogel Reschke</author>
<affiliation confidence="0.54583">Stanford</affiliation>
<address confidence="0.730347">Stanford, CA,</address>
<abstract confidence="0.998489666666667">Recommendation dialog systems help users navigate e-commerce listings by asking questions about users’ preferences toward relevant domain attributes. We present a framework for generating and ranking fine-grained, highly relevant questions from user-generated reviews. We demonstrate our approach on a new dataset just released by Yelp, and release a new sentiment lexicon with 1329 adjectives for the restaurant domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
<author>Tyler Neylon</author>
<author>George A Reis</author>
<author>Jeff Reynar</author>
</authors>
<title>Building a sentiment summarizer for local service reviews.</title>
<date>2008</date>
<booktitle>In WWW Workshop on NLP in the Information Explosion Era.</booktitle>
<contexts>
<context position="9194" citStr="Blair-Goldensohn et al., 2008" startWordPosition="1356" endWordPosition="1359">, bustling, lush Table 2: Sample of Learned Sentiment Adjectives derived by this graph propagation method. The final lexicon has 1329 adjectives4, including 853 terms not in the original seed set. The lexicon is available for download.5 Evaluative Verbs In addition to this adjective lexicon, we take 56 evaluative verbs such as love and hate from admire-class VerbNet predicates (Kipper-Schuler, 2005). 2.2.2 Extraction Patterns To identify noun-phrases which are targeted by predicates in our sentiment lexicon, we develop hand-crafted extraction patterns defined over syntactic dependency parses (Blair-Goldensohn et al., 2008; Somasundaran and Wiebe, 2009) generated by the Stanford parser (Klein and Manning, 2003). Table 3 shows a sample of the aspects generated by these methods. Adj + NP It is common practice to extract any NP modified by a sentiment adjective. However, this simple extraction rule suffers from precision problems. First, reviews often contain sentiment toward irrelevant, non-business targets (Wayne is the target of excellent job in (1)). Second, hypothetical contexts lead to spurious extractions. In (2), the extraction +service is clearly wrong–in fact, the opposite sentiment is being expressed. (</context>
</contexts>
<marker>Blair-Goldensohn, Hannan, McDonald, Neylon, Reis, Reynar, 2008</marker>
<rawString>Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDonald, Tyler Neylon, George A Reis, and Jeff Reynar. 2008. Building a sentiment summarizer for local service reviews. In WWW Workshop on NLP in the Information Explosion Era.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2992" citStr="Blei et al., 2003" startWordPosition="436" endWordPosition="439">p. An evaluation based on simulated dialogs shows that both the ranking method and the automatically generated questions improve recall. 2 Generating Questions from Reviews 2.1 Subcategory Questions Yelp provides each business with category labels for top-level cuisine types like Japanese, Coffee &amp; Tea, and Vegetarian. Many of these top-level categories have natural subcategories (e.g., ramen vs. sushi). By identifying these subcategories, we enable questions which probe one step deeper than the top-level category label. To identify these subcategories, we run Latent Dirichlet Analysis (LDA) (Blei et al., 2003) on the reviews of each set of businesses in the twenty most common top-level categories, using 10 topics and concatenating all of a business’s reviews into one document.2 Several researchers have used sentence-level documents to model topics in reviews, but these tend to generate topics about finegrained aspects of the sort we discuss in Section 2.2 (Jo and Oh, 2011; Brody and Elhadad, 2010). We then manually labeled the topics, discarding junk topics and merging similar topics. Table 1 displays sample extracted subcategories. Using these topic models, we assign a business 2We use the Topic M</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derek Bridge</author>
<author>Mehmet H G¨oker</author>
<author>Lorraine McGinty</author>
<author>Barry Smyth</author>
</authors>
<title>Case-based recommender systems.</title>
<date>2005</date>
<journal>Knowledge Engineering Review,</journal>
<volume>20</volume>
<issue>3</issue>
<marker>Bridge, G¨oker, McGinty, Smyth, 2005</marker>
<rawString>Derek Bridge, Mehmet H. G¨oker, Lorraine McGinty, and Barry Smyth. 2005. Case-based recommender systems. Knowledge Engineering Review, 20(3):315–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Noemie Elhadad</author>
</authors>
<title>An unsupervised aspect-sentiment model for online reviews.</title>
<date>2010</date>
<contexts>
<context position="3387" citStr="Brody and Elhadad, 2010" startWordPosition="503" endWordPosition="506"> ramen vs. sushi). By identifying these subcategories, we enable questions which probe one step deeper than the top-level category label. To identify these subcategories, we run Latent Dirichlet Analysis (LDA) (Blei et al., 2003) on the reviews of each set of businesses in the twenty most common top-level categories, using 10 topics and concatenating all of a business’s reviews into one document.2 Several researchers have used sentence-level documents to model topics in reviews, but these tend to generate topics about finegrained aspects of the sort we discuss in Section 2.2 (Jo and Oh, 2011; Brody and Elhadad, 2010). We then manually labeled the topics, discarding junk topics and merging similar topics. Table 1 displays sample extracted subcategories. Using these topic models, we assign a business 2We use the Topic Modeling Toolkit implementation: http://nlp.stanford.edu/software/tmt 499 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 499–504, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Category Topic Label Top Words Italian pizza crust sauce pizza garlic sausage slice salad traditional pasta sauce delicious ravioli vea</context>
<context position="5676" citStr="Brody and Elhadad, 2010" startWordPosition="833" endWordPosition="836">p ramen Table 1: A sample of subcategory topics with hand-labels and top words. to a subcategory based on the topic with highest probability in that business’s topic distribution. Finally, we use these subcategory topics to generate questions for our recommender dialog system. Each top-level category corresponds to a single question whose potential answers are the set of subcategories: e.g., “What type of Japanese cuisine do you want?” 2.2 Questions from Fine-Grained Aspects Our second source for questions is based on aspect extraction in sentiment summarization (BlairGoldensohn et al., 2008; Brody and Elhadad, 2010). We define an aspect as any noun-phrase which is targeted by a sentiment predicate. For example, from the sentence “The place had great atmosphere, but the service was slow.” we extract two aspects: +atmosphere and –service. Our aspect extraction system has two steps. First we develop a domain specific sentiment lexicon. Second, we apply syntactic patterns to identify NPs targeted by these sentiment predicates. 2.2.1 Sentiment Lexicon Coordination Graph We generate a list of domain-specific sentiment adjectives using graph propagation. We begin with a seed set combining PARADIGM+ (Jo and Oh, </context>
<context position="7088" citStr="Brody and Elhadad (2010)" startWordPosition="1056" endWordPosition="1059"> that links adjectives modifying the same noun, but to increase precision we require that the adjectives also be conjoined by and (Hatzivassiloglou and McKeown, 1997). This reduces problems like propagating positive sentiment to orange in good orange chicken. We marked adjectives that follow too or lie in the scope of negation with special prefixes and treated them as distinct lexical entries. Sentiment Propagation Negative and positive seeds are assigned values of 0 and 1 respectively. All other adjectives begin at 0.5. Then a standard propagation update is computed iteratively (see Eq. 3 of Brody and Elhadad (2010)). In Brody and Elhadad’s implementation of this propagation method, seed sentiment values are fixed, and the update step is repeated until the nonseed values converge. We found that three modifications significantly improved precision. First, we omit candidate nodes that don’t link to at least two positive or two negative seeds. This eliminated spurious propagation caused by one-off parsing errors. Second, we run the propagation algorithm for fewer iterations (two iterations for negative terms and one for positive terms). We found that additional iterations led to significant error propagatio</context>
</contexts>
<marker>Brody, Elhadad, 2010</marker>
<rawString>Samuel Brody and Noemie Elhadad. 2010. An unsupervised aspect-sentiment model for online reviews.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of HLT NAACL 2010,</booktitle>
<pages>804--812</pages>
<marker></marker>
<rawString>In Proceedings of HLT NAACL 2010, pages 804– 812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joyce Chai</author>
<author>Veronika Horvath</author>
<author>Nicolas Nicolov</author>
<author>Margo Stys</author>
<author>A Kambhatla</author>
<author>Wlodek Zadrozny</author>
<author>Prem Melville</author>
</authors>
<title>Natural language assistant - a dialog system for online product recommendation.</title>
<date>2002</date>
<journal>AI Magazine,</journal>
<pages>23--63</pages>
<contexts>
<context position="793" citStr="Chai et al., 2002" startWordPosition="105" endWordPosition="108">l,jurafsky}@stanford.edu Abstract Recommendation dialog systems help users navigate e-commerce listings by asking questions about users’ preferences toward relevant domain attributes. We present a framework for generating and ranking fine-grained, highly relevant questions from user-generated reviews. We demonstrate our approach on a new dataset just released by Yelp, and release a new sentiment lexicon with 1329 adjectives for the restaurant domain. 1 Introduction Recommendation dialog systems have been developed for a number of tasks ranging from product search to restaurant recommendation (Chai et al., 2002; Thompson et al., 2004; Bridge et al., 2005; Young et al., 2010). These systems learn user requirements through spoken or text-based dialog, asking questions about particular attributes to filter the space of relevant documents. Traditionally, these systems draw questions from a small, fixed set of attributes, such as cuisine or price in the restaurant domain. However, these systems overlook an important element in users’ interactions with online product listings: usergenerated reviews. Huang et al. (2012) show that information extracted from user reviews greatly improves user experience in v</context>
</contexts>
<marker>Chai, Horvath, Nicolov, Stys, Kambhatla, Zadrozny, Melville, 2002</marker>
<rawString>Joyce Chai, Veronika Horvath, Nicolas Nicolov, Margo Stys, A Kambhatla, Wlodek Zadrozny, and Prem Melville. 2002. Natural language assistant - a dialog system for online product recommendation. AI Magazine, 23:63–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Chung</author>
</authors>
<title>Developing a flexible spoken dialog system using simulation.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>63--70</pages>
<contexts>
<context position="14429" citStr="Chung, 2004" startWordPosition="2225" endWordPosition="2226">mize the entropy of the resulting document set, in a manner similar to decision tree learning (Mitchell, 1997). Formally, we define a function infogain : Att x P(B) -+ R: infogain(att, B) = �− valsEP(dom(att)) |Batt=vals |lo I Batt=vals |B |g |B| The agent then selects questions att E Att that maximize the information gain with respect to the set of businesses satisfying the dialog history H: π(B, H) = arg max infogain(att, B|H) attEAtt 4 Evaluation 4.1 Experimental Setup We follow the standard approach of using the attributes of an individual business as a simulation of a user’s preferences (Chung, 2004; Young et al., 2010). For every business b E B we form an information need composed of all of b’s attributes: UIb = (att, att(b)) {attEAttjatt(b)�0} To evaluate a recommendation agent, we use the recall metric, which measures how well an information need is satisfied. For each information need I, let BI be the set of businesses that satisfy the questions of an agent. We define the recall of the set of businesses with respect to the information need as recall(BI, I) = EbEBI E(att,val)EI ✶[val E att(b)] |BI||I| We average recall across all information needs, yielding average recall. We compare </context>
</contexts>
<marker>Chung, 2004</marker>
<rawString>Grace Chung. 2004. Developing a flexible spoken dialog system using simulation. In Proceedings of ACL 2004, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>174--181</pages>
<contexts>
<context position="6630" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="982" endWordPosition="985">con. Second, we apply syntactic patterns to identify NPs targeted by these sentiment predicates. 2.2.1 Sentiment Lexicon Coordination Graph We generate a list of domain-specific sentiment adjectives using graph propagation. We begin with a seed set combining PARADIGM+ (Jo and Oh, 2011) with ‘strongly subjective’ adjectives from the OpinionFinder lexicon (Wilson et al., 2005), yielding 1342 seeds. Like Brody and Elhadad (2010), we then construct a coordination graph that links adjectives modifying the same noun, but to increase precision we require that the adjectives also be conjoined by and (Hatzivassiloglou and McKeown, 1997). This reduces problems like propagating positive sentiment to orange in good orange chicken. We marked adjectives that follow too or lie in the scope of negation with special prefixes and treated them as distinct lexical entries. Sentiment Propagation Negative and positive seeds are assigned values of 0 and 1 respectively. All other adjectives begin at 0.5. Then a standard propagation update is computed iteratively (see Eq. 3 of Brody and Elhadad (2010)). In Brody and Elhadad’s implementation of this propagation method, seed sentiment values are fixed, and the update step is repeated until th</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of EACL 1997, pages 174– 181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Huang</author>
<author>Oren Etzioni</author>
<author>Luke Zettlemoyer</author>
<author>Kevin Clark</author>
<author>Christian Lee</author>
</authors>
<title>Revminer: An extractive interface for navigating reviews on a smartphone.</title>
<date>2012</date>
<booktitle>In Proceedings of UIST</booktitle>
<contexts>
<context position="1305" citStr="Huang et al. (2012)" startWordPosition="183" endWordPosition="186">developed for a number of tasks ranging from product search to restaurant recommendation (Chai et al., 2002; Thompson et al., 2004; Bridge et al., 2005; Young et al., 2010). These systems learn user requirements through spoken or text-based dialog, asking questions about particular attributes to filter the space of relevant documents. Traditionally, these systems draw questions from a small, fixed set of attributes, such as cuisine or price in the restaurant domain. However, these systems overlook an important element in users’ interactions with online product listings: usergenerated reviews. Huang et al. (2012) show that information extracted from user reviews greatly improves user experience in visual search interfaces. In this paper, we present a dialog-based interface that takes advantage of review texts. We demonstrate our system on a new challenge corpus of 11,537 businesses and 229,907 user reviews released by the popular review website Yelp1, focusing on the dataset’s 4724 restaurants and bars (164,106 reviews). This paper makes two main contributions. First, we describe and qualitatively evaluate a frame1https://www.yelp.com/dataset_challenge/ work for generating new, highly-relevant questio</context>
</contexts>
<marker>Huang, Etzioni, Zettlemoyer, Clark, Lee, 2012</marker>
<rawString>Jeff Huang, Oren Etzioni, Luke Zettlemoyer, Kevin Clark, and Christian Lee. 2012. Revminer: An extractive interface for navigating reviews on a smartphone. In Proceedings of UIST 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohan Jo</author>
<author>Alice H Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>815--824</pages>
<contexts>
<context position="3361" citStr="Jo and Oh, 2011" startWordPosition="499" endWordPosition="502">categories (e.g., ramen vs. sushi). By identifying these subcategories, we enable questions which probe one step deeper than the top-level category label. To identify these subcategories, we run Latent Dirichlet Analysis (LDA) (Blei et al., 2003) on the reviews of each set of businesses in the twenty most common top-level categories, using 10 topics and concatenating all of a business’s reviews into one document.2 Several researchers have used sentence-level documents to model topics in reviews, but these tend to generate topics about finegrained aspects of the sort we discuss in Section 2.2 (Jo and Oh, 2011; Brody and Elhadad, 2010). We then manually labeled the topics, discarding junk topics and merging similar topics. Table 1 displays sample extracted subcategories. Using these topic models, we assign a business 2We use the Topic Modeling Toolkit implementation: http://nlp.stanford.edu/software/tmt 499 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 499–504, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Category Topic Label Top Words Italian pizza crust sauce pizza garlic sausage slice salad traditional pasta s</context>
<context position="6281" citStr="Jo and Oh, 2011" startWordPosition="929" endWordPosition="932">adad, 2010). We define an aspect as any noun-phrase which is targeted by a sentiment predicate. For example, from the sentence “The place had great atmosphere, but the service was slow.” we extract two aspects: +atmosphere and –service. Our aspect extraction system has two steps. First we develop a domain specific sentiment lexicon. Second, we apply syntactic patterns to identify NPs targeted by these sentiment predicates. 2.2.1 Sentiment Lexicon Coordination Graph We generate a list of domain-specific sentiment adjectives using graph propagation. We begin with a seed set combining PARADIGM+ (Jo and Oh, 2011) with ‘strongly subjective’ adjectives from the OpinionFinder lexicon (Wilson et al., 2005), yielding 1342 seeds. Like Brody and Elhadad (2010), we then construct a coordination graph that links adjectives modifying the same noun, but to increase precision we require that the adjectives also be conjoined by and (Hatzivassiloglou and McKeown, 1997). This reduces problems like propagating positive sentiment to orange in good orange chicken. We marked adjectives that follow too or lie in the scope of negation with special prefixes and treated them as distinct lexical entries. Sentiment Propagatio</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Yohan Jo and Alice H Oh. 2011. Aspect and sentiment unification model for online review analysis. In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, pages 815– 824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper-Schuler</author>
</authors>
<title>Verbnet: A broadcoverage, comprehensive verb lexicon.</title>
<date>2005</date>
<contexts>
<context position="8967" citStr="Kipper-Schuler, 2005" startWordPosition="1328" endWordPosition="1329">, dismayed, torn Positive Sentiment decadent, satisfied, lovely, stupendous, sizable, nutritious, intense, peaceful, not expensive, elegant, rustic, fast, affordable, efficient, congenial, rich, not too heavy, wholesome, bustling, lush Table 2: Sample of Learned Sentiment Adjectives derived by this graph propagation method. The final lexicon has 1329 adjectives4, including 853 terms not in the original seed set. The lexicon is available for download.5 Evaluative Verbs In addition to this adjective lexicon, we take 56 evaluative verbs such as love and hate from admire-class VerbNet predicates (Kipper-Schuler, 2005). 2.2.2 Extraction Patterns To identify noun-phrases which are targeted by predicates in our sentiment lexicon, we develop hand-crafted extraction patterns defined over syntactic dependency parses (Blair-Goldensohn et al., 2008; Somasundaran and Wiebe, 2009) generated by the Stanford parser (Klein and Manning, 2003). Table 3 shows a sample of the aspects generated by these methods. Adj + NP It is common practice to extract any NP modified by a sentiment adjective. However, this simple extraction rule suffers from precision problems. First, reviews often contain sentiment toward irrelevant, non</context>
</contexts>
<marker>Kipper-Schuler, 2005</marker>
<rawString>Karin Kipper-Schuler. 2005. Verbnet: A broadcoverage, comprehensive verb lexicon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ACL</booktitle>
<pages>423--430</pages>
<contexts>
<context position="9284" citStr="Klein and Manning, 2003" startWordPosition="1369" endWordPosition="1372">ion method. The final lexicon has 1329 adjectives4, including 853 terms not in the original seed set. The lexicon is available for download.5 Evaluative Verbs In addition to this adjective lexicon, we take 56 evaluative verbs such as love and hate from admire-class VerbNet predicates (Kipper-Schuler, 2005). 2.2.2 Extraction Patterns To identify noun-phrases which are targeted by predicates in our sentiment lexicon, we develop hand-crafted extraction patterns defined over syntactic dependency parses (Blair-Goldensohn et al., 2008; Somasundaran and Wiebe, 2009) generated by the Stanford parser (Klein and Manning, 2003). Table 3 shows a sample of the aspects generated by these methods. Adj + NP It is common practice to extract any NP modified by a sentiment adjective. However, this simple extraction rule suffers from precision problems. First, reviews often contain sentiment toward irrelevant, non-business targets (Wayne is the target of excellent job in (1)). Second, hypothetical contexts lead to spurious extractions. In (2), the extraction +service is clearly wrong–in fact, the opposite sentiment is being expressed. (1) Wayne did an excellent job addressing our needs and giving us our options. (2) Nice and</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings ACL 2003, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kopeˇcek</author>
</authors>
<title>Modeling of the information retrieval dialogue systems.</title>
<date>1999</date>
<booktitle>In Proceedings of the Workshop on Text, Speech and Dialogue-TSD 99, Lectures Notes in Artificial Intelligence 1692,</booktitle>
<pages>302--307</pages>
<publisher>Springer-Verlag.</publisher>
<marker>Kopeˇcek, 1999</marker>
<rawString>I. Kopeˇcek. 1999. Modeling of the information retrieval dialogue systems. In Proceedings of the Workshop on Text, Speech and Dialogue-TSD 99, Lectures Notes in Artificial Intelligence 1692, pages 302–307. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom M Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning.</booktitle>
<publisher>McGrawHill,</publisher>
<location>New York.</location>
<contexts>
<context position="13928" citStr="Mitchell, 1997" startWordPosition="2140" endWordPosition="2141">d attribute value, and selects another query. Algorithm 1 presents this process more formally. The recommendation agent can use both the set of businesses B and the history of question and answers H from the user to select the next query. Thus, formally a recommendation agent is a function π : B x H -+ Att. The dialog ends after a fixed number of queries K. 3.2 Information Gain Agent The information gain recommendation agent chooses questions to ask the user by selecting question attributes that maximize the entropy of the resulting document set, in a manner similar to decision tree learning (Mitchell, 1997). Formally, we define a function infogain : Att x P(B) -+ R: infogain(att, B) = �− valsEP(dom(att)) |Batt=vals |lo I Batt=vals |B |g |B| The agent then selects questions att E Att that maximize the information gain with respect to the set of businesses satisfying the dialog history H: π(B, H) = arg max infogain(att, B|H) attEAtt 4 Evaluation 4.1 Experimental Setup We follow the standard approach of using the attributes of an individual business as a simulation of a user’s preferences (Chung, 2004; Young et al., 2010). For every business b E B we form an information need composed of all of b’s </context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Tom M. Mitchell. 1997. Machine Learning. McGrawHill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>226--234</pages>
<contexts>
<context position="9225" citStr="Somasundaran and Wiebe, 2009" startWordPosition="1360" endWordPosition="1363">e of Learned Sentiment Adjectives derived by this graph propagation method. The final lexicon has 1329 adjectives4, including 853 terms not in the original seed set. The lexicon is available for download.5 Evaluative Verbs In addition to this adjective lexicon, we take 56 evaluative verbs such as love and hate from admire-class VerbNet predicates (Kipper-Schuler, 2005). 2.2.2 Extraction Patterns To identify noun-phrases which are targeted by predicates in our sentiment lexicon, we develop hand-crafted extraction patterns defined over syntactic dependency parses (Blair-Goldensohn et al., 2008; Somasundaran and Wiebe, 2009) generated by the Stanford parser (Klein and Manning, 2003). Table 3 shows a sample of the aspects generated by these methods. Adj + NP It is common practice to extract any NP modified by a sentiment adjective. However, this simple extraction rule suffers from precision problems. First, reviews often contain sentiment toward irrelevant, non-business targets (Wayne is the target of excellent job in (1)). Second, hypothetical contexts lead to spurious extractions. In (2), the extraction +service is clearly wrong–in fact, the opposite sentiment is being expressed. (1) Wayne did an excellent job a</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings of ACL 2009, pages 226–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Mehmet H Goeker</author>
<author>Pat Langley</author>
</authors>
<title>A personalized system for conversational recommendations.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>21--393</pages>
<contexts>
<context position="816" citStr="Thompson et al., 2004" startWordPosition="109" endWordPosition="112">d.edu Abstract Recommendation dialog systems help users navigate e-commerce listings by asking questions about users’ preferences toward relevant domain attributes. We present a framework for generating and ranking fine-grained, highly relevant questions from user-generated reviews. We demonstrate our approach on a new dataset just released by Yelp, and release a new sentiment lexicon with 1329 adjectives for the restaurant domain. 1 Introduction Recommendation dialog systems have been developed for a number of tasks ranging from product search to restaurant recommendation (Chai et al., 2002; Thompson et al., 2004; Bridge et al., 2005; Young et al., 2010). These systems learn user requirements through spoken or text-based dialog, asking questions about particular attributes to filter the space of relevant documents. Traditionally, these systems draw questions from a small, fixed set of attributes, such as cuisine or price in the restaurant domain. However, these systems overlook an important element in users’ interactions with online product listings: usergenerated reviews. Huang et al. (2012) show that information extracted from user reviews greatly improves user experience in visual search interfaces</context>
</contexts>
<marker>Thompson, Goeker, Langley, 2004</marker>
<rawString>Cynthia A. Thompson, Mehmet H. Goeker, and Pat Langley. 2004. A personalized system for conversational recommendations. Journal of Artificial Intelligence Research (JAIR), 21:393–428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Whitney</author>
<author>Anoop Sarkar</author>
</authors>
<title>Bootstrapping via graph propagation.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012,</booktitle>
<pages>620--628</pages>
<location>Jeju Island,</location>
<contexts>
<context position="8053" citStr="Whitney and Sarkar (2012)" startWordPosition="1207" endWordPosition="1211">minated spurious propagation caused by one-off parsing errors. Second, we run the propagation algorithm for fewer iterations (two iterations for negative terms and one for positive terms). We found that additional iterations led to significant error propagation when neutral (italian) or ambiguous (thick) terms were assigned sentiment.3 Third, we update both non-seed and seed adjectives. This allows us to learn, for example, that the negative seed decadent is positive in the restaurant domain. Table 2 shows a sample of sentiment adjectives 3Our results are consistent with the recent finding of Whitney and Sarkar (2012) that cautious systems are better when bootstrapping from seeds. 500 Negative Sentiment institutional, underwhelming, not nice, burntish, unidentifiable, inefficient, not attentive, grotesque, confused, trashy, insufferable, grandiose, not pleasant, timid, degrading, laughable, under-seasoned, dismayed, torn Positive Sentiment decadent, satisfied, lovely, stupendous, sizable, nutritious, intense, peaceful, not expensive, elegant, rustic, fast, affordable, efficient, congenial, rich, not too heavy, wholesome, bustling, lush Table 2: Sample of Learned Sentiment Adjectives derived by this graph p</context>
</contexts>
<marker>Whitney, Sarkar, 2012</marker>
<rawString>Max Whitney and Anoop Sarkar. 2012. Bootstrapping via graph propagation. In Proceedings of the ACL 2012, pages 620–628, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP 2005 on Interactive Demonstrations,</booktitle>
<pages>34--35</pages>
<contexts>
<context position="6372" citStr="Wilson et al., 2005" startWordPosition="942" endWordPosition="945">edicate. For example, from the sentence “The place had great atmosphere, but the service was slow.” we extract two aspects: +atmosphere and –service. Our aspect extraction system has two steps. First we develop a domain specific sentiment lexicon. Second, we apply syntactic patterns to identify NPs targeted by these sentiment predicates. 2.2.1 Sentiment Lexicon Coordination Graph We generate a list of domain-specific sentiment adjectives using graph propagation. We begin with a seed set combining PARADIGM+ (Jo and Oh, 2011) with ‘strongly subjective’ adjectives from the OpinionFinder lexicon (Wilson et al., 2005), yielding 1342 seeds. Like Brody and Elhadad (2010), we then construct a coordination graph that links adjectives modifying the same noun, but to increase precision we require that the adjectives also be conjoined by and (Hatzivassiloglou and McKeown, 1997). This reduces problems like propagating positive sentiment to orange in good orange chicken. We marked adjectives that follow too or lie in the scope of negation with special prefixes and treated them as distinct lexical entries. Sentiment Propagation Negative and positive seeds are assigned values of 0 and 1 respectively. All other adject</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Opinionfinder: A system for subjectivity analysis. In Proceedings of HLT/EMNLP 2005 on Interactive Demonstrations, pages 34–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Milica Gaˇsi´c</author>
<author>Simon Keizer</author>
<author>Franc¸ois Mairesse</author>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
</authors>
<title>The hidden information state model: A practical framework for POMDP-based spoken dialogue management.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Young, Gaˇsi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu. 2010. The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2):150–174, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>