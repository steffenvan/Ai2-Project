<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001567">
<title confidence="0.978153">
Annotating Participant Reference in English Spoken Conversation
</title>
<author confidence="0.996586">
John Niekrasz and Johanna D. Moore
</author>
<affiliation confidence="0.998142">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.985659">
Edinburgh, EH8 9AB, UK
</address>
<email confidence="0.999692">
{jniekras,jmoore}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957681818182">
In conversational language, references to
people (especially to the conversation par-
ticipants, e.g., I, you, and we) are an es-
sential part of many expressed meanings.
In most conversational settings, however,
many such expressions have numerous po-
tential meanings, are frequently vague,
and are highly dependent on social and sit-
uational context. This is a significant chal-
lenge to conversational language under-
standing systems — one which has seen
little attention in annotation studies. In this
paper, we present a method for annotat-
ing verbal reference to people in conver-
sational speech, with a focus on reference
to conversation participants. Our goal is
to provide a resource that tackles the is-
sues of vagueness, ambiguity, and contex-
tual dependency in a nuanced yet reliable
way, with the ultimate aim of supporting
work on summarization and information
extraction for conversation.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974978723404">
Spoken conversation — the face-to-face verbal in-
teraction we have every day with colleagues, fam-
ily, and friends — is the most natural setting for
language use. It is how we learn to use language
and is universal to the world’s societies. This
makes it an ideal subject for research on the ba-
sic nature of language and an essential subject for
the development of technologies supporting natu-
ral communication. In this paper, we describe our
research on designing and applying an annotation
procedure for a problem of particular relevance to
conversational language —person reference.
The procedure is a coreference annotation of all
references to people, and the focus of our scheme
is on distinguishing different types of participant
reference (references to the conversation’s partic-
ipants), the predominant type of person reference
in face-to-face multi-party conversation. Partici-
pant reference is exemplified by the use of proper
names such as James or most commonly by the
pronouns I, you, and we.
Participant reference plays an essential role in
many of the most important types of expressed
meanings and actions in conversation, including
subjective language, inter-personal agreements,
commitments, narrative story-telling, establishing
social relationships, and meta-discourse. In fact,
some person-referring words are the most frequent
words in conversation.1
Perhaps contrary to intuition, however, in-
terpreting person-referring expressions can be
rather complex. Person-reference interpretation
is strongly dependent on social, situational, and
discourse context. The words you and we
are especially problematic. Either can be used
for generic, plural, or singular reference, as
addressee-inclusive or addressee-exclusive, in ref-
erence to hypothetical individuals or non-human
entities, or even metonymically in reference to ob-
jects connected to individuals (M¨uhlh¨ausler and
Harr´e, 1990; Wales, 1996). In addition, these and
many other issues are not simply occasional prob-
lems but arise regularly.
Consider the following utterance from the AMI
corpus of remote control design meetings, which
is typical of the corpus in terms of complexity of
person-reference.
</bodyText>
<footnote confidence="0.9676357">
1The words I and you are the most frequently used nom-
inals in several conversational corpora, including Switch-
board (Godfrey et al., 1992) and the AMI Meeting Cor-
pus (McCowan et al., 2005). In the British National Corpus
they are the two most common of any words in the demo-
graphic (i.e., conversational) subcorpus (Burnard, 2007), and
Google’s Web 1T 5-gram statistics (Brants and Franz, 2006)
list I and you as more frequent even than the word it. The
word we falls within the top 10 most frequent words in all of
these corpora.
</footnote>
<page confidence="0.928742">
256
</page>
<note confidence="0.9574195">
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 256–264,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9986894375">
“Current remote controls do not match well with
the operating behaviour of the user overall. For
example, you can see below there, seventy five
percent of users zap a lot, so you’ve got your
person sunk back in the sofa channel-hopping.”
As this example demonstrates, person-referring
expressions have many potential meanings and are
often vague or non-specific. In this case, “the
user” refers to a non-specific representative of a
hypothetical group, which is referred to itself as
“users.” The first use of “you” refers to the ad-
dressees, but the second use has a more ‘generic’
meaning whilst retaining an addressee-oriented
meaning as well. The phrase “your person” refers
to a specific hypothetical example of the “users”
referred to previously.
</bodyText>
<subsectionHeader confidence="0.998322">
1.1 Purpose of the Annotations
</subsectionHeader>
<bodyText confidence="0.997081159090909">
The annotation research we describe here aims at
addressing the fact that if conversational language
applications are to be useful and effective (our
interest is primarily with abstractive summariza-
tion), then accurate interpretation of reference to
the conversation’s participants is of critical impor-
tance. Our work looks at language as a means for
action (Clark, 1996), and our focus is on those ac-
tions that the participants themselves consider as
relevant and salient, such as the events occurring
in a meeting that might appear in the minutes of
the meeting. For our system to identify, distin-
guish, or describe such events, it is essential for
it to understand the participants’ roles and rela-
tionships to those events through interpreting their
linguistic expression within the dialogue. This in-
cludes understanding direct reference to partici-
pants and recognizing discourse structure through
evidence of referential coherence.
Another aim of our research is to increase un-
derstanding of the nature of participant reference
through presenting a nuanced yet reliable set of
type and property distinctions. We propose novel
distinctions concerning three main issues. The
first distinction concerns vagueness and indetermi-
nacy, which is often exploited by speakers when
using words such as you, they, and we. Our aim
is to provide a reliable basis for making an ex-
plicit distinction between specific and vague uses,
motivated by usefulness to the aforementioned ap-
plications. The second distinction concerns an
issue faced frequently in informal conversation,
where words typically used to do person-referring
are also commonly used in non-person-referring
ways. A principal goal is thus establishing reliable
person/non-person and referential/non-referential
distinctions for these words. The third issue con-
cerns addressing roles (i.e., speaker, addressee,
and non-addressee), which we propose can be a
useful means for further distinguishing between
different types of underspecified and generic refer-
ences, beyond the specific/underspecified/generic
distinctions made in schemes such as ACE (Lin-
guistic Data Consortium, 2008).
</bodyText>
<subsectionHeader confidence="0.997955">
1.2 Summary and Scope of Contributions
</subsectionHeader>
<bodyText confidence="0.999938333333333">
The work described in this paper includes the de-
sign of an annotation procedure and a statistical
analysis of a corpus of annotations and their re-
liability. The procedure we propose (Section 3)
is based on a simple non-anaphoric coreference-
like scheme, modest in comparison to much pre-
vious work. The produced dataset (Section 4) in-
cludes annotations of 11,000 occasions of person-
referring in recorded workplace meetings. Our
analysis of the dataset includes a statistical sum-
mary of interesting results (Section 4.1) and an
analysis of inter-coder agreement (with discussion
of specific disagreements) for the introduced dis-
tinctions (Section 4.2).
Though our annotation procedure is designed
primarily for multi-party spoken conversation,
some of the central issues that concern us, such
as addressee inclusion and vagueness, arise in
textual and non-conversational settings as well.
Our scheme therefore has relevance to general
work on reference annotation, though principally
to settings where social relationships between
the participants (i.e., speakers/authors and ad-
dressees/readers) are important.
</bodyText>
<sectionHeader confidence="0.998494" genericHeader="introduction">
2 Related Annotation Schemes
</sectionHeader>
<bodyText confidence="0.9996778">
Previous work on reference annotation has cov-
ered a wide range of issues surrounding reference
generally. It is useful to categorize this work ac-
cording to the natural language processing tasks
the annotations are designed to support.
</bodyText>
<subsectionHeader confidence="0.968733">
2.1 Schemes for anaphora and generation
</subsectionHeader>
<bodyText confidence="0.999426333333333">
Several schemes have been designed with the goal
of testing linguistic theoretical models of dis-
course structure or for use in the study of discourse
processing problems like anaphora resolution and
reference generation. These schemes have been
applied to both text and dialogue and label dis-
</bodyText>
<page confidence="0.99314">
257
</page>
<bodyText confidence="0.999981137931035">
course references with a rich set of syntactic, se-
mantic, and pragmatic properties. For example,
the DRAMA scheme (Passonneau, 1997) and the
GNOME scheme (Poesio, 2000; Poesio, 2004) in-
clude labels for features such as bridging relation
type and NP type in addition to a rich representa-
tion of referent semantics. Other schemes label an-
imacy, prosody, and information structure to study
their relationship to the organization and salience
of discourse reference (Nissim et al., 2004; Cal-
houn et al., 2005). Recent developments include
the explicit handling of anaphoric ambiguity and
discourse deixis (Poesio and Artstein, 2008).
Despite the depth and detail of these schemes,
participant reference has not been their main con-
cern. The annotations by Poesio et al. (2000;
2004) include dialogue source material, but the
rather constrained interactional situations do not
elicit a rich set of references to participants. The
scheme thus employs simple default labels for
words like I and you. The work by Nissim et
al., (2004) is an annotation of the Switchboard cor-
pus (Godfrey et al., 1992), which contains only
two participants who are neither co-present nor
socially connected. Participant reference is thus
rather constrained. Other than labeling corefer-
entiality, the Nissim scheme includes only a sin-
gle distinction between referential and generic in-
stances of the word you.
</bodyText>
<subsectionHeader confidence="0.989531">
2.2 Schemes for information extraction
</subsectionHeader>
<bodyText confidence="0.999981688888889">
In contrast to the schemes described above, which
are mainly driven toward investigating linguistic
theories of discourse processing, some reference
annotation projects are motivated instead by infor-
mation extraction applications. For these projects
(which includes our own), a priority is placed on
entity semantics and coreference to known entities
in the world. For example, the objective of the Au-
tomatic Content Extraction (ACE) program (Dod-
dington et al., 2004) is to recognize and extract
entities, events, and relations between them, di-
rectly from written and spoken sources, mostly
from broadcast news. The schemes thus focus
on identifying and labeling the properties of en-
tities in the real world, and then marking expres-
sions as referring to these entities. Recent work
in the ACE project has expanded the scope of
this task to include cross-document recognition
and resolution (Strassel et al., 2008). In the ACE
scheme (Linguistic Data Consortium, 2008), per-
son reference is a central component, and in the
broadcast conversation component of the corpus
there is an extensive inventory of participant refer-
ences. The annotation scheme contains a distinc-
tion between specific, underspecified, and general
entities, as well as a distinction between persons
and organizations.
Another closely related set of studies are four
recent investigations of second-person reference
resolution (Gupta et al., 2007a; Gupta et al.,
2007b; Frampton et al., 2009; Purver et al.,
2009). These studies are based upon a common
set of annotations of the word you in source mate-
rial from the Switchboard and ICSI Meeting cor-
pora. The purpose for the annotations was to
support learning of classifiers for two main prob-
lems: disambiguation of the generic/referential
distinction, and reference resolution for referential
cases. In addition to the generic/referential dis-
tinction and an addressing-based reference anno-
tation, the scheme employed special classes for re-
ported speech and fillers and allowed annotators to
indicate vague or difficult cases. Our work builds
directly upon this work by extending the annota-
tion scheme to all person-referring expressions.
</bodyText>
<sectionHeader confidence="0.980276" genericHeader="method">
3 Annotation Method
</sectionHeader>
<bodyText confidence="0.9999765">
Our person-reference annotation method consists
of two main phases: a preliminary phase where
the first names of the conversation participants are
identified, and a subsequent person reference la-
beling process. The first phase is not of central
concern in this paper, though we provide a brief
summary below (Section 3.2). The primary focus
of this paper is the second phase (Section 3.3), dur-
ing which every instance of person-referring oc-
curring in a given meeting is labelled. We pro-
vide more detail concerning the most novel and
challenging aspects of the person-referring label-
ing process in Section 3.4 and present a brief sum-
mary of the annotation tool in Section 3.5.
</bodyText>
<subsectionHeader confidence="0.999699">
3.1 Source Material
</subsectionHeader>
<bodyText confidence="0.999982625">
The source material is drawn from two source
corpora: the AMI corpus (McCowan et al.,
2005), which contains experimentally-controlled
scenario-driven design meetings, and the ICSI cor-
pus (Janin et al., 2003), which contains naturally
occurring workplace meetings. All the meetings
have at least four participants and have an average
duration of about 45 minutes. In the AMI corpus,
</bodyText>
<page confidence="0.982927">
258
</page>
<bodyText confidence="0.9999896">
the participants are experimental subjects who are
assigned institutional roles, e.g. project manager
and industrial designer. This helps to establish
controlled social relationships within the group,
but generally limits the types of person referring.
The ICSI meetings are naturally occurring and ex-
hibit complex pre-existing social relationships be-
tween the participants. Person referring in this cor-
pus is quite complex and often includes other in-
dividuals from the larger institution and beyond.
</bodyText>
<subsectionHeader confidence="0.999927">
3.2 Labeling Participant Names
</subsectionHeader>
<bodyText confidence="0.999997805555556">
The first phase of annotation consists of identify-
ing the names of the participants. We perform this
task for every participant in every meeting in the
AMI and ICSI source corpora, which totals 275
unique participants in 246 meetings. Despite the
fact that the participants’ are given anonymized
identifiers by the corpus creators, determining par-
ticipants’ names is possible because name men-
tions are not excised from the speech transcript.
This allows identification of the names of any par-
ticipants who are referred to by name in the dia-
logue, as long as the referent is disambiguated by
contextual clues such as addressing.
To extract name information, the list of capi-
talized words in the speech transcript is scanned
manually for likely person names. This was done
manually due to the difficulty of training a suffi-
ciently robust named-entity recognizer for these
corpora. Proceeding through each meeting for
which any participant names are yet unidentified,
and taking each potential name token in order
of frequency of occurrence in that meeting, short
segments of the recording surrounding the occur-
rences were replayed. In most cases, the name was
used in reference to a participant and it was clear
from discourse context which participant was the
intended referent. In the AMI meetings, 158 of
223 (71%) of the participants’ first names were
identified. In the ICSI meetings, 36 of 52 (69%)
were identified. While these numbers may seem
low, failure to determine a name was generally as-
sociated with a low level of participation of the
individual either in terms of amount of speech or
number of meetings attended. As such, the propor-
tion of utterances across both corpora for which
the speaker’s name is identified is actually 91%.
</bodyText>
<subsectionHeader confidence="0.963821">
3.3 Person-reference Annotation
</subsectionHeader>
<bodyText confidence="0.9999849">
The second, principal phase of annotation con-
sists of annotating person-referring — instances
of verbal reference to people. The recognition
of person-referring requires the annotator to si-
multaneously identify whether a referring event
has occurred, and whether the referent is a per-
son. In practice, this is divided into four an-
notation steps: markable identification, referent
identification, functional category labeling, and
co-reference linking. For non-specific references,
there is an additional step of labeling addressing
properties. For each meeting, annotators label ev-
ery instance of person-referring in every utterance
in the meeting, performing the steps in sequence
for each utterance. Section 4 describes the set of
meetings annotated. The UML diagram in Fig-
ure 1 depicts the formal data structure produced
by the procedure.2
The first step is markable identification, which
involves recognizing person-referring expres-
sions in the transcript. Only expressions that are
noun phrases are considered, and only the head
noun is actually labeled by the annotator — the
extent of the expression is not labeled. These iden-
tified head nouns are called markables. Note,
however, that before human annotation begins, an
automatic process identifies occurrences of words
that are likely to be head nouns in person-referring
expressions. The list of words includes all per-
sonal pronouns except it, them, and they (these
are more likely to be non-person-referring in our
dataset) and the wh-pronouns (not labeled in our
scheme). It also includes any occurrences of
the previously identified proper names. Some of
the automatically identified words might not be
person-referring. Also, there may be instances of
person-referring that are not automatically iden-
tified. Annotators do not unmark any of the au-
tomatically identified words, even if they are not
person-referring. The resulting set of manually
and automatically identified words, which may or
may not be person-referring, constitute the com-
plete set of markables.
The second step is the labeling of person refer-
ents. Any people or groups of people that are re-
ferred to specifically and unambiguously (see Sec-
tion 3.4.3 for details) are added by the annotator
to a conversation referent list. The list is auto-
matically populated with each of the conversation
participants.
</bodyText>
<footnote confidence="0.58126725">
2The diagram may also be viewed informally as loosely
reflecting a decision tree for the main annotation steps. A
complete coding manual is available from the author’s web
site.
</footnote>
<page confidence="0.978009">
259
</page>
<table confidence="0.931823473684211">
Referent List Conversation Transcript
* 1
Word
speaker: Participant
word: String
startTime: Double
Markable Word
Non-Markable Word
Non-Referring Markable
category: Enum: {
FUNC-FILLER,
FUNC-NON-PREF }
1
Person Referent
Person-Referring Markable
ATTR-QUANTIFIED-SUPERSET: Boolean
category: Enum: {
FUNC-PREF-VOCATIVE,
FUNC-PREF-INTRODUCTION,
</table>
<figure confidence="0.729377533333333">
1..* FUNC-PREF-TROUBLE,
FUNC-PREF-DEFAULT }
1
*
Specific Real Referent
id: String
NICKNAME: String
Underspecified Referent (PERSON-OTHER)
ATTR-SPEAKER-INCL: Boolean
ATTR-ADDRESSEE-INCL: Boolean
ATTR-OTHER-INCL: Boolean
PERSON-SINGLE
2..* 0..*
members
PERSON-MULTIPLE
</figure>
<figureCaption confidence="0.999988">
Figure 1: A UML diagram depicting the data structure used to represent and store the annotations.
</figureCaption>
<bodyText confidence="0.99998585">
The third step consists of labeling markables
with a functional category (FUNC-*). The func-
tional categories serve two main purposes. They
are used to distinguish person-referring markables
from all others (corresponding to the two main
boxes in the diagram), and they are used to distin-
guish between specific dialogue purposes (the cat-
egories listed within the boxes, see Section 3.4.4).
The final step is to link the markables that were
labeled as person-referring to the appropriate ref-
erent in the referent list. This is only done for
specific and unambiguous referring. Otherwise,
the referent is said to be underspecified, and in-
stead of linking the markable to a referent, it is la-
beled with three binary addressing inclusion at-
tributes. Inclusion attributes label whether the
speaker, addressee, or any other individuals are in-
cluded in the set of people being referred to, given
the social, situational, and discourse context (de-
tails in Section 3.4.5).
</bodyText>
<subsectionHeader confidence="0.871242">
3.4 Special Issues
3.4.1 Defining ‘person’ and ‘referring’
</subsectionHeader>
<bodyText confidence="0.999941851851852">
To be person-referring, an expression must sat-
isfy two conditions. First, the expression’s pri-
mary contribution to the speaker’s intended mean-
ing or purpose must be either to identify, label,
describe, specify, or address. These are the ba-
sic types of referring. Second, the referent being
identified, labeled, etc., must be a person, which
we define to include any of the following: a dis-
tinct person in the real world; a fictitious or hypo-
thetical person; a human agent, perceiver, or par-
ticipant in a described event, scene, or fact; a class,
type, or kind of person, or representative thereof;
a specification or description of a person or set of
people; a (possibly vaguely defined) group or col-
lection of any of the above; the human race as a
whole, or a representative thereof.
If a noun phrase is used to do person-referring
as defined, the associated markable is labeled with
one of the four person-referring functional cat-
egories (FUNC-PREF-*). If a markable is not
person-referring (either non-referring or referring
to a non-person referent), it is labeled with the
functional category FUNC-NON-PREF. The one
exception to this is the use of a pre-defined list of
common discourse fillers such as you know and I
mean. When used as fillers, these are labeled with
the non-referential FUNC-FILLER category.
</bodyText>
<page confidence="0.98531">
260
</page>
<subsectionHeader confidence="0.786154">
3.4.2 Joint action and referring ‘trouble’
</subsectionHeader>
<bodyText confidence="0.999984166666667">
Annotators are asked to consider occasions of re-
ferring to be joint actions between the speaker and
the addressee(s) of the utterance. The annotator
assumes the role of an overhearer and considers
as referring any case where the speaker’s intended
purpose is to refer. If the instance of referring
is not successfully negotiated between the partic-
ipants (i.e., common ground is not achieved), but
the speaker’s intended purpose is to refer, then the
annotator marks this as FUNC-PREF-TROUBLE.
This is used to identify problematic cases for fu-
ture study.
</bodyText>
<subsubsectionHeader confidence="0.523862">
3.4.3 Specific, Unambiguous Referring
</subsubsectionHeader>
<bodyText confidence="0.999720090909091">
Only the referents of specific, unambiguous re-
ferring to a person in the real world (PERSON-
SINGLE) are included in the conversation referent
list and made the subject of coreference annota-
tion. References to more than one such individual
can qualify (PERSON-MULTIPLE), but only if the
members are precisely enumerable and qualify in-
dividually. The motivation for this distinction is to
distinguish references that would be directly use-
ful to applications. Coreference for underspecified
references is not labeled.
</bodyText>
<subsectionHeader confidence="0.933584">
3.4.4 Special Functional Categories
</subsectionHeader>
<bodyText confidence="0.999991666666667">
Two functional categories are used to distinguish
special uses of person-referring for subsequent
use in speaker name induction (the task of auto-
matically learning participants’ names). The two
categories are FUNC-PREF-INTRODUCTION and
FUNC-PREF-VOCATIVE, which specify personal
introductions such as “Hi, I’m John,” and vocative
addressing such as “What do you think, Jane?”
These categories are used only for proper names.
</bodyText>
<subsectionHeader confidence="0.898592">
3.4.5 Addressing-based Inclusion Attributes
</subsectionHeader>
<bodyText confidence="0.999995884615385">
A major novelty in our annotation scheme is the
use of addressing-based distinctions for under-
specified referents. Rather than using the labels
‘generic’ or ‘indeterminate’, we employ three bi-
nary attributes (ATTR-*-INCL) that label whether
the speaker, addressee or any other real individuals
are members of the set of people referred to.
The use of this distinction is informed by the no-
tion that addressing distinctions are of central im-
portance to the recognition of joint activity type,
structure, and participation roles. A generic pro-
noun, for example, will often have all three cat-
egories labeled positively. But as an example
of where this scheme creates a novel distinction,
consider the phrase “You really take a beating
out there on the pitch!”, where the speaker is a
football player describing the nature of play to
someone who has never played the game. This
‘generic’ use of you, used in an activity of autobi-
ographical description, is intuitively interpreted as
not including the addressee (ATTR-ADDRESSEE-
INCL=FALSE) but including the speaker and others
(ATTR-{SPEAKER,OTHER}-INCL=TRUE). These
distinctions are hard to motivate linguistically yet
critical to identifying useful properties relating to
participation in the communicative activity.
</bodyText>
<subsectionHeader confidence="0.958737">
3.4.6 Special or Difficult Cases
</subsectionHeader>
<bodyText confidence="0.999967909090909">
In some cases, an annotator can determine that a
reference is specific and unambiguous for the par-
ticipants but the annotator himself is unable to de-
termine the identity of the referent. This is gener-
ally due to a lack of contextual awareness such as
not having adequate video. In such cases, the an-
notator assigns a special REF-UNKNOWN referent.
Other difficult aspects of our annotation proce-
dure are covered in the annotation manual, includ-
ing handling of disfluencies, quantification, and
identifying lexical heads.
</bodyText>
<subsectionHeader confidence="0.998093">
3.5 Annotation Tool
</subsectionHeader>
<bodyText confidence="0.9999932">
The annotations were collected using a software
tool we have designed for discrete event-based an-
notation of multi-modal corpora. The tool uses a
simple, low-latency text-based interface that dis-
plays multiple streams of discrete events in tempo-
ral order across the screen. In our case, the events
are time-synchronized words that are distributed
to different rows according to speaker. The inter-
face allows keyboard input only and is synchro-
nized with the MPlayer playback engine.
</bodyText>
<sectionHeader confidence="0.998744" genericHeader="evaluation">
4 Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.999721">
4.1 Statistical summary
</subsectionHeader>
<bodyText confidence="0.9999763">
The dataset consists of approximately 11,000 in-
dividually annotated referring expressions in 16
experimentally-controlled, scenario-driven design
meetings from the AMI corpus (McCowan et al.,
2005) and 3 natural workplace meetings from
the ICSI corpus (Janin et al., 2003). Figure 2
shows, for each grammatical type of referring ex-
pression, the frequency of occurrence of the five
principal markable types, which are defined to
consist of the two non-person-referring functional
</bodyText>
<page confidence="0.995179">
261
</page>
<table confidence="0.998311666666667">
Gram. Freq. Ent.
(%) (bits)
1PS 33.7 .57
1PP 24.6 .67
2P 23.7 1.78
3PS .9 .66
3PP 7.2 1.25
QUANT 1.0 1.14
OTHER 8.9 1.57
</table>
<tableCaption confidence="0.999216">
Table 1: A statistical summary of all the mark-
</tableCaption>
<bodyText confidence="0.8862446">
ables in the dataset by grammatical type (gram.),
showing their frequency relative to all markables
(freq.), the entropy of the referring type given the
grammatical type (ent.), and a list of the most fre-
quent examples (freq. words).
</bodyText>
<figure confidence="0.81507025">
1PS
1PP
2P
3PS
3PP
QUANT
OTHER
0 500 1000 2000 3000
Freq. words
I, my, me
we, our, us
you, your, yours
he, his, she
they, them, their
everyone, everybody
people, guys, user
</figure>
<figureCaption confidence="0.709482">
Figure 2: Frequency of occurrence of referring
types for the whole corpus, by grammatical type
of the referring expression.
</figureCaption>
<bodyText confidence="0.998862517241379">
categories (FUNC-NON-PREF and FUNC-FILLER),
and a breakdown of person-referring according
to the type of person referent: a specific indi-
vidual (PERSON-SINGLE), multiple specific indi-
viduals (PERSON-MULTIPLE), or underspecified
(PERSON-OTHER). The grammatical types in-
clude a grouping of the personal pronouns by
grammatical person and number (1PS, 1PP, 2P,
3PS, 3PP), the quantified pronouns (QUANT), and
a group including all other expressions (OTHER).
Table 1 shows the relative frequency for the gram-
matical types and the most frequent expressions.
As is usually found in conversation, first-
person and second-person pronouns are the most
frequent, collectively comprising 82.0% of all
person-referring expressions. Of particular inter-
est, due to their high frequency and multiple possi-
ble referential meanings, are the 1PP and 2P cate-
gories (e.g., we and you), comprising respectively
24.6% and 23.7% of all person-referring expres-
sions. In Table 1, we show the information en-
tropy of the referring type, given the grammati-
cal category. This measures the uncertainty one
has about the type, given knowledge of only the
grammatical type of the expression. The analysis
reveals that second-person pronouns are a partic-
ularly challenging reference resolution problem,
with a broad and relatively even distribution across
referring types.
</bodyText>
<subsectionHeader confidence="0.955129">
4.2 Reliability and Error Analysis
</subsectionHeader>
<bodyText confidence="0.999623636363636">
To show that our annotations are credible and suit-
able for empirical testing, we must establish that
the subjective distinctions defined in our scheme
may be applied by individuals other than the
scheme developers. To do this, we assess inter-
coder agreement between two independent anno-
tators on four meetings from the AMI corpus, us-
ing Cohen’s Kappa (Cohen, 1960). Each of the
decisions in the annotation procedure are assessed
separately: markable identification, labeling ref-
erentiality, labeling specificity of person refer-
ents, and labeling addressing inclusion attributes.
Because each decision depends on the previous,
we employ a hierarchical assessment procedure
that considers only instances where the annota-
tors have agreed on previous decisions. This kind
of multi-level assessment corresponds to that de-
scribed and used in Carletta et al., (1997).
Markables The first annotation decision of in-
terest is the identification of markables. Markables
are either automatically identified occurrences of
a pre-defined list of pronouns, or they are identi-
</bodyText>
<figure confidence="0.9216008">
FUNC−PREF / PERSON−SINGLE
FUNC−PREF / PERSON−MULTIPLE
FUNC−PREF / PERSON−OTHER
FUNC−NON−PREF
FUNC−FILLER
</figure>
<page confidence="0.991145">
262
</page>
<bodyText confidence="0.999149571428572">
fied manually by the annotators. Agreement on
this task, assessed only for manually identified
words, was very good (n=.94). Error analysis
shows that the main issue with this decision was
not determining lexical heads, but rather deter-
mining whether phrases such as “all age groups,”
“the older generation,” and “the business market”
should be considered as referring to people or not.
Person referentiality The next annotation deci-
sion is between person-referring and non-person-
referring markables. For assessment of this
choice, we measure agreement on a three-way
categorization of the agreed markables as either
FUNC-NON-PREF, FUNC-FILLER, or one of the
FUNC-PREF-* categories. Agreement on this task
was good (n=.77). The only errors occurred on
first- and second-person pronouns and between the
FUNC-NON-PREF and FUNC-PREF-* categories.
Error analysis suggests confusion tends to occur
when pronouns are used with semantically light
verbs like go, get, and have, for example in phrases
such as “there we go” and “you’ve got the main
things on the front.” As in the latter example,
some of the difficult choices appear to involve de-
scriptions of states, which the speaker can choose
to express either from various participants’ points
of view, as above, or alternatively without ex-
plicit subjectivity, e.g., “the main things are on the
front.”
Specificity and cardinality The next choice we
assess is the decision between referring specif-
ically to a single person (PERSON-SINGLE), to
multiple people (PERSON-MULTIPLE), or as un-
derspecified (also referred to as PERSON-OTHER).
Agreement on this choice was very good (n=.91),
though considering only the difficult 1PP and 2P
grammatical categories (e.g., we and you), agree-
ment was less strong (n=.75). Note that due to the
hierarchical nature of the scheme, evaluation con-
sidered only cases where both annotators labeled
a word as person-referring. Errors on this decision
often involved ambiguities in addressing, where
one annotator believed a particular individual was
being addressed by you and the other thought the
whole group was being addressed. Another com-
mon disagreement was on cases such as “we want
it to be original,” where we was interpreted by one
annotator as referring to the present group of par-
ticipants, but by the other as (presumably) refer-
ring to the organization to which the participants
belong.
Addressing inclusion attributes For the three
inclusion attributes for underspecified referents
(ATTR-*-INCL), agreement is calculated three
times, once for each of the binary attributes.
Agreement was good, though slightly problematic
for addressee inclusion (speaker n=.72; addressee
n=.50; other n=.66). Disagreements were mainly
for occurrences of you like the example of autobi-
ography in Section 3.4.5. For example, “it’s your
best friend” was used to explain why a dog is the
speaker’s favorite animal, and the annotators dis-
agreed on whether the addressee was included.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999916407407407">
We have presented an annotation scheme and a
set of annotations that address participant refer-
ence — a conversational language problem that
has seen little previous annotation work. Our fo-
cus has been on eliciting novel distinctions that we
hypothesize will help us to distinguish, label, and
summarize conversational activities. We also ad-
dress the issues of vagueness, ambiguity, and con-
textual dependency in participant referring.
Based on analysis of inter-annotator agreement,
the major distinctions proposed by the scheme ap-
pear to be reliably codable. In addition, our sta-
tistical analysis shows that our dataset contains a
wide variety of participant references and should
be a useful resource for several reference resolu-
tion problems for conversation. Our novel method
for distinguishing specific reference to real indi-
viduals appears to be very reliably codable. Our
novel addressing-based distinctions for underspec-
ified reference are less reliable but adequate as a
resource for some dialogue structuring tasks.
Further work proposed for this task includes
labeling a variety of conversational and non-
conversation genres. Our immediate concern is to
apply our annotations in the training and/or test-
ing of machine learning approaches to discourse
segmentation and abstractive summarization.
</bodyText>
<sectionHeader confidence="0.99964" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999275571428571">
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram, Version 1. Linguistic Data Consortium. Cat-
alog ID: LDC2006T13.
Lou Burnard, 2007. Reference Guide for the British
National Corpus (XML Edition). Research Tech-
nologies Service at Oxford University Computing
Services.
</reference>
<page confidence="0.981694">
263
</page>
<reference confidence="0.999714158536585">
Sasha Calhoun, Malvina Nissim, Mark Steedman, and
Jason Brenier. 2005. A framework for annotating
information structure in discourse. In Proceedings
of the ACL Workshop on Frontiers in Corpus Anno-
tation II: Pie in the Sky.
Jean Carletta, Stephen Isard, Anne H. Anderson,
Gwyneth Doherty-Sneddon, Amy Isard, and Jacque-
line C. Kowtko. 1997. The reliability of a dialogue
structure coding scheme. Computational Linguis-
tics, 23(1):13–31.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37–46.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program: Tasks, data, and evaluation. In
Proc. LREC.
Matthew Frampton, Raquel Fernndez, Patrick Ehlen,
Mario Christoudias, Trevor Darrell, and Stanley Pe-
ters. 2009. Who is ”you”? Combining linguis-
tic and gaze features to resolve second-person ref-
erences in dialogue. In Proc. EACL.
John J. Godfrey, Edward Holliman, and J. McDaniel.
1992. SWITCHBOARD: Telephone speech corpus
for research and development. In Proc. ICASSP,
pages 517–520, San Francisco, CA.
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007a. Resolving “you” in multi-
party dialog. In Proc. SIGdial, pages 227–230.
Surabhi Gupta, Matthew Purver, and Daniel Jurafsky.
2007b. Disambiguating between generic and refer-
ential “you” in dialog. In Proc. ACL.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting cor-
pus. In Proc. ICASSP, volume 1, pages 364–367.
Linguistic Data Consortium, 2008. ACE (Au-
tomatic Content Extraction) English Annotation
Guidelines for Entities, Version 6.5. Down-
loaded from http://projects.ldc.upenn.
edu/ace/annotation/.
I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-
ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lin-
coln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Cor-
pus. In Proceedings of Measuring Behavior 2005,
the 5th International Conference on Methods and
Techniques in Behavioral Research, Wageningen,
Netherlands.
Peter M¨uhlh¨ausler and Rom Harr´e. 1990. Pronouns
and People: The Linguistic Construction of Social
and Personal Identity. Blackwell, Oxford.
Malvina Nissim, Shipra Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for
information status in dialogue. In Proc. LREC.
R. Passonneau, 1997. Instructions for applying dis-
course reference annotation for multiple applica-
tions (DRAMA).
Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proc. LREC.
Massimo Poesio, 2000. The GNOME Annotation
Scheme Manual, Version 4. University of Edin-
burgh, HCRC and Informatics.
Massimo Poesio. 2004. Discourse annotation and se-
mantic annotation in the GNOME corpus. In Pro-
ceedings of the ACL 2004 Workshop on Discourse
Annotation, pages 72–79.
Matthew Purver, Raquel Fernndez, Matthew Framp-
ton, and Stanley Peters. 2009. Cascaded lexicalised
classifiers for second-person reference resolution.
In Proc. SIGdial, pages 306–309.
Stephanie Strassel, Mark Przybocki, Kay Peterson,
Zhiyi Song, and Kazuaki Maeda1. 2008. Linguistic
resources and evaluation techniques for evaluation
of cross-document automatic content extraction. In
Proc. LREC.
Katie Wales. 1996. Personal pronouns in present-day
English. Cambridge University Press, Cambridge.
</reference>
<page confidence="0.998226">
264
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.458586">
<title confidence="0.999842">Annotating Participant Reference in English Spoken Conversation</title>
<author confidence="0.997987">John Niekrasz</author>
<author confidence="0.997987">D Johanna</author>
<affiliation confidence="0.9958085">School of University of</affiliation>
<address confidence="0.508718">Edinburgh, EH8 9AB,</address>
<abstract confidence="0.995777043478261">In conversational language, references to people (especially to the conversation pare.g., and are an essential part of many expressed meanings. In most conversational settings, however, many such expressions have numerous potential meanings, are frequently vague, and are highly dependent on social and situational context. This is a significant challenge to conversational language understanding systems — one which has seen little attention in annotation studies. In this paper, we present a method for annotatverbal reference to conversational speech, with a focus on reference conversation Our goal is to provide a resource that tackles the issues of vagueness, ambiguity, and contextual dependency in a nuanced yet reliable way, with the ultimate aim of supporting work on summarization and information extraction for conversation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram, Version 1. Linguistic Data Consortium. Catalog ID: LDC2006T13.</booktitle>
<contexts>
<context position="3686" citStr="Brants and Franz, 2006" startWordPosition="546" endWordPosition="549">er issues are not simply occasional problems but arise regularly. Consider the following utterance from the AMI corpus of remote control design meetings, which is typical of the corpus in terms of complexity of person-reference. 1The words I and you are the most frequently used nominals in several conversational corpora, including Switchboard (Godfrey et al., 1992) and the AMI Meeting Corpus (McCowan et al., 2005). In the British National Corpus they are the two most common of any words in the demographic (i.e., conversational) subcorpus (Burnard, 2007), and Google’s Web 1T 5-gram statistics (Brants and Franz, 2006) list I and you as more frequent even than the word it. The word we falls within the top 10 most frequent words in all of these corpora. 256 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 256–264, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics “Current remote controls do not match well with the operating behaviour of the user overall. For example, you can see below there, seventy five percent of users zap a lot, so you’ve got your person sunk back in the sofa channel-hopping.” As this example demonstrates, person-referring expressi</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram, Version 1. Linguistic Data Consortium. Catalog ID: LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<date>2007</date>
<booktitle>Reference Guide for the British National Corpus (XML Edition). Research Technologies Service at</booktitle>
<institution>Oxford University Computing Services.</institution>
<contexts>
<context position="3622" citStr="Burnard, 2007" startWordPosition="538" endWordPosition="539">´e, 1990; Wales, 1996). In addition, these and many other issues are not simply occasional problems but arise regularly. Consider the following utterance from the AMI corpus of remote control design meetings, which is typical of the corpus in terms of complexity of person-reference. 1The words I and you are the most frequently used nominals in several conversational corpora, including Switchboard (Godfrey et al., 1992) and the AMI Meeting Corpus (McCowan et al., 2005). In the British National Corpus they are the two most common of any words in the demographic (i.e., conversational) subcorpus (Burnard, 2007), and Google’s Web 1T 5-gram statistics (Brants and Franz, 2006) list I and you as more frequent even than the word it. The word we falls within the top 10 most frequent words in all of these corpora. 256 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 256–264, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics “Current remote controls do not match well with the operating behaviour of the user overall. For example, you can see below there, seventy five percent of users zap a lot, so you’ve got your person sunk back in the sofa channel-h</context>
</contexts>
<marker>Burnard, 2007</marker>
<rawString>Lou Burnard, 2007. Reference Guide for the British National Corpus (XML Edition). Research Technologies Service at Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Calhoun</author>
<author>Malvina Nissim</author>
<author>Mark Steedman</author>
<author>Jason Brenier</author>
</authors>
<title>A framework for annotating information structure in discourse.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Frontiers in Corpus Annotation II: Pie in the Sky.</booktitle>
<contexts>
<context position="9164" citStr="Calhoun et al., 2005" startWordPosition="1378" endWordPosition="1382">ora resolution and reference generation. These schemes have been applied to both text and dialogue and label dis257 course references with a rich set of syntactic, semantic, and pragmatic properties. For example, the DRAMA scheme (Passonneau, 1997) and the GNOME scheme (Poesio, 2000; Poesio, 2004) include labels for features such as bridging relation type and NP type in addition to a rich representation of referent semantics. Other schemes label animacy, prosody, and information structure to study their relationship to the organization and salience of discourse reference (Nissim et al., 2004; Calhoun et al., 2005). Recent developments include the explicit handling of anaphoric ambiguity and discourse deixis (Poesio and Artstein, 2008). Despite the depth and detail of these schemes, participant reference has not been their main concern. The annotations by Poesio et al. (2000; 2004) include dialogue source material, but the rather constrained interactional situations do not elicit a rich set of references to participants. The scheme thus employs simple default labels for words like I and you. The work by Nissim et al., (2004) is an annotation of the Switchboard corpus (Godfrey et al., 1992), which contai</context>
</contexts>
<marker>Calhoun, Nissim, Steedman, Brenier, 2005</marker>
<rawString>Sasha Calhoun, Malvina Nissim, Mark Steedman, and Jason Brenier. 2005. A framework for annotating information structure in discourse. In Proceedings of the ACL Workshop on Frontiers in Corpus Annotation II: Pie in the Sky.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
<author>Stephen Isard</author>
<author>Anne H Anderson</author>
<author>Gwyneth Doherty-Sneddon</author>
<author>Amy Isard</author>
<author>Jacqueline C Kowtko</author>
</authors>
<title>The reliability of a dialogue structure coding scheme.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="28648" citStr="Carletta et al., (1997)" startWordPosition="4375" endWordPosition="4378"> intercoder agreement between two independent annotators on four meetings from the AMI corpus, using Cohen’s Kappa (Cohen, 1960). Each of the decisions in the annotation procedure are assessed separately: markable identification, labeling referentiality, labeling specificity of person referents, and labeling addressing inclusion attributes. Because each decision depends on the previous, we employ a hierarchical assessment procedure that considers only instances where the annotators have agreed on previous decisions. This kind of multi-level assessment corresponds to that described and used in Carletta et al., (1997). Markables The first annotation decision of interest is the identification of markables. Markables are either automatically identified occurrences of a pre-defined list of pronouns, or they are identiFUNC−PREF / PERSON−SINGLE FUNC−PREF / PERSON−MULTIPLE FUNC−PREF / PERSON−OTHER FUNC−NON−PREF FUNC−FILLER 262 fied manually by the annotators. Agreement on this task, assessed only for manually identified words, was very good (n=.94). Error analysis shows that the main issue with this decision was not determining lexical heads, but rather determining whether phrases such as “all age groups,” “the </context>
</contexts>
<marker>Carletta, Isard, Anderson, Doherty-Sneddon, Isard, Kowtko, 1997</marker>
<rawString>Jean Carletta, Stephen Isard, Anne H. Anderson, Gwyneth Doherty-Sneddon, Amy Isard, and Jacqueline C. Kowtko. 1997. The reliability of a dialogue structure coding scheme. Computational Linguistics, 23(1):13–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="5147" citStr="Clark, 1996" startWordPosition="779" endWordPosition="780">but the second use has a more ‘generic’ meaning whilst retaining an addressee-oriented meaning as well. The phrase “your person” refers to a specific hypothetical example of the “users” referred to previously. 1.1 Purpose of the Annotations The annotation research we describe here aims at addressing the fact that if conversational language applications are to be useful and effective (our interest is primarily with abstractive summarization), then accurate interpretation of reference to the conversation’s participants is of critical importance. Our work looks at language as a means for action (Clark, 1996), and our focus is on those actions that the participants themselves consider as relevant and salient, such as the events occurring in a meeting that might appear in the minutes of the meeting. For our system to identify, distinguish, or describe such events, it is essential for it to understand the participants’ roles and relationships to those events through interpreting their linguistic expression within the dialogue. This includes understanding direct reference to participants and recognizing discourse structure through evidence of referential coherence. Another aim of our research is to i</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert H. Clark. 1996. Using Language. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="28153" citStr="Cohen, 1960" startWordPosition="4307" endWordPosition="4308">mmatical type of the expression. The analysis reveals that second-person pronouns are a particularly challenging reference resolution problem, with a broad and relatively even distribution across referring types. 4.2 Reliability and Error Analysis To show that our annotations are credible and suitable for empirical testing, we must establish that the subjective distinctions defined in our scheme may be applied by individuals other than the scheme developers. To do this, we assess intercoder agreement between two independent annotators on four meetings from the AMI corpus, using Cohen’s Kappa (Cohen, 1960). Each of the decisions in the annotation procedure are assessed separately: markable identification, labeling referentiality, labeling specificity of person referents, and labeling addressing inclusion attributes. Because each decision depends on the previous, we employ a hierarchical assessment procedure that considers only instances where the annotators have agreed on previous decisions. This kind of multi-level assessment corresponds to that described and used in Carletta et al., (1997). Markables The first annotation decision of interest is the identification of markables. Markables are e</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lance Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<title>The Automatic Content Extraction (ACE) program: Tasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In Proc. LREC.</booktitle>
<contexts>
<context position="10546" citStr="Doddington et al., 2004" startWordPosition="1587" endWordPosition="1591">lity, the Nissim scheme includes only a single distinction between referential and generic instances of the word you. 2.2 Schemes for information extraction In contrast to the schemes described above, which are mainly driven toward investigating linguistic theories of discourse processing, some reference annotation projects are motivated instead by information extraction applications. For these projects (which includes our own), a priority is placed on entity semantics and coreference to known entities in the world. For example, the objective of the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is to recognize and extract entities, events, and relations between them, directly from written and spoken sources, mostly from broadcast news. The schemes thus focus on identifying and labeling the properties of entities in the real world, and then marking expressions as referring to these entities. Recent work in the ACE project has expanded the scope of this task to include cross-document recognition and resolution (Strassel et al., 2008). In the ACE scheme (Linguistic Data Consortium, 2008), person reference is a central component, and in the broadcast conversation component of the corpus</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The Automatic Content Extraction (ACE) program: Tasks, data, and evaluation. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Frampton</author>
<author>Raquel Fernndez</author>
<author>Patrick Ehlen</author>
<author>Mario Christoudias</author>
<author>Trevor Darrell</author>
<author>Stanley Peters</author>
</authors>
<title>Who is ”you”? Combining linguistic and gaze features to resolve second-person references in dialogue.</title>
<date>2009</date>
<booktitle>In Proc. EACL.</booktitle>
<contexts>
<context position="11539" citStr="Frampton et al., 2009" startWordPosition="1741" endWordPosition="1744"> include cross-document recognition and resolution (Strassel et al., 2008). In the ACE scheme (Linguistic Data Consortium, 2008), person reference is a central component, and in the broadcast conversation component of the corpus there is an extensive inventory of participant references. The annotation scheme contains a distinction between specific, underspecified, and general entities, as well as a distinction between persons and organizations. Another closely related set of studies are four recent investigations of second-person reference resolution (Gupta et al., 2007a; Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). These studies are based upon a common set of annotations of the word you in source material from the Switchboard and ICSI Meeting corpora. The purpose for the annotations was to support learning of classifiers for two main problems: disambiguation of the generic/referential distinction, and reference resolution for referential cases. In addition to the generic/referential distinction and an addressing-based reference annotation, the scheme employed special classes for reported speech and fillers and allowed annotators to indicate vague or difficult cases. Our work build</context>
</contexts>
<marker>Frampton, Fernndez, Ehlen, Christoudias, Darrell, Peters, 2009</marker>
<rawString>Matthew Frampton, Raquel Fernndez, Patrick Ehlen, Mario Christoudias, Trevor Darrell, and Stanley Peters. 2009. Who is ”you”? Combining linguistic and gaze features to resolve second-person references in dialogue. In Proc. EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>517--520</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="3430" citStr="Godfrey et al., 1992" startWordPosition="503" endWordPosition="506">ressee-inclusive or addressee-exclusive, in reference to hypothetical individuals or non-human entities, or even metonymically in reference to objects connected to individuals (M¨uhlh¨ausler and Harr´e, 1990; Wales, 1996). In addition, these and many other issues are not simply occasional problems but arise regularly. Consider the following utterance from the AMI corpus of remote control design meetings, which is typical of the corpus in terms of complexity of person-reference. 1The words I and you are the most frequently used nominals in several conversational corpora, including Switchboard (Godfrey et al., 1992) and the AMI Meeting Corpus (McCowan et al., 2005). In the British National Corpus they are the two most common of any words in the demographic (i.e., conversational) subcorpus (Burnard, 2007), and Google’s Web 1T 5-gram statistics (Brants and Franz, 2006) list I and you as more frequent even than the word it. The word we falls within the top 10 most frequent words in all of these corpora. 256 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 256–264, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics “Current remote controls do not match</context>
<context position="9750" citStr="Godfrey et al., 1992" startWordPosition="1472" endWordPosition="1475">et al., 2004; Calhoun et al., 2005). Recent developments include the explicit handling of anaphoric ambiguity and discourse deixis (Poesio and Artstein, 2008). Despite the depth and detail of these schemes, participant reference has not been their main concern. The annotations by Poesio et al. (2000; 2004) include dialogue source material, but the rather constrained interactional situations do not elicit a rich set of references to participants. The scheme thus employs simple default labels for words like I and you. The work by Nissim et al., (2004) is an annotation of the Switchboard corpus (Godfrey et al., 1992), which contains only two participants who are neither co-present nor socially connected. Participant reference is thus rather constrained. Other than labeling coreferentiality, the Nissim scheme includes only a single distinction between referential and generic instances of the word you. 2.2 Schemes for information extraction In contrast to the schemes described above, which are mainly driven toward investigating linguistic theories of discourse processing, some reference annotation projects are motivated instead by information extraction applications. For these projects (which includes our o</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J. Godfrey, Edward Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. In Proc. ICASSP, pages 517–520, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Surabhi Gupta</author>
<author>John Niekrasz</author>
<author>Matthew Purver</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Resolving “you” in multiparty dialog.</title>
<date>2007</date>
<booktitle>In Proc. SIGdial,</booktitle>
<pages>227--230</pages>
<contexts>
<context position="11494" citStr="Gupta et al., 2007" startWordPosition="1733" endWordPosition="1736">ect has expanded the scope of this task to include cross-document recognition and resolution (Strassel et al., 2008). In the ACE scheme (Linguistic Data Consortium, 2008), person reference is a central component, and in the broadcast conversation component of the corpus there is an extensive inventory of participant references. The annotation scheme contains a distinction between specific, underspecified, and general entities, as well as a distinction between persons and organizations. Another closely related set of studies are four recent investigations of second-person reference resolution (Gupta et al., 2007a; Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). These studies are based upon a common set of annotations of the word you in source material from the Switchboard and ICSI Meeting corpora. The purpose for the annotations was to support learning of classifiers for two main problems: disambiguation of the generic/referential distinction, and reference resolution for referential cases. In addition to the generic/referential distinction and an addressing-based reference annotation, the scheme employed special classes for reported speech and fillers and allowed annotators to indi</context>
</contexts>
<marker>Gupta, Niekrasz, Purver, Jurafsky, 2007</marker>
<rawString>Surabhi Gupta, John Niekrasz, Matthew Purver, and Daniel Jurafsky. 2007a. Resolving “you” in multiparty dialog. In Proc. SIGdial, pages 227–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Surabhi Gupta</author>
<author>Matthew Purver</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Disambiguating between generic and referential “you” in dialog.</title>
<date>2007</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="11494" citStr="Gupta et al., 2007" startWordPosition="1733" endWordPosition="1736">ect has expanded the scope of this task to include cross-document recognition and resolution (Strassel et al., 2008). In the ACE scheme (Linguistic Data Consortium, 2008), person reference is a central component, and in the broadcast conversation component of the corpus there is an extensive inventory of participant references. The annotation scheme contains a distinction between specific, underspecified, and general entities, as well as a distinction between persons and organizations. Another closely related set of studies are four recent investigations of second-person reference resolution (Gupta et al., 2007a; Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). These studies are based upon a common set of annotations of the word you in source material from the Switchboard and ICSI Meeting corpora. The purpose for the annotations was to support learning of classifiers for two main problems: disambiguation of the generic/referential distinction, and reference resolution for referential cases. In addition to the generic/referential distinction and an addressing-based reference annotation, the scheme employed special classes for reported speech and fillers and allowed annotators to indi</context>
</contexts>
<marker>Gupta, Purver, Jurafsky, 2007</marker>
<rawString>Surabhi Gupta, Matthew Purver, and Daniel Jurafsky. 2007b. Disambiguating between generic and referential “you” in dialog. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The ICSI meeting corpus.</title>
<date>2003</date>
<booktitle>In Proc. ICASSP,</booktitle>
<volume>1</volume>
<pages>364--367</pages>
<contexts>
<context position="13160" citStr="Janin et al., 2003" startWordPosition="1996" endWordPosition="1999">e a brief summary below (Section 3.2). The primary focus of this paper is the second phase (Section 3.3), during which every instance of person-referring occurring in a given meeting is labelled. We provide more detail concerning the most novel and challenging aspects of the person-referring labeling process in Section 3.4 and present a brief summary of the annotation tool in Section 3.5. 3.1 Source Material The source material is drawn from two source corpora: the AMI corpus (McCowan et al., 2005), which contains experimentally-controlled scenario-driven design meetings, and the ICSI corpus (Janin et al., 2003), which contains naturally occurring workplace meetings. All the meetings have at least four participants and have an average duration of about 45 minutes. In the AMI corpus, 258 the participants are experimental subjects who are assigned institutional roles, e.g. project manager and industrial designer. This helps to establish controlled social relationships within the group, but generally limits the types of person referring. The ICSI meetings are naturally occurring and exhibit complex pre-existing social relationships between the participants. Person referring in this corpus is quite compl</context>
<context position="25506" citStr="Janin et al., 2003" startWordPosition="3885" endWordPosition="3888">face that displays multiple streams of discrete events in temporal order across the screen. In our case, the events are time-synchronized words that are distributed to different rows according to speaker. The interface allows keyboard input only and is synchronized with the MPlayer playback engine. 4 Results and Analysis 4.1 Statistical summary The dataset consists of approximately 11,000 individually annotated referring expressions in 16 experimentally-controlled, scenario-driven design meetings from the AMI corpus (McCowan et al., 2005) and 3 natural workplace meetings from the ICSI corpus (Janin et al., 2003). Figure 2 shows, for each grammatical type of referring expression, the frequency of occurrence of the five principal markable types, which are defined to consist of the two non-person-referring functional 261 Gram. Freq. Ent. (%) (bits) 1PS 33.7 .57 1PP 24.6 .67 2P 23.7 1.78 3PS .9 .66 3PP 7.2 1.25 QUANT 1.0 1.14 OTHER 8.9 1.57 Table 1: A statistical summary of all the markables in the dataset by grammatical type (gram.), showing their frequency relative to all markables (freq.), the entropy of the referring type given the grammatical type (ent.), and a list of the most frequent examples (fr</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The ICSI meeting corpus. In Proc. ICASSP, volume 1, pages 364–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<date>2008</date>
<journal>ACE (Automatic Content Extraction) English Annotation Guidelines for Entities, Version</journal>
<volume>6</volume>
<note>Downloaded from http://projects.ldc.upenn. edu/ace/annotation/.</note>
<contexts>
<context position="6910" citStr="Consortium, 2008" startWordPosition="1038" endWordPosition="1039">ssue faced frequently in informal conversation, where words typically used to do person-referring are also commonly used in non-person-referring ways. A principal goal is thus establishing reliable person/non-person and referential/non-referential distinctions for these words. The third issue concerns addressing roles (i.e., speaker, addressee, and non-addressee), which we propose can be a useful means for further distinguishing between different types of underspecified and generic references, beyond the specific/underspecified/generic distinctions made in schemes such as ACE (Linguistic Data Consortium, 2008). 1.2 Summary and Scope of Contributions The work described in this paper includes the design of an annotation procedure and a statistical analysis of a corpus of annotations and their reliability. The procedure we propose (Section 3) is based on a simple non-anaphoric coreferencelike scheme, modest in comparison to much previous work. The produced dataset (Section 4) includes annotations of 11,000 occasions of personreferring in recorded workplace meetings. Our analysis of the dataset includes a statistical summary of interesting results (Section 4.1) and an analysis of inter-coder agreement </context>
<context position="11046" citStr="Consortium, 2008" startWordPosition="1670" endWordPosition="1671"> in the world. For example, the objective of the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is to recognize and extract entities, events, and relations between them, directly from written and spoken sources, mostly from broadcast news. The schemes thus focus on identifying and labeling the properties of entities in the real world, and then marking expressions as referring to these entities. Recent work in the ACE project has expanded the scope of this task to include cross-document recognition and resolution (Strassel et al., 2008). In the ACE scheme (Linguistic Data Consortium, 2008), person reference is a central component, and in the broadcast conversation component of the corpus there is an extensive inventory of participant references. The annotation scheme contains a distinction between specific, underspecified, and general entities, as well as a distinction between persons and organizations. Another closely related set of studies are four recent investigations of second-person reference resolution (Gupta et al., 2007a; Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). These studies are based upon a common set of annotations of the word you in source </context>
</contexts>
<marker>Consortium, 2008</marker>
<rawString>Linguistic Data Consortium, 2008. ACE (Automatic Content Extraction) English Annotation Guidelines for Entities, Version 6.5. Downloaded from http://projects.ldc.upenn. edu/ace/annotation/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I McCowan</author>
<author>J Carletta</author>
<author>W Kraaij</author>
<author>S Ashby</author>
<author>S Bourban</author>
<author>M Flynn</author>
<author>M Guillemot</author>
<author>T Hain</author>
<author>J Kadlec</author>
<author>V Karaiskos</author>
<author>M Kronenthal</author>
<author>G Lathoud</author>
<author>M Lincoln</author>
<author>A Lisowska</author>
<author>W Post</author>
<author>D Reidsma</author>
<author>P Wellner</author>
</authors>
<title>The AMI Meeting Corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of Measuring Behavior 2005, the 5th International Conference on Methods and Techniques in Behavioral Research,</booktitle>
<location>Wageningen, Netherlands.</location>
<contexts>
<context position="3480" citStr="McCowan et al., 2005" startWordPosition="513" endWordPosition="516">nce to hypothetical individuals or non-human entities, or even metonymically in reference to objects connected to individuals (M¨uhlh¨ausler and Harr´e, 1990; Wales, 1996). In addition, these and many other issues are not simply occasional problems but arise regularly. Consider the following utterance from the AMI corpus of remote control design meetings, which is typical of the corpus in terms of complexity of person-reference. 1The words I and you are the most frequently used nominals in several conversational corpora, including Switchboard (Godfrey et al., 1992) and the AMI Meeting Corpus (McCowan et al., 2005). In the British National Corpus they are the two most common of any words in the demographic (i.e., conversational) subcorpus (Burnard, 2007), and Google’s Web 1T 5-gram statistics (Brants and Franz, 2006) list I and you as more frequent even than the word it. The word we falls within the top 10 most frequent words in all of these corpora. 256 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 256–264, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics “Current remote controls do not match well with the operating behaviour of the user ove</context>
<context position="13044" citStr="McCowan et al., 2005" startWordPosition="1981" endWordPosition="1984">ubsequent person reference labeling process. The first phase is not of central concern in this paper, though we provide a brief summary below (Section 3.2). The primary focus of this paper is the second phase (Section 3.3), during which every instance of person-referring occurring in a given meeting is labelled. We provide more detail concerning the most novel and challenging aspects of the person-referring labeling process in Section 3.4 and present a brief summary of the annotation tool in Section 3.5. 3.1 Source Material The source material is drawn from two source corpora: the AMI corpus (McCowan et al., 2005), which contains experimentally-controlled scenario-driven design meetings, and the ICSI corpus (Janin et al., 2003), which contains naturally occurring workplace meetings. All the meetings have at least four participants and have an average duration of about 45 minutes. In the AMI corpus, 258 the participants are experimental subjects who are assigned institutional roles, e.g. project manager and industrial designer. This helps to establish controlled social relationships within the group, but generally limits the types of person referring. The ICSI meetings are naturally occurring and exhibi</context>
<context position="25431" citStr="McCowan et al., 2005" startWordPosition="3872" endWordPosition="3875"> of multi-modal corpora. The tool uses a simple, low-latency text-based interface that displays multiple streams of discrete events in temporal order across the screen. In our case, the events are time-synchronized words that are distributed to different rows according to speaker. The interface allows keyboard input only and is synchronized with the MPlayer playback engine. 4 Results and Analysis 4.1 Statistical summary The dataset consists of approximately 11,000 individually annotated referring expressions in 16 experimentally-controlled, scenario-driven design meetings from the AMI corpus (McCowan et al., 2005) and 3 natural workplace meetings from the ICSI corpus (Janin et al., 2003). Figure 2 shows, for each grammatical type of referring expression, the frequency of occurrence of the five principal markable types, which are defined to consist of the two non-person-referring functional 261 Gram. Freq. Ent. (%) (bits) 1PS 33.7 .57 1PP 24.6 .67 2P 23.7 1.78 3PS .9 .66 3PP 7.2 1.25 QUANT 1.0 1.14 OTHER 8.9 1.57 Table 1: A statistical summary of all the markables in the dataset by grammatical type (gram.), showing their frequency relative to all markables (freq.), the entropy of the referring type give</context>
</contexts>
<marker>McCowan, Carletta, Kraaij, Ashby, Bourban, Flynn, Guillemot, Hain, Kadlec, Karaiskos, Kronenthal, Lathoud, Lincoln, Lisowska, Post, Reidsma, Wellner, 2005</marker>
<rawString>I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and P. Wellner. 2005. The AMI Meeting Corpus. In Proceedings of Measuring Behavior 2005, the 5th International Conference on Methods and Techniques in Behavioral Research, Wageningen, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter M¨uhlh¨ausler</author>
<author>Rom Harr´e</author>
</authors>
<title>Pronouns and People: The Linguistic Construction of Social and Personal Identity.</title>
<date>1990</date>
<location>Blackwell, Oxford.</location>
<marker>M¨uhlh¨ausler, Harr´e, 1990</marker>
<rawString>Peter M¨uhlh¨ausler and Rom Harr´e. 1990. Pronouns and People: The Linguistic Construction of Social and Personal Identity. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malvina Nissim</author>
<author>Shipra Dingare</author>
<author>Jean Carletta</author>
<author>Mark Steedman</author>
</authors>
<title>An annotation scheme for information status in dialogue.</title>
<date>2004</date>
<booktitle>In Proc. LREC.</booktitle>
<contexts>
<context position="9141" citStr="Nissim et al., 2004" startWordPosition="1374" endWordPosition="1377">g problems like anaphora resolution and reference generation. These schemes have been applied to both text and dialogue and label dis257 course references with a rich set of syntactic, semantic, and pragmatic properties. For example, the DRAMA scheme (Passonneau, 1997) and the GNOME scheme (Poesio, 2000; Poesio, 2004) include labels for features such as bridging relation type and NP type in addition to a rich representation of referent semantics. Other schemes label animacy, prosody, and information structure to study their relationship to the organization and salience of discourse reference (Nissim et al., 2004; Calhoun et al., 2005). Recent developments include the explicit handling of anaphoric ambiguity and discourse deixis (Poesio and Artstein, 2008). Despite the depth and detail of these schemes, participant reference has not been their main concern. The annotations by Poesio et al. (2000; 2004) include dialogue source material, but the rather constrained interactional situations do not elicit a rich set of references to participants. The scheme thus employs simple default labels for words like I and you. The work by Nissim et al., (2004) is an annotation of the Switchboard corpus (Godfrey et a</context>
</contexts>
<marker>Nissim, Dingare, Carletta, Steedman, 2004</marker>
<rawString>Malvina Nissim, Shipra Dingare, Jean Carletta, and Mark Steedman. 2004. An annotation scheme for information status in dialogue. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Passonneau</author>
</authors>
<title>Instructions for applying discourse reference annotation for multiple applications (DRAMA).</title>
<date>1997</date>
<contexts>
<context position="8791" citStr="Passonneau, 1997" startWordPosition="1320" endWordPosition="1321">eference generally. It is useful to categorize this work according to the natural language processing tasks the annotations are designed to support. 2.1 Schemes for anaphora and generation Several schemes have been designed with the goal of testing linguistic theoretical models of discourse structure or for use in the study of discourse processing problems like anaphora resolution and reference generation. These schemes have been applied to both text and dialogue and label dis257 course references with a rich set of syntactic, semantic, and pragmatic properties. For example, the DRAMA scheme (Passonneau, 1997) and the GNOME scheme (Poesio, 2000; Poesio, 2004) include labels for features such as bridging relation type and NP type in addition to a rich representation of referent semantics. Other schemes label animacy, prosody, and information structure to study their relationship to the organization and salience of discourse reference (Nissim et al., 2004; Calhoun et al., 2005). Recent developments include the explicit handling of anaphoric ambiguity and discourse deixis (Poesio and Artstein, 2008). Despite the depth and detail of these schemes, participant reference has not been their main concern. </context>
</contexts>
<marker>Passonneau, 1997</marker>
<rawString>R. Passonneau, 1997. Instructions for applying discourse reference annotation for multiple applications (DRAMA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Ron Artstein</author>
</authors>
<title>Anaphoric annotation in the ARRAU corpus.</title>
<date>2008</date>
<booktitle>In Proc. LREC.</booktitle>
<contexts>
<context position="9287" citStr="Poesio and Artstein, 2008" startWordPosition="1395" endWordPosition="1398">urse references with a rich set of syntactic, semantic, and pragmatic properties. For example, the DRAMA scheme (Passonneau, 1997) and the GNOME scheme (Poesio, 2000; Poesio, 2004) include labels for features such as bridging relation type and NP type in addition to a rich representation of referent semantics. Other schemes label animacy, prosody, and information structure to study their relationship to the organization and salience of discourse reference (Nissim et al., 2004; Calhoun et al., 2005). Recent developments include the explicit handling of anaphoric ambiguity and discourse deixis (Poesio and Artstein, 2008). Despite the depth and detail of these schemes, participant reference has not been their main concern. The annotations by Poesio et al. (2000; 2004) include dialogue source material, but the rather constrained interactional situations do not elicit a rich set of references to participants. The scheme thus employs simple default labels for words like I and you. The work by Nissim et al., (2004) is an annotation of the Switchboard corpus (Godfrey et al., 1992), which contains only two participants who are neither co-present nor socially connected. Participant reference is thus rather constraine</context>
</contexts>
<marker>Poesio, Artstein, 2008</marker>
<rawString>Massimo Poesio and Ron Artstein. 2008. Anaphoric annotation in the ARRAU corpus. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
</authors>
<title>The GNOME Annotation Scheme Manual, Version 4.</title>
<date>2000</date>
<institution>University of Edinburgh, HCRC and Informatics.</institution>
<contexts>
<context position="8826" citStr="Poesio, 2000" startWordPosition="1326" endWordPosition="1327">gorize this work according to the natural language processing tasks the annotations are designed to support. 2.1 Schemes for anaphora and generation Several schemes have been designed with the goal of testing linguistic theoretical models of discourse structure or for use in the study of discourse processing problems like anaphora resolution and reference generation. These schemes have been applied to both text and dialogue and label dis257 course references with a rich set of syntactic, semantic, and pragmatic properties. For example, the DRAMA scheme (Passonneau, 1997) and the GNOME scheme (Poesio, 2000; Poesio, 2004) include labels for features such as bridging relation type and NP type in addition to a rich representation of referent semantics. Other schemes label animacy, prosody, and information structure to study their relationship to the organization and salience of discourse reference (Nissim et al., 2004; Calhoun et al., 2005). Recent developments include the explicit handling of anaphoric ambiguity and discourse deixis (Poesio and Artstein, 2008). Despite the depth and detail of these schemes, participant reference has not been their main concern. The annotations by Poesio et al. (2</context>
</contexts>
<marker>Poesio, 2000</marker>
<rawString>Massimo Poesio, 2000. The GNOME Annotation Scheme Manual, Version 4. University of Edinburgh, HCRC and Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
</authors>
<title>Discourse annotation and semantic annotation in the GNOME corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL 2004 Workshop on Discourse Annotation,</booktitle>
<pages>72--79</pages>
<contexts>
<context position="8841" citStr="Poesio, 2004" startWordPosition="1328" endWordPosition="1329">rk according to the natural language processing tasks the annotations are designed to support. 2.1 Schemes for anaphora and generation Several schemes have been designed with the goal of testing linguistic theoretical models of discourse structure or for use in the study of discourse processing problems like anaphora resolution and reference generation. These schemes have been applied to both text and dialogue and label dis257 course references with a rich set of syntactic, semantic, and pragmatic properties. For example, the DRAMA scheme (Passonneau, 1997) and the GNOME scheme (Poesio, 2000; Poesio, 2004) include labels for features such as bridging relation type and NP type in addition to a rich representation of referent semantics. Other schemes label animacy, prosody, and information structure to study their relationship to the organization and salience of discourse reference (Nissim et al., 2004; Calhoun et al., 2005). Recent developments include the explicit handling of anaphoric ambiguity and discourse deixis (Poesio and Artstein, 2008). Despite the depth and detail of these schemes, participant reference has not been their main concern. The annotations by Poesio et al. (2000; 2004) incl</context>
</contexts>
<marker>Poesio, 2004</marker>
<rawString>Massimo Poesio. 2004. Discourse annotation and semantic annotation in the GNOME corpus. In Proceedings of the ACL 2004 Workshop on Discourse Annotation, pages 72–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Raquel Fernndez</author>
<author>Matthew Frampton</author>
<author>Stanley Peters</author>
</authors>
<title>Cascaded lexicalised classifiers for second-person reference resolution.</title>
<date>2009</date>
<booktitle>In Proc. SIGdial,</booktitle>
<pages>306--309</pages>
<contexts>
<context position="11561" citStr="Purver et al., 2009" startWordPosition="1745" endWordPosition="1748"> recognition and resolution (Strassel et al., 2008). In the ACE scheme (Linguistic Data Consortium, 2008), person reference is a central component, and in the broadcast conversation component of the corpus there is an extensive inventory of participant references. The annotation scheme contains a distinction between specific, underspecified, and general entities, as well as a distinction between persons and organizations. Another closely related set of studies are four recent investigations of second-person reference resolution (Gupta et al., 2007a; Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). These studies are based upon a common set of annotations of the word you in source material from the Switchboard and ICSI Meeting corpora. The purpose for the annotations was to support learning of classifiers for two main problems: disambiguation of the generic/referential distinction, and reference resolution for referential cases. In addition to the generic/referential distinction and an addressing-based reference annotation, the scheme employed special classes for reported speech and fillers and allowed annotators to indicate vague or difficult cases. Our work builds directly upon this w</context>
</contexts>
<marker>Purver, Fernndez, Frampton, Peters, 2009</marker>
<rawString>Matthew Purver, Raquel Fernndez, Matthew Frampton, and Stanley Peters. 2009. Cascaded lexicalised classifiers for second-person reference resolution. In Proc. SIGdial, pages 306–309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Strassel</author>
<author>Mark Przybocki</author>
<author>Kay Peterson</author>
<author>Zhiyi Song</author>
<author>Kazuaki Maeda1</author>
</authors>
<title>Linguistic resources and evaluation techniques for evaluation of cross-document automatic content extraction.</title>
<date>2008</date>
<booktitle>In Proc. LREC.</booktitle>
<marker>Strassel, Przybocki, Peterson, Song, Maeda1, 2008</marker>
<rawString>Stephanie Strassel, Mark Przybocki, Kay Peterson, Zhiyi Song, and Kazuaki Maeda1. 2008. Linguistic resources and evaluation techniques for evaluation of cross-document automatic content extraction. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katie Wales</author>
</authors>
<title>Personal pronouns in present-day English.</title>
<date>1996</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="3030" citStr="Wales, 1996" startWordPosition="441" endWordPosition="442">rson-referring words are the most frequent words in conversation.1 Perhaps contrary to intuition, however, interpreting person-referring expressions can be rather complex. Person-reference interpretation is strongly dependent on social, situational, and discourse context. The words you and we are especially problematic. Either can be used for generic, plural, or singular reference, as addressee-inclusive or addressee-exclusive, in reference to hypothetical individuals or non-human entities, or even metonymically in reference to objects connected to individuals (M¨uhlh¨ausler and Harr´e, 1990; Wales, 1996). In addition, these and many other issues are not simply occasional problems but arise regularly. Consider the following utterance from the AMI corpus of remote control design meetings, which is typical of the corpus in terms of complexity of person-reference. 1The words I and you are the most frequently used nominals in several conversational corpora, including Switchboard (Godfrey et al., 1992) and the AMI Meeting Corpus (McCowan et al., 2005). In the British National Corpus they are the two most common of any words in the demographic (i.e., conversational) subcorpus (Burnard, 2007), and Go</context>
</contexts>
<marker>Wales, 1996</marker>
<rawString>Katie Wales. 1996. Personal pronouns in present-day English. Cambridge University Press, Cambridge.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>