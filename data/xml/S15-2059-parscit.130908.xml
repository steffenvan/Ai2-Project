<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003817">
<title confidence="0.803606">
TeamUFAL:
WSD+EL as Document Retrieval∗
</title>
<author confidence="0.998812">
Petr Fanta, Roman Sudarikov, Ondˇrej Bojar
</author>
<affiliation confidence="0.998669333333333">
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.922469">
Malostransk´e n´amˇest´ı 25, 11800 Praha 1, Czech Republic
</address>
<email confidence="0.998045">
p.fanta@seznam.cz,{sudarikov, bojar}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999344875">
This paper describes our system for SemEval-
2015 Task 13: Multilingual All-Words Sense
Disambiguation and Entity Linking. We have
participated with our system in the sub-task
which aims at monolingual all-words disam-
biguation and entity linking. Aside from sys-
tem description, we pay closer attention to the
evaluation of system outputs.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990612379310345">
Word sense disambiguation (WSD, i.e. picking the
right sense for a given word from a fixed inventory)
and entity linking (EL, i.e. identifying a particular
named entity listed in a database given its mention
in a text) are among the fashionable tasks in com-
putational linguistics and natural language process-
ing these days. WSD has been, after some debate,
shown to help machine translation (Carpuat and Wu,
2007), other applications include knowledge discov-
ery or machine reading in general (Etzioni et al.,
2006; Schubert, 2006). WSD and EL are usually
applied with large and rich context available (Nav-
igli, 2009), but the arguably harder setting of short
context has a wider range of applications, including
text similarity measurements (Abdalgader and Sk-
abar, 2011), Named Entities Extraction and Named
Entities Disambiguation (Habib and Keulen, 2012)
∗This research was supported by the grants FP7-ICT-2013-
10-610516 (QTLeap). This research was partially supported by
SVV project number 260 224. This work has been using lan-
guage resources developed, stored and distributed by the LIN-
DAT/CLARIN project of the Ministry of Education, Youth and
Sports of the Czech Republic (project LM2010013).
or handling data from social networks, such as at-
tempts to translate tweets (ˇSubert and Bojar, 2014).
Our attempt at WSD and EL can be classified as
unsupervised, corpus-based and our implementation
relies on an information retrieval tool. We do not
take longer context into account.
</bodyText>
<sectionHeader confidence="0.982257" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.999900333333333">
As participants of SemEval-2015 Task 13 (Moro and
Navigli, 2015), we were given only a very brief in-
structions, effectively just one example of a POS-
</bodyText>
<equation confidence="0.363231666666667">
tagged sentence:
The/X European/J/european Medicines/N/medicine
Agency/N/agency (/X EMA/N/ema )/X is/V/be ,.. .
</equation>
<bodyText confidence="0.99884875">
We were expected to provide such input
with labels indicating that e.g. the words
“European Medicines Agency” refer to the
entity described in the English Wikipedia
under the title European Medicines Agency
(“wiki:European Medicines Agency”), the
word “Medicines” refers to the BabelNet concept
00054128n etc. The repertoire of word sense and
entities came from BabelNet 2.5 which included:
Wikipedia page titles (2012/10 dump), WordNet
3.0 synsets, OmegaWiki senses (2013/09 dump)
and Open Multilingual WordNet synsets (2013/08
dump). The output format accepted Wikipedia
titles, BabelNet IDs and Wordnet sense keys.
The test set for SemEval-2015 task 13 was re-
leased for three languages: English, Italian and
Spanish. We joined only the English task. All the
data was gathered from 3 domains: biomedicine,
mathematics and computers and general domain.
At the time of the shared task, neither a scoring
</bodyText>
<page confidence="0.958858">
350
</page>
<bodyText confidence="0.869896">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 350–354,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
script, nor any development set with annotations was
provided. It was also not very clear how the dif-
ferent allowed ID sources (Wikipedia, BabelNet and
Wordnet) will be used concurrently.
The official scoring script and golden annotation
was provided later, and we use it here to report the
scores of our submission and a few variations of it.
</bodyText>
<sectionHeader confidence="0.969771" genericHeader="method">
3 Our System
</sectionHeader>
<bodyText confidence="0.999947894736842">
Our system is unsupervised and relies on an infor-
mation retrieval (IR) tool applied to a large collec-
tion of documents. We thus call it corpus-based.
Given an input sentence, we remove all stop-
words (as defined by the IR tool) and punctuation,
putting together even words which were originally
not adjacent. For each span up to a given length in
this abridged sentence, the system tries to find a doc-
ument in the database. If found, this document im-
plies the sense or entity ID for the given span.
The words in the span and (separately) other
words in the sentence are used to construct the query
for the IR engine. We construct multiple queries and
merge their results in a candidate selection process,
possibly returning no document at all.
Due to the different nature of our sources (see
Section 3.1), we run sub-systems with different con-
figurations for each of them. We return the union of
responses from these sub-systems.
</bodyText>
<subsectionHeader confidence="0.999048">
3.1 Data Sources
</subsectionHeader>
<bodyText confidence="0.9998675">
BabelNet alone is not a good resource for our ap-
proach, because it does not include textual data. We
resort to the original sources of BabelNet and map
them back to BabelNet. Our sources are thus the En-
glish Wikipedia, English Wiktionary and WordNet.
Short of the original versions as used in BabelNet
2.5, we used Wiki dumps from November 2014 and
WordNet 3.0, facing some ID mismatches.
</bodyText>
<subsectionHeader confidence="0.998363">
3.2 Indexing
</subsectionHeader>
<bodyText confidence="0.998589857142857">
For each source, we create a full text index using
Apache Lucene search engine which provides sev-
eral ranking models. We experiment with models
based on TF-IDF (Salton et al., 1975) and Okapi
BM25 (Robertson et al., 1995), selecting the better
one for each subsystem in our submission.
All indexes have a similar structure, they contain:
</bodyText>
<table confidence="0.974617">
Score Document ID (ie. Wikipedia Title)
8.201 Medical condition → Disease
8.201 Medical conditions → Disease
6.561 Frostbite (medical condition) → Frostbite
</table>
<figureCaption confidence="0.982036">
Figure 1: Query results (→ means redirection closure).
</figureCaption>
<bodyText confidence="0.984773">
ID of the element in the given source (Wikipedia
ID, Wiktionary ID or Wordnet sense key),
Title of Wikipedia or Wiktionary article or word
from WordNet,
Body text of articles from Wiki sources (markup re-
moved) or all textual data from Wordnet synset
(including other words in the synset),
POS tag (only in WordNet index).
The Title and the Body field are stemmed by Porter
stemmer implemented in Lucene.
</bodyText>
<subsectionHeader confidence="0.999506">
3.3 Proposing Candidates
</subsectionHeader>
<bodyText confidence="0.999989642857143">
We use different sets of queries for each source.
We query Wiktionary and Wordnet for single-word
spans only, while Wikipedia seems suitable for both,
single and multi-word spans.
The queries typically require all the words from
the span to appear in the Title field of the document
and the words from the context to appear in the Body
field of the document. A number of slightly differ-
ent queries, incl. queries that use n-grams of words
or some boosting for some of the terms, is run in
parallel, giving us multiple lists scored by the se-
lected IR model (see Section 3.2). The results for
a simple query +TITLE:medical +TITLE:condition
for Wikipedia documents are shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.985075">
3.4 Final Candidate Selection
</subsectionHeader>
<bodyText confidence="0.999924307692307">
Final candidates are picked from the results of the
queries. Before this selection, the results for each
span are grouped and scores for the same ID (com-
ing from different lists or redirection) are summed.
For Wikipedia, we select the highest-scoring can-
didate and it is returned only if its score is greater
than double the score of the second candidate. After
this selection, the system checks if there are over-
lapping spans labeled with same ID and returns only
the span with the best score.
For Wordnet and Wiktionary, we simply return
the highest-scoring candidate for each span. Since
Wiktionary IDs are not expected in the shared task,
</bodyText>
<page confidence="0.984122">
351
</page>
<table confidence="0.999974333333333">
System P Official F1 Offic+penalty P Our Exact P Our Partial Bag of IDs
R P R F1 R F1 P R F1
Submitted 40.4 36.5 38.3 30.4 25.9 48.2 33.7 26.6 49.4 34.6 24.0 50.5 32.5
Submitted-fix 41.2 37.3 39.1 30.7 25.7 49.6 33.9 26.3 50.8 34.7 23.4 52.0 32.3
DFKI 67.4 52.6 59.1 55.2 51.5 49.2 50.3 52.1 49.8 50.9 51.3 49.2 50.2
EBL-Hope 48.4 44.4 46.3 40.4 36.8 40.4 38.5 37.1 40.8 38.9 37.3 41.0 39.0
el92-Run1 69.9 21.4 32.8 62.6 59.9 20.4 30.5 61.2 20.9 31.1 62.2 21.2 31.7
el92-Run2 71.9 19.1 30.2 64.8 61.8 18.2 28.2 62.5 18.4 28.5 62.9 18.5 28.6
el92-Run3 75.2 18.5 29.6 69.6 66.0 17.5 27.7 66.8 17.7 28.0 66.9 17.8 28.1
LIMSI 68.7 63.1 65.8 57.3 55.4 60.9 58.0 55.6 61.2 58.3 55.6 61.2 58.2
SUDOKU-Run1 60.1 52.1 55.8 50.3 47.0 48.6 47.8 47.2 48.8 48.0 47.0 48.6 47.8
SUDOKU-Run2 62.9 60.4 61.6 53.0 49.2 56.1 52.4 49.7 56.6 52.9 49.3 56.2 52.5
SUDOKU-Run3 61.9 59.4 60.6 52.2 48.6 55.4 51.8 49.0 55.8 52.1 48.7 55.5 51.9
UNIBA-Run1 66.2 52.3 58.4 54.3 51.6 49.8 50.7 51.9 50.0 50.9 51.9 50.0 50.9
UNIBA-Run2 66.1 52.1 58.3 53.5 50.9 49.6 50.2 51.5 50.2 50.8 51.4 50.1 50.7
UNIBA-Run3 66.1 52.1 58.3 53.0 50.5 49.7 50.1 51.3 50.4 50.8 51.1 50.2 50.7
vua-background 67.5 51.4 58.4 56.3 52.1 47.5 49.7 52.3 47.8 50.0 52.3 47.7 49.9
WSD-games-Run1 57.4 48.8 52.8 47.9 44.1 45.0 44.6 44.3 45.2 44.7 44.3 45.2 44.7
WSD-games-Run2 58.8 50.0 54.0 49.0 45.3 46.2 45.7 45.5 46.4 45.9 45.5 46.4 45.9
WSD-games-Run3 53.5 45.4 49.1 44.6 40.7 41.5 41.1 41.0 41.8 41.4 41.0 41.8 41.4
MFS 67.9 67.1 67.5 67.9 65.2 64.5 64.9 65.5 64.8 65.2 65.2 64.5 64.9
</table>
<tableCaption confidence="0.999953">
Table 1: All submissions evaluated on all domains using various official and our scorings.
</tableCaption>
<bodyText confidence="0.988418333333333">
we map them to BabelNet IDs prior to picking the
highest-scoring one. (Wiktionary IDs that cannot be
mapped are discarded.)
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9988315">
Having thoroughly reviewed the official scoring
script, we find some of its features unusual:
</bodyText>
<listItem confidence="0.94029025">
• The precision of a system is not penalized for
spans, which don’t occur in the golden set.
• The recall should consider only to what extent the
expected answers are covered by the system’s an-
swers. The official scoring script reduces the re-
call score for any ’unexpected’ answers.
• An exact match in span is needed to give any
credit to the system answer.
</listItem>
<bodyText confidence="0.999119">
We thus propose a slightly different evaluation pro-
cedure and apply it to all submitted systems.
</bodyText>
<subsectionHeader confidence="0.989514">
4.1 Our Proposed Scoring
</subsectionHeader>
<bodyText confidence="0.999939428571428">
Our scoring is based on a credit for partially overlap-
ping spans, similarly to Cornolti et al. (2013), who
however disregard the overlap size. We call a ‘label’
l = (l1, l2) the pair of a span (a range of words in the
sentence; denoted l1) and an ID attached to the span,
l2. For a label s in the system output and a label g in
the golden annotation, we define their match as:
</bodyText>
<equation confidence="0.91787125">
�|g1ns1 |if |g1 ∩ s1 g &gt; 0 n 2 = s2
9)
match(s Islus1 |_
0 otherwise
</equation>
<bodyText confidence="0.999832666666667">
In other words overlapping spans labeled with the
same ID get a credit proportional to the size of the
overlap. We define precision and recall as follows:
</bodyText>
<equation confidence="0.9663225">
precision = K ES,gEG match(s, g)
|S|
recall = KES,gEG match(s, g)
|G|
</equation>
<bodyText confidence="0.996496809523809">
where G and S are sets of labels from the gold
standard and a system output, respectively. Our ap-
proach gives a partial credit for inexact, but overlap-
ping, spans with correct identifiers.
Our precision and recall are only meaningful, if
all IDs come from a single source. We pick Babel-
Net IDs for this purpose and map all system out-
puts as necessary. Note that the mapping from the
Wikipedia IDs to the BabelNet IDs is ambiguous but
not in more than 1 %o cases.
WSD-games and vua-background report only
WordNet sense keys. We map them unambigu-
ously to BabelNet IDs.
el92 produces lowercase Wikipedia titles so the am-
biguous mapping to BabelNet IDs is slightly
worse.
DFKI and our system produce both Wikipedia ti-
tles and WordNet IDs, we map both as above
and union the results.
SUDOKU produces BabelNet IDs but some spans
have no ID at all. We ignore these spans.
</bodyText>
<page confidence="0.993059">
352
</page>
<table confidence="0.999793285714286">
Fix Wikt→BN Model for Use context Precision Recall F1
mapping Wikipedia Wiktionary in Wikt. search
Submitted - TF-IDF BM25 no 40.4% 36.5% 36.5%
Submitted fix yes TF-IDF BM25 no 41.2% 37.3% 39.1%
Wiki BM25 no BM25 BM25 no 38.4% 35.0% 36.7%
Wikt+context BM25 no TF-IDF BM25 yes 38.4% 35.3% 36.8%
Wikt+context TF-IDF no TF-IDF TF-IDF yes 40.3% 37.0% 38.6%
</table>
<tableCaption confidence="0.989976">
Table 2: Our system outputs.
</tableCaption>
<sectionHeader confidence="0.418242" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<tableCaption confidence="0.7945955">
Table 1 reports systems’ scores using these evalua-
tion metrics:
</tableCaption>
<bodyText confidence="0.98329588">
Official Precision and recall as reported by the offi-
cial scoring script.
Official+penalty A modified version of the official
scoring script which treats spans in system out-
put and no counterpart in the golden set in the
same way as if the golden set assigned a differ-
ent ID to the span.
Our Exact Our method (Section 4.1), but round-
ing the ‘match’ down to zero, so only exactly
matching spans get the credit (of 1).
Our Partial Our method (Section 4.1).
Bag of IDs disregards spans altogether, checking
just the match of the BabelNet IDs needed and
produced. Precision is the fraction of correct
(confirmed by the golden data) IDs among all
labels produced by the system. Recall is the
number of correct IDs divided by the number
of labels in the gold set. This scoring gives an
idea of how well the system guesses the “mean-
ing” (bag of concepts) of the whole sentence.
The Table 1 documents that the official scoring
heavily boosted our precision and hurt our recall.
The performances of other systems are affected as
well, but fortunately, the overall impression is simi-
lar across the scoring techniques.
</bodyText>
<subsectionHeader confidence="0.998905">
4.3 Variants of Our Submission
</subsectionHeader>
<bodyText confidence="0.999973">
As the official scores in the overview paper (Moro
and Navigli, 2015) show, our system performed ac-
ceptably on Named Entities Recognition task, but it
clearly failed on word senses disambiguation.
Table 2 reports the scores (official scoring) of a
few variations of our approach. The first row is
the submitted system, the second row is a correc-
tion which allows Wiktionary results to map to Ba-
belNet senses of all parts of speech, not just nouns.
The remaining rows use a different IR model or in-
clude sentence context in Wiktionary search but no
improvement is obtained.
</bodyText>
<subsectionHeader confidence="0.874924">
4.4 Recommendations for Future Evaluation
</subsectionHeader>
<bodyText confidence="0.996976">
For future shared tasks, we recommend:
</bodyText>
<listItem confidence="0.9989922">
• Define precision and recall to better match the
common meaning, e.g. as in our proposal.
• Preserve letter case in IDs to avoid ambiguity in
Wikipedia to BabelNet mapping.
• Use only one repertoire of IDs in the gold set.
</listItem>
<subsectionHeader confidence="0.757754">
4.5 Future Work
</subsectionHeader>
<bodyText confidence="0.99999425">
In future we want to evaluate other heuristics such as
weighted words picking instead of first one, offered
by search algorithms. Also we’ll examine possi-
bilities to enhance Wordnet and Wiktionary records
to make search results more reliable. Another way
of improvement is using Named Entities Recogni-
tion systems to define correct span boundaries and
to achieve better results for Named Entities.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999965285714286">
We described our system for SemEval Task 13 based
on information retrieval. The system performs ac-
ceptably in Named Entity Linking (NEL) but fails
in Word Sense Disambiguation. One of the reasons
is that we used small information records for Wik-
tionary and especially for Wordnet and little or no
sentence context in WSD queries, so the informa-
tion retrieval algorithms performed poorly.
Additionally, we proposed different scoring tech-
niques that, in our opinion, better reflect the perfor-
mance of the systems. Fortunately, the overall rank-
ing of systems ends up similar to the official scor-
ing. We nevertheless recommend a few changes for
future shared tasks.
</bodyText>
<page confidence="0.998944">
353
</page>
<sectionHeader confidence="0.998301" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998661428571429">
Khaled Abdalgader and Andrew Skabar. Short-text
similarity measurement using word sense disam-
biguation and synonym expansion. In AI 2010:
Advances in Artificial Intelligence, pages 435–
444. 2011.
Marine Carpuat and Dekai Wu. Improving Sta-
tistical Machine Translation Using Word Sense
Disambiguation. In EMNLP-CoNLL, volume 7,
pages 61–72, 2007.
Marco Cornolti, Paolo Ferragina, and Massimil-
iano Ciaramita. A framework for benchmark-
ing entity-annotation systems. In Proceedings of
the 22nd international conference on World Wide
Web, pages 249–260, 2013.
Oren Etzioni, Michele Banko, and Michael J. Ca-
farella. Machine Reading. In AAAI, volume 6,
pages 1517–1519, 2006.
Mena B. Habib and Maurice Keulen. Unsuper-
vised improvement of named entity extraction
in short informal context using disambiguation
clues. 2012.
Andrea Moro and Roberto Navigli. SemEval-
2015 Task 13: Multilingual All-Words Sense Dis-
ambiguation and Entity Linking. In Proc. of
SemEval-2015, 2015.
Roberto Navigli. Word sense disambiguation: A sur-
vey. ACM Computing Surveys (CSUR), 41(2):10,
2009.
Stephen E. Robertson, Steve Walker, Susan Jones,
Micheline M. Hancock-Beaulieu, Mike Gatford,
et al. Okapi at TREC-3. NIST SPECIAL PUBLI-
CATION SP, pages 109–109, 1995.
Gerard Salton, Anita Wong, and Chung-Shu Yang.
A vector space model for automatic indexing.
Communications of the ACM, 18(11):613–620,
1975.
Lenhart Schubert. Turing’s Dream and the Knowl-
edge Challenge. In Proc. of the national confer-
ence on artificial intelligence, volume 21, page
1534, 2006.
Eduard ˇSubert and Ondˇrej Bojar. Twitter Crowd
Translation – Design and Objectives. 2014.
</reference>
<page confidence="0.999136">
354
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.493488">
<title confidence="0.981015">TeamUFAL: as Document</title>
<author confidence="0.979203">Petr Fanta</author>
<author confidence="0.979203">Roman Sudarikov</author>
<author confidence="0.979203">Ondˇrej</author>
<affiliation confidence="0.919635333333333">Charles University in Faculty of Mathematics and Institute of Formal and Applied</affiliation>
<address confidence="0.540761">Malostransk´e n´amˇest´ı 25, 11800 Praha 1, Czech</address>
<abstract confidence="0.995728333333333">This paper describes our system for SemEval- 2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking. We have participated with our system in the sub-task which aims at monolingual all-words disambiguation and entity linking. Aside from system description, we pay closer attention to the evaluation of system outputs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Khaled Abdalgader</author>
<author>Andrew Skabar</author>
</authors>
<title>Short-text similarity measurement using word sense disambiguation and synonym expansion.</title>
<date>2011</date>
<booktitle>In AI 2010: Advances in Artificial Intelligence,</booktitle>
<pages>435--444</pages>
<contexts>
<context position="1435" citStr="Abdalgader and Skabar, 2011" startWordPosition="208" endWordPosition="212"> a particular named entity listed in a database given its mention in a text) are among the fashionable tasks in computational linguistics and natural language processing these days. WSD has been, after some debate, shown to help machine translation (Carpuat and Wu, 2007), other applications include knowledge discovery or machine reading in general (Etzioni et al., 2006; Schubert, 2006). WSD and EL are usually applied with large and rich context available (Navigli, 2009), but the arguably harder setting of short context has a wider range of applications, including text similarity measurements (Abdalgader and Skabar, 2011), Named Entities Extraction and Named Entities Disambiguation (Habib and Keulen, 2012) ∗This research was supported by the grants FP7-ICT-2013- 10-610516 (QTLeap). This research was partially supported by SVV project number 260 224. This work has been using language resources developed, stored and distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2010013). or handling data from social networks, such as attempts to translate tweets (ˇSubert and Bojar, 2014). Our attempt at WSD and EL can be classified as unsupervised, corpus</context>
</contexts>
<marker>Abdalgader, Skabar, 2011</marker>
<rawString>Khaled Abdalgader and Andrew Skabar. Short-text similarity measurement using word sense disambiguation and synonym expansion. In AI 2010: Advances in Artificial Intelligence, pages 435– 444. 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving Statistical Machine Translation Using Word Sense Disambiguation. In EMNLP-CoNLL,</title>
<date>2007</date>
<volume>7</volume>
<pages>61--72</pages>
<contexts>
<context position="1078" citStr="Carpuat and Wu, 2007" startWordPosition="154" endWordPosition="157">th our system in the sub-task which aims at monolingual all-words disambiguation and entity linking. Aside from system description, we pay closer attention to the evaluation of system outputs. 1 Introduction Word sense disambiguation (WSD, i.e. picking the right sense for a given word from a fixed inventory) and entity linking (EL, i.e. identifying a particular named entity listed in a database given its mention in a text) are among the fashionable tasks in computational linguistics and natural language processing these days. WSD has been, after some debate, shown to help machine translation (Carpuat and Wu, 2007), other applications include knowledge discovery or machine reading in general (Etzioni et al., 2006; Schubert, 2006). WSD and EL are usually applied with large and rich context available (Navigli, 2009), but the arguably harder setting of short context has a wider range of applications, including text similarity measurements (Abdalgader and Skabar, 2011), Named Entities Extraction and Named Entities Disambiguation (Habib and Keulen, 2012) ∗This research was supported by the grants FP7-ICT-2013- 10-610516 (QTLeap). This research was partially supported by SVV project number 260 224. This work </context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. Improving Statistical Machine Translation Using Word Sense Disambiguation. In EMNLP-CoNLL, volume 7, pages 61–72, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Cornolti</author>
<author>Paolo Ferragina</author>
<author>Massimiliano Ciaramita</author>
</authors>
<title>A framework for benchmarking entity-annotation systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd international conference on World Wide Web,</booktitle>
<pages>249--260</pages>
<contexts>
<context position="10006" citStr="Cornolti et al. (2013)" startWordPosition="1649" endWordPosition="1652">, we find some of its features unusual: • The precision of a system is not penalized for spans, which don’t occur in the golden set. • The recall should consider only to what extent the expected answers are covered by the system’s answers. The official scoring script reduces the recall score for any ’unexpected’ answers. • An exact match in span is needed to give any credit to the system answer. We thus propose a slightly different evaluation procedure and apply it to all submitted systems. 4.1 Our Proposed Scoring Our scoring is based on a credit for partially overlapping spans, similarly to Cornolti et al. (2013), who however disregard the overlap size. We call a ‘label’ l = (l1, l2) the pair of a span (a range of words in the sentence; denoted l1) and an ID attached to the span, l2. For a label s in the system output and a label g in the golden annotation, we define their match as: �|g1ns1 |if |g1 ∩ s1 g &gt; 0 n 2 = s2 9) match(s Islus1 |_ 0 otherwise In other words overlapping spans labeled with the same ID get a credit proportional to the size of the overlap. We define precision and recall as follows: precision = K ES,gEG match(s, g) |S| recall = KES,gEG match(s, g) |G| where G and S are sets of labe</context>
</contexts>
<marker>Cornolti, Ferragina, Ciaramita, 2013</marker>
<rawString>Marco Cornolti, Paolo Ferragina, and Massimiliano Ciaramita. A framework for benchmarking entity-annotation systems. In Proceedings of the 22nd international conference on World Wide Web, pages 249–260, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
</authors>
<title>Machine Reading.</title>
<date>2006</date>
<booktitle>In AAAI,</booktitle>
<volume>6</volume>
<pages>1517--1519</pages>
<contexts>
<context position="1178" citStr="Etzioni et al., 2006" startWordPosition="169" endWordPosition="172">Aside from system description, we pay closer attention to the evaluation of system outputs. 1 Introduction Word sense disambiguation (WSD, i.e. picking the right sense for a given word from a fixed inventory) and entity linking (EL, i.e. identifying a particular named entity listed in a database given its mention in a text) are among the fashionable tasks in computational linguistics and natural language processing these days. WSD has been, after some debate, shown to help machine translation (Carpuat and Wu, 2007), other applications include knowledge discovery or machine reading in general (Etzioni et al., 2006; Schubert, 2006). WSD and EL are usually applied with large and rich context available (Navigli, 2009), but the arguably harder setting of short context has a wider range of applications, including text similarity measurements (Abdalgader and Skabar, 2011), Named Entities Extraction and Named Entities Disambiguation (Habib and Keulen, 2012) ∗This research was supported by the grants FP7-ICT-2013- 10-610516 (QTLeap). This research was partially supported by SVV project number 260 224. This work has been using language resources developed, stored and distributed by the LINDAT/CLARIN project of </context>
</contexts>
<marker>Etzioni, Banko, Cafarella, 2006</marker>
<rawString>Oren Etzioni, Michele Banko, and Michael J. Cafarella. Machine Reading. In AAAI, volume 6, pages 1517–1519, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mena B Habib</author>
<author>Maurice Keulen</author>
</authors>
<title>Unsupervised improvement of named entity extraction in short informal context using disambiguation clues.</title>
<date>2012</date>
<contexts>
<context position="1521" citStr="Habib and Keulen, 2012" startWordPosition="220" endWordPosition="223">fashionable tasks in computational linguistics and natural language processing these days. WSD has been, after some debate, shown to help machine translation (Carpuat and Wu, 2007), other applications include knowledge discovery or machine reading in general (Etzioni et al., 2006; Schubert, 2006). WSD and EL are usually applied with large and rich context available (Navigli, 2009), but the arguably harder setting of short context has a wider range of applications, including text similarity measurements (Abdalgader and Skabar, 2011), Named Entities Extraction and Named Entities Disambiguation (Habib and Keulen, 2012) ∗This research was supported by the grants FP7-ICT-2013- 10-610516 (QTLeap). This research was partially supported by SVV project number 260 224. This work has been using language resources developed, stored and distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2010013). or handling data from social networks, such as attempts to translate tweets (ˇSubert and Bojar, 2014). Our attempt at WSD and EL can be classified as unsupervised, corpus-based and our implementation relies on an information retrieval tool. We do not take </context>
</contexts>
<marker>Habib, Keulen, 2012</marker>
<rawString>Mena B. Habib and Maurice Keulen. Unsupervised improvement of named entity extraction in short informal context using disambiguation clues. 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking.</title>
<date>2015</date>
<booktitle>In Proc. of SemEval-2015,</booktitle>
<contexts>
<context position="2233" citStr="Moro and Navigli, 2015" startWordPosition="331" endWordPosition="334">ch was partially supported by SVV project number 260 224. This work has been using language resources developed, stored and distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2010013). or handling data from social networks, such as attempts to translate tweets (ˇSubert and Bojar, 2014). Our attempt at WSD and EL can be classified as unsupervised, corpus-based and our implementation relies on an information retrieval tool. We do not take longer context into account. 2 Task Description As participants of SemEval-2015 Task 13 (Moro and Navigli, 2015), we were given only a very brief instructions, effectively just one example of a POStagged sentence: The/X European/J/european Medicines/N/medicine Agency/N/agency (/X EMA/N/ema )/X is/V/be ,.. . We were expected to provide such input with labels indicating that e.g. the words “European Medicines Agency” refer to the entity described in the English Wikipedia under the title European Medicines Agency (“wiki:European Medicines Agency”), the word “Medicines” refers to the BabelNet concept 00054128n etc. The repertoire of word sense and entities came from BabelNet 2.5 which included: Wikipedia pa</context>
<context position="13111" citStr="Moro and Navigli, 2015" startWordPosition="2206" endWordPosition="2209"> of correct (confirmed by the golden data) IDs among all labels produced by the system. Recall is the number of correct IDs divided by the number of labels in the gold set. This scoring gives an idea of how well the system guesses the “meaning” (bag of concepts) of the whole sentence. The Table 1 documents that the official scoring heavily boosted our precision and hurt our recall. The performances of other systems are affected as well, but fortunately, the overall impression is similar across the scoring techniques. 4.3 Variants of Our Submission As the official scores in the overview paper (Moro and Navigli, 2015) show, our system performed acceptably on Named Entities Recognition task, but it clearly failed on word senses disambiguation. Table 2 reports the scores (official scoring) of a few variations of our approach. The first row is the submitted system, the second row is a correction which allows Wiktionary results to map to BabelNet senses of all parts of speech, not just nouns. The remaining rows use a different IR model or include sentence context in Wiktionary search but no improvement is obtained. 4.4 Recommendations for Future Evaluation For future shared tasks, we recommend: • Define precis</context>
</contexts>
<marker>Moro, Navigli, 2015</marker>
<rawString>Andrea Moro and Roberto Navigli. SemEval2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking. In Proc. of SemEval-2015, 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1281" citStr="Navigli, 2009" startWordPosition="187" endWordPosition="189">rd sense disambiguation (WSD, i.e. picking the right sense for a given word from a fixed inventory) and entity linking (EL, i.e. identifying a particular named entity listed in a database given its mention in a text) are among the fashionable tasks in computational linguistics and natural language processing these days. WSD has been, after some debate, shown to help machine translation (Carpuat and Wu, 2007), other applications include knowledge discovery or machine reading in general (Etzioni et al., 2006; Schubert, 2006). WSD and EL are usually applied with large and rich context available (Navigli, 2009), but the arguably harder setting of short context has a wider range of applications, including text similarity measurements (Abdalgader and Skabar, 2011), Named Entities Extraction and Named Entities Disambiguation (Habib and Keulen, 2012) ∗This research was supported by the grants FP7-ICT-2013- 10-610516 (QTLeap). This research was partially supported by SVV project number 260 224. This work has been using language resources developed, stored and distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2010013). or handling data</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR), 41(2):10, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Steve Walker</author>
<author>Susan Jones</author>
<author>Micheline M Hancock-Beaulieu</author>
<author>Mike Gatford</author>
</authors>
<date>1995</date>
<booktitle>Okapi at TREC-3. NIST SPECIAL PUBLICATION SP,</booktitle>
<pages>109--109</pages>
<contexts>
<context position="5447" citStr="Robertson et al., 1995" startWordPosition="854" endWordPosition="857">BabelNet alone is not a good resource for our approach, because it does not include textual data. We resort to the original sources of BabelNet and map them back to BabelNet. Our sources are thus the English Wikipedia, English Wiktionary and WordNet. Short of the original versions as used in BabelNet 2.5, we used Wiki dumps from November 2014 and WordNet 3.0, facing some ID mismatches. 3.2 Indexing For each source, we create a full text index using Apache Lucene search engine which provides several ranking models. We experiment with models based on TF-IDF (Salton et al., 1975) and Okapi BM25 (Robertson et al., 1995), selecting the better one for each subsystem in our submission. All indexes have a similar structure, they contain: Score Document ID (ie. Wikipedia Title) 8.201 Medical condition → Disease 8.201 Medical conditions → Disease 6.561 Frostbite (medical condition) → Frostbite Figure 1: Query results (→ means redirection closure). ID of the element in the given source (Wikipedia ID, Wiktionary ID or Wordnet sense key), Title of Wikipedia or Wiktionary article or word from WordNet, Body text of articles from Wiki sources (markup removed) or all textual data from Wordnet synset (including other word</context>
</contexts>
<marker>Robertson, Walker, Jones, Hancock-Beaulieu, Gatford, 1995</marker>
<rawString>Stephen E. Robertson, Steve Walker, Susan Jones, Micheline M. Hancock-Beaulieu, Mike Gatford, et al. Okapi at TREC-3. NIST SPECIAL PUBLICATION SP, pages 109–109, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Anita Wong</author>
<author>Chung-Shu Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="5407" citStr="Salton et al., 1975" startWordPosition="847" endWordPosition="850"> these sub-systems. 3.1 Data Sources BabelNet alone is not a good resource for our approach, because it does not include textual data. We resort to the original sources of BabelNet and map them back to BabelNet. Our sources are thus the English Wikipedia, English Wiktionary and WordNet. Short of the original versions as used in BabelNet 2.5, we used Wiki dumps from November 2014 and WordNet 3.0, facing some ID mismatches. 3.2 Indexing For each source, we create a full text index using Apache Lucene search engine which provides several ranking models. We experiment with models based on TF-IDF (Salton et al., 1975) and Okapi BM25 (Robertson et al., 1995), selecting the better one for each subsystem in our submission. All indexes have a similar structure, they contain: Score Document ID (ie. Wikipedia Title) 8.201 Medical condition → Disease 8.201 Medical conditions → Disease 6.561 Frostbite (medical condition) → Frostbite Figure 1: Query results (→ means redirection closure). ID of the element in the given source (Wikipedia ID, Wiktionary ID or Wordnet sense key), Title of Wikipedia or Wiktionary article or word from WordNet, Body text of articles from Wiki sources (markup removed) or all textual data f</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Gerard Salton, Anita Wong, and Chung-Shu Yang. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart Schubert</author>
</authors>
<title>Turing’s Dream and the Knowledge Challenge.</title>
<date>2006</date>
<booktitle>In Proc. of the national conference on artificial intelligence,</booktitle>
<volume>21</volume>
<pages>1534</pages>
<contexts>
<context position="1195" citStr="Schubert, 2006" startWordPosition="173" endWordPosition="174">ription, we pay closer attention to the evaluation of system outputs. 1 Introduction Word sense disambiguation (WSD, i.e. picking the right sense for a given word from a fixed inventory) and entity linking (EL, i.e. identifying a particular named entity listed in a database given its mention in a text) are among the fashionable tasks in computational linguistics and natural language processing these days. WSD has been, after some debate, shown to help machine translation (Carpuat and Wu, 2007), other applications include knowledge discovery or machine reading in general (Etzioni et al., 2006; Schubert, 2006). WSD and EL are usually applied with large and rich context available (Navigli, 2009), but the arguably harder setting of short context has a wider range of applications, including text similarity measurements (Abdalgader and Skabar, 2011), Named Entities Extraction and Named Entities Disambiguation (Habib and Keulen, 2012) ∗This research was supported by the grants FP7-ICT-2013- 10-610516 (QTLeap). This research was partially supported by SVV project number 260 224. This work has been using language resources developed, stored and distributed by the LINDAT/CLARIN project of the Ministry of E</context>
</contexts>
<marker>Schubert, 2006</marker>
<rawString>Lenhart Schubert. Turing’s Dream and the Knowledge Challenge. In Proc. of the national conference on artificial intelligence, volume 21, page 1534, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard ˇSubert</author>
<author>Ondˇrej Bojar</author>
</authors>
<title>Twitter Crowd Translation – Design and Objectives.</title>
<date>2014</date>
<marker>ˇSubert, Bojar, 2014</marker>
<rawString>Eduard ˇSubert and Ondˇrej Bojar. Twitter Crowd Translation – Design and Objectives. 2014.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>