<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002169">
<title confidence="0.995174">
UBC: Cubes for English Semantic Textual Similarity and Supervised
Approaches for Interpretable STS
</title>
<author confidence="0.9211435">
Eneko Agirre, Aitor Gonzalez-Agirre, I˜nigo Lopez-Gazpio,
Montse Maritxalar, German Rigau, Larraitz Uria
</author>
<affiliation confidence="0.937649">
University of the Basque Country
</affiliation>
<address confidence="0.6529605">
Donostia, 20018, Basque Country
{e.agirre, aitor.gonzalez-agirre, inigo.lopez,
</address>
<email confidence="0.98076">
montse.maritxalar, german.rigau, larraitz.uria}@ehu.eus
</email>
<sectionHeader confidence="0.99374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998275384615385">
In Semantic Textual Similarity, systems rate
the degree of semantic equivalence on a
graded scale from 0 to 5, with 5 being the most
similar. For the English subtask, we present
a system which relies on several resources for
token-to-token and phrase-to-phrase similarity
to build a data-structure which holds all the in-
formation, and then combine the information
to get a similarity score. We also participated
in the pilot on Interpretable STS, where we ap-
ply a pipeline which first aligns tokens, then
chunks, and finally uses supervised systems to
label and score each chunk alignment.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99975405">
In Semantic Textual Similarity (STS), systems rate
the degree of semantic equivalence on a graded scale
from 0 to 5, with 5 being the most similar. We partic-
ipated in two of the subtask for STS in 2015 (Agirre
et al., 2015). For the English subtask, we present a
system which relies on several resources for token-
to-token and phrase-to-phrase similarity to build a
data-structure which holds all the information, and
then combine the information to get a similarity
score. We also participated in the pilot on Inter-
pretable STS, where we apply a pipeline which first
aligns tokens, then chunks, and finally uses super-
vised systems to label and score each chunk align-
ment.
Note that some of the authors participated in the
organization of the task. We scrupulously separated
the tasks in such a way that the developers of the
systems did not have access to the test sets, and that
they only had access to the same training data as the
rest of the participants.
</bodyText>
<sectionHeader confidence="0.995193" genericHeader="method">
2 Cubes for English STS
</sectionHeader>
<bodyText confidence="0.999902285714286">
In this section we describe a novel approach to com-
pute similarity scores between two sentences using
a cube where each layer contains token-to-token and
phrase-to-phrase similarity scores from a different
method and/or resource. Our assumption is that we
can obtain better results using this similarity scores
together than independently.
</bodyText>
<subsectionHeader confidence="0.980682">
2.1 Building Cubes
</subsectionHeader>
<bodyText confidence="0.999854416666667">
The first step is to produce parse trees for the sen-
tences using the Stanford Parser (Toutanova et al.,
2003). After parsing the sentences each pair of sen-
tences can be represented by a NxM matrix, being N
is the number of nodes of the parse tree of the first
sentence, and M the number of nodes of the parse
tree of the second sentence. Note that some nodes
(terminals) correspond to words, while others (non-
terminals) represent phrases. We can have as many
matrices as we wish, and fill them with different sim-
ilarity scores, forming a cube.
In this first attempt we used three layers:
</bodyText>
<listItem confidence="0.9905872">
1. Euclidean distance between Collobert and We-
ston Word Vector (Collobert and Weston,
2008). The vector representations for each
non-terminal node in the tree were learnt us-
ing Recursive Autoencoder (RAE) (Socher et
al., 2011).
2. Euclidean distance between Mikolov Word
Vectors (Mikolov et al., 2013a; Mikolov et
al., 2013b). To compute the vector represen-
tations for each non-terminal node in the tree,
we summed the vectors and normalize them di-
viding by the number of words in the phrase.
3. PPDB Paraphrase database values (Ganitke-
vitch et al., 2013). We used the XXXL ver-
sion. In this case both words and some phrases
</listItem>
<page confidence="0.959423">
178
</page>
<bodyText confidence="0.89898725">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 178–183,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
are contained in the resource. This resource
yields conditional probabilities. As our scores
are undirected, in case the database contains
values for both directions, we average.
The first two produce a dense layer, where most
of the cells have a value. The third one produces a
sparse layer, where only the pairs occurring in the
resource have a value. Note that some of the phrases
in PPDB do not correspond to a node in the tree. In
this case, we add extra columns and rows.
</bodyText>
<subsectionHeader confidence="0.999836">
2.2 Producing STS Score
</subsectionHeader>
<bodyText confidence="0.999589470588235">
Before computing a similarity score we flatten our
cube into a single layer, where each of the element
in the new NxM matrix is the maximum between
the values for that position across all the layers. We
do that because we studied the different resources
and we think that these resources have less False
Positives (FP) than False Negatives (FN). In other
words, if one of the resources says that something
is very similar we trust on it and take that similarity
score instead of the other (even if they are very low).
Moreover, our assumption is that the final similarity
score is specially based in similarities between to-
kens/phrases in the sentences, and not on dissimilar-
ities.
Once we have this matrix, we compute the final
STS score using the scoring function seen in (Mi-
halcea et al., 2006).
</bodyText>
<equation confidence="0.996436">
1 (r,w∈S1 (maxSim(w∈S2) * idf (w))
2 1\ Pw∈S1 idf(w)
Pw∈S2 (maxSim(w∈S1) * idf (w))1
Pw∈S2 idf (w) /I
</equation>
<subsectionHeader confidence="0.843981">
2.3 Results
</subsectionHeader>
<bodyText confidence="0.99988125">
Due to time constraints we submitted a single run,
which ranked 54 among 74 runs. We expect to im-
prove this results adding more layers and combining
them using more sophisticated aggregation methods.
</bodyText>
<sectionHeader confidence="0.5520255" genericHeader="method">
3 Participating on the Interpretable STS
Pilot Subtask
</sectionHeader>
<bodyText confidence="0.9636345">
The SemEval 2015 STS task offered a new pilot sub-
task on interpretable STS1. Given a sentence pair,
</bodyText>
<footnote confidence="0.9552265">
1http://alt.qcri.org/semeval2015/task2/
index.php?id=proba
</footnote>
<bodyText confidence="0.999935894736842">
the objective of the subtask is to align segments per-
taining to one sentence with the segments pertaining
to the other sentence. The whole subtask is in deep
described in (Agirre et al., 2015).
In sum, every alignment may consist of a similar-
ity score and a relatedness tag. The similarity score
is a real number bounded by [0,5] where 0 means
no relation at all and 5 means complete equivalence.
As regards the relatedness tag, there exists a set of
categorical values to choose on, such as: equiva-
lence, opposition, specialization (direction is rele-
vant), similarity and one more tag for other kind of
relatedness.
For the case of unaligned segments there are an-
other two possible categorical values. The one for
declaring segments unaligned (not aligned); and
the other to declare that the segment related to the
current segment has already been aligned (context
alignment). Notice that due to the limitations of
the current pilot the only way to align segments
is making 1:1 alignments. Thus, 1:N alignments
are simulated making an 1:1 alignment and sev-
eral context alignments. This concept is relevant
to the work done in section 3.1.2 when we extend
the Hungarian-Munkres (Clapper, 2009) algorithm
to identify already aligned chunks.
In addition, factuality or polarity connotations can
be added as requested to the previously mentioned
tags. Two different scenarios are provided in the pi-
lot subtask, the first one makes gold standard seg-
ments available for participants (Gold Chunks or GS
scenario); and, the second one, only provides sen-
tence raw text (System Chunks or SYS scenario).
In conclusion, the first pilot on interpretable STS
seems challenging because participating systems
must not only discover and score the relatedness be-
tween segments, but also identify the inner relation
between them.
</bodyText>
<subsectionHeader confidence="0.999241">
3.1 System Description
</subsectionHeader>
<bodyText confidence="0.999721222222222">
This section describes the principal algorithm and
the distinct modules it uses, modules are further de-
scribed in the following subsections (3.1.1, 3.1.2,
3.1.3 and 3.1.4). System configurations (runs) used
to submit results are described in section 3.1.5.
The system makes use of several modules to
identify segments over sentence pairs, and then,
make alignments between them. First of all, the
input handling and chunking module is responsible
</bodyText>
<equation confidence="0.909459">
sim(S1, S2) =
+
</equation>
<page confidence="0.983711">
179
</page>
<bodyText confidence="0.999976125">
for linguistically processing the given input, and for
creating the internal representation of the sentences.
Once the input is processed the alignment module
identifies related and unrelated segments among sen-
tences. Finally, by using segment pair based fea-
tures the classification module and the scoring mod-
ule produce respectively the final relatedness tag and
the similarity score.
</bodyText>
<subsectionHeader confidence="0.730747">
3.1.1 Input Handling and Chunking Module
</subsectionHeader>
<bodyText confidence="0.999939807692308">
We use the Stanford NLP parser (Klein and
Manning, 2003) to linguistically process input sen-
tences and register lowercased token information
(lemma, part of speech analysis and dependency
structure is also needed for the following module).
The next step consists of determining segments or
token regions. This information is gathered accord-
ing to the specified scenario (GS or SYS). In the case
of the GS scenario the baseline obviously uses gold
standard input; and, in the SYS scenario the baseline
uses the ixa-pipes-chunker (Agerri et al., 2014).
Ixa-pipes-chunk has been trained using the
Apache OpenNLP API (OpenNLP, 2011), which is a
maximum entropy chunker. Nevertheless, the chun-
ker’s output has been improved using simple regular
expressions to fit to our task proposal. Actually, we
developed four rules to optimize how conjunctions,
punctuations and prepositions are handled. In brief,
the developed rules try to join consequent chunks
forming new chunks consisting of the previous ones,
for instance, we found significant improvement if
prepositional phrases followed by a nominal phrase
were unified as a single chunk. We also developed
some rules to unify nominal phrases separated by
punctuations or conjunctions, or a combination of
those.
</bodyText>
<subsectionHeader confidence="0.837126">
3.1.2 Alignment Module
</subsectionHeader>
<bodyText confidence="0.999484548387097">
The alignment module mainly focuses on the
work done by the monolingual word aligner de-
scribed in (Sultan et al., 2014), and Hungarian-
Munkres algorithm.
The monolingual word aligner is a simple and
ready-to-use system that has demonstrated state-of-
the-art performance. To begin with we start by con-
structing the token to token link matrix in which each
element at position (i,j) determines that there exists
a link between token i (from sentence 1) and token j
(from sentence 2). A link exists in the matrix if and
only if the monolingual word aligner has determined
that both tokens are related.
Then, the system uses token regions to group in-
dividual tokens into segments, and calculates the
weight between every segment in the sentence pair.
The weight among two segments is proportional to
the number of links that interconnect tokens inside
those segments. In other words, by summing regions
we collapse the token to token link matrix onto a
chunk to chunk link matrix. After that, we use the
mentioned Hungarian-Munkres algorithm to dis-
cover which are the segments (x,y) which score the
highest weight (link ratio); but also, we extend it to
discover which are the segments that are linked to
either segment x or segment y, but not with a max-
imum alignment ratio. This processing to find not-
maximal weights is essential to effectively assign the
context alignment tag for 1:N relations. In addition,
the system is also aware of chunks that have been
left unaligned.
</bodyText>
<subsectionHeader confidence="0.636777">
3.1.3 Classification Module
</subsectionHeader>
<bodyText confidence="0.999983925925926">
The system can use one of the following ap-
proaches to assign relatedness tags to segment pairs:
the naive approach and the machine learning ap-
proach. The naive approach directly assigns the
tag as a majority classifier would do, that is: for
the segments with highest weight it always assigns
the equivalence tag, for the segments that are linked
with lower weights it always assigns the context
alignment tag, and for the not aligned segments it
always assigns the not aligned tag.
The machine learning approach makes use of
the segment-pair to calculate a total of 21 features
to improve the tag assignment. The objective of
the induced model is to refine the output given by
the naive approach only for segment pairs tagged as
equivalent. The features used to induce the model
can be classified in the following groups: Jaccard
overlap related features, segment length related fea-
tures, WordNet similarity related features among
segment heads, WordNet depth related features, and
other kind of features obtained by means of the cube
described in section 2.
To induce the model we use the Support Vec-
tor Machine (SVM) implementation described in
(Chang and Lin, 2011) under the latest experimental
version of Weka (Hall et al., 2009) using randomly
shuffled 5-fold cross validation. We indistinctly join
</bodyText>
<page confidence="0.993617">
180
</page>
<bodyText confidence="0.9980355">
the available datasets and grid search to optimize the
cost and gamma parameters.
</bodyText>
<subsectionHeader confidence="0.868463">
3.1.4 Scoring Module
</subsectionHeader>
<bodyText confidence="0.999981461538461">
To assign segment pair similarity scores the sys-
tem can also use two distinct approaches: the naive
approach and the cube based regression approach.
The naive scorer directly assigns a certain score to
each one of the tags, which has been previously as-
signed using the naive tagger: for equivalence tags it
assigns a score of 5 and for not aligned and context
aligned tags it assigns ’NIL’. (as requested by the
guidelines). The regression approach uses the cube
described in section 2 to improve the score given to
segment pairs tagged by the machine learning tag-
ger. Its returning value is used directly as the value
for the pair similarity score.
</bodyText>
<subsectionHeader confidence="0.645916">
3.1.5 Submitted Runs
</subsectionHeader>
<bodyText confidence="0.999940333333333">
Even the subtask allows the submission of up to
three runs, we only submitted two distinct configu-
rations, named run] and runt. run1 and run2 are
mainly the same system, but run1 makes use of the
naive approaches for both classification and scor-
ing tasks; whereas run2 makes use of the machine
learning approach for the tag assignment and the
cube based regression approach for the scoring as-
signment.
</bodyText>
<subsectionHeader confidence="0.998119">
3.2 Result Analysis
</subsectionHeader>
<bodyText confidence="0.999985444444444">
Participating runs were evaluated using the official
scorer provided by task organizers, which computes
four distinct metrics: F] ALI (segment pair align-
ment correctness regardless of the tag), F] Type
(segment pair alignment correctness taking tag into
account), F] Score (segment pair alignment correct-
ness taking score into account) and F] Type + Score
(segment pair alignment correctness taking tag and
score into account).
</bodyText>
<subsectionHeader confidence="0.663471">
3.2.1 Development
</subsectionHeader>
<bodyText confidence="0.999894875">
We developed two runs as above described using
the training data provided by task organizers. Train-
ing data consists of two datasets (images dataset and
headlines dataset) with 750 sentence pairs each. We
built and evaluated our system using 5-fold cross
validation and using a grid search optimization to
tune the SVM parameters. Results for both runs are
shown in table 1.
</bodyText>
<table confidence="0.997849416666667">
I GS Ali Type Score Type + Score
Run1 0.8942 0.5115 0.7776 0.5115
Run2 0.8942 0.7408 0.8175 0.6934
I SYS Ali Type Score Type + Score
Run1 0.8379 0.4734 0.7271 0.4734
Run2 0.8379 0.6499 0.7627 0.6106
H GS Ali Type Score Type + Score
Run1 0.8920 0.5740 0.7869 0.5738
Run2 0.8920 0.6908 0.8133 0.6544
H SYS Ali Type Score Type + Score
Run1 0.7650 0.4808 0.6707 0.4808
Run2 0.7650 0.5210 0.6862 0.4902
</table>
<tableCaption confidence="0.978549">
Table 1: Development results for both datasets in the
two scenarios. ’I’ stands for the images dataset, and ’H’
stands for the headlines dataset.
</tableCaption>
<bodyText confidence="0.999994684210527">
The table shows that run2 outperforms run1 in all
of the scenarios, which was expected as run1 is us-
ing the naive approach for both: the relatedness tag
and the similarity score assignment. Notice that both
runs obtain the same F1 Alignment score as both
runs are using the same input handling and chunk-
ing module. Without the shadow of a doubt, we can
observe that for both datasets the F1 alignment is
noticeable higher in the GS scenario than in the SYS
scenario. Moreover, as evaluation measures are in-
cremental, F1 Type, F1 Score and F1 Type + Score
are also lower for the SYS scenario.
It is also important to mention that the difference
in performance (F Type+Score) between run1 and
run2 is more noticeable in the images dataset, ac-
tually, for the headlines dataset in the SYS scenario,
the difference between both runs is under 0.01. This
difference increases up to 0.08 for the headlines
dataset in the GS scenario.
</bodyText>
<subsectionHeader confidence="0.988904">
3.2.2 Test
</subsectionHeader>
<bodyText confidence="0.9999855">
The test dataset was composed of 378 sentence
pairs for the headlines dataset and of 375 sentence
pairs for the images dataset. Table 2 illustrates the
results obtained by run1 and run2. The results ob-
tained for the test datasets follow in general the same
tendency as the one seen for the development. In
fact, run2 most of the times outperforms run1;
being this difference in performance more notice-
able in the images dataset than in the headlines
dataset. It might be necessary to further analyze the
</bodyText>
<page confidence="0.993493">
181
</page>
<table confidence="0.999735375">
I GS Ali Type Score Type + Score
Baseline 0.8388 0.4328 0.721 0.4326
Run1 0.8846 0.4749 0.7709 0.4746
Run2 0.8846 0.6557 0.8085 0.6159
MAX Par 0.887 0.6143 0.7968 0.5964
AVG Par 0.8193 0.5004 0.7197 0.4748
I SYS Ali Type Score Type + Score
Baseline 0.706 0.3696 0.6092 0.3693
Run1 0.8388 0.445 0.728 0.4447
Run2 0.8388 0.6019 0.7634 0.5643
MAX Par 0.8336 0.5759 0.7511 0.5634
AVG Par 0.67 0.4086 0.5892 0.3912
H GS Ali Type Score Type + Score
Baseline 0.8448 0.5556 0.7551 0.5556
Run1 0.8991 0.5882 0.8031 0.5882
Run2 0.8991 0.6402 0.8211 0.6185
MAX Par 0.8984 0.6666 0.8263 0.6426
AVG Par 0.8365 0.5576 0.7468 0.5381
H SYS Ali Type Score Type + Score
Baseline 0.6701 0.4571 0.6066 0.4571
Run1 0.7709 0.5019 0.6892 0.5019
Run2 0.7709 0.4865 0.7014 0.4705
MAX Par 0.782 0.5154 0.7024 0.5098
AVG Par 0.6870 0.4498 0.6094 0.4335
</table>
<tableCaption confidence="0.910329333333333">
Table 2: Test results for both datasets in the two scenar-
ios. ’I’ stands for the images dataset, ’H’ stands for the
headlines dataset and ’Par’ stands for participants.
</tableCaption>
<bodyText confidence="0.9994732">
unique scenario in which run1 obtains higher accu-
racy than run2 (Headlines SYS), but actually, results
have been also very close at development in this con-
text.
The baseline seems to be not that trivial as it
sometimes outperforms participants average perfor-
mance; but as we can see both of our runs obtain
higher accuracy than the baseline, in both cases by
large margin. For example, in the images dataset
the difference between the baseline and the second
run is 0.18 and 0.19 respectively for the GS and the
SYS scenario. Regarding other participants, we can
conclude that our runs obtain quite good results, spe-
cially for the images dataset where run2 obtains the
highest score.
</bodyText>
<sectionHeader confidence="0.996491" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999738838709677">
Through this paper we have described the systems
that participated in the Semantic Textual Similarity
task 2A (English STS) and 2C (Interpretable STS).
Our main focus in the English subtask was on de-
ploying our idea of building a cube with similarity
information from several sources. We are currently
working on more layers, including Random Walks
over WordNet and Wikipedia, string similarity (Fer-
rone and Zanzotto, 2014), and also a special layer
to deal with numbers. Additionally, we are consid-
ering the idea of dissimilarity layers, for instance,
adding information about negation and antonymy.
We are also developing new methods to combine
these knowledge to generate the final STS score.
Regarding the interpretable STS system, this was
the first time a pilot was put in place. We obtained
excellent results, even if we had very little time to
develop the system. Future work will focus on fur-
ther improvements. For instance, our experiments
showed that grouping chunks lead to a considerable
improvement for the F1 Type evaluation score. We
would also like to incorparate factuality or polarity
information.
Although our original idea was to combine the
cube and the interpretable system, we did not have
time for that. In one direction, we would like to in-
corparate some of the semantic similarity informa-
tion in the cube into our system, including similarity
between chunks. On the other direction, the infor-
mation from the similarity module might be a good
feature to improve the overall STS score.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999878">
This work was partially funded by MINECO
(CHIST-ERA READERS project – PCIN-2013-
002-C02-01, SKaTeR project – TIN2012-38584-
C06-02), and the European Commission (QTLEAP
– FP7-ICT-2013.4.1-610516). Aitor Gonzalez-
Agirre and I˜nigo Lopez-Gazpio are supported by
doctoral grants from MINECO. The IXA group is
funded by the Basque Government (A type Research
Group).
</bodyText>
<sectionHeader confidence="0.998192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.963769">
Rodrigo Agerri, Josu Bermudez, and German Rigau.
2014. Ixa pipeline: Efficient and ready to use multi-
</reference>
<page confidence="0.991215">
182
</page>
<reference confidence="0.985092342857143">
lingual nlp tools. In Proceedings of the 9th Language
Resources and Evaluation Conference (LREC2014),
Reykjavik, Iceland, May, pages 26–31.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
I˜nigo Lopez-Gazpio, Montse Maritxalar, Rada Mihal-
cea, German Rigau, Larraitz Uria, and Janyce Wiebe.
2015. SemEval-2015 Task 2: Semantic Textual Simi-
larity, English, Spanish and Pilot on Interpretability. In
Proceedings of the 9th International Workshop on Se-
mantic Evaluation (SemEval 2015), Denver, CO, June.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
BM Clapper. 2009. munkres: a python module
implementing the “hungarian method” described by
munkres (1957). version 1.0. 5.3.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceed-
ings of the 25th International Conference on Machine
Learning, ICML ’08, pages 160–167, New York, NY,
USA.
Lorenzo Ferrone and Massimo Fabio Zanzotto. 2014.
Towards syntax-aware compositional distributional se-
mantic models. In Proceedings of COLING 2014, the
25th International Conference on Computational Lin-
guistics: Technical Papers, pages 721–730.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of NAACL-HLT, pages 758–
764, Atlanta, Georgia, June.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten. 2009.
The weka data mining software: an update. ACM
SIGKDD explorations newsletter, 11(1):10–18.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Associ-
ation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of the
American Association forArtificial Intelligence (AAAI
2006), Boston, Massachusetts, July.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word representa-
tions in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed represen-
tations of words and phrases and their compositional-
ity. CoRR, abs/1310.4546.
Apache OpenNLP. 2011. Apache software foundation.
URL http://opennlp. apache. org.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems 24.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual evi-
dence. Transactions of the Association for Computa-
tional Linguistics, pages 219–230.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252–259.
</reference>
<page confidence="0.999197">
183
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.341585">
<title confidence="0.823217666666667">UBC: Cubes for English Semantic Textual Similarity and Approaches for Interpretable STS Agirre, Aitor Gonzalez-Agirre,</title>
<author confidence="0.535293">Montse Maritxalar</author>
<author confidence="0.535293">German Rigau</author>
<author confidence="0.535293">Larraitz</author>
<affiliation confidence="0.998363">University of the Basque</affiliation>
<address confidence="0.997815">Donostia, 20018, Basque</address>
<email confidence="0.941822">aitor.gonzalez-agirre,german.rigau,</email>
<abstract confidence="0.996972">In Semantic Textual Similarity, systems rate the degree of semantic equivalence on a graded scale from 0 to 5, with 5 being the most similar. For the English subtask, we present a system which relies on several resources for token-to-token and phrase-to-phrase similarity to build a data-structure which holds all the information, and then combine the information to get a similarity score. We also participated in the pilot on Interpretable STS, where we apply a pipeline which first aligns tokens, then chunks, and finally uses supervised systems to label and score each chunk alignment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rodrigo Agerri</author>
<author>Josu Bermudez</author>
<author>German Rigau</author>
</authors>
<title>Ixa pipeline: Efficient and ready to use multilingual nlp tools.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th Language Resources and Evaluation Conference (LREC2014),</booktitle>
<pages>26--31</pages>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="8835" citStr="Agerri et al., 2014" startWordPosition="1427" endWordPosition="1430">tag and the similarity score. 3.1.1 Input Handling and Chunking Module We use the Stanford NLP parser (Klein and Manning, 2003) to linguistically process input sentences and register lowercased token information (lemma, part of speech analysis and dependency structure is also needed for the following module). The next step consists of determining segments or token regions. This information is gathered according to the specified scenario (GS or SYS). In the case of the GS scenario the baseline obviously uses gold standard input; and, in the SYS scenario the baseline uses the ixa-pipes-chunker (Agerri et al., 2014). Ixa-pipes-chunk has been trained using the Apache OpenNLP API (OpenNLP, 2011), which is a maximum entropy chunker. Nevertheless, the chunker’s output has been improved using simple regular expressions to fit to our task proposal. Actually, we developed four rules to optimize how conjunctions, punctuations and prepositions are handled. In brief, the developed rules try to join consequent chunks forming new chunks consisting of the previous ones, for instance, we found significant improvement if prepositional phrases followed by a nominal phrase were unified as a single chunk. We also develope</context>
</contexts>
<marker>Agerri, Bermudez, Rigau, 2014</marker>
<rawString>Rodrigo Agerri, Josu Bermudez, and German Rigau. 2014. Ixa pipeline: Efficient and ready to use multilingual nlp tools. In Proceedings of the 9th Language Resources and Evaluation Conference (LREC2014), Reykjavik, Iceland, May, pages 26–31.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirre, Weiwei Guo, I˜nigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe.</title>
<date>2015</date>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, CO,</location>
<contexts>
<context position="1209" citStr="Agirre et al., 2015" startWordPosition="177" endWordPosition="180">al resources for token-to-token and phrase-to-phrase similarity to build a data-structure which holds all the information, and then combine the information to get a similarity score. We also participated in the pilot on Interpretable STS, where we apply a pipeline which first aligns tokens, then chunks, and finally uses supervised systems to label and score each chunk alignment. 1 Introduction In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence on a graded scale from 0 to 5, with 5 being the most similar. We participated in two of the subtask for STS in 2015 (Agirre et al., 2015). For the English subtask, we present a system which relies on several resources for tokento-token and phrase-to-phrase similarity to build a data-structure which holds all the information, and then combine the information to get a similarity score. We also participated in the pilot on Interpretable STS, where we apply a pipeline which first aligns tokens, then chunks, and finally uses supervised systems to label and score each chunk alignment. Note that some of the authors participated in the organization of the task. We scrupulously separated the tasks in such a way that the developers of th</context>
<context position="5749" citStr="Agirre et al., 2015" startWordPosition="941" endWordPosition="944">I 2.3 Results Due to time constraints we submitted a single run, which ranked 54 among 74 runs. We expect to improve this results adding more layers and combining them using more sophisticated aggregation methods. 3 Participating on the Interpretable STS Pilot Subtask The SemEval 2015 STS task offered a new pilot subtask on interpretable STS1. Given a sentence pair, 1http://alt.qcri.org/semeval2015/task2/ index.php?id=proba the objective of the subtask is to align segments pertaining to one sentence with the segments pertaining to the other sentence. The whole subtask is in deep described in (Agirre et al., 2015). In sum, every alignment may consist of a similarity score and a relatedness tag. The similarity score is a real number bounded by [0,5] where 0 means no relation at all and 5 means complete equivalence. As regards the relatedness tag, there exists a set of categorical values to choose on, such as: equivalence, opposition, specialization (direction is relevant), similarity and one more tag for other kind of relatedness. For the case of unaligned segments there are another two possible categorical values. The one for declaring segments unaligned (not aligned); and the other to declare that the</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, I˜nigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, CO, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: a library for support vector machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="12235" citStr="Chang and Lin, 2011" startWordPosition="1982" endWordPosition="1985">culate a total of 21 features to improve the tag assignment. The objective of the induced model is to refine the output given by the naive approach only for segment pairs tagged as equivalent. The features used to induce the model can be classified in the following groups: Jaccard overlap related features, segment length related features, WordNet similarity related features among segment heads, WordNet depth related features, and other kind of features obtained by means of the cube described in section 2. To induce the model we use the Support Vector Machine (SVM) implementation described in (Chang and Lin, 2011) under the latest experimental version of Weka (Hall et al., 2009) using randomly shuffled 5-fold cross validation. We indistinctly join 180 the available datasets and grid search to optimize the cost and gamma parameters. 3.1.4 Scoring Module To assign segment pair similarity scores the system can also use two distinct approaches: the naive approach and the cube based regression approach. The naive scorer directly assigns a certain score to each one of the tags, which has been previously assigned using the naive tagger: for equivalence tags it assigns a score of 5 and for not aligned and cont</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BM Clapper</author>
</authors>
<title>munkres: a python module implementing the “hungarian method” described by munkres</title>
<date>2009</date>
<pages>1--0</pages>
<contexts>
<context position="6750" citStr="Clapper, 2009" startWordPosition="1108" endWordPosition="1109">re tag for other kind of relatedness. For the case of unaligned segments there are another two possible categorical values. The one for declaring segments unaligned (not aligned); and the other to declare that the segment related to the current segment has already been aligned (context alignment). Notice that due to the limitations of the current pilot the only way to align segments is making 1:1 alignments. Thus, 1:N alignments are simulated making an 1:1 alignment and several context alignments. This concept is relevant to the work done in section 3.1.2 when we extend the Hungarian-Munkres (Clapper, 2009) algorithm to identify already aligned chunks. In addition, factuality or polarity connotations can be added as requested to the previously mentioned tags. Two different scenarios are provided in the pilot subtask, the first one makes gold standard segments available for participants (Gold Chunks or GS scenario); and, the second one, only provides sentence raw text (System Chunks or SYS scenario). In conclusion, the first pilot on interpretable STS seems challenging because participating systems must not only discover and score the relatedness between segments, but also identify the inner rela</context>
</contexts>
<marker>Clapper, 2009</marker>
<rawString>BM Clapper. 2009. munkres: a python module implementing the “hungarian method” described by munkres (1957). version 1.0. 5.3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning, ICML ’08,</booktitle>
<pages>160--167</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="3008" citStr="Collobert and Weston, 2008" startWordPosition="482" endWordPosition="485">tences using the Stanford Parser (Toutanova et al., 2003). After parsing the sentences each pair of sentences can be represented by a NxM matrix, being N is the number of nodes of the parse tree of the first sentence, and M the number of nodes of the parse tree of the second sentence. Note that some nodes (terminals) correspond to words, while others (nonterminals) represent phrases. We can have as many matrices as we wish, and fill them with different similarity scores, forming a cube. In this first attempt we used three layers: 1. Euclidean distance between Collobert and Weston Word Vector (Collobert and Weston, 2008). The vector representations for each non-terminal node in the tree were learnt using Recursive Autoencoder (RAE) (Socher et al., 2011). 2. Euclidean distance between Mikolov Word Vectors (Mikolov et al., 2013a; Mikolov et al., 2013b). To compute the vector representations for each non-terminal node in the tree, we summed the vectors and normalize them dividing by the number of words in the phrase. 3. PPDB Paraphrase database values (Ganitkevitch et al., 2013). We used the XXXL version. In this case both words and some phrases 178 Proceedings of the 9th International Workshop on Semantic Evalu</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages 160–167, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorenzo Ferrone</author>
<author>Massimo Fabio Zanzotto</author>
</authors>
<title>Towards syntax-aware compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>721--730</pages>
<contexts>
<context position="18514" citStr="Ferrone and Zanzotto, 2014" startWordPosition="3039" endWordPosition="3043">e SYS scenario. Regarding other participants, we can conclude that our runs obtain quite good results, specially for the images dataset where run2 obtains the highest score. 4 Conclusions and Future Work Through this paper we have described the systems that participated in the Semantic Textual Similarity task 2A (English STS) and 2C (Interpretable STS). Our main focus in the English subtask was on deploying our idea of building a cube with similarity information from several sources. We are currently working on more layers, including Random Walks over WordNet and Wikipedia, string similarity (Ferrone and Zanzotto, 2014), and also a special layer to deal with numbers. Additionally, we are considering the idea of dissimilarity layers, for instance, adding information about negation and antonymy. We are also developing new methods to combine these knowledge to generate the final STS score. Regarding the interpretable STS system, this was the first time a pilot was put in place. We obtained excellent results, even if we had very little time to develop the system. Future work will focus on further improvements. For instance, our experiments showed that grouping chunks lead to a considerable improvement for the F1</context>
</contexts>
<marker>Ferrone, Zanzotto, 2014</marker>
<rawString>Lorenzo Ferrone and Massimo Fabio Zanzotto. 2014. Towards syntax-aware compositional distributional semantic models. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 721–730.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>758--764</pages>
<location>Atlanta, Georgia,</location>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proceedings of NAACL-HLT, pages 758– 764, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="12301" citStr="Hall et al., 2009" startWordPosition="1993" endWordPosition="1996">ective of the induced model is to refine the output given by the naive approach only for segment pairs tagged as equivalent. The features used to induce the model can be classified in the following groups: Jaccard overlap related features, segment length related features, WordNet similarity related features among segment heads, WordNet depth related features, and other kind of features obtained by means of the cube described in section 2. To induce the model we use the Support Vector Machine (SVM) implementation described in (Chang and Lin, 2011) under the latest experimental version of Weka (Hall et al., 2009) using randomly shuffled 5-fold cross validation. We indistinctly join 180 the available datasets and grid search to optimize the cost and gamma parameters. 3.1.4 Scoring Module To assign segment pair similarity scores the system can also use two distinct approaches: the naive approach and the cube based regression approach. The naive scorer directly assigns a certain score to each one of the tags, which has been previously assigned using the naive tagger: for equivalence tags it assigns a score of 5 and for not aligned and context aligned tags it assigns ’NIL’. (as requested by the guidelines</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The weka data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8342" citStr="Klein and Manning, 2003" startWordPosition="1350" endWordPosition="1353">tence pairs, and then, make alignments between them. First of all, the input handling and chunking module is responsible sim(S1, S2) = + 179 for linguistically processing the given input, and for creating the internal representation of the sentences. Once the input is processed the alignment module identifies related and unrelated segments among sentences. Finally, by using segment pair based features the classification module and the scoring module produce respectively the final relatedness tag and the similarity score. 3.1.1 Input Handling and Chunking Module We use the Stanford NLP parser (Klein and Manning, 2003) to linguistically process input sentences and register lowercased token information (lemma, part of speech analysis and dependency structure is also needed for the following module). The next step consists of determining segments or token regions. This information is gathered according to the specified scenario (GS or SYS). In the case of the GS scenario the baseline obviously uses gold standard input; and, in the SYS scenario the baseline uses the ixa-pipes-chunker (Agerri et al., 2014). Ixa-pipes-chunk has been trained using the Apache OpenNLP API (OpenNLP, 2011), which is a maximum entropy</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the American Association forArtificial Intelligence (AAAI</booktitle>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="5027" citStr="Mihalcea et al., 2006" startWordPosition="824" endWordPosition="828">n across all the layers. We do that because we studied the different resources and we think that these resources have less False Positives (FP) than False Negatives (FN). In other words, if one of the resources says that something is very similar we trust on it and take that similarity score instead of the other (even if they are very low). Moreover, our assumption is that the final similarity score is specially based in similarities between tokens/phrases in the sentences, and not on dissimilarities. Once we have this matrix, we compute the final STS score using the scoring function seen in (Mihalcea et al., 2006). 1 (r,w∈S1 (maxSim(w∈S2) * idf (w)) 2 1\ Pw∈S1 idf(w) Pw∈S2 (maxSim(w∈S1) * idf (w))1 Pw∈S2 idf (w) /I 2.3 Results Due to time constraints we submitted a single run, which ranked 54 among 74 runs. We expect to improve this results adding more layers and combining them using more sophisticated aggregation methods. 3 Participating on the Interpretable STS Pilot Subtask The SemEval 2015 STS task offered a new pilot subtask on interpretable STS1. Given a sentence pair, 1http://alt.qcri.org/semeval2015/task2/ index.php?id=proba the objective of the subtask is to align segments pertaining to one se</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the American Association forArtificial Intelligence (AAAI 2006), Boston, Massachusetts, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="3217" citStr="Mikolov et al., 2013" startWordPosition="514" endWordPosition="517">, and M the number of nodes of the parse tree of the second sentence. Note that some nodes (terminals) correspond to words, while others (nonterminals) represent phrases. We can have as many matrices as we wish, and fill them with different similarity scores, forming a cube. In this first attempt we used three layers: 1. Euclidean distance between Collobert and Weston Word Vector (Collobert and Weston, 2008). The vector representations for each non-terminal node in the tree were learnt using Recursive Autoencoder (RAE) (Socher et al., 2011). 2. Euclidean distance between Mikolov Word Vectors (Mikolov et al., 2013a; Mikolov et al., 2013b). To compute the vector representations for each non-terminal node in the tree, we summed the vectors and normalize them dividing by the number of words in the phrase. 3. PPDB Paraphrase database values (Ganitkevitch et al., 2013). We used the XXXL version. In this case both words and some phrases 178 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 178–183, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics are contained in the resource. This resource yields conditional probabilities. As our sco</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<location>CoRR, abs/1310.4546.</location>
<contexts>
<context position="3217" citStr="Mikolov et al., 2013" startWordPosition="514" endWordPosition="517">, and M the number of nodes of the parse tree of the second sentence. Note that some nodes (terminals) correspond to words, while others (nonterminals) represent phrases. We can have as many matrices as we wish, and fill them with different similarity scores, forming a cube. In this first attempt we used three layers: 1. Euclidean distance between Collobert and Weston Word Vector (Collobert and Weston, 2008). The vector representations for each non-terminal node in the tree were learnt using Recursive Autoencoder (RAE) (Socher et al., 2011). 2. Euclidean distance between Mikolov Word Vectors (Mikolov et al., 2013a; Mikolov et al., 2013b). To compute the vector representations for each non-terminal node in the tree, we summed the vectors and normalize them dividing by the number of words in the phrase. 3. PPDB Paraphrase database values (Ganitkevitch et al., 2013). We used the XXXL version. In this case both words and some phrases 178 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 178–183, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics are contained in the resource. This resource yields conditional probabilities. As our sco</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. CoRR, abs/1310.4546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apache OpenNLP</author>
</authors>
<title>Apache software foundation.</title>
<date>2011</date>
<note>URL http://opennlp. apache. org.</note>
<contexts>
<context position="8914" citStr="OpenNLP, 2011" startWordPosition="1440" endWordPosition="1441">nford NLP parser (Klein and Manning, 2003) to linguistically process input sentences and register lowercased token information (lemma, part of speech analysis and dependency structure is also needed for the following module). The next step consists of determining segments or token regions. This information is gathered according to the specified scenario (GS or SYS). In the case of the GS scenario the baseline obviously uses gold standard input; and, in the SYS scenario the baseline uses the ixa-pipes-chunker (Agerri et al., 2014). Ixa-pipes-chunk has been trained using the Apache OpenNLP API (OpenNLP, 2011), which is a maximum entropy chunker. Nevertheless, the chunker’s output has been improved using simple regular expressions to fit to our task proposal. Actually, we developed four rules to optimize how conjunctions, punctuations and prepositions are handled. In brief, the developed rules try to join consequent chunks forming new chunks consisting of the previous ones, for instance, we found significant improvement if prepositional phrases followed by a nominal phrase were unified as a single chunk. We also developed some rules to unify nominal phrases separated by punctuations or conjunctions</context>
</contexts>
<marker>OpenNLP, 2011</marker>
<rawString>Apache OpenNLP. 2011. Apache software foundation. URL http://opennlp. apache. org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems 24.</booktitle>
<contexts>
<context position="3143" citStr="Socher et al., 2011" startWordPosition="503" endWordPosition="506">ix, being N is the number of nodes of the parse tree of the first sentence, and M the number of nodes of the parse tree of the second sentence. Note that some nodes (terminals) correspond to words, while others (nonterminals) represent phrases. We can have as many matrices as we wish, and fill them with different similarity scores, forming a cube. In this first attempt we used three layers: 1. Euclidean distance between Collobert and Weston Word Vector (Collobert and Weston, 2008). The vector representations for each non-terminal node in the tree were learnt using Recursive Autoencoder (RAE) (Socher et al., 2011). 2. Euclidean distance between Mikolov Word Vectors (Mikolov et al., 2013a; Mikolov et al., 2013b). To compute the vector representations for each non-terminal node in the tree, we summed the vectors and normalize them dividing by the number of words in the phrase. 3. PPDB Paraphrase database values (Ganitkevitch et al., 2013). We used the XXXL version. In this case both words and some phrases 178 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 178–183, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics are contained i</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Md Arafat Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<title>Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence. Transactions of the Association for Computational Linguistics,</title>
<date>2014</date>
<pages>219--230</pages>
<contexts>
<context position="9685" citStr="Sultan et al., 2014" startWordPosition="1557" endWordPosition="1560">. Actually, we developed four rules to optimize how conjunctions, punctuations and prepositions are handled. In brief, the developed rules try to join consequent chunks forming new chunks consisting of the previous ones, for instance, we found significant improvement if prepositional phrases followed by a nominal phrase were unified as a single chunk. We also developed some rules to unify nominal phrases separated by punctuations or conjunctions, or a combination of those. 3.1.2 Alignment Module The alignment module mainly focuses on the work done by the monolingual word aligner described in (Sultan et al., 2014), and HungarianMunkres algorithm. The monolingual word aligner is a simple and ready-to-use system that has demonstrated state-ofthe-art performance. To begin with we start by constructing the token to token link matrix in which each element at position (i,j) determines that there exists a link between token i (from sentence 1) and token j (from sentence 2). A link exists in the matrix if and only if the monolingual word aligner has determined that both tokens are related. Then, the system uses token regions to group individual tokens into segments, and calculates the weight between every segm</context>
</contexts>
<marker>Sultan, Bethard, Sumner, 2014</marker>
<rawString>Md Arafat Sultan, Steven Bethard, and Tamara Sumner. 2014. Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence. Transactions of the Association for Computational Linguistics, pages 219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="2438" citStr="Toutanova et al., 2003" startWordPosition="381" endWordPosition="384">ems did not have access to the test sets, and that they only had access to the same training data as the rest of the participants. 2 Cubes for English STS In this section we describe a novel approach to compute similarity scores between two sentences using a cube where each layer contains token-to-token and phrase-to-phrase similarity scores from a different method and/or resource. Our assumption is that we can obtain better results using this similarity scores together than independently. 2.1 Building Cubes The first step is to produce parse trees for the sentences using the Stanford Parser (Toutanova et al., 2003). After parsing the sentences each pair of sentences can be represented by a NxM matrix, being N is the number of nodes of the parse tree of the first sentence, and M the number of nodes of the parse tree of the second sentence. Note that some nodes (terminals) correspond to words, while others (nonterminals) represent phrases. We can have as many matrices as we wish, and fill them with different similarity scores, forming a cube. In this first attempt we used three layers: 1. Euclidean distance between Collobert and Weston Word Vector (Collobert and Weston, 2008). The vector representations f</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, pages 252–259.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>