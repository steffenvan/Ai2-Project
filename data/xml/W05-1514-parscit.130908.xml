<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003305">
<title confidence="0.977939">
Chunk Parsing Revisited
</title>
<author confidence="0.988389">
Yoshimasa Tsuruoka and Jun’ichi Tsujii
</author>
<affiliation confidence="0.997474">
CREST, JST (Japan Science and Technology Corporation)
Department of Computer Science, University of Tokyo
School of Informatics, University of Manchester
</affiliation>
<email confidence="0.996462">
tsuruoka,tsujii @is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.995609" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999859277777778">
Chunk parsing is conceptually appealing
but its performance has not been satis-
factory for practical use. In this pa-
per we show that chunk parsing can
perform significantly better than previ-
ously reported by using a simple sliding-
window method and maximum entropy
classifiers for phrase recognition in each
level of chunking. Experimental results
with the Penn Treebank corpus show that
our chunk parser can give high-precision
parsing outputs with very high speed (14
msec/sentence). We also present a pars-
ing method for searching the best parse by
considering the probabilities output by the
maximum entropy classifiers, and show
that the search method can further im-
prove the parsing accuracy.
</bodyText>
<sectionHeader confidence="0.999122" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999564844444445">
Chunk parsing (Tjong Kim Sang, 2001; Brants,
1999) is a simple parsing strategy both in imple-
mentation and concept. The parser first performs
chunking by identifying base phrases, and convert
the identified phrases to non-terminal symbols. The
parser again performs chunking on the updated se-
quence and convert the newly recognized phrases
into non-terminal symbols. The parser repeats this
procedure until there are no phrases to be chunked.
After finishing these chunking processes, we can
reconstruct the complete parse tree of the sentence
from the chunking results.
Although the conceptual simplicity of chunk pars-
ing is appealing, satisfactory performance for prac-
tical use has not yet been achieved with this pars-
ing strategy. Sang achieved an f-score of 80.49 on
the Penn Treebank by using the IOB tagging method
for each level of chunking (Tjong Kim Sang, 2001).
However, there is a very large gap between their per-
formance and that of widely-used practical parsers
(Charniak, 2000; Collins, 1999).
The performance of chunk parsing is heavily de-
pendent on the performance of phrase recognition in
each level of chunking. We show in this paper that
the chunk parsing strategy is indeed appealing in that
it can give considerably better performance than pre-
viously reported by using a different approach for
phrase recognition and that it enables us to build a
very fast parser that gives high-precision outputs.
This advantage could open up the possibility of
using full parsers for large-scale information extrac-
tion from the Web corpus and real-time information
extraction where the system needs to analyze the
documents provided by the users on run-time.
This paper is organized as follows. Section 2
introduces the overall chunk parsing strategy em-
ployed in this work. Section 3 describes the sliding-
window based method for identifying chunks. Two
filtering methods to reduce the computational cost
are presented in sections 4 and 5. Section 6 explains
the maximum entropy classifier and the feature set.
Section 7 describes methods for searching the best
parse. Experimental results on the Penn Treebank
corpus are given in Section 8. Section 10 offers
some concluding remarks.
</bodyText>
<page confidence="0.987823">
133
</page>
<note confidence="0.911374">
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 133–140,
Vancouver, October 2005. c�2005 Association for Computational Linguistics
</note>
<figure confidence="0.895682166666667">
QP
VBN NN VBD DT JJ CD CD NNS .
NP
VP
NP VBD NP .
Estimated volume was a light 2.4 million ounces .
</figure>
<figureCaption confidence="0.997293">
Figure 1: Chunk parsing, the 1st iteration.
</figureCaption>
<figure confidence="0.936744333333333">
NP
NP VBD DT JJ QP NNS .
volume was a light million ounces .
</figure>
<figureCaption confidence="0.98939">
Figure 2: Chunk parsing, the 2nd iteration.
</figureCaption>
<bodyText confidence="0.48979">
volume was ounces .
</bodyText>
<figureCaption confidence="0.996163">
Figure 3: Chunk parsing, the 3rd iteration.
</figureCaption>
<figure confidence="0.997186666666667">
S
NP VP .
volume was .
</figure>
<figureCaption confidence="0.999823">
Figure 4: Chunk parsing, the 4th iteration.
</figureCaption>
<sectionHeader confidence="0.950154" genericHeader="introduction">
2 Chunk Parsing
</sectionHeader>
<bodyText confidence="0.999975954545454">
For the overall strategy of chunk parsing, we fol-
low the method proposed by Sang (Tjong Kim Sang,
2001). Figures 1 to 4 show an example of chunk
parsing. In the first iteration, the chunker identifies
two base phrases, (NP Estimated volume) and (QP
2.4 million), and replaces each phrase with its non-
terminal symbol and head. The head word is identi-
fied by using the head-percolation table (Magerman,
1995). In the second iteration, the chunker identifies
(NP a light million ounces) and converts this phrase
into NP. This chunking procedure is repeated until
the whole sentence is chunked at the fourth itera-
tion, and the full parse tree is easily recovered from
the chunking history.
This parsing strategy converts the problem of full
parsing into smaller and simpler problems, namely,
chunking, where we only need to recognize flat
structures (base phrases). Sang used the IOB tag-
ging method proposed by Ramshow(Ramshaw and
Marcus, 1995) and memory-based learning for each
level of chunking and achieved an f-score of 80.49
on the Penn Treebank corpus.
</bodyText>
<sectionHeader confidence="0.8523325" genericHeader="method">
3 Chunking with a sliding-window
approach
</sectionHeader>
<bodyText confidence="0.957731">
The performance of chunk parsing heavily depends
on the performance of each level of chunking. The
popular approach to this shallow parsing is to con-
vert the problem into a tagging task and use a variety
of machine learning techniques that have been de-
veloped for sequence labeling problems such as Hid-
den Markov Models, sequential classification with
SVMs (Kudo and Matsumoto, 2001), and Condi-
tional Random Fields (Sha and Pereira, 2003).
One of our claims in this paper is that we should
not convert the chunking problem into a tagging
task. Instead, we use a classical sliding-window
method for chunking, where we consider all sub-
sequences as phrase candidates and classify them
with a machine learning algorithm. Suppose, for ex-
ample, we are about to perform chunking on the se-
quence in Figure 4.
NP-volume VBD-was .-.
We consider the following sub sequences as the
phrase candidates in this level of chunking.
</bodyText>
<listItem confidence="0.999471333333333">
1. (NP-volume) VBD-was .-.
2. NP-volume (VBD-was) .-.
3. NP-volume VBD-was (.-.)
4. (NP-volume VBD-was) .-.
5. NP-volume (VBD-was .-.)
6. (NP-volume VBD-was .-.)
</listItem>
<bodyText confidence="0.991363333333333">
The merit of taking the sliding window approach
is that we can make use of a richer set of features on
recognizing a phrase than in the sequential labeling
</bodyText>
<page confidence="0.997912">
134
</page>
<bodyText confidence="0.999961785714286">
approach. We can define arbitrary features on the
target candidate (e.g. the whole sequence of non-
terminal symbols of the target) and the surrounding
context, which are, in general, not available in se-
quential labeling approaches.
We should mention here that there are some other
modeling methods for sequence labeling which al-
low us to define arbitrary features on the target
phrase. Semi-markov conditional random fields
(Semi-CRFs) are one of such modeling methods
(Sarawagi and Cohen, 2004). Semi-CRFs could
give better performance than the sliding-window
approach because they can incorporate features on
other phrase candidates on the same level of chunk-
ing. However, they require additional computational
resources for training and parsing, and the use of
Semi-CRFs is left for future work.
The biggest disadvantage of the sliding window
approach is the cost for training and parsing. Since
there are phrase candidates when the
length of the sequence is , a naive application of
machine learning easily leads to prohibitive con-
sumption of memory and time.
In order to reduce the number of phrase candi-
dates to be considered by machine learning, we in-
troduce two filtering phases into training and pars-
ing. One is done by a rule dictionary. The other is
done by a naive Bayes classifier.
</bodyText>
<sectionHeader confidence="0.550099" genericHeader="method">
4 Filtering with the CFG Rule Dictionary
</sectionHeader>
<bodyText confidence="0.999984416666667">
We use an idea that is similar to the method pro-
posed by Ratnaparkhi (Ratnaparkhi, 1996) for part-
of-speech tagging. They used a Tag Dictionary, with
which the tagger considers only the tag-word pairs
that appear in the training sentences as the candidate
tags.
A similar method can be used for reducing the
number of phrase candidates. We first construct a
rule dictionary consisting of all the CFG rules used
in the training data. In both training and parsing, we
filter out all the sub-sequences that do not match any
of the entry in the dictionary.
</bodyText>
<subsectionHeader confidence="0.978048">
4.1 Normalization
</subsectionHeader>
<bodyText confidence="0.994996666666667">
The rules used in the training data do not cover all
the rules in unseen sentences. Therefore, if we take
a naive filtering method using the rule dictionary, we
</bodyText>
<table confidence="0.987254285714286">
Original Symbol Normalized Symbol
NNP, NNS, NNPS, PRP NN
RBR, RBS RB
JJR, JJS, PRP$ JJ
VBD,VBZ VBP
: ,
”, NULL
</table>
<figure confidence="0.9871045">
0 10000 20000 30000 40000
Number of Sentences
</figure>
<figureCaption confidence="0.969327">
Figure 5: Number of sentences vs the size of the rule
dictionary..
</figureCaption>
<bodyText confidence="0.971012210526316">
substantially lose recall in parsing unseen data.
To alleviate the problem of the coverage of rules,
we conduct normalization of the rules. We first con-
vert preterminal symbols into equivalent sets using
the conversion table provided in Table 1. This con-
version reduces the sparseness of the rules.
We further normalize the Right-Hand-Side (RHS)
of the rules with the following heuristics.
“X CC X” is converted to “X”.
“X , X” is converted to “X”.
Figure 5 shows the effectiveness of this normal-
ization method. The figure illustrates how the num-
ber of rules increases in the rule dictionary as we
add training sentences. Without the normalization,
the number of rules continues to grow rapidly even
when the entire training set is read. The normaliza-
tion methods reduce the growing rate, which con-
siderably alleviates the sparseness problem (i.e. the
problems of unknown rules).
</bodyText>
<tableCaption confidence="0.989771">
Table 1: Normalizing preterminals.
</tableCaption>
<figure confidence="0.9958305">
Size of Rule Dictionary
16000
14000
12000
10000
4000
8000
6000
2000
0
Original
Normalized
</figure>
<page confidence="0.954526">
135
</page>
<bodyText confidence="0.984280805555556">
5 Filtering with the Naive Bayes classifier
Although the use of the rule dictionary significantly
reduced the number of phrase candidates, we still
found it difficult to train the parser using the entire
training set when we used a rich set of features.
To further reduce the cost required for training
and parsing, we propose to use a naive Bayes classi-
fier for filtering the candidates. A naive Bayes clas-
sifier is simple and requires little storage and com-
putational cost.
We construct a binary naive Bayes classifier for
each phrase type using the entire training data. We
considered the following information as the features.
The Right-Hand-Side (RHS) of the CFG rule
The left-adjacent nonterminal symbol.
The right-adjacent nonterminal symbol.
By assuming the conditional independence
among the features, we can compute the probability
for filtering as follows:
where is a binary output indicating whether the
candidate is a phrase of the target type or not, is
the RHS of the CFG rule, is the symbol on the
left, and is the symbol on the right. We used
the Laplace smoothing method for computing each
probability. Note that the information about the re-
sult of the rule application, i.e., the LHS symbol, is
considered in this filtering scheme because different
naive Bayes classifiers are used for different LHS
symbols (phrase types).
Table 2 shows the filtering performance in train-
ing with sections 02-21 on the Penn Treebank. We
set the threshold probability for filtering to be 0.0001
for the experiments reported in this paper. The
naive Bayes classifiers effectively reduced the num-
ber of candidates with little positive samples that
were wrongly filtered out.
</bodyText>
<sectionHeader confidence="0.938519" genericHeader="method">
6 Phrase Recognition with a Maximum
Entropy Classifier
</sectionHeader>
<bodyText confidence="0.999963">
For the candidates which are not filtered out in the
above two phases, we perform classification with
maximum entropy classifiers (Berger et al., 1996).
We construct a binary classifier for each type of
phrases using the entire training set. The training
samples for maximum entropy consist of the phrase
candidates that have not been filtered out by the CFG
rule dictionary and the naive Bayes classifier.
One of the merits of using a maximum entropy
classifier is that we can obtain a probability from
the classifier in each decision. The probability of
each decision represents how likely the candidate is
a correct chunk. We accept a chunk only when the
probability is larger than the predefined threshold.
With this thresholding scheme, we can control the
trade-off between precision and recall by changing
the threshold value.
Regularization is important in maximum entropy
modeling to avoid overfitting to the training data.
For this purpose, we use the maximum entropy mod-
eling with inequality constraints (Kazama and Tsu-
jii, 2003). This modeling has one parameter to
tune as in Gaussian prior modeling. The parame-
ter is called the width factor. We set this parame-
ter to be 1.0 throughout the experiments. For nu-
merical optimization, we used the Limited-Memory
Variable-Metric (LMVM) algorithm (Benson and
Mor´e, 2001).
</bodyText>
<subsectionHeader confidence="0.832975">
6.1 Features
</subsectionHeader>
<bodyText confidence="0.999941444444444">
Table 3 lists the features used in phrase recognition
with the maximum entropy classifier. Information
about the adjacent non-terminal symbols is impor-
tant. We use unigrams, bigrams, and trigrams of the
adjacent symbols. Head information is also useful.
We use unigrams and bigrams of the neighboring
heads. The RHS of the CFG rule is also informa-
tive. We use the features on RHSs combined with
symbol features.
</bodyText>
<sectionHeader confidence="0.923253" genericHeader="method">
7 Searching the best parse
</sectionHeader>
<subsectionHeader confidence="0.985978">
7.1 Deterministic parsing
</subsectionHeader>
<bodyText confidence="0.998674">
The deterministic version of chunk parsing is
straight-forward. All we need to do is to repeat
chunking until there are no phrases to be chunked.
</bodyText>
<page confidence="0.987252">
136
</page>
<table confidence="0.999617714285714">
Symbol # candidates # remaining candidates # positives # false negative
ADJP 4,043,409 1,052,983 14,389 53
ADVP 3,459,616 1,159,351 19,765 78
NP 7,122,168 3,935,563 313,042 117
PP 3,889,302 1,181,250 94,568 126
S 3,184,827 1,627,243 95,305 99
VP 4,903,020 2,013,229 145,878 144
</table>
<tableCaption confidence="0.980887">
Table 2: Effectiveness of the naive Bayes filtering on some representative nonterminals.
</tableCaption>
<table confidence="0.999829777777778">
Symbol Unigrams ,
Symbol Bigrams , ,,
Symbol Trigrams , ,,
Head Unigrams ,
Head Bigrams ,,
Symbol-Head Unigrams ,,
CFG Rule
CFG Rule + Symbol Unigram ,
CFG Rule + Symbol Bigram
</table>
<tableCaption confidence="0.996127">
Table 3: Feature templates used in chunking. and represent the non-terminal symbols at the beginning
</tableCaption>
<bodyText confidence="0.994735833333333">
and the ending of the target phrase respectively. and represent the head at the beginning and the ending
of the target phrase respectively. RHS represents the Right-Hand-Side of the CFG rule.
If the maximum entropy classifiers give contra-
dictory chunks in each level of chunking, we choose
the chunk which has a larger probability than the
other ones.
</bodyText>
<subsectionHeader confidence="0.999886">
7.2 Parsing with search
</subsectionHeader>
<bodyText confidence="0.999993888888889">
We tried to perform searching in chunk parsing in
order to investigate whether or not extra effort of
searching gives a gain in parsing performance.
The problem is that because the modeling of our
chunk parsing provides no explicit probabilistic dis-
tribution over the entire parse tree, there is no deci-
sive way to properly evaluate the correctness of each
parse. Nevertheless, we can consider the following
score on each parse tree.
</bodyText>
<equation confidence="0.930601">
(1)
</equation>
<bodyText confidence="0.999939388888889">
where is the probability of a phrase given by the
maximum entropy classifier.
Because exploring all the possibilities of chunk-
ing requires prohibitive computational cost, we re-
duce the search space by focusing only on “uncer-
tain” chunk candidates for the search. In each level
of chunking, the chunker provides chunks with their
probabilities. We consider only the chunks whose
probabilities are within the predefined margin from
. In other words, the chunks whose probabilities
are larger than are considered as
assured chunks, and thus are fixed when we gener-
ate alternative hypotheses of chunking. The chunks
whose probabilities are smaller than
are simply ignored.
We generate alternative hypotheses in each level
of chunking, and search the best parse in a depth-
first manner.
</bodyText>
<subsectionHeader confidence="0.991937">
7.3 Iterative parsing
</subsectionHeader>
<bodyText confidence="0.99988725">
We also tried an iterative parsing strategy, which
was successfully used in probabilistic HPSG pars-
ing (Ninomiya et al., 2005). The parsing strategy is
simple. The parser starts with a very low margin and
tries to find a successful parse. If the parser cannot
find a successful parse, then it increases the margin
by a certain step and tries to parse with the wider
margin.
</bodyText>
<page confidence="0.998215">
137
</page>
<sectionHeader confidence="0.997827" genericHeader="evaluation">
8 Experiments
</sectionHeader>
<bodyText confidence="0.999874692307692">
We ran parsing experiments using the Penn Tree-
bank corpus, which is widely used for evaluating
parsing algorithms. The training set consists of sec-
tions 02-21. We used section 22 as the development
data, with which we tuned the feature set and param-
eters for parsing. The test set consists of section 23
and we report the performance of the parser on the
set.
We used the evalb script provided by Sekine and
Collins for evaluating the labeled recall/precision
(LR/LP) of the parser outputs 1. All the experiments
were carried out on a server having a 2.6 GHz AMD
Opteron CPU and 16GB memory.
</bodyText>
<subsectionHeader confidence="0.970259">
8.1 Speed and Accuracy
</subsectionHeader>
<bodyText confidence="0.988800935483871">
First, we show the performance that achieved by de-
terministic parsing. Table 4 shows the results. We
parsed all the sentences in section 23 using gold-
standard part-of-speech (POS) tags. The trade-off
between precision and recall can be controlled by
changing the threshold for recognizing chunks. The
fifth row gives the performance achieved with the
default threshold (=0.5), where the precision is over
90% but the recall is low (75%). By lowering the
threshold, we can improve the recall up to around
81% with 2% loss of precision. The best f-score is
85.06.
The parsing speed is very high. The parser takes
only about 34 seconds to parse the entire section.
Since this section contains 2,416 sentences, the av-
erage time required for parsing one sentence is 14
msec. The parsing speed slightly dropped when we
used a lower threshold (0.1).
Table 5 shows the performance achieved when we
used the search algorithm described in Section 7.2.
We limited the maximum number of the nodes in
the search space to 100 because further increase of
the nodes had shown little improvement in parsing
accuracy.
The search algorithm significantly boosted the
precisions and recalls and achieved an f-score of
86.52 when the margin was 0.3. It should be noted
that we obtain no gain when we use a tight margin.
We need to consider phrases having low probabili-
ties in order for the search to work.
&apos;We used the parameter file “COLLINS.prm”
</bodyText>
<table confidence="0.9995903">
Threshold LR LP F-score Time (sec)
0.9 47.61 96.43 63.75 30.6
0.8 58.06 94.29 71.87 32.4
0.7 65.33 92.82 76.69 33.2
0.6 70.89 91.67 79.95 33.2
0.5 75.38 90.71 82.34 34.5
0.4 79.11 89.87 84.15 34.2
0.3 80.95 88.80 84.69 33.9
0.2 82.59 87.69 85.06 33.6
0.1 82.32 85.02 83.65 46.9
</table>
<tableCaption confidence="0.995010333333333">
Table 4: Parsing performance on section 23 (all sen-
tences, gold-standard POS tags) with the determin-
istic algorithm.
</tableCaption>
<table confidence="0.9996515">
Margin LR LP F-score Time (sec)
0.0 75.65 90.81 82.54 41.2
0.1 79.63 90.16 84.57 74.4
0.2 82.70 89.57 86.00 94.8
0.3 84.60 88.53 86.52 110.2
0.4 84.91 86.99 85.94 116.3
</table>
<tableCaption confidence="0.997786">
Table 5: Parsing performance on section 23 (all sen-
</tableCaption>
<bodyText confidence="0.965522944444445">
tences, gold-standard POS tags) with the search al-
gorithm.
One of the advantages of our chunk parser is its
parsing speed. For comparison, we show the trade-
off between parsing time and performance in Collins
parser (Collins, 1999) and our chunk parser in Fig-
ure 6. Collins parser allows the user to change the
size of the beam in parsing. We used Model-2 be-
cause it gave better performance than Model-3 when
the beam size was smaller than 1000. As for the
chunk parser, we controlled the trade-off by chang-
ing the maximum number of nodes in the search.
The uncertainty margin for chunk recognition was
0.3. Figure 6 shows that Collins parser clearly out-
performs our chunk parser when the beam size is
large. However, the performance significantly drops
with a smaller beam size. The break-even point is at
around 200 sec (83 msec/sentence).
</bodyText>
<subsectionHeader confidence="0.999762">
8.2 Comparison with previous work
</subsectionHeader>
<bodyText confidence="0.999637">
Table 6 summarizes our parsing performance on sec-
tion 23 together with the results of previous studies.
In order to make the results directly comparable, we
produced POS tags as the input of our parsers by us-
ing a POS tagger (Tsuruoka and Tsujii, 2005) which
was trained on sections 0-18 in the WSJ corpus.
The table also shows the performance achieved
</bodyText>
<page confidence="0.987558">
138
</page>
<figure confidence="0.9965245">
0 50 100 150 200 250 300 350 400 450 500
Time (sec)
</figure>
<figureCaption confidence="0.984526">
Figure 6: Time vs F-score on section 23. The x-
axis represents the time required to parse the entire
section. The time required for making a hash table
in Collins parser is excluded.
</figureCaption>
<table confidence="0.99992075">
LR LP F-score
Ratnaparkhi (1997) 86.3 87.5 86.9
Collins (1999) 88.1 88.3 88.2
Charniak (2000) 89.6 89.5 89.5
Kudo (2005) 89.3 89.6 89.4
Sang (2001) 78.7 82.3 80.5
Deterministic (tagger-POSs) 81.2 86.5 83.8
Deterministic (gold-POSs) 82.6 87.7 85.1
Search (tagger-POSs) 83.2 87.1 85.1
Search (gold-POSs) 84.6 88.5 86.5
Iterative Search (tagger-POSs) 85.0 86.8 85.9
Iterative Search (gold-POSs) 86.2 88.0 87.1
</table>
<tableCaption confidence="0.9417735">
Table 6: Comparison with other work. Parsing per-
formance on section 23 (all sentences).
</tableCaption>
<bodyText confidence="0.999831307692308">
with the iterative parsing method presented in sec-
tion 7.3. Our chunk parser achieved an f-score of
83.8 with the deterministic parsing methods using
the POS-tagger tags. This f-score is better than that
achieved by the previous study on chunk parsing by
3.3 points (Tjong Kim Sang, 2001). The search al-
gorithms gave an additional 1.3 point improvement.
Finally, the iterative parsing method achieved an f-
score of 85.9.
Although our chunk parser showed considerably
better performance than the previous study on chunk
parsing, the performance is still significantly lower
than those achieved by state-of-the-art parsers.
</bodyText>
<sectionHeader confidence="0.997264" genericHeader="conclusions">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999979086956522">
There is a number of possible improvements in our
chunk parser. We used a rule dictionary to reduce
the cost required for training and parsing. However,
the use of the rule dictionary makes the parser fail
to identify a correct phrase if the phrase is not con-
tained in the rule dictionary. Although we applied
some normalization techniques in order to allevi-
ate this problem, we have not completely solved the
problem. Figure 5 indicates that still we will face
unknown rules even when we have constructed the
rule dictionary using the whole training data (note
that the dotted line does not saturate).
Additional feature sets for the maximum entropy
classifiers could improve the performance. The
bottom-up parsing strategy allows us to use infor-
mation about sub-trees that have already been con-
structed. We thus do not need to restrict ourselves
to use only head-information of the partial parses.
Since many researchers have reported that informa-
tion on partial parse trees plays an important role
for achieving high performance (Bod, 1992; Collins
and Duffy, 2002; Kudo et al., 2005), we expect that
additional features will improve the performance of
chunk parsing.
Also, the methods for searching the best parse
presented in sections 7.2 and 7.3 have much room
for improvement. the search method does not have
the device to avoid repetitive computations on the
same nonterminal sequence in parsing. A chart-like
structure which effectively stores the partial parse
results could enable the parser to explore a broader
search space and produce better parses.
Our chunk parser exhibited a considerable im-
provement in parsing accuracy over the previous
study on chunk parsing. However, the reason is not
completely clear. We believe that the sliding win-
dow approach, which enabled us to exploit a richer
set of features than the so-called IOB approach,
was the main contributer of the better performance.
However, the combination of the IOB approach and
a state-of-the-art machine learning algorithm such
as support vector machines could produce a simi-
lar level of performance. In our preliminary experi-
ments, the IOB tagging method with maximum en-
tropy markov models has not yet achieved a compa-
rable performance to the sliding window method.
</bodyText>
<figure confidence="0.971965">
Chunk parser
Collins parser
F-Score 90
85
80
75
70
65
</figure>
<page confidence="0.990183">
139
</page>
<sectionHeader confidence="0.990951" genericHeader="acknowledgments">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.9997345625">
In this paper we have shown that chunk parsing
can perform significantly better than previously re-
ported by using a simple sliding-window method
and maximum entropy classifiers in each level of
chunking. Experimental results on the Penn Tree-
bank corpus show that our chunk parser can give
high-precision parsing outputs with very high speed
(14 msec/sentence). We also show that searching
can improve the performance and the f-score reaches
85.9.
Although there is still a large gap between the
accuracy of our chunk parser and the state-of-the-
art, our parser can produce better f-scores than a
widely-used parser when the parsing speed is really
needed. This could open up the possibility of using
full-parsing for large-scale information extraction.
</bodyText>
<sectionHeader confidence="0.99886" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998685317460317">
Steven J. Benson and Jorge Mor´e. 2001. A
limited-memory variable-metric algorithm for bound-
constrained minimization. Technical report, Mathe-
matics and Computer Science Division, Argonne Na-
tional Laboratory. ANL/MCS-P909-0901.
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39–71.
Rens Bod. 1992. Data oriented parsing. In Proceedings
of COLING 1992.
Thorsten Brants. 1999. Cascaded markov models. In
Proceedings of EACL 1999.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL 2000, pages 132–
139.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Proceed-
ings ofACL 2002.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proceedings of EMNLP 2003.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
2001.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings ofACL 2005.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL 1995,
pages 276–283.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao,
and Jun’ichi Tsujii. 2005. Efficacy of beam threshold-
ing, unification filtering and hybrid parsing in proba-
bilistic hpsg parsing. In Proceedings ofIWPT 2005.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In Proceed-
ings of the Third Workshop on Very Large Corpora,
pages 82–94.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133–142.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models. In
Proceedings of EMNLP 1997, pages 1–10.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In Proceedings ofICML 2004.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings of
HLT-NAACL 2003.
Erik Tjong Kim Sang. 2001. Transforming a chunker
to a parser. In J. Veenstra W. Daelemans, K. Sima‘an
and J. Zavrel, editors, Computational Linguistics in the
Netherlands 2000, pages 177–188. Rodopi.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirec-
tional inference with the easiest-first strategy for tag-
ging sequence data. In Proceedings of HLT/EMNLP
2005.
</reference>
<page confidence="0.997514">
140
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.713005">
<title confidence="0.996164">Chunk Parsing Revisited</title>
<author confidence="0.979248">Yoshimasa Tsuruoka</author>
<author confidence="0.979248">Jun’ichi Tsujii</author>
<affiliation confidence="0.995151333333333">CREST, JST (Japan Science and Technology Corporation) Department of Computer Science, University of Tokyo School of Informatics, University of Manchester</affiliation>
<email confidence="0.744209">tsuruoka,tsujii@is.s.u-tokyo.ac.jp</email>
<abstract confidence="0.999398631578947">Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use. In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple slidingwindow method and maximum entropy classifiers for phrase recognition in each level of chunking. Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/sentence). We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers, and show that the search method can further improve the parsing accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven J Benson</author>
<author>Jorge Mor´e</author>
</authors>
<title>A limited-memory variable-metric algorithm for boundconstrained minimization.</title>
<date>2001</date>
<tech>Technical report,</tech>
<pages>909--0901</pages>
<institution>Mathematics and Computer Science Division, Argonne National Laboratory.</institution>
<marker>Benson, Mor´e, 2001</marker>
<rawString>Steven J. Benson and Jorge Mor´e. 2001. A limited-memory variable-metric algorithm for boundconstrained minimization. Technical report, Mathematics and Computer Science Division, Argonne National Laboratory. ANL/MCS-P909-0901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="11248" citStr="Berger et al., 1996" startWordPosition="1831" endWordPosition="1834">ferent naive Bayes classifiers are used for different LHS symbols (phrase types). Table 2 shows the filtering performance in training with sections 02-21 on the Penn Treebank. We set the threshold probability for filtering to be 0.0001 for the experiments reported in this paper. The naive Bayes classifiers effectively reduced the number of candidates with little positive samples that were wrongly filtered out. 6 Phrase Recognition with a Maximum Entropy Classifier For the candidates which are not filtered out in the above two phases, we perform classification with maximum entropy classifiers (Berger et al., 1996). We construct a binary classifier for each type of phrases using the entire training set. The training samples for maximum entropy consist of the phrase candidates that have not been filtered out by the CFG rule dictionary and the naive Bayes classifier. One of the merits of using a maximum entropy classifier is that we can obtain a probability from the classifier in each decision. The probability of each decision represents how likely the candidate is a correct chunk. We accept a chunk only when the probability is larger than the predefined threshold. With this thresholding scheme, we can co</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Data oriented parsing.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="22000" citStr="Bod, 1992" startWordPosition="3616" endWordPosition="3617">ates that still we will face unknown rules even when we have constructed the rule dictionary using the whole training data (note that the dotted line does not saturate). Additional feature sets for the maximum entropy classifiers could improve the performance. The bottom-up parsing strategy allows us to use information about sub-trees that have already been constructed. We thus do not need to restrict ourselves to use only head-information of the partial parses. Since many researchers have reported that information on partial parse trees plays an important role for achieving high performance (Bod, 1992; Collins and Duffy, 2002; Kudo et al., 2005), we expect that additional features will improve the performance of chunk parsing. Also, the methods for searching the best parse presented in sections 7.2 and 7.3 have much room for improvement. the search method does not have the device to avoid repetitive computations on the same nonterminal sequence in parsing. A chart-like structure which effectively stores the partial parse results could enable the parser to explore a broader search space and produce better parses. Our chunk parser exhibited a considerable improvement in parsing accuracy over</context>
</contexts>
<marker>Bod, 1992</marker>
<rawString>Rens Bod. 1992. Data oriented parsing. In Proceedings of COLING 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Cascaded markov models.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="1023" citStr="Brants, 1999" startWordPosition="149" endWordPosition="150">n perform significantly better than previously reported by using a simple slidingwindow method and maximum entropy classifiers for phrase recognition in each level of chunking. Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/sentence). We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers, and show that the search method can further improve the parsing accuracy. 1 Introduction Chunk parsing (Tjong Kim Sang, 2001; Brants, 1999) is a simple parsing strategy both in implementation and concept. The parser first performs chunking by identifying base phrases, and convert the identified phrases to non-terminal symbols. The parser again performs chunking on the updated sequence and convert the newly recognized phrases into non-terminal symbols. The parser repeats this procedure until there are no phrases to be chunked. After finishing these chunking processes, we can reconstruct the complete parse tree of the sentence from the chunking results. Although the conceptual simplicity of chunk parsing is appealing, satisfactory </context>
</contexts>
<marker>Brants, 1999</marker>
<rawString>Thorsten Brants. 1999. Cascaded markov models. In Proceedings of EACL 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL 2000,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1963" citStr="Charniak, 2000" startWordPosition="298" endWordPosition="299">r repeats this procedure until there are no phrases to be chunked. After finishing these chunking processes, we can reconstruct the complete parse tree of the sentence from the chunking results. Although the conceptual simplicity of chunk parsing is appealing, satisfactory performance for practical use has not yet been achieved with this parsing strategy. Sang achieved an f-score of 80.49 on the Penn Treebank by using the IOB tagging method for each level of chunking (Tjong Kim Sang, 2001). However, there is a very large gap between their performance and that of widely-used practical parsers (Charniak, 2000; Collins, 1999). The performance of chunk parsing is heavily dependent on the performance of phrase recognition in each level of chunking. We show in this paper that the chunk parsing strategy is indeed appealing in that it can give considerably better performance than previously reported by using a different approach for phrase recognition and that it enables us to build a very fast parser that gives high-precision outputs. This advantage could open up the possibility of using full parsers for large-scale information extraction from the Web corpus and real-time information extraction where t</context>
<context position="19922" citStr="Charniak (2000)" startWordPosition="3288" endWordPosition="3289">h the results of previous studies. In order to make the results directly comparable, we produced POS tags as the input of our parsers by using a POS tagger (Tsuruoka and Tsujii, 2005) which was trained on sections 0-18 in the WSJ corpus. The table also shows the performance achieved 138 0 50 100 150 200 250 300 350 400 450 500 Time (sec) Figure 6: Time vs F-score on section 23. The xaxis represents the time required to parse the entire section. The time required for making a hash table in Collins parser is excluded. LR LP F-score Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.6 89.5 89.5 Kudo (2005) 89.3 89.6 89.4 Sang (2001) 78.7 82.3 80.5 Deterministic (tagger-POSs) 81.2 86.5 83.8 Deterministic (gold-POSs) 82.6 87.7 85.1 Search (tagger-POSs) 83.2 87.1 85.1 Search (gold-POSs) 84.6 88.5 86.5 Iterative Search (tagger-POSs) 85.0 86.8 85.9 Iterative Search (gold-POSs) 86.2 88.0 87.1 Table 6: Comparison with other work. Parsing performance on section 23 (all sentences). with the iterative parsing method presented in section 7.3. Our chunk parser achieved an f-score of 83.8 with the deterministic parsing methods using the POS-tagger tags. This f-score is better than</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL 2000, pages 132– 139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="22025" citStr="Collins and Duffy, 2002" startWordPosition="3618" endWordPosition="3621">till we will face unknown rules even when we have constructed the rule dictionary using the whole training data (note that the dotted line does not saturate). Additional feature sets for the maximum entropy classifiers could improve the performance. The bottom-up parsing strategy allows us to use information about sub-trees that have already been constructed. We thus do not need to restrict ourselves to use only head-information of the partial parses. Since many researchers have reported that information on partial parse trees plays an important role for achieving high performance (Bod, 1992; Collins and Duffy, 2002; Kudo et al., 2005), we expect that additional features will improve the performance of chunk parsing. Also, the methods for searching the best parse presented in sections 7.2 and 7.3 have much room for improvement. the search method does not have the device to avoid repetitive computations on the same nonterminal sequence in parsing. A chart-like structure which effectively stores the partial parse results could enable the parser to explore a broader search space and produce better parses. Our chunk parser exhibited a considerable improvement in parsing accuracy over the previous study on ch</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings ofACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1979" citStr="Collins, 1999" startWordPosition="300" endWordPosition="301">rocedure until there are no phrases to be chunked. After finishing these chunking processes, we can reconstruct the complete parse tree of the sentence from the chunking results. Although the conceptual simplicity of chunk parsing is appealing, satisfactory performance for practical use has not yet been achieved with this parsing strategy. Sang achieved an f-score of 80.49 on the Penn Treebank by using the IOB tagging method for each level of chunking (Tjong Kim Sang, 2001). However, there is a very large gap between their performance and that of widely-used practical parsers (Charniak, 2000; Collins, 1999). The performance of chunk parsing is heavily dependent on the performance of phrase recognition in each level of chunking. We show in this paper that the chunk parsing strategy is indeed appealing in that it can give considerably better performance than previously reported by using a different approach for phrase recognition and that it enables us to build a very fast parser that gives high-precision outputs. This advantage could open up the possibility of using full parsers for large-scale information extraction from the Web corpus and real-time information extraction where the system needs </context>
<context position="18593" citStr="Collins, 1999" startWordPosition="3052" endWordPosition="3053">59 87.69 85.06 33.6 0.1 82.32 85.02 83.65 46.9 Table 4: Parsing performance on section 23 (all sentences, gold-standard POS tags) with the deterministic algorithm. Margin LR LP F-score Time (sec) 0.0 75.65 90.81 82.54 41.2 0.1 79.63 90.16 84.57 74.4 0.2 82.70 89.57 86.00 94.8 0.3 84.60 88.53 86.52 110.2 0.4 84.91 86.99 85.94 116.3 Table 5: Parsing performance on section 23 (all sentences, gold-standard POS tags) with the search algorithm. One of the advantages of our chunk parser is its parsing speed. For comparison, we show the tradeoff between parsing time and performance in Collins parser (Collins, 1999) and our chunk parser in Figure 6. Collins parser allows the user to change the size of the beam in parsing. We used Model-2 because it gave better performance than Model-3 when the beam size was smaller than 1000. As for the chunk parser, we controlled the trade-off by changing the maximum number of nodes in the search. The uncertainty margin for chunk recognition was 0.3. Figure 6 shows that Collins parser clearly outperforms our chunk parser when the beam size is large. However, the performance significantly drops with a smaller beam size. The break-even point is at around 200 sec (83 msec/</context>
<context position="19891" citStr="Collins (1999)" startWordPosition="3283" endWordPosition="3284">nce on section 23 together with the results of previous studies. In order to make the results directly comparable, we produced POS tags as the input of our parsers by using a POS tagger (Tsuruoka and Tsujii, 2005) which was trained on sections 0-18 in the WSJ corpus. The table also shows the performance achieved 138 0 50 100 150 200 250 300 350 400 450 500 Time (sec) Figure 6: Time vs F-score on section 23. The xaxis represents the time required to parse the entire section. The time required for making a hash table in Collins parser is excluded. LR LP F-score Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.6 89.5 89.5 Kudo (2005) 89.3 89.6 89.4 Sang (2001) 78.7 82.3 80.5 Deterministic (tagger-POSs) 81.2 86.5 83.8 Deterministic (gold-POSs) 82.6 87.7 85.1 Search (tagger-POSs) 83.2 87.1 85.1 Search (gold-POSs) 84.6 88.5 86.5 Iterative Search (tagger-POSs) 85.0 86.8 85.9 Iterative Search (gold-POSs) 86.2 88.0 87.1 Table 6: Comparison with other work. Parsing performance on section 23 (all sentences). with the iterative parsing method presented in section 7.3. Our chunk parser achieved an f-score of 83.8 with the deterministic parsing methods using the POS-tagger ta</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluation and extension of maximum entropy models with inequality constraints.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="12136" citStr="Kazama and Tsujii, 2003" startWordPosition="1973" endWordPosition="1977">he merits of using a maximum entropy classifier is that we can obtain a probability from the classifier in each decision. The probability of each decision represents how likely the candidate is a correct chunk. We accept a chunk only when the probability is larger than the predefined threshold. With this thresholding scheme, we can control the trade-off between precision and recall by changing the threshold value. Regularization is important in maximum entropy modeling to avoid overfitting to the training data. For this purpose, we use the maximum entropy modeling with inequality constraints (Kazama and Tsujii, 2003). This modeling has one parameter to tune as in Gaussian prior modeling. The parameter is called the width factor. We set this parameter to be 1.0 throughout the experiments. For numerical optimization, we used the Limited-Memory Variable-Metric (LMVM) algorithm (Benson and Mor´e, 2001). 6.1 Features Table 3 lists the features used in phrase recognition with the maximum entropy classifier. Information about the adjacent non-terminal symbols is important. We use unigrams, bigrams, and trigrams of the adjacent symbols. Head information is also useful. We use unigrams and bigrams of the neighbori</context>
</contexts>
<marker>Kazama, Tsujii, 2003</marker>
<rawString>Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation and extension of maximum entropy models with inequality constraints. In Proceedings of EMNLP 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="5214" citStr="Kudo and Matsumoto, 2001" startWordPosition="833" endWordPosition="836">ses). Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved an f-score of 80.49 on the Penn Treebank corpus. 3 Chunking with a sliding-window approach The performance of chunk parsing heavily depends on the performance of each level of chunking. The popular approach to this shallow parsing is to convert the problem into a tagging task and use a variety of machine learning techniques that have been developed for sequence labeling problems such as Hidden Markov Models, sequential classification with SVMs (Kudo and Matsumoto, 2001), and Conditional Random Fields (Sha and Pereira, 2003). One of our claims in this paper is that we should not convert the chunking problem into a tagging task. Instead, we use a classical sliding-window method for chunking, where we consider all subsequences as phrase candidates and classify them with a machine learning algorithm. Suppose, for example, we are about to perform chunking on the sequence in Figure 4. NP-volume VBD-was .-. We consider the following sub sequences as the phrase candidates in this level of chunking. 1. (NP-volume) VBD-was .-. 2. NP-volume (VBD-was) .-. 3. NP-volume V</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proceedings of NAACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Boosting-based parse reranking with subtree features.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="22045" citStr="Kudo et al., 2005" startWordPosition="3622" endWordPosition="3625"> rules even when we have constructed the rule dictionary using the whole training data (note that the dotted line does not saturate). Additional feature sets for the maximum entropy classifiers could improve the performance. The bottom-up parsing strategy allows us to use information about sub-trees that have already been constructed. We thus do not need to restrict ourselves to use only head-information of the partial parses. Since many researchers have reported that information on partial parse trees plays an important role for achieving high performance (Bod, 1992; Collins and Duffy, 2002; Kudo et al., 2005), we expect that additional features will improve the performance of chunk parsing. Also, the methods for searching the best parse presented in sections 7.2 and 7.3 have much room for improvement. the search method does not have the device to avoid repetitive computations on the same nonterminal sequence in parsing. A chart-like structure which effectively stores the partial parse results could enable the parser to explore a broader search space and produce better parses. Our chunk parser exhibited a considerable improvement in parsing accuracy over the previous study on chunk parsing. However</context>
</contexts>
<marker>Kudo, Suzuki, Isozaki, 2005</marker>
<rawString>Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In Proceedings ofACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>276--283</pages>
<contexts>
<context position="4139" citStr="Magerman, 1995" startWordPosition="662" endWordPosition="663"> million ounces . Figure 2: Chunk parsing, the 2nd iteration. volume was ounces . Figure 3: Chunk parsing, the 3rd iteration. S NP VP . volume was . Figure 4: Chunk parsing, the 4th iteration. 2 Chunk Parsing For the overall strategy of chunk parsing, we follow the method proposed by Sang (Tjong Kim Sang, 2001). Figures 1 to 4 show an example of chunk parsing. In the first iteration, the chunker identifies two base phrases, (NP Estimated volume) and (QP 2.4 million), and replaces each phrase with its nonterminal symbol and head. The head word is identified by using the head-percolation table (Magerman, 1995). In the second iteration, the chunker identifies (NP a light million ounces) and converts this phrase into NP. This chunking procedure is repeated until the whole sentence is chunked at the fourth iteration, and the full parse tree is easily recovered from the chunking history. This parsing strategy converts the problem of full parsing into smaller and simpler problems, namely, chunking, where we only need to recognize flat structures (base phrases). Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings of ACL 1995, pages 276–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Efficacy of beam thresholding, unification filtering and hybrid parsing in probabilistic hpsg parsing.</title>
<date>2005</date>
<booktitle>In Proceedings ofIWPT</booktitle>
<contexts>
<context position="15441" citStr="Ninomiya et al., 2005" startWordPosition="2506" endWordPosition="2509">des chunks with their probabilities. We consider only the chunks whose probabilities are within the predefined margin from . In other words, the chunks whose probabilities are larger than are considered as assured chunks, and thus are fixed when we generate alternative hypotheses of chunking. The chunks whose probabilities are smaller than are simply ignored. We generate alternative hypotheses in each level of chunking, and search the best parse in a depthfirst manner. 7.3 Iterative parsing We also tried an iterative parsing strategy, which was successfully used in probabilistic HPSG parsing (Ninomiya et al., 2005). The parsing strategy is simple. The parser starts with a very low margin and tries to find a successful parse. If the parser cannot find a successful parse, then it increases the margin by a certain step and tries to parse with the wider margin. 137 8 Experiments We ran parsing experiments using the Penn Treebank corpus, which is widely used for evaluating parsing algorithms. The training set consists of sections 02-21. We used section 22 as the development data, with which we tuned the feature set and parameters for parsing. The test set consists of section 23 and we report the performance </context>
</contexts>
<marker>Ninomiya, Tsuruoka, Miyao, Tsujii, 2005</marker>
<rawString>Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Efficacy of beam thresholding, unification filtering and hybrid parsing in probabilistic hpsg parsing. In Proceedings ofIWPT 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Mitch Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<contexts>
<context position="4673" citStr="Ramshaw and Marcus, 1995" startWordPosition="745" endWordPosition="748"> and head. The head word is identified by using the head-percolation table (Magerman, 1995). In the second iteration, the chunker identifies (NP a light million ounces) and converts this phrase into NP. This chunking procedure is repeated until the whole sentence is chunked at the fourth iteration, and the full parse tree is easily recovered from the chunking history. This parsing strategy converts the problem of full parsing into smaller and simpler problems, namely, chunking, where we only need to recognize flat structures (base phrases). Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved an f-score of 80.49 on the Penn Treebank corpus. 3 Chunking with a sliding-window approach The performance of chunk parsing heavily depends on the performance of each level of chunking. The popular approach to this shallow parsing is to convert the problem into a tagging task and use a variety of machine learning techniques that have been developed for sequence labeling problems such as Hidden Markov Models, sequential classification with SVMs (Kudo and Matsumoto, 2001), and Conditional Random Fields (Sha and Pereira, 2003). On</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance Ramshaw and Mitch Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora, pages 82–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>133--142</pages>
<contexts>
<context position="7491" citStr="Ratnaparkhi, 1996" startWordPosition="1211" endWordPosition="1212">gest disadvantage of the sliding window approach is the cost for training and parsing. Since there are phrase candidates when the length of the sequence is , a naive application of machine learning easily leads to prohibitive consumption of memory and time. In order to reduce the number of phrase candidates to be considered by machine learning, we introduce two filtering phases into training and parsing. One is done by a rule dictionary. The other is done by a naive Bayes classifier. 4 Filtering with the CFG Rule Dictionary We use an idea that is similar to the method proposed by Ratnaparkhi (Ratnaparkhi, 1996) for partof-speech tagging. They used a Tag Dictionary, with which the tagger considers only the tag-word pairs that appear in the training sentences as the candidate tags. A similar method can be used for reducing the number of phrase candidates. We first construct a rule dictionary consisting of all the CFG rules used in the training data. In both training and parsing, we filter out all the sub-sequences that do not match any of the entry in the dictionary. 4.1 Normalization The rules used in the training data do not cover all the rules in unseen sentences. Therefore, if we take a naive filt</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of EMNLP 1996, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>1--10</pages>
<contexts>
<context position="19861" citStr="Ratnaparkhi (1997)" startWordPosition="3278" endWordPosition="3279"> 6 summarizes our parsing performance on section 23 together with the results of previous studies. In order to make the results directly comparable, we produced POS tags as the input of our parsers by using a POS tagger (Tsuruoka and Tsujii, 2005) which was trained on sections 0-18 in the WSJ corpus. The table also shows the performance achieved 138 0 50 100 150 200 250 300 350 400 450 500 Time (sec) Figure 6: Time vs F-score on section 23. The xaxis represents the time required to parse the entire section. The time required for making a hash table in Collins parser is excluded. LR LP F-score Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.6 89.5 89.5 Kudo (2005) 89.3 89.6 89.4 Sang (2001) 78.7 82.3 80.5 Deterministic (tagger-POSs) 81.2 86.5 83.8 Deterministic (gold-POSs) 82.6 87.7 85.1 Search (tagger-POSs) 83.2 87.1 85.1 Search (gold-POSs) 84.6 88.5 86.5 Iterative Search (tagger-POSs) 85.0 86.8 85.9 Iterative Search (gold-POSs) 86.2 88.0 87.1 Table 6: Comparison with other work. Parsing performance on section 23 (all sentences). with the iterative parsing method presented in section 7.3. Our chunk parser achieved an f-score of 83.8 with the deterministic parsing m</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of EMNLP 1997, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings ofICML</booktitle>
<contexts>
<context position="6562" citStr="Sarawagi and Cohen, 2004" startWordPosition="1054" endWordPosition="1057">ng window approach is that we can make use of a richer set of features on recognizing a phrase than in the sequential labeling 134 approach. We can define arbitrary features on the target candidate (e.g. the whole sequence of nonterminal symbols of the target) and the surrounding context, which are, in general, not available in sequential labeling approaches. We should mention here that there are some other modeling methods for sequence labeling which allow us to define arbitrary features on the target phrase. Semi-markov conditional random fields (Semi-CRFs) are one of such modeling methods (Sarawagi and Cohen, 2004). Semi-CRFs could give better performance than the sliding-window approach because they can incorporate features on other phrase candidates on the same level of chunking. However, they require additional computational resources for training and parsing, and the use of Semi-CRFs is left for future work. The biggest disadvantage of the sliding window approach is the cost for training and parsing. Since there are phrase candidates when the length of the sequence is , a naive application of machine learning easily leads to prohibitive consumption of memory and time. In order to reduce the number o</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2004. Semimarkov conditional random fields for information extraction. In Proceedings ofICML 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="5269" citStr="Sha and Pereira, 2003" startWordPosition="842" endWordPosition="845">(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved an f-score of 80.49 on the Penn Treebank corpus. 3 Chunking with a sliding-window approach The performance of chunk parsing heavily depends on the performance of each level of chunking. The popular approach to this shallow parsing is to convert the problem into a tagging task and use a variety of machine learning techniques that have been developed for sequence labeling problems such as Hidden Markov Models, sequential classification with SVMs (Kudo and Matsumoto, 2001), and Conditional Random Fields (Sha and Pereira, 2003). One of our claims in this paper is that we should not convert the chunking problem into a tagging task. Instead, we use a classical sliding-window method for chunking, where we consider all subsequences as phrase candidates and classify them with a machine learning algorithm. Suppose, for example, we are about to perform chunking on the sequence in Figure 4. NP-volume VBD-was .-. We consider the following sub sequences as the phrase candidates in this level of chunking. 1. (NP-volume) VBD-was .-. 2. NP-volume (VBD-was) .-. 3. NP-volume VBD-was (.-.) 4. (NP-volume VBD-was) .-. 5. NP-volume (V</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
</authors>
<title>Transforming a chunker to a parser.</title>
<date>2001</date>
<booktitle>Computational Linguistics in the Netherlands</booktitle>
<pages>177--188</pages>
<editor>In J. Veenstra W. Daelemans, K. Sima‘an and J. Zavrel, editors,</editor>
<publisher>Rodopi.</publisher>
<contexts>
<context position="1008" citStr="Sang, 2001" startWordPosition="147" endWordPosition="148">k parsing can perform significantly better than previously reported by using a simple slidingwindow method and maximum entropy classifiers for phrase recognition in each level of chunking. Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/sentence). We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers, and show that the search method can further improve the parsing accuracy. 1 Introduction Chunk parsing (Tjong Kim Sang, 2001; Brants, 1999) is a simple parsing strategy both in implementation and concept. The parser first performs chunking by identifying base phrases, and convert the identified phrases to non-terminal symbols. The parser again performs chunking on the updated sequence and convert the newly recognized phrases into non-terminal symbols. The parser repeats this procedure until there are no phrases to be chunked. After finishing these chunking processes, we can reconstruct the complete parse tree of the sentence from the chunking results. Although the conceptual simplicity of chunk parsing is appealing</context>
<context position="3836" citStr="Sang, 2001" startWordPosition="611" endWordPosition="612">g Technologies (IWPT), pages 133–140, Vancouver, October 2005. c�2005 Association for Computational Linguistics QP VBN NN VBD DT JJ CD CD NNS . NP VP NP VBD NP . Estimated volume was a light 2.4 million ounces . Figure 1: Chunk parsing, the 1st iteration. NP NP VBD DT JJ QP NNS . volume was a light million ounces . Figure 2: Chunk parsing, the 2nd iteration. volume was ounces . Figure 3: Chunk parsing, the 3rd iteration. S NP VP . volume was . Figure 4: Chunk parsing, the 4th iteration. 2 Chunk Parsing For the overall strategy of chunk parsing, we follow the method proposed by Sang (Tjong Kim Sang, 2001). Figures 1 to 4 show an example of chunk parsing. In the first iteration, the chunker identifies two base phrases, (NP Estimated volume) and (QP 2.4 million), and replaces each phrase with its nonterminal symbol and head. The head word is identified by using the head-percolation table (Magerman, 1995). In the second iteration, the chunker identifies (NP a light million ounces) and converts this phrase into NP. This chunking procedure is repeated until the whole sentence is chunked at the fourth iteration, and the full parse tree is easily recovered from the chunking history. This parsing stra</context>
<context position="19976" citStr="Sang (2001)" startWordPosition="3298" endWordPosition="3299">sults directly comparable, we produced POS tags as the input of our parsers by using a POS tagger (Tsuruoka and Tsujii, 2005) which was trained on sections 0-18 in the WSJ corpus. The table also shows the performance achieved 138 0 50 100 150 200 250 300 350 400 450 500 Time (sec) Figure 6: Time vs F-score on section 23. The xaxis represents the time required to parse the entire section. The time required for making a hash table in Collins parser is excluded. LR LP F-score Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.6 89.5 89.5 Kudo (2005) 89.3 89.6 89.4 Sang (2001) 78.7 82.3 80.5 Deterministic (tagger-POSs) 81.2 86.5 83.8 Deterministic (gold-POSs) 82.6 87.7 85.1 Search (tagger-POSs) 83.2 87.1 85.1 Search (gold-POSs) 84.6 88.5 86.5 Iterative Search (tagger-POSs) 85.0 86.8 85.9 Iterative Search (gold-POSs) 86.2 88.0 87.1 Table 6: Comparison with other work. Parsing performance on section 23 (all sentences). with the iterative parsing method presented in section 7.3. Our chunk parser achieved an f-score of 83.8 with the deterministic parsing methods using the POS-tagger tags. This f-score is better than that achieved by the previous study on chunk parsing </context>
</contexts>
<marker>Sang, 2001</marker>
<rawString>Erik Tjong Kim Sang. 2001. Transforming a chunker to a parser. In J. Veenstra W. Daelemans, K. Sima‘an and J. Zavrel, editors, Computational Linguistics in the Netherlands 2000, pages 177–188. Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<contexts>
<context position="19490" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="3207" endWordPosition="3210">changing the maximum number of nodes in the search. The uncertainty margin for chunk recognition was 0.3. Figure 6 shows that Collins parser clearly outperforms our chunk parser when the beam size is large. However, the performance significantly drops with a smaller beam size. The break-even point is at around 200 sec (83 msec/sentence). 8.2 Comparison with previous work Table 6 summarizes our parsing performance on section 23 together with the results of previous studies. In order to make the results directly comparable, we produced POS tags as the input of our parsers by using a POS tagger (Tsuruoka and Tsujii, 2005) which was trained on sections 0-18 in the WSJ corpus. The table also shows the performance achieved 138 0 50 100 150 200 250 300 350 400 450 500 Time (sec) Figure 6: Time vs F-score on section 23. The xaxis represents the time required to parse the entire section. The time required for making a hash table in Collins parser is excluded. LR LP F-score Ratnaparkhi (1997) 86.3 87.5 86.9 Collins (1999) 88.1 88.3 88.2 Charniak (2000) 89.6 89.5 89.5 Kudo (2005) 89.3 89.6 89.4 Sang (2001) 78.7 82.3 80.5 Deterministic (tagger-POSs) 81.2 86.5 83.8 Deterministic (gold-POSs) 82.6 87.7 85.1 Search (tagger</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In Proceedings of HLT/EMNLP 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>