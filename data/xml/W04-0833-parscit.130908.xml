<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.043624">
<note confidence="0.8673185">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
</note>
<title confidence="0.8501265">
Association for Computational Linguistics
Simple Features for Statistical Word Sense Disambiguation
</title>
<author confidence="0.989938">
Abolfazl K. Lamjiri, Osama El Demerdash, Leila Kosseim
</author>
<affiliation confidence="0.992120333333333">
CLaC Laboratory
Department of Computer Science
Concordia University, Montreal, Canada
</affiliation>
<email confidence="0.986276">
{a keigho,osama el,kosseim}@cs.concordia.ca
</email>
<sectionHeader confidence="0.995353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991645">
In this paper, we describe our experiments on
statistical word sense disambiguation (WSD)
using two systems based on different ap-
proaches: Naive Bayes on word tokens and Max-
imum Entropy on local syntactic and seman-
tic features. In the first approach, we consider
a context window and a sub-window within it
around the word to disambiguate. Within the
outside window, only content words are con-
sidered, but within the sub-window, all words
are taken into account. Both window sizes are
tuned by the system for each word to disam-
biguate and accuracies of 75% and 67% were re-
spectively obtained for coarse and fine grained
evaluations. In the second system, sense res-
olution is done using an approximate syntac-
tic structure as well as semantics of neighbor-
ing nouns as features to a Maximum Entropy
learner. Accuracies of 70% and 63% were ob-
tained for coarse and fine grained evaluations.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999711533333333">
In this paper, we present the two systems we
built for our first participation in the English
lexical sample task at Senseval-3. In the first
system, a Naive Bayes learner based on context
words as features is implemented. In the second
system, an approximate syntactic structure, in
addition to semantics of the nouns around the
ambiguous word are selected as features to learn
with a Maximum Entropy learner.
In Section 2, a brief overview of related work
on WSD is presented. Sections 3 and 4 provide
specifications of our two systems. Section 5
discusses the results obtained and remarks on
them, and finally in Section 6, conclusion and
our future work direction are given.
</bodyText>
<sectionHeader confidence="0.999636" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999936578947369">
In 1950, Kaplan carried out one of the earliest
WSD experiments and showed that the accu-
racy of sense resolution does not improve when
more than four words around the target are
considered (Ide and V´eronis, 1998). While re-
searchers such as Masterman (1961), Gougen-
heim and Michea (1961), agree with this ob-
servation (Ide and V´eronis, 1998), our results
demonstrate that this does not generally ap-
ply to all words. A large context window pro-
vides domain information which increases the
accuracy for some target words such as bank.n,
but not others like different.a or use.v (see Sec-
tion 3). This confirms Mihalcea’s observations
(Mihalcea, 2002). In our system we allow a
larger context window size and for most of the
words such context window is selected by the
system.
Another trend consists in defining and us-
ing semantic preferences for the target word.
For example, the verb drink prefers an ani-
mate subject in its imbibe sense. Boguraev
shows that this does not work for polysemous
verbs because of metaphoric expressions (Ide
and V´eronis, 1998).
Furthermore, the grammatical structures the
target word takes part in can be used as a distin-
guishing tool: “the word ‘keep’, can be disam-
biguated by determining whether its object is
gerund (He kept eating), adjectival phrase (He
kept calm), or noun phrase (He kept a record)”
(Reifler, 1955). In our second system we ap-
proximate the syntactic structures of a word, in
its different senses.
Mooney (Mooney, 1996) has discussed the
effect of bias on inductive learning methods.
In this work we also show sensitivity of Naive
Bayes to the distribution of samples.
</bodyText>
<sectionHeader confidence="0.992954" genericHeader="method">
3 Naive Bayes for Learning Context
Words
</sectionHeader>
<bodyText confidence="0.9996578">
In our approach, a large window and a smaller
sub-window are centered around the target
word. We account for all words within the sub-
window but use a POS filter as well as a short
stop-word list to filter out non-content words
</bodyText>
<figureCaption confidence="0.859805">
Figure 1: The effect of choosing different win-
</figureCaption>
<bodyText confidence="0.992372875">
dow and sub-window sizes for the word bank.n.
The best accuracy is achieved with a window
and sub-window size of around 450 and 50 char-
acters respectively, while for example 50 and 25
provide very low accuracy.
from the context. The filter retains only open
class words, i.e. nouns, adjectives, adverbs, and
verbs, and rejects words tagged otherwise.
</bodyText>
<subsectionHeader confidence="0.999948">
3.1 Changing the context window size
</subsectionHeader>
<bodyText confidence="0.999974277777778">
Figure 1 shows the effect of selecting differ-
ent window and sub-window sizes for the word
bank.n. It is clear that precision is very sensitive
to the selected window size. Other words also
have such variations in their precision results.
The system decides on the best window sizes
for every word by examining possible window
size values ranging from 25 to 750 characters1.
Table 1 shows the optimal window sizes se-
lected for a number of words from different word
classes. The baseline is considered individually
for every word as the ratio of the most com-
mon sense in the training samples. We used the
Senseval-3 training set for the English lexical
sample task for training. It includes a total of
7860 tagged samples for 57 ambiguous words.
15% of this data was used for validation, while
the rest was used for training.
</bodyText>
<subsectionHeader confidence="0.99986">
3.2 Approximate Smoothing
</subsectionHeader>
<bodyText confidence="0.999958333333333">
During the testing phase, given the context of
the target word, the score of every sense is com-
puted using the Naive Bayes formula:
</bodyText>
<equation confidence="0.9851865">
�Scoresensei = log p(sensei) + log p(wordk)
k
</equation>
<bodyText confidence="0.991303289473684">
where, wordk is every word inside the context
window (recall that these are all the words in
1For technical reasons, character is used instead of
word as the unit, making sure no word is cut at the
extremities.
the sub-window, and filtered words in the large
window).
Various smoothing algorithms could be used
to reduce the probability of seen words and dis-
tributing them among unseen words. However,
tuning various smoothing parameters is delicate
as it involves keeping an appropriate amount
of held-out data. Instead, we implemented an
approximate smoothing method, which seems
to perform better compared to Ng’s (Ng, 1997)
approximate smoothing. In our simple approxi-
mate smoothing the probability of seen words
is not discounted to compensate for those of
unseen words2. Finding a proper value to
assign to unseen words was done experimen-
tally; for a relatively large training data set,
p(an unseen word) = 10−10 and for a small
set, 10−9 resulted in the highest accuracy with
our 15% validation seta. The intuition is that,
with a small training set, more unseen words
are likely to be seen during the testing phase,
and in order to prevent the accumulating score
penalty value from becoming relatively high, a
lower probability value is selected. Additionally,
the selection should not result in large differ-
ences in the computed scores of different senses
of the target word.
A simple function assigns 10−10 in any of the
following conditions: the total number of words
seen is larger than 4300, the number of train-
ing instances is greater than 230, or the context
window size is larger than 400 characters. The
function returns 10−9 otherwise.
</bodyText>
<sectionHeader confidence="0.992179" genericHeader="method">
4 Maximum Entropy learning of
syntax and semantics
</sectionHeader>
<bodyText confidence="0.999400777777778">
Syntactic structures as well as semantics of the
words around the ambiguous word are strong
clues for sense resolution in many words. How-
ever, deriving and using exact syntactic infor-
mation introduces its own difficulties. So, we
tried to use approximate syntactic structures by
learning the following features in a context win-
dow bounded by the last punctuation before and
the first punctuation after the ambiguous word:
</bodyText>
<listItem confidence="0.988342333333333">
1. Article Bef: If there is any article before,
the string token is considered as the value
of this feature.
2. POS, POS Bef, POS Aft: The part of
speech of the target, the part of speech of
the word before (after) if any.
</listItem>
<footnote confidence="0.996288666666667">
2This results in a total probability mass larger than
1; but still allows us to rank the probability of the senses.
3The logarithm was taken in base 10.
</footnote>
<table confidence="0.999924666666667">
Word WS SW Diff Base Bys Ent
add.v 100 25 4.1 46 69 82
argument.n 175 75 3.1 47 45 54
ask.v 725 150 5.2 36 37 65
decide.v 725 375 5.2 77 65 75
different.a 175 0 4.0 47 34 48
eat.v 550 150 3.1 81 76 86
miss.v 425 125 5.1 28 40 53
simple.a 400 25 9.0 40 11 33
sort.n 175 75 4.0 66 60 71
use.v 50 25 5.6 58 57 79
wash.v 50 25 5.5 56 62 71
</table>
<tableCaption confidence="0.99767">
Table 1: Optimal window configuration and
</tableCaption>
<bodyText confidence="0.9584855">
performance of both systems for the words
on which Max Entropy has performed bet-
ter than Naive Bayes. (WS=Optimal win-
dow size; SW=Optimal sub-window size;
Diff=Average absolute difference between the
distribution of training and test samples;
Accuracy (Base=Baseline; Bys=Naive Bayes;
Ent=Max Entropy)).
</bodyText>
<listItem confidence="0.85112575">
3. Prep Bef, Prep Aft: The last preposition
before, and the first preposition after the
target, if any.
4. Sem Bef, Sem Aft: The general semantic
</listItem>
<bodyText confidence="0.973765">
category of the noun before (after) the tar-
get. The category, which can be ‘animate’,
‘inanimate’, or ‘abstract’, is computed by
traversing hypernym synsets of WordNet
for all the senses of that noun. The first
semantic category observed is returned, or
‘inanimate’ is returned as the default value.
The first three items are taken from Mihal-
cea’s work (Mihalcea, 2002) which are useful
features for most of the words. The range of all
these features are closed sets; so Maximum En-
tropy is not biased by the distribution of train-
ing samples among senses, which is a side-effect
of Naive Bayes learners (see Section 5.2)4.
The following is an example of the fea-
tures extracted for sample miss.v.bnc.00045286:
“ ... ? I’ll miss the kids. But ... ”:
</bodyText>
<footnote confidence="0.983589857142857">
Article Bef=null,
POS Bef=&amp;quot;MD&amp;quot;, POS=&amp;quot;VB&amp;quot;, POS Aft=&amp;quot;DT&amp;quot;,
Prep Bef=null, Prep Aft=null,
Sem Bef=null, Sem After=&amp;quot;animate&amp;quot;
4The Maximum Entropy program we used to learn
these features was obtained from the OpenNLP site:
http://maxent.sourceforge.net/index.html
</footnote>
<table confidence="0.999261166666667">
Word Naive Bayes Max Entropy fine
Category coarse fine coarse
nouns 76% 70% 70% 61%
verbs 76% 67% 74% 66%
adjectives 59% 45% 59% 47%
Total 75% 67% 70% 63%
</table>
<tableCaption confidence="0.9927135">
Table 2: Results of both approaches in fine and
coarse grain evaluation.
</tableCaption>
<sectionHeader confidence="0.997906" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.719699">
The Word Sense Disambiguator program has
</subsectionHeader>
<bodyText confidence="0.999539333333333">
been written as a Processing Resource in the
Gate Architecture5. It uses the ANNIE Tok-
enizer and POS Tagger which are provided as
components of Gate.
Table 2 shows the results of both systems for
each category of words. It can be seen that ap-
proximate syntactic information has performed
relatively better with adjectives which are gen-
erally harder to disambiguate.
</bodyText>
<subsectionHeader confidence="0.999258">
5.1 Window size and the commonest
effect
</subsectionHeader>
<bodyText confidence="0.999918230769231">
The optimal window size seems to be related
to the distribution of the senses in the train-
ing samples and the number of training sam-
ples available for a word. Indeed, a large win-
dow size is selected when the number of samples
is large, and the samples are not evenly dis-
tributed among senses. Basically because the
words in Senseval are not mostly topical words,
Naive Bayes is working strongly with the com-
monest effect. On the other hand, when a small
window size is selected, the commonest effect
mostly vanishes and instead, collocations are re-
lied upon.
</bodyText>
<subsectionHeader confidence="0.999982">
5.2 Distribution of samples
</subsectionHeader>
<bodyText confidence="0.995413923076923">
A Naive Bayes method is quite sensitive to the
proportion of training and test samples: if the
commonest class presented as test is different
from the commonest class in training for ex-
ample, this method performs poorly. This is
a serious problem of Naive Bayes towards real
world WSD. For testing this claim, we made the
following hypothesis: When the mean of abso-
lute difference of the test samples and training
samples among classes of senses is more than
4%, Na¨ıve Bayes method performs at most 20%
above baseline6. Table 3 shows that this hypoth-
esis is confirmed in 82% of the cases (41 words
</bodyText>
<footnote confidence="0.992978">
5http://www.gate.ac.uk/
6The following exceptional cases are not considered:
1) When baseline is above 70%, getting 20% above base-
</footnote>
<table confidence="0.996008333333333">
Acc &lt; 20 Acc &gt; 20
Dist &lt; 4.0 5 26
Dist &gt; 4.0 15 4
</table>
<tableCaption confidence="0.996911">
Table 3: Sensitivity of Naive Bayes to the dis-
</tableCaption>
<bodyText confidence="0.9346686">
tribution of samples (Acc=Accuracy amount
higher than baseline; Dist=Mean of distribution
change.)
out of 50 ambiguous words that satisfy the con-
ditions). Furthermore, such words are not nec-
essarily difficult words. Our Maximum Entropy
method performed on average 25% above the
baseline on 7 of them (ask.v, decide.v, differ-
ent.a, difficulty.n, sort.n, use.v, wash.v some of
which are shown in Table 1).
</bodyText>
<subsectionHeader confidence="0.986926">
5.3 Rare samples
</subsectionHeader>
<bodyText confidence="0.999699833333333">
Naive Bayes mostly ignores the senses with a
few samples in the training and gets its score
on the senses with large number of training in-
stances, while Maximum Entropy exploits fea-
tures from senses which have had a few training
samples.
</bodyText>
<subsectionHeader confidence="0.999789">
5.4 Using lemmas and synsets
</subsectionHeader>
<bodyText confidence="0.999987466666667">
We tried working with word lemmas instead
of derivated forms; however, for some words
it causes loss in accuracy. For example, for
the adjective different.a, with window and sub-
window size of 175 and 0, it reduces the accu-
racy from 60% to 46% with the validation set.
However, for the noun sort.n, precision increases
from 62% to 72% with a window size of 650 and
sub-window size of 50. We believe that some
senses come with a specific form of their neigh-
boring tokens and lemmatization removes this
distinguishing feature.
We also tried storing synsets of words as fea-
tures for the Naive Bayes learner, but obtained
no significant change in the results.
</bodyText>
<sectionHeader confidence="0.997623" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999340848484848">
There is no fixed context window size applica-
ble to all ambiguous words in the Naive Bayes
approach: keeping a large context window pro-
vides domain information which increases the
resolution accuracy for some target words but
not others. For non-topical words, large win-
dow size is selected only in order to exploit the
distribution of samples.
line is really difficult, 2) When the difference is mostly
on the commonest sense being seen more than expected,
so the score is favored (7 words out of 57 satisfy these
conditions.)
Rough syntactic information performed well
in our second system using Maximum Entropy
modeling. This suggests that some senses can
be strongly identified by syntax, leaving resolu-
tion of other senses to other methods. A simple,
rough heuristic for recognizing when to rely on
syntactic information in our system is when the
selected window size by Naive Bayes is relatively
small.
We tried two simple methods for combining
the two methods: considering context words as
features in Max Entropy learner, and, establish-
ing a separate Naive Bayes learner for each syn-
tactic/semantic feature and adding their scores
to the basic contextual Naive Bayes. These pre-
liminary experiments did not result in any no-
ticeable improvement.
Finally, using more semantic features from
WordNet, such as verb sub-categorization
frames (which are not consistently available)
may help in distinguishing the senses.
</bodyText>
<sectionHeader confidence="0.998219" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9863665">
Many thanks to Glenda B. Anaya and Michelle
Khalif´e for their invaluable help.
</bodyText>
<sectionHeader confidence="0.998294" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9954434">
N. Ide and J. V´eronis. 1998. Introduction to the
special issue on word sense disambiguation:
the state of the art. Computational Linguis-
tics, 24(1):1–40.
R. Mihalcea. 2002. Instance based learning
with automatic feature selection applied to
word sense disambiguation. In Proceedings of
COLING’02, Taiwan.
R. J. Mooney. 1996. Comparative experiments
on disambiguating word senses: An illustra-
tion of the role of bias in machine learning. In
Proceedings of EMNLP-96, pages 82–91, PA.
H. T. Ng. 1997. Exemplar-based word sense
disambiguation: Some recent improvements.
In Proceedings of EMNLP-97, pages 208–213.
NJ.
E. Reifler. 1955. The mechanical determination
of meaning. In Machine translation of lan-
guages, pages 136–164, New York. John Wi-
ley and Sons.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.306824">
<note confidence="0.7439255">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004</note>
<title confidence="0.820554">Association for Computational Linguistics Simple Features for Statistical Word Sense Disambiguation</title>
<author confidence="0.990271">Abolfazl K Lamjiri</author>
<author confidence="0.990271">Osama El_Demerdash</author>
<author confidence="0.990271">Leila</author>
<email confidence="0.861505">CLaC</email>
<affiliation confidence="0.998126">Department of Computer Concordia University, Montreal,</affiliation>
<email confidence="0.99123">keigho,osama</email>
<abstract confidence="0.997021571428572">In this paper, we describe our experiments on statistical word sense disambiguation (WSD) using two systems based on different approaches: Naive Bayes on word tokens and Maximum Entropy on local syntactic and semantic features. In the first approach, we consider a context window and a sub-window within it around the word to disambiguate. Within the outside window, only content words are considered, but within the sub-window, all words are taken into account. Both window sizes are tuned by the system for each word to disambiguate and accuracies of 75% and 67% were respectively obtained for coarse and fine grained evaluations. In the second system, sense resolution is done using an approximate syntactic structure as well as semantics of neighboring nouns as features to a Maximum Entropy learner. Accuracies of 70% and 63% were obtained for coarse and fine grained evaluations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J V´eronis</author>
</authors>
<title>Introduction to the special issue on word sense disambiguation: the state of the art.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Ide, V´eronis, 1998</marker>
<rawString>N. Ide and J. V´eronis. 1998. Introduction to the special issue on word sense disambiguation: the state of the art. Computational Linguistics, 24(1):1–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Instance based learning with automatic feature selection applied to word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING’02,</booktitle>
<contexts>
<context position="2668" citStr="Mihalcea, 2002" startWordPosition="426" endWordPosition="427"> of the earliest WSD experiments and showed that the accuracy of sense resolution does not improve when more than four words around the target are considered (Ide and V´eronis, 1998). While researchers such as Masterman (1961), Gougenheim and Michea (1961), agree with this observation (Ide and V´eronis, 1998), our results demonstrate that this does not generally apply to all words. A large context window provides domain information which increases the accuracy for some target words such as bank.n, but not others like different.a or use.v (see Section 3). This confirms Mihalcea’s observations (Mihalcea, 2002). In our system we allow a larger context window size and for most of the words such context window is selected by the system. Another trend consists in defining and using semantic preferences for the target word. For example, the verb drink prefers an animate subject in its imbibe sense. Boguraev shows that this does not work for polysemous verbs because of metaphoric expressions (Ide and V´eronis, 1998). Furthermore, the grammatical structures the target word takes part in can be used as a distinguishing tool: “the word ‘keep’, can be disambiguated by determining whether its object is gerund</context>
<context position="9071" citStr="Mihalcea, 2002" startWordPosition="1527" endWordPosition="1528">ribution of training and test samples; Accuracy (Base=Baseline; Bys=Naive Bayes; Ent=Max Entropy)). 3. Prep Bef, Prep Aft: The last preposition before, and the first preposition after the target, if any. 4. Sem Bef, Sem Aft: The general semantic category of the noun before (after) the target. The category, which can be ‘animate’, ‘inanimate’, or ‘abstract’, is computed by traversing hypernym synsets of WordNet for all the senses of that noun. The first semantic category observed is returned, or ‘inanimate’ is returned as the default value. The first three items are taken from Mihalcea’s work (Mihalcea, 2002) which are useful features for most of the words. The range of all these features are closed sets; so Maximum Entropy is not biased by the distribution of training samples among senses, which is a side-effect of Naive Bayes learners (see Section 5.2)4. The following is an example of the features extracted for sample miss.v.bnc.00045286: “ ... ? I’ll miss the kids. But ... ”: Article Bef=null, POS Bef=&amp;quot;MD&amp;quot;, POS=&amp;quot;VB&amp;quot;, POS Aft=&amp;quot;DT&amp;quot;, Prep Bef=null, Prep Aft=null, Sem Bef=null, Sem After=&amp;quot;animate&amp;quot; 4The Maximum Entropy program we used to learn these features was obtained from the OpenNLP site: http:</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>R. Mihalcea. 2002. Instance based learning with automatic feature selection applied to word sense disambiguation. In Proceedings of COLING’02, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Mooney</author>
</authors>
<title>Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP-96,</booktitle>
<pages>82--91</pages>
<contexts>
<context position="3491" citStr="Mooney, 1996" startWordPosition="565" endWordPosition="566">rd. For example, the verb drink prefers an animate subject in its imbibe sense. Boguraev shows that this does not work for polysemous verbs because of metaphoric expressions (Ide and V´eronis, 1998). Furthermore, the grammatical structures the target word takes part in can be used as a distinguishing tool: “the word ‘keep’, can be disambiguated by determining whether its object is gerund (He kept eating), adjectival phrase (He kept calm), or noun phrase (He kept a record)” (Reifler, 1955). In our second system we approximate the syntactic structures of a word, in its different senses. Mooney (Mooney, 1996) has discussed the effect of bias on inductive learning methods. In this work we also show sensitivity of Naive Bayes to the distribution of samples. 3 Naive Bayes for Learning Context Words In our approach, a large window and a smaller sub-window are centered around the target word. We account for all words within the subwindow but use a POS filter as well as a short stop-word list to filter out non-content words Figure 1: The effect of choosing different window and sub-window sizes for the word bank.n. The best accuracy is achieved with a window and sub-window size of around 450 and 50 chara</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>R. J. Mooney. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. In Proceedings of EMNLP-96, pages 82–91, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
</authors>
<title>Exemplar-based word sense disambiguation: Some recent improvements.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP-97,</booktitle>
<pages>208--213</pages>
<publisher>NJ.</publisher>
<contexts>
<context position="5998" citStr="Ng, 1997" startWordPosition="990" endWordPosition="991">ery word inside the context window (recall that these are all the words in 1For technical reasons, character is used instead of word as the unit, making sure no word is cut at the extremities. the sub-window, and filtered words in the large window). Various smoothing algorithms could be used to reduce the probability of seen words and distributing them among unseen words. However, tuning various smoothing parameters is delicate as it involves keeping an appropriate amount of held-out data. Instead, we implemented an approximate smoothing method, which seems to perform better compared to Ng’s (Ng, 1997) approximate smoothing. In our simple approximate smoothing the probability of seen words is not discounted to compensate for those of unseen words2. Finding a proper value to assign to unseen words was done experimentally; for a relatively large training data set, p(an unseen word) = 10−10 and for a small set, 10−9 resulted in the highest accuracy with our 15% validation seta. The intuition is that, with a small training set, more unseen words are likely to be seen during the testing phase, and in order to prevent the accumulating score penalty value from becoming relatively high, a lower pro</context>
</contexts>
<marker>Ng, 1997</marker>
<rawString>H. T. Ng. 1997. Exemplar-based word sense disambiguation: Some recent improvements. In Proceedings of EMNLP-97, pages 208–213. NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reifler</author>
</authors>
<title>The mechanical determination of meaning.</title>
<date>1955</date>
<booktitle>In Machine translation of languages,</booktitle>
<pages>136--164</pages>
<publisher>John Wiley and Sons.</publisher>
<location>New York.</location>
<contexts>
<context position="3371" citStr="Reifler, 1955" startWordPosition="545" endWordPosition="546">ext window is selected by the system. Another trend consists in defining and using semantic preferences for the target word. For example, the verb drink prefers an animate subject in its imbibe sense. Boguraev shows that this does not work for polysemous verbs because of metaphoric expressions (Ide and V´eronis, 1998). Furthermore, the grammatical structures the target word takes part in can be used as a distinguishing tool: “the word ‘keep’, can be disambiguated by determining whether its object is gerund (He kept eating), adjectival phrase (He kept calm), or noun phrase (He kept a record)” (Reifler, 1955). In our second system we approximate the syntactic structures of a word, in its different senses. Mooney (Mooney, 1996) has discussed the effect of bias on inductive learning methods. In this work we also show sensitivity of Naive Bayes to the distribution of samples. 3 Naive Bayes for Learning Context Words In our approach, a large window and a smaller sub-window are centered around the target word. We account for all words within the subwindow but use a POS filter as well as a short stop-word list to filter out non-content words Figure 1: The effect of choosing different window and sub-wind</context>
</contexts>
<marker>Reifler, 1955</marker>
<rawString>E. Reifler. 1955. The mechanical determination of meaning. In Machine translation of languages, pages 136–164, New York. John Wiley and Sons.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>