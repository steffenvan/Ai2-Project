<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007827">
<title confidence="0.9918115">
USPwlv and WLVusp: Combining Dictionaries and Contextual
Information for Cross-Lingual Lexical Substitution
</title>
<author confidence="0.995355">
Wilker Aziz
</author>
<affiliation confidence="0.995072">
University of S˜ao Paulo
</affiliation>
<address confidence="0.920411">
S˜ao Carlos, SP, Brazil
</address>
<email confidence="0.997886">
wilker.aziz@usp.br
</email>
<sectionHeader confidence="0.997375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992685">
We describe two systems participating
in Semeval-2010’s Cross-Lingual Lexical
Substitution task: USPwlv and WLVusp.
Both systems are based on two main com-
ponents: (i) a dictionary to provide a num-
ber of possible translations for each source
word, and (ii) a contextual model to select
the best translation according to the con-
text where the source word occurs. These
components and the way they are inte-
grated are different in the two systems:
they exploit corpus-based and linguistic
resources, and supervised and unsuper-
vised learning methods. Among the 14
participants in the subtask to identify the
best translation, our systems were ranked
2nd and 4th in terms of recall, 3rd and 4th
in terms of precision. Both systems out-
performed the baselines in all subtasks ac-
cording to all metrics used.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999811058823529">
The goal of the Cross-Lingual Lexical Substitu-
tion task in Semeval-2010 (Mihalcea et al., 2010)
is to find the best (best subtask) Spanish transla-
tion or the 10-best (oot subtask) translations for
100 different English source words depending on
their context of occurrence. Source words include
nouns, adjectives, adverbs and verbs. 1, 000 oc-
currences of such words are given along with a
short context (a sentence).
This task resembles that of Word Sense Dis-
ambiguation (WSD) within Machine Translation
(MT). A few approaches have recently been pro-
posed using standard WSD features to learn mod-
els using translations instead of senses (Specia et
al., 2007; Carpuat and Wu, 2007; Chan and Ng,
2007). In such approaches, the global WSD score
is added as a feature to statistical MT systems,
</bodyText>
<author confidence="0.522218">
Lucia Specia
</author>
<affiliation confidence="0.7106505">
University of Wolverhampton
Wolverhampton, UK
</affiliation>
<email confidence="0.961137">
l.specia@wlv.ac.uk
</email>
<bodyText confidence="0.999979853658536">
along with additional features, to help the system
on its choice for the best translation of a source
word or phrase.
We exploit contextual information in alternative
ways to standard WSD features and supervised ap-
proaches. Our two systems - USPwlv and WLV
usp - use two main components: (i) a list of pos-
sible translations for the source word regardless of
its context; and (ii) a contextual model that ranks
such translations for each occurrence of the source
word given its context.
While these components constitute the core of
most WSD systems, the way they are created and
integrated in our systems differs from standard ap-
proaches. Our systems do not require a model
to disambiguate / translate each particular source
word, but instead use general models. We experi-
mented with both corpus-based and standard dic-
tionaries, and different learning methodologies to
rank the candidate translations. Our main goal was
to maximize the accuracy of the system in choos-
ing the best translation.
WLVusp is a very simple system based es-
sentially on (i) a Statistical Machine Translation
(SMT) system trained using a large parallel cor-
pus to generate the n-best translations for each
occurrence of the source words and (ii) a stan-
dard English-Spanish dictionary to filter out noisy
translations and provide additional translations in
case the SMT system was not able to produce a
large enough number of legitimate translations,
particularly for the oot subtask.
USPwlv uses a dictionary built from a large par-
allel corpus using inter-language information the-
ory metrics and an online-learning supervised al-
gorithm to rank the options from the dictionary.
The ranking is based on global and local contex-
tual features, such as the mutual information be-
tween the translation and the words in the source
context, which are trained using human annotation
on the trial dataset.
</bodyText>
<page confidence="0.972126">
117
</page>
<note confidence="0.535218">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 117–122,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.99917" genericHeader="introduction">
2 Resources
</sectionHeader>
<subsectionHeader confidence="0.99958">
2.1 Parallel corpus
</subsectionHeader>
<bodyText confidence="0.9999774">
The English-Spanish part of Europarl (Koehn,
2005), a parallel corpus from the European Par-
liament proceedings, was used as a source of sen-
tence level aligned data. The nearly 1.7M sentence
pairs of English-Spanish translations, as provided
by the Fourth Workshop on Machine Translation
(WMT091), sum up to approximately 48M tokens
in each language. Europarl was used both to train
the SMT system and to generate dictionaries based
on inter-language mutual information.
</bodyText>
<subsectionHeader confidence="0.996303">
2.2 Dictionaries
</subsectionHeader>
<bodyText confidence="0.950393222222222">
The dictionary used by WLVusp was extracted us-
ing the free online service Word Reference2, which
provides two dictionaries: Espasa Concise and
Pocket Oxford Spanish Dictionary. Regular ex-
pressions were used to extract the content of the
webpages, keeping only the translations of the
words or phrasal expressions, and the outcome
was manually revised. The manual revision was
necessary to remove translations of long idiomatic
expressions which were only defined through ex-
amples, for example, for the verb check: “we
checked up and found out he was lying – hicimos
averiguaciones y comprobamos que mentia”. The
resulting dictionary contains a number of open do-
main (single or multi-word) translations for each
of the 100 source words. This number varies from
3 to 91, with an average of 12.87 translations per
word. For example:
</bodyText>
<listItem confidence="0.950111">
• yet.r = todavia, a´un, ya, hasta ahora, sin em-
bargo
• paper.n = articulo, papel, envoltorio, diario,
peri´odico, trabajo, ponencia, examen, parte,
documento, libro
</listItem>
<bodyText confidence="0.998010111111111">
Any other dictionary can in principle be used to
produce the list of translations, possibly without
manual intervention. More comprehensive dictio-
naries could result in better results, particularly
those with explicit information about the frequen-
cies of different translations. Automatic metrics
based on parallel corpus to learn the dictionary can
also be used, but we would expect the accuracy of
the system to drop in that case.
</bodyText>
<footnote confidence="0.999878">
1http://www.statmt.org/wmt09/
translation-task.html
2http://www.wordreference.com/
</footnote>
<bodyText confidence="0.9990705">
The process to generate the corpus-based dic-
tionary for USPwlv is described in Section 4.
</bodyText>
<subsectionHeader confidence="0.999654">
2.3 Pre-processing techniques
</subsectionHeader>
<bodyText confidence="0.999860538461538">
The Europarl parallel corpus was tokenized and
lowercased using standard tools provided by the
WMT09 competition. Additionally, the sentences
that were longer than 100 tokens after tokenization
were discarded.
Since the task specifies that translations should
be given in their basic forms, and also in order to
decrease the sparsity due to the rich morphology
of Spanish, the parallel corpus was lemmatized us-
ing TreeTagger (Schmid, 2006), a freely available
part-of-speech (POS) tagger and lemmatizer. Two
different versions of the parallel corpus were built
using both lemmatized words and their POS tags:
Lemma Words are represented by their lemma-
tized form. In case of ambiguity, the original
form was kept, in order to avoid incorrect choices.
Words that could not be lemmatized were also kept
as in their original form.
Lemma.pos Words are represented by their lem-
matized form followed by their POS tags. POS
tags representing content words are generalized
into four groups: verbs, nouns, adjectives and ad-
verbs. When the system could not identify a POS
tag, a dummy tag was used.
The same techniques were used to pre-process
the trial and test data.
</bodyText>
<subsectionHeader confidence="0.998949">
2.4 Training samples
</subsectionHeader>
<bodyText confidence="0.9999948">
The trial data available for this task was used as a
training set for the USPwlv system, which uses a
supervised learning algorithm to learn the weights
of a number of global features. For the 300 oc-
currences of 30 words in the trial data, the ex-
pected lexical substitutions were given by the task
organizers, and therefore the feature weights could
be optimized in a way to make the system result
in good translations. These sentences were pre-
processed in the same way the parallel corpus.
</bodyText>
<sectionHeader confidence="0.979881" genericHeader="method">
3 WLVusp system
</sectionHeader>
<bodyText confidence="0.9996266">
This system is based on a combination of the
Statistical Machine Translation (SMT) frame-
work using the English-Spanish Europarl data
and an English-Spanish dictionary built semi-
automatically (Section 2.2). The parallel corpus
</bodyText>
<page confidence="0.992844">
118
</page>
<bodyText confidence="0.999988">
was lowercased, tokenized and lemmatized (Sec-
tion 2.3) and then used to train the standard SMT
system Moses (Koehn et al., 2007) and translate
the trial/test sentences, producing the 1000-best
translations for each input sentence.
Moses produces its own dictionary from the
parallel corpus by using a word alignment tool
and heuristics to build parallel phrases of up to
seven source words and their corresponding target
words, to which are assigned translation probabil-
ities using frequency counts in the corpus. This
methodology provides some very localized con-
textual information, which can help guiding the
system towards choosing a correct translation. Ad-
ditional contextual information is used by the lan-
guage model component in Moses, which con-
siders how likely the sentence translation is in
the Spanish language (with a 5-gram language
model).
Using the phrase alignment information, the
translation of each occurrence of a source word
is identified in the output of Moses. Since the
phrase translations are learned using the Europarl
corpus, some translations are very specific to that
domain. Moreover, translations can be very noisy,
given that the process is unsupervised. We there-
fore filter the translations given by Moses to keep
only those also given as possible Spanish trans-
lations according to the semi-automatically built
English-Spanish dictionary (Section 2.2). This is
a general-domain dictionary, but it is less likely to
contain noise.
For best results, only the top translation pro-
duced by Moses is considered. If the actual trans-
lation does not belong to the dictionary, the first
translation in that dictionary is used. Although
there is no information about the order of the
translations in the dictionaries used, by looking at
the translations provided, we believe that the first
translation is in general one of the most frequent.
For oot results, the alternative translations pro-
vided by the 1000-best translations are consid-
ered. In cases where fewer than 10 translations
are found, we extract the remaining ones from the
handcrafted dictionary following their given order
until 10 translations (when available) are found,
without repetition.
WLVusp system therefore combines contextual
information as provided by Moses (via its phrases
and language model) and general translation infor-
mation as provided by a dictionary.
</bodyText>
<sectionHeader confidence="0.984499" genericHeader="method">
4 USPwlv System
</sectionHeader>
<bodyText confidence="0.9989815">
For each source word occurring in the context of
a specific sentence, this system uses a linear com-
bination of features to rank the options from an
automatically built English-Spanish dictionary.
For the best subtask, the translation ranked first
is chosen, while for the oot subtask, the 10 best
ranked translations are used without repetition.
The building of the dictionary, the features used
and the learning scheme are described in what fol-
lows.
</bodyText>
<equation confidence="0.9991078">
Cpen,es(s,t) 3)
Pen(s)pes(t) (
�l i=1 w(|i − j|)MI(si, tj)
AvgMI(tj) = (4)
i=1 w(|i − j|)
</equation>
<bodyText confidence="0.97438775">
In the equations, countl(x) is the number of sen-
tences in which the word x appear in a corpus of
l-language texts; counten,es(s, t) is the number of
sentences in which source and target words co-
occur in the parallel corpus; and Total is the to-
tal number of sentences in the corpus of the lan-
guage(s) under consideration. The distributions
pen and pes are monolingual and can been ex-
tracted from any monolingual corpus.
To prevent discontinuities in Equation 3, we
used a smoothing technique to avoid null proba-
bilities. We assume that any monolingual event
occurs at least once and the joint distribution is
smoothed by a Guo’s factor α = 0.1 (Guo et al.,
2004):
pen,es(s, t) ← pen,es(s, t) + αpen(s)pes(t)
</bodyText>
<figureCaption confidence="0.919389">
Dictionary Building The dictionary building is
based on the concept of inter-language Mutual In-
formation (MI) (Raybaud et al., 2009). It consists
in detecting which words in a source-language
sentence trigger the appearance of other words in
its target-language translation. The inter-language
MI in Equation 3 can be defined for pairs of source
(s) and target (t) words by observing their occur-
rences at the sentence level in a parallel, sentence
aligned corpus. Both simple (Equation 1) and
joint distributions (Equation 2) were built based
on the English-Spanish Europarl corpus using its
Lemma.pos version (Section 2.3).
</figureCaption>
<equation confidence="0.99655325">
countl(x)
pl(x) = (1)
Total
pen,es(s,t) = fen,es(s, t) (2)
Total
MI(s, t) = pen,es(s,
t)log
1 + α
</equation>
<page confidence="0.982026">
119
</page>
<bodyText confidence="0.999738718750001">
For each English source word, a list of Span-
ish translations was produced and ranked accord-
ing to inter-language MI. From the resulting list,
the 50-best translations constrained by the POS of
the original English word were selected.
Features The inter-language MI is a feature
which indicates the global suitability of translat-
ing a source token s into a target one t. However,
inter-language MI is not able to provide local con-
textual information, since it does not take into ac-
count the source context sentence c. The following
features were defined to achieve such capability:
Weighted Average MI (aMI) consists in averag-
ing the inter-language MI between the target
word tj and every source word s in the con-
text sentence c (Raybaud et al., 2009). The
MI component is scaled in a way that long
range dependencies are considered less im-
portant, as shown in Equation 4. The scaling
factor w(·) is assigned 1 for verbs, nouns, ad-
jectives and adverbs up to five positions from
the source word, and 0 otherwise. This fea-
ture gives an idea of how well the elements in
a window centered in the source word head
(sj) align to the target word tj, representing
the suitability of tj translating sj in the given
context.
Modified Weighted Average MI (mMI) takes
the average MI as previously defined, except
that the source word head is not taken into
account. In other words, the scaling function
in Equation 4 equals 0 also when |i − j |= 0.
It gives an idea of how well the source words
align to the target word tj without the strong
influence of its source translation sj. This
should provide less biased information to the
learning.
Best from WLVusp (B) consists in a flag that in-
dicates whether a candidate t is taken as the
best ranked option according to the WLVusp
system. The goal is to exploit the informa-
tion from the SMT system and handcrafted
dictionary used by that system.
10-best from WLVusp (T) this feature is a flag
which indicates whether a candidate t was
among the 10 best ranked translations pro-
vided by the WLVusp system.
Online Learning In order to train a binary rank-
ing system based on the trial dataset as our train-
ing set, we used the online passive-aggressive al-
gorithm MIRA (Crammer et al., 2006). MIRA is
said to be passive-aggressive because it updates
the parameters only when a misprediction is de-
tected. At training time, for each sentence a set
of pairs of candidate translations is retrieved. For
each of these pairs, the rank given by the system
with the current parameters is compared to the cor-
rect rankh(·). A loss function loss(·) controls the
updates attributing non 0 values only for mispre-
dictions. In our implementation, it equals 1 for
any mistake made by the model.
Each element of the kind (c, s, t) = (source
context sentence, source head, translation can-
didate) is assigned a feature vector f(c, s, t) =
(MI, aMI, mMI, B, T), which is modeled by a
vector of parameters w E R5.
The binary ranking is defined as the task of find-
ing the best parameters w which maximize the
number of successful predictions. A successful
prediction happens when the system is able to rank
two translation candidates as expected. For do-
ing so, we define an oriented pair x = (a, b) of
candidate translations of s in the context of c and
a feature vector F(x) = f(c, s, a) − f(c, s, b).
signal(w·F (x)) is the orientation the model gives
to x, that is, whether the system believes a is bet-
ter than b or vice versa. Based on whether or not
that orientation is the same as that of the reference
3, the algorithm takes the decision between updat-
ing or not the parameters. When an update occurs,
it is the one that results in the minimal changes in
the parameters leading to correct labeling x, that
is, guaranteeing that after the update the system
will rank (a, b) correctly. Algorithm 1 presents
the general method, as proposed in (Crammer et
al., 2006).
In the case of this binary ranking, the minimiza-
tion problem has an analytic solution well defined
as long as f(c, s, a) =� f(c, s, b) and rankh(a) =�
rankh(b), otherwise signal(w · F(x)) or the hu-
man label would not be defined, respectively.
These conditions have an impact on the content of
Pairs(c), the set of training points built upon the
system outputs for c, which can only contain pairs
of differently ranked translations.
The learning scheme was initialized with a uni-
</bodyText>
<footnote confidence="0.833205666666667">
3Given s in the context of c and (a, b) a pair of candidate
translations of s, the reference produces 1 if rankh(a) &gt;
rankh(b) and −1 if rankh(b) &gt; rankh(a).
</footnote>
<page confidence="0.976012">
120
</page>
<figure confidence="0.893304461538462">
Algorithm 1 MIRA
1: for c E Training Set do
2: for x = (a, b) E Pairs(c) do
3: yˆ signal(w · F(x))
4: z correct label(x)
5: w = argmaxu 2||w − u||2
1
6: s.t. u · F(x) &gt; loss(ˆy, z)
7: v v+w
8: T T + 1
9: end for
10: end for
11: return T1 v
</figure>
<bodyText confidence="0.742339">
form vector. The average parameters after N = 5
iterations over the training set was taken.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.998258">
5.1 Official results
</subsectionHeader>
<bodyText confidence="0.999924125">
Tables 1 and 2 show the main results obtained by
our two systems in the official competition. We
contrast our systems’ results against the best base-
line provided by the organizers, DIC, which con-
siders translations from a dictionary and frequency
information from WordNet, and show the relative
position of the system among the 14 participants.
The metrics are defined in (Mihalcea et al., 2010).
</bodyText>
<table confidence="0.999658777777778">
Subtask Metric Baseline WLVusp Position
Best R 24.34 25.27 4th
P 24.34 25.27 3rd
Mode R 50.34 52.81 3rd
Mode P 50.34 52.81 4th
OOT R 44.04 48.48 6th
P 44.04 48.48 6th
Mode R 73.53 77.91 5th
Mode P 73.53 77.91 5th
</table>
<tableCaption confidence="0.977519">
Table 1: Official results for WLVusp on the test set, com-
pared to the highest baseline, DICT. P = precision, R = recall.
The last column shows the relative position of the system.
</tableCaption>
<table confidence="0.999941444444444">
Subtask Metric Baseline USPwlv Position
Best R 24.34 26.81 2&amp;quot;d
P 24.34 26.81 3rd
Mode R 50.34 58.85 1st
Mode P 50.34 58.85 2&amp;quot;d
OOT R 44.04 47.60 8th
P 44.04 47.60 8th
Mode R 73.53 79.84 3rd
Mode P 73.53 79.84 3rd
</table>
<tableCaption confidence="0.933167666666667">
Table 2: Official results for USPwlv on the test set, com-
pared to the highest baseline, DICT. The last column shows
the relative position of the system.
</tableCaption>
<bodyText confidence="0.998694444444444">
In the oot subtask, the original systems were
able to output the mode translation approximately
80% of the times. From those translations, nearly
50% were actually considered as best options ac-
cording to human annotators. It is worth noticing
that we focused on the best subtask. Therefore,
for the oot subtask we did not exploit the fact that
translations could be repeated to form the set of 10
best translations. For certain source words, our re-
sulting set of translations is smaller than 10. For
example, in the WLVusp system, whenever the
set of alternative translations identified in Moses’
top 1000-best list did not contain 10 legitimate
translations, that is, 10 translations also found in
the handcrafted dictionary, we simply copied other
translations from that dictionary to amount 10 dif-
ferent translations. If they did not sum to 10 be-
cause the list of translations in the dictionary was
too short, we left the set as it was. As a result, 58%
of the 1000 test cases had fewer than 10 transla-
tions, many of them with as few as two or three
translations. In fact, the list of oot results for the
complete test set resulted in only 1, 950 transla-
tions, when there could be 10, 000 (1, 000 test case
occurrences ∗ 10 translations). In the next section
we describe some additional experiments to take
this issue into account.
</bodyText>
<subsectionHeader confidence="0.998209">
5.2 Additional results
</subsectionHeader>
<bodyText confidence="0.999916347826087">
After receiving the gold-standard data, we com-
puted the scores for a number of variations of our
two systems. For example, we checked whether
the performance of USPwlv is too dependent on
the handcrafted dictionary, via the features B and
T. Table 3 presents the performance of two varia-
tions of USPwlv: MI-aMI-mMI was trained with-
out the two contextual flag features which depend
on WLVusp. MI-B-T was trained without the mu-
tual information contextual features. The variation
MI-aMI-mMI of USPwlv performs well even in
the absence of the features coming from WLVusp,
although the scores are lower. These results show
the effectiveness of the learning scheme, since
USPwlv achieves better performance by combin-
ing these feature variations, as compared to their
individual performance.
To provide an intuition on the contribution
of the two different components in the system
WLVusp, we checked the proportion of times a
translation was provided by each of the compo-
nents. In the best subtask, 48% of the translations
came from Moses, while the remaining 52% pro-
</bodyText>
<page confidence="0.990284">
121
</page>
<table confidence="0.999899555555556">
Subtask Metric Baseline MI-aMI-mMI MI-B-T
Best R 24.34 22.59 20.50
P 24.34 22.59 20.50
Mode R 50.34 50.21 44.01
Mode P 50.34 50.21 44.01
OOT R 39.65 47.60 32.75
P 44.04 39.65 32.75
Mode R 73.53 74.19 56.70
Mode P 73.53 74.19 56.70
</table>
<tableCaption confidence="0.98831675">
Table 3: Comparing between variations of the system
USPwlv on the test set and the highest baseline, DICT. The
variations are different sources of contextual knowledge: MI
(MI–aMI–mMI) and the WLVusp (MI–B–T) system.
</tableCaption>
<bodyText confidence="0.999830125">
vided by Moses were not found in the dictionary.
In those cases, the first translation in the dictio-
nary was used. In the oot subtask, only 12% (246)
of the translations came from Moses, while the re-
maining (1, 704) came from the dictionary. This
can be explained by the little variation in the n-
best lists produced by Moses: most of the varia-
tions account for word-order, punctuation, etc.
Finally, we performed additional experiments in
order to exploit the possibility of replicating well
ranked translations for the oot subtask. Table 4
presents the results of some strategies arbitrarily
chosen for such replications. For example, in the
colums labelled “5” we show the scores for re-
peating (once) the 5 top translations. Notice that
precision and recall increase as we take fewer top
translation and repeat them more times. In terms
of mode metrics, by reducing the number of dis-
tinct translations from 10 to 5, USPwlv still out-
performs (marginally) the baseline. In general, the
new systems outperform the baseline and our pre-
vious results (see Table 1 and 2) in terms of pre-
cision and recall. However, according to the other
mode metrics, they are below our official systems.
</bodyText>
<table confidence="0.998586333333333">
System Metric 5 4 3 2
WLVusp R 69.09 88.36 105.32 122.29
P 69.09 88.36 105.32 122.29
Mode R 68.27 63.05 63.05 52.47
Mode P 68.27 63.05 63.05 52.47
USPwlv R 73.50 94.78 102.96 129.09
P 73.50 94.78 102.96 129.09
Mode R 73.77 68.27 62.62 57.40
Mode P 73.77 68.27 62.62 57.40
</table>
<tableCaption confidence="0.999273">
Table 4: Comparison between different strategies for dupli-
</tableCaption>
<bodyText confidence="0.801876333333333">
cating answers in the task oot. The systems output a number
of distinct guesses and through arbitrarily schemes replicate
them in order to complete a list of 10 translations.
</bodyText>
<sectionHeader confidence="0.996768" genericHeader="conclusions">
6 Discussion and future work
</sectionHeader>
<bodyText confidence="0.999981166666667">
We have presented two systems combining con-
textual information and a pre-defined set of trans-
lations for cross-lingual lexical substitution. Both
systems performed particularly well in the best
subtask. A handcrafted dictionary has shown to be
essential for the WLVusp system and also helpful
for the USPwlv system, which uses an additional
dictionary automatically build from a parallel cor-
pus. We plan to investigate how such systems can
be improved by enhancing the corpus-based re-
sources to further minimize the dependency on the
handcrafted dictionary.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999843394736842">
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 61–72.
Yee Seng Chan and Hwee Tou Ng. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In 45th Annual Meeting of the Association for
Computational Linguistics, pages 33–40.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-agressive algorithms. Jornal of Machine
Learning Research, 7:551–585.
Gang Guo, Chao Huang, Hui Jiang, and Ren-Hua
Wang. 2004. A comparative study on various con-
fidence measures in large vocabulary speech recog-
nition. In International Symposium on Chinese Spo-
ken Language Processing, pages 9–12.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. Semeval-2010 task 2: Cross-lingual lexical
substitution. In SemEval-2010: 5th International
Workshop on Semantic Evaluations.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
and Kamel Smaili. 2009. Word- and sentence-
level confidence measures for machine translation.
In 13th Annual Conference of the European Associ-
ation for Machine Translation, pages 104–111.
Helmut Schmid. 2006. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Natural Language Pro-
cessing, pages 44–49.
Lucia Specia, Mark Stevenson, and Maria das Grac¸as
Volpe Nunes. 2007. Learning expressive models for
word sense disambiguation. In 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 41–148.
</reference>
<page confidence="0.997489">
122
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.510947">
<title confidence="0.99935">Combining Dictionaries and Contextual Information for Cross-Lingual Lexical Substitution</title>
<author confidence="0.976584">Wilker Aziz</author>
<affiliation confidence="0.588606">of Paulo</affiliation>
<address confidence="0.6758">Carlos, SP, Brazil</address>
<email confidence="0.99168">wilker.aziz@usp.br</email>
<abstract confidence="0.99580465">We describe two systems participating Semeval-2010’s Lexical Both systems are based on two main components: (i) a dictionary to provide a number of possible translations for each source word, and (ii) a contextual model to select the best translation according to the context where the source word occurs. These components and the way they are integrated are different in the two systems: they exploit corpus-based and linguistic resources, and supervised and unsupervised learning methods. Among the 14 participants in the subtask to identify the our systems were ranked 2nd and 4th in terms of recall, 3rd and 4th in terms of precision. Both systems outperformed the baselines in all subtasks according to all metrics used.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="1689" citStr="Carpuat and Wu, 2007" startWordPosition="263" endWordPosition="266"> Semeval-2010 (Mihalcea et al., 2010) is to find the best (best subtask) Spanish translation or the 10-best (oot subtask) translations for 100 different English source words depending on their context of occurrence. Source words include nouns, adjectives, adverbs and verbs. 1, 000 occurrences of such words are given along with a short context (a sentence). This task resembles that of Word Sense Disambiguation (WSD) within Machine Translation (MT). A few approaches have recently been proposed using standard WSD features to learn models using translations instead of senses (Specia et al., 2007; Carpuat and Wu, 2007; Chan and Ng, 2007). In such approaches, the global WSD score is added as a feature to statistical MT systems, Lucia Specia University of Wolverhampton Wolverhampton, UK l.specia@wlv.ac.uk along with additional features, to help the system on its choice for the best translation of a source word or phrase. We exploit contextual information in alternative ways to standard WSD features and supervised approaches. Our two systems - USPwlv and WLV usp - use two main components: (i) a list of possible translations for the source word regardless of its context; and (ii) a contextual model that ranks </context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="1709" citStr="Chan and Ng, 2007" startWordPosition="267" endWordPosition="270">a et al., 2010) is to find the best (best subtask) Spanish translation or the 10-best (oot subtask) translations for 100 different English source words depending on their context of occurrence. Source words include nouns, adjectives, adverbs and verbs. 1, 000 occurrences of such words are given along with a short context (a sentence). This task resembles that of Word Sense Disambiguation (WSD) within Machine Translation (MT). A few approaches have recently been proposed using standard WSD features to learn models using translations instead of senses (Specia et al., 2007; Carpuat and Wu, 2007; Chan and Ng, 2007). In such approaches, the global WSD score is added as a feature to statistical MT systems, Lucia Specia University of Wolverhampton Wolverhampton, UK l.specia@wlv.ac.uk along with additional features, to help the system on its choice for the best translation of a source word or phrase. We exploit contextual information in alternative ways to standard WSD features and supervised approaches. Our two systems - USPwlv and WLV usp - use two main components: (i) a list of possible translations for the source word regardless of its context; and (ii) a contextual model that ranks such translations fo</context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2007. Word sense disambiguation improves statistical machine translation. In 45th Annual Meeting of the Association for Computational Linguistics, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-agressive algorithms.</title>
<date>2006</date>
<journal>Jornal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="14562" citStr="Crammer et al., 2006" startWordPosition="2349" endWordPosition="2352">d information to the learning. Best from WLVusp (B) consists in a flag that indicates whether a candidate t is taken as the best ranked option according to the WLVusp system. The goal is to exploit the information from the SMT system and handcrafted dictionary used by that system. 10-best from WLVusp (T) this feature is a flag which indicates whether a candidate t was among the 10 best ranked translations provided by the WLVusp system. Online Learning In order to train a binary ranking system based on the trial dataset as our training set, we used the online passive-aggressive algorithm MIRA (Crammer et al., 2006). MIRA is said to be passive-aggressive because it updates the parameters only when a misprediction is detected. At training time, for each sentence a set of pairs of candidate translations is retrieved. For each of these pairs, the rank given by the system with the current parameters is compared to the correct rankh(·). A loss function loss(·) controls the updates attributing non 0 values only for mispredictions. In our implementation, it equals 1 for any mistake made by the model. Each element of the kind (c, s, t) = (source context sentence, source head, translation candidate) is assigned a</context>
<context position="16217" citStr="Crammer et al., 2006" startWordPosition="2650" endWordPosition="2653"> c and a feature vector F(x) = f(c, s, a) − f(c, s, b). signal(w·F (x)) is the orientation the model gives to x, that is, whether the system believes a is better than b or vice versa. Based on whether or not that orientation is the same as that of the reference 3, the algorithm takes the decision between updating or not the parameters. When an update occurs, it is the one that results in the minimal changes in the parameters leading to correct labeling x, that is, guaranteeing that after the update the system will rank (a, b) correctly. Algorithm 1 presents the general method, as proposed in (Crammer et al., 2006). In the case of this binary ranking, the minimization problem has an analytic solution well defined as long as f(c, s, a) =� f(c, s, b) and rankh(a) =� rankh(b), otherwise signal(w · F(x)) or the human label would not be defined, respectively. These conditions have an impact on the content of Pairs(c), the set of training points built upon the system outputs for c, which can only contain pairs of differently ranked translations. The learning scheme was initialized with a uni3Given s in the context of c and (a, b) a pair of candidate translations of s, the reference produces 1 if rankh(a) &gt; ra</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-agressive algorithms. Jornal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gang Guo</author>
<author>Chao Huang</author>
<author>Hui Jiang</author>
<author>Ren-Hua Wang</author>
</authors>
<title>A comparative study on various confidence measures in large vocabulary speech recognition.</title>
<date>2004</date>
<booktitle>In International Symposium on Chinese Spoken Language Processing,</booktitle>
<pages>9--12</pages>
<contexts>
<context position="11569" citStr="Guo et al., 2004" startWordPosition="1832" endWordPosition="1835"> sentences in which the word x appear in a corpus of l-language texts; counten,es(s, t) is the number of sentences in which source and target words cooccur in the parallel corpus; and Total is the total number of sentences in the corpus of the language(s) under consideration. The distributions pen and pes are monolingual and can been extracted from any monolingual corpus. To prevent discontinuities in Equation 3, we used a smoothing technique to avoid null probabilities. We assume that any monolingual event occurs at least once and the joint distribution is smoothed by a Guo’s factor α = 0.1 (Guo et al., 2004): pen,es(s, t) ← pen,es(s, t) + αpen(s)pes(t) Dictionary Building The dictionary building is based on the concept of inter-language Mutual Information (MI) (Raybaud et al., 2009). It consists in detecting which words in a source-language sentence trigger the appearance of other words in its target-language translation. The inter-language MI in Equation 3 can be defined for pairs of source (s) and target (t) words by observing their occurrences at the sentence level in a parallel, sentence aligned corpus. Both simple (Equation 1) and joint distributions (Equation 2) were built based on the Engl</context>
</contexts>
<marker>Guo, Huang, Jiang, Wang, 2004</marker>
<rawString>Gang Guo, Chao Huang, Hui Jiang, and Ren-Hua Wang. 2004. A comparative study on various confidence measures in large vocabulary speech recognition. In International Symposium on Chinese Spoken Language Processing, pages 9–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="4010" citStr="Koehn, 2005" startWordPosition="634" endWordPosition="635">ing inter-language information theory metrics and an online-learning supervised algorithm to rank the options from the dictionary. The ranking is based on global and local contextual features, such as the mutual information between the translation and the words in the source context, which are trained using human annotation on the trial dataset. 117 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 117–122, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Resources 2.1 Parallel corpus The English-Spanish part of Europarl (Koehn, 2005), a parallel corpus from the European Parliament proceedings, was used as a source of sentence level aligned data. The nearly 1.7M sentence pairs of English-Spanish translations, as provided by the Fourth Workshop on Machine Translation (WMT091), sum up to approximately 48M tokens in each language. Europarl was used both to train the SMT system and to generate dictionaries based on inter-language mutual information. 2.2 Dictionaries The dictionary used by WLVusp was extracted using the free online service Word Reference2, which provides two dictionaries: Espasa Concise and Pocket Oxford Spanis</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ravi Sinha</author>
<author>Diana McCarthy</author>
</authors>
<date>2010</date>
<booktitle>Semeval-2010 task 2: Cross-lingual lexical substitution. In SemEval-2010: 5th International Workshop on Semantic Evaluations.</booktitle>
<contexts>
<context position="1106" citStr="Mihalcea et al., 2010" startWordPosition="168" endWordPosition="171">e best translation according to the context where the source word occurs. These components and the way they are integrated are different in the two systems: they exploit corpus-based and linguistic resources, and supervised and unsupervised learning methods. Among the 14 participants in the subtask to identify the best translation, our systems were ranked 2nd and 4th in terms of recall, 3rd and 4th in terms of precision. Both systems outperformed the baselines in all subtasks according to all metrics used. 1 Introduction The goal of the Cross-Lingual Lexical Substitution task in Semeval-2010 (Mihalcea et al., 2010) is to find the best (best subtask) Spanish translation or the 10-best (oot subtask) translations for 100 different English source words depending on their context of occurrence. Source words include nouns, adjectives, adverbs and verbs. 1, 000 occurrences of such words are given along with a short context (a sentence). This task resembles that of Word Sense Disambiguation (WSD) within Machine Translation (MT). A few approaches have recently been proposed using standard WSD features to learn models using translations instead of senses (Specia et al., 2007; Carpuat and Wu, 2007; Chan and Ng, 20</context>
<context position="17617" citStr="Mihalcea et al., 2010" startWordPosition="2914" endWordPosition="2917">2||w − u||2 1 6: s.t. u · F(x) &gt; loss(ˆy, z) 7: v v+w 8: T T + 1 9: end for 10: end for 11: return T1 v form vector. The average parameters after N = 5 iterations over the training set was taken. 5 Results 5.1 Official results Tables 1 and 2 show the main results obtained by our two systems in the official competition. We contrast our systems’ results against the best baseline provided by the organizers, DIC, which considers translations from a dictionary and frequency information from WordNet, and show the relative position of the system among the 14 participants. The metrics are defined in (Mihalcea et al., 2010). Subtask Metric Baseline WLVusp Position Best R 24.34 25.27 4th P 24.34 25.27 3rd Mode R 50.34 52.81 3rd Mode P 50.34 52.81 4th OOT R 44.04 48.48 6th P 44.04 48.48 6th Mode R 73.53 77.91 5th Mode P 73.53 77.91 5th Table 1: Official results for WLVusp on the test set, compared to the highest baseline, DICT. P = precision, R = recall. The last column shows the relative position of the system. Subtask Metric Baseline USPwlv Position Best R 24.34 26.81 2&amp;quot;d P 24.34 26.81 3rd Mode R 50.34 58.85 1st Mode P 50.34 58.85 2&amp;quot;d OOT R 44.04 47.60 8th P 44.04 47.60 8th Mode R 73.53 79.84 3rd Mode P 73.53 79</context>
</contexts>
<marker>Mihalcea, Sinha, McCarthy, 2010</marker>
<rawString>Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010. Semeval-2010 task 2: Cross-lingual lexical substitution. In SemEval-2010: 5th International Workshop on Semantic Evaluations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Raybaud</author>
<author>Caroline Lavecchia</author>
<author>David Langlois</author>
<author>Kamel Smaili</author>
</authors>
<title>Word- and sentencelevel confidence measures for machine translation.</title>
<date>2009</date>
<booktitle>In 13th Annual Conference of the European Association for Machine Translation,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="11747" citStr="Raybaud et al., 2009" startWordPosition="1859" endWordPosition="1862">rpus; and Total is the total number of sentences in the corpus of the language(s) under consideration. The distributions pen and pes are monolingual and can been extracted from any monolingual corpus. To prevent discontinuities in Equation 3, we used a smoothing technique to avoid null probabilities. We assume that any monolingual event occurs at least once and the joint distribution is smoothed by a Guo’s factor α = 0.1 (Guo et al., 2004): pen,es(s, t) ← pen,es(s, t) + αpen(s)pes(t) Dictionary Building The dictionary building is based on the concept of inter-language Mutual Information (MI) (Raybaud et al., 2009). It consists in detecting which words in a source-language sentence trigger the appearance of other words in its target-language translation. The inter-language MI in Equation 3 can be defined for pairs of source (s) and target (t) words by observing their occurrences at the sentence level in a parallel, sentence aligned corpus. Both simple (Equation 1) and joint distributions (Equation 2) were built based on the English-Spanish Europarl corpus using its Lemma.pos version (Section 2.3). countl(x) pl(x) = (1) Total pen,es(s,t) = fen,es(s, t) (2) Total MI(s, t) = pen,es(s, t)log 1 + α 119 For e</context>
<context position="13092" citStr="Raybaud et al., 2009" startWordPosition="2082" endWordPosition="2085">ulting list, the 50-best translations constrained by the POS of the original English word were selected. Features The inter-language MI is a feature which indicates the global suitability of translating a source token s into a target one t. However, inter-language MI is not able to provide local contextual information, since it does not take into account the source context sentence c. The following features were defined to achieve such capability: Weighted Average MI (aMI) consists in averaging the inter-language MI between the target word tj and every source word s in the context sentence c (Raybaud et al., 2009). The MI component is scaled in a way that long range dependencies are considered less important, as shown in Equation 4. The scaling factor w(·) is assigned 1 for verbs, nouns, adjectives and adverbs up to five positions from the source word, and 0 otherwise. This feature gives an idea of how well the elements in a window centered in the source word head (sj) align to the target word tj, representing the suitability of tj translating sj in the given context. Modified Weighted Average MI (mMI) takes the average MI as previously defined, except that the source word head is not taken into accoun</context>
</contexts>
<marker>Raybaud, Lavecchia, Langlois, Smaili, 2009</marker>
<rawString>Sylvain Raybaud, Caroline Lavecchia, David Langlois, and Kamel Smaili. 2009. Word- and sentencelevel confidence measures for machine translation. In 13th Annual Conference of the European Association for Machine Translation, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>2006</date>
<booktitle>In International Conference on New Methods in Natural Language Processing,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="6518" citStr="Schmid, 2006" startWordPosition="1014" endWordPosition="1015"> translation-task.html 2http://www.wordreference.com/ The process to generate the corpus-based dictionary for USPwlv is described in Section 4. 2.3 Pre-processing techniques The Europarl parallel corpus was tokenized and lowercased using standard tools provided by the WMT09 competition. Additionally, the sentences that were longer than 100 tokens after tokenization were discarded. Since the task specifies that translations should be given in their basic forms, and also in order to decrease the sparsity due to the rich morphology of Spanish, the parallel corpus was lemmatized using TreeTagger (Schmid, 2006), a freely available part-of-speech (POS) tagger and lemmatizer. Two different versions of the parallel corpus were built using both lemmatized words and their POS tags: Lemma Words are represented by their lemmatized form. In case of ambiguity, the original form was kept, in order to avoid incorrect choices. Words that could not be lemmatized were also kept as in their original form. Lemma.pos Words are represented by their lemmatized form followed by their POS tags. POS tags representing content words are generalized into four groups: verbs, nouns, adjectives and adverbs. When the system cou</context>
</contexts>
<marker>Schmid, 2006</marker>
<rawString>Helmut Schmid. 2006. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Natural Language Processing, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Mark Stevenson</author>
<author>Maria das Grac¸as Volpe Nunes</author>
</authors>
<title>Learning expressive models for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>41--148</pages>
<contexts>
<context position="1667" citStr="Specia et al., 2007" startWordPosition="259" endWordPosition="262"> Substitution task in Semeval-2010 (Mihalcea et al., 2010) is to find the best (best subtask) Spanish translation or the 10-best (oot subtask) translations for 100 different English source words depending on their context of occurrence. Source words include nouns, adjectives, adverbs and verbs. 1, 000 occurrences of such words are given along with a short context (a sentence). This task resembles that of Word Sense Disambiguation (WSD) within Machine Translation (MT). A few approaches have recently been proposed using standard WSD features to learn models using translations instead of senses (Specia et al., 2007; Carpuat and Wu, 2007; Chan and Ng, 2007). In such approaches, the global WSD score is added as a feature to statistical MT systems, Lucia Specia University of Wolverhampton Wolverhampton, UK l.specia@wlv.ac.uk along with additional features, to help the system on its choice for the best translation of a source word or phrase. We exploit contextual information in alternative ways to standard WSD features and supervised approaches. Our two systems - USPwlv and WLV usp - use two main components: (i) a list of possible translations for the source word regardless of its context; and (ii) a contex</context>
</contexts>
<marker>Specia, Stevenson, Nunes, 2007</marker>
<rawString>Lucia Specia, Mark Stevenson, and Maria das Grac¸as Volpe Nunes. 2007. Learning expressive models for word sense disambiguation. In 45th Annual Meeting of the Association for Computational Linguistics, pages 41–148.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>