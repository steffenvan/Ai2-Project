<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.997572">
A Novel Feature-based Bayesian Model for Query Focused Multi-document
Summarization
</title>
<author confidence="0.998794">
Jiwei Li Sujian Li
</author>
<affiliation confidence="0.992693">
School of Computer Science Laboratory of Computational Linguistics
Carnegie Mellon University Peking University
</affiliation>
<email confidence="0.996709">
bdlijiwei@gmail.com lisujian@pku.edu.cn
</email>
<sectionHeader confidence="0.994956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993212">
Supervised learning methods and LDA based topic
model have been successfully applied in the field of
multi-document summarization. In this paper, we
propose a novel supervised approach that can in-
corporate rich sentence features into Bayesian topic
models in a principled way, thus taking advantages of
both topic model and feature based supervised learn-
ing methods. Experimental results on DUC2007,
TAC2008 and TAC2009 demonstrate the effective-
ness of our approach.
</bodyText>
<sectionHeader confidence="0.998053" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998245754098361">
Query-focused multi-document summarization
(Nenkova et al., 2006; Wan et al., 2007; Ouyang et
al., 2010) can facilitate users to grasp the main idea
of documents. In query-focused summarization, a
specific topic description, such as a query, which
expresses the most important topic information is
proposed before the document collection, and a
summary would be generated according to the given
topic.
Supervised models have been widely used in sum-
marization (Li, et al., 2009, Shen et al., 2007,
Ouyang et al., 2010). Supervised models usually re-
gard summarization as a classification or regression
problem and use various sentence features to build a
classifier based on labeled negative or positive sam-
ples. However, existing supervised approaches sel-
dom exploit the intrinsic structure among sentences.
This disadvantage usually gives rise to serious prob-
lems such as unbalance and low recall in summaries.
Recently, LDA-based (Blei et al., 2003) Bayesian
topic models have widely been applied in multi-
document summarization in that Bayesian ap-
proaches can offer clear and rigorous probabilis-
tic interpretations for summaries(Daume and Marcu,
2006; Haghighi and Vanderwende, 2009; Jin et al.,
2010; Mason and Charniak, 2011; Delort and Alfon-
seca, 2012). Exiting Bayesian approaches label sen-
tences or words with topics and sentences which are
closely related with query or can highly generalize
documents are selected into summaries. However,
LDA topic model suffers from the intrinsic disad-
vantages that it only uses word frequency for topic
modeling and can not use useful text features such as
position, word order etc (Zhu and Xing, 2010). For
example, the first sentence in a document may be
more important for summary since it is more likely
to give a global generalization about the document.
It is hard for LDA model to consider such informa-
tion, making useful information lost.
It naturally comes to our minds that we can im-
prove summarization performance by making full
use of both useful text features and the latent seman-
tic structures from by LDA topic model. One related
work is from Celikyilmaz and Hakkani-Tur (2010).
They built a hierarchical topic model called Hybh-
sum based on LDA for topic discovery and assumed
this model can produce appropriate scores for sen-
tence evaluation. Then the scores are used for tun-
ing the weights of various features that helpful for
summary generation. Their work made a good step
of combining topic model with feature based super-
vised learning. However, what their approach con-
fuses us is that whether a topic model only based
on word frequency is good enough to generate an
appropriate sentence score for regression. Actually,
how to incorporate features into LDA topic model
has been a open problem. Supervised topic models
such as sLDA(Blei and MacAuliffe 2007) give us
some inspiration. In sLDA, each document is asso-
ciated with a labeled feature and sLDA can integrate
such feature into LDA for topic modeling in a prin-
</bodyText>
<page confidence="0.998799">
89
</page>
<bodyText confidence="0.952866291666667">
Transactions of the Association for Computational Linguistics, 1 (2013) 89–98. Action Editor: Noah Smith.
Submitted 12/2012; Published 5/2013. c�2013 Association for Computational Linguistics.
cipled way.
With reference to the work of supervised LDA
models, in this paper, we propose a novel sentence
feature based Bayesian model S-sLDA for multi-
document summarization. Our approach can natu-
rally combine feature based supervised methods and
topic models. The most important and challeng-
ing problem in our model is the tuning of feature
weights. To solve this problem, we transform the
problem of finding optimum feature weights into an
optimization algorithm and learn these weights in
a supervised way. A set of experiments are con-
ducted based on the benchmark data of DUC2007,
TAC2008 and TAC2009, and experimental results
show the effectiveness of our model.
The rest of the paper is organized as follows. Sec-
tion 2 describes some background and related works.
Section 3 describes our details of S-sLDA model.
Section 4 demonstrates details of our approaches,
including learning, inference and summary gener-
ation. Section 5 provides experiments results and
Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99994876">
A variety of approaches have been proposed
for query-focused multi-document summarizations
such as unsupervised (semi-supervised) approaches,
supervised approaches, and Bayesian approaches.
Unsupervised (semi-supervised) approaches such
as Lexrank (Erkan and Radex, 2004), manifold
(Wan et al., 2007) treat summarization as a graph-
based ranking problem. The relatedness between
the query and each sentence is achieved by impos-
ing querys influence on each sentence along with
the propagation of graph. Most supervised ap-
proaches regard summarization task as a sentence
level two class classification problem. Supervised
machine learning methods such as Support Vector
Machine(SVM) (Li, et al., 2009), Maximum En-
tropy (Osborne, 2002) , Conditional Random Field
(Shen et al., 2007) and regression models (Ouyang
et al., 2010) have been adopted to leverage the rich
sentence features for summarization.
Recently, Bayesian topic models have shown their
power in summarization for its clear probabilistic
interpretation. Daume and Marcu (2006) proposed
Bayesum model for sentence extraction based on
query expansion concept in information retrieval.
Haghighi and Vanderwende (2009) proposed topic-
sum and hiersum which use a LDA-like topic model
and assign each sentence a distribution over back-
ground topic, doc-specific topic and content topics.
Celikyilmaz and Hakkani-Tur (2010) made a good
step in combining topic model with supervised fea-
ture based regression for sentence scoring in sum-
marization. In their model, the score of training
sentences are firstly got through a novel hierarchi-
cal topic model. Then a featured based support vec-
tor regression (SVR) is used for sentence score pre-
diction. The problem of Celikyilmaz and Hakkani-
Turs model is that topic model and feature based re-
gression are two separate processes and the score of
training sentences may be biased because their topic
model only consider word frequency and fail to con-
sider other important features. Supervised feature
based topic models have been proposed in recent
years to incorporate different kinds of features into
LDA model. Blei (2007) proposed sLDA for doc-
ument response pairs and Daniel et al. (2009) pro-
posed Labeled LDA by defining a one to one corre-
spondence between latent topic and user tags. Zhu
and Xing (2010) proposed conditional topic random
field (CTRF) which addresses feature and indepen-
dent limitation in LDA.
</bodyText>
<sectionHeader confidence="0.932118" genericHeader="method">
3 Model description
</sectionHeader>
<subsectionHeader confidence="0.993672">
3.1 LDA and sLDA
</subsectionHeader>
<bodyText confidence="0.989414764705882">
The hierarchical Bayesian LDA (Blei et al., 2003)
models the probability of a corpus on hidden topics
as shown in Figure 1(a). Let K be the number of
topics , M be the number of documents in the cor-
pus and V be vocabulary size. The topic distribution
of each document Bm is drawn from a prior Dirichlet
distribution Dir(α), and each document word wmn
is sampled from a topic-word distribution φz spec-
ified by a drawn from the topic-document distribu-
tion Bm. o is a K × M dimensional matrix and each
ok is a distribution over the V terms. The generat-
ing procedure of LDA is illustrated in Figure 2. Bm
is a mixture proportion over topics of document m
and zmn is a K dimensional variable that presents
the topic assignment distribution of different words.
Supervised LDA (sLDA) (Blei and McAuliffe
2007) is a document feature based model and intro-
</bodyText>
<page confidence="0.99808">
90
</page>
<figureCaption confidence="0.9883635">
Figure 1: Graphical models for (a) LDA model and (b)
sLDA model.
</figureCaption>
<figure confidence="0.7633995">
1. Draw a document proportion vector θm|α ∼ Dir(α)
2. For each word in m
(a)draw topic assignment zmn|θ ∼ Multi(θzmn)
(b)draw word wmn|zmn, β ∼ Multi(βzmn)
</figure>
<figureCaption confidence="0.999763">
Figure 2: Generation process for LDA
</figureCaption>
<bodyText confidence="0.931494833333333">
duces a response variable to each document for topic
discovering, as shown in Figure 1(b). In the gener-
ative procedure of sLDA, the document pairwise la-
bel is draw from y|−→zm, η, δ2 ∼ p(y|−→zm, η, δ2), where
−→ = 1 N
zm N Pn=1 zm,n.
</bodyText>
<subsectionHeader confidence="0.998964">
3.2 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.950059571428571">
Here we firstly give a standard formulation of the
task. Let K be the number of topics, V be the vo-
cabulary size and M be the number of documents.
Each document Dm is represented with a collection
of sentence Dm = {Ss}s=Nm
s=1 where Nm denotes
the number of sentences in mth document. Each
sentence is represented with a collection of words
{wmsn}n=Nms
n=1 where Nms denotes the number of
−−→
words in current sentence. Yms denotes the feature
vector of current sentence and we assume that these
features are independent.
</bodyText>
<subsectionHeader confidence="0.999373">
3.3 S-sLDA
</subsectionHeader>
<bodyText confidence="0.9998755">
zms is the hidden variable indicating the topic of
current sentence. In S-sLDA, we make an assump-
tion that words in the same sentence are generated
from the same topic which was proposed by Gruber
(2007). zmsn denotes the topic assignment of cur-
rent word. According to our assumption, zmsn =
</bodyText>
<figureCaption confidence="0.862073">
Figure 3: Graph model for S-sLDA model
</figureCaption>
<listItem confidence="0.970144">
1. Draw a document proportion vector θm|α ∼ Dir(α)
2. For each sentence in m
</listItem>
<bodyText confidence="0.74696">
(a)draw topic assignment zms|θ ∼ Multi(θzmn)
(b)draw feature vector Yms|zms, η ∼ p(−−→
</bodyText>
<equation confidence="0.99065">
−−→ Yms|zms, η)
(c)for each word wmsn in current sentence
draw wmsn|zms, β ∼ Multi(βzms)
</equation>
<figureCaption confidence="0.996851">
Figure 4: generation process for S-sLDA
</figureCaption>
<bodyText confidence="0.998825285714286">
zms for any n ∈ [1, Nms]. The generative approach
of S-sLDA is shown in Figure 3 and Figure 4. We
can see that the generative process involves not only
the words within current sentence, but also a series
of sentence features. The mixture weights over fea-
tures in S-sLDA are defined with a generalized lin-
ear model (GLM).
</bodyText>
<equation confidence="0.993400333333333">
msη)−−→
p(−−→ Yms|zms, η) = exp(zT Yms
P zms exp(zTmsη)−−→Yms
</equation>
<bodyText confidence="0.999827">
Here we assume that each sentence has T features
and Yms → is a T × 1 dimensional vector. η is a
K × T weight matrix of each feature upon topics,
which largely controls the feature generation proce-
dure. Unlike s-LDA where η is a latent variable esti-
mated from the maximum likelihood estimation al-
gorithm, in S-sLDA the value of η is trained through
a supervised algorithm which will be illustrated in
detail in Section 3.
</bodyText>
<subsectionHeader confidence="0.997427">
3.4 Posterior Inference and Estimation
</subsectionHeader>
<bodyText confidence="0.99997">
Given a document and labels for each sentence, the
posterior distribution of the latent variables is:
</bodyText>
<equation confidence="0.989262388888889">
ms
E[logP(−−→
Yms|zms, η)]+
E[logP(wmsn|zms,β)] + H(q)
(3)
(1)
L =X
ms
X
E[logP (zms|θ)] +
X
E[logP(θ|α)] +
m msn
p(θ, z1:N|w1:N, Y, α, β1:K, η) =
Qm p(θm|α) Qs[p(zms|θm)p(−−→
Yms|zms, η) Qn p(wmsn|zmsn, βzmsn]
R dθp(θm|α) Pz Qs [p(zms  |θm)p(−−→
Yms  |zms, η) Qn p(wmsn |βzmsn)]
</equation>
<bodyText confidence="0.94570475">
(2)
Eqn. (2) cannot be efficiently computed. By
applying the Jensens inequality, we obtain a
lower bound of the log likelihood of document
</bodyText>
<equation confidence="0.483461">
−−→
p(θ, z1:N|w1:N, Yms, α, β1:K, η)
≥
L, where
</equation>
<page confidence="0.972655">
91
</page>
<bodyText confidence="0.999855">
where H(q) = −E[logq] and it is the entropy of
variational distribution q is defined as
</bodyText>
<equation confidence="0.99052">
q(θ, z|γ, φ) = Y q(θm|γ) Y q(zmsn|φms) (4)
mk sn
</equation>
<bodyText confidence="0.999449714285714">
here γ a K-dimensional Dirichlet parameter vector
and multinomial parameters. The first, third and
forth terms of Eqn. (3) are identical to the corre-
sponding terms for unsupervised LDA (Blei et al.,
2003). The second term is the expectation of log
probability of features given the latent topic assign-
ments.
</bodyText>
<equation confidence="0.957123333333333">
E[logP(−−→Yms|zms, η)] =
XE(zms)T η−−→Yms − log exp(zTmsη−−→Yms)
zms
</equation>
<bodyText confidence="0.882325153846154">
(5)
where E(zms)T is a 1 × K dimensional vector
[φmsk]k=K
k=1 . The Bayes estimation for S-sLDA
model can be got via a variational EM algorithm. In
EM procedure, the lower bound is firstly minimized
with respect to γ and φ, and then minimized with α
and β by fixing γ and φ.
E-step:
The updating of Dirichlet parameter γ is identical
to that of unsupervised LDA, and does not involve
−−→
feature vector Yms.
</bodyText>
<equation confidence="0.977662">
γnew
m ← α + X φs (6)
s∈m
φnew N_.X E[log(wmsn|β1:K)]+
sk ∝ exp{E[logθm|γ] + n=1
T ηktYst} = exp[Ψ(γmk) − Ψ( K γmk) + T ηktYst]
X X X
t=1 k=1 t=1
(7)
</equation>
<bodyText confidence="0.965967375">
where Ψ(·) denotes the log Γ function. ms denotes
the document that current sentence comes from and
Yst denotes the tth feature of sentence s.
M-step:
The M-step for updating β is the same as the pro-
cedure in unsupervised LDA, where the probability
of a word generated from a topic is proportional to
the number of times this word assigned to the topic.
</bodyText>
<equation confidence="0.74138">
1(wmsn = w)φk(8)
ms
</equation>
<sectionHeader confidence="0.947694" genericHeader="method">
4 Our Approach
</sectionHeader>
<subsectionHeader confidence="0.997706">
4.1 Learning
</subsectionHeader>
<bodyText confidence="0.999950866666667">
In this subsection, we describe how we learn the fea-
ture weight η in a supervised way. The learning pro-
cess of η is a supervised algorithm combined with
variational inference of S-sLDA. Given a topic de-
scription Q1 and a collection of training sentences S
from related documents, human assessors assign a
score v(v = −2, −1, 0,1,1) to each sentence in S.
The score is an integer between −2 (the least desired
summary sentences) and +2 (the most desired sum-
mary sentences), and score 0 denotes neutral atti-
tude. Ov = {ov1, ov2, ..., vvk}(v = −2, −1, 0, 1, 2)
is the set containing sentences with score v. Let φQk
denote the probability that query is generated from
topic k. Since query does not belong to any docu-
ment, we use the following strategy to leverage φQk
</bodyText>
<equation confidence="0.988840714285714">
YφQk = M exp[Ψ(γmk)−Ψ( K γmk)]
w∈Q βkw· M X X
m=1 k=1
(9)
In Equ.(9), Qw∈Q βkw denotes the probability that
all terms in query are generated from topic k
PM
</equation>
<bodyText confidence="0.959709166666667">
and 1 m=1 exp[Ψ(γmk)−Ψ(PK k=1 γmk)] can be
M
seen as the average probability that all documents in
the corpus are talking about topic k. Eqn. (9) is
based on the assumption that query topic is relevant
to the main topic discussed by the document corpus.
This is a reasonable assumption and most previous
LDA summarization models are based on similar as-
sumptions.
Next, we define φOv,k for sentence set Ov, which
can be interpreted as the probability that all sen-
tences in collection Ov are generated from topic k.
</bodyText>
<equation confidence="0.998717">
1 X φO„,k = |Ov |s∈O„
</equation>
<bodyText confidence="0.9999732">
|Ov |denotes the number of sentences in set Ov. In-
spired by the idea that desired summary sentences
would be more semantically related with the query,
we transform problem of finding optimum η to the
following optimization problem:
</bodyText>
<equation confidence="0.9774195">
minηL(η) = v=2X v · KL(Ov||Q);
v=−2
</equation>
<footnote confidence="0.678896">
1We select multiple queries and their related sentences for
training
</footnote>
<equation confidence="0.992445357142857">
Nm
X
s=1
βnew
kw =
M
X
m=1
NmsX
n=1
φsk, k ∈ [1, K], v ∈ [−2, 2] (10)
T
X ηkt = 1 (11)
t=1
</equation>
<page confidence="0.839527">
92
</page>
<bodyText confidence="0.999748666666667">
where KL(Ov||Q) is the Kullback-Leibler diver-
gence between the topic and sentence set Ov as
shown in Eqn.(12).
</bodyText>
<equation confidence="0.991291">
Q
φ
φOvklog O k (12)
</equation>
<bodyText confidence="0.999534111111111">
In Eqn. (11), we can see that O2, which contain de-
sirable sentences, would be given the largest penalty
for its KL divergence from Query. The case is just
opposite for undesired set.
Our idea is to incorporate the minimization pro-
cess of Eqn.(11) into variational inference process
of S-sLDA model. Here we perform gradient based
optimization method to minimize Eqn.(11). Firstly,
we derive the gradient of L(η) with respect to η.
</bodyText>
<equation confidence="0.99788275">
∂KL(Qv||Q)
v · (13)
∂ηxy
(14)
</equation>
<bodyText confidence="0.996745">
For simplification, we regard β and γ as constant
during updating process of η, soth a��� = 0.2 We can
further get first derivative for each labeled sentence.
</bodyText>
<equation confidence="0.965876">
0 if k =6x
(15)
</equation>
<subsectionHeader confidence="0.984331">
4.2 Feature Space
</subsectionHeader>
<bodyText confidence="0.95851396875">
Lots of features have been proven to be useful for
summarization (Louis et al., 2010). Here we dis-
cuss several types of features which are adopted in
S-sLDA model. The feature values are either binary
or normalized to the interval [0,1]. The following
features are used in S-sLDA:
Cosine Similarity with query: Cosine similarity is
based on the tf-idf value of terms.
2This is reasonable because the influence of γ and β have
been embodied in φ during each iteration.
Local Inner-document Degree Order: Local Inner
document Degree Order is a binary feature which
indicates whether Inner-document Degree (IDD) of
sentence s is the largest among its neighbors. IDD
means the edge number between s and other sen-
tences in the same document.
Document Specific Word: 1 if a sentence contains
document specific word, 0 otherwise.
Average Unigram Probability (Nenkova and Van-
derwende, 2005; Celikyilmaz and Hakkani-Tur
2010): As for sentence s, p(s) = PwEs|3|pD(w),
where pD(w) is the observed unigram probability in
document collection.
In addition, we also use the commonly used fea-
tures including sentence position, paragraph po-
sition, sentence length and sentence bigram fre-
quency.
E-step
initialize φ0sk := 1/K for all i and s.
initialize γmi := αmi + N)m/K for all i.
initialize ηkt = 0 for all k and t.
while not convergence
</bodyText>
<equation confidence="0.921417833333333">
form = 1 : M
update γt+1
m according to Eqn.(6)
for s = 1 : Nm
fork = 1 : K
updateφt+1
</equation>
<bodyText confidence="0.976653">
sk according to Eqn.(7)
normalize the sum of φt+1
sk to 1.
Minimize L(η) according to Eqn.(11)-(15).
M-step:
update β according to Eqn.(8)
</bodyText>
<figureCaption confidence="0.991187">
Figure 5: Learning process of η in S-sLDA
</figureCaption>
<subsectionHeader confidence="0.961131">
4.3 Sentence Selection Strategy
</subsectionHeader>
<bodyText confidence="0.999926">
Next we explain our sentence selection strategy. Ac-
cording to our intuition that the desired summary
should have a small KL divergence with query, we
propose a function to score a set of sentences Sum.
We use a decreasing logistic function ζ(x) = 1/(1+
ex) to refine the score to the range of (0,1).
</bodyText>
<equation confidence="0.979628">
Score(Sum) = ζ(KL(sum||Q)) (16)
</equation>
<bodyText confidence="0.998226">
Let Sum? denote the optimum update summary. We
can get Sum? by maximizing the scoring function.
</bodyText>
<equation confidence="0.919339142857143">
Sum? = arg max Score(Sum)
SumES&amp;&amp;words(Sum)≤L
(17)
XK
k=1
∂φsk
∂ηxy
=
∂KL(Qv||Q)
∂ηxy
P
1 X
s∈Qv
|Qv|(1 + log |Qv |)
s∈Qv
1
∂φsk
∂Qsk
ηxy
−
XK
k=1
∂ηxy
K
X
k=1
Ps∈Qvφsk
Qv φQk
1
X
|Qv |s∈Qv
K
KL(Ov||Q) = X
=
∂L(η)
ηxy
v=2X
v=−2
⎧
⎨⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎩
∝
∂φsk
ηxy
Ysyexp[tiF(γmsi) − tiF( K γmsk) + T ηktYsy]
X X
k=1 t=1
Y× βkw if k = x
w∈s
</equation>
<page confidence="0.990106">
93
</page>
<listItem confidence="0.995427625">
1. Learning: Given labeled set Ov, learn the feature
weight vector η using algorithm in Figure 5.
2. Given new data set and η, use algorithm in section
3.3 for inference. (The only difference between
this step and step (1) is that in this step we do not
need minimize L(η).
3. Select sentences for summarization from algo-
rithm in Figure 6.
</listItem>
<figureCaption confidence="0.999033">
Figure 6: Summarization Generation by S-sLDA.
</figureCaption>
<bodyText confidence="0.956232111111111">
A greedy algorithm is applied by adding sentence
one by one to obtain Sum*. We use G to denote
the sentence set containing selected sentences. The
algorithm first initializes G to Φ and X to SU. Dur-
ing each iteration, we select one sentence from X
which maximize Score(sm U G). To avoid topic re-
dundancy in the summary, we also revise the MMR
strategy (Goldstein et al., 1999; Ouyang et al., 2007)
in the process of sentence selection. For each sm,
we compute the semantic similarity between sm and
each sentence st in set Y in Eqn.(18).
cos − sem (sm, st) = Ek φsm kφstk�2 (18)
o �Ek sm k&apos; Ek stk
We need to assure that the value of semantic similar-
ity between two sentences is less than Thsem. The
whole procedure for summarization using S-sLDA
model is illustrated in Figure 6. Thsem is set to 0.5
in the experiments.
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.98004">
5.1 Experiments Set-up
</subsectionHeader>
<bodyText confidence="0.9999067">
The query-focused multi-document summarization
task defined in DUC3(Document Understanding
Conference) and TAC4(Text Analysis Conference)
evaluations requires generating a concise and well
organized summary for a collection of related news
documents according to a given query which de-
scribes the users information need. The query
usually consists of a title and one or more narra-
tive/question sentences. The system-generated sum-
maries for DUC and TAC are respectively limited to
</bodyText>
<footnote confidence="0.9992575">
3http://duc.nist.gov/.
4http://www.nist.gov/tac/.
</footnote>
<bodyText confidence="0.9998649">
250 words and 100 words. Our experiment data is
composed of DUC 2007, TAC5 2008 and TAC 2009
data which have 45, 48 and 44 collections respec-
tively. In our experiments, DUC 2007 data is used
as training data and TAC (2008-2009) data is used
as the test data.
Stop-words in both documents and queries are
removed using a stop-word list of 598 words, and
the remaining words are stemmed by Porter Stem-
mer6. As for the automatic evaluation of summa-
rization, ROUGE (Recall-Oriented Understudy for
Gisting Evaluation) measures, including ROUGE-
1, ROUGE-2, and ROUGE-SU47 and their corre-
sponding 95% confidence intervals, are used to eval-
uate the performance of the summaries. In order to
obtain a more comprehensive measure of summary
quality, we also conduct manual evaluation on TAC
data with reference to (Haghighi and Vanderwende,
2009; Celikyilmaz and Hakkani-Tur, 2011; Delort
and Alfonseca, 2011).
</bodyText>
<subsectionHeader confidence="0.999777">
5.2 Comparison with other Bayesian models
</subsectionHeader>
<bodyText confidence="0.9990958">
In this subsection, we compare our model with the
following Bayesian baselines:
KL-sum: It is developed by Haghighi and
Vanderwende (Lin et al., 2006) by using a KL-
divergence based sentence selection strategy.
</bodyText>
<equation confidence="0.958826333333333">
� P (w)
KL(Ps||Qd) = P(w)log Q(w) (19)
w
</equation>
<bodyText confidence="0.999513833333333">
where Ps is the unigram distribution of candidate
summary and Qd denotes the unigram distribution of
document collection. Sentences with higher ranking
score is selected into the summary.
HierSum: A LDA based approach proposed by
Haghighi and Vanderwende (2009), where unigram
distribution is calculated from LDA topic model in
Equ.(14).
Hybhsum: A supervised approach developed by
Celikyilmaz and Hakkani-Tur (2010).
For fair comparison, baselines use the same pro-
precessing methods with our model and all sum-
</bodyText>
<footnote confidence="0.997978333333333">
5Here, we only use the docset-A data in TAC, since TAC
data is composed of docset-A and docset-B data, and the docset-
B data is mainly for the update summarization task.
6http://tartarus.org/ martin/PorterStemmer/.
7Jackknife scoring for ROUGE is used in order to compare
with the human summaries.
</footnote>
<page confidence="0.998705">
94
</page>
<table confidence="0.938447923076923">
maries are truncated to the same length of 100
words. From Table 1 and Table 2, we can
Methods ROUGE-1 ROUGE-2 ROUGE-SU4
Our 0.3724 0.1030 0.1342
approach (0.3660-0.3788) (0.0999-0.1061) (0.1290-0.1394)
Hybhsum 0.3703 0.1007 0.1314
(0.3600-0.3806) (0.0952-0.1059) (0.1241-0.1387)
HierSum 0.3613 0.0948 0.1278
(0.3374-0.3752) (0.0899-0.0998) (0.1197-0.1359)
KLsum 0.3504 0.0917 0.1234
(0.3411-0.3597) (0.0842-0.0992) (0.1155-0.1315)
StandLDA 0.3368 0.0797 0.1156
(0.3252-0.3386) (0.0758-0.0836) (0.1072-0.1240)
</table>
<tableCaption confidence="0.999654">
Table 1: Comparison of Bayesian models on TAC2008
</tableCaption>
<table confidence="0.999906181818182">
Methods ROUGE-1 ROUGE-2 ROUGE-SU4
Our 0.3903 0.1223 0.1488
approach (0.3819-0.3987) (0.1167-0.1279) (0.1446-0.1530)
Hybhsum 0.3824 0.1173 0.1436
(0.3686-0.3952) (0.1132-0.1214) (0.1358-0.1514)
HierSum 0.3706 0.1088 0.1386
(0.3624-0.3788) (0.0950-0.1144) (0.1312-0.1464)
KLsum 0.3619 0.0972 0.1299
(0.3510-0.3728) (0.0917-0.1047) (0.1213-0.1385)
StandLDA 0.3552 0.0847 0.1214
(0.3447-0.3657) (0.0813-0.0881) (0.1141-0.1286)
</table>
<tableCaption confidence="0.999885">
Table 2: Comparison of Bayesian models on TAC2009
</tableCaption>
<bodyText confidence="0.9998785625">
see that among all the Bayesian baselines, Hybh-
sum achieves the best result. This further illus-
trates the advantages of combining topic model with
supervised method. In Table 1, we can see that
our S-sLDA model performs better than Hybhsum
and the improvements are 3.4% and 3.7% with re-
spect to ROUGE-2 and ROUGE-SU4 on TAC2008
data. The comparison can be extended to TAC2009
data as shown in Table 2: the performance of S-
sLDA is above Hybhsum by 4.3% in ROUGE-2
and 5.1% in ROUGE-SU4. It is worth explaining
that these achievements are significant, because in
the TAC2008 evaluation, the performance of the top
ranking systems are very close, i.e. the best system
is only 4.2% above the 4th best system on ROUGE-
2 and 1.2% on ROUGE-SU4.
</bodyText>
<subsectionHeader confidence="0.999783">
5.3 Comparison with other baselines.
</subsectionHeader>
<bodyText confidence="0.999781341463415">
In this subsection, we compare our model with some
widely used models in summarization.
Manifold: It is the one-layer graph based semi-
supervised summarization approach developed by
Wan et al.(2008). The graph is constructed only con-
sidering sentence relations using tf-idf and neglects
topic information.
LexRank: Graph based summarization approach
(Erkan and Radev, 2004), which is a revised version
of famous web ranking algorithm PageRank. It is
an unsupervised ranking algorithms compared with
Manifold.
SVM: A supervised method - Support Vector Ma-
chine (SVM) (Vapnik 1995) which uses the same
features as our approach.
MEAD: A centroid based summary algorithm by
Radev et al. (2004). Cluster centroids in MEAD
consists of words which are central not only to one
article in a cluster, but to all the articles. Similarity
is measure using tf-idf.
At the same time, we also present the top three
participating systems with regard to ROUGE-2 on
TAC2008 and TAC2009 for comparison, denoted as
(denoted as SysRank 1st, 2nd and 3rd)(Gillick et al.,
2008; Zhang et al., 2008; Gillick et al., 2009; Varma
et al., 2009). The ROUGE scores of the top TAC
system are directly provided by the TAC evaluation.
From Table 3 and Table 4, we can see that
our approach outperforms the baselines in terms of
ROUGE metrics consistently. When compared with
the standard supervised method SVM, the relative
improvements over the ROUGE-1, ROUGE-2 and
ROUGE-SU4 scores are 4.3%, 13.1%, 8.3% respec-
tively on TAC2008 and 7.2%, 14.9%, 14.3% on
TAC2009. Our model is not as good as top par-
ticipating systems on TAC2008 and TAC2009. But
considering the fact that our model neither uses sen-
tence compression algorithm nor leverage domain
knowledge bases like Wikipedia or training data,
such small difference in ROUGE scores is reason-
able.
</bodyText>
<subsectionHeader confidence="0.997434">
5.4 Manual Evaluations
</subsectionHeader>
<bodyText confidence="0.998531909090909">
In order to obtain a more accurate measure of sum-
mary quality for our S-sLDA model and Hybhsum,
we performed a simple user study concerning the
following aspects: (1) Overall quality: Which sum-
mary is better overall? (2) Focus: Which summary
contains less irrelevant content? (3)Responsiveness:
Which summary is more responsive to the query.
(4) Non-Redundancy: Which summary is less re-
dundant? 8 judges who specialize in NLP partic-
ipated in the blind evaluation task. Evaluators are
presented with two summaries generated by S-sLDA
</bodyText>
<page confidence="0.99497">
95
</page>
<table confidence="0.999640588235294">
Methods ROUGE-1 ROUGE-2 ROUGE-SU4
Our 0.3724 0.1030 0.1342
approach (0.3660-0.3788) (0.0999-0.1061) (0.1290-0.1394)
SysRank 1st 0.3742 0.1039 0.1364
(0.3639-0.3845) (0.0974-0.1104) (0.1285-0.1443)
SysRank 2nd 0.3717 0.0990 0.1326
(0.3610-0.3824 (0.0944-0.1038) (0.1269-0.1385)
SysRank 3rd 0.3710 0.0977 0.1329
(0.3550-0.3849) (0.0920-0.1034) (0.1267-0.1391)
PageRank 0.3597 0.0879 0.1221
(0.3499-0.3695) (0.0809-0.0950) (0.1173-0.1269)
Manifold 0.3621 0.0931 0.1243
(0.3506-0.3736) (0.0868-0.0994) (0.1206-0.1280)
SVM 0.3588 0.0921 0.1258
(0.3489-0.3687) (0.0882-0.0960) (0.1204-0.1302)
MEAD 0.3558 0.0917 0.1226
(0.3489-0.3627) (0.0882-0.0952) (0.1174-0.1278)
</table>
<tableCaption confidence="0.998647">
Table 3: Comparison with baselines on TAC2008
</tableCaption>
<table confidence="0.998514411764706">
Methods ROUGE-1 ROUGE-2 ROUGE-SU4
Our 0.3903 0.1223 0.1488
approach (0.3819-0.3987) (0.1167-0.1279) (0.1446-0.1530)
SysRank 1st 0.3917 0.1218 0.1505
(0.3778-0.4057) (0.1122-0.1314) (0.1414-0.1596)
SysRank 2nd 0.3914 0.1212 0.1513
(0.3808-0.4020) (0.1147-0.1277) (0.1455-0.1571)
SysRank 3rd 0.3851 0.1084 0.1447
(0.3762-0.3932) (0.1025-0.1144) (0.1398-0.1496)
PageRank 0.3616 0.0849 0.1249
(0.3532-0.3700) (0.0802-0.0896) (0.1221-0.1277)
Manifold 0.3713 0.1014 0.1342
(0.3586-0.3841) (0.0950-0.1178) (0.1299-0.1385)
SVM 0.3649 0.1028 0.1319
(0.3536-0.3762) (0.0957-0.1099) (0.1258-0.1380)
MEAD 0.3601 0.1001 0.1287
(0.3536-0.3666) (0.0953-0.1049) (0.1228-0.1346)
</table>
<tableCaption confidence="0.999914">
Table 4: Comparison with baselines on TAC2009
</tableCaption>
<bodyText confidence="0.9997508">
and Hybhsum, as well as the four questions above.
Then they need to answer which summary is better
(tie). We randomly select 20 document collections
from TAC 2008 data and randomly assign two sum-
maries for each collection to three different evalua-
tors to judge which model is better in each aspect.
As we can see from Table 5, the two models al-
most tie with respect to Non-redundancy, mainly
because both models have used appropriate MMR
strategies. But as for Overall quality, Focus and
</bodyText>
<table confidence="0.927156666666667">
Our(win) Hybhsum(win) Tie
Overall 37 14 9
Focus 32 18 10
Responsiveness 33 13 14
Non-redundancy 13 11 36
Responsiveness, S-sLDA model outputs Hybhsum
</table>
<bodyText confidence="0.998098372093024">
based on t-test on 95% confidence level. Ta-
ble 6 shows the example summaries generated re-
spectively by two models for document collection
D0803A-A in TAC2008, whose query is “Describe
the coal mine accidents in China and actions taken“.
From table 6, we can see that each sentence in these
two summaries is somewhat related to topics of coal
mines in China. We also observe that the summary
in Table 6(a) is better than that in Table 6(b), tend-
ing to select shorter sentences and provide more in-
formation. This is because, in S-sLDA model, topic
modeling is determined simultaneously by various
features including terms and other ones such as sen-
tence length, sentence position and so on, which
can contribute to summary quality. As we can see,
in Table 6(b), sentences (3) and (5) provide some
unimportant information such as “somebody said“,
though they contain some words which are related
to topics about coal mines.
(1)China to close at least 4,000 coal mines this year:
official (2)By Oct. 10 this year there had been 43 coal
mine accidents that killed 10 or more people, (3)Offi-
cials had stakes in coal mines. (4)All the coal mines
will be closed down this year. (5) In the first eight
months, the death toll of coal mine accidents rose
8.5 percent last year. (6) The government has issued
a series of regulations and measures to improve the
coun.try’s coal mine safety situation. (7)The mining
safety technology and equipments have been sold to
countries. (8)More than 6,000 miners died in accidents
in China
(1) In the first eight months, the death toll of coal mine
accidents across China rose 8.5 percent from the same
period last year. (2)China will close down a number of
ill-operated coal mines at the end of this month, said
a work safety official here Monday. (3) Li Yizhong,
director of the National Bureau of Production Safety
Supervision and Administration, has said the collusion
between mine owners and officials is to be condemned.
(4)from January to September this year, 4,228 people
were killed in 2,337 coal mine accidents. (5) Chen
said officials who refused to register their stakes in
coal mines within the required time
</bodyText>
<tableCaption confidence="0.999476">
Table 6: Example summary text generated by systems
(a)S-sLDA and (b) Hybhsum. (D0803A-A, TAC2008)
Table 5: Comparison with baselines on TAC2009
</tableCaption>
<page confidence="0.995075">
96
</page>
<sectionHeader confidence="0.99848" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999970571428571">
In this paper, we propose a novel supervised ap-
proach based on revised supervised topic model for
query-focused multi document summarization. Our
approach naturally combines Bayesian topic model
with supervised method and enjoy the advantages of
both models. Experiments on benchmark demon-
strate good performance of our model.
</bodyText>
<sectionHeader confidence="0.996885" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99925725">
This research work has been supported by
NSFC grants (No.90920011 and No.61273278),
National Key Technology R&amp;D Program
(No:2011BAH1B0403), and National High Tech-
nology R&amp;D Program (No.2012AA011101). We
also thank the three anonymous reviewers for their
helpful comments. Corresponding author: Sujian
Li.
</bodyText>
<sectionHeader confidence="0.997131" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99634406097561">
David Blei and Jon McAuliffe. Supervised topic models.
2007. In Neural Information Processing Systems
David Blei, Andrew Ng and Micheal Jordan. Latent
dirichlet allocation. In The Journal of Machine Learn-
ing Research, page: 993-1022.
Charles Broyden. 1965. A class of methods for solv-
ing nonlinear simultaneous equations. In Math. Comp.
volume 19, page 577-593.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A Hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics. page:
815-825
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal and
Jaime Carbonell. 1999. Summarizing Text Docu-
ments: Sentence Selection and Evaluation Metrics. In
Proceedings of the 22nd annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, page: 121-128.
Amit Grubber, Micheal Rosen-zvi and Yair Weiss. 2007.
Hidden Topic Markov Model. In Artificial Intelligence
and Statistics.
Hal Daume and Daniel Marcu H. 2006. Bayesian Query-
Focused Summarization. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, page 305-312.
Gune Erkan and Dragomir Radev. 2004. Lexrank: graph-
based lexical centrality as salience in text summariza-
tion. In J. Artif. Intell. Res. (JAIR), page 457-479.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, The ICSI
Summarization System at TAC, TAC 2008.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur,
Berndt Bohnet, Yang Liu, Shasha Xie. The ICSI/UTD
Summarization System at TAC 2009. TAC 2009
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362370.
Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. The
summarization systems at tac 2010. In Proceedings of
the third TextAnalysis Conference, TAC-2010.
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha and
Yong Yu. 2009. Enhancing diversity, coverage and bal-
ance for summarization through structure learning. In
Proceedings of the 18th international conference on
World wide web, page 71-80.
Chin-Yew Lin, Guihong Gao, Jianfeng Gao and Jian-Yun
Nie. 2006. An information-theoretic approach to au-
tomatic evaluation of summaries. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, page:462-470.
Annie Louis, Aravind Joshi, Ani Nenkova. 2010. Dis-
course indicators for content selection in summariza-
tion. In Proceedings of the 11th Annual Meeting of
the Special Interest Group on Discourse and Dialogue,
page:147-156.
Tengfei Ma, Xiaojun Wan. 2010. Multi-document sum-
marization using minimum distortion, in Proceedings
of International Conference of Data Mining. page
354363.
Rebecca Mason and Eugene Charniak. 2011. Extractive
multi-document summaries should explicitly not con-
tain document-specific content. In proceedings of ACL
HLT, page:49-54.
Ani Nenkova and Lucy Vanderwende. The impact of fre-
quency on summarization. In Tech. Report MSR-TR-
2005-101, Microsoft Research, Redwood, Washing-
ton, 2005.
Ani Nenkova, Lucy Vanderwende and Kathleen McKe-
own. 2006. A compositional context sensitive multi-
document summarizer: exploring the factors that inu-
ence summarization. In Proceedings of the 29th an-
nual International ACM SIGIR Conference on Re-
</reference>
<page confidence="0.993108">
97
</page>
<reference confidence="0.999650059701493">
search and Development in Information Retrieval,
page 573-580.
Miles Osborne. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Work-
shop on Automatic Summarization, Volume 4 page:1-
8.
Jahna Otterbacher, Gunes Erkan and Dragomir Radev.
2005. Using random walks for question-focused sen-
tence retrieval. In Proceedings of the Conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, page 915-922
You Ouyang, Wenjie Li, Sujian Li and Qin Lua. 2011.
Applying regression models to query-focused multi-
document summarization. In Information Processing
and Management, page 227-237.
You Ouyang, Sujian. Li, and Wenjie. Li. 2007, Develop-
ing learning strategies for topic-based summarization.
In Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, page: 7986.
Daniel Ramage, David Hall, Ramesh Nallapati and
Christopher Manning. 2009. Labeled LDA: A super-
vised topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
Vol 1, page 248-256.
Dou She, Jian-Tao Sun, Hua Li, Qiang Yang and
Zheng Chen. 2007. Document summarization using
conditional random elds. In Proceedings of Inter-
national Joint Conference on Artificial Intelligence,
page: 28622867.
V. Varma, V. Bharat, S. Kovelamudi, P. Bysani, S. GSK,
K. Kumar N, K. Reddy, N. Maganti , IIIT Hyderabad
at TAC 2009. TAC2009
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
Summarization using cluster-based link analysis. In
Proceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, page: 299-306.
Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2007.
Manifold-ranking based topic-focused multi-
document summarization. In Proceedings of In-
ternational Joint Conference on Artificial Intelligence,
page 2903-2908.
Furu Wei, Wenjie Li, Qin Lu and Yanxiang He. 2008. Ex-
ploiting Query-Sensitive Similarity for Graph-Based
Query-Oriented Summarization. In Proceedings of the
31st annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
page 283-290.
Jin Zhang, Xueqi Cheng, Hongbo Xu, Xiaolei Wang, Yil-
ing Zeng. ICTCAS’s ICTGrasper at TAC 2008: Sum-
marizing Dynamic Information with Signature Terms
Based Content Filtering, TAC 2008.
Dengzhong Zhou, Jason Weston, Arthur Gretton, Olivier
Bousquet and Bernhard Schlkopf. 2003. Ranking on
Data Manifolds. In Proceedings of the Conference on
Advances in Neural Information Processing Systems,
page 169-176.
Jun Zhu and Eric Xing. 2010. Conditional Topic Random
Fields. In Proceedings of the 27th International Con-
ference on Machine Learning.
Xiaojin Zhu, Zoubin Ghahramani and John Laf-
ferty. 2003. Semi-supervised Learning using Gaussian
Fields and Harmonic Functions. In Proceedings of In-
ternational Conference of Machine Learning, page:
912-919.
</reference>
<page confidence="0.996232">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.827426">
<title confidence="0.9993445">A Novel Feature-based Bayesian Model for Query Focused Multi-document Summarization</title>
<author confidence="0.999595">Jiwei Li Sujian Li</author>
<affiliation confidence="0.9999465">School of Computer Science Laboratory of Computational Linguistics Carnegie Mellon University Peking University</affiliation>
<email confidence="0.86253">bdlijiwei@gmail.comlisujian@pku.edu.cn</email>
<abstract confidence="0.996255363636363">Supervised learning methods and LDA based topic model have been successfully applied in the field of multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experimental results on DUC2007, TAC2008 and TAC2009 demonstrate the effectiveness of our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Jon McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2007</date>
<booktitle>In Neural Information Processing Systems</booktitle>
<contexts>
<context position="8183" citStr="Blei and McAuliffe 2007" startWordPosition="1281" endWordPosition="1284">documents in the corpus and V be vocabulary size. The topic distribution of each document Bm is drawn from a prior Dirichlet distribution Dir(α), and each document word wmn is sampled from a topic-word distribution φz specified by a drawn from the topic-document distribution Bm. o is a K × M dimensional matrix and each ok is a distribution over the V terms. The generating procedure of LDA is illustrated in Figure 2. Bm is a mixture proportion over topics of document m and zmn is a K dimensional variable that presents the topic assignment distribution of different words. Supervised LDA (sLDA) (Blei and McAuliffe 2007) is a document feature based model and intro90 Figure 1: Graphical models for (a) LDA model and (b) sLDA model. 1. Draw a document proportion vector θm|α ∼ Dir(α) 2. For each word in m (a)draw topic assignment zmn|θ ∼ Multi(θzmn) (b)draw word wmn|zmn, β ∼ Multi(βzmn) Figure 2: Generation process for LDA duces a response variable to each document for topic discovering, as shown in Figure 1(b). In the generative procedure of sLDA, the document pairwise label is draw from y|−→zm, η, δ2 ∼ p(y|−→zm, η, δ2), where −→ = 1 N zm N Pn=1 zm,n. 3.2 Problem Formulation Here we firstly give a standard formu</context>
</contexts>
<marker>Blei, McAuliffe, 2007</marker>
<rawString>David Blei and Jon McAuliffe. Supervised topic models. 2007. In Neural Information Processing Systems</rawString>
</citation>
<citation valid="false">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Micheal Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<booktitle>In The Journal of Machine Learning Research,</booktitle>
<pages>993--1022</pages>
<marker>Blei, Ng, Jordan, </marker>
<rawString>David Blei, Andrew Ng and Micheal Jordan. Latent dirichlet allocation. In The Journal of Machine Learning Research, page: 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Broyden</author>
</authors>
<title>A class of methods for solving nonlinear simultaneous equations.</title>
<date>1965</date>
<journal>In Math. Comp.</journal>
<volume>19</volume>
<pages>577--593</pages>
<marker>Broyden, 1965</marker>
<rawString>Charles Broyden. 1965. A class of methods for solving nonlinear simultaneous equations. In Math. Comp. volume 19, page 577-593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>A Hybrid hierarchical model for multi-document summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>815--825</pages>
<contexts>
<context position="2882" citStr="Celikyilmaz and Hakkani-Tur (2010)" startWordPosition="436" endWordPosition="439">s that it only uses word frequency for topic modeling and can not use useful text features such as position, word order etc (Zhu and Xing, 2010). For example, the first sentence in a document may be more important for summary since it is more likely to give a global generalization about the document. It is hard for LDA model to consider such information, making useful information lost. It naturally comes to our minds that we can improve summarization performance by making full use of both useful text features and the latent semantic structures from by LDA topic model. One related work is from Celikyilmaz and Hakkani-Tur (2010). They built a hierarchical topic model called Hybhsum based on LDA for topic discovery and assumed this model can produce appropriate scores for sentence evaluation. Then the scores are used for tuning the weights of various features that helpful for summary generation. Their work made a good step of combining topic model with feature based supervised learning. However, what their approach confuses us is that whether a topic model only based on word frequency is good enough to generate an appropriate sentence score for regression. Actually, how to incorporate features into LDA topic model has</context>
<context position="6311" citStr="Celikyilmaz and Hakkani-Tur (2010)" startWordPosition="958" endWordPosition="961">onal Random Field (Shen et al., 2007) and regression models (Ouyang et al., 2010) have been adopted to leverage the rich sentence features for summarization. Recently, Bayesian topic models have shown their power in summarization for its clear probabilistic interpretation. Daume and Marcu (2006) proposed Bayesum model for sentence extraction based on query expansion concept in information retrieval. Haghighi and Vanderwende (2009) proposed topicsum and hiersum which use a LDA-like topic model and assign each sentence a distribution over background topic, doc-specific topic and content topics. Celikyilmaz and Hakkani-Tur (2010) made a good step in combining topic model with supervised feature based regression for sentence scoring in summarization. In their model, the score of training sentences are firstly got through a novel hierarchical topic model. Then a featured based support vector regression (SVR) is used for sentence score prediction. The problem of Celikyilmaz and HakkaniTurs model is that topic model and feature based regression are two separate processes and the score of training sentences may be biased because their topic model only consider word frequency and fail to consider other important features. S</context>
<context position="16382" citStr="Celikyilmaz and Hakkani-Tur 2010" startWordPosition="2743" endWordPosition="2746">sine Similarity with query: Cosine similarity is based on the tf-idf value of terms. 2This is reasonable because the influence of γ and β have been embodied in φ during each iteration. Local Inner-document Degree Order: Local Inner document Degree Order is a binary feature which indicates whether Inner-document Degree (IDD) of sentence s is the largest among its neighbors. IDD means the edge number between s and other sentences in the same document. Document Specific Word: 1 if a sentence contains document specific word, 0 otherwise. Average Unigram Probability (Nenkova and Vanderwende, 2005; Celikyilmaz and Hakkani-Tur 2010): As for sentence s, p(s) = PwEs|3|pD(w), where pD(w) is the observed unigram probability in document collection. In addition, we also use the commonly used features including sentence position, paragraph position, sentence length and sentence bigram frequency. E-step initialize φ0sk := 1/K for all i and s. initialize γmi := αmi + N)m/K for all i. initialize ηkt = 0 for all k and t. while not convergence form = 1 : M update γt+1 m according to Eqn.(6) for s = 1 : Nm fork = 1 : K updateφt+1 sk according to Eqn.(7) normalize the sum of φt+1 sk to 1. Minimize L(η) according to Eqn.(11)-(15). M-st</context>
<context position="21236" citStr="Celikyilmaz and Hakkani-Tur (2010)" startWordPosition="3573" endWordPosition="3576">r model with the following Bayesian baselines: KL-sum: It is developed by Haghighi and Vanderwende (Lin et al., 2006) by using a KLdivergence based sentence selection strategy. � P (w) KL(Ps||Qd) = P(w)log Q(w) (19) w where Ps is the unigram distribution of candidate summary and Qd denotes the unigram distribution of document collection. Sentences with higher ranking score is selected into the summary. HierSum: A LDA based approach proposed by Haghighi and Vanderwende (2009), where unigram distribution is calculated from LDA topic model in Equ.(14). Hybhsum: A supervised approach developed by Celikyilmaz and Hakkani-Tur (2010). For fair comparison, baselines use the same proprecessing methods with our model and all sum5Here, we only use the docset-A data in TAC, since TAC data is composed of docset-A and docset-B data, and the docsetB data is mainly for the update summarization task. 6http://tartarus.org/ martin/PorterStemmer/. 7Jackknife scoring for ROUGE is used in order to compare with the human summaries. 94 maries are truncated to the same length of 100 words. From Table 1 and Table 2, we can Methods ROUGE-1 ROUGE-2 ROUGE-SU4 Our 0.3724 0.1030 0.1342 approach (0.3660-0.3788) (0.0999-0.1061) (0.1290-0.1394) Hyb</context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, 2010</marker>
<rawString>Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A Hybrid hierarchical model for multi-document summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. page: 815-825</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
</authors>
<title>Summarizing Text Documents: Sentence Selection and Evaluation Metrics.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="18615" citStr="Goldstein et al., 1999" startWordPosition="3160" endWordPosition="3163">m in section 3.3 for inference. (The only difference between this step and step (1) is that in this step we do not need minimize L(η). 3. Select sentences for summarization from algorithm in Figure 6. Figure 6: Summarization Generation by S-sLDA. A greedy algorithm is applied by adding sentence one by one to obtain Sum*. We use G to denote the sentence set containing selected sentences. The algorithm first initializes G to Φ and X to SU. During each iteration, we select one sentence from X which maximize Score(sm U G). To avoid topic redundancy in the summary, we also revise the MMR strategy (Goldstein et al., 1999; Ouyang et al., 2007) in the process of sentence selection. For each sm, we compute the semantic similarity between sm and each sentence st in set Y in Eqn.(18). cos − sem (sm, st) = Ek φsm kφstk�2 (18) o �Ek sm k&apos; Ek stk We need to assure that the value of semantic similarity between two sentences is less than Thsem. The whole procedure for summarization using S-sLDA model is illustrated in Figure 6. Thsem is set to 0.5 in the experiments. 5 Experiments 5.1 Experiments Set-up The query-focused multi-document summarization task defined in DUC3(Document Understanding Conference) and TAC4(Text </context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Jade Goldstein, Mark Kantrowitz, Vibhu Mittal and Jaime Carbonell. 1999. Summarizing Text Documents: Sentence Selection and Evaluation Metrics. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, page: 121-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Grubber</author>
<author>Micheal Rosen-zvi</author>
<author>Yair Weiss</author>
</authors>
<title>Hidden Topic Markov Model.</title>
<date>2007</date>
<booktitle>In Artificial Intelligence and Statistics.</booktitle>
<marker>Grubber, Rosen-zvi, Weiss, 2007</marker>
<rawString>Amit Grubber, Micheal Rosen-zvi and Yair Weiss. 2007. Hidden Topic Markov Model. In Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
<author>Daniel Marcu H</author>
</authors>
<title>Bayesian QueryFocused Summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>305--312</pages>
<marker>Daume, H, 2006</marker>
<rawString>Hal Daume and Daniel Marcu H. 2006. Bayesian QueryFocused Summarization. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, page 305-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gune Erkan</author>
<author>Dragomir Radev</author>
</authors>
<title>Lexrank: graphbased lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>In J. Artif. Intell. Res. (JAIR),</journal>
<pages>457--479</pages>
<contexts>
<context position="23809" citStr="Erkan and Radev, 2004" startWordPosition="3939" endWordPosition="3942">evements are significant, because in the TAC2008 evaluation, the performance of the top ranking systems are very close, i.e. the best system is only 4.2% above the 4th best system on ROUGE2 and 1.2% on ROUGE-SU4. 5.3 Comparison with other baselines. In this subsection, we compare our model with some widely used models in summarization. Manifold: It is the one-layer graph based semisupervised summarization approach developed by Wan et al.(2008). The graph is constructed only considering sentence relations using tf-idf and neglects topic information. LexRank: Graph based summarization approach (Erkan and Radev, 2004), which is a revised version of famous web ranking algorithm PageRank. It is an unsupervised ranking algorithms compared with Manifold. SVM: A supervised method - Support Vector Machine (SVM) (Vapnik 1995) which uses the same features as our approach. MEAD: A centroid based summary algorithm by Radev et al. (2004). Cluster centroids in MEAD consists of words which are central not only to one article in a cluster, but to all the articles. Similarity is measure using tf-idf. At the same time, we also present the top three participating systems with regard to ROUGE-2 on TAC2008 and TAC2009 for co</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Gune Erkan and Dragomir Radev. 2004. Lexrank: graphbased lexical centrality as salience in text summarization. In J. Artif. Intell. Res. (JAIR), page 457-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
</authors>
<title>Benoit Favre, Dilek Hakkani-Tur, The ICSI Summarization System at TAC, TAC</title>
<date>2008</date>
<marker>Gillick, 2008</marker>
<rawString>Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, The ICSI Summarization System at TAC, TAC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-Tur</author>
<author>Berndt Bohnet</author>
</authors>
<title>Yang Liu, Shasha Xie. The ICSI/UTD Summarization System at TAC</title>
<date>2009</date>
<publisher>TAC</publisher>
<contexts>
<context position="24530" citStr="Gillick et al., 2009" startWordPosition="4060" endWordPosition="4063">algorithms compared with Manifold. SVM: A supervised method - Support Vector Machine (SVM) (Vapnik 1995) which uses the same features as our approach. MEAD: A centroid based summary algorithm by Radev et al. (2004). Cluster centroids in MEAD consists of words which are central not only to one article in a cluster, but to all the articles. Similarity is measure using tf-idf. At the same time, we also present the top three participating systems with regard to ROUGE-2 on TAC2008 and TAC2009 for comparison, denoted as (denoted as SysRank 1st, 2nd and 3rd)(Gillick et al., 2008; Zhang et al., 2008; Gillick et al., 2009; Varma et al., 2009). The ROUGE scores of the top TAC system are directly provided by the TAC evaluation. From Table 3 and Table 4, we can see that our approach outperforms the baselines in terms of ROUGE metrics consistently. When compared with the standard supervised method SVM, the relative improvements over the ROUGE-1, ROUGE-2 and ROUGE-SU4 scores are 4.3%, 13.1%, 8.3% respectively on TAC2008 and 7.2%, 14.9%, 14.3% on TAC2009. Our model is not as good as top participating systems on TAC2008 and TAC2009. But considering the fact that our model neither uses sentence compression algorithm n</context>
</contexts>
<marker>Gillick, Favre, Hakkani-Tur, Bohnet, 2009</marker>
<rawString>Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur, Berndt Bohnet, Yang Liu, Shasha Xie. The ICSI/UTD Summarization System at TAC 2009. TAC 2009</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362370</pages>
<contexts>
<context position="1929" citStr="Haghighi and Vanderwende, 2009" startWordPosition="276" endWordPosition="279">ummarization as a classification or regression problem and use various sentence features to build a classifier based on labeled negative or positive samples. However, existing supervised approaches seldom exploit the intrinsic structure among sentences. This disadvantage usually gives rise to serious problems such as unbalance and low recall in summaries. Recently, LDA-based (Blei et al., 2003) Bayesian topic models have widely been applied in multidocument summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries(Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012). Exiting Bayesian approaches label sentences or words with topics and sentences which are closely related with query or can highly generalize documents are selected into summaries. However, LDA topic model suffers from the intrinsic disadvantages that it only uses word frequency for topic modeling and can not use useful text features such as position, word order etc (Zhu and Xing, 2010). For example, the first sentence in a document may be more important for summary since it is more likely to give a global generalization</context>
<context position="6111" citStr="Haghighi and Vanderwende (2009)" startWordPosition="928" endWordPosition="931">tion task as a sentence level two class classification problem. Supervised machine learning methods such as Support Vector Machine(SVM) (Li, et al., 2009), Maximum Entropy (Osborne, 2002) , Conditional Random Field (Shen et al., 2007) and regression models (Ouyang et al., 2010) have been adopted to leverage the rich sentence features for summarization. Recently, Bayesian topic models have shown their power in summarization for its clear probabilistic interpretation. Daume and Marcu (2006) proposed Bayesum model for sentence extraction based on query expansion concept in information retrieval. Haghighi and Vanderwende (2009) proposed topicsum and hiersum which use a LDA-like topic model and assign each sentence a distribution over background topic, doc-specific topic and content topics. Celikyilmaz and Hakkani-Tur (2010) made a good step in combining topic model with supervised feature based regression for sentence scoring in summarization. In their model, the score of training sentences are firstly got through a novel hierarchical topic model. Then a featured based support vector regression (SVR) is used for sentence score prediction. The problem of Celikyilmaz and HakkaniTurs model is that topic model and featu</context>
<context position="20461" citStr="Haghighi and Vanderwende, 2009" startWordPosition="3457" endWordPosition="3460">data and TAC (2008-2009) data is used as the test data. Stop-words in both documents and queries are removed using a stop-word list of 598 words, and the remaining words are stemmed by Porter Stemmer6. As for the automatic evaluation of summarization, ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures, including ROUGE1, ROUGE-2, and ROUGE-SU47 and their corresponding 95% confidence intervals, are used to evaluate the performance of the summaries. In order to obtain a more comprehensive measure of summary quality, we also conduct manual evaluation on TAC data with reference to (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2011; Delort and Alfonseca, 2011). 5.2 Comparison with other Bayesian models In this subsection, we compare our model with the following Bayesian baselines: KL-sum: It is developed by Haghighi and Vanderwende (Lin et al., 2006) by using a KLdivergence based sentence selection strategy. � P (w) KL(Ps||Qd) = P(w)log Q(w) (19) w where Ps is the unigram distribution of candidate summary and Qd denotes the unigram distribution of document collection. Sentences with higher ranking score is selected into the summary. HierSum: A LDA based approach proposed by Haghighi an</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Jin</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>The summarization systems at tac</title>
<date>2010</date>
<booktitle>In Proceedings of the third TextAnalysis Conference, TAC-2010.</booktitle>
<contexts>
<context position="1947" citStr="Jin et al., 2010" startWordPosition="280" endWordPosition="283"> or regression problem and use various sentence features to build a classifier based on labeled negative or positive samples. However, existing supervised approaches seldom exploit the intrinsic structure among sentences. This disadvantage usually gives rise to serious problems such as unbalance and low recall in summaries. Recently, LDA-based (Blei et al., 2003) Bayesian topic models have widely been applied in multidocument summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries(Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012). Exiting Bayesian approaches label sentences or words with topics and sentences which are closely related with query or can highly generalize documents are selected into summaries. However, LDA topic model suffers from the intrinsic disadvantages that it only uses word frequency for topic modeling and can not use useful text features such as position, word order etc (Zhu and Xing, 2010). For example, the first sentence in a document may be more important for summary since it is more likely to give a global generalization about the documen</context>
</contexts>
<marker>Jin, Huang, Zhu, 2010</marker>
<rawString>Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. The summarization systems at tac 2010. In Proceedings of the third TextAnalysis Conference, TAC-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangda Li</author>
<author>Ke Zhou</author>
<author>Gui-Rong Xue</author>
<author>Hongyuan Zha</author>
<author>Yong Yu</author>
</authors>
<title>Enhancing diversity, coverage and balance for summarization through structure learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th international conference on World wide web,</booktitle>
<pages>71--80</pages>
<contexts>
<context position="1222" citStr="Li, et al., 2009" startWordPosition="171" endWordPosition="174">rvised learning methods. Experimental results on DUC2007, TAC2008 and TAC2009 demonstrate the effectiveness of our approach. 1 Introduction Query-focused multi-document summarization (Nenkova et al., 2006; Wan et al., 2007; Ouyang et al., 2010) can facilitate users to grasp the main idea of documents. In query-focused summarization, a specific topic description, such as a query, which expresses the most important topic information is proposed before the document collection, and a summary would be generated according to the given topic. Supervised models have been widely used in summarization (Li, et al., 2009, Shen et al., 2007, Ouyang et al., 2010). Supervised models usually regard summarization as a classification or regression problem and use various sentence features to build a classifier based on labeled negative or positive samples. However, existing supervised approaches seldom exploit the intrinsic structure among sentences. This disadvantage usually gives rise to serious problems such as unbalance and low recall in summaries. Recently, LDA-based (Blei et al., 2003) Bayesian topic models have widely been applied in multidocument summarization in that Bayesian approaches can offer clear and</context>
<context position="5634" citStr="Li, et al., 2009" startWordPosition="860" endWordPosition="863">rizations such as unsupervised (semi-supervised) approaches, supervised approaches, and Bayesian approaches. Unsupervised (semi-supervised) approaches such as Lexrank (Erkan and Radex, 2004), manifold (Wan et al., 2007) treat summarization as a graphbased ranking problem. The relatedness between the query and each sentence is achieved by imposing querys influence on each sentence along with the propagation of graph. Most supervised approaches regard summarization task as a sentence level two class classification problem. Supervised machine learning methods such as Support Vector Machine(SVM) (Li, et al., 2009), Maximum Entropy (Osborne, 2002) , Conditional Random Field (Shen et al., 2007) and regression models (Ouyang et al., 2010) have been adopted to leverage the rich sentence features for summarization. Recently, Bayesian topic models have shown their power in summarization for its clear probabilistic interpretation. Daume and Marcu (2006) proposed Bayesum model for sentence extraction based on query expansion concept in information retrieval. Haghighi and Vanderwende (2009) proposed topicsum and hiersum which use a LDA-like topic model and assign each sentence a distribution over background top</context>
</contexts>
<marker>Li, Zhou, Xue, Zha, Yu, 2009</marker>
<rawString>Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha and Yong Yu. 2009. Enhancing diversity, coverage and balance for summarization through structure learning. In Proceedings of the 18th international conference on World wide web, page 71-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Guihong Gao</author>
<author>Jianfeng Gao</author>
<author>Jian-Yun Nie</author>
</authors>
<title>An information-theoretic approach to automatic evaluation of summaries.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>462--470</pages>
<contexts>
<context position="20719" citStr="Lin et al., 2006" startWordPosition="3495" endWordPosition="3498">ted Understudy for Gisting Evaluation) measures, including ROUGE1, ROUGE-2, and ROUGE-SU47 and their corresponding 95% confidence intervals, are used to evaluate the performance of the summaries. In order to obtain a more comprehensive measure of summary quality, we also conduct manual evaluation on TAC data with reference to (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2011; Delort and Alfonseca, 2011). 5.2 Comparison with other Bayesian models In this subsection, we compare our model with the following Bayesian baselines: KL-sum: It is developed by Haghighi and Vanderwende (Lin et al., 2006) by using a KLdivergence based sentence selection strategy. � P (w) KL(Ps||Qd) = P(w)log Q(w) (19) w where Ps is the unigram distribution of candidate summary and Qd denotes the unigram distribution of document collection. Sentences with higher ranking score is selected into the summary. HierSum: A LDA based approach proposed by Haghighi and Vanderwende (2009), where unigram distribution is calculated from LDA topic model in Equ.(14). Hybhsum: A supervised approach developed by Celikyilmaz and Hakkani-Tur (2010). For fair comparison, baselines use the same proprecessing methods with our model </context>
</contexts>
<marker>Lin, Gao, Gao, Nie, 2006</marker>
<rawString>Chin-Yew Lin, Guihong Gao, Jianfeng Gao and Jian-Yun Nie. 2006. An information-theoretic approach to automatic evaluation of summaries. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, page:462-470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Aravind Joshi</author>
<author>Ani Nenkova</author>
</authors>
<title>Discourse indicators for content selection in summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>147--156</pages>
<contexts>
<context position="15551" citStr="Louis et al., 2010" startWordPosition="2610" endWordPosition="2613">gence from Query. The case is just opposite for undesired set. Our idea is to incorporate the minimization process of Eqn.(11) into variational inference process of S-sLDA model. Here we perform gradient based optimization method to minimize Eqn.(11). Firstly, we derive the gradient of L(η) with respect to η. ∂KL(Qv||Q) v · (13) ∂ηxy (14) For simplification, we regard β and γ as constant during updating process of η, soth a��� = 0.2 We can further get first derivative for each labeled sentence. 0 if k =6x (15) 4.2 Feature Space Lots of features have been proven to be useful for summarization (Louis et al., 2010). Here we discuss several types of features which are adopted in S-sLDA model. The feature values are either binary or normalized to the interval [0,1]. The following features are used in S-sLDA: Cosine Similarity with query: Cosine similarity is based on the tf-idf value of terms. 2This is reasonable because the influence of γ and β have been embodied in φ during each iteration. Local Inner-document Degree Order: Local Inner document Degree Order is a binary feature which indicates whether Inner-document Degree (IDD) of sentence s is the largest among its neighbors. IDD means the edge number </context>
</contexts>
<marker>Louis, Joshi, Nenkova, 2010</marker>
<rawString>Annie Louis, Aravind Joshi, Ani Nenkova. 2010. Discourse indicators for content selection in summarization. In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page:147-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tengfei Ma</author>
<author>Xiaojun Wan</author>
</authors>
<title>Multi-document summarization using minimum distortion, in</title>
<date>2010</date>
<booktitle>Proceedings of International Conference of Data Mining.</booktitle>
<pages>354363</pages>
<marker>Ma, Wan, 2010</marker>
<rawString>Tengfei Ma, Xiaojun Wan. 2010. Multi-document summarization using minimum distortion, in Proceedings of International Conference of Data Mining. page 354363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Mason</author>
<author>Eugene Charniak</author>
</authors>
<title>Extractive multi-document summaries should explicitly not contain document-specific content.</title>
<date>2011</date>
<booktitle>In proceedings of ACL HLT,</booktitle>
<pages>49--54</pages>
<contexts>
<context position="1973" citStr="Mason and Charniak, 2011" startWordPosition="284" endWordPosition="287">blem and use various sentence features to build a classifier based on labeled negative or positive samples. However, existing supervised approaches seldom exploit the intrinsic structure among sentences. This disadvantage usually gives rise to serious problems such as unbalance and low recall in summaries. Recently, LDA-based (Blei et al., 2003) Bayesian topic models have widely been applied in multidocument summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries(Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012). Exiting Bayesian approaches label sentences or words with topics and sentences which are closely related with query or can highly generalize documents are selected into summaries. However, LDA topic model suffers from the intrinsic disadvantages that it only uses word frequency for topic modeling and can not use useful text features such as position, word order etc (Zhu and Xing, 2010). For example, the first sentence in a document may be more important for summary since it is more likely to give a global generalization about the document. It is hard for LDA mode</context>
</contexts>
<marker>Mason, Charniak, 2011</marker>
<rawString>Rebecca Mason and Eugene Charniak. 2011. Extractive multi-document summaries should explicitly not contain document-specific content. In proceedings of ACL HLT, page:49-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
</authors>
<title>The impact of frequency on summarization. In</title>
<date>2005</date>
<tech>Tech. Report MSR-TR2005-101,</tech>
<institution>Microsoft Research,</institution>
<location>Redwood, Washington,</location>
<contexts>
<context position="16347" citStr="Nenkova and Vanderwende, 2005" startWordPosition="2738" endWordPosition="2742">features are used in S-sLDA: Cosine Similarity with query: Cosine similarity is based on the tf-idf value of terms. 2This is reasonable because the influence of γ and β have been embodied in φ during each iteration. Local Inner-document Degree Order: Local Inner document Degree Order is a binary feature which indicates whether Inner-document Degree (IDD) of sentence s is the largest among its neighbors. IDD means the edge number between s and other sentences in the same document. Document Specific Word: 1 if a sentence contains document specific word, 0 otherwise. Average Unigram Probability (Nenkova and Vanderwende, 2005; Celikyilmaz and Hakkani-Tur 2010): As for sentence s, p(s) = PwEs|3|pD(w), where pD(w) is the observed unigram probability in document collection. In addition, we also use the commonly used features including sentence position, paragraph position, sentence length and sentence bigram frequency. E-step initialize φ0sk := 1/K for all i and s. initialize γmi := αmi + N)m/K for all i. initialize ηkt = 0 for all k and t. while not convergence form = 1 : M update γt+1 m according to Eqn.(6) for s = 1 : Nm fork = 1 : K updateφt+1 sk according to Eqn.(7) normalize the sum of φt+1 sk to 1. Minimize L(</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>Ani Nenkova and Lucy Vanderwende. The impact of frequency on summarization. In Tech. Report MSR-TR2005-101, Microsoft Research, Redwood, Washington, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
<author>Kathleen McKeown</author>
</authors>
<title>A compositional context sensitive multidocument summarizer: exploring the factors that inuence summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>573--580</pages>
<contexts>
<context position="810" citStr="Nenkova et al., 2006" startWordPosition="105" endWordPosition="108">ersity Peking University bdlijiwei@gmail.com lisujian@pku.edu.cn Abstract Supervised learning methods and LDA based topic model have been successfully applied in the field of multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experimental results on DUC2007, TAC2008 and TAC2009 demonstrate the effectiveness of our approach. 1 Introduction Query-focused multi-document summarization (Nenkova et al., 2006; Wan et al., 2007; Ouyang et al., 2010) can facilitate users to grasp the main idea of documents. In query-focused summarization, a specific topic description, such as a query, which expresses the most important topic information is proposed before the document collection, and a summary would be generated according to the given topic. Supervised models have been widely used in summarization (Li, et al., 2009, Shen et al., 2007, Ouyang et al., 2010). Supervised models usually regard summarization as a classification or regression problem and use various sentence features to build a classifier </context>
</contexts>
<marker>Nenkova, Vanderwende, McKeown, 2006</marker>
<rawString>Ani Nenkova, Lucy Vanderwende and Kathleen McKeown. 2006. A compositional context sensitive multidocument summarizer: exploring the factors that inuence summarization. In Proceedings of the 29th annual International ACM SIGIR Conference on Research and Development in Information Retrieval, page 573-580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
</authors>
<title>Using maximum entropy for sentence extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Automatic Summarization, Volume</booktitle>
<volume>4</volume>
<pages>1--8</pages>
<contexts>
<context position="5667" citStr="Osborne, 2002" startWordPosition="867" endWordPosition="868">-supervised) approaches, supervised approaches, and Bayesian approaches. Unsupervised (semi-supervised) approaches such as Lexrank (Erkan and Radex, 2004), manifold (Wan et al., 2007) treat summarization as a graphbased ranking problem. The relatedness between the query and each sentence is achieved by imposing querys influence on each sentence along with the propagation of graph. Most supervised approaches regard summarization task as a sentence level two class classification problem. Supervised machine learning methods such as Support Vector Machine(SVM) (Li, et al., 2009), Maximum Entropy (Osborne, 2002) , Conditional Random Field (Shen et al., 2007) and regression models (Ouyang et al., 2010) have been adopted to leverage the rich sentence features for summarization. Recently, Bayesian topic models have shown their power in summarization for its clear probabilistic interpretation. Daume and Marcu (2006) proposed Bayesum model for sentence extraction based on query expansion concept in information retrieval. Haghighi and Vanderwende (2009) proposed topicsum and hiersum which use a LDA-like topic model and assign each sentence a distribution over background topic, doc-specific topic and conten</context>
</contexts>
<marker>Osborne, 2002</marker>
<rawString>Miles Osborne. 2002. Using maximum entropy for sentence extraction. In Proceedings of the ACL-02 Workshop on Automatic Summarization, Volume 4 page:1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
</authors>
<title>Gunes Erkan and Dragomir Radev.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>915--922</pages>
<marker>Otterbacher, 2005</marker>
<rawString>Jahna Otterbacher, Gunes Erkan and Dragomir Radev. 2005. Using random walks for question-focused sentence retrieval. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, page 915-922</rawString>
</citation>
<citation valid="true">
<authors>
<author>You Ouyang</author>
<author>Wenjie Li</author>
<author>Sujian Li</author>
<author>Qin Lua</author>
</authors>
<title>Applying regression models to query-focused multidocument summarization.</title>
<date>2011</date>
<booktitle>In Information Processing and Management,</booktitle>
<pages>227--237</pages>
<marker>Ouyang, Li, Li, Lua, 2011</marker>
<rawString>You Ouyang, Wenjie Li, Sujian Li and Qin Lua. 2011. Applying regression models to query-focused multidocument summarization. In Information Processing and Management, page 227-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li</author>
<author>Wenjie Li</author>
</authors>
<title>Developing learning strategies for topic-based summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>7986</pages>
<marker>Li, Li, 2007</marker>
<rawString>You Ouyang, Sujian. Li, and Wenjie. Li. 2007, Developing learning strategies for topic-based summarization. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, page: 7986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher Manning</author>
</authors>
<title>Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<volume>1</volume>
<pages>248--256</pages>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati and Christopher Manning. 2009. Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Vol 1, page 248-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou She</author>
<author>Jian-Tao Sun</author>
<author>Hua Li</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Document summarization using conditional random elds.</title>
<date>2007</date>
<booktitle>In Proceedings of International Joint Conference on Artificial Intelligence,</booktitle>
<pages>28622867</pages>
<marker>She, Sun, Li, Yang, Chen, 2007</marker>
<rawString>Dou She, Jian-Tao Sun, Hua Li, Qiang Yang and Zheng Chen. 2007. Document summarization using conditional random elds. In Proceedings of International Joint Conference on Artificial Intelligence, page: 28622867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Varma</author>
<author>V Bharat</author>
<author>S Kovelamudi</author>
<author>P Bysani</author>
<author>S GSK</author>
<author>K Kumar N</author>
<author>K Reddy</author>
<author>N Maganti</author>
</authors>
<title>IIIT Hyderabad at TAC</title>
<date>2009</date>
<pages>2009</pages>
<contexts>
<context position="24551" citStr="Varma et al., 2009" startWordPosition="4064" endWordPosition="4067">th Manifold. SVM: A supervised method - Support Vector Machine (SVM) (Vapnik 1995) which uses the same features as our approach. MEAD: A centroid based summary algorithm by Radev et al. (2004). Cluster centroids in MEAD consists of words which are central not only to one article in a cluster, but to all the articles. Similarity is measure using tf-idf. At the same time, we also present the top three participating systems with regard to ROUGE-2 on TAC2008 and TAC2009 for comparison, denoted as (denoted as SysRank 1st, 2nd and 3rd)(Gillick et al., 2008; Zhang et al., 2008; Gillick et al., 2009; Varma et al., 2009). The ROUGE scores of the top TAC system are directly provided by the TAC evaluation. From Table 3 and Table 4, we can see that our approach outperforms the baselines in terms of ROUGE metrics consistently. When compared with the standard supervised method SVM, the relative improvements over the ROUGE-1, ROUGE-2 and ROUGE-SU4 scores are 4.3%, 13.1%, 8.3% respectively on TAC2008 and 7.2%, 14.9%, 14.3% on TAC2009. Our model is not as good as top participating systems on TAC2008 and TAC2009. But considering the fact that our model neither uses sentence compression algorithm nor leverage domain kn</context>
</contexts>
<marker>Varma, Bharat, Kovelamudi, Bysani, GSK, N, Reddy, Maganti, 2009</marker>
<rawString>V. Varma, V. Bharat, S. Kovelamudi, P. Bysani, S. GSK, K. Kumar N, K. Reddy, N. Maganti , IIIT Hyderabad at TAC 2009. TAC2009</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
</authors>
<title>Multi-document Summarization using cluster-based link analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>299--306</pages>
<marker>Wan, Yang, 2008</marker>
<rawString>Xiaojun Wan and Jianwu Yang. 2008. Multi-document Summarization using cluster-based link analysis. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, page: 299-306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Manifold-ranking based topic-focused multidocument summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2903--2908</pages>
<contexts>
<context position="828" citStr="Wan et al., 2007" startWordPosition="109" endWordPosition="112">ty bdlijiwei@gmail.com lisujian@pku.edu.cn Abstract Supervised learning methods and LDA based topic model have been successfully applied in the field of multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experimental results on DUC2007, TAC2008 and TAC2009 demonstrate the effectiveness of our approach. 1 Introduction Query-focused multi-document summarization (Nenkova et al., 2006; Wan et al., 2007; Ouyang et al., 2010) can facilitate users to grasp the main idea of documents. In query-focused summarization, a specific topic description, such as a query, which expresses the most important topic information is proposed before the document collection, and a summary would be generated according to the given topic. Supervised models have been widely used in summarization (Li, et al., 2009, Shen et al., 2007, Ouyang et al., 2010). Supervised models usually regard summarization as a classification or regression problem and use various sentence features to build a classifier based on labeled n</context>
<context position="5236" citStr="Wan et al., 2007" startWordPosition="800" endWordPosition="803">ganized as follows. Section 2 describes some background and related works. Section 3 describes our details of S-sLDA model. Section 4 demonstrates details of our approaches, including learning, inference and summary generation. Section 5 provides experiments results and Section 6 concludes the paper. 2 Related Work A variety of approaches have been proposed for query-focused multi-document summarizations such as unsupervised (semi-supervised) approaches, supervised approaches, and Bayesian approaches. Unsupervised (semi-supervised) approaches such as Lexrank (Erkan and Radex, 2004), manifold (Wan et al., 2007) treat summarization as a graphbased ranking problem. The relatedness between the query and each sentence is achieved by imposing querys influence on each sentence along with the propagation of graph. Most supervised approaches regard summarization task as a sentence level two class classification problem. Supervised machine learning methods such as Support Vector Machine(SVM) (Li, et al., 2009), Maximum Entropy (Osborne, 2002) , Conditional Random Field (Shen et al., 2007) and regression models (Ouyang et al., 2010) have been adopted to leverage the rich sentence features for summarization. R</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2007. Manifold-ranking based topic-focused multidocument summarization. In Proceedings of International Joint Conference on Artificial Intelligence, page 2903-2908.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Furu Wei</author>
<author>Wenjie Li</author>
<author>Qin Lu</author>
<author>Yanxiang He</author>
</authors>
<title>Exploiting Query-Sensitive Similarity for Graph-Based Query-Oriented Summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>283--290</pages>
<marker>Wei, Li, Lu, He, 2008</marker>
<rawString>Furu Wei, Wenjie Li, Qin Lu and Yanxiang He. 2008. Exploiting Query-Sensitive Similarity for Graph-Based Query-Oriented Summarization. In Proceedings of the 31st annual International ACM SIGIR Conference on Research and Development in Information Retrieval, page 283-290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Zhang</author>
</authors>
<title>Xueqi Cheng, Hongbo Xu, Xiaolei Wang, Yiling Zeng. ICTCAS’s ICTGrasper at TAC 2008: Summarizing Dynamic Information with Signature Terms Based Content Filtering,</title>
<date>2008</date>
<location>TAC</location>
<marker>Zhang, 2008</marker>
<rawString>Jin Zhang, Xueqi Cheng, Hongbo Xu, Xiaolei Wang, Yiling Zeng. ICTCAS’s ICTGrasper at TAC 2008: Summarizing Dynamic Information with Signature Terms Based Content Filtering, TAC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengzhong Zhou</author>
<author>Jason Weston</author>
<author>Arthur Gretton</author>
<author>Olivier Bousquet</author>
<author>Bernhard Schlkopf</author>
</authors>
<title>Ranking on Data Manifolds.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Advances in Neural Information Processing Systems,</booktitle>
<pages>169--176</pages>
<marker>Zhou, Weston, Gretton, Bousquet, Schlkopf, 2003</marker>
<rawString>Dengzhong Zhou, Jason Weston, Arthur Gretton, Olivier Bousquet and Bernhard Schlkopf. 2003. Ranking on Data Manifolds. In Proceedings of the Conference on Advances in Neural Information Processing Systems, page 169-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Eric Xing</author>
</authors>
<title>Conditional Topic Random Fields.</title>
<date>2010</date>
<booktitle>In Proceedings of the 27th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="2392" citStr="Zhu and Xing, 2010" startWordPosition="352" endWordPosition="355">n in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries(Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012). Exiting Bayesian approaches label sentences or words with topics and sentences which are closely related with query or can highly generalize documents are selected into summaries. However, LDA topic model suffers from the intrinsic disadvantages that it only uses word frequency for topic modeling and can not use useful text features such as position, word order etc (Zhu and Xing, 2010). For example, the first sentence in a document may be more important for summary since it is more likely to give a global generalization about the document. It is hard for LDA model to consider such information, making useful information lost. It naturally comes to our minds that we can improve summarization performance by making full use of both useful text features and the latent semantic structures from by LDA topic model. One related work is from Celikyilmaz and Hakkani-Tur (2010). They built a hierarchical topic model called Hybhsum based on LDA for topic discovery and assumed this model</context>
<context position="7237" citStr="Zhu and Xing (2010)" startWordPosition="1114" endWordPosition="1117">re prediction. The problem of Celikyilmaz and HakkaniTurs model is that topic model and feature based regression are two separate processes and the score of training sentences may be biased because their topic model only consider word frequency and fail to consider other important features. Supervised feature based topic models have been proposed in recent years to incorporate different kinds of features into LDA model. Blei (2007) proposed sLDA for document response pairs and Daniel et al. (2009) proposed Labeled LDA by defining a one to one correspondence between latent topic and user tags. Zhu and Xing (2010) proposed conditional topic random field (CTRF) which addresses feature and independent limitation in LDA. 3 Model description 3.1 LDA and sLDA The hierarchical Bayesian LDA (Blei et al., 2003) models the probability of a corpus on hidden topics as shown in Figure 1(a). Let K be the number of topics , M be the number of documents in the corpus and V be vocabulary size. The topic distribution of each document Bm is drawn from a prior Dirichlet distribution Dir(α), and each document word wmn is sampled from a topic-word distribution φz specified by a drawn from the topic-document distribution Bm</context>
</contexts>
<marker>Zhu, Xing, 2010</marker>
<rawString>Jun Zhu and Eric Xing. 2010. Conditional Topic Random Fields. In Proceedings of the 27th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised Learning using Gaussian Fields and Harmonic Functions.</title>
<date>2003</date>
<booktitle>In Proceedings of International Conference of Machine Learning,</booktitle>
<pages>912--919</pages>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani and John Lafferty. 2003. Semi-supervised Learning using Gaussian Fields and Harmonic Functions. In Proceedings of International Conference of Machine Learning, page: 912-919.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>