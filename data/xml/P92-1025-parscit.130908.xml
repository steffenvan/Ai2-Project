<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000512">
<title confidence="0.81763">
MODELING NEGOTIATION SUBDIALOGUES1
</title>
<author confidence="0.976719">
Lynn Lambert and Sandra Carberry
</author>
<affiliation confidence="0.997476">
Department of Computer and Information Sciences
University of Delaware
</affiliation>
<address confidence="0.580349">
Newark, Delaware 19716, USA
</address>
<email confidence="0.999539">
email: lambert@cis.udel.edu, carberryecis.udel.edu
</email>
<sectionHeader confidence="0.994016" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999894133333333">
This paper presents a plan-based model that han-
dles negotiation subdialogues by inferring both the
communicative actions that people pursue when
speaking and the beliefs underlying these actions.
We contend that recognizing the complex dis-
course actions pursued in negotiation subdialogues
(e.g., expressing doubt) requires both a multi-
strength belief model and a process model that
combines different knowledge sources in a unified
framework. We show how our model identifies the
structure of negotiation subdialogues, including
recognizing expressions of doubt, implicit accep-
tance of communicated propositions, and negotia-
tion subdialogues embedded within other negotia-
tion subdialogues.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951304347826">
Since negotiation is an integral part of
multi-agent activity, a robust natural language un-
derstanding system must be able to handle subdi-
alogues in which participants negotiate what has
been claimed in order to try to come to some
agreement about those claims. To handle such
dialogues, the system must be able to recognize
when a dialogue participant has initiated a nego-
tiation subdialogue and why the participant began
the negotiation (i.e., what beliefs led the partici-
pant to start the negotiation). This paper presents
a plan-based model of task-oriented interactions
that assimilates negotiation subdialogues by in-
ferring both the communicative actions that peo-
ple pursue when speaking and the beliefs under-
lying these actions. We will argue that recogniz-
ing the complex discourse actions pursued in ne-
gotiation subdialogues (e.g., expressing doubt) re-
quires both a multi-strength belief model and a
processing strategy that combines different knowl-
edge sources in a unified framework, and we will
show how our model incorporates these and rec-
ognizes the structure of negotiation subdialogues.
</bodyText>
<sectionHeader confidence="0.997297" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.9993996">
Several researchers have built argument un-
derstanding systems, but none of these has ad-
dressed participants coming to an agreement or
mutual belief about a particular situation, ei-
ther because the arguments were only monologues
</bodyText>
<footnote confidence="0.928574666666667">
&apos;This work is being supported by the National Science
Foundation under Grant No. IRI-9122026. The Govern-
ment has certain rights in this material.
</footnote>
<bodyText confidence="0.99927652">
(Cohen, 1987; Cohen and Young, 1991), or be-
cause they assumed that dialogue participants do
not change their minds (Flowers, McGuire and
Birnbaum, 1982; Quilici, 1991). Others have ex-
amined more cooperative dialogues. Clark and
Schaefer (1989) contend that utterances must be
grounded, or understood, by both parties, but they
do not address conflicts in belief, only lack of un-
derstanding. Walker (1991) has shown that evi-
dence is often provided to ensure both understand-
ing and believing an utterance, but she does not
address recognizing lack of belief or lack of under-
standing. Reichman (1981) outlines a model for
informal debate, but does not provide a detailed
computational mechanism for recognizing the role
of each utterance in a debate.
In previous work (Lambert and Carberry,
1991), we described a tripartite plan-based model
of dialogue that recognizes and differentiates three
different kinds of actions: domain, problem-
solving, and discourse. Domain actions relate to
performing tasks in a given domain. We are mod-
eling cooperative dialogues in which one agent
has a domain goal and is working with another
helpful, more expert agent to determine what do-
main actions to perform in order to accomplish
this goal. Many researchers (Allen, 1979; Car-
berry, 1987; Goodman and Litman, 1992; Pol-
lack, 1990; Sidner, 1985) have shown that recog-
nition of domain plans and goals gives a system
the ability to address many difficult problems in
understanding. Problem-solving actions relate to
how the two dialogue participants are going about
building a plan to achieve the planning agent&apos;s
domain goal. Ramshaw, Litman, and Wilensky
(Ramshaw, 1991; Litman and Allen, 1987; Wilen-
sky, 1981) have noted the need for recognizing
problem-solving actions. Discourse actions are the
communicative actions that people perform in say-
ing something, e.g., asking a question or express-
ing doubt. Recognition of discourse actions pro-
vides expectations for subsequent utterances, and
explains the purpose of an utterance and how it
should be interpreted.
Our system&apos;s knowledge about how to per-
form actions is contained in a library of discourse,
problem-solving, and domain recipes (Pollack,
1990). Although domain recipes are not mutually
known by the participants (Pollack, 1990), how to
communicate and how to solve problems are corn-
</bodyText>
<page confidence="0.998409">
193
</page>
<table confidence="0.753143043478261">
Discourse Recipe-C3: {_agentl informs _agent2 of _prop}
Action: Inform(_agentl, _agent2, _prop)
Recipe-type: Decomposition
App Cond: believe(_agentl, _prop, [0:0])
believe(_agentl, believe(_agent2, _prop, [ON :S]) , [0:0])
Body: Tell(_agent1, _agent2, _prop)
Address-Believability(_agent2, _agentl, _prop)
Effects: believe(_agent2, want(_agentl, believe(_agent2, _prop, [C:C])), [C:C])
Goal: believe(_agent2, _prop, [C:C])
Discourse Recipe-C2:
{_agentl expresses doubt to _agent2 about _propl because _agentl believes _prop2 to be true}
Action: Express-Doubt(_agentl, _agent2, _propl, _prop2, _rule)
Recipe-type: Decomposition
App Cond: believe(_agentl, _prop2, [W:S])
believe(_agentl, believe(_agent2, _propl, [S:C]), [S:C])
believe(_agentl, ((_prop2 A _rule) -..propl), [S:C])
believe(_agentl, _rule, [S:C])
in-focus(_prop1))
Body: Convey-Uncertain-Belief(_agentl, _agent2, _prop2)
Address-Q-Acceptance(_agent2, _agentl, _prop2)
Effects: believe(_agent2, believe(_agentl, _propl, [SN :WN]), [S:C])
believe(_agent2, want(_agentl, Resolve-Conflict(_agent2, _agentl, _propl, _prop2)), [S:C])
Goal: want(_agent2, Resolve-Conflict(_agent2, _agentl, _propl, _prop2))
</table>
<figureCaption confidence="0.994944">
Figure 1. Two Sample Discourse Recipes
</figureCaption>
<bodyText confidence="0.999924677419355">
mon skills that people use in a wide variety of
contexts, so the system can assume that knowl-
edge about discourse and problem-solving recipes
is shared knowledge. Figure 1 contains two dis-
course recipes. Our representation of a recipe in-
cludes a header giving the name of the recipe and
the action that it accomplishes, preconditions, ap-
plicability conditions, constraints, a body, effects,
and a goal. Constraints limit the allowable instan-
tiation of variables in each of the components of
a recipe (Litman and Allen, 1987). Applicability
conditions (Carberry, 1987) represent conditions
that must be satisfied in order for the recipe to
be reasonable to apply in the given situation and,
in the case of many of our discourse recipes, the
applicability conditions capture beliefs that the di-
alogue participants must hold. Especially in the
case of discourse recipes, the goals and effects are
likely to be different. This allows us to differen-
tiate between illocutionary and perlocutionary ef-
fects and to capture the notion that one can, for
example, perform an inform act without the hearer
adopting the communicated proposition.&apos;
As actions are inferred by our process
model, a structure of the discourse is built which is
referred to as the Dialogue Model, or DM. In the
DM, discourse, problem-solving, and domain ac-
tions are each modeled on a separate level. Within
each of these levels, actions may contribute to
other actions in the dialogue, and this is captured
with specialization (Kautz and Allen, 1986), sub-
</bodyText>
<page confidence="0.461713">
2 Consider, for example, someone saying &amp;quot;I informed you
of X but you wouldn&apos;t believe me.&amp;quot;
</page>
<bodyText confidence="0.999979058823529">
action, and enablement arcs. Thus, actions at each
level form a tree structure in which each node rep-
resents an action that a participant is performing
and the children of a node represent actions pur-
sued in order to contribute to the parent action.
By using a tree structure to model actions at each
level and by allowing the tree structures to grow at
the root as well as at the leaves, we are able to in-
crementally recognize discourse, problem-solving,
and domain intentions, and can recognize the re-
lationship among several utterances that are all
part of the same higher-level discourse act even
when that act cannot be recognized from the first
utterance alone. Other advantages of our tripar-
tite model are discussed in Lambert and Carberry
(1991).
An action on one level in the DM may also
contribute to an action on an immediately higher
level. For example, discourse actions may be ex-
ecuted in order to obtain the information neces-
sary for performing a problem-solving action and
problem-solving actions may be executed in order
to construct a domain plan. We capture this with
links between actions on adjacent levels of the DM.
Figure 2 gives a DM built by our proto-
type system whose implementation is currently be-
ing expanded to include belief ascription and use
of linguistic information. It shows that a ques-
tion has been asked and answered, that this ques-
tion/answer pair contributes to the higher-level
discourse action of obtaining information about
what course Dr. Smith is teaching, that this dis-
course action enables the problem-solving action
of instantiating a parameter in a Learn-Material
</bodyText>
<page confidence="0.996554">
194
</page>
<figure confidence="0.61053">
Domain Level
</figure>
<figureCaption confidence="0.993682">
Figure 2. Dialogue Model for two utterances
</figureCaption>
<figure confidence="0.8702816">
- - -31•• = Enable Arc
aTalce-Course(SI, _course)
= Subaction Arc
Problem-Solving Level
Discourse Level
</figure>
<equation confidence="0.866706">
IObtain-Info-Ref(SI, S2, _course, Teaches(Dr. Smith, _course))
wer-Ref(S2, S , _course, Teaches(Dr. Smith,
course), Teaches(Dr. Smith, Arch))
Infonn(S2, SI, Teaches(Dr. Smith, Arch)) I
Ask-Ref(S I , S2, _course, Teaches(Dr. Smith, course))
* Tell(S2, Si, Teaches(Dr. Smith, Arch))
Surface-WH-Question(S1, S2, Inform-Ref
(S2, Si, _course, Teaches(Dr. Smith, _course))
Surface-Inform(S2, Si, Teaches(Dr. Smith, Arch))
IBuild-Plan(S1, S2, ake-Course(S1, _course)) I
IInstantiate-Vars(S1, S2, Learn-Material(S1, _course, Dr. Smith)) I
I* Instantiate-Single-Var(S1, S2, _course, Learn-Material(S1, _course, Dr. Smith)) I
Request(S1, S2, Inform-Ref(S2,
Si, course, Teaches(Dr. Smith, course))
</equation>
<bodyText confidence="0.99862475">
action, and that this problem-solving action con-
tributes to the problem-solving action of building
a plan in order to perform the domain action of
taking a course.
The work described in this paper uses our
tripartite model, but addresses the recognition of
discourse actions and their use in the modeling of
negotiation sub dialogues.
</bodyText>
<sectionHeader confidence="0.9872095" genericHeader="method">
3 Discourse Actions and Implicit
Acceptance
</sectionHeader>
<bodyText confidence="0.99916925">
One of the most important aspects of as-
similating dialogue is the recognition of discourse
actions and the role that an utterance plays with
respect to the rest of the dialogue. For example,
in (3), if Si believes that each course has a sin-
gle instructor, then Si is expressing doubt at the
proposition conveyed in (2). But in another con-
text, (3) might simply be asking for verification.
</bodyText>
<listItem confidence="0.998006333333333">
(1) 51: What is Dr. Smith teaching?
(2) S2: Dr. Smith is teaching Architecture.
(3) 51: Isn&apos;t Dr. Brown teaching Architecture?
</listItem>
<bodyText confidence="0.998766477272727">
Unless a natural language system is able to iden-
tify the role that an utterance is intended to play
in a dialogue, the system will not be able to gener-
ate cooperative responses which address the par-
ticipants&apos; goals.
In addition to recognizing discourse ac-
tions, it is also necessary for a cooperative sys-
tem to recognize a user&apos;s changing beliefs as the
dialogue progresses. Allen&apos;s representation of an
Inform speech act (Allen, 1979) assumed that a
listener adopted the communicated proposition.
Clearly, listeners do not adopt everything they
are told (e.g., (3) indicates that Si does not im-
mediately accept that Dr. Smith is teaching Ar-
chitecture). Perrault&apos;s persistence model of belief
(Perrault, 1990) assumed that a listener adopted
the communicated proposition unless the listener
had conflicting beliefs. Since Perrault&apos;s model as-
sumes that people&apos;s beliefs persist, it cannot ac-
count for Si eventually accepting the proposition
that Dr. Smith is teaching Architecture. We show
in Section 6 how our model overcomes this limita-
tion.
Our investigation of naturally occurring di-
alogues indicates that listeners are not passive par-
ticipants, but instead assimilate each utterance
into a dialogue in a multi-step acceptance phase.
For statements,3 a listener first attempts to un-
derstand the utterance because if the utterance is
not understood, then nothing else about it can be
determined. Second, the listener determines if the
utterance is consistent with the listener&apos;s beliefs;
and finally, the listener determines the appropri-
ateness of the utterance to the current context.
Since we are assuming that people are engaged
in a cooperative dialogue, a listener must indicate
when the listener does not understand, believe, or
consider relevant a particular utterance, address-
ing understandability first, then believability, then
relevance. We model this acceptance process by
including acceptance actions in the body of many
of our discourse recipes. For example, the actions
in the body of an Inform recipe (see Figure 1) are:
1) the speaker (_agentl) tells the listener (_agent2)
</bodyText>
<footnote confidence="0.941697">
3 Questions must also be accepted and assimilated into
a dialogue, but we are concentrating on statements here.
</footnote>
<page confidence="0.998229">
195
</page>
<bodyText confidence="0.999990804878049">
the proposition that the speaker wants the listener
to believe (_prop); and 2) the listener and speaker
address believability by discussing whatever is nec-
essary in order for the listener and speaker to come
to an agreement about what the speaker said.4
This second action, and the subactions executed
as part of performing it, account for subdialogues
which address the believability of the proposition
communicated in the Inform action. Similar ac-
ceptance actions appear in other discourse recipes.
The Tell action has a body containing a Surface-
Inform action and an Address-Understanding ac-
tion; the latter enables both participants to ensure
that the utterance has been understood.
The combination of the inclusion of accep-
tance actions in our discourse recipes and the or-
dered manner in which people address acceptance
allows our model to recognize the implicit accep-
tance of discourse actions. For example, Figure 2
presents the DM derived from utterances (1) and
(2), with the current focus of attention on the dis-
course level, the Tell action, marked with an aster-
isk. In attempting to assimilate (3) into this DM,
the system first tries to interpret (3) as address-
ing the understanding of (2) (i.e., as part of the
Tell action which is the current focus of attention
in Figure 2). Since a satisfactory interpretation is
not found, the system next tries to relate (3) to the
Inform action in Figure 2, trying to interpret (3)
as addressing the believability of (2). The system
finds that the best interpretation of (3) is that of
expressing doubt at (2), thus confirming the hy-
pothesis that (3) is addressing the believability of
(2). This recognition of (3) as contributing to the
Inform action in Figure 2 indicates that Si has
implicitly indicated understanding by passing up
the opportunity to address understanding in the
Tell action that appears in the DM and instead
moving to a relevant higher-level discourse action,
thus conveying that the Tell action has been suc-
cessful.
</bodyText>
<sectionHeader confidence="0.956949" genericHeader="method">
4 Recognizing Beliefs
</sectionHeader>
<bodyText confidence="0.99931275">
In the dialogue in the preceding section, in
order for Si to use the proposition communicated
in (3) to express doubt at the proposition conveyed
in (2), Si must believe
</bodyText>
<listItem confidence="0.9989265">
(a) that Dr. Brown teaches Architecture;
(b) that S2 believes that Dr. Smith is
teaching Architecture; and
(c) that Dr. Brown teaching Architecture is
</listItem>
<bodyText confidence="0.9150814">
an indication that Dr. Smith does not
teach Architecture.
We capture these beliefs in the applicability condi-
tions for an Express-Doubt discourse act (see Fig-
ure 1). In order for the system to recognize (3)
</bodyText>
<footnote confidence="0.9574458">
4This is where our model differs from Allen&apos;s and Per-
rault&apos;s; we allow the listener to adopt, reject, or negoti-
ate the speaker&apos;s claims, which might result in the listener
eventually adopting the speakers claims, the listener chang-
ing the mind of the speaker, or both agreeing to disagree.
</footnote>
<bodyText confidence="0.998855571428572">
as an expression of doubt, it must come to be-
lieve that these applicability conditions are satis-
fied. The system&apos;s evidence that Si believes (a)
is provided by Sl&apos;s utterance, (3). But (3) does
not state that Dr. Brown teaches Architecture;
instead, Si uses a negative yes-no question to ask
whether or not Dr. Brown teaches Architecture.
The surface form of this utterance indicates that
Si thinks that Dr. Brown teaches Architecture
but is not sure of it. Thus, from the surface form
of utterance (3), a listener can attribute to Si an
uncertain belief in the proposition that Dr. Brown
teaches Architecture.
This recognition of uncertain beliefs is an
important part of recognizing complex discourse
actions such as expressing doubt. If the system
were limited to recognizing only lack of belief and
belief, then yes-no questions would have to be in-
terpreted as conveying lack of belief about the
queried proposition, since a question in a cooper-
ative consultation setting would not be felicitous
if the speaker already knew the answer. Thus it
would be impossible to attribute (a) to Si from a
question such as (3). And without this belief at-
tribution, it would not be possible to recognize
expressions of doubt. Furthermore, the system
must be able to differentiate between expressions
of doubt and objections; since we are assuming
that people are engaged in a cooperative dialogue
and communicate beliefs that they intend to be
recognized, if Si were certain of both (a) and (c),
then Si would object to (2), not simply express
doubt at it. In summary, the surface form of ut-
terances is one way that speakers convey belief.
But these surface forms convey more than just be-
lief and disbelief; they convey multiple strengths
of belief, the recognition of which is necessary for
identifying whether an agent holds the requisite
beliefs for some discourse actions.
We maintain a belief model for each partic-
ipant which captures these multiple strengths of
belief. We contend that at least three strengths
of belief must be represented: certain belief (a be-
lief strength of C); strong but uncertain belief, as
in (3) above (a belief strength of S); and a weak
belief, as in I think that Dr. C might be an edu-
cation instructor (a belief strength of W). There-
fore, our model maintains three degrees of belief,
three degrees of disbelief (indicated by attaching
a subscript of N, such as SN to represent strong
disbelief and WN to represent weak disbelief), and
one degree indicating no belief about a proposition
(a belief strength of 0).5 Our belief model uses
belief intervals to specify the range of strengths
5 Others (Walker, 1991; Galliers, 1991) have also argued
for multiple strengths of belief, basing the strength of belief
on the amount and kind of evidence available for that be-
lief. We have not investigated how much evidence is needed
for an agent to have a particular amount of confidence in
a belief; our work has concentrated on recognizing how the
strength of belief is communicated in a discourse and the
impact that the different belief strengths have on the recog-
nition of discourse acts.
</bodyText>
<page confidence="0.995749">
196
</page>
<bodyText confidence="0.999888604651163">
within which an agent&apos;s beliefs are thought to fall,
and our discourse recipes use belief intervals to
specify the range of strengths that an agent&apos;s be-
liefs may assume. Intervals such as [bi:bi] spec-
ify a strength of belief within bi and bi , inclu-
sive. For example, the goal of the Inform recipe
in Figure 1, (believe (_agent2 , _prop,
is that _agentl be certain that _prop is true; on the
other hand, believe Lagent , _prop, : C] ) ,
means that _agentl must have some belief in _prop.
In order to recognize other beliefs, such as
(b) and (c), it is necessary to use more informa-
tion than just a speaker&apos;s utterances. For exam-
ple, S2 might attribute (c) to Si because S2 be-
lieves that most people think that only one pro-
fessor teaches each course. Our system incorpo-
rates these commonly held beliefs by maintaining
a model of a stereotypical user whose beliefs may
be attributed to the user during the conversation
as appropriate. People also communicate their be-
liefs by their acceptance (explicit and implicit) and
non-acceptance of other people&apos;s actions. Thus,
explicit or implicit acceptance of discourse actions
provides another mechanism for updating the be-
lief model: when an action is recognized as suc-
cessful, we update our model of the user&apos;s beliefs
with the effects and goals of the completed ac-
tion. For example, in determining whether (3) is
expressing doubt at (2), thereby implicitly indi-
cating that (2) has been understood and that the
Tell action has therefore been successful, the sys-
tem tentatively hypothesizes that the effects and
goals of the Tell action hold, the goal being that
Si believes that S2 believes that Dr. Smith is
teaching Architecture (belief (b) above). If the
system determines that this Express-Doubt infer-
ence is the most coherent interpretation of (3), it
attributes the hypothesized beliefs to Si. So, our
model captures many of the ways in which people
infer beliefs: 1) from the surface form of utter-
ances; 2) from stereotype models; and 3) from ac-
ceptance (explicit or implicit) or non-acceptance
of previous actions.
</bodyText>
<sectionHeader confidence="0.96431" genericHeader="method">
5 Combining Knowledge Sources
</sectionHeader>
<bodyText confidence="0.996350487500001">
Grosz and Sidner (1986) contend that mod-
eling discourse requires integrating different kinds
of knowledge in a unified framework in order to
constrain the possible role that an utterance might
be serving. We use three kinds of knowledge,
1) contextual information provided by previous
utterances; 2) world knowledge; and 3) the lin-
guistic information contained in each utterance.
Contextual knowledge in our model is captured by
the DM and the current focus of attention within
it. The system&apos;s world knowledge contains facts
about. the world, the system&apos;s beliefs (including
its beliefs about a stereotypical user&apos;s beliefs), and
knowledge about how to go about performing dis-
course, problem-solving, and domain actions. The
linguistic knowledge that we exploit includes the
surface form of the utterance, which conveys be-
liefs and the strength of belief, as discussed in the
preceding section, and linguistic clue words. Cer-
tain words often suggest what type of discourse
action the speaker might be pursuing (Litman and
Allen, 1987; Hinkelman, 1989). For example, the
linguistic clue please suggests a request discourse
act (Hinkelman, 1989) while the clue word but sug-
gests a non-acceptance discourse act. Our model
takes these linguistic clues into consideration in
identifying the discourse acts performed by an ut-
terance.
Our investigation of naturally occurring di-
alogues indicates that listeners use a combination
of information to determine what a speaker is try-
ing to do in saying something. For example, S2&apos;s
world knowledge of commonly held beliefs enabled
S2 to determine that Si probably believes (c), and
therefore infer that Si was expressing doubt at (2).
However, Si might have said (4) instead of (3).
(4) But didn&apos;t Dr. Smith win a teaching award?
It is not likely that S2 would think that people typ-
ically believe that Dr. Smith winning a teaching
award implies that she is not teaching Architec-
ture. However, S2 would probably still recognize
(4) as an expression of doubt because the linguis-
tic clue but suggests that (4) may be some sort of
non-acceptance action, there is nothing to suggest
that Si does not believe that Dr. Smith winning a
teaching award implies that she is not teaching Ar-
chitecture, and no other interpretation seems more
coherent. Since linguistic knowledge is present,
less evidence is needed from world knowledge to
recognize the discourse actions being performed
(Grosz and Sidner, 1986).
In our model, if a new utterance contributes
to a discourse action already in the DM, then there
must be an inference path from the utterance that
links the utterance up to the current tree structure
on the discourse level. This inference path will
contain an action that determines the relationship
of the utterance to the DM by introducing new
parameters for which there are many possible in-
stantiations, but which must be instantiated based
on values from the DM in order for the path to ter-
minate with an action already in the DM. We will
refer to such actions as e-actions since we contend
that there must be evidence to support the infer-
ence of these actions. By substituting values from
the DM that are not present in the semantic repre-
sentation of the utterance for the new parameters
in e-actions, we are hypothesizing a relationship
between the new, utterance and the existing dis-
course level of the DM.
Express-Doubt is an example of an e-action
(Figure 1). From the speaker&apos;s conveying uncer-
tain belief in the proposition _prop2, plan chain-
ing suggests that the speaker might be expressing
doubt at some proposition _propl, and from this
Express-Doubt action, further plan chaining may
suggest a sequence of actions terminating at an
Inform action already in the DM. The ability of
_propl to unify with the proposition that was con-
veyed by the Inform action (and _rule to unify
</bodyText>
<page confidence="0.996343">
197
</page>
<bodyText confidence="0.999987659574468">
with a rule in the system&apos;s world knowledge) is
not sufficient to justify inferring that the current
utterance contributes to an Express-Doubt action
which contributes to an Inform action; more evi-
dence is needed. This is further discussed in Lam-
bert and Carberry (1992).
Thus we need evidence for including e-
actions on an inference path. The required evi-
dence for e-actions may be provided by linguistic
knowledge that suggests certain discourse actions
(e.g., the evidence that (4) is expressing doubt)
or may be provided by world knowledge that in-
dicates that the applicability conditions for a par-
ticular action hold (e.g., the evidence that (3) is
expressing doubt).
Our model combines these different knowl-
edge sources in our plan recognition algorithm.
From the semantic representation of an utterance,
higher level actions are inferred using plan infer-
ence rules (Allen, 1979). If the applicability condi-
tions for an inferred action are not plausible, this
action is rejected. If the applicability conditions
are plausible, then the beliefs contained in them
are temporarily ascribed to the user (if an infer-
ence line containing this action is later adopted as
the correct interpretation, these applicability con-
ditions are added to the belief model of the user).
The focus of attention and focusing heuristics (dis-
cussed in Lambert and Carberry (1991)) order
these sequences of inferred actions, or inference
lines, in terms of coherence. For those inference
lines with an e-action, linguistic clues are checked
to determine if the action is suggested by linguistic
knowledge, and world knowledge is checked to de-
termine if there is evidence that the applicability
conditions for the e-action hold. If there is world
and linguistic evidence for the e-action of one or
more inference lines, the inference line that is clos-
est to the focus of attention (i.e., the most contex-
tually coherent) is chosen. Otherwise, if there is
world or linguistic evidence for the e-action of one
or more inference lines, again the inference line
that is closest to the focus of attention is chosen.
Otherwise, there is no evidence for the e-action in
any inference line, so the inference line that is clos-
est to the current focus of attention and contains
no e-action is chosen.
</bodyText>
<sectionHeader confidence="0.99677" genericHeader="method">
6 Example
</sectionHeader>
<bodyText confidence="0.9999449">
The following example, an expansion of ut-
terances (1), (2), and (3) from Section 3, illustrates
how our model handles 1) implicit and explicit ac-
ceptance; 2) negotiation subdialogues embedded
within other negotiation subdialogues; 3) expres-
sions of doubt at both immediately preceding and
earlier utterances; and 4) multiple expressions of
doubt at the same proposition. We will concen-
trate on how Sl&apos;s utterances are understood and
assimilated into the DM.
</bodyText>
<note confidence="0.980776454545454">
Si: What is Dr. Smith teaching?
S2: Dr. Smith is teaching Architecture.
Si: Isn&apos;t Dr. Brown teaching Architecture?
S2: No.
Dr. Brown is on sabbatical.
Si: But didn&apos;t I see him on campus
yesterday?
S2: Yes.
He was giving a University colloquium.
Si: OK.
But isn&apos;t Dr. Smith a theory person?
</note>
<figureCaption confidence="0.6115415">
The inferencing for utterances similar to (5)
and (6) is discussed in depth in Lambert and Car-
berry (1992), and the resultant DM is given in
Figure 2. No clarification or justification of the
</figureCaption>
<bodyText confidence="0.979968347826087">
Request action or of the content of the question has
been addressed by either Si or S2, and S2 has pro-
vided a relevant answer, so both parties have im-
plicitly indicated (Clark and Schaefer, 1989) that
they think that Si has made a reasonable and un-
derstandable request in asking the question in (5).
The surface form of (7) suggests that Si
thinks that Dr. Brown is teaching Architecture,
but isn&apos;t certain of it. This belief is entered
into the system&apos;s model of Sl&apos;s beliefs. This sur-
face question is one way to Convey-Uncertain-
Belief As discussed in Section 3, the most coher-
ent interpretation of (7) based on focusing heuris-
tics, addressing the understandability of (6), is
rejected (because there is not evidence to sup-
port this inference), so the system tries to relate
(7) to the Inform action in (6); that is, the sys-
tem tries to interpret (7) as addressing the believ-
ability of (6). Plan chaining determines that the
Convey-Uncertain-Belief action could be part of
an Express-Doubt action which could be part of
an Address-Unacceptance action which could be
an action in an Address-Believability discourse ac-
tion which could in turn be an action in the In-
form action of (6). Express-Doubt is an e-action
because the action header introduces new argu-
ments that have not appeared previously on the
inference path (see Figure 1). Since there is evi-
dence from world knowledge that the applicability
conditions hold for interpreting (7) as an expres-
sion of doubt and since there is no other evidence
for any other e-action, the system infers that this
is the correct interpretation and stops. Thus, (7)
is interpreted as an Express-Doubt action. S2&apos;s re-
sponse in (8) and (9) indicates that S2 is trying to
resolve Si and S2&apos;s conflicting beliefs. The struc-
ture that the DM has built after these utterances
is contained in Figure 3,6 above the numbers (5) -
(9).
The Surface-Neg-YN-Question in utterance
(10) is one way to Convey-Uncertain-Belief The
linguistic clue but suggests that Si is execut-
&apos;For space reasons, only inferencing of discourse actions
will be discussed here, and only action names on the dis-
course level are shown; the problem-solving and domain
levels are as shown in Figure 2.
</bodyText>
<page confidence="0.996656">
198
</page>
<figureCaption confidence="0.998632">
Figure 3. Discourse Level of DM for Dialogue in Section 6
</figureCaption>
<figure confidence="0.99884816">
711 e 11.11M.
forirq-4.- inria:NINm
Inform-If
ddress-Believability
Tell-If
Surface-Inform
Conve -Ac tance
ddress-Be eyili
ddress-Unacce tance
Resolve-Conflict
-Doubt
114117M1011111
Inform-If
Surface-Neg
YN-Question
(7)
urface-Inform
No
(8)
111711M11211
Inform
(14)
(13)
Embedzied
Negoti tion
Subdial ue
lSNurface-Neg
y
-Question
(10) (11) (12)
(9)
lit
Address -MI ieva
Inform
=MIMS
it411
1St:dace-MI-
(5 )
Cation
Surface-Inform
(6)
Address-Unacce • tance
Resolve-Conflict
Ex • ress-Doubt
Tell-If
urface-Inform
UT ace orrn
Convey-.
Convey -
1/1-
</figure>
<bodyText confidence="0.999007898305085">
ing a non-acceptance discourse action; this non-
acceptance action might be addressing either (9)
or (6). Focusing heuristics suggest that the most
likely candidate is the Inform act attempted in
(9), and plan chaining suggests that the Convey-
Uncertain-Belief could be part of an Express-
Doubt action which in turn could be part of an
Address- Unacceptance action which could be part
of an Address-Believability action which could be
part of the In form action in (9). Again, there is
evidence that the applicability conditions for the
e-action (the Express-Doubt action) hold: world
knowledge indicates that a typical user believes
that professors who are on sabbatical are not on
campus. Thus, there is both linguistic and world
knowledge giving evidence for the Express-Doubt
action (and no other e-action has both linguistic
and world knowledge evidence), so (10) is inter-
preted as expressing doubt at (9).
In (11) and (12), S2 clears up the confu-
sion that Si has expressed in (10), by telling Si
that the rule that people on sabbatical are not
on campus does not hold in this case. In (13),
Si indicates explicit acceptance of the previously
communicated proposition, so the system is able
to determine that Si has accepted S2&apos;s response in
(12). This additional negotiation, utterances (10)-
(13), illustrates our model&apos;s handling of negotia-
tion subdialogues embedded within other negoti-
ation subdialogues. The subtree contained within
the dashed lines in Figure 3 shows the structure
of this embedded negotiation subdialogue.
The linguistic clue but in (14) then again
suggests non-acceptance. Since (12) has been ex-
plicitly accepted, (14) could be expressing non-
acceptance of the information conveyed in either
(9) or (6). Focusing heuristics suggest that (14)
is most likely expressing doubt at (9). World
knowledge, however, provides no evidence that the
applicability conditions hold for (14) expressing
doubt at (9). Thus, there is evidence from lin-
guistic knowledge for this inference, but not from
world knowledge. The system&apos;s stereotype model
does indicate, however, that it is typically believed
that faculty only teach courses in their field and
that Architecture and Theory are different fields.
So in this case, the system&apos;s world knowledge pro-
vides evidence that Dr. Smith being a theory
person is an indication that Dr. Smith does not
teach Architecture. Therefore, the system inter-
prets (14) as again expressing doubt at (6) because
there is evidence for this inference from both world
and linguistic knowledge. The system infers there-
fore that Si has implicitly accepted the statement
in (9), that Dr. Smith is on sabbatical. Thus, the
system is able to recognize and assimilate a second
expression of doubt at the proposition conveyed in
(6). The DM for the discourse level of the entire
dialogue is given in Figure 3.
</bodyText>
<page confidence="0.998487">
199
</page>
<sectionHeader confidence="0.993295" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999958588235294">
We have presented a plan-based model that
handles cooperative negotiation subdialogues by
inferring both the communicative actions that
people pursue when speaking and the beliefs un-
derlying these actions. Beliefs, and the strength of
those beliefs, are recognized from the surface form
of utterances and from the explicit and implicit ac-
ceptance of previous utterances. Our model com-
bines linguistic, contextual, and world knowledge
in a unified framework that enables recognition
not only of when an agent is negotiating a con-
flict between the agent&apos;s beliefs and the preceding
dialogue but also which part of the dialogue the
agent&apos;s beliefs conflict with. Since negotiation is
an integral part of multi-agent activity, our model
addresses an important aspect of cooperative in-
teraction and communication.
</bodyText>
<sectionHeader confidence="0.998376" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849021739131">
Allen, James F. (1979). A Plan-Based Approach
to Speech Act Recognition. PhD thesis, Uni-
versity of Toronto, Toronto, Ontario, Canada.
Carberry, Sandra (1987). Pragmatic Modeling:
Toward a Robust Natural Language Interface.
Computational Intelligence, 3, 117-136.
Clark, Herbert and Schaefer, Edward (1989). Con-
tributing to Discourse. Cognitive Science,
259-294.
Cohen, Robin (1987). Analyzing the Structure
of Argumentative Discourse. Computational
Linguistics, /3(1-2), 11-24.
Cohen, Robin and Young, Mark A. (1991). Deter-
mining Intended Evidence Relations in Natu-
ral Language Arguments. Computational In-
telligence, 7,110-118.
Flowers, Margot, McGuire, Rod, and Birnbaum,
Lawrence (1982). Adversary Arguments and
the Logic of Personal Attack. In W. Lehn-
ert and M. Ringle (Eds.), Strategies for Natu-
ral Language Processing (pp. 275-294). Hills-
dage, New Jersey: Lawrence Erlbaum Assoc.
Galliers, Julia R. (1991). Belief Revision and a
Theory of Communication. Technical Report
193, University of Cambridge, Cambridge,
England.
Goodman, Bradley A. and Litman, Diane J.
(1992). On the Interaction between Plan
Recognition and Intelligent Interfaces. User
Modeling and User-Adapted Interaction, 2,
83-115.
Grosz, Barbara and Sidner, Candace (1986). At-
tention, Intention, and the Structure of Dis-
course. Computational Linguistics, 12(3),
175-204.
Hinkelman, Elizabeth (1989). Two Constraints on
Speech Act Ambiguity. In Proceedings of the
27th Annual Meeting of the ACL (pp. 212-
219), Vancouver, Canada.
Kautz, Henry and Allen, James (1986). General-
ized Plan Recognition. In Proceedings of the
Fifth National Conference on Artificial Intel-
ligence (pp. 32-37), Philadelphia, Pennsylva-
nia.
Lambert, Lynn and Carberry, Sandra (1991). A
Tripartite Plan-based Model of Dialogue. In
Proceedings of the 29th Annual Meeting of the
ACL (pp. 47-54), Berkeley, CA.
Lambert, Lynn and Carberry, Sandra (1992). Us-
ing Linguistic, World, and Contextual Knowl-
edge in a Plan Recognition Model of Dia-
logue. In Proceedings of COLING-92, Nantes,
France. To appear.
Litman, Diane and Allen, James (1987). A Plan
Recognition Model for Subdialogues in Con-
versation. Cognitive Science, 11, 163-200.
Perrault, Raymond (1990). An Application of De-
fault Logic to Speech Act Theory. In P. Co-
hen, J. Morgan, and M. Pollack (Eds.), Inten-
tions in Communication (pp. 161-185). Cam-
bridge, Massachusetts: MIT Press.
Pollack, Martha (1990). Plans as Complex Men-
tal Attitudes. In P. R. Cohen, J. Morgan, and
M. E. Pollack (Eds.), Intentions in Commu-
nication (pp 77-104). MIT Press.
Quilici, Alexander (1991). The Correction Ma-
chine: A Computer Model of Recognizing and
Producing Belief Justifications in Argumenta-
tive Dialogs. PhD thesis, Department of Com-
puter Science, University of California at Los
Angeles, Los Angeles, California.
Ramshaw, Lance A. (1991). A Three-Level Model
for Plan Exploration. In Proceedings of the
29th Annual Meeting of the ACL (pp. 36-46),
Berkeley, California.
Reichman, Rachel (1981). Modeling Informal De-
bates. In Proceedings of the 1981 Interna-
tional Joint Conference on Artificial Intelli-
gence (pp. 19-24), Vancouver, B.C. IJCAI.
Sidner, Candace L. (1985). Plan Parsing for In-
tended Response Recognition in Discourse.
Computational Intelligence, 1, 1-10.
Walker, Marilyn (1991). Redundancy in Collabo-
rative Dialogue. Presented at The AAAI Fall
Symposium: Discourse Structure in Natural
Language Understanding and Generation (pp.
124-129), Asilomar, CA.
Wilensky, Robert (1981). Meta-Planning: Rep-
resenting and Using Knowledge About Plan-
ning in Problem Solving and Natural Lan-
guage Understanding. Cognitive Science, 5,
197-233.
</reference>
<page confidence="0.996594">
200
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970555">
<title confidence="0.995853">NEGOTIATION</title>
<author confidence="0.999981">Lynn Lambert</author>
<author confidence="0.999981">Sandra Carberry</author>
<affiliation confidence="0.9999075">Department of Computer and Information Sciences University of Delaware</affiliation>
<address confidence="0.999936">Newark, Delaware 19716, USA</address>
<email confidence="0.999973">lambert@cis.udel.edu,carberryecis.udel.edu</email>
<abstract confidence="0.998417">This paper presents a plan-based model that handles negotiation subdialogues by inferring both the communicative actions that people pursue when speaking and the beliefs underlying these actions. We contend that recognizing the complex discourse actions pursued in negotiation subdialogues (e.g., expressing doubt) requires both a multistrength belief model and a process model that combines different knowledge sources in a unified framework. We show how our model identifies the structure of negotiation subdialogues, including recognizing expressions of doubt, implicit acceptance of communicated propositions, and negotiation subdialogues embedded within other negotiation subdialogues.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>A Plan-Based Approach to Speech Act Recognition.</title>
<date>1979</date>
<tech>PhD thesis,</tech>
<institution>University of Toronto,</institution>
<location>Toronto, Ontario, Canada.</location>
<contexts>
<context position="3681" citStr="Allen, 1979" startWordPosition="553" endWordPosition="554">not provide a detailed computational mechanism for recognizing the role of each utterance in a debate. In previous work (Lambert and Carberry, 1991), we described a tripartite plan-based model of dialogue that recognizes and differentiates three different kinds of actions: domain, problemsolving, and discourse. Domain actions relate to performing tasks in a given domain. We are modeling cooperative dialogues in which one agent has a domain goal and is working with another helpful, more expert agent to determine what domain actions to perform in order to accomplish this goal. Many researchers (Allen, 1979; Carberry, 1987; Goodman and Litman, 1992; Pollack, 1990; Sidner, 1985) have shown that recognition of domain plans and goals gives a system the ability to address many difficult problems in understanding. Problem-solving actions relate to how the two dialogue participants are going about building a plan to achieve the planning agent&apos;s domain goal. Ramshaw, Litman, and Wilensky (Ramshaw, 1991; Litman and Allen, 1987; Wilensky, 1981) have noted the need for recognizing problem-solving actions. Discourse actions are the communicative actions that people perform in saying something, e.g., asking</context>
<context position="11364" citStr="Allen, 1979" startWordPosition="1700" endWordPosition="1701">ext, (3) might simply be asking for verification. (1) 51: What is Dr. Smith teaching? (2) S2: Dr. Smith is teaching Architecture. (3) 51: Isn&apos;t Dr. Brown teaching Architecture? Unless a natural language system is able to identify the role that an utterance is intended to play in a dialogue, the system will not be able to generate cooperative responses which address the participants&apos; goals. In addition to recognizing discourse actions, it is also necessary for a cooperative system to recognize a user&apos;s changing beliefs as the dialogue progresses. Allen&apos;s representation of an Inform speech act (Allen, 1979) assumed that a listener adopted the communicated proposition. Clearly, listeners do not adopt everything they are told (e.g., (3) indicates that Si does not immediately accept that Dr. Smith is teaching Architecture). Perrault&apos;s persistence model of belief (Perrault, 1990) assumed that a listener adopted the communicated proposition unless the listener had conflicting beliefs. Since Perrault&apos;s model assumes that people&apos;s beliefs persist, it cannot account for Si eventually accepting the proposition that Dr. Smith is teaching Architecture. We show in Section 6 how our model overcomes this limi</context>
<context position="25898" citStr="Allen, 1979" startWordPosition="4098" endWordPosition="4099">Thus we need evidence for including eactions on an inference path. The required evidence for e-actions may be provided by linguistic knowledge that suggests certain discourse actions (e.g., the evidence that (4) is expressing doubt) or may be provided by world knowledge that indicates that the applicability conditions for a particular action hold (e.g., the evidence that (3) is expressing doubt). Our model combines these different knowledge sources in our plan recognition algorithm. From the semantic representation of an utterance, higher level actions are inferred using plan inference rules (Allen, 1979). If the applicability conditions for an inferred action are not plausible, this action is rejected. If the applicability conditions are plausible, then the beliefs contained in them are temporarily ascribed to the user (if an inference line containing this action is later adopted as the correct interpretation, these applicability conditions are added to the belief model of the user). The focus of attention and focusing heuristics (discussed in Lambert and Carberry (1991)) order these sequences of inferred actions, or inference lines, in terms of coherence. For those inference lines with an e-</context>
</contexts>
<marker>Allen, 1979</marker>
<rawString>Allen, James F. (1979). A Plan-Based Approach to Speech Act Recognition. PhD thesis, University of Toronto, Toronto, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Carberry</author>
</authors>
<title>Pragmatic Modeling: Toward a Robust Natural Language Interface.</title>
<date>1987</date>
<journal>Computational Intelligence,</journal>
<volume>3</volume>
<pages>117--136</pages>
<contexts>
<context position="3697" citStr="Carberry, 1987" startWordPosition="555" endWordPosition="557"> detailed computational mechanism for recognizing the role of each utterance in a debate. In previous work (Lambert and Carberry, 1991), we described a tripartite plan-based model of dialogue that recognizes and differentiates three different kinds of actions: domain, problemsolving, and discourse. Domain actions relate to performing tasks in a given domain. We are modeling cooperative dialogues in which one agent has a domain goal and is working with another helpful, more expert agent to determine what domain actions to perform in order to accomplish this goal. Many researchers (Allen, 1979; Carberry, 1987; Goodman and Litman, 1992; Pollack, 1990; Sidner, 1985) have shown that recognition of domain plans and goals gives a system the ability to address many difficult problems in understanding. Problem-solving actions relate to how the two dialogue participants are going about building a plan to achieve the planning agent&apos;s domain goal. Ramshaw, Litman, and Wilensky (Ramshaw, 1991; Litman and Allen, 1987; Wilensky, 1981) have noted the need for recognizing problem-solving actions. Discourse actions are the communicative actions that people perform in saying something, e.g., asking a question or e</context>
<context position="6526" citStr="Carberry, 1987" startWordPosition="929" endWordPosition="930">_prop2)) Figure 1. Two Sample Discourse Recipes mon skills that people use in a wide variety of contexts, so the system can assume that knowledge about discourse and problem-solving recipes is shared knowledge. Figure 1 contains two discourse recipes. Our representation of a recipe includes a header giving the name of the recipe and the action that it accomplishes, preconditions, applicability conditions, constraints, a body, effects, and a goal. Constraints limit the allowable instantiation of variables in each of the components of a recipe (Litman and Allen, 1987). Applicability conditions (Carberry, 1987) represent conditions that must be satisfied in order for the recipe to be reasonable to apply in the given situation and, in the case of many of our discourse recipes, the applicability conditions capture beliefs that the dialogue participants must hold. Especially in the case of discourse recipes, the goals and effects are likely to be different. This allows us to differentiate between illocutionary and perlocutionary effects and to capture the notion that one can, for example, perform an inform act without the hearer adopting the communicated proposition.&apos; As actions are inferred by our pro</context>
</contexts>
<marker>Carberry, 1987</marker>
<rawString>Carberry, Sandra (1987). Pragmatic Modeling: Toward a Robust Natural Language Interface. Computational Intelligence, 3, 117-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Clark</author>
<author>Edward Schaefer</author>
</authors>
<title>Contributing to Discourse.</title>
<date>1989</date>
<journal>Cognitive Science,</journal>
<pages>259--294</pages>
<contexts>
<context position="2668" citStr="Clark and Schaefer (1989)" startWordPosition="388" endWordPosition="391">us Work Several researchers have built argument understanding systems, but none of these has addressed participants coming to an agreement or mutual belief about a particular situation, either because the arguments were only monologues &apos;This work is being supported by the National Science Foundation under Grant No. IRI-9122026. The Government has certain rights in this material. (Cohen, 1987; Cohen and Young, 1991), or because they assumed that dialogue participants do not change their minds (Flowers, McGuire and Birnbaum, 1982; Quilici, 1991). Others have examined more cooperative dialogues. Clark and Schaefer (1989) contend that utterances must be grounded, or understood, by both parties, but they do not address conflicts in belief, only lack of understanding. Walker (1991) has shown that evidence is often provided to ensure both understanding and believing an utterance, but she does not address recognizing lack of belief or lack of understanding. Reichman (1981) outlines a model for informal debate, but does not provide a detailed computational mechanism for recognizing the role of each utterance in a debate. In previous work (Lambert and Carberry, 1991), we described a tripartite plan-based model of di</context>
<context position="28428" citStr="Clark and Schaefer, 1989" startWordPosition="4519" endWordPosition="4522">ching Architecture. Si: Isn&apos;t Dr. Brown teaching Architecture? S2: No. Dr. Brown is on sabbatical. Si: But didn&apos;t I see him on campus yesterday? S2: Yes. He was giving a University colloquium. Si: OK. But isn&apos;t Dr. Smith a theory person? The inferencing for utterances similar to (5) and (6) is discussed in depth in Lambert and Carberry (1992), and the resultant DM is given in Figure 2. No clarification or justification of the Request action or of the content of the question has been addressed by either Si or S2, and S2 has provided a relevant answer, so both parties have implicitly indicated (Clark and Schaefer, 1989) that they think that Si has made a reasonable and understandable request in asking the question in (5). The surface form of (7) suggests that Si thinks that Dr. Brown is teaching Architecture, but isn&apos;t certain of it. This belief is entered into the system&apos;s model of Sl&apos;s beliefs. This surface question is one way to Convey-UncertainBelief As discussed in Section 3, the most coherent interpretation of (7) based on focusing heuristics, addressing the understandability of (6), is rejected (because there is not evidence to support this inference), so the system tries to relate (7) to the Inform a</context>
</contexts>
<marker>Clark, Schaefer, 1989</marker>
<rawString>Clark, Herbert and Schaefer, Edward (1989). Contributing to Discourse. Cognitive Science, 259-294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cohen</author>
</authors>
<title>Analyzing the Structure of Argumentative Discourse.</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>11--24</pages>
<contexts>
<context position="2437" citStr="Cohen, 1987" startWordPosition="355" endWordPosition="356">ef model and a processing strategy that combines different knowledge sources in a unified framework, and we will show how our model incorporates these and recognizes the structure of negotiation subdialogues. 2 Previous Work Several researchers have built argument understanding systems, but none of these has addressed participants coming to an agreement or mutual belief about a particular situation, either because the arguments were only monologues &apos;This work is being supported by the National Science Foundation under Grant No. IRI-9122026. The Government has certain rights in this material. (Cohen, 1987; Cohen and Young, 1991), or because they assumed that dialogue participants do not change their minds (Flowers, McGuire and Birnbaum, 1982; Quilici, 1991). Others have examined more cooperative dialogues. Clark and Schaefer (1989) contend that utterances must be grounded, or understood, by both parties, but they do not address conflicts in belief, only lack of understanding. Walker (1991) has shown that evidence is often provided to ensure both understanding and believing an utterance, but she does not address recognizing lack of belief or lack of understanding. Reichman (1981) outlines a mod</context>
</contexts>
<marker>Cohen, 1987</marker>
<rawString>Cohen, Robin (1987). Analyzing the Structure of Argumentative Discourse. Computational Linguistics, /3(1-2), 11-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cohen</author>
<author>Mark A Young</author>
</authors>
<title>Determining Intended Evidence Relations in Natural Language Arguments.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<pages>7--110</pages>
<contexts>
<context position="2461" citStr="Cohen and Young, 1991" startWordPosition="357" endWordPosition="360">a processing strategy that combines different knowledge sources in a unified framework, and we will show how our model incorporates these and recognizes the structure of negotiation subdialogues. 2 Previous Work Several researchers have built argument understanding systems, but none of these has addressed participants coming to an agreement or mutual belief about a particular situation, either because the arguments were only monologues &apos;This work is being supported by the National Science Foundation under Grant No. IRI-9122026. The Government has certain rights in this material. (Cohen, 1987; Cohen and Young, 1991), or because they assumed that dialogue participants do not change their minds (Flowers, McGuire and Birnbaum, 1982; Quilici, 1991). Others have examined more cooperative dialogues. Clark and Schaefer (1989) contend that utterances must be grounded, or understood, by both parties, but they do not address conflicts in belief, only lack of understanding. Walker (1991) has shown that evidence is often provided to ensure both understanding and believing an utterance, but she does not address recognizing lack of belief or lack of understanding. Reichman (1981) outlines a model for informal debate, </context>
</contexts>
<marker>Cohen, Young, 1991</marker>
<rawString>Cohen, Robin and Young, Mark A. (1991). Determining Intended Evidence Relations in Natural Language Arguments. Computational Intelligence, 7,110-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margot Flowers</author>
<author>Rod McGuire</author>
<author>Lawrence Birnbaum</author>
</authors>
<title>Adversary Arguments and the Logic of Personal Attack.</title>
<date>1982</date>
<booktitle>In W. Lehnert and M. Ringle (Eds.), Strategies for Natural Language Processing</booktitle>
<pages>275--294</pages>
<location>Hillsdage, New Jersey: Lawrence Erlbaum Assoc.</location>
<marker>Flowers, McGuire, Birnbaum, 1982</marker>
<rawString>Flowers, Margot, McGuire, Rod, and Birnbaum, Lawrence (1982). Adversary Arguments and the Logic of Personal Attack. In W. Lehnert and M. Ringle (Eds.), Strategies for Natural Language Processing (pp. 275-294). Hillsdage, New Jersey: Lawrence Erlbaum Assoc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia R Galliers</author>
</authors>
<title>Belief Revision and a Theory of Communication.</title>
<date>1991</date>
<tech>Technical Report 193,</tech>
<institution>University of Cambridge,</institution>
<location>Cambridge, England.</location>
<contexts>
<context position="18621" citStr="Galliers, 1991" startWordPosition="2889" endWordPosition="2890">: certain belief (a belief strength of C); strong but uncertain belief, as in (3) above (a belief strength of S); and a weak belief, as in I think that Dr. C might be an education instructor (a belief strength of W). Therefore, our model maintains three degrees of belief, three degrees of disbelief (indicated by attaching a subscript of N, such as SN to represent strong disbelief and WN to represent weak disbelief), and one degree indicating no belief about a proposition (a belief strength of 0).5 Our belief model uses belief intervals to specify the range of strengths 5 Others (Walker, 1991; Galliers, 1991) have also argued for multiple strengths of belief, basing the strength of belief on the amount and kind of evidence available for that belief. We have not investigated how much evidence is needed for an agent to have a particular amount of confidence in a belief; our work has concentrated on recognizing how the strength of belief is communicated in a discourse and the impact that the different belief strengths have on the recognition of discourse acts. 196 within which an agent&apos;s beliefs are thought to fall, and our discourse recipes use belief intervals to specify the range of strengths that</context>
</contexts>
<marker>Galliers, 1991</marker>
<rawString>Galliers, Julia R. (1991). Belief Revision and a Theory of Communication. Technical Report 193, University of Cambridge, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley A Goodman</author>
<author>Diane J Litman</author>
</authors>
<title>On the Interaction between Plan Recognition and Intelligent Interfaces. User Modeling and User-Adapted Interaction,</title>
<date>1992</date>
<volume>2</volume>
<pages>83--115</pages>
<contexts>
<context position="3723" citStr="Goodman and Litman, 1992" startWordPosition="558" endWordPosition="561">ational mechanism for recognizing the role of each utterance in a debate. In previous work (Lambert and Carberry, 1991), we described a tripartite plan-based model of dialogue that recognizes and differentiates three different kinds of actions: domain, problemsolving, and discourse. Domain actions relate to performing tasks in a given domain. We are modeling cooperative dialogues in which one agent has a domain goal and is working with another helpful, more expert agent to determine what domain actions to perform in order to accomplish this goal. Many researchers (Allen, 1979; Carberry, 1987; Goodman and Litman, 1992; Pollack, 1990; Sidner, 1985) have shown that recognition of domain plans and goals gives a system the ability to address many difficult problems in understanding. Problem-solving actions relate to how the two dialogue participants are going about building a plan to achieve the planning agent&apos;s domain goal. Ramshaw, Litman, and Wilensky (Ramshaw, 1991; Litman and Allen, 1987; Wilensky, 1981) have noted the need for recognizing problem-solving actions. Discourse actions are the communicative actions that people perform in saying something, e.g., asking a question or expressing doubt. Recogniti</context>
</contexts>
<marker>Goodman, Litman, 1992</marker>
<rawString>Goodman, Bradley A. and Litman, Diane J. (1992). On the Interaction between Plan Recognition and Intelligent Interfaces. User Modeling and User-Adapted Interaction, 2, 83-115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candace Sidner</author>
</authors>
<date>1986</date>
<booktitle>Attention, Intention, and the Structure of Discourse. Computational Linguistics,</booktitle>
<volume>12</volume>
<issue>3</issue>
<pages>175--204</pages>
<contexts>
<context position="21204" citStr="Grosz and Sidner (1986)" startWordPosition="3328" endWordPosition="3331"> the system tentatively hypothesizes that the effects and goals of the Tell action hold, the goal being that Si believes that S2 believes that Dr. Smith is teaching Architecture (belief (b) above). If the system determines that this Express-Doubt inference is the most coherent interpretation of (3), it attributes the hypothesized beliefs to Si. So, our model captures many of the ways in which people infer beliefs: 1) from the surface form of utterances; 2) from stereotype models; and 3) from acceptance (explicit or implicit) or non-acceptance of previous actions. 5 Combining Knowledge Sources Grosz and Sidner (1986) contend that modeling discourse requires integrating different kinds of knowledge in a unified framework in order to constrain the possible role that an utterance might be serving. We use three kinds of knowledge, 1) contextual information provided by previous utterances; 2) world knowledge; and 3) the linguistic information contained in each utterance. Contextual knowledge in our model is captured by the DM and the current focus of attention within it. The system&apos;s world knowledge contains facts about. the world, the system&apos;s beliefs (including its beliefs about a stereotypical user&apos;s belief</context>
<context position="23618" citStr="Grosz and Sidner, 1986" startWordPosition="3714" endWordPosition="3717">ple typically believe that Dr. Smith winning a teaching award implies that she is not teaching Architecture. However, S2 would probably still recognize (4) as an expression of doubt because the linguistic clue but suggests that (4) may be some sort of non-acceptance action, there is nothing to suggest that Si does not believe that Dr. Smith winning a teaching award implies that she is not teaching Architecture, and no other interpretation seems more coherent. Since linguistic knowledge is present, less evidence is needed from world knowledge to recognize the discourse actions being performed (Grosz and Sidner, 1986). In our model, if a new utterance contributes to a discourse action already in the DM, then there must be an inference path from the utterance that links the utterance up to the current tree structure on the discourse level. This inference path will contain an action that determines the relationship of the utterance to the DM by introducing new parameters for which there are many possible instantiations, but which must be instantiated based on values from the DM in order for the path to terminate with an action already in the DM. We will refer to such actions as e-actions since we contend tha</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, Barbara and Sidner, Candace (1986). Attention, Intention, and the Structure of Discourse. Computational Linguistics, 12(3), 175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Hinkelman</author>
</authors>
<title>Two Constraints on Speech Act Ambiguity.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the ACL</booktitle>
<pages>212--219</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="22233" citStr="Hinkelman, 1989" startWordPosition="3489" endWordPosition="3490">and the current focus of attention within it. The system&apos;s world knowledge contains facts about. the world, the system&apos;s beliefs (including its beliefs about a stereotypical user&apos;s beliefs), and knowledge about how to go about performing discourse, problem-solving, and domain actions. The linguistic knowledge that we exploit includes the surface form of the utterance, which conveys beliefs and the strength of belief, as discussed in the preceding section, and linguistic clue words. Certain words often suggest what type of discourse action the speaker might be pursuing (Litman and Allen, 1987; Hinkelman, 1989). For example, the linguistic clue please suggests a request discourse act (Hinkelman, 1989) while the clue word but suggests a non-acceptance discourse act. Our model takes these linguistic clues into consideration in identifying the discourse acts performed by an utterance. Our investigation of naturally occurring dialogues indicates that listeners use a combination of information to determine what a speaker is trying to do in saying something. For example, S2&apos;s world knowledge of commonly held beliefs enabled S2 to determine that Si probably believes (c), and therefore infer that Si was exp</context>
</contexts>
<marker>Hinkelman, 1989</marker>
<rawString>Hinkelman, Elizabeth (1989). Two Constraints on Speech Act Ambiguity. In Proceedings of the 27th Annual Meeting of the ACL (pp. 212-219), Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Kautz</author>
<author>James Allen</author>
</authors>
<title>Generalized Plan Recognition.</title>
<date>1986</date>
<booktitle>In Proceedings of the Fifth National Conference on Artificial Intelligence</booktitle>
<pages>32--37</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="7473" citStr="Kautz and Allen, 1986" startWordPosition="1083" endWordPosition="1086">e likely to be different. This allows us to differentiate between illocutionary and perlocutionary effects and to capture the notion that one can, for example, perform an inform act without the hearer adopting the communicated proposition.&apos; As actions are inferred by our process model, a structure of the discourse is built which is referred to as the Dialogue Model, or DM. In the DM, discourse, problem-solving, and domain actions are each modeled on a separate level. Within each of these levels, actions may contribute to other actions in the dialogue, and this is captured with specialization (Kautz and Allen, 1986), sub2 Consider, for example, someone saying &amp;quot;I informed you of X but you wouldn&apos;t believe me.&amp;quot; action, and enablement arcs. Thus, actions at each level form a tree structure in which each node represents an action that a participant is performing and the children of a node represent actions pursued in order to contribute to the parent action. By using a tree structure to model actions at each level and by allowing the tree structures to grow at the root as well as at the leaves, we are able to incrementally recognize discourse, problem-solving, and domain intentions, and can recognize the rel</context>
</contexts>
<marker>Kautz, Allen, 1986</marker>
<rawString>Kautz, Henry and Allen, James (1986). Generalized Plan Recognition. In Proceedings of the Fifth National Conference on Artificial Intelligence (pp. 32-37), Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Lambert</author>
<author>Sandra Carberry</author>
</authors>
<title>A Tripartite Plan-based Model of Dialogue.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the ACL</booktitle>
<pages>47--54</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="3218" citStr="Lambert and Carberry, 1991" startWordPosition="478" endWordPosition="481">). Others have examined more cooperative dialogues. Clark and Schaefer (1989) contend that utterances must be grounded, or understood, by both parties, but they do not address conflicts in belief, only lack of understanding. Walker (1991) has shown that evidence is often provided to ensure both understanding and believing an utterance, but she does not address recognizing lack of belief or lack of understanding. Reichman (1981) outlines a model for informal debate, but does not provide a detailed computational mechanism for recognizing the role of each utterance in a debate. In previous work (Lambert and Carberry, 1991), we described a tripartite plan-based model of dialogue that recognizes and differentiates three different kinds of actions: domain, problemsolving, and discourse. Domain actions relate to performing tasks in a given domain. We are modeling cooperative dialogues in which one agent has a domain goal and is working with another helpful, more expert agent to determine what domain actions to perform in order to accomplish this goal. Many researchers (Allen, 1979; Carberry, 1987; Goodman and Litman, 1992; Pollack, 1990; Sidner, 1985) have shown that recognition of domain plans and goals gives a sy</context>
<context position="8322" citStr="Lambert and Carberry (1991)" startWordPosition="1229" endWordPosition="1232">participant is performing and the children of a node represent actions pursued in order to contribute to the parent action. By using a tree structure to model actions at each level and by allowing the tree structures to grow at the root as well as at the leaves, we are able to incrementally recognize discourse, problem-solving, and domain intentions, and can recognize the relationship among several utterances that are all part of the same higher-level discourse act even when that act cannot be recognized from the first utterance alone. Other advantages of our tripartite model are discussed in Lambert and Carberry (1991). An action on one level in the DM may also contribute to an action on an immediately higher level. For example, discourse actions may be executed in order to obtain the information necessary for performing a problem-solving action and problem-solving actions may be executed in order to construct a domain plan. We capture this with links between actions on adjacent levels of the DM. Figure 2 gives a DM built by our prototype system whose implementation is currently being expanded to include belief ascription and use of linguistic information. It shows that a question has been asked and answere</context>
<context position="26374" citStr="Lambert and Carberry (1991)" startWordPosition="4172" endWordPosition="4175">lan recognition algorithm. From the semantic representation of an utterance, higher level actions are inferred using plan inference rules (Allen, 1979). If the applicability conditions for an inferred action are not plausible, this action is rejected. If the applicability conditions are plausible, then the beliefs contained in them are temporarily ascribed to the user (if an inference line containing this action is later adopted as the correct interpretation, these applicability conditions are added to the belief model of the user). The focus of attention and focusing heuristics (discussed in Lambert and Carberry (1991)) order these sequences of inferred actions, or inference lines, in terms of coherence. For those inference lines with an e-action, linguistic clues are checked to determine if the action is suggested by linguistic knowledge, and world knowledge is checked to determine if there is evidence that the applicability conditions for the e-action hold. If there is world and linguistic evidence for the e-action of one or more inference lines, the inference line that is closest to the focus of attention (i.e., the most contextually coherent) is chosen. Otherwise, if there is world or linguistic evidenc</context>
</contexts>
<marker>Lambert, Carberry, 1991</marker>
<rawString>Lambert, Lynn and Carberry, Sandra (1991). A Tripartite Plan-based Model of Dialogue. In Proceedings of the 29th Annual Meeting of the ACL (pp. 47-54), Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Lambert</author>
<author>Sandra Carberry</author>
</authors>
<title>Using Linguistic, World, and Contextual Knowledge in a Plan Recognition Model of Dialogue.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<location>Nantes, France.</location>
<note>To appear.</note>
<contexts>
<context position="25284" citStr="Lambert and Carberry (1992)" startWordPosition="3998" endWordPosition="4002">op2, plan chaining suggests that the speaker might be expressing doubt at some proposition _propl, and from this Express-Doubt action, further plan chaining may suggest a sequence of actions terminating at an Inform action already in the DM. The ability of _propl to unify with the proposition that was conveyed by the Inform action (and _rule to unify 197 with a rule in the system&apos;s world knowledge) is not sufficient to justify inferring that the current utterance contributes to an Express-Doubt action which contributes to an Inform action; more evidence is needed. This is further discussed in Lambert and Carberry (1992). Thus we need evidence for including eactions on an inference path. The required evidence for e-actions may be provided by linguistic knowledge that suggests certain discourse actions (e.g., the evidence that (4) is expressing doubt) or may be provided by world knowledge that indicates that the applicability conditions for a particular action hold (e.g., the evidence that (3) is expressing doubt). Our model combines these different knowledge sources in our plan recognition algorithm. From the semantic representation of an utterance, higher level actions are inferred using plan inference rules</context>
<context position="28147" citStr="Lambert and Carberry (1992)" startWordPosition="4467" endWordPosition="4471">es; 3) expressions of doubt at both immediately preceding and earlier utterances; and 4) multiple expressions of doubt at the same proposition. We will concentrate on how Sl&apos;s utterances are understood and assimilated into the DM. Si: What is Dr. Smith teaching? S2: Dr. Smith is teaching Architecture. Si: Isn&apos;t Dr. Brown teaching Architecture? S2: No. Dr. Brown is on sabbatical. Si: But didn&apos;t I see him on campus yesterday? S2: Yes. He was giving a University colloquium. Si: OK. But isn&apos;t Dr. Smith a theory person? The inferencing for utterances similar to (5) and (6) is discussed in depth in Lambert and Carberry (1992), and the resultant DM is given in Figure 2. No clarification or justification of the Request action or of the content of the question has been addressed by either Si or S2, and S2 has provided a relevant answer, so both parties have implicitly indicated (Clark and Schaefer, 1989) that they think that Si has made a reasonable and understandable request in asking the question in (5). The surface form of (7) suggests that Si thinks that Dr. Brown is teaching Architecture, but isn&apos;t certain of it. This belief is entered into the system&apos;s model of Sl&apos;s beliefs. This surface question is one way to </context>
</contexts>
<marker>Lambert, Carberry, 1992</marker>
<rawString>Lambert, Lynn and Carberry, Sandra (1992). Using Linguistic, World, and Contextual Knowledge in a Plan Recognition Model of Dialogue. In Proceedings of COLING-92, Nantes, France. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>James Allen</author>
</authors>
<title>A Plan Recognition Model for Subdialogues in Conversation.</title>
<date>1987</date>
<journal>Cognitive Science,</journal>
<volume>11</volume>
<pages>163--200</pages>
<contexts>
<context position="4101" citStr="Litman and Allen, 1987" startWordPosition="617" endWordPosition="620">es in which one agent has a domain goal and is working with another helpful, more expert agent to determine what domain actions to perform in order to accomplish this goal. Many researchers (Allen, 1979; Carberry, 1987; Goodman and Litman, 1992; Pollack, 1990; Sidner, 1985) have shown that recognition of domain plans and goals gives a system the ability to address many difficult problems in understanding. Problem-solving actions relate to how the two dialogue participants are going about building a plan to achieve the planning agent&apos;s domain goal. Ramshaw, Litman, and Wilensky (Ramshaw, 1991; Litman and Allen, 1987; Wilensky, 1981) have noted the need for recognizing problem-solving actions. Discourse actions are the communicative actions that people perform in saying something, e.g., asking a question or expressing doubt. Recognition of discourse actions provides expectations for subsequent utterances, and explains the purpose of an utterance and how it should be interpreted. Our system&apos;s knowledge about how to perform actions is contained in a library of discourse, problem-solving, and domain recipes (Pollack, 1990). Although domain recipes are not mutually known by the participants (Pollack, 1990), h</context>
<context position="6483" citStr="Litman and Allen, 1987" startWordPosition="923" endWordPosition="926">agent2, Resolve-Conflict(_agent2, _agentl, _propl, _prop2)) Figure 1. Two Sample Discourse Recipes mon skills that people use in a wide variety of contexts, so the system can assume that knowledge about discourse and problem-solving recipes is shared knowledge. Figure 1 contains two discourse recipes. Our representation of a recipe includes a header giving the name of the recipe and the action that it accomplishes, preconditions, applicability conditions, constraints, a body, effects, and a goal. Constraints limit the allowable instantiation of variables in each of the components of a recipe (Litman and Allen, 1987). Applicability conditions (Carberry, 1987) represent conditions that must be satisfied in order for the recipe to be reasonable to apply in the given situation and, in the case of many of our discourse recipes, the applicability conditions capture beliefs that the dialogue participants must hold. Especially in the case of discourse recipes, the goals and effects are likely to be different. This allows us to differentiate between illocutionary and perlocutionary effects and to capture the notion that one can, for example, perform an inform act without the hearer adopting the communicated propo</context>
<context position="22215" citStr="Litman and Allen, 1987" startWordPosition="3485" endWordPosition="3488">l is captured by the DM and the current focus of attention within it. The system&apos;s world knowledge contains facts about. the world, the system&apos;s beliefs (including its beliefs about a stereotypical user&apos;s beliefs), and knowledge about how to go about performing discourse, problem-solving, and domain actions. The linguistic knowledge that we exploit includes the surface form of the utterance, which conveys beliefs and the strength of belief, as discussed in the preceding section, and linguistic clue words. Certain words often suggest what type of discourse action the speaker might be pursuing (Litman and Allen, 1987; Hinkelman, 1989). For example, the linguistic clue please suggests a request discourse act (Hinkelman, 1989) while the clue word but suggests a non-acceptance discourse act. Our model takes these linguistic clues into consideration in identifying the discourse acts performed by an utterance. Our investigation of naturally occurring dialogues indicates that listeners use a combination of information to determine what a speaker is trying to do in saying something. For example, S2&apos;s world knowledge of commonly held beliefs enabled S2 to determine that Si probably believes (c), and therefore inf</context>
</contexts>
<marker>Litman, Allen, 1987</marker>
<rawString>Litman, Diane and Allen, James (1987). A Plan Recognition Model for Subdialogues in Conversation. Cognitive Science, 11, 163-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Perrault</author>
</authors>
<title>An Application of Default Logic to Speech Act Theory. In</title>
<date>1990</date>
<booktitle>Intentions in Communication</booktitle>
<pages>161--185</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="11638" citStr="Perrault, 1990" startWordPosition="1741" endWordPosition="1742"> play in a dialogue, the system will not be able to generate cooperative responses which address the participants&apos; goals. In addition to recognizing discourse actions, it is also necessary for a cooperative system to recognize a user&apos;s changing beliefs as the dialogue progresses. Allen&apos;s representation of an Inform speech act (Allen, 1979) assumed that a listener adopted the communicated proposition. Clearly, listeners do not adopt everything they are told (e.g., (3) indicates that Si does not immediately accept that Dr. Smith is teaching Architecture). Perrault&apos;s persistence model of belief (Perrault, 1990) assumed that a listener adopted the communicated proposition unless the listener had conflicting beliefs. Since Perrault&apos;s model assumes that people&apos;s beliefs persist, it cannot account for Si eventually accepting the proposition that Dr. Smith is teaching Architecture. We show in Section 6 how our model overcomes this limitation. Our investigation of naturally occurring dialogues indicates that listeners are not passive participants, but instead assimilate each utterance into a dialogue in a multi-step acceptance phase. For statements,3 a listener first attempts to understand the utterance b</context>
</contexts>
<marker>Perrault, 1990</marker>
<rawString>Perrault, Raymond (1990). An Application of Default Logic to Speech Act Theory. In P. Cohen, J. Morgan, and M. Pollack (Eds.), Intentions in Communication (pp. 161-185). Cambridge, Massachusetts: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Pollack</author>
</authors>
<title>Plans as Complex Mental Attitudes. In</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3738" citStr="Pollack, 1990" startWordPosition="562" endWordPosition="564">gnizing the role of each utterance in a debate. In previous work (Lambert and Carberry, 1991), we described a tripartite plan-based model of dialogue that recognizes and differentiates three different kinds of actions: domain, problemsolving, and discourse. Domain actions relate to performing tasks in a given domain. We are modeling cooperative dialogues in which one agent has a domain goal and is working with another helpful, more expert agent to determine what domain actions to perform in order to accomplish this goal. Many researchers (Allen, 1979; Carberry, 1987; Goodman and Litman, 1992; Pollack, 1990; Sidner, 1985) have shown that recognition of domain plans and goals gives a system the ability to address many difficult problems in understanding. Problem-solving actions relate to how the two dialogue participants are going about building a plan to achieve the planning agent&apos;s domain goal. Ramshaw, Litman, and Wilensky (Ramshaw, 1991; Litman and Allen, 1987; Wilensky, 1981) have noted the need for recognizing problem-solving actions. Discourse actions are the communicative actions that people perform in saying something, e.g., asking a question or expressing doubt. Recognition of discourse</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>Pollack, Martha (1990). Plans as Complex Mental Attitudes. In P. R. Cohen, J. Morgan, and M. E. Pollack (Eds.), Intentions in Communication (pp 77-104). MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Quilici</author>
</authors>
<title>The Correction Machine: A Computer Model of Recognizing and Producing Belief Justifications in Argumentative Dialogs.</title>
<date>1991</date>
<tech>PhD thesis,</tech>
<institution>Department of Computer Science, University of California at</institution>
<location>Los Angeles, Los Angeles, California.</location>
<contexts>
<context position="2592" citStr="Quilici, 1991" startWordPosition="379" endWordPosition="380">nd recognizes the structure of negotiation subdialogues. 2 Previous Work Several researchers have built argument understanding systems, but none of these has addressed participants coming to an agreement or mutual belief about a particular situation, either because the arguments were only monologues &apos;This work is being supported by the National Science Foundation under Grant No. IRI-9122026. The Government has certain rights in this material. (Cohen, 1987; Cohen and Young, 1991), or because they assumed that dialogue participants do not change their minds (Flowers, McGuire and Birnbaum, 1982; Quilici, 1991). Others have examined more cooperative dialogues. Clark and Schaefer (1989) contend that utterances must be grounded, or understood, by both parties, but they do not address conflicts in belief, only lack of understanding. Walker (1991) has shown that evidence is often provided to ensure both understanding and believing an utterance, but she does not address recognizing lack of belief or lack of understanding. Reichman (1981) outlines a model for informal debate, but does not provide a detailed computational mechanism for recognizing the role of each utterance in a debate. In previous work (L</context>
</contexts>
<marker>Quilici, 1991</marker>
<rawString>Quilici, Alexander (1991). The Correction Machine: A Computer Model of Recognizing and Producing Belief Justifications in Argumentative Dialogs. PhD thesis, Department of Computer Science, University of California at Los Angeles, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
</authors>
<title>A Three-Level Model for Plan Exploration.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the ACL</booktitle>
<pages>36--46</pages>
<location>Berkeley, California.</location>
<contexts>
<context position="4077" citStr="Ramshaw, 1991" startWordPosition="615" endWordPosition="616">erative dialogues in which one agent has a domain goal and is working with another helpful, more expert agent to determine what domain actions to perform in order to accomplish this goal. Many researchers (Allen, 1979; Carberry, 1987; Goodman and Litman, 1992; Pollack, 1990; Sidner, 1985) have shown that recognition of domain plans and goals gives a system the ability to address many difficult problems in understanding. Problem-solving actions relate to how the two dialogue participants are going about building a plan to achieve the planning agent&apos;s domain goal. Ramshaw, Litman, and Wilensky (Ramshaw, 1991; Litman and Allen, 1987; Wilensky, 1981) have noted the need for recognizing problem-solving actions. Discourse actions are the communicative actions that people perform in saying something, e.g., asking a question or expressing doubt. Recognition of discourse actions provides expectations for subsequent utterances, and explains the purpose of an utterance and how it should be interpreted. Our system&apos;s knowledge about how to perform actions is contained in a library of discourse, problem-solving, and domain recipes (Pollack, 1990). Although domain recipes are not mutually known by the partici</context>
</contexts>
<marker>Ramshaw, 1991</marker>
<rawString>Ramshaw, Lance A. (1991). A Three-Level Model for Plan Exploration. In Proceedings of the 29th Annual Meeting of the ACL (pp. 36-46), Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel Reichman</author>
</authors>
<title>Modeling Informal Debates.</title>
<date>1981</date>
<booktitle>In Proceedings of the 1981 International Joint Conference on Artificial Intelligence (pp. 19-24),</booktitle>
<publisher>IJCAI.</publisher>
<location>Vancouver, B.C.</location>
<contexts>
<context position="3022" citStr="Reichman (1981)" startWordPosition="449" endWordPosition="450">in this material. (Cohen, 1987; Cohen and Young, 1991), or because they assumed that dialogue participants do not change their minds (Flowers, McGuire and Birnbaum, 1982; Quilici, 1991). Others have examined more cooperative dialogues. Clark and Schaefer (1989) contend that utterances must be grounded, or understood, by both parties, but they do not address conflicts in belief, only lack of understanding. Walker (1991) has shown that evidence is often provided to ensure both understanding and believing an utterance, but she does not address recognizing lack of belief or lack of understanding. Reichman (1981) outlines a model for informal debate, but does not provide a detailed computational mechanism for recognizing the role of each utterance in a debate. In previous work (Lambert and Carberry, 1991), we described a tripartite plan-based model of dialogue that recognizes and differentiates three different kinds of actions: domain, problemsolving, and discourse. Domain actions relate to performing tasks in a given domain. We are modeling cooperative dialogues in which one agent has a domain goal and is working with another helpful, more expert agent to determine what domain actions to perform in o</context>
</contexts>
<marker>Reichman, 1981</marker>
<rawString>Reichman, Rachel (1981). Modeling Informal Debates. In Proceedings of the 1981 International Joint Conference on Artificial Intelligence (pp. 19-24), Vancouver, B.C. IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace L Sidner</author>
</authors>
<title>Plan Parsing for Intended Response Recognition in Discourse.</title>
<date>1985</date>
<journal>Computational Intelligence,</journal>
<volume>1</volume>
<pages>1--10</pages>
<contexts>
<context position="3753" citStr="Sidner, 1985" startWordPosition="565" endWordPosition="566">e of each utterance in a debate. In previous work (Lambert and Carberry, 1991), we described a tripartite plan-based model of dialogue that recognizes and differentiates three different kinds of actions: domain, problemsolving, and discourse. Domain actions relate to performing tasks in a given domain. We are modeling cooperative dialogues in which one agent has a domain goal and is working with another helpful, more expert agent to determine what domain actions to perform in order to accomplish this goal. Many researchers (Allen, 1979; Carberry, 1987; Goodman and Litman, 1992; Pollack, 1990; Sidner, 1985) have shown that recognition of domain plans and goals gives a system the ability to address many difficult problems in understanding. Problem-solving actions relate to how the two dialogue participants are going about building a plan to achieve the planning agent&apos;s domain goal. Ramshaw, Litman, and Wilensky (Ramshaw, 1991; Litman and Allen, 1987; Wilensky, 1981) have noted the need for recognizing problem-solving actions. Discourse actions are the communicative actions that people perform in saying something, e.g., asking a question or expressing doubt. Recognition of discourse actions provid</context>
</contexts>
<marker>Sidner, 1985</marker>
<rawString>Sidner, Candace L. (1985). Plan Parsing for Intended Response Recognition in Discourse. Computational Intelligence, 1, 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
</authors>
<title>Redundancy in Collaborative Dialogue. Presented at The AAAI Fall Symposium:</title>
<date>1991</date>
<booktitle>Discourse Structure in Natural Language Understanding and Generation</booktitle>
<pages>124--129</pages>
<location>Asilomar, CA.</location>
<contexts>
<context position="2829" citStr="Walker (1991)" startWordPosition="416" endWordPosition="417">r situation, either because the arguments were only monologues &apos;This work is being supported by the National Science Foundation under Grant No. IRI-9122026. The Government has certain rights in this material. (Cohen, 1987; Cohen and Young, 1991), or because they assumed that dialogue participants do not change their minds (Flowers, McGuire and Birnbaum, 1982; Quilici, 1991). Others have examined more cooperative dialogues. Clark and Schaefer (1989) contend that utterances must be grounded, or understood, by both parties, but they do not address conflicts in belief, only lack of understanding. Walker (1991) has shown that evidence is often provided to ensure both understanding and believing an utterance, but she does not address recognizing lack of belief or lack of understanding. Reichman (1981) outlines a model for informal debate, but does not provide a detailed computational mechanism for recognizing the role of each utterance in a debate. In previous work (Lambert and Carberry, 1991), we described a tripartite plan-based model of dialogue that recognizes and differentiates three different kinds of actions: domain, problemsolving, and discourse. Domain actions relate to performing tasks in a</context>
<context position="18604" citStr="Walker, 1991" startWordPosition="2887" endWordPosition="2888">be represented: certain belief (a belief strength of C); strong but uncertain belief, as in (3) above (a belief strength of S); and a weak belief, as in I think that Dr. C might be an education instructor (a belief strength of W). Therefore, our model maintains three degrees of belief, three degrees of disbelief (indicated by attaching a subscript of N, such as SN to represent strong disbelief and WN to represent weak disbelief), and one degree indicating no belief about a proposition (a belief strength of 0).5 Our belief model uses belief intervals to specify the range of strengths 5 Others (Walker, 1991; Galliers, 1991) have also argued for multiple strengths of belief, basing the strength of belief on the amount and kind of evidence available for that belief. We have not investigated how much evidence is needed for an agent to have a particular amount of confidence in a belief; our work has concentrated on recognizing how the strength of belief is communicated in a discourse and the impact that the different belief strengths have on the recognition of discourse acts. 196 within which an agent&apos;s beliefs are thought to fall, and our discourse recipes use belief intervals to specify the range </context>
</contexts>
<marker>Walker, 1991</marker>
<rawString>Walker, Marilyn (1991). Redundancy in Collaborative Dialogue. Presented at The AAAI Fall Symposium: Discourse Structure in Natural Language Understanding and Generation (pp. 124-129), Asilomar, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Wilensky</author>
</authors>
<title>Meta-Planning: Representing and Using Knowledge About Planning in Problem Solving and Natural Language Understanding.</title>
<date>1981</date>
<journal>Cognitive Science,</journal>
<volume>5</volume>
<pages>197--233</pages>
<contexts>
<context position="4118" citStr="Wilensky, 1981" startWordPosition="621" endWordPosition="623">s a domain goal and is working with another helpful, more expert agent to determine what domain actions to perform in order to accomplish this goal. Many researchers (Allen, 1979; Carberry, 1987; Goodman and Litman, 1992; Pollack, 1990; Sidner, 1985) have shown that recognition of domain plans and goals gives a system the ability to address many difficult problems in understanding. Problem-solving actions relate to how the two dialogue participants are going about building a plan to achieve the planning agent&apos;s domain goal. Ramshaw, Litman, and Wilensky (Ramshaw, 1991; Litman and Allen, 1987; Wilensky, 1981) have noted the need for recognizing problem-solving actions. Discourse actions are the communicative actions that people perform in saying something, e.g., asking a question or expressing doubt. Recognition of discourse actions provides expectations for subsequent utterances, and explains the purpose of an utterance and how it should be interpreted. Our system&apos;s knowledge about how to perform actions is contained in a library of discourse, problem-solving, and domain recipes (Pollack, 1990). Although domain recipes are not mutually known by the participants (Pollack, 1990), how to communicate</context>
</contexts>
<marker>Wilensky, 1981</marker>
<rawString>Wilensky, Robert (1981). Meta-Planning: Representing and Using Knowledge About Planning in Problem Solving and Natural Language Understanding. Cognitive Science, 5, 197-233.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>