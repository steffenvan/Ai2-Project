<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002558">
<title confidence="0.784875">
SRCB-WSD: Supervised Chinese Word Sense Disambiguation
with Key Features
</title>
<author confidence="0.994007">
Yun Xing
</author>
<affiliation confidence="0.998987">
Ricoh Software Research Center Beijing Co., Ltd
</affiliation>
<address confidence="0.596297">
Beijing, China
</address>
<email confidence="0.99824">
yun.xing@srcb.ricoh.com
</email>
<sectionHeader confidence="0.996651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972666666667">
This article describes the implementation
of Word Sense Disambiguation system that
participated in the SemEval-2007 multilin-
gual Chinese-English lexical sample task.
We adopted a supervised learning approach
with Maximum Entropy classifier. The fea-
tures used were neighboring words and their
part-of-speech, as well as single words in the
context, and other syntactic features based
on shallow parsing. In addition, we used
word category information of a Chinese the-
saurus as features for verb disambiguation.
For the task we participated in, we obtained
precision of 0.716 in micro-average, which
is the best among all participated systems.
</bodyText>
<sectionHeader confidence="0.998744" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999627708333333">
Word Sense Disambiguation(WSD) is the process of
assigning a meaning to a word based on the context
in which it occurs. It is very important to many re-
search fields such as Machine Translation, Informa-
tion Retrieval. The goal of the multilingual Chinese-
English lexical sample task in SemEval-2007 is to
predict the correct English translation for an am-
biguous Chinese word w.
We considered this task as a classification prob-
lem, and our system adopted a supervised learning
approach with Maximum Entropy classifier, which
is widely used in natural language processing(NLP).
Within the Maximum Entropy framework, evidence
from different features can be combined with no as-
sumptions of feature independence. The used fea-
tures include neighboring words and their part-of-
speech(POS), single words in the context, and other
syntactic features based on shallow parsing. In ad-
dition, we used word category information of a Chi-
nese thesaurus for verb disambiguation. Note that
we did not do any feature selection in this work.
Next, we will describe the Maximum Entropy
framework and detail the features used in our WSD
system.
</bodyText>
<sectionHeader confidence="0.930012" genericHeader="method">
2 Maximum Entropy
</sectionHeader>
<bodyText confidence="0.999953666666667">
Maximum entropy modelling is a framework for in-
tegrating information from many heterogeneous in-
formation sources for classification (Manning and
Sch¨utze, 1999). It has been successfully applied
to a wide range of NLP tasks, including sentence
boundary detection, POS tagging, and parsing (Rat-
naparkhi, 1998) . The system estimates the condi-
tional probability that an ambiguous word has sense
x given that it occurs in context y, where y is a con-
junction of features. The estimated probability is
derived from feature weights which are determined
automatically from training data so as to produce a
probability distribution that has maximum entropy,
under the constraint that it is consistent with ob-
served evidence (Dang et al., 2002). We used the im-
plementation of Maximum Entropy framework with
OpenNLP MAXENT1, where each nominal feature
was represented as “feature code=value”. Based on
this framework, we defined the feature set and im-
plemented the interface of feature extraction. For
the convenient of evaluation, the default parameters
</bodyText>
<footnote confidence="0.988722">
1http://maxent.sourceforge.net/
</footnote>
<page confidence="0.970158">
300
</page>
<bodyText confidence="0.909762">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 300–303,
Prague, June 2007. c�2007 Association for Computational Linguistics
of training model were used.
</bodyText>
<sectionHeader confidence="0.984156" genericHeader="method">
3 Used Features
</sectionHeader>
<bodyText confidence="0.9976667">
Many research (Stevenson and Wilks, 2001; Lee
and Ng, 2002) have indicated that a combination of
knowledge sources improves WSD accuracy, but not
any kind of knowledge source contributes the im-
provement of Chinese WSD (Dang et al., 2002). For
multilingual Chinese-English lexical sample task,
some basic features can be obtained directly. Also,
we extracted other syntactic features through shal-
low parsing. In addition, we used word category in-
formation for verb disambiguation.
</bodyText>
<subsectionHeader confidence="0.99907">
3.1 Basic Features
</subsectionHeader>
<bodyText confidence="0.999976333333333">
Since the data of multilingual Chinese-English lex-
ical sample task are word-segmented and POS-
tagged, we can get the following features directly.
</bodyText>
<listItem confidence="0.9977518">
• W_1(+1): the words (if any) immediately pre-
ceding and following w
• P_1(+1): the POS of the words(if any) imme-
diately preceding and following w
•
</listItem>
<bodyText confidence="0.9994153">
SW: single words in the context. We did not
consider all words in the context as features
for WSD, because our experiment shows that
it will bring some noise in small scale super-
vised learning if we add all words in the con-
text to feature set(See Section 4.1 for details).
After carefully analyzing the POS set specifi-
cation which is provided by task organizers, we
only picked out words of POS listed in Table 1
as features.
</bodyText>
<subsectionHeader confidence="0.994034">
3.2 Syntactic Features based on Shallow
Parsing
</subsectionHeader>
<bodyText confidence="0.999398777777778">
To get further syntactic features from context, we
implemented a simple rule-based parser to do shal-
low parsing on each instance. The parser only identi-
fies phrases such as noun phrase, verb phrase, adjec-
tival phrase, time phrase, position phrase and quan-
tity phrase. These phrases are considered as con-
stituents of context, as well as words and punctua-
tions which do not belong to any phrase. Table 2
lists the constituent types and relative tags.
</bodyText>
<table confidence="0.999534615384615">
POS Tag Specification
Ng Nominal morpheme
n Noun
nr Personal name
ns Place name
nt Institution and Group
nz Any other proper names
Vg Verbal morpheme
v Verb
vd Verb with the attribute of adverb
vn Verb with the attribute of noun
r Pronoun
j Abbreviation
</table>
<tableCaption confidence="0.956767">
Table 1: POS of single words in the context to be
considered in our WSD system
</tableCaption>
<bodyText confidence="0.860124">
For example, a word-segmented and POS-tagged
instance in Figure 1 would be processed as a con-
stituent list in Figure 2 after shallow parsing.
</bodyText>
<figure confidence="0.302144">
lNr �_,QL-A/d !�Y/v lA/n [6/u _ M/n o/w
</figure>
<figureCaption confidence="0.676676857142857">
Figure 1: A word-segmented and POS-tagged in-
stance. Note that the instance is not illustrated
in XML format as data of multilingual Chinese-
English lexical sample task, instead, it is illustrated
in the form of “word/pos” for convenient.
lNentity &amp;1 !M/action &apos;1�61[6_ Ufl/entity o/w
Figure 2: After shallow parsing, instance is orga-
</figureCaption>
<bodyText confidence="0.8400325">
nized in the form of “constituent/tag”, that is, the
word “it” is identified as an entity, and words “a
Tf” and “iAH)J” are merged together as an action.
Suppose C0 is the constituent which the target
word w belongs to , then we add following infor-
mation to feature set:
</bodyText>
<listItem confidence="0.986544">
• CT0: the constituent tag of C0
• CT�Z(+Z), 0 &lt; i &lt; 3: the tag of ith constituent
to the left(right) of C0
• KCT�Z(+Z), 0 &lt; i &lt; 3: the tag of ith con-
stituent to the left(right) of C0, and the type
must be entity or action
</listItem>
<page confidence="0.994314">
301
</page>
<table confidence="0.999470375">
Constituent type Tag
noun phrase entity
verb phrase action
adjective phrase adjective
time phrase time
place phrase place
quantity phrase quantity
non-phrase same as POS tag
</table>
<tableCaption confidence="0.994916">
Table 2: Constituent type and relative tag
</tableCaption>
<listItem confidence="0.9982155">
• LPO5_Z(+z): the POS of ith word in the same
constituent of w.
</listItem>
<subsectionHeader confidence="0.994198">
3.3 Word Category Information
</subsectionHeader>
<bodyText confidence="0.930206888888889">
We considered word category information as an im-
portant knowledge source for verb disambiguation.
The word category information comes from a Chi-
nese thesaurus (Mei et al., 1983). If w is a verb, then
the word category information of nouns in the right
side of w is added into feature set. Figure 3 shows
an example of how to use word category information
for verb disambiguation.
lNr V/v _�P/n Q/v JDV /ns
</bodyText>
<figureCaption confidence="0.990161">
Figure 3: A word-segmented and POS-tagged in-
</figureCaption>
<bodyText confidence="0.9125938">
stance of ambiguous verb “�”. The word category
information of noun “--I, tR” has to be added into fea-
ture set.
Note that some nouns can belong to more than
two categories, in this case, we do not use the word
category information of this kind of noun for disam-
biguation.
Our experiment showed that this extra knowledge
source did improve the accuracy of WSD (See 4.1
for detail).
</bodyText>
<sectionHeader confidence="0.997182" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.986888857142857">
Since the multilingual Chinese-English lexical sam-
ple task of SemEval-2007 is quite similar to the Chi-
nese lexical sample task of SENSEVAL-3, we firstly
evaluated feature set on the data of SENSEVAL-3
Chinese lexical sample task, and then gave the of-
ficial SemEval-2007 scores of our system based on
the best feature set.
</bodyText>
<table confidence="0.936688">
Feature Set Micro-average precision
FS1 0.630
FS2 0.635
FS3 0.654
</table>
<tableCaption confidence="0.912153">
Table 3: Result of feature set evaluation on
SENSEVAL-3 test data
</tableCaption>
<table confidence="0.999082">
System Micro-average Macro-average
precision precision
SRCB-WSD 0.716 0.749
</table>
<tableCaption confidence="0.999091">
Table 4: Official result on SemEval-2007 test data
</tableCaption>
<subsectionHeader confidence="0.961682">
4.1 Evaluation on SENSEVAL-3 Data
</subsectionHeader>
<bodyText confidence="0.999992722222222">
We did three experiments on the data of
SENSEVAL-3 Chinese lexical sample task to
evaluate if all the single words in the context should
be included in feature set, and if the word category
information of Chinese thesaurus is helpful for
WSD. The first experiment used feature set (FS1)
included almost the same features listed in Section
3.1 and 3.2, the only difference is that all single
words in the context were considered. The second
experiment used feature set (FS2) included all the
features listed in Section 3.1 and 3.2. The third
experiment used feature set (FS3) included all the
features listed in Section 3.1, 3.2 and 3.3. The
experimental result is given in Table 3. It shows
that considering all single words in the context
as features did not improve the performance of
WSD, while word category information of Chinese
thesaurus improved the accuracy obviously.
</bodyText>
<subsectionHeader confidence="0.986713">
4.2 Official SemEval-2007 Scores
</subsectionHeader>
<bodyText confidence="0.9960788">
In multilingual Chinese-English lexical sample task,
there are 2686 instances in training data for 40 Chi-
nese ambiguous words. All these ambiguous words
are noun or verb. Test data consist of 935 untagged
instances of the same target words. The official re-
sult of our system in multilingual Chinese-English
lexical sample task is reported in Table 4.
According to the task organizers, our system
achieved the best performance out of all the partici-
pated systems.
</bodyText>
<page confidence="0.997031">
302
</page>
<sectionHeader confidence="0.998998" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999856875">
In this paper, we described our participating system
in the SemEval-2007 multilingual Chinese-English
lexical sample task. We adopted Maximum Entropy
method, and collected features not only from con-
text provided by task organizers, but also from extra
knowledge source. Evaluation results show that this
feature set is much effective for supervised Chinese
WSD.
</bodyText>
<sectionHeader confidence="0.996365" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999798">
We would like to thank the anonymous reviewers for
their constructive comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999207" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999839789473684">
Manning, C. and Sch¨utze, H. 1999. Foundations of
Statistical Natural Language Processing.. The MIT
Press, Cambridge, Massachusetts.
Ratnaparkhi, A. 1998. Maximum Entropy Models for
Natural Language Ambiguity Resolution. Ph.D. thesis
University of Pennsylvania.
Dang, H.T., Chia, C.Y., Palmer, M. and Chiou, F.D.
2002. Simple Features for Chinese Word Sense Dis-
ambiguation. In Proc. of COLING.
Mei, J.J., Li, Y.M., Gao, Y.Q. and et al. 1983. Chinese
thesaurus(Tongyici Cilin). Shanghai thesaurus Press.
Stevenson, M. and Wilks, Y. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321-349.
Lee, Y.K. and Ng, H.T. 2002. An empirical evaluation of
knowledge sources and learning algorithms for word
sense disambiguation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing(EMNLP), pages 41-48.
</reference>
<page confidence="0.999469">
303
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.892734">
<title confidence="0.998247">SRCB-WSD: Supervised Chinese Word Sense Disambiguation with Key Features</title>
<author confidence="0.997416">Yun Xing</author>
<affiliation confidence="0.981309">Ricoh Software Research Center Beijing Co., Ltd</affiliation>
<address confidence="0.920747">Beijing, China</address>
<email confidence="0.999907">yun.xing@srcb.ricoh.com</email>
<abstract confidence="0.999119875">This article describes the implementation of Word Sense Disambiguation system that participated in the SemEval-2007 multilingual Chinese-English lexical sample task. We adopted a supervised learning approach with Maximum Entropy classifier. The features used were neighboring words and their part-of-speech, as well as single words in the context, and other syntactic features based on shallow parsing. In addition, we used word category information of a Chinese thesaurus as features for verb disambiguation. For the task we participated in, we obtained precision of 0.716 in micro-average, which is the best among all participated systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing..</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Manning, C. and Sch¨utze, H. 1999. Foundations of Statistical Natural Language Processing.. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2281" citStr="Ratnaparkhi, 1998" startWordPosition="344" endWordPosition="346"> features based on shallow parsing. In addition, we used word category information of a Chinese thesaurus for verb disambiguation. Note that we did not do any feature selection in this work. Next, we will describe the Maximum Entropy framework and detail the features used in our WSD system. 2 Maximum Entropy Maximum entropy modelling is a framework for integrating information from many heterogeneous information sources for classification (Manning and Sch¨utze, 1999). It has been successfully applied to a wide range of NLP tasks, including sentence boundary detection, POS tagging, and parsing (Ratnaparkhi, 1998) . The system estimates the conditional probability that an ambiguous word has sense x given that it occurs in context y, where y is a conjunction of features. The estimated probability is derived from feature weights which are determined automatically from training data so as to produce a probability distribution that has maximum entropy, under the constraint that it is consistent with observed evidence (Dang et al., 2002). We used the implementation of Maximum Entropy framework with OpenNLP MAXENT1, where each nominal feature was represented as “feature code=value”. Based on this framework, </context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Ratnaparkhi, A. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
<author>C Y Chia</author>
<author>M Palmer</author>
<author>F D Chiou</author>
</authors>
<title>Simple Features for Chinese Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="2708" citStr="Dang et al., 2002" startWordPosition="414" endWordPosition="417">classification (Manning and Sch¨utze, 1999). It has been successfully applied to a wide range of NLP tasks, including sentence boundary detection, POS tagging, and parsing (Ratnaparkhi, 1998) . The system estimates the conditional probability that an ambiguous word has sense x given that it occurs in context y, where y is a conjunction of features. The estimated probability is derived from feature weights which are determined automatically from training data so as to produce a probability distribution that has maximum entropy, under the constraint that it is consistent with observed evidence (Dang et al., 2002). We used the implementation of Maximum Entropy framework with OpenNLP MAXENT1, where each nominal feature was represented as “feature code=value”. Based on this framework, we defined the feature set and implemented the interface of feature extraction. For the convenient of evaluation, the default parameters 1http://maxent.sourceforge.net/ 300 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 300–303, Prague, June 2007. c�2007 Association for Computational Linguistics of training model were used. 3 Used Features Many research (Stevenson and Wilks, 2001</context>
</contexts>
<marker>Dang, Chia, Palmer, Chiou, 2002</marker>
<rawString>Dang, H.T., Chia, C.Y., Palmer, M. and Chiou, F.D. 2002. Simple Features for Chinese Word Sense Disambiguation. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Mei</author>
<author>Y M Li</author>
<author>Y Q Gao</author>
</authors>
<date>1983</date>
<booktitle>Chinese thesaurus(Tongyici Cilin). Shanghai thesaurus</booktitle>
<publisher>Press.</publisher>
<contexts>
<context position="6816" citStr="Mei et al., 1983" startWordPosition="1101" endWordPosition="1104">ht) of C0 • KCT�Z(+Z), 0 &lt; i &lt; 3: the tag of ith constituent to the left(right) of C0, and the type must be entity or action 301 Constituent type Tag noun phrase entity verb phrase action adjective phrase adjective time phrase time place phrase place quantity phrase quantity non-phrase same as POS tag Table 2: Constituent type and relative tag • LPO5_Z(+z): the POS of ith word in the same constituent of w. 3.3 Word Category Information We considered word category information as an important knowledge source for verb disambiguation. The word category information comes from a Chinese thesaurus (Mei et al., 1983). If w is a verb, then the word category information of nouns in the right side of w is added into feature set. Figure 3 shows an example of how to use word category information for verb disambiguation. lNr V/v _�P/n Q/v JDV /ns Figure 3: A word-segmented and POS-tagged instance of ambiguous verb “�”. The word category information of noun “--I, tR” has to be added into feature set. Note that some nouns can belong to more than two categories, in this case, we do not use the word category information of this kind of noun for disambiguation. Our experiment showed that this extra knowledge source </context>
</contexts>
<marker>Mei, Li, Gao, 1983</marker>
<rawString>Mei, J.J., Li, Y.M., Gao, Y.Q. and et al. 1983. Chinese thesaurus(Tongyici Cilin). Shanghai thesaurus Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>Y Wilks</author>
</authors>
<title>The interaction of knowledge sources in word sense disambiguation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--3</pages>
<contexts>
<context position="3308" citStr="Stevenson and Wilks, 2001" startWordPosition="496" endWordPosition="499">dence (Dang et al., 2002). We used the implementation of Maximum Entropy framework with OpenNLP MAXENT1, where each nominal feature was represented as “feature code=value”. Based on this framework, we defined the feature set and implemented the interface of feature extraction. For the convenient of evaluation, the default parameters 1http://maxent.sourceforge.net/ 300 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 300–303, Prague, June 2007. c�2007 Association for Computational Linguistics of training model were used. 3 Used Features Many research (Stevenson and Wilks, 2001; Lee and Ng, 2002) have indicated that a combination of knowledge sources improves WSD accuracy, but not any kind of knowledge source contributes the improvement of Chinese WSD (Dang et al., 2002). For multilingual Chinese-English lexical sample task, some basic features can be obtained directly. Also, we extracted other syntactic features through shallow parsing. In addition, we used word category information for verb disambiguation. 3.1 Basic Features Since the data of multilingual Chinese-English lexical sample task are word-segmented and POStagged, we can get the following features direct</context>
</contexts>
<marker>Stevenson, Wilks, 2001</marker>
<rawString>Stevenson, M. and Wilks, Y. 2001. The interaction of knowledge sources in word sense disambiguation. Computational Linguistics, 27(3):321-349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y K Lee</author>
<author>H T Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing(EMNLP),</booktitle>
<pages>41--48</pages>
<contexts>
<context position="3327" citStr="Lee and Ng, 2002" startWordPosition="500" endWordPosition="503">We used the implementation of Maximum Entropy framework with OpenNLP MAXENT1, where each nominal feature was represented as “feature code=value”. Based on this framework, we defined the feature set and implemented the interface of feature extraction. For the convenient of evaluation, the default parameters 1http://maxent.sourceforge.net/ 300 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 300–303, Prague, June 2007. c�2007 Association for Computational Linguistics of training model were used. 3 Used Features Many research (Stevenson and Wilks, 2001; Lee and Ng, 2002) have indicated that a combination of knowledge sources improves WSD accuracy, but not any kind of knowledge source contributes the improvement of Chinese WSD (Dang et al., 2002). For multilingual Chinese-English lexical sample task, some basic features can be obtained directly. Also, we extracted other syntactic features through shallow parsing. In addition, we used word category information for verb disambiguation. 3.1 Basic Features Since the data of multilingual Chinese-English lexical sample task are word-segmented and POStagged, we can get the following features directly. • W_1(+1): the </context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Lee, Y.K. and Ng, H.T. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing(EMNLP), pages 41-48.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>