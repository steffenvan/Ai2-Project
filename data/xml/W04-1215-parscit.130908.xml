<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.043883">
<title confidence="0.999635">
Annotating Multiple Types of Biomedical Entities:
A Single Word Classification Approach
</title>
<author confidence="0.982474">
Chih Lee, Wen-Juan Hou and Hsin-Hsi Chen
</author>
<affiliation confidence="0.992741666666667">
Natural Language Processing Laboratory
Department of Computer Science and Information Engineering
National Taiwan University
</affiliation>
<address confidence="0.927769">
1 Roosevelt Road, Section 4, Taipei, Taiwan, 106
</address>
<email confidence="0.991872">
{clee, wjhou}@nlg.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw
</email>
<sectionHeader confidence="0.992446" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928">
Named entity recognition is a fundamental
task in biomedical data mining. Multiple-class
annotation is more challenging than single-
class annotation. In this paper, we took a
single word classification approach to dealing
with the multiple-class annotation problem
using Support Vector Machines (SVMs).
Word attributes, results of existing
gene/protein name taggers, context, and other
information are important features for
classification. During training, the size of
training data and the distribution of named
entities are considered. The preliminary
results showed that the approach might be
feasible when more training data is used to
alleviate the data imbalance problem.
</bodyText>
<sectionHeader confidence="0.998692" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998813365384616">
The volumn of on-line material in the biomedical
field has been growing steadily for more than 20
years. Several attempts have been made to mine
knowledge from biomedical documents, such as
identifying gene/protein names, recognizing
protein interactions, and capturing specific
relations in databases. Among these, named entity
recognition is a fundamental step to mine
knowledge from biological articles.
Previous approaches on biological named entity
extraction can be classified into two types – rule-
based (Fukuda et al., 1998; Olsson et al., 2002;
Tanabe and Wilbur, 2002) and corpus-based
(Collier et al., 2000; Chang et al., 2004). Yapex
(Olsson et al., 2002) implemented some heuristic
steps described by Fukuda, et al., and applied
filters and knowledge bases to remove false alarms.
Syntactic information obtained from the parser was
incorporated as well. GAPSCORE (Chang et al.,
2004) scored words on the basis of statistical
models that quantified their appearance,
morphology and context. The models includes
Naive Bayes (Manning and Schutze, 1999),
Maximum Entropy (Ratnaparkhi, 1998) and
Support Vector Machines (Burges, 1998).
GAPSCORE also used Brill’s tagger (Brill, 1994)
to get the POS tag to filter out some words that are
clearly not gene or protein names. Efforts have
been made (Hou and Chen, 2002, 2003; Tsuruoka
and Tsujii, 2003) to improve the performance. The
nature of classification makes it possible to
integrate existing approaches by extracting good
features from them. Several works employing
SVM classifier have been done (Kazama et al.,
2002; Lee et al., 2003; Takeuchi and Collier, 2003;
Yamamoto et al., 2003), and will be discussed
further in the rest of this paper.
Collocation denotes two or more words having
strong relationships (Manning and Schutze, 1999).
Hou and Chen (2003) showed that protein/gene
collocates are capable of assisting existing
protein/gene taggers. In this paper, we addressed
this task as a multi-class classification problem
with SVMs and extended the idea of collocation to
generate features at word and pattern level in our
method. Existing protein/gene recognizers were
used to perform feature extraction as well.
The rest of this paper is organized as follows.
The methods used in this study are introduced in
Section 2. The experimental results are shown and
discussed in Section 3. Finally, Section 4
concludes the remarks and lists some future works.
</bodyText>
<sectionHeader confidence="0.993147" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.999720285714286">
Most of the works in the past on recognizing
named entities in the biomedical domain focused
on identifying a single type of entities like protein
and/or gene names. It is obviously more
challenging to annotate multiple types of named
entities simultaneously. Intuitively, one can
develop a specific recognizer for each type of
named entities, run the recognizers one by one to
annotate all types of named entities, and merge the
results. The problem results from the boundary
decision and the annotation conflicts. Instead of
constructing five individual recognizers, we
regarded the multiple-class annotation as a
classification problem, and tried to learn a
</bodyText>
<page confidence="0.994386">
80
</page>
<bodyText confidence="0.999076636363636">
classifier capable of identifying all the five types of
named entities.
Before classification, we have to decide the unit
of classification. Since it is difficult to correctly
mark the boundary of a name to be identified, the
simplest way is to consider an individual word as
an instance and assign a type to it. After the type
assignment, continuous words of the same type
will be marked as a complete named entity of that
type. The feature extraction process will be
described in the following subsections.
</bodyText>
<subsectionHeader confidence="0.993348">
2.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999981142857143">
The first step in classification is to extract
informative and useful features to represent an
instance to be classified. In our work, one word is
represented by the attributes carried per se, the
attributes contributed by two surrounding words,
and other contextual information. The details are
as follows.
</bodyText>
<subsubsectionHeader confidence="0.492539">
2.1.1 Word Attributes
</subsubsectionHeader>
<bodyText confidence="0.999981833333333">
The word “attribute” is sometimes used
interchangeably with “feature”, but in this article
they denote two different concepts. Features are
those used to represent a classification instance,
and the information enclosed in the features is not
necessarily contributed by the word itself.
Attributes are defined to be the information that
can be derived from the word alone in this paper.
The attributes assigned to each word are whether
it is part of a gene/protein name, whether it is part
of a species name, whether it is part of a tissue
name, whether it is a stop word, whether it is a
number, whether it is punctuation, and the part of
speech of this word. Instead of using a lexicon for
gene/protein name annotation, we employed two
gene/protein name taggers, Yapex and
GAPSCORE, to do this job. As for part of speech
tagging, Brill’s part of speech tagger was adopted.
</bodyText>
<subsubsectionHeader confidence="0.642532">
2.1.2 Context Information Preparation
</subsubsectionHeader>
<bodyText confidence="0.999911764705882">
Contextual information has been shown helpful in
annotating gene/protein names, and therefore two
strategies for extracting contextual information at
different levels are used. One is the usual practice
at a word level, and the other is at a pattern level.
Since the training data released in the beginning
does not define the abstract boundary, we have to
assume that sentences are independent of each
other, and the contextual information extraction
was thus limited to be within a sentence.
For contextual information extraction at word
level (Hou and Chen, 2003), collocates along with
4 statistics including frequency, the average and
standard error of distance between word and entity
and t-test score, were extracted. The frequency
and t-test score were normalized to [0, 1]. Five
lists of collocates were obtained for cell-line, cell-
type, DNA, RNA, and protein, respectively.
As for contextual information extraction at
pattern level, we first gathered a list of words
constituting a specific type of named entities.
Then a hierarchical clustering with cutoff threshold
was performed on the words. Edit distance was
adopted as the measure of dissimilarity (see Figure
1). Afterwards, common substrings were obtained
to form the list of patterns. With a list of patterns
at hand, we estimated the pattern distribution, the
occurrence frequencies at and around the current
position, given the type of word at the current
position. Figure 2 showed an example of the
estimated distribution. The average KL-
Divergence between any two distributions was
computed to discriminate the power of each pattern.
The formula is as follows:
</bodyText>
<equation confidence="0.988972666666667">
n n
∑ ∑ D(p  ||pj) , where pi and pj
n(n−1) i=1 j=1,j≠i
</equation>
<bodyText confidence="0.9967395">
are the distributions of a pattern given the word at
position 0 being type i and j, respectively.
</bodyText>
<figureCaption confidence="0.999862333333333">
Figure 1: Example of common substring extraction
Figure 2: Pattern distributions given the type of
word at position 0
</figureCaption>
<subsectionHeader confidence="0.999832">
2.2 Constructing Training Data
</subsectionHeader>
<bodyText confidence="0.981631428571429">
For each word in a sentence, the attributes of the
word and the two adjacent words are put into the
feature vector. Then, the left five and the right five
words are searched for previously extracted
collocates. The 15 variables thus added are shown
below.
Freq w type
</bodyText>
<equation confidence="0.864825916666667">
( i  |)
t test score w type
− _ ( i  |)
1
5
∑
i =−
5, 0
i ≠
5
∑
i =−
</equation>
<page confidence="0.891168">
5,0
i≠
81
</page>
<equation confidence="0.90878875">
5
E f (i  |F&apos;`,
witype,awi,type) , where f is the pdf of
i =−5j10
</equation>
<bodyText confidence="0.97709275">
normal distribution, type is one of the five types, wi
denotes the surrounding words, µwi and
(ype
the maximum likelihood estimates of mean and
standard deviation for wi given the type. Next, the
left three and right three words along with the
current word are searched for patterns, adding 6
variables to the feature vector.
</bodyText>
<equation confidence="0.967614">
3
E E Probp (i  |type) , where type is one of the
i =−3 pÎ Pwi
</equation>
<bodyText confidence="0.9992418">
six types including ‘O’, Pwi is the set of patterns
matching wi, Probp denotes the pmf for pattern p.
Finally, the type of the previous word is added to
the feature vector, mimicking the concept of a
stochastic model.
</bodyText>
<subsectionHeader confidence="0.982431">
2.3 Classification
</subsectionHeader>
<bodyText confidence="0.999703785714286">
Support Vector Machines classification with radial
basis kernel was adopted in this task, and the
package LIBSVM – A Library for Support Vector
Machines (Hsu et al., 2003) was used for training
and prediction. The penalty coefficient C in
optimization and gamma in kernel function were
tuned using a script provided in this package.
The constructed training data contains 492,551
instances, which is too large for training. Also, the
training data is extremely unbalanced (see Table 1)
and this is a known problem in SVMs
classification. Therefore, we performed stratified
sampling to form a smaller and balanced data set
for training.
</bodyText>
<table confidence="0.994282285714286">
Type # of instances (words)
cell-type 15,466
DNA 25,307
cell-line 11,217
RNA 2,481
protein 55,117
O 382,963
</table>
<tableCaption confidence="0.999725">
Table 1: Number of instances for each type
</tableCaption>
<sectionHeader confidence="0.998558" genericHeader="related work">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.998725586956522">
Since there is a huge amount of training instances
and we do not have enough time to tune the
parameters and train a model with all the training
instances available, we first randomly selected one
tenth and one fourth of the complete training data.
The results, as we expected, showed that model
trained with more instances performed better (see
Table 2). However, we noticed that the
performances vary among the 6 types and one of
the possible causes is the imbalance of training
data among classes (see Table 1). Therefore we
decided to balance the training data.
First, the training data was constructed to
comprise equal number of instances from each
class. However, it didn’t perform well and lots of
type ‘O’ words were misclassified, indicating that
using only less than 1% of type ‘O’ training
instances is not sufficient to train a good model.
Thus two more models were trained to see if the
performance can be enhanced. One model has
slightly more type ‘O’ instances than the equally
balanced one, and the other model has the ratio
among classes being 4:8:4:1:8:16. The results
showed increase in recall but drop in precision.
Kazama et al. (2002) addressed the data
imbalance problem and sped up the training
process by splitting the type ‘O’ instances into sub-
classes using part-of-speech information. However,
we missed their work while we were doing this
task, and hence didn’t have the chance to use and
extend this idea.
After carefully examining the classification
results, we found that many of the ‘DNA’
instances were classified as ‘protein’ and many of
the ‘protein’ instances were classified as ‘DNA’.
For example, 904 out of 2,845 ‘DNA’ instances
were categorized as ‘protein’ under ‘model 1/4’.
The reason may be that Yapex and GAPSCORE do
not distinguish gene name from protein names.
Even humans don’t do very well at this
(Krauthammer et al., 2002).
We originally planned to verify the contribution
of each type of features. For example, how much
noise was introduced by using existing taggers
instead of lexicons. This would have helped gain
more insights into the proposed features.
</bodyText>
<sectionHeader confidence="0.98026" genericHeader="conclusions">
4 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.999858533333333">
This paper presented the preliminary results of our
study. We introduced the use of existing taggers
and presented a way to collect common substrings
shared by entities. Due to lack of time, the models
were not well tuned against the two parameters – C
and gamma, influencing the capabilities of the
models. Further, not all of the training instances
provided were used to train the model, and it will
be interesting and worthwhile to investigate. How
to deal with data imbalance is another important
issue. By solving this problem, further evaluation
of feature effectiveness would be facilitated. We
believe there is much left for our approach to
improve and it may perform better if more time is
given.
</bodyText>
<figure confidence="0.969956666666667">
ˆ
a witype are
,
</figure>
<page confidence="0.980343">
82
</page>
<table confidence="0.999782">
Model 1/10 Model 1/4
Recall Prec. F-score Recall Prec. F-score Recall Prec. F-score
Full (Object) 0.4756 0.4399 0.4571 0.5080 0.4759 0.4914
Full (protein) 0.5846 0.4392 0.5016 0.6213 0.4614 0.5296
Full (cell-line) 0.2420 0.2909 0.2642 0.2820 0.3341 0.3059
Full (DNA) 0.2784 0.3249 0.2998 0.2888 0.4479 0.3512
Full (cell-type) 0.3863 0.5752 0.4622 0.4196 0.6115 0.4977
Full (RNA) 0.0085 0.1000 0.0156 0.0000 0.0000 0.0000
Model balanced equally Model slightly more ‘O’ Model 4:8:4:1:8:16
Full (Object) 0.1480 0.0990 0.1186 0.1512 0.1002 0.1206 0.5036 0.3936 0.4419
Full (protein) 0.1451 0.1533 0.1491 0.1458 0.1527 0.1492 0.5629 0.4280 0.4863
Full (cell-line) 0.1580 0.0651 0.0922 0.2280 0.0319 0.0560 0.4060 0.2261 0.2904
Full (DNA) 0.1326 0.0466 0.0690 0.1591 0.0582 0.0852 0.3759 0.2457 0.2972
Full (cell-type) 0.1650 0.1375 0.1500 0.1494 0.1908 0.1676 0.4701 0.4900 0.4798
Full (RNA) 0.0932 0.0067 0.0126 0.0169 0.0075 0.0104 0.0593 0.1148 0.0782
</table>
<tableCaption confidence="0.998799">
Table 2: Performance of each model (only FULL is shown)
</tableCaption>
<sectionHeader confidence="0.995463" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999484108108108">
E. Brill. 1994. Some Advances in Transformation-
Based Part of Speech Tagging. Proceedings of
the National Conference on Artificial
Intelligence. AAAI Press; 722-727.
C. Burges. 1998. A Tutorial on Support Vector
Machines for Pattern Recognition. Data Mining
and Knowledge Discovery, 2: 121-167.
J.T. Chang, H. Schutze and R.B. Altman. 2004.
GAPSCORE: Finding Gene and Protein Names
One Word at a Time. Bioinformatics, 20(2): 216-
225.
N. Collier, C. Nobata and J.I. Tsujii. 2000.
Extracting the Names of Genes and Gene
Products with a Hidden Markov Model.
Proceedings of 18th International Conference on
Computational Linguistics, 201-207.
K. Fukuda, T. Tsunoda, A. Tamura and T. Takagi.
1998. Toward Information Extraction:
Identifying Protein Names from Biological
Papers. Proceedings of Pacific Symposium on
Biocomputing, 707-718.
W.J. Hou and H.H. Chen 2002. Extracting
Biological Keywords from Scientific Text.
Proceedings of 13th International Conference on
Genome Informatics; 571-573.
W.J. Hou and H.H. Chen. 2003. Enhancing
Performance of Protein Name Recognizers
Using Collocation. Proceedings of the ACL 2003
Workshop on NLP in Biomedicine, 25-32.
C.W. Hsu, C.C Chang and C.J. Lin. 2003. A
Practical Guide to Support Vector Classification.
http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.h
tml.
J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002.
Tuning Support Vector Machines for Biomedical
Named Entity Recognition. Proceedings of the
ACL 2002 workshop on NLP in the Biomedical
Domain, 1-8.
M. Krauthammer, P. Kra, I. Iossifov, S.M. Gomez,
G. Hripcsak, V. Hatzivassiloglou, C. Friedman
and A. Rzhetsky. 2002. Of truth and pathways:
chasing bits of information through myriads of
articles. Bioinformatics, 18(sup.1):S249-S257.
K.J. Lee, Y.S. Hwang and H.C. Rim. 2003. Two-
Phase Biomedical NE Recognition based on
SVMs. Proceedings of the ACL 2003 Workshop
on NLP in Biomedicine, 33-40.
C.D. Manning and H. Schutze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press.
F. Olsson, G. Eriksson, K. Franzen, L. Asker and P.
Liden. 2002. Notions of Correctness when
Evaluating Protein Name Taggers. Proceedings
of the 19th International Conference on
Computational Linguistics, 765-771.
A. Ratnaparkrhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution.
PhD Thesis, University of Pennsylvania.
K. Takeuchi and N. Collier. 2003. Bio-Medical
Entity Extraction using Support Vector
Machines. Proceedings of the ACL 2003
workshop on NLP in Biomedicine, 57-64.
L. Tanabe and W.J. Wilbur. 2002. Tagging Gene
and Protein Names in Biomedical Text.
Bioimformatics, 18(8): 1124-1132.
Y. Tsuruoka and J. Tsujii. 2003. Boosting
Precision and Recall of Dictionary-based Protein
Name Recognition. Proceedings of the ACL
2003 Workshop on NLP in Biomedicine, 41-48.
K. Yamamoto, T. Kudo, A. Konagaya and Y.
Matsumoto. 2003. Protein Name Tagging for
Biomedical Annotation in Text. Proceedings of
the ACL 2003 workshop on NLP in Biomedicine,
65-72.
</reference>
<page confidence="0.999304">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.793690">
<title confidence="0.999431">Annotating Multiple Types of Biomedical Entities: A Single Word Classification Approach</title>
<author confidence="0.985965">Wen-Juan Hou Lee</author>
<affiliation confidence="0.936890666666667">Natural Language Processing Department of Computer Science and Information National Taiwan</affiliation>
<address confidence="0.983927">1 Roosevelt Road, Section 4, Taipei, Taiwan,</address>
<email confidence="0.987442">clee@nlg.csie.ntu.edu.tw,hh_chen@csie.ntu.edu.tw</email>
<email confidence="0.987442">wjhou@nlg.csie.ntu.edu.tw,hh_chen@csie.ntu.edu.tw</email>
<abstract confidence="0.999735705882353">Named entity recognition is a fundamental task in biomedical data mining. Multiple-class annotation is more challenging than singleclass annotation. In this paper, we took a single word classification approach to dealing with the multiple-class annotation problem using Support Vector Machines (SVMs). Word attributes, results of existing gene/protein name taggers, context, and other information are important features for classification. During training, the size of training data and the distribution of named entities are considered. The results showed that the approach might be feasible when more training data is used to alleviate the data imbalance problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Some Advances in TransformationBased Part of Speech Tagging.</title>
<date>1994</date>
<booktitle>Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<pages>722--727</pages>
<publisher>AAAI Press;</publisher>
<contexts>
<context position="2254" citStr="Brill, 1994" startWordPosition="316" endWordPosition="317">and corpus-based (Collier et al., 2000; Chang et al., 2004). Yapex (Olsson et al., 2002) implemented some heuristic steps described by Fukuda, et al., and applied filters and knowledge bases to remove false alarms. Syntactic information obtained from the parser was incorporated as well. GAPSCORE (Chang et al., 2004) scored words on the basis of statistical models that quantified their appearance, morphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more words having strong relationships (Manning and Schut</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>E. Brill. 1994. Some Advances in TransformationBased Part of Speech Tagging. Proceedings of the National Conference on Artificial Intelligence. AAAI Press; 722-727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Burges</author>
</authors>
<title>A Tutorial on Support Vector Machines for Pattern Recognition.</title>
<date>1998</date>
<journal>Data Mining and Knowledge Discovery,</journal>
<volume>2</volume>
<pages>121--167</pages>
<contexts>
<context position="2205" citStr="Burges, 1998" startWordPosition="309" endWordPosition="310">98; Olsson et al., 2002; Tanabe and Wilbur, 2002) and corpus-based (Collier et al., 2000; Chang et al., 2004). Yapex (Olsson et al., 2002) implemented some heuristic steps described by Fukuda, et al., and applied filters and knowledge bases to remove false alarms. Syntactic information obtained from the parser was incorporated as well. GAPSCORE (Chang et al., 2004) scored words on the basis of statistical models that quantified their appearance, morphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more wor</context>
</contexts>
<marker>Burges, 1998</marker>
<rawString>C. Burges. 1998. A Tutorial on Support Vector Machines for Pattern Recognition. Data Mining and Knowledge Discovery, 2: 121-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Chang</author>
<author>H Schutze</author>
<author>R B Altman</author>
</authors>
<title>GAPSCORE: Finding Gene and Protein Names One Word at a Time.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>2</issue>
<pages>216--225</pages>
<contexts>
<context position="1701" citStr="Chang et al., 2004" startWordPosition="234" endWordPosition="237">ine material in the biomedical field has been growing steadily for more than 20 years. Several attempts have been made to mine knowledge from biomedical documents, such as identifying gene/protein names, recognizing protein interactions, and capturing specific relations in databases. Among these, named entity recognition is a fundamental step to mine knowledge from biological articles. Previous approaches on biological named entity extraction can be classified into two types – rulebased (Fukuda et al., 1998; Olsson et al., 2002; Tanabe and Wilbur, 2002) and corpus-based (Collier et al., 2000; Chang et al., 2004). Yapex (Olsson et al., 2002) implemented some heuristic steps described by Fukuda, et al., and applied filters and knowledge bases to remove false alarms. Syntactic information obtained from the parser was incorporated as well. GAPSCORE (Chang et al., 2004) scored words on the basis of statistical models that quantified their appearance, morphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words th</context>
</contexts>
<marker>Chang, Schutze, Altman, 2004</marker>
<rawString>J.T. Chang, H. Schutze and R.B. Altman. 2004. GAPSCORE: Finding Gene and Protein Names One Word at a Time. Bioinformatics, 20(2): 216-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Collier</author>
<author>C Nobata</author>
<author>J I Tsujii</author>
</authors>
<title>Extracting the Names of Genes and Gene Products with a Hidden Markov Model.</title>
<date>2000</date>
<booktitle>Proceedings of 18th International Conference on Computational Linguistics,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="1680" citStr="Collier et al., 2000" startWordPosition="230" endWordPosition="233">ion The volumn of on-line material in the biomedical field has been growing steadily for more than 20 years. Several attempts have been made to mine knowledge from biomedical documents, such as identifying gene/protein names, recognizing protein interactions, and capturing specific relations in databases. Among these, named entity recognition is a fundamental step to mine knowledge from biological articles. Previous approaches on biological named entity extraction can be classified into two types – rulebased (Fukuda et al., 1998; Olsson et al., 2002; Tanabe and Wilbur, 2002) and corpus-based (Collier et al., 2000; Chang et al., 2004). Yapex (Olsson et al., 2002) implemented some heuristic steps described by Fukuda, et al., and applied filters and knowledge bases to remove false alarms. Syntactic information obtained from the parser was incorporated as well. GAPSCORE (Chang et al., 2004) scored words on the basis of statistical models that quantified their appearance, morphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to fil</context>
</contexts>
<marker>Collier, Nobata, Tsujii, 2000</marker>
<rawString>N. Collier, C. Nobata and J.I. Tsujii. 2000. Extracting the Names of Genes and Gene Products with a Hidden Markov Model. Proceedings of 18th International Conference on Computational Linguistics, 201-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fukuda</author>
<author>T Tsunoda</author>
<author>A Tamura</author>
<author>T Takagi</author>
</authors>
<title>Toward Information Extraction: Identifying Protein Names from Biological Papers.</title>
<date>1998</date>
<booktitle>Proceedings of Pacific Symposium on Biocomputing,</booktitle>
<pages>707--718</pages>
<contexts>
<context position="1594" citStr="Fukuda et al., 1998" startWordPosition="216" endWordPosition="219"> when more training data is used to alleviate the data imbalance problem. 1 Introduction The volumn of on-line material in the biomedical field has been growing steadily for more than 20 years. Several attempts have been made to mine knowledge from biomedical documents, such as identifying gene/protein names, recognizing protein interactions, and capturing specific relations in databases. Among these, named entity recognition is a fundamental step to mine knowledge from biological articles. Previous approaches on biological named entity extraction can be classified into two types – rulebased (Fukuda et al., 1998; Olsson et al., 2002; Tanabe and Wilbur, 2002) and corpus-based (Collier et al., 2000; Chang et al., 2004). Yapex (Olsson et al., 2002) implemented some heuristic steps described by Fukuda, et al., and applied filters and knowledge bases to remove false alarms. Syntactic information obtained from the parser was incorporated as well. GAPSCORE (Chang et al., 2004) scored words on the basis of statistical models that quantified their appearance, morphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Bu</context>
</contexts>
<marker>Fukuda, Tsunoda, Tamura, Takagi, 1998</marker>
<rawString>K. Fukuda, T. Tsunoda, A. Tamura and T. Takagi. 1998. Toward Information Extraction: Identifying Protein Names from Biological Papers. Proceedings of Pacific Symposium on Biocomputing, 707-718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Hou</author>
<author>H H Chen</author>
</authors>
<title>Extracting Biological Keywords from Scientific Text.</title>
<date>2002</date>
<booktitle>Proceedings of 13th International Conference on Genome Informatics;</booktitle>
<pages>571--573</pages>
<contexts>
<context position="2385" citStr="Hou and Chen, 2002" startWordPosition="340" endWordPosition="343">cribed by Fukuda, et al., and applied filters and knowledge bases to remove false alarms. Syntactic information obtained from the parser was incorporated as well. GAPSCORE (Chang et al., 2004) scored words on the basis of statistical models that quantified their appearance, morphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). Hou and Chen (2003) showed that protein/gene collocates are capable of assisting existing protein/gene taggers. In this </context>
</contexts>
<marker>Hou, Chen, 2002</marker>
<rawString>W.J. Hou and H.H. Chen 2002. Extracting Biological Keywords from Scientific Text. Proceedings of 13th International Conference on Genome Informatics; 571-573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Hou</author>
<author>H H Chen</author>
</authors>
<title>Enhancing Performance of Protein Name Recognizers Using Collocation.</title>
<date>2003</date>
<booktitle>Proceedings of the ACL 2003 Workshop on NLP in Biomedicine,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="2884" citStr="Hou and Chen (2003)" startWordPosition="418" endWordPosition="421"> POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). Hou and Chen (2003) showed that protein/gene collocates are capable of assisting existing protein/gene taggers. In this paper, we addressed this task as a multi-class classification problem with SVMs and extended the idea of collocation to generate features at word and pattern level in our method. Existing protein/gene recognizers were used to perform feature extraction as well. The rest of this paper is organized as follows. The methods used in this study are introduced in Section 2. The experimental results are shown and discussed in Section 3. Finally, Section 4 concludes the remarks and lists some future wor</context>
<context position="6503" citStr="Hou and Chen, 2003" startWordPosition="999" endWordPosition="1002">r was adopted. 2.1.2 Context Information Preparation Contextual information has been shown helpful in annotating gene/protein names, and therefore two strategies for extracting contextual information at different levels are used. One is the usual practice at a word level, and the other is at a pattern level. Since the training data released in the beginning does not define the abstract boundary, we have to assume that sentences are independent of each other, and the contextual information extraction was thus limited to be within a sentence. For contextual information extraction at word level (Hou and Chen, 2003), collocates along with 4 statistics including frequency, the average and standard error of distance between word and entity and t-test score, were extracted. The frequency and t-test score were normalized to [0, 1]. Five lists of collocates were obtained for cell-line, celltype, DNA, RNA, and protein, respectively. As for contextual information extraction at pattern level, we first gathered a list of words constituting a specific type of named entities. Then a hierarchical clustering with cutoff threshold was performed on the words. Edit distance was adopted as the measure of dissimilarity (s</context>
</contexts>
<marker>Hou, Chen, 2003</marker>
<rawString>W.J. Hou and H.H. Chen. 2003. Enhancing Performance of Protein Name Recognizers Using Collocation. Proceedings of the ACL 2003 Workshop on NLP in Biomedicine, 25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C W Hsu</author>
<author>C C Chang</author>
<author>C J Lin</author>
</authors>
<title>A Practical Guide to Support Vector Classification.</title>
<date>2003</date>
<note>http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.h tml.</note>
<contexts>
<context position="9078" citStr="Hsu et al., 2003" startWordPosition="1453" endWordPosition="1456"> type. Next, the left three and right three words along with the current word are searched for patterns, adding 6 variables to the feature vector. 3 E E Probp (i |type) , where type is one of the i =−3 pÎ Pwi six types including ‘O’, Pwi is the set of patterns matching wi, Probp denotes the pmf for pattern p. Finally, the type of the previous word is added to the feature vector, mimicking the concept of a stochastic model. 2.3 Classification Support Vector Machines classification with radial basis kernel was adopted in this task, and the package LIBSVM – A Library for Support Vector Machines (Hsu et al., 2003) was used for training and prediction. The penalty coefficient C in optimization and gamma in kernel function were tuned using a script provided in this package. The constructed training data contains 492,551 instances, which is too large for training. Also, the training data is extremely unbalanced (see Table 1) and this is a known problem in SVMs classification. Therefore, we performed stratified sampling to form a smaller and balanced data set for training. Type # of instances (words) cell-type 15,466 DNA 25,307 cell-line 11,217 RNA 2,481 protein 55,117 O 382,963 Table 1: Number of instance</context>
</contexts>
<marker>Hsu, Chang, Lin, 2003</marker>
<rawString>C.W. Hsu, C.C Chang and C.J. Lin. 2003. A Practical Guide to Support Vector Classification. http://www.csie.ntu.edu.tw/~cjlin/libsvm/index.h tml.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>T Makino</author>
<author>Y Ohta</author>
<author>J Tsujii</author>
</authors>
<title>Tuning Support Vector Machines for Biomedical Named Entity Recognition.</title>
<date>2002</date>
<booktitle>Proceedings of the ACL 2002 workshop on NLP in the Biomedical Domain,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2641" citStr="Kazama et al., 2002" startWordPosition="378" endWordPosition="381">d their appearance, morphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). Hou and Chen (2003) showed that protein/gene collocates are capable of assisting existing protein/gene taggers. In this paper, we addressed this task as a multi-class classification problem with SVMs and extended the idea of collocation to generate features at word and pattern level in our method. Existing protein/gene recognizers were used to perform feature extraction as </context>
<context position="10876" citStr="Kazama et al. (2002)" startWordPosition="1752" endWordPosition="1755">o balance the training data. First, the training data was constructed to comprise equal number of instances from each class. However, it didn’t perform well and lots of type ‘O’ words were misclassified, indicating that using only less than 1% of type ‘O’ training instances is not sufficient to train a good model. Thus two more models were trained to see if the performance can be enhanced. One model has slightly more type ‘O’ instances than the equally balanced one, and the other model has the ratio among classes being 4:8:4:1:8:16. The results showed increase in recall but drop in precision. Kazama et al. (2002) addressed the data imbalance problem and sped up the training process by splitting the type ‘O’ instances into subclasses using part-of-speech information. However, we missed their work while we were doing this task, and hence didn’t have the chance to use and extend this idea. After carefully examining the classification results, we found that many of the ‘DNA’ instances were classified as ‘protein’ and many of the ‘protein’ instances were classified as ‘DNA’. For example, 904 out of 2,845 ‘DNA’ instances were categorized as ‘protein’ under ‘model 1/4’. The reason may be that Yapex and GAPSC</context>
</contexts>
<marker>Kazama, Makino, Ohta, Tsujii, 2002</marker>
<rawString>J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002. Tuning Support Vector Machines for Biomedical Named Entity Recognition. Proceedings of the ACL 2002 workshop on NLP in the Biomedical Domain, 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Krauthammer</author>
<author>P Kra</author>
<author>I Iossifov</author>
<author>S M Gomez</author>
<author>G Hripcsak</author>
<author>V Hatzivassiloglou</author>
<author>C Friedman</author>
<author>A Rzhetsky</author>
</authors>
<title>Of truth and pathways: chasing bits of information through myriads of articles.</title>
<date>2002</date>
<journal>Bioinformatics,</journal>
<volume>18</volume>
<contexts>
<context position="11594" citStr="Krauthammer et al., 2002" startWordPosition="1869" endWordPosition="1872">‘O’ instances into subclasses using part-of-speech information. However, we missed their work while we were doing this task, and hence didn’t have the chance to use and extend this idea. After carefully examining the classification results, we found that many of the ‘DNA’ instances were classified as ‘protein’ and many of the ‘protein’ instances were classified as ‘DNA’. For example, 904 out of 2,845 ‘DNA’ instances were categorized as ‘protein’ under ‘model 1/4’. The reason may be that Yapex and GAPSCORE do not distinguish gene name from protein names. Even humans don’t do very well at this (Krauthammer et al., 2002). We originally planned to verify the contribution of each type of features. For example, how much noise was introduced by using existing taggers instead of lexicons. This would have helped gain more insights into the proposed features. 4 Conclusion and Future work This paper presented the preliminary results of our study. We introduced the use of existing taggers and presented a way to collect common substrings shared by entities. Due to lack of time, the models were not well tuned against the two parameters – C and gamma, influencing the capabilities of the models. Further, not all of the tr</context>
</contexts>
<marker>Krauthammer, Kra, Iossifov, Gomez, Hripcsak, Hatzivassiloglou, Friedman, Rzhetsky, 2002</marker>
<rawString>M. Krauthammer, P. Kra, I. Iossifov, S.M. Gomez, G. Hripcsak, V. Hatzivassiloglou, C. Friedman and A. Rzhetsky. 2002. Of truth and pathways: chasing bits of information through myriads of articles. Bioinformatics, 18(sup.1):S249-S257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Lee</author>
<author>Y S Hwang</author>
<author>H C Rim</author>
</authors>
<title>TwoPhase Biomedical NE Recognition based on SVMs.</title>
<date>2003</date>
<booktitle>Proceedings of the ACL 2003 Workshop on NLP in Biomedicine,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2659" citStr="Lee et al., 2003" startWordPosition="382" endWordPosition="385">orphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). Hou and Chen (2003) showed that protein/gene collocates are capable of assisting existing protein/gene taggers. In this paper, we addressed this task as a multi-class classification problem with SVMs and extended the idea of collocation to generate features at word and pattern level in our method. Existing protein/gene recognizers were used to perform feature extraction as well. The rest of </context>
</contexts>
<marker>Lee, Hwang, Rim, 2003</marker>
<rawString>K.J. Lee, Y.S. Hwang and H.C. Rim. 2003. TwoPhase Biomedical NE Recognition based on SVMs. Proceedings of the ACL 2003 Workshop on NLP in Biomedicine, 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2125" citStr="Manning and Schutze, 1999" startWordPosition="297" endWordPosition="300">gical named entity extraction can be classified into two types – rulebased (Fukuda et al., 1998; Olsson et al., 2002; Tanabe and Wilbur, 2002) and corpus-based (Collier et al., 2000; Chang et al., 2004). Yapex (Olsson et al., 2002) implemented some heuristic steps described by Fukuda, et al., and applied filters and knowledge bases to remove false alarms. Syntactic information obtained from the parser was incorporated as well. GAPSCORE (Chang et al., 2004) scored words on the basis of statistical models that quantified their appearance, morphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be </context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>C.D. Manning and H. Schutze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Olsson</author>
<author>G Eriksson</author>
<author>K Franzen</author>
<author>L Asker</author>
<author>P Liden</author>
</authors>
<date>2002</date>
<booktitle>Notions of Correctness when Evaluating Protein Name Taggers. Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>765--771</pages>
<contexts>
<context position="1615" citStr="Olsson et al., 2002" startWordPosition="220" endWordPosition="223">ata is used to alleviate the data imbalance problem. 1 Introduction The volumn of on-line material in the biomedical field has been growing steadily for more than 20 years. Several attempts have been made to mine knowledge from biomedical documents, such as identifying gene/protein names, recognizing protein interactions, and capturing specific relations in databases. Among these, named entity recognition is a fundamental step to mine knowledge from biological articles. Previous approaches on biological named entity extraction can be classified into two types – rulebased (Fukuda et al., 1998; Olsson et al., 2002; Tanabe and Wilbur, 2002) and corpus-based (Collier et al., 2000; Chang et al., 2004). Yapex (Olsson et al., 2002) implemented some heuristic steps described by Fukuda, et al., and applied filters and knowledge bases to remove false alarms. Syntactic information obtained from the parser was incorporated as well. GAPSCORE (Chang et al., 2004) scored words on the basis of statistical models that quantified their appearance, morphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE</context>
</contexts>
<marker>Olsson, Eriksson, Franzen, Asker, Liden, 2002</marker>
<rawString>F. Olsson, G. Eriksson, K. Franzen, L. Asker and P. Liden. 2002. Notions of Correctness when Evaluating Protein Name Taggers. Proceedings of the 19th International Conference on Computational Linguistics, 765-771.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkrhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>PhD Thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Ratnaparkrhi, 1998</marker>
<rawString>A. Ratnaparkrhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. PhD Thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Takeuchi</author>
<author>N Collier</author>
</authors>
<title>Bio-Medical Entity Extraction using Support Vector Machines.</title>
<date>2003</date>
<booktitle>Proceedings of the ACL 2003 workshop on NLP in Biomedicine,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="2687" citStr="Takeuchi and Collier, 2003" startWordPosition="386" endWordPosition="389">ext. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). Hou and Chen (2003) showed that protein/gene collocates are capable of assisting existing protein/gene taggers. In this paper, we addressed this task as a multi-class classification problem with SVMs and extended the idea of collocation to generate features at word and pattern level in our method. Existing protein/gene recognizers were used to perform feature extraction as well. The rest of this paper is organized as f</context>
</contexts>
<marker>Takeuchi, Collier, 2003</marker>
<rawString>K. Takeuchi and N. Collier. 2003. Bio-Medical Entity Extraction using Support Vector Machines. Proceedings of the ACL 2003 workshop on NLP in Biomedicine, 57-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tanabe</author>
<author>W J Wilbur</author>
</authors>
<date>2002</date>
<booktitle>Tagging Gene and Protein Names in Biomedical Text. Bioimformatics,</booktitle>
<volume>18</volume>
<issue>8</issue>
<pages>1124--1132</pages>
<contexts>
<context position="1641" citStr="Tanabe and Wilbur, 2002" startWordPosition="224" endWordPosition="227">ate the data imbalance problem. 1 Introduction The volumn of on-line material in the biomedical field has been growing steadily for more than 20 years. Several attempts have been made to mine knowledge from biomedical documents, such as identifying gene/protein names, recognizing protein interactions, and capturing specific relations in databases. Among these, named entity recognition is a fundamental step to mine knowledge from biological articles. Previous approaches on biological named entity extraction can be classified into two types – rulebased (Fukuda et al., 1998; Olsson et al., 2002; Tanabe and Wilbur, 2002) and corpus-based (Collier et al., 2000; Chang et al., 2004). Yapex (Olsson et al., 2002) implemented some heuristic steps described by Fukuda, et al., and applied filters and knowledge bases to remove false alarms. Syntactic information obtained from the parser was incorporated as well. GAPSCORE (Chang et al., 2004) scored words on the basis of statistical models that quantified their appearance, morphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger </context>
</contexts>
<marker>Tanabe, Wilbur, 2002</marker>
<rawString>L. Tanabe and W.J. Wilbur. 2002. Tagging Gene and Protein Names in Biomedical Text. Bioimformatics, 18(8): 1124-1132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
</authors>
<title>Boosting Precision and Recall of Dictionary-based Protein Name Recognition.</title>
<date>2003</date>
<booktitle>Proceedings of the ACL 2003 Workshop on NLP in Biomedicine,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="2419" citStr="Tsuruoka and Tsujii, 2003" startWordPosition="345" endWordPosition="348">and applied filters and knowledge bases to remove false alarms. Syntactic information obtained from the parser was incorporated as well. GAPSCORE (Chang et al., 2004) scored words on the basis of statistical models that quantified their appearance, morphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). Hou and Chen (2003) showed that protein/gene collocates are capable of assisting existing protein/gene taggers. In this paper, we addressed this task as a</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2003</marker>
<rawString>Y. Tsuruoka and J. Tsujii. 2003. Boosting Precision and Recall of Dictionary-based Protein Name Recognition. Proceedings of the ACL 2003 Workshop on NLP in Biomedicine, 41-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamamoto</author>
<author>T Kudo</author>
<author>A Konagaya</author>
<author>Y Matsumoto</author>
</authors>
<title>Protein Name Tagging for Biomedical Annotation in Text.</title>
<date>2003</date>
<booktitle>Proceedings of the ACL 2003 workshop on NLP in Biomedicine,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="2711" citStr="Yamamoto et al., 2003" startWordPosition="390" endWordPosition="393">ve Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). Hou and Chen (2003) showed that protein/gene collocates are capable of assisting existing protein/gene taggers. In this paper, we addressed this task as a multi-class classification problem with SVMs and extended the idea of collocation to generate features at word and pattern level in our method. Existing protein/gene recognizers were used to perform feature extraction as well. The rest of this paper is organized as follows. The methods used</context>
</contexts>
<marker>Yamamoto, Kudo, Konagaya, Matsumoto, 2003</marker>
<rawString>K. Yamamoto, T. Kudo, A. Konagaya and Y. Matsumoto. 2003. Protein Name Tagging for Biomedical Annotation in Text. Proceedings of the ACL 2003 workshop on NLP in Biomedicine, 65-72.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>