<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9889855">
SemEval 2015 Task 18:
Broad-Coverage Semantic Dependency Parsing
</title>
<author confidence="0.999372">
Stephan Oepen44, Marco Kuhlmann°, Yusuke Miyao♦, Daniel Zeman&apos;,
Silvie Cinková°, Dan Flickinger&apos;, Jan Hajifc�, and Zdefnka Urešová°
</author>
<affiliation confidence="0.997151833333333">
* University of Oslo, Department of Informatics
4 Potsdam University, Department of Linguistics
n Linköping University, Department of Computer and Information Science
0 National Institute of Informatics, Tokyo
° Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
&apos; Stanford University, Center for the Study of Language and Information
</affiliation>
<email confidence="0.965547">
sdp-organizers@emmtee.net
</email>
<sectionHeader confidence="0.805825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.890423666666667">
Task 18 at SemEval 2015 defines Broad-
Coverage Semantic Dependency Parsing (SDP)
as the problem of recovering sentence-internal
predicate–argument relationships for all con-
tent words, i.e. the semantic structure consti-
tuting the relational core of sentence meaning.
In this task description, we position the prob-
lem in comparison to other language analysis
sub-tasks, introduce and compare the semantic
dependency target representations used, and
summarize the task setup, participating sys-
tems, and main results.
</bodyText>
<sectionHeader confidence="0.692146" genericHeader="categories and subject descriptors">
1 Background and Motivation
</sectionHeader>
<bodyText confidence="0.999964785714286">
Syntactic dependency parsing has seen great ad-
vances in the past decade, but tree-oriented parsers
are ill-suited for producing meaning representations,
i.e. moving from the analysis of grammatical struc-
ture to sentence semantics. Even if syntactic parsing
arguably can be limited to tree structures, this is not
the case in semantic analysis, where a node will often
be the argument of multiple predicates (i.e. have more
than one incoming arc), and it will often be desirable
to leave nodes corresponding to semantically vacu-
ous word classes unattached (with no incoming arcs).
Thus, Task 18 at SemEval 2015, Broad-Coverage
Semantic Dependency Parsing (SDP 2015),1 seeks
to stimulate the parsing community to move towards
</bodyText>
<footnote confidence="0.852139">
1See http://alt.qcri.org/semeval2015/
</footnote>
<bodyText confidence="0.959700342857143">
task18/ for further technical details, information on how to
obtain the data, and official results.
more general graph processing, to thus enable a more
direct analysis of Who did What to Whom?
Extending the very similar predecessor task
SDP 2014 (Oepen et al., 2014), we make use of three
distinct, parallel semantic annotations over the same
common texts, viz. the venerable Wall Street Journal
(WSJ) and Brown segments of the Penn Treebank
(PTB; Marcus et al., 1993) for English, as well as
comparable resources for Chinese and Czech. Fig-
ure 1 below shows example target representations,
bi-lexical semantic dependency graphs in all cases,
for the WSJ sentence:
(1) A similar technique is almost impossible to apply to other
crops, such as cotton, soybeans, and rice.
Semantically, technique arguably is dependent on the
determiner (the quantificational locus), the modifier
similar, and the predicate apply. Conversely, the
predicative copula, infinitival to, and the vacuous
preposition marking the deep object of apply can
be argued to not have a semantic contribution of
their own. Besides calling for node re-entrancies
and partial connectivity, semantic dependency graphs
may also exhibit higher degrees of non-projectivity
than is typical of syntactic dependency trees.
Besides its relation to syntactic dependency pars-
ing, the task also has some overlap with Se-
mantic Role Labeling (SRL; Gildea &amp; Jurafsky,
2002).2 However, we require parsers to identify ‘full-
2In much previous SRL work, target representations typi-
cally draw on resources like PropBank and NomBank (Palmer
et al., 2005; Meyers et al., 2004), which are limited to argu-
ment identification and labeling for verbal and nominal predi-
cates. A plethora of semantic phenomena—for example negation
</bodyText>
<note confidence="0.840452333333333">
915
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.9441715">
A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice .
q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-i p:e-u-i n:x n:x _ n:x _
(a) DELPH-IN Minimal Recursion Semantics–derived bi-lexical dependencies (DM).
(c) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PSD).
</figure>
<figureCaption confidence="0.993892">
Figure 1: Sample semantic dependency graphs for Example (1).
</figureCaption>
<figure confidence="0.997900962264151">
top
BV
ARG1
ARG1
ARG2 ARG3 ARG1
ARG1
ARG1
mwe
ARG2
conj
_and_c
ev-w218f2
_ _
_ _ _
_ev-w119f2 _ _ _ _ _ _ _ _ _ _ _ _
ARG1
ARG2
ARG2
top
ARG1
ARG1
ARG1
ARG2
ARG1
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
(b) Enju Predicate–Argument Structures (PAS).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
ARG2
ARG1 ARG1
ARG1
ARG1
ARG2
ARG1
ARG1
RSTR
top
PAT-arg
PAT-arg
EXT
ADDR-arg
ACT-arg
CONJ.m
ADDR-arg
RSTR
ADDR-arg
APPS.m
APPS.m
CONJ.m
CONJ.m
ADDR-arg
ARG1 ARG2
ARG1
ARG2
</figure>
<bodyText confidence="0.9998727">
sentence’ semantic dependencies, i.e. compute a rep-
resentation that integrates all content words in one
structure. Finally, a third related area of much interest
is often dubbed ‘semantic parsing’, which Kate and
Wong (2010) define as “the task of mapping natural
language sentences into complete formal meaning
representations which a computer can execute for
some domain-specific application.” In contrast to
much work in this tradition, our SDP target represen-
tations aim to be task- and domain-independent.
</bodyText>
<sectionHeader confidence="0.950461" genericHeader="method">
2 Target Representations
</sectionHeader>
<bodyText confidence="0.9961635625">
We use three distinct target representations for seman-
tic dependencies. As is evident in our running exam-
ple (Figure 1), showing what are called the DM, PAS,
and PSD semantic dependencies, there are contentful
differences among these annotations, and there is of
course not one obvious (or even objective) truth. Ad-
vancing in-depth comparison of representations and
underlying design decisions, in fact, is among the mo-
and other scopal embedding, comparatives, possessives, various
types of modification, and even conjunction—often remain un-
analyzed in SRL. Thus, its target representations are partial to
a degree that can prohibit semantic downstream processing, for
example inference-based techniques.
tivations for the SDP task series. Please see Oepen
et al. (2014) and Miyao et al. (2014) for additional
background.
</bodyText>
<sectionHeader confidence="0.616838" genericHeader="method">
DM: DELPH-IN MRS-Derived Bi-Lexical De-
</sectionHeader>
<bodyText confidence="0.976641473684211">
pendencies These semantic dependency graphs
originate in a manual re-annotation, dubbed Deep-
Bank, of Sections 00–21 of the WSJ Corpus and of
selected parts of the Brown Corpus with syntactico-
semantic analyses of the LinGO English Resource
Grammar (Flickinger, 2000; Flickinger et al., 2012).
For this target representation, top nodes designate
the highest-scoping (non-quantifier) predicate in the
graph, e.g. the (scopal) adverb almost in Figure 1.3
PAS: Enju Predicate–Argument Structures The
Enju Treebank and parser4 are derived from the au-
tomatic HPSG-style annotation of the PTB (Miyao,
2006). Our PAS semantic dependency graphs are
extracted from the Enju Treebank, without contentful
conversion, and from the application of the same ba-
sic techniques to the Penn Chinese Treebank (CTB;
3However, non-scopal adverbs act as mere intersective modi-
fiers, e.g. in a structure like Abrams sang loudly, the adverb is a
predicate in DM, but the main verb nevertheless is the top node.
</bodyText>
<footnote confidence="0.961465">
4See http://kmcs.nii.ac.jp/enju/.
</footnote>
<page confidence="0.900258">
916
</page>
<bodyText confidence="0.97280725">
Xue et al., 2005). Top nodes in this representation
denote semantic heads.
PSD: Prague Semantic Dependencies The
Prague Czech-English Dependency Treebank
(PCEDT; Hajiˇc et al., 2012)5 is a set of parallel
dependency trees over the WSJ texts from the PTB,
and their Czech translations. Our PSD bi-lexical
dependencies have been extracted from what is called
the tectogrammatical annotation layer (t-trees). Top
nodes are derived from t-tree roots; i.e. they mostly
correspond to main verbs. In case of coordinate
clauses, there are multiple top nodes per sentence.
</bodyText>
<sectionHeader confidence="0.983194" genericHeader="method">
3 Data Format
</sectionHeader>
<bodyText confidence="0.999973285714286">
The SDP target representations can be characterized
as labeled, directed graphs. Nodes are labeled with
five pieces of information: word form, lemma, part
of speech, a Boolean flag indicating whether the
node represents a top predicate, and optional frame
(or sense) information—for example the distinction
between causative vs. inchoative predicates like in-
crease. Edges are labeled with semantic relations
that hold between source and target.
All data provided for the task uses a column-based
file format that extends the format of the SDP 2014
task by a new frame column (thus making it a little
more SRL-like). More details about the file format
are available at the task website.
</bodyText>
<sectionHeader confidence="0.982251" genericHeader="method">
4 Data Sets
</sectionHeader>
<bodyText confidence="0.995583523076923">
All three target representations for English are anno-
tations of the same text, Sections 00–21 of the WSJ
Corpus, as well as of a balanced sample of twenty
files from the Brown Corpus (Francis &amp; Kuˇcera,
1982). For this task, we have synchronized these
resources at the sentence and tokenization levels and
excluded from the SDP 2015 training and testing
data any sentences for which (a) one or more of the
treebanks lacked a gold-standard analysis; (b) a one-
to-one alignment of tokens could not be established
across all three representations; or (c) at least one
of the graphs was cyclic. Of the 43,746 sentences
in these 22 first sections of WSJ text, DeepBank
lacks analyses for some 11%, and the Enju Tree-
5Seehttp://ufal.mff.cuni.cz/pcedt2.0/.
bank has gaps for a little more than four percent.6
Finally, 139 of the WSJ graphs obtained through the
above conversions were cyclic. In total, we were
left with 35,657 sentences (or 802,717 tokens; eight
percent more than for SDP 20147) as training data
(Sections 00–20), 1,410 in-domain testing sentences
(31,948 tokens) from WSJ Section 21, and 1,849 out-
of-domain testing sentences (31,583 tokens) from the
Brown Corpus.
Besides the additions of out-of-domain test data
and frame (or sense) identifiers for English, another
extension beyond the SDP 2014 task concerns the
inclusion of additional languages, albeit only for se-
lect target representations. Our training data included
an additional 31,113 Chinese sentences (649,036 to-
kens), taken from Release 7.0 of the CTB, for the
PAS target representation, and 42,076 Czech sen-
tences (985,302 tokens), drawing on the translations
of the WSJ Corpus in PCEDT 2.0, for the PSD target
representation. Additional out-of-domain Czech test
data was drawn from the Prague Dependency Tree-
bank 3.0 (PDT; Bejˇcek et al., 2013). For these addi-
tional languages, the task comprised 1,670 sentences
(38,397 tokens) of in-domain Chinese test data, and
1,670 sentences (38397 tokens) and 5,226 sentences
(87,927 tokens) of in- and out-of-domain Czech data,
respectively.
Quantitative Comparison As a first attempt at
contrasting our three target representations, Table 1
shows some high-level statistics of the graphs com-
prising the training and testing data.8 In terms
of distinctions drawn in dependency labels (1),
there are clear differences between the representa-
tions, with PSD appearing linguistically most fine-
6Additionally, some 500 sentences show tokenization mis-
matches, most owing to DeepBank correcting PTB idiosyn-
crasies like hG.m.b, H.i, hS.p, A.i, and hU.S., .i, and
introducing a few new ones (Fares et al., 2013).
7In comparison to the SDP 2014 data, our DM graphs were
extracted from a newer, improved release of DeepBank (Version
1.1), and its conversion to bi-lexical dependencies was moder-
ately revised to provide more systematic analyses of contracted
negated auxiliaries and comparatives. At the same time, the
extraction of PSD graphs from the PCEDT t-trees was refined
to include edges representing grammatical coreference, e.g. re-
entrancies introduced by control verbs.
8These statistics are obtained using the ‘official’ SDP toolkit.
Our notions of singletons, roots, re-entrancies, and projectivity
follow common graph terminology, but see Oepen et al. (2014)
for formal definitions.
</bodyText>
<page confidence="0.460362">
917
</page>
<table confidence="0.9997345">
EN i-d CS i-d ZH i-d EN o-o-d CS o-o-d
PSD PAS PSD
DM PAS PSD DM PAS PSD
# labels 59 42 91 61 32 47 41 74 64
% singletons 22.97 4.38 35.76 28.91 0.11 25.40 5.84 39.11 29.04
edge density 0.96 1.02 1.01 1.03 0.98 0.95 1.02 0.99 1.00
%y trees 2.30 1.22 42.19 37.66 3.49 9.68 2.38 51.43 51.49
%y noncrossing 69.03 59.57 64.58 63.22 67.61 74.58 65.28 74.26 72.41
%y projective 2.91 1.64 41.92 38.32 12.89 8.82 3.46 54.35 53.02
%y fragmented 6.55 0.23 0.69 1.17 15.22 4.71 0.65 1.73 3.50
%,,, reentrancies 27.44 29.36 11.42 11.80 24.96 26.14 29.36 11.46 11.44
%y topless 0.31 0.02 – 0.04 6.92 1.41 – – 0.02
# top nodes 0.9969 0.9998 1.1276 1.2242 0.9308 0.9859 1.0000 1.2645 1.2771
%,,, non-top roots 44.91 55.98 4.35 4.73 46.65 39.89 50.93 5.27 5.31
# frames 297 – 5426 – – 172 – 1208 –
%,,, frames 13.52 – 16.77 – – 15.79 – 19.50 –
average treewidth 1.30 1.72 1.61 1.66 1.35 1.31 1.69 1.50 1.49
maximum treewidth 3 3 7 6 3 3 3 5 5
</table>
<tableCaption confidence="0.99995">
Table 1: Contrastive high-level graph statistics across target representations, languages, and domains.
</tableCaption>
<bodyText confidence="0.9998735">
grained, and PAS showing the smallest label inven-
tory. Unattached singleton nodes (2) in our setup
correspond to tokens analyzed as semantically vacu-
ous, which (as seen in Figure 1) include most punc-
tuation marks in PSD and DM, but not PAS. Fur-
thermore, PSD (unlike the other two) analyzes some
high-frequency determiners as semantically vacuous.
Conversely, PAS on average has more edges per (non-
singleton) nodes than the other two (3), which likely
reflects its approach to the analysis of functional
words (see below).
Judging from both the percentage of actual trees
(4), the proportions of noncrossing graphs (5), pro-
jective graphs (6), and the proportions of reentrant
nodes (8), PSD is more ‘tree-oriented’ than the other
two, which at least in part reflects its approach to
the analysis of modifiers and determiners (again, see
below). We view the small percentages of graphs
without at least one top node (9) and of graphs with
at least two non-singleton components that are not
interconnected (7) as tentative indicators of general
well-formedness. Intuitively, there should always be
a ‘top’ predicate, and the whole graph should ‘hang
together’. Only DM exhibits non-trivial (if small) de-
grees of topless and fragmented graphs, which may
indicate imperfections in DeepBank annotations or
room for improvement in the conversion from full
logical forms to bi-lexical dependencies, but possi-
bly also exceptions to our intuitions about semantic
dependency graphs.
</bodyText>
<table confidence="0.8543998">
Directed Undirected
DM PAS PSD DM PAS PSD
DM − .6425 .2612 − .6719 .5675
PAS .6688 − .2963 .6993 − .5490
PSD .2636 .2963 − .5743 .5630 −
</table>
<tableCaption confidence="0.973293">
Table 2: Pairwise F1 similarities, including punctua-
</tableCaption>
<bodyText confidence="0.981622045454545">
tion (upper right diagonals) or not (lower left).
Frame or sense distinctions are a new property in
SDP 2015 and currently are only available for the
English DM and PSD data. Table 1 reveals a stark
difference in granularity: DM limits itself to argu-
ment structure distinctions that are grammaticized,
e.g. causative vs. inchoative contrasts or differences
in the arity or coarse semantic typing of argument
frames; PSD, on the other hand, draws on the much
richer sense inventory of the EngValLex database
(Cinková, 2006). Accordingly, the two target repre-
sentations represent quite different challenges for the
predicate disambiguation sub-task of SDP 2015.
Finally, in Table 2 we seek to quantify pairwise
structural similarity between the three representations
in terms of unlabeled dependency F1 (dubbed OF
in Section 5 below). We provide four variants of
this metric, (a) taking into account the directionality
of edges or not and (b) including edges involving
punctuation marks or not. On this view, DM and PAS
are structurally much closer to each other than either
of the two is to PSD, even more so when discarding
</bodyText>
<page confidence="0.654518">
918
</page>
<bodyText confidence="0.997818968750001">
punctuation. While relaxing the comparison to ignore
edge directionality also increases similarity scores
for this pair, the effect is much more pronounced
when comparing either to PSD. This suggests that
directionality of semantic dependencies is a major
source of diversion between DM and PAS on the one
hand, and PSD on the other hand.
Linguistic Comparison Among other aspects,
Ivanova et al. (2012) categorize a range of syntactic
and semantic dependency annotation schemes accord-
ing to the role that functional elements take. In Fig-
ure 1 and the discussion of Table 1 above, we already
observed that PAS differs from the other represen-
tations in integrating into the graph auxiliaries, the
infinitival marker, the case-marking preposition in-
troducing the argument of apply (to), and most punc-
tuation marks;9 while these (and other functional
elements, e.g. complementizers) are analyzed as se-
mantically vacuous in DM and PSD, they function
as predicates in PAS, though do not always serve as
‘local’ top nodes (i.e. the semantic head of the cor-
responding sub-graph): For example, the infinitival
marker in Figure 1 takes the verb as its argument, but
the ‘upstairs’ predicate impossible links directly to
the verb, rather than to the infinitival marker as an
intermediate.
At the same time, DM and PAS pattern alike in their
approach to modifiers, e.g. attributive adjectives, ad-
verbs, and prepositional phrases. Unlike in PSD (or
common syntactic dependency schemes), these are
analyzed as semantic predicates and, thus, contribute
to higher degrees of node reentrancy and non-top
(structural) roots. Roughly the same holds for de-
terminers, but here our PSD projection of Prague
tectogrammatical trees onto bi-lexical dependencies
leaves ‘vanilla’ articles (like a and the) as singleton
nodes.
The analysis of coordination is distinct in the three
representations, as also evident in Figure 1. By de-
sign, DM opts for what is often called the Mel’ˇcukian
analysis of coordinate structures (Mel’ˇcuk, 1988),
with a chain of dependencies rooted at the first con-
junct (which is thus considered the head, ‘standing
in’ for the structure at large); in the DM approach,
9In all formats, punctuation marks like dashes, colons, and
sometimes commas can be contentful, i.e. at times occur as both
predicates, arguments, and top nodes.
coordinating conjunctions are not integrated with the
graph but rather contribute different types of depen-
dencies. In PAS, the final coordinating conjunction
is the head of the structure and each coordinating
conjunction (or intervening punctuation mark that
acts like one) is a two-place predicate, taking left and
right conjuncts as its arguments. Conversely, in PSD
the last coordinating conjunction takes all conjuncts
as its arguments (in case there is no overt conjunc-
tion, a punctuation mark is used instead); additional
conjunctions or punctuation marks are not connected
to the graph.10
A linguistic difference between our representations
that highlights variable granularities of analysis and,
relatedly, diverging views on the scope of the prob-
lem can be observed in Figure 2. Much noun phrase–
internal structure is not made explicit in the PTB, and
the Enju Treebank from which our PAS representa-
tion derives predates the bracketing work of Vadas
and Curran (2007). In the four-way nominal com-
pounding example of Figure 2, thus, PAS arrives at
a strictly left-branching tree, and there is no attempt
at interpreting semantic roles among the members of
the compound either; PSD, on the other hand, anno-
tates both the actual compound-internal bracketing
and the assignment of roles, e.g. making stock the
PAT(ient) of investment. In this spirit, the PSD anno-
tations could be directly paraphrased along the lines
of plans by employees for investment in stocks. In a
middle position between the other two, DM disam-
biguates the bracketing but, by design, merely assigns
an underspecified, construction-specific dependency
type; its compound dependency, then, is to be inter-
preted as the most general type of dependency that
can hold between the elements of this construction
(i.e. to a first approximation either an argument role
or a relation parallel to a preposition, as in the above
paraphrase). The DM and PSD annotations of this
specific example happen to diverge in their bracket-
ing decisions, where the DM analysis corresponds to
[...I investments in stock for employees, i.e. grouping
10As detailed by Miyao et al. (2014), individual conjuncts can
be (and usually are) arguments of other predicates, whereas the
topmost conjunction only has incoming edges in nested coordi-
nate structures. Similarly, a ‘shared’ modifier of the coordinate
structure as a whole would take as its argument the local top
node of the coordination in DM or PAS (i.e. the first conjunct or
final conjunction, respectively), whereas it would depend as an
argument on all conjuncts in PSD.
</bodyText>
<figure confidence="0.8479317">
919
compound compound compound
ARG1
ARG1
ACT
ARG1
PAT REG
employee stock investment plans
employee stock investment plans
employee stock investment plans
</figure>
<figureCaption confidence="0.99826">
Figure 2: Analysis of nominal compounding in DM, PAS, and PSD, respectively.
</figureCaption>
<bodyText confidence="0.999961347826087">
the concept employee stock (in contrast to ‘common
stock’).
Without context and expert knowledge, these deci-
sions are hard to call, and indeed there has been much
previous work seeking to identify and annotate the re-
lations that hold between members of a nominal com-
pound (see Nakov, 2013, for a recent overview). To
what degree the bracketing and role disambiguation
in this example are determined by the linguistic signal
(rather than by context and world knowledge, say)
can be debated, and thus the observed differences
among our representations in this example relate
to the classic contrast between ‘sentence’ (or ‘con-
ventional’) meaning, on the one hand, and ‘speaker’
(or ‘occasion’) meaning, on the other hand (Quine,
1960; Grice, 1968; Bender et al., 2015). In turn,
we acknowledge different plausible points of view
about which level of semantic representation should
be the target representation for data-driven parsing
(i.e. structural analysis guided by the grammatical
system), and which refinements like the above could
be construed as part of a subsequent task of interpre-
tation.
</bodyText>
<sectionHeader confidence="0.989854" genericHeader="method">
5 Task Setup
</sectionHeader>
<bodyText confidence="0.999820103448276">
English training data for the task, providing all
columns in the file format sketched in Section 3
above, together with a first version of the SDP
toolkit—including graph input, basic statistics, and
scoring—were released to candidate participants in
early August 2014. In mid-November, cross-lingual
training data, a minor update to the English data, and
optional syntactic ‘companion’ analyses (see below)
were provided. Anytime between mid-December
2014 and mid-January 2015, participants could re-
quest an input-only version of the test data, with just
columns (1) to (4) pre-filled; participants then had
six days to run their systems on these inputs, fill
in columns (5), (6), (7), and upwards, and submit
their results (from up to two different runs) for scor-
ing. Upon completion of the testing phase, we have
shared the gold-standard test data, official scores, and
system results for all submissions with participants
and are currently preparing all data for general re-
lease through the Linguistic Data Consortium.
Evaluation Systems participating in the task were
evaluated based on the accuracy with which they can
produce semantic dependency graphs for previously
unseen text, measured relative to the gold-standard
testing data. For comparability with SDP 2014,
the primary measures for this evaluation were la-
beled and unlabeled precision and recall with respect
to predicted dependencies (predicate–role–argument
triples) and labeled and unlabeled exact match with
respect to complete graphs. In both contexts, identifi-
cation of the top node(s) of a graph was considered
as the identification of additional, ‘virtual’ dependen-
cies from an artificial root node (at position 0). Below
we abbreviate these metrics as (a) labeled precision,
recall, and F1: LP, LR, LF; (b) unlabeled precision,
recall, and F1: UP, UR, UF; and (c) labeled and unla-
beled exact match: LM, UM.
The ‘official’ ranking of participating systems is
determined based on the arithmetic mean of the la-
beled dependency F1 scores (i.e. the geometric mean
of labeled precision and labeled recall) on the three
target representations (DM, PAS, and PSD). Thus, to
be competitive in the overall ranking, a system had
to submit semantic dependencies for all three target
representations.
In addition to these metrics, we apply two addi-
tional metrics that aim to capture fragments of seman-
tics that are ‘larger’ than individual dependencies but
‘smaller’ than the semantic dependency graph for the
complete sentence, viz. what we call (a) complete
predications and (b) semantic frames. A complete
predication is comprised of the set of all core argu-
ments to one predicate, which for the DM and PAS
target representations corresponds to all outgoing
dependency edges, and for the PSD target represen-
tation to only those outgoing dependencies marked
by an ‘-arg’ suffix on the edge label. Pushing the
units of evaluation one step further towards inter-
</bodyText>
<table confidence="0.975608238095238">
920
LF DM PAS PSD
LF LP LR FF LF LP LR PF LF LP LR FF
Turku4 86.81 88.29 89.52 87.09 58.39 95.58 95.94 95.21 87.99 76.57 78.24 74.97 56.85
Lisbon* 86.23 89.44 90.52 88.39 00.20 91.67 92.45 90.90 84.18 77.58 79.88 75.41 00.06
Peking 85.33 89.09 90.93 87.32 63.08 91.26 92.90 89.67 79.08 75.66 78.60 72.93 49.95
Lisbon 85.15 88.21 89.84 86.64 00.15 90.88 91.87 89.92 81.74 76.36 78.62 74.23 00.03
Riga 84.00 87.90 88.57 87.24 58.12 90.75 91.50 90.02 80.03 73.34 75.25 71.52 52.54
Turku* 83.47 86.17 87.80 84.60 54.67 90.62 91.38 89.87 80.60 73.63 76.10 71.32 53.20
Minsk 80.74 84.13 86.28 82.09 54.24 85.24 87.28 83.28 64.66 72.84 74.65 71.13 51.63
In-House* 61.61 92.80 92.85 92.75 83.79 92.03 92.07 91.99 87.24 – – – –
LF DM PAS PSD
LF LP LR FF LF LP LR PF LF LP LR FF
Turku4 83.50 82.11 84.26 80.07 42.89 92.92 93.52 92.33 83.80 75.47 77.77 73.31 42.37
Lisbon* 82.53 83.77 85.79 81.84 00.35 87.63 88.88 86.41 80.19 76.18 80.12 72.61 02.25
Lisbon 81.15 81.75 84.81 78.90 00.27 86.88 88.52 85.30 78.47 74.82 78.68 71.31 02.09
Peking 80.78 81.84 84.29 79.53 47.49 87.23 89.47 85.10 74.75 73.28 77.36 69.61 34.28
Riga 79.23 80.69 81.69 79.72 41.88 86.63 87.56 85.72 76.26 70.37 73.23 67.71 40.76
Turku* 78.85 79.01 81.54 76.63 39.15 85.95 86.95 84.98 76.38 71.59 74.92 68.55 38.75
Minsk 75.79 77.24 80.24 74.46 42.18 80.44 83.07 77.96 62.00 69.68 72.26 67.27 41.25
In-House* 59.24 89.69 89.80 89.58 76.39 88.03 88.10 87.96 81.69 – – – –
</table>
<tableCaption confidence="0.99125675">
Table 3: Results of the gold track (marked 4), open track (marked *) and closed track (unmarked) submissions
for the English in-domain (top) and out-of-domain (bottom) data. For each system, the second column (LF)
indicates the averaged LF score across all representations, used to rank the systems. The best closed track
scores are highlighted in italices.
</tableCaption>
<table confidence="0.999798">
LF LP LR PF LF LP LR PF LF LP LR PF
Peking 83.43 84.75 82.15 66.09 Lisbon 79.33 83.52 75.54 55.91 Peking 64.37 69.41 60.02 48.82
Riga 82.47 83.12 81.84 66.05 Peking 78.45 83.61 73.89 55.36 Turku* 63.70 65.11 62.35 51.04
Lisbon 82.02 83.81 80.31 66.05 Riga 75.34 78.77 72.19 50.90 Lisbon 63.50 67.94 59.61 43.10
Turku* 79.64 80.81 78.51 62.04 Turku* 75.30 77.53 73.20 54.26 Riga 61.32 64.50 58.44 44.34
Minsk 77.68 79.27 76.15 58.23
</table>
<tableCaption confidence="0.949789">
Table 4: Results of the open (Turku) and closed (other teams) tracks for the Chinese in-domain (left) and
Czech in- (center) and out-of-domain (right) data. The systems are ranked according to their LF scores.
</tableCaption>
<bodyText confidence="0.998690130434783">
pretation, a semantic frame is comprised of a com-
plete predication combined with the frame (or sense)
identifier of its predicate. Both complete-predicate
and semantic-frame evaluation are restricted to pred-
icates corresponding to verbal parts of speech (as
determined by the gold-standard part of speech), and
semantic frames are further restricted to those target
representations for which frame or sense information
is available in our data (English DM and PSD). As
with the other metrics, we score precision, recall, and
Fl, which we abbreviate as PP, PR, and PF for com-
plete predications, and FP, FR, and FF for semantic
frames.
Closed vs. Open vs. Gold Tracks Much like in
2014, the task distinguished a closed track and an
open track, where systems in the closed track could
only be trained on the gold-standard semantic de-
pendencies distributed for the task. Systems in
the open track, on the other hand, could use ad-
ditional resources, such as a syntactic parser, for
example—provided that they make sure to not use
any tools or resources that encompass knowledge of
the gold-standard syntactic or semantic analyses of
</bodyText>
<page confidence="0.748747">
921
</page>
<bodyText confidence="0.999894705882353">
the SDP 2015 test data.11 To simplify participation
in the open track, the organizers prepared ready-to-
use ‘companion’ syntactic analyses, sentence- and
token-aligned to the SDP data, in the form of Stan-
ford Basic syntactic dependencies (de Marneffe et al.,
2006) produced by the parser of Bohnet and Nivre
(2012).
Finally, to more directly gauge the the contribu-
tions of syntactic structure on the semantic depen-
dency parsing problem, an idealized gold track was
introduced in SDP 2015. For this track, gold-standard
syntactic companion files were provided in a varity
of formats, viz. (a) Stanford Basic dependencies, de-
rived from the PTB, (b) HPSG syntactic dependen-
cies in the form called DM by Ivanova et al. (2012),
derived from DeepBank, and (c) HPSG syntactic de-
pendencies derived from the Enju Treebank.
</bodyText>
<sectionHeader confidence="0.894338" genericHeader="method">
6 Submissions and Results
</sectionHeader>
<bodyText confidence="0.999820810344828">
From almost 40 teams who had registered for the
task, twelve teams obtained the test data, and test
runs were submitted for six systems—including one
‘inofficial’ submission by a sub-set of the task orga-
nizers (Miyao et al., 2014). Each team submitted up
to two test runs per track. In total, there were seven
runs submitted to the English closed track, five to
the open track and two to the gold track; seven runs
were submitted to the Chinese closed track, two to
the open track; and five runs submitted to the Czech
closed track, two to the open track. One team sub-
mitted only to the open and gold tracks, three teams
submitted only to the closed track, one team submit-
ted to open and closed tracks in English but only to
the closed tracks in the other two languages. The
main results are summarized and ranked in Tables 3
and 4. The ranking is based on the average LF score
across all three target representations. Besides LF,
LP and LR we also indicate the F1 score of prediction
of semantic frames (FF), or, where frame (or sense)
identifiers are not available, of complete predications
(PF). In cases where a team submitted two runs to
a track, only the highest-ranked score is included in
the table.
In the English closed track, the average LF scores
11This restriction implies that typical off-the-shelf syntactic
parsers have to be re-trained, as many data-driven parsers for
English include WSJ Section 21 in their default training data.
across target representations range from 85.33 to
80.74. Comparing the results for different target rep-
resentations, the average LF scores across systems
are 89.13 for PAS, 87.09 for DM, and 74.24 for PSD.
The scores for semantic frames show a much larger
variation across representations and systems.12
The Lisbon team is the only one that submitted to
both the open and the closed tracks; with the addi-
tional resources allowed in the open track, they were
able to improve over all closed-track submissions.
Similarly, the perfect Stanford dependencies in the
gold track helped the Turku team a lot in PAS and
somewhat in DM and PSD; interestingly, they did not
obtain the best results in the latter two representa-
tions, but their cross-representation average was still
the best. The In-House system is ranked low because
its submission was incomplete (no of-the-shelf parser
for PSD being available); however, for DM and PAS
they yielded the best open-track scores.
We see very similar trends for the out-of-domain
data, though the scores are a few points lower.
Chinese PAS seems to be more difficult than En-
glish (cross-system average LF being 81.05, as op-
posed to English 90.07). The Czech and English
in-domain data are actually parallel translations and
the Czech PSD average LF is slightly higher (77.11,
as opposed to English 74.90). The Turku open-track
system shined in the Czech out-of-domain data, pre-
sumably because the additional dependency parser
they used was trained on data from the target domain.
</bodyText>
<sectionHeader confidence="0.485087" genericHeader="method">
7 Overview of Approaches
</sectionHeader>
<bodyText confidence="0.992327333333333">
Table 5 shows a summary of the tracks in which each
submitted system participated, and Table 6 shows
an overview of approaches and additionally used re-
sources. All the teams except In-House submitted
results for cross-lingual data (Czech and Chinese).
Teams except Lisbon also tackled with predicate dis-
ambiguation. Only Turku participated in the Gold
track.
The submitted teams explored a variety of ap-
proaches. Riga and Peking relied on the graph-to-tree
transformation of Du et al. (2014) as a basis. This
method converts semantic dependency graphs into
tree structures. Training data of semantic dependency
12Please see the task web page at the address indicated above
for full labeled and unlabeled scores.
</bodyText>
<note confidence="0.506682">
922
</note>
<title confidence="0.284474">
Team Closed Open Cross-Lingual Predicate Disambiguation Gold
</title>
<equation confidence="0.946109833333333">
In-House ✓ ✓
Lisbon ✓ ✓ ✓
Minsk ✓ ✓ ✓
Peking ✓ ✓ ✓
Riga ✓ ✓ ✓
Turku ✓ ✓ ✓ ✓
</equation>
<tableCaption confidence="0.99344">
Table 5: Summary of tracks in which submitted systems participated
</tableCaption>
<table confidence="0.949827125">
Team Approach Resources
In-House grammar-based parsing (Miyao et al., 2014) ERG &amp; Enju
Lisbon graph parsing with dual decomposition (Martins &amp; Almeida, 2014) companion
Minsk transition-based dependency graph parsing in the spirit of Titov et al. (2009) —
Peking (Du et al., 2014) extended with weighted tree approximation, parser ensemble —
Riga (Du et al., 2014)’s graph-to-tree transformation, Mate, C6.0, parser ensemble —
Turku sequence labeling for argument detection for each predicate, SVM classifiers companion
for top node recognition and sense prediction
</table>
<tableCaption confidence="0.998565">
Table 6: Overview of approaches and additional resources used (if any).
</tableCaption>
<bodyText confidence="0.999917410714286">
graphs are converted into tree structures, and well-
established parsing methods for tree structures are
applied to converted structures. In run-time, the tree
parser is applied, and predicted trees are converted
back into graph structures. Labels of tree edges en-
code additional information to recover original graph
structures. This idea was applied in Du et al. (2014)
and contributed to their best-performing system in
the 2014 SDP task.
In addition to applying the Mate parser to the tree-
transformed data of Du et al. (2014), Riga developed
a high-precision but low-recall semantic parser. This
method applies a decision tree classifier (C6.0) to
edge detection. C6.0 learns patterns of semantic de-
pendencies, which means it outputs highly reliable
prediction when a learned pattern applies, while in
most cases it cannot produce any predictions. These
two types of parsers are finally combined by parser
ensemble. They also applied C6.0 to frame (or sense)
label prediction for DM and PSD. Graph parsing and
frame prediction are performed independently.
Peking proposed a novel method for graph-to-tree
transformation, namely weighted tree approximation.
The intuition behind this method is that the core part
of graph-to-tree transformation is the extraction of
an essential tree-forming subset of edges from se-
mantic dependency graphs, but it is not trivial to
determine a reasonable subset. Therefore, the idea
of weighted tree approximation is to define an edge
score to quantify importance of each edge, and ex-
tract tree-forming edges that maximizes the sum of
edge scores globally. After defining edge scores, tree-
forming edges with optimal scores can be extracted
by applying decoding methods like maximum span-
ning tree and the Eisner algorithm. They applied this
method as well as the previous method proposed in
Du et al. (2014) with several variations on encod-
ing edge labels, finally obtaining nine tree parsers.
In the final submission, outputs from these parsers
are combined by the parser ensemble technique. For
predicate disambiguation, they independently applied
a sequence labeling technique.
Turku took a completely different approach. They
consider each predicate separately, and apply se-
quence labeling for each predicate individually, to
recognize arguments of the target predicate. That is,
the task is reduced to assign each word an argument
tag (e.g. ARG1) or a negative ‘pseudo-’label indi-
cating it is not an argument of the target predicate.
Outputs from sequence labeling for each predicate
are combined to derive final semantic dependencies.
Top node recognition and frame label prediction are
performed separately. Turku is the only team who
participated in the Gold track; they used gold syntac-
tic dependencies as features for sequence labeling.
Lisbon and In-House applied their parsers from
</bodyText>
<page confidence="0.778598">
923
</page>
<bodyText confidence="0.999953966666667">
SDP 2014 without substantive changes. The Lisbon
parser (TurboSemanticParser) computes globally op-
timal semantic dependencies using rich second order
features on semantic dependencies, such as siblings
and grand parents. This optimization is impractical
in general, but they achieve tractable parsing time by
applying dual decomposition. In-House uses deep
parsers with specifically developed linguistically mo-
tivated grammars, namely the LinGO English Re-
source Grammar and the Enju grammar. As described
in Section 2, these same grammars were used for de-
riving the training and test data sets of this task, i.e.
these components of the In-House ensemble exclu-
sively support the DM and PAS target representations,
respectively.
Peking and Lisbon tend to attain high scores in
their participated tracks in LF. Riga ranked third in
LF in the closed tracks (both in-domain and out-of-
domain), while it achieved higher scores than others
in FF. This might be due to high-precision rules ob-
tained by their model, although this does not apply in
the cross-lingual track. The Turku results in the gold
track achieved considerably higher scores, which in-
dicate that better syntactic parsing will help improve
semantic dependency parsing.13 It is difficult to de-
scribe a tendency in the out-of-domain track; all the
systems scree three to five points lower scores than
the in-domain track, indicating that domain variation
is still a significant challenge in semantic dependency
parsing.
</bodyText>
<sectionHeader confidence="0.994296" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99945752173913">
We have described the motivation, design, and out-
comes of the SDP 2015 task on semantic dependency
parsing, i.e. retrieving bi-lexical predicate–argument
relations between all content words within an En-
glish sentence. We have converted to a common
format three existing annotations (DM, PAS, and
PSD) over the same text and have put this to use
in training and testing data-driven semantic depen-
dency parsers. In contrast to SDP 2014 the task was
extended by cross-domain testing and evaluation at
the level of ‘complete’ predications and semantic
frame (or sense) disambiguation. Furthermore, we
13The SDP 2014 and 2015 task setups, however, somewhat
artificially constrain the possible contributions of syntactic anal-
ysis, as all training and testing data (even in the closed track)
includes high-quality parts of speech and lemmata.
provided comparable annotations of Czech and Chi-
nese texts to enable cross-linguistic comparison. To
start further probing of the role of syntax in the re-
covery of predicate–argument relations, we added a
third (idealized) ‘gold’ track, where syntactic depen-
dencies are provided directly from available syntactic
annotations of the underlying treebanks.
</bodyText>
<sectionHeader confidence="0.990278" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999939681818182">
We are grateful to Angelina Ivanova for help in DM
data preparation and contrastive analysis, to Željko
Agi´c and Bernd Bohnet for consultation and assis-
tance in preparing our companion parses, to the Lin-
guistic Data Consortium (LDC) for support in dis-
tributing the SDP data to participants, as well as to
Emily M. Bender and two anonymous reviewers for
feedback on an earlier version of this manuscript.
We warmly thank the general SemEval 2015 chairs,
Preslav Nakov and Torsten Zesch, for always being
role-model organizers, equipped with an outstanding
balance of structure, flexibility, and community spirit.
Data preparation was supported through the ABEL
high-performance computing facilities at the Uni-
versity of Oslo, and we acknowledge the Scientific
Computing staff at UiO, the Norwegian Metacen-
ter for Computational Science, and the Norwegian
taxpayers. Part of the work was supported by the
grants 15-10472S, GP13-03351P and 15-20031S of
the Czech Science Foundation, and by the infrastruc-
tural funding by the Ministry of Education, Youth
and Sports of the Czech Republic (LM2010013).
</bodyText>
<page confidence="0.839089">
924
</page>
<sectionHeader confidence="0.994641" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999736788888889">
Bejˇcek, E., Hajiˇcová, E., Hajiˇc, J., Jínová, P.,
Kettnerová, V., Koláˇrová, V., ... Zikánová, Š.
(2013). Prague dependency treebank 3.0. Re-
trieved from http://hdl.handle.net/11858/
00-097C-0000-0023-1AAF-3
Bender, E. M., Flickinger, D., Oepen, S., Packard, W.,
&amp; Copestake, A. (2015). Layers of interpretation.
On grammar and compositionality. In Proceedings of
the 11th International Conference on Computational
Semantics. London, UK.
Bohnet, B., &amp; Nivre, J. (2012). A transition-based sys-
tem for joint part-of-speech tagging and labeled non-
projective dependency parsing. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Conference on Natural
Language Learning (p. 1455 –1465). Jeju Island, Ko-
rea.
Cinková, S. (2006). From PropBank to EngValLex. Adapt-
ing the PropBank lexicon to the valency theory of the
Functional Generative Description. In Proceedings
of the 5th International Conference on Language Re-
sources and Evaluation. Genoa, Italy.
de Marneffe, M.-C., MacCartney, B., &amp; Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 449 – 454). Genoa, Italy.
Du, Y., Zhang, F., Sun, W., &amp; Wan, X. (2014). Peking:
Profiling syntactic tree parsing techniques for semantic
graph parsing. In Proceedings of the 8th international
workshop on semantic evaluation (semeval 2014).
Fares, M., Oepen, S., &amp; Zhang, Y. (2013). Machine learn-
ing for high-quality tokenization. Replicating variable
tokenization schemes. In Computational linguistics
and intelligent text processing (p. 231 – 244). Springer.
Flickinger, D. (2000). On building a more efficient gram-
mar by exploiting types. Natural Language Engineer-
ing, 6 (1), 15 – 28.
Flickinger, D., Zhang, Y., &amp; Kordoni, V. (2012). Deep-
Bank. A dynamically annotated treebank of the Wall
Street Journal. In Proceedings of the 11th International
Workshop on Treebanks and Linguistic Theories (p. 85 –
96). Lisbon, Portugal: Edições Colibri.
Francis, W. N., &amp; Kuˇcera, H. (1982). Frequency analysis
of English usage. Lexicon and grammar. New York,
USA: Houghton Mifflin.
Gildea, D., &amp; Jurafsky, D. (2002). Automatic labeling of
semantic roles. Computational Linguistics, 28, 245 –
288.
Grice, H. P. (1968). Utterer’s meaning, sentence-meaning,
and word-meaning. Foundations of Language, 4(3),
225–242.
Hajiˇc, J., Hajiˇcová, E., Panevová, J., Sgall, P., Bojar, O.,
Cinková, S., ... Žabokrtský, Z. (2012). Announcing
Prague Czech-English Dependency Treebank 2.0. In
Proceedings of the 8th International Conference on
Language Resources and Evaluation (p. 3153 – 3160).
Istanbul, Turkey.
Ivanova, A., Oepen, S., Øvrelid, L., &amp; Flickinger, D.
(2012). Who did what to whom? A contrastive study of
syntacto-semantic dependencies. In Proceedings of the
Sixth Linguistic Annotation Workshop (p. 2 –11). Jeju,
Republic of Korea.
Kate, R. J., &amp; Wong, Y. W. (2010). Semantic parsing.
The task, the state of the art and the future. In Tutorial
abstracts of the 20th Meeting of the Association for
Computational Linguistics (p. 6). Uppsala, Sweden.
Marcus, M., Santorini, B., &amp; Marcinkiewicz, M. A. (1993).
Building a large annotated corpora of English. The
Penn Treebank. Computational Linguistics, 19, 313 –
330.
Martins, A. F. T., &amp; Almeida, M. S. C. (2014). Priberam:
A turbo semantic parser with second order features. In
In proceedings of the 8th international workshop on
semantic evaluation (semeval 2014).
Mel’ˇcuk, I. (1988). Dependency syntax. Theory and
practice. Albany, NY, USA: SUNY Press.
Meyers, A., Reeves, R., Macleod, C., Szekely, R., Zielin-
ska, V., Young, B., &amp; Grishman, R. (2004). Annotating
noun argument structure for NomBank. In Proceed-
ings of the 4th International Conference on Language
Resources and Evaluation (p. 803 – 806). Lisbon, Por-
tugal.
Miyao, Y. (2006). From linguistic theory to syntactic
analysis. Corpus-oriented grammar development and
feature forest model. Unpublished doctoral dissertation,
University of Tokyo, Tokyo, Japan.
Miyao, Y., Oepen, S., &amp; Zeman, D. (2014). In-House. An
ensemble of pre-existing off-the-shelf parsers. In Pro-
ceedings of the 8th International Workshop on Semantic
</reference>
<page confidence="0.577331">
925
</page>
<reference confidence="0.997983148148148">
Evaluation (p. 63 –72). Dublin, Ireland.
Nakov, P. (2013). On the interpretation of noun com-
pounds: Syntax, semantics, and entailment. Natural
Language Engineering, 19(3), 291–330.
Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D.,
Flickinger, D., Hajiˇc, J., ... Zhang, Y. (2014). Se-
mEval 2014 Task 8. Broad-coverage semantic depen-
dency parsing. In Proceedings of the 8th International
Workshop on Semantic Evaluation. Dublin, Ireland.
Palmer, M., Gildea, D., &amp; Kingsbury, P. (2005). The
Proposition Bank. A corpus annotated with semantic
roles. Computational Linguistics, 31(1), 71–106.
Quine, W. V. O. (1960). Word and object. Cambridge,
MA, USA: MIT Press.
Titov, I., Henderson, J., Merlo, P., &amp; Musillo, G. (2009).
Online graph planarisation for synchronous parsing of
semantic and syntactic dependencies. In Proceedings
of the 21st International Joint Conference on Artificial
Intelligence. Pasadena, CA, USA.
Vadas, D., &amp; Curran, J. (2007). Adding Noun Phrase
Structure to the Penn Treebank. In Proceedings of
the 45th Meeting of the Association for Computational
Linguistics (p. 240–247). Prague, Czech Republic.
Xue, N., Xia, F., Chiou, F.-D., &amp; Palmer, M. (2005). The
Penn Chinese TreeBank. Phrase structure annotation
of a large corpus. Natural Language Engineering, 11,
207–238.
</reference>
<page confidence="0.908846">
926
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9061345">SemEval 2015 Task 18: Broad-Coverage Semantic Dependency Parsing</title>
<author confidence="0.8250985">Marco Yusuke Daniel Dan Jan</author>
<author confidence="0.8250985">Zdefnka</author>
<affiliation confidence="0.9341206">of Oslo, Department of University, Department of University, Department of Computer and Information Institute of Informatics, University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied</affiliation>
<address confidence="0.52623">University, Center for the Study of Language and Information</address>
<email confidence="0.964932">sdp-organizers@emmtee.net</email>
<abstract confidence="0.958624866666666">18 at SemEval 2015 defines Broad- Semantic Dependency Parsing as the problem of recovering sentence-internal relationships for coni.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic dependency target representations used, and summarize the task setup, participating systems, and main results. 1 Background and Motivation Syntactic dependency parsing has seen great advances in the past decade, but tree-oriented parsers are ill-suited for producing meaning representations, i.e. moving from the analysis of grammatical structure to sentence semantics. Even if syntactic parsing arguably can be limited to tree structures, this is not the case in semantic analysis, where a node will often be the argument of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs). Task 18 at SemEval 2015, Dependency Parsing seeks to stimulate the parsing community to move towards further technical details, information on how to obtain the data, and official results. more general graph processing, to thus enable a more analysis of did What to Whom? Extending the very similar predecessor task SDP 2014 (Oepen et al., 2014), we make use of three distinct, parallel semantic annotations over the same common texts, viz. the venerable Wall Street Journal (WSJ) and Brown segments of the Penn Treebank (PTB; Marcus et al., 1993) for English, as well as comparable resources for Chinese and Czech. Figure 1 below shows example target representations, bi-lexical semantic dependency graphs in all cases, for the WSJ sentence: (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice. is dependent on the determiner (the quantificational locus), the modifier and the predicate Conversely, the copula, infinitival and the vacuous marking the deep object of be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea &amp; Jurafsky, However, we require parsers to identify ‘fullmuch previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 of the 9th International Workshop on Semantic Evaluation (SemEval pages 915–926, Colorado, June 4-5, 2015. Association for Computational Linguistics A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-i p:e-u-i n:x n:x _ n:x _ (a) DELPH-IN Minimal Recursion Semantics–derived bi-lexical dependencies (DM). (c) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PSD). Figure 1: Sample semantic dependency graphs for Example (1). top BV ARG1 ARG1 ARG2 ARG3 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c ev-w218f2 _ _ _ _ _ _ev-w119f2 _ _ _ _ _ _ _ _ _ _ _ _ ARG1 ARG2 ARG2 top</abstract>
<note confidence="0.904922214285714">ARG1 ARG1 ARG1 ARG2 ARG1 A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice . (b) Enju Predicate–Argument Structures (PAS). A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice . ARG2 ARG1 ARG1 ARG1 ARG1 ARG2 ARG1</note>
<abstract confidence="0.988778714285714">ARG1 RSTR top PAT-arg PAT-arg EXT ADDR-arg ACT-arg CONJ.m ADDR-arg RSTR ADDR-arg APPS.m APPS.m CONJ.m CONJ.m ADDR-arg ARG1 ARG2 ARG1 ARG2 sentence’ semantic dependencies, i.e. compute a repthat integrates words in one structure. Finally, a third related area of much interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to much work in this tradition, our SDP target representations aim to be taskand domain-independent. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running exam- (Figure 1), showing what are called the PAS, dependencies, there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. Advancing in-depth comparison of representations and design decisions, in fact, is among the moand other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—often remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inference-based techniques. tivations for the SDP task series. Please see Oepen et al. (2014) and Miyao et al. (2014) for additional background. DM: DELPH-IN MRS-Derived Bi-Lexical Desemantic dependency graphs originate in a manual re-annotation, dubbed Deep- Bank, of Sections 00–21 of the WSJ Corpus and of selected parts of the Brown Corpus with syntacticosemantic analyses of the LinGO English Resource Grammar (Flickinger, 2000; Flickinger et al., 2012). For this target representation, top nodes designate the highest-scoping (non-quantifier) predicate in the e.g. the (scopal) adverb Figure Enju Predicate–Argument Structures Treebank and are derived from the automatic HPSG-style annotation of the PTB (Miyao, Our dependency graphs are extracted from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; non-scopal adverbs act as mere intersective modie.g. in a structure like sang the adverb is a predicate in DM, but the main verb nevertheless is the top node. 916 Xue et al., 2005). Top nodes in this representation denote semantic heads.</abstract>
<title confidence="0.5260095">Prague Semantic Dependencies Prague Czech-English Dependency Treebank</title>
<abstract confidence="0.987770805970149">Hajiˇc et al., is a set of parallel dependency trees over the WSJ texts from the PTB, their Czech translations. Our dependencies have been extracted from what is called layer (t-trees). Top nodes are derived from t-tree roots; i.e. they mostly correspond to main verbs. In case of coordinate clauses, there are multiple top nodes per sentence. 3 Data Format The SDP target representations can be characterized as labeled, directed graphs. Nodes are labeled with pieces of information: word a Boolean flag indicating whether the represents a and optional (or sense) information—for example the distinction causative vs. inchoative predicates like in- Edges are labeled with semantic relations that hold between source and target. All data provided for the task uses a column-based file format that extends the format of the SDP 2014 by a new (thus making it a little more SRL-like). More details about the file format are available at the task website. 4 Data Sets All three target representations for English are annotations of the same text, Sections 00–21 of the WSJ Corpus, as well as of a balanced sample of twenty files from the Brown Corpus (Francis &amp; Kuˇcera, 1982). For this task, we have synchronized these resources at the sentence and tokenization levels and excluded from the SDP 2015 training and testing data any sentences for which (a) one or more of the treebanks lacked a gold-standard analysis; (b) a oneto-one alignment of tokens could not be established across all three representations; or (c) at least one of the graphs was cyclic. Of the 43,746 sentences in these 22 first sections of WSJ text, DeepBank analyses for some 11%, and the Enju Treehas gaps for a little more than four Finally, 139 of the WSJ graphs obtained through the above conversions were cyclic. In total, we were left with 35,657 sentences (or 802,717 tokens; eight more than for SDP as training data (Sections 00–20), 1,410 in-domain testing sentences (31,948 tokens) from WSJ Section 21, and 1,849 outof-domain testing sentences (31,583 tokens) from the Brown Corpus. Besides the additions of out-of-domain test data and frame (or sense) identifiers for English, another extension beyond the SDP 2014 task concerns the inclusion of additional languages, albeit only for select target representations. Our training data included an additional 31,113 Chinese sentences (649,036 tokens), taken from Release 7.0 of the CTB, for the representation, and 42,076 Czech sentences (985,302 tokens), drawing on the translations the WSJ Corpus in PCEDT 2.0, for the representation. Additional out-of-domain Czech test data was drawn from the Prague Dependency Treebank 3.0 (PDT; Bejˇcek et al., 2013). For these additional languages, the task comprised 1,670 sentences (38,397 tokens) of in-domain Chinese test data, and 1,670 sentences (38397 tokens) and 5,226 sentences (87,927 tokens) of inand out-of-domain Czech data, respectively. Comparison a first attempt at contrasting our three target representations, Table 1 shows some high-level statistics of the graphs comthe training and testing In terms of distinctions drawn in dependency labels (1), there are clear differences between the representawith linguistically most finesome 500 sentences show tokenization mismatches, most owing to DeepBank correcting PTB idiosynlike and and introducing a few new ones (Fares et al., 2013). comparison to the SDP 2014 data, our DM graphs were extracted from a newer, improved release of DeepBank (Version 1.1), and its conversion to bi-lexical dependencies was moderately revised to provide more systematic analyses of contracted negated auxiliaries and comparatives. At the same time, the extraction of PSD graphs from the PCEDT t-trees was refined to include edges representing grammatical coreference, e.g. reentrancies introduced by control verbs. statistics are obtained using the ‘official’ SDP toolkit. Our notions of singletons, roots, re-entrancies, and projectivity follow common graph terminology, but see Oepen et al. (2014) for formal definitions. 917 PSD PAS PSD DM PAS PSD DM PAS PSD % singletons 22.97 4.38 35.76 28.91 0.11 25.40 5.84 39.11 29.04 edge density 0.96 1.02 1.01 1.03 0.98 0.95 1.02 0.99 1.00 trees 2.30 1.22 42.19 37.66 3.49 9.68 2.38 51.43 51.49 noncrossing 69.03 59.57 64.58 63.22 67.61 74.58 65.28 74.26 72.41 projective 2.91 1.64 41.92 38.32 12.89 8.82 3.46 54.35 53.02 fragmented 6.55 0.23 0.69 1.17 15.22 4.71 0.65 1.73 3.50 27.44 29.36 11.42 11.80 24.96 26.14 29.36 11.46 11.44 topless 0.31 0.02 – 0.04 6.92 1.41 – – 0.02 roots 44.91 55.98 4.35 4.73 46.65 39.89 50.93 5.27 5.31 13.52 – 16.77 – – 15.79 – 19.50 – average treewidth 1.30 1.72 1.61 1.66 1.35 1.31 1.69 1.50 1.49 maximum treewidth 3 3 7 6 3 3 3 5 5 Table 1: Contrastive high-level graph statistics across target representations, languages, and domains. and the smallest label inventory. Unattached singleton nodes (2) in our setup correspond to tokens analyzed as semantically vacuous, which (as seen in Figure 1) include most puncmarks in but not Furthe other two) analyzes some high-frequency determiners as semantically vacuous. average has more edges per (nonsingleton) nodes than the other two (3), which likely reflects its approach to the analysis of functional words (see below). Judging from both the percentage of actual trees (4), the proportions of noncrossing graphs (5), projective graphs (6), and the proportions of reentrant (8), more ‘tree-oriented’ than the other two, which at least in part reflects its approach to the analysis of modifiers and determiners (again, see below). We view the small percentages of graphs without at least one top node (9) and of graphs with at least two non-singleton components that are not interconnected (7) as tentative indicators of general well-formedness. Intuitively, there should always be a ‘top’ predicate, and the whole graph should ‘hang Only non-trivial (if small) degrees of topless and fragmented graphs, which may indicate imperfections in DeepBank annotations or room for improvement in the conversion from full logical forms to bi-lexical dependencies, but possibly also exceptions to our intuitions about semantic dependency graphs.</abstract>
<note confidence="0.789168333333333">Directed Undirected DM PAS PSD DM PAS PSD DM − .6425 .2612 − .6719 .5675</note>
<phone confidence="0.256851">PAS .6688 − .2963 .6993 − .5490</phone>
<abstract confidence="0.99193979342723">PSD .2636 .2963 − .5743 .5630 − 2: Pairwise including punctuation (upper right diagonals) or not (lower left). Frame or sense distinctions are a new property in SDP 2015 and currently are only available for the Table 1 reveals a stark in granularity: itself to argument structure distinctions that are grammaticized, e.g. causative vs. inchoative contrasts or differences in the arity or coarse semantic typing of argument on the other hand, draws on the much richer sense inventory of the EngValLex database (Cinková, 2006). Accordingly, the two target representations represent quite different challenges for the predicate disambiguation sub-task of SDP 2015. Finally, in Table 2 we seek to quantify pairwise structural similarity between the three representations terms of unlabeled dependency in Section 5 below). We provide four variants of this metric, (a) taking into account the directionality of edges or not and (b) including edges involving marks or not. On this view, are structurally much closer to each other than either the two is to even more so when discarding 918 punctuation. While relaxing the comparison to ignore edge directionality also increases similarity scores for this pair, the effect is much more pronounced comparing either to suggests that directionality of semantic dependencies is a major of diversion between the one and the other hand. Comparison other aspects, Ivanova et al. (2012) categorize a range of syntactic and semantic dependency annotation schemes according to the role that functional elements take. In Figure 1 and the discussion of Table 1 above, we already that from the other representations in integrating into the graph auxiliaries, the infinitival marker, the case-marking preposition inthe argument of and most puncwhile these (and other functional elements, e.g. complementizers) are analyzed as sevacuous in function predicates in do not always serve as ‘local’ top nodes (i.e. the semantic head of the corresponding sub-graph): For example, the infinitival marker in Figure 1 takes the verb as its argument, but ‘upstairs’ predicate directly to the verb, rather than to the infinitival marker as an intermediate. the same time, alike in their approach to modifiers, e.g. attributive adjectives, adand prepositional phrases. Unlike in common syntactic dependency schemes), these are analyzed as semantic predicates and, thus, contribute to higher degrees of node reentrancy and non-top (structural) roots. Roughly the same holds for debut here our of Prague tectogrammatical trees onto bi-lexical dependencies ‘vanilla’ articles (like as singleton nodes. The analysis of coordination is distinct in the three representations, as also evident in Figure 1. By defor what is often called the Mel’ˇcukian analysis of coordinate structures (Mel’ˇcuk, 1988), with a chain of dependencies rooted at the first conjunct (which is thus considered the head, ‘standing for the structure at large); in the all formats, punctuation marks like dashes, colons, and sometimes commas can be contentful, i.e. at times occur as both predicates, arguments, and top nodes. coordinating conjunctions are not integrated with the graph but rather contribute different types of depen- In final coordinating conjunction is the head of the structure and each coordinating conjunction (or intervening punctuation mark that acts like one) is a two-place predicate, taking left and conjuncts as its arguments. Conversely, in the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected the A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase– internal structure is not made explicit in the PTB, and Enju Treebank from which our representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal comexample of Figure 2, thus, at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of compound either; the other hand, annoboth the bracketing the assignment of roles, e.g. making of In this spirit, the annotations could be directly paraphrased along the lines by employees for investment in stocks. a position between the other two, disambiguates the bracketing but, by design, merely assigns an underspecified, construction-specific dependency its then, is to be interpreted as the most general type of dependency that can hold between the elements of this construction (i.e. to a first approximation either an argument role or a relation parallel to a preposition, as in the above The of this specific example happen to diverge in their bracketdecisions, where the corresponds to investments in stock for i.e. grouping detailed by Miyao et al. (2014), individual conjuncts can be (and usually are) arguments of other predicates, whereas the topmost conjunction only has incoming edges in nested coordinate structures. Similarly, a ‘shared’ modifier of the coordinate structure as a whole would take as its argument the local top node of the coordination in DM or PAS (i.e. the first conjunct or final conjunction, respectively), whereas it would depend as an argument on all conjuncts in PSD. 919 compound compound compound ARG1 ARG1 ACT ARG1 PAT REG employee stock investment plans employee stock investment plans employee stock investment plans 2: Analysis of nominal compounding in PAS, concept stock contrast to ‘common stock’). Without context and expert knowledge, these decisions are hard to call, and indeed there has been much previous work seeking to identify and annotate the relations that hold between members of a nominal compound (see Nakov, 2013, for a recent overview). To what degree the bracketing and role disambiguation in this example are determined by the linguistic signal (rather than by context and world knowledge, say) can be debated, and thus the observed differences among our representations in this example relate to the classic contrast between ‘sentence’ (or ‘conventional’) meaning, on the one hand, and ‘speaker’ (or ‘occasion’) meaning, on the other hand (Quine, 1960; Grice, 1968; Bender et al., 2015). In turn, we acknowledge different plausible points of view about which level of semantic representation should the target representation for data-driven (i.e. structural analysis guided by the grammatical system), and which refinements like the above could construed as part of a subsequent task of interpre- 5 Task Setup English training data for the task, providing all columns in the file format sketched in Section 3 above, together with a first version of the SDP toolkit—including graph input, basic statistics, and scoring—were released to candidate participants in early August 2014. In mid-November, cross-lingual training data, a minor update to the English data, and optional syntactic ‘companion’ analyses (see below) were provided. Anytime between mid-December 2014 and mid-January 2015, participants could request an input-only version of the test data, with just columns (1) to (4) pre-filled; participants then had six days to run their systems on these inputs, fill in columns (5), (6), (7), and upwards, and submit their results (from up to two different runs) for scoring. Upon completion of the testing phase, we have shared the gold-standard test data, official scores, and system results for all submissions with participants and are currently preparing all data for general release through the Linguistic Data Consortium. participating in the task were evaluated based on the accuracy with which they can produce semantic dependency graphs for previously unseen text, measured relative to the gold-standard testing data. For comparability with SDP 2014, the primary measures for this evaluation were labeled and unlabeled precision and recall with respect to predicted dependencies (predicate–role–argument triples) and labeled and unlabeled exact match with respect to complete graphs. In both contexts, identification of the top node(s) of a graph was considered as the identification of additional, ‘virtual’ dependencies from an artificial root node (at position 0). Below we abbreviate these metrics as (a) labeled precision, and LR, (b) unlabeled precision, and UR, and (c) labeled and unlaexact match: UM. The ‘official’ ranking of participating systems is determined based on the arithmetic mean of the ladependency (i.e. the geometric mean of labeled precision and labeled recall) on the three representations PAS, Thus, to be competitive in the overall ranking, a system had to submit semantic dependencies for all three target representations. In addition to these metrics, we apply two additional metrics that aim to capture fragments of semantics that are ‘larger’ than individual dependencies but ‘smaller’ than the semantic dependency graph for the sentence, viz. what we call (a) (b) A complete predication is comprised of the set of all core arguto one predicate, which for the target representations corresponds to all outgoing edges, and for the representation to only those outgoing dependencies marked an suffix on the edge label. Pushing the of evaluation one step further towards inter-</abstract>
<address confidence="0.447054">920</address>
<affiliation confidence="0.762935">LF DM PAS PSD LF LP LR FF LF LP LR PF LF LP LR FF</affiliation>
<address confidence="0.896758625">86.81 88.29 89.52 87.09 58.39 95.58 95.94 95.21 87.99 76.57 78.24 74.97 56.85 Lisbon* 86.23 89.44 90.52 88.39 00.20 91.67 92.45 90.90 84.18 77.58 79.88 75.41 00.06 Peking 85.33 89.09 90.93 87.32 63.08 91.26 92.90 89.67 79.08 75.66 78.60 72.93 49.95 Lisbon 85.15 88.21 89.84 86.64 00.15 90.88 91.87 89.92 81.74 76.36 78.62 74.23 00.03 Riga 84.00 87.90 88.57 87.24 58.12 90.75 91.50 90.02 80.03 73.34 75.25 71.52 52.54 Turku* 83.47 86.17 87.80 84.60 54.67 90.62 91.38 89.87 80.60 73.63 76.10 71.32 53.20 Minsk 80.74 84.13 86.28 82.09 54.24 85.24 87.28 83.28 64.66 72.84 74.65 71.13 51.63 In-House* 61.61 92.80 92.85 92.75 83.79 92.03 92.07 91.99 87.24 – – – –</address>
<affiliation confidence="0.7302555">LF DM PAS PSD LF LP LR FF LF LP LR PF LF LP LR FF</affiliation>
<address confidence="0.882911625">83.50 82.11 84.26 80.07 42.89 92.92 93.52 92.33 83.80 75.47 77.77 73.31 42.37 Lisbon* 82.53 83.77 85.79 81.84 00.35 87.63 88.88 86.41 80.19 76.18 80.12 72.61 02.25 Lisbon 81.15 81.75 84.81 78.90 00.27 86.88 88.52 85.30 78.47 74.82 78.68 71.31 02.09 Peking 80.78 81.84 84.29 79.53 47.49 87.23 89.47 85.10 74.75 73.28 77.36 69.61 34.28 Riga 79.23 80.69 81.69 79.72 41.88 86.63 87.56 85.72 76.26 70.37 73.23 67.71 40.76 Turku* 78.85 79.01 81.54 76.63 39.15 85.95 86.95 84.98 76.38 71.59 74.92 68.55 38.75 Minsk 75.79 77.24 80.24 74.46 42.18 80.44 83.07 77.96 62.00 69.68 72.26 67.27 41.25 In-House* 59.24 89.69 89.80 89.58 76.39 88.03 88.10 87.96 81.69 – – – –</address>
<abstract confidence="0.99013425">3: Results of the gold track (marked open track (marked *) and closed track (unmarked) submissions the English in-domain (top) and out-of-domain (bottom) data. For each system, the second column the averaged across all representations, used to rank the systems. The best track scores are highlighted in italices.</abstract>
<affiliation confidence="0.757086">LF LP LR PF LF LP LR PF LF LP LR PF</affiliation>
<address confidence="0.8464422">Peking 83.43 84.75 82.15 66.09 Lisbon 79.33 83.52 75.54 55.91 Peking 64.37 69.41 60.02 48.82 Riga 82.47 83.12 81.84 66.05 Peking 78.45 83.61 73.89 55.36 Turku* 63.70 65.11 62.35 51.04 Lisbon 82.02 83.81 80.31 66.05 Riga 75.34 78.77 72.19 50.90 Lisbon 63.50 67.94 59.61 43.10 Turku* 79.64 80.81 78.51 62.04 Turku* 75.30 77.53 73.20 54.26 Riga 61.32 64.50 58.44 44.34 Minsk 77.68 79.27 76.15 58.23</address>
<abstract confidence="0.98223296958175">Table 4: Results of the open (Turku) and closed (other teams) tracks for the Chinese in-domain (left) and in- (center) and out-of-domain (right) data. The systems are ranked according to their pretation, a semantic frame is comprised of a complete predication combined with the frame (or sense) identifier of its predicate. Both complete-predicate and semantic-frame evaluation are restricted to predicates corresponding to verbal parts of speech (as determined by the gold-standard part of speech), and semantic frames are further restricted to those target representations for which frame or sense information available in our data (English As with the other metrics, we score precision, recall, and which we abbreviate as and compredications, and and semantic frames. vs. Open vs. Gold Tracks like in the task distinguished a and an where systems in the closed track could only be trained on the gold-standard semantic dependencies distributed for the task. Systems in the open track, on the other hand, could use additional resources, such as a syntactic parser, for example—provided that they make sure to not use any tools or resources that encompass knowledge of the gold-standard syntactic or semantic analyses of 921 SDP 2015 test To simplify participation in the open track, the organizers prepared ready-touse ‘companion’ syntactic analyses, sentenceand token-aligned to the SDP data, in the form of Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). Finally, to more directly gauge the the contributions of syntactic structure on the semantic depenparsing problem, an idealized was introduced in SDP 2015. For this track, gold-standard syntactic companion files were provided in a varity of formats, viz. (a) Stanford Basic dependencies, derived from the PTB, (b) HPSG syntactic dependenin the form called Ivanova et al. (2012), derived from DeepBank, and (c) HPSG syntactic dependencies derived from the Enju Treebank. 6 Submissions and Results From almost 40 teams who had registered for the task, twelve teams obtained the test data, and test runs were submitted for six systems—including one ‘inofficial’ submission by a sub-set of the task organizers (Miyao et al., 2014). Each team submitted up to two test runs per track. In total, there were seven runs submitted to the English closed track, five to the open track and two to the gold track; seven runs were submitted to the Chinese closed track, two to the open track; and five runs submitted to the Czech closed track, two to the open track. One team submitted only to the open and gold tracks, three teams submitted only to the closed track, one team submitted to open and closed tracks in English but only to the closed tracks in the other two languages. The main results are summarized and ranked in Tables 3 4. The ranking is based on the average all three target representations. Besides also indicate the of prediction semantic frames or, where frame (or sense) identifiers are not available, of complete predications In cases where a team submitted two runs to a track, only the highest-ranked score is included in the table. the English closed track, the average restriction implies that typical off-the-shelf syntactic parsers have to be re-trained, as many data-driven parsers for English include WSJ Section 21 in their default training data. across target representations range from 85.33 to 80.74. Comparing the results for different target repthe average across systems 89.13 for for 74.24 for The scores for semantic frames show a much larger across representations and The Lisbon team is the only one that submitted to both the open and the closed tracks; with the additional resources allowed in the open track, they were able to improve over all closed-track submissions. Similarly, the perfect Stanford dependencies in the track helped the Turku team a lot in in interestingly, they did not obtain the best results in the latter two representations, but their cross-representation average was still the best. The In-House system is ranked low because its submission was incomplete (no of-the-shelf parser available); however, for they yielded the best open-track scores. We see very similar trends for the out-of-domain data, though the scores are a few points lower. to be more difficult than En- (cross-system average 81.05, as opposed to English 90.07). The Czech and English in-domain data are actually parallel translations and Czech slightly higher (77.11, as opposed to English 74.90). The Turku open-track system shined in the Czech out-of-domain data, presumably because the additional dependency parser they used was trained on data from the target domain. 7 Overview of Approaches Table 5 shows a summary of the tracks in which each submitted system participated, and Table 6 shows an overview of approaches and additionally used resources. All the teams except In-House submitted results for cross-lingual data (Czech and Chinese). Teams except Lisbon also tackled with predicate disambiguation. Only Turku participated in the Gold track. The submitted teams explored a variety of approaches. Riga and Peking relied on the graph-to-tree transformation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency see the task web page at the address indicated above for full labeled and unlabeled scores. 922 Team Closed Open Cross-Lingual Predicate Disambiguation Gold ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House grammar-based parsing (Miyao et al., 2014) ERG &amp; Enju Lisbon graph parsing with dual decomposition (Martins &amp; Almeida, 2014) companion Minsk transition-based dependency graph parsing in the spirit of Titov et al. (2009) — Peking (Du et al., 2014) extended with weighted tree approximation, parser ensemble — Riga (Du et al., 2014)’s graph-to-tree transformation, Mate, C6.0, parser ensemble — Turku sequence labeling for argument detection for each predicate, SVM classifiers companion for top node recognition and sense prediction Table 6: Overview of approaches and additional resources used (if any). graphs are converted into tree structures, and wellestablished parsing methods for tree structures are applied to converted structures. In run-time, the tree parser is applied, and predicted trees are converted back into graph structures. Labels of tree edges encode additional information to recover original graph structures. This idea was applied in Du et al. (2014) and contributed to their best-performing system in the 2014 SDP task. In addition to applying the Mate parser to the treetransformed data of Du et al. (2014), Riga developed a high-precision but low-recall semantic parser. This method applies a decision tree classifier (C6.0) to edge detection. C6.0 learns patterns of semantic dependencies, which means it outputs highly reliable prediction when a learned pattern applies, while in most cases it cannot produce any predictions. These two types of parsers are finally combined by parser ensemble. They also applied C6.0 to frame (or sense) prediction for Graph parsing and frame prediction are performed independently. Peking proposed a novel method for graph-to-tree transformation, namely weighted tree approximation. The intuition behind this method is that the core part of graph-to-tree transformation is the extraction of an essential tree-forming subset of edges from semantic dependency graphs, but it is not trivial to determine a reasonable subset. Therefore, the idea of weighted tree approximation is to define an edge score to quantify importance of each edge, and extract tree-forming edges that maximizes the sum of edge scores globally. After defining edge scores, treeforming edges with optimal scores can be extracted by applying decoding methods like maximum spanning tree and the Eisner algorithm. They applied this method as well as the previous method proposed in Du et al. (2014) with several variations on encoding edge labels, finally obtaining nine tree parsers. In the final submission, outputs from these parsers are combined by the parser ensemble technique. For predicate disambiguation, they independently applied a sequence labeling technique. Turku took a completely different approach. They consider each predicate separately, and apply sequence labeling for each predicate individually, to recognize arguments of the target predicate. That is, the task is reduced to assign each word an argument tag (e.g. ARG1) or a negative ‘pseudo-’label indicating it is not an argument of the target predicate. Outputs from sequence labeling for each predicate are combined to derive final semantic dependencies. Top node recognition and frame label prediction are performed separately. Turku is the only team who participated in the Gold track; they used gold syntactic dependencies as features for sequence labeling. Lisbon and In-House applied their parsers from 923 SDP 2014 without substantive changes. The Lisbon computes globally optimal semantic dependencies using rich second order features on semantic dependencies, such as siblings and grand parents. This optimization is impractical in general, but they achieve tractable parsing time by applying dual decomposition. In-House uses deep parsers with specifically developed linguistically motivated grammars, namely the LinGO English Resource Grammar and the Enju grammar. As described in Section 2, these same grammars were used for deriving the training and test data sets of this task, i.e. these components of the In-House ensemble exclusupport the representations, respectively. Peking and Lisbon tend to attain high scores in participated tracks in ranked third in the closed tracks (both in-domain and out-ofdomain), while it achieved higher scores than others might be due to high-precision rules obtained by their model, although this does not apply in the cross-lingual track. The Turku results in the gold track achieved considerably higher scores, which indicate that better syntactic parsing will help improve dependency It is difficult to describe a tendency in the out-of-domain track; all the systems scree three to five points lower scores than the in-domain track, indicating that domain variation is still a significant challenge in semantic dependency parsing. 8 Conclusion We have described the motivation, design, and outcomes of the SDP 2015 task on semantic dependency parsing, i.e. retrieving bi-lexical predicate–argument relations between all content words within an English sentence. We have converted to a common three existing annotations PAS, over the same text and have put this to use in training and testing data-driven semantic dependency parsers. In contrast to SDP 2014 the task was extended by cross-domain testing and evaluation at the level of ‘complete’ predications and semantic frame (or sense) disambiguation. Furthermore, we SDP 2014 and 2015 task setups, however, somewhat artificially constrain the possible contributions of syntactic analysis, as all training and testing data (even in the closed track) includes high-quality parts of speech and lemmata. provided comparable annotations of Czech and Chinese texts to enable cross-linguistic comparison. To start further probing of the role of syntax in the recovery of predicate–argument relations, we added a third (idealized) ‘gold’ track, where syntactic dependencies are provided directly from available syntactic annotations of the underlying treebanks. Acknowledgements are grateful to Angelina Ivanova for help in data preparation and contrastive analysis, to Željko Agi´c and Bernd Bohnet for consultation and assistance in preparing our companion parses, to the Linguistic Data Consortium (LDC) for support in distributing the SDP data to participants, as well as to Emily M. Bender and two anonymous reviewers for feedback on an earlier version of this manuscript. We warmly thank the general SemEval 2015 chairs, Preslav Nakov and Torsten Zesch, for always being role-model organizers, equipped with an outstanding balance of structure, flexibility, and community spirit. preparation was supported through the high-performance computing facilities at the University of Oslo, and we acknowledge the Scientific</abstract>
<note confidence="0.71745085">Computing staff at UiO, the Norwegian Metacenter for Computational Science, and the Norwegian taxpayers. Part of the work was supported by the grants 15-10472S, GP13-03351P and 15-20031S of the Czech Science Foundation, and by the infrastructural funding by the Ministry of Education, Youth and Sports of the Czech Republic (LM2010013). 924 References Bejˇcek, E., Hajiˇcová, E., Hajiˇc, J., Jínová, P., Kettnerová, V., Koláˇrová, V., ... Zikánová, Š. dependency treebank 3.0. Refrom 00-097C-0000-0023-1AAF-3 Bender, E. M., Flickinger, D., Oepen, S., Packard, W., &amp; Copestake, A. (2015). Layers of interpretation. grammar and compositionality. In of the 11th International Conference on Computational UK. Bohnet, B., &amp; Nivre, J. (2012). A transition-based sys-</note>
<abstract confidence="0.895813533333333">tem for joint part-of-speech tagging and labeled nondependency parsing. In of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Learning 1455 –1465). Jeju Island, Korea. Cinková, S. (2006). From PropBank to EngValLex. Adapting the PropBank lexicon to the valency theory of the Generative Description. In of the 5th International Conference on Language Reand Evaluation. Italy. de Marneffe, M.-C., MacCartney, B., &amp; Manning, C. D. (2006). Generating typed dependency parses from structure parses. In of the 5th International Conference on Language Resources and</abstract>
<note confidence="0.720692">449 – 454). Genoa, Italy. Du, Y., Zhang, F., Sun, W., &amp; Wan, X. (2014). Peking:</note>
<title confidence="0.59442">Profiling syntactic tree parsing techniques for semantic</title>
<abstract confidence="0.761715">parsing. In of the 8th international workshop on semantic evaluation (semeval 2014). Fares, M., Oepen, S., &amp; Zhang, Y. (2013). Machine learning for high-quality tokenization. Replicating variable schemes. In linguistics intelligent text processing 231 – 244). Springer.</abstract>
<note confidence="0.893116837209302">Flickinger, D. (2000). On building a more efficient gramby exploiting types. Language Engineer- 15 – 28. Flickinger, D., Zhang, Y., &amp; Kordoni, V. (2012). Deep- Bank. A dynamically annotated treebank of the Wall Journal. In of the 11th International on Treebanks and Linguistic Theories 85 – 96). Lisbon, Portugal: Edições Colibri. W. N., &amp; Kuˇcera, H. (1982). analysis English usage. Lexicon and New York, USA: Houghton Mifflin. Gildea, D., &amp; Jurafsky, D. (2002). Automatic labeling of roles. 245 – 288. Grice, H. P. (1968). Utterer’s meaning, sentence-meaning, word-meaning. of 225–242. Hajiˇc, J., Hajiˇcová, E., Panevová, J., Sgall, P., Bojar, O., Cinková, S., ... Žabokrtský, Z. (2012). Announcing Prague Czech-English Dependency Treebank 2.0. In Proceedings of the 8th International Conference on Resources and Evaluation 3153 – 3160). Istanbul, Turkey. Ivanova, A., Oepen, S., Øvrelid, L., &amp; Flickinger, D. (2012). Who did what to whom? A contrastive study of dependencies. In of the Linguistic Annotation Workshop 2 –11). Jeju, Republic of Korea. Kate, R. J., &amp; Wong, Y. W. (2010). Semantic parsing. task, the state of the art and the future. In abstracts of the 20th Meeting of the Association for Linguistics 6). Uppsala, Sweden. Marcus, M., Santorini, B., &amp; Marcinkiewicz, M. A. (1993). Building a large annotated corpora of English. The Treebank. 313 – 330. Martins, A. F. T., &amp; Almeida, M. S. C. (2014). Priberam: A turbo semantic parser with second order features. In In proceedings of the 8th international workshop on semantic evaluation (semeval 2014). I. (1988). syntax. Theory and Albany, NY, USA: SUNY Press. Meyers, A., Reeves, R., Macleod, C., Szekely, R., Zielin-</note>
<abstract confidence="0.878640857142857">ska, V., Young, B., &amp; Grishman, R. (2004). Annotating argument structure for NomBank. In Proceedings of the 4th International Conference on Language and Evaluation 803 – 806). Lisbon, Portugal. Y. (2006). linguistic theory to syntactic analysis. Corpus-oriented grammar development and</abstract>
<affiliation confidence="0.653817">forest Unpublished doctoral dissertation, University of Tokyo, Tokyo, Japan.</affiliation>
<address confidence="0.673772">Miyao, Y., Oepen, S., &amp; Zeman, D. (2014). In-House. An</address>
<note confidence="0.818072709677419">of pre-existing off-the-shelf parsers. In Proceedings of the 8th International Workshop on Semantic 925 63 –72). Dublin, Ireland. Nakov, P. (2013). On the interpretation of noun com- Syntax, semantics, and entailment. 291–330. Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D., Flickinger, D., Hajiˇc, J., ... Zhang, Y. (2014). SemEval 2014 Task 8. Broad-coverage semantic depenparsing. In of the 8th International on Semantic Evaluation. Ireland. Palmer, M., Gildea, D., &amp; Kingsbury, P. (2005). The Proposition Bank. A corpus annotated with semantic 71–106. W. V. O. (1960). and Cambridge, MA, USA: MIT Press. Titov, I., Henderson, J., Merlo, P., &amp; Musillo, G. (2009). Online graph planarisation for synchronous parsing of and syntactic dependencies. In of the 21st International Joint Conference on Artificial CA, USA. Vadas, D., &amp; Curran, J. (2007). Adding Noun Phrase to the Penn Treebank. In of the 45th Meeting of the Association for Computational 240–247). Prague, Czech Republic. Xue, N., Xia, F., Chiou, F.-D., &amp; Palmer, M. (2005). The Penn Chinese TreeBank. Phrase structure annotation a large corpus. Language 207–238. 926</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Bejˇcek</author>
<author>E Hajiˇcová</author>
<author>J Hajiˇc</author>
<author>P Jínová</author>
<author>V Kettnerová</author>
<author>V Koláˇrová</author>
</authors>
<title>Prague dependency treebank 3.0. Retrieved from http://hdl.handle.net/11858/</title>
<date>2013</date>
<pages>00--097</pages>
<marker>Bejˇcek, Hajiˇcová, Hajiˇc, Jínová, Kettnerová, Koláˇrová, 2013</marker>
<rawString>Bejˇcek, E., Hajiˇcová, E., Hajiˇc, J., Jínová, P., Kettnerová, V., Koláˇrová, V., ... Zikánová, Š. (2013). Prague dependency treebank 3.0. Retrieved from http://hdl.handle.net/11858/ 00-097C-0000-0023-1AAF-3</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Bender</author>
<author>D Flickinger</author>
<author>S Oepen</author>
<author>W Packard</author>
<author>A Copestake</author>
</authors>
<title>Layers of interpretation. On grammar and compositionality.</title>
<date>2015</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Semantics.</booktitle>
<location>London, UK.</location>
<contexts>
<context position="21639" citStr="Bender et al., 2015" startWordPosition="3406" endWordPosition="3409">re has been much previous work seeking to identify and annotate the relations that hold between members of a nominal compound (see Nakov, 2013, for a recent overview). To what degree the bracketing and role disambiguation in this example are determined by the linguistic signal (rather than by context and world knowledge, say) can be debated, and thus the observed differences among our representations in this example relate to the classic contrast between ‘sentence’ (or ‘conventional’) meaning, on the one hand, and ‘speaker’ (or ‘occasion’) meaning, on the other hand (Quine, 1960; Grice, 1968; Bender et al., 2015). In turn, we acknowledge different plausible points of view about which level of semantic representation should be the target representation for data-driven parsing (i.e. structural analysis guided by the grammatical system), and which refinements like the above could be construed as part of a subsequent task of interpretation. 5 Task Setup English training data for the task, providing all columns in the file format sketched in Section 3 above, together with a first version of the SDP toolkit—including graph input, basic statistics, and scoring—were released to candidate participants in early</context>
</contexts>
<marker>Bender, Flickinger, Oepen, Packard, Copestake, 2015</marker>
<rawString>Bender, E. M., Flickinger, D., Oepen, S., Packard, W., &amp; Copestake, A. (2015). Layers of interpretation. On grammar and compositionality. In Proceedings of the 11th International Conference on Computational Semantics. London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
<author>J Nivre</author>
</authors>
<title>A transition-based system for joint part-of-speech tagging and labeled nonprojective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning (p. 1455 –1465). Jeju Island,</booktitle>
<contexts>
<context position="28785" citStr="Bohnet and Nivre (2012)" startWordPosition="4574" endWordPosition="4577">antic dependencies distributed for the task. Systems in the open track, on the other hand, could use additional resources, such as a syntactic parser, for example—provided that they make sure to not use any tools or resources that encompass knowledge of the gold-standard syntactic or semantic analyses of 921 the SDP 2015 test data.11 To simplify participation in the open track, the organizers prepared ready-touse ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in the form of Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). Finally, to more directly gauge the the contributions of syntactic structure on the semantic dependency parsing problem, an idealized gold track was introduced in SDP 2015. For this track, gold-standard syntactic companion files were provided in a varity of formats, viz. (a) Stanford Basic dependencies, derived from the PTB, (b) HPSG syntactic dependencies in the form called DM by Ivanova et al. (2012), derived from DeepBank, and (c) HPSG syntactic dependencies derived from the Enju Treebank. 6 Submissions and Results From almost 40 teams who had registered for the task, twelve teams obtaine</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bohnet, B., &amp; Nivre, J. (2012). A transition-based system for joint part-of-speech tagging and labeled nonprojective dependency parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning (p. 1455 –1465). Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cinková</author>
</authors>
<title>From PropBank to EngValLex. Adapting the PropBank lexicon to the valency theory of the Functional Generative Description.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation.</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="15123" citStr="Cinková, 2006" startWordPosition="2375" endWordPosition="2376">− .5490 PSD .2636 .2963 − .5743 .5630 − Table 2: Pairwise F1 similarities, including punctuation (upper right diagonals) or not (lower left). Frame or sense distinctions are a new property in SDP 2015 and currently are only available for the English DM and PSD data. Table 1 reveals a stark difference in granularity: DM limits itself to argument structure distinctions that are grammaticized, e.g. causative vs. inchoative contrasts or differences in the arity or coarse semantic typing of argument frames; PSD, on the other hand, draws on the much richer sense inventory of the EngValLex database (Cinková, 2006). Accordingly, the two target representations represent quite different challenges for the predicate disambiguation sub-task of SDP 2015. Finally, in Table 2 we seek to quantify pairwise structural similarity between the three representations in terms of unlabeled dependency F1 (dubbed OF in Section 5 below). We provide four variants of this metric, (a) taking into account the directionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discardin</context>
</contexts>
<marker>Cinková, 2006</marker>
<rawString>Cinková, S. (2006). From PropBank to EngValLex. Adapting the PropBank lexicon to the valency theory of the Functional Generative Description. In Proceedings of the 5th International Conference on Language Resources and Evaluation. Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-C de Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (p. 449 – 454).</booktitle>
<location>Genoa, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>de Marneffe, M.-C., MacCartney, B., &amp; Manning, C. D. (2006). Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (p. 449 – 454). Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Du</author>
<author>F Zhang</author>
<author>W Sun</author>
<author>X Wan</author>
</authors>
<title>Peking: Profiling syntactic tree parsing techniques for semantic graph parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th international workshop on semantic evaluation (semeval</booktitle>
<contexts>
<context position="32793" citStr="Du et al. (2014)" startWordPosition="5241" endWordPosition="5244">ably because the additional dependency parser they used was trained on data from the target domain. 7 Overview of Approaches Table 5 shows a summary of the tracks in which each submitted system participated, and Table 6 shows an overview of approaches and additionally used resources. All the teams except In-House submitted results for cross-lingual data (Czech and Chinese). Teams except Lisbon also tackled with predicate disambiguation. Only Turku participated in the Gold track. The submitted teams explored a variety of approaches. Riga and Peking relied on the graph-to-tree transformation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12Please see the task web page at the address indicated above for full labeled and unlabeled scores. 922 Team Closed Open Cross-Lingual Predicate Disambiguation Gold In-House ✓ ✓ Lisbon ✓ ✓ ✓ Minsk ✓ ✓ ✓ Peking ✓ ✓ ✓ Riga ✓ ✓ ✓ Turku ✓ ✓ ✓ ✓ Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House grammar-based parsing (Miyao et al., 2014) ERG &amp; Enju Lisbon graph parsing with dual decomposition (Martins &amp; Almeida, 2014) companion Min</context>
<context position="34228" citStr="Du et al. (2014)" startWordPosition="5467" endWordPosition="5470">tion, Mate, C6.0, parser ensemble — Turku sequence labeling for argument detection for each predicate, SVM classifiers companion for top node recognition and sense prediction Table 6: Overview of approaches and additional resources used (if any). graphs are converted into tree structures, and wellestablished parsing methods for tree structures are applied to converted structures. In run-time, the tree parser is applied, and predicted trees are converted back into graph structures. Labels of tree edges encode additional information to recover original graph structures. This idea was applied in Du et al. (2014) and contributed to their best-performing system in the 2014 SDP task. In addition to applying the Mate parser to the treetransformed data of Du et al. (2014), Riga developed a high-precision but low-recall semantic parser. This method applies a decision tree classifier (C6.0) to edge detection. C6.0 learns patterns of semantic dependencies, which means it outputs highly reliable prediction when a learned pattern applies, while in most cases it cannot produce any predictions. These two types of parsers are finally combined by parser ensemble. They also applied C6.0 to frame (or sense) label pr</context>
<context position="35700" citStr="Du et al. (2014)" startWordPosition="5699" endWordPosition="5702">-tree transformation is the extraction of an essential tree-forming subset of edges from semantic dependency graphs, but it is not trivial to determine a reasonable subset. Therefore, the idea of weighted tree approximation is to define an edge score to quantify importance of each edge, and extract tree-forming edges that maximizes the sum of edge scores globally. After defining edge scores, treeforming edges with optimal scores can be extracted by applying decoding methods like maximum spanning tree and the Eisner algorithm. They applied this method as well as the previous method proposed in Du et al. (2014) with several variations on encoding edge labels, finally obtaining nine tree parsers. In the final submission, outputs from these parsers are combined by the parser ensemble technique. For predicate disambiguation, they independently applied a sequence labeling technique. Turku took a completely different approach. They consider each predicate separately, and apply sequence labeling for each predicate individually, to recognize arguments of the target predicate. That is, the task is reduced to assign each word an argument tag (e.g. ARG1) or a negative ‘pseudo-’label indicating it is not an ar</context>
</contexts>
<marker>Du, Zhang, Sun, Wan, 2014</marker>
<rawString>Du, Y., Zhang, F., Sun, W., &amp; Wan, X. (2014). Peking: Profiling syntactic tree parsing techniques for semantic graph parsing. In Proceedings of the 8th international workshop on semantic evaluation (semeval 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fares</author>
<author>S Oepen</author>
<author>Y Zhang</author>
</authors>
<title>Machine learning for high-quality tokenization. Replicating variable tokenization schemes.</title>
<date>2013</date>
<booktitle>In Computational linguistics and intelligent text processing (p. 231 – 244).</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="11222" citStr="Fares et al., 2013" startWordPosition="1721" endWordPosition="1724"> tokens) of in- and out-of-domain Czech data, respectively. Quantitative Comparison As a first attempt at contrasting our three target representations, Table 1 shows some high-level statistics of the graphs comprising the training and testing data.8 In terms of distinctions drawn in dependency labels (1), there are clear differences between the representations, with PSD appearing linguistically most fine6Additionally, some 500 sentences show tokenization mismatches, most owing to DeepBank correcting PTB idiosyncrasies like hG.m.b, H.i, hS.p, A.i, and hU.S., .i, and introducing a few new ones (Fares et al., 2013). 7In comparison to the SDP 2014 data, our DM graphs were extracted from a newer, improved release of DeepBank (Version 1.1), and its conversion to bi-lexical dependencies was moderately revised to provide more systematic analyses of contracted negated auxiliaries and comparatives. At the same time, the extraction of PSD graphs from the PCEDT t-trees was refined to include edges representing grammatical coreference, e.g. reentrancies introduced by control verbs. 8These statistics are obtained using the ‘official’ SDP toolkit. Our notions of singletons, roots, re-entrancies, and projectivity fo</context>
</contexts>
<marker>Fares, Oepen, Zhang, 2013</marker>
<rawString>Fares, M., Oepen, S., &amp; Zhang, Y. (2013). Machine learning for high-quality tokenization. Replicating variable tokenization schemes. In Computational linguistics and intelligent text processing (p. 231 – 244). Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<pages>15--28</pages>
<contexts>
<context position="6572" citStr="Flickinger, 2000" startWordPosition="997" endWordPosition="998"> even conjunction—often remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inference-based techniques. tivations for the SDP task series. Please see Oepen et al. (2014) and Miyao et al. (2014) for additional background. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation, dubbed DeepBank, of Sections 00–21 of the WSJ Corpus and of selected parts of the Brown Corpus with syntacticosemantic analyses of the LinGO English Resource Grammar (Flickinger, 2000; Flickinger et al., 2012). For this target representation, top nodes designate the highest-scoping (non-quantifier) predicate in the graph, e.g. the (scopal) adverb almost in Figure 1.3 PAS: Enju Predicate–Argument Structures The Enju Treebank and parser4 are derived from the automatic HPSG-style annotation of the PTB (Miyao, 2006). Our PAS semantic dependency graphs are extracted from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; 3However, non-scopal adverbs act as mere intersective modifiers, e.g. i</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Flickinger, D. (2000). On building a more efficient grammar by exploiting types. Natural Language Engineering, 6 (1), 15 – 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
<author>Y Zhang</author>
<author>V Kordoni</author>
</authors>
<title>DeepBank. A dynamically annotated treebank of the Wall Street Journal.</title>
<date>2012</date>
<booktitle>In Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories (p. 85 – 96).</booktitle>
<location>Lisbon, Portugal: Edições Colibri.</location>
<contexts>
<context position="6598" citStr="Flickinger et al., 2012" startWordPosition="999" endWordPosition="1002">often remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inference-based techniques. tivations for the SDP task series. Please see Oepen et al. (2014) and Miyao et al. (2014) for additional background. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation, dubbed DeepBank, of Sections 00–21 of the WSJ Corpus and of selected parts of the Brown Corpus with syntacticosemantic analyses of the LinGO English Resource Grammar (Flickinger, 2000; Flickinger et al., 2012). For this target representation, top nodes designate the highest-scoping (non-quantifier) predicate in the graph, e.g. the (scopal) adverb almost in Figure 1.3 PAS: Enju Predicate–Argument Structures The Enju Treebank and parser4 are derived from the automatic HPSG-style annotation of the PTB (Miyao, 2006). Our PAS semantic dependency graphs are extracted from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; 3However, non-scopal adverbs act as mere intersective modifiers, e.g. in a structure like Abrams </context>
</contexts>
<marker>Flickinger, Zhang, Kordoni, 2012</marker>
<rawString>Flickinger, D., Zhang, Y., &amp; Kordoni, V. (2012). DeepBank. A dynamically annotated treebank of the Wall Street Journal. In Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories (p. 85 – 96). Lisbon, Portugal: Edições Colibri.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kuˇcera</author>
</authors>
<title>Frequency analysis of English usage. Lexicon and grammar.</title>
<date>1982</date>
<location>New York, USA: Houghton Mifflin.</location>
<marker>Francis, Kuˇcera, 1982</marker>
<rawString>Francis, W. N., &amp; Kuˇcera, H. (1982). Frequency analysis of English usage. Lexicon and grammar. New York, USA: Houghton Mifflin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<pages>245--288</pages>
<contexts>
<context position="3352" citStr="Gildea &amp; Jurafsky, 2002" startWordPosition="489" endWordPosition="492">ably is dependent on the determiner (the quantificational locus), the modifier similar, and the predicate apply. Conversely, the predicative copula, infinitival to, and the vacuous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea &amp; Jurafsky, 2002).2 However, we require parsers to identify ‘full2In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics A similar technique is almost impossible to apply to other crops , su</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D., &amp; Jurafsky, D. (2002). Automatic labeling of semantic roles. Computational Linguistics, 28, 245 – 288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Utterer’s meaning, sentence-meaning, and word-meaning.</title>
<date>1968</date>
<journal>Foundations of Language,</journal>
<volume>4</volume>
<issue>3</issue>
<pages>225--242</pages>
<contexts>
<context position="21617" citStr="Grice, 1968" startWordPosition="3404" endWordPosition="3405">nd indeed there has been much previous work seeking to identify and annotate the relations that hold between members of a nominal compound (see Nakov, 2013, for a recent overview). To what degree the bracketing and role disambiguation in this example are determined by the linguistic signal (rather than by context and world knowledge, say) can be debated, and thus the observed differences among our representations in this example relate to the classic contrast between ‘sentence’ (or ‘conventional’) meaning, on the one hand, and ‘speaker’ (or ‘occasion’) meaning, on the other hand (Quine, 1960; Grice, 1968; Bender et al., 2015). In turn, we acknowledge different plausible points of view about which level of semantic representation should be the target representation for data-driven parsing (i.e. structural analysis guided by the grammatical system), and which refinements like the above could be construed as part of a subsequent task of interpretation. 5 Task Setup English training data for the task, providing all columns in the file format sketched in Section 3 above, together with a first version of the SDP toolkit—including graph input, basic statistics, and scoring—were released to candidate</context>
</contexts>
<marker>Grice, 1968</marker>
<rawString>Grice, H. P. (1968). Utterer’s meaning, sentence-meaning, and word-meaning. Foundations of Language, 4(3), 225–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>E Hajiˇcová</author>
<author>J Panevová</author>
<author>P Sgall</author>
<author>O Bojar</author>
<author>S Cinková</author>
</authors>
<title>Announcing Prague Czech-English Dependency Treebank 2.0.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (p. 3153 – 3160).</booktitle>
<location>Istanbul, Turkey.</location>
<marker>Hajiˇc, Hajiˇcová, Panevová, Sgall, Bojar, Cinková, 2012</marker>
<rawString>Hajiˇc, J., Hajiˇcová, E., Panevová, J., Sgall, P., Bojar, O., Cinková, S., ... Žabokrtský, Z. (2012). Announcing Prague Czech-English Dependency Treebank 2.0. In Proceedings of the 8th International Conference on Language Resources and Evaluation (p. 3153 – 3160). Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ivanova</author>
<author>S Oepen</author>
<author>L Øvrelid</author>
<author>D Flickinger</author>
</authors>
<title>Who did what to whom? A contrastive study of syntacto-semantic dependencies.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth Linguistic Annotation Workshop (p. 2 –11). Jeju,</booktitle>
<location>Republic of</location>
<contexts>
<context position="16132" citStr="Ivanova et al. (2012)" startWordPosition="2534" endWordPosition="2537">nality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discarding 918 punctuation. While relaxing the comparison to ignore edge directionality also increases similarity scores for this pair, the effect is much more pronounced when comparing either to PSD. This suggests that directionality of semantic dependencies is a major source of diversion between DM and PAS on the one hand, and PSD on the other hand. Linguistic Comparison Among other aspects, Ivanova et al. (2012) categorize a range of syntactic and semantic dependency annotation schemes according to the role that functional elements take. In Figure 1 and the discussion of Table 1 above, we already observed that PAS differs from the other representations in integrating into the graph auxiliaries, the infinitival marker, the case-marking preposition introducing the argument of apply (to), and most punctuation marks;9 while these (and other functional elements, e.g. complementizers) are analyzed as semantically vacuous in DM and PSD, they function as predicates in PAS, though do not always serve as ‘loca</context>
<context position="29192" citStr="Ivanova et al. (2012)" startWordPosition="4641" endWordPosition="4644">y-touse ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in the form of Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). Finally, to more directly gauge the the contributions of syntactic structure on the semantic dependency parsing problem, an idealized gold track was introduced in SDP 2015. For this track, gold-standard syntactic companion files were provided in a varity of formats, viz. (a) Stanford Basic dependencies, derived from the PTB, (b) HPSG syntactic dependencies in the form called DM by Ivanova et al. (2012), derived from DeepBank, and (c) HPSG syntactic dependencies derived from the Enju Treebank. 6 Submissions and Results From almost 40 teams who had registered for the task, twelve teams obtained the test data, and test runs were submitted for six systems—including one ‘inofficial’ submission by a sub-set of the task organizers (Miyao et al., 2014). Each team submitted up to two test runs per track. In total, there were seven runs submitted to the English closed track, five to the open track and two to the gold track; seven runs were submitted to the Chinese closed track, two to the open track;</context>
</contexts>
<marker>Ivanova, Oepen, Øvrelid, Flickinger, 2012</marker>
<rawString>Ivanova, A., Oepen, S., Øvrelid, L., &amp; Flickinger, D. (2012). Who did what to whom? A contrastive study of syntacto-semantic dependencies. In Proceedings of the Sixth Linguistic Annotation Workshop (p. 2 –11). Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>Y W Wong</author>
</authors>
<title>Semantic parsing. The task, the state of the art and the future.</title>
<date>2010</date>
<booktitle>In Tutorial abstracts of the 20th Meeting of the Association for Computational Linguistics (p. 6).</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="5136" citStr="Kate and Wong (2010)" startWordPosition="782" endWordPosition="785">mpossible to apply to other crops , such as cotton , soybeans and rice . (b) Enju Predicate–Argument Structures (PAS). A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice . ARG2 ARG1 ARG1 ARG1 ARG1 ARG2 ARG1 ARG1 RSTR top PAT-arg PAT-arg EXT ADDR-arg ACT-arg CONJ.m ADDR-arg RSTR ADDR-arg APPS.m APPS.m CONJ.m CONJ.m ADDR-arg ARG1 ARG2 ARG1 ARG2 sentence’ semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Finally, a third related area of much interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to much work in this tradition, our SDP target representations aim to be task- and domain-independent. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure 1), showing what are called the DM, PAS, and PSD semantic dependencies, there are contentful differences among these annotations, and there is of course not one obvious (o</context>
</contexts>
<marker>Kate, Wong, 2010</marker>
<rawString>Kate, R. J., &amp; Wong, Y. W. (2010). Semantic parsing. The task, the state of the art and the future. In Tutorial abstracts of the 20th Meeting of the Association for Computational Linguistics (p. 6). Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpora of English. The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<pages>313--330</pages>
<contexts>
<context position="2398" citStr="Marcus et al., 1993" startWordPosition="346" endWordPosition="349">Broad-Coverage Semantic Dependency Parsing (SDP 2015),1 seeks to stimulate the parsing community to move towards 1See http://alt.qcri.org/semeval2015/ task18/ for further technical details, information on how to obtain the data, and official results. more general graph processing, to thus enable a more direct analysis of Who did What to Whom? Extending the very similar predecessor task SDP 2014 (Oepen et al., 2014), we make use of three distinct, parallel semantic annotations over the same common texts, viz. the venerable Wall Street Journal (WSJ) and Brown segments of the Penn Treebank (PTB; Marcus et al., 1993) for English, as well as comparable resources for Chinese and Czech. Figure 1 below shows example target representations, bi-lexical semantic dependency graphs in all cases, for the WSJ sentence: (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice. Semantically, technique arguably is dependent on the determiner (the quantificational locus), the modifier similar, and the predicate apply. Conversely, the predicative copula, infinitival to, and the vacuous preposition marking the deep object of apply can be argued to not have a semantic contrib</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M., Santorini, B., &amp; Marcinkiewicz, M. A. (1993). Building a large annotated corpora of English. The Penn Treebank. Computational Linguistics, 19, 313 – 330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>M S C Almeida</author>
</authors>
<title>Priberam: A turbo semantic parser with second order features. In</title>
<date>2014</date>
<booktitle>In proceedings of the 8th international workshop on semantic evaluation (semeval</booktitle>
<contexts>
<context position="33379" citStr="Martins &amp; Almeida, 2014" startWordPosition="5340" endWordPosition="5343">ree transformation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12Please see the task web page at the address indicated above for full labeled and unlabeled scores. 922 Team Closed Open Cross-Lingual Predicate Disambiguation Gold In-House ✓ ✓ Lisbon ✓ ✓ ✓ Minsk ✓ ✓ ✓ Peking ✓ ✓ ✓ Riga ✓ ✓ ✓ Turku ✓ ✓ ✓ ✓ Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House grammar-based parsing (Miyao et al., 2014) ERG &amp; Enju Lisbon graph parsing with dual decomposition (Martins &amp; Almeida, 2014) companion Minsk transition-based dependency graph parsing in the spirit of Titov et al. (2009) — Peking (Du et al., 2014) extended with weighted tree approximation, parser ensemble — Riga (Du et al., 2014)’s graph-to-tree transformation, Mate, C6.0, parser ensemble — Turku sequence labeling for argument detection for each predicate, SVM classifiers companion for top node recognition and sense prediction Table 6: Overview of approaches and additional resources used (if any). graphs are converted into tree structures, and wellestablished parsing methods for tree structures are applied to conver</context>
</contexts>
<marker>Martins, Almeida, 2014</marker>
<rawString>Martins, A. F. T., &amp; Almeida, M. S. C. (2014). Priberam: A turbo semantic parser with second order features. In In proceedings of the 8th international workshop on semantic evaluation (semeval 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mel’ˇcuk</author>
</authors>
<title>Dependency syntax. Theory and practice.</title>
<date>1988</date>
<publisher>SUNY Press.</publisher>
<location>Albany, NY, USA:</location>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Mel’ˇcuk, I. (1988). Dependency syntax. Theory and practice. Albany, NY, USA: SUNY Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>Annotating noun argument structure for NomBank.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (p. 803 – 806).</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="3547" citStr="Meyers et al., 2004" startWordPosition="520" endWordPosition="523">g the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea &amp; Jurafsky, 2002).2 However, we require parsers to identify ‘full2In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-i p:e-u-i n:x n:x _ n:x _ (a) DELPH-IN Minimal Recursion Semantics–derived bi-lexical</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Meyers, A., Reeves, R., Macleod, C., Szekely, R., Zielinska, V., Young, B., &amp; Grishman, R. (2004). Annotating noun argument structure for NomBank. In Proceedings of the 4th International Conference on Language Resources and Evaluation (p. 803 – 806). Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
</authors>
<title>From linguistic theory to syntactic analysis. Corpus-oriented grammar development and feature forest model.</title>
<date>2006</date>
<institution>University of Tokyo,</institution>
<location>Tokyo, Japan.</location>
<note>Unpublished doctoral dissertation,</note>
<contexts>
<context position="6906" citStr="Miyao, 2006" startWordPosition="1045" endWordPosition="1046"> Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation, dubbed DeepBank, of Sections 00–21 of the WSJ Corpus and of selected parts of the Brown Corpus with syntacticosemantic analyses of the LinGO English Resource Grammar (Flickinger, 2000; Flickinger et al., 2012). For this target representation, top nodes designate the highest-scoping (non-quantifier) predicate in the graph, e.g. the (scopal) adverb almost in Figure 1.3 PAS: Enju Predicate–Argument Structures The Enju Treebank and parser4 are derived from the automatic HPSG-style annotation of the PTB (Miyao, 2006). Our PAS semantic dependency graphs are extracted from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; 3However, non-scopal adverbs act as mere intersective modifiers, e.g. in a structure like Abrams sang loudly, the adverb is a predicate in DM, but the main verb nevertheless is the top node. 4See http://kmcs.nii.ac.jp/enju/. 916 Xue et al., 2005). Top nodes in this representation denote semantic heads. PSD: Prague Semantic Dependencies The Prague Czech-English Dependency Treebank (PCEDT; Hajiˇc et al.,</context>
</contexts>
<marker>Miyao, 2006</marker>
<rawString>Miyao, Y. (2006). From linguistic theory to syntactic analysis. Corpus-oriented grammar development and feature forest model. Unpublished doctoral dissertation, University of Tokyo, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>S Oepen</author>
<author>D Zeman</author>
</authors>
<title>In-House. An ensemble of pre-existing off-the-shelf parsers.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (p. 63 –72).</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="6242" citStr="Miyao et al. (2014)" startWordPosition="947" endWordPosition="950">dependencies, there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. Advancing in-depth comparison of representations and underlying design decisions, in fact, is among the moand other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—often remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inference-based techniques. tivations for the SDP task series. Please see Oepen et al. (2014) and Miyao et al. (2014) for additional background. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation, dubbed DeepBank, of Sections 00–21 of the WSJ Corpus and of selected parts of the Brown Corpus with syntacticosemantic analyses of the LinGO English Resource Grammar (Flickinger, 2000; Flickinger et al., 2012). For this target representation, top nodes designate the highest-scoping (non-quantifier) predicate in the graph, e.g. the (scopal) adverb almost in Figure 1.3 PAS: Enju Predicate–Argument Structures The Enju Treebank and parser4 are derived f</context>
<context position="20199" citStr="Miyao et al. (2014)" startWordPosition="3177" endWordPosition="3180">isambiguates the bracketing but, by design, merely assigns an underspecified, construction-specific dependency type; its compound dependency, then, is to be interpreted as the most general type of dependency that can hold between the elements of this construction (i.e. to a first approximation either an argument role or a relation parallel to a preposition, as in the above paraphrase). The DM and PSD annotations of this specific example happen to diverge in their bracketing decisions, where the DM analysis corresponds to [...I investments in stock for employees, i.e. grouping 10As detailed by Miyao et al. (2014), individual conjuncts can be (and usually are) arguments of other predicates, whereas the topmost conjunction only has incoming edges in nested coordinate structures. Similarly, a ‘shared’ modifier of the coordinate structure as a whole would take as its argument the local top node of the coordination in DM or PAS (i.e. the first conjunct or final conjunction, respectively), whereas it would depend as an argument on all conjuncts in PSD. 919 compound compound compound ARG1 ARG1 ACT ARG1 PAT REG employee stock investment plans employee stock investment plans employee stock investment plans Fig</context>
<context position="29541" citStr="Miyao et al., 2014" startWordPosition="4698" endWordPosition="4701">old track was introduced in SDP 2015. For this track, gold-standard syntactic companion files were provided in a varity of formats, viz. (a) Stanford Basic dependencies, derived from the PTB, (b) HPSG syntactic dependencies in the form called DM by Ivanova et al. (2012), derived from DeepBank, and (c) HPSG syntactic dependencies derived from the Enju Treebank. 6 Submissions and Results From almost 40 teams who had registered for the task, twelve teams obtained the test data, and test runs were submitted for six systems—including one ‘inofficial’ submission by a sub-set of the task organizers (Miyao et al., 2014). Each team submitted up to two test runs per track. In total, there were seven runs submitted to the English closed track, five to the open track and two to the gold track; seven runs were submitted to the Chinese closed track, two to the open track; and five runs submitted to the Czech closed track, two to the open track. One team submitted only to the open and gold tracks, three teams submitted only to the closed track, one team submitted to open and closed tracks in English but only to the closed tracks in the other two languages. The main results are summarized and ranked in Tables 3 and </context>
<context position="33297" citStr="Miyao et al., 2014" startWordPosition="5327" endWordPosition="5330">ms explored a variety of approaches. Riga and Peking relied on the graph-to-tree transformation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12Please see the task web page at the address indicated above for full labeled and unlabeled scores. 922 Team Closed Open Cross-Lingual Predicate Disambiguation Gold In-House ✓ ✓ Lisbon ✓ ✓ ✓ Minsk ✓ ✓ ✓ Peking ✓ ✓ ✓ Riga ✓ ✓ ✓ Turku ✓ ✓ ✓ ✓ Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House grammar-based parsing (Miyao et al., 2014) ERG &amp; Enju Lisbon graph parsing with dual decomposition (Martins &amp; Almeida, 2014) companion Minsk transition-based dependency graph parsing in the spirit of Titov et al. (2009) — Peking (Du et al., 2014) extended with weighted tree approximation, parser ensemble — Riga (Du et al., 2014)’s graph-to-tree transformation, Mate, C6.0, parser ensemble — Turku sequence labeling for argument detection for each predicate, SVM classifiers companion for top node recognition and sense prediction Table 6: Overview of approaches and additional resources used (if any). graphs are converted into tree structu</context>
</contexts>
<marker>Miyao, Oepen, Zeman, 2014</marker>
<rawString>Miyao, Y., Oepen, S., &amp; Zeman, D. (2014). In-House. An ensemble of pre-existing off-the-shelf parsers. In Proceedings of the 8th International Workshop on Semantic Evaluation (p. 63 –72). Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
</authors>
<title>On the interpretation of noun compounds: Syntax, semantics, and entailment.</title>
<date>2013</date>
<journal>Natural Language Engineering,</journal>
<volume>19</volume>
<issue>3</issue>
<pages>291--330</pages>
<contexts>
<context position="21161" citStr="Nakov, 2013" startWordPosition="3333" endWordPosition="3334">conjunction, respectively), whereas it would depend as an argument on all conjuncts in PSD. 919 compound compound compound ARG1 ARG1 ACT ARG1 PAT REG employee stock investment plans employee stock investment plans employee stock investment plans Figure 2: Analysis of nominal compounding in DM, PAS, and PSD, respectively. the concept employee stock (in contrast to ‘common stock’). Without context and expert knowledge, these decisions are hard to call, and indeed there has been much previous work seeking to identify and annotate the relations that hold between members of a nominal compound (see Nakov, 2013, for a recent overview). To what degree the bracketing and role disambiguation in this example are determined by the linguistic signal (rather than by context and world knowledge, say) can be debated, and thus the observed differences among our representations in this example relate to the classic contrast between ‘sentence’ (or ‘conventional’) meaning, on the one hand, and ‘speaker’ (or ‘occasion’) meaning, on the other hand (Quine, 1960; Grice, 1968; Bender et al., 2015). In turn, we acknowledge different plausible points of view about which level of semantic representation should be the ta</context>
</contexts>
<marker>Nakov, 2013</marker>
<rawString>Nakov, P. (2013). On the interpretation of noun compounds: Syntax, semantics, and entailment. Natural Language Engineering, 19(3), 291–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>M Kuhlmann</author>
<author>Y Miyao</author>
<author>D Zeman</author>
<author>D Flickinger</author>
<author>J Hajiˇc</author>
</authors>
<title>Task 8. Broad-coverage semantic dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation.</booktitle>
<location>Dublin, Ireland.</location>
<note>SemEval</note>
<marker>Oepen, Kuhlmann, Miyao, Zeman, Flickinger, Hajiˇc, 2014</marker>
<rawString>Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D., Flickinger, D., Hajiˇc, J., ... Zhang, Y. (2014). SemEval 2014 Task 8. Broad-coverage semantic dependency parsing. In Proceedings of the 8th International Workshop on Semantic Evaluation. Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank. A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>71--106</pages>
<contexts>
<context position="3525" citStr="Palmer et al., 2005" startWordPosition="516" endWordPosition="519">us preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea &amp; Jurafsky, 2002).2 However, we require parsers to identify ‘full2In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-i p:e-u-i n:x n:x _ n:x _ (a) DELPH-IN Minimal Recursion Semant</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., Gildea, D., &amp; Kingsbury, P. (2005). The Proposition Bank. A corpus annotated with semantic roles. Computational Linguistics, 31(1), 71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W V O Quine</author>
</authors>
<title>Word and object.</title>
<date>1960</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA, USA:</location>
<contexts>
<context position="21604" citStr="Quine, 1960" startWordPosition="3402" endWordPosition="3403">rd to call, and indeed there has been much previous work seeking to identify and annotate the relations that hold between members of a nominal compound (see Nakov, 2013, for a recent overview). To what degree the bracketing and role disambiguation in this example are determined by the linguistic signal (rather than by context and world knowledge, say) can be debated, and thus the observed differences among our representations in this example relate to the classic contrast between ‘sentence’ (or ‘conventional’) meaning, on the one hand, and ‘speaker’ (or ‘occasion’) meaning, on the other hand (Quine, 1960; Grice, 1968; Bender et al., 2015). In turn, we acknowledge different plausible points of view about which level of semantic representation should be the target representation for data-driven parsing (i.e. structural analysis guided by the grammatical system), and which refinements like the above could be construed as part of a subsequent task of interpretation. 5 Task Setup English training data for the task, providing all columns in the file format sketched in Section 3 above, together with a first version of the SDP toolkit—including graph input, basic statistics, and scoring—were released</context>
</contexts>
<marker>Quine, 1960</marker>
<rawString>Quine, W. V. O. (1960). Word and object. Cambridge, MA, USA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
<author>P Merlo</author>
<author>G Musillo</author>
</authors>
<title>Online graph planarisation for synchronous parsing of semantic and syntactic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artificial Intelligence.</booktitle>
<location>Pasadena, CA, USA.</location>
<contexts>
<context position="33474" citStr="Titov et al. (2009)" startWordPosition="5354" endWordPosition="5357">nto tree structures. Training data of semantic dependency 12Please see the task web page at the address indicated above for full labeled and unlabeled scores. 922 Team Closed Open Cross-Lingual Predicate Disambiguation Gold In-House ✓ ✓ Lisbon ✓ ✓ ✓ Minsk ✓ ✓ ✓ Peking ✓ ✓ ✓ Riga ✓ ✓ ✓ Turku ✓ ✓ ✓ ✓ Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House grammar-based parsing (Miyao et al., 2014) ERG &amp; Enju Lisbon graph parsing with dual decomposition (Martins &amp; Almeida, 2014) companion Minsk transition-based dependency graph parsing in the spirit of Titov et al. (2009) — Peking (Du et al., 2014) extended with weighted tree approximation, parser ensemble — Riga (Du et al., 2014)’s graph-to-tree transformation, Mate, C6.0, parser ensemble — Turku sequence labeling for argument detection for each predicate, SVM classifiers companion for top node recognition and sense prediction Table 6: Overview of approaches and additional resources used (if any). graphs are converted into tree structures, and wellestablished parsing methods for tree structures are applied to converted structures. In run-time, the tree parser is applied, and predicted trees are converted back</context>
</contexts>
<marker>Titov, Henderson, Merlo, Musillo, 2009</marker>
<rawString>Titov, I., Henderson, J., Merlo, P., &amp; Musillo, G. (2009). Online graph planarisation for synchronous parsing of semantic and syntactic dependencies. In Proceedings of the 21st International Joint Conference on Artificial Intelligence. Pasadena, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vadas</author>
<author>J Curran</author>
</authors>
<title>Adding Noun Phrase Structure to the Penn Treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Meeting of the Association for Computational Linguistics (p.</booktitle>
<pages>240--247</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="19038" citStr="Vadas and Curran (2007)" startWordPosition="2990" endWordPosition="2993">ersely, in PSD the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.10 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase– internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PSD, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PSD annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely assigns</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>Vadas, D., &amp; Curran, J. (2007). Adding Noun Phrase Structure to the Penn Treebank. In Proceedings of the 45th Meeting of the Association for Computational Linguistics (p. 240–247). Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F-D Chiou</author>
<author>M Palmer</author>
</authors>
<title>The Penn Chinese TreeBank. Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<pages>207--238</pages>
<contexts>
<context position="7347" citStr="Xue et al., 2005" startWordPosition="1113" endWordPosition="1116"> adverb almost in Figure 1.3 PAS: Enju Predicate–Argument Structures The Enju Treebank and parser4 are derived from the automatic HPSG-style annotation of the PTB (Miyao, 2006). Our PAS semantic dependency graphs are extracted from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; 3However, non-scopal adverbs act as mere intersective modifiers, e.g. in a structure like Abrams sang loudly, the adverb is a predicate in DM, but the main verb nevertheless is the top node. 4See http://kmcs.nii.ac.jp/enju/. 916 Xue et al., 2005). Top nodes in this representation denote semantic heads. PSD: Prague Semantic Dependencies The Prague Czech-English Dependency Treebank (PCEDT; Hajiˇc et al., 2012)5 is a set of parallel dependency trees over the WSJ texts from the PTB, and their Czech translations. Our PSD bi-lexical dependencies have been extracted from what is called the tectogrammatical annotation layer (t-trees). Top nodes are derived from t-tree roots; i.e. they mostly correspond to main verbs. In case of coordinate clauses, there are multiple top nodes per sentence. 3 Data Format The SDP target representations can be c</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Xue, N., Xia, F., Chiou, F.-D., &amp; Palmer, M. (2005). The Penn Chinese TreeBank. Phrase structure annotation of a large corpus. Natural Language Engineering, 11, 207–238.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>