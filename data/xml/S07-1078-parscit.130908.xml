<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.104706">
<title confidence="0.9951985">
UBC-ZAS: A k-NN based Multiclassifier System to perform WSD in a
Reduced Dimensional Vector Space
</title>
<author confidence="0.993592">
Ana Zelaia, Olatz Arregi and Basilio Sierra
</author>
<affiliation confidence="0.9927635">
Computer Science Faculty
University of the Basque Country
</affiliation>
<email confidence="0.99535">
ana.zelaia@ehu.es
</email>
<sectionHeader confidence="0.995601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999881294117647">
In this article a multiclassifier approach for
word sense disambiguation (WSD) prob-
lems is presented, where a set of k-NN
classifiers is used to predict the category
(sense) of each word. In order to combine
the predictions generated by the multiclas-
sifier, Bayesian voting is applied. Through
all the classification process, a reduced di-
mensional vector representation obtained by
Singular Value Decomposition (SVD) is
used. Each word is considered an indepen-
dent classification problem, and so differ-
ent parameter setting, selected after a tun-
ing phase, is applied to each word. The ap-
proach has been applied to the lexical sam-
ple WSD subtask of SemEval 2007 (task
17).
</bodyText>
<sectionHeader confidence="0.99906" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998731763157895">
Word Sense Disambiguation (WSD) is an impor-
tant component in many information organization
and management tasks. Both, word representation
and classification method are crucial steps in the
word sense disambiguation process. In this article
both issues are considered. On the one hand, Latent
Semantic Indexing (LSI) (Deerwester et al., 1990),
which is a variant of the vector space model (VSM)
(Salton and McGill, 1983), is used in order to ob-
tain the vector representation of the corresponding
word. This technique compresses vectors represent-
ing word related contexts into vectors of a lower-
dimensional space. LSI, which is based on Singu-
lar Value Decomposition (SVD) (Berry and Browne,
1999) of matrices, has shown to have the ability
to extract the relations among features representing
words by means of their context of use, and has been
successfully applied to Information Retrieval tasks.
On the other hand, a multiclassifier (Ho et al.,
1994) which uses different training databases is con-
structed. These databases are obtained from the
original training set by random subsampling. The
implementation of this approach is made by a model
inspired in bagging (Breiman, 1996), and the k-NN
classification algorithm (Dasarathy, 1991) is used to
make the sense predictions for testing words.
Our group (UBC-ZAS) has participated in the lex-
ical sample subtask of SemEval-2007 for task 17,
which consists on 100 different words for which a
training and testing database have been provided.
The aim of this article is to give a brief descrip-
tion of our approach to deal with the WSD task and
to show the results obtained. In Section 2, our ap-
proach is presented. In Section 3, the experimen-
tal setup is introduced. The experimental results are
presented and discussed in Section 4, and finally,
Section 5 contains some conclusions and comments
on future work.
</bodyText>
<sectionHeader confidence="0.946055" genericHeader="method">
2 Proposed Approach
</sectionHeader>
<bodyText confidence="0.999783625">
In this article a multiclassifier based WSD system
which classifies word senses represented in a re-
duced dimensional vector space is proposed.
In Figure 1 an illustration of the experiment per-
formed for each one of the 100 words can be seen.
First, vectors in the VSM are projected to the re-
duced space by using SVD. Next, random subsam-
pling is applied to the training database TD to obtain
</bodyText>
<page confidence="0.979028">
358
</page>
<bodyText confidence="0.956591">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 358–361,
Prague, June 2007. c�2007 Association for Computational Linguistics
different training databases TDi. Afterwards the k-
NN classifier is applied for each TDi to make sense
label predictions. Finally, Bayesian voting scheme
is used to combine predictions, and cj will be the
final sense label prediction for testing word q.
</bodyText>
<figureCaption confidence="0.999818">
Figure 1: Proposed approach for WSD task
</figureCaption>
<bodyText confidence="0.99834975">
In the rest of this section, the preprocessing ap-
plied, the SVD dimensionality reduction technique,
the k-NN algorithm and the combination of classi-
fiers used are briefly reviewed.
</bodyText>
<subsectionHeader confidence="0.98954">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999966545454545">
In order to obtain the vector representation for each
of the word contexts (documents, cases) given by the
organizers of the SemEval-2007 task, we used the
features extracted by the UBC-ALM participating
group (Agirre and Lopez de Lacalle, 2007). These
features are local collocations (bigrams and trigrams
formed with the words around the target), syn-
tactic dependencies (object, subject, noun-modifier,
preposition, and sibling) and Bag-of-words features
(basically lemmas of the content words in the whole
context, and in a ±4-word window).
</bodyText>
<subsectionHeader confidence="0.978438">
2.2 The SVD Dimensionality Reduction
Technique
</subsectionHeader>
<bodyText confidence="0.989342409090909">
The classical Vector Space Model (VSM) has been
successfully employed to represent documents in
text categorization tasks. The newer method of
Latent Semantic Indexing (LSI) 1 (Deerwester et
&apos;http://lsi.research.telcordia.com,
http://www.cs.utk.edu/∼lsi
al., 1990) is a variant of the VSM in which docu-
ments are represented in a lower dimensional space
created from the input training dataset. The SVD
technique used by LSI consists in factoring term-
document matrix M into the product of three ma-
trices, M = UEVT where E is a diagonal matrix of
singular values, and U and V are orthogonal matri-
ces of singular vectors (term and document vectors,
respectively).
For classification purposes (Dumais, 2004), the
training and testing documents are projected to the
reduced dimensional space, qp = qT UpE�1
p , by us-
ing p singular values and the cosine is usually calcu-
lated to measure the similarity between training and
testing document vectors.
</bodyText>
<subsectionHeader confidence="0.997412">
2.3 The k-NN classification algorithm
</subsectionHeader>
<bodyText confidence="0.999829714285714">
k-NN is a distance based classification approach.
According to this approach, given an arbitrary test-
ing case, the k-NN classifier ranks its nearest neigh-
bors among the training word senses, and uses the
sense of the k top-ranking neighbors to predict the
corresponding to the word which is being analyzed
(Dasarathy, 1991).
</bodyText>
<subsectionHeader confidence="0.999913">
2.4 Combination of classifiers
</subsectionHeader>
<bodyText confidence="0.993739173913044">
The combination of multiple classifiers has been in-
tensively studied with the aim of improving the ac-
curacy of individual components (Ho et al., 1994).
A widely used technique to implement this approach
is bagging (Breiman, 1996), where a set of training
databases TDi is generated by selecting n training
cases drawn randomly with replacement from the
original training database TD of n cases. When a
set of n1 &lt; n training cases is chosen from the orig-
inal training collection, the bagging is said to be ap-
plied by random subsampling. In fact, this is the
approach used in our work and the n1 parameter has
been selected via tuning.
According to the random subsampling, given a
testing case q, the classifier will make a label predic-
tion ci based on each one of the training databases
TDi. One way to combine the predictions is by
Bayesian voting (Dietterich, 1998), where a con-
fidence value cvi is calculated for each training
cj
database TDi and sense cj to be predicted. These
confidence values have been calculated based on the
training collection. Confidence values are summed
</bodyText>
<figure confidence="0.999679106060606">
Random
Subsampling
R p
Rm Rm
d2
1
d
d7
d509
dn
TD1
k−NN k−NN
c1 c2 ci cj
dn
d7
..
qp
d135
.
d6 d5
d1 d2
d6 d5
d1
Train
...
d34
d23
dn
d2
d3
d4
d3
d4
d61
R p R p R p
TD
M
Mp = UpEpPT
qp
SVD
VSM
Bayesian voting
d256
d98
d638 d848
d50
d778
TD2
q
VSM
qp
d9
q1 q2
qp = qT UpE�1
p
d4612
k−NN
Test
q
...
d33
qn’
d2787
d1989
d55
TDi
</figure>
<page confidence="0.996144">
359
</page>
<bodyText confidence="0.999720666666667">
by sense; the sense cj that gets the highest value is
finally proposed as a prediction for the testing exam-
ples.
</bodyText>
<sectionHeader confidence="0.997927" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9999606">
In the approach proposed in this article there are
some decisions that need to be taken, because it is
not clear (1) how many examples should be selected
from the TD of each word in order to create each one
of the TDZ; (2) which is the appropriate dimension
to be used in order to represent word related con-
texts (cases) for each word database; (3) which is
the appropriate number of TDZ that should be cre-
ated (number of classifiers to be used) and (4) which
is the appropriate number of neighbors to be consid-
ered by the k-NN algorithm.
Therefore, a parameter tuning phase was carried
out in order to fix the parameters. We decided to
adjust them for each word independently.
In the following, the parameters are introduced
and the tuning process carried out is explained. For
two of the parameters (the number of classifiers and
the number of neigbors for k-NN), the tuning phase
was performed based on our previous experiments
on document categorization tasks.
</bodyText>
<subsectionHeader confidence="0.998041">
3.1 The size of each TDZ
</subsectionHeader>
<bodyText confidence="0.984094696969697">
As it was mentioned, the multiclassifier is imple-
mented by random subsampling, where a set of n1 &lt;
n vectors is chosen from the original training collec-
tion of n examples for a given word (n is a differ-
ent value for each one of the 100 words). Conse-
quently, the size of each TDZ will vary depending
on the value of n1. The selection of different num-
bers of cases was experimented for each word in two
different ways:
a) according to the following equation:
(2 + LtZj D, j = 1,... ,10
where tZ is the total number of training cases
in the sense cZ and s is the total number of
senses for the given word. By dividing param-
eter tZ by j, the number of cases selected from
each sense preserves the proportion of cases per
sense in the original one. However, it has to be
taken into account that some of the senses have
a very low number of cases assigned to them.
By summing 2, at least 2 cases will be selected
from each sense. In order to decide the optimal
value for parameter j, the classification experi-
ment was carried out varying j from 1 to 10 for
each word.
b) selecting a fixed number of cases for each of
the senses which appeared for the word in the
training database. Again, in the tuning phase,
different numbers of cases (from 1 to 10) have
been used for each of the 100 words in order to
select a value for each of the words.
We optimized the size of each TDZ for each word by
selecting the number of cases sometimes by proce-
dure a) and sometimes by b).
</bodyText>
<subsectionHeader confidence="0.984264">
3.2 The dimension of the reduced Vector Space
Model
</subsectionHeader>
<bodyText confidence="0.99999275">
Taking into account the wide differences among the
training case numbers for different words, we de-
cided to project vectors representing them to differ-
ent reduced dimensional spaces. The selection of
those dimensions is based on the number of training
cases available for each word, and limited to 500; the
used dimensions vary from 19 (for the word grant)
to 481 (for the word part).
</bodyText>
<subsectionHeader confidence="0.999845">
3.3 The number of classifiers (TDZ)
</subsectionHeader>
<bodyText confidence="0.998317666666667">
Based on previous experiments carried out for docu-
ment categorization (Zelaia et al., 2006), we decided
to create 30 classifiers for some words and 50 for
others, i.e. 30 or 50 individual k-NN algorithms will
be used by the multiclassifier in order to combine
opinions by Bayesian voting.
</bodyText>
<subsectionHeader confidence="0.998564">
3.4 Number of neigbors for k-NN
</subsectionHeader>
<bodyText confidence="0.99998875">
Based on our previous experiments, we decided to
use k = 1 and k = 5, and to select the best for each
of the words. The cosine similarity measure is used
in order to find the nearest or the 5 nearests.
</bodyText>
<sectionHeader confidence="0.999156" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.996465333333333">
The experiment was conducted by considering the
optimal values for parameters tuned by using the
training case set.
</bodyText>
<equation confidence="0.655675">
s
n1 =
Z=1
</equation>
<page confidence="0.980494">
360
</page>
<bodyText confidence="0.9475146">
Results published in this section were calculated
by the SemEval-2007 organizers. Table 1 shows ac-
curacy rates obtained by the 13 participants in the
SemEval-2007, 17 task, lexical sample WSD sub-
task.
</bodyText>
<table confidence="0.99839025">
System Accuracy System Accuracy
1. 0.887 8. 0.803
2. 0.869 9. 0.799
3. 0.864 10. 0.796
4. 0.857 11. 0.743
5. 0.851 12. 0.538
6. 0.851 13. 0.521
7. 0.838
</table>
<tableCaption confidence="0.942681">
Table 1: Accuracy rates obtained by the 13 partici-
pants. SemEval-2007, 17 task (Lexical Sample)
</tableCaption>
<bodyText confidence="0.931241666666667">
The result obtained by our system is 0.799 (the
9th among 13 participants), 1 point over the mean
accuracy (0.786).
</bodyText>
<sectionHeader confidence="0.99788" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999919333333333">
Results obtained show that the construction of a
multiclassifier, together with the use of Bayesian
voting to combine label predictions, plays an im-
portant role in the improvement of results. We also
want to remark that we used the SVD dimensional-
ity reduction technique in order to reduce the vector
representation of cases.
The approach presented in this paper was already
used in a document categorization task. However,
we never used it for WSD task. Therefore, in order
to adapt the method to the new task, we fixed some
parameters based on our previous experiments (30-
50 classifiers, k = 1, 5 for the k-NN algorithm) and
tuned some other parameters by experimenting quite
a high number of TDZ sizes and using different di-
mensions for each word. However, we noticed that
the application of our approach to a different task is
not straightforward. Greater effort will have to be
made in order to tune the different parameters to this
specific task of WSD.
One of the main difficulties we found was the dif-
ference in the number of training cases, comparing
with the high number usually available in other tasks
like text categorization.
As future work, we can think of applying a new
preprocessing approach in order to extract better fea-
tures from the training database which could help
the SVD technique improving the accuracy after
a dimensionality reduction is applied. The use of
Wordnet may help.
</bodyText>
<sectionHeader confidence="0.999053" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999594">
This research was supported by the University of
the Basque Country by the project “ANHITZ 2006:
Language Technologies for Multilingual Interaction
in Intelligent Environments”, IE 06-185
We wish to thank to the UBC-ALM group for
helping us extracting learning features.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999026727272727">
E. Agirre and O. Lopez de Lacalle. 2007. Ubc-alm:
Combining k-nn with svd for wsd. submited for pub-
lication to SemEval-2007.
M.W. Berry and M. Browne. 1999. Understanding
Search Engines: Mathematical Modeling and Text Re-
trieval. SIAM Society for Industrial and Applied
Mathematics, ISBN: 0-89871-437-0, Philadelphia.
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123–140.
B.V. Dasarathy. 1991. Nearest Neighbor (NN) Norms:
NN Pattern Recognition Classification Techniques.
IEEE Computer Society Press.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Societyfor Informa-
tion Science, 41:391–407.
T.G. Dietterich. 1998. Machine learning research: Four
current directions. The AI Magazine, 18(4):97–136.
S. Dumais. 2004. Latent semantic analysis. In ARIST
(Annual Review of Information Science Technology),
volume 38, pages 189–230.
T.K. Ho, J.J. Hull, and S.N. Srihari. 1994. Decision com-
bination in multiple classifier systems. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
16(1):66–75.
G. Salton and M. McGill. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill.
A. Zelaia, I. Alegria, O. Arregi, and B. Sierra. 2006.
A multiclassifier based document categorization sys-
tem: profiting from the singular value decomposition
dimensionality reduction technique. In Proceedings of
the Workshop on Learning Structured Information in
Natural Language Applications, pages 25–32.
</reference>
<page confidence="0.998675">
361
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.322100">
<title confidence="0.9880975">A based Multiclassifier System to perform WSD in a Reduced Dimensional Vector Space</title>
<author confidence="0.999903">Ana Zelaia</author>
<author confidence="0.999903">Olatz Arregi</author>
<author confidence="0.999903">Basilio Sierra</author>
<affiliation confidence="0.98568">Computer Science Faculty University of the Basque Country</affiliation>
<email confidence="0.835569">ana.zelaia@ehu.es</email>
<abstract confidence="0.948764">In this article a multiclassifier approach for word sense disambiguation (WSD) probis presented, where a set of classifiers is used to predict the category (sense) of each word. In order to combine the predictions generated by the multiclassifier, Bayesian voting is applied. Through all the classification process, a reduced dimensional vector representation obtained by Singular Value Decomposition (SVD) is used. Each word is considered an independent classification problem, and so different parameter setting, selected after a tuning phase, is applied to each word. The approach has been applied to the lexical sample WSD subtask of SemEval 2007 (task 17).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>O Lopez de Lacalle</author>
</authors>
<title>Ubc-alm: Combining k-nn with svd for wsd. submited for publication to SemEval-2007.</title>
<date>2007</date>
<marker>Agirre, de Lacalle, 2007</marker>
<rawString>E. Agirre and O. Lopez de Lacalle. 2007. Ubc-alm: Combining k-nn with svd for wsd. submited for publication to SemEval-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Berry</author>
<author>M Browne</author>
</authors>
<title>Understanding Search Engines: Mathematical Modeling and Text Retrieval.</title>
<date>1999</date>
<journal>SIAM Society for Industrial and Applied Mathematics, ISBN:</journal>
<pages>0--89871</pages>
<location>Philadelphia.</location>
<contexts>
<context position="1609" citStr="Berry and Browne, 1999" startWordPosition="247" endWordPosition="250">ation organization and management tasks. Both, word representation and classification method are crucial steps in the word sense disambiguation process. In this article both issues are considered. On the one hand, Latent Semantic Indexing (LSI) (Deerwester et al., 1990), which is a variant of the vector space model (VSM) (Salton and McGill, 1983), is used in order to obtain the vector representation of the corresponding word. This technique compresses vectors representing word related contexts into vectors of a lowerdimensional space. LSI, which is based on Singular Value Decomposition (SVD) (Berry and Browne, 1999) of matrices, has shown to have the ability to extract the relations among features representing words by means of their context of use, and has been successfully applied to Information Retrieval tasks. On the other hand, a multiclassifier (Ho et al., 1994) which uses different training databases is constructed. These databases are obtained from the original training set by random subsampling. The implementation of this approach is made by a model inspired in bagging (Breiman, 1996), and the k-NN classification algorithm (Dasarathy, 1991) is used to make the sense predictions for testing words</context>
</contexts>
<marker>Berry, Browne, 1999</marker>
<rawString>M.W. Berry and M. Browne. 1999. Understanding Search Engines: Mathematical Modeling and Text Retrieval. SIAM Society for Industrial and Applied Mathematics, ISBN: 0-89871-437-0, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="2096" citStr="Breiman, 1996" startWordPosition="326" endWordPosition="327">ts into vectors of a lowerdimensional space. LSI, which is based on Singular Value Decomposition (SVD) (Berry and Browne, 1999) of matrices, has shown to have the ability to extract the relations among features representing words by means of their context of use, and has been successfully applied to Information Retrieval tasks. On the other hand, a multiclassifier (Ho et al., 1994) which uses different training databases is constructed. These databases are obtained from the original training set by random subsampling. The implementation of this approach is made by a model inspired in bagging (Breiman, 1996), and the k-NN classification algorithm (Dasarathy, 1991) is used to make the sense predictions for testing words. Our group (UBC-ZAS) has participated in the lexical sample subtask of SemEval-2007 for task 17, which consists on 100 different words for which a training and testing database have been provided. The aim of this article is to give a brief description of our approach to deal with the WSD task and to show the results obtained. In Section 2, our approach is presented. In Section 3, the experimental setup is introduced. The experimental results are presented and discussed in Section 4</context>
<context position="6010" citStr="Breiman, 1996" startWordPosition="947" endWordPosition="948">-NN classification algorithm k-NN is a distance based classification approach. According to this approach, given an arbitrary testing case, the k-NN classifier ranks its nearest neighbors among the training word senses, and uses the sense of the k top-ranking neighbors to predict the corresponding to the word which is being analyzed (Dasarathy, 1991). 2.4 Combination of classifiers The combination of multiple classifiers has been intensively studied with the aim of improving the accuracy of individual components (Ho et al., 1994). A widely used technique to implement this approach is bagging (Breiman, 1996), where a set of training databases TDi is generated by selecting n training cases drawn randomly with replacement from the original training database TD of n cases. When a set of n1 &lt; n training cases is chosen from the original training collection, the bagging is said to be applied by random subsampling. In fact, this is the approach used in our work and the n1 parameter has been selected via tuning. According to the random subsampling, given a testing case q, the classifier will make a label prediction ci based on each one of the training databases TDi. One way to combine the predictions is</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>L. Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B V Dasarathy</author>
</authors>
<title>Nearest Neighbor (NN) Norms: NN Pattern Recognition Classification Techniques.</title>
<date>1991</date>
<publisher>IEEE Computer Society Press.</publisher>
<contexts>
<context position="2153" citStr="Dasarathy, 1991" startWordPosition="333" endWordPosition="334"> is based on Singular Value Decomposition (SVD) (Berry and Browne, 1999) of matrices, has shown to have the ability to extract the relations among features representing words by means of their context of use, and has been successfully applied to Information Retrieval tasks. On the other hand, a multiclassifier (Ho et al., 1994) which uses different training databases is constructed. These databases are obtained from the original training set by random subsampling. The implementation of this approach is made by a model inspired in bagging (Breiman, 1996), and the k-NN classification algorithm (Dasarathy, 1991) is used to make the sense predictions for testing words. Our group (UBC-ZAS) has participated in the lexical sample subtask of SemEval-2007 for task 17, which consists on 100 different words for which a training and testing database have been provided. The aim of this article is to give a brief description of our approach to deal with the WSD task and to show the results obtained. In Section 2, our approach is presented. In Section 3, the experimental setup is introduced. The experimental results are presented and discussed in Section 4, and finally, Section 5 contains some conclusions and co</context>
<context position="5748" citStr="Dasarathy, 1991" startWordPosition="906" endWordPosition="907"> (Dumais, 2004), the training and testing documents are projected to the reduced dimensional space, qp = qT UpE�1 p , by using p singular values and the cosine is usually calculated to measure the similarity between training and testing document vectors. 2.3 The k-NN classification algorithm k-NN is a distance based classification approach. According to this approach, given an arbitrary testing case, the k-NN classifier ranks its nearest neighbors among the training word senses, and uses the sense of the k top-ranking neighbors to predict the corresponding to the word which is being analyzed (Dasarathy, 1991). 2.4 Combination of classifiers The combination of multiple classifiers has been intensively studied with the aim of improving the accuracy of individual components (Ho et al., 1994). A widely used technique to implement this approach is bagging (Breiman, 1996), where a set of training databases TDi is generated by selecting n training cases drawn randomly with replacement from the original training database TD of n cases. When a set of n1 &lt; n training cases is chosen from the original training collection, the bagging is said to be applied by random subsampling. In fact, this is the approach </context>
</contexts>
<marker>Dasarathy, 1991</marker>
<rawString>B.V. Dasarathy. 1991. Nearest Neighbor (NN) Norms: NN Pattern Recognition Classification Techniques. IEEE Computer Society Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Societyfor Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="1256" citStr="Deerwester et al., 1990" startWordPosition="189" endWordPosition="192">ition (SVD) is used. Each word is considered an independent classification problem, and so different parameter setting, selected after a tuning phase, is applied to each word. The approach has been applied to the lexical sample WSD subtask of SemEval 2007 (task 17). 1 Introduction Word Sense Disambiguation (WSD) is an important component in many information organization and management tasks. Both, word representation and classification method are crucial steps in the word sense disambiguation process. In this article both issues are considered. On the one hand, Latent Semantic Indexing (LSI) (Deerwester et al., 1990), which is a variant of the vector space model (VSM) (Salton and McGill, 1983), is used in order to obtain the vector representation of the corresponding word. This technique compresses vectors representing word related contexts into vectors of a lowerdimensional space. LSI, which is based on Singular Value Decomposition (SVD) (Berry and Browne, 1999) of matrices, has shown to have the ability to extract the relations among features representing words by means of their context of use, and has been successfully applied to Information Retrieval tasks. On the other hand, a multiclassifier (Ho et </context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Societyfor Information Science, 41:391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Machine learning research: Four current directions.</title>
<date>1998</date>
<journal>The AI Magazine,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="6648" citStr="Dietterich, 1998" startWordPosition="1061" endWordPosition="1062">ning databases TDi is generated by selecting n training cases drawn randomly with replacement from the original training database TD of n cases. When a set of n1 &lt; n training cases is chosen from the original training collection, the bagging is said to be applied by random subsampling. In fact, this is the approach used in our work and the n1 parameter has been selected via tuning. According to the random subsampling, given a testing case q, the classifier will make a label prediction ci based on each one of the training databases TDi. One way to combine the predictions is by Bayesian voting (Dietterich, 1998), where a confidence value cvi is calculated for each training cj database TDi and sense cj to be predicted. These confidence values have been calculated based on the training collection. Confidence values are summed Random Subsampling R p Rm Rm d2 1 d d7 d509 dn TD1 k−NN k−NN c1 c2 ci cj dn d7 .. qp d135 . d6 d5 d1 d2 d6 d5 d1 Train ... d34 d23 dn d2 d3 d4 d3 d4 d61 R p R p R p TD M Mp = UpEpPT qp SVD VSM Bayesian voting d256 d98 d638 d848 d50 d778 TD2 q VSM qp d9 q1 q2 qp = qT UpE�1 p d4612 k−NN Test q ... d33 qn’ d2787 d1989 d55 TDi 359 by sense; the sense cj that gets the highest value is </context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>T.G. Dietterich. 1998. Machine learning research: Four current directions. The AI Magazine, 18(4):97–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
</authors>
<title>Latent semantic analysis.</title>
<date>2004</date>
<booktitle>In ARIST (Annual Review of Information Science Technology),</booktitle>
<volume>38</volume>
<pages>189--230</pages>
<contexts>
<context position="5147" citStr="Dumais, 2004" startWordPosition="808" endWordPosition="809">ts in text categorization tasks. The newer method of Latent Semantic Indexing (LSI) 1 (Deerwester et &apos;http://lsi.research.telcordia.com, http://www.cs.utk.edu/∼lsi al., 1990) is a variant of the VSM in which documents are represented in a lower dimensional space created from the input training dataset. The SVD technique used by LSI consists in factoring termdocument matrix M into the product of three matrices, M = UEVT where E is a diagonal matrix of singular values, and U and V are orthogonal matrices of singular vectors (term and document vectors, respectively). For classification purposes (Dumais, 2004), the training and testing documents are projected to the reduced dimensional space, qp = qT UpE�1 p , by using p singular values and the cosine is usually calculated to measure the similarity between training and testing document vectors. 2.3 The k-NN classification algorithm k-NN is a distance based classification approach. According to this approach, given an arbitrary testing case, the k-NN classifier ranks its nearest neighbors among the training word senses, and uses the sense of the k top-ranking neighbors to predict the corresponding to the word which is being analyzed (Dasarathy, 1991</context>
</contexts>
<marker>Dumais, 2004</marker>
<rawString>S. Dumais. 2004. Latent semantic analysis. In ARIST (Annual Review of Information Science Technology), volume 38, pages 189–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Ho</author>
<author>J J Hull</author>
<author>S N Srihari</author>
</authors>
<title>Decision combination in multiple classifier systems.</title>
<date>1994</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="1866" citStr="Ho et al., 1994" startWordPosition="289" endWordPosition="292"> 1990), which is a variant of the vector space model (VSM) (Salton and McGill, 1983), is used in order to obtain the vector representation of the corresponding word. This technique compresses vectors representing word related contexts into vectors of a lowerdimensional space. LSI, which is based on Singular Value Decomposition (SVD) (Berry and Browne, 1999) of matrices, has shown to have the ability to extract the relations among features representing words by means of their context of use, and has been successfully applied to Information Retrieval tasks. On the other hand, a multiclassifier (Ho et al., 1994) which uses different training databases is constructed. These databases are obtained from the original training set by random subsampling. The implementation of this approach is made by a model inspired in bagging (Breiman, 1996), and the k-NN classification algorithm (Dasarathy, 1991) is used to make the sense predictions for testing words. Our group (UBC-ZAS) has participated in the lexical sample subtask of SemEval-2007 for task 17, which consists on 100 different words for which a training and testing database have been provided. The aim of this article is to give a brief description of o</context>
<context position="5931" citStr="Ho et al., 1994" startWordPosition="933" endWordPosition="936">o measure the similarity between training and testing document vectors. 2.3 The k-NN classification algorithm k-NN is a distance based classification approach. According to this approach, given an arbitrary testing case, the k-NN classifier ranks its nearest neighbors among the training word senses, and uses the sense of the k top-ranking neighbors to predict the corresponding to the word which is being analyzed (Dasarathy, 1991). 2.4 Combination of classifiers The combination of multiple classifiers has been intensively studied with the aim of improving the accuracy of individual components (Ho et al., 1994). A widely used technique to implement this approach is bagging (Breiman, 1996), where a set of training databases TDi is generated by selecting n training cases drawn randomly with replacement from the original training database TD of n cases. When a set of n1 &lt; n training cases is chosen from the original training collection, the bagging is said to be applied by random subsampling. In fact, this is the approach used in our work and the n1 parameter has been selected via tuning. According to the random subsampling, given a testing case q, the classifier will make a label prediction ci based o</context>
</contexts>
<marker>Ho, Hull, Srihari, 1994</marker>
<rawString>T.K. Ho, J.J. Hull, and S.N. Srihari. 1994. Decision combination in multiple classifier systems. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(1):66–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="1334" citStr="Salton and McGill, 1983" startWordPosition="203" endWordPosition="206">blem, and so different parameter setting, selected after a tuning phase, is applied to each word. The approach has been applied to the lexical sample WSD subtask of SemEval 2007 (task 17). 1 Introduction Word Sense Disambiguation (WSD) is an important component in many information organization and management tasks. Both, word representation and classification method are crucial steps in the word sense disambiguation process. In this article both issues are considered. On the one hand, Latent Semantic Indexing (LSI) (Deerwester et al., 1990), which is a variant of the vector space model (VSM) (Salton and McGill, 1983), is used in order to obtain the vector representation of the corresponding word. This technique compresses vectors representing word related contexts into vectors of a lowerdimensional space. LSI, which is based on Singular Value Decomposition (SVD) (Berry and Browne, 1999) of matrices, has shown to have the ability to extract the relations among features representing words by means of their context of use, and has been successfully applied to Information Retrieval tasks. On the other hand, a multiclassifier (Ho et al., 1994) which uses different training databases is constructed. These datab</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zelaia</author>
<author>I Alegria</author>
<author>O Arregi</author>
<author>B Sierra</author>
</authors>
<title>A multiclassifier based document categorization system: profiting from the singular value decomposition dimensionality reduction technique.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Learning Structured Information in Natural Language Applications,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="10347" citStr="Zelaia et al., 2006" startWordPosition="1762" endWordPosition="1765">of cases sometimes by procedure a) and sometimes by b). 3.2 The dimension of the reduced Vector Space Model Taking into account the wide differences among the training case numbers for different words, we decided to project vectors representing them to different reduced dimensional spaces. The selection of those dimensions is based on the number of training cases available for each word, and limited to 500; the used dimensions vary from 19 (for the word grant) to 481 (for the word part). 3.3 The number of classifiers (TDZ) Based on previous experiments carried out for document categorization (Zelaia et al., 2006), we decided to create 30 classifiers for some words and 50 for others, i.e. 30 or 50 individual k-NN algorithms will be used by the multiclassifier in order to combine opinions by Bayesian voting. 3.4 Number of neigbors for k-NN Based on our previous experiments, we decided to use k = 1 and k = 5, and to select the best for each of the words. The cosine similarity measure is used in order to find the nearest or the 5 nearests. 4 Experimental Results The experiment was conducted by considering the optimal values for parameters tuned by using the training case set. s n1 = Z=1 360 Results publis</context>
</contexts>
<marker>Zelaia, Alegria, Arregi, Sierra, 2006</marker>
<rawString>A. Zelaia, I. Alegria, O. Arregi, and B. Sierra. 2006. A multiclassifier based document categorization system: profiting from the singular value decomposition dimensionality reduction technique. In Proceedings of the Workshop on Learning Structured Information in Natural Language Applications, pages 25–32.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>