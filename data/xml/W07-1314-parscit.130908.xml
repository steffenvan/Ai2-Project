<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002796">
<title confidence="0.954832">
Cognate identification and alignment using practical orthographies
</title>
<author confidence="0.961212">
Michael Cysouw
</author>
<affiliation confidence="0.795022">
Max Planck Institute for Evolutionary
Anthropology, Leipzig
</affiliation>
<email confidence="0.98818">
cysouw@eva.mpg.de
</email>
<author confidence="0.98848">
Hagen Jung
</author>
<affiliation confidence="0.8104065">
Max Planck Institute for Evolutionary
Anthropology, Leipzig
</affiliation>
<email confidence="0.994378">
jung@eva.mpg.de
</email>
<sectionHeader confidence="0.995587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999823111111111">
We use an iterative process of multi-gram
alignment between associated words in dif-
ferent languages in an attempt to identify
cognates. To maximise the amount of data,
we use practical orthographies instead of
consistently coded phonetic transcriptions.
First results indicate that using practical or-
thographies can be useful, the more so when
dealing with large amounts of data.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961785714286">
The comparison of lexemes across languages is a
powerful method to investigate the historical rela-
tions between languages. A central prerequisite for
any interpretation of historical relatedness is to es-
tablish lexical cognates, i.e. lexemes in different
languages that are of shared descend (in contrast to
similarity by chance). If a pair of lexemes in two dif-
ferent languages stem from the same origin, this can
be due to the fact that both languages derive from a
common ancestor language, but it can also be caused
by influence from one language on another (or influ-
ence on both language from a third language). To
decide whether cognates are indicative of a common
ancestor language (“vertical transmission”) or due to
language influence (“horizontal transmission”) is a
difficult problem with no shortcuts. We do not think
that one kind of cognacy is more interesting that an-
other. Both loans (be it from a substrate or a super-
strate) and lexemes derived from a shared ancestor
are indicative of the history of a language, and both
should be acknowledged in the unravelling of lin-
guistic (pre)history.
In this paper, we approach the identification of
cognate lexemes on the basis of large parallel lex-
ica between languages. This approach is an explicit
attempt to reverse the “Swadesh-style” wordlist
method. In the Swadesh-style approach, first mean-
ings are selected that are assumed to be less prone
to borrowing, then cognates are identified in those
lists, and these cognates are then interpreted as in-
dicative of shared descend. In contrast, we propose
to first identify (possible) cognates among all avail-
able information, then divide these cognates into
strata, and then interpret these strata in historical
terms. (Because of limitations of space, we will
only deal with the first step, the identification of cog-
nates, in this paper.) This is of course exactly the
route of the traditional historical-comparative ap-
proach to language comparison. However, we think
that much can be gained by applying computational
approaches to this approach.
A major problem arises when dealing with large
quantities of lexical material from many different
languages. In most cases it will be difficult (or very
costly and time consuming in the least) to use co-
herent and consistent phonetic transcriptions of all
available information. Even if we would have dictio-
naries with phonetic transcriptions for all languages
that we are interested in, this would not necessarily
help, as the details of phonetic transcription are nor-
mally not consistent across different authors. In this
paper, we will therefore attempt to deal with unpro-
cessed material in practical orthographies. This will
of course pose problems for history-ridden orthogra-
phies like in English or French. However, we beleve
that for most of the world’s languages the practical
</bodyText>
<page confidence="0.98862">
109
</page>
<bodyText confidence="0.988892727272727">
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 109–116,
Prague, June 2007. c�2007 Association for Computational Linguistics
orthographies are not as inconsistent as those (be-
cause they are much younger) and might very well
be useful for linguistic purposes.
In this paper, we will first discuss the data used in
this investigation. Then we will describe the algo-
rithm that we used to infer alignments between word
pairs. Finally, we will discuss a few of the results
using this algorithm on large wordlists in practical
orthography.
</bodyText>
<sectionHeader confidence="0.995861" genericHeader="introduction">
2 Resources
</sectionHeader>
<bodyText confidence="0.999879346153846">
In this study we used parallel wordlists that
we extracted from the Intercontinental Dictio-
nary Series (IDS) database, currently under
development at the Max Planck Institute for
Evolutionary Anthropology in Leipzig (see
http://www.eva.mpg.de/lingua/files/ids.html for
more information). The IDS wordlists contain
more than thousand entries of basic words from
each language, and many entries contain alternative
wordforms. At this time, there are only a few
basic transcription languages (English, French
and Portuguese) and some Caucasian languages
available. We choose some of them for the purpose
of the present study and preprocessed the data.
To compare languages, we chose only word pairs
that were available and non-compound in both
languages. For all words that occurred several times
in the whole collection of a language, we accepted
only one randomly choosen wordform and left out
all others. We also deleted content in brackets or
in between other special characters. If, after these
preparation, a wordform is still longer than twelve
UTF-8 characters, we disregard these for reasons
of computational efficiency. After this, we are still
left with a large number of about 900 word pairs for
each pair of languages.
</bodyText>
<sectionHeader confidence="0.995339" genericHeader="method">
3 Alignment
</sectionHeader>
<bodyText confidence="0.999927444444444">
An alignment of two words wa and wb is a bijective
and maintained ordered one-to-one correspondence
from all subsequences sa of the word wa with wa =
concat(sa1, sa2, ... , sak) to all subsequences sb of
the word wb with wb = concat(sb1, sb2, ... , sbk). It
is possible that one of the associated subsequences
is the empty word E. In general one may construct
a distance measure from such a linked sequence of
two given words by assigning a cost for each single
link of the alignment. There are many such align-
ment/cost functions described in the literature, and
they are often used to calculate a distance measure
between two sequences of characters (Inkpen et al.,
2005). A measurement regularly used for linguistic
sequences is the Levenshtein distance, or a modi-
fications of it. Other distance measures detect, for
example, the longest common subsequences or the
longest increasing subsequences.
It is our special interest to use multi-character
mappings for calculating a distance between two
words. Therefore, we adapt and extend the Leven-
shtein measurement. First, we allow for mapping
of any arbitrary string length (not just strings of one
character as in Levenshtein) and, second, we assign
a continuous cost between 0 and 1 for every map-
ping.
Our algorithm consist basically of two steps. In
the first step, all possible subsequence pairs between
associated words are considered, and a cost function
is extracted for every multi-gram pair from their co-
occurrences in the whole wordlist. In a second step,
this cost function is used to infer an alignment be-
tween whole words. On the basis of this alignment
a new cost function is established for all multi-gram
pairs. This second step can be iterated until the cost
function stabilizes.
</bodyText>
<subsectionHeader confidence="0.999896">
3.1 Cost of an multi-gram pair
</subsectionHeader>
<bodyText confidence="0.999980611111111">
For every pair of subsequences sai and sb, we count
the number of co-occurrences. The subsequences
sai and sb, co-occur when they are found in two as-
sociated words wa and wb from a language wordlist
of two languages La and Lb. We then use a sim-
ple Dice coefficient as a cost function between all
possible subsequences. For computational reasons,
it is necessary to limit the size of the multi-grams
considered. We decided to limit the multi-gram
size to a number of maximally four UTF-8 char-
acters. Still, in the first step of our algorithm,
there is a very large set of such subsequence pairs
because all possible combinations are considered.
When an alignment is inferred in the iterative pro-
cess, only the aligned subsequences are counted as
co-occurrences, so the number of possible combi-
nations is considerably lower. Further, to prevent
low frequent co-occurrences to have a dispropor-
</bodyText>
<page confidence="0.986631">
110
</page>
<bodyText confidence="0.999163666666667">
tional impact, we added an attestation threshold of
2% of the wordlist size for two subsequences to be
accepted for the alignment process.
</bodyText>
<subsectionHeader confidence="0.999958">
3.2 Alignment of words
</subsectionHeader>
<bodyText confidence="0.999687">
An alignment of two words is a complete ordered
linking of subsequences. We annotate it in the
following way (vertical dashes delimit the subse-
quences; note that subsequences may be empty):
</bodyText>
<equation confidence="0.659409">
(  |w  |ool)(mepc  |Tb  |)
</equation>
<bodyText confidence="0.999964866666667">
There is a huge amount of possible combinations
of aligned subsequences. On the basis of the cost
function, a distance is established for every word
pair alignment. The summation of all multi-gram
mapping costs represents the distance of the align-
ment. Because we are dealing with multi-grams of
variable length, alternative alignments of the same
word pair will consist of a different number of sub-
sequences. So, simple summation would lead to dis-
tances out of the range from 0 to 1. To counteract
this, we normalized the word distance. We weighted
each subsequence relative to the number of charac-
ters in the subsequence. For example, the mapping
of w and Tb in the example above would be multi-
plied by 3 , because w and Tb have together 3 char-
</bodyText>
<page confidence="0.510371">
10
</page>
<bodyText confidence="0.999824">
acters and the complete words have in total 10 char-
acters.
To make use of efficient divide and conquer solv-
ing strategies and to get meaningful linguistic state-
ments with the base of the calculated best align-
ments, we decided to look for a special subset of
best alignments. As (Kondrak, 2002) pointed out,
there are some situations in which the consideration
of local alignment gets the required results. If only
a part of a word aligning sequence is of high simi-
larity then sometimes a linguistic justification of the
whole word similarity is given. Those alignments
contain the lowest cost multi-gram pairs, but are not
necessarily of best similarity in total.
To illustrate the difference between local and
global alignment, consider an example that shows
different results, depending whether the total sum of
multi-gram similarities is taken or the best local one.
Look at the two words ‘abc’ and ‘αβγ’ and a part of
its multi-gram cost function in Table 1. The sum-
mation of the costs would prefer alignment A2, as
can be seen in Table 2. But we prefer A1, because
it contains the subsequence pair (ab  |αβ) with the
</bodyText>
<table confidence="0.98370225">
multi-gram 1 multi-gram 2 cost
ab ... 0.1
bc . ... 0.3
a αβ 0.4
c βγ 0.8
α .
γ
.
</table>
<tableCaption confidence="0.920398">
Table 1: Costs for constructed subsequence pairs
(ordered by cost)
</tableCaption>
<table confidence="0.999763666666667">
Index Alignment Distance
A2 ... 0.4 + 0.3 = 0.7
A1 . ... 0.1 + 0.8 = 0.9
(a  |bc)(α  |βγ) .
(ab  |c)(αβ  |γ)
.
</table>
<tableCaption confidence="0.999248">
Table 2: Alignments with distance
</tableCaption>
<bodyText confidence="0.996675222222222">
lowest cost.
With these assumptions, we composed a fast and
easy method to find the best alignment. We pre-
fer alignments where some links are very good,
but the rest might not be. We assume that words
are more related to each other, if there are such
highly rated pairs. This approach can also be found
in other string based comparing methods like, for
example, the Longest Common Increasing Subse-
quence method, which calculates the longest equal
multi-gram and neglects the rest of the word. We
first order all possible multi-gram mappings by their
costs and pick the subsequence pair with the low-
est cost. Starting from this mapping seed, we look
for mappings for the rest of the word pair, both be-
fore and after the initial mapped subsequence. For
both these suffixes and prefixes, we again search for
the subsequence with the lowest cost. This process
is re-applied until the whole words are mapped. If
there is more than one optimal linking subsequence
pair, then all possible alignments are considered. In
this way, we do not restrict, in contrast to Kondrak,
which position for the multi-gram mapping will be
preferred for the local alignment. The algorithm
runs in O(n6). It takes O(n4) time for all combina-
tions of different multi-gram pairs within O(n) steps
in O(n) iterations.
</bodyText>
<page confidence="0.997905">
111
</page>
<sectionHeader confidence="0.99852" genericHeader="evaluation">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999778772727273">
As mentioned above, we applied our model to some
test data from the IDS database. For later anal-
yses, we also constructed some random wordlists.
With these we are able to say something about how
significant our results are. To make these random
wordlists we remap each word wa from La to an ar-
bitrarily chosen word wb from collection Lb. This
new mapped word was adjusted to the size of the
originally associated word from Lb. The adjustment
works by stretching or shrinking the new word to the
required length by doubling the word several times
and cutting of the overlaying head or tail afterwards.
In this way, we controlled for word length and multi-
gram frequencies. This randomization process was
performed five times from La to Lb, and five the
times from Lb to La, and the results were averaged
over all these ten cases.
For the calculation process, we stored all lists in
SQL tables. We first built a preprocessed work-
ing table with the lexemes from the languages to be
compared, and afterwards we constructed the result-
ing tables that hold all the results:
</bodyText>
<listItem confidence="0.996945625">
• compare table: the word pairs, their alignments
and alignment goodness;
• subsequence table: the subsequence pairs
found and their co-occurrence coefficients;
• random compare table: pseudo random word
pairs like the compare table;
• random subsequence table: the subsequence
pairs found from random compare table.
</listItem>
<bodyText confidence="0.99973778">
Table 3 consists of the best alignments for word pairs
of English and French after 30 iterations, and Table
4 shows the best alignments for the comparison of
English and Hunzib (a Caucasian language). First
note that our algorithm works independently of the
orthography used. We do not assume that the same
UTF-8 characters in the two languages are identi-
cal. The fact that (c) is mapped between English
clan and French clan is a result of the statistical dis-
tribution of these characters in the two languages.
This orthography-independence means that we can
apply our algorithm without modifications to cyrillic
scripts as shown with the English-Hunzib compari-
son. Second, we payed close attention to the fact that
the word similarity values are comparable among
different language comparisons. This means that it
is highly significant that the highest word similar-
ities between English and French are much higher
than those between English and Hunzib (actually,
the alignments between English and Hunzib are non-
sensical, but more about that later). Further, our al-
gorithm finds vowel-consonant multi-grams in some
cases (e.g. see Table 5). As far as we can see, there
are not linguistically meaningful and should be con-
sidered an artifact of our current approach. We hope
to fine-tune the algorithm in the future to prevent this
behavior.
Our method finds alignments, but also the subse-
quences in the alignments are of interest. The best
mapped multi-grams between English and French
are illustrated in Table 5. Strangely, the highest
ranked ones are a few vowel+consonant bigrams,
that occur not very often. Since the Dice coefficient
depends on the size of the investigated collection, we
assumed a minimum frequency of co-occurrences in
each calculation step of 2% of the collection size
(which is 20 cases in the English-French compari-
son). The high-ranked bigrams are all just above this
threshold. Therefore, we might argue that all the bi-
grams from the top of the list are a side-effect of the
collection size itself.
Following these bigrams are many one-to-
one matches of all alphabetic characters except
O,k,q,w,x,y,z). These mappings are found without
assuming any similarity based on the UTF-8 encod-
ing of the characters. What we actually find here is
a mapping for the orthography of the stratum of the
French loan words in English. As can be seen in the
histogram in Figure 1, the mapping between multi-
grams falls off dramatically after these links.
</bodyText>
<page confidence="0.995523">
112
</page>
<table confidence="0.997601642857143">
English French Alignment similarity
tribe,clan tribu,clan (  |c   ||l   ||an  |) (  |c   ||l   ||an  |) 0.955872
long long (  |l  ||on  ||g  |) (  |l   ||on  ||g  |) 0.925542
lion lion (  |l   ||i   ||on  |) (  |l   ||i   ||on  |) 0.916239
canoe canoe,pirogue (  |c  ||an  ||o  ||e  |) (  |c  ||an  ||o  ||e  |) 0.911236
famine famine,disette (  |f  ||a  ||m  ||in  ||e  |) (  |f  ||a  ||m  ||in  ||e  |) 0.910465
innocent innocent (  |in  ||n  ||o  ||c  ||e  ||n  ||t  |) (  |in  ||n  ||o  ||c  ||e  ||n  ||t  |) 0.908913
prison,jail prison (  |p  ||r  ||i  ||s   ||on  |) (  |p  ||r  ||i  ||s   ||on  |) 0.9089
poncho poncho (  |p  ||on  ||c  ||h  ||o  |) (  |p  ||on  ||c  ||h  ||o  |) 0.907496
sure,certain sˆur,certain (  |c  ||e  ||r  ||t  ||a  ||in  |) (  |c  ||e  ||r  ||t  ||a  ||in  |) 0.905022
tapioca,manioc manioc (  |m   ||an  ||i  ||o  ||c  |) (  |m   ||an  ||i  ||o  ||c  |) 0.904811
. .. .... .
...
.
</table>
<tableCaption confidence="0.998473">
Table 3: English-French best rated alignments after 30 iterations
</tableCaption>
<table confidence="0.979429142857143">
English Hunzib Alignment similarity
jewel wavg~ar,Hak•ut (  |j   ||e   ||w   ||e   ||l  |) (  |w   ||a  |v  |g~   ||a   ||r  |) 0.507094
see nacIa (  |s  ||e   ||e  |) (  |n   ||a  |cI  |a  |) 0.489442
grease,fat ma•a (g  |r   ||e  |a  |s   ||e  |) (  |m   ||a   ||•   ||a  |) 0.464667
heaven gIalwan (  |h   ||e   ||a   ||v   ||e   ||n  |) (g  |I   ||a   ||l   ||w   ||a  ||n  |) 0.445626
ocean akean (  |o   ||c   ||e  |a  |n  |) (a  |k   ||e  ||a  ||n  |) 0.419629
pocket kisa,wibi (p  |o   ||c   ||k   ||e  |t) (  |k   ||i   ||s   ||a  |) 0.410143
sweep l•alIa (  |s  |w  |e   ||e  |p) (l  |•   ||a  |lI  |a  |) 0.395264
measure masa (  |m   ||e  |a  |s  |ur  |e  |) (  |m   ||a   ||s   ||a  |) 0.393806
flower g~akI (flo  |w   ||e   ||r  |) (  |g~   ||a   ||k  |I) 0.391867
rebuke,scold ak~a (r  |e   ||b   ||u  |k  |e  |) (  |a   ||k   ||~   ||a  |) 0.387163
. ...
. ...
.
</table>
<tableCaption confidence="0.999687">
Table 4: English-Hunzib best rated alignments after 30 iterations
</tableCaption>
<page confidence="0.968916">
113
</page>
<table confidence="0.999944518518519">
E F freq dice
ar ar 21 1
in in 26 1
on on 22 1
an an 22 1
m m 80 0.92786
n n 188 0.92161
c c 120 0.91815
p p 78 0.91798
r r 277 0.91665
f f 35 0.90647
l l 132 0.90534
v v 26 0.90346
t t 165 0.8719
b b 44 0.86301
s s 126 0.85915
d d 66 0.82913
o o 192 0.82325
e e 417 0.81479
a a 229 0.81367
g g 34 0.79683
h h 53 0.7856
i i 183 0.75961
u u 94 0.69546
. .. .... .
...
.
</table>
<tableCaption confidence="0.919261">
Table 5: Best English (E) and French (F) multi-gram
mappings after 30 iterations.
</tableCaption>
<bodyText confidence="0.999888529411765">
The character-independence of our method is il-
lustrated by the character mapping between English
and Russian in Table 6. Shown in the table are only
the highest ranked orthographic mappings. Again
we see an almost complete alphabetic linkage, prob-
ably caused by the French loanwords shared by both
English and Russian.
With this approach, we are also able to find some
vestiges of sound changes, as illustrated by the char-
acter mapping between Spanish and Portuguese in
Table 7. Shown here are only the highest ranked
non-identical multi-grams. The dice coefficients of
the pairs (h)−(ll), (f)−(h) show the results of sound
changes that were dramatically enough to be repre-
sented in the orthography. The pairs (c¸) − (z) and
(n)−(˜n) show difference in orthographic convention
(though the best pair should have been (nh) − (˜n)).
</bodyText>
<figure confidence="0.643833333333333">
number of mapped multigrams] 0 10 20 30 40
0.0 0.2 0.4 0.6 0.8 1.0
dice coefficient
</figure>
<figureCaption confidence="0.7017845">
Figure 1: Histogram of dice-coefficients for
English-French multi-gram mappings.
</figureCaption>
<table confidence="0.999923761904762">
E R freq dice
r p 184 0.88874745
n x 115 0.8461936
l .a 104 0.79646295
s c 114 0.7927922
t T 165 0.7701921
m M 47 0.7699933
o o 184 0.7510106
k Tb 21 0.74458015
p n 50 0.7388723
i 14 102 0.7034591
a a 221 0.6866478
u y 40 0.6449104
c x 77 0.6251676
e e 219 0.59066784
b 6 32 0.525643
w s 46 0.46787763
d R 42 0.381996
. .. .... .
...
.
</table>
<tableCaption confidence="0.966356">
Table 6: Best English (E) and Russian (R) multi-
gram mappings after 30 iterations.
</tableCaption>
<page confidence="0.997089">
114
</page>
<tableCaption confidence="0.926207">
Table 7: Spanish (S) and Portuguese (P) multi-gram
</tableCaption>
<table confidence="0.9484659">
mappings after 30 iterations. Only the
highest ranking non-identical mappings are
shown
P S freq dice
c¸ . .. .... .
h . .. .... 0.6316202
f z ... 0.4552776
n ll ... 0.43381172
a˜ h . 0.37720457
h n˜ 20 0.31106696
v n 20 0.23646937
t h 34 0.2165933
z b 24 0.2127131
o h 33 0.15424858
c 23 0.12838262
e 32 .
29
24
305
.
</table>
<figureCaption confidence="0.994765333333333">
Figure 2: English-French similarities for word
alignments plotted against the similarities
with random language entries.
</figureCaption>
<figure confidence="0.989734">
pseudo random coefficient
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4
coefficient
0.6 0.8
1.0
</figure>
<bodyText confidence="0.999928916666667">
A promising indicator for cognate identification is
the comparison of word alignment similarities with
the similarities between randomly associated word
pairs. We generated pseudo random word pairs as
described above. Therefore we caluclate for each
word from one language one coeffiecent value for
the linkage with the assocciated word and a sec-
ond avarage value for the linkage with some ran-
dom words. In Figure 2 we plot these two values
for all words of English and all words of French (af-
ter 30 iterations) against each other. Each dot repre-
sents a word. The x-axis shows the similarity coef-
ficient between the real words and the y-axis shows
the similarity coefficient from the comparison with
the pseudo random words. As can be seen, many
of the actual similarities are more to the right of the
y = x line indicating more than chance frequency
similarity.
In contrast, in comparing English with Hunzib in
Figure 3 there is only a slight tendency of stretching
of the scatterplot. So one could conclude that En-
glish and Hunzib have probably no cognates at all,
although there are some strongly related word pairs.
However, some slight stretching will always be seen,
because of the usage of an algorithm with iterations.
Such a process will always strengthen some random
tendencies.
The iterative process is illustrated in Figure 4.
Shown here are the alignment similarities for all
word pairs between French and Portuguese. After
the first round of alignment, there is only a slight
stretch in the scatterplot. Already after the second
iteration, the plot is stretched strongly. In the further
iterations the situation changes only slightly. Appar-
ently, two rounds of alignment and reassignment of
the cost function suffice for convergence.
</bodyText>
<sectionHeader confidence="0.999271" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999969923076923">
The big advantage of using original orthographies
in the study of linguistic relationships is that much
more information is readily available. Because of
the wealth of available data, we can use computa-
tional approaches for the comparison of wordlists.
In principle, the kind of approach that we have
sketched out in this paper can just as well be used
for the comparison of complete dictionaries. The
comparison of real wordlists with randomly shuf-
fled wordlists indicated that even on purely statis-
tical grounds it might be possible to separate mean-
ingful alignments from random alignments.
The most promising result of our investigation is
</bodyText>
<page confidence="0.988006">
115
</page>
<figure confidence="0.99927415">
pseudo random coefficient
pseudo random coefficient
pseudo random coefficient
pseudo random coefficient
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
coefficient
0.0 0.2 0.4 0.6 0.8 1.0
coefficient
0.0 0.2 0.4 0.6 0.8 1.0
coefficient
0.0 0.2 0.4 0.6 0.8 1.0
coefficient
iteration 29
iteration 1
iteration 2
iteration 0
</figure>
<figureCaption confidence="0.965535">
Figure 3: English-Hunzib similarities for word
alignments plotted against the similarities
with random language entries.
</figureCaption>
<bodyText confidence="0.999986571428572">
that we were able to find cognates even without any
knowledge about the orthographic conventions used
in the languages that were compared. In the com-
parison English-French and English-Russian there
appear to be many French loanwords among the
well-aligned wordpairs. If this impresion holds, we
are in fact only able to infer the stratum of French
influence in European languages. An interesting
next step would then be to redo the analyses af-
ter removing this stratum from the data and look
for deeper strata in the lexicon. As shown by the
Spanish-Portuguese comparison, sound changes can
be picked up by our approach as long as the changes
have left a trace in the orthography.
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998186166666667">
Diana Inkpen, Oana Frunza, and Grzegorz Kondrak.
2005. Automatic identification of cognates and false
friends in french and english. In RANLP-2005, Bul-
garia, pages 251–257, September.
Grzegorz Kondrak. 2002. Algorithms for language re-
construction. Ph.D. thesis, University of Toronto.
</reference>
<figure confidence="0.99740325">
0.0 0.2 0.4 0.6 0.8 1.0
coefficient
0.0 0.2 0.4 0.6 0.8 1.0
pseudo random coefficient
</figure>
<figureCaption confidence="0.88161375">
Figure 4: Plots of four iterations after 1, 2, 3 and 30
rounds of the French-Portuguese compar-
ison. The coefficients are plotted against
coefficients that were build with random-
</figureCaption>
<page confidence="0.385907">
116 ized language entries.
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.071756">
<title confidence="0.998876">Cognate identification and alignment using practical orthographies</title>
<author confidence="0.70988">Michael Max Planck Institute for Anthropology</author>
<email confidence="0.968154">cysouw@eva.mpg.de</email>
<author confidence="0.721403">Hagen Max Planck Institute for Anthropology</author>
<email confidence="0.99122">jung@eva.mpg.de</email>
<abstract confidence="0.998137">We use an iterative process of multi-gram alignment between associated words in different languages in an attempt to identify cognates. To maximise the amount of data, we use practical orthographies instead of consistently coded phonetic transcriptions. First results indicate that using practical orthographies can be useful, the more so when dealing with large amounts of data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Diana Inkpen</author>
</authors>
<title>Oana Frunza, and Grzegorz Kondrak.</title>
<marker>Inkpen, </marker>
<rawString> Diana Inkpen, Oana Frunza, and Grzegorz Kondrak.</rawString>
</citation>
<citation valid="true">
<title>Automatic identification of cognates and false friends in french and english.</title>
<date></date>
<booktitle>In RANLP-2005, Bulgaria,</booktitle>
<tech>Ph.D. thesis,</tech>
<pages>251--257</pages>
<institution>University of Toronto.</institution>
<marker>2005.</marker>
<rawString>Automatic identification of cognates and false friends in french and english. In RANLP-2005, Bulgaria, pages 251–257, September. Grzegorz Kondrak. 2002. Algorithms for language reconstruction. Ph.D. thesis, University of Toronto.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>