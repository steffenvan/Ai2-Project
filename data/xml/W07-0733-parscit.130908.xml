<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003851">
<title confidence="0.983079">
Experiments in Domain Adaptation for Statistical Machine Translation
</title>
<author confidence="0.865229">
Philipp Koehn and Josh Schroeder
</author>
<affiliation confidence="0.789564">
pkoehn@inf.ed.ac.uk, j.schroeder@ed.ac.uk
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW
Scotland, United Kingdom
</affiliation>
<sectionHeader confidence="0.9776" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980818181818">
The special challenge of the WMT 2007
shared task was domain adaptation. We
took this opportunity to experiment with
various ways of adapting a statistical ma-
chine translation systems to a special do-
main (here: news commentary), when
most of the training data is from a dif-
ferent domain (here: European Parliament
speeches). This paper also gives a descrip-
tion of the submission of the University of
Edinburgh to the shared task.
</bodyText>
<sectionHeader confidence="0.868605" genericHeader="method">
1 Our framework: the Moses MT system
</sectionHeader>
<bodyText confidence="0.999915454545454">
The open source Moses (Koehn et al., 2007) MT
system was originally developed at the University
of Edinburgh and received a major boost through a
2007 Johns Hopkins workshop. It is now used at
several academic institutions as the basic infrastruc-
ture for statistical machine translation research.
The Moses system is an implementation of the
phrase-based machine translation approach (Koehn
et al., 2003). In this approach, an input sentence is
first split into text chunks (so-called phrases), which
are then mapped one-to-one to target phrases using
a large phrase translation table. Phrases may be re-
ordered, but typically a reordering limit (in our ex-
periments a maximum movement over 6 words) is
used. See Figure 1 for an illustration.
Phrase translation probabilities, reordering prob-
abilities and language model probabilities are com-
bined to give each possible sentence translation a
score. The best-scoring translation is searched for by
the decoding algorithm and outputted by the system
as the best translation. The different system compo-
nents hi (phrase translation probabilities, language
</bodyText>
<figureCaption confidence="0.9155908">
Figure 1: Phrase-based statistical machine transla-
tion model: Input is split into text chunks (phrases)
which are mapped using a large phrase translation
table. Phrases are mapped one-to-one, and may be
reordered.
</figureCaption>
<bodyText confidence="0.953187666666667">
model, etc.) are combined in a log-linear model to
obtain the score for the translation e for an input sen-
tence f:
</bodyText>
<equation confidence="0.9428075">
�score(e, f) = exp Ai hi(e,f) (1)
i
</equation>
<bodyText confidence="0.997156545454546">
The weights of the components Ai are set by a
discriminative training method on held-out develop-
ment data (Och, 2003). The basic components used
in our experiments are: (a) two phrase translation
probabilities (both p(e|f) and p(f|e)), (b) two word
translation probabilities (both p(e|f) and p(f|e)),
(c) phrase count, (d) output word count, (e) language
model, (f) distance-based reordering model, and (g)
lexicalized reordering model.
For a more detailed description of this model,
please refer to (Koehn et al., 2005).
</bodyText>
<sectionHeader confidence="0.978251" genericHeader="method">
2 Domain adaption
</sectionHeader>
<bodyText confidence="0.998833142857143">
Since training data for statistical machine translation
is typically collected opportunistically from wher-
ever it is available, the application domain for a ma-
chine translation system may be very different from
the domain of the system’s training data.
For the WMT 2007 shared task, the challenge was
to use a large amount of out-of-domain training data
</bodyText>
<page confidence="0.974566">
224
</page>
<bodyText confidence="0.881324285714286">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227,
Prague, June 2007. c�2007 Association for Computational Linguistics
(about 40 million words) combined with a much
smaller amount of in-domain training data (about 1
million words) to optimize translation performance
on that particular domain. We carried out these ex-
periments on French–English.
</bodyText>
<subsectionHeader confidence="0.952975">
2.1 Only out-of-domain training data
</subsectionHeader>
<bodyText confidence="0.818476333333333">
The first baseline system is trained only on the out-
of-domain Europarl corpus, which has the following
corpus statistics:
</bodyText>
<table confidence="0.693307666666667">
French English
Sentences 1,257,419
Words 37,489,556 33,787,890
</table>
<subsectionHeader confidence="0.936346">
2.2 Only in-domain training data
</subsectionHeader>
<bodyText confidence="0.6931385">
The second baseline system is trained only on the
in-domain NewsCommentary corpus. This corpus
is much smaller:
French English
Sentences 42,884
Words 1,198,041 1,018,503
</bodyText>
<subsectionHeader confidence="0.997676">
2.3 Combined training data
</subsectionHeader>
<bodyText confidence="0.9999795">
To make use of all the training data, the straight-
forward way is to simply concatenate the two train-
ing corpora and use the combined data for both
translation model and language model training. In
our situation, however, the out-of-domain training
data overwhelms the in-domain training data due to
the sheer relative size. Hence, we do not expect the
best performance from this simplistic approach.
</bodyText>
<subsectionHeader confidence="0.9759">
2.4 In-domain language model
</subsectionHeader>
<bodyText confidence="0.999909857142857">
One way to force a drift to the jargon of the target
domain is the use of the language model. In our next
setup, we used only in-domain data for training the
language model. This enables the system to use all
the translation knowledge from the combined cor-
pus, but it gives a preference to word choices that
are dominant in the in-domain training data.
</bodyText>
<subsectionHeader confidence="0.759303">
2.5 Interpolated language model
</subsectionHeader>
<bodyText confidence="0.999929714285714">
Essentially, the goal of our subsequent approaches is
to make use of all the training data, but to include a
preference for the in-domain jargon by giving more
weight to the in-domain training data. This and the
next approach explore methods to bias the language
model, while the final approach biases the transla-
tion model.
</bodyText>
<figure confidence="0.9528778">
perplexity
159 weight
158
157
0.2 0.3 0.4 0.5 0.6 0.7 0.8
</figure>
<figureCaption confidence="0.983158333333333">
Figure 2: Interpolating in-domain and out-of-
domain language models: effect of interpolation
weight on perplexity of LM on development set.
</figureCaption>
<bodyText confidence="0.999758105263158">
We trained two language models, one for each the
out-of-domain and the in-domain training data. Lan-
guage modeling software such as the SRILM toolkit
we used (Stolke, 2002) allows the interpolation of
these language models. When interpolating, we give
the out-of-domain language model a weight in re-
spect to the in-domain language model.
Since we want to obtain a language model that
gives us the best performance on the target domain,
we set this weight so that the perplexity of the de-
velopment set from that target domain is optimized.
We searched for the optimal weight setting by sim-
ply testing a set of weights and focusing on the most
promising range of weights.
Figure 2 displays all the weights we explored dur-
ing this process and the corresponding perplexity of
the resulting language model on the development set
(nc-dev2007). The optimal weight can be picked out
easily from this very smooth curve.
</bodyText>
<subsectionHeader confidence="0.987268">
2.6 Two language models
</subsectionHeader>
<bodyText confidence="0.999996571428571">
The log-linear modeling approach of statistical ma-
chine translation enables a straight-forward combi-
nation of the in-domain and out-of-domain language
models. We included them as two separate fea-
tures, whose weights are set with minimum error
rate training. The relative weight for each model is
set directly by optimizing translation performance.
</bodyText>
<subsectionHeader confidence="0.977909">
2.7 Two translation models
</subsectionHeader>
<bodyText confidence="0.9955406">
Finally, besides biasing the language model to a spe-
cific target domain, we may also bias the translation
model. Here, we take advantage of a feature of the
Moses decoder’s factored translation model frame-
work. In factored translation models, the representa-
</bodyText>
<figure confidence="0.9428542">
164
163
162
161
160
</figure>
<page confidence="0.975508">
225
</page>
<table confidence="0.9995365">
Method %BLEU
Large out-of-domain training data 25.11
Small in-domain training data 25.88
Combined training data 26.69
In-domain language model 27.46
Interpolated language model 27.12
Two language models 27.30
Two translation models 27.64
</table>
<tableCaption confidence="0.999945">
Table 1: Results of domain adaptation experiments
</tableCaption>
<bodyText confidence="0.999928411764706">
tion of words is extended to a vector of factors (e.g.,
surface form, lemma, POS, morphology).
The mapping of an input phrase to an output
phrase is decomposed into several translation and
generation steps, each using a different translation
or generation table, respectively. Such a decomposi-
tion is called a decoding path.
A more recent feature of the factored translation
model framework is the possible use of multiple al-
ternative decoding paths. This alternate decoding
path model was developed by Birch et al. (2007).
For our purposes, we use two decoding paths, each
consisting of only one translation step. One decod-
ing path is the in-domain translation table, and the
other decoding path is the out-of-domain translation
table. Again, respective weights are set with mini-
mum error rate training.
</bodyText>
<sectionHeader confidence="0.992078" genericHeader="method">
3 Domain adaptation results
</sectionHeader>
<bodyText confidence="0.999896466666667">
Table 1 shows results of our domain adaptation ex-
periments on the development test set (nc-devtest-
2007). The results suggest that the language model
is a useful tool for domain adaptation. While train-
ing on all the data is essential for good performance,
using an in-domain language model alone already
gives fairly high performance (27.46). The perfor-
mance with the interpolated language model (27.12)
and two language models (27.30) are similar. All
perform better than the three baseline approaches.
The results also suggest that higher performance
can be obtained by using two translation models
through the Moses decoder’s alternative decoding
path framework. We saw our best results under this
condition (27.64).
</bodyText>
<sectionHeader confidence="0.995974" genericHeader="method">
4 WMT 2007 shared task submissions
</sectionHeader>
<bodyText confidence="0.999470636363636">
We participated in all categories. Given the four lan-
guage pairs, with two translation directions and (ex-
cept for Czech) two test domains, this required us to
build 14 translation systems.
We had access to a fairly large computer cluster to
carry out our experiments over the course of a few
weeks. However, speed issues with the decoder and
load issues on the crowded cluster caused us to take
a few shortcuts. Also, a bug crept in to our English-
French experiments where we used the wrong deto-
kenizer, resulting drop of 2–3 points in %BLEU.
</bodyText>
<subsectionHeader confidence="0.999241">
4.1 Tuning
</subsectionHeader>
<bodyText confidence="0.9999916">
Minimum error rate training is the most time-
consuming aspects of the training process. Due to
time constraints, we did not carry out this step for all
but the Czech systems (a new language for us). For
the other systems, we re-used weight settings from
our last year’s submission.
One of the most crucial outcomes of tuning is a
proper weight setting for output length, which is es-
pecially important for the BLEU score. Since the
training corpus and tokenization changed, our re-
used weights are not always optimal in this respect.
But only in one case we felt compelled to manually
adjust the weight for the word count feature, since
the original setup led to a output/reference length ra-
tio of 0.88 on the development test set.
</bodyText>
<subsectionHeader confidence="0.998063">
4.2 Domain adaptation
</subsectionHeader>
<bodyText confidence="0.997488083333333">
For the Europarl test sets, we did not use any do-
main adaptation techniques, but simply used either
just the Europarl training data or the combined data
— whatever gave the higher score on the develop-
ment test set, although scores differed by only about
0.1–0.2 %BLEU.
In order to be able to re-use the old weights, we
were limited to domain adaptation methods that did
not change the number of components. We decided
to use the interpolated language model method de-
scribed in Section 2.5. For the different language
pairs, optimal interpolation weights differed:
</bodyText>
<table confidence="0.981058388888889">
Language pair Weight for Europarl LM
French–English 0.43
Spanish–English 0.41
German–English 0.40
English–French 0.51
English–Spanish 0.42
English–German 0.45
226
Language pair Europarl NewsCommentary
%BLEU Length NIST %BLEU Length NIST
French–English 32.66 0.96 7.94 28.27 1.03 7.50
Spanish–English 33.26 1.00 7.82 34.17 1.06 8.35
German–English 28.49 0.94 7.32 25.45 1.01 7.19
Czech–English – – – 22.68 0.98 6.96
English–French 26.76 1.08 6.66 24.38 1.02 6.73
English–Spanish 32.55 0.98 7.66 33.59 0.94 8.46
English–German 20.59 0.97 6.18 17.06 1.00 6.04
English–Czech – – – 12.34 1.02 4.85
</table>
<tableCaption confidence="0.999201">
Table 2: Test set performance of our systems: BLEU and NIST scores, and output/reference length ratio.
</tableCaption>
<subsectionHeader confidence="0.997105">
4.3 Training and decoding parameters
</subsectionHeader>
<bodyText confidence="0.999998263157895">
We tried to improve performance by increasing
some of the limits imposed on the training and de-
coding setup. During training, long sentences are
removed from the training data to speed up the
GIZA++ word alignment process. Traditionally, we
worked with a sentence length limit of 40. We found
that increasing this limit to about 80 gave better re-
sults without causing undue problems with running
the word alignment (GIZA++ increasingly fails and
runs much slower with long sentences).
We also tried to increase beam sizes and the
limit on the number of translation options per cov-
erage span (ttable-limit). This has shown to be suc-
cessful in our experiments with Arabic–English and
Chinese–English systems. Surprisingly, increasing
the maximum stack size to 1000 (from 200) and
ttable-limit to 100 (from 20) has barely any ef-
fect on translation performance. The %BLEU score
changed only by less than 0.05, and often worsened.
</bodyText>
<subsectionHeader confidence="0.989397">
4.4 German–English system
</subsectionHeader>
<bodyText confidence="0.999983714285714">
The German–English language pair is especially
challenging due to the large differences in word or-
der. Collins et al. (2005) suggest a method to reorder
the German input before translating using a set of
manually crafted rules. In our German–English sub-
missions, this is done both to the training data and
the input to the machine translation system.
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99997125">
Our submission to the WMT 2007 shared task is a
fairly straight-forward use of the Moses MT system
using default parameters. In a sense, we submitted
a baseline performance of this system. BLEU and
NIST scores for all our systems on the test sets are
displayed in Table 2. Compared to other submitted
systems, these are very good scores, often the best
or second highest scores for these tasks.
We made a special effort in two areas: We ex-
plored domain adaptation methods for the News-
Commentary test sets and we used reordering rules
for the German–English language pair.
</bodyText>
<sectionHeader confidence="0.995348" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997838">
This work was supported in part under the GALE program
of the Defense Advanced Research Projects Agency, Contract
No. HR0011-06-C-0022 and in part under the EuroMatrix
project funded by the European Commission (6th Framework
Programme).
</bodyText>
<sectionHeader confidence="0.999376" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997235">
Birch, A., Osborne, M., and Koehn, P. (2007). CCG supertags
in factored statistical machine translation. In Proceedings
of the Workshop on Statistical Machine Translation, Prague.
Association for Computational Linguistics.
Collins, M., Koehn, P., and Kucerova, I. (2005). Clause re-
structuring for statistical machine translation. In Proceed-
ings of the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL’05), pages 531–540, Ann Arbor,
Michigan. Association for Computational Linguistics.
Koehn, P., Axelrod, A., Mayne, A. B., Callison-Burch, C., Os-
borne, M., and Talbot, D. (2005). Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evaluation. In
Proc. of the International Workshop on Spoken Language
Translation.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico,
M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R.,
Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open source toolkit for statistical machine transla-
tion. In Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics, demonstation session.
Koehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase
based translation. In Proceedings of the Joint Conference on
Human Language Technologies and the Annual Meeting of
the North American Chapter of the Association of Computa-
tional Linguistics (HLT-NAACL).
Och, F. J. (2003). Minimum error rate training in statistical
machine translation. In Hinrichs, E. and Roth, D., editors,
Proceedings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167.
Stolke, A. (2002). SRILM - an extensible language modeling
toolkit. In Proceedings of the International Conference on
Spoken Language Processing.
</reference>
<page confidence="0.99794">
227
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000309">
<title confidence="0.999594">Experiments in Domain Adaptation for Statistical Machine Translation</title>
<author confidence="0.99539">Philipp Koehn</author>
<author confidence="0.99539">Josh</author>
<email confidence="0.866875">pkoehn@inf.ed.ac.uk,</email>
<affiliation confidence="0.916475">School of University of 2 Buccleuch Place, Edinburgh EH8</affiliation>
<address confidence="0.869638">Scotland, United Kingdom</address>
<abstract confidence="0.995487266666667">The special challenge of the WMT 2007 shared task was domain adaptation. We took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain (here: news commentary), when most of the training data is from a different domain (here: European Parliament speeches). This paper also gives a description of the submission of the University of Edinburgh to the shared task. 1 Our framework: the Moses MT system The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop. It is now used at several academic institutions as the basic infrastructure for statistical machine translation research. The Moses system is an implementation of the phrase-based machine translation approach (Koehn et al., 2003). In this approach, an input sentence is first split into text chunks (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table. Phrases may be reordered, but typically a reordering limit (in our experiments a maximum movement over 6 words) is used. See Figure 1 for an illustration. Phrase translation probabilities, reordering probabilities and language model probabilities are combined to give each possible sentence translation a score. The best-scoring translation is searched for by the decoding algorithm and outputted by the system as the best translation. The different system compotranslation probabilities, language Figure 1: Phrase-based statistical machine translation model: Input is split into text chunks (phrases) which are mapped using a large phrase translation table. Phrases are mapped one-to-one, and may be reordered. model, etc.) are combined in a log-linear model to the score for the translation an input sen- = i weights of the components set by a discriminative training method on held-out development data (Och, 2003). The basic components used in our experiments are: (a) two phrase translation (both (b) two word probabilities (both (c) phrase count, (d) output word count, (e) language model, (f) distance-based reordering model, and (g) lexicalized reordering model. For a more detailed description of this model, please refer to (Koehn et al., 2005). 2 Domain adaption Since training data for statistical machine translation is typically collected opportunistically from wherever it is available, the application domain for a machine translation system may be very different from the domain of the system’s training data. For the WMT 2007 shared task, the challenge was to use a large amount of out-of-domain training data 224 of the Second Workshop on Statistical Machine pages 224–227, June 2007. Association for Computational Linguistics (about 40 million words) combined with a much smaller amount of in-domain training data (about 1 million words) to optimize translation performance on that particular domain. We carried out these experiments on French–English. 2.1 Only out-of-domain training data The first baseline system is trained only on the outof-domain Europarl corpus, which has the following corpus statistics: French English Sentences 1,257,419 Words 37,489,556 33,787,890 2.2 Only in-domain training data The second baseline system is trained only on the in-domain NewsCommentary corpus. This corpus is much smaller: French English Sentences 42,884 Words 1,198,041 1,018,503 2.3 Combined training data To make use of all the training data, the straightforward way is to simply concatenate the two training corpora and use the combined data for both translation model and language model training. In our situation, however, the out-of-domain training data overwhelms the in-domain training data due to the sheer relative size. Hence, we do not expect the best performance from this simplistic approach. 2.4 In-domain language model One way to force a drift to the jargon of the target domain is the use of the language model. In our next setup, we used only in-domain data for training the language model. This enables the system to use all the translation knowledge from the combined corpus, but it gives a preference to word choices that are dominant in the in-domain training data. 2.5 Interpolated language model Essentially, the goal of our subsequent approaches is to make use of all the training data, but to include a preference for the in-domain jargon by giving more weight to the in-domain training data. This and the next approach explore methods to bias the language model, while the final approach biases the translation model. perplexity 159 weight 158 157 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Figure 2: Interpolating in-domain and out-ofdomain language models: effect of interpolation weight on perplexity of LM on development set. We trained two language models, one for each the out-of-domain and the in-domain training data. Language modeling software such as the SRILM toolkit we used (Stolke, 2002) allows the interpolation of these language models. When interpolating, we give the out-of-domain language model a weight in respect to the in-domain language model. Since we want to obtain a language model that gives us the best performance on the target domain, we set this weight so that the perplexity of the development set from that target domain is optimized. We searched for the optimal weight setting by simply testing a set of weights and focusing on the most promising range of weights. Figure 2 displays all the weights we explored during this process and the corresponding perplexity of the resulting language model on the development set (nc-dev2007). The optimal weight can be picked out easily from this very smooth curve. 2.6 Two language models The log-linear modeling approach of statistical machine translation enables a straight-forward combination of the in-domain and out-of-domain language models. We included them as two separate features, whose weights are set with minimum error rate training. The relative weight for each model is set directly by optimizing translation performance. 2.7 Two translation models Finally, besides biasing the language model to a specific target domain, we may also bias the translation model. Here, we take advantage of a feature of the Moses decoder’s factored translation model frame-</abstract>
<note confidence="0.75918725">In factored translation models, the representa- 164 163 162 161 160 225 Method Large out-of-domain training data 25.11 Small in-domain training data 25.88 Combined training data 26.69 In-domain language model 27.46 Interpolated language model 27.12 Two language models 27.30 Two translation models 27.64 Table 1: Results of domain adaptation experiments</note>
<abstract confidence="0.996243835616438">tion of words is extended to a vector of factors (e.g., surface form, lemma, POS, morphology). The mapping of an input phrase to an output phrase is decomposed into several translation and generation steps, each using a different translation or generation table, respectively. Such a decomposition is called a decoding path. A more recent feature of the factored translation model framework is the possible use of multiple alternative decoding paths. This alternate decoding path model was developed by Birch et al. (2007). For our purposes, we use two decoding paths, each consisting of only one translation step. One decoding path is the in-domain translation table, and the other decoding path is the out-of-domain translation table. Again, respective weights are set with minimum error rate training. 3 Domain adaptation results Table 1 shows results of our domain adaptation experiments on the development test set (nc-devtest- 2007). The results suggest that the language model is a useful tool for domain adaptation. While training on all the data is essential for good performance, using an in-domain language model alone already gives fairly high performance (27.46). The performance with the interpolated language model (27.12) and two language models (27.30) are similar. All perform better than the three baseline approaches. The results also suggest that higher performance can be obtained by using two translation models through the Moses decoder’s alternative decoding path framework. We saw our best results under this condition (27.64). 4 WMT 2007 shared task submissions We participated in all categories. Given the four lanpairs, with two translation directions and (except for Czech) two test domains, this required us to build 14 translation systems. We had access to a fairly large computer cluster to carry out our experiments over the course of a few weeks. However, speed issues with the decoder and load issues on the crowded cluster caused us to take a few shortcuts. Also, a bug crept in to our English- French experiments where we used the wrong detokenizer, resulting drop of 2–3 points in %BLEU. 4.1 Tuning Minimum error rate training is the most timeconsuming aspects of the training process. Due to time constraints, we did not carry out this step for all but the Czech systems (a new language for us). For the other systems, we re-used weight settings from our last year’s submission. One of the most crucial outcomes of tuning is a proper weight setting for output length, which is esimportant for the Since the training corpus and tokenization changed, our reused weights are not always optimal in this respect. But only in one case we felt compelled to manually adjust the weight for the word count feature, since the original setup led to a output/reference length ratio of 0.88 on the development test set. 4.2 Domain adaptation For the Europarl test sets, we did not use any domain adaptation techniques, but simply used either just the Europarl training data or the combined data — whatever gave the higher score on the development test set, although scores differed by only about In order to be able to re-use the old weights, we were limited to domain adaptation methods that did not change the number of components. We decided to use the interpolated language model method described in Section 2.5. For the different language pairs, optimal interpolation weights differed:</abstract>
<note confidence="0.835264166666667">Language pair Weight for Europarl LM French–English 0.43 Spanish–English 0.41 German–English 0.40 English–French 0.51 English–Spanish 0.42 English–German 0.45 226 Language pair Europarl NewsCommentary Length Length French–English 32.66 0.96 7.94 28.27 1.03 7.50 Spanish–English 33.26 1.00 7.82 34.17 1.06 8.35 German–English 28.49 0.94 7.32 25.45 1.01 7.19 Czech–English – – – 22.68 0.98 6.96 English–French 26.76 1.08 6.66 24.38 1.02 6.73 English–Spanish 32.55 0.98 7.66 33.59 0.94 8.46 English–German 20.59 0.97 6.18 17.06 1.00 6.04 English–Czech – – – 12.34 1.02 4.85</note>
<abstract confidence="0.997778571428571">2: Test set performance of our systems: and output/reference length ratio. 4.3 Training and decoding parameters We tried to improve performance by increasing some of the limits imposed on the training and decoding setup. During training, long sentences are removed from the training data to speed up the GIZA++ word alignment process. Traditionally, we worked with a sentence length limit of 40. We found that increasing this limit to about 80 gave better results without causing undue problems with running the word alignment (GIZA++ increasingly fails and runs much slower with long sentences). We also tried to increase beam sizes and the limit on the number of translation options per coverage span (ttable-limit). This has shown to be successful in our experiments with Arabic–English and Chinese–English systems. Surprisingly, increasing the maximum stack size to 1000 (from 200) and ttable-limit to 100 (from 20) has barely any efon translation performance. The changed only by less than 0.05, and often worsened. 4.4 German–English system The German–English language pair is especially challenging due to the large differences in word order. Collins et al. (2005) suggest a method to reorder the German input before translating using a set of manually crafted rules. In our German–English submissions, this is done both to the training data and the input to the machine translation system. 5 Conclusions Our submission to the WMT 2007 shared task is a fairly straight-forward use of the Moses MT system using default parameters. In a sense, we submitted baseline performance of this system. for all our systems on the test sets are displayed in Table 2. Compared to other submitted systems, these are very good scores, often the best or second highest scores for these tasks. We made a special effort in two areas: We explored domain adaptation methods for the News- Commentary test sets and we used reordering rules for the German–English language pair.</abstract>
<note confidence="0.871407512820513">Acknowledgments This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme). References Birch, A., Osborne, M., and Koehn, P. (2007). CCG supertags factored statistical machine translation. In the Workshop on Statistical Machine Prague. Association for Computational Linguistics. Collins, M., Koehn, P., and Kucerova, I. (2005). Clause refor statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Com- Linguistics pages 531–540, Ann Arbor, Michigan. Association for Computational Linguistics. Koehn, P., Axelrod, A., Mayne, A. B., Callison-Burch, C., Osborne, M., and Talbot, D. (2005). Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proc. of the International Workshop on Spoken Language Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007). Moses: Open source toolkit for statistical machine transla- In of the Annual Meeting of the Associafor Computational Linguistics, demonstation Koehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase translation. In of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computa- Linguistics Och, F. J. (2003). Minimum error rate training in statistical machine translation. In Hinrichs, E. and Roth, D., editors, Proceedings of the 41st Annual Meeting of the Association Computational pages 160–167. Stolke, A. (2002). SRILM an extensible language modeling In of the International Conference on Language 227</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Birch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>CCG supertags in factored statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague.</location>
<contexts>
<context position="7661" citStr="Birch et al. (2007)" startWordPosition="1201" endWordPosition="1204">uage models 27.30 Two translation models 27.64 Table 1: Results of domain adaptation experiments tion of words is extended to a vector of factors (e.g., surface form, lemma, POS, morphology). The mapping of an input phrase to an output phrase is decomposed into several translation and generation steps, each using a different translation or generation table, respectively. Such a decomposition is called a decoding path. A more recent feature of the factored translation model framework is the possible use of multiple alternative decoding paths. This alternate decoding path model was developed by Birch et al. (2007). For our purposes, we use two decoding paths, each consisting of only one translation step. One decoding path is the in-domain translation table, and the other decoding path is the out-of-domain translation table. Again, respective weights are set with minimum error rate training. 3 Domain adaptation results Table 1 shows results of our domain adaptation experiments on the development test set (nc-devtest2007). The results suggest that the language model is a useful tool for domain adaptation. While training on all the data is essential for good performance, using an in-domain language model </context>
</contexts>
<marker>Birch, Osborne, Koehn, 2007</marker>
<rawString>Birch, A., Osborne, M., and Koehn, P. (2007). CCG supertags in factored statistical machine translation. In Proceedings of the Workshop on Statistical Machine Translation, Prague. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>531--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="12403" citStr="Collins et al. (2005)" startWordPosition="1971" endWordPosition="1974"> slower with long sentences). We also tried to increase beam sizes and the limit on the number of translation options per coverage span (ttable-limit). This has shown to be successful in our experiments with Arabic–English and Chinese–English systems. Surprisingly, increasing the maximum stack size to 1000 (from 200) and ttable-limit to 100 (from 20) has barely any effect on translation performance. The %BLEU score changed only by less than 0.05, and often worsened. 4.4 German–English system The German–English language pair is especially challenging due to the large differences in word order. Collins et al. (2005) suggest a method to reorder the German input before translating using a set of manually crafted rules. In our German–English submissions, this is done both to the training data and the input to the machine translation system. 5 Conclusions Our submission to the WMT 2007 shared task is a fairly straight-forward use of the Moses MT system using default parameters. In a sense, we submitted a baseline performance of this system. BLEU and NIST scores for all our systems on the test sets are displayed in Table 2. Compared to other submitted systems, these are very good scores, often the best or sec</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Collins, M., Koehn, P., and Kucerova, I. (2005). Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 531–540, Ann Arbor, Michigan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>A Axelrod</author>
<author>A B Mayne</author>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>D Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proc. of the International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="2714" citStr="Koehn et al., 2005" startWordPosition="417" endWordPosition="420">btain the score for the translation e for an input sentence f: �score(e, f) = exp Ai hi(e,f) (1) i The weights of the components Ai are set by a discriminative training method on held-out development data (Och, 2003). The basic components used in our experiments are: (a) two phrase translation probabilities (both p(e|f) and p(f|e)), (b) two word translation probabilities (both p(e|f) and p(f|e)), (c) phrase count, (d) output word count, (e) language model, (f) distance-based reordering model, and (g) lexicalized reordering model. For a more detailed description of this model, please refer to (Koehn et al., 2005). 2 Domain adaption Since training data for statistical machine translation is typically collected opportunistically from wherever it is available, the application domain for a machine translation system may be very different from the domain of the system’s training data. For the WMT 2007 shared task, the challenge was to use a large amount of out-of-domain training data 224 Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227, Prague, June 2007. c�2007 Association for Computational Linguistics (about 40 million words) combined with a much smaller amount of in-d</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Koehn, P., Axelrod, A., Mayne, A. B., Callison-Burch, C., Osborne, M., and Talbot, D. (2005). Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proc. of the International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics, demonstation session.</booktitle>
<contexts>
<context position="770" citStr="Koehn et al., 2007" startWordPosition="114" endWordPosition="117">of Informatics University of Edinburgh 2 Buccleuch Place, Edinburgh EH8 9LW Scotland, United Kingdom Abstract The special challenge of the WMT 2007 shared task was domain adaptation. We took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain (here: news commentary), when most of the training data is from a different domain (here: European Parliament speeches). This paper also gives a description of the submission of the University of Edinburgh to the shared task. 1 Our framework: the Moses MT system The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop. It is now used at several academic institutions as the basic infrastructure for statistical machine translation research. The Moses system is an implementation of the phrase-based machine translation approach (Koehn et al., 2003). In this approach, an input sentence is first split into text chunks (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table. Phrases may be reordered, but typically a reordering lim</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007). Moses: Open source toolkit for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, demonstation session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="1132" citStr="Koehn et al., 2003" startWordPosition="169" endWordPosition="172">ng data is from a different domain (here: European Parliament speeches). This paper also gives a description of the submission of the University of Edinburgh to the shared task. 1 Our framework: the Moses MT system The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop. It is now used at several academic institutions as the basic infrastructure for statistical machine translation research. The Moses system is an implementation of the phrase-based machine translation approach (Koehn et al., 2003). In this approach, an input sentence is first split into text chunks (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table. Phrases may be reordered, but typically a reordering limit (in our experiments a maximum movement over 6 words) is used. See Figure 1 for an illustration. Phrase translation probabilities, reordering probabilities and language model probabilities are combined to give each possible sentence translation a score. The best-scoring translation is searched for by the decoding algorithm and outputted by the system as the </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase based translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<editor>In Hinrichs, E. and Roth, D., editors,</editor>
<contexts>
<context position="2311" citStr="Och, 2003" startWordPosition="359" endWordPosition="360">outputted by the system as the best translation. The different system components hi (phrase translation probabilities, language Figure 1: Phrase-based statistical machine translation model: Input is split into text chunks (phrases) which are mapped using a large phrase translation table. Phrases are mapped one-to-one, and may be reordered. model, etc.) are combined in a log-linear model to obtain the score for the translation e for an input sentence f: �score(e, f) = exp Ai hi(e,f) (1) i The weights of the components Ai are set by a discriminative training method on held-out development data (Och, 2003). The basic components used in our experiments are: (a) two phrase translation probabilities (both p(e|f) and p(f|e)), (b) two word translation probabilities (both p(e|f) and p(f|e)), (c) phrase count, (d) output word count, (e) language model, (f) distance-based reordering model, and (g) lexicalized reordering model. For a more detailed description of this model, please refer to (Koehn et al., 2005). 2 Domain adaption Since training data for statistical machine translation is typically collected opportunistically from wherever it is available, the application domain for a machine translation </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, F. J. (2003). Minimum error rate training in statistical machine translation. In Hinrichs, E. and Roth, D., editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="5432" citStr="Stolke, 2002" startWordPosition="849" endWordPosition="850">ining data, but to include a preference for the in-domain jargon by giving more weight to the in-domain training data. This and the next approach explore methods to bias the language model, while the final approach biases the translation model. perplexity 159 weight 158 157 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Figure 2: Interpolating in-domain and out-ofdomain language models: effect of interpolation weight on perplexity of LM on development set. We trained two language models, one for each the out-of-domain and the in-domain training data. Language modeling software such as the SRILM toolkit we used (Stolke, 2002) allows the interpolation of these language models. When interpolating, we give the out-of-domain language model a weight in respect to the in-domain language model. Since we want to obtain a language model that gives us the best performance on the target domain, we set this weight so that the perplexity of the development set from that target domain is optimized. We searched for the optimal weight setting by simply testing a set of weights and focusing on the most promising range of weights. Figure 2 displays all the weights we explored during this process and the corresponding perplexity of </context>
</contexts>
<marker>Stolke, 2002</marker>
<rawString>Stolke, A. (2002). SRILM - an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>