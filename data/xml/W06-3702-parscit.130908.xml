<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014793">
<title confidence="0.9993255">
Evaluating Task Performance for a Unidirectional Controlled Language
Medical Speech Translation System
</title>
<author confidence="0.985089">
Nikos Chatzichrisafis, Pierrette Bouillon, Manny Rayner, Marianne Santaholma,
Marianne Starlander
</author>
<affiliation confidence="0.999063">
University of Geneva, TIM/ISSCO
</affiliation>
<address confidence="0.980772">
40 bvd du Pont-d&apos;Arve, CH-1211 Geneva 4, Switzerland
</address>
<email confidence="0.853675666666667">
Nikos.Chatzichrisafis@vozZup.com, Pierrette.Bouillon@issco.unige.ch,
Emmanuel.Rayner@issco.unige.ch, Marianne.Santaholma@eti.unige.ch,
Marianne.Starlander@eti.unige.ch
</email>
<author confidence="0.991966">
Beth Ann Hockey
</author>
<affiliation confidence="0.987574">
UCSC
NASA Ames Research Center
</affiliation>
<address confidence="0.96992">
Moffett Field, CA 94035
</address>
<email confidence="0.973201">
bahockey@email.arc.nasa.gov
</email>
<sectionHeader confidence="0.992301" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999718125">
We present a task-level evaluation of the
French to English version of MedSLT, a
medium-vocabulary unidirectional con-
trolled language medical speech transla-
tion system designed for doctor-patient
diagnosis interviews. Our main goal was
to establish task performance levels of
novice users and compare them to expert
users. Tests were carried out on eight
medical students with no previous expo-
sure to the system, with each student us-
ing the system for a total of three
sessions. By the end of the third session,
all the students were able to use the sys-
tem confidently, with an average task
completion time of about 4 minutes.
</bodyText>
<sectionHeader confidence="0.9989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998812291139241">
Medical applications have emerged as one of the
most promising application areas for spoken lan-
guage translation, but there is still little agreement
about the question of architectures. There are in
particular two architectural dimensions which we
will address: general processing strategy (statistical
or grammar-based), and top-level translation func-
tionality (unidirectional or bidirectional transla-
tion). Given the current state of the art in
recognition and machine translation technology,
what is the most appropriate combination of
choices along these two dimensions?
Reflecting current trends, a common approach
for speech translation systems is the statistical one.
Statistical translation systems rely on parallel cor-
pora of source and target language texts, from
which a translation model is trained. However, this
is not necessarily the best alternative in safety-
critical medical applications. Anecdotally, many
doctors express reluctance to trust a translation
device whose output is not readily predictable, and
most of the speech translation systems which have
reached the stage of field testing rely on various
types of grammar-based recognition and rule-based
translation (Phraselator, 2006; S-MINDS, 2006;
MedBridge, 2006). Even though statistical systems
exhibit many desirable properties (purely data-
driven, domain independence), grammar-based
systems utilizing probabilistic context-free gram-
mar tuning appear to deliver better results when
training data is sparse (Rayner et al., 2005a).
One drawback of grammar-based systems is that
out-of-coverage utterances will be neither recog-
nized nor translated, an objection that critics have
sometimes painted as decisive. It is by no means
obvious, however, that restricted coverage is such
a serious problem. In text processing, work on sev-
eral generations of controlled language systems has
developed a range of techniques for keeping users
within the bounds of system coverage (Kittredge,
2003; Mitamura, 1999). If these techniques work
for text processing, it is surely not inconceivable
that variants of them will be equally successful for
spoken language applications. Users are usually
able to adapt to a controlled language system given
enough time. The critical questions are how to
provide efficient support to guide them towards the
system&apos;s coverage, and how much time they will
then need before they have acclimatized.
With regard to top-level translation functional-
ity, the choice is between unidirectional and bidi-
rectional systems. Bidirectional systems are
certainly possible today1, but the arguments in fa-
vor of them are not as clear-cut as might first ap-
pear. Ceteris paribus, doctors would certainly
prefer bidirectional systems; in particular, medical
students are trained to conduct examination dia-
logues using “open questions” (WH-questions),
and to avoid leading the patient by asking YN-
questions.
The problem with a bidirectional system is,
however, that open questions only really work well
if the system can reliably handle a broad spectrum
of replies from the patients, which is over-
optimistic given the current state of the art. In prac-
tice, the system&apos;s coverage is always more or less
restricted, and some experimentation is required
before the user can understand what language it is
capable of handling. A doctor, who uses the system
regularly, will acquire the necessary familiarity.
The same might be true for a few patients, if spe-
cial circumstances mean that they encounter
speech translation applications reasonably fre-
quently. Most patients, however, will have had no
previous exposure to the system, and may be un-
willing to use a type of technology which they
have trouble understanding.
A unidirectional system, in which the doctor
mostly asks YN-questions, will never be ideal. If,
</bodyText>
<footnote confidence="0.805535">
1 For example, the S-MINDS system (S-MINDS, 2006)
offers bidirectional translation.
</footnote>
<bodyText confidence="0.99982068">
however, the doctor can become proficient in using
it, it may still be very much better than the alterna-
tive of no translation assistance at all.
To summarize, today’s technology definitely
lets us build unidirectional grammar-based medical
speech translation systems which work for regular
users who have had time to adapt to their limita-
tions. While bidirectional systems are possible, the
case for them is less obvious, since users on the
patient side may not in practice be able to use them
effectively.
In this paper, we will empirically investigate the
ability of medical students to adapt to the coverage
of unidirectional spoken language translation sys-
tem. We report a series of experiments, carried out
using a French to English speech translation sys-
tem, in which medical students with no previous
experience to the system were asked to use it to
carry out a series of verbal examinations on sub-
jects who were simulating the symptoms of various
types of medical conditions. Evaluation will be
focused on usability. We primarily want to know
how quickly subjects learn to use the system, and
how their performance compares to that of expert
users.
</bodyText>
<sectionHeader confidence="0.976113" genericHeader="method">
2 The MedSLT system
</sectionHeader>
<bodyText confidence="0.998480906250001">
MedSLT (MedSLT, 2005; Bouillon et al., 2005)
is a unidirectional, grammar-based medical speech
translation system intended for use in doctor-
patient diagnosis dialogues. The system is built on
top of Regulus (Regulus, 2006), an Open Source
platform for developing grammar-based speech
applications. Regulus supports rapid construction
of complex grammar-based language models using
an example-based method (Rayner et al., 2003;
Rayner et al., 2006), which extracts most of the
structure of the model from a general linguistically
motivated resource grammar. Regulus-based rec-
ognizers are reasonably easy to maintain, and
grammar structure is shared automatically across
different subdomains. Resource grammars are now
available for several languages, including English,
Japanese (Rayner et al., 2005b), French (Bouillon
et al., 2006) and Spanish.
MedSLT includes a help module, whose purpose
is to add robustness to the system and guide the
user towards the supported coverage. The help
module uses a second backup recognizer, equipped
with a statistical language model; it matches the
results from this second recognizer against a cor-
pus of utterances, which are within system cover-
age and have already been judged to give correct
translations. In previous studies (Rayner et al.,
2005a; Starlander et al., 2005), we showed that the
grammar-based recognizer performs much better
than the statistical one on in-coverage utterances,
and rather worse on out-of-coverage ones. We also
found that having the help module available ap-
proximately doubled the speed at which subjects
learned to use the system, measured as the average
difference in semantic error rate between the re-
sults for their first quarter-session and their last
quarter-session. It is also possible to recover from
recognition errors by selecting one of the displayed
help sentences; in the cited studies, we found that
this increased the number of acceptably processed
utterances by about 10%.
The version of MedSLT used for the experi-
ments described in the present paper was config-
ured to translate from spoken French into spoken
English in the headache subdomain. Coverage is
based on standard headache-related examination
questions obtained from a doctor, and consists
mostly of yes/no questions. WH-questions and el-
liptical constructions are also supported. A typical
short session with MedSLT might be as follows:
- is the pain in the side of the head?
- does the pain radiate to the neck?
- to the jaw?
- do you usually have headaches in the morn-
ing ?
The recognizer’s vocabulary is about 1000 sur-
face words; on in-grammar material, Word Error
Rate is about 8% and semantic error rate (per ut-
terance) about 10% (Bouillon et al., 2006). Both
the main grammar-based recognizer and the statis-
tical recognizer used by the help system were
trained from the same corpus of about 975 utter-
ances. Help sentences were also taken from this
corpus.
</bodyText>
<sectionHeader confidence="0.997927" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999955191780822">
In previous work, we have shown how to build a
robust and extendable speech translation system.
We have focused on performance metrics defined
in terms of recognition and translation quality, and
tested the system on naïve users without any medi-
cal background (Bouillon et al., 2005; Rayner et
al., 2005a; Starlander et al., 2005).
In this paper, our primary goal was rather to fo-
cus on task performance evaluation using plausible
potential users. The basic methodology used is
common in evaluating usability in software sys-
tems in general, and spoken language systems in
particular (Cohen et. al 2000). We defined a simu-
lated situation, where a French-speaking doctor
was required to carry out a verbal examination of
an English-speaking patient who claimed to be suf-
fering from a headache, using the MedSLT system
to translate all their questions. The patients were
played by members of the development team, who
had been trained to answer questions consistently
with the symptoms of different medical conditions
which could cause headaches. We recruited eight
native French-speaking medical students to play
the part of the doctor. All of the students had com-
pleted at least four years of medical school; five of
them were already familiar with the symptoms of
different types of headaches, and were experienced
in real diagnosis situations.
The experiment was designed to study how well
users were able to perform the task using the
MedSLT system. In particular, we wished to de-
termine how quickly they could adapt to the re-
stricted language and limited coverage of the
system. As a comparison point, representing near-
perfect performance, we also carried out the same
test on two developers who had been active in im-
plementing the system, and were familiar with its
coverage.
Since it seemed reasonable to assume that most
users would not interact with the system on a daily
basis, we conducted testing in three sessions, with
an interval of two days between each session. At
the beginning of the first session, subjects were
given a standardized 10-minute introduction to the
system. This consisted of instruction on how to set
up the microphone, a detailed description of the
MedSLT push-to-talk interface, and a video clip
showing the system in action. At the end of the
presentation, the subject was given four sample
sentences to get familiar with the system.
After the training was completed, subjects were
asked to play the part of a doctor, and conduct an
examination through the system. Their task was to
identify the headache-related condition simulated
by the “patient”, out of nine possible conditions.
Subjects were given definitions of the simulated
headache types, which included conceptual infor-
mation about location, duration, frequency, onset
and possible other symptoms the particular type of
headache might exhibit.
Subjects were instructed to signal the conclusion
of their examination when they were sure about the
type of simulated headache. The time required to
reach a conclusion was noted in the experiment
protocols by the experiment supervisor.
The subjects repeated the same diagnosis task on
different predetermined sets of simulated condi-
tions during the second and third sessions. The ses-
sions were concluded either when a time limit of
30 minutes was reached, or when the subject com-
pleted three headache diagnoses. At the end of the
third session, the subject was asked to fill out a
questionnaire.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999876666666667">
Performance of a speech translation system is
best evaluated by looking at system performance
as a whole, and not separately for each subcompo-
nent in the systems processing pipeline (Rayner et.
al. 2000, pp. 297-pp. 312). In this paper, we conse-
quently focus our analysis on objective and subjec-
tive usability-oriented measures.
In Section 4.1, we present objective usability
measures obtained by analyzing user-system inter-
actions and measuring task performance. In Sec-
tion 4.2, we present subjective usability figures and
a preliminary analysis of translation quality.
</bodyText>
<subsectionHeader confidence="0.998635">
4.1 Objective Usability Figures
4.1.1 Analysis of User Interactions
</subsectionHeader>
<bodyText confidence="0.999835691176471">
Most of our analysis is based on data from the
MedSLT system log, which records all interactions
between the user and the system. An interaction is
initiated when the user presses the “Start Recogni-
tion” button. The system then attempts to recog-
nize what the user says. If it can do so, it next
attempts to show the user how it has interpreted the
recognition result, by first translating it into the
Interlingua, and then translating it back into the
source language (in this case, French). If the user
decides that the back-translation is correct, they
press the “Translate” button. This results in the
system attempting to translate the Interlingua rep-
resentation into the target language (in this case,
English), and speak it using a Text-To-Speech en-
gine. The system also displays a list of “help sen-
tences”, consisting of examples that are known to
be within coverage, and which approximately
match the result of performing recognition with the
statistical language model. The user has the option
of choosing a help sentence from the list, using the
mouse, and submitting this to translation instead.
We classify each interaction as either “success-
ful” or “unsuccessful”. An interaction is defined to
be unsuccessful if either
i) the user re-initiates recognition without
asking the system for a translation, or
ii) the system fails to produce a correct
translation or back translation.
Our definition of “unsuccessful interaction” in-
cludes instances where users accidentally press the
wrong button (i.e. “Start Recognition” instead of
“Translate”), press the button and then say nothing,
or press the button and change their minds about
what they want to ask half way through. We ob-
served all of these behaviors during the tests.
Interactions where the system produced a trans-
lation were counted as successful, irrespective of
whether the translation came directly from the
user’s spoken input or from the help list. In at least
some examples, we found that when the translation
came from a help sentence it did not correspond
directly to the sentence the user had spoken; to our
surprise, it could even be the case that the help sen-
tence expressed the directly opposite question to
the one the user had actually asked. This type of
interaction was usually caused by some deficiency
in the system, normally bad recognition or missing
coverage. Our informal observation, however, was
that, when this kind of thing happened, the user
perceived the help module positively: it enabled
them to elicit at least some information from the
patient, and was less frustrating than being forced
to ask the question again.
Table I to Table III show the number of total in-
teractions per session, the proportion of successful
interactions, and the proportion of interactions
completed by selecting a sentence from the help
list. The total number of interactions required to
complete a session decreased over the three ses-
sions, declining from an average of 98.6 interac-
tions in the first session to 63.4 in the second (36%
relative) and 53.9 in the third (45% relative). It is
interesting to note that interactions involving the
help system did not decrease in frequency, but re-
mained almost constant over the first two sessions
(15.5% and 14.0%), and were in fact most com-
mon during the third session (21.7%).
</bodyText>
<table confidence="0.999392818181818">
Session 1
Subject Interactions % Successful % Help
User 1 57 56.1% 0.0%
User 2 98 52.0% 25.5%
User 3 91 63.7% 15.4%
User 4 156 69.9% 10.3%
User 5 86 64.0% 22.1%
User 6 134 47.0% 19.4%
User 7 56 53.6% 5.4%
User 8 111 63.1% 26.1%
AVG 98.6 58.7% 15.5%
</table>
<tableCaption confidence="0.789692333333333">
Table I Total interaction rounds, percentage of
successful interactions, and interactions involving
the help system by subject for the 1st session
</tableCaption>
<table confidence="0.999715909090909">
Session 2
Subject Interactions % Successful % Help
User 1 50 74.0% 2.0%
User 2 63 55.6% 27.0%
User 3 34 88.2% 23.5%
User 4 96 57.3% 17.7%
User 5 64 65.6% 21.9%
User 6 93 68.8% 10.8%
User 7 48 60.4% 4.2%
User 8 59 79.7% 5.1%
AVG 63.4 68.7% 14.0%
</table>
<tableCaption confidence="0.623585666666667">
Table II Total interaction rounds, percentage of
successful interactions, and interactions involving
the help system by subject for the 2nd session
</tableCaption>
<table confidence="0.99914">
Session 3
Subject Interactions % Successful % Help
User 1 33 90.9% 33.3%
User 2 57 56.1% 22.8%
User 3 48 72.9% 29.2%
User 4 67 70.2% 16.4%
User 5 68 73.5% 27.9%
User 6 60 70.0% 6.7%
User 7 41 65.9% 14.6%
User 8 57 56.1% 22.8%
AVG 53.9 69.5% 21.7%
</table>
<tableCaption confidence="0.780177">
Table III Total interaction rounds, percentage of
</tableCaption>
<bodyText confidence="0.990317538461538">
successful interactions, and interactions involving
the help system by subject for the 3rd session
In order to establish a performance baseline, we
also analyzed interaction data for two expert users,
who performed the same experiment. The expert
users were two native French-speaking system de-
velopers, which were both familiar with the diag-
nosis domain. Table IV summarizes the results of
those users. One of our expert users, listed as Ex-
pert 2, is the French grammar developer, and had
no failed interactions. This confirms that recogni-
tion is very accurate for users who know the cov-
erage.
</bodyText>
<table confidence="0.9997164">
Session 1 / Expert Users
Subject Interactions % Successful % Help
Expert 1 36 77.8% 13.9%
Expert 2 30 100.0% 3.3%
AVG 33 88.9% 8.6%
</table>
<tableCaption confidence="0.693581666666667">
Table IV Number of interactions, and percentages
of successful interactions, and interactions
involving the help component
</tableCaption>
<bodyText confidence="0.999935571428571">
The expert users were able to complete the ex-
periment using an average of 33 interaction rounds.
Similar performance levels were achieved by some
subjects during the second and third session, which
suggests that it is possible for at least some new
users to achieve performance close to expert level
within a few sessions.
</bodyText>
<subsectionHeader confidence="0.913383">
4.1.2 Task Level Performance
</subsectionHeader>
<bodyText confidence="0.999606714285714">
One of the important performance indicators for
end users is how long it takes to perform a given
task. During the experiments, the instructors noted
completion times required to reach a definite diag-
nosis in the experiment log. Table VI shows task
completion times, categorized by session (col-
umns) and task within the session (rows).
</bodyText>
<table confidence="0.98987625">
Session 1 Session 2 Session 3
Diagnosis 1 17:00 min 11:00 min 7:54 min
Diagnosis 2 11:00 min 6:18 min 5:34 min
Diagnosis 3 7:54 min 4:10 min 4:00 min
</table>
<tableCaption confidence="0.992236">
Table V Average time required by subjects to
complete diagnoses
</tableCaption>
<bodyText confidence="0.9996036">
In the last two sessions, after subjects had ac-
climatized to the system, a diagnosis takes an aver-
age of about four minutes to complete. This
compares to a three-minute average required to
complete a diagnosis by our expert users.
</bodyText>
<subsectionHeader confidence="0.838989">
4.1.3 System coverage
</subsectionHeader>
<bodyText confidence="0.996456666666667">
Table VI shows the percentage of in-coverage
sentences uttered by the users on interactions that
did not involve invocation of the help component.
</bodyText>
<table confidence="0.98953225">
IN-COVERAGE SENTENCES
Session 1 54.9%
Session 2 60.7%
Session 3 64.6%
</table>
<tableCaption confidence="0.997961">
Table VI Percentage of in-coverage sentences
</tableCaption>
<bodyText confidence="0.99921">
This indicates that subjects learn and adapt to
the system coverage as they use the system more.
The average proportion of in-coverage utterances
is 10 percent higher during the third session than
during the first session.
</bodyText>
<subsectionHeader confidence="0.997606">
4.2 Subjective Usability Measures
4.2.1 Results of Questionnaire
</subsectionHeader>
<bodyText confidence="0.991182454545454">
After finishing the third session, subjects were
asked to fill in a short questionnaire, where re-
sponses were on a five-point scale ranging from 1
(“strongly disagree”) to 5 (“strongly agree”). The
results are presented in Table VIII.
STATEMENT SCORE
I quickly learned how to use the system. 4.4
System response times were generally 4.5
satisfactory.
When the system did not understand me, 4.6
the help system usually showed me an-
other way to ask the question.
When I knew what I could say, the sys- 4.3
tem usually recognized me correctly.
I was often unable to ask the questions I 3.8
wanted.
I could ask enough questions that I was 4.3
sure of my diagnosis.
This system is more effective than non- 4.3
verbal communication using gestures.
I would use this system again in a simi- 4.1
lar situation.
Table VIII Subject responses to questionnaire.
Scores are on a 5-point scale, averaged over all
answers.
Answers were in general positive, and most of
the subjects were clearly very comfortable with the
system after just an hour and a half of use. Interest-
ingly, even though most of the subjects answered
“yes” to the question “I was often unable to ask the
questions I wanted”, the good performance of the
help system appeared to compensate adequately for
missing coverage.
</bodyText>
<subsubsectionHeader confidence="0.834099">
4.2.2 Translation Performance
</subsubsectionHeader>
<bodyText confidence="0.999871666666667">
In order to evaluate the translation quality of the
newly developed French-to-English system, we
conducted a preliminary performance evaluation,
similar to the evaluation method described in
(Bouillon 2005).
We performed translation judgment in two
rounds. In the first round, an English-speaking
judge was asked to categorize target utterances as
comprehensible or not without looking at corre-
sponding source sentences. 91.1% of the sentences
were judged as comprehensible. The remaining
8.9% consisted of sentences where the terminology
used was not familiar to the judge and of sentences
where the translation component failed to produce
a sufficiently good translation. An example sen-
tence is
- Are the headaches better when you experi-
ence dark room?
which stems from the French source sentence
- Vos maux de tête sont ils soulagés par obs-
curité?
In the second round, English-speaking judges,
sufficiently fluent in French to understand source
language utterances, were shown the French source
utterance, and asked to decide whether the target
language utterance correctly reflected the meaning
of the source language utterance. They were also
asked to judge the style of the target language ut-
terance. Specifically, judges were asked to classify
sentences as “BAD” if the meaning of the English
sentence did not reflect the meaning of the French
sentence. Sentences were categorized as “OK” if
the meaning was transferred correctly and the sen-
tence was comprehensible, but the style of the re-
sulting English sentence was not perfect. Sentences
were judged as “GOOD” when they were compre-
hensible, and both meaning and style were consid-
ered to be completely correct. Table VIII
summarizes results of two judges.
</bodyText>
<table confidence="0.833052333333333">
Good OK Bad
Judge 1 15.8% 73.80% 10.3%
Judge 2 46.6% 47.1% 6.3%
</table>
<tableCaption confidence="0.734724">
Table VIII Judgments of the quality of the transla-
tions of 546 utterances
</tableCaption>
<bodyText confidence="0.999969454545455">
It is apparent that translation judging is a highly
subjective process. When translations were marked
as “bad”, the problem most often seemed to be re-
lated to lexical items where it was challenging to
find an exact correspondence between French and
English. Two common examples were “troubles de
la vision”, which was translated as “blurred vi-
sion”, and “faiblesse musculaire”, which was trans-
lated as “weakness”. It is likely that a more careful
choice of lexical translation rules would deal with
at least some of these cases.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999992884615384">
We have presented a first end-to-end evaluation
of the MedSLT spoken language translation sys-
tem. The medical students who tested it were all
able to use the system well, with performance in
some cases comparable to that of that of system
developers after only two sessions. At least for the
fairly simple type of diagnoses covered by our sce-
nario, the system’s performance appeared clearly
adequate for the task.
This is particularly encouraging, since the
French to English version of the system is quite
new, and has not yet received the level of attention
required for a clinical system. The robustness
added by the help system was sufficient to com-
pensate for that, and in most cases, subjects were
able to find ways to maneuver around coverage
holes and other problems. It is entirely reasonable
to hope that performance, which is already fairly
good, would be substantially better with another
couple of months of development work.
In summary, we feel that this study shows that
the conservative architecture we have chosen
shows genuine potential for use in medical diagno-
sis situations. Before the end of 2006, we hope to
have advanced to the stage where we can start ini-
tial trials with real doctors and patients.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997818">
We would like to thank Agnes Lisowska, Alia
Rahal, and Nancy Underwood for being impartial
judges over our system’s results.
This work was funded by the Swiss National
Science Foundation.
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998798880597015">
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao, K.
Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
Machine Translation (EAMT), Budapest, Hungary.
P. Bouillon, M. Rayner, B. Novellas, Y. Nakao, M. San-
taholma, M. Starlander, and N. Chatzichrisafis. 2006.
Une grammaire multilingue partagée pour la recon-
naissance et la génération. In Proceedings of TALN
2006, Leuwen, Belgium.
M. Cohen, J. Giangola, and J. Balogh. 2004, Voice User
Interface Design. Addison Wesley Publishing.
R. I. Kittredge. 2003. Sublanguages and comtrolled
languages. In R. Mitkov, editor, The Oxford Hand-
book of Computational Linguistics, pages 430–447.
Oxford University Press.
MedBridge, 2006. http://www.medtablet.com/. As of
15th March 2006.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 15th March 2006.
T. Mitamura. 1999. Controlled language for multilin-
gual machine translation. In Proceedings of Machine
Translation Summit VII, Singapore.
Phraselator, 2006. http://www.phraselator.com. As of
15 February 2006.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifi-
cation grammars into speech recognisers. In Pro-
ceedings of the 10th EACL (demo track), Budapest,
Hungary.
M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao,
H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b.
Japanese speech understanding using grammar spe-
cialization. In HLT-NAACL 2005: Demo Session,
Vancouver, British Columbia, Canada. Association
for Computational Linguistics.
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma,M. Starlander, H. Isahara,
K. Kankazi, and Y. Nakao. 2005a. A methodology for
comparing grammar-based and robust approaches to
speech understanding. In Proceedings of the 9th In-
ternational Conference on Spoken Language Process-
ing (ICSLP), Lisboa, Portugal.
M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and M.
Wirén. 2000. The Spoken Language Translator,
Cambridge University Press.
M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao,
H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b.
Japanese speech understanding using grammar spe-
cialization. In HLT-NAACL 2005: Demo Session,
Vancouver, British Columbia, Canada. Association
for Computational Linguistics.
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Put-
ting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
Regulus, 2006. http://sourceforge.net/projects/regulus/.
As of 15 March 2006.
S-MINDS, 2006. http://www.sehda.com/. As of 15
March 2006.
M. Starlander, P. Bouillon, N. Chatzichrisafis, M. San-
taholma, M. Rayner, B.A. Hockey, H. Isahara, K.
Kanzaki, and Y. Nakao. 2005. Practicing controlled
language through a help system integrated into the
medical speech translation system (MedSLT). In Pro-
ceedings of the MT Summit X, Phuket, Thailand
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.181103">
<title confidence="0.999239">Evaluating Task Performance for a Unidirectional Controlled Language Medical Speech Translation System</title>
<author confidence="0.9921545">Nikos Chatzichrisafis</author>
<author confidence="0.9921545">Pierrette Bouillon</author>
<author confidence="0.9921545">Manny Rayner</author>
<author confidence="0.9921545">Marianne Santaholma</author>
<author confidence="0.9921545">Marianne Starlander</author>
<affiliation confidence="0.99393">University of Geneva, TIM/ISSCO</affiliation>
<address confidence="0.991737">40 bvd du Pont-d&apos;Arve, CH-1211 Geneva 4, Switzerland</address>
<email confidence="0.68026">Nikos.Chatzichrisafis@vozZup.com,Emmanuel.Rayner@issco.unige.ch,</email>
<title confidence="0.522333">Marianne.Starlander@eti.unige.ch</title>
<author confidence="0.998325">Beth Ann Hockey</author>
<affiliation confidence="0.982603">NASA Ames Research</affiliation>
<address confidence="0.817332">Moffett Field, CA</address>
<email confidence="0.992171">bahockey@email.arc.nasa.gov</email>
<abstract confidence="0.99846594117647">We present a task-level evaluation of the French to English version of MedSLT, a medium-vocabulary unidirectional controlled language medical speech translation system designed for doctor-patient diagnosis interviews. Our main goal was to establish task performance levels of novice users and compare them to expert users. Tests were carried out on eight medical students with no previous exposure to the system, with each student using the system for a total of three sessions. By the end of the third session, all the students were able to use the system confidently, with an average task completion time of about 4 minutes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Bouillon</author>
<author>M Rayner</author>
<author>N Chatzichrisafis</author>
<author>B A Hockey</author>
<author>M Santaholma</author>
<author>M Starlander</author>
<author>Y Nakao</author>
<author>K Kanzaki</author>
<author>H Isahara</author>
</authors>
<title>A generic multilingual open source platform for limited-domain medical speech translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="6302" citStr="Bouillon et al., 2005" startWordPosition="934" endWordPosition="937">he coverage of unidirectional spoken language translation system. We report a series of experiments, carried out using a French to English speech translation system, in which medical students with no previous experience to the system were asked to use it to carry out a series of verbal examinations on subjects who were simulating the symptoms of various types of medical conditions. Evaluation will be focused on usability. We primarily want to know how quickly subjects learn to use the system, and how their performance compares to that of expert users. 2 The MedSLT system MedSLT (MedSLT, 2005; Bouillon et al., 2005) is a unidirectional, grammar-based medical speech translation system intended for use in doctorpatient diagnosis dialogues. The system is built on top of Regulus (Regulus, 2006), an Open Source platform for developing grammar-based speech applications. Regulus supports rapid construction of complex grammar-based language models using an example-based method (Rayner et al., 2003; Rayner et al., 2006), which extracts most of the structure of the model from a general linguistically motivated resource grammar. Regulus-based recognizers are reasonably easy to maintain, and grammar structure is sha</context>
<context position="9471" citStr="Bouillon et al., 2005" startWordPosition="1433" endWordPosition="1436">ar material, Word Error Rate is about 8% and semantic error rate (per utterance) about 10% (Bouillon et al., 2006). Both the main grammar-based recognizer and the statistical recognizer used by the help system were trained from the same corpus of about 975 utterances. Help sentences were also taken from this corpus. 3 Experimental Setup In previous work, we have shown how to build a robust and extendable speech translation system. We have focused on performance metrics defined in terms of recognition and translation quality, and tested the system on naïve users without any medical background (Bouillon et al., 2005; Rayner et al., 2005a; Starlander et al., 2005). In this paper, our primary goal was rather to focus on task performance evaluation using plausible potential users. The basic methodology used is common in evaluating usability in software systems in general, and spoken language systems in particular (Cohen et. al 2000). We defined a simulated situation, where a French-speaking doctor was required to carry out a verbal examination of an English-speaking patient who claimed to be suffering from a headache, using the MedSLT system to translate all their questions. The patients were played by memb</context>
</contexts>
<marker>Bouillon, Rayner, Chatzichrisafis, Hockey, Santaholma, Starlander, Nakao, Kanzaki, Isahara, 2005</marker>
<rawString>P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey, M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara. 2005. A generic multilingual open source platform for limited-domain medical speech translation. In Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT), Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bouillon</author>
<author>M Rayner</author>
<author>B Novellas</author>
<author>Y Nakao</author>
<author>M Santaholma</author>
<author>M Starlander</author>
<author>N Chatzichrisafis</author>
</authors>
<title>Une grammaire multilingue partagée pour la reconnaissance et la génération.</title>
<date>2006</date>
<booktitle>In Proceedings of TALN</booktitle>
<location>Leuwen, Belgium.</location>
<contexts>
<context position="7090" citStr="Bouillon et al., 2006" startWordPosition="1043" endWordPosition="1046">gulus, 2006), an Open Source platform for developing grammar-based speech applications. Regulus supports rapid construction of complex grammar-based language models using an example-based method (Rayner et al., 2003; Rayner et al., 2006), which extracts most of the structure of the model from a general linguistically motivated resource grammar. Regulus-based recognizers are reasonably easy to maintain, and grammar structure is shared automatically across different subdomains. Resource grammars are now available for several languages, including English, Japanese (Rayner et al., 2005b), French (Bouillon et al., 2006) and Spanish. MedSLT includes a help module, whose purpose is to add robustness to the system and guide the user towards the supported coverage. The help module uses a second backup recognizer, equipped with a statistical language model; it matches the results from this second recognizer against a corpus of utterances, which are within system coverage and have already been judged to give correct translations. In previous studies (Rayner et al., 2005a; Starlander et al., 2005), we showed that the grammar-based recognizer performs much better than the statistical one on in-coverage utterances, a</context>
<context position="8964" citStr="Bouillon et al., 2006" startWordPosition="1350" endWordPosition="1353"> into spoken English in the headache subdomain. Coverage is based on standard headache-related examination questions obtained from a doctor, and consists mostly of yes/no questions. WH-questions and elliptical constructions are also supported. A typical short session with MedSLT might be as follows: - is the pain in the side of the head? - does the pain radiate to the neck? - to the jaw? - do you usually have headaches in the morning ? The recognizer’s vocabulary is about 1000 surface words; on in-grammar material, Word Error Rate is about 8% and semantic error rate (per utterance) about 10% (Bouillon et al., 2006). Both the main grammar-based recognizer and the statistical recognizer used by the help system were trained from the same corpus of about 975 utterances. Help sentences were also taken from this corpus. 3 Experimental Setup In previous work, we have shown how to build a robust and extendable speech translation system. We have focused on performance metrics defined in terms of recognition and translation quality, and tested the system on naïve users without any medical background (Bouillon et al., 2005; Rayner et al., 2005a; Starlander et al., 2005). In this paper, our primary goal was rather </context>
</contexts>
<marker>Bouillon, Rayner, Novellas, Nakao, Santaholma, Starlander, Chatzichrisafis, 2006</marker>
<rawString>P. Bouillon, M. Rayner, B. Novellas, Y. Nakao, M. Santaholma, M. Starlander, and N. Chatzichrisafis. 2006. Une grammaire multilingue partagée pour la reconnaissance et la génération. In Proceedings of TALN 2006, Leuwen, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cohen</author>
<author>J Giangola</author>
<author>J Balogh</author>
</authors>
<title>Voice User Interface Design.</title>
<date>2004</date>
<publisher>Addison Wesley Publishing.</publisher>
<marker>Cohen, Giangola, Balogh, 2004</marker>
<rawString>M. Cohen, J. Giangola, and J. Balogh. 2004, Voice User Interface Design. Addison Wesley Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R I Kittredge</author>
</authors>
<title>Sublanguages and comtrolled languages. In</title>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics,</booktitle>
<pages>430--447</pages>
<editor>R. Mitkov, editor,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="3167" citStr="Kittredge, 2003" startWordPosition="436" endWordPosition="437">), grammar-based systems utilizing probabilistic context-free grammar tuning appear to deliver better results when training data is sparse (Rayner et al., 2005a). One drawback of grammar-based systems is that out-of-coverage utterances will be neither recognized nor translated, an objection that critics have sometimes painted as decisive. It is by no means obvious, however, that restricted coverage is such a serious problem. In text processing, work on several generations of controlled language systems has developed a range of techniques for keeping users within the bounds of system coverage (Kittredge, 2003; Mitamura, 1999). If these techniques work for text processing, it is surely not inconceivable that variants of them will be equally successful for spoken language applications. Users are usually able to adapt to a controlled language system given enough time. The critical questions are how to provide efficient support to guide them towards the system&apos;s coverage, and how much time they will then need before they have acclimatized. With regard to top-level translation functionality, the choice is between unidirectional and bidirectional systems. Bidirectional systems are certainly possible tod</context>
</contexts>
<marker>Kittredge, 2003</marker>
<rawString>R. I. Kittredge. 2003. Sublanguages and comtrolled languages. In R. Mitkov, editor, The Oxford Handbook of Computational Linguistics, pages 430–447. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MedBridge</author>
</authors>
<title>http://www.medtablet.com/.</title>
<date>2006</date>
<journal>As of</journal>
<volume>15</volume>
<contexts>
<context position="2445" citStr="MedBridge, 2006" startWordPosition="332" endWordPosition="333">speech translation systems is the statistical one. Statistical translation systems rely on parallel corpora of source and target language texts, from which a translation model is trained. However, this is not necessarily the best alternative in safetycritical medical applications. Anecdotally, many doctors express reluctance to trust a translation device whose output is not readily predictable, and most of the speech translation systems which have reached the stage of field testing rely on various types of grammar-based recognition and rule-based translation (Phraselator, 2006; S-MINDS, 2006; MedBridge, 2006). Even though statistical systems exhibit many desirable properties (purely datadriven, domain independence), grammar-based systems utilizing probabilistic context-free grammar tuning appear to deliver better results when training data is sparse (Rayner et al., 2005a). One drawback of grammar-based systems is that out-of-coverage utterances will be neither recognized nor translated, an objection that critics have sometimes painted as decisive. It is by no means obvious, however, that restricted coverage is such a serious problem. In text processing, work on several generations of controlled la</context>
</contexts>
<marker>MedBridge, 2006</marker>
<rawString>MedBridge, 2006. http://www.medtablet.com/. As of 15th March 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MedSLT</author>
</authors>
<title>http://sourceforge.net/projects/medslt/.</title>
<date>2005</date>
<journal>As of</journal>
<volume>15</volume>
<contexts>
<context position="6278" citStr="MedSLT, 2005" startWordPosition="932" endWordPosition="933"> to adapt to the coverage of unidirectional spoken language translation system. We report a series of experiments, carried out using a French to English speech translation system, in which medical students with no previous experience to the system were asked to use it to carry out a series of verbal examinations on subjects who were simulating the symptoms of various types of medical conditions. Evaluation will be focused on usability. We primarily want to know how quickly subjects learn to use the system, and how their performance compares to that of expert users. 2 The MedSLT system MedSLT (MedSLT, 2005; Bouillon et al., 2005) is a unidirectional, grammar-based medical speech translation system intended for use in doctorpatient diagnosis dialogues. The system is built on top of Regulus (Regulus, 2006), an Open Source platform for developing grammar-based speech applications. Regulus supports rapid construction of complex grammar-based language models using an example-based method (Rayner et al., 2003; Rayner et al., 2006), which extracts most of the structure of the model from a general linguistically motivated resource grammar. Regulus-based recognizers are reasonably easy to maintain, and </context>
</contexts>
<marker>MedSLT, 2005</marker>
<rawString>MedSLT, 2005. http://sourceforge.net/projects/medslt/. As of 15th March 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitamura</author>
</authors>
<title>Controlled language for multilingual machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of Machine Translation Summit VII,</booktitle>
<contexts>
<context position="3184" citStr="Mitamura, 1999" startWordPosition="438" endWordPosition="439">systems utilizing probabilistic context-free grammar tuning appear to deliver better results when training data is sparse (Rayner et al., 2005a). One drawback of grammar-based systems is that out-of-coverage utterances will be neither recognized nor translated, an objection that critics have sometimes painted as decisive. It is by no means obvious, however, that restricted coverage is such a serious problem. In text processing, work on several generations of controlled language systems has developed a range of techniques for keeping users within the bounds of system coverage (Kittredge, 2003; Mitamura, 1999). If these techniques work for text processing, it is surely not inconceivable that variants of them will be equally successful for spoken language applications. Users are usually able to adapt to a controlled language system given enough time. The critical questions are how to provide efficient support to guide them towards the system&apos;s coverage, and how much time they will then need before they have acclimatized. With regard to top-level translation functionality, the choice is between unidirectional and bidirectional systems. Bidirectional systems are certainly possible today1, but the argu</context>
</contexts>
<marker>Mitamura, 1999</marker>
<rawString>T. Mitamura. 1999. Controlled language for multilingual machine translation. In Proceedings of Machine Translation Summit VII, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phraselator</author>
</authors>
<title>http://www.phraselator.com.</title>
<date>2006</date>
<journal>As of</journal>
<volume>15</volume>
<contexts>
<context position="2412" citStr="Phraselator, 2006" startWordPosition="328" endWordPosition="329">ent trends, a common approach for speech translation systems is the statistical one. Statistical translation systems rely on parallel corpora of source and target language texts, from which a translation model is trained. However, this is not necessarily the best alternative in safetycritical medical applications. Anecdotally, many doctors express reluctance to trust a translation device whose output is not readily predictable, and most of the speech translation systems which have reached the stage of field testing rely on various types of grammar-based recognition and rule-based translation (Phraselator, 2006; S-MINDS, 2006; MedBridge, 2006). Even though statistical systems exhibit many desirable properties (purely datadriven, domain independence), grammar-based systems utilizing probabilistic context-free grammar tuning appear to deliver better results when training data is sparse (Rayner et al., 2005a). One drawback of grammar-based systems is that out-of-coverage utterances will be neither recognized nor translated, an objection that critics have sometimes painted as decisive. It is by no means obvious, however, that restricted coverage is such a serious problem. In text processing, work on sev</context>
</contexts>
<marker>Phraselator, 2006</marker>
<rawString>Phraselator, 2006. http://www.phraselator.com. As of 15 February 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>B A Hockey</author>
<author>J Dowding</author>
</authors>
<title>An open source environment for compiling typed unification grammars into speech recognisers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th EACL (demo track),</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="6683" citStr="Rayner et al., 2003" startWordPosition="986" endWordPosition="989"> Evaluation will be focused on usability. We primarily want to know how quickly subjects learn to use the system, and how their performance compares to that of expert users. 2 The MedSLT system MedSLT (MedSLT, 2005; Bouillon et al., 2005) is a unidirectional, grammar-based medical speech translation system intended for use in doctorpatient diagnosis dialogues. The system is built on top of Regulus (Regulus, 2006), an Open Source platform for developing grammar-based speech applications. Regulus supports rapid construction of complex grammar-based language models using an example-based method (Rayner et al., 2003; Rayner et al., 2006), which extracts most of the structure of the model from a general linguistically motivated resource grammar. Regulus-based recognizers are reasonably easy to maintain, and grammar structure is shared automatically across different subdomains. Resource grammars are now available for several languages, including English, Japanese (Rayner et al., 2005b), French (Bouillon et al., 2006) and Spanish. MedSLT includes a help module, whose purpose is to add robustness to the system and guide the user towards the supported coverage. The help module uses a second backup recognizer,</context>
</contexts>
<marker>Rayner, Hockey, Dowding, 2003</marker>
<rawString>M. Rayner, B.A. Hockey, and J. Dowding. 2003. An open source environment for compiling typed unification grammars into speech recognisers. In Proceedings of the 10th EACL (demo track), Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>N Chatzichrisafis</author>
<author>P Bouillon</author>
<author>Y Nakao</author>
<author>H Isahara</author>
<author>K Kanzaki</author>
<author>B A Hockey</author>
</authors>
<title>Japanese speech understanding using grammar specialization.</title>
<date>2005</date>
<booktitle>In HLT-NAACL 2005: Demo Session,</booktitle>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="2711" citStr="Rayner et al., 2005" startWordPosition="365" endWordPosition="368">ical applications. Anecdotally, many doctors express reluctance to trust a translation device whose output is not readily predictable, and most of the speech translation systems which have reached the stage of field testing rely on various types of grammar-based recognition and rule-based translation (Phraselator, 2006; S-MINDS, 2006; MedBridge, 2006). Even though statistical systems exhibit many desirable properties (purely datadriven, domain independence), grammar-based systems utilizing probabilistic context-free grammar tuning appear to deliver better results when training data is sparse (Rayner et al., 2005a). One drawback of grammar-based systems is that out-of-coverage utterances will be neither recognized nor translated, an objection that critics have sometimes painted as decisive. It is by no means obvious, however, that restricted coverage is such a serious problem. In text processing, work on several generations of controlled language systems has developed a range of techniques for keeping users within the bounds of system coverage (Kittredge, 2003; Mitamura, 1999). If these techniques work for text processing, it is surely not inconceivable that variants of them will be equally successful</context>
<context position="7056" citStr="Rayner et al., 2005" startWordPosition="1038" endWordPosition="1041"> is built on top of Regulus (Regulus, 2006), an Open Source platform for developing grammar-based speech applications. Regulus supports rapid construction of complex grammar-based language models using an example-based method (Rayner et al., 2003; Rayner et al., 2006), which extracts most of the structure of the model from a general linguistically motivated resource grammar. Regulus-based recognizers are reasonably easy to maintain, and grammar structure is shared automatically across different subdomains. Resource grammars are now available for several languages, including English, Japanese (Rayner et al., 2005b), French (Bouillon et al., 2006) and Spanish. MedSLT includes a help module, whose purpose is to add robustness to the system and guide the user towards the supported coverage. The help module uses a second backup recognizer, equipped with a statistical language model; it matches the results from this second recognizer against a corpus of utterances, which are within system coverage and have already been judged to give correct translations. In previous studies (Rayner et al., 2005a; Starlander et al., 2005), we showed that the grammar-based recognizer performs much better than the statistica</context>
<context position="9492" citStr="Rayner et al., 2005" startWordPosition="1437" endWordPosition="1440"> Rate is about 8% and semantic error rate (per utterance) about 10% (Bouillon et al., 2006). Both the main grammar-based recognizer and the statistical recognizer used by the help system were trained from the same corpus of about 975 utterances. Help sentences were also taken from this corpus. 3 Experimental Setup In previous work, we have shown how to build a robust and extendable speech translation system. We have focused on performance metrics defined in terms of recognition and translation quality, and tested the system on naïve users without any medical background (Bouillon et al., 2005; Rayner et al., 2005a; Starlander et al., 2005). In this paper, our primary goal was rather to focus on task performance evaluation using plausible potential users. The basic methodology used is common in evaluating usability in software systems in general, and spoken language systems in particular (Cohen et. al 2000). We defined a simulated situation, where a French-speaking doctor was required to carry out a verbal examination of an English-speaking patient who claimed to be suffering from a headache, using the MedSLT system to translate all their questions. The patients were played by members of the developmen</context>
</contexts>
<marker>Rayner, Chatzichrisafis, Bouillon, Nakao, Isahara, Kanzaki, Hockey, 2005</marker>
<rawString>M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao, H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b. Japanese speech understanding using grammar specialization. In HLT-NAACL 2005: Demo Session, Vancouver, British Columbia, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>P Bouillon</author>
<author>N Chatzichrisafis</author>
<author>B A Hockey</author>
<author>M Santaholma</author>
<author>M Starlander</author>
<author>H Isahara</author>
<author>K Kankazi</author>
<author>Y Nakao</author>
</authors>
<title>A methodology for comparing grammar-based and robust approaches to speech understanding.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Conference on Spoken Language Processing (ICSLP),</booktitle>
<location>Lisboa, Portugal.</location>
<contexts>
<context position="2711" citStr="Rayner et al., 2005" startWordPosition="365" endWordPosition="368">ical applications. Anecdotally, many doctors express reluctance to trust a translation device whose output is not readily predictable, and most of the speech translation systems which have reached the stage of field testing rely on various types of grammar-based recognition and rule-based translation (Phraselator, 2006; S-MINDS, 2006; MedBridge, 2006). Even though statistical systems exhibit many desirable properties (purely datadriven, domain independence), grammar-based systems utilizing probabilistic context-free grammar tuning appear to deliver better results when training data is sparse (Rayner et al., 2005a). One drawback of grammar-based systems is that out-of-coverage utterances will be neither recognized nor translated, an objection that critics have sometimes painted as decisive. It is by no means obvious, however, that restricted coverage is such a serious problem. In text processing, work on several generations of controlled language systems has developed a range of techniques for keeping users within the bounds of system coverage (Kittredge, 2003; Mitamura, 1999). If these techniques work for text processing, it is surely not inconceivable that variants of them will be equally successful</context>
<context position="7056" citStr="Rayner et al., 2005" startWordPosition="1038" endWordPosition="1041"> is built on top of Regulus (Regulus, 2006), an Open Source platform for developing grammar-based speech applications. Regulus supports rapid construction of complex grammar-based language models using an example-based method (Rayner et al., 2003; Rayner et al., 2006), which extracts most of the structure of the model from a general linguistically motivated resource grammar. Regulus-based recognizers are reasonably easy to maintain, and grammar structure is shared automatically across different subdomains. Resource grammars are now available for several languages, including English, Japanese (Rayner et al., 2005b), French (Bouillon et al., 2006) and Spanish. MedSLT includes a help module, whose purpose is to add robustness to the system and guide the user towards the supported coverage. The help module uses a second backup recognizer, equipped with a statistical language model; it matches the results from this second recognizer against a corpus of utterances, which are within system coverage and have already been judged to give correct translations. In previous studies (Rayner et al., 2005a; Starlander et al., 2005), we showed that the grammar-based recognizer performs much better than the statistica</context>
<context position="9492" citStr="Rayner et al., 2005" startWordPosition="1437" endWordPosition="1440"> Rate is about 8% and semantic error rate (per utterance) about 10% (Bouillon et al., 2006). Both the main grammar-based recognizer and the statistical recognizer used by the help system were trained from the same corpus of about 975 utterances. Help sentences were also taken from this corpus. 3 Experimental Setup In previous work, we have shown how to build a robust and extendable speech translation system. We have focused on performance metrics defined in terms of recognition and translation quality, and tested the system on naïve users without any medical background (Bouillon et al., 2005; Rayner et al., 2005a; Starlander et al., 2005). In this paper, our primary goal was rather to focus on task performance evaluation using plausible potential users. The basic methodology used is common in evaluating usability in software systems in general, and spoken language systems in particular (Cohen et. al 2000). We defined a simulated situation, where a French-speaking doctor was required to carry out a verbal examination of an English-speaking patient who claimed to be suffering from a headache, using the MedSLT system to translate all their questions. The patients were played by members of the developmen</context>
</contexts>
<marker>Rayner, Bouillon, Chatzichrisafis, Hockey, Santaholma, Starlander, Isahara, Kankazi, Nakao, 2005</marker>
<rawString>M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. Hockey, M. Santaholma,M. Starlander, H. Isahara, K. Kankazi, and Y. Nakao. 2005a. A methodology for comparing grammar-based and robust approaches to speech understanding. In Proceedings of the 9th International Conference on Spoken Language Processing (ICSLP), Lisboa, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>D Carter</author>
<author>P Bouillon</author>
<author>V Digalakis</author>
<author>M Wirén</author>
</authors>
<title>The Spoken Language Translator,</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<marker>Rayner, Carter, Bouillon, Digalakis, Wirén, 2000</marker>
<rawString>M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and M. Wirén. 2000. The Spoken Language Translator, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>N Chatzichrisafis</author>
<author>P Bouillon</author>
<author>Y Nakao</author>
<author>H Isahara</author>
<author>K Kanzaki</author>
<author>B A Hockey</author>
</authors>
<title>Japanese speech understanding using grammar specialization.</title>
<date>2005</date>
<booktitle>In HLT-NAACL 2005: Demo Session,</booktitle>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="2711" citStr="Rayner et al., 2005" startWordPosition="365" endWordPosition="368">ical applications. Anecdotally, many doctors express reluctance to trust a translation device whose output is not readily predictable, and most of the speech translation systems which have reached the stage of field testing rely on various types of grammar-based recognition and rule-based translation (Phraselator, 2006; S-MINDS, 2006; MedBridge, 2006). Even though statistical systems exhibit many desirable properties (purely datadriven, domain independence), grammar-based systems utilizing probabilistic context-free grammar tuning appear to deliver better results when training data is sparse (Rayner et al., 2005a). One drawback of grammar-based systems is that out-of-coverage utterances will be neither recognized nor translated, an objection that critics have sometimes painted as decisive. It is by no means obvious, however, that restricted coverage is such a serious problem. In text processing, work on several generations of controlled language systems has developed a range of techniques for keeping users within the bounds of system coverage (Kittredge, 2003; Mitamura, 1999). If these techniques work for text processing, it is surely not inconceivable that variants of them will be equally successful</context>
<context position="7056" citStr="Rayner et al., 2005" startWordPosition="1038" endWordPosition="1041"> is built on top of Regulus (Regulus, 2006), an Open Source platform for developing grammar-based speech applications. Regulus supports rapid construction of complex grammar-based language models using an example-based method (Rayner et al., 2003; Rayner et al., 2006), which extracts most of the structure of the model from a general linguistically motivated resource grammar. Regulus-based recognizers are reasonably easy to maintain, and grammar structure is shared automatically across different subdomains. Resource grammars are now available for several languages, including English, Japanese (Rayner et al., 2005b), French (Bouillon et al., 2006) and Spanish. MedSLT includes a help module, whose purpose is to add robustness to the system and guide the user towards the supported coverage. The help module uses a second backup recognizer, equipped with a statistical language model; it matches the results from this second recognizer against a corpus of utterances, which are within system coverage and have already been judged to give correct translations. In previous studies (Rayner et al., 2005a; Starlander et al., 2005), we showed that the grammar-based recognizer performs much better than the statistica</context>
<context position="9492" citStr="Rayner et al., 2005" startWordPosition="1437" endWordPosition="1440"> Rate is about 8% and semantic error rate (per utterance) about 10% (Bouillon et al., 2006). Both the main grammar-based recognizer and the statistical recognizer used by the help system were trained from the same corpus of about 975 utterances. Help sentences were also taken from this corpus. 3 Experimental Setup In previous work, we have shown how to build a robust and extendable speech translation system. We have focused on performance metrics defined in terms of recognition and translation quality, and tested the system on naïve users without any medical background (Bouillon et al., 2005; Rayner et al., 2005a; Starlander et al., 2005). In this paper, our primary goal was rather to focus on task performance evaluation using plausible potential users. The basic methodology used is common in evaluating usability in software systems in general, and spoken language systems in particular (Cohen et. al 2000). We defined a simulated situation, where a French-speaking doctor was required to carry out a verbal examination of an English-speaking patient who claimed to be suffering from a headache, using the MedSLT system to translate all their questions. The patients were played by members of the developmen</context>
</contexts>
<marker>Rayner, Chatzichrisafis, Bouillon, Nakao, Isahara, Kanzaki, Hockey, 2005</marker>
<rawString>M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao, H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b. Japanese speech understanding using grammar specialization. In HLT-NAACL 2005: Demo Session, Vancouver, British Columbia, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>B A Hockey</author>
<author>P Bouillon</author>
</authors>
<title>Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler.</title>
<date>2006</date>
<publisher>CSLI Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="6705" citStr="Rayner et al., 2006" startWordPosition="990" endWordPosition="993">ocused on usability. We primarily want to know how quickly subjects learn to use the system, and how their performance compares to that of expert users. 2 The MedSLT system MedSLT (MedSLT, 2005; Bouillon et al., 2005) is a unidirectional, grammar-based medical speech translation system intended for use in doctorpatient diagnosis dialogues. The system is built on top of Regulus (Regulus, 2006), an Open Source platform for developing grammar-based speech applications. Regulus supports rapid construction of complex grammar-based language models using an example-based method (Rayner et al., 2003; Rayner et al., 2006), which extracts most of the structure of the model from a general linguistically motivated resource grammar. Regulus-based recognizers are reasonably easy to maintain, and grammar structure is shared automatically across different subdomains. Resource grammars are now available for several languages, including English, Japanese (Rayner et al., 2005b), French (Bouillon et al., 2006) and Spanish. MedSLT includes a help module, whose purpose is to add robustness to the system and guide the user towards the supported coverage. The help module uses a second backup recognizer, equipped with a stati</context>
</contexts>
<marker>Rayner, Hockey, Bouillon, 2006</marker>
<rawString>M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler. CSLI Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regulus</author>
</authors>
<title>http://sourceforge.net/projects/regulus/.</title>
<date>2006</date>
<journal>As of</journal>
<volume>15</volume>
<contexts>
<context position="6480" citStr="Regulus, 2006" startWordPosition="962" endWordPosition="963">dents with no previous experience to the system were asked to use it to carry out a series of verbal examinations on subjects who were simulating the symptoms of various types of medical conditions. Evaluation will be focused on usability. We primarily want to know how quickly subjects learn to use the system, and how their performance compares to that of expert users. 2 The MedSLT system MedSLT (MedSLT, 2005; Bouillon et al., 2005) is a unidirectional, grammar-based medical speech translation system intended for use in doctorpatient diagnosis dialogues. The system is built on top of Regulus (Regulus, 2006), an Open Source platform for developing grammar-based speech applications. Regulus supports rapid construction of complex grammar-based language models using an example-based method (Rayner et al., 2003; Rayner et al., 2006), which extracts most of the structure of the model from a general linguistically motivated resource grammar. Regulus-based recognizers are reasonably easy to maintain, and grammar structure is shared automatically across different subdomains. Resource grammars are now available for several languages, including English, Japanese (Rayner et al., 2005b), French (Bouillon et </context>
</contexts>
<marker>Regulus, 2006</marker>
<rawString>Regulus, 2006. http://sourceforge.net/projects/regulus/. As of 15 March 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-MINDS</author>
</authors>
<title>http://www.sehda.com/.</title>
<date>2006</date>
<journal>As of</journal>
<volume>15</volume>
<contexts>
<context position="2427" citStr="S-MINDS, 2006" startWordPosition="330" endWordPosition="331">n approach for speech translation systems is the statistical one. Statistical translation systems rely on parallel corpora of source and target language texts, from which a translation model is trained. However, this is not necessarily the best alternative in safetycritical medical applications. Anecdotally, many doctors express reluctance to trust a translation device whose output is not readily predictable, and most of the speech translation systems which have reached the stage of field testing rely on various types of grammar-based recognition and rule-based translation (Phraselator, 2006; S-MINDS, 2006; MedBridge, 2006). Even though statistical systems exhibit many desirable properties (purely datadriven, domain independence), grammar-based systems utilizing probabilistic context-free grammar tuning appear to deliver better results when training data is sparse (Rayner et al., 2005a). One drawback of grammar-based systems is that out-of-coverage utterances will be neither recognized nor translated, an objection that critics have sometimes painted as decisive. It is by no means obvious, however, that restricted coverage is such a serious problem. In text processing, work on several generation</context>
<context position="5045" citStr="S-MINDS, 2006" startWordPosition="731" endWordPosition="732">uired before the user can understand what language it is capable of handling. A doctor, who uses the system regularly, will acquire the necessary familiarity. The same might be true for a few patients, if special circumstances mean that they encounter speech translation applications reasonably frequently. Most patients, however, will have had no previous exposure to the system, and may be unwilling to use a type of technology which they have trouble understanding. A unidirectional system, in which the doctor mostly asks YN-questions, will never be ideal. If, 1 For example, the S-MINDS system (S-MINDS, 2006) offers bidirectional translation. however, the doctor can become proficient in using it, it may still be very much better than the alternative of no translation assistance at all. To summarize, today’s technology definitely lets us build unidirectional grammar-based medical speech translation systems which work for regular users who have had time to adapt to their limitations. While bidirectional systems are possible, the case for them is less obvious, since users on the patient side may not in practice be able to use them effectively. In this paper, we will empirically investigate the abilit</context>
</contexts>
<marker>S-MINDS, 2006</marker>
<rawString>S-MINDS, 2006. http://www.sehda.com/. As of 15 March 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Starlander</author>
<author>P Bouillon</author>
<author>N Chatzichrisafis</author>
<author>M Santaholma</author>
<author>M Rayner</author>
<author>B A Hockey</author>
<author>H Isahara</author>
<author>K Kanzaki</author>
<author>Y Nakao</author>
</authors>
<title>Practicing controlled language through a help system integrated into the medical speech translation system (MedSLT).</title>
<date>2005</date>
<booktitle>In Proceedings of the MT Summit X,</booktitle>
<location>Phuket, Thailand</location>
<contexts>
<context position="7570" citStr="Starlander et al., 2005" startWordPosition="1121" endWordPosition="1124">s. Resource grammars are now available for several languages, including English, Japanese (Rayner et al., 2005b), French (Bouillon et al., 2006) and Spanish. MedSLT includes a help module, whose purpose is to add robustness to the system and guide the user towards the supported coverage. The help module uses a second backup recognizer, equipped with a statistical language model; it matches the results from this second recognizer against a corpus of utterances, which are within system coverage and have already been judged to give correct translations. In previous studies (Rayner et al., 2005a; Starlander et al., 2005), we showed that the grammar-based recognizer performs much better than the statistical one on in-coverage utterances, and rather worse on out-of-coverage ones. We also found that having the help module available approximately doubled the speed at which subjects learned to use the system, measured as the average difference in semantic error rate between the results for their first quarter-session and their last quarter-session. It is also possible to recover from recognition errors by selecting one of the displayed help sentences; in the cited studies, we found that this increased the number o</context>
<context position="9519" citStr="Starlander et al., 2005" startWordPosition="1441" endWordPosition="1444">semantic error rate (per utterance) about 10% (Bouillon et al., 2006). Both the main grammar-based recognizer and the statistical recognizer used by the help system were trained from the same corpus of about 975 utterances. Help sentences were also taken from this corpus. 3 Experimental Setup In previous work, we have shown how to build a robust and extendable speech translation system. We have focused on performance metrics defined in terms of recognition and translation quality, and tested the system on naïve users without any medical background (Bouillon et al., 2005; Rayner et al., 2005a; Starlander et al., 2005). In this paper, our primary goal was rather to focus on task performance evaluation using plausible potential users. The basic methodology used is common in evaluating usability in software systems in general, and spoken language systems in particular (Cohen et. al 2000). We defined a simulated situation, where a French-speaking doctor was required to carry out a verbal examination of an English-speaking patient who claimed to be suffering from a headache, using the MedSLT system to translate all their questions. The patients were played by members of the development team, who had been traine</context>
</contexts>
<marker>Starlander, Bouillon, Chatzichrisafis, Santaholma, Rayner, Hockey, Isahara, Kanzaki, Nakao, 2005</marker>
<rawString>M. Starlander, P. Bouillon, N. Chatzichrisafis, M. Santaholma, M. Rayner, B.A. Hockey, H. Isahara, K. Kanzaki, and Y. Nakao. 2005. Practicing controlled language through a help system integrated into the medical speech translation system (MedSLT). In Proceedings of the MT Summit X, Phuket, Thailand</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>