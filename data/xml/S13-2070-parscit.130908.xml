<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.160138">
<title confidence="0.968675">
SwatCS: Combining simple classifiers with estimated accuracy
</title>
<author confidence="0.993988">
Sam Clark and Richard Wicentowski
</author>
<affiliation confidence="0.950203">
Department of Computer Science
Swarthmore College
</affiliation>
<address confidence="0.705668">
Swarthmore, PA 19081 USA
</address>
<email confidence="0.995893">
sclark2@sccs.swarthmore.edu and richardw@cs.swarthmore.edu
</email>
<sectionHeader confidence="0.995535" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999060909090909">
This paper is an overview of the SwatCS
system submitted to SemEval-2013 Task 2A:
Contextual Polarity Disambiguation. The sen-
timent of individual phrases within a tweet
are labeled using a combination of classifiers
trained on a range of lexical features. The
classifiers are combined by estimating the ac-
curacy of the classifiers on each tweet. Perfor-
mance is measured when using only the pro-
vided training data, and separately when in-
cluding external data.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999876285714286">
Spurred on by the wide-spread use of the social net-
works to communicate with friends, fans and cus-
tomers around the globe, Twitter has been adopted
by celebrities, athletes, politicians, and major com-
panies as a platform that mitigates the interaction be-
tween individuals.
Analysis of this Twitter data can provide insights
into how users express themselves. For example,
many new forms of expression and language fea-
tures have emerged on Twitter, including expres-
sions containing mentions, hashtags, emoticons, and
abbreviations. This research leverages the lexical
features in tweets to predict whether a phrase within
a tweet conveys a positive or negative sentiment.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99764275">
A common goal of past research has been to discover
and extract features from tweets that accurately in-
dicate sentiment (Liu, 2010). The importance of
feature selection and machine learning in sentiment
analysis has been explored prior to the rise of so-
cial networks. For example, Pang and Lee (2004)
apply machine learning techniques to extracted fea-
tures from movie reviews.
More recent feature-based systems include a
lexicon-based approach (Taboada et al., 2011), and
a more focused study on the importance of both ad-
verbs and adjectives in determining sentiment (Be-
namara et al., 2007). Other examples include us-
ing looser descriptions of sentiment rather than rigid
positive/negative labelings (Whitelaw et al., 2005)
and investigating how connections between users
can be used to predict sentiment (Tan et al., 2011).
This task differs from past work in sentiment anal-
ysis of tweets because we aim to build a model capa-
ble of predicting the sentiment of sub-phrases within
the tweet rather than considering the entire tweet.
Specifically, “given a message containing a marked
instance of a word or a phrase, determine whether
that instance is positive, negative or neutral in that
context” (Wilson et al., 2013). Research on context-
oriented polarity predates the emergence of social
networks: (Nasukawa and Yi, 2003) predict senti-
ment of subsections in a larger document.
N-gram features, part of speech features and
“micro-blogging features” have been used as accu-
rate indicators of polarity (Kouloumpis et al., 2011).
The “micro-blogging features” are of particular in-
terest as they provide insight into how users have
adapted Twitter tokens to natural language to por-
tray sentiment. These features include hashtags and
emoticons (Kouloumpis et al., 2011).
</bodyText>
<page confidence="0.988748">
425
</page>
<bodyText confidence="0.809706">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 425–429, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.994445" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999952">
The task organizers provided a manually-labeled set
of tweets. For parts of this study, their data was sup-
plemented with external data (Go et al., 2009).
As part of pre-processing, all tweets were
part-of-speech tagged using the ARK TweetNLP
tools (Owoputi et al., 2013). All punctuation was
stripped, except for #hashtags, @mentions,
emoticons :), and exclamation marks. All hyper-
links were replaced with a common string, “URL”.
</bodyText>
<subsectionHeader confidence="0.999883">
3.1 Common Data
</subsectionHeader>
<bodyText confidence="0.9999923">
The provided training data was a collection of ap-
proximately 15K tweets, manually labeled for senti-
ment (positive, negative, neutral, or objective) (Wil-
son et al., 2013). These sentiment labels applied
to a specific phrase within the tweet and did not
necessarily match the sentiment of the entire tweet.
Each tweet had at least one labeled phrase, though
some tweets had multiple phrases labeled individu-
ally. Overall, 37% of tweets had one labeled phrase,
with an average of 2.58 labeled phrases per tweet.
Each of our classifiers were binary classifiers, la-
beling phrases as either positive or negative. As
such, approximately 10.5K phrases labeled as objec-
tive or neutral were pruned from the training data,
resulting in a final training set containing 5362 la-
beled phrases, 3445 positive and 1917 negative.
The test data consisted of tweets and SMS mes-
sages, although the training data contained only
tweets. The test set for the phrase-level task (Task A)
contained 4435 tweets and 2334 SMS messages.
</bodyText>
<subsectionHeader confidence="0.99975">
3.2 Outside Data
</subsectionHeader>
<bodyText confidence="0.999962375">
Task organizers allowed two submissions, a con-
strained submission using only the provided training
data, and an unconstrained submission allowing the
use of external data. For the unconstrained submis-
sion, we used a data set built by Go et al. (2009). The
data set was automatically labeled using emoticons
to predict sentiment. We used a 50K tweet subset
containing 25K positive and 25K negative tweets.
</bodyText>
<subsectionHeader confidence="0.999187">
3.3 Phrase Isolation
</subsectionHeader>
<bodyText confidence="0.999080444444445">
For tweets containing a single labeled phrase, we use
the entire tweet as the context for the phrase. For
tweets containing two labeled phrases, we use the
unigram label bigram label
happy pos not going neg
good pos looking forward pos
great pos happy birthday pos
love pos last episode neg
best pos i’m mad neg
</bodyText>
<tableCaption confidence="0.996908">
Table 1: The 5 most influential unigram and bigrams
ranked by information gain.
</tableCaption>
<bodyText confidence="0.999938857142857">
context from the start of the tweet to the end of the
first phrase as the context for the first phrase, and the
context from the start of the second phrase to the end
of the tweet for the second phrase. If more than two
phrases are present, the context for any phrase in the
middle of the tweet is limited to only the words in
the labeled phrase.
</bodyText>
<sectionHeader confidence="0.997763" genericHeader="method">
4 Classifiers
</sectionHeader>
<bodyText confidence="0.9999838">
The system uses a combination of naive Bayes clas-
sifiers to label the input. Each classifier is trained on
a single feature extracted from the tweet. The classi-
fiers are combined using a confidence-weighted vot-
ing scheme. The system applies a simple negation
scheme to all of the language features used by the
classifiers. Any word following a negation term in
the phrase has the substring “NOT” prefixed to it.
This negation scheme was applied to n-gram fea-
tures and lexicon features.
</bodyText>
<subsectionHeader confidence="0.985738">
4.1 N-gram Features
</subsectionHeader>
<bodyText confidence="0.99999375">
Rather than use all of the n-grams as features, we
ranked each n-gram (w/POS tags) by calculating its
chi-square-based information gain. The top 2000
n-grams (1000 positive, 1000 negative) are used as
features in the n-gram classifier. Both a unigram and
bigram classifier use these ranked (word/POS) fea-
tures. Table 1 shows the highest ranked unigrams
and bigrams using this method.
</bodyText>
<subsectionHeader confidence="0.996942">
4.2 Sentiment Lexicon Features
</subsectionHeader>
<bodyText confidence="0.995051166666666">
A second classifier uses the MPQA subjectivity lex-
icon (Wiebe et al., 2005). We extract both the po-
larity and the polarity strength for each word/POS
in the lexicon matching a word/POS in the phrase’s
context. We refer to this classifier as the lexicon
classifier.
</bodyText>
<page confidence="0.993531">
426
</page>
<figure confidence="0.998128333333333">
rank classifier data polarity acc
1 unigrams (C) positive 0.89
2 unigrams (U) positive 0.88
3 lexicon (C) negative 0.83
4 lexicon (U) negative 0.81
5 tagcount (C) positive 0.78
6 bigrams (C) positive 0.75
7 tagcount (U) novote &lt;0.65
8 bigrams (U) novote &lt;0.65
Alpha vs. Classifier accuracy
Confidence/classifier accuracy
0.98
0.96
0.94
0.92
0.88
0.86
0.84
0.82
0.78
0.9
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
alpha = abs(P(pos) - P(neg))
</figure>
<figureCaption confidence="0.998194">
Figure 1: Classifier accuracy increases as the difference
between the probabilities of the labelings increases.
</figureCaption>
<subsectionHeader confidence="0.998523">
4.3 Part of Speech and Special Token Features
</subsectionHeader>
<bodyText confidence="0.999951285714286">
Three additional classifiers were built using features
extracted from the tweets. Our third classifier uses
only the raw counts of specific part of speech tags:
adjectives, adverbs, interjections, and emoticons.
The fourth classifier uses the emoticons as a fea-
ture. To reduce the noise in the emoticon feature set,
many (over 25) different emoticons are mapped to
the basic “:)” and “:(” expressions. Some emoticons
such as “xD” did not map to these basic expressions.
A fifth classifier gives added weight to words with
extraneous repeated letters. Words containing two
or more repeated letters (that are not in a dictionary,
e.g. “heyyyyy”, “sweeeet”) are mapped to their pre-
sumed correct spelling (e.g. “hey”, “sweet”).
</bodyText>
<sectionHeader confidence="0.998588" genericHeader="method">
5 Confidence-Based Classification
</sectionHeader>
<bodyText confidence="0.9999386">
To combine all of the classifiers, the system esti-
mates the confidence of each classifier and only ac-
cepts the classification output if the confidence is
higher than a specified baseline. To establish a clas-
sifier’s confidence, we take the absolute value of
the difference between a classifier’s positive output
probability and negative output probability, which
we call alpha. Alpha values close to 1 indicate high
confidence in the predicted label; values close to 0
indicate low confidence in the predicted label.
</bodyText>
<subsectionHeader confidence="0.990815">
5.1 Classifier Voting
</subsectionHeader>
<bodyText confidence="0.99975925">
The predicted accuracy of each classifier is deter-
mined after the trained classifiers are evaluated us-
ing a development set with known labels. Using the
dev set, we calculate the accuracy of each classi-
</bodyText>
<tableCaption confidence="0.8993128">
Table 2: An example of the polarity and corresponding
accuracy output for each classifier for a single tweet. The
labels (C) and (U) indicate whether the classifier was
trained on constrained training data or on unconstrained
data (Go et al., 2009).
</tableCaption>
<bodyText confidence="0.99989236">
fier at alpha values between 0 and 1. The result is
a trained classifier with an approximation of overall
classification accuracy at a given alpha value. Fig-
ure 1 shows the relationship between alpha value
and overall classifier accuracy. As expected, classi-
fication accuracy increases as confidence increases.
Table 2 shows the breakdown of classifier accu-
racy for a single tweet using both provided and ex-
ternal data. The accuracy listed is the classifier-
specific accuracy determined by the alpha value for
that phrase in the tweet. Using a dev set, we ex-
perimentally established the most effective baseline
to be 0.65. In the voting system described below,
only classifiers with confidence above the baseline
(per marked phrase) are used. Therefore, the spe-
cific combination of classifiers used for each phrase
may be different.
An unlabeled phrase is assigned a polarity and
confidence value from each classifier. These proba-
bilities are combined using a voting system to deter-
mine a single output. This voting system calculates
the final labeling by computing the average proba-
bility for each label only for those classifiers with
estimated accuracies above the baseline. The label
with the highest overall probability is selected.
</bodyText>
<sectionHeader confidence="0.999958" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999111">
The constrained submission only allowed for train-
ing on the provided data and placed 17 out of 23
entries. The unconstrained submission was trained
on both the provided data and the external data and
placed 6 out of 8 entries. Both submissions were
</bodyText>
<page confidence="0.971624">
427
</page>
<bodyText confidence="0.982620545454545">
unigram label bigram label lexicon label
aint neg school tomorrow neg bad neg
excited pos not going neg excited pos
sucks neg didn’t get neg tired neg
sick neg might not neg dead neg
poor neg gonna miss neg poor neg
smh pos still haven’t neg happy pos
tough pos breakout kings neg black neg
greatest pos work tomorrow neg good pos
f*ck neg ray lewis pos hate neg
nets neg can’t wait pos sorry neg
</bodyText>
<tableCaption confidence="0.998565">
Table 3: The most influential features from the unigram,
bigram, and lexicon classifiers.
</tableCaption>
<bodyText confidence="0.999973642857143">
evaluated using the Twitter and SMS data described
in Section 3.1. As mentioned, our system used a bi-
nary classifier, predicting only positive and negative
labels, making no neutral classifications.
The constrained system evaluated on the Twitter
test set had an F-measure of .672, with a high dis-
parity between the F-measure for tweets labeled as
positive versus those labeled as negative (.79 vs .53).
The unconstrained system on the Twitter test set un-
derperformed our constrained system, with an F-
measure of only .639.
The constrained system on the SMS test set
yielded an F-measure of .660; the unconstrained sys-
tem on the same data yielded an F-measure of .679.
</bodyText>
<subsectionHeader confidence="0.998559">
6.1 Features Extracted
</subsectionHeader>
<bodyText confidence="0.998030052631579">
The most important features extracted by the un-
igram, bigram and lexicon classifiers are shown
in Table 3. Features such as “ray lewis”, “smh”,
“school tomorrow”, “work tomorrow”, “breakout
kings” and “nets” demonstrate that the classifiers
formed a relationship between sentiment and collo-
quial language. An example of this understanding is
assigning a strong negative sentiment to “sucks” (as
the verb “to suck” does not carry sentiment). The bi-
grams “breakout kings”, “ray lewis” and “nets” are
interesting features because their sentiment is highly
cultural: “breakout kings” is a popular TV show that
was canceled, “ray lewis” a high profile player for
an NFL team, and “nets” a reference to the strug-
gling NBA basketball team. Expressions such as
“smh” (a widely-used abbreviation for “shaking my
head”) show how detecting tweet- and SMS-specific
language is important to understanding sentiment in
this domain.
</bodyText>
<sectionHeader confidence="0.998808" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999952428571429">
This supervised system combines many features
to classify positive and negative sentiment at the
phrase-level. Phrase-based isolation (Section 3.3)
limits irrelevant context in the model. By estimat-
ing classifier confidence on a per-phrase basis, the
system can prioritize confident classifiers and ignore
less-confident ones before combination.
Similar results on the Twitter and SMS data sets
indicates the similarity between the domains. The
external data improved the system on the SMS data
and reduced system accuracy on the Twitter data.
This difference in performance may be an indication
that the supplemental data set was noisier than we
expected, or that it was more applicable to the SMS
domain (SMS) than we anticipated.
There was a noticeable difference between pos-
itive and negative classification accuracy for all of
the submissions. This difference is likely due to ei-
ther a positive bias in training set used (the provided
training data is 64% positive, 36% negative) or a se-
lection of features that favored positive sentiment.
</bodyText>
<subsectionHeader confidence="0.964193">
7.1 Improvements and Future Work
</subsectionHeader>
<bodyText confidence="0.999992388888889">
Unfortunately, the time constraints of the evalua-
tion exercise led to a programming bug that wasn’t
caught until after the submission deadline. In pre-
processing, we accidentally stripped most of the
emoticon features out of the text. While it is un-
clear how much this would have effected our final
performance, such features have been demonstrated
as valuable in similar tasks. After fixing this bug
the system performs better in both constrained and
unconstrained situations (as evaluated on the devel-
opment set).
We would like to increase the size of external data
set to include all of the approximately 380K tweets
(rather than the 50K subset we used). This expanded
training set would likely improve the robustness of
the system. Specifically, we would expect classifiers
with limited coverage, such as the repeat-letter clas-
sifier, to yield increased performance.
</bodyText>
<page confidence="0.998353">
428
</page>
<sectionHeader confidence="0.98624" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.976551727272727">
Farah Benamara, Carmine Cesarano, Antonio Picariello,
Diego Reforgiato, and V Subrahmanian. 2007. Senti-
ment analysis: Adjectives and adverbs are better than
adjectives alone. In Proceedings of the International
Conference on Weblogs and Social Media (ICWSM).
A. Go, R. Bhayani, and Huang. L. 2009. Twitter senti-
ment classification using distant supervision. Techni-
cal report, Stanford University.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg. In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media, pages 538–541.
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of natural language processing, 2:568.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In Proceedings of the 2nd international
conference on Knowledge capture, K-CAP ’03, pages
70–77.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL 2013.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ’04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational linguistics,
37(2):267–307.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings of
the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1397–
1405.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proceedings of the 14th ACM international con-
ference on Information and knowledge management,
CIKM ’05, pages 625–631.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165–210.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval’13.
</reference>
<page confidence="0.99889">
429
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.534960">
<title confidence="0.99022">SwatCS: Combining simple classifiers with estimated accuracy</title>
<author confidence="0.924963">Clark</author>
<affiliation confidence="0.999791">Department of Computer</affiliation>
<address confidence="0.805007">Swarthmore Swarthmore, PA 19081</address>
<abstract confidence="0.995710416666667">This paper is an overview of the SwatCS system submitted to SemEval-2013 Task 2A: Contextual Polarity Disambiguation. The sentiment of individual phrases within a tweet are labeled using a combination of classifiers trained on a range of lexical features. The classifiers are combined by estimating the accuracy of the classifiers on each tweet. Performance is measured when using only the provided training data, and separately when including external data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Farah Benamara</author>
<author>Carmine Cesarano</author>
<author>Antonio Picariello</author>
<author>Diego Reforgiato</author>
<author>V Subrahmanian</author>
</authors>
<title>Sentiment analysis: Adjectives and adverbs are better than adjectives alone.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="1988" citStr="Benamara et al., 2007" startWordPosition="298" endWordPosition="302">ative sentiment. 2 Related Work A common goal of past research has been to discover and extract features from tweets that accurately indicate sentiment (Liu, 2010). The importance of feature selection and machine learning in sentiment analysis has been explored prior to the rise of social networks. For example, Pang and Lee (2004) apply machine learning techniques to extracted features from movie reviews. More recent feature-based systems include a lexicon-based approach (Taboada et al., 2011), and a more focused study on the importance of both adverbs and adjectives in determining sentiment (Benamara et al., 2007). Other examples include using looser descriptions of sentiment rather than rigid positive/negative labelings (Whitelaw et al., 2005) and investigating how connections between users can be used to predict sentiment (Tan et al., 2011). This task differs from past work in sentiment analysis of tweets because we aim to build a model capable of predicting the sentiment of sub-phrases within the tweet rather than considering the entire tweet. Specifically, “given a message containing a marked instance of a word or a phrase, determine whether that instance is positive, negative or neutral in that co</context>
</contexts>
<marker>Benamara, Cesarano, Picariello, Reforgiato, Subrahmanian, 2007</marker>
<rawString>Farah Benamara, Carmine Cesarano, Antonio Picariello, Diego Reforgiato, and V Subrahmanian. 2007. Sentiment analysis: Adjectives and adverbs are better than adjectives alone. In Proceedings of the International Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<marker>L, 2009</marker>
<rawString>A. Go, R. Bhayani, and Huang. L. 2009. Twitter sentiment classification using distant supervision. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the omg.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>538--541</pages>
<contexts>
<context position="2923" citStr="Kouloumpis et al., 2011" startWordPosition="445" endWordPosition="448">aim to build a model capable of predicting the sentiment of sub-phrases within the tweet rather than considering the entire tweet. Specifically, “given a message containing a marked instance of a word or a phrase, determine whether that instance is positive, negative or neutral in that context” (Wilson et al., 2013). Research on contextoriented polarity predates the emergence of social networks: (Nasukawa and Yi, 2003) predict sentiment of subsections in a larger document. N-gram features, part of speech features and “micro-blogging features” have been used as accurate indicators of polarity (Kouloumpis et al., 2011). The “micro-blogging features” are of particular interest as they provide insight into how users have adapted Twitter tokens to natural language to portray sentiment. These features include hashtags and emoticons (Kouloumpis et al., 2011). 425 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 425–429, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics 3 Data The task organizers provided a manually-labeled set of tweets. For parts of this study, their dat</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad and the omg. In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media, pages 538–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and subjectivity.</title>
<date>2010</date>
<booktitle>Handbook of natural language processing,</booktitle>
<pages>2--568</pages>
<contexts>
<context position="1529" citStr="Liu, 2010" startWordPosition="228" endWordPosition="229">as a platform that mitigates the interaction between individuals. Analysis of this Twitter data can provide insights into how users express themselves. For example, many new forms of expression and language features have emerged on Twitter, including expressions containing mentions, hashtags, emoticons, and abbreviations. This research leverages the lexical features in tweets to predict whether a phrase within a tweet conveys a positive or negative sentiment. 2 Related Work A common goal of past research has been to discover and extract features from tweets that accurately indicate sentiment (Liu, 2010). The importance of feature selection and machine learning in sentiment analysis has been explored prior to the rise of social networks. For example, Pang and Lee (2004) apply machine learning techniques to extracted features from movie reviews. More recent feature-based systems include a lexicon-based approach (Taboada et al., 2011), and a more focused study on the importance of both adverbs and adjectives in determining sentiment (Benamara et al., 2007). Other examples include using looser descriptions of sentiment rather than rigid positive/negative labelings (Whitelaw et al., 2005) and inv</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>Bing Liu. 2010. Sentiment analysis and subjectivity. Handbook of natural language processing, 2:568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Nasukawa</author>
<author>Jeonghee Yi</author>
</authors>
<title>Sentiment analysis: capturing favorability using natural language processing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd international conference on Knowledge capture, K-CAP ’03,</booktitle>
<pages>70--77</pages>
<contexts>
<context position="2721" citStr="Nasukawa and Yi, 2003" startWordPosition="414" endWordPosition="417">Whitelaw et al., 2005) and investigating how connections between users can be used to predict sentiment (Tan et al., 2011). This task differs from past work in sentiment analysis of tweets because we aim to build a model capable of predicting the sentiment of sub-phrases within the tweet rather than considering the entire tweet. Specifically, “given a message containing a marked instance of a word or a phrase, determine whether that instance is positive, negative or neutral in that context” (Wilson et al., 2013). Research on contextoriented polarity predates the emergence of social networks: (Nasukawa and Yi, 2003) predict sentiment of subsections in a larger document. N-gram features, part of speech features and “micro-blogging features” have been used as accurate indicators of polarity (Kouloumpis et al., 2011). The “micro-blogging features” are of particular interest as they provide insight into how users have adapted Twitter tokens to natural language to portray sentiment. These features include hashtags and emoticons (Kouloumpis et al., 2011). 425 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pa</context>
</contexts>
<marker>Nasukawa, Yi, 2003</marker>
<rawString>Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment analysis: capturing favorability using natural language processing. In Proceedings of the 2nd international conference on Knowledge capture, K-CAP ’03, pages 70–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1698" citStr="Pang and Lee (2004)" startWordPosition="254" endWordPosition="257">ple, many new forms of expression and language features have emerged on Twitter, including expressions containing mentions, hashtags, emoticons, and abbreviations. This research leverages the lexical features in tweets to predict whether a phrase within a tweet conveys a positive or negative sentiment. 2 Related Work A common goal of past research has been to discover and extract features from tweets that accurately indicate sentiment (Liu, 2010). The importance of feature selection and machine learning in sentiment analysis has been explored prior to the rise of social networks. For example, Pang and Lee (2004) apply machine learning techniques to extracted features from movie reviews. More recent feature-based systems include a lexicon-based approach (Taboada et al., 2011), and a more focused study on the importance of both adverbs and adjectives in determining sentiment (Benamara et al., 2007). Other examples include using looser descriptions of sentiment rather than rigid positive/negative labelings (Whitelaw et al., 2005) and investigating how connections between users can be used to predict sentiment (Tan et al., 2011). This task differs from past work in sentiment analysis of tweets because we</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexicon-based methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational linguistics,</journal>
<pages>37--2</pages>
<contexts>
<context position="1864" citStr="Taboada et al., 2011" startWordPosition="277" endWordPosition="280">This research leverages the lexical features in tweets to predict whether a phrase within a tweet conveys a positive or negative sentiment. 2 Related Work A common goal of past research has been to discover and extract features from tweets that accurately indicate sentiment (Liu, 2010). The importance of feature selection and machine learning in sentiment analysis has been explored prior to the rise of social networks. For example, Pang and Lee (2004) apply machine learning techniques to extracted features from movie reviews. More recent feature-based systems include a lexicon-based approach (Taboada et al., 2011), and a more focused study on the importance of both adverbs and adjectives in determining sentiment (Benamara et al., 2007). Other examples include using looser descriptions of sentiment rather than rigid positive/negative labelings (Whitelaw et al., 2005) and investigating how connections between users can be used to predict sentiment (Tan et al., 2011). This task differs from past work in sentiment analysis of tweets because we aim to build a model capable of predicting the sentiment of sub-phrases within the tweet rather than considering the entire tweet. Specifically, “given a message con</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexicon-based methods for sentiment analysis. Computational linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhao Tan</author>
<author>Lillian Lee</author>
<author>Jie Tang</author>
<author>Long Jiang</author>
<author>Ming Zhou</author>
<author>Ping Li</author>
</authors>
<title>User-level sentiment analysis incorporating social networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>1397--1405</pages>
<contexts>
<context position="2221" citStr="Tan et al., 2011" startWordPosition="333" endWordPosition="336">lysis has been explored prior to the rise of social networks. For example, Pang and Lee (2004) apply machine learning techniques to extracted features from movie reviews. More recent feature-based systems include a lexicon-based approach (Taboada et al., 2011), and a more focused study on the importance of both adverbs and adjectives in determining sentiment (Benamara et al., 2007). Other examples include using looser descriptions of sentiment rather than rigid positive/negative labelings (Whitelaw et al., 2005) and investigating how connections between users can be used to predict sentiment (Tan et al., 2011). This task differs from past work in sentiment analysis of tweets because we aim to build a model capable of predicting the sentiment of sub-phrases within the tweet rather than considering the entire tweet. Specifically, “given a message containing a marked instance of a word or a phrase, determine whether that instance is positive, negative or neutral in that context” (Wilson et al., 2013). Research on contextoriented polarity predates the emergence of social networks: (Nasukawa and Yi, 2003) predict sentiment of subsections in a larger document. N-gram features, part of speech features and</context>
</contexts>
<marker>Tan, Lee, Tang, Jiang, Zhou, Li, 2011</marker>
<rawString>Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming Zhou, and Ping Li. 2011. User-level sentiment analysis incorporating social networks. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1397– 1405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Navendu Garg</author>
<author>Shlomo Argamon</author>
</authors>
<title>Using appraisal groups for sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management, CIKM ’05,</booktitle>
<pages>625--631</pages>
<contexts>
<context position="2121" citStr="Whitelaw et al., 2005" startWordPosition="317" endWordPosition="320">indicate sentiment (Liu, 2010). The importance of feature selection and machine learning in sentiment analysis has been explored prior to the rise of social networks. For example, Pang and Lee (2004) apply machine learning techniques to extracted features from movie reviews. More recent feature-based systems include a lexicon-based approach (Taboada et al., 2011), and a more focused study on the importance of both adverbs and adjectives in determining sentiment (Benamara et al., 2007). Other examples include using looser descriptions of sentiment rather than rigid positive/negative labelings (Whitelaw et al., 2005) and investigating how connections between users can be used to predict sentiment (Tan et al., 2011). This task differs from past work in sentiment analysis of tweets because we aim to build a model capable of predicting the sentiment of sub-phrases within the tweet rather than considering the entire tweet. Specifically, “given a message containing a marked instance of a word or a phrase, determine whether that instance is positive, negative or neutral in that context” (Wilson et al., 2013). Research on contextoriented polarity predates the emergence of social networks: (Nasukawa and Yi, 2003)</context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 2005. Using appraisal groups for sentiment analysis. In Proceedings of the 14th ACM international conference on Information and knowledge management, CIKM ’05, pages 625–631.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="7071" citStr="Wiebe et al., 2005" startWordPosition="1123" endWordPosition="1126">he substring “NOT” prefixed to it. This negation scheme was applied to n-gram features and lexicon features. 4.1 N-gram Features Rather than use all of the n-grams as features, we ranked each n-gram (w/POS tags) by calculating its chi-square-based information gain. The top 2000 n-grams (1000 positive, 1000 negative) are used as features in the n-gram classifier. Both a unigram and bigram classifier use these ranked (word/POS) features. Table 1 shows the highest ranked unigrams and bigrams using this method. 4.2 Sentiment Lexicon Features A second classifier uses the MPQA subjectivity lexicon (Wiebe et al., 2005). We extract both the polarity and the polarity strength for each word/POS in the lexicon matching a word/POS in the phrase’s context. We refer to this classifier as the lexicon classifier. 426 rank classifier data polarity acc 1 unigrams (C) positive 0.89 2 unigrams (U) positive 0.88 3 lexicon (C) negative 0.83 4 lexicon (U) negative 0.81 5 tagcount (C) positive 0.78 6 bigrams (C) positive 0.75 7 tagcount (U) novote &lt;0.65 8 bigrams (U) novote &lt;0.65 Alpha vs. Classifier accuracy Confidence/classifier accuracy 0.98 0.96 0.94 0.92 0.88 0.86 0.84 0.82 0.78 0.9 0.8 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
</authors>
<title>Zornitsa Kozareva, Preslav Nakov,</title>
<date>2013</date>
<location>Sara Rosenthal, Veselin</location>
<marker>Wilson, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.</rawString>
</citation>
<citation valid="false">
<title>SemEval-2013 task 2: Sentiment analysis in twitter.</title>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval’13.</booktitle>
<marker></marker>
<rawString>SemEval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation, SemEval’13.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>