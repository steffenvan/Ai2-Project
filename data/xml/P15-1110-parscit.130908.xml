<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99674">
Feature Optimization for Constituent Parsing via Neural Networks
</title>
<author confidence="0.975335">
Zhiguo Wang
</author>
<affiliation confidence="0.847403">
IBM Watson
</affiliation>
<address confidence="0.644919">
1101 Kitchawan
Yorktown Heights, NY, USA
</address>
<email confidence="0.985614">
zhigwang@us.ibm.com
</email>
<author confidence="0.844084">
Haitao Mi
</author>
<affiliation confidence="0.735747">
IBM Watson
</affiliation>
<address confidence="0.647805">
1101 Kitchawan
Yorktown Heights, NY, USA
</address>
<email confidence="0.983605">
hmi@us.ibm.com
</email>
<author confidence="0.942113">
Nianwen Xue
</author>
<affiliation confidence="0.919866">
Brandeis University
</affiliation>
<address confidence="0.818442">
415 South St
Waltham, MA, USA
</address>
<email confidence="0.996594">
xuen@brandeis.edu
</email>
<sectionHeader confidence="0.98289" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99976375862069">
The performance of discriminative con-
stituent parsing relies crucially on feature
engineering, and effective features usu-
ally have to be carefully selected through
a painful manual process. In this paper,
we propose to automatically learn a set
of effective features via neural networks.
Specifically, we build a feedforward neu-
ral network model, which takes as input
a few primitive units (words, POS tags
and certain contextual tokens) from the lo-
cal context, induces the feature represen-
tation in the hidden layer and makes pars-
ing predictions in the output layer. The
network simultaneously learns the feature
representation and the prediction model
parameters using a back propagation al-
gorithm. By pre-training the model on a
large amount of automatically parsed data,
and then fine-tuning on the manually an-
notated Treebank data, our parser achieves
the highest F1 score at 86.6% on Chi-
nese Treebank 5.1, and a competitive F1
score at 90.7% on English Treebank. More
importantly, our parser generalizes well
on cross-domain test sets, where we sig-
nificantly outperform Berkeley parser by
3.4 points on average for Chinese and 2.5
points for English.
</bodyText>
<sectionHeader confidence="0.995137" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98711302">
Constituent parsing seeks to uncover the phrase
structure representation of sentences that can be
used in a variety of natural language applications
such as machine translation, information extrac-
tion and question answering (Jurafsky and Martin,
2008). One of the major challenges for this task is
that constituent parsers require an inference algo-
rithm of high computational complexity in order
to search over their large structural space, which
makes it very hard to efficiently train discrimina-
tive models. So, for a long time, the task was
mainly solved with generative models (Collins,
1999; Charniak, 2000; Petrov et al., 2006). In
the last few years, however, with the use of ef-
fective parsing strategies, approximate inference
algorithms, and more efficient training methods,
discriminative models began to surpass the gen-
erative models (Carreras et al., 2008; Zhu et al.,
2013; Wang and Xue, 2014).
Just like other NLP tasks, the performance of
discriminative constituent parsing crucially relies
on feature engineering. If the feature set is too
small, it might underfit the model and leads to low
performance. On the other hand, too many fea-
tures may result in an overfitting problem. Usu-
ally, an effective set of features have to be de-
signed manually and selected through repeated ex-
periments (Sagae and Lavie, 2005; Wang et al.,
2006; Zhang and Clark, 2009). Not only does
this procedure require a lot of expertise, but it
is also tedious and time-consuming. Even af-
ter this painstaking process, it is still hard to say
whether the selected feature set is complete or op-
timal to obtain the best possible results. A more
desirable alternative is to learn features automat-
ically with machine learning algorithms. Lei et
al. (2014) proposed to learn features by represent-
ing the cross-products of some primitive units with
low-rank tensors for dependency parsing. How-
ever, to achieve competitive performance, they had
to combine the learned features with the tradi-
tional hand-crafted features. For constituent pars-
ing, Henderson (2003) employed a recurrent neu-
ral network to induce features from an unbounded
parsing history. However, the final performance
was below the state of the art.
In this work, we design a much simpler neu-
ral network to automatically induce features from
just the local context for constituent parsing. Con-
1138
</bodyText>
<note confidence="0.997653666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1138–1147,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999943071428572">
cretely, we choose the shift-reduce parsing strat-
egy to build the constituent structure of a sentence,
and train a feedforward neural network model
to jointly learn feature representations and make
parsing predictions. The input layer of the net-
work takes as input a few primitive units (words,
POS tags and certain contextual tokens) from the
local context, the hidden layer aims to induce
a distributed feature representation by combining
all the primitive units with different weights, and
the output layer attempts to make parsing predic-
tions based on the feature representation. Dur-
ing the training process, the model simultaneously
learns the feature representation and prediction
model parameters using a backpropagation algo-
rithm. Theoretically, the learned feature represen-
tation is optimal (or at least locally optimal) for
the parsing predictions. In practice, however, our
model does not work well if it is only trained on
the manually annotated Treebank data sets. How-
ever, when pre-trained on a large amount of auto-
matically parsed data and then fine-tuned on the
Treebank data sets, our model achieves a fairly
large improvement in performance. We evaluated
our model on both Chinese and English. On stan-
dard data sets, our model reaches F1 = 86.6%
for Chinese and outperforms all the state-of-the-
art systems, and for English our final performance
is F1 = 90.7% and this result surpasses that of
all the previous neural network based models and
is comparable to the state-of-the-art systems. On
cross-domain data sets, our model outperforms the
Berkeley Parser 1 by 3.4 percentage points for Chi-
nese and 2.5 percentage points for English.
The remainder of this paper is organized as fol-
lows: Section 2 introduces the shift-reduce con-
stituent parsing approach. Section 3 describes our
feature optimization model and some parameter
estimation techniques. We discuss and analyze
our experimental results in Section 4. Section 5
discusses related work. Finally, we conclude this
paper in Section 6.
</bodyText>
<sectionHeader confidence="0.866744" genericHeader="method">
2 Shift-Reduce Constituent Parsing
</sectionHeader>
<bodyText confidence="0.829355">
Shift-reduce constituent parsing utilizes a series of
shift-reduce decisions to construct syntactic trees.
Formally, the shift-reduce system is a quadruple
C = (S, T, s0, St), where S is a set of parser
states (sometimes called configurations), T is a fi-
nite set of actions, s0 is an initialization function
</bodyText>
<figure confidence="0.915270142857143">
g h t (
1https://code.google.com/p/berkeleyparser/
FT-X(sh-x):removethe first wordfrom β,a
ssi gna POS tagXtoth ewor d a ndpushi tont
othe t opo fσ;• REDU C E-UN ARY-X(r u-x) :p
opth etop unary node subtree fromσ, construct a new
e push he labeledwith X for the subtree, th
f the nw subre bck onto σ. The head o
ld; nw subtree is inhrited from its chi
op • REDUCE-BINARY-{L/R}-X (rl/r-x): p
the top two subtrees from σ, combin them
t a new tre wth a nod labeled with X, in
h th new subtree back onto σ. The thn pus
R) versions of left the (L) and action ri
</figure>
<bodyText confidence="0.886506434782609">
indicate whether the head of the new subtree
is inherited from its left or right child.
With these actions, our parser can process
trees with unary and binary branches easily.
For example, in Figure 1, for the sentence “the
assets are sold”, our parser can construct the
action t E T is a tranNPtisp functiJJ tota lDTt h
eVBGl eavi n g,, SBARSV P VPVBNs oldVB P a r eN
PNNSas se t sDTth e I Nas PRT RPoffV B Np a idVBb
eMDwo ul daiNNdebtD TT hatd Figure1 : Anexamp leof
con stituen t tree.tomap e ach i np u t sent encei
nt oa un iqu einitia lstate,an dSt∈S is a set ofterm
in alstates.Ea cha ction t∈ T isat ran si tionf uncti o
nth a tmaps a stateintoa new state. Apars erstate s ∈
Sis definedasatuples= (σ,β),whereσ isastack
h ichis maintainedtoholdpartialsubtrees that w
realready constructed, and βis aqueue which a
used forstoringremainingunprocessed words. is
particular,theinitialstatehasanemptystackσ In
da ni queue ng β the contai entireinputsentence, a n
stateshavean emptyqueueβand andthe terminal astac k σcontain in
gonly one completep arse tre e.T het askofp ars in
gistoscan thei npu tse ntencef
</bodyText>
<equation confidence="0.83317975">
r omleft t o right a ndperf orm a seq uenc eofs
hi ft-red u cea cti o ns tot rans for mthe in
itia lst ate in to
a terminal state. Inorder toj oin tly
</equation>
<bodyText confidence="0.85848325">
assignP OSta gs andconstr u cta const itue
ntstruc ture f ora nin putsente nce, wede fine
the followi ngac tion s f ort heac ti ons
etT ,follow in gWangandX ue(2 014 ):•SHI
</bodyText>
<page confidence="0.560419">
1139
</page>
<bodyText confidence="0.999461454545455">
parse tree by performing the action sequence
{sh-DT, sh-NNS, rr-NP, sh-VBP,
sh-VBN, ru-VP, rr-VP, rr-S}. To pro-
cess multi-branch trees, we employ binarization
and debinarization processes described in Zhang
and Clark (2009) to transform multi-branch trees
into binary trees and restore the generated binary
trees back to their original forms. For inference,
we employ the beam search decoding algorithm
(Zhang and Clark, 2009) to balance the tradeoff
between accuracy and efficiency.
</bodyText>
<sectionHeader confidence="0.99687" genericHeader="method">
3 Feature Optimization Model
</sectionHeader>
<subsectionHeader confidence="0.99806">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.999986189189189">
To determine which action t E T should be per-
formed at a given state s E S, we need a model
to score each possible (s, t) combination. In pre-
vious approaches (Sagae and Lavie, 2005; Wang
et al., 2006; Zhang and Clark, 2009), the model is
usually defined as a linear model Score(s, t) =
2Vi · Φ(s, t), where Φ(s, t) is a vector of hand-
crafted features for each state-action pair and 2Vi
is the weight vector for these features. The hand-
crafted features are usually constructed by com-
pounding primitive units according to some fea-
ture templates. For example, almost all the pre-
vious work employed the list of primitive units in
Table 1(a), and constructed hand-crafted features
by concatenating these primitive units according
to the feature templates in Table 1(b). Obviously,
these feature templates are only a small subset of
the cross products of all the primitive units. This
feature set is the result of a large number of exper-
iments through trial and error from previous work.
Still we cannot say for sure that this is the optimal
subset of features for the parsing task.
To cope with this problem, we propose to si-
multaneously optimize feature representation and
parsing accuracy via a neural network model. Fig-
ure 2 illustrates the architecture of our model. Our
model consists of input, projection, hidden and
output layers. First, in the input layer, all primi-
tive units (shown in Table 1(a)) are imported to the
network. We also import the suffixes and prefixes
of the first word in the queue, because these units
have been shown to be very effective for predict-
ing POS tags (Ratnaparkhi, 1996). Then, in the
projection layer, each primitive unit is projected
into a vector. Specifically, word-type units are
represented as word embeddings, and other units
are transformed into one-hot representations. The
</bodyText>
<equation confidence="0.963471">
(1) p0w, p0t,p0c, p1w, p1t,p1c,
p2w, p2t,p2c, p3w, p3t,p3c
(2) p0lw, p0lc, p0rw, p0rc,p0uw, p0uc,
p1lw, p1lc, p1rw, p1rc,p1uw, p1uc
(3) q0w, q1w, q2w, q3w
(a) Primitive Units
p0tc, p0wc, p1tc, p1wc, p2tc
p2wc, p3tc, p3wc, q0wt, q1wt
q2wt, q3wt, p0lwc, p0rwc
p0uwc, p1lwc, p1rwc, p1uwc
bigrams p0wp1w, p0wp1c, p0cp1w, p0cp1c
p0wq0w, p0wq0t, p0cq0w, p0cq0t
q0wq1w, q0wq1t, q0tq1w, q0tq1t
p1wq0w, p1wq0t, p1cq0w, p1cq0t
p0cp1cp2c, p0wp1cp2c, p0cp1wq0t
trigrams p0cp1cp2w, p0cp1cq0t, p0wp1cq0t
p0cp1wq0t, p0cp1cq0w
</equation>
<listItem confidence="0.414561">
(b) Feature Templates
</listItem>
<tableCaption confidence="0.6347515">
Table 1: Primitive units (a) and feature templates
(b) for shift-reduce constituent parsing, where pi
</tableCaption>
<bodyText confidence="0.999388608695652">
represents the ith subtree in the stack and qi de-
notes the ith word in the queue. w refers to the
head word, t refers to the head POS, and c refers
to the constituent label. pil and pir refer to the
left and right child for a binary subtree pi, and piu
refers to the child of a unary subtree pi.
vectors of all primitive units are concatenated to
form a holistic vector for the projection layer. The
hidden layer corresponds to the feature representa-
tion we want to learn. Each dimension in the hid-
den layer can be seen as an abstract factor of all
primitive units, and it calculates a weighted sum
of all nodes from the projection layer and applies
a non-linear activation function to yield its acti-
vation. We choose the logistic sigmoid function
for the hidden layer. The output layer is used for
making parsing predictions. Each node in the out-
put layer corresponds to a shift-reduce action. We
want to interpret the activation of the output layer
as a probability distribution over all possible shift-
reduce actions, therefore we normalize the out-
put activations (weighted summations of all nodes
from the hidden layer) with the softmax function.
</bodyText>
<subsectionHeader confidence="0.997732">
3.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.9309105">
Our model consists of three groups of parameters:
(1) the word embedding for each word type unit,
</bodyText>
<figure confidence="0.9961635">
unigrams
1140
word embedding one-hot representation
dense real-valued
low-dimensional vector
p(t|s )
softmax
... ...
sigmoid
... ... ... ...
...
...
... ...
... ...
...
...
... ... ...
w0 wm t0 tn c0 cn suffix1
...
prefix4
</figure>
<bodyText confidence="0.998454444444444">
lem. However, different from Hinton et al. (2012),
we only use Dropout during testing, because we
found that using Dropout during training did not
improve the parsing performan
ce (on the dev set)
while greatly slowing down the training process.
stituent parsing, where
denotes word type unit,
ti denotes POS tag unit,
</bodyText>
<equation confidence="0.761925166666667">
denotes constituent la-
bel unit,
an
wi
ci
suffixi
</equation>
<bodyText confidence="0.968539678571428">
d prefixi (1 &lt;_ i &lt;_ 4) de-
notes i-character word suffix or prefix for the first
word in the queue.
301-325 as the development set, and Articles 271-
ce.
the learning rate is adapted differently for
different training steps. With this
technique, we can start with a very large learning
rate which decreases during training, and can thus
perform a far more thorough search within the pa-
rameter space. In our experiments, we got a much
faster convergence rate with slightly better accu-
racy by using the learning rate
= 1 instead of
the commonly-used
= 0.01. Second, we initial-
ize the model parameters by pre-training. Unsu-
pervised pre-training has demonstrated its effec-
tiveness as a way of initializing neural network
models (Erhan et al., 2010). Since our model re-
quires many run-time primitive units (POS tags
and constituent labels), we employ an in-house
shift-reduce parser to parse a large amount of unla-
beled sentences, and pre-train the model with the
automatically parsed data. Third, we utilize the
Dropout strategy to address the overfitt
differ-
ent parameters at
</bodyText>
<figure confidence="0.806639333333333">
α
α
ing prob-
</figure>
<figureCaption confidence="0.986178">
Figure 2: Neural network architecture for con-
</figureCaption>
<bodyText confidence="0.9991978">
(2) the connections between the projection layer
and the hidden layer which are used for learning
an optimal feature representation and (3) the con-
nections between the hidden layer and the output
layer which are used for making accurate pars-
ing predictions. We decided to learn word em-
beddings separately, so that we can take advantage
of a large amount of unlabeled data. The remain-
ing two groups of parameters can be trained si-
multaneously by the back propagation algorithm
(Rumelhart et al., 1988) to maximize the likeli-
hood over the training data.
We also employ three crucial techniques to seek
more effective parameters. First, we utilize mini-
batched AdaGrad (Duchi et al., 2011), in which
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.997681">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.9999647">
We conducted experiments on the Penn Chinese
Treebank (CTB) version 5.1 (Xue et al., 2005) and
the Wall Street Journal (WSJ) portion of Penn En-
glish Treebank (Marcus et al., 1993). To fairly
compare with other work, we follow the standard
data division. For Chinese, we allocated Articles
001-270 and 400-1151 as the training set, Articles
300 as the testing set. For English, we use sec-
tions 2-21 for training, section 22 for developing
and section 23 for testing.
We also utilized some unlabeled corpora and
used the word2vec2 toolkit to train word em-
beddings. For Chinese, we used the unlabeled
Chinese Gigaword (LDC2003T09) and performed
Chinese word segmentation using our in-house
segmenter. For English, we randomly selected 9
million sentences from our in-house newswire cor-
pus, which has no overlap with our training, test-
ing and development sets. We use Evalb3 toolkit
to evaluate parsing performan
</bodyText>
<subsectionHeader confidence="0.989333">
4.2 Characteristics of Our Model
</subsectionHeader>
<bodyText confidence="0.9989908">
There are several hyper-parameters in our model,
e.g., the word embedding dimension (wordDim),
the hidden layer node size (hidden5ize), the
Dropout ratio (dropRatio) and the beam size for
inference (beam5ize). The choice of these hyper-
parameters may affect the final performance. In
this subsection, we present some experiments to
demonstrate the characteristics of our model, and
select a group of proper hyper-parameters that we
use to evaluate our final model. All the experi-
ments in this subsection were performed on Chi-
nese data and the evaluation is performed on Chi-
nese development set.
First, we evaluated the effectiveness of vari-
ous primitive units. We set wordDim = 300,
</bodyText>
<figure confidence="0.752854428571428">
2https://code.google.com/p/word2vec/
3http://nlp.cs.nyu.edu/evalb/
hidden5ize = 300, beam5ize = 8, an
d did not
apply Dropout (dropRatio = 0). Table 2 presents
the results. By comparing numbers in other rows
1141
</figure>
<figureCaption confidence="0.992381">
Figure 3: Influence of hyper-parameters.
</figureCaption>
<figure confidence="0.988852692307692">
50 100 300 500 1000
(b) hidden layer size
1 2 3 4 5 6 7 8 9 10
(d) beam size
0 0.2 0.4 0.6 0.8
(c) Dropout ratio
50 100 300 500 1000
(a) word embedding dimension
87
88
86
86
76
88
87
86
85
84
83
82
81
80
90
84
82
80
78
88
85
84
83
82
81
80
87
86
85
84
83
</figure>
<bodyText confidence="0.9990337">
with row “All Units”, we found that ablating the
Prefix and Suffix units (“w/o Prefix &amp; Suffix”)
significantly hurts both POS tagging and parsing
performance. Ablating POS units (“w/o POS”)
or constituent label units (“w/o NT”) has little ef-
fect on POS tagging accuracy, but hurts parsing
performance. When only keeping the word type
units (“Only Word”), both the POS tagging and
parsing accuracy drops drastically. So the Prefix
and Suffix units are crucial for POS tagging, and
POS units and constituent label units are helpful
for parsing performance. All these primitive units
are indispensable to better performance.
Second, we uncovered the effect of the dimen-
sion of word embedding. We set hidden5ize =
300, beam5ize = 8, dropRatio = 0 and var-
ied wordDim among 150, 100, 300, 500, 10001.
Figure 3(a) draws the parsing performance curve.
When increasing wordDim from 50 to 300, pars-
ing performance improves more than 1.5 percent-
age points. After that, the curve flattens out, and
parsing performance only gets marginal improve-
ment. Therefore, in the following experiments, we
fixed wordDim = 300.
Third, we tested the effect of hidden layer node
size. We varied hidden5ize among 150, 100,
300, 500, 10001. Figure 3(b) draws the pars-
ing performance curve. We found increasing
hidden5ize is helpful for parsing performance.
However, higher hidden5ize would greatly in-
crease the amount of computation. To keep the
efficiency of our model, we fixed hidden5ize =
300 in the following experiments.
Fourth, we applied Dropout and tuned the
Dropout ratio through experiments. Figure 3(c)
shows the results. We found that the peak
performance occurred at dropRatio = 0.5,
which brought about an improvement of more
than 1 percentage point over the model without
Dropout (dropRatio = 0). Therefore, we fixed
</bodyText>
<table confidence="0.9979875">
Primitive Units F1 POS
All Units 86.7 96.7
w/o Prefix &amp; Suffix 85.7 95.4
w/o POS 86.0 96.7
w/o NT 86.2 96.6
Only Word 82.7 95.2
</table>
<tableCaption confidence="0.998613">
Table 2: Influence of primitive units.
</tableCaption>
<bodyText confidence="0.981443571428572">
dropRatio = 0.5.
Finally, we investigated the effect of beam size.
Figure 3(d) shows the curve. We found increasing
beam5ize greatly improves the performance ini-
tially, but no further improvement is observed after
beam5ize is greater than 8. Therefore, we fixed
beam5ize = 8 in the following experiments.
</bodyText>
<subsectionHeader confidence="0.996716">
4.3 Semi-supervised Training
</subsectionHeader>
<bodyText confidence="0.99849335">
In this subsection, we investigated whether we
can train more effective models using automati-
cally parsed data. We randomly selected 200K
sentences from our unlabeled data sets for both
Chinese and English. Then, we used an in-house
shift-reduce parser4 to parse these selected sen-
tences. The size of the automatically parsed data
set may have an impact on the final model. So
we trained many models with varying amounts of
automatically parsed data. We also designed two
strategies to exploit the automatically parsed data.
The first strategy (Mix-Train) is to directly add the
automatically parsed data to the hand-annotated
training set and train models with the mixed data
set. The second strategy (Pre-Train) is to first pre-
train models with the automatically parsed data,
and then fine-tune models with the hand-annotated
training set.
Table 3 shows results of different experimen-
tal configurations for Chinese. For the Mix-Train
</bodyText>
<table confidence="0.840694217391304">
4Its performance is Fl =83.9 on Chinese and Fl =90.8%
on English.
1142
Mix-Train Pre-Train
# Auto Sent F1 POS F1 POS
0 87.8 97.0 — —
50K 87.2 96.8 88.4 97.1
100K 88.7 96.9 89.5 97.1
200K 89.2 97.2 89.5 97.4
Type System F1
Supervised*$ 83.2
Ours Pretrain-Finetune*$ 86.6
Petrov and Klein (2007) 83.3
SI Wang and Xue (2014)$ 83.6
Zhu et al. (2013)$ 85.6
Wang and Xue (2014)$ 86.3
Table 3: Semi-supervised training for Chinese. SE
Mix-Train Pre-Train
# Auto Sent F1 POS F1 POS
0 89.7 96.6 — —
50K 89.4 96.1 90.2 96.4
100K 89.5 96.0 90.4 96.5
200K 89.2 95.8 90.8 96.7
</table>
<tableCaption confidence="0.999681">
Table 4: Semi-supervised training for English.
</tableCaption>
<bodyText confidence="0.9999773125">
strategy, when we only use 50K automatically
parsed sentences, the performance drops in com-
parison with the model trained without using any
automatically parsed data. When we increase the
automatically parsed data to 100K sentences, the
parsing performance improves about 1 percent but
the POS tagging accuracy drops slightly. When
we further increase the automatically parsed data
to 200K sentences, both the parsing performance
and POS tagging accuracy improve. For the Pre-
Train strategy, the performance of all three config-
urations improves performance against the model
that does not use any automatically parsed data.
The Pre-Train strategy consistently outperforms
the Mix-Train strategy when the same amount of
automatically parsed data is used. Therefore, for
Chinese, the Pre-Train strategy is much more help-
ful, and the more automatically parsed data we use
the better performance we get.
Table 4 presents results of different experimen-
tal configurations for English. The performance
trend for the Mix-Train strategy is different from
that of Chinese. Here, no matter how much auto-
matically parsed data we use, there is a consistent
degradation in performance against the model that
does not use any automatically parsed data at all.
And the more automatically parsed data we use,
the larger the drop in accuracy. For the Pre-Train
strategy, the trend is similar to Chinese. The pars-
ing performance of the Pre-Train setting consis-
tently improves as the size of automatically parsed
data increases.
</bodyText>
<note confidence="0.6738345">
Charniak and Johnson (2005) 82.3
RE Wang and Zong (2011) 85.7
</note>
<tableCaption confidence="0.88867">
Table 5: Comparison with the state-of-the-art sys-
tems on Chinese test set. * marks neural network
</tableCaption>
<bodyText confidence="0.8715185">
based systems. $ marks shift-reduce parsing sys-
tems.
</bodyText>
<subsectionHeader confidence="0.9984865">
4.4 Comparing With State-of-the-art
Systems
</subsectionHeader>
<bodyText confidence="0.9999770625">
In this subsection, we present the performance
of our models on the testing sets. We trained
two systems. The first system (“Supervised”)
is trained only with the hand-annotated training
set, and the second system (“Pretrain-Finetune”)
is trained with the Pre-Train strategy described
in subsection 4.3 using additional automatically
parsed data. The best parameters for the two sys-
tems are set based on their performance on the de-
velopment set. To further illustrate the effective-
ness of our systems, we also compare them with
some state-of-the-art systems. We group parsing
systems into three categories: supervised single
systems (SI), semi-supervised single systems (SE)
and reranking systems (RE). Both of our two mod-
els belong to semi-supervised single systems, be-
cause our “Supervised” system utilized word em-
beddings in its input layer.
Table 5 lists the performance of our systems as
well as the state-of-the-art systems on Chinese test
set. Comparing the performance of our two sys-
tems, we see that our “Pretrain-Finetune” system
shows a fairly large gain over the “Supervised”
system. One explanation is that our neural net-
work model is a non-linear model, so the back
propagation algorithm can only reach a local op-
timum. In our “Supervised” system the starting
points are randomly initialized in the parameter
space, so it only reaches local optimum. In com-
parison, our “Pretrain-Finetune” system gets to
see large amount of automatically parsed data, and
initializes the starting points with the pre-trained
</bodyText>
<table confidence="0.964433833333333">
1143
Type System F1
Supervised*$ 89.4
Ours Pretrain-Finetune*$ 90.7
Collins (1999) 88.2
Charniak (2000) 89.5
Henderson (2003)* 88.8
SI Petrov and Klein (2007) 90.1
Carreras et al. (2008) 91.1
Zhu et al. (2013)$ 90.4
Huang et al. (2010) 91.6
SE Zhu et al. (2013)$ 91.3
Collobert (2011)* 89.1
Henderson (2004)* 90.1
Charniak and Johnson (2005) 91.5
McClosky et al. (2006) 92.3
RE Huang (2008) 91.7
Socher et al. (2013)* 90.4
</table>
<tableCaption confidence="0.918209">
Table 6: Comparing with the state-of-the-art sys-
tems on English test set. * marks neural network
</tableCaption>
<bodyText confidence="0.998246576923077">
based systems. $ marks shift-reduce parsing sys-
tems.
parameters. So it finds a much better local opti-
mum than the “Supervised” system. Comparing
our “Pretrain-Finetune” system with all the state-
of-the-art systems, we see our system surpass all
the other systems. Although our system only uti-
lizes some basic primitive units (in Table 1(a)),
it still outperforms Wang and Xue (2014)’s shift-
reduce parsing system which uses more complex
structural features and semi-supervised word clus-
ter features. Therefore, our model can simultane-
ously learn an effective feature representation and
make accurate parsing predictions for Chinese.
Table 6 presents the performance of our systems
as well as the state-of-the-art systems on the En-
glish test set. Our “Pretrain-Finetune” system still
achieves much better performance than the “Su-
pervised” system, although the gap is smaller than
that of Chinese. Our “Pretrain-Finetune” system
also outperforms all other neural network based
systems (systems marked with *). Although our
system does not outperform all the state-of-the-art
systems, the performance is comparable to most
of them. So our model is also effective for English
parsing.
</bodyText>
<subsectionHeader confidence="0.998836">
4.5 Cross Domain Evaluation
</subsectionHeader>
<bodyText confidence="0.999948020833334">
In this subsection, we examined the robustness of
our model by evaluating it on data sets from var-
ious domains. We use the Berkeley Parser as our
baseline parser, and trained it on our training set.
For Chinese, we performed our experiments on
the cross domain data sets from Chinese Treebank
8.0 (Xue et al., 2013). It consists of six domains:
newswire (nw), magazine articles (mz), broadcast
news (bn), broadcast conversation (bc), weblogs
(wb) and discussion forums (df). Since all of the
mz domain data is already included in our train-
ing set, we only selected sample sentences from
the other five domains as the test sets 5, and made
sure these test sets had no overlap with our tree-
bank training, development and test sets. Note
that we did not use any data from these five do-
mains for training or development. The models
are still the ones described in the previous sub-
section. The results are presented in Table 7. Al-
though our “Supervised” model got slightly worse
performance than the Berkeley Parser (Petrov and
Klein, 2007), as shown in Table 5, it outper-
formed the Berkeley Parser on the cross-domain
data sets. This suggests that the learned fea-
tures can better adapt to cross-domain situations.
Compared with the Berkeley Parser, on average
our “Pretrain-Finetune” model is 3.4 percentage
points better in terms of parsing accuracy, and
3.2 percentage points better in terms of POS tag-
ging accuracy. We also presented the performance
of our pre-trained model (“Only-Pretrain”). We
found the “Only-Pretrain” model performs poorly
on this cross-domain data sets. But even pre-
training based on this less than competitive model,
our “Pretrain-Finetune” model achieves signifi-
cant improvement over the “Supervised” model.
So the Pre-Train strategy is crucial to our model.
For English, we performed our experiments on
the cross-domain data sets from OntoNote 5.0
(Weischedel et al., 2013), which consists of nw,
mz, bn, bc, wb, df and telephone conversations
(tc). We also performed experiments on the SMS
domain, using data annotated by the LDC for
the DARPA BOLT Program. We randomly se-
lected 300 sentences for each domain as the test
sets 5. Table 8 presents our experimental results.
To save space, we only presented the results of
our “Pretrain-Finetune” model and the Berkeley
</bodyText>
<footnote confidence="0.9757695">
5The selected sentences can be downloaded from
http://www.cs.brandeis.edu/ xuen/publications.html
</footnote>
<table confidence="0.982721">
1144
domain Only-Pretrain Supervised Pretrain-Finetune BerkeleyParser
F1 POS F1 POS F1 POS F1 POS
bc 61.6 81.1 72.9 90.2 74.9 91.2 68.2 86.4
bn — — 78.2 93.2 80.8 94.2 78.3 91.2
df 65.6 84.5 76.2 91.7 78.5 92.6 75.9 90.3
nw 72.0 86.1 82.1 95.2 85.0 95.8 82.9 93.6
wb 65.4 81.5 74.6 89.5 76.9 90.2 73.8 86.7
average 66.2 83.3 76.8 92.0 79.2 92.8 75.8 89.6
</table>
<tableCaption confidence="0.9334815">
Table 7: Cross-domain performance for Chinese. The “Only-Pretrain” model cannot successfully parse
some sentences in bn domain, so we didn’t give the numbers.
</tableCaption>
<table confidence="0.9999794">
Domain Pretrain-Finetune BerkeleyParser
F1 POS F1 POS
bc 77.7 92.2 76.0 91.1
bn 88.1 95.4 88.2 95.0
df 82.5 93.3 79.4 92.4
nw 89.6 95.3 86.2 94.6
wb 83.3 93.1 82.0 91.2
sms 79.2 85.8 74.6 85.3
tc 74.2 88.0 71.1 87.6
average 82.1 91.9 79.6 91.0
</table>
<tableCaption confidence="0.999673">
Table 8: Cross-domain performance for English.
</tableCaption>
<bodyText confidence="0.9997404">
Parser. Except for the slightly worse performance
on the bn domain, our model outperformed the
Berkeley Parser on all the other domains. While
our model is only 0.6 percentage point better than
the Berkeley Parser (Petrov and Klein, 2007) when
evaluated on the standard Penn TreeBank test set
(Table 6), our parser is 2.5 percentage points bet-
ter on average on the cross domain data sets. So
our parser is also very robust for English on cross-
domain data sets.
</bodyText>
<sectionHeader confidence="0.999898" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999993511111111">
There has been some work on feature optimization
in dependency parsing, but most prior work in this
area is limited to selecting an optimal subset of
features from a set of candidate features (Nilsson
and Nugues, 2010; Ballesteros and Bohnet, 2014).
Lei et al. (2014) proposed to learn features for de-
pendency parsing automatically. They first repre-
sented all possible features with a multi-way ten-
sor, and then transformed it into a low-rank tensor
as the final features that are actually used by their
system. However, to obtain competitive perfor-
mance, they had to combine the learned features
with traditional hand-crafted features. Chen and
Manning (2014) proposed to learn a dense fea-
ture vector for transition-based dependency pars-
ing via neural networks. Their model had to learn
POS tag embeddings and dependency label em-
beddings first, and then induced the dense feature
vector based on these embeddings. Comparing
with their method, our model is much simpler. Our
model learned features directly based on the orig-
inal form of primitive units.
There have also been some attempts to use
neural networks for constituent parsing. Hender-
son (2003) presented the first neural network for
broad coverage parsing. Later, he also proposed
to rerank k-best parse trees with a neural net-
work model which achieved state-of-the-art per-
formance (Henderson, 2004). Collobert (2011)
designed a recurrent neural network model to con-
struct parse tree by stacks of sequences labeling,
but its final performance is significantly lower than
the state-of-the-art performance. Socher et al.
(2013) built a recursive neural network for con-
stituent parsing. However, rather than performing
full inference, their model can only score parse
candidates generated from another parser. Our
model also requires a parser to generate training
samples for pre-training. However, our system is
different in that, during testing, our model per-
forms full inference with no need of other parsers.
Vinyals et al. (2014) employed a Long Short-Term
Memory (LSTM) neural network for parsing. By
training on a much larger hand-annotated data set,
their performance reached 91.6% for English.
</bodyText>
<sectionHeader confidence="0.998617" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.972862823529412">
In this paper, we proposed to learn features via
a neural network model. By taking as input the
primitive units, our neural network model learns
1145
feature representations in the hidden layer and
made parsing predictions based on the learned fea-
tures in the output layer. By employing the back-
propagation algorithm, our model simultaneously
induced features and learned prediction model pa-
rameters. We show that our model achieved signif-
icant improvement from pretraining on a substan-
tial amount of pre-parsed data. Evaluated on stan-
dard data sets, our model outperformed all state-
of-the-art parsers on Chinese and all neural net-
work based models on English. We also show
that our model is particularly effective on cross-
domain tasks for both Chinese and English.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999965333333333">
We thank the anonymous reviewers for comments.
Haitao Mi is supported by DARPA HR0011-12-C-
0015 (BOLT) and Nianwen Xue is supported by
DAPRA HR0011-11-C-0145 (BOLT). The views
and findings in this paper are those of the authors
and are not endorsed by the DARPA.
</bodyText>
<sectionHeader confidence="0.990049" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999015393939394">
Miguel Ballesteros and Bernd Bohnet. 2014. Au-
tomatic feature selection for agenda-based depen-
dency parsing.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the Twelfth Conference on Computational
Natural Language Learning, pages 9–16. Associa-
tion for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173–180. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 132–139. Asso-
ciation for Computational Linguistics.
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750.
Michael Collins. 1999. HEAD-DRIVEN STATISTI-
CAL MODELS FOR NATURAL LANGUAGE PARS-
ING. Ph.D. thesis, University of Pennsylvania.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? The Journal of Machine Learn-
ing Research, 11:625–660.
James Henderson. 2003. Neural network probabil-
ity estimation for broad coverage parsing. In Pro-
ceedings of the tenth conference on European chap-
ter of the Association for Computational Linguistics-
Volume 1, pages 131–138. Association for Compu-
tational Linguistics.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 95. Association for Com-
putational Linguistics.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010. Self-training with products of latent vari-
able grammars. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 12–22. Association for Computa-
tional Linguistics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586–
594.
Daniel Jurafsky and James H Martin. 2008. Speech
and language processing.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, volume 1, pages 1381–1391.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the main conference on human lan-
guage technology conference of the North American
Chapter of the Association of Computational Lin-
guistics, pages 152–159. Association for Computa-
tional Linguistics.
1146
Peter Nilsson and Pierre Nugues. 2010. Automatic
discovery of feature sets for dependency parsing. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 824–832. Associ-
ation for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL, pages
404–411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433–
440. Association for Computational Linguistics.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the conference on empirical methods in natu-
ral language processing, volume 1, pages 133–142.
Philadelphia, PA.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1988. Learning representations by back-
propagating errors.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technology, pages 125–132. Association for Com-
putational Linguistics.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2014.
Grammar as a foreign language. arXiv preprint
arXiv:1412.7449.
Zhiguo Wang and Nianwen Xue. 2014. Joint pos tag-
ging and transition-based constituent parsing in chi-
nese with non-local features. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
733–742. Association for Computational Linguis-
tics.
Zhiguo Wang and Chengqing Zong. 2011. Parse
reranking based on higher-order lexical dependen-
cies. In IJCNLP, pages 1251–1259.
Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.
2006. A fast, accurate deterministic parser for chi-
nese. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computa-
tional Linguistics, pages 425–432. Association for
Computational Linguistics.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Jeff Kaufman, Michelle Franchini, Mohammed El-
Bachouti, Nianwen Xue, Martha Palmer, Mitchell
Marcus, Ann Taylor, et al. 2013. Ontonotes release
5.0. Linguistic Data Consortium, Philadelphia.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(2):207–238.
Nianwen Xue, Xiuhong Zhang, Zixin Jiang,
Martha Palmer, Fei Xia, Fu-Dong Chiou, and
Meiyu Chang. 2013. Chinese treebank 8.0.
Philadelphia: Linguistic Data Consortium, page
https://catalog.ldc.upenn.edu/LDC2013T21.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the chinese treebank using a global dis-
criminative model. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, pages
162–171. Association for Computational Linguis-
tics.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
434–443, Sofia, Bulgaria, August. Association for
Computational Linguistics.
</reference>
<page confidence="0.826386">
1147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.039055">
<title confidence="0.999961">Feature Optimization for Constituent Parsing via Neural Networks</title>
<author confidence="0.980358">Zhiguo</author>
<affiliation confidence="0.987687">IBM</affiliation>
<address confidence="0.85342">1101</address>
<affiliation confidence="0.71587">Yorktown Heights, NY,</affiliation>
<email confidence="0.998873">zhigwang@us.ibm.com</email>
<author confidence="0.886591">Haitao</author>
<affiliation confidence="0.984621">IBM</affiliation>
<address confidence="0.850924">1101</address>
<affiliation confidence="0.734515">Yorktown Heights, NY,</affiliation>
<email confidence="0.997868">hmi@us.ibm.com</email>
<author confidence="0.380875">Nianwen</author>
<affiliation confidence="0.346455">Brandeis</affiliation>
<address confidence="0.715601">415 South Waltham, MA,</address>
<email confidence="0.999046">xuen@brandeis.edu</email>
<abstract confidence="0.978109966666667">The performance of discriminative constituent parsing relies crucially on feature engineering, and effective features usually have to be carefully selected through a painful manual process. In this paper, we propose to automatically learn a set of effective features via neural networks. Specifically, we build a feedforward neural network model, which takes as input a few primitive units (words, POS tags and certain contextual tokens) from the local context, induces the feature representation in the hidden layer and makes parsing predictions in the output layer. The network simultaneously learns the feature representation and the prediction model parameters using a back propagation algorithm. By pre-training the model on a large amount of automatically parsed data, and then fine-tuning on the manually annotated Treebank data, our parser achieves highest at 86.6% on Chi- Treebank 5.1, and a competitive score at 90.7% on English Treebank. More importantly, our parser generalizes well on cross-domain test sets, where we significantly outperform Berkeley parser by 3.4 points on average for Chinese and 2.5 points for English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Miguel Ballesteros</author>
<author>Bernd Bohnet</author>
</authors>
<title>Automatic feature selection for agenda-based dependency parsing.</title>
<date>2014</date>
<contexts>
<context position="30069" citStr="Ballesteros and Bohnet, 2014" startWordPosition="4923" endWordPosition="4926">Berkeley Parser on all the other domains. While our model is only 0.6 percentage point better than the Berkeley Parser (Petrov and Klein, 2007) when evaluated on the standard Penn TreeBank test set (Table 6), our parser is 2.5 percentage points better on average on the cross domain data sets. So our parser is also very robust for English on crossdomain data sets. 5 Related Work There has been some work on feature optimization in dependency parsing, but most prior work in this area is limited to selecting an optimal subset of features from a set of candidate features (Nilsson and Nugues, 2010; Ballesteros and Bohnet, 2014). Lei et al. (2014) proposed to learn features for dependency parsing automatically. They first represented all possible features with a multi-way tensor, and then transformed it into a low-rank tensor as the final features that are actually used by their system. However, to obtain competitive performance, they had to combine the learned features with traditional hand-crafted features. Chen and Manning (2014) proposed to learn a dense feature vector for transition-based dependency parsing via neural networks. Their model had to learn POS tag embeddings and dependency label embeddings first, an</context>
</contexts>
<marker>Ballesteros, Bohnet, 2014</marker>
<rawString>Miguel Ballesteros and Bernd Bohnet. 2014. Automatic feature selection for agenda-based dependency parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2354" citStr="Carreras et al., 2008" startWordPosition="356" endWordPosition="359">2008). One of the major challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consum</context>
<context position="24608" citStr="Carreras et al. (2008)" startWordPosition="4027" endWordPosition="4030">” system. One explanation is that our neural network model is a non-linear model, so the back propagation algorithm can only reach a local optimum. In our “Supervised” system the starting points are randomly initialized in the parameter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with all the stateof-the-art systems, we see our system surpass all the other systems. Alt</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 9–16. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22670" citStr="Charniak and Johnson (2005)" startWordPosition="3725" endWordPosition="3728">resents results of different experimental configurations for English. The performance trend for the Mix-Train strategy is different from that of Chinese. Here, no matter how much automatically parsed data we use, there is a consistent degradation in performance against the model that does not use any automatically parsed data at all. And the more automatically parsed data we use, the larger the drop in accuracy. For the Pre-Train strategy, the trend is similar to Chinese. The parsing performance of the Pre-Train setting consistently improves as the size of automatically parsed data increases. Charniak and Johnson (2005) 82.3 RE Wang and Zong (2011) 85.7 Table 5: Comparison with the state-of-the-art systems on Chinese test set. * marks neural network based systems. $ marks shift-reduce parsing systems. 4.4 Comparing With State-of-the-art Systems In this subsection, we present the performance of our models on the testing sets. We trained two systems. The first system (“Supervised”) is trained only with the hand-annotated training set, and the second system (“Pretrain-Finetune”) is trained with the Pre-Train strategy described in subsection 4.3 using additional automatically parsed data. The best parameters for</context>
<context position="24763" citStr="Charniak and Johnson (2005)" startWordPosition="4054" endWordPosition="4057"> our “Supervised” system the starting points are randomly initialized in the parameter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with all the stateof-the-art systems, we see our system surpass all the other systems. Although our system only utilizes some basic primitive units (in Table 1(a)), it still outperforms Wang and Xue (2014)’s shiftreduce parsing system which uses</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>132--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2098" citStr="Charniak, 2000" startWordPosition="319" endWordPosition="320">on Constituent parsing seeks to uncover the phrase structure representation of sentences that can be used in a variety of natural language applications such as machine translation, information extraction and question answering (Jurafsky and Martin, 2008). One of the major challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an eff</context>
<context position="24525" citStr="Charniak (2000)" startWordPosition="4015" endWordPosition="4016">ur “Pretrain-Finetune” system shows a fairly large gain over the “Supervised” system. One explanation is that our neural network model is a non-linear model, so the back propagation algorithm can only reach a local optimum. In our “Supervised” system the starting points are randomly initialized in the parameter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with al</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 132–139. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>740--750</pages>
<contexts>
<context position="30481" citStr="Chen and Manning (2014)" startWordPosition="4988" endWordPosition="4991">ptimization in dependency parsing, but most prior work in this area is limited to selecting an optimal subset of features from a set of candidate features (Nilsson and Nugues, 2010; Ballesteros and Bohnet, 2014). Lei et al. (2014) proposed to learn features for dependency parsing automatically. They first represented all possible features with a multi-way tensor, and then transformed it into a low-rank tensor as the final features that are actually used by their system. However, to obtain competitive performance, they had to combine the learned features with traditional hand-crafted features. Chen and Manning (2014) proposed to learn a dense feature vector for transition-based dependency parsing via neural networks. Their model had to learn POS tag embeddings and dependency label embeddings first, and then induced the dense feature vector based on these embeddings. Comparing with their method, our model is much simpler. Our model learned features directly based on the original form of primitive units. There have also been some attempts to use neural networks for constituent parsing. Henderson (2003) presented the first neural network for broad coverage parsing. Later, he also proposed to rerank k-best pa</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<date>1999</date>
<tech>HEAD-DRIVEN STATISTICAL MODELS FOR NATURAL LANGUAGE PARSING. Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2082" citStr="Collins, 1999" startWordPosition="317" endWordPosition="318">h. 1 Introduction Constituent parsing seeks to uncover the phrase structure representation of sentences that can be used in a variety of natural language applications such as machine translation, information extraction and question answering (Jurafsky and Martin, 2008). One of the major challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem.</context>
<context position="24504" citStr="Collins (1999)" startWordPosition="4012" endWordPosition="4013">stems, we see that our “Pretrain-Finetune” system shows a fairly large gain over the “Supervised” system. One explanation is that our neural network model is a non-linear model, so the back propagation algorithm can only reach a local optimum. In our “Supervised” system the starting points are randomly initialized in the parameter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Fin</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. HEAD-DRIVEN STATISTICAL MODELS FOR NATURAL LANGUAGE PARSING. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics, number EPFLCONF-192374.</booktitle>
<contexts>
<context position="24706" citStr="Collobert (2011)" startWordPosition="4048" endWordPosition="4049">n algorithm can only reach a local optimum. In our “Supervised” system the starting points are randomly initialized in the parameter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with all the stateof-the-art systems, we see our system surpass all the other systems. Although our system only utilizes some basic primitive units (in Table 1(a)), it still outperforms Wa</context>
<context position="31198" citStr="Collobert (2011)" startWordPosition="5103" endWordPosition="5104">. Their model had to learn POS tag embeddings and dependency label embeddings first, and then induced the dense feature vector based on these embeddings. Comparing with their method, our model is much simpler. Our model learned features directly based on the original form of primitive units. There have also been some attempts to use neural networks for constituent parsing. Henderson (2003) presented the first neural network for broad coverage parsing. Later, he also proposed to rerank k-best parse trees with a neural network model which achieved state-of-the-art performance (Henderson, 2004). Collobert (2011) designed a recurrent neural network model to construct parse tree by stacks of sequences labeling, but its final performance is significantly lower than the state-of-the-art performance. Socher et al. (2013) built a recursive neural network for constituent parsing. However, rather than performing full inference, their model can only score parse candidates generated from another parser. Our model also requires a parser to generate training samples for pre-training. However, our system is different in that, during testing, our model performs full inference with no need of other parsers. Vinyals</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In International Conference on Artificial Intelligence and Statistics, number EPFLCONF-192374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="15101" citStr="Duchi et al., 2011" startWordPosition="2482" endWordPosition="2485">yer which are used for learning an optimal feature representation and (3) the connections between the hidden layer and the output layer which are used for making accurate parsing predictions. We decided to learn word embeddings separately, so that we can take advantage of a large amount of unlabeled data. The remaining two groups of parameters can be trained simultaneously by the back propagation algorithm (Rumelhart et al., 1988) to maximize the likelihood over the training data. We also employ three crucial techniques to seek more effective parameters. First, we utilize minibatched AdaGrad (Duchi et al., 2011), in which 4 Experiment 4.1 Experimental Setting We conducted experiments on the Penn Chinese Treebank (CTB) version 5.1 (Xue et al., 2005) and the Wall Street Journal (WSJ) portion of Penn English Treebank (Marcus et al., 1993). To fairly compare with other work, we follow the standard data division. For Chinese, we allocated Articles 001-270 and 400-1151 as the training set, Articles 300 as the testing set. For English, we use sections 2-21 for training, section 22 for developing and section 23 for testing. We also utilized some unlabeled corpora and used the word2vec2 toolkit to train word </context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dumitru Erhan</author>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pierre-Antoine Manzagol</author>
<author>Pascal Vincent</author>
<author>Samy Bengio</author>
</authors>
<title>Why does unsupervised pre-training help deep learning?</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--625</pages>
<contexts>
<context position="14030" citStr="Erhan et al., 2010" startWordPosition="2305" endWordPosition="2308">t set, and Articles 271- ce. the learning rate is adapted differently for different training steps. With this technique, we can start with a very large learning rate which decreases during training, and can thus perform a far more thorough search within the parameter space. In our experiments, we got a much faster convergence rate with slightly better accuracy by using the learning rate = 1 instead of the commonly-used = 0.01. Second, we initialize the model parameters by pre-training. Unsupervised pre-training has demonstrated its effectiveness as a way of initializing neural network models (Erhan et al., 2010). Since our model requires many run-time primitive units (POS tags and constituent labels), we employ an in-house shift-reduce parser to parse a large amount of unlabeled sentences, and pre-train the model with the automatically parsed data. Third, we utilize the Dropout strategy to address the overfitt different parameters at α α ing probFigure 2: Neural network architecture for con(2) the connections between the projection layer and the hidden layer which are used for learning an optimal feature representation and (3) the connections between the hidden layer and the output layer which are us</context>
</contexts>
<marker>Erhan, Bengio, Courville, Manzagol, Vincent, Bengio, 2010</marker>
<rawString>Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. 2010. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625–660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Neural network probability estimation for broad coverage parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational LinguisticsVolume 1,</booktitle>
<pages>131--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3532" citStr="Henderson (2003)" startWordPosition="551" endWordPosition="552">but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the learned features with the traditional hand-crafted features. For constituent parsing, Henderson (2003) employed a recurrent neural network to induce features from an unbounded parsing history. However, the final performance was below the state of the art. In this work, we design a much simpler neural network to automatically induce features from just the local context for constituent parsing. Con1138 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1138–1147, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics cretely, we choose the shift-reduce</context>
<context position="24547" citStr="Henderson (2003)" startWordPosition="4018" endWordPosition="4019">” system shows a fairly large gain over the “Supervised” system. One explanation is that our neural network model is a non-linear model, so the back propagation algorithm can only reach a local optimum. In our “Supervised” system the starting points are randomly initialized in the parameter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with all the stateof-the-art </context>
<context position="30974" citStr="Henderson (2003)" startWordPosition="5069" endWordPosition="5071">itive performance, they had to combine the learned features with traditional hand-crafted features. Chen and Manning (2014) proposed to learn a dense feature vector for transition-based dependency parsing via neural networks. Their model had to learn POS tag embeddings and dependency label embeddings first, and then induced the dense feature vector based on these embeddings. Comparing with their method, our model is much simpler. Our model learned features directly based on the original form of primitive units. There have also been some attempts to use neural networks for constituent parsing. Henderson (2003) presented the first neural network for broad coverage parsing. Later, he also proposed to rerank k-best parse trees with a neural network model which achieved state-of-the-art performance (Henderson, 2004). Collobert (2011) designed a recurrent neural network model to construct parse tree by stacks of sequences labeling, but its final performance is significantly lower than the state-of-the-art performance. Socher et al. (2013) built a recursive neural network for constituent parsing. However, rather than performing full inference, their model can only score parse candidates generated from an</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>James Henderson. 2003. Neural network probability estimation for broad coverage parsing. In Proceedings of the tenth conference on European chapter of the Association for Computational LinguisticsVolume 1, pages 131–138. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>page</pages>
<contexts>
<context position="24729" citStr="Henderson (2004)" startWordPosition="4051" endWordPosition="4052">ach a local optimum. In our “Supervised” system the starting points are randomly initialized in the parameter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with all the stateof-the-art systems, we see our system surpass all the other systems. Although our system only utilizes some basic primitive units (in Table 1(a)), it still outperforms Wang and Xue (2014)’s shi</context>
<context position="31180" citStr="Henderson, 2004" startWordPosition="5101" endWordPosition="5102">ia neural networks. Their model had to learn POS tag embeddings and dependency label embeddings first, and then induced the dense feature vector based on these embeddings. Comparing with their method, our model is much simpler. Our model learned features directly based on the original form of primitive units. There have also been some attempts to use neural networks for constituent parsing. Henderson (2003) presented the first neural network for broad coverage parsing. Later, he also proposed to rerank k-best parse trees with a neural network model which achieved state-of-the-art performance (Henderson, 2004). Collobert (2011) designed a recurrent neural network model to construct parse tree by stacks of sequences labeling, but its final performance is significantly lower than the state-of-the-art performance. Socher et al. (2013) built a recursive neural network for constituent parsing. However, rather than performing full inference, their model can only score parse candidates generated from another parser. Our model also requires a parser to generate training samples for pre-training. However, our system is different in that, during testing, our model performs full inference with no need of othe</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Discriminative training of a neural network statistical parser. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 95. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580.</title>
<date>2012</date>
<contexts>
<context position="12968" citStr="Hinton et al. (2012)" startWordPosition="2125" endWordPosition="2128">ation of the output layer as a probability distribution over all possible shiftreduce actions, therefore we normalize the output activations (weighted summations of all nodes from the hidden layer) with the softmax function. 3.2 Parameter Estimation Our model consists of three groups of parameters: (1) the word embedding for each word type unit, unigrams 1140 word embedding one-hot representation dense real-valued low-dimensional vector p(t|s ) softmax ... ... sigmoid ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... w0 wm t0 tn c0 cn suffix1 ... prefix4 lem. However, different from Hinton et al. (2012), we only use Dropout during testing, because we found that using Dropout during training did not improve the parsing performan ce (on the dev set) while greatly slowing down the training process. stituent parsing, where denotes word type unit, ti denotes POS tag unit, denotes constituent label unit, an wi ci suffixi d prefixi (1 &lt;_ i &lt;_ 4) denotes i-character word suffix or prefix for the first word in the queue. 301-325 as the development set, and Articles 271- ce. the learning rate is adapted differently for different training steps. With this technique, we can start with a very large learn</context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
<author>Slav Petrov</author>
</authors>
<title>Self-training with products of latent variable grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>12--22</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24657" citStr="Huang et al. (2010)" startWordPosition="4037" endWordPosition="4040"> model is a non-linear model, so the back propagation algorithm can only reach a local optimum. In our “Supervised” system the starting points are randomly initialized in the parameter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with all the stateof-the-art systems, we see our system surpass all the other systems. Although our system only utilizes some basic primiti</context>
</contexts>
<marker>Huang, Harper, Petrov, 2010</marker>
<rawString>Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010. Self-training with products of latent variable grammars. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 12–22. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="24812" citStr="Huang (2008)" startWordPosition="4065" endWordPosition="4066">lized in the parameter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with all the stateof-the-art systems, we see our system surpass all the other systems. Although our system only utilizes some basic primitive units (in Table 1(a)), it still outperforms Wang and Xue (2014)’s shiftreduce parsing system which uses more complex structural features and semi-superv</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In ACL, pages 586– 594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and language processing.</title>
<date>2008</date>
<contexts>
<context position="1738" citStr="Jurafsky and Martin, 2008" startWordPosition="259" endWordPosition="262">n the manually annotated Treebank data, our parser achieves the highest F1 score at 86.6% on Chinese Treebank 5.1, and a competitive F1 score at 90.7% on English Treebank. More importantly, our parser generalizes well on cross-domain test sets, where we significantly outperform Berkeley parser by 3.4 points on average for Chinese and 2.5 points for English. 1 Introduction Constituent parsing seeks to uncover the phrase structure representation of sentences that can be used in a variety of natural language applications such as machine translation, information extraction and question answering (Jurafsky and Martin, 2008). One of the major challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carre</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James H Martin. 2008. Speech and language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yu Xin</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Low-rank tensors for scoring dependency structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>1381--1391</pages>
<contexts>
<context position="3228" citStr="Lei et al. (2014)" startWordPosition="505" endWordPosition="508">the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the learned features with the traditional hand-crafted features. For constituent parsing, Henderson (2003) employed a recurrent neural network to induce features from an unbounded parsing history. However, the final performance was below the state of the art. In this work, we design a much simpler neural network to automatically induce features from just the local context for constituent parsing. Co</context>
<context position="30088" citStr="Lei et al. (2014)" startWordPosition="4927" endWordPosition="4930">r domains. While our model is only 0.6 percentage point better than the Berkeley Parser (Petrov and Klein, 2007) when evaluated on the standard Penn TreeBank test set (Table 6), our parser is 2.5 percentage points better on average on the cross domain data sets. So our parser is also very robust for English on crossdomain data sets. 5 Related Work There has been some work on feature optimization in dependency parsing, but most prior work in this area is limited to selecting an optimal subset of features from a set of candidate features (Nilsson and Nugues, 2010; Ballesteros and Bohnet, 2014). Lei et al. (2014) proposed to learn features for dependency parsing automatically. They first represented all possible features with a multi-way tensor, and then transformed it into a low-rank tensor as the final features that are actually used by their system. However, to obtain competitive performance, they had to combine the learned features with traditional hand-crafted features. Chen and Manning (2014) proposed to learn a dense feature vector for transition-based dependency parsing via neural networks. Their model had to learn POS tag embeddings and dependency label embeddings first, and then induced the </context>
</contexts>
<marker>Lei, Xin, Zhang, Barzilay, Jaakkola, 2014</marker>
<rawString>Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scoring dependency structures. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1381–1391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="15329" citStr="Marcus et al., 1993" startWordPosition="2520" endWordPosition="2523"> separately, so that we can take advantage of a large amount of unlabeled data. The remaining two groups of parameters can be trained simultaneously by the back propagation algorithm (Rumelhart et al., 1988) to maximize the likelihood over the training data. We also employ three crucial techniques to seek more effective parameters. First, we utilize minibatched AdaGrad (Duchi et al., 2011), in which 4 Experiment 4.1 Experimental Setting We conducted experiments on the Penn Chinese Treebank (CTB) version 5.1 (Xue et al., 2005) and the Wall Street Journal (WSJ) portion of Penn English Treebank (Marcus et al., 1993). To fairly compare with other work, we follow the standard data division. For Chinese, we allocated Articles 001-270 and 400-1151 as the training set, Articles 300 as the testing set. For English, we use sections 2-21 for training, section 22 for developing and section 23 for testing. We also utilized some unlabeled corpora and used the word2vec2 toolkit to train word embeddings. For Chinese, we used the unlabeled Chinese Gigaword (LDC2003T09) and performed Chinese word segmentation using our in-house segmenter. For English, we randomly selected 9 million sentences from our in-house newswire </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24791" citStr="McClosky et al. (2006)" startWordPosition="4059" endWordPosition="4062">ting points are randomly initialized in the parameter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with all the stateof-the-art systems, we see our system surpass all the other systems. Although our system only utilizes some basic primitive units (in Table 1(a)), it still outperforms Wang and Xue (2014)’s shiftreduce parsing system which uses more complex structural fea</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics, pages 152–159. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Nilsson</author>
<author>Pierre Nugues</author>
</authors>
<title>Automatic discovery of feature sets for dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>824--832</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="30038" citStr="Nilsson and Nugues, 2010" startWordPosition="4919" endWordPosition="4922">ur model outperformed the Berkeley Parser on all the other domains. While our model is only 0.6 percentage point better than the Berkeley Parser (Petrov and Klein, 2007) when evaluated on the standard Penn TreeBank test set (Table 6), our parser is 2.5 percentage points better on average on the cross domain data sets. So our parser is also very robust for English on crossdomain data sets. 5 Related Work There has been some work on feature optimization in dependency parsing, but most prior work in this area is limited to selecting an optimal subset of features from a set of candidate features (Nilsson and Nugues, 2010; Ballesteros and Bohnet, 2014). Lei et al. (2014) proposed to learn features for dependency parsing automatically. They first represented all possible features with a multi-way tensor, and then transformed it into a low-rank tensor as the final features that are actually used by their system. However, to obtain competitive performance, they had to combine the learned features with traditional hand-crafted features. Chen and Manning (2014) proposed to learn a dense feature vector for transition-based dependency parsing via neural networks. Their model had to learn POS tag embeddings and depend</context>
</contexts>
<marker>Nilsson, Nugues, 2010</marker>
<rawString>Peter Nilsson and Pierre Nugues. 2010. Automatic discovery of feature sets for dependency parsing. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 824–832. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In</title>
<date>2007</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="20817" citStr="Petrov and Klein (2007)" startWordPosition="3428" endWordPosition="3431">o the hand-annotated training set and train models with the mixed data set. The second strategy (Pre-Train) is to first pretrain models with the automatically parsed data, and then fine-tune models with the hand-annotated training set. Table 3 shows results of different experimental configurations for Chinese. For the Mix-Train 4Its performance is Fl =83.9 on Chinese and Fl =90.8% on English. 1142 Mix-Train Pre-Train # Auto Sent F1 POS F1 POS 0 87.8 97.0 — — 50K 87.2 96.8 88.4 97.1 100K 88.7 96.9 89.5 97.1 200K 89.2 97.2 89.5 97.4 Type System F1 Supervised*$ 83.2 Ours Pretrain-Finetune*$ 86.6 Petrov and Klein (2007) 83.3 SI Wang and Xue (2014)$ 83.6 Zhu et al. (2013)$ 85.6 Wang and Xue (2014)$ 86.3 Table 3: Semi-supervised training for Chinese. SE Mix-Train Pre-Train # Auto Sent F1 POS F1 POS 0 89.7 96.6 — — 50K 89.4 96.1 90.2 96.4 100K 89.5 96.0 90.4 96.5 200K 89.2 95.8 90.8 96.7 Table 4: Semi-supervised training for English. strategy, when we only use 50K automatically parsed sentences, the performance drops in comparison with the model trained without using any automatically parsed data. When we increase the automatically parsed data to 100K sentences, the parsing performance improves about 1 percent </context>
<context position="24580" citStr="Petrov and Klein (2007)" startWordPosition="4022" endWordPosition="4025">rge gain over the “Supervised” system. One explanation is that our neural network model is a non-linear model, so the back propagation algorithm can only reach a local optimum. In our “Supervised” system the starting points are randomly initialized in the parameter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with all the stateof-the-art systems, we see our system surpas</context>
<context position="27184" citStr="Petrov and Klein, 2007" startWordPosition="4447" endWordPosition="4450">st conversation (bc), weblogs (wb) and discussion forums (df). Since all of the mz domain data is already included in our training set, we only selected sample sentences from the other five domains as the test sets 5, and made sure these test sets had no overlap with our treebank training, development and test sets. Note that we did not use any data from these five domains for training or development. The models are still the ones described in the previous subsection. The results are presented in Table 7. Although our “Supervised” model got slightly worse performance than the Berkeley Parser (Petrov and Klein, 2007), as shown in Table 5, it outperformed the Berkeley Parser on the cross-domain data sets. This suggests that the learned features can better adapt to cross-domain situations. Compared with the Berkeley Parser, on average our “Pretrain-Finetune” model is 3.4 percentage points better in terms of parsing accuracy, and 3.2 percentage points better in terms of POS tagging accuracy. We also presented the performance of our pre-trained model (“Only-Pretrain”). We found the “Only-Pretrain” model performs poorly on this cross-domain data sets. But even pretraining based on this less than competitive mo</context>
<context position="29583" citStr="Petrov and Klein, 2007" startWordPosition="4838" endWordPosition="4841">” model cannot successfully parse some sentences in bn domain, so we didn’t give the numbers. Domain Pretrain-Finetune BerkeleyParser F1 POS F1 POS bc 77.7 92.2 76.0 91.1 bn 88.1 95.4 88.2 95.0 df 82.5 93.3 79.4 92.4 nw 89.6 95.3 86.2 94.6 wb 83.3 93.1 82.0 91.2 sms 79.2 85.8 74.6 85.3 tc 74.2 88.0 71.1 87.6 average 82.1 91.9 79.6 91.0 Table 8: Cross-domain performance for English. Parser. Except for the slightly worse performance on the bn domain, our model outperformed the Berkeley Parser on all the other domains. While our model is only 0.6 percentage point better than the Berkeley Parser (Petrov and Klein, 2007) when evaluated on the standard Penn TreeBank test set (Table 6), our parser is 2.5 percentage points better on average on the cross domain data sets. So our parser is also very robust for English on crossdomain data sets. 5 Related Work There has been some work on feature optimization in dependency parsing, but most prior work in this area is limited to selecting an optimal subset of features from a set of candidate features (Nilsson and Nugues, 2010; Ballesteros and Bohnet, 2014). Lei et al. (2014) proposed to learn features for dependency parsing automatically. They first represented all po</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2120" citStr="Petrov et al., 2006" startWordPosition="321" endWordPosition="324">arsing seeks to uncover the phrase structure representation of sentences that can be used in a variety of natural language applications such as machine translation, information extraction and question answering (Jurafsky and Martin, 2008). One of the major challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 433– 440. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing,</booktitle>
<volume>1</volume>
<pages>133--142</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="10575" citStr="Ratnaparkhi, 1996" startWordPosition="1735" endWordPosition="1736">l we cannot say for sure that this is the optimal subset of features for the parsing task. To cope with this problem, we propose to simultaneously optimize feature representation and parsing accuracy via a neural network model. Figure 2 illustrates the architecture of our model. Our model consists of input, projection, hidden and output layers. First, in the input layer, all primitive units (shown in Table 1(a)) are imported to the network. We also import the suffixes and prefixes of the first word in the queue, because these units have been shown to be very effective for predicting POS tags (Ratnaparkhi, 1996). Then, in the projection layer, each primitive unit is projected into a vector. Specifically, word-type units are represented as word embeddings, and other units are transformed into one-hot representations. The (1) p0w, p0t,p0c, p1w, p1t,p1c, p2w, p2t,p2c, p3w, p3t,p3c (2) p0lw, p0lc, p0rw, p0rc,p0uw, p0uc, p1lw, p1lc, p1rw, p1rc,p1uw, p1uc (3) q0w, q1w, q2w, q3w (a) Primitive Units p0tc, p0wc, p1tc, p1wc, p2tc p2wc, p3tc, p3wc, q0wt, q1wt q2wt, q3wt, p0lwc, p0rwc p0uwc, p1lwc, p1rwc, p1uwc bigrams p0wp1w, p0wp1c, p0cp1w, p0cp1c p0wq0w, p0wq0t, p0cq0w, p0cq0t q0wq1w, q0wq1t, q0tq1w, q0tq1t p</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the conference on empirical methods in natural language processing, volume 1, pages 133–142. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning representations by backpropagating errors.</title>
<date>1988</date>
<contexts>
<context position="14916" citStr="Rumelhart et al., 1988" startWordPosition="2452" endWordPosition="2455"> Dropout strategy to address the overfitt different parameters at α α ing probFigure 2: Neural network architecture for con(2) the connections between the projection layer and the hidden layer which are used for learning an optimal feature representation and (3) the connections between the hidden layer and the output layer which are used for making accurate parsing predictions. We decided to learn word embeddings separately, so that we can take advantage of a large amount of unlabeled data. The remaining two groups of parameters can be trained simultaneously by the back propagation algorithm (Rumelhart et al., 1988) to maximize the likelihood over the training data. We also employ three crucial techniques to seek more effective parameters. First, we utilize minibatched AdaGrad (Duchi et al., 2011), in which 4 Experiment 4.1 Experimental Setting We conducted experiments on the Penn Chinese Treebank (CTB) version 5.1 (Xue et al., 2005) and the Wall Street Journal (WSJ) portion of Penn English Treebank (Marcus et al., 1993). To fairly compare with other work, we follow the standard data division. For Chinese, we allocated Articles 001-270 and 400-1151 as the training set, Articles 300 as the testing set. Fo</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1988</marker>
<rawString>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1988. Learning representations by backpropagating errors.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>125--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2814" citStr="Sagae and Lavie, 2005" startWordPosition="434" endWordPosition="437">egies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had</context>
<context position="9151" citStr="Sagae and Lavie, 2005" startWordPosition="1489" endWordPosition="1492">, ru-VP, rr-VP, rr-S}. To process multi-branch trees, we employ binarization and debinarization processes described in Zhang and Clark (2009) to transform multi-branch trees into binary trees and restore the generated binary trees back to their original forms. For inference, we employ the beam search decoding algorithm (Zhang and Clark, 2009) to balance the tradeoff between accuracy and efficiency. 3 Feature Optimization Model 3.1 Model To determine which action t E T should be performed at a given state s E S, we need a model to score each possible (s, t) combination. In previous approaches (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009), the model is usually defined as a linear model Score(s, t) = 2Vi · Φ(s, t), where Φ(s, t) is a vector of handcrafted features for each state-action pair and 2Vi is the weight vector for these features. The handcrafted features are usually constructed by compounding primitive units according to some feature templates. For example, almost all the previous work employed the list of primitive units in Table 1(a), and constructed hand-crafted features by concatenating these primitive units according to the feature templates in Table 1(b). Obviously, thes</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Kenji Sagae and Alon Lavie. 2005. A classifier-based parser with linear run-time complexity. In Proceedings of the Ninth International Workshop on Parsing Technology, pages 125–132. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars. In</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL conference. Citeseer.</booktitle>
<contexts>
<context position="24838" citStr="Socher et al. (2013)" startWordPosition="4068" endWordPosition="4071">eter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with all the stateof-the-art systems, we see our system surpass all the other systems. Although our system only utilizes some basic primitive units (in Table 1(a)), it still outperforms Wang and Xue (2014)’s shiftreduce parsing system which uses more complex structural features and semi-supervised word cluster features</context>
<context position="31406" citStr="Socher et al. (2013)" startWordPosition="5132" endWordPosition="5135">pler. Our model learned features directly based on the original form of primitive units. There have also been some attempts to use neural networks for constituent parsing. Henderson (2003) presented the first neural network for broad coverage parsing. Later, he also proposed to rerank k-best parse trees with a neural network model which achieved state-of-the-art performance (Henderson, 2004). Collobert (2011) designed a recurrent neural network model to construct parse tree by stacks of sequences labeling, but its final performance is significantly lower than the state-of-the-art performance. Socher et al. (2013) built a recursive neural network for constituent parsing. However, rather than performing full inference, their model can only score parse candidates generated from another parser. Our model also requires a parser to generate training samples for pre-training. However, our system is different in that, during testing, our model performs full inference with no need of other parsers. Vinyals et al. (2014) employed a Long Short-Term Memory (LSTM) neural network for parsing. By training on a much larger hand-annotated data set, their performance reached 91.6% for English. 6 Conclusion In this pape</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with compositional vector grammars. In In Proceedings of the ACL conference. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oriol Vinyals</author>
<author>Lukasz Kaiser</author>
<author>Terry Koo</author>
<author>Slav Petrov</author>
<author>Ilya Sutskever</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Grammar as a foreign language. arXiv preprint arXiv:1412.7449.</title>
<date>2014</date>
<contexts>
<context position="31812" citStr="Vinyals et al. (2014)" startWordPosition="5195" endWordPosition="5198"> (2011) designed a recurrent neural network model to construct parse tree by stacks of sequences labeling, but its final performance is significantly lower than the state-of-the-art performance. Socher et al. (2013) built a recursive neural network for constituent parsing. However, rather than performing full inference, their model can only score parse candidates generated from another parser. Our model also requires a parser to generate training samples for pre-training. However, our system is different in that, during testing, our model performs full inference with no need of other parsers. Vinyals et al. (2014) employed a Long Short-Term Memory (LSTM) neural network for parsing. By training on a much larger hand-annotated data set, their performance reached 91.6% for English. 6 Conclusion In this paper, we proposed to learn features via a neural network model. By taking as input the primitive units, our neural network model learns 1145 feature representations in the hidden layer and made parsing predictions based on the learned features in the output layer. By employing the backpropagation algorithm, our model simultaneously induced features and learned prediction model parameters. We show that our </context>
</contexts>
<marker>Vinyals, Kaiser, Koo, Petrov, Sutskever, Hinton, 2014</marker>
<rawString>Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2014. Grammar as a foreign language. arXiv preprint arXiv:1412.7449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiguo Wang</author>
<author>Nianwen Xue</author>
</authors>
<title>Joint pos tagging and transition-based constituent parsing in chinese with non-local features.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>733--742</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2393" citStr="Wang and Xue, 2014" startWordPosition="364" endWordPosition="367">is task is that constituent parsers require an inference algorithm of high computational complexity in order to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking proces</context>
<context position="20845" citStr="Wang and Xue (2014)" startWordPosition="3434" endWordPosition="3437">t and train models with the mixed data set. The second strategy (Pre-Train) is to first pretrain models with the automatically parsed data, and then fine-tune models with the hand-annotated training set. Table 3 shows results of different experimental configurations for Chinese. For the Mix-Train 4Its performance is Fl =83.9 on Chinese and Fl =90.8% on English. 1142 Mix-Train Pre-Train # Auto Sent F1 POS F1 POS 0 87.8 97.0 — — 50K 87.2 96.8 88.4 97.1 100K 88.7 96.9 89.5 97.1 200K 89.2 97.2 89.5 97.4 Type System F1 Supervised*$ 83.2 Ours Pretrain-Finetune*$ 86.6 Petrov and Klein (2007) 83.3 SI Wang and Xue (2014)$ 83.6 Zhu et al. (2013)$ 85.6 Wang and Xue (2014)$ 86.3 Table 3: Semi-supervised training for Chinese. SE Mix-Train Pre-Train # Auto Sent F1 POS F1 POS 0 89.7 96.6 — — 50K 89.4 96.1 90.2 96.4 100K 89.5 96.0 90.4 96.5 200K 89.2 95.8 90.8 96.7 Table 4: Semi-supervised training for English. strategy, when we only use 50K automatically parsed sentences, the performance drops in comparison with the model trained without using any automatically parsed data. When we increase the automatically parsed data to 100K sentences, the parsing performance improves about 1 percent but the POS tagging accuracy</context>
<context position="25323" citStr="Wang and Xue (2014)" startWordPosition="4146" endWordPosition="4149">1)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with all the stateof-the-art systems, we see our system surpass all the other systems. Although our system only utilizes some basic primitive units (in Table 1(a)), it still outperforms Wang and Xue (2014)’s shiftreduce parsing system which uses more complex structural features and semi-supervised word cluster features. Therefore, our model can simultaneously learn an effective feature representation and make accurate parsing predictions for Chinese. Table 6 presents the performance of our systems as well as the state-of-the-art systems on the English test set. Our “Pretrain-Finetune” system still achieves much better performance than the “Supervised” system, although the gap is smaller than that of Chinese. Our “Pretrain-Finetune” system also outperforms all other neural network based systems </context>
</contexts>
<marker>Wang, Xue, 2014</marker>
<rawString>Zhiguo Wang and Nianwen Xue. 2014. Joint pos tagging and transition-based constituent parsing in chinese with non-local features. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 733–742. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiguo Wang</author>
<author>Chengqing Zong</author>
</authors>
<title>Parse reranking based on higher-order lexical dependencies.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<pages>1251--1259</pages>
<contexts>
<context position="22699" citStr="Wang and Zong (2011)" startWordPosition="3731" endWordPosition="3734">ental configurations for English. The performance trend for the Mix-Train strategy is different from that of Chinese. Here, no matter how much automatically parsed data we use, there is a consistent degradation in performance against the model that does not use any automatically parsed data at all. And the more automatically parsed data we use, the larger the drop in accuracy. For the Pre-Train strategy, the trend is similar to Chinese. The parsing performance of the Pre-Train setting consistently improves as the size of automatically parsed data increases. Charniak and Johnson (2005) 82.3 RE Wang and Zong (2011) 85.7 Table 5: Comparison with the state-of-the-art systems on Chinese test set. * marks neural network based systems. $ marks shift-reduce parsing systems. 4.4 Comparing With State-of-the-art Systems In this subsection, we present the performance of our models on the testing sets. We trained two systems. The first system (“Supervised”) is trained only with the hand-annotated training set, and the second system (“Pretrain-Finetune”) is trained with the Pre-Train strategy described in subsection 4.3 using additional automatically parsed data. The best parameters for the two systems are set base</context>
</contexts>
<marker>Wang, Zong, 2011</marker>
<rawString>Zhiguo Wang and Chengqing Zong. 2011. Parse reranking based on higher-order lexical dependencies. In IJCNLP, pages 1251–1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Kenji Sagae</author>
<author>Teruko Mitamura</author>
</authors>
<title>A fast, accurate deterministic parser for chinese.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>425--432</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2833" citStr="Wang et al., 2006" startWordPosition="438" endWordPosition="441">rence algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the lea</context>
<context position="9170" citStr="Wang et al., 2006" startWordPosition="1493" endWordPosition="1496">To process multi-branch trees, we employ binarization and debinarization processes described in Zhang and Clark (2009) to transform multi-branch trees into binary trees and restore the generated binary trees back to their original forms. For inference, we employ the beam search decoding algorithm (Zhang and Clark, 2009) to balance the tradeoff between accuracy and efficiency. 3 Feature Optimization Model 3.1 Model To determine which action t E T should be performed at a given state s E S, we need a model to score each possible (s, t) combination. In previous approaches (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009), the model is usually defined as a linear model Score(s, t) = 2Vi · Φ(s, t), where Φ(s, t) is a vector of handcrafted features for each state-action pair and 2Vi is the weight vector for these features. The handcrafted features are usually constructed by compounding primitive units according to some feature templates. For example, almost all the previous work employed the list of primitive units in Table 1(a), and constructed hand-crafted features by concatenating these primitive units according to the feature templates in Table 1(b). Obviously, these feature templates</context>
</contexts>
<marker>Wang, Sagae, Mitamura, 2006</marker>
<rawString>Mengqiu Wang, Kenji Sagae, and Teruko Mitamura. 2006. A fast, accurate deterministic parser for chinese. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 425–432. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Jeff Kaufman</author>
<author>Michelle Franchini</author>
<author>Mohammed ElBachouti</author>
</authors>
<title>Ontonotes release 5.0. Linguistic Data Consortium,</title>
<date>2013</date>
<location>Nianwen Xue, Martha Palmer, Mitchell Marcus, Ann Taylor, et</location>
<contexts>
<context position="28047" citStr="Weischedel et al., 2013" startWordPosition="4579" endWordPosition="4582">tune” model is 3.4 percentage points better in terms of parsing accuracy, and 3.2 percentage points better in terms of POS tagging accuracy. We also presented the performance of our pre-trained model (“Only-Pretrain”). We found the “Only-Pretrain” model performs poorly on this cross-domain data sets. But even pretraining based on this less than competitive model, our “Pretrain-Finetune” model achieves significant improvement over the “Supervised” model. So the Pre-Train strategy is crucial to our model. For English, we performed our experiments on the cross-domain data sets from OntoNote 5.0 (Weischedel et al., 2013), which consists of nw, mz, bn, bc, wb, df and telephone conversations (tc). We also performed experiments on the SMS domain, using data annotated by the LDC for the DARPA BOLT Program. We randomly selected 300 sentences for each domain as the test sets 5. Table 8 presents our experimental results. To save space, we only presented the results of our “Pretrain-Finetune” model and the Berkeley 5The selected sentences can be downloaded from http://www.cs.brandeis.edu/ xuen/publications.html 1144 domain Only-Pretrain Supervised Pretrain-Finetune BerkeleyParser F1 POS F1 POS F1 POS F1 POS bc 61.6 8</context>
</contexts>
<marker>Weischedel, Pradhan, Ramshaw, Kaufman, Franchini, ElBachouti, 2013</marker>
<rawString>Ralph Weischedel, Sameer Pradhan, Lance Ramshaw, Jeff Kaufman, Michelle Franchini, Mohammed ElBachouti, Nianwen Xue, Martha Palmer, Mitchell Marcus, Ann Taylor, et al. 2013. Ontonotes release 5.0. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naiwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus. Natural language engineering,</title>
<date>2005</date>
<pages>11--2</pages>
<contexts>
<context position="15240" citStr="Xue et al., 2005" startWordPosition="2504" endWordPosition="2507"> are used for making accurate parsing predictions. We decided to learn word embeddings separately, so that we can take advantage of a large amount of unlabeled data. The remaining two groups of parameters can be trained simultaneously by the back propagation algorithm (Rumelhart et al., 1988) to maximize the likelihood over the training data. We also employ three crucial techniques to seek more effective parameters. First, we utilize minibatched AdaGrad (Duchi et al., 2011), in which 4 Experiment 4.1 Experimental Setting We conducted experiments on the Penn Chinese Treebank (CTB) version 5.1 (Xue et al., 2005) and the Wall Street Journal (WSJ) portion of Penn English Treebank (Marcus et al., 1993). To fairly compare with other work, we follow the standard data division. For Chinese, we allocated Articles 001-270 and 400-1151 as the training set, Articles 300 as the testing set. For English, we use sections 2-21 for training, section 22 for developing and section 23 for testing. We also utilized some unlabeled corpora and used the word2vec2 toolkit to train word embeddings. For Chinese, we used the unlabeled Chinese Gigaword (LDC2003T09) and performed Chinese word segmentation using our in-house seg</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. Natural language engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Xiuhong Zhang</author>
<author>Zixin Jiang</author>
<author>Martha Palmer</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Meiyu Chang</author>
</authors>
<title>Chinese treebank 8.0. Philadelphia: Linguistic Data Consortium,</title>
<date>2013</date>
<pages>2013--21</pages>
<contexts>
<context position="26464" citStr="Xue et al., 2013" startWordPosition="4324" endWordPosition="4327">in-Finetune” system also outperforms all other neural network based systems (systems marked with *). Although our system does not outperform all the state-of-the-art systems, the performance is comparable to most of them. So our model is also effective for English parsing. 4.5 Cross Domain Evaluation In this subsection, we examined the robustness of our model by evaluating it on data sets from various domains. We use the Berkeley Parser as our baseline parser, and trained it on our training set. For Chinese, we performed our experiments on the cross domain data sets from Chinese Treebank 8.0 (Xue et al., 2013). It consists of six domains: newswire (nw), magazine articles (mz), broadcast news (bn), broadcast conversation (bc), weblogs (wb) and discussion forums (df). Since all of the mz domain data is already included in our training set, we only selected sample sentences from the other five domains as the test sets 5, and made sure these test sets had no overlap with our treebank training, development and test sets. Note that we did not use any data from these five domains for training or development. The models are still the ones described in the previous subsection. The results are presented in T</context>
</contexts>
<marker>Xue, Zhang, Jiang, Palmer, Xia, Chiou, Chang, 2013</marker>
<rawString>Nianwen Xue, Xiuhong Zhang, Zixin Jiang, Martha Palmer, Fei Xia, Fu-Dong Chiou, and Meiyu Chang. 2013. Chinese treebank 8.0. Philadelphia: Linguistic Data Consortium, page https://catalog.ldc.upenn.edu/LDC2013T21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Transition-based parsing of the chinese treebank using a global discriminative model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies,</booktitle>
<pages>162--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2857" citStr="Zhang and Clark, 2009" startWordPosition="442" endWordPosition="445">nd more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after this painstaking process, it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results. A more desirable alternative is to learn features automatically with machine learning algorithms. Lei et al. (2014) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing. However, to achieve competitive performance, they had to combine the learned features with the t</context>
<context position="8671" citStr="Zhang and Clark (2009)" startWordPosition="1407" endWordPosition="1410">mpletep arse tre e.T het askofp ars in gistoscan thei npu tse ntencef r omleft t o right a ndperf orm a seq uenc eofs hi ft-red u cea cti o ns tot rans for mthe in itia lst ate in to a terminal state. Inorder toj oin tly assignP OSta gs andconstr u cta const itue ntstruc ture f ora nin putsente nce, wede fine the followi ngac tion s f ort heac ti ons etT ,follow in gWangandX ue(2 014 ):•SHI 1139 parse tree by performing the action sequence {sh-DT, sh-NNS, rr-NP, sh-VBP, sh-VBN, ru-VP, rr-VP, rr-S}. To process multi-branch trees, we employ binarization and debinarization processes described in Zhang and Clark (2009) to transform multi-branch trees into binary trees and restore the generated binary trees back to their original forms. For inference, we employ the beam search decoding algorithm (Zhang and Clark, 2009) to balance the tradeoff between accuracy and efficiency. 3 Feature Optimization Model 3.1 Model To determine which action t E T should be performed at a given state s E S, we need a model to score each possible (s, t) combination. In previous approaches (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009), the model is usually defined as a linear model Score(s, t) = 2Vi · Φ(s, t),</context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Yue Zhang and Stephen Clark. 2009. Transition-based parsing of the chinese treebank using a global discriminative model. In Proceedings of the 11th International Conference on Parsing Technologies, pages 162–171. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Yue Zhang</author>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Fast and accurate shiftreduce constituent parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>434--443</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2372" citStr="Zhu et al., 2013" startWordPosition="360" endWordPosition="363"> challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order to search over their large structural space, which makes it very hard to efficiently train discriminative models. So, for a long time, the task was mainly solved with generative models (Collins, 1999; Charniak, 2000; Petrov et al., 2006). In the last few years, however, with the use of effective parsing strategies, approximate inference algorithms, and more efficient training methods, discriminative models began to surpass the generative models (Carreras et al., 2008; Zhu et al., 2013; Wang and Xue, 2014). Just like other NLP tasks, the performance of discriminative constituent parsing crucially relies on feature engineering. If the feature set is too small, it might underfit the model and leads to low performance. On the other hand, too many features may result in an overfitting problem. Usually, an effective set of features have to be designed manually and selected through repeated experiments (Sagae and Lavie, 2005; Wang et al., 2006; Zhang and Clark, 2009). Not only does this procedure require a lot of expertise, but it is also tedious and time-consuming. Even after th</context>
<context position="20869" citStr="Zhu et al. (2013)" startWordPosition="3439" endWordPosition="3442">e mixed data set. The second strategy (Pre-Train) is to first pretrain models with the automatically parsed data, and then fine-tune models with the hand-annotated training set. Table 3 shows results of different experimental configurations for Chinese. For the Mix-Train 4Its performance is Fl =83.9 on Chinese and Fl =90.8% on English. 1142 Mix-Train Pre-Train # Auto Sent F1 POS F1 POS 0 87.8 97.0 — — 50K 87.2 96.8 88.4 97.1 100K 88.7 96.9 89.5 97.1 200K 89.2 97.2 89.5 97.4 Type System F1 Supervised*$ 83.2 Ours Pretrain-Finetune*$ 86.6 Petrov and Klein (2007) 83.3 SI Wang and Xue (2014)$ 83.6 Zhu et al. (2013)$ 85.6 Wang and Xue (2014)$ 86.3 Table 3: Semi-supervised training for Chinese. SE Mix-Train Pre-Train # Auto Sent F1 POS F1 POS 0 89.7 96.6 — — 50K 89.4 96.1 90.2 96.4 100K 89.5 96.0 90.4 96.5 200K 89.2 95.8 90.8 96.7 Table 4: Semi-supervised training for English. strategy, when we only use 50K automatically parsed sentences, the performance drops in comparison with the model trained without using any automatically parsed data. When we increase the automatically parsed data to 100K sentences, the parsing performance improves about 1 percent but the POS tagging accuracy drops slightly. When we</context>
<context position="24631" citStr="Zhu et al. (2013)" startWordPosition="4032" endWordPosition="4035"> that our neural network model is a non-linear model, so the back propagation algorithm can only reach a local optimum. In our “Supervised” system the starting points are randomly initialized in the parameter space, so it only reaches local optimum. In comparison, our “Pretrain-Finetune” system gets to see large amount of automatically parsed data, and initializes the starting points with the pre-trained 1143 Type System F1 Supervised*$ 89.4 Ours Pretrain-Finetune*$ 90.7 Collins (1999) 88.2 Charniak (2000) 89.5 Henderson (2003)* 88.8 SI Petrov and Klein (2007) 90.1 Carreras et al. (2008) 91.1 Zhu et al. (2013)$ 90.4 Huang et al. (2010) 91.6 SE Zhu et al. (2013)$ 91.3 Collobert (2011)* 89.1 Henderson (2004)* 90.1 Charniak and Johnson (2005) 91.5 McClosky et al. (2006) 92.3 RE Huang (2008) 91.7 Socher et al. (2013)* 90.4 Table 6: Comparing with the state-of-the-art systems on English test set. * marks neural network based systems. $ marks shift-reduce parsing systems. parameters. So it finds a much better local optimum than the “Supervised” system. Comparing our “Pretrain-Finetune” system with all the stateof-the-art systems, we see our system surpass all the other systems. Although our system only u</context>
</contexts>
<marker>Zhu, Zhang, Chen, Zhang, Zhu, 2013</marker>
<rawString>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013. Fast and accurate shiftreduce constituent parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 434–443, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>