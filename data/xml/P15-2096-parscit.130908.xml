<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002996">
<title confidence="0.990131">
Automatic Identification of Age-Appropriate Ratings of Song Lyrics
</title>
<author confidence="0.991828">
Anggi Maulidyani and Ruli Manurung
</author>
<affiliation confidence="0.7858675">
Faculty of Computer Science, Universitas Indonesia
Depok 16424, West Java, Indonesia
</affiliation>
<email confidence="0.98766">
anggi.maulidyani@ui.ac.id,maruli@cs.ui.ac.id
</email>
<sectionHeader confidence="0.993666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930285714286">
This paper presents a novel task, namely
the automatic identification of age-
appropriate ratings of a musical track, or
album, based on its lyrics. Details are
provided regarding the construction of a
dataset of lyrics from 12,242 tracks across
1,798 albums along with age-appropriate
ratings obtained from various web re-
sources, along with results from various
text classification experiments. The best
accuracy of 71.02% for classifying albums
by age groups is achieved by combining
vector space model and psycholinguistic
features.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999836714285714">
Media age-appropriateness can be defined as the
suitability of the consumption of a media item,
e.g. a song, book, film, videogame, etc., by a
child of a given age based on norms that are gen-
erally agreed upon within a society. Such norms
may include behavioral, sociological, psycholog-
ical, and other factors. Whilst we acknowledge
that this is largely a subjective judgment, and that
there may be wide variance between very small
circles that could be considered demographically
homogenous, nevertheless, parents, educators, and
policymakers may find such judgments valuable in
the process of guiding and supervising the media
consumption of children.
This topic is closely related to well-known con-
tent rating schemes such as the MPAA film rating
system1, but whereas such schemes are focused
more on whether a film contains adult material or
not, age-appropriatness can be thought of as being
more nuanced, and takes into consideration more
factors such as educational value.
</bodyText>
<footnote confidence="0.863277">
1http://www.mpaa.org/film-ratings
</footnote>
<bodyText confidence="0.999715464285714">
One popular resource for such ratings is Com-
mon Sense Media2, a website that provides re-
views for various media, with a focus on age ap-
propriateness and learning potential for children.
Whilst acknowledging that such ratings are of
interest to many people, the position of this re-
search is neutral towards the efficacy and utility
of such ratings: we only seek to ask the question
of whether it is possible to automate the identifi-
cation of these age-appropriateness ratings.
This work focuses on song lyrics. There are
many aspects that can contribute to the age-
appropriateness of a song, but we believe that by
far the most dominant factor is its lyrics. Thus, the
approach that is taken to automating the identifi-
cation of age-appropriatness ratings is to treat it as
a supervised text classification task: first, a corpus
of song lyrics along with age-appropriateness rat-
ings is constructed, and subsequently this corpus
is used to train a model based on various textual
features.
To give the reader an idea of this task, Fig-
ures 1 to 3 show a sampler of snippets of lyrics3
from songs along with their age-appropriate rat-
ings according to Common Sense Media. Our
goal is to be able to automatically predict the age-
appropriate rating given the lyrics of a song in such
cases.
</bodyText>
<figure confidence="0.851044857142857">
Oh, I’m Sammy the snake
And I look like the letter ”S”ssss.
Oh, yes.
I’m all wiggly and curvy,
And I look like the letter ”S”ssss.
I confess.
(age-appropriate rating: 2)
</figure>
<figureCaption confidence="0.8238065">
Figure 1: Snippet of “Sammy the Snake”, from
Sesame Street Halloween Collection
</figureCaption>
<footnote confidence="0.9993465">
2http://www.commonsensemedia.org
3All works are copyrighted to their respective owners.
</footnote>
<page confidence="0.693578">
583
</page>
<footnote confidence="0.524562333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 583–587,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</footnote>
<construct confidence="0.665010333333333">
Do you want to build a snowman?
Come on, let’s go and play
I never see you anymore
Come out the door
It’s like you’ve gone away
(age-appropriate rating: 5)
</construct>
<figureCaption confidence="0.909051">
Figure 2: Snippet of “Do you want to build a
snowman?”, from Frozen Original Motion Picture
Soundtrack
</figureCaption>
<figure confidence="0.951087111111111">
You can take everything I have
You can break everything I am
Like I’m made of glass
Like I’m made ofpaper
Go on and try to tear me down
I will be rising from the ground
Like a skyscraper
Like a skyscraper
(age-appropriate rating: 9)
</figure>
<figureCaption confidence="0.9928465">
Figure 3: Snippet of “Skyscraper”, from Unbro-
ken - Demi Lovato
</figureCaption>
<bodyText confidence="0.9992132">
In Section 2 we discuss related work, before
presenting our work on constructing the corpus
(Section 3) and carrying out text classification ex-
periments (Section 4). Finally, we present a tenta-
tive summary in Section 5.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999555666666666">
To our knowledge, there is no previous work
that has attempted what is described in this pa-
per. There is some thematically related work,
such as automatic filtering of pornographic con-
tent (Polpinij et al., 2006; Sood et al., 2012; Xiang
et al., 2012; Su et al., 2004), but we believe the na-
ture of the task is significantly different such that
a different approach is required.
However, text or document classification, the
general technique employed in this paper, is a very
common task (Manning et al., 2008). In text clas-
sification, given a document d, the task is to assign
it a class, or label, c, from a fixed, human-defined
set of possible classes C = {c1, c2, ... , cn}. In or-
der to achieve this, a training set of labelled doc-
uments (d, c) is given to a learning algorithm to
learn a classifier that maps documents to classes.
Documents are typically represented as a vec-
tor in a high-dimensional space, such as term-
document matrices, or results of dimensional-
ity reduction techniques such as Latent Semantic
Analysis (Landauer et al., 1998), or more recently,
using vector representations of words produced by
neural networks (Pennington et al., 2014).
Text classification has many applications,
among others spam filtering (Androutsopoulos et
al., 2000) and sentiment analysis (Pang and Lee,
2008).
One particular application that could be deemed
of relevance with respect to our work is that of
readability assessment (Pitler and Nenkova, 2008;
Feng et al., 2010), i.e. determining the ease with
which a written text can be understood by a reader,
since age is certainly a dimension along which
readability varies. However, our literature re-
view of this area suggested that the aspects be-
ing considered in readability assessment are suf-
ficiently different from the dimensions that seem
to be most relevant for media age appropriatness
ratings. Following Manurung et al. (2008), we hy-
pothesize that utilizing resources such as the MRC
Psycholinguistic Database (Coltheart, 1981) could
be valuable in determining age appropriateness, in
particular various features such as familiarity, im-
ageability, age-of-acquisition, and concreteness.
</bodyText>
<sectionHeader confidence="0.983377" genericHeader="method">
3 Corpus Construction
</sectionHeader>
<bodyText confidence="0.999995571428571">
There are three steps in obtaining the data required
for our corpus: obtaining album details and age-
appropriateness ratings, searching for the track-
listing of each album, and obtaining the lyrics for
each song. Each step is carried out by querying a
different website. To achieve this, a Java applica-
tion that utilizes the jsoup library4 was developed.
</bodyText>
<subsectionHeader confidence="0.9988475">
3.1 Obtaining album details and
age-appropriateness ratings
</subsectionHeader>
<bodyText confidence="0.999758230769231">
The Common Sense Media website provides re-
views for various music albums. The reviews con-
sist of a textual review, the age-appropriate rating
for the album, which consists of an integer in the
interval [2,17] or the label ’Not For Kids’, and
metadata about the album such as title, artist, and
genre. Aside from that, there are also other an-
notations such as a quality rating (1-5 stars), and
specific aspectual ratings such as positive mes-
sages, role models, violence, sex, language, con-
sumerism, drinking, drugs &amp; smoking. The web-
site also allows visitors to contribute user ratings
and reviews. In our experiments we only utilize
</bodyText>
<footnote confidence="0.995035">
4http://www.jsoup.org
</footnote>
<page confidence="0.995639">
584
</page>
<bodyText confidence="0.9764305">
the album metadata and integer indicating the age-
appropriate rating.
</bodyText>
<subsectionHeader confidence="0.998886">
3.2 Tracklist searching
</subsectionHeader>
<bodyText confidence="0.999982043478261">
A tracklist is a list of all the songs, or tracks, con-
tained within an album. From the information pre-
viously obtained from Common Sense Media, the
next step is to obtain the tracklist of each album.
For this we query the MusicBrainz website5, an
open music encyclopedia that makes music meta-
data available to the public. To obtain the tracklists
we employed the advanced query search mode that
allows the use of boolean operators. We tried sev-
eral combinations of queries involving album ti-
tle, singer, and label information, and it turned out
that queries consisting of album title and singer
produced the highest recall. When MusicBrainz
returns multiple results for a given query, we sim-
ply select the first result. For special cases where
the tracks on an album are performed by vari-
ous artists, e.g. a compilation album, or a sound-
track album, it is during this stage that we also ex-
tract information regarding the track-specific artist
name. Finally, we assume that if the album title
contains the string ‘CD Single’ then it only con-
tains one track and we skip forward to the next
step.
</bodyText>
<subsectionHeader confidence="0.999064">
3.3 Lyrics searching
</subsectionHeader>
<bodyText confidence="0.9999835">
For this step, we consulted two websites as the
source reference for song lyrics, songlyrics.com
and lyricsmode.com. The former is first consulted,
and only if it fails to yield any results is the latter
consulted. If a track is not found on both websites,
we discard it from our data set. Similar to the pre-
vious step, we perform a query to obtain results,
however during this step the query consists of the
song title and singer. Once again, given multiple
results we simply choose the first result. In to-
tal, we were able to retrieve lyrics from 12,242
songs across 1,798 albums. Table 1 provides an
overview of the number of tracks and albums ob-
tained per age rating.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="method">
4 Experimentation
</sectionHeader>
<bodyText confidence="0.9996348">
Since the constructed data set is imbalanced, we
use the SMOTE oversampling technique to over-
come this problem (Chawla et al., 2002). This re-
sults in a balanced dataset with the same number
of samples in each class.
</bodyText>
<footnote confidence="0.987434">
5http://www.musicbrainz.org
</footnote>
<table confidence="0.999951684210526">
Group Age #Tracks #Albums
Toddler 2 696 119
3 130 23
Pre-schooler 4 251 46
5 204 31
6 281 41
Middle childhood 1 7 358 71
8 654 118
9 237 50
Middle childhood 2 10 1,590 253
11 580 105
12 1,849 253
Young teen 13 1,767 242
14 1,453 177
15 653 116
Teenager 16 521 64
17 180 16
Adult &gt;17 838 73
Total 12,242 1,798
</table>
<tableCaption confidence="0.999889">
Table 1: Statistics of the dataset
</tableCaption>
<bodyText confidence="0.992970964285715">
Once the dataset is complete, classifiers were
trained and used to carry out experiment scenarios
that vary along several factors. For the class labels,
two scenarios are considered: one where each age
rating from 2 to 17 and ’Not For Kids’ is a sepa-
rate class, and another where the data is clustered
together based on some conventional developmen-
tal age groupings6, i.e. toddlers (ages 2 &amp; 3), pre-
schoolers (ages 4 &amp; 5), middle-childhood 1 (ages
6 to 8), middle-childhood 2 (ages 9 to 11), young-
teens (ages 12 to 14), and teenagers (ages 15 to
17), with an additional category for ages beyond
17 using the ’Not For Kids’ labelled data.
For the instance data, two scenarios are also
considered: one where classification is done on a
per-track basis, and one on a per-album basis (i.e.
where lyrics from all its constituent tracks are con-
catenated).
As for the feature representation, three primary
variations are considered:
Vector Space Model. This is a baseline method
where each word appearing in the dataset becomes
a feature, and a vector representing an instance
consists of the tf.idf values of all words. Addi-
tionally, stemming is first performed on the words,
and information gain-based attribute selection is
applied.
MRC Psycholinguistic data. For this feature
</bodyText>
<footnote confidence="0.977892">
6http://www.cdc.gov/ncbddd/childdevelopment/positiveparenting/
</footnote>
<page confidence="0.998524">
585
</page>
<bodyText confidence="0.999929774193549">
representation, given each distinct word appear-
ing in the lyrics of a track (or album), a lookup is
performed on the MRC psycholinguistic database,
and if appropriate values exist, they are added to
the tally for the familiarity, imageability, age-of-
acquisition, and concreteness scores. Thus, an in-
stance is represented by a vector with four real val-
ues. The vectors are normalized with respect to the
number of words contributing to the values.
GloVe vectors. GloVe7 is a tool that produces
vector representations of words trained on very
large corpora (Pennington et al., 2014). It is sim-
ilar to dimensionality reduction approaches such
as latent semantic analysis. For this experiment,
the 50-dimensional pre-trained vectors trained on
Wikipedia and Gigaword corpora were used.
When combining feature representations, we
simply concatenate their vectors.
Finally, for the classification itself, the Weka
toolkit is used. Given the ordinal nature of the
class labels, classification is carried out via regres-
sion (Frank et al., 1998), using the M5P-based
classifier (Wang and Witten, 1997). The experi-
ments were run using 4-fold cross validation.
For the initial experiment, only the baseline
VSM feature representation was used, and the
treatment of class labels and instance granularity
was varied. The results can be seen in Table 2,
which shows the average accuracy, i.e. the per-
centage of test instances that were correctly la-
belled, across 4 folds.
</bodyText>
<table confidence="0.991008333333333">
Age group Year
Per-track 69.77% 58.58%
Per-album 70.60% 57.15%
</table>
<tableCaption confidence="0.969614">
Table 2: Initial experiment varying class and in-
stance granularity
</tableCaption>
<bodyText confidence="0.941745714285714">
For the follow-up experiment, we focus on the
task of classifying at the per-album level of gran-
ularity, as ultimately this is the level at which
the original annotations are obtained. For the
class labels, both age groups and separate ages are
used. The feature representation was varied rang-
ing from VSM, VSM + MRC, VSM + GloVe, and
</bodyText>
<tableCaption confidence="0.67406">
VSM + GloVe + MRC. The results can be seen in
Table 3.
</tableCaption>
<footnote confidence="0.873697">
7http://nlp.stanford.edu/projects/glove/
</footnote>
<table confidence="0.9980334">
Features Age group Year
VSM 70.60% 57.15%
VSM + MRC 71.02% 56.80%
VSM + GloVe 70.58% 57.68%
VSM + GloVe + MRC 70.47% 57.85%
</table>
<tableCaption confidence="0.999729">
Table 3: Results varying feature representations
</tableCaption>
<sectionHeader confidence="0.933701" genericHeader="conclusions">
5 Discussion &amp; Summary
</sectionHeader>
<bodyText confidence="0.999987523809524">
From the initial experiment, it appears that distin-
guishing tracks at the level of granularity of spe-
cific year/age (e.g. “is this song more appropriate
for a 4 or 5 year old?”) is very difficult, as indi-
cated by an accuracy of only 57% to 58%. Bear in
mind, however, that this is a seventeen-way clas-
sification task. Shifting the level of granularity to
that of age groups transforms the task into a more
feasible one, with an accuracy around the 70%
mark. It is surprising to note that the per-track
performance is better than the per-album perfor-
mance when tracks are distinguished by specific
age/year rather than age groups. We had initially
hypothesized that classifying albums would be a
more consistent task given the increased context
and evidence available.
As for the various feature representations, we
note that the addition of the MRC psycholinguis-
tic features of familiarity, imageability, concrete-
ness, and age-of-acquisition does provide a small
accuracy increase in certain cases, as evidenced by
the highest accuracy of 71.02% when classifying
albums by age group using the VSM + MRC fea-
tures. The use of the GloVe vectors gives a slight
contribution in the case of classifying albums by
specific age/year, where the highest accuracy of
57.85% is obtained when combining VSM with
both the MRC and GloVe features.
There are many other features and contexts that
can also be utilized. For instance, given the meta-
data of artist, album, and genre, additional infor-
mation may be extracted from the web, e.g. the
artist’s biography, general-purpose album reviews,
genre tendencies, etc., all of which may contribute
to discerning age-appropriateness. Another set of
features that can be utilized are readability met-
rics, as they are often correlated with the age of
the reader.
To summarize, this paper has introduced a novel
task with clear practical applications in the form of
automatically identifying age-appropriate ratings
of songs and albums based on lyrics. The work
</bodyText>
<page confidence="0.994518">
586
</page>
<bodyText confidence="0.9999579">
reported is still in its very early stages, neverthe-
less we believe the findings are of interest to NLP
researchers.
Another question that needs to be addressed
is what sort of competence and agreement hu-
mans achieve on this task. To that end, we plan
to conduct a manual annotation experiment in-
volving several human subjects, themselves varied
across different age groups, and to measure inter-
annotator reliability (Passonneau et al., 2006).
</bodyText>
<sectionHeader confidence="0.998496" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883573170732">
Ion Androutsopoulos, John Koutsias, Konstantinos
Chandrinos, Georgios Paliouras, and Constantine D.
Spyropoulos. 2000. An evaluation of naive
Bayesian anti-spam filtering. In Proceedings of the
workshop on Machine Learning in the New Infor-
mation Age, 11th European Conference on Machine
Learning, pages 9–17, Barcelona, Spain.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.
Hall, and W. Philip Kegelmeyer. 2002. Smote: Syn-
thetic minority over-sampling technique. J. Artif.
Int. Res., 16(1):321–357, June.
Max Coltheart. 1981. The MRC psycholinguistic
database. The Quarterly Journal of Experimental
Psychology, 33(4):497–505.
Lijun Feng, Martin Jansche, Matt Huenerfauth, and
No´emie Elhadad. 2010. A comparison of features
for automatic readability assessment. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics: Posters, COLING ’10, pages
276–284, Stroudsburg, PA, USA. Association for
Computational Linguistics.
E. Frank, Y. Wang, S. Inglis, G. Holmes, and I.H. Wit-
ten. 1998. Using model trees for classification. Ma-
chine Learning, 32(1):63–76.
Thomas Landauer, Peter Foltz, and Darrell Laham.
1998. An introduction to latent semantic analysis.
Discourse Processes, 25:259–284.
Christopher Manning, Prabhakar Raghavan, and Hin-
rich Schutze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Ruli Manurung, Graeme Ritchie, Helen Pain, An-
nalu Waller, Dave O’Mara, and Rolf Black. 2008.
The construction of a pun generator for language
skills development. Applied Artificial Intelligence,
22(9):841–869.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.
Rebecca Passonneau, Nizar Habash, and Owen Ram-
bow. 2006. Inter-annotator agreement on a multi-
lingual semantic annotation task. In Proceedings of
the Fifth International Conference on Language Re-
sources and Evaluation (LREC 2006), Genoa, Italy,
May.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543. Associa-
tion for Computational Linguistics.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting
text quality. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’08, pages 186–195, Stroudsburg, PA,
USA. Association for Computational Linguistics.
J. Polpinij, A. Chotthanom, C. Sibunruang, R. Cham-
chong, and S. Puangpronpitag. 2006. Content-
based text classifiers for pornographic web filtering.
In Systems, Man and Cybernetics, 2006. SMC ’06.
IEEE International Conference on, volume 2, pages
1481–1485, Oct.
Sara Owsley Sood, Judd Antin, and Elizabeth F
Churchill. 2012. Using crowdsourcing to improve
profanity detection. In AAAI Spring Symposium:
Wisdom of the Crowd.
Gui-yang Su, Jian-hua Li, Ying-hua Ma, and Sheng-
hong Li. 2004. Improving the precision of
the keyword-matching pornographic text filtering
method using a hybrid model. Journal of Zhejiang
University Science, 5(9):1106–1113.
Y. Wang and I. H. Witten. 1997. Induction of model
trees for predicting continuous classes. In Poster
papers of the 9th European Conference on Machine
Learning. Springer.
Guang Xiang, Bin Fan, Ling Wang, Jason Hong, and
Carolyn Rose. 2012. Detecting offensive tweets via
topical feature discovery over a large scale twitter
corpus. In Proceedings of the 21st ACM Interna-
tional Conference on Information and Knowledge
Management, CIKM ’12, pages 1980–1984, New
York, NY, USA. ACM.
</reference>
<page confidence="0.997799">
587
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.440207">
<title confidence="0.99374">Automatic Identification of Age-Appropriate Ratings of Song Lyrics</title>
<author confidence="0.776247">Anggi Maulidyani</author>
<author confidence="0.776247">Ruli</author>
<affiliation confidence="0.675635">Faculty of Computer Science, Universitas</affiliation>
<address confidence="0.517784">Depok 16424, West Java,</address>
<abstract confidence="0.993461">This paper presents a novel task, namely the automatic identification of ageappropriate ratings of a musical track, or album, based on its lyrics. Details are provided regarding the construction of a dataset of lyrics from 12,242 tracks across 1,798 albums along with age-appropriate ratings obtained from various web resources, along with results from various text classification experiments. The best accuracy of 71.02% for classifying albums by age groups is achieved by combining vector space model and psycholinguistic features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>John Koutsias</author>
<author>Konstantinos Chandrinos</author>
<author>Georgios Paliouras</author>
<author>Constantine D Spyropoulos</author>
</authors>
<title>An evaluation of naive Bayesian anti-spam filtering.</title>
<date>2000</date>
<booktitle>In Proceedings of the workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning,</booktitle>
<pages>9--17</pages>
<location>Barcelona,</location>
<contexts>
<context position="5757" citStr="Androutsopoulos et al., 2000" startWordPosition="925" endWordPosition="928">of possible classes C = {c1, c2, ... , cn}. In order to achieve this, a training set of labelled documents (d, c) is given to a learning algorithm to learn a classifier that maps documents to classes. Documents are typically represented as a vector in a high-dimensional space, such as termdocument matrices, or results of dimensionality reduction techniques such as Latent Semantic Analysis (Landauer et al., 1998), or more recently, using vector representations of words produced by neural networks (Pennington et al., 2014). Text classification has many applications, among others spam filtering (Androutsopoulos et al., 2000) and sentiment analysis (Pang and Lee, 2008). One particular application that could be deemed of relevance with respect to our work is that of readability assessment (Pitler and Nenkova, 2008; Feng et al., 2010), i.e. determining the ease with which a written text can be understood by a reader, since age is certainly a dimension along which readability varies. However, our literature review of this area suggested that the aspects being considered in readability assessment are sufficiently different from the dimensions that seem to be most relevant for media age appropriatness ratings. Followin</context>
</contexts>
<marker>Androutsopoulos, Koutsias, Chandrinos, Paliouras, Spyropoulos, 2000</marker>
<rawString>Ion Androutsopoulos, John Koutsias, Konstantinos Chandrinos, Georgios Paliouras, and Constantine D. Spyropoulos. 2000. An evaluation of naive Bayesian anti-spam filtering. In Proceedings of the workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning, pages 9–17, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitesh V Chawla</author>
<author>Kevin W Bowyer</author>
<author>Lawrence O Hall</author>
<author>W Philip Kegelmeyer</author>
</authors>
<title>Smote: Synthetic minority over-sampling technique.</title>
<date>2002</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="9765" citStr="Chawla et al., 2002" startWordPosition="1584" endWordPosition="1587">r consulted. If a track is not found on both websites, we discard it from our data set. Similar to the previous step, we perform a query to obtain results, however during this step the query consists of the song title and singer. Once again, given multiple results we simply choose the first result. In total, we were able to retrieve lyrics from 12,242 songs across 1,798 albums. Table 1 provides an overview of the number of tracks and albums obtained per age rating. 4 Experimentation Since the constructed data set is imbalanced, we use the SMOTE oversampling technique to overcome this problem (Chawla et al., 2002). This results in a balanced dataset with the same number of samples in each class. 5http://www.musicbrainz.org Group Age #Tracks #Albums Toddler 2 696 119 3 130 23 Pre-schooler 4 251 46 5 204 31 6 281 41 Middle childhood 1 7 358 71 8 654 118 9 237 50 Middle childhood 2 10 1,590 253 11 580 105 12 1,849 253 Young teen 13 1,767 242 14 1,453 177 15 653 116 Teenager 16 521 64 17 180 16 Adult &gt;17 838 73 Total 12,242 1,798 Table 1: Statistics of the dataset Once the dataset is complete, classifiers were trained and used to carry out experiment scenarios that vary along several factors. For the class</context>
</contexts>
<marker>Chawla, Bowyer, Hall, Kegelmeyer, 2002</marker>
<rawString>Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. 2002. Smote: Synthetic minority over-sampling technique. J. Artif. Int. Res., 16(1):321–357, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Coltheart</author>
</authors>
<title>The MRC psycholinguistic database.</title>
<date>1981</date>
<journal>The Quarterly Journal of Experimental Psychology,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="6482" citStr="Coltheart, 1981" startWordPosition="1041" endWordPosition="1042">th respect to our work is that of readability assessment (Pitler and Nenkova, 2008; Feng et al., 2010), i.e. determining the ease with which a written text can be understood by a reader, since age is certainly a dimension along which readability varies. However, our literature review of this area suggested that the aspects being considered in readability assessment are sufficiently different from the dimensions that seem to be most relevant for media age appropriatness ratings. Following Manurung et al. (2008), we hypothesize that utilizing resources such as the MRC Psycholinguistic Database (Coltheart, 1981) could be valuable in determining age appropriateness, in particular various features such as familiarity, imageability, age-of-acquisition, and concreteness. 3 Corpus Construction There are three steps in obtaining the data required for our corpus: obtaining album details and ageappropriateness ratings, searching for the tracklisting of each album, and obtaining the lyrics for each song. Each step is carried out by querying a different website. To achieve this, a Java application that utilizes the jsoup library4 was developed. 3.1 Obtaining album details and age-appropriateness ratings The Co</context>
</contexts>
<marker>Coltheart, 1981</marker>
<rawString>Max Coltheart. 1981. The MRC psycholinguistic database. The Quarterly Journal of Experimental Psychology, 33(4):497–505.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijun Feng</author>
<author>Martin Jansche</author>
<author>Matt Huenerfauth</author>
<author>No´emie Elhadad</author>
</authors>
<title>A comparison of features for automatic readability assessment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>276--284</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5968" citStr="Feng et al., 2010" startWordPosition="959" endWordPosition="962">ly represented as a vector in a high-dimensional space, such as termdocument matrices, or results of dimensionality reduction techniques such as Latent Semantic Analysis (Landauer et al., 1998), or more recently, using vector representations of words produced by neural networks (Pennington et al., 2014). Text classification has many applications, among others spam filtering (Androutsopoulos et al., 2000) and sentiment analysis (Pang and Lee, 2008). One particular application that could be deemed of relevance with respect to our work is that of readability assessment (Pitler and Nenkova, 2008; Feng et al., 2010), i.e. determining the ease with which a written text can be understood by a reader, since age is certainly a dimension along which readability varies. However, our literature review of this area suggested that the aspects being considered in readability assessment are sufficiently different from the dimensions that seem to be most relevant for media age appropriatness ratings. Following Manurung et al. (2008), we hypothesize that utilizing resources such as the MRC Psycholinguistic Database (Coltheart, 1981) could be valuable in determining age appropriateness, in particular various features </context>
</contexts>
<marker>Feng, Jansche, Huenerfauth, Elhadad, 2010</marker>
<rawString>Lijun Feng, Martin Jansche, Matt Huenerfauth, and No´emie Elhadad. 2010. A comparison of features for automatic readability assessment. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 276–284, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Frank</author>
<author>Y Wang</author>
<author>S Inglis</author>
<author>G Holmes</author>
<author>I H Witten</author>
</authors>
<title>Using model trees for classification.</title>
<date>1998</date>
<booktitle>Machine Learning,</booktitle>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="12597" citStr="Frank et al., 1998" startWordPosition="2051" endWordPosition="2054">s contributing to the values. GloVe vectors. GloVe7 is a tool that produces vector representations of words trained on very large corpora (Pennington et al., 2014). It is similar to dimensionality reduction approaches such as latent semantic analysis. For this experiment, the 50-dimensional pre-trained vectors trained on Wikipedia and Gigaword corpora were used. When combining feature representations, we simply concatenate their vectors. Finally, for the classification itself, the Weka toolkit is used. Given the ordinal nature of the class labels, classification is carried out via regression (Frank et al., 1998), using the M5P-based classifier (Wang and Witten, 1997). The experiments were run using 4-fold cross validation. For the initial experiment, only the baseline VSM feature representation was used, and the treatment of class labels and instance granularity was varied. The results can be seen in Table 2, which shows the average accuracy, i.e. the percentage of test instances that were correctly labelled, across 4 folds. Age group Year Per-track 69.77% 58.58% Per-album 70.60% 57.15% Table 2: Initial experiment varying class and instance granularity For the follow-up experiment, we focus on the ta</context>
</contexts>
<marker>Frank, Wang, Inglis, Holmes, Witten, 1998</marker>
<rawString>E. Frank, Y. Wang, S. Inglis, G. Holmes, and I.H. Witten. 1998. Using model trees for classification. Machine Learning, 32(1):63–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Peter Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="5543" citStr="Landauer et al., 1998" startWordPosition="896" endWordPosition="899">ral technique employed in this paper, is a very common task (Manning et al., 2008). In text classification, given a document d, the task is to assign it a class, or label, c, from a fixed, human-defined set of possible classes C = {c1, c2, ... , cn}. In order to achieve this, a training set of labelled documents (d, c) is given to a learning algorithm to learn a classifier that maps documents to classes. Documents are typically represented as a vector in a high-dimensional space, such as termdocument matrices, or results of dimensionality reduction techniques such as Latent Semantic Analysis (Landauer et al., 1998), or more recently, using vector representations of words produced by neural networks (Pennington et al., 2014). Text classification has many applications, among others spam filtering (Androutsopoulos et al., 2000) and sentiment analysis (Pang and Lee, 2008). One particular application that could be deemed of relevance with respect to our work is that of readability assessment (Pitler and Nenkova, 2008; Feng et al., 2010), i.e. determining the ease with which a written text can be understood by a reader, since age is certainly a dimension along which readability varies. However, our literature</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas Landauer, Peter Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse Processes, 25:259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schutze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5003" citStr="Manning et al., 2008" startWordPosition="799" endWordPosition="802"> classification experiments (Section 4). Finally, we present a tentative summary in Section 5. 2 Related Work To our knowledge, there is no previous work that has attempted what is described in this paper. There is some thematically related work, such as automatic filtering of pornographic content (Polpinij et al., 2006; Sood et al., 2012; Xiang et al., 2012; Su et al., 2004), but we believe the nature of the task is significantly different such that a different approach is required. However, text or document classification, the general technique employed in this paper, is a very common task (Manning et al., 2008). In text classification, given a document d, the task is to assign it a class, or label, c, from a fixed, human-defined set of possible classes C = {c1, c2, ... , cn}. In order to achieve this, a training set of labelled documents (d, c) is given to a learning algorithm to learn a classifier that maps documents to classes. Documents are typically represented as a vector in a high-dimensional space, such as termdocument matrices, or results of dimensionality reduction techniques such as Latent Semantic Analysis (Landauer et al., 1998), or more recently, using vector representations of words pr</context>
</contexts>
<marker>Manning, Raghavan, Schutze, 2008</marker>
<rawString>Christopher Manning, Prabhakar Raghavan, and Hinrich Schutze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruli Manurung</author>
<author>Graeme Ritchie</author>
<author>Helen Pain</author>
<author>Annalu Waller</author>
<author>Dave O’Mara</author>
<author>Rolf Black</author>
</authors>
<title>The construction of a pun generator for language skills development.</title>
<date>2008</date>
<journal>Applied Artificial Intelligence,</journal>
<volume>22</volume>
<issue>9</issue>
<marker>Manurung, Ritchie, Pain, Waller, O’Mara, Black, 2008</marker>
<rawString>Ruli Manurung, Graeme Ritchie, Helen Pain, Annalu Waller, Dave O’Mara, and Rolf Black. 2008. The construction of a pun generator for language skills development. Applied Artificial Intelligence, 22(9):841–869.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="5801" citStr="Pang and Lee, 2008" startWordPosition="932" endWordPosition="935">o achieve this, a training set of labelled documents (d, c) is given to a learning algorithm to learn a classifier that maps documents to classes. Documents are typically represented as a vector in a high-dimensional space, such as termdocument matrices, or results of dimensionality reduction techniques such as Latent Semantic Analysis (Landauer et al., 1998), or more recently, using vector representations of words produced by neural networks (Pennington et al., 2014). Text classification has many applications, among others spam filtering (Androutsopoulos et al., 2000) and sentiment analysis (Pang and Lee, 2008). One particular application that could be deemed of relevance with respect to our work is that of readability assessment (Pitler and Nenkova, 2008; Feng et al., 2010), i.e. determining the ease with which a written text can be understood by a reader, since age is certainly a dimension along which readability varies. However, our literature review of this area suggested that the aspects being considered in readability assessment are sufficiently different from the dimensions that seem to be most relevant for media age appropriatness ratings. Following Manurung et al. (2008), we hypothesize tha</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Passonneau</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Inter-annotator agreement on a multilingual semantic annotation task.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Genoa, Italy,</location>
<marker>Passonneau, Habash, Rambow, 2006</marker>
<rawString>Rebecca Passonneau, Nizar Habash, and Owen Rambow. 2006. Inter-annotator agreement on a multilingual semantic annotation task. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC 2006), Genoa, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>GloVe: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1532--1543</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5654" citStr="Pennington et al., 2014" startWordPosition="912" endWordPosition="915">iven a document d, the task is to assign it a class, or label, c, from a fixed, human-defined set of possible classes C = {c1, c2, ... , cn}. In order to achieve this, a training set of labelled documents (d, c) is given to a learning algorithm to learn a classifier that maps documents to classes. Documents are typically represented as a vector in a high-dimensional space, such as termdocument matrices, or results of dimensionality reduction techniques such as Latent Semantic Analysis (Landauer et al., 1998), or more recently, using vector representations of words produced by neural networks (Pennington et al., 2014). Text classification has many applications, among others spam filtering (Androutsopoulos et al., 2000) and sentiment analysis (Pang and Lee, 2008). One particular application that could be deemed of relevance with respect to our work is that of readability assessment (Pitler and Nenkova, 2008; Feng et al., 2010), i.e. determining the ease with which a written text can be understood by a reader, since age is certainly a dimension along which readability varies. However, our literature review of this area suggested that the aspects being considered in readability assessment are sufficiently dif</context>
<context position="12141" citStr="Pennington et al., 2014" startWordPosition="1985" endWordPosition="1988">ddd/childdevelopment/positiveparenting/ 585 representation, given each distinct word appearing in the lyrics of a track (or album), a lookup is performed on the MRC psycholinguistic database, and if appropriate values exist, they are added to the tally for the familiarity, imageability, age-ofacquisition, and concreteness scores. Thus, an instance is represented by a vector with four real values. The vectors are normalized with respect to the number of words contributing to the values. GloVe vectors. GloVe7 is a tool that produces vector representations of words trained on very large corpora (Pennington et al., 2014). It is similar to dimensionality reduction approaches such as latent semantic analysis. For this experiment, the 50-dimensional pre-trained vectors trained on Wikipedia and Gigaword corpora were used. When combining feature representations, we simply concatenate their vectors. Finally, for the classification itself, the Weka toolkit is used. Given the ordinal nature of the class labels, classification is carried out via regression (Frank et al., 1998), using the M5P-based classifier (Wang and Witten, 1997). The experiments were run using 4-fold cross validation. For the initial experiment, on</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>186--195</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5948" citStr="Pitler and Nenkova, 2008" startWordPosition="955" endWordPosition="958">ses. Documents are typically represented as a vector in a high-dimensional space, such as termdocument matrices, or results of dimensionality reduction techniques such as Latent Semantic Analysis (Landauer et al., 1998), or more recently, using vector representations of words produced by neural networks (Pennington et al., 2014). Text classification has many applications, among others spam filtering (Androutsopoulos et al., 2000) and sentiment analysis (Pang and Lee, 2008). One particular application that could be deemed of relevance with respect to our work is that of readability assessment (Pitler and Nenkova, 2008; Feng et al., 2010), i.e. determining the ease with which a written text can be understood by a reader, since age is certainly a dimension along which readability varies. However, our literature review of this area suggested that the aspects being considered in readability assessment are sufficiently different from the dimensions that seem to be most relevant for media age appropriatness ratings. Following Manurung et al. (2008), we hypothesize that utilizing resources such as the MRC Psycholinguistic Database (Coltheart, 1981) could be valuable in determining age appropriateness, in particul</context>
</contexts>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 186–195, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Polpinij</author>
<author>A Chotthanom</author>
<author>C Sibunruang</author>
<author>R Chamchong</author>
<author>S Puangpronpitag</author>
</authors>
<title>Contentbased text classifiers for pornographic web filtering.</title>
<date>2006</date>
<booktitle>In Systems, Man and Cybernetics,</booktitle>
<volume>2</volume>
<pages>1481--1485</pages>
<contexts>
<context position="4703" citStr="Polpinij et al., 2006" startWordPosition="747" endWordPosition="750">o tear me down I will be rising from the ground Like a skyscraper Like a skyscraper (age-appropriate rating: 9) Figure 3: Snippet of “Skyscraper”, from Unbroken - Demi Lovato In Section 2 we discuss related work, before presenting our work on constructing the corpus (Section 3) and carrying out text classification experiments (Section 4). Finally, we present a tentative summary in Section 5. 2 Related Work To our knowledge, there is no previous work that has attempted what is described in this paper. There is some thematically related work, such as automatic filtering of pornographic content (Polpinij et al., 2006; Sood et al., 2012; Xiang et al., 2012; Su et al., 2004), but we believe the nature of the task is significantly different such that a different approach is required. However, text or document classification, the general technique employed in this paper, is a very common task (Manning et al., 2008). In text classification, given a document d, the task is to assign it a class, or label, c, from a fixed, human-defined set of possible classes C = {c1, c2, ... , cn}. In order to achieve this, a training set of labelled documents (d, c) is given to a learning algorithm to learn a classifier that m</context>
</contexts>
<marker>Polpinij, Chotthanom, Sibunruang, Chamchong, Puangpronpitag, 2006</marker>
<rawString>J. Polpinij, A. Chotthanom, C. Sibunruang, R. Chamchong, and S. Puangpronpitag. 2006. Contentbased text classifiers for pornographic web filtering. In Systems, Man and Cybernetics, 2006. SMC ’06. IEEE International Conference on, volume 2, pages 1481–1485, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Owsley Sood</author>
<author>Judd Antin</author>
<author>Elizabeth F Churchill</author>
</authors>
<title>Using crowdsourcing to improve profanity detection.</title>
<date>2012</date>
<booktitle>In AAAI Spring Symposium: Wisdom of the Crowd.</booktitle>
<contexts>
<context position="4722" citStr="Sood et al., 2012" startWordPosition="751" endWordPosition="754">e rising from the ground Like a skyscraper Like a skyscraper (age-appropriate rating: 9) Figure 3: Snippet of “Skyscraper”, from Unbroken - Demi Lovato In Section 2 we discuss related work, before presenting our work on constructing the corpus (Section 3) and carrying out text classification experiments (Section 4). Finally, we present a tentative summary in Section 5. 2 Related Work To our knowledge, there is no previous work that has attempted what is described in this paper. There is some thematically related work, such as automatic filtering of pornographic content (Polpinij et al., 2006; Sood et al., 2012; Xiang et al., 2012; Su et al., 2004), but we believe the nature of the task is significantly different such that a different approach is required. However, text or document classification, the general technique employed in this paper, is a very common task (Manning et al., 2008). In text classification, given a document d, the task is to assign it a class, or label, c, from a fixed, human-defined set of possible classes C = {c1, c2, ... , cn}. In order to achieve this, a training set of labelled documents (d, c) is given to a learning algorithm to learn a classifier that maps documents to cl</context>
</contexts>
<marker>Sood, Antin, Churchill, 2012</marker>
<rawString>Sara Owsley Sood, Judd Antin, and Elizabeth F Churchill. 2012. Using crowdsourcing to improve profanity detection. In AAAI Spring Symposium: Wisdom of the Crowd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gui-yang Su</author>
<author>Jian-hua Li</author>
<author>Ying-hua Ma</author>
<author>Shenghong Li</author>
</authors>
<title>Improving the precision of the keyword-matching pornographic text filtering method using a hybrid model.</title>
<date>2004</date>
<journal>Journal of Zhejiang University Science,</journal>
<volume>5</volume>
<issue>9</issue>
<contexts>
<context position="4760" citStr="Su et al., 2004" startWordPosition="759" endWordPosition="762">per Like a skyscraper (age-appropriate rating: 9) Figure 3: Snippet of “Skyscraper”, from Unbroken - Demi Lovato In Section 2 we discuss related work, before presenting our work on constructing the corpus (Section 3) and carrying out text classification experiments (Section 4). Finally, we present a tentative summary in Section 5. 2 Related Work To our knowledge, there is no previous work that has attempted what is described in this paper. There is some thematically related work, such as automatic filtering of pornographic content (Polpinij et al., 2006; Sood et al., 2012; Xiang et al., 2012; Su et al., 2004), but we believe the nature of the task is significantly different such that a different approach is required. However, text or document classification, the general technique employed in this paper, is a very common task (Manning et al., 2008). In text classification, given a document d, the task is to assign it a class, or label, c, from a fixed, human-defined set of possible classes C = {c1, c2, ... , cn}. In order to achieve this, a training set of labelled documents (d, c) is given to a learning algorithm to learn a classifier that maps documents to classes. Documents are typically represe</context>
</contexts>
<marker>Su, Li, Ma, Li, 2004</marker>
<rawString>Gui-yang Su, Jian-hua Li, Ying-hua Ma, and Shenghong Li. 2004. Improving the precision of the keyword-matching pornographic text filtering method using a hybrid model. Journal of Zhejiang University Science, 5(9):1106–1113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>I H Witten</author>
</authors>
<title>Induction of model trees for predicting continuous classes.</title>
<date>1997</date>
<booktitle>In Poster papers of the 9th European Conference on Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="12653" citStr="Wang and Witten, 1997" startWordPosition="2059" endWordPosition="2062">s a tool that produces vector representations of words trained on very large corpora (Pennington et al., 2014). It is similar to dimensionality reduction approaches such as latent semantic analysis. For this experiment, the 50-dimensional pre-trained vectors trained on Wikipedia and Gigaword corpora were used. When combining feature representations, we simply concatenate their vectors. Finally, for the classification itself, the Weka toolkit is used. Given the ordinal nature of the class labels, classification is carried out via regression (Frank et al., 1998), using the M5P-based classifier (Wang and Witten, 1997). The experiments were run using 4-fold cross validation. For the initial experiment, only the baseline VSM feature representation was used, and the treatment of class labels and instance granularity was varied. The results can be seen in Table 2, which shows the average accuracy, i.e. the percentage of test instances that were correctly labelled, across 4 folds. Age group Year Per-track 69.77% 58.58% Per-album 70.60% 57.15% Table 2: Initial experiment varying class and instance granularity For the follow-up experiment, we focus on the task of classifying at the per-album level of granularity,</context>
</contexts>
<marker>Wang, Witten, 1997</marker>
<rawString>Y. Wang and I. H. Witten. 1997. Induction of model trees for predicting continuous classes. In Poster papers of the 9th European Conference on Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Xiang</author>
<author>Bin Fan</author>
<author>Ling Wang</author>
<author>Jason Hong</author>
<author>Carolyn Rose</author>
</authors>
<title>Detecting offensive tweets via topical feature discovery over a large scale twitter corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12,</booktitle>
<pages>1980--1984</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4742" citStr="Xiang et al., 2012" startWordPosition="755" endWordPosition="758">round Like a skyscraper Like a skyscraper (age-appropriate rating: 9) Figure 3: Snippet of “Skyscraper”, from Unbroken - Demi Lovato In Section 2 we discuss related work, before presenting our work on constructing the corpus (Section 3) and carrying out text classification experiments (Section 4). Finally, we present a tentative summary in Section 5. 2 Related Work To our knowledge, there is no previous work that has attempted what is described in this paper. There is some thematically related work, such as automatic filtering of pornographic content (Polpinij et al., 2006; Sood et al., 2012; Xiang et al., 2012; Su et al., 2004), but we believe the nature of the task is significantly different such that a different approach is required. However, text or document classification, the general technique employed in this paper, is a very common task (Manning et al., 2008). In text classification, given a document d, the task is to assign it a class, or label, c, from a fixed, human-defined set of possible classes C = {c1, c2, ... , cn}. In order to achieve this, a training set of labelled documents (d, c) is given to a learning algorithm to learn a classifier that maps documents to classes. Documents are</context>
</contexts>
<marker>Xiang, Fan, Wang, Hong, Rose, 2012</marker>
<rawString>Guang Xiang, Bin Fan, Ling Wang, Jason Hong, and Carolyn Rose. 2012. Detecting offensive tweets via topical feature discovery over a large scale twitter corpus. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12, pages 1980–1984, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>