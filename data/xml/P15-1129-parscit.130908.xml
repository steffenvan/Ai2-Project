<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001144">
<title confidence="0.992592">
Building a Semantic Parser Overnight
</title>
<author confidence="0.992249">
Yushi Wang*
</author>
<affiliation confidence="0.837339">
Stanford University
</affiliation>
<email confidence="0.979956">
yushiw@cs.stanford.edu
</email>
<author confidence="0.963197">
Jonathan Berant*
</author>
<affiliation confidence="0.831305">
Stanford University
</affiliation>
<email confidence="0.981251">
joberant@stanford.edu
</email>
<author confidence="0.994721">
Percy Liang
</author>
<affiliation confidence="0.981857">
Stanford University
</affiliation>
<email confidence="0.990285">
pliang@cs.stanford.edu
</email>
<sectionHeader confidence="0.993715" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999894105263158">
How do we build a semantic parser in a
new domain starting with zero training ex-
amples? We introduce a new methodol-
ogy for this setting: First, we use a simple
grammar to generate logical forms paired
with canonical utterances. The logical
forms are meant to cover the desired set
of compositional operators, and the canon-
ical utterances are meant to capture the
meaning of the logical forms (although
clumsily). We then use crowdsourcing to
paraphrase these canonical utterances into
natural utterances. The resulting data is
used to train the semantic parser. We fur-
ther study the role of compositionality in
the resulting paraphrases. Finally, we test
our methodology on seven domains and
show that we can build an adequate se-
mantic parser in just a few hours.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999262647058824">
By mapping natural language utterances to exe-
cutable logical forms, semantic parsers have been
useful for a variety of applications requiring pre-
cise language understanding (Zelle and Mooney,
1996; Zettlemoyer and Collins, 2005; Liang et
al., 2011; Berant et al., 2013; Kwiatkowski et al.,
2013; Artzi and Zettlemoyer, 2013; Kushman and
Barzilay, 2013). Previous work has focused on
how to train a semantic parser given input utter-
ances, but suppose we wanted to build a seman-
tic parser for a new domain—for example, a natu-
ral language interface into a publications database.
Since no such interface exists, we do not even have
a naturally occurring source of input utterances
that we can annotate. So where do we start?
In this paper, we advocate a functionality-
driven process for rapidly building a semantic
</bodyText>
<note confidence="0.732137">
∗ Both authors equally contributed to the paper.
</note>
<figure confidence="0.991776">
Domain
(1) by builder (-30 minutes)
(1) by training a paraphrasing model
Semantic parser
</figure>
<figureCaption confidence="0.6383758">
Figure 1: Functionality-driven process for build-
ing semantic parsers. The two red boxes are the
domain-specific parts provided by the builder of
the semantic parser, and the other two are gener-
ated by the framework.
</figureCaption>
<bodyText confidence="0.973824916666667">
parser in a new domain. At a high-level, we
seek to minimize the amount of work needed
for a new domain by factoring out the domain-
general aspects (done by our framework) from
the domain-specific ones (done by the builder
of the semantic parser). We assume that the
builder already has the desired functionality of
the semantic parser in mind—e.g., the publica-
tions database is set up and the schema is fixed.
Figure 1 depicts the functionality-driven process:
First, the builder writes a seed lexicon specifying
a canonical phrase (“publication date”) for
</bodyText>
<figure confidence="0.940871411764706">
Seed lexicon
article -+ TYPENP[article]
publication date -+ RELNP[publicationDate]
cites -+ VP/NP[cites]
...
(2) via domain-general grammar
Logical forms and canonical utterances
article that has the largest publication date
argmax(type.article, publicationDate)
person that is author of the most number of article
argmax(type.person, R(λx.count(type.article fl author.x)))
...
(3) via crowdsourcing (-5 hours)
Paraphrases
what is the newest published article?
who has published the most articles?
...
</figure>
<page confidence="0.949892">
1332
</page>
<note confidence="0.976070333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1332–1342,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999712166666667">
each predicate (publicationDate). Second,
our framework uses a domain-general grammar,
along with the seed lexicon and the database, to
automatically generate a few hundred canonical
utterances paired with their logical forms (e.g.,
“article that has the largest publication date” and
arg max(type.article, publicationDate)).
These utterances need not be the most elegant,
but they should retain the semantics of the logical
forms. Third, the builder leverages crowdsourcing
to paraphrase each canonical utterance into a
few natural utterances (e.g., “what is the newest
published article?”). Finally, our framework uses
this data to train a semantic parser.
Practical advantages. There are two main ad-
vantages of our approach: completeness and ease
of supervision. Traditionally, training data is
collected in a best-effort manner, which can re-
sult in an incomplete coverage of functionality.
For example, the WebQuestions dataset (Berant
et al., 2013) contains no questions with numeric
answers, so any semantic parser trained on that
dataset would lack that functionality. These bi-
ases are not codified, which results in an idiosyn-
cratic and mysterious user experience, a major
drawback of natural language interfaces (Rangel
et al., 2014). In contrast, our compact grammar
precisely specifies the logical functionality. We
enforce completeness by generating canonical ut-
terances that exercise every grammar rule.
In terms of supervision, state-of-the-art seman-
tic parsers are trained from question-answer pairs
(Kwiatkowski et al., 2013; Berant and Liang,
2014). Although this is a marked improvement in
cost and scalability compared to annotated logical
forms, it still requires non-trivial effort: the an-
notator must (i) understand the question and (ii)
figure out the answer, which becomes even harder
with compositional utterances. In contrast, our
main source of supervision is paraphrases, which
only requires (i), not (ii). Such data is thus cheaper
and faster to obtain.
Linguistic reflections. The centerpiece of our
framework is a domain-general grammar that con-
nects logical forms with canonical utterances.
This connection warrants further scrutiny, as the
structural mismatch between logic and language
is the chief source of difficulty in semantic pars-
ing (Liang et al., 2011; Kwiatkowski et al., 2013;
Berant and Liang, 2014).
There are two important questions here. First, is
it possible to design a simple grammar that simul-
taneously generates both logical forms and canon-
ical utterances so that the utterances are under-
standable by a human? In Section 3, we show how
to choose appropriate canonical utterances to max-
imize alignment with the logical forms.
Second, our grammar can generate an infinite
number of canonical utterances. How many do
we need for adequate coverage? Certainly, single
relations is insufficient: just knowing that “pub-
lication date of X” paraphrases to “when X was
published” would offer insufficient information to
generalize to “articles that came after X” mapping
to “article whose publication date is larger than
publication date of X”. We call this phenomena
sublexical compositionality—when a short lexical
unit (“came after”) maps onto a multi-predicate
logical form. Our hypothesis is that the sublexi-
cal compositional units are small, so we only need
to crowdsource a small number of canonical utter-
ances to learn about most of the language variabil-
ity in the given domain (Section 4).
We applied our functionality-driven process to
seven domains, which were chosen to explore par-
ticular types of phenomena, such as spatial lan-
guage, temporal language, and high-arity rela-
tions. This resulted in seven new semantic parsing
datasets, totaling 12.6K examples. Our approach,
which was not tuned on any one domain, was able
to obtain an average accuracy of 59% over all do-
mains. On the day of this paper submission, we
created an eighth domain and trained a semantic
parser overnight.
</bodyText>
<sectionHeader confidence="0.911993" genericHeader="introduction">
2 Approach Overview
</sectionHeader>
<bodyText confidence="0.999927666666667">
In our functionality-driven process (Figure 1),
there are two parties: the builder, who provides
domain-specific information, and the framework,
which provides domain-general information. We
assume that the builder has a fixed database w,
represented as a set of triples (e1, p, e2), where e1
and e2 are entities (e.g., article1, 2015) and p is
a property (e.g., publicationDate). The database
w can be queried using lambda DCS logical forms,
described further in Section 2.1.
The builder supplies a seed lexicon L, which
contains for each database property p (e.g.,
publicationDate) a lexical entry of the form
(t → s[p]), where t is a natural language phrase
(e.g., “publication date”) and s is a syntactic cat-
</bodyText>
<page confidence="0.976464">
1333
</page>
<bodyText confidence="0.999869604651163">
egory (e.g., RELNP). In addition, L contains
two typical entities for each semantic type in the
database (e.g., (alice —* NP[alice]) for the type
person). The purpose of L is to simply connect
each predicate with some representation in natural
language.
The framework supplies a grammar G, which
specifies the modes of composition, both on log-
ical forms and canonical utterances. Formally, G
is a set of rules of the form (α1 ... αn —* s[z]),
where α1 ... αn is a sequence of tokens or cate-
gories, s is a syntactic category and z is the log-
ical form constructed. For example, one rule in
G is (RELNP[r] of NP[x] —* NP[R(r).x]), which
constructs z by reversing the binary predicate
r and joining it with a the unary predicate x.
We use the rules G U L to generate a set of
(z, c) pairs, where z is a logical form (e.g.,
R(publicationDate).article1), and c is the
corresponding canonical utterance (e.g., “publica-
tion date of article 1”). The set of (z, c) is denoted
by GEN(G U L). See Section 3 for details.
Next, the builder (backed by crowdsourcing)
paraphrases each canonical utterance c output
above into a set of natural utterances P(c) (e.g.,
“when was article 1 published?”). This defines a
set of training examples D = {(x, c, z)}, for each
(z, c) E GEN(G U L) and x E P(c). The crowd-
sourcing setup is detailed in Section 5.
Finally, the framework trains a semantic parser
on D. Our semantic parser is a log-linear distribu-
tion pθ(z, c  |x, w) over logical forms and canon-
ical utterances specified by the grammar G. Note
that the grammar G will in general not parse x, so
the semantic parsing model will be based on para-
phrasing, in the spirit of Berant and Liang (2014).
To summarize, (1) the builder produces a seed
lexicon L; (2) the framework produces logical
forms and canonical utterances GEN(G U L) =
{(z, c)}; (3) the builder (via crowdsourcing) uses
P(�) to produce a dataset D = {(x, c, z)}; and (4)
the framework uses D to train a semantic parser
pθ(z,c  |x, w).
</bodyText>
<subsectionHeader confidence="0.973136">
2.1 Lambda DCS
</subsectionHeader>
<bodyText confidence="0.9999703125">
Our logical forms are represented in lambda DCS,
a logical language where composition operates on
sets rather than truth values. Here we give a brief
description; see Liang (2013) for details.
Every logical form z in this paper is either a
unary (denoting a set of entities) or a binary (de-
noting a set of entity-pairs). In the base case, each
entity e (e.g., 2015) is a unary denoting the single-
ton set: JeKw = {e}; and each property p (e.g.,
publicationDate) is a binary denoting all entity-
pairs (e1, e2) that satisfy the property p. Unaries
and binaries can be composed: Given a binary b
and unary u, the join b.u denotes all entities e1 for
which there exists an e2 E JuKw with (e1, e2) E
JbKw. For example, publicationDate.2015 de-
note entities published in 2015.
The intersection u1 n u2, union u1 U u2, com-
plement -,u denote the corresponding set op-
erations on the denotations. We let R(b) de-
note the reversal of b: (e1, e2) E JbKw iff
(e2, e1) E JR(b)Kw. This allows us to define
R(publicationDate).article1 as the publica-
tion date of article 1. We also include aggregation
operations (count(u), sum(u) and average(u, b)),
and superlatives (argmax(u, b)).
Finally, we can construct binaries using lambda
abstraction: Ax.u denotes a set of (e1, e2) where
e1 E Ju[x/e2]Kw and u[x/e2] is the logical form
where free occurrences of x are replaced with e2.
For example, R(Ax.count(R(cites).x)) denotes
the set of entities (e1, e2), where e2 is the number
of entities that e1 cites.
</bodyText>
<sectionHeader confidence="0.917262" genericHeader="method">
3 Generation and canonical
compositionality
</sectionHeader>
<bodyText confidence="0.991040454545455">
Our functionality-driven process hinges on having
a domain-general grammar that can connect logi-
cal forms with canonical utterances composition-
ally. The motivation is that while it is hard to write
a grammar that parses all utterances, it is possible
to write one that generates one canonical utterance
for each logical form. To make this explicit:
Assumption 1(Canonical compositionality)
Using a small grammar, all logical forms ex-
pressible in natural language can be realized
compositionally based on the logical form.
Grammar. We target database querying appli-
cations, where the parser needs to handle superla-
tives, comparatives, negation, and coordination.
We define a simple grammar that captures these
forms of compositionality using canonical utter-
ances in a domain-general way. Figure 2 illustrates
a derivation produced by the grammar.
The seed lexicon specified by the builder con-
tains canonical utterances for types, entities, and
properties. All types (e.g., person) have the syn-
tactic category TYPENP, and all entities (e.g.,
</bodyText>
<page confidence="0.974884">
1334
</page>
<figure confidence="0.701011">
NP[type.article fl publicationDate.1950]
</figure>
<figureCaption confidence="0.796353">
Figure 2: Deriving a logical form z (red) and a
canonical utterance c (green) from the grammar
G. Each node contains a syntactic category and
a logical form, which is generated by applying a
rule. Nodes with only leaves as children are pro-
duced using the seed lexicon; all other nodes are
produced by rules in the domain-general grammar.
</figureCaption>
<bodyText confidence="0.999784944444444">
alice) are ENTITYNP’s. Unary predicates are
realized as verb phrases VP (e.g., “has a private
bath”). The builder can choose to represent bina-
ries as either relational noun phrases (RELNP) or
generalized transitive verbs (VP/NP). RELNP’s
are usually used to describe functional proper-
ties (e.g., “publication date”), especially numer-
ical properties. VP/NP’s include transitive verbs
(“cites”) but also longer phrases with the same
syntactic interface (“is the president of”). Table
1 shows the seed lexicon for the SOCIAL domain.
From the seed lexicon, the domain-general
grammar (Table 2) constructs noun phrases (NP),
verbs phrases (VP), and complementizer phrase
(CP), all of which denote unary logical forms.
Broadly speaking, the rules (R1)–(R4), (C1)–(C4)
take a binary and a noun phrase, and compose
them (optionally via comparatives, counting, and
negation) to produce a complementizer phrase CP
representing a unary (e.g., “that cites article 1”
or “that cites more than three article”). (G3)
combines these CP’s with an NP (e.g., “article”).
In addition, (S0)–(S4) handle superlatives (we in-
clude argmin in addition to argmax), which take
an NP and return the extremum-attaining subset of
its denotation. Finally, we support transformations
such as join (T1) and disjunction (T4), as well as
aggregation (A1)–(A2).
Rendering utterances for multi-arity predicates
was a major challenge. The predicate in-
stances are typically reified in a graph database,
akin to a neo-Davidsonian treatment of events:
There is an abstract entity with binary predi-
cates relating it to its arguments. For exam-
ple, in the SOCIAL domain, Alice’s education
can be represented in the database as five triples:
</bodyText>
<construct confidence="0.904598285714286">
birthdate → RELNP[birthdate]
person|university|field → TYPENP[person |· · · ]
company|job title → TYPENP[company |· · · ]
student|university|field of study → RELNP[student |· · · ]
employee|employer|job title → RELNP[employee |· · · ]
start date|end date → RELNP[startDate |· · · ]
is friends with → VP/NP[friends |· · · ]
</construct>
<tableCaption confidence="0.973793">
Table 1: The seed lexicon for the SOCIAL do-
</tableCaption>
<bodyText confidence="0.995594902439024">
main, which specifies for each predicate (e.g.,
birthdate) a phrase (e.g., “birthdate”) that real-
izes that predicate and its syntactic category (e.g.,
RELNP).
(e17, student, alice), (e17, university, ucla),
(e17,fieldOfStudy,music),
(e17, startDate, 2005), (e17, endDate, 2009).
All five properties here are represented as
RELNP’s, with the first one designated as the sub-
ject (RELNPO). We support two ways of querying
multi-arity relations: “student whose university is
ucla” (T2) and “university of student Alice whose
start date is 2005” (T3).
Generating directly from the grammar in Ta-
ble 2 would result in many uninterpretable canon-
ical utterances. Thus, we perform type checking
on the logical forms to rule out “article that cites
2004”, and limit the amount of recursion, which
keeps the canonical utterances understandable.
Still, the utterances generated by our grammar
are not perfectly grammatical; we do not use de-
terminers and make all nouns singular. Nonethe-
less, AMT workers found most canonical utter-
ances understandable (see Table 3 and Section 5
for details on crowdsourcing). One tip for the
builder is to keep the RELNP’s and VP/NP’s as
context-independent as possible; e.g., using “pub-
lication date” instead of “date”. In cases where
more context is required, we use parenthetical re-
marks (e.g., “number of assists (over a season)”
→ RELNP[...]) to pack more context into the
confines of the part-of-speech.
Limitations. While our domain-general gram-
mar covers most of the common logical forms
in a database querying application, there are sev-
eral phenomena which are out of scope, notably
nested quantification (e.g., “show me each au-
thor’s most cited work”) and anaphora (e.g., “au-
thor who cites herself at least twice”). Handling
these would require a more radical change to the
grammar, but is still within scope.
</bodyText>
<figure confidence="0.984727285714286">
NP[type.article]
TYPENP[article]
article
whose RELNP[publicationDate]
publication date
is ENTITYNP[1950]
1950
</figure>
<page confidence="0.807738">
1335
</page>
<table confidence="0.989186096774194">
[glue] — NP[x]
ENTITYNP[x] — NP[type.x]
TYPENP[x] — NP[x n f n g]
NP[x] CP[f] (and CP[g])*
[simple]
(R0) that VP[x] — CP[x]
whose RELNP[r] CMP[c] NP[y] — CP[r.c.y]
is|is not|is smaller than|is larger than|is at least|is at most — CMP[=  |=�  |&lt;  |&gt;  |:5  |?]
that (not)? VP/NP[r] NP[y] — CP[(-)r.y]
that is (not)? RELNP[r] of NP[y] — CP[(-)R(r).y]
that NP[y] (not)? VP/NP[r] — CP[(-)(R(r).y)]
[counting]
that has CNT[c] RELNP[r] — CP[R(ax.count(R(r).x)).c]
that VP/NP[r] CNT[c] NP[y] — CP[R(ax.count(y n R(r).x)).c]
that is RELNP[r] of CNT[c] NP[y] — CP[R(ax.count(y n r.x)).c]
that CNT[c] NP[y] VP/NP[r] — CP[R(ax.count(y n r.x)).c]
(less than|more than) NUM[n] — CNT[(&lt; . |&gt; .)n]
[superlatives]
(S0) NP[x] that has the largest RELNP[r] — NP[arg max(x, r)]
NP[x] that has the most number of RELNP[r] — NP[arg max(x, R(ay.count(R(r).y)))]
NP[x] that VP/NP[r] the most number of NP[y] — NP[arg max(x, R(ay.count(R(r).y)))]
NP[x] that is RELNP[r] of the most number of NP[y] — NP[arg max(x, R(az.count(y n r.z)))]
NP[x] that the most number of NP[y] VP/NP[r] — NP[arg max(x, R(az.count(y n r.z)))]
[transformation]
RELNP[r] of NP[y] — NP[R(r).y]
RELNP0[h]CP[f] (and CP[g])* — NP[R(h).(fng)]
RELNP[r] of RELNPo[h] NP[x] CP[f] (and CP[g])* — NP[R(r).(h.x n f n g)]
NP[x] or NP[y] — NP[x u y]
[aggregation]
number ofNP[x] — NP[count(x)]
total|average RELNP[r] of NP[x] — NP[sum|average(x, r)]
</table>
<tableCaption confidence="0.9816005">
Table 2: The domain-general grammar which is combined with the seed lexicon to generate logical forms
and canonical utterances that cover the supported logical functionality.
</tableCaption>
<sectionHeader confidence="0.6189995" genericHeader="method">
4 Paraphrasing and bounded
non-compositionality
</sectionHeader>
<bodyText confidence="0.999250276595745">
While the canonical utterance c is generated
compositionally along with the logical form z,
natural paraphrases x E P(c) generally devi-
ate from this compositional structure. For ex-
ample, the canonical utterance “NP[number of
NP[article CP[whose publication date is larger
than NP[publication date of article 1]]]]” might
get paraphrased to “How many articles were pub-
lished after article 1?”. Here, “published after”
non-compositionally straddles the inner NP, intu-
itively responsible for both instances of “publica-
tion date”. But how non-compositional can para-
phrases be? Our framework rests on the assump-
tion that the answer is “not very”:
Assumption 2 (Bounded non-compositionality)
Natural utterances for expressing complex logical
forms are compositional with respect to fragments
of bounded size.
In the above example, note that while “published
after” is non-compositional with respect to the
grammar, the rewriting of “number of” to “how
many” is compositional. The upshot of Assump-
tion 2 is that we only need to ask for paraphrases
of canonical utterances generated by the grammar
up to some small depth to learn about all the non-
compositional uses of language, and still be able
generalize (compositionally) beyond that.
We now explore the nature of the possible para-
phrases. Broadly speaking, most paraphrases in-
volve some sort of compression, where the clunky
but faithful canonical utterance is smoothed out
into graceful prose.
Alternations of single rules. The most basic
paraphrase happens at the single phrase level with
synonyms (“block” to “brick”), which preserve
the part-of-speech. However, many of our prop-
erties are specified using relational noun phrases,
which are more naturally realized using preposi-
tions (“meeting whose attendee is alice ==&gt;. meet-
ing with alice”) or verbs (“author of article 1 ==&gt;.
who wrote article 1”). If the RELNP is com-
plex, then the argument can become embedded:
“player whose number of points is 15 ==&gt;. player
who scored 15 points”. Superlative and compara-
tive constructions reveal other RELNP-dependent
words: “article that has the largest publication
date ==&gt;. newest article”. When the value of the
</bodyText>
<page confidence="0.966267">
1336
</page>
<bodyText confidence="0.999015176470588">
relation has enough context, then the relation is
elided completely: “housing unit whose housing
type is apartment ==&gt;- apartment”.
Multi-arity predicates are compressed into a
single frame: “university of student alice whose
field of study is music” becomes “At which uni-
versity did Alice study music?”, where the se-
mantic roles of the verb “study” carry the bur-
den of expressing the multiple relations: student,
university, and fieldOfStudy. With a different
combination of arguments, the natural verb would
change: “Which university did Alice attend?”
Sublexical compositionality. The most interest-
ing paraphrases occur across multiple rules, a phe-
nomenon which we called sublexical composition-
ality. The idea is that common, multi-part con-
cepts are compressed to single words or simpler
constructions. The simplest compression is a lex-
ical one: “parent of alice whose gender is female
==&gt;- mother of alice”. Compression often occurs
when we have the same predicate chained twice
in a join: “person that is author of paper whose
author is X ==&gt;- co-author of X” or “person whose
birthdate is birthdate of X ==&gt;- person born on the
same day as X”. When two CP’s combined via
coordination have some similarity, then the co-
ordination can be pushed down (“meeting whose
start time is 3pm and whose end time is 5pm ==&gt;-
meetings between 3pm and 5pm”) and sometimes
even generalized (“that allows cats and that al-
lows dogs ==&gt;- that allows pets”). Sometimes, com-
pression happens due to metonymy, where people
stand in for their papers: “author of article that ar-
ticle whose author is X cites ==&gt;- who does X cite”.
</bodyText>
<sectionHeader confidence="0.990593" genericHeader="method">
5 Crowdsourcing
</sectionHeader>
<bodyText confidence="0.999963222222222">
We tackled seven domains covering various lin-
guistic phenomena. Table 3 lists the domains, their
principal phenomena, statistics about their predi-
cates and dataset, and an example from the dataset.
We use Amazon Mechanical Turk (AMT) to
paraphrase the canonical utterances generated by
the domain-general grammar. In each AMT task,
a worker is presented with four canonical utter-
ances and is asked to reformulate them in natu-
ral language or state that they are incomprehensi-
ble. Each canonical utterance was presented to 10
workers. Over all domains, we collected 18,032
responses. The average time for paraphrasing one
utterance was 28 seconds. Paraphrases that share
the same canonical utterance are collapsed, while
identical paraphrases that have distinct canonical
utterances are deleted. This produced a total of
12,602 examples over all domains.
To estimate the level of noise in the data, we
manually judged the correctness of 20 examples in
each domain, and found that 17% of the utterances
were inaccurate. There are two main reasons: lex-
ical ambiguity on our part (“player that has the
least number of team ==&gt;- player with the lowest
jersey number”), and failure on the worker’s part
(“restaurant whose star rating is 3 stars ==&gt;- hotel
which has a 3 star rating”).
</bodyText>
<sectionHeader confidence="0.958564" genericHeader="method">
6 Model and Learning
</sectionHeader>
<bodyText confidence="0.98694">
Our semantic parsing model defines a distribu-
tion over logical forms given by the domain-
general grammar G and additional rules trig-
gered by the input utterance x. Specifically,
given an utterance x, we detect numbers, dates,
and perform string matching with database en-
tities to recognize named entities. This results
in a set of rules T(x). For example, if x
is “article published in 2015 that cites article
1”, then T(x) contains (2015 —* NP[2015]) and
(article 1 —* NP[article1]). Let Lx be the rules
in the seed lexicon L where the entity rules (e.g.,
(alice —* NP[alice])) are replaced by T(x).
Our semantic parsing model defines a log-
linear distribution over candidate pairs (z, c) E
GEN(G U Lx):
pθ(z, c  |x, w) a exp(O(c, z, x, w)T0), (1)
where O(z, c, x, w) E Rd is a feature vector and
0 E Rd is a parameter vector.
To generate candidate logical forms, we use a
simple beam search: For each search state, which
includes the syntactic category s (e.g., NP) and
the depth of the logical form, we generate at most
K = 20 candidates by applying the rules in Ta-
ble 2. In practice, the lexical rules T(x) are ap-
plied first, and composition is performed, but not
constrained to the utterance. For example, the ut-
terance “article” would generate the logical form
count(type.article). Instead, soft paraphras-
ing features are used to guide the search. This
rather unorthodox approach to semantic parsing
can be seen as a generalization of Berant and
Liang (2014) and is explained in more detail in
Pasupat and Liang (2015).
Training. We train our model by maximiz-
ing the regularized log-likelihood O(0) =
</bodyText>
<page confidence="0.983652">
1337
</page>
<table confidence="0.999864454545455">
Domain # pred. # ex. Phenomena Example
CALENDAR 22 837 temporal language x: “Show me meetings after the weekly standup day”
c: “meeting whose date is at least date of weekly standup”
z: type.meeting fl date. &gt; R(date).weeklyStandup
BLOCKS 19 1995 spatial language x: “Select the brick that is to the furthest left.”
c: “block that the most number of block is right of”
z: argmax(type.block, R(Ax.count(R(right).x)))
HOUSING 24 941 measurement units x: “Housing that is 800 square feet or bigger?”
c: “housing unit whose size is at least 800 square feet”
z: type.housingUnit fl area. &gt; .800
RESTAURANTS 32 1657 long unary relations x: “What restaurant can you eat lunch outside at?”
c: “restaurant that has outdoor seating and that serves lunch”
z: type.restaurant fl hasOutdoorSeating fl serveslunch
PUBLICATIONS 15 801 sublexical compositionality x: “Who has co-authored articles with Efron?”
c: “person that is author of article whose author is efron”
z: type.person fl R(author).(type.article fl author.efron)
SOCIAL 45 4419 multi-arity relations x: “When did alice start attending brown university?”
c: “start date of student alice whose university is brown university”
z: R(date).(student.Alice fl university.Brown)
BASKETBALL 24 1952 parentheticals x: “How many fouls were played by Kobe Bryant in 2004?”
c: “number offouls (over a season) ofplayer kobe bryant whose season is 2004”
z: count(R(fouls).(player.KobeBryant fl season.2004)
</table>
<tableCaption confidence="0.999123">
Table 3: We experimented on seven domains, covering a variety of phenomena. For each domain, we
show the number of predicates, number of examples, and a (c, z) generated by our framework along with
a paraphrased utterance x.
</tableCaption>
<table confidence="0.997525214285714">
Category Description
Basic #words and bigram matches in (x, c)
#words and bigram PPDB matches in (x, c)
#unmatched words in x
#unmatched words in c
size of denotation of z, (|Qzjw|)
pos(x0:0) conjoined with type(Qzjw)
#nodes in tree generating z
Lexical ∀(i, j) ∈ A. (xi:i, cj:j)
∀(i, j) ∈ A. (xi:i, cj:j+1)
∀(i, j) ∈ A. (xi:i, cj−1:j)
∀(i, j), (i + 1, j + 1) ∈ A. (xi:i+1, cj:j+1)
all unaligned words in x and c
(xi:j, ci,:j,) if in phrase table
</table>
<tableCaption confidence="0.803806">
Table 4: Features for the paraphrasing model.
</tableCaption>
<bodyText confidence="0.999439566666667">
pos(xi:i) is the POS tag; type(Qz�w) is a coarse se-
mantic type for the denotation (an entity or a num-
ber). A is a maximum weight alignment between
x and c.
E(x,c,z)∈D log pe(z, c  |x, w) − A11θ111. To opti-
mize, we use AdaGrad (Duchi et al., 2010).
Features Table 4 describes the features. Our
basic features mainly match words and bigrams
in x and c, if they share a lemma or are aligned
in the PPDB resource (Ganitkevitch et al., 2013).
We count the number of exact matches, PPDB
matches, and unmatched words.
To obtain lexical features, we run the Berkeley
Aligner (Liang et al., 2006) on the training set and
compute conditional probabilities of aligning one
word type to another. Based on these probabilities
we compute a maximum weight alignment A be-
tween words in x and c. We define features over A
(see Table 4). We also use the word alignments to
construct a phrase table by applying the consistent
phrase pair heuristic (Och and Ney, 2004). We de-
fine an indicator feature for every phrase pair of
x and c that appear in the phrase table. Examples
from the PUBLICATIONS domain include fewest–
least number and by–whose author is. Note that
we do not build a hard lexicon but only use A
and the phrase table to define features, allowing
the model to learn useful paraphrases during train-
ing. Finally, we define standard features on logical
forms and denotations (Berant et al., 2013).
</bodyText>
<sectionHeader confidence="0.990951" genericHeader="method">
7 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999996583333334">
We evaluated our functionality-driven process on
the seven domains described in Section 5 and one
new domain we describe in Section 7.3. For each
domain, we held out a random 20% of the exam-
ples as the test set, and performed development on
the remaining 80%, further splitting it to a train-
ing and development set (80%/20%). We created a
database for each domain by randomly generating
facts using entities and properties in the domain
(with type-checking). We evaluated using accu-
racy, which is the fraction of examples that yield
the correct denotation.
</bodyText>
<subsectionHeader confidence="0.989112">
7.1 Domain-specific linguistic variability
</subsectionHeader>
<bodyText confidence="0.9986245">
Our functionality-driven process is predicated on
the fact that each domain exhibits domain-specific
</bodyText>
<page confidence="0.968384">
1338
</page>
<table confidence="0.9992134">
Method CALENDAR BLOCKS HOUSING RESTAURANTS PUBLICATIONS RECIPES SOCIAL BASKETBALL Avg.
FULL 74.4 41.9 54.0 75.9 59.0 70.8 48.2 46.3 58.8
NOLEX 25.0 35.3 51.9 64.6 50.6 32.3 15.3 19.4 36.8
NOPPDB 73.2 41.4 54.5 73.8 56.5 68.1 43.6 44.5 56.9
BASELINE 17.3 27.7 45.9 61.3 46.7 26.3 9.7 15.6 31.3
</table>
<tableCaption confidence="0.999763">
Table 5: Test set results on all domains and baselines.
</tableCaption>
<bodyText confidence="0.99975303125">
phenomena. To corroborate this, we compare our
full system to NOLEX, a baseline that omits all
lexical features (Table 4), but uses PPDB as a
domain-general paraphrasing component. We per-
form the complementary experiment and compare
to NOPPDB, a baseline that omits PPDB match
features. We also run BASELINE, where we omit
both lexical and PPDB features.
Table 5 presents the results of this experiment.
Overall, our framework obtains an average accu-
racy of 59% across all eight domains. The per-
formance of NOLEX is dramatically lower than
FULL, indicating that it is important to learn
domain-specific paraphrases using lexical fea-
tures. The accuracy of NOPPDB is only slightly
lower than FULL, showing that most of the re-
quired paraphrases can be learned during training.
As expected, removing both lexical and PPDB fea-
tures results in poor performance (BASELINE).
Analysis. We performed error analysis on 10 er-
rors in each domain. Almost 70% of the errors
are due to problems in the paraphrasing model,
where the canonical utterance has extra material,
is missing some content, or results in an incorrect
paraphrase. For example, “restaurants that have
waiters and you can sit outside” is paraphrased to
“restaurant that has waiter service and that takes
reservations”. Another 12.5% result from reorder-
ing issues, e.g, we paraphrase “What venue has
fewer than two articles” to “article that has less
than two venue”. Inaccurate paraphrases provided
by AMT workers account for the rest of the errors.
</bodyText>
<subsectionHeader confidence="0.998967">
7.2 Bounded non-compositionality
</subsectionHeader>
<bodyText confidence="0.999862454545455">
We hypothesized that we need to obtain para-
phrases of canonical utterances corresponding to
logical forms of only small depth. We ran the
following experiment in the CALENDAR domain
to test this claim. First, we define by NP0, NP1,
and NP2 the set of utterances generated by an NP
that has exactly zero, one, and two NPs embed-
ded in it. We define the training scenario 0 -* 1,
where we train on examples from NP0 and test
on examples from NP1; 0 U 1 -* 1, 0 U 1 -* 2,
and 0 U 1 U 2 -* 2 are defined analogously. Our
</bodyText>
<table confidence="0.991775666666667">
Scenario Acc. Scenario Acc.
0 - *1 22.9 0 U 1 - *2 28.1
0 U 1 - *1 85.8 0 U 1 U 2 - *2 47.5
</table>
<tableCaption confidence="0.954917">
Table 6: Test set results in the CALENDAR domain
on bounded non-compositionality.
</tableCaption>
<bodyText confidence="0.995001125">
hypothesis is that generalization on 0 U 1 -* 2
should be better than for 0 -* 1, since NP1 ut-
terances have non-compositional paraphrases, but
training on NP0 does not expose them.
The results in Table 6 verify this hypothesis.
The accuracy of 0 -* 1 is almost 65% lower than
0 U 1 -* 1. On the other hand, the accuracy of
0 U 1 -* 2 is only 19% lower than 0 U 1 U 2 -* 2.
</bodyText>
<subsectionHeader confidence="0.995742">
7.3 An overnight experiment
</subsectionHeader>
<bodyText confidence="0.999996">
To verify the title of this paper, we attempted
to create a semantic parser for a new domain
(RECIPES) exactly 24 hours before the submission
deadline. Starting at midnight, we created a seed
lexicon in less than 30 minutes. Then we gener-
ated canonical utterances and allowed AMT work-
ers to provide paraphrases overnight. In the morn-
ing, we trained our parser and obtained an accu-
racy of 70.8% on the test set.
</bodyText>
<subsectionHeader confidence="0.998565">
7.4 Testing on independent data
</subsectionHeader>
<bodyText confidence="0.996413117647059">
Geo880. To test how our parser generalizes to
utterances independent of our framework, we cre-
ated a semantic parser for the domain of US ge-
ography, and tested on the standard 280 test ex-
amples from GEO880 (Zelle and Mooney, 1996).
We did not use the standard 600 training examples.
Our parser obtained 56.4% accuracy, which is sub-
stantially lower than state-of-the-art (- 90%).
We performed error analysis on 100 random
sentences from the development set where accu-
racy was 60%. We found that the parser learns
from the training data to prefer shorter para-
phrases, which accounts for 30% of the errors. In
most of these cases, the correct logical form is
ranked at the top-3 results (accuracy for the top-
3 derivations is 73%). GEO880 contains highly
compositional utterances, and in 25% of the errors
</bodyText>
<page confidence="0.985318">
1339
</page>
<bodyText confidence="0.999548">
the correct derivation tree exceeds the maximum
depth used for our parser. Another 17.5% of the
errors are caused by problems in the paraphrasing
model. For example, in the utterance “what is the
size of california”, the model learns that “size”
corresponds to “population” rather than “area”.
Errors related to reordering and the syntactic struc-
ture of the input utterance account for 7.5% of the
errors. For example, the utterance “what is the
area of the largest state” is paraphrased to “state
that has the largest area”.
Calendar. In Section 7.1, we evaluated on ut-
terances obtained by paraphrasing canonical utter-
ances from the grammar. To examine the cover-
age of our parser on independently-produced ut-
terances, we asked AMT workers to freely come
up with queries. We collected 186 such queries; 5
were spam and discarded. We replaced all entities
(people, dates, etc.) with entities from our seed
lexicon to avoid focusing on entity detection.
We were able to annotate 52% of the utterances
with logical forms from our grammar. We could
not annotate 20% of the utterances due to relative
time references, such as “What time is my next
meeting?”. 14% of the utterances were not cov-
ered due to binary predicates not in the grammar
(“What is the agenda of the meeting?”) or missing
entities (“When is Dan’s birthday?”). Another 2%
required unsupported calculations (“How much
free time do I have tomorrow?”), and the rest are
out of scope for other reasons (“When does my
Verizon data plan start over?”).
We evaluated our trained semantic parser on the
95 utterances annotated with logical forms. Our
parser obtained an accuracy of 46.3% and oracle
accuracy of 84.2%, which measures how often the
correct denotation is on the final beam. The large
gap shows that there is considerable room for im-
provement in the paraphrasing model.
</bodyText>
<sectionHeader confidence="0.999247" genericHeader="conclusions">
8 Related work and discussion
</sectionHeader>
<bodyText confidence="0.999946803571429">
Much of current excitement around semantic pars-
ing emphasizes large knowledge bases such as
Freebase (Cai and Yates, 2013; Kwiatkowski et
al., 2013; Berant et al., 2013). However, despite
the apparent scale, the actual question answering
datasets (Free917 and WebQuestions) are limited
in compositionality. Moreover, specialized do-
mains with specialized jargon will always exist,
e.g., in regular expressions (Kushman and Barzi-
lay, 2013) or grounding to perception (Matuszek
et al., 2012; Tellex et al., 2011; Krishnamurthy
and Kollar, 2013). Therefore, we believe build-
ing a targeted domain-specific semantic parser for
a new website or device is a very practical goal.
Recent work has made significant strides in
reducing supervision from logical forms (Zettle-
moyer and Collins, 2005; Wong and Mooney,
2007) to denotations (Clarke et al., 2010; Liang et
al., 2011) and to weaker forms (Artzi and Zettle-
moyer, 2011; Reddy et al., 2014). All of these
works presuppose having input utterances, which
do not exist in a new domain. Our methodol-
ogy overcomes this hurdle by exploiting a very
lightweight form of annotation: paraphrasing.
Paraphrasing has been applied to single-
property question answering (Fader et al., 2013)
and semantic parsing (Berant and Liang, 2014).
We not only use paraphrasing in the semantic
parser, but also for data collection.
Table 2 might evoke rule-based systems (Woods
et al., 1972; Warren and Pereira, 1982) or con-
trolled natural languages (Schwitter, 2010). How-
ever, there is an important distinction: the gram-
mar need only connect a logical form to one
canonical utterance; it is not used directly for pars-
ing. This relaxation allows the grammar to be
much simpler. Our philosophy is to use the simple
domain-general grammar to carry the torch just to
the point of being understandable by a human, and
let the human perform the remaining correction to
produce a natural utterance.
In summary, our contributions are two-fold: a
new functionality-driven process and an explo-
ration of some of its linguistic implications. We
believe that our methodology is a promising way
to build semantic parsers, and in future work, we
would like to extend it to handle anaphora and
nested quantification.
Acknowledgments. We gratefully acknowledge
the support of the Google Natural Language Un-
derstanding Focused Program and the DARPA
Deep Exploration and Filtering of Text (DEFT)
Program contract no. FA8750-13-2-0040.
Reproducibility. All code,1 data, and
experiments for this paper are avail-
able on the CodaLab platform at
</bodyText>
<footnote confidence="0.79497675">
https://www.codalab.org/worksheets/
0x269ef752f8c344a28383240f7bb2be9c/.
1Our system uses the SEMPRE toolkit (http://nlp.
stanford.edu/software/sempre).
</footnote>
<page confidence="0.99303">
1340
</page>
<sectionHeader confidence="0.99573" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999673173076923">
Y. Artzi and L. Zettlemoyer. 2011. Bootstrap-
ping semantic parsers from conversations. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 421–432.
Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instruc-
tions to actions. Transactions of the Association for
Computational Linguistics (TACL), 1:49–62.
J. Berant and P. Liang. 2014. Semantic parsing via
paraphrasing. In Association for Computational
Linguistics (ACL).
J. Berant, A. Chou, R. Frostig, and P. Liang. 2013.
Semantic parsing on Freebase from question-answer
pairs. In Empirical Methods in Natural Language
Processing (EMNLP).
Q. Cai and A. Yates. 2013. Large-scale semantic pars-
ing via schema matching and lexicon extension. In
Association for Computational Linguistics (ACL).
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL), pages 18–27.
J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In Conference on Learning Theory
(COLT).
A. Fader, L. Zettlemoyer, and O. Etzioni. 2013.
Paraphrase-driven learning for open question an-
swering. In Association for Computational Linguis-
tics (ACL).
J. Ganitkevitch, B. V. Durme, and C. Callison-Burch.
2013. PPDB: The paraphrase database. In Human
Language Technology and North American Associ-
ation for Computational Linguistics (HLT/NAACL),
pages 758–764.
J. Krishnamurthy and T. Kollar. 2013. Jointly learning
to parse and perceive: Connecting natural language
to the physical world. Transactions of the Associa-
tion for Computational Linguistics (TACL), 1:193–
206.
N. Kushman and R. Barzilay. 2013. Using semantic
unification to generate regular expressions from nat-
ural language. In Human Language Technology and
North American Association for Computational Lin-
guistics (HLT/NAACL), pages 826–836.
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Empirical Methods in Natural
Language Processing (EMNLP).
P. Liang, B. Taskar, and D. Klein. 2006. Align-
ment by agreement. In North American Association
for Computational Linguistics (NAACL), pages 104–
111.
P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL),
pages 590–599.
P. Liang. 2013. Lambda dependency-based composi-
tional semantics. arXiv.
C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo,
and D. Fox. 2012. A joint model of language and
perception for grounded attribute learning. In Inter-
national Conference on Machine Learning (ICML),
pages 1671–1678.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30:417–449.
P. Pasupat and P. Liang. 2015. Compositional semantic
parsing on semi-structured tables. In Association for
Computational Linguistics (ACL).
R. A. P. Rangel, M. A. Aguirre, J. J. Gonzlez, and J. M.
Carpio. 2014. Features and pitfalls that users should
seek in natural language interfaces to databases. In
Recent Advances on Hybrid Approaches for Design-
ing Intelligent Systems, pages 617–630.
S. Reddy, M. Lapata, and M. Steedman. 2014. Large-
scale semantic parsing without question-answer
pairs. Transactions of the Association for Compu-
tational Linguistics (TACL), 2(10):377–392.
R. Schwitter. 2010. Controlled natural languages for
knowledge representation. In International Con-
ference on Computational Linguistics (COLING),
pages 1113–1121.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G.
Banerjee, S. J. Teller, and N. Roy. 2011. Un-
derstanding natural language commands for robotic
navigation and mobile manipulation. In Associa-
tion for the Advancement of Artificial Intelligence
(AAAI).
D. Warren and F. Pereira. 1982. An efficient easily
adaptable system for interpreting natural language
queries. Computational Linguistics, 8:110–122.
Y. W. Wong and R. J. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Association for Computational
Linguistics (ACL), pages 960–967.
W. A. Woods, R. M. Kaplan, and B. N. Webber. 1972.
The lunar sciences natural language information sys-
tem: Final report. Technical report, BBN Report
2378, Bolt Beranek and Newman Inc.
M. Zelle and R. J. Mooney. 1996. Learning to
parse database queries using inductive logic pro-
gramming. In Association for the Advancement of
Artificial Intelligence (AAAI), pages 1050–1055.
</reference>
<page confidence="0.820082">
1341
</page>
<reference confidence="0.9876116">
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Un-
certainty in Artificial Intelligence (UAI), pages 658–
666.
</reference>
<page confidence="0.994417">
1342
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902503">
<title confidence="0.994868">Building a Semantic Parser Overnight</title>
<affiliation confidence="0.998366">Stanford University</affiliation>
<email confidence="0.997908">yushiw@cs.stanford.edu</email>
<affiliation confidence="0.999728">Stanford University</affiliation>
<email confidence="0.998822">joberant@stanford.edu</email>
<author confidence="0.936192">Percy</author>
<affiliation confidence="0.999936">Stanford University</affiliation>
<email confidence="0.998787">pliang@cs.stanford.edu</email>
<abstract confidence="0.9987584">How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Bootstrapping semantic parsers from conversations.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>421--432</pages>
<contexts>
<context position="36720" citStr="Artzi and Zettlemoyer, 2011" startWordPosition="5967" endWordPosition="5971"> compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an impor</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2011</marker>
<rawString>Y. Artzi and L. Zettlemoyer. 2011. Bootstrapping semantic parsers from conversations. In Empirical Methods in Natural Language Processing (EMNLP), pages 421–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<pages>1--49</pages>
<contexts>
<context position="1317" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="196" endWordPosition="199">nces into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours. 1 Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to the paper. Domain (1) by builder (-30 minutes) (1) by training a paraphrasi</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics (TACL), 1:49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>P Liang</author>
</authors>
<title>Semantic parsing via paraphrasing.</title>
<date>2014</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5050" citStr="Berant and Liang, 2014" startWordPosition="755" endWordPosition="758"> al., 2013) contains no questions with numeric answers, so any semantic parser trained on that dataset would lack that functionality. These biases are not codified, which results in an idiosyncratic and mysterious user experience, a major drawback of natural language interfaces (Rangel et al., 2014). In contrast, our compact grammar precisely specifies the logical functionality. We enforce completeness by generating canonical utterances that exercise every grammar rule. In terms of supervision, state-of-the-art semantic parsers are trained from question-answer pairs (Kwiatkowski et al., 2013; Berant and Liang, 2014). Although this is a marked improvement in cost and scalability compared to annotated logical forms, it still requires non-trivial effort: the annotator must (i) understand the question and (ii) figure out the answer, which becomes even harder with compositional utterances. In contrast, our main source of supervision is paraphrases, which only requires (i), not (ii). Such data is thus cheaper and faster to obtain. Linguistic reflections. The centerpiece of our framework is a domain-general grammar that connects logical forms with canonical utterances. This connection warrants further scrutiny,</context>
<context position="9836" citStr="Berant and Liang (2014)" startWordPosition="1555" endWordPosition="1558">raphrases each canonical utterance c output above into a set of natural utterances P(c) (e.g., “when was article 1 published?”). This defines a set of training examples D = {(x, c, z)}, for each (z, c) E GEN(G U L) and x E P(c). The crowdsourcing setup is detailed in Section 5. Finally, the framework trains a semantic parser on D. Our semantic parser is a log-linear distribution pθ(z, c |x, w) over logical forms and canonical utterances specified by the grammar G. Note that the grammar G will in general not parse x, so the semantic parsing model will be based on paraphrasing, in the spirit of Berant and Liang (2014). To summarize, (1) the builder produces a seed lexicon L; (2) the framework produces logical forms and canonical utterances GEN(G U L) = {(z, c)}; (3) the builder (via crowdsourcing) uses P(�) to produce a dataset D = {(x, c, z)}; and (4) the framework uses D to train a semantic parser pθ(z,c |x, w). 2.1 Lambda DCS Our logical forms are represented in lambda DCS, a logical language where composition operates on sets rather than truth values. Here we give a brief description; see Liang (2013) for details. Every logical form z in this paper is either a unary (denoting a set of entities) or a bi</context>
<context position="25280" citStr="Berant and Liang (2014)" startWordPosition="4031" endWordPosition="4034">andidate logical forms, we use a simple beam search: For each search state, which includes the syntactic category s (e.g., NP) and the depth of the logical form, we generate at most K = 20 candidates by applying the rules in Table 2. In practice, the lexical rules T(x) are applied first, and composition is performed, but not constrained to the utterance. For example, the utterance “article” would generate the logical form count(type.article). Instead, soft paraphrasing features are used to guide the search. This rather unorthodox approach to semantic parsing can be seen as a generalization of Berant and Liang (2014) and is explained in more detail in Pasupat and Liang (2015). Training. We train our model by maximizing the regularized log-likelihood O(0) = 1337 Domain # pred. # ex. Phenomena Example CALENDAR 22 837 temporal language x: “Show me meetings after the weekly standup day” c: “meeting whose date is at least date of weekly standup” z: type.meeting fl date. &gt; R(date).weeklyStandup BLOCKS 19 1995 spatial language x: “Select the brick that is to the furthest left.” c: “block that the most number of block is right of” z: argmax(type.block, R(Ax.count(R(right).x))) HOUSING 24 941 measurement units x: </context>
<context position="37072" citStr="Berant and Liang, 2014" startWordPosition="6023" endWordPosition="6026"> is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaini</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>J. Berant and P. Liang. 2014. Semantic parsing via paraphrasing. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>A Chou</author>
<author>R Frostig</author>
<author>P Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1262" citStr="Berant et al., 2013" startWordPosition="188" endWordPosition="191">wdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours. 1 Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to the paper. Domain (1</context>
<context position="4438" citStr="Berant et al., 2013" startWordPosition="666" endWordPosition="669">tterances need not be the most elegant, but they should retain the semantics of the logical forms. Third, the builder leverages crowdsourcing to paraphrase each canonical utterance into a few natural utterances (e.g., “what is the newest published article?”). Finally, our framework uses this data to train a semantic parser. Practical advantages. There are two main advantages of our approach: completeness and ease of supervision. Traditionally, training data is collected in a best-effort manner, which can result in an incomplete coverage of functionality. For example, the WebQuestions dataset (Berant et al., 2013) contains no questions with numeric answers, so any semantic parser trained on that dataset would lack that functionality. These biases are not codified, which results in an idiosyncratic and mysterious user experience, a major drawback of natural language interfaces (Rangel et al., 2014). In contrast, our compact grammar precisely specifies the logical functionality. We enforce completeness by generating canonical utterances that exercise every grammar rule. In terms of supervision, state-of-the-art semantic parsers are trained from question-answer pairs (Kwiatkowski et al., 2013; Berant and </context>
<context position="28977" citStr="Berant et al., 2013" startWordPosition="4652" endWordPosition="4655">x and c. We define features over A (see Table 4). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004). We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the PUBLICATIONS domain include fewest– least number and by–whose author is. Note that we do not build a hard lexicon but only use A and the phrase table to define features, allowing the model to learn useful paraphrases during training. Finally, we define standard features on logical forms and denotations (Berant et al., 2013). 7 Experimental Evaluation We evaluated our functionality-driven process on the seven domains described in Section 5 and one new domain we describe in Section 7.3. For each domain, we held out a random 20% of the examples as the test set, and performed development on the remaining 80%, further splitting it to a training and development set (80%/20%). We created a database for each domain by randomly generating facts using entities and properties in the domain (with type-checking). We evaluated using accuracy, which is the fraction of examples that yield the correct denotation. 7.1 Domain-spec</context>
<context position="35974" citStr="Berant et al., 2013" startWordPosition="5855" endWordPosition="5858"> the rest are out of scope for other reasons (“When does my Verizon data plan start over?”). We evaluated our trained semantic parser on the 95 utterances annotated with logical forms. Our parser obtained an accuracy of 46.3% and oracle accuracy of 84.2%, which measures how often the correct denotation is on the final beam. The large gap shows that there is considerable room for improvement in the paraphrasing model. 8 Related work and discussion Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013). However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer a</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Cai</author>
<author>A Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="35926" citStr="Cai and Yates, 2013" startWordPosition="5847" endWordPosition="5850">(“How much free time do I have tomorrow?”), and the rest are out of scope for other reasons (“When does my Verizon data plan start over?”). We evaluated our trained semantic parser on the 95 utterances annotated with logical forms. Our parser obtained an accuracy of 46.3% and oracle accuracy of 84.2%, which measures how often the correct denotation is on the final beam. The large gap shows that there is considerable room for improvement in the paraphrasing model. 8 Related work and discussion Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013). However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reduci</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Q. Cai and A. Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>D Goldwasser</author>
<author>M Chang</author>
<author>D Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<journal>In Computational Natural Language Learning (CoNLL),</journal>
<pages>18--27</pages>
<contexts>
<context position="36650" citStr="Clarke et al., 2010" startWordPosition="5955" endWordPosition="5958">n answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or cont</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010. Driving semantic parsing from the world’s response. In Computational Natural Language Learning (CoNLL), pages 18–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2010</date>
<booktitle>In Conference on Learning Theory (COLT).</booktitle>
<contexts>
<context position="27833" citStr="Duchi et al., 2010" startWordPosition="4453" endWordPosition="4456">s in x #unmatched words in c size of denotation of z, (|Qzjw|) pos(x0:0) conjoined with type(Qzjw) #nodes in tree generating z Lexical ∀(i, j) ∈ A. (xi:i, cj:j) ∀(i, j) ∈ A. (xi:i, cj:j+1) ∀(i, j) ∈ A. (xi:i, cj−1:j) ∀(i, j), (i + 1, j + 1) ∈ A. (xi:i+1, cj:j+1) all unaligned words in x and c (xi:j, ci,:j,) if in phrase table Table 4: Features for the paraphrasing model. pos(xi:i) is the POS tag; type(Qz�w) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c. E(x,c,z)∈D log pe(z, c |x, w) − A11θ111. To optimize, we use AdaGrad (Duchi et al., 2010). Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013). We count the number of exact matches, PPDB matches, and unmatched words. To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4). We also use the word align</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2010</marker>
<rawString>J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive subgradient methods for online learning and stochastic optimization. In Conference on Learning Theory (COLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>L Zettlemoyer</author>
<author>O Etzioni</author>
</authors>
<title>Paraphrase-driven learning for open question answering.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="37026" citStr="Fader et al., 2013" startWordPosition="6016" endWordPosition="6019">emantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by </context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>A. Fader, L. Zettlemoyer, and O. Etzioni. 2013. Paraphrase-driven learning for open question answering. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ganitkevitch</author>
<author>B V Durme</author>
<author>C Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL),</booktitle>
<pages>758--764</pages>
<contexts>
<context position="28023" citStr="Ganitkevitch et al., 2013" startWordPosition="4487" endWordPosition="4490">:j+1) ∀(i, j) ∈ A. (xi:i, cj−1:j) ∀(i, j), (i + 1, j + 1) ∈ A. (xi:i+1, cj:j+1) all unaligned words in x and c (xi:j, ci,:j,) if in phrase table Table 4: Features for the paraphrasing model. pos(xi:i) is the POS tag; type(Qz�w) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c. E(x,c,z)∈D log pe(z, c |x, w) − A11θ111. To optimize, we use AdaGrad (Duchi et al., 2010). Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013). We count the number of exact matches, PPDB matches, and unmatched words. To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004). We define an indicator feature for every phrase pair of x and c that appear in the phr</context>
</contexts>
<marker>Ganitkevitch, Durme, Callison-Burch, 2013</marker>
<rawString>J. Ganitkevitch, B. V. Durme, and C. Callison-Burch. 2013. PPDB: The paraphrase database. In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL), pages 758–764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Krishnamurthy</author>
<author>T Kollar</author>
</authors>
<title>Jointly learning to parse and perceive: Connecting natural language to the physical world.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<volume>1</volume>
<pages>206</pages>
<contexts>
<context position="36346" citStr="Krishnamurthy and Kollar, 2013" startWordPosition="5907" endWordPosition="5910">onsiderable room for improvement in the paraphrasing model. 8 Related work and discussion Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013). However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphr</context>
</contexts>
<marker>Krishnamurthy, Kollar, 2013</marker>
<rawString>J. Krishnamurthy and T. Kollar. 2013. Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics (TACL), 1:193– 206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kushman</author>
<author>R Barzilay</author>
</authors>
<title>Using semantic unification to generate regular expressions from natural language.</title>
<date>2013</date>
<booktitle>In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL),</booktitle>
<pages>826--836</pages>
<contexts>
<context position="1346" citStr="Kushman and Barzilay, 2013" startWordPosition="200" endWordPosition="203"> The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours. 1 Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to the paper. Domain (1) by builder (-30 minutes) (1) by training a paraphrasing model Semantic parser Figu</context>
<context position="36242" citStr="Kushman and Barzilay, 2013" startWordPosition="5890" endWordPosition="5894"> measures how often the correct denotation is on the final beam. The large gap shows that there is considerable room for improvement in the paraphrasing model. 8 Related work and discussion Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013). However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our meth</context>
</contexts>
<marker>Kushman, Barzilay, 2013</marker>
<rawString>N. Kushman and R. Barzilay. 2013. Using semantic unification to generate regular expressions from natural language. In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL), pages 826–836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>E Choi</author>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1288" citStr="Kwiatkowski et al., 2013" startWordPosition="192" endWordPosition="195">ase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours. 1 Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to the paper. Domain (1) by builder (-30 minutes)</context>
<context position="5025" citStr="Kwiatkowski et al., 2013" startWordPosition="751" endWordPosition="754">estions dataset (Berant et al., 2013) contains no questions with numeric answers, so any semantic parser trained on that dataset would lack that functionality. These biases are not codified, which results in an idiosyncratic and mysterious user experience, a major drawback of natural language interfaces (Rangel et al., 2014). In contrast, our compact grammar precisely specifies the logical functionality. We enforce completeness by generating canonical utterances that exercise every grammar rule. In terms of supervision, state-of-the-art semantic parsers are trained from question-answer pairs (Kwiatkowski et al., 2013; Berant and Liang, 2014). Although this is a marked improvement in cost and scalability compared to annotated logical forms, it still requires non-trivial effort: the annotator must (i) understand the question and (ii) figure out the answer, which becomes even harder with compositional utterances. In contrast, our main source of supervision is paraphrases, which only requires (i), not (ii). Such data is thus cheaper and faster to obtain. Linguistic reflections. The centerpiece of our framework is a domain-general grammar that connects logical forms with canonical utterances. This connection w</context>
<context position="35952" citStr="Kwiatkowski et al., 2013" startWordPosition="5851" endWordPosition="5854">do I have tomorrow?”), and the rest are out of scope for other reasons (“When does my Verizon data plan start over?”). We evaluated our trained semantic parser on the 95 utterances annotated with logical forms. Our parser obtained an accuracy of 46.3% and oracle accuracy of 84.2%, which measures how often the correct denotation is on the final beam. The large gap shows that there is considerable room for improvement in the paraphrasing model. 8 Related work and discussion Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013). However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logica</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In North American Association for Computational Linguistics (NAACL),</booktitle>
<pages>104--111</pages>
<contexts>
<context position="28174" citStr="Liang et al., 2006" startWordPosition="4512" endWordPosition="4515">ures for the paraphrasing model. pos(xi:i) is the POS tag; type(Qz�w) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c. E(x,c,z)∈D log pe(z, c |x, w) − A11θ111. To optimize, we use AdaGrad (Duchi et al., 2010). Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013). We count the number of exact matches, PPDB matches, and unmatched words. To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004). We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the PUBLICATIONS domain include fewest– least number and by–whose author is. Note that we do not build a hard lexicon but only</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agreement. In North American Association for Computational Linguistics (NAACL), pages 104– 111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>590--599</pages>
<contexts>
<context position="1241" citStr="Liang et al., 2011" startWordPosition="184" endWordPosition="187">ly). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours. 1 Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to</context>
<context position="5778" citStr="Liang et al., 2011" startWordPosition="866" endWordPosition="869">requires non-trivial effort: the annotator must (i) understand the question and (ii) figure out the answer, which becomes even harder with compositional utterances. In contrast, our main source of supervision is paraphrases, which only requires (i), not (ii). Such data is thus cheaper and faster to obtain. Linguistic reflections. The centerpiece of our framework is a domain-general grammar that connects logical forms with canonical utterances. This connection warrants further scrutiny, as the structural mismatch between logic and language is the chief source of difficulty in semantic parsing (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014). There are two important questions here. First, is it possible to design a simple grammar that simultaneously generates both logical forms and canonical utterances so that the utterances are understandable by a human? In Section 3, we show how to choose appropriate canonical utterances to maximize alignment with the logical forms. Second, our grammar can generate an infinite number of canonical utterances. How many do we need for adequate coverage? Certainly, single relations is insufficient: just knowing that “publication date of X” paraphra</context>
<context position="36671" citStr="Liang et al., 2011" startWordPosition="5959" endWordPosition="5962">(Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural langua</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2011. Learning dependency-based compositional semantics. In Association for Computational Linguistics (ACL), pages 590–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<date>2013</date>
<note>Lambda dependency-based compositional semantics. arXiv.</note>
<contexts>
<context position="10333" citStr="Liang (2013)" startWordPosition="1644" endWordPosition="1645"> not parse x, so the semantic parsing model will be based on paraphrasing, in the spirit of Berant and Liang (2014). To summarize, (1) the builder produces a seed lexicon L; (2) the framework produces logical forms and canonical utterances GEN(G U L) = {(z, c)}; (3) the builder (via crowdsourcing) uses P(�) to produce a dataset D = {(x, c, z)}; and (4) the framework uses D to train a semantic parser pθ(z,c |x, w). 2.1 Lambda DCS Our logical forms are represented in lambda DCS, a logical language where composition operates on sets rather than truth values. Here we give a brief description; see Liang (2013) for details. Every logical form z in this paper is either a unary (denoting a set of entities) or a binary (denoting a set of entity-pairs). In the base case, each entity e (e.g., 2015) is a unary denoting the singleton set: JeKw = {e}; and each property p (e.g., publicationDate) is a binary denoting all entitypairs (e1, e2) that satisfy the property p. Unaries and binaries can be composed: Given a binary b and unary u, the join b.u denotes all entities e1 for which there exists an e2 E JuKw with (e1, e2) E JbKw. For example, publicationDate.2015 denote entities published in 2015. The interse</context>
</contexts>
<marker>Liang, 2013</marker>
<rawString>P. Liang. 2013. Lambda dependency-based compositional semantics. arXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Matuszek</author>
<author>N FitzGerald</author>
<author>L Zettlemoyer</author>
<author>L Bo</author>
<author>D Fox</author>
</authors>
<title>A joint model of language and perception for grounded attribute learning.</title>
<date>2012</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>1671--1678</pages>
<contexts>
<context position="36292" citStr="Matuszek et al., 2012" startWordPosition="5899" endWordPosition="5902">al beam. The large gap shows that there is considerable room for improvement in the paraphrasing model. 8 Related work and discussion Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013). However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very</context>
</contexts>
<marker>Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and D. Fox. 2012. A joint model of language and perception for grounded attribute learning. In International Conference on Machine Learning (ICML), pages 1671–1678.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="28535" citStr="Och and Ney, 2004" startWordPosition="4575" endWordPosition="4578"> bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013). We count the number of exact matches, PPDB matches, and unmatched words. To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004). We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the PUBLICATIONS domain include fewest– least number and by–whose author is. Note that we do not build a hard lexicon but only use A and the phrase table to define features, allowing the model to learn useful paraphrases during training. Finally, we define standard features on logical forms and denotations (Berant et al., 2013). 7 Experimental Evaluation We evaluated our functionality-driven process on the seven domains described in Section 5 and one new domain we describe in Sectio</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pasupat</author>
<author>P Liang</author>
</authors>
<title>Compositional semantic parsing on semi-structured tables.</title>
<date>2015</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="25340" citStr="Pasupat and Liang (2015)" startWordPosition="4042" endWordPosition="4045">ch search state, which includes the syntactic category s (e.g., NP) and the depth of the logical form, we generate at most K = 20 candidates by applying the rules in Table 2. In practice, the lexical rules T(x) are applied first, and composition is performed, but not constrained to the utterance. For example, the utterance “article” would generate the logical form count(type.article). Instead, soft paraphrasing features are used to guide the search. This rather unorthodox approach to semantic parsing can be seen as a generalization of Berant and Liang (2014) and is explained in more detail in Pasupat and Liang (2015). Training. We train our model by maximizing the regularized log-likelihood O(0) = 1337 Domain # pred. # ex. Phenomena Example CALENDAR 22 837 temporal language x: “Show me meetings after the weekly standup day” c: “meeting whose date is at least date of weekly standup” z: type.meeting fl date. &gt; R(date).weeklyStandup BLOCKS 19 1995 spatial language x: “Select the brick that is to the furthest left.” c: “block that the most number of block is right of” z: argmax(type.block, R(Ax.count(R(right).x))) HOUSING 24 941 measurement units x: “Housing that is 800 square feet or bigger?” c: “housing uni</context>
</contexts>
<marker>Pasupat, Liang, 2015</marker>
<rawString>P. Pasupat and P. Liang. 2015. Compositional semantic parsing on semi-structured tables. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A P Rangel</author>
<author>M A Aguirre</author>
<author>J J Gonzlez</author>
<author>J M Carpio</author>
</authors>
<title>Features and pitfalls that users should seek in natural language interfaces to databases. In Recent Advances on Hybrid Approaches for Designing Intelligent Systems,</title>
<date>2014</date>
<pages>617--630</pages>
<contexts>
<context position="4727" citStr="Rangel et al., 2014" startWordPosition="711" endWordPosition="714">his data to train a semantic parser. Practical advantages. There are two main advantages of our approach: completeness and ease of supervision. Traditionally, training data is collected in a best-effort manner, which can result in an incomplete coverage of functionality. For example, the WebQuestions dataset (Berant et al., 2013) contains no questions with numeric answers, so any semantic parser trained on that dataset would lack that functionality. These biases are not codified, which results in an idiosyncratic and mysterious user experience, a major drawback of natural language interfaces (Rangel et al., 2014). In contrast, our compact grammar precisely specifies the logical functionality. We enforce completeness by generating canonical utterances that exercise every grammar rule. In terms of supervision, state-of-the-art semantic parsers are trained from question-answer pairs (Kwiatkowski et al., 2013; Berant and Liang, 2014). Although this is a marked improvement in cost and scalability compared to annotated logical forms, it still requires non-trivial effort: the annotator must (i) understand the question and (ii) figure out the answer, which becomes even harder with compositional utterances. In</context>
</contexts>
<marker>Rangel, Aguirre, Gonzlez, Carpio, 2014</marker>
<rawString>R. A. P. Rangel, M. A. Aguirre, J. J. Gonzlez, and J. M. Carpio. 2014. Features and pitfalls that users should seek in natural language interfaces to databases. In Recent Advances on Hybrid Approaches for Designing Intelligent Systems, pages 617–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Reddy</author>
<author>M Lapata</author>
<author>M Steedman</author>
</authors>
<title>Largescale semantic parsing without question-answer pairs.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<volume>2</volume>
<issue>10</issue>
<contexts>
<context position="36741" citStr="Reddy et al., 2014" startWordPosition="5972" endWordPosition="5975">specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the</context>
</contexts>
<marker>Reddy, Lapata, Steedman, 2014</marker>
<rawString>S. Reddy, M. Lapata, and M. Steedman. 2014. Largescale semantic parsing without question-answer pairs. Transactions of the Association for Computational Linguistics (TACL), 2(10):377–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schwitter</author>
</authors>
<title>Controlled natural languages for knowledge representation.</title>
<date>2010</date>
<booktitle>In International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1113--1121</pages>
<contexts>
<context position="37292" citStr="Schwitter, 2010" startWordPosition="6060" endWordPosition="6061">to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance. In summary, our contributions are two-fold: a new functionality-driven process and an exploration of some of its linguistic implications. We believe that our methodology is a</context>
</contexts>
<marker>Schwitter, 2010</marker>
<rawString>R. Schwitter. 2010. Controlled natural languages for knowledge representation. In International Conference on Computational Linguistics (COLING), pages 1113–1121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>T Kollar</author>
<author>S Dickerson</author>
<author>M R Walter</author>
<author>A G Banerjee</author>
<author>S J Teller</author>
<author>N Roy</author>
</authors>
<title>Understanding natural language commands for robotic navigation and mobile manipulation.</title>
<date>2011</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="36313" citStr="Tellex et al., 2011" startWordPosition="5903" endWordPosition="5906">shows that there is considerable room for improvement in the paraphrasing model. 8 Related work and discussion Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013). However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of </context>
</contexts>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. J. Teller, and N. Roy. 2011. Understanding natural language commands for robotic navigation and mobile manipulation. In Association for the Advancement of Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Warren</author>
<author>F Pereira</author>
</authors>
<title>An efficient easily adaptable system for interpreting natural language queries.</title>
<date>1982</date>
<journal>Computational Linguistics,</journal>
<pages>8--110</pages>
<contexts>
<context position="37242" citStr="Warren and Pereira, 1982" startWordPosition="6051" endWordPosition="6054"> denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance. In summary, our contributions are two-fold: a new functionality-driven process and an exploration of some of its linguistic </context>
</contexts>
<marker>Warren, Pereira, 1982</marker>
<rawString>D. Warren and F. Pereira. 1982. An efficient easily adaptable system for interpreting natural language queries. Computational Linguistics, 8:110–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>960--967</pages>
<contexts>
<context position="36614" citStr="Wong and Mooney, 2007" startWordPosition="5949" endWordPosition="5952"> the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 197</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Y. W. Wong and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Association for Computational Linguistics (ACL), pages 960–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
<author>R M Kaplan</author>
<author>B N Webber</author>
</authors>
<title>The lunar sciences natural language information system: Final report.</title>
<date>1972</date>
<tech>Technical report, BBN Report 2378,</tech>
<institution>Bolt Beranek and Newman Inc.</institution>
<contexts>
<context position="37215" citStr="Woods et al., 1972" startWordPosition="6047" endWordPosition="6050">and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance. In summary, our contributions are two-fold: a new functionality-driven process and an exploration</context>
</contexts>
<marker>Woods, Kaplan, Webber, 1972</marker>
<rawString>W. A. Woods, R. M. Kaplan, and B. N. Webber. 1972. The lunar sciences natural language information system: Final report. Technical report, BBN Report 2378, Bolt Beranek and Newman Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="1190" citStr="Zelle and Mooney, 1996" startWordPosition="176" endWordPosition="179">pture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours. 1 Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly buildi</context>
<context position="33360" citStr="Zelle and Mooney, 1996" startWordPosition="5419" endWordPosition="5422">o create a semantic parser for a new domain (RECIPES) exactly 24 hours before the submission deadline. Starting at midnight, we created a seed lexicon in less than 30 minutes. Then we generated canonical utterances and allowed AMT workers to provide paraphrases overnight. In the morning, we trained our parser and obtained an accuracy of 70.8% on the test set. 7.4 Testing on independent data Geo880. To test how our parser generalizes to utterances independent of our framework, we created a semantic parser for the domain of US geography, and tested on the standard 280 test examples from GEO880 (Zelle and Mooney, 1996). We did not use the standard 600 training examples. Our parser obtained 56.4% accuracy, which is substantially lower than state-of-the-art (- 90%). We performed error analysis on 100 random sentences from the development set where accuracy was 60%. We found that the parser learns from the training data to prefer shorter paraphrases, which accounts for 30% of the errors. In most of these cases, the correct logical form is ranked at the top-3 results (accuracy for the top3 derivations is 73%). GEO880 contains highly compositional utterances, and in 25% of the errors 1339 the correct derivation </context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>M. Zelle and R. J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Uncertainty in Artificial Intelligence (UAI),</booktitle>
<pages>658--666</pages>
<contexts>
<context position="1221" citStr="Zettlemoyer and Collins, 2005" startWordPosition="180" endWordPosition="183"> logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours. 1 Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors eq</context>
<context position="36590" citStr="Zettlemoyer and Collins, 2005" startWordPosition="5944" endWordPosition="5948">et al., 2013). However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based sy</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Uncertainty in Artificial Intelligence (UAI), pages 658– 666.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>