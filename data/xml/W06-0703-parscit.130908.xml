<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997331">
Question Pre-Processing in a QA System on Internet
Discussion Groups
</title>
<author confidence="0.994096">
Chuan-Jie Lin and Chun-Hung Cho
</author>
<affiliation confidence="0.9938575">
Department of Computer Science and Engineering
National Taiwan Ocean University
</affiliation>
<address confidence="0.938608">
No 2, Rd Pei-Ning, Keelung 202, Taiwan, R.O.C
</address>
<email confidence="0.99825">
cjlin@mail.ntou.edu.tw; futurehero@seed.net.tw
</email>
<sectionHeader confidence="0.995625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992568">
This paper proposes methods to
pre-process questions in the postings
before a QA system can find answers in a
discussion group in the Internet.
Pre-processing includes garbage text
removal and question segmentation.
Garbage keywords are collected and
different length thresholds are assigned to
them for garbage text identification.
Interrogative forms and question types
are used to segment questions. The best
performance on the test set achieves
92.57% accuracy in garbage text removal
and 85.87% accuracy in question
segmentation, respectively.
</bodyText>
<sectionHeader confidence="0.998698" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999482785714286">
Question answering has been a hot research topic
in recent years. Large scale QA evaluation
projects (e.g. TREC QA-Track1, QA@CLEF2,
and NTCIR 3 QAC and CLQA Tracks) are
helpful to the developments of question
answering.
However, real automatic QA services are not
ready in the Internet. One popular way for
Internet users to ask questions and get answers is
to visit discussion groups, such as Usenet
newsgroups 4 or Yahoo! Answers 5 . Each
discussion group focuses on one topic so that
users can easily find one to post their questions.
There are two ways a user can try to find
</bodyText>
<footnote confidence="0.998453166666667">
1 http://trec.nist.gov/data/qa.html
2 http://clef-qa.itc.it/
3 http://research.nii.ac.jp/ntcir/index-en.html
4 Now they can be accessed via Google Groups:
http://groups.google.com/
5 http://answers.yahoo.com/
</footnote>
<bodyText confidence="0.997099470588235">
answers. You can post your question in a
related discussion group and wait for other users
to provide answers. Some discussion groups
provide search toolbars so that you can search
your question first to see if there are similar
postings asking the same question. In Yahoo!
Answers, you can also judge answers offered by
other users and mark the best one.
Postings in discussion groups are good
materials to develop a FAQ-style QA system in
the Internet. By finding questions in the
discussion groups similar to a new posting,
responses to these questions can provide answers
or relevant information.
But without pre-processing, measuring
similarity with original texts will arise some
problems:
</bodyText>
<listItem confidence="0.793938307692308">
1. Some phrases such as “many thanks” or
“help me please” are not part of a
question. These kinds of phrases will
introduce noise and harm matching
performance.
2. Quite often there is more than one
question in one posting. If the question
which is most similar to the user&apos;s
question appears in an existed posting
together with other different questions, it
will get a lower similarity score than the
one it is supposed to have because of
other questions.
</listItem>
<bodyText confidence="0.999904333333333">
Therefore, inappropriate phrases should be
removed and different questions in one posting
should be separated before question comparison.
There is no research focusing on this topic.
FAQ finders (Lai et al., 2002; Lytinen and
Tomuro, 2002; Burke, 1997) are closely related
to this topic. However, there are differences
between them. First of all, questions in a FAQ
set are often written in perfect grammar without
</bodyText>
<page confidence="0.97477">
16
</page>
<note confidence="0.6949735">
Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 16–23,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998374">
garbage text. Second, questions are often
paired with answers separately. I.e. there is
often one question in one QA pair.
There were some research groups who
divided questions into segments. Soricut and
Brill (2004) chunked questions and used them as
queries to search engines. Saquete et al. (2004)
focused on decomposition of a complex question
into several sub-questions. In this paper,
question segmentation is to identify different
questions posed in one posting.
</bodyText>
<sectionHeader confidence="0.933634" genericHeader="method">
2 Garbage Text Removal
</sectionHeader>
<subsectionHeader confidence="0.823037">
2.1 Garbage Texts
</subsectionHeader>
<bodyText confidence="0.999119">
Articles in discussion groups are colloquial.
Users often write articles as if they are talking to
other users. For this reason, phrases expressing
appreciation, begging, or emotions of writers are
often seen in the postings. For example:
</bodyText>
<equation confidence="0.5802535">
有關 powerpoint 問題 我想請問一下 1 該如何把 access 的整個視窗放到簡報上撥放
謝謝2
</equation>
<bodyText confidence="0.9851046">
(About Powerpoint, I’d like to ask1, how to
put the whole window seen in Access onto a
slide? Thank you2!)
The phrases “我想請問一下” (“I’d like to ask”)
and “謝謝” (“Thank you”) are unimportant to the
question itself.
These phrases often contain content words,
not stop words, and thus are hard to be
distinguished with the real questions. If these
phrases are not removed, it can happen that two
questions are judged “similar” because one of
these phrases appears in both questions.
A phrase which contributes no information
about a question is called garbage text in this
paper and should be removed beforehand in
order to reduce noise. The term theme text is
used to refer to the remaining text.
After examining real querying postings,
some characteristics of garbage texts are
observed:
</bodyText>
<listItem confidence="0.891497777777778">
1. Some words strongly suggest themselves
being in a garbage text, such as “thank”
in “thank you so much”, or “help” in
“who can help me”.
2. Some words appear in both theme texts
and garbage texts, hence ambiguity
arises. For example:
“請教高手” (Any expert please help)
“快閃高手” (Flash Expert)
</listItem>
<bodyText confidence="0.9995705">
The first phrase is a garbage text, while
the second phrase is a product name.
The word “expert” suggests an existence
of a garbage text but not in all cases.
Because punctuation marks are not reliable in
Chinese, we use sentence fragment as the unit to
be processed. A sentence fragment is defined
to be a fragment of text segmented by commas,
periods, question marks, exclamation marks, or
space marks. A space mark can be a boundary
of a sentence fragment only when both
characters preceding and following the space
mark are not the English letters, digits, or
punctuation marks.
</bodyText>
<subsectionHeader confidence="0.998369">
2.2 Strategies to Remove Garbage Texts
</subsectionHeader>
<bodyText confidence="0.9706206">
Frequent terms seen in garbage texts are
collected as garbage keywords and grouped into
classes according to their meanings and usages.
Table 1 gives some examples of classes of
garbage keywords collected from the training set.
</bodyText>
<table confidence="0.9926192">
Class Garbage Keywords
Please 請問一下, 煩請,不好意思...
Thanks 感謝,謝謝,感恩,感溫...
Help 賜教,請教,幫我解答,救我...
Urgent 緊急,緊迫,急迫,急...
</table>
<tableCaption confidence="0.999561">
Table 1. Some Classes of Garbage Keywords
</tableCaption>
<bodyText confidence="0.999743238095238">
To handle ambiguity, this paper proposes a
length information strategy to determine garbage
texts as follows:
If a sentence fragment contains a garbage
keyword and the length of the fragment after
removing the garbage keyword is less than a
threshold, the whole fragment will be judged as a
garbage text. Otherwise, only the garbage
keyword itself is judged as garbage text if it is
never in an ambiguous case.
Different length thresholds are assigned to
different classes of garbage keywords. If more
than one garbage keyword occurring in a
fragment, discard all the keywords first, and then
compare the length of the remaining fragment
with the maximal threshold among the ones
corresponding to these garbage keywords.
In order to increase the coverage of garbage
keywords, other linguistic resources are used to
expand the list of garbage keywords.
Synonyms in Tongyici Cilin (同義詞詞林), a
</bodyText>
<page confidence="0.998274">
17
</page>
<bodyText confidence="0.971671333333333">
thesaurus of Chinese words, are added into the
list. More garbage keywords are added by
common knowledge.
</bodyText>
<sectionHeader confidence="0.970707" genericHeader="method">
3 Question Segmentation
</sectionHeader>
<bodyText confidence="0.99571775">
When a user posts an article in a discussion
group, he may pose more than one question at
one time. For example, in the following
posting:
</bodyText>
<equation confidence="0.565553">
Office 2003 和 XP←有何不同之處呢? 哪
一 個 比 較 新 呢 ? 最 新 的 版
是???????????
</equation>
<bodyText confidence="0.997583214285714">
(Office 2003 and XP ← What are the
differences between them? Which
version is newer? What is the latest
version???????????)
there are 3 questions submitted at a time. If a
new user wants to know the latest version of
Office, responses to the previous posting will
give answers.
Table 2 lists the statistics of number of
questions in the training set. The first column is
the number of questions in one posting. The
second and the third columns are the number and
the percentage of postings which contain such
number of questions, respectively.
</bodyText>
<table confidence="0.999688111111111">
Q# Post# Perc (%)
1 494 56.98
2 259 29.87
3 82 9.46
4 22 2.54
5 4 0.46
&gt;_ 6 6 0.69
&gt;_ 2 373 43.02
Total 867 100.00
</table>
<tableCaption confidence="0.9544635">
Table 2. Statistics of Number of Questions
in Postings
</tableCaption>
<bodyText confidence="0.999953666666667">
As we can see in Table 2, nearly half (43.02%) of
the postings contain two or more questions.
That is why question segmentation is necessary.
</bodyText>
<subsectionHeader confidence="0.999845">
3.1 Characteristics of Questions in a Posting
</subsectionHeader>
<bodyText confidence="0.9975965">
Several characteristics of question texts in
postings were found in real discussion groups:
</bodyText>
<listItem confidence="0.845742869565217">
1. Some people use ‘?’ (question mark) at
the end of a question while some people
do not. In Chinese, some people even
separate sentences only by spaces
instead of punctuation marks. (Note
that there is no space mark between
words in Chinese text.)
2. Questions are usually in interrogative
form. Either interrogatives or question
marks appear in the questions.
3. One question may occur repeatedly in
the same posting. It is often the case
that a question appears both in the title
and in the content. Sometimes a user
repeats a sentence several times to show
his anxiety.
4. One question may be expressed in
different ways in the same posting. The
sentences may be similar. For example:
A: Office2000 的剪貼簿只能維持 12 個
項目?
B: Office2000 的剪貼簿只能保持12
項目?
</listItem>
<bodyText confidence="0.9469345">
(Can the clipboard of Office2000 only
keep 12 items?)
“維持” and “保持” are synonyms in the
meaning of “keep”.
Dissimilar sentences may also refer
to the same question. For example,
</bodyText>
<listItem confidence="0.9997846">
(1) How to use automatic text
wrapping in Excel?
(2) If I want to put two or more lines in
one cell, what can I do?
(3) How to use it?
</listItem>
<bodyText confidence="0.990382166666667">
These three sentences ask the same
question: “How to use automatic text
wrapping in Excel?” The second
sentence makes a detailed description of
what he wants to do. Topic of the third
sentence is the same as the first sentence
hence is omitted. Topic ellipsis is quite
often seen in Chinese.
5. Some users will give examples to
explain the questions. These sentences
often start with phrases like “for
example” or “such as”.
</bodyText>
<subsectionHeader confidence="0.999655">
3.2 Strategies to Separate Questions
</subsectionHeader>
<bodyText confidence="0.999318">
According to the observations in Section 3.1,
several strategies are proposed to separate
questions:
</bodyText>
<page confidence="0.990429">
18
</page>
<listItem confidence="0.724479">
(1) Separating by Question Mark (‘?’)
</listItem>
<bodyText confidence="0.924222">
It is the simplest method. We use it as a
baseline strategy.
</bodyText>
<listItem confidence="0.730515">
(2) Identifying Questions by Interrogative
Forms
</listItem>
<bodyText confidence="0.999765555555556">
Questions are usually in interrogative forms
including subject inversion (“is he...”, “does
it...”), using interrogatives (“who is...”), or a
declarative sentence attached with a question
mark (“Office2000 is better?”). Only the
third form requires a question mark. The
first two forms can specify themselves as
questions by text only. Moreover, there are
particles in Chinese indicating a question as
well, such as “嗎” or “呢”.
If a sentence fragment is in interrogative
form, it will be judged as a question and
separated from the others. A fragment not
in interrogative form is merged with the
nearest question fragment preceding it (or
following it if no preceding one). Note that
garbage texts have been removed before
question separation.
</bodyText>
<listItem confidence="0.732954">
(3) Merging or Removing Similar Sentences
</listItem>
<bodyText confidence="0.999309888888889">
If two sentence fragments are exactly the
same, one of them will be removed. If two
sentence fragments are similar, they are
merged into one question fragment.
Similarity is measured by the Dice
coefficient (Dice, 1945) using weights of
common words in the two sentence
fragments. The similarity of two sentence
fragments X and Y is defined as follows:
</bodyText>
<equation confidence="0.8909302">
∑ Wt( k)
Sim X,Y)= ∑Wt(w) EWt(t) (1)
( k∈X+Y
w∈X t Y
∈
</equation>
<table confidence="0.98693325">
Vt (Transitive Verb),100
FW (Foreign Word)
N (Noun) 90
Vi (Intran
</table>
<tableCaption confidence="0.999242">
Table 3. Weights of Part-of-Speeches
</tableCaption>
<bodyText confidence="0.998655">
Before computing similarity, word
segmentation is performed to identify words
in Chinese text. After that, a part-of-speech
tagger is used to obtain POS information of
each word.
</bodyText>
<equation confidence="0.5516925">
2
×
</equation>
<bodyText confidence="0.9948637">
where Wt(w) is the weight of a word w. In
Equation 1, k is one of the words appearing
in both X and Y. Fragments with similarity
higher than a threshold are merged together.
The weight of a word is designed as the
weight of its part-of-speech as listed in Table
3. Nouns and verbs have higher weights,
while adverbs and particles have lower
weights. Note that foreign words are
assigned a rather high weight, because names
</bodyText>
<table confidence="0.992776">
sitive Verb) 80
A (Adjective) 40
ADV (Adverb), ASP (Tense),
C (Connective), DET (Determiner), 0
P (Preposition), T (Particle)
</table>
<listItem confidence="0.920433">
(4) Merging Questions with the Same Type
</listItem>
<bodyText confidence="0.990755888888889">
The information of question type has been
widely adopted in QA systems (Zhang and
Lee, 2003; Hovy et al., 2002; Harabagiu et
al., 2001). Question type often refers to the
possible type of its answer, such as a person
name, a location name, or a temporal
expression. The question types used in this
paper are PERSON, LOCATION, REASON,
QUANTITY, TEMPORAL, COMPARISON,
DEFINITION, METHOD, SELECTION,
YESNO, and OTHER. Rules to determine
question types are created manually.
This strategy tries to merge two question
fragments of the same question type. This
paper proposes two features to determine the
threshold to merge two question fragments:
length and sum of term weights of a fragment.
Length is measured in characters and term
weights are designed as in Table 3.
Merging algorithm is as follows: if the
feature value of a question fragment is
smaller than a threshold, it will be merged
into the preceding question fragment (or the
following fragment if no preceding one).
This strategy applies recursively until no
question fragment has a feature value lower
than
</bodyText>
<figure confidence="0.545252222222222">
the threshold.
(5) Merging Example Fragments
If a fragment starts with a phrase such as
or
it will be merged into
its preceding question fr
“forexample”
“suchas”,
agment.
</figure>
<listItem confidence="0.2689505">
of software products such as
or
</listItem>
<bodyText confidence="0.7832298">
are often writt
“Office”
“Oracle”
en in English, which
are foreign words with respect to Chinese.
</bodyText>
<note confidence="0.212072">
19 POS Weight
</note>
<sectionHeader confidence="0.977883" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.951069">
4.1 Experimental Data
</subsectionHeader>
<bodyText confidence="0.999967928571428">
All the experimental data were collected from
Yahoo! Knowledge+ (Yahoo!奇 摩 知 識 +) 6 ,
discussion groups similar to Yahoo! Answers but
using Chinese instead of English.
Three discussion groups, “Business
Application” (Aj務應用), “Website Building”
(網站架設), and “Image Processing” (影像處理),
were selected to collect querying postings. The
reason that we chose these three discussion
groups was their moderate growing rates. We
could collect enough amount of querying
postings published in the same period of time.
The following kinds of postings were not
selected as our experimental data:
</bodyText>
<listItem confidence="0.999137125">
1. No questions inside
2. Full of algorithms or program codes
3. Full of emoticons or Martian texts (火星
文, a funny term used in Chinese to refer
to a writing style that uses words with
similar pronunciation to replace the
original text)
4. Redundant postings
</listItem>
<bodyText confidence="0.999695">
Totally 598 querying postings were collected as
the training set and 269 postings as the test set.
The real numbers of postings collected from each
group are listed in Table 4, where “BA”, “WB”,
and “IP” stand for “Business Application”,
“Website Building”, and “Image Processing”,
respectively.
</bodyText>
<table confidence="0.997357666666667">
Group BA WB IP
Training Set 198 207 193
Test Set 101 69 99
</table>
<tableCaption confidence="0.999027">
Table 4. Numbers of Postings in the Data Set
</tableCaption>
<bodyText confidence="0.9996614">
Two persons were asked to mark garbage texts
and separate questions in the whole data set. If
a conflicting case occurred, a third person (who
was one of the authors of this paper) would solve
the inconsistency.
</bodyText>
<subsectionHeader confidence="0.983134">
4.2 Garbage Texts Removal
</subsectionHeader>
<bodyText confidence="0.999868666666667">
The first factor examined in garbage text
removal is the length threshold. Table 5 lists
the experimental results on the training set and
</bodyText>
<footnote confidence="0.656557">
6 http://tw.knowledge.yahoo.com/
</footnote>
<bodyText confidence="0.991753904761905">
Table 6 on the test set. All garbage keywords
are collected from the training set.
Eight experiments were conducted to use
different values as length thresholds. The
strategy Lenk sets the length threshold to be k
characters (no matter in Chinese or English).
Hence, Len0 is one baseline strategy which
removes only the garbage keyword itself. LenS
is the other baseline strategy which removes the
whole sentence fragment where a garbage
keyword appears.
The strategy Heu uses different length
thresholds for different classes of garbage
keywords. The thresholds are heuristic values
after observing many examples in the training
set.
Accuracy is defined as the percentage of
successful removal. In one posting, if all real
garbage texts are correctly removed and no other
text is wrongly deleted, it counts one successful
removal.
</bodyText>
<figure confidence="0.7460879">
Strategy Accuracy (%)
Len0 64.21
LenS 27.59
Len1 73.91
Len2 78.43
Len3 80.60
Len4 78.26
Len5 71.91
Heu 99.67
HeuExp 99.67
</figure>
<tableCaption confidence="0.871022">
Table 5. Accuracy of Garbage Text Removal
with Different Length Thresholds (Training)
</tableCaption>
<table confidence="0.9991567">
Strategy Accuracy (%)
Len0 62.08
LenS 24.54
Len1 69.52
Len2 75.09
Len3 75.46
Len4 71.75
Len5 65.80
Heu 87.73
HeuExp 92.57
</table>
<tableCaption confidence="0.903111">
Table 6. Accuracy of Garbage Text Removal
with Different Length Thresholds (Test Set)
</tableCaption>
<bodyText confidence="0.999949666666667">
As we can see in both tables, the two baseline
strategies are poorer than any other strategy. It
means that length threshold is useful to decide
garbage existence.
Heu is the best strategy (99.67% on the
training set and 87.73% on the test set). Len3 is
</bodyText>
<page confidence="0.987592">
20
</page>
<bodyText confidence="0.999944391304348">
the best strategy (80.60% on the training set and
75.49% on the test set) among Lenk, but it is far
worse than Heu. We can conclude that the
length threshold should be assigned individually
for each class of garbage words. If it is
assigned carefully, the performance of garbage
removal will be good.
The second factor is the expansion of
garbage keywords. The strategy HeuExp is the
same as Heu except that the list of garbage
keywords was expanded as described in Section
2.2.
Comparing the last two rows in Table 6,
HeuExp strategy improves the performance from
87.73% to 92.57%. It shows that a small
amount of postings can provide good coverage of
garbage keywords after keyword expansion by
using available linguistic resources.
The results of HeuExp and Heu on the
training set are the same. It makes sense
because the expanded list suggests garbage
existence in the training set no more than the
original list does.
</bodyText>
<subsectionHeader confidence="0.9977675">
4.3 Question Segmentation
Overall Strategies
</subsectionHeader>
<bodyText confidence="0.999457666666666">
Six experiments were conducted to see the
performance of different strategies for question
segmentation. The strategies used in each
experiment are:
Baseline: using only ‘?’ (question mark) to
separate questions
SameS: removing repeated sentence
fragments then separating by ‘?’
Interrg: after removing repeated sentence
fragments, separating questions which
are in interrogative forms
SimlrS: following the strategy Interrg,
removing or merging similar sentence
fragments of the same question type
ForInst: following the strategy SimlrS,
merging a sentence fragment beginning
with “for instance” and alike with its
preceding question fragment
SameQT: following the strategy ForInst,
merging question fragments of the same
question type without considering
similarity
Table 7 and Table 8 depict the results of the six
experiments on the training set and the test set,
respectively. The second column in each table
lists the accuracy which is defined as the
percentage of postings which are separated into
the same number of questions as manually
tagged. The third column gives the number of
postings which are correctly separated. The
fourth and the fifth columns contain the numbers
of postings which are separated into more and
fewer questions, respectively.
</bodyText>
<table confidence="0.999064285714286">
Strategy Acc (%) Same More Fewer
Baseline 50.67 303 213 82
SameS 59.03 353 156 89
Interrg 64.88 388 204 6
SimlrS 75.08 449 141 8
ForInst 75.75 453 137 8
SameQT 88.29 528 13 57
</table>
<tableCaption confidence="0.916812">
Table 7. Accuracy of Question Segmentation
by Different Strategies (Training Set)
</tableCaption>
<table confidence="0.999754714285714">
Strategy Acc (%) Same More Fewer
Baseline 54.28 146 84 39
SameS 65.43 176 54 39
Interrg 65.43 176 93 0
SimlrS 74.35 200 68 1
ForInst 74.35 200 68 1
SameQT 85.87 231 16 22
</table>
<tableCaption confidence="0.970546">
Table 8. Accuracy of Question Segmentation
by Different Strategies (Test Set)
</tableCaption>
<bodyText confidence="0.999957444444444">
As we can see in Table 7, performance is
improved gradually after adding new strategies.
SameQT achieves the best performance with
88.29% accuracy. Same conclusion could also
be made by the results on the test set. SameQT
is the best one with 85.87% accuracy.
In Table 7, Baseline achieves only 50.67%
accuracy. That matches our observations: (1)
one question is often stated many times by
sentences ended with question marks in one
posting (as 213 postings were separated into
more questions); (2) some users do not use ‘?’ in
writing (as 82 postings were separated into fewer
questions).
SameS greatly reduces the cases (57 postings)
of separation into more questions by removing
repeated sentences.
On the other hand, Interrg greatly reduces the
cases (76 postings) of separation into fewer
questions. Many question sentences without
question marks were successfully captured by
detecting the interrogative forms.
SimlrS also improves a lot (successfully
reducing number of questions separated in 63
postings). But ForInst only improves a little.
It is more common to express one question
several times in different way than giving
</bodyText>
<page confidence="0.996831">
21
</page>
<bodyText confidence="0.999536222222222">
examples.
SameQT achieves the best performance,
which means that question type is a good strategy.
Different ways to express a question are usually
in the same question type. Comparing with
SimlrS which also considers sentence fragments
in the same question type, more improvement
comes from the successful merging of fragments
with topic ellipses, co-references, or paraphrases.
However, there may be other questions in the
same question type which are wrongly merged
together (as 49 failures in the training set).
Considering the results on the test set, Interrg
does not improve the overall performance
comparing to SameS because the improvement
equals the drop. ForInst does not improve
either. It seems that giving examples is not
common in the discussion groups.
</bodyText>
<subsectionHeader confidence="0.429797">
Thresholds in SameQT
</subsectionHeader>
<bodyText confidence="0.999877166666667">
In the strategy SameQT, two features, length and
sum of term weights, are used to determine
thresholds to merge question fragments as
mentioned in Section 3.2. In order to decide
which feature is better and which threshold value
should be set, two experiments were conducted.
</bodyText>
<figure confidence="0.922350625">
LenThr Acc (%) LenThr Acc (%)
0 75.75 9 85.62
3 76.25 10 86.62
4 78.60 15 88.29
5 81.94 20 88.13
6 84.95 30 88.63
7 85.79 40 88.29
8 86.29 o 88.29
</figure>
<figureCaption confidence="0.76589275">
Table 9. Accuracy of Question Segmentation
with Different Length Thresholds
Figure 1. Accuracy of Question Segmentation
with Different Length Thresholds
</figureCaption>
<bodyText confidence="0.999801555555555">
Table 9 depicts the experimental results of using
length of sentence fragments as merging
threshold. The column “LenThr” lists different
settings of length threshold and the column
“Acc” gives the accuracy.
The performance is gradually improved as
the value of length threshold increases. The
best one is LenThr=30 with 88.63% accuracy.
However, “Always Merging” (LenThr=o)
achieves 88.29% accuracy, which is also
acceptable comparing to the best performance.
Fig 1 shows the curve of accuracy against length
threshold.
Table 10 presents the experimental results of
using sum of term weights as merging thresold.
The column “WgtThr” lists different settings of
length threshold and the column “Acc” gives the
accuracy.
The performance is also gradually improved
as the value of weight threshold increases.
When WgtThr is set to be 500, 700, or 900, the
performance is the best, with 88.46% accuracy.
But the same as the threshold settings of length
feature, the best one does not outperform
“Always Merging” strategy (WgtThr=o, 88.29%
accuracy) too much. Fig 2 shows the curve of
accuracy against similarity threshold.
</bodyText>
<table confidence="0.999197625">
WgtThr Acc (%) WgtThr Acc (%)
0 75.75 350 87.29
50 77.93 400 88.13
100 83.11 450 88.29
150 85.28 500 88.46
200 86.29 700 88.46
250 86.79 900 88.46
300 87.46 o 88.29
</table>
<tableCaption confidence="0.8798635">
Table 10. Accuracy of Question Segmentation
with Different Weight Thresholds
</tableCaption>
<figureCaption confidence="0.918371">
Figure 2. Accuracy of Question Segmentation
with Different Weight Thresholds
</figureCaption>
<bodyText confidence="0.960632">
From the results of above experiments, we can
see that although using length feature with a
</bodyText>
<figure confidence="0.997210125">
(%)
90
75
70
85
80
0 3 4 5 6 7 8 9 10 15 20 30 40 ∞
Length Threshold
(%)
90
75
70
85
80
0 50 100 150 200 250 300 350 400 450 500 700 900 ∞
Weight Threshold
</figure>
<page confidence="0.994192">
22
</page>
<bodyText confidence="0.999438714285714">
threshold LenThr=30 achieves the best
performance, “Always Merging” is more
welcome for a online system because no feature
extraction or computation is needed with only a
little sacrifice of performance. Hence we
choose “Always Merging” as merging strategy in
SameQT.
</bodyText>
<sectionHeader confidence="0.989727" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999970794871795">
This paper proposes question pre-processing
methods for a FQA-style QA system on
discussion groups in the Internet. For a posting
already existing or being submitted to a
discussion group, garbage texts in it are removed
first, and then different questions in it are
identified so that they can be compared with
other questions individually.
An expanded list of garbage keywords is
used to detect garbage texts. If there is a
garbage keyword appearing in a sentence
fragment and the fragment has a length shorter
than a threshold corresponding to the class of the
garbage keyword, the fragment will be judged as
a garbage text. This method achieves 92.57%
accuracy on the test set. It means that a small
set is sufficient to collect all classes of garbage
keywords.
In question segmentation, sentence fragments
in interrogative forms are considered as question
fragments. Besides, repeated fragments are
removed and fragments of the same question
types are merged into one fragment. The
overall accuracy is 85.87% on the test set.
In the future, performance of a QA system
with or without question pre-processing will be
evaluated to verify its value.
New methods to create the list of garbage
keywords more robotically should be studied, as
well as the automatic assignments of the length
thresholds of classes of garbage keywords.
New feature should be discovered in the
future in order to segment questions more
accurately.
Although the strategies and the thresholds are
developed according to experimental data in
Chinese, we can see that many of them are
language-independent or can be adapted with not
too much effort.
</bodyText>
<sectionHeader confidence="0.991582" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999883772727273">
Burke, Robin, Kristian Hammond, Vladimir
Kulyukin, Steven Lytinen, Noriko Tomuro,
and Scott Schoenberg (1997) “Natural
language processing in the FAQFinder
system: Results and prospects,” Proceedings
of the 1997 AAAI Spring Symposium on
Natural Language Processing for the World
Wide Web, pp. 17-26.
Dice, Lee R. (1945) “Measures of the amount of
ecologic association between species,”
Journal of Ecology, Vol. 26, pp. 297-302.
Harabagiu, Sanda, Dan Moldovan, Marius Paşca,
Rada Mihalcea, Mihai Surdeanu, Răzvan
Bunescu, Roxana Gîrju, Vasile Rus, and Paul
Morărescu (2001) “The Role of
Lexico-Semantic Feedback in Open-Domain
Textual Question-Answering,” Proceedings
of ACL-EACL 2001, pp. 274-281.
Hovy, Eduard, Ulf Hermjakob, and Chin-Yew
Lin (2002) “The Use of External Knowledge
in Factoid QA,” Proceedings of TREC-10, pp.
644-652.
Lai, Yu-Sheng, Kuao-Ann Fung, and
Chung-Hsien Wu (2002) “FAQ Mining via
List Detection,” Proceedings of the COLING
Workshop on Multilingual Summarization
and Question Answering.
Lytinen, Steven and Noriko Tomuro (2002) “The
use of question types to match questions in
FAQFinder,” Proceedings of the 2002 AAAI
Spring Symposium on Mining Answers from
Texts and Knowledge Bases, pp. 46-53.
Saquete, Estela, Patricio Martinez-Barco, Rafael
Munoz, and Jose Luis Vicedo Gonzalez
(2004) “Splitting Complex Temporal
Questions for Question Answering Systems,”
Proceedings of ACL 2004, pp. 566-573.
Soricut, Radu and Eric Brill (2004) “Automatic
Question Answering: Beyond the Factoid,”
Proceedings of HLT-NAACL 2004, pp. 57-64.
Zhang, Dell and Wee Sun Lee (2003) “Question
Classification using Support Vector
Machines,” Proceedings of SIGIR 2003, pp.
26-32.
</reference>
<page confidence="0.998935">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.749766">
<title confidence="0.99106">Question Pre-Processing in a QA System on Internet Discussion Groups</title>
<author confidence="0.950861">Chuan-Jie Lin</author>
<author confidence="0.950861">Chun-Hung</author>
<affiliation confidence="0.997321">Department of Computer Science and National Taiwan Ocean</affiliation>
<address confidence="0.984182">No 2, Rd Pei-Ning, Keelung 202, Taiwan,</address>
<email confidence="0.9902">cjlin@mail.ntou.edu.tw;futurehero@seed.net.tw</email>
<abstract confidence="0.9889045625">This paper proposes methods to pre-process questions in the postings before a QA system can find answers in a discussion group in the Internet. Pre-processing includes garbage text removal and question segmentation. Garbage keywords are collected and different length thresholds are assigned to them for garbage text identification. Interrogative forms and question types are used to segment questions. The best performance on the test set achieves 92.57% accuracy in garbage text removal and 85.87% accuracy in question segmentation, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robin Burke</author>
<author>Kristian Hammond</author>
<author>Vladimir Kulyukin</author>
<author>Steven Lytinen</author>
<author>Noriko Tomuro</author>
<author>Scott Schoenberg</author>
</authors>
<title>Natural language processing in the FAQFinder system: Results and prospects,”</title>
<date>1997</date>
<booktitle>Proceedings of the</booktitle>
<pages>17--26</pages>
<publisher>AAAI Spring</publisher>
<marker>Burke, Hammond, Kulyukin, Lytinen, Tomuro, Schoenberg, 1997</marker>
<rawString>Burke, Robin, Kristian Hammond, Vladimir Kulyukin, Steven Lytinen, Noriko Tomuro, and Scott Schoenberg (1997) “Natural language processing in the FAQFinder system: Results and prospects,” Proceedings of the 1997 AAAI Spring Symposium on Natural Language Processing for the World Wide Web, pp. 17-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee R Dice</author>
</authors>
<title>Measures of the amount of ecologic association between species,”</title>
<date>1945</date>
<journal>Journal of Ecology,</journal>
<volume>26</volume>
<pages>297--302</pages>
<contexts>
<context position="11255" citStr="Dice, 1945" startWordPosition="1828" endWordPosition="1829">tion as well, such as “嗎” or “呢”. If a sentence fragment is in interrogative form, it will be judged as a question and separated from the others. A fragment not in interrogative form is merged with the nearest question fragment preceding it (or following it if no preceding one). Note that garbage texts have been removed before question separation. (3) Merging or Removing Similar Sentences If two sentence fragments are exactly the same, one of them will be removed. If two sentence fragments are similar, they are merged into one question fragment. Similarity is measured by the Dice coefficient (Dice, 1945) using weights of common words in the two sentence fragments. The similarity of two sentence fragments X and Y is defined as follows: ∑ Wt( k) Sim X,Y)= ∑Wt(w) EWt(t) (1) ( k∈X+Y w∈X t Y ∈ Vt (Transitive Verb),100 FW (Foreign Word) N (Noun) 90 Vi (Intran Table 3. Weights of Part-of-Speeches Before computing similarity, word segmentation is performed to identify words in Chinese text. After that, a part-of-speech tagger is used to obtain POS information of each word. 2 × where Wt(w) is the weight of a word w. In Equation 1, k is one of the words appearing in both X and Y. Fragments with similar</context>
</contexts>
<marker>Dice, 1945</marker>
<rawString>Dice, Lee R. (1945) “Measures of the amount of ecologic association between species,” Journal of Ecology, Vol. 26, pp. 297-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Dan Moldovan</author>
<author>Marius Paşca</author>
</authors>
<title>Rada Mihalcea, Mihai Surdeanu, Răzvan Bunescu, Roxana Gîrju, Vasile Rus, and Paul Morărescu</title>
<date>2001</date>
<booktitle>The Role of Lexico-Semantic Feedback in Open-Domain Textual Question-Answering,” Proceedings of ACL-EACL</booktitle>
<pages>274--281</pages>
<contexts>
<context position="12455" citStr="Harabagiu et al., 2001" startWordPosition="2035" endWordPosition="2038"> Fragments with similarity higher than a threshold are merged together. The weight of a word is designed as the weight of its part-of-speech as listed in Table 3. Nouns and verbs have higher weights, while adverbs and particles have lower weights. Note that foreign words are assigned a rather high weight, because names sitive Verb) 80 A (Adjective) 40 ADV (Adverb), ASP (Tense), C (Connective), DET (Determiner), 0 P (Preposition), T (Particle) (4) Merging Questions with the Same Type The information of question type has been widely adopted in QA systems (Zhang and Lee, 2003; Hovy et al., 2002; Harabagiu et al., 2001). Question type often refers to the possible type of its answer, such as a person name, a location name, or a temporal expression. The question types used in this paper are PERSON, LOCATION, REASON, QUANTITY, TEMPORAL, COMPARISON, DEFINITION, METHOD, SELECTION, YESNO, and OTHER. Rules to determine question types are created manually. This strategy tries to merge two question fragments of the same question type. This paper proposes two features to determine the threshold to merge two question fragments: length and sum of term weights of a fragment. Length is measured in characters and term weig</context>
</contexts>
<marker>Harabagiu, Moldovan, Paşca, 2001</marker>
<rawString>Harabagiu, Sanda, Dan Moldovan, Marius Paşca, Rada Mihalcea, Mihai Surdeanu, Răzvan Bunescu, Roxana Gîrju, Vasile Rus, and Paul Morărescu (2001) “The Role of Lexico-Semantic Feedback in Open-Domain Textual Question-Answering,” Proceedings of ACL-EACL 2001, pp. 274-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Ulf Hermjakob</author>
<author>Chin-Yew Lin</author>
</authors>
<date>2002</date>
<booktitle>The Use of External Knowledge in Factoid QA,” Proceedings of TREC-10,</booktitle>
<pages>644--652</pages>
<contexts>
<context position="12430" citStr="Hovy et al., 2002" startWordPosition="2031" endWordPosition="2034">ng in both X and Y. Fragments with similarity higher than a threshold are merged together. The weight of a word is designed as the weight of its part-of-speech as listed in Table 3. Nouns and verbs have higher weights, while adverbs and particles have lower weights. Note that foreign words are assigned a rather high weight, because names sitive Verb) 80 A (Adjective) 40 ADV (Adverb), ASP (Tense), C (Connective), DET (Determiner), 0 P (Preposition), T (Particle) (4) Merging Questions with the Same Type The information of question type has been widely adopted in QA systems (Zhang and Lee, 2003; Hovy et al., 2002; Harabagiu et al., 2001). Question type often refers to the possible type of its answer, such as a person name, a location name, or a temporal expression. The question types used in this paper are PERSON, LOCATION, REASON, QUANTITY, TEMPORAL, COMPARISON, DEFINITION, METHOD, SELECTION, YESNO, and OTHER. Rules to determine question types are created manually. This strategy tries to merge two question fragments of the same question type. This paper proposes two features to determine the threshold to merge two question fragments: length and sum of term weights of a fragment. Length is measured in</context>
</contexts>
<marker>Hovy, Hermjakob, Lin, 2002</marker>
<rawString>Hovy, Eduard, Ulf Hermjakob, and Chin-Yew Lin (2002) “The Use of External Knowledge in Factoid QA,” Proceedings of TREC-10, pp. 644-652.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Sheng Lai</author>
<author>Kuao-Ann Fung</author>
<author>Chung-Hsien Wu</author>
</authors>
<title>FAQ Mining via List Detection,”</title>
<date>2002</date>
<booktitle>Proceedings of the COLING Workshop on Multilingual Summarization and Question Answering.</booktitle>
<contexts>
<context position="3002" citStr="Lai et al., 2002" startWordPosition="455" endWordPosition="458">ase” are not part of a question. These kinds of phrases will introduce noise and harm matching performance. 2. Quite often there is more than one question in one posting. If the question which is most similar to the user&apos;s question appears in an existed posting together with other different questions, it will get a lower similarity score than the one it is supposed to have because of other questions. Therefore, inappropriate phrases should be removed and different questions in one posting should be separated before question comparison. There is no research focusing on this topic. FAQ finders (Lai et al., 2002; Lytinen and Tomuro, 2002; Burke, 1997) are closely related to this topic. However, there are differences between them. First of all, questions in a FAQ set are often written in perfect grammar without 16 Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 16–23, Sydney, July 2006. c�2006 Association for Computational Linguistics garbage text. Second, questions are often paired with answers separately. I.e. there is often one question in one QA pair. There were some research groups who divided questions into segments. Soricut and Brill (2004) chunked questi</context>
</contexts>
<marker>Lai, Fung, Wu, 2002</marker>
<rawString>Lai, Yu-Sheng, Kuao-Ann Fung, and Chung-Hsien Wu (2002) “FAQ Mining via List Detection,” Proceedings of the COLING Workshop on Multilingual Summarization and Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Lytinen</author>
<author>Noriko Tomuro</author>
</authors>
<title>The use of question types to match questions in FAQFinder,”</title>
<date>2002</date>
<booktitle>Proceedings of the 2002 AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases,</booktitle>
<pages>46--53</pages>
<contexts>
<context position="3028" citStr="Lytinen and Tomuro, 2002" startWordPosition="459" endWordPosition="462">of a question. These kinds of phrases will introduce noise and harm matching performance. 2. Quite often there is more than one question in one posting. If the question which is most similar to the user&apos;s question appears in an existed posting together with other different questions, it will get a lower similarity score than the one it is supposed to have because of other questions. Therefore, inappropriate phrases should be removed and different questions in one posting should be separated before question comparison. There is no research focusing on this topic. FAQ finders (Lai et al., 2002; Lytinen and Tomuro, 2002; Burke, 1997) are closely related to this topic. However, there are differences between them. First of all, questions in a FAQ set are often written in perfect grammar without 16 Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 16–23, Sydney, July 2006. c�2006 Association for Computational Linguistics garbage text. Second, questions are often paired with answers separately. I.e. there is often one question in one QA pair. There were some research groups who divided questions into segments. Soricut and Brill (2004) chunked questions and used them as queri</context>
</contexts>
<marker>Lytinen, Tomuro, 2002</marker>
<rawString>Lytinen, Steven and Noriko Tomuro (2002) “The use of question types to match questions in FAQFinder,” Proceedings of the 2002 AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases, pp. 46-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Estela Saquete</author>
<author>Patricio Martinez-Barco</author>
<author>Rafael Munoz</author>
<author>Jose Luis</author>
</authors>
<title>Vicedo Gonzalez</title>
<date>2004</date>
<booktitle>Proceedings of ACL</booktitle>
<pages>566--573</pages>
<contexts>
<context position="3671" citStr="Saquete et al. (2004)" startWordPosition="557" endWordPosition="560">sely related to this topic. However, there are differences between them. First of all, questions in a FAQ set are often written in perfect grammar without 16 Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 16–23, Sydney, July 2006. c�2006 Association for Computational Linguistics garbage text. Second, questions are often paired with answers separately. I.e. there is often one question in one QA pair. There were some research groups who divided questions into segments. Soricut and Brill (2004) chunked questions and used them as queries to search engines. Saquete et al. (2004) focused on decomposition of a complex question into several sub-questions. In this paper, question segmentation is to identify different questions posed in one posting. 2 Garbage Text Removal 2.1 Garbage Texts Articles in discussion groups are colloquial. Users often write articles as if they are talking to other users. For this reason, phrases expressing appreciation, begging, or emotions of writers are often seen in the postings. For example: 有關 powerpoint 問題 我想請問一下 1 該如何把 access 的整個視窗放到簡報上撥放 謝謝2 (About Powerpoint, I’d like to ask1, how to put the whole window seen in Access onto a slide? T</context>
</contexts>
<marker>Saquete, Martinez-Barco, Munoz, Luis, 2004</marker>
<rawString>Saquete, Estela, Patricio Martinez-Barco, Rafael Munoz, and Jose Luis Vicedo Gonzalez (2004) “Splitting Complex Temporal Questions for Question Answering Systems,” Proceedings of ACL 2004, pp. 566-573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soricut</author>
</authors>
<title>Radu and Eric Brill</title>
<date>2004</date>
<booktitle>Proceedings of HLT-NAACL</booktitle>
<pages>57--64</pages>
<marker>Soricut, 2004</marker>
<rawString>Soricut, Radu and Eric Brill (2004) “Automatic Question Answering: Beyond the Factoid,” Proceedings of HLT-NAACL 2004, pp. 57-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>Wee Sun Lee</author>
</authors>
<title>Question Classification using Support Vector Machines,”</title>
<date>2003</date>
<booktitle>Proceedings of SIGIR</booktitle>
<pages>26--32</pages>
<contexts>
<context position="12411" citStr="Zhang and Lee, 2003" startWordPosition="2027" endWordPosition="2030"> of the words appearing in both X and Y. Fragments with similarity higher than a threshold are merged together. The weight of a word is designed as the weight of its part-of-speech as listed in Table 3. Nouns and verbs have higher weights, while adverbs and particles have lower weights. Note that foreign words are assigned a rather high weight, because names sitive Verb) 80 A (Adjective) 40 ADV (Adverb), ASP (Tense), C (Connective), DET (Determiner), 0 P (Preposition), T (Particle) (4) Merging Questions with the Same Type The information of question type has been widely adopted in QA systems (Zhang and Lee, 2003; Hovy et al., 2002; Harabagiu et al., 2001). Question type often refers to the possible type of its answer, such as a person name, a location name, or a temporal expression. The question types used in this paper are PERSON, LOCATION, REASON, QUANTITY, TEMPORAL, COMPARISON, DEFINITION, METHOD, SELECTION, YESNO, and OTHER. Rules to determine question types are created manually. This strategy tries to merge two question fragments of the same question type. This paper proposes two features to determine the threshold to merge two question fragments: length and sum of term weights of a fragment. Le</context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>Zhang, Dell and Wee Sun Lee (2003) “Question Classification using Support Vector Machines,” Proceedings of SIGIR 2003, pp. 26-32.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>