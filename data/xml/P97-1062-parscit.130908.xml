<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000024">
<title confidence="0.9980385">
Learning Parse and Translation Decisions
From Examples With Rich Context
</title>
<author confidence="0.991276">
Ulf Hermjakob and Raymond J. Mooney
</author>
<affiliation confidence="0.998544">
Dept. of Computer Sciences
University of Texas at Austin
</affiliation>
<address confidence="0.75721">
Austin, TX 78712, USA
</address>
<email confidence="0.999459">
ulf@cs.utexas.edu mooney@cs.utexas.edu
</email>
<sectionHeader confidence="0.997395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987666666667">
We present a knowledge and context-based
system for parsing and translating natu-
ral language and evaluate it on sentences
from the Wall Street Journal. Applying
machine learning techniques, the system
uses parse action examples acquired un-
der supervision to generate a determinis-
tic shift-reduce parser in the form of a de-
cision structure. It relies heavily on con-
text, as encoded in features which describe
the morphological, syntactic, semantic and
other aspects of a given parse state.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951953488372">
The parsing of unrestricted text, with its enormous
lexical and structural ambiguity, still poses a great
challenge in natural language processing. The tradi-
tional approach of trying to master the complexity of
parse grammars with hand-coded rules turned out to
be much more difficult than expected, if not impos-
sible. Newer statistical approaches with often only
very limited context sensitivity seem to have hit a
performance ceiling even when trained on very large
corpora.
To cope with the complexity of unrestricted text,
parse rules in any kind of formalism will have to
consider a complex context with many different mor-
phological, syntactic or semantic features. This can
present a significant problem, because even linguisti-
cally trained natural language developers have great
difficulties writing and even more so extending ex-
plicit parse grammars covering a wide range of nat-
ural language. On the other hand it is much easier
for humans to decide how specific sentences should
be analyzed.
We therefore propose an approach to parsing
based on learning from examples with a very strong
emphasis on context, integrating morphological,
syntactic, semantic and other aspects relevant to
making good parse decisions, thereby also allowing
the parsing to be deterministic. Applying machine
learning techniques, the system uses parse action ex-
amples acquired under supervision to generate a de-
terministic shift-reduce type parser in the form of a
decision structure. The generated parser transforms
input sentences into an integrated phrase-structure
and case-frame tree, powerful enough to be fed into
a transfer and a generation module to complete the
full process of machine translation.
Balanced by rich context and some background
knowledge, our corpus based approach relieves the
NL-developer from the hard if not impossible task of
writing explicit grammar rules and keeps grammar
coverage increases very manageable. Compared with
standard statistical methods, our system relies on
deeper analysis and more supervision, but radically
fewer examples.
</bodyText>
<sectionHeader confidence="0.975685" genericHeader="method">
2 Basic Parsing Paradigm
</sectionHeader>
<bodyText confidence="0.999988263157895">
As the basic mechanism for parsing text into a
shallow semantic representation, we choose a shift-
reduce type parser (Marcus, 1980). It breaks parsing
into an ordered sequence of small and manageable
parse actions such as shift and reduce. This ordered
`left-to-right&apos; parsing is much closer to how humans
parse a sentence than, for example, chart oriented
parsers; it allows a very transparent control struc-
ture and makes the parsing process relatively intu-
itive for humans. This is very important, because
during the training phase, the system is guided by a
human supervisor for whom the flow of control needs
to be as transparent and intuitive as possible.
The parsing does not have separate phases for
part-of-speech selection and syntactic and semantic
processing, but rather integrates all of them into a
single parsing phase. Since the system has all mor-
phological, syntactic and semantic context informa-
tion available at all times, the system can make well-
</bodyText>
<page confidence="0.996915">
482
</page>
<bodyText confidence="0.9994317">
based decisions very early, allowing a single path, i.e.
deterministic parse, which eliminates wasting com-
putation on &apos;dead end&apos; alternatives.
Before the parsing itself starts, the input string
is segmented into a list of words incl. punctuation
marks, which then are sent through a morphological
analyzer that, using a lexicon&apos;, produces primitive
frames for the segmented words. A word gets a prim-
itive frame for each possible part of speech. (Mor-
phological ambiguity is captured within a frame.)
</bodyText>
<figureCaption confidence="0.96661">
Figure 1: Example of a parse action (simplified);
boxes represent frames
</figureCaption>
<bodyText confidence="0.9779178">
The central data structure for the parser consists
of a parse stack and an input list. The parse stack
and the input list contain trees of frames of words
or phrases. Core slots of frames are surface and lexi-
cal form, syntactic and semantic category, subframes
with syntactic and semantic roles, and form restric-
&apos;The lexicon provides part-of-speech information and
links words to concepts, as used in the KB (see next
section). Additional information includes irregular forms
and grammatical gender etc. (in the German lexicon).
</bodyText>
<figure confidence="0.994972555555556">
&amp;quot;John bought a new computer science book
today.&amp;quot;:
synt/sem: S-SNT/I-EV-BUY
forms: (3rd_person sing past_tense)
lex: &amp;quot;buy&amp;quot;
subs:
(SUBJ AGENT) &amp;quot;John&amp;quot;:
synt/sem: S-NP/I-EN-JOHN
(FRED) &amp;quot;John&amp;quot;
synt/sem: S-NOUN/I-EN-JOHN
(FRED) &amp;quot;bought&amp;quot;:
synt/sem: S-TR-VERB/I-EV-BUY
(OBJ THEME) &amp;quot;a new computer science book&amp;quot;:
synt/sem: S-NP/I-EN-BOOK
(DET) &amp;quot;a&amp;quot;
(MOD) &amp;quot;new&amp;quot;
(FRED) &amp;quot;computer science book&amp;quot;
(MOD) &amp;quot;computer science&amp;quot;
(MOD) &amp;quot;computer&amp;quot;
(FRED) &amp;quot;science&amp;quot;
(FRED) &amp;quot;book&amp;quot;
(TIME) &amp;quot;today&amp;quot;:
synt/sem: S-ADV/C-AT-TIME
(FRED) &amp;quot;today&amp;quot;
synt/sem: S-ADV/I-EADV-TODAY
(DUMMY) &amp;quot;.&amp;quot;:
synt: D-PERIOD
</figure>
<figureCaption confidence="0.999965">
Figure 2: Example of a parse tree (simplified).
</figureCaption>
<bodyText confidence="0.991260961538462">
tions such as number, person, and tense. Optional
slots include special information like the numerical
value of number words.
Initially, the parse stack is empty and the input
list contains the primitive frames produced by the
morphological analyzer. After initialization, the de-
terministic parser applies a sequence of parse actions
to the parse structure. The most frequent parse ac-
tions are shift, which shifts a frame from the input
list onto the parse stack or backwards, and reduce,
which combines one or several frames on the parse
stack into one new frame. The frames to be com-
bined are typically, but not necessarily, next to each
other at the top of the stack. As shown in figure 1,
the action
(Ft 2 TO VP AS FRED (OBJ PAT) )
for example reduces the two top frames of the stack
into a new frame that is marked as a verb phrase
and contains the next-to-the-top frame as its pred-
icate (or head) and the top frame of the stack as
its object and patient. Other parse actions include
add-into, which adds frames arbitrarily deep into an
existing frame tree, mark, which can mark any slot
of any frame with any value, and operations to in-
troduce empty categories (i.e. traces and &apos;PRO&apos;, as
in &amp;quot;Shea wanted PROi to win.&amp;quot;). Parse actions can
</bodyText>
<figure confidence="0.99330205">
parse stack top of top of
stack list
&amp;quot;input list ›-
&amp;quot;John&amp;quot; &amp;quot;bought&amp;quot; &amp;quot;a book&amp;quot; &amp;quot;today&amp;quot;
synt: np synt: verb synt: np synt: adv
(R 2 TO S-VP AS PRED (OBJ PAT))
&amp;quot;reduce the 2 top elements of the parse stack
to a frame with syntax &apos;VP&apos;
and roles `pred&apos; and `obj and pat&amp;quot;
&amp;quot;today&amp;quot;
synt: adv
&amp;quot;bought&amp;quot;
synt: verb
&amp;quot;a book&amp;quot;
synt: np
&amp;quot;John&amp;quot;
synt: np
&amp;quot;bought a book&amp;quot;
synt: vp
Iii sub: (pred) (obj pat)
</figure>
<page confidence="0.999397">
483
</page>
<bodyText confidence="0.999977">
have numerous arguments, making the parse action
language very powerful.
The parse action sequences needed for training the
system are acquired interactively. For each train-
ing sentence, the system and the supervisor parse
the sentence step by step, with the supervisor enter-
ing the next parse action, e.g. (R 2 TO VP AS PRED
(OBJ PAT)), and the system executing it, repeating
this sequence until the sentence is fully parsed. At
least for the very first sentence, the supervisor actu-
ally has to type in the entire parse action sequence.
With a growing number of parse action examples
available, the system, as described below in more de-
tail, can be trained using those previous examples.
In such a partially trained system, the parse actions
are then proposed by the system using a parse deci-
sion structure which &amp;quot;classifies&amp;quot; the current context.
The proper classification is the specific action or se-
quence of actions that (the system believes) should
be performed next. During further training, the su-
pervisor then enters parse action commands by ei-
ther confirming what the system proposes or overrul-
ing it by providing the proper action. As the corpus
of parse examples grows and the system is trained
on more and more data, the system becomes more
refined, so that the supervisor has to overrule the
system with decreasing frequency. The sequence of
correct parse actions for a sentence is then recorded
in a log file.
</bodyText>
<sectionHeader confidence="0.997966" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.983058541666667">
To make good parse decisions, a wide range of fea-
tures at various degrees of abstraction have to be
considered. To express such a wide range of fea-
tures, we defined a feature language. Parse features
can be thought of as functions that map from par-
tially parsed sentences to a value. Applied to the
target parse state of figure 1, the feature (SYNT
OF OBJ OF -1 AT S-SYNT-ELEM), for example,
designates the general syntactic class of the object
of the first frame of the parse stack&apos;, in our example
np3. So, features do not a priori operate on words or
phrases, but only do so if their description references
such words or phrases, as in our example through the
path &apos;OBJ OF -1&apos;.
Given a particular parse state and a feature, the
system can interpret the feature and compute its
2S-SYNT-ELEM designates the top syntactic level;
since -1 is negative, the feature refers to the 1st frame
of the parse stack. Note that the top of stack is at the
right end for the parse stack.
&apos;If a feature is not defined in a specific parse state, the
feature interpreter assigns the special value unavailable.
value for the given parse state, often using additional
background knowledge such as
</bodyText>
<listItem confidence="0.966737">
1. A knowledge base (KB), which currently con-
sists of a directed acyclic graph of 4356 mostly
semantic and syntactic concepts connected by
4518 is-a links, e.g. &amp;quot;booknoun-concept is-a
</listItem>
<bodyText confidence="0.999869">
tangible — objectnoun-concept&amp;quot;• Most concepts
representing words are at a fairly shallow level
of the KB, e.g. under &apos;tangible object&apos;, &apos;ab-
stract&apos;, &apos;process verb&apos;, or &apos;adjective&apos;, with more
depth used only in concept areas more relevant
for making parse and translation decisions, such
as temporal, spatial and animate concepts.4
</bodyText>
<listItem confidence="0.820595">
2. A subcategorization table that describes the syn-
</listItem>
<bodyText confidence="0.908501666666667">
tactic and semantic role structures for verbs,
with currently 242 entries.
The following representative examples, for easier
understanding rendered in English and not in fea-
ture language syntax, further illustrate the expres-
siveness of the feature language:
</bodyText>
<listItem confidence="0.997222533333333">
• the general syntactic class of frame...3 (the
third element of the parse stack): e.g. verb, adj,
np,
• whether or not the adverbial alternative of
framei (the top element of the input list) is
an adjectival degree adverb,
• the specific finite tense of frame_i, e.g. present
tense,
• whether or not frame_i contains an object,
• the semantic role of frame_i with respect to
frame_2: e.g. agent, time; this involves pattern
matching with corresponding entries in the verb
subcategorization table,
• whether or not frame_2 and frame_ I satisfy
subject-verb agreement.
</listItem>
<subsectionHeader confidence="0.422719">
Features can in principal refer to any one or sev-
</subsectionHeader>
<bodyText confidence="0.987317555555555">
eral elements on the parse stack or input list, and
any of their subelements, at any depth. Since the
currently 205 features are supposed to bear some
linguistic relevance, none of them are unjustifiably
remote from the current focus of a parse state.
The feature collection is basically independent
from the supervised parse action acquisition. Before
learning a decision structure for the first time, the
supervisor has to provide an initial set of features
</bodyText>
<footnote confidence="0.712402">
4Supported by acquisition tools, word/concept pairs
are typically entered into the lexicon and the KB at the
same time, typically requiring less than a minute per
word or group of closely related words.
</footnote>
<page confidence="0.997191">
484
</page>
<figure confidence="0.702131625">
done-operation-p
tree
reduce-operation-p
tree
shift-in-operation-p
tree
reduce 1... reduce 3...
shift -art
,
shift s-noun shift s-verb
uce ol.er
oth r redu
-----
START
do
do e do e oth _o_ er - re
</figure>
<figureCaption confidence="0.999985">
Figure 3: Example of a hybrid decision structure
</figureCaption>
<bodyText confidence="0.999859739130435">
that can be considered obviously relevant. Partic-
ularly during the early development of our system,
this set was increased whenever parse examples had
identical values for all current features but neverthe-
less demanded different parse actions. Given a spe-
cific conflict pair of partially parsed sentences, the
supervisor would add a new relevant feature that dis-
criminates the two examples. We expect our feature
set to grow to eventually about 300 features when
scaling up further within the Wall Street Journal do-
main, and quite possibly to a higher number when
expanding into new domains. However, such feature
set additions require fairly little supervisor effort.
Given (1) a log file with the correct parse action
sequence of training sentences as acquired under su-
pervision and (2) a set of features, the system revis-
its the training sentences and computes values for
all features at each parse step. Together with the
recorded parse actions these feature vectors form
parse examples that serve as input to the learning
unit. Whenever the feature set is modified, this step
must be repeated, but this is unproblematic, because
this process is both fully automatic and fast.
</bodyText>
<sectionHeader confidence="0.981719" genericHeader="method">
4 Learning Decision Structures
</sectionHeader>
<bodyText confidence="0.999984380952381">
Traditional statistical techniques also use features,
but often have to sharply limit their number (for
some trigram approaches to three fairly simple fea-
tures) to avoid the loss of statistical significance.
In parsing, only a very small number of features
are crucial over a wide range of examples, while
most features are critical in only a few examples,
being used to &apos;fine-tune&apos; the decision structure for
special cases. So in order to overcome the antago-
nism between the importance of having a large num-
ber of features and the need to control the num-
ber of examples required for learning, particularly
when acquiring parse action sequence under super-
vision, we choose a decision-tree based learning al-
gorithm, which recursively selects the most discrim-
inating feature of the corresponding subset of train-
ing examples, eventually ignoring all locally irrele-
vant features, thereby tailoring the size of the final
decision structure to the complexity of the training
data.
While parse actions might be complex for the ac-
tion interpreter, they are atomic with respect to the
decision structure learner; e.g. &amp;quot;(R 2 TO VP AS
PRED (OBJ PAT))&amp;quot; would be such an atomic clas-
sification. A set of parse examples, as already de-
scribed in the previous section, is then fed into an
1D3-based learning routine that generates a deci-
sion structure, which can then &apos;classify&apos; any given
parse state by proposing what parse action to per-
form next.
We extended the standard ID3 model (Quinlan,
1986) to more general hybrid decision structures.
In our tests, the best performing structure was a
decision list (Rivest, 1987) of hierarchical decision
trees, whose simplified basic structure is illustrated
in figure 3. Note that in the &apos;reduce operation tree&apos;,
the system first decides whether or not to perform
a reduction before deciding on a specific reduction.
Using our knowledge of similarity of parse actions
and the exceptionality vs. generality of parse action
groups, we can provide an overhead structure that
helps prevent data fragmentation.
</bodyText>
<page confidence="0.998304">
485
</page>
<sectionHeader confidence="0.937279" genericHeader="method">
5 Transfer and Generation
</sectionHeader>
<bodyText confidence="0.999611342857143">
The output tree generated by the parser can be used
for translation. A transfer module recursively maps
the source language parse tree to an equivalent tree
in the target language, reusing the methods devel-
oped for parsing with only minor adaptations. The
main purpose of learning here is to resolve trans-
lation ambiguities, which arise for example when
translating the English &amp;quot;to know&amp;quot; to German (wis-
sen/kennen) or Spanish (saber/conocer).
Besides word pair entries, the bilingual dictionary
also contains pairs of phrases and expressions in a
format closely resembling traditional (paper) dictio-
naries, e.g. &amp;quot;to comment on SOMETHING_1&amp;quot; / &amp;quot;sich
zu ETWAS_DATA auf3ern&amp;quot;. Even if a complex
translation pair does not bridge a structural mis-
match, it can make a valuable contribution to dis-
ambiguation. Consider for example the term &amp;quot;inter-
est rate&amp;quot;. Both element nouns are highly. ambigu-
ous with respect to German, but the English com-
pound conclusively maps to the German compound
&amp;quot;Zinssatz&amp;quot;. We believe that an extensive collection
of complex translation pairs in the bilingual dictio-
nary is critical for translation quality and we are
confident that its acquisition can be at least partially
automated by using techniques like those described
in (Smadja et al., 1996). Complex translation en-
tries are preprocessed using the same parser as for
normal text. During the transfer process, the result-
ing parse tree pairs are then accessed using pattern
matching.
The generation module orders the components of
phrases, adds appropriate punctuation, and propa-
gates morphologically relevant information in order
to compute the proper form of surface words in the
target language.
</bodyText>
<sectionHeader confidence="0.984709" genericHeader="method">
6 Wall Street Journal Experiments
</sectionHeader>
<bodyText confidence="0.999302266666667">
We now present intermediate results on training
and testing a prototype implementation of the sys-
tem with sentences from the Wall Street Journal, a
prominent corpus of &apos;real&apos; text, as collected on the
ACL-CD.
In order to limit the size of the required lexicon,
we work on a reduced corpus of 105,356 sentences,
a tenth of the full corpus, that includes all those
sentences that are fully covered by the 3000 most
frequently occurring words (ignoring numbers etc.)
in the entire corpus. The first 272 sentences used in
this experiment vary in length from 4 to 45 words,
averaging at 17.1 words and 43.5 parse actions per
sentence. One of these sentence is &amp;quot;Canadian man-
ufacturers&apos; new orders fell to $20.80 billion (Cana-
</bodyText>
<table confidence="0.9997115625">
Tr. snt. 16 32 64 128 256
Prec. 85.1% 86.6% 87.7% 90.4% 92.7%
Recall 82.8% 85.3% 87.7% 89.9% 92.8%
L. pr. 77.2% 80.4% 82.5% 86.6% 89.8%
L. rec. 75.0% 77.7% 81.6% 85.3% 89.6%
Tagging 96.6% 96.5% 97.1% 97.5% 98.4%
Cr/snt 2.5 2.1 1.9 1.3 1.0
0 cr 27.6% 35.3% 35.7% 50.4% 56.3%
&lt; 1 cr 44.1% 50.7% 54.8% 68.4% 73.5%
&lt; 2 cr 61.4% 65.1% 66.9% 80.9% 84.9%
&lt; 3 cr 72.4% 77.2% 79.4% 87.1% 93.0%
&lt; 4 cr 84.9% 86.4% 89.7% 93.0% 94.9%
Ops 79.1% 82.9% 86.8% 89.1% 91.7%
OpSeq 1.8% 4.4% 5.9% 10.7% 16.5%
Str&amp;L 5.5% 8.8% 10.3% 18.8% 26.8%
Loops 13 6 0 1 1
</table>
<tableCaption confidence="0.999374">
Table 1: Evaluation results with varying number of
</tableCaption>
<bodyText confidence="0.933017166666667">
training sentences; with all 205 features and hybrid
decision structure; Train. = number of training sen-
tences; pr/prec. = precision; rec. = recall; I. = la-
beled; Tagging = tagging accuracy; Cr/snt = cross-
ings per sentence; Ops = correct operations; OpSeq
= Operation Sequence
</bodyText>
<figure confidence="0.98054275">
labeled precision
95% ---
90%
85%
80%
75% I I I I I I I
16 32 64 128 256 512 1024
number of training sentences
</figure>
<figureCaption confidence="0.9992885">
Figure 4: Learning curve for labeled precision in ta-
ble 1.
</figureCaption>
<bodyText confidence="0.989897">
than) in January, down 4% from December&apos;s $21.67
billion billion on a seasonally adjusted basis, Statis-
tics Canada, a federal agency, said.&amp;quot;.
For our parsing test series, we use 17-fold cross-
validation. The corpus of 272 sentences that cur-
rently have parse action logs associated with them
is divided into 17 blocks of 16 sentences each. The 17
blocks are then consecutively used for testing. For
each of the 17 sub-tests, a varying number of sen-
tences from the other blocks is used for training the
parse decision structure, so that within a sub-test,
none of the training sentences are ever used as a test
sentence. The results of the 17 sub-tests of each se-
ries are then averaged.
</bodyText>
<page confidence="0.997923">
486
</page>
<table confidence="0.99987775">
Features 6 25 50 100 205
Prec. 88.0% 88.7% 90.8% 91.7% 92.7%
Recall 87.3% 88.7% 90.8% 91.7% 92.8%
L. pr. 79.8% 86.7% 87.2% 88.6% 89.8%
L. rec. 81.5% 84.1% 86.9% 88.1% 89.6%
Tagging 97.6% 97.9% 98.1% 98.2% 98.4%
Cr/snt 1.8 1.7 1.3 1.1 1.0
0 cr 39.0% 43.4% 50.4% 54.0% 56.3%
&lt; 1 cr 57.4% 59.6% 70.6% 72.1% 73.5%
&lt; 2 cr 72.1% 73.9% 80.5% 84.2% 84.9%
&lt; 3 cr 82.7% 84.9% 88.6% 92.3% 93.0%
&lt; 4 cr 89.0% 89.7% 93.8% 94.5% 94.9%
Ops 80.6% 81.9% 88.9% 90.7% 91.7%
OpSeq 2.6% 1.5% 8.8% 13.6% 16.5%
Str&amp;L 8.8% 9.2% 15.1% 23.5% 26.8%
Loops 8 2 2 2 1
</table>
<tableCaption confidence="0.984325">
Table 2: Evaluation results with varying number of
features; with 256 training sentences
</tableCaption>
<table confidence="0.290482">
Precision (pr.):
</table>
<bodyText confidence="0.953659">
number of correct constituents in system parse
number of constituents in system parse
</bodyText>
<subsectionHeader confidence="0.881899">
Recall (rec.):
</subsectionHeader>
<bodyText confidence="0.999248535714286">
number of correct constituents in system parse
number of constituents in logged parse
Crossing brackets (Cr): number of constituents
which violate constituent boundaries with a con-
stituent in the logged parse.
Labeled (/.) precision/recall measures not only
structural correctness, but also the correctness of
the syntactic label. Correct operations (Ops)
measures the number of correct operations during
a parse that is continuously corrected based on the
logged sequence. The correct operations ratio is im-
portant for example acquisition, because it describes
the percentage of parse actions that the supervisor
can confirm by just hitting the return key. A sen-
tence has a correct operating sequence (OpSeq),
if the system fully predicts the logged parse action
sequence, and a correct structure and labeling
(Str&amp;L), if the structure and syntactic labeling of
the final system parse of a sentence is 100% correct,
regardless of the operations leading to it.
The current set of 205 features was sufficient to
always discriminate examples with different parse
actions, resulting in a 100% accuracy on sentences
already seen during training. While that percentage
is certainly less important than the accuracy figures
for unseen sentences, it nevertheless represents an
important upper ceiling.
Many of the mistakes are due to encountering con-
</bodyText>
<table confidence="0.999503235294118">
Type of deci- plain hier. plain hybrid
sion structure list list tree tree
Precision 87.8% 91.0% 87.6% 92.7%
Recall 89.9% 88.2% 89.7% 92.8%
Lab. precision 28.6% 87.4% 38.5% 89.8%
Lab. recall 86.1% 84.7% 85.6% 89.6%
Tagging acc. 97.9% 96.0% 97.9% 98.4%
Crossings/snt 1.2 1.3 1.3 1.0
0 crossings 55.2% 52.9% 51.5% 56.3%
&lt; 1 crossings 72.8% 71.0% 65.8% 73.5%
&lt; 2 crossings 82.7% 82.7% 81.6% 84.9%
&lt; 3 crossings 89.0% 89.0% 90.1% 93.0%
&lt; 4 crossings 93.4% 93.4% 93.4% 94.9%
Ops 86.5% 90.3% 90.2% 91.7%
OpSeq 12.9% 11.8% 13.6% 16.5%
Str&amp;L 22.4% 22.8% 21.7% 26.8%
Endless loops 26 23 32 1
</table>
<tableCaption confidence="0.878398333333333">
Table 3: Evaluation results with varying types of
decision structures; with 256 training sentences and
205 features
</tableCaption>
<bodyText confidence="0.990288666666666">
structions that just have not been seen before at all,
typically causing several erroneous parse decisions in
a row. This observation further supports our expec-
tation, based on the results shown in table 1 and fig-
ure 4, that with more training sentences, the testing
accuracy for unseen sentences will still rise signifi-
cantly.
Table 2 shows the impact of reducing the feature
set to a set of N core features. While the loss of a few
specialized features will not cause a major degrada-
tion, the relatively high number of features used in
our system finds a clear justification when evaluating
compound test characteristics, such as the number
of structurally completely correct sentences. When
25 or fewer features are used, all of them are syn-
tactic. Therefore the 25 feature test is a relatively
good indicator for the contribution of the semantic
knowledge base.
In another test, we deleted all 10 features relating
to the subcategorization table and found that the
only metrics with degrading values were those mea-
suring semantic role assignment; in particular, none
of the precision, recall and crossing bracket values
changed significantly. This suggests that, at least in
the presence of other semantic features, the subcat-
egorization table does not play as critical a role in
resolving structural ambiguity as might have been
expected.
Table 3 compares four different machine learning
variants: plain decision lists, hierarchical decision
</bodyText>
<page confidence="0.996726">
487
</page>
<bodyText confidence="0.995704">
lists, plain decision trees and a hybrid structure,
namely a decision list of hierarchical decision trees,
as sketched in figure 3. The results show that ex-
tensions to the basic decision tree model can signif-
icantly improve learning results.
</bodyText>
<table confidence="0.99842">
System Syntax Semantics
Human translation 1.18 1.41
CONTEX on correct parse 2.20 2.19
CONTEX (full translation) 2.36 2.38
Logos 2.57 3.24
SYSTRAN 2.68 3.35
Globalink 3.30 3.83
</table>
<tableCaption confidence="0.894795">
Table 4: Translation evaluation results (best possi-
ble = 1.00, worst possible = 6.00)
</tableCaption>
<bodyText confidence="0.973678722222222">
Table 4 summarizes the evaluation results of
translating 32 randomly selected sentences from our
Wall Street Journal corpus from English to German.
Besides our system, CONTEX, we tested three com-
mercial systems, Logos, SYSTRAN, and Globalink.
In order to better assess the contribution of the
parser, we also added a version that let our system
start with the correct parse, effectively just testing
the transfer and generation module. The resulting
translations, in randomized order and without iden-
tification, were evaluated by ten bilingual graduate
students, both native German speakers living in the
U.S. and native English speakers teaching college
level German. As a control, half of the evaluators
were also given translations by a bilingual human.
Note that the translation results using our parser
are fairly close to those starting with a correct parse.
This means that the errors made by the parser
have had a relatively moderate impact on transla-
tion quality. The transfer and generation modules
were developed and trained based on only 48 sen-
tences, so we expect a significant translation quality
improvement by further development of those mod-
ules.
Our system performed better than the commercial
systems, but this has to be interpreted with caution,
since our system was trained and tested on sentences
from the same lexically limited corpus (but of course
without overlap), whereas the other systems were
developed on and for texts from a larger variety of
domains, making lexical choices more difficult in par-
ticular.
Table 5 shows the correlation between various
parse and translation metrics. Labeled precision has
the strongest correlation with both the syntactic and
semantic translation evaluation grades.
</bodyText>
<table confidence="0.999220666666667">
Metric Syntax Semantics
Precision -0.63 -0.63
Recall -0.64 -0.66
Labeled precision -0.75 -0.78
Labeled recall -0.65 -0.65
Tagging accuracy -0.66 -0.56
Number of crossing brackets 0.58 0.54
Operations -0.45 -0.41
Operation sequence -0.39 -0.36
</table>
<tableCaption confidence="0.99892">
Table 5: Correlation between various parse and
</tableCaption>
<bodyText confidence="0.8600355">
translation metrics. Values near -1.0 or 1.0 indi-
cate very strong correlation, whereas values near 0.0
indicate a weak or no correlation. Most correlation
values, incl. for labeled precision are negative, be-
cause a higher (better) labeled precision correlates
with a numerically lower (better) translation score
on the 1.0 (best) to 6.0 (worst) translation evalua-
tion scale.
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999947225806451">
Our basic parsing and interactive training paradigm
is based on (Simmons and Yu, 1992). We have
extended their work by significantly increasing the
expressiveness of the parse action and feature lan-
guages, in particular by moving far beyond the few
simple features that were limited to syntax only, by
adding more background knowledge and by intro-
ducing a sophisticated machine learning component.
(Magerman, 1995) uses a decision tree model sim-
ilar to ours, training his system SPATTER with parse
action sequences for 40,000 Wall Street Journal sen-
tences derived from the Penn Treebank (Marcus
et al., 1993). Questioning the traditional n-grams,
Magerman already advocates a heavier reliance on
contextual information. Going beyond Magerman&apos;s
still relatively rigid set of 36 features, we propose a
yet richer, basically unlimited feature language set.
Our parse action sequences are too complex to be
derived from a treebank like Penn&apos;s. Not only do
our parse trees contain semantic annotations, roles
and more syntactic detail, we also rely on the more
informative parse action sequence. While this neces-
sitates the involvement of a parsing supervisor for
training, we are able to perform deterministic pars-
ing and get already very good test results for only
256 training sentences.
(Collins, 1996) focuses on bigram lexical depen-
dencies (BLD). Trained on the same 40,000 sen-
tences as Spatter, it relies on a much more limited
type of context than our system and needs little
background knowledge.
</bodyText>
<page confidence="0.995093">
488
</page>
<table confidence="0.998871333333333">
Model SPATTER BLD CONTEX
Labeled precision 84.9% 86.3% 89.8%
Labeled recall 84.6% 85.8% 89.6%
Crossings/sentence 1.26 1.14 1.02
Sent. with 0 cr. 56.6% 59.9% 56.3%
Sent. with &lt; 2 cr. 81.4% 83.6% 84.9%
</table>
<tableCaption confidence="0.6487876">
Table 6: Comparing our system CONTEX with
Magerman&apos;s SPATTER and Collins&apos; BLD; results for
SPATTER and BLD are for sentences of up to 40
words.
Table 6 compares our results with SPATTER and
</tableCaption>
<bodyText confidence="0.9960595">
BLD. The results have to be interpreted cautiously
since they are not based on the exact same sentences
and detail of bracketing. Due to lexical restrictions,
our average sentence length (17.1) is below the one
used in SPATTER and BLD (22.3), but some of our
test sentences have more than 40 words; and while
the Penn Treebank leaves many phrases such as &amp;quot;the
New York Stock Exchange&amp;quot; without internal struc-
ture, our system performs a complete bracketing,
thereby increasing the risk of crossing brackets.
</bodyText>
<sectionHeader confidence="0.997955" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9796906">
We try to bridge the gap between the typically hard-
to-scale hand-crafted approach and the typically
large-scale but context-poor statistical approach for
unrestricted text parsing.
Using
</bodyText>
<listItem confidence="0.9923329">
• a rich and unified context with 205 features,
• a complex parse action language that allows in-
tegrated part of speech tagging and syntactic
and semantic processing,
• a sophisticated decision structure that general-
izes traditional decision trees and lists,
• a balanced use of machine learning and micro-
modular background knowledge, i.e. very small
pieces of highly independent information
• a modest number of interactively acquired ex-
amples from the Wall Street Journal,
our system CONTEX
• computes parse trees and translations fast, be-
cause it uses a deterministic single-pass parser,
• shows good robustness when encountering novel
constructions,
• produces good parsing results comparable to
those of the leading statistical methods, and
• delivers competitive results for machine trans-
lations.
</listItem>
<bodyText confidence="0.9987183">
While many limited-context statistical approaches
have already reached a performance ceiling, we still
expect to significantly improve our results when in-
creasing our training base beyond the currently 256
sentences, because the learning curve hasn&apos;t flat-
tened out yet and adding substantially more exam-
ples is still very feasible. Even then the training
size will compare favorably with the huge number
of training sentences necessary for many statistical
systems.
</bodyText>
<sectionHeader confidence="0.999238" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99950311627907">
E. Black, J. Lafferty, and S. Roukos. 1992. Devel-
opment and evaluation of a broad-coverage prob-
abilistic grammar of English-language computer
manuals. In 30th Proceedings of the ACL, pages
185-192.
M. J. Collins. 1996. A New Statistical Parser Based
on Bigram Lexical Dependencies. In 34th Proceed-
ings of the ACL, pages 184-191.
U. Hermjakob. 1997. Learning Parse and Trans-
lation Decisions From Examples With Rich Con-
text. Ph.D. thesis, University of Texas at
Austin, Dept. of Computer Sciences TR 97-12.
file://ftp.cs.utexas.edu/pub/mooney/papers/herm
jakob-dissertation-97.ps.Z
D. M. Magerman. 1995. Statistical Decision-Tree
Models for Parsing In 33rd Proceedings of the
ACL, pages 276-283.
M. P. Marcus. 1980. A Theory of Syntactic Recog-
nition for Natural Language. MIT Press.
M. P. Marcus, B. Santorini, and M. A. Marcinkie-
wicz. 1993. Building a Large Annotated Corpus
of English: The Penn Treebank. In Computa-
tional Linguistics 19 (2), pages 184-191.
S. Nirenburg, J. Carbonell, M. Tomita, and K.
Goodman. 1992. Machine Translation: A
Knowledge-Based Approach. San Mateo, CA:
Morgan Kaufmann.
J. R. Quinlan. 1986. Induction of decision trees. In
Machine Learning 1 (1), pages 81-106.
R. L. Rivest. 1987. Learning Decision Lists. In
Machine Learning 2, pages 229-246.
R. F. Simmons and Yeong-Ho Yu. 1992. The Acqui-
sition and Use of Context-Dependent Grammars
for English. In Computational Linguistics 18 (4),
pages 391-418.
F. Smadja, K. R. KcKeown and V. Hatzivassiloglou.
1996. Translating Collocations for Bilingual Lex-
icons: A Statistical Approach. In Computational
Linguistics 22 (1), pages 1-38.
Globalink. http://www.globalink.com/home.html
Oct. 1996.
Logos. http://www.logos-ca.com/ Oct. 1996.
SYSTRAN. http://systranmt.com/ Oct. 1996.
</reference>
<page confidence="0.999142">
489
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.656251">
<title confidence="0.999952">Learning Parse and Translation Decisions</title>
<author confidence="0.831763">From Examples With Rich Context Ulf Hermjakob</author>
<author confidence="0.831763">Raymond J Mooney</author>
<affiliation confidence="0.999944">Dept. of Computer Sciences University of Texas at Austin</affiliation>
<address confidence="0.999645">Austin, TX 78712, USA</address>
<email confidence="0.999865">ulf@cs.utexas.edumooney@cs.utexas.edu</email>
<abstract confidence="0.999096692307692">We present a knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal. Applying machine learning techniques, the system uses parse action examples acquired under supervision to generate a deterministic shift-reduce parser in the form of a decision structure. It relies heavily on context, as encoded in features which describe the morphological, syntactic, semantic and other aspects of a given parse state.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
<author>J Lafferty</author>
<author>S Roukos</author>
</authors>
<title>Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals.</title>
<date>1992</date>
<booktitle>In 30th Proceedings of the ACL,</booktitle>
<pages>185--192</pages>
<marker>Black, Lafferty, Roukos, 1992</marker>
<rawString>E. Black, J. Lafferty, and S. Roukos. 1992. Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals. In 30th Proceedings of the ACL, pages 185-192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>A New Statistical Parser Based on Bigram Lexical Dependencies.</title>
<date>1996</date>
<booktitle>In 34th Proceedings of the ACL,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="27993" citStr="Collins, 1996" startWordPosition="4539" endWordPosition="4540">iance on contextual information. Going beyond Magerman&apos;s still relatively rigid set of 36 features, we propose a yet richer, basically unlimited feature language set. Our parse action sequences are too complex to be derived from a treebank like Penn&apos;s. Not only do our parse trees contain semantic annotations, roles and more syntactic detail, we also rely on the more informative parse action sequence. While this necessitates the involvement of a parsing supervisor for training, we are able to perform deterministic parsing and get already very good test results for only 256 training sentences. (Collins, 1996) focuses on bigram lexical dependencies (BLD). Trained on the same 40,000 sentences as Spatter, it relies on a much more limited type of context than our system and needs little background knowledge. 488 Model SPATTER BLD CONTEX Labeled precision 84.9% 86.3% 89.8% Labeled recall 84.6% 85.8% 89.6% Crossings/sentence 1.26 1.14 1.02 Sent. with 0 cr. 56.6% 59.9% 56.3% Sent. with &lt; 2 cr. 81.4% 83.6% 84.9% Table 6: Comparing our system CONTEX with Magerman&apos;s SPATTER and Collins&apos; BLD; results for SPATTER and BLD are for sentences of up to 40 words. Table 6 compares our results with SPATTER and BLD. T</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>M. J. Collins. 1996. A New Statistical Parser Based on Bigram Lexical Dependencies. In 34th Proceedings of the ACL, pages 184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hermjakob</author>
</authors>
<title>Learning Parse and Translation Decisions From Examples With Rich Context.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Texas at Austin, Dept. of Computer Sciences</institution>
<note>file://ftp.cs.utexas.edu/pub/mooney/papers/herm jakob-dissertation-97.ps.Z</note>
<marker>Hermjakob, 1997</marker>
<rawString>U. Hermjakob. 1997. Learning Parse and Translation Decisions From Examples With Rich Context. Ph.D. thesis, University of Texas at Austin, Dept. of Computer Sciences TR 97-12. file://ftp.cs.utexas.edu/pub/mooney/papers/herm jakob-dissertation-97.ps.Z</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
</authors>
<title>Statistical Decision-Tree Models for Parsing</title>
<date>1995</date>
<booktitle>In 33rd Proceedings of the ACL,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="27106" citStr="Magerman, 1995" startWordPosition="4400" endWordPosition="4401">n are negative, because a higher (better) labeled precision correlates with a numerically lower (better) translation score on the 1.0 (best) to 6.0 (worst) translation evaluation scale. 7 Related Work Our basic parsing and interactive training paradigm is based on (Simmons and Yu, 1992). We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component. (Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993). Questioning the traditional n-grams, Magerman already advocates a heavier reliance on contextual information. Going beyond Magerman&apos;s still relatively rigid set of 36 features, we propose a yet richer, basically unlimited feature language set. Our parse action sequences are too complex to be derived from a treebank like Penn&apos;s. Not only do our parse trees contain semantic annotations, roles and more s</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>D. M. Magerman. 1995. Statistical Decision-Tree Models for Parsing In 33rd Proceedings of the ACL, pages 276-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2956" citStr="Marcus, 1980" startWordPosition="446" endWordPosition="447">nto a transfer and a generation module to complete the full process of machine translation. Balanced by rich context and some background knowledge, our corpus based approach relieves the NL-developer from the hard if not impossible task of writing explicit grammar rules and keeps grammar coverage increases very manageable. Compared with standard statistical methods, our system relies on deeper analysis and more supervision, but radically fewer examples. 2 Basic Parsing Paradigm As the basic mechanism for parsing text into a shallow semantic representation, we choose a shiftreduce type parser (Marcus, 1980). It breaks parsing into an ordered sequence of small and manageable parse actions such as shift and reduce. This ordered `left-to-right&apos; parsing is much closer to how humans parse a sentence than, for example, chart oriented parsers; it allows a very transparent control structure and makes the parsing process relatively intuitive for humans. This is very important, because during the training phase, the system is guided by a human supervisor for whom the flow of control needs to be as transparent and intuitive as possible. The parsing does not have separate phases for part-of-speech selection</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>M. P. Marcus. 1980. A Theory of Syntactic Recognition for Natural Language. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>In Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>184--191</pages>
<contexts>
<context position="27300" citStr="Marcus et al., 1993" startWordPosition="4431" endWordPosition="4434">elated Work Our basic parsing and interactive training paradigm is based on (Simmons and Yu, 1992). We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component. (Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993). Questioning the traditional n-grams, Magerman already advocates a heavier reliance on contextual information. Going beyond Magerman&apos;s still relatively rigid set of 36 features, we propose a yet richer, basically unlimited feature language set. Our parse action sequences are too complex to be derived from a treebank like Penn&apos;s. Not only do our parse trees contain semantic annotations, roles and more syntactic detail, we also rely on the more informative parse action sequence. While this necessitates the involvement of a parsing supervisor for training, we are able to perform deterministic pa</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. In Computational Linguistics 19 (2), pages 184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nirenburg</author>
<author>J Carbonell</author>
<author>M Tomita</author>
<author>K Goodman</author>
</authors>
<title>Machine Translation: A Knowledge-Based Approach.</title>
<date>1992</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA:</location>
<marker>Nirenburg, Carbonell, Tomita, Goodman, 1992</marker>
<rawString>S. Nirenburg, J. Carbonell, M. Tomita, and K. Goodman. 1992. Machine Translation: A Knowledge-Based Approach. San Mateo, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Induction of decision trees.</title>
<date>1986</date>
<booktitle>In Machine Learning</booktitle>
<volume>1</volume>
<issue>1</issue>
<pages>81--106</pages>
<contexts>
<context position="14809" citStr="Quinlan, 1986" startWordPosition="2398" endWordPosition="2399">atures, thereby tailoring the size of the final decision structure to the complexity of the training data. While parse actions might be complex for the action interpreter, they are atomic with respect to the decision structure learner; e.g. &amp;quot;(R 2 TO VP AS PRED (OBJ PAT))&amp;quot; would be such an atomic classification. A set of parse examples, as already described in the previous section, is then fed into an 1D3-based learning routine that generates a decision structure, which can then &apos;classify&apos; any given parse state by proposing what parse action to perform next. We extended the standard ID3 model (Quinlan, 1986) to more general hybrid decision structures. In our tests, the best performing structure was a decision list (Rivest, 1987) of hierarchical decision trees, whose simplified basic structure is illustrated in figure 3. Note that in the &apos;reduce operation tree&apos;, the system first decides whether or not to perform a reduction before deciding on a specific reduction. Using our knowledge of similarity of parse actions and the exceptionality vs. generality of parse action groups, we can provide an overhead structure that helps prevent data fragmentation. 485 5 Transfer and Generation The output tree ge</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>J. R. Quinlan. 1986. Induction of decision trees. In Machine Learning 1 (1), pages 81-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Rivest</author>
</authors>
<title>Learning Decision Lists.</title>
<date>1987</date>
<booktitle>In Machine Learning 2,</booktitle>
<pages>229--246</pages>
<contexts>
<context position="14932" citStr="Rivest, 1987" startWordPosition="2417" endWordPosition="2418">ns might be complex for the action interpreter, they are atomic with respect to the decision structure learner; e.g. &amp;quot;(R 2 TO VP AS PRED (OBJ PAT))&amp;quot; would be such an atomic classification. A set of parse examples, as already described in the previous section, is then fed into an 1D3-based learning routine that generates a decision structure, which can then &apos;classify&apos; any given parse state by proposing what parse action to perform next. We extended the standard ID3 model (Quinlan, 1986) to more general hybrid decision structures. In our tests, the best performing structure was a decision list (Rivest, 1987) of hierarchical decision trees, whose simplified basic structure is illustrated in figure 3. Note that in the &apos;reduce operation tree&apos;, the system first decides whether or not to perform a reduction before deciding on a specific reduction. Using our knowledge of similarity of parse actions and the exceptionality vs. generality of parse action groups, we can provide an overhead structure that helps prevent data fragmentation. 485 5 Transfer and Generation The output tree generated by the parser can be used for translation. A transfer module recursively maps the source language parse tree to an </context>
</contexts>
<marker>Rivest, 1987</marker>
<rawString>R. L. Rivest. 1987. Learning Decision Lists. In Machine Learning 2, pages 229-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R F Simmons</author>
<author>Yeong-Ho Yu</author>
</authors>
<title>The Acquisition and Use of Context-Dependent Grammars for English.</title>
<date>1992</date>
<journal>In Computational Linguistics</journal>
<volume>18</volume>
<issue>4</issue>
<pages>391--418</pages>
<contexts>
<context position="26778" citStr="Simmons and Yu, 1992" startWordPosition="4348" endWordPosition="4351">-0.56 Number of crossing brackets 0.58 0.54 Operations -0.45 -0.41 Operation sequence -0.39 -0.36 Table 5: Correlation between various parse and translation metrics. Values near -1.0 or 1.0 indicate very strong correlation, whereas values near 0.0 indicate a weak or no correlation. Most correlation values, incl. for labeled precision are negative, because a higher (better) labeled precision correlates with a numerically lower (better) translation score on the 1.0 (best) to 6.0 (worst) translation evaluation scale. 7 Related Work Our basic parsing and interactive training paradigm is based on (Simmons and Yu, 1992). We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component. (Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993). Questioning the traditional n-grams, Magerman already advocates a heavier re</context>
</contexts>
<marker>Simmons, Yu, 1992</marker>
<rawString>R. F. Simmons and Yeong-Ho Yu. 1992. The Acquisition and Use of Context-Dependent Grammars for English. In Computational Linguistics 18 (4), pages 391-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
<author>K R KcKeown</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Translating Collocations for Bilingual Lexicons: A Statistical Approach.</title>
<date>1996</date>
<journal>In Computational Linguistics</journal>
<volume>22</volume>
<issue>1</issue>
<pages>1--38</pages>
<contexts>
<context position="16657" citStr="Smadja et al., 1996" startWordPosition="2685" endWordPosition="2688"> zu ETWAS_DATA auf3ern&amp;quot;. Even if a complex translation pair does not bridge a structural mismatch, it can make a valuable contribution to disambiguation. Consider for example the term &amp;quot;interest rate&amp;quot;. Both element nouns are highly. ambiguous with respect to German, but the English compound conclusively maps to the German compound &amp;quot;Zinssatz&amp;quot;. We believe that an extensive collection of complex translation pairs in the bilingual dictionary is critical for translation quality and we are confident that its acquisition can be at least partially automated by using techniques like those described in (Smadja et al., 1996). Complex translation entries are preprocessed using the same parser as for normal text. During the transfer process, the resulting parse tree pairs are then accessed using pattern matching. The generation module orders the components of phrases, adds appropriate punctuation, and propagates morphologically relevant information in order to compute the proper form of surface words in the target language. 6 Wall Street Journal Experiments We now present intermediate results on training and testing a prototype implementation of the system with sentences from the Wall Street Journal, a prominent co</context>
</contexts>
<marker>Smadja, KcKeown, Hatzivassiloglou, 1996</marker>
<rawString>F. Smadja, K. R. KcKeown and V. Hatzivassiloglou. 1996. Translating Collocations for Bilingual Lexicons: A Statistical Approach. In Computational Linguistics 22 (1), pages 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Globalink</author>
</authors>
<date>1996</date>
<note>http://www.globalink.com/home.html</note>
<marker>Globalink, 1996</marker>
<rawString>Globalink. http://www.globalink.com/home.html Oct. 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>http www logos-ca com Oct</author>
</authors>
<date>1996</date>
<note>SYSTRAN. http://systranmt.com/</note>
<marker>Oct, 1996</marker>
<rawString>Logos. http://www.logos-ca.com/ Oct. 1996. SYSTRAN. http://systranmt.com/ Oct. 1996.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>