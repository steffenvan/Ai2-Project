<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9951025">
Hubness and Pollution:
Delving into Cross-Space Mapping for Zero-Shot Learning
</title>
<author confidence="0.9882">
Angelilii Lazaridou Georgiana Dinu Marco Baroni
</author>
<affiliation confidence="0.9950895">
Center for Mind/Brain Sciences
University of Trento
</affiliation>
<email confidence="0.992172">
{angeliki.lazaridou|georgiana.dinu|marco.baroni}@unitn.it
</email>
<sectionHeader confidence="0.997299" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999883785714286">
Zero-shot methods in language, vision and
other domains rely on a cross-space map-
ping function that projects vectors from
the relevant feature space (e.g., visual-
feature-based image representations) to a
large semantic word space (induced in
an unsupervised way from corpus data),
where the entities of interest (e.g., objects
images depict) are labeled with the words
associated to the nearest neighbours of the
mapped vectors. Zero-shot cross-space
mapping methods hold great promise as a
way to scale up annotation tasks well be-
yond the labels in the training data (e.g.,
recognizing objects that were never seen
in training). However, the current perfor-
mance of cross-space mapping functions
is still quite low, so that the strategy is
not yet usable in practical applications.
In this paper, we explore some general
properties, both theoretical and empirical,
of the cross-space mapping function, and
we build on them to propose better meth-
ods to estimate it. In this way, we attain
large improvements over the state of the
art, both in cross-linguistic (word trans-
lation) and cross-modal (image labeling)
zero-shot experiments.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992026">
In many supervised problems, the parameters of
a classification function are estimated on (x, y)
pairs, where x is a vector representing a training
instance in some feature space, and y is the label
assigned to the instance. For example, in image
labeling x contains visual features extracted from
a picture and y is the name of the object depicted
in the picture (Grauman and Leibe, 2011). Since
each label is treated as an unanalyzed primitive,
this approach requires ad-hoc annotation for each
label of interest, and it will not scale up to chal-
lenges where the potential label set is vast (for ex-
ample, bilingual dictionary induction, where the
label set corresponds to the full vocabulary of the
target language).
Zero-shot methods (Palatucci et al., 2009) ad-
dress the scalability problem by building on the
observation that the labels of interest are often
words (or longer linguistic expressions), which
stand in a semantic similarity relation to each
other. Moreover, distributional approaches allow
us to estimate very large semantic word spaces
in an efficient and unsupervised manner, using
just unannotated text corpora as input (Turney and
Pantel, 2010). Extensive evidence has shown that
the similarity estimates obtained by representing
words as vectors in such corpus-induced seman-
tic spaces are extremely accurate (Baroni et al.,
2014). Under the assumption that the domain of
interest (e.g., objects in pictures, words in a source
language) exhibits comparable similarity structure
to that manifested in language, we can rephrase the
learning task, from inducing multiple functions
from the source feature space onto independent
atomic labels, to that of estimating a single cross-
space mapping function from vectors in the source
feature space onto vectors for the corresponding
word labels in distributional semantic space. The
induced function can then also be applied to a
data-point whose label was not used for training.
The word corresponding to the nearest neighbour
of the mapped vector in the latter space is used
as the label of the data point. Zero-shot learn-
ing using distributional semantic spaces was origi-
nally proposed for brain signal decoding (Mitchell
et al., 2008), but it has since been extensively ap-
plied in other domains, including image labeling
(Frome et al., 2013; Lazaridou et al., 2014; Socher
et al., 2013) and bilingual dictionary/phrase table
induction (Dinu and Baroni, 2014; Mikolov et al.,
</bodyText>
<page confidence="0.941356">
270
</page>
<note confidence="0.977011">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 270–280,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999120137254902">
2013a), the two applications we focus on here.
Effective zero-shot learning by cross-space
mapping could get us through the manual anno-
tation bottleneck that hampers many applications.
However, in practice, the accuracy in label re-
trieval with current mapping methods is still too
low for practical uses. In image labeling, when
a search space of realistic size is considered, ac-
curacy is just above 1% (which is still well above
chance for large search spaces). In bilingual lex-
icon induction, accuracy reaches values around
30% (across words of varying frequency), which
are definitely more encouraging, but still indicate
that only 1 word in 3 will be translated correctly.
In this article, we look at some general prop-
erties of the linear cross-modal mapping function
standardly used for zero-shot learning, in order
to achieve a better understanding of its shortcom-
ings, and improve its quality by devising meth-
ods to overcome them. First, when the mapping
function is estimated with least-squares error tech-
niques, we observe a systematic increase in hub-
ness (Radovanovi´c et al., 2010b), that is, in the
tendency of some vectors (“hubs”) to appear in the
top neighbour lists of many test items. We connect
hubness to least-squares estimation, and we show
how it is greatly mitigated when the mapping func-
tion is estimated with a max-margin ranking loss
instead. Still, switching to max-margin greatly
improves accuracy in the cross-linguistic context,
but not for vision-to-language mapping. In the
cross-modal setting, we observe indeed a differ-
ent problem, that we name (training instance) pol-
lution: The neighbourhoods of mapped test items
are “polluted” by the target vectors used in train-
ing. This suggests that cross-modal mapping
suffers from overfitting issues, and consequently
from poor generalization power. Taking inspi-
ration from domain adaptation, which addresses
similar generalization concerns, and self-learning,
we propose a technique to augment the training
data with automatically constructed examples that
force the function to generalize better. Having
shown the advantages of a ranking loss, our fi-
nal contribution is the adaptation of some insights
from the max-margin literature to our setting, in
particular concerning the choice of negative ex-
amples. This leads to further accuracy improve-
ments. We thus conclude the paper by reporting
zero-shot performances in both cross-modal and
cross-language settings that are well above the cur-
</bodyText>
<figure confidence="0.323822166666667">
cross-linguistic cross-modal
33.0 0.5
29.7 1.1
39.4 1.9
NA 3.7
40.2 5.6
</figure>
<tableCaption confidence="0.898928333333333">
Table 1: Roadmap. Proposed changes to cross-
space mapping training and resulting percentage
Precision @1 in our two experimental setups.
</tableCaption>
<bodyText confidence="0.9279955">
rent state of the art. Table 1 provides a roadmap
and summary of our results.
</bodyText>
<sectionHeader confidence="0.992714" genericHeader="method">
2 Experimental Setup
</sectionHeader>
<bodyText confidence="0.998694142857143">
Cross-linguistic experiments In the cross-
linguistic experiments, we learn a mapping from
the semantic space of language A to the semantic
space of language B, which can then be used for
translating words outside the training set. Specifi-
cally, given the vector representation of a word in
language A, we apply the mapping to obtain an
estimate of the vector representation of its mean-
ing in language B, returning the nearest neigh-
bour of the mapped vector in the B space as can-
didate translation. We focus on translating from
English to Italian and adopt the setup (word vec-
tors, training and test data) of Dinu et al. (2015).
For a set of 200K words, 300-dimensional vectors
were built using the word2vec toolkit,1 choosing
the CBOW method.2 CBOW, which learns to pre-
dict a target word from the ones surrounding it,
produces state-of-the-art results in many linguis-
tic tasks (Baroni et al., 2014). The word vectors
were induced from corpora of 2.8 and 1.6 billion
tokens, respectively, for English and Italian.3 The
train and test English-to-Italian translation pairs
were extracted from a Europarl-derived dictionary
(Tiedemann, 2012).4 The 5K most frequent trans-
lation pairs were used for training, while the test
set includes 1.5K English words equally split into
5 frequency bins. The search for the correct trans-
lation is performed in a semantic space of 200K
</bodyText>
<footnote confidence="0.9995048">
1https://code.google.com/p/word2vec/
2Other hyperparameters, which we adopted without fur-
ther tuning, include a context window size of 5 words to
either side of the target, setting the sub-sampling option to
1e-05 and estimating the probability of target words by neg-
ative sampling, drawing 10 samples from the noise distribu-
tion (Mikolov et al., 2013b).
3Corpus sources: http://wacky.sslmit.unibo.
it,http://www.natcorp.ox.ac.uk
4http://opus.lingfil.uu.se/
</footnote>
<bodyText confidence="0.8714758">
former state of art
standard mapping
max-margin - §3
data augmentation - §4
negative evidence - §5
</bodyText>
<page confidence="0.959837">
271
</page>
<bodyText confidence="0.992086268292683">
Italian words.5
Cross-modal experiments In the cross-modal
experiments, we induce a mapping from visual
to linguistic space. Specifically, given an image,
we apply the mapping to its visual vector repre-
sentation to obtain an estimate of its representa-
tion in linguistic space, where the word associated
to the nearest neighbour is retrieved as the image
label. Similarly to translation pairs in the cross-
linguistic setup, we create a list of “visual transla-
tion” pairs between images and their correspond-
ing noun labels. Our starting point are the 5.1K
labels in ImageNet (Deng et al., 2009) that oc-
cur at least 500 times in our English corpus and
have concreteness score &gt;5, according to Turney
et al. (2011). For each label, we sample 100 pic-
tures from its ImageNet entry, and associate each
picture with the 4094-dimensional layer (fc7) at
the top of the pre-trained convolutional neural net-
work model of Krizhevsky et al. (2012), using the
Caffe toolkit (Jia et al., 2014). The target word
space is identical to the English space used in the
cross-linguistic experiment. Finally, we use 75%
of the labels (and the respective images) for train-
ing and the remaining 25% of the labels for test-
ing.6 From the 127.5K images corresponding to
test labels, we sample 1K images as our test set.
For zero-shot evaluation purposes, the search for
the correct label is performed in the space of 5.1K
possible labels, unless otherwise specified. How-
ever, when quantifying hubness and pollution, in
order to have a setting comparable to that of cross-
language mapping, we use the full set of 200K En-
glish words as search space.
Learning objectives We assume that we have
cross-space “translation” pairs available for a set
of ITrI items (xi, yi) = {xi E Rd1, yi E Rd2}.
Moreover, following previous work, we assume
that the mapping function is linear. For estimat-
ing its parameters W E Rd1×d2, we consider two
objectives. The first is L2-penalized least squares
</bodyText>
<footnote confidence="0.990852111111111">
5Faithful to the zero-shot setup, in our experiments there
is never any overlap between train and test words; however,
to make the task more challenging, we include the train words
in the search space, except where expressly indicated.
6At training time, we average the 100 vectors associated
to a label into a single representation, to reduce training set
size while minimizing information loss. At test time, as nor-
mally done, we present the model with single image visual
vectors.
</footnote>
<equation confidence="0.987563">
(ridge):
Wˆ = argmin IIXW − YI I+ AIIWII,
W∈Rd1×d2
</equation>
<bodyText confidence="0.999039625">
which has an analytical solution.
The second objective is a margin-based rank-
ing loss (max-margin) similar in spirit to the one
used in similar cross-modal experiments with WS-
ABIE (Weston et al., 2011) and DeViSE (Frome
et al., 2013). The loss for a given pair of train-
ing items (xi, yi) and the corresponding mapping-
based prediction ˆyi = Wxi is defined as
</bodyText>
<equation confidence="0.992491">
k
E max{0, -y + dist(ˆyi, yi) − dist(ˆyi, yj)},
jai
</equation>
<bodyText confidence="0.967046783783784">
where dist is a distance measure, in our case the
inverse cosine, and -y and k are tunable hyperpa-
rameters denoting the margin and the number of
negative examples, respectively. Intuitively, the
goal of the max-margin objective is to rank the
correct translation yi of xi higher than any other
possible translation yj. In theory, the summation
in the equation could range over all possible la-
bels, but in practice this is too expensive (e.g., in
the cross-linguistic experiments the search space
contains 200K candidate labels!), and it is usually
computed over just a portion of the label space.
In Weston et al. (2011), the authors propose an
efficient way of selecting negative examples, in
which they randomly sample, for each training
item, labels from the complete set, and pick as
negative sample the first label violating the mar-
gin. This guarantees that there will be exactly as
many weight updates as training items. Another
possibility is proposed in Mikolov et al. (2013b),
where negative samples are picked from a non-
item specific distribution (e.g., the uniform distri-
bution).7 For the experiments in Sections 3 and
4, we follow a more general setup in which the
size of the margin and number of negative sam-
ples is tuned for each task. In this way, for a
sufficiently large margin and number of negative
samples, we increase the probability of perform-
ing a weight update per training item. We estimate
the mapping parameters W with stochastic gradi-
ent descent and per-parameter learning rates tuned
with Adagrad (Duchi et al., 2011). The tuning of
hyperparameters -y and k is performed on a ran-
dom 25% subset of the training data.
7The notion of negative samples is not unique to margin-
based learning; in Mikolov et al. (2013b), the authors used it
to efficiently estimate a word probability distribution.
</bodyText>
<page confidence="0.983663">
272
</page>
<figure confidence="0.995820777777778">
Hubness in Cross−lingual Experiment
Hubness in Cross−modal Experiment
0 10 20 30 40 50
N20 values
Pr(N20)
0.009
0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0.01
0
ridge
max−margin
gold
0.01
ridge
max−margin
gold
0.008
0.007
0.006
0.005
0.004
0.003
0.002
0.001
0
5 10 15 20 25 30 35 40
N20 values
Pr(N20)
0.009
</figure>
<figureCaption confidence="0.820011333333333">
Figure 1: Hubness distribution in cross-linguistic (left) and cross-modal (right) search spaces. The
hubness score (N20) is computed on the top-20 neighbour lists of the test items, using their original
(gold), ridge- or max-margin-mapped vectors as query terms.
</figureCaption>
<sectionHeader confidence="0.996403" genericHeader="method">
3 Hubness
</sectionHeader>
<bodyText confidence="0.999986466666667">
High-dimensional spaces are often affected by
hubness (Radovanovi´c et al., 2010b; Radovanovi´c
et al., 2010a), that is, they contain certain ele-
ments – hubs – that are near many other points
in space without being similar to the latter in any
meaningful way. As recently noted by Dinu et
al. (2015), the hubness problem is greatly exacer-
bated when one looks at the nearest neighbours of
vectors that have been mapped across spaces with
ridge.8 Given a set of query vectors with the cor-
responding top-k nearest neighbour lists, we can
quantify the degree of hubness of an item in the
search space (parameterized by k) by the number
of lists in which it occurs. Nk(y), the hubness at k
of an item y, is computed as follows:
</bodyText>
<equation confidence="0.849624">
Nk(y) = |{x E T|y E NNk(x, S)}|,
</equation>
<bodyText confidence="0.899834">
where S denotes the search space, T denotes the
set of query items and NNk(x, S) denotes the k
nearest neighbors of x in S.
Figure 1 reports N20 distributions across the
cross-linguistic and cross-modal search spaces,
using the respective test items as query vectors.
The blue line shows the distributions for the
“gold” vectors (that is, the vectors in the target
space we would like to approximate). The red line
shows the same distributions when neighbours are
8Dinu et al. (2015) observe, but do not attempt to under-
stand hubness, as we do here. They propose to address it with
methods to re-rank neighbour lists, which are less general and
should be largely complementary to our effort to improve es-
timation of the cross-mapping function.
</bodyText>
<equation confidence="0.980137833333333">
Cross-linguistic Cross-modal
blockmonthon (50)
knurled (27)
autogiustificazione (27)
black-footed (23)
flatbread (22)
</equation>
<bodyText confidence="0.99710624">
Table 2: Top ridge hubs, together with N20
scores. Note that cross-linguistic hubs are sup-
posed to be Italian words.
queried for the ridge-mapped test vectors (ignore
black lines for now). In both spaces, when the
query vectors are mapped, hubness increases dra-
matically. The largest hubs for the original test
items occur in 15 neighbour lists or less. With
the mapped vectors, we find hubs occurring in
40 lists or more. The figure also shows that, in
both spaces, we observe more points with smaller
but non-negligible N20 (e.g., around 10) when
mapped vectors are queried. In both spaces, the
difference in hubness is very significant according
to a cross-tab test (p&lt;10−30). Finally, as Table 2
shows, the largest hubs are by no means terms that
we might expect to occur as neighbours of many
other items on semantic grounds (e.g., very gen-
eral terms), but rather very specific and rare words
whose high hubness cannot possibly be a genuine
semantic property.
Causes of hubness Why should the mapping
function lead to an increase in hubness? We con-
jecture that this is due to an intrinsic property of
least-squares estimation. Given the training ma-
</bodyText>
<equation confidence="0.999871222222222">
hashim (28)
akayev (27)
smilodon (40)
pintle (33)
limassol (26)
regulars (26)
18 (25)
handwheel (24)
circlip (23)
</equation>
<page confidence="0.98981">
273
</page>
<bodyText confidence="0.992747647058824">
trices X and Y, and the projection matrix W ob-
tained by minimizing squared error, each column
ˆy∗,i of Yˆ = XW is the orthogonal projection of
y∗,i, the corresponding Y column onto the col-
umn space of X (Strang, 2003, Ch. 4). Conse-
quently, y∗,i = Ei + ˆy∗ i, where the ci error vector
is orthogonal to ˆy∗,i. It follows that ||y∗,i||2 ≥=
||ˆy∗,i||2. Since y∗,i and ˆy∗,i have equal means (be-
cause the error terms in ci must sum to 0), it imme-
diately follows from the squared length inequality
that ˆy∗,i has lower or equal variance to y∗,i. Since
this holds for all columns of ˆY, it follows in turn
that the set of mapped vectors in Yˆ has lower or
equal variance to the corresponding set of origi-
nal vectors in Y. Coming back to hubness, a set
of lower variance points (such as the mapped vec-
tors) will result in higher hubness since the points
will on average be closer to each other. The prob-
lem is likely to be further exacerbated by the prop-
erty of least-squares to ignore relative distances
between points (the objective only aims at mak-
ing predicted and observed vectors look like each
other),
Strictly, the theoretical result only holds for the
training points. However, to the extent that the
training set is representative of what will be en-
countered in the test set, it should also extend
to test data (and if training and testing data are
very different, the mapping function will gener-
alize very poorly anyway). Moreover, the result
holds for a pure least-squares solution, without the
ridge L2 regularization term. Whether it also ap-
plies to ridge-based estimates will depend on the
relative impact of the least-squares and L2 terms
on the final solution (and it is not excluded that
the L2 term might also independently reduce vari-
ance, of course). Empirically, we find that, in-
deed, lower variance also characterizes test vectors
mapped with a ridge-estimated function.
Interestingly, in the literature on cross-space
mapping we find that authors choose a different
cost function than ridge, without motivating the
choice. Socher et al. (2014) mention in pass-
ing that max-margin outperforms a least-squared-
error cost for cross-modal mapping.
Max-margin as a solution to hubness Re-
ferring back to Figure 1, we see that when
ridge estimation is replaced by max-margin (black
line), there is a considerable decrease in hub-
ness in both settings. This is directly reflected
in a large increase in performance in our cross-
linguistic (English-to-Italian) zero-shot task (left
two columns of Table 3), with the largest im-
provement for the all important P@1 measure
(equivalent to accuracy).9 These results are well
above the current best cross-language accuracy for
cross-modal mapping without added orthographic
cues (33%), attained by Mikolov et al. (2013a).10
The absolute performance figures are low in the
challenging cross-modal setting, but here too we
observe a considerable improvement in accuracy
when max-margin is applied. Indeed, we are al-
ready above the cross-modal zero-shot mapping
state of the art for a search space of similar size
(0.5% accuracy in Frome et al. (2013)). Still, the
improvement over ridge (while present) is not as
large for the less strict (higher ranks) performance
scores.
Table 4 confirms that the improvement brought
about by max-margin is indeed (at least partially)
due to hubness reduction. A large proportion
of vectors retrieved as top-1 predictions (trans-
lations/labels) are hubs when mapping is trained
with ridge, but the proportion drops dramatically
with max-margin. Still, more than 1/5 top predic-
tions for cross-modal mapping with max-margin
are hubs (vs. less than 1/10 for the original vec-
tors). Now, the mathematical properties we re-
viewed above suggest that, for least-squares es-
timation, hubness is caused by general reduced
variance of the space after mapping. Thus, hubs
should be vectors that are near the mean of the
space. The first row of Table 5 confirms that
the hubs found in the neighbourhoods of ridge-
mapped query terms are items that tend to be
closer to the search space mean vector, and that
this effect is radically reduced with max-margin
estimation. However, the second row of the table
shows another factor at play, that has a major role
in the cross-modal setting, and it is only partially
addressed by max-margin estimation: Namely, in
vision-to-language mapping, there is a strong ten-
dency for hubs (that, recall, have an important ef-
fect on performance, as they enter many nearest
neighbour lists) to be close to a training data point.
9We have no realistic upper-bound estimate, but due to
different word senses, synonymy, etc., it is certainly not
100%.
10Although the numbers are not fully comparable because
of different language pairs and various methodological de-
tails, their method is essentially equivalent to our ridge ap-
proach we are clearly outperforming.
</bodyText>
<page confidence="0.990913">
274
</page>
<table confidence="0.971284">
Cross-linguistic Cross-modal
ridge max-margin ridge max-margin
P@1 29.7 38.4 1.1 1.9
P@5 44.2 54.2 4.8 5.4
P@10 49.1 60.4 7.9 9.0
</table>
<tableCaption confidence="0.9535294">
Table 3: Ridge vs. max-margin in zero-
shot experiments. Precision @N results cross-
linguistically (test items: 1.5K, search space:
200K) and cross-modally (test items: 1K, search
space: 5.1K).
</tableCaption>
<table confidence="0.625618666666667">
Cross-linguistic Cross-modal
ridge max-margin gold ridge max-margin gold
19.6 9.8 0.6 55.8 21.6 7.8
</table>
<tableCaption confidence="0.944164666666667">
Table 4: Hubs as top predictions. Percentage of
top-1 neighbours of test vectors in zero-shot ex-
periments of Table 3 with N20 &gt; 5.
</tableCaption>
<table confidence="0.8763915">
Cross-linguistic Cross-modal
cosine with ridge max-margin ridge max-margin
full-space mean 0.21 0.06 0.13 -0.01
training point 0.15 0.12 0.34 0.24
</table>
<tableCaption confidence="0.889532">
Table 5: Properties of hubs. Spearman p of
</tableCaption>
<bodyText confidence="0.8331212">
N20 scores with cosines to mean vector of full
search space (top) and nearest training item (bot-
tom), across all search space elements. All corre-
lations significant (p&lt;0.001) except cross-modal
max-margin hubness/full-space mean.
</bodyText>
<sectionHeader confidence="0.997724" genericHeader="method">
4 Pollution
</sectionHeader>
<bodyText confidence="0.999608888888889">
The quantitative results and post-hoc analysis of
hubs in Section 3 suggest that cross-modal map-
ping is facing a serious generalization problem. To
get a better grasp of the phenomenon, we define a
binary measure of (training data) pollution for a
queried item x and parameterized by k, such that
pollution is 1 if x has a (target) training item y
among its k nearest neighbours, 0 otherwise. For-
mally:
</bodyText>
<equation confidence="0.942002">
Npol
k,S(x) = [∃y ∈ YT r : y ∈ NNk,S(x)],
</equation>
<bodyText confidence="0.961552409836066">
where YT r is the matrix of target vectors used in
training, NNk,S(y) denotes the top k neighbors of
y in search space S, and [z] is an indicator func-
tion.11
11Pollution is of course an effect of overfitting, but we use
this more specific term to refer to the tendency of training
vectors to “pollute” nearest neighbour lists of mapped vec-
tors.
The average pollution Npol
1,S of all test items in
the cross-modal experiment, when |S|=200K is
18%, which indicates that in 1/5 of cases the re-
turned label is that of a training point. The equiv-
alent statistic in the cross-linguistic experiment
drops to 8.7% (words tend to be more varied than
the set of concrete, imageable concepts used for
image annotation tasks, and so the cross-linguistic
training set is probably less uniform than the one
used in the vision-to-language setting).
The real extent of the generalization problem
in the cross-modal setup becomes more obvious
if we restrict the search space to labels effectively
associated to an image in our data set (|S|=5.1K).
In this case, the average pollution Npol
1,S across all
test items jumps to 88%, that is, the vast major-
ity of test images are annotated with a label com-
ing from the training data. Clearly, there is a seri-
ous problem of overfitting to the training subspace.
While we came to this observation by inspecting
the properties of hubs, other work in zero-shot
for image labeling has indirectly noted the same.
Frome et al. (2013) empirically showed that the
performance of the system is higher when remov-
ing training labels from the search space, while
Norouzi et al. (2014) proposed a zero-shot method
that avoids explicit cross-modal mapping.
Adapting to the full search space by data
augmentation High training-data pollution in-
dicates that cross-modal mapping does not gener-
alize well beyond the kind of data points it encoun-
tered in learning. This is a special case of the data-
set bias problem (Torralba and Efros, 2011) and,
given that the latter has been addressed as a do-
main adaptation problem (Gong et al., 2012; Don-
ahue et al., 2013), we adopt here a similar view.
Self-training has been successfully used for do-
main adaptation in NLP, e.g., in syntactic parsing.
Given the limited amount of syntactically anno-
tated data coming from monotonous sources (e.g.,
the Wall Street Journal), parsers show a big drop
in performance when applied to different domains
(e.g., reviews), since training and test domains dif-
fer dramatically, thus affecting their generalization
performance. In a nutshell, the idea behind self-
training (McClosky et al., 2006; Reichart and Rap-
poport, 2007) is to use manually annotated data
(xAi , .., xAN, yAi , .., yAN) from domain A to train a
parser, feed the trained parser with data xBi , .., xBK
from domain B in order to obtain their automated
annotations ˆyB i , .., ˆyB K and then retrain the parser
</bodyText>
<page confidence="0.991972">
275
</page>
<figure confidence="0.8626487">
none chimera-5 chimera-10
P@1 1.9 3.7 3.2
P@5 5.4 10.9 10.5
P@10 9.0 15.8 15.9
dolphin tarantula highland
anteater whisky
arachnid lowland
spider bagpipe
opossum glen
scorpion distillery
</figure>
<tableCaption confidence="0.996592666666667">
Table 6: Visual chimeras for dolphin, tarantula
and highland.
Table 7: Cross-modal zero-shot experiment
</tableCaption>
<bodyText confidence="0.998149586206896">
with data augmentation. Labeling precision @N
with no data augmentation (none) and when us-
ing top 5 (chimera-5) and top 10 (chimera-10) near-
est neighbors from training set of each item in the
search space to build the corresponding chimeras
(1K test items, 5.1K search space).
whale
orca
porpoise
cetacean
shark
with a combination of “clean” data from domain
A and “noisy” data from domain B.
In our setup, self-training would be applied by
labeling a larger set of images with a cross-modal
mapping function estimated on the initial train-
ing data, and then using both sources of labeled
data to retrain the function. Although the idea
of self-training for inducing cross-modal map-
ping functions is appealing, especially given the
vast amount of unlabeled data available out there,
the very low performance of current cross-modal
mapping functions makes the effort questionable.
We would like to exploit unannotated data repre-
sentative of the search space, without relying on
the output of cross-modal mapping for their an-
notation. One way to achieve this is to use data
augmentation techniques that are representative of
the search space. Data augmentation is popular
in computer vision, where it is performed (among
others) by data jittering, visual sampling or image
perturbations. It has proven beneficial for both
“deep” (Krizhevsky et al., 2012; Zeiler and Fer-
gus, 2014) and “shallow” (Chatfield et al., 2014)
systems, and it was recently introduced to NLP
tasks (Zhang and LeCun, 2015).
Specifically, in order to train the mapping func-
tion using both annotated data and points that are
representative of the full search space, we rely on
a form of data augmentation that we call visual
chimera creation. For every item yi ∈/ YTr in the
search space S, we use linguistic similarity as a
proxy of visual similarity, and create its visual vec-
tor ˆxi by averaging the visual vectors correspond-
ing to the nearest words in language space that do
occur as labels in the training set. Table 6 presents
some examples of visual chimeras. For yi=dol-
phin, the visual vectors of other cetacean mam-
mals are averaged to create the chimera ˆxi. Since
linguistic similarity is not always determined by
visual factors, the method also produces noisy data
points. For yi=tarantula, opossums enter the pic-
ture, while for yi=highland images of “topically”
similar concepts are used (e.g., bagpipe).
Table 7 reports cross-modal zero-shot labeling
when training with max-margin and data augmen-
tation. We experiment with visual chimeras con-
structed using 5 vs. 10 nearest neighbours. While
the examples above suggest that the process injects
some noise in the training data, we also observe a
decrease of pollution Npol
1,S from 88% when using
the “clean” training data, to 71% and 73% when
expanding them with chimeras (for chimera-5 and
chimera-10, respectively). Reflecting this drop in
pollution, we see large improvements in precision
at all levels, when chimeras are used (no big dif-
ferences between 5 or 10 neighbours).
The improvements brought about by the
chimera method are robust. First, Table 8 reports
performance when the search space excludes the
training labels, showing that data augmentation is
beneficial beyond mitigating the bias in favor of
the latter. In this setup, chimera-5 is clearly out-
performing chimera-10 (longer neighbour lists will
include more noise), and we focus on it from here
on.
All experiments up to here follow the stan-
dard cross-modal zero-shot protocol, in which the
search space is given by the union of the test and
training labels, or a subset thereof. Next, we make
the task more challenging by increasing it with 1K
extra elements acting as distractors. The distrac-
tors are either randomly sampled from our usual
200K English word space, or, in the most chal-
lenging scenario, picked among those words, in
the same space, that are among the top-5 near-
</bodyText>
<page confidence="0.995841">
276
</page>
<table confidence="0.947322">
none chimera-5 chimera-10
P@1 6.7 9.3 8.3
P@5 21.7 25.2 21.3
P@10 29.9 34.3 29.7
</table>
<tableCaption confidence="0.4326258">
Table 8: Cross-modal zero-shot experiment
with data augmentation, disjoint train/search
spaces. Same setup as Table 8, but search space
excludes training elements (1K test items, 1K
search space).
</tableCaption>
<table confidence="0.877813">
random related
none chimera-5 none chimera-5
P@1 0.8 3.3 1.9 2.8
P@5 5.3 9.0 4.8 8.8
P@10 8.8 13.3 7.9 12.6
</table>
<tableCaption confidence="0.994532">
Table 9: Cross-modal zero-shot experiment
</tableCaption>
<bodyText confidence="0.98299559375">
with data augmentation, enlarged search space.
Labeling precision @N with no data augmenta-
tion (none) and when using top 5 (chimera-5) near-
est neighbors from training set of each item in the
search space to build the corresponding chimeras.
Test items: 1K. Search space: 5.1K+1K extra dis-
tractors from a 200K word space, either randomly
picked (random), or related to the training items.
est neighbours of a training element. Again, we
create one visual chimera for each label in the
search space. Results are presented in Table 9.
As expected, performance is negatively affected
with both plain and data-augmented models, but
the latter is still better in absolute terms. While
chimera-5 undergoes a larger drop when the search
contains many elements similar to the training data
(“related” column), which is explained by the fact
that visual chimeras will often include the distrac-
tor items of this setup, it appears to be more resis-
tant against random labels, which in many cases
are words that bear no resemblance to the training
data (e.g., naushad, yamato, 13-14). The picture
when using no data augmentation is exactly the
opposite, with the model being more harmed, at
P@1, by the random labels.
Finally, Table 10 presents results in the cross-
linguistic setup, when applying the same data aug-
mentation technique. In this case, we augment
the 5K training elements with 11.5K chimeras, for
the 1.5K test elements and 10K randomly sam-
pled distractors. For these 11.5K elements, we as-
sociate their Italian (target space) label yi with a
</bodyText>
<table confidence="0.824809">
none chimera-5
P@1 38.4 31.1
P@5 54.2 46.1
P@10 60.4 51.3
</table>
<tableCaption confidence="0.925992">
Table 10: Cross-linguistic zero-shot experiment
</tableCaption>
<bodyText confidence="0.8926895">
with data augmentation. Translation precision
@N when learning with max-margin and no data
augmentation (none) or data augmentation using
the top 5 (chimera-5) nearest neighbors of 11.5K
items in the 200K-word search space (1.5K test
items).
Figure 2: Looking for intruders. We pick truck
rather than dog as negative example for cat.
“pseudo-translation” vector ˆxi obtained by averag-
ing the vectors of the English (source space) trans-
lations of the nearest Italian words to yi included
in the training set. Results, in Table 10, show that
in this case our data augmentation method is ac-
tually hampering performance. We saw that pol-
lution affects the cross-linguistic setup much less
than it affects the cross-modal one, and we con-
jecture that, consequently, in the translation task,
there is not a large-enough generalization gain to
make up for the extra noise introduced by augmen-
tation.
</bodyText>
<sectionHeader confidence="0.980346" genericHeader="method">
5 Picking informative negative examples
</sectionHeader>
<bodyText confidence="0.999934666666667">
An interesting feature of the ranking max-margin
objective lies in its active use of negative exam-
ples. While previous work in cross-space map-
ping has paid little attention to the properties that
negative samples should possess, this has not gone
unnoticed in the NLP literature on structured pre-
diction tasks. Smith and Eisner (2005) propose a
contrastive estimation framework in the context of
POS-tagging, in which positive evidence derived
from gold sentence annotations is extended with
negative evidence derived by various neighbour-
hood functions that corrupt the data in particular
ways (e.g., by deleting 1 word).
Having shown the effectiveness of max-margin
estimation in the previous sections, we now take
</bodyText>
<figure confidence="0.899789">
dog
cat
truck
</figure>
<page confidence="0.720515">
277
</page>
<table confidence="0.908318">
Cross-linguistic Cross-modal
random intruder random intruder
P@1 38.4 40.2 3.7 5.6
P@5 54.2 55.5 10.9 12.4
P@10 60.4 61.8 15.8 17.8
</table>
<tableCaption confidence="0.995839">
Table 11: Random vs. intruding negative exam-
</tableCaption>
<bodyText confidence="0.999553341463415">
ples. Zero-shot precision @N results when cross-
space function is estimated using max-margin with
random or “intruder” negative examples, cross-
linguistically (test items: 1.5K, search space:
200K) and cross-modally (test items: 1K, search
space: 5.1K).
a first step towards engineering the negative evi-
dence exploited by this method, in the context of
inducing cross-space mapping functions. In par-
ticular, our idea is that, given a training instance
xi, an informative negative example would be near
the mapped vector ˆyi, but far from the actual gold
target space vector yi. Intuitively, such “intruders”
correspond to cases where the mapping function
is getting the predictions seriously wrong, and thus
they should be very informative in “correcting” the
function mapping trajectories. This can seen as a
vector-space interpretation of the max-loss update
protocol (Crammer et al., 2006) that picks nega-
tive samples expected to harm performance more.
Figure 2 illustrates the idea with a cartoon exam-
ple. If cat is the gold target vector yi and ˆyi the
corresponding mapped vector, then we are going
to pick truck as negative example, since it is an in-
truder (near the mapped vector, far from the gold
one).
More formally, at each step of stochastic gra-
dient descent, given a source space vector xi, its
target gold label/translation yi in YTr and the
mapped vector ˆyi, we compute sj = cos(ˆyi, yj) −
cos(yi, yj), for all vectors yj in YTr s.t. j =� i, and
pick as negative example for xi the vector with the
largest sj.
Table 11 presents zero-shot mapping results
when intruding negative examples are used for
max-margin estimation. For cross-modal map-
ping, we apply data augmentation as described in
the previous section. While the absolute perfor-
mance increase is relatively small (less than 2% in
both setups), it is consistent. Furthermore, the pro-
posed protocol results in lower Npol
</bodyText>
<sectionHeader confidence="0.33864" genericHeader="method">
1,S pollution in
</sectionHeader>
<bodyText confidence="0.7031055">
the cross-modal setup (from 71% to 63%). Finally,
we observe that the learning behaviour of the two
</bodyText>
<figure confidence="0.987465">
0 5 10 15 20 25 30 35 40 45 50
Number of Epochs
</figure>
<figureCaption confidence="0.722927333333333">
Figure 3: Learning curve with random or in-
truding negative samples in the cross-linguistic
experiment.
</figureCaption>
<bodyText confidence="0.9921668">
protocols (intruders vs. random) is different; the
intruder approach is already achieving good perfor-
mance after just few training epochs, since it can
rely on more informative negative samples (see
Figure 3).
</bodyText>
<sectionHeader confidence="0.999415" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999920538461538">
We have considered some general mathemati-
cal and empirical properties of linear cross-space
mapping functions, suggesting one well-known
(max-margin estimation) and two new (chimera
augmentation and “intruder” negative sample ad-
justment) methods to improve their performance.
With them, we achieve results well above the state
of the art in both the cross-linguistic and the cross-
modal setting. Both chimera and the intruder
methods are flexible, and we plan to explore them
further in future research. In particular, we want
to devise more semantically-motivated methods to
select chimera components and negative samples.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99991425">
We thank Adam Liska, Yoav Goldberg and the
anonymous reviewers for useful comments. We
acknowledge ERC 2011 Starting Independent Re-
search Grant n. 283554 (COMPOSES).
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9867338">
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
ofACL, pages 238–247, Baltimore, MD.
</reference>
<figure confidence="0.997766888888889">
intruder
random
Precision@1 0.45
0.4
0.35
0.3
0.25
0.2
0.15
</figure>
<page confidence="0.967882">
278
</page>
<reference confidence="0.997459064814815">
Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and
Andrew Zisserman. 2014. Return of the devil in the
details: Delving deep into convolutional nets. arXiv
preprint arXiv:1405.3531.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research, 7:551–585.
Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, and
Li Fei-Fei. 2009. Imagenet: A large-scale hierarchi-
cal image database. In Proceedings of CVPR, pages
248–255, Miami Beach, FL.
Georgiana Dinu and Marco Baroni. 2014. How to
make words with vectors: Phrase generation in dis-
tributional semantics. In Proceedings of ACL, pages
624–633, Baltimore, MD.
Georgiana Dinu, Angeliki Lazaridou, and Marco Ba-
roni. 2015. Improving zero-shot learning by miti-
gating the hubness problem. In Proceedings ofICLR
Workshop Track, San Diego, CA. Published on-
line: http://www.iclr.cc/doku.php?id=
iclr2015:main.
Jeff Donahue, Judy Hoffman, Erik Rodner, Kate
Saenko, and Trevor Darrell. 2013. Semi-supervised
domain adaptation with instance constraints. In In
Proceedings of CVPR, pages 668–675.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-
gio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas
Mikolov. 2013. DeViSE: A deep visual-semantic
embedding model. In Proceedings of NIPS, pages
2121–2129, Lake Tahoe, NV.
Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grau-
man. 2012. Geodesic flow kernel for unsupervised
domain adaptation. In In Proceedings of CVPR,
pages 2066–2073.
Kristen Grauman and Bastian Leibe. 2011. Visual Ob-
ject Recognition. Morgan &amp; Claypool, San Fran-
cisco.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio
Guadarrama, and Trevor Darrell. 2014. Caffe: Con-
volutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.
2012. ImageNet classification with deep convolu-
tional neural networks. In Proceedings of NIPS,
pages 1097–1105, Lake Tahoe, Nevada.
Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014. Is this a wampimuk? cross-modal map-
ping between distributional semantics and the visual
world. In Proceedings of ACL, pages 1403–1414,
Baltimore, MD.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of HLT-NAACL, pages 152–159.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013a. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL,
pages 746–751, Atlanta, Georgia.
Tom Mitchell, Svetlana Shinkareva, Andrew Carlson,
Kai-Min Chang, Vincente Malave, Robert Mason,
and Marcel Just. 2008. Predicting human brain ac-
tivity associated with the meanings of nouns. Sci-
ence, 320:1191–1195.
Mohammad Norouzi, Tomas Mikolov, Samy Bengio,
Yoram Singer, Jonathon Shlens, Andrea Frome,
Greg S Corrado, and Jeffrey Dean. 2014. Zero-shot
learning by convex combination of semantic embed-
dings. In Proceedings of ICLR.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Proceedings of NIPS,
pages 1410–1418, Vancouver, Canada.
Miloˇs Radovanovi´c, Alexandros Nanopoulos, and Mir-
jana Ivanovi´c. 2010a. Hubs in space: Popular near-
est neighbors in high-dimensional data. Journal of
Machine Learning Research, 11:2487–2531.
Miloˇs Radovanovi´c, Alexandros Nanopoulos, and
Mirjana Ivanovi´c. 2010b. On the existence of obsti-
nate results in vector space models. In Proceedings
of SIGIR, pages 186–193, Geneva, Switzerland.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In In Proceedings
ofACL, pages 616–623.
Noah A Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of ACL, pages 354–362.
Richard Socher, Milind Ganjoo, Christopher Manning,
and Andrew Ng. 2013. Zero-shot learning through
cross-modal transfer. In Proceedings of NIPS, pages
935–943, Lake Tahoe, NV.
Richard Socher, Quoc Le, Christopher Manning, and
Andrew Ng. 2014. Grounded compositional se-
mantics for finding and describing images with sen-
tences. Transactions of the Association for Compu-
tational Linguistics, 2:207–218.
Gilbert Strang. 2003. Introduction to linear algebra,
3d edition. Wellesley-Cambridge Press, Wellesley,
MA.
</reference>
<page confidence="0.977651">
279
</page>
<reference confidence="0.99945584">
J¨org Tiedemann. 2012. Parallel data, tools and in-
terfaces in OPUS. In Proceedings of LREC, pages
2214–2218.
Antonio Torralba and Alexei A Efros. 2011. Unbiased
look at dataset bias. In In Proceedings of CVPR,
pages 1521–1528.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-
hen. 2011. Literal and metaphorical sense identi-
fication through concrete and abstract context. In
Proceedings of EMNLP, pages 680–690, Edinburgh,
UK.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. Wsabie: Scaling up to large vocabulary image
annotation. In Proceedings of IJCAI, pages 2764–
2770.
Matthew Zeiler and Rob Fergus. 2014. Visualizing
and understanding convolutional networks. In Pro-
ceedings of ECCV (Part 1), pages 818–833, Zurich,
Switzerland.
Xiang Zhang and Yann LeCun. 2015. Text understand-
ing from scrath. arXiv preprint arXiv:1502.01710.
</reference>
<page confidence="0.997112">
280
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912409">
<title confidence="0.998086">Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning</title>
<author confidence="0.954005">Angelilii Lazaridou Georgiana Dinu Marco</author>
<affiliation confidence="0.9971295">Center for Mind/Brain University of Trento</affiliation>
<abstract confidence="0.998679931034483">Zero-shot methods in language, vision and domains rely on a mapthat projects vectors from the relevant feature space (e.g., visualfeature-based image representations) to a large semantic word space (induced in an unsupervised way from corpus data), where the entities of interest (e.g., objects images depict) are labeled with the words associated to the nearest neighbours of the mapped vectors. Zero-shot cross-space mapping methods hold great promise as a way to scale up annotation tasks well beyond the labels in the training data (e.g., recognizing objects that were never seen in training). However, the current performance of cross-space mapping functions is still quite low, so that the strategy is not yet usable in practical applications. In this paper, we explore some general properties, both theoretical and empirical, of the cross-space mapping function, and we build on them to propose better methods to estimate it. In this way, we attain large improvements over the state of the art, both in cross-linguistic (word translation) and cross-modal (image labeling) zero-shot experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>238--247</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="2747" citStr="Baroni et al., 2014" startWordPosition="415" endWordPosition="418">hods (Palatucci et al., 2009) address the scalability problem by building on the observation that the labels of interest are often words (or longer linguistic expressions), which stand in a semantic similarity relation to each other. Moreover, distributional approaches allow us to estimate very large semantic word spaces in an efficient and unsupervised manner, using just unannotated text corpora as input (Turney and Pantel, 2010). Extensive evidence has shown that the similarity estimates obtained by representing words as vectors in such corpus-induced semantic spaces are extremely accurate (Baroni et al., 2014). Under the assumption that the domain of interest (e.g., objects in pictures, words in a source language) exhibits comparable similarity structure to that manifested in language, we can rephrase the learning task, from inducing multiple functions from the source feature space onto independent atomic labels, to that of estimating a single crossspace mapping function from vectors in the source feature space onto vectors for the corresponding word labels in distributional semantic space. The induced function can then also be applied to a data-point whose label was not used for training. The word</context>
<context position="7775" citStr="Baroni et al., 2014" startWordPosition="1207" endWordPosition="1210">f a word in language A, we apply the mapping to obtain an estimate of the vector representation of its meaning in language B, returning the nearest neighbour of the mapped vector in the B space as candidate translation. We focus on translating from English to Italian and adopt the setup (word vectors, training and test data) of Dinu et al. (2015). For a set of 200K words, 300-dimensional vectors were built using the word2vec toolkit,1 choosing the CBOW method.2 CBOW, which learns to predict a target word from the ones surrounding it, produces state-of-the-art results in many linguistic tasks (Baroni et al., 2014). The word vectors were induced from corpora of 2.8 and 1.6 billion tokens, respectively, for English and Italian.3 The train and test English-to-Italian translation pairs were extracted from a Europarl-derived dictionary (Tiedemann, 2012).4 The 5K most frequent translation pairs were used for training, while the test set includes 1.5K English words equally split into 5 frequency bins. The search for the correct translation is performed in a semantic space of 200K 1https://code.google.com/p/word2vec/ 2Other hyperparameters, which we adopted without further tuning, include a context window size</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings ofACL, pages 238–247, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Chatfield</author>
<author>Karen Simonyan</author>
<author>Andrea Vedaldi</author>
<author>Andrew Zisserman</author>
</authors>
<title>Return of the devil in the details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531.</title>
<date>2014</date>
<contexts>
<context position="27864" citStr="Chatfield et al., 2014" startWordPosition="4537" endWordPosition="4540">ow performance of current cross-modal mapping functions makes the effort questionable. We would like to exploit unannotated data representative of the search space, without relying on the output of cross-modal mapping for their annotation. One way to achieve this is to use data augmentation techniques that are representative of the search space. Data augmentation is popular in computer vision, where it is performed (among others) by data jittering, visual sampling or image perturbations. It has proven beneficial for both “deep” (Krizhevsky et al., 2012; Zeiler and Fergus, 2014) and “shallow” (Chatfield et al., 2014) systems, and it was recently introduced to NLP tasks (Zhang and LeCun, 2015). Specifically, in order to train the mapping function using both annotated data and points that are representative of the full search space, we rely on a form of data augmentation that we call visual chimera creation. For every item yi ∈/ YTr in the search space S, we use linguistic similarity as a proxy of visual similarity, and create its visual vector ˆxi by averaging the visual vectors corresponding to the nearest words in language space that do occur as labels in the training set. Table 6 presents some examples </context>
</contexts>
<marker>Chatfield, Simonyan, Vedaldi, Zisserman, 2014</marker>
<rawString>Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Return of the devil in the details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="35141" citStr="Crammer et al., 2006" startWordPosition="5710" endWordPosition="5713">rds engineering the negative evidence exploited by this method, in the context of inducing cross-space mapping functions. In particular, our idea is that, given a training instance xi, an informative negative example would be near the mapped vector ˆyi, but far from the actual gold target space vector yi. Intuitively, such “intruders” correspond to cases where the mapping function is getting the predictions seriously wrong, and thus they should be very informative in “correcting” the function mapping trajectories. This can seen as a vector-space interpretation of the max-loss update protocol (Crammer et al., 2006) that picks negative samples expected to harm performance more. Figure 2 illustrates the idea with a cartoon example. If cat is the gold target vector yi and ˆyi the corresponding mapped vector, then we are going to pick truck as negative example, since it is an intruder (near the mapped vector, far from the gold one). More formally, at each step of stochastic gradient descent, given a source space vector xi, its target gold label/translation yi in YTr and the mapped vector ˆyi, we compute sj = cos(ˆyi, yj) − cos(yi, yj), for all vectors yj in YTr s.t. j =� i, and pick as negative example for </context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. The Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Lia-Ji Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>Imagenet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>248--255</pages>
<location>Miami Beach, FL.</location>
<contexts>
<context position="9396" citStr="Deng et al., 2009" startWordPosition="1451" endWordPosition="1454">negative evidence - §5 271 Italian words.5 Cross-modal experiments In the cross-modal experiments, we induce a mapping from visual to linguistic space. Specifically, given an image, we apply the mapping to its visual vector representation to obtain an estimate of its representation in linguistic space, where the word associated to the nearest neighbour is retrieved as the image label. Similarly to translation pairs in the crosslinguistic setup, we create a list of “visual translation” pairs between images and their corresponding noun labels. Our starting point are the 5.1K labels in ImageNet (Deng et al., 2009) that occur at least 500 times in our English corpus and have concreteness score &gt;5, according to Turney et al. (2011). For each label, we sample 100 pictures from its ImageNet entry, and associate each picture with the 4094-dimensional layer (fc7) at the top of the pre-trained convolutional neural network model of Krizhevsky et al. (2012), using the Caffe toolkit (Jia et al., 2014). The target word space is identical to the English space used in the cross-linguistic experiment. Finally, we use 75% of the labels (and the respective images) for training and the remaining 25% of the labels for t</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Proceedings of CVPR, pages 248–255, Miami Beach, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Marco Baroni</author>
</authors>
<title>How to make words with vectors: Phrase generation in distributional semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>624--633</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="3820" citStr="Dinu and Baroni, 2014" startWordPosition="584" endWordPosition="587">bels in distributional semantic space. The induced function can then also be applied to a data-point whose label was not used for training. The word corresponding to the nearest neighbour of the mapped vector in the latter space is used as the label of the data point. Zero-shot learning using distributional semantic spaces was originally proposed for brain signal decoding (Mitchell et al., 2008), but it has since been extensively applied in other domains, including image labeling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) and bilingual dictionary/phrase table induction (Dinu and Baroni, 2014; Mikolov et al., 270 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 270–280, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2013a), the two applications we focus on here. Effective zero-shot learning by cross-space mapping could get us through the manual annotation bottleneck that hampers many applications. However, in practice, the accuracy in label retrieval with current mapping methods is still too low for practical uses. In image lab</context>
</contexts>
<marker>Dinu, Baroni, 2014</marker>
<rawString>Georgiana Dinu and Marco Baroni. 2014. How to make words with vectors: Phrase generation in distributional semantics. In Proceedings of ACL, pages 624–633, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Angeliki Lazaridou</author>
<author>Marco Baroni</author>
</authors>
<title>Improving zero-shot learning by mitigating the hubness problem.</title>
<date>2015</date>
<booktitle>In Proceedings ofICLR Workshop Track,</booktitle>
<location>San Diego, CA.</location>
<note>Published online: http://www.iclr.cc/doku.php?id= iclr2015:main.</note>
<contexts>
<context position="7503" citStr="Dinu et al. (2015)" startWordPosition="1163" endWordPosition="1166">inguistic experiments In the crosslinguistic experiments, we learn a mapping from the semantic space of language A to the semantic space of language B, which can then be used for translating words outside the training set. Specifically, given the vector representation of a word in language A, we apply the mapping to obtain an estimate of the vector representation of its meaning in language B, returning the nearest neighbour of the mapped vector in the B space as candidate translation. We focus on translating from English to Italian and adopt the setup (word vectors, training and test data) of Dinu et al. (2015). For a set of 200K words, 300-dimensional vectors were built using the word2vec toolkit,1 choosing the CBOW method.2 CBOW, which learns to predict a target word from the ones surrounding it, produces state-of-the-art results in many linguistic tasks (Baroni et al., 2014). The word vectors were induced from corpora of 2.8 and 1.6 billion tokens, respectively, for English and Italian.3 The train and test English-to-Italian translation pairs were extracted from a Europarl-derived dictionary (Tiedemann, 2012).4 The 5K most frequent translation pairs were used for training, while the test set incl</context>
<context position="14412" citStr="Dinu et al. (2015)" startWordPosition="2301" endWordPosition="2304">10 15 20 25 30 35 40 N20 values Pr(N20) 0.009 Figure 1: Hubness distribution in cross-linguistic (left) and cross-modal (right) search spaces. The hubness score (N20) is computed on the top-20 neighbour lists of the test items, using their original (gold), ridge- or max-margin-mapped vectors as query terms. 3 Hubness High-dimensional spaces are often affected by hubness (Radovanovi´c et al., 2010b; Radovanovi´c et al., 2010a), that is, they contain certain elements – hubs – that are near many other points in space without being similar to the latter in any meaningful way. As recently noted by Dinu et al. (2015), the hubness problem is greatly exacerbated when one looks at the nearest neighbours of vectors that have been mapped across spaces with ridge.8 Given a set of query vectors with the corresponding top-k nearest neighbour lists, we can quantify the degree of hubness of an item in the search space (parameterized by k) by the number of lists in which it occurs. Nk(y), the hubness at k of an item y, is computed as follows: Nk(y) = |{x E T|y E NNk(x, S)}|, where S denotes the search space, T denotes the set of query items and NNk(x, S) denotes the k nearest neighbors of x in S. Figure 1 reports N2</context>
</contexts>
<marker>Dinu, Lazaridou, Baroni, 2015</marker>
<rawString>Georgiana Dinu, Angeliki Lazaridou, and Marco Baroni. 2015. Improving zero-shot learning by mitigating the hubness problem. In Proceedings ofICLR Workshop Track, San Diego, CA. Published online: http://www.iclr.cc/doku.php?id= iclr2015:main.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Donahue</author>
<author>Judy Hoffman</author>
<author>Erik Rodner</author>
<author>Kate Saenko</author>
<author>Trevor Darrell</author>
</authors>
<title>Semi-supervised domain adaptation with instance constraints. In</title>
<date>2013</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>668--675</pages>
<contexts>
<context position="25359" citStr="Donahue et al., 2013" startWordPosition="4131" endWordPosition="4135">(2013) empirically showed that the performance of the system is higher when removing training labels from the search space, while Norouzi et al. (2014) proposed a zero-shot method that avoids explicit cross-modal mapping. Adapting to the full search space by data augmentation High training-data pollution indicates that cross-modal mapping does not generalize well beyond the kind of data points it encountered in learning. This is a special case of the dataset bias problem (Torralba and Efros, 2011) and, given that the latter has been addressed as a domain adaptation problem (Gong et al., 2012; Donahue et al., 2013), we adopt here a similar view. Self-training has been successfully used for domain adaptation in NLP, e.g., in syntactic parsing. Given the limited amount of syntactically annotated data coming from monotonous sources (e.g., the Wall Street Journal), parsers show a big drop in performance when applied to different domains (e.g., reviews), since training and test domains differ dramatically, thus affecting their generalization performance. In a nutshell, the idea behind selftraining (McClosky et al., 2006; Reichart and Rappoport, 2007) is to use manually annotated data (xAi , .., xAN, yAi , ..</context>
</contexts>
<marker>Donahue, Hoffman, Rodner, Saenko, Darrell, 2013</marker>
<rawString>Jeff Donahue, Judy Hoffman, Erik Rodner, Kate Saenko, and Trevor Darrell. 2013. Semi-supervised domain adaptation with instance constraints. In In Proceedings of CVPR, pages 668–675.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="13248" citStr="Duchi et al., 2011" startWordPosition="2106" endWordPosition="2109">nother possibility is proposed in Mikolov et al. (2013b), where negative samples are picked from a nonitem specific distribution (e.g., the uniform distribution).7 For the experiments in Sections 3 and 4, we follow a more general setup in which the size of the margin and number of negative samples is tuned for each task. In this way, for a sufficiently large margin and number of negative samples, we increase the probability of performing a weight update per training item. We estimate the mapping parameters W with stochastic gradient descent and per-parameter learning rates tuned with Adagrad (Duchi et al., 2011). The tuning of hyperparameters -y and k is performed on a random 25% subset of the training data. 7The notion of negative samples is not unique to marginbased learning; in Mikolov et al. (2013b), the authors used it to efficiently estimate a word probability distribution. 272 Hubness in Cross−lingual Experiment Hubness in Cross−modal Experiment 0 10 20 30 40 50 N20 values Pr(N20) 0.009 0.008 0.007 0.006 0.005 0.004 0.003 0.002 0.001 0.01 0 ridge max−margin gold 0.01 ridge max−margin gold 0.008 0.007 0.006 0.005 0.004 0.003 0.002 0.001 0 5 10 15 20 25 30 35 40 N20 values Pr(N20) 0.009 Figure 1</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Frome</author>
<author>Greg Corrado</author>
<author>Jon Shlens</author>
<author>Samy Bengio</author>
<author>Jeff Dean</author>
<author>Marc’Aurelio Ranzato</author>
<author>Tomas Mikolov</author>
</authors>
<title>DeViSE: A deep visual-semantic embedding model.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2121--2129</pages>
<location>Lake Tahoe, NV.</location>
<contexts>
<context position="3703" citStr="Frome et al., 2013" startWordPosition="567" endWordPosition="570">le crossspace mapping function from vectors in the source feature space onto vectors for the corresponding word labels in distributional semantic space. The induced function can then also be applied to a data-point whose label was not used for training. The word corresponding to the nearest neighbour of the mapped vector in the latter space is used as the label of the data point. Zero-shot learning using distributional semantic spaces was originally proposed for brain signal decoding (Mitchell et al., 2008), but it has since been extensively applied in other domains, including image labeling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) and bilingual dictionary/phrase table induction (Dinu and Baroni, 2014; Mikolov et al., 270 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 270–280, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2013a), the two applications we focus on here. Effective zero-shot learning by cross-space mapping could get us through the manual annotation bottleneck that hampers many applications. However, in pra</context>
<context position="11524" citStr="Frome et al., 2013" startWordPosition="1812" endWordPosition="1815">ing, we include the train words in the search space, except where expressly indicated. 6At training time, we average the 100 vectors associated to a label into a single representation, to reduce training set size while minimizing information loss. At test time, as normally done, we present the model with single image visual vectors. (ridge): Wˆ = argmin IIXW − YI I+ AIIWII, W∈Rd1×d2 which has an analytical solution. The second objective is a margin-based ranking loss (max-margin) similar in spirit to the one used in similar cross-modal experiments with WSABIE (Weston et al., 2011) and DeViSE (Frome et al., 2013). The loss for a given pair of training items (xi, yi) and the corresponding mappingbased prediction ˆyi = Wxi is defined as k E max{0, -y + dist(ˆyi, yi) − dist(ˆyi, yj)}, jai where dist is a distance measure, in our case the inverse cosine, and -y and k are tunable hyperparameters denoting the margin and the number of negative examples, respectively. Intuitively, the goal of the max-margin objective is to rank the correct translation yi of xi higher than any other possible translation yj. In theory, the summation in the equation could range over all possible labels, but in practice this is t</context>
<context position="20098" citStr="Frome et al. (2013)" startWordPosition="3260" endWordPosition="3263">sk (left two columns of Table 3), with the largest improvement for the all important P@1 measure (equivalent to accuracy).9 These results are well above the current best cross-language accuracy for cross-modal mapping without added orthographic cues (33%), attained by Mikolov et al. (2013a).10 The absolute performance figures are low in the challenging cross-modal setting, but here too we observe a considerable improvement in accuracy when max-margin is applied. Indeed, we are already above the cross-modal zero-shot mapping state of the art for a search space of similar size (0.5% accuracy in Frome et al. (2013)). Still, the improvement over ridge (while present) is not as large for the less strict (higher ranks) performance scores. Table 4 confirms that the improvement brought about by max-margin is indeed (at least partially) due to hubness reduction. A large proportion of vectors retrieved as top-1 predictions (translations/labels) are hubs when mapping is trained with ridge, but the proportion drops dramatically with max-margin. Still, more than 1/5 top predictions for cross-modal mapping with max-margin are hubs (vs. less than 1/10 for the original vectors). Now, the mathematical properties we r</context>
<context position="24744" citStr="Frome et al. (2013)" startWordPosition="4028" endWordPosition="4031">he real extent of the generalization problem in the cross-modal setup becomes more obvious if we restrict the search space to labels effectively associated to an image in our data set (|S|=5.1K). In this case, the average pollution Npol 1,S across all test items jumps to 88%, that is, the vast majority of test images are annotated with a label coming from the training data. Clearly, there is a serious problem of overfitting to the training subspace. While we came to this observation by inspecting the properties of hubs, other work in zero-shot for image labeling has indirectly noted the same. Frome et al. (2013) empirically showed that the performance of the system is higher when removing training labels from the search space, while Norouzi et al. (2014) proposed a zero-shot method that avoids explicit cross-modal mapping. Adapting to the full search space by data augmentation High training-data pollution indicates that cross-modal mapping does not generalize well beyond the kind of data points it encountered in learning. This is a special case of the dataset bias problem (Torralba and Efros, 2011) and, given that the latter has been addressed as a domain adaptation problem (Gong et al., 2012; Donahu</context>
</contexts>
<marker>Frome, Corrado, Shlens, Bengio, Dean, Ranzato, Mikolov, 2013</marker>
<rawString>Andrea Frome, Greg Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. DeViSE: A deep visual-semantic embedding model. In Proceedings of NIPS, pages 2121–2129, Lake Tahoe, NV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boqing Gong</author>
<author>Yuan Shi</author>
<author>Fei Sha</author>
<author>Kristen Grauman</author>
</authors>
<title>Geodesic flow kernel for unsupervised domain adaptation. In</title>
<date>2012</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>2066--2073</pages>
<contexts>
<context position="25336" citStr="Gong et al., 2012" startWordPosition="4127" endWordPosition="4130">same. Frome et al. (2013) empirically showed that the performance of the system is higher when removing training labels from the search space, while Norouzi et al. (2014) proposed a zero-shot method that avoids explicit cross-modal mapping. Adapting to the full search space by data augmentation High training-data pollution indicates that cross-modal mapping does not generalize well beyond the kind of data points it encountered in learning. This is a special case of the dataset bias problem (Torralba and Efros, 2011) and, given that the latter has been addressed as a domain adaptation problem (Gong et al., 2012; Donahue et al., 2013), we adopt here a similar view. Self-training has been successfully used for domain adaptation in NLP, e.g., in syntactic parsing. Given the limited amount of syntactically annotated data coming from monotonous sources (e.g., the Wall Street Journal), parsers show a big drop in performance when applied to different domains (e.g., reviews), since training and test domains differ dramatically, thus affecting their generalization performance. In a nutshell, the idea behind selftraining (McClosky et al., 2006; Reichart and Rappoport, 2007) is to use manually annotated data (</context>
</contexts>
<marker>Gong, Shi, Sha, Grauman, 2012</marker>
<rawString>Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. 2012. Geodesic flow kernel for unsupervised domain adaptation. In In Proceedings of CVPR, pages 2066–2073.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristen Grauman</author>
<author>Bastian Leibe</author>
</authors>
<title>Visual Object Recognition.</title>
<date>2011</date>
<publisher>Morgan &amp; Claypool,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="1784" citStr="Grauman and Leibe, 2011" startWordPosition="267" endWordPosition="270"> on them to propose better methods to estimate it. In this way, we attain large improvements over the state of the art, both in cross-linguistic (word translation) and cross-modal (image labeling) zero-shot experiments. 1 Introduction In many supervised problems, the parameters of a classification function are estimated on (x, y) pairs, where x is a vector representing a training instance in some feature space, and y is the label assigned to the instance. For example, in image labeling x contains visual features extracted from a picture and y is the name of the object depicted in the picture (Grauman and Leibe, 2011). Since each label is treated as an unanalyzed primitive, this approach requires ad-hoc annotation for each label of interest, and it will not scale up to challenges where the potential label set is vast (for example, bilingual dictionary induction, where the label set corresponds to the full vocabulary of the target language). Zero-shot methods (Palatucci et al., 2009) address the scalability problem by building on the observation that the labels of interest are often words (or longer linguistic expressions), which stand in a semantic similarity relation to each other. Moreover, distributiona</context>
</contexts>
<marker>Grauman, Leibe, 2011</marker>
<rawString>Kristen Grauman and Bastian Leibe. 2011. Visual Object Recognition. Morgan &amp; Claypool, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangqing Jia</author>
<author>Evan Shelhamer</author>
<author>Jeff Donahue</author>
<author>Sergey Karayev</author>
<author>Jonathan Long</author>
<author>Ross Girshick</author>
<author>Sergio Guadarrama</author>
<author>Trevor Darrell</author>
</authors>
<title>Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093.</title>
<date>2014</date>
<contexts>
<context position="9781" citStr="Jia et al., 2014" startWordPosition="1518" endWordPosition="1521">. Similarly to translation pairs in the crosslinguistic setup, we create a list of “visual translation” pairs between images and their corresponding noun labels. Our starting point are the 5.1K labels in ImageNet (Deng et al., 2009) that occur at least 500 times in our English corpus and have concreteness score &gt;5, according to Turney et al. (2011). For each label, we sample 100 pictures from its ImageNet entry, and associate each picture with the 4094-dimensional layer (fc7) at the top of the pre-trained convolutional neural network model of Krizhevsky et al. (2012), using the Caffe toolkit (Jia et al., 2014). The target word space is identical to the English space used in the cross-linguistic experiment. Finally, we use 75% of the labels (and the respective images) for training and the remaining 25% of the labels for testing.6 From the 127.5K images corresponding to test labels, we sample 1K images as our test set. For zero-shot evaluation purposes, the search for the correct label is performed in the space of 5.1K possible labels, unless otherwise specified. However, when quantifying hubness and pollution, in order to have a setting comparable to that of crosslanguage mapping, we use the full se</context>
</contexts>
<marker>Jia, Shelhamer, Donahue, Karayev, Long, Girshick, Guadarrama, Darrell, 2014</marker>
<rawString>Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey Hinton</author>
</authors>
<title>ImageNet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1097--1105</pages>
<location>Lake Tahoe, Nevada.</location>
<contexts>
<context position="9737" citStr="Krizhevsky et al. (2012)" startWordPosition="1510" endWordPosition="1513">e nearest neighbour is retrieved as the image label. Similarly to translation pairs in the crosslinguistic setup, we create a list of “visual translation” pairs between images and their corresponding noun labels. Our starting point are the 5.1K labels in ImageNet (Deng et al., 2009) that occur at least 500 times in our English corpus and have concreteness score &gt;5, according to Turney et al. (2011). For each label, we sample 100 pictures from its ImageNet entry, and associate each picture with the 4094-dimensional layer (fc7) at the top of the pre-trained convolutional neural network model of Krizhevsky et al. (2012), using the Caffe toolkit (Jia et al., 2014). The target word space is identical to the English space used in the cross-linguistic experiment. Finally, we use 75% of the labels (and the respective images) for training and the remaining 25% of the labels for testing.6 From the 127.5K images corresponding to test labels, we sample 1K images as our test set. For zero-shot evaluation purposes, the search for the correct label is performed in the space of 5.1K possible labels, unless otherwise specified. However, when quantifying hubness and pollution, in order to have a setting comparable to that </context>
<context position="27799" citStr="Krizhevsky et al., 2012" startWordPosition="4526" endWordPosition="4529">the vast amount of unlabeled data available out there, the very low performance of current cross-modal mapping functions makes the effort questionable. We would like to exploit unannotated data representative of the search space, without relying on the output of cross-modal mapping for their annotation. One way to achieve this is to use data augmentation techniques that are representative of the search space. Data augmentation is popular in computer vision, where it is performed (among others) by data jittering, visual sampling or image perturbations. It has proven beneficial for both “deep” (Krizhevsky et al., 2012; Zeiler and Fergus, 2014) and “shallow” (Chatfield et al., 2014) systems, and it was recently introduced to NLP tasks (Zhang and LeCun, 2015). Specifically, in order to train the mapping function using both annotated data and points that are representative of the full search space, we rely on a form of data augmentation that we call visual chimera creation. For every item yi ∈/ YTr in the search space S, we use linguistic similarity as a proxy of visual similarity, and create its visual vector ˆxi by averaging the visual vectors corresponding to the nearest words in language space that do occ</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. 2012. ImageNet classification with deep convolutional neural networks. In Proceedings of NIPS, pages 1097–1105, Lake Tahoe, Nevada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Elia Bruni</author>
<author>Marco Baroni</author>
</authors>
<title>Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1403--1414</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="3727" citStr="Lazaridou et al., 2014" startWordPosition="571" endWordPosition="574">g function from vectors in the source feature space onto vectors for the corresponding word labels in distributional semantic space. The induced function can then also be applied to a data-point whose label was not used for training. The word corresponding to the nearest neighbour of the mapped vector in the latter space is used as the label of the data point. Zero-shot learning using distributional semantic spaces was originally proposed for brain signal decoding (Mitchell et al., 2008), but it has since been extensively applied in other domains, including image labeling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) and bilingual dictionary/phrase table induction (Dinu and Baroni, 2014; Mikolov et al., 270 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 270–280, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2013a), the two applications we focus on here. Effective zero-shot learning by cross-space mapping could get us through the manual annotation bottleneck that hampers many applications. However, in practice, the accuracy in l</context>
</contexts>
<marker>Lazaridou, Bruni, Baroni, 2014</marker>
<rawString>Angeliki Lazaridou, Elia Bruni, and Marco Baroni. 2014. Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world. In Proceedings of ACL, pages 1403–1414, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="25869" citStr="McClosky et al., 2006" startWordPosition="4210" endWordPosition="4213">ven that the latter has been addressed as a domain adaptation problem (Gong et al., 2012; Donahue et al., 2013), we adopt here a similar view. Self-training has been successfully used for domain adaptation in NLP, e.g., in syntactic parsing. Given the limited amount of syntactically annotated data coming from monotonous sources (e.g., the Wall Street Journal), parsers show a big drop in performance when applied to different domains (e.g., reviews), since training and test domains differ dramatically, thus affecting their generalization performance. In a nutshell, the idea behind selftraining (McClosky et al., 2006; Reichart and Rappoport, 2007) is to use manually annotated data (xAi , .., xAN, yAi , .., yAN) from domain A to train a parser, feed the trained parser with data xBi , .., xBK from domain B in order to obtain their automated annotations ˆyB i , .., ˆyB K and then retrain the parser 275 none chimera-5 chimera-10 P@1 1.9 3.7 3.2 P@5 5.4 10.9 10.5 P@10 9.0 15.8 15.9 dolphin tarantula highland anteater whisky arachnid lowland spider bagpipe opossum glen scorpion distillery Table 6: Visual chimeras for dolphin, tarantula and highland. Table 7: Cross-modal zero-shot experiment with data augmentati</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of HLT-NAACL, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</title>
<date>2013</date>
<contexts>
<context position="8595" citStr="Mikolov et al., 2013" startWordPosition="1333" endWordPosition="1336">rl-derived dictionary (Tiedemann, 2012).4 The 5K most frequent translation pairs were used for training, while the test set includes 1.5K English words equally split into 5 frequency bins. The search for the correct translation is performed in a semantic space of 200K 1https://code.google.com/p/word2vec/ 2Other hyperparameters, which we adopted without further tuning, include a context window size of 5 words to either side of the target, setting the sub-sampling option to 1e-05 and estimating the probability of target words by negative sampling, drawing 10 samples from the noise distribution (Mikolov et al., 2013b). 3Corpus sources: http://wacky.sslmit.unibo. it,http://www.natcorp.ox.ac.uk 4http://opus.lingfil.uu.se/ former state of art standard mapping max-margin - §3 data augmentation - §4 negative evidence - §5 271 Italian words.5 Cross-modal experiments In the cross-modal experiments, we induce a mapping from visual to linguistic space. Specifically, given an image, we apply the mapping to its visual vector representation to obtain an estimate of its representation in linguistic space, where the word associated to the nearest neighbour is retrieved as the image label. Similarly to translation pair</context>
<context position="12683" citStr="Mikolov et al. (2013" startWordPosition="2010" endWordPosition="2013">uld range over all possible labels, but in practice this is too expensive (e.g., in the cross-linguistic experiments the search space contains 200K candidate labels!), and it is usually computed over just a portion of the label space. In Weston et al. (2011), the authors propose an efficient way of selecting negative examples, in which they randomly sample, for each training item, labels from the complete set, and pick as negative sample the first label violating the margin. This guarantees that there will be exactly as many weight updates as training items. Another possibility is proposed in Mikolov et al. (2013b), where negative samples are picked from a nonitem specific distribution (e.g., the uniform distribution).7 For the experiments in Sections 3 and 4, we follow a more general setup in which the size of the margin and number of negative samples is tuned for each task. In this way, for a sufficiently large margin and number of negative samples, we increase the probability of performing a weight update per training item. We estimate the mapping parameters W with stochastic gradient descent and per-parameter learning rates tuned with Adagrad (Duchi et al., 2011). The tuning of hyperparameters -y </context>
<context position="19768" citStr="Mikolov et al. (2013" startWordPosition="3207" endWordPosition="3210">dal mapping. Max-margin as a solution to hubness Referring back to Figure 1, we see that when ridge estimation is replaced by max-margin (black line), there is a considerable decrease in hubness in both settings. This is directly reflected in a large increase in performance in our crosslinguistic (English-to-Italian) zero-shot task (left two columns of Table 3), with the largest improvement for the all important P@1 measure (equivalent to accuracy).9 These results are well above the current best cross-language accuracy for cross-modal mapping without added orthographic cues (33%), attained by Mikolov et al. (2013a).10 The absolute performance figures are low in the challenging cross-modal setting, but here too we observe a considerable improvement in accuracy when max-margin is applied. Indeed, we are already above the cross-modal zero-shot mapping state of the art for a search space of similar size (0.5% accuracy in Frome et al. (2013)). Still, the improvement over ridge (while present) is not as large for the less strict (higher ranks) performance scores. Table 4 confirms that the improvement brought about by max-margin is indeed (at least partially) due to hubness reduction. A large proportion of v</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>746--751</pages>
<location>Atlanta,</location>
<contexts>
<context position="8595" citStr="Mikolov et al., 2013" startWordPosition="1333" endWordPosition="1336">rl-derived dictionary (Tiedemann, 2012).4 The 5K most frequent translation pairs were used for training, while the test set includes 1.5K English words equally split into 5 frequency bins. The search for the correct translation is performed in a semantic space of 200K 1https://code.google.com/p/word2vec/ 2Other hyperparameters, which we adopted without further tuning, include a context window size of 5 words to either side of the target, setting the sub-sampling option to 1e-05 and estimating the probability of target words by negative sampling, drawing 10 samples from the noise distribution (Mikolov et al., 2013b). 3Corpus sources: http://wacky.sslmit.unibo. it,http://www.natcorp.ox.ac.uk 4http://opus.lingfil.uu.se/ former state of art standard mapping max-margin - §3 data augmentation - §4 negative evidence - §5 271 Italian words.5 Cross-modal experiments In the cross-modal experiments, we induce a mapping from visual to linguistic space. Specifically, given an image, we apply the mapping to its visual vector representation to obtain an estimate of its representation in linguistic space, where the word associated to the nearest neighbour is retrieved as the image label. Similarly to translation pair</context>
<context position="12683" citStr="Mikolov et al. (2013" startWordPosition="2010" endWordPosition="2013">uld range over all possible labels, but in practice this is too expensive (e.g., in the cross-linguistic experiments the search space contains 200K candidate labels!), and it is usually computed over just a portion of the label space. In Weston et al. (2011), the authors propose an efficient way of selecting negative examples, in which they randomly sample, for each training item, labels from the complete set, and pick as negative sample the first label violating the margin. This guarantees that there will be exactly as many weight updates as training items. Another possibility is proposed in Mikolov et al. (2013b), where negative samples are picked from a nonitem specific distribution (e.g., the uniform distribution).7 For the experiments in Sections 3 and 4, we follow a more general setup in which the size of the margin and number of negative samples is tuned for each task. In this way, for a sufficiently large margin and number of negative samples, we increase the probability of performing a weight update per training item. We estimate the mapping parameters W with stochastic gradient descent and per-parameter learning rates tuned with Adagrad (Duchi et al., 2011). The tuning of hyperparameters -y </context>
<context position="19768" citStr="Mikolov et al. (2013" startWordPosition="3207" endWordPosition="3210">dal mapping. Max-margin as a solution to hubness Referring back to Figure 1, we see that when ridge estimation is replaced by max-margin (black line), there is a considerable decrease in hubness in both settings. This is directly reflected in a large increase in performance in our crosslinguistic (English-to-Italian) zero-shot task (left two columns of Table 3), with the largest improvement for the all important P@1 measure (equivalent to accuracy).9 These results are well above the current best cross-language accuracy for cross-modal mapping without added orthographic cues (33%), attained by Mikolov et al. (2013a).10 The absolute performance figures are low in the challenging cross-modal setting, but here too we observe a considerable improvement in accuracy when max-margin is applied. Indeed, we are already above the cross-modal zero-shot mapping state of the art for a search space of similar size (0.5% accuracy in Frome et al. (2013)). Still, the improvement over ridge (while present) is not as large for the less strict (higher ranks) performance scores. Table 4 confirms that the improvement brought about by max-margin is indeed (at least partially) due to hubness reduction. A large proportion of v</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In Proceedings of NAACL, pages 746–751, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mitchell</author>
<author>Svetlana Shinkareva</author>
<author>Andrew Carlson</author>
<author>Kai-Min Chang</author>
<author>Vincente Malave</author>
<author>Robert Mason</author>
<author>Marcel Just</author>
</authors>
<title>Predicting human brain activity associated with the meanings of nouns.</title>
<date>2008</date>
<journal>Science,</journal>
<pages>320--1191</pages>
<contexts>
<context position="3597" citStr="Mitchell et al., 2008" startWordPosition="549" endWordPosition="552"> multiple functions from the source feature space onto independent atomic labels, to that of estimating a single crossspace mapping function from vectors in the source feature space onto vectors for the corresponding word labels in distributional semantic space. The induced function can then also be applied to a data-point whose label was not used for training. The word corresponding to the nearest neighbour of the mapped vector in the latter space is used as the label of the data point. Zero-shot learning using distributional semantic spaces was originally proposed for brain signal decoding (Mitchell et al., 2008), but it has since been extensively applied in other domains, including image labeling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) and bilingual dictionary/phrase table induction (Dinu and Baroni, 2014; Mikolov et al., 270 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 270–280, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2013a), the two applications we focus on here. Effective zero-shot learning by cross-space map</context>
</contexts>
<marker>Mitchell, Shinkareva, Carlson, Chang, Malave, Mason, Just, 2008</marker>
<rawString>Tom Mitchell, Svetlana Shinkareva, Andrew Carlson, Kai-Min Chang, Vincente Malave, Robert Mason, and Marcel Just. 2008. Predicting human brain activity associated with the meanings of nouns. Science, 320:1191–1195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Norouzi</author>
<author>Tomas Mikolov</author>
<author>Samy Bengio</author>
<author>Yoram Singer</author>
<author>Jonathon Shlens</author>
<author>Andrea Frome</author>
<author>Greg S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Zero-shot learning by convex combination of semantic embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of ICLR.</booktitle>
<contexts>
<context position="24889" citStr="Norouzi et al. (2014)" startWordPosition="4052" endWordPosition="4055">y associated to an image in our data set (|S|=5.1K). In this case, the average pollution Npol 1,S across all test items jumps to 88%, that is, the vast majority of test images are annotated with a label coming from the training data. Clearly, there is a serious problem of overfitting to the training subspace. While we came to this observation by inspecting the properties of hubs, other work in zero-shot for image labeling has indirectly noted the same. Frome et al. (2013) empirically showed that the performance of the system is higher when removing training labels from the search space, while Norouzi et al. (2014) proposed a zero-shot method that avoids explicit cross-modal mapping. Adapting to the full search space by data augmentation High training-data pollution indicates that cross-modal mapping does not generalize well beyond the kind of data points it encountered in learning. This is a special case of the dataset bias problem (Torralba and Efros, 2011) and, given that the latter has been addressed as a domain adaptation problem (Gong et al., 2012; Donahue et al., 2013), we adopt here a similar view. Self-training has been successfully used for domain adaptation in NLP, e.g., in syntactic parsing.</context>
</contexts>
<marker>Norouzi, Mikolov, Bengio, Singer, Shlens, Frome, Corrado, Dean, 2014</marker>
<rawString>Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg S Corrado, and Jeffrey Dean. 2014. Zero-shot learning by convex combination of semantic embeddings. In Proceedings of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Palatucci</author>
<author>Dean Pomerleau</author>
<author>Geoffrey Hinton</author>
<author>Tom Mitchell</author>
</authors>
<title>Zero-shot learning with semantic output codes.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1410--1418</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2156" citStr="Palatucci et al., 2009" startWordPosition="327" endWordPosition="330"> training instance in some feature space, and y is the label assigned to the instance. For example, in image labeling x contains visual features extracted from a picture and y is the name of the object depicted in the picture (Grauman and Leibe, 2011). Since each label is treated as an unanalyzed primitive, this approach requires ad-hoc annotation for each label of interest, and it will not scale up to challenges where the potential label set is vast (for example, bilingual dictionary induction, where the label set corresponds to the full vocabulary of the target language). Zero-shot methods (Palatucci et al., 2009) address the scalability problem by building on the observation that the labels of interest are often words (or longer linguistic expressions), which stand in a semantic similarity relation to each other. Moreover, distributional approaches allow us to estimate very large semantic word spaces in an efficient and unsupervised manner, using just unannotated text corpora as input (Turney and Pantel, 2010). Extensive evidence has shown that the similarity estimates obtained by representing words as vectors in such corpus-induced semantic spaces are extremely accurate (Baroni et al., 2014). Under t</context>
</contexts>
<marker>Palatucci, Pomerleau, Hinton, Mitchell, 2009</marker>
<rawString>Mark Palatucci, Dean Pomerleau, Geoffrey Hinton, and Tom Mitchell. 2009. Zero-shot learning with semantic output codes. In Proceedings of NIPS, pages 1410–1418, Vancouver, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Miloˇs Radovanovi´c</author>
</authors>
<title>Alexandros Nanopoulos, and Mirjana Ivanovi´c. 2010a. Hubs in space: Popular nearest neighbors in high-dimensional data.</title>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--2487</pages>
<marker>Radovanovi´c, </marker>
<rawString>Miloˇs Radovanovi´c, Alexandros Nanopoulos, and Mirjana Ivanovi´c. 2010a. Hubs in space: Popular nearest neighbors in high-dimensional data. Journal of Machine Learning Research, 11:2487–2531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miloˇs Radovanovi´c</author>
<author>Alexandros Nanopoulos</author>
<author>Mirjana Ivanovi´c</author>
</authors>
<title>On the existence of obstinate results in vector space models.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>186--193</pages>
<location>Geneva, Switzerland.</location>
<marker>Radovanovi´c, Nanopoulos, Ivanovi´c, 2010</marker>
<rawString>Miloˇs Radovanovi´c, Alexandros Nanopoulos, and Mirjana Ivanovi´c. 2010b. On the existence of obstinate results in vector space models. In Proceedings of SIGIR, pages 186–193, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets.</title>
<date>2007</date>
<booktitle>In In Proceedings ofACL,</booktitle>
<pages>616--623</pages>
<contexts>
<context position="25900" citStr="Reichart and Rappoport, 2007" startWordPosition="4214" endWordPosition="4218"> been addressed as a domain adaptation problem (Gong et al., 2012; Donahue et al., 2013), we adopt here a similar view. Self-training has been successfully used for domain adaptation in NLP, e.g., in syntactic parsing. Given the limited amount of syntactically annotated data coming from monotonous sources (e.g., the Wall Street Journal), parsers show a big drop in performance when applied to different domains (e.g., reviews), since training and test domains differ dramatically, thus affecting their generalization performance. In a nutshell, the idea behind selftraining (McClosky et al., 2006; Reichart and Rappoport, 2007) is to use manually annotated data (xAi , .., xAN, yAi , .., yAN) from domain A to train a parser, feed the trained parser with data xBi , .., xBK from domain B in order to obtain their automated annotations ˆyB i , .., ˆyB K and then retrain the parser 275 none chimera-5 chimera-10 P@1 1.9 3.7 3.2 P@5 5.4 10.9 10.5 P@10 9.0 15.8 15.9 dolphin tarantula highland anteater whisky arachnid lowland spider bagpipe opossum glen scorpion distillery Table 6: Visual chimeras for dolphin, tarantula and highland. Table 7: Cross-modal zero-shot experiment with data augmentation. Labeling precision @N with </context>
</contexts>
<marker>Reichart, Rappoport, 2007</marker>
<rawString>Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets. In In Proceedings ofACL, pages 616–623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>354--362</pages>
<contexts>
<context position="33675" citStr="Smith and Eisner (2005)" startWordPosition="5488" endWordPosition="5491">ects the cross-linguistic setup much less than it affects the cross-modal one, and we conjecture that, consequently, in the translation task, there is not a large-enough generalization gain to make up for the extra noise introduced by augmentation. 5 Picking informative negative examples An interesting feature of the ranking max-margin objective lies in its active use of negative examples. While previous work in cross-space mapping has paid little attention to the properties that negative samples should possess, this has not gone unnoticed in the NLP literature on structured prediction tasks. Smith and Eisner (2005) propose a contrastive estimation framework in the context of POS-tagging, in which positive evidence derived from gold sentence annotations is extended with negative evidence derived by various neighbourhood functions that corrupt the data in particular ways (e.g., by deleting 1 word). Having shown the effectiveness of max-margin estimation in the previous sections, we now take dog cat truck 277 Cross-linguistic Cross-modal random intruder random intruder P@1 38.4 40.2 3.7 5.6 P@5 54.2 55.5 10.9 12.4 P@10 60.4 61.8 15.8 17.8 Table 11: Random vs. intruding negative examples. Zero-shot precisio</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of ACL, pages 354–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Milind Ganjoo</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Zero-shot learning through cross-modal transfer.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>935--943</pages>
<location>Lake Tahoe, NV.</location>
<contexts>
<context position="3749" citStr="Socher et al., 2013" startWordPosition="575" endWordPosition="578">in the source feature space onto vectors for the corresponding word labels in distributional semantic space. The induced function can then also be applied to a data-point whose label was not used for training. The word corresponding to the nearest neighbour of the mapped vector in the latter space is used as the label of the data point. Zero-shot learning using distributional semantic spaces was originally proposed for brain signal decoding (Mitchell et al., 2008), but it has since been extensively applied in other domains, including image labeling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) and bilingual dictionary/phrase table induction (Dinu and Baroni, 2014; Mikolov et al., 270 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 270–280, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2013a), the two applications we focus on here. Effective zero-shot learning by cross-space mapping could get us through the manual annotation bottleneck that hampers many applications. However, in practice, the accuracy in label retrieval with cu</context>
</contexts>
<marker>Socher, Ganjoo, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Milind Ganjoo, Christopher Manning, and Andrew Ng. 2013. Zero-shot learning through cross-modal transfer. In Proceedings of NIPS, pages 935–943, Lake Tahoe, NV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Quoc Le</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--207</pages>
<contexts>
<context position="19062" citStr="Socher et al. (2014)" startWordPosition="3097" endWordPosition="3100">result holds for a pure least-squares solution, without the ridge L2 regularization term. Whether it also applies to ridge-based estimates will depend on the relative impact of the least-squares and L2 terms on the final solution (and it is not excluded that the L2 term might also independently reduce variance, of course). Empirically, we find that, indeed, lower variance also characterizes test vectors mapped with a ridge-estimated function. Interestingly, in the literature on cross-space mapping we find that authors choose a different cost function than ridge, without motivating the choice. Socher et al. (2014) mention in passing that max-margin outperforms a least-squarederror cost for cross-modal mapping. Max-margin as a solution to hubness Referring back to Figure 1, we see that when ridge estimation is replaced by max-margin (black line), there is a considerable decrease in hubness in both settings. This is directly reflected in a large increase in performance in our crosslinguistic (English-to-Italian) zero-shot task (left two columns of Table 3), with the largest improvement for the all important P@1 measure (equivalent to accuracy).9 These results are well above the current best cross-languag</context>
</contexts>
<marker>Socher, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Quoc Le, Christopher Manning, and Andrew Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilbert Strang</author>
</authors>
<title>Introduction to linear algebra, 3d edition.</title>
<date>2003</date>
<publisher>Wellesley-Cambridge Press,</publisher>
<location>Wellesley, MA.</location>
<contexts>
<context position="17218" citStr="Strang, 2003" startWordPosition="2781" endWordPosition="2782">pecific and rare words whose high hubness cannot possibly be a genuine semantic property. Causes of hubness Why should the mapping function lead to an increase in hubness? We conjecture that this is due to an intrinsic property of least-squares estimation. Given the training mahashim (28) akayev (27) smilodon (40) pintle (33) limassol (26) regulars (26) 18 (25) handwheel (24) circlip (23) 273 trices X and Y, and the projection matrix W obtained by minimizing squared error, each column ˆy∗,i of Yˆ = XW is the orthogonal projection of y∗,i, the corresponding Y column onto the column space of X (Strang, 2003, Ch. 4). Consequently, y∗,i = Ei + ˆy∗ i, where the ci error vector is orthogonal to ˆy∗,i. It follows that ||y∗,i||2 ≥= ||ˆy∗,i||2. Since y∗,i and ˆy∗,i have equal means (because the error terms in ci must sum to 0), it immediately follows from the squared length inequality that ˆy∗,i has lower or equal variance to y∗,i. Since this holds for all columns of ˆY, it follows in turn that the set of mapped vectors in Yˆ has lower or equal variance to the corresponding set of original vectors in Y. Coming back to hubness, a set of lower variance points (such as the mapped vectors) will result in h</context>
</contexts>
<marker>Strang, 2003</marker>
<rawString>Gilbert Strang. 2003. Introduction to linear algebra, 3d edition. Wellesley-Cambridge Press, Wellesley, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Parallel data, tools and interfaces in OPUS.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>2214--2218</pages>
<contexts>
<context position="8014" citStr="Tiedemann, 2012" startWordPosition="1242" endWordPosition="1243"> from English to Italian and adopt the setup (word vectors, training and test data) of Dinu et al. (2015). For a set of 200K words, 300-dimensional vectors were built using the word2vec toolkit,1 choosing the CBOW method.2 CBOW, which learns to predict a target word from the ones surrounding it, produces state-of-the-art results in many linguistic tasks (Baroni et al., 2014). The word vectors were induced from corpora of 2.8 and 1.6 billion tokens, respectively, for English and Italian.3 The train and test English-to-Italian translation pairs were extracted from a Europarl-derived dictionary (Tiedemann, 2012).4 The 5K most frequent translation pairs were used for training, while the test set includes 1.5K English words equally split into 5 frequency bins. The search for the correct translation is performed in a semantic space of 200K 1https://code.google.com/p/word2vec/ 2Other hyperparameters, which we adopted without further tuning, include a context window size of 5 words to either side of the target, setting the sub-sampling option to 1e-05 and estimating the probability of target words by negative sampling, drawing 10 samples from the noise distribution (Mikolov et al., 2013b). 3Corpus sources</context>
</contexts>
<marker>Tiedemann, 2012</marker>
<rawString>J¨org Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of LREC, pages 2214–2218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Torralba</author>
<author>Alexei A Efros</author>
</authors>
<title>Unbiased look at dataset bias. In</title>
<date>2011</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>1521--1528</pages>
<contexts>
<context position="25240" citStr="Torralba and Efros, 2011" startWordPosition="4109" endWordPosition="4112"> inspecting the properties of hubs, other work in zero-shot for image labeling has indirectly noted the same. Frome et al. (2013) empirically showed that the performance of the system is higher when removing training labels from the search space, while Norouzi et al. (2014) proposed a zero-shot method that avoids explicit cross-modal mapping. Adapting to the full search space by data augmentation High training-data pollution indicates that cross-modal mapping does not generalize well beyond the kind of data points it encountered in learning. This is a special case of the dataset bias problem (Torralba and Efros, 2011) and, given that the latter has been addressed as a domain adaptation problem (Gong et al., 2012; Donahue et al., 2013), we adopt here a similar view. Self-training has been successfully used for domain adaptation in NLP, e.g., in syntactic parsing. Given the limited amount of syntactically annotated data coming from monotonous sources (e.g., the Wall Street Journal), parsers show a big drop in performance when applied to different domains (e.g., reviews), since training and test domains differ dramatically, thus affecting their generalization performance. In a nutshell, the idea behind selftr</context>
</contexts>
<marker>Torralba, Efros, 2011</marker>
<rawString>Antonio Torralba and Alexei A Efros. 2011. Unbiased look at dataset bias. In In Proceedings of CVPR, pages 1521–1528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="2561" citStr="Turney and Pantel, 2010" startWordPosition="388" endWordPosition="391">o challenges where the potential label set is vast (for example, bilingual dictionary induction, where the label set corresponds to the full vocabulary of the target language). Zero-shot methods (Palatucci et al., 2009) address the scalability problem by building on the observation that the labels of interest are often words (or longer linguistic expressions), which stand in a semantic similarity relation to each other. Moreover, distributional approaches allow us to estimate very large semantic word spaces in an efficient and unsupervised manner, using just unannotated text corpora as input (Turney and Pantel, 2010). Extensive evidence has shown that the similarity estimates obtained by representing words as vectors in such corpus-induced semantic spaces are extremely accurate (Baroni et al., 2014). Under the assumption that the domain of interest (e.g., objects in pictures, words in a source language) exhibits comparable similarity structure to that manifested in language, we can rephrase the learning task, from inducing multiple functions from the source feature space onto independent atomic labels, to that of estimating a single crossspace mapping function from vectors in the source feature space onto</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Yair Neuman</author>
<author>Dan Assaf</author>
<author>Yohai Cohen</author>
</authors>
<title>Literal and metaphorical sense identification through concrete and abstract context.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>680--690</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="9514" citStr="Turney et al. (2011)" startWordPosition="1473" endWordPosition="1476">ng from visual to linguistic space. Specifically, given an image, we apply the mapping to its visual vector representation to obtain an estimate of its representation in linguistic space, where the word associated to the nearest neighbour is retrieved as the image label. Similarly to translation pairs in the crosslinguistic setup, we create a list of “visual translation” pairs between images and their corresponding noun labels. Our starting point are the 5.1K labels in ImageNet (Deng et al., 2009) that occur at least 500 times in our English corpus and have concreteness score &gt;5, according to Turney et al. (2011). For each label, we sample 100 pictures from its ImageNet entry, and associate each picture with the 4094-dimensional layer (fc7) at the top of the pre-trained convolutional neural network model of Krizhevsky et al. (2012), using the Caffe toolkit (Jia et al., 2014). The target word space is identical to the English space used in the cross-linguistic experiment. Finally, we use 75% of the labels (and the respective images) for training and the remaining 25% of the labels for testing.6 From the 127.5K images corresponding to test labels, we sample 1K images as our test set. For zero-shot evalu</context>
</contexts>
<marker>Turney, Neuman, Assaf, Cohen, 2011</marker>
<rawString>Peter Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identification through concrete and abstract context. In Proceedings of EMNLP, pages 680–690, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Samy Bengio</author>
<author>Nicolas Usunier</author>
</authors>
<title>Wsabie: Scaling up to large vocabulary image annotation.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>2764--2770</pages>
<contexts>
<context position="11492" citStr="Weston et al., 2011" startWordPosition="1806" endWordPosition="1809">r, to make the task more challenging, we include the train words in the search space, except where expressly indicated. 6At training time, we average the 100 vectors associated to a label into a single representation, to reduce training set size while minimizing information loss. At test time, as normally done, we present the model with single image visual vectors. (ridge): Wˆ = argmin IIXW − YI I+ AIIWII, W∈Rd1×d2 which has an analytical solution. The second objective is a margin-based ranking loss (max-margin) similar in spirit to the one used in similar cross-modal experiments with WSABIE (Weston et al., 2011) and DeViSE (Frome et al., 2013). The loss for a given pair of training items (xi, yi) and the corresponding mappingbased prediction ˆyi = Wxi is defined as k E max{0, -y + dist(ˆyi, yi) − dist(ˆyi, yj)}, jai where dist is a distance measure, in our case the inverse cosine, and -y and k are tunable hyperparameters denoting the margin and the number of negative examples, respectively. Intuitively, the goal of the max-margin objective is to rank the correct translation yi of xi higher than any other possible translation yj. In theory, the summation in the equation could range over all possible l</context>
</contexts>
<marker>Weston, Bengio, Usunier, 2011</marker>
<rawString>Jason Weston, Samy Bengio, and Nicolas Usunier. 2011. Wsabie: Scaling up to large vocabulary image annotation. In Proceedings of IJCAI, pages 2764– 2770.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Zeiler</author>
<author>Rob Fergus</author>
</authors>
<title>Visualizing and understanding convolutional networks.</title>
<date>2014</date>
<journal>In Proceedings of ECCV (Part</journal>
<volume>1</volume>
<pages>818--833</pages>
<location>Zurich, Switzerland.</location>
<contexts>
<context position="27825" citStr="Zeiler and Fergus, 2014" startWordPosition="4530" endWordPosition="4534">led data available out there, the very low performance of current cross-modal mapping functions makes the effort questionable. We would like to exploit unannotated data representative of the search space, without relying on the output of cross-modal mapping for their annotation. One way to achieve this is to use data augmentation techniques that are representative of the search space. Data augmentation is popular in computer vision, where it is performed (among others) by data jittering, visual sampling or image perturbations. It has proven beneficial for both “deep” (Krizhevsky et al., 2012; Zeiler and Fergus, 2014) and “shallow” (Chatfield et al., 2014) systems, and it was recently introduced to NLP tasks (Zhang and LeCun, 2015). Specifically, in order to train the mapping function using both annotated data and points that are representative of the full search space, we rely on a form of data augmentation that we call visual chimera creation. For every item yi ∈/ YTr in the search space S, we use linguistic similarity as a proxy of visual similarity, and create its visual vector ˆxi by averaging the visual vectors corresponding to the nearest words in language space that do occur as labels in the traini</context>
</contexts>
<marker>Zeiler, Fergus, 2014</marker>
<rawString>Matthew Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In Proceedings of ECCV (Part 1), pages 818–833, Zurich, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Zhang</author>
<author>Yann LeCun</author>
</authors>
<title>Text understanding from scrath. arXiv preprint arXiv:1502.01710.</title>
<date>2015</date>
<contexts>
<context position="27941" citStr="Zhang and LeCun, 2015" startWordPosition="4550" endWordPosition="4553">ionable. We would like to exploit unannotated data representative of the search space, without relying on the output of cross-modal mapping for their annotation. One way to achieve this is to use data augmentation techniques that are representative of the search space. Data augmentation is popular in computer vision, where it is performed (among others) by data jittering, visual sampling or image perturbations. It has proven beneficial for both “deep” (Krizhevsky et al., 2012; Zeiler and Fergus, 2014) and “shallow” (Chatfield et al., 2014) systems, and it was recently introduced to NLP tasks (Zhang and LeCun, 2015). Specifically, in order to train the mapping function using both annotated data and points that are representative of the full search space, we rely on a form of data augmentation that we call visual chimera creation. For every item yi ∈/ YTr in the search space S, we use linguistic similarity as a proxy of visual similarity, and create its visual vector ˆxi by averaging the visual vectors corresponding to the nearest words in language space that do occur as labels in the training set. Table 6 presents some examples of visual chimeras. For yi=dolphin, the visual vectors of other cetacean mamm</context>
</contexts>
<marker>Zhang, LeCun, 2015</marker>
<rawString>Xiang Zhang and Yann LeCun. 2015. Text understanding from scrath. arXiv preprint arXiv:1502.01710.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>