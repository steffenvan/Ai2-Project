<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000074">
<title confidence="0.985661">
Opinion Graphs for Polarity and Discourse Classification ∗
</title>
<author confidence="0.622885">
Galileo Namata
</author>
<affiliation confidence="0.580122">
Univ. of Maryland
</affiliation>
<address confidence="0.58321">
College Park, MD 20742
</address>
<email confidence="0.843289">
namatag@cs.umd.edu
</email>
<author confidence="0.888812">
Swapna Somasundaran
</author>
<affiliation confidence="0.6686755">
Univ. of Pittsburgh
Pittsburgh, PA 15260
</affiliation>
<email confidence="0.962734">
swapna@cs.pitt.edu
</email>
<author confidence="0.941858">
Lise Getoor
</author>
<affiliation confidence="0.944879">
Univ. of Maryland
</affiliation>
<address confidence="0.840642">
College Park, MD 20742
</address>
<email confidence="0.981334">
getoor@cs.umd.edu
</email>
<author confidence="0.820401">
Janyce Wiebe
</author>
<affiliation confidence="0.6162235">
Univ. of Pittsburgh
Pittsburgh, PA 15260
</affiliation>
<email confidence="0.973394">
wiebe@cs.pitt.edu
</email>
<sectionHeader confidence="0.993455" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999771">
This work shows how to construct
discourse-level opinion graphs to perform
a joint interpretation of opinions and dis-
course relations. Specifically, our opinion
graphs enable us to factor in discourse in-
formation for polarity classification, and
polarity information for discourse-link
classification. This inter-dependent frame-
work can be used to augment and im-
prove the performance of local polarity
and discourse-link classifiers.
</bodyText>
<sectionHeader confidence="0.998776" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989938278688525">
Much research in opinion analysis has focused on
information from words, phrases and semantic ori-
entation lexicons to perform sentiment classifica-
tion. While these are vital for opinion analysis,
they do not capture discourse-level associations
that arise from relations between opinions. To cap-
ture this information, we propose discourse-level
opinion graphs for classifying opinion polarity.
In order to build our computational model, we
combine a linguistic scheme opinion frames (So-
masundaran et al., 2008) with a collective classifi-
cation framework (Bilgic et al., 2007). According
to this scheme, two opinions are related in the dis-
course when their targets (what they are about) are
related. Further, these pair-wise discourse-level
relations between opinions are either reinforcing
or non-reinforcing frames. Reinforcing frames
capture reinforcing discourse scenarios where the
individual opinions reinforce one another, con-
tributing to the same opinion polarity or stance.
Non-reinforcing frames, on the other hand, cap-
ture discourse scenarios where the individual opin-
ions do not support the same stance. The indi-
vidual opinion polarities and the type of relation
∗This research was supported in part by the Department
of Homeland Security under grant N000140710152.
between their targets determine whether the dis-
course frame is reinforcing or non-reinforcing.
Our polarity classifier begins with information
from opinion lexicons to perform polarity classifi-
cation locally at each node. It then uses discourse-
level links, provided by the opinion frames, to
transmit the polarity information between nodes.
Thus the opinion classification of a node is not
just dependent on its local features, but also on the
class labels of related opinions and the nature of
these links. We design two discourse-level link
classifiers: the target-link classifier, which deter-
mines if a given node pair has unrelated targets (no
link), or if their targets have a same or alternative
relation, and the frame-link classifier, which deter-
mines if a given node pair has no link, reinforcing
or non-reinforcing link relation. Both these classi-
fiers too first start with local classifiers that use lo-
cal information. The opinion graph then provides
a means to factor in the related opinion informa-
tion into the link classifiers. Our approach enables
using the information in the nodes (and links) to
establish or remove links in the graph. Thus in-
formation flows to and fro between all the opinion
nodes and discourse-level links to achieve a joint
inference.
The paper is organized as follows: We first de-
scribe opinion graphs, a structure that can capture
discourse-level opinion relationships in Section 2,
and then describe our joint interpretation approach
to opinion analysis in Section 3. Next, we describe
our algorithm for joint interpretation in Section 4.
Our experimental results are reported in Section 5.
We discuss related work in Section 6 and conclude
in Section 7.
</bodyText>
<sectionHeader confidence="0.991732" genericHeader="method">
2 Discourse-Level Opinion Graphs
</sectionHeader>
<bodyText confidence="0.99968425">
The pairwise relationships that compose opinion
frames can be used to construct a graph over opin-
ion expressions in a discourse, which we refer
to as the discourse-level opinion graph (DLOG).
</bodyText>
<page confidence="0.918307">
66
</page>
<note confidence="0.999879">
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 66–74,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.992561">
Figure 1 Opinion Frame Annotations.
</figureCaption>
<bodyText confidence="0.99990884">
In this section, we describe these graphs and il-
lustrate their applicability to goal-oriented multi-
party conversations.
The nodes in the DLOG represent opinions, and
there are two kinds of links: target links and frame
links. Each opinion node has a polarity (positive,
negative or neutral) and type (sentiment or argu-
ing). Sentiment opinions are evaluations, feelings
or judgments about the target. Arguing opinions
argue for or against something. Target links are
labeled as either same or alternatives. Same links
hold between targets that refer to the same en-
tity or proposition, while alternative links hold be-
tween targets that are related by virtue of being op-
posing (mutually exclusive) options in the context
of the discourse. The frame links correspond to
the opinion frame relation between opinions.
We illustrate the construction of the opinion
graph with an example (Example 1, from Soma-
sundaran et al. (2008)) from a multi-party meet-
ing corpus where participants discuss and design a
new TV remote control. The opinion expressions
are in bold and their targets are in italics. Notice
here that speaker D has a positive sentiment to-
wards the rubbery material for the TV remote.
</bodyText>
<listItem confidence="0.611892">
(1) D::... this kind of rubbery material, it’s a bit more
</listItem>
<bodyText confidence="0.998909">
bouncy, like you said they get chucked around a lot.
A bit more durable and that can also be ergonomic
and it kind of feels a bit different from all the other
remote controls.
All the individual opinions in this example are
essentially regarding the same thing – the rub-
bery material. The speaker’s positive sentiment is
apparent from the text spans bit more bouncy,
bit more durable, ergonomic and a bit different
from all the other remote controls. The explicit
targets of these opinions (it’s, that, and it) and the
implicit target of “a bit more durable” are thus all
linked with same relations.
Figure 1 illustrates the individual opinion anno-
tations, target annotations (shown in italics) and
the relations between the targets (shown in dotted
lines). Note that the target of a bit more durable
is a zero span ellipsis that refers back to the rub-
bery material. The opinion frames resulting from
the individual annotations make pairwise connec-
tions between opinion instances, as shown in bold
lines in the figure. For example, the two opinions
bit more bouncy and ergonomic, and the same
link between their targets (it’s and that), make up
an opinion frame. An opinion frame type is de-
rived from the details (type and polarity) of the
opinions it relates and the target relation involved.
Even though the different combinations of opin-
ion type (sentiment and arguing), polarity (posi-
tive and negative) and target links (same and al-
ternative) result in many distinct frames types (32
in total), they can be grouped, according to their
discourse-level characteristics, into the two cat-
egories reinforcing and non-reinforcing. In this
work, we only make this category distinction for
opinion frames and the corresponding frame links.
The next example (Example 2, also from So-
masundaran et al. (2008)) illustrates an alterna-
tive target relation. In the domain of TV remote
controls, the set of all shapes are alternatives to
one another, since a remote control may have only
one shape at a time. In such scenarios, a positive
opinion regarding one choice may imply a nega-
tive opinion toward competing choices, and vice
versa. In this passage, speaker C’s positive stance
towards the curved shape is brought out even more
strongly with his negative opinions toward the al-
ternative, square-like, shapes.
</bodyText>
<listItem confidence="0.938078">
(2) C:: ... shapes should be curved, so round shapes.
</listItem>
<bodyText confidence="0.9582956875">
Nothing square-like.
...
C:: ... So we shouldn’t have too square corners
and that kind of thing.
The reinforcing frames characteristically show
a reinforcement of an opinion or stance in the dis-
course. Both the examples presented above depict
a reinforcing scenario. In the first example, the
opinion towards the rubbery material is reinforced
by repeated positive sentiments towards it, while
in the second example the positive stance towards
the curved shapes is further reinforced by nega-
tive opinions toward the alternative option. Ex-
amples of non-reinforcing scenarios are ambiva-
lence between alternative options (for e.g., “I like
the rubbery material but the plastic will be much
</bodyText>
<page confidence="0.998982">
67
</page>
<bodyText confidence="0.998526">
cheaper”) or mixed opinions about the same tar-
get (for e.g., weighing pros and cons “The rubbery
material is good but it will be just so expensive”).
</bodyText>
<sectionHeader confidence="0.99729" genericHeader="method">
3 Interdependent Interpretation
</sectionHeader>
<bodyText confidence="0.999977432835821">
Our interdependent interpretation in DLOGs is
motivated by the observation that, when two opin-
ions are related, a clear knowledge of the polarity
of one of them makes interpreting the other much
easier. For instance, suppose an opinion classi-
fier wants to find the polarity of all the opinion
expressions in Example 1. As a first step, it can
look up opinion lexicons to infer that words like
“bouncy”, “durable” and “ ergonomic” are pos-
itive. However, “a bit different ” cannot be re-
solved via this method, as its polarity can be dif-
ferent in different scenarios.
Suppose now we relate the targets of opinions.
There are clues in the passage that the targets are
related via the same relation; for instance they
are all third person pronouns occurring in adja-
cent clauses and sentences. Once we relate the
targets, the opinions of the passage are related via
target links in the discourse opinion graph. We
are also able to establish frames using the opinion
information and target link information wherever
they are available, i.e., a reinforcing link between
bit more bouncy and ergonomic. For the places
where all the information is not available (between
ergonomic and a bit different) there are multiple
possibilities. Depending on the polarity, either a
reinforcing frame (if a bit different has positive
polarity) or a non-reinforcing frame (if a bit dif-
ferent has negative polarity) can exist. There are
clues in the discourse that this passage represents
a reinforcing scenario. For instance there are rein-
forcing frames between the first few opinions, the
repeated use of “and” indicates a list, conjunction
or expansion relation between clauses (according
to the Penn Discourse TreeBank (PDTB) (Prasad
et al., 2008)), and there is a lack of contrastive
clues that would indicate a change in the opin-
ion. Thus the reinforcing frame link emerges as
being the most likely candidate. This in turn dis-
ambiguates the polarity of a bit different. Thus,
by establishing target links and frame links be-
tween the opinion instances, we are able to per-
form a joint interpretation of the opinions.
The interdependent framework of this example
is iterative and dynamic — the information in the
nodes can be used to change the structure (i.e.,
establish new links), and the structure provides a
framework to change node polarity. We build our
classification framework and feature sets with re-
spect to this general framework, where the node
labels as well as the structure of the graph are pre-
dicted in a joint manner.
Thus our interdependent interpretation frame-
work has three main units: an instance polarity
classifier (IPC), a target-link classifier (TLC), and
a frame-link classifier (FLC). IPC classifies each
node (instance), which may be a sentence, utter-
ance or an other text span, as positive, negative
or neutral. The TLC determines if a given node
pair has related targets and whether they are linked
by a same or alternative relation. The FLC deter-
mines if a given node pair is related via frames,
and whether it is a reinforcing or non-reinforcing
link. As we saw in the example, there are local
clues available for each unit to arrive at its classi-
fication. The discourse augments this information
to aid in further disambiguation.
</bodyText>
<sectionHeader confidence="0.994114" genericHeader="method">
4 Collective Classification Framework
</sectionHeader>
<bodyText confidence="0.999993142857143">
For our collective classification framework, we
use a variant of the iterative classification al-
gorithm (ICA) proposed by Bilgic et al (2007).
It combines several common prediction tasks in
graphs: object classification (predicting the label
of an object) and link prediction (predicting the
existence and class of a link between objects).
For our tasks, object classification directly corre-
sponds to predicting opinion polarity and the link
prediction corresponds to predicting the existence
of a same or alternative target link or a reinforc-
ing or non-reinforcing frame link between opin-
ions. We note that given the nature of our problem
formulation and approach, we use the terms link
prediction and link classification interchangeably.
In the collective classification framework, there
are two sets of features to use. The first are local
features which can be generated for each object or
link, independent of the links in which they par-
ticipate, or the objects they connect. For example,
the opinion instance may contain words that oc-
cur in sentiment lexicons. The local features are
described in Section 4.2. The second set of fea-
tures, the relational features, reflect neighborhood
information in the graph. For frame link classifi-
cation, for example, there is a feature indicating
whether the connected nodes are predicted to have
the same polarity. The relational features are de-
</bodyText>
<page confidence="0.996786">
68
</page>
<bodyText confidence="0.915715">
scribed in Section 4.3.
</bodyText>
<subsectionHeader confidence="0.880295">
4.1 DLOG-ICA Algorithm
</subsectionHeader>
<bodyText confidence="0.99998975">
Our variant of the ICA algorithm begins by pre-
dicting the opinion polarity, and link type using
only the local features. We then randomly order
the set of all opinions and links and, in turn, pre-
dict the polarity or class using the local features
and the values of the currently predicted relational
features based on previous predictions. We repeat
this until some stopping criterion is met. For our
experiments, we use a fixed number of 30 itera-
tions which was sufficient, in most of our datasets,
for ICA to converge to a solution. The pseudocode
for the algorithm is shown in Algorithm 4.1.
</bodyText>
<equation confidence="0.490373625">
Algorithm 1 DLOG-ICA Algorithm
for each opinion o do {bootstrapping}
Compute polarity for o using local attributes
end for
for each target link t do {bootstrapping}
Compute label for t using local attributes
end for
for each frame link f do {bootstrapping}
</equation>
<bodyText confidence="0.934711">
Compute label for f using local attributes
</bodyText>
<subsectionHeader confidence="0.536838">
end for
</subsectionHeader>
<bodyText confidence="0.966048416666667">
repeat {iterative classification}
Generate ordering I over all nodes and links
for each i in I do
if i is an opinion instance then
Compute polarity for i using local and
relational attributes
else if i is a target link then
Compute class for i using local and re-
lational attributes
else if i is a frame link then
Compute class for i using local and re-
lational attributes
</bodyText>
<listItem confidence="0.458036">
end if
end for
until Stopping criterion is met
</listItem>
<bodyText confidence="0.999946833333333">
The algorithm is one very simple way of making
classifications that are interdependent. Once the
local and relational features are defined, a variety
of classifiers can be used. For our experiments, we
use SVMs. Additional details are provided in the
experiments section.
</bodyText>
<subsectionHeader confidence="0.969192">
4.2 Local Features
</subsectionHeader>
<bodyText confidence="0.9978935">
For the local polarity classifier, we employ opin-
ion lexicons, dialog information, and unigram fea-
</bodyText>
<table confidence="0.999333909090909">
Feature Task
Time difference between the node pair TLC, FLC
Number of intervening instances TLC, FLC
Content word overlap between the node pair TLC,FLC
Focus space overlap between the node pair TLC, FLC
Bigram overlap between the node pair * TLC, FLC
Are both nodes from same speaker * TLC, FLC
Bag of words for each node TLC, FLC
Anaphoric indicator in the second node TLC
Adjacency pair between the node pair FLC
Discourse relation between node pair * FLC
</table>
<tableCaption confidence="0.983709666666667">
Table 1: Features and the classification task it is used for;
TLC = target-link classification, FLC = Frame-link classifi-
cation
</tableCaption>
<bodyText confidence="0.99965027027027">
tures. We use lexicons that have been success-
fully used in previous work (the polarity lexicon
from (Wilson et al., 2005) and the arguing lexi-
con (Somasundaran et al., 2007)). Previous work
used features based on parse trees, e.g., (Wilson et
al., 2005; Kanayama and Nasukawa, 2006), but
our data has very different characteristics from
monologic texts – the utterances and sentences are
much shorter, and there are frequent disfluencies,
restarts, hedging and repetitions. Because of this,
we cannot rely on parsing features. On the other
hand, in this data, we have dialog act information1
(Dialog Acts), which we can exploit. Note that the
IPC uses only the Dialog Act tags (instance level
tags like Inform, Suggest) and not the dialog struc-
ture information.
Opinion frame detection between sentences has
been previously attempted (Somasundaran et al.,
2008) by using features that capture discourse
and dialog continuity. Even though our link
classification tasks are not directly comparable
(the previous work performs binary classifica-
tion of frame-present/frame-absent between opin-
ion bearing sentences, while this work performs
three-way classification: no-link/reinforcing/non-
reinforcing between DA pairs), we adapt the fea-
tures for the link classification tasks addressed
here. These features depend on properties of the
nodes that the link connects. We also create some
new features that capture discourse relations and
lexical overlap.
Table 1 lists the link classification features.
New features are indicated with a ‘*’. Continu-
ous discourse indicators, like time difference be-
tween the node pair and number of intervening
instances are useful for determining if the two
nodes can be related. The content word over-
</bodyText>
<footnote confidence="0.997096">
1Manual annotations for Dialog act tags and adjacency
pairs are available for the AMI corpus.
</footnote>
<page confidence="0.998764">
69
</page>
<bodyText confidence="0.999890966666667">
lap, and focus space overlap features (the focus
space for an instance is a list of the most recently
used NP chunks; i.e., NP chunks in that instance
and a few previous instances) capture the overlap
in topicality within the node pair; while the bi-
gram overlap feature captures the alignment be-
tween instances in terms of function words as well
as content words. The entity-level relations are
captured by the anaphoric indicator feature that
checks for the presence of pronouns such as it and
that in the second node in the node pair. The adja-
cency pair and discourse relation are actually fea-
ture sets that indicate specific dialog-structure and
discourse-level relations. We group the list of dis-
course relations from the PDTB into the following
sets: expansion, contingency, alternative, tempo-
ral, comparison. Each discourse relation in PDTB
is associated with a list of discourse connective
words.2 Given a node pair, if the first word of the
later instance (or the last word first instance) is a
discourse connective word, then we assume that
this node is connecting back (or forward) in the
discourse and the feature set to which the connec-
tive belongs is set to true (e.g., if a latter instance
is “because we should ...”, it starts with the con-
nective “because”, and connects backwards via a
contingency relation). The adjacency pair feature
indicates the presence of a particular dialog struc-
ture (e.g., support, positive-assessment) between
the nodes.
</bodyText>
<subsectionHeader confidence="0.989846">
4.3 Relational Features
</subsectionHeader>
<bodyText confidence="0.999674529411765">
In addition to the local features, we introduce re-
lational features (Table 2) that incorporate related
class information as well as transfer label informa-
tion between classifiers. As we saw in our example
in Figure 1, we need to know not only the polar-
ity of the related opinions, but also the type of the
relation between them. For example, if the frame
relation between ergonomic and a bit different is
non-reinforcing, then the polarity of a bit differ-
ent is likely to be negative. Thus link labels play
an important role in disambiguating the polarity.
Accordingly, our relational features transfer infor-
mation of class labels from other instances of the
same classifier as well as between different clas-
sifiers. Table 2 lists our relational features. Each
row represents a set of features. Features are gen-
erated for all combinations of x, y and z for each
</bodyText>
<footnote confidence="0.6838765">
2The PDTB provides a list of discourse connectives and
the list of discourse relations each connective signifies.
</footnote>
<bodyText confidence="0.999193">
row. For example, one of the features in the first
row is Number of neighbors with polarity type pos-
itive, that are related via a reinforcing frame link.
Thus each feature for the polarity classifier iden-
tifies neighbors for a given node via a specific re-
lation (z or y) and factors in their polarity values.
Similarly, both link classifiers use polarity infor-
mation of the node pair, and other link relations
involving the nodes of the pair.
</bodyText>
<sectionHeader confidence="0.997957" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999992325">
We experimentally test our hypothesis that
discourse-level information is useful and non-
redundant with local information. We also wanted
to test how the DLOG performs for varying
amounts of available annotations: from full neigh-
borhood information to absolutely no neighbor-
hood information.
Accordingly, for polarity classification, we im-
plemented three scenarios: ICA-LinkNeigh, ICA-
LinkOnly and ICA-noInfo. The ICA-LinkNeigh
scenario measures the performance of the DLOG
under ideal conditions (full neighborhood infor-
mation) — the structure of the graph (link infor-
mation) as well as the neighbors’ class are pro-
vided (by an oracle). Here we do not need the
TLC, or the FLC to predict links and the Instance
Polarity Classifier (IPC) is not dependent on its
predictions from the previous iteration. On the
other hand, the ICA-noInfo scenario is the other
extreme, and has absolutely no neighborhood in-
formation. Each node does not know which nodes
in the network it is connected to apriori, and also
has no information about the polarity of any other
node in the network. Here, the structure of the
graph, as well as the node classes, have to be in-
ferred via the collective classification framework
described in Sections 3 and 4. The ICA-LinkOnly
is an intermediate condition, and is representative
of scenarios where the discourse relationships be-
tween nodes is known. Here we start with the link
information (from an oracle) and the IPC uses the
collective classification framework to infer neigh-
bor polarity information.
Similarly, we vary the amounts of neighbor-
hood information for the TLC and FLC classifiers.
In the ICA-LinkNeigh condition, TLC and FLC
have full neighborhood information. In the ICA-
noInfo condition, TLC and FLC are fully depen-
dent on the classifications of the previous rounds.
In the ICA-Partial condition, the TLC classifier
</bodyText>
<page confidence="0.9967">
70
</page>
<table confidence="0.990443375">
Feature
Opinion Polarity Classification
Number of neighbors with polarity type x linked via frame link z
Number of neighbors with polarity type x linked via target link y
Number of neighbors with polarity type x and same speaker linked via frame link z
Number of neighbors with polarity type x and same speaker linked via target link y
Target Link Classification
Polarity of the DA nodes
Number of other target links y involving the given DA nodes
Number of other target links y involving the given DA nodes and other same-speaker nodes
Presence of a frame link z between the nodes
Frame Link Classification
Polarity of the DA nodes
Number of other frame links z involving the given DA nodes
Number of other frame links z involving the given DA nodes and other same-speaker nodes
Presence of a target link y between the nodes
</table>
<tableCaption confidence="0.9406345">
Table 2: Relational features: x E {non-neutral (i.e., positive or negative), positive, negative}, y E {same, alt}, z E
{reinforcing, non-reinforcing}
</tableCaption>
<bodyText confidence="0.997086333333333">
uses true frame-links and polarity information,
and previous-stage classifications for information
about neighborhood target links; the FLC classi-
fier uses true target-links and polarity information,
and previous-stage classifications for information
about neighborhood frame-links.
</bodyText>
<subsectionHeader confidence="0.960409">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999945659574468">
For our experiments, we use the opinion frame
annotations from previous work (Somasundaran
et al., 2008). These annotations consist of the
opinion spans that reveal opinions, their targets,
the polarity information for opinions, the labeled
links between the targets and the frame links be-
tween the opinions. The annotated data consists
of 7 scenario-based, multi-party meetings from the
AMI meeting corpus (Carletta et al., 2005). The
manual Dialog Act (DA) annotations, provided by
AMI, segment the meeting transcription into sep-
arate dialog acts. We use these DAs as nodes or
instances in our opinion graph.
A DA is assigned the opinion orientation of the
words it contains (for example, if a DA contains a
positive opinion expression, then the DA assigned
the positive opinion category). We filter out very
small DAs (DAs with fewer than 3 tokens, punctu-
ation included) in order to alleviate data skewness
problem in the link classifiers. This gives us a to-
tal of 4606 DA instances, of which 1935 (42%)
have opinions. Out of these 1935, 61.7% are posi-
tive, 30% are negative and the rest are neutral. The
DAs that do not have opinions are considered neu-
tral, and have no links in the DLOG. We create
DA pairs by first ordering the DAs by their start
time, and then pairing a DA with five DAs before
it, and five DAs after it. The classes for target-
link classification are no-link, same, alt. The gold
standard target-link class is decided for a DA pair
based on the target link between the targets of the
opinions contained in that pair. Similarly, the la-
bels for the frame-link labeling task are no-link,
reinforcing, non-reinforcing. The gold standard
frame link class is decided for a DA pair based on
the frame between opinions contained by that pair.
In our data, of the 4606 DAs, 1118 (24.27%) par-
ticipate in target links with other DAs, and 1056
(22.9%) form frame links. The gold standard data
for links, which has pair-wise information, has a
total of 22,925 DA pairs, of which 1371 (6%) pairs
have target links and 1264 (5.5%) pairs have frame
links.
We perform 7-fold cross-validation experi-
ments, using the 7 meetings. In each fold, 6 meet-
ings are used for training and one meeting is used
for testing.
</bodyText>
<subsectionHeader confidence="0.99851">
5.2 Classifiers
</subsectionHeader>
<bodyText confidence="0.9998304">
Our baseline (Base) classifies the test data based
on the distribution of the classes in the training
data. Note that due to the heavily skewed nature of
our link data, this classifier performs very poorly
for minority class prediction, even though it may
achieve good overall accuracy.
For our local classifiers, we used the classifiers
from the Weka toolkit (Witten and Frank, 2002).
For opinion polarity, we used the Weka’s SVM
implementation. For the target link and frame link
classes, the huge class skew caused SVM to learn a
trivial model and always predict the majority class.
To address this, we used a cost sensitive classifier
in Weka where we set the cost of misclassifying a
less frequent class, A, to a more frequent class, B,
</bodyText>
<page confidence="0.997619">
71
</page>
<table confidence="0.999840333333333">
Base Local ICA
LinkNeigh LinkOnly noInfo
Acc 45.9 68.7 78.8 72.9 68.4
Class: neutral (majority class)
Prec 61.2 76.3 83.9 78.2 73.5
Rec 61.5 83.9 89.6 89.1 86.6
F1 61.1 79.6 86.6 83.2 79.3
Class: positive polarity
Prec 26.3 56.2 70.9 63.3 57.6
Rec 26.1 46.6 62.0 47.0 42.8
F1 25.8 50.4 65.9 53.5 48.5
Class: negative polarity
Prec 12.4 52.3 64.6 56.3 55.2
Rec 12.2 44.3 60.2 48.2 38.2
F1 12.2 46.0 61.9 51.2 43.9
</table>
<tableCaption confidence="0.999927">
Table 3: Performance of Polarity Classifiers
</tableCaption>
<bodyText confidence="0.999690235294118">
as |B|/|A |where |class |is the size of the class in
the training set. All other misclassification costs
are set to 1.
For our collective classification, we use the
above classifiers for local features (l) and use sim-
ilar, separate classifiers for relational features (r).
For example, we learned an SVM for predicting
opinion polarity using only the local features and
learned another SVM using only relational fea-
tures. For the ICA-noInfo condition, where we
use TLC and FLC classifiers, we combine the
predictions using a weighted combination where
P(class|l, r) = α * P(class|l) + (1 − α) *
P(class|r). This allows us to vary the influence
each feature set has to the overall prediction. The
results for ICA-noInfo are reported on the best per-
forming α (0.7).
</bodyText>
<subsectionHeader confidence="0.863089">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999978">
Our polarity classification results are presented
in Table 3, specifically accuracy (Acc), precision
(Prec), recall (Rec) and F-measure (F1). As we
can see, the results are mixed. First, we no-
tice that the Local classifier shows substantial im-
provement over the baseline classifier. This shows
that the lexical and dialog features we use are in-
formative of opinion polarity in multi-party meet-
ings.
Next, notice that the ICA-LinkNeigh classifier
performs substantially better than the Local clas-
sifier for all metrics and all classes. The accuracy
improves by 10 percentage points, while the F-
measure improves by about 15 percentage points
for the minority (positive and negative) classes.
This result confirms that our discourse-level opin-
ion graphs are useful and discourse-level informa-
tion is non-redundant with lexical and dialog-act
</bodyText>
<table confidence="0.99964425">
Base Local ICA
LinkNeigh Partial noInfo
TLC
Acc 88.5 85.8 98.1 98.2 86.3
P-M 33.3 35.9 76.1 76.1 36.3
R-M 33.3 38.1 78.1 78.1 38.1
F1-M 33.1 36.0 74.6 74.6 36.5
FLC
Acc 89.3 86.2 98.9 98.9 87.6
P-M 33.3 36.9 81.3 82.8 38.0
R-M 33.4 41.2 82.2 84.4 41.7
F1-M 33.1 37.2 80.7 82.3 38.1
</table>
<tableCaption confidence="0.999884">
Table 4: Performance of Link Classifiers
</tableCaption>
<bodyText confidence="0.999396516129032">
information.
The results for ICA-LinkOnly follow the same
trend as for ICA-LinkNeigh, with a 3 to 5 percent-
age point improvement. These results show that
even when the neighbors’ classes are not known
a priori, joint inference using discourse-level rela-
tions helps reduce errors from local classification.
However, the performance of the ICA-noInfo
system, which is given absolutely no starting in-
formation, is comparable to the Local classifier for
the overall accuracy and F-measure metrics for the
neutral class. There is slight improvement in pre-
cision for both the positive and negative classes,
but there is a drop in their recall. The reason this
classifier does no better than the Local classifier is
because the link classifiers TLC and FLC predict
“none” predominantly due to the heavy class skew.
The performance of the link classifiers are re-
ported in Table 4, specifically the accuracy (Acc)
and macro averages over all classes for preci-
sion (P-M), recall (R-M) and F-measure (F1-M).
Due to the heavy skew in the data, accuracy
of all classifiers is high; however, the macro F-
measure, which depends on the F1 of the minor-
ity classes, is poor for the ICA-noInfo. Note,
however, that when we provide some (Partial) or
full (LinkNeigh) neighborhood information for the
Link classifiers, the performance of these classi-
fiers improve considerably. This overall observed
trend is similar to that observed with the polarity
classifiers.
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.993012833333333">
Previous work on polarity disambiguation has
used contextual clues and reversal words (Wil-
son et al., 2005; Kennedy and Inkpen, 2006;
Kanayama and Nasukawa, 2006; Devitt and Ah-
mad, 2007; Sadamitsu et al., 2008). However,
these do not capture discourse-level relations.
</bodyText>
<page confidence="0.994093">
72
</page>
<bodyText confidence="0.999932671232877">
Polanyi and Zaenen (2006) observe that a cen-
tral topic may be divided into subtopics in or-
der to perform evaluations. Similar to Somasun-
daran et al. (2008), Asher et al. (2008) advo-
cate a discourse-level analysis in order to get a
deeper understanding of contextual polarity and
the strength of opinions. However, these works do
not provide an implementation for their insights.
In this work we demonstrate a concrete way that
discourse-level interpretation can improve recog-
nition of individual opinions and their polarities.
Graph-based approaches for joint inference in
sentiment analysis have been explored previously
by many researchers. The biggest difference be-
tween this work and theirs is in what the links
represent linguistically. Some of these are not
related to discourse at all (e.g., lexical similari-
ties (Takamura et al., 2007), morphosyntactic sim-
ilarities (Popescu and Etzioni, 2005) and word
based measures like TF-IDF (Goldberg and Zhu,
2006)). Some of these work on sentence cohesion
(Pang and Lee, 2004) or agreement/disagreement
between speakers (Thomas et al., 2006; Bansal
et al., 2008). Our model is not based on sen-
tence cohesion or structural adjacency. The re-
lations due to the opinion frames are based on
relationships between targets and discourse-level
functions of opinions being mutually reinforcing
or non-reinforcing. Adjacent instances need not be
related via opinion frames, while long distant rela-
tions can be present if opinion targets are same or
alternatives. Also, previous efforts in graph-based
joint inference in opinion analysis has been text-
based, while our work is over multi-party conver-
sations.
McDonald et al. (2007) propose a joint model
for sentiment classification based on relations de-
fined by granularity (sentence and document).
Snyder and Barzilay (2007) combine an agree-
ment model based on contrastive RST relations
with a local aspect (topic) model. Their aspects
would be related as same and their high contrast
relations would correspond to (a subset of) the
non-reinforcing frames.
In the field of product review mining, senti-
ments and features (aspects or targets) have been
mined (for example, Yi et al. (2003), Popescu and
Etzioni (2005), and Hu and Liu (2006)). More re-
cently there has been work on creating joint mod-
els of topic and sentiments (Mei et al., 2007; Titov
and McDonald, 2008) to improve topic-sentiment
summaries. We do not model topics; instead we
directly model the relations between targets. The
focus of our work is to jointly model opinion po-
larities via target relations. The task of finding co-
referent opinion topics by (Stoyanov and Cardie,
2008) is similar to our target link classification
task, and we use somewhat similar features. Even
though their genre is different, we plan to experi-
ment with their full feature set for improving our
TLC system.
Turning to collective classification, there have
been various collective classification frameworks
proposed (for example, Neville and Jensen (2000),
Lu and Getoor (2003), Taskar et al. (2004),
Richardson and Domingos (2006)). In this pa-
per, we use an approach proposed by (Bilgic et
al., 2007) which iteratively predicts class and link
existence using local classifiers. Other joint mod-
els used in sentiment classification include the spin
model (Takamura et al., 2007), relaxation labeling
(Popescu and Etzioni, 2005), and label propaga-
tion (Goldberg and Zhu, 2006).
</bodyText>
<sectionHeader confidence="0.998377" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999964391304348">
This work uses an opinion graph framework,
DLOG, to create an interdependent classifica-
tion of polarity and discourse relations. We em-
ployed this graph to augment lexicon-based meth-
ods to improve polarity classification. We found
that polarity classification in multi-party conver-
sations benefits from opinion lexicons, unigram
and dialog-act information. We found that the
DLOGs are valuable for further improving polar-
ity classification, even with partial neighborhood
information. Our experiments showed three to
five percentage points improvement in F-measure
with link information, and 15 percentage point
improvement with full neighborhood information.
These results show that lexical and discourse in-
formation are non-redundant for polarity classi-
fication, and our DLOG, that employs both, im-
proves performance.
We discovered that link classification is a dif-
ficult problem. Here again, we found that by us-
ing the DLOG framework, and using even partial
neighborhood information, improvements can be
achieved.
</bodyText>
<sectionHeader confidence="0.999226" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9935865">
N. Asher, F. Benamara, and Y. Mathieu. 2008. Dis-
tilling opinion in discourse: A preliminary study.
</reference>
<page confidence="0.99948">
73
</page>
<bodyText confidence="0.964790363636364">
L. Polanyi and A. Zaenen, 2006. Contextual Valence
Shifters. Computing Attitude and Affect in Text:
Theory and Applications.
A.-M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In HLT-
EMNLP 2005.
COLING-2008.
M. Bansal, C. Cardie, and L. Lee. 2008. The power of
negative thinking: Exploiting label disagreement in
the min-cut classification framework. In COLING-
2008.
</bodyText>
<reference confidence="0.999826471910113">
M. Bilgic, G. M. Namata, and L. Getoor. 2007. Com-
bining collective classification and link prediction.
In Workshop on Mining Graphs and Complex Struc-
tures at the IEEE International Conference on Data
Mining.
J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meetings Corpus. In
Proceedings of the Measuring Behavior Symposium
on ”Annotating and measuring Meeting Behavior”.
A. Devitt and K. Ahmad. 2007. Sentiment polarity
identification in financial news: A cohesion-based
approach. In ACL 2007.
A. B. Goldberg and X. Zhu. 2006. Seeing stars
when there aren’t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
M. Hu and B. Liu. 2006. Opinion extraction and sum-
marization on the Web. In 21st National Conference
on Artificial Intelligence (AAAI-2006).
H. Kanayama and T. Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented sen-
timent analysis. In EMNLP-2006, pages 355–363,
Sydney, Australia.
A. Kennedy and D. Inkpen. 2006. Sentiment classi-
fication of movie reviews using contextual valence
shifters. Computational Intelligence, 22(2):110–
125.
Q. Lu and L. Getoor. 2003. Link-based classification.
In Proceedings of the International Conference on
Machine Learning (ICML).
R. McDonald, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. 2007. Structured models for fine-to-
coarse sentiment analysis. In ACL 2007.
Q. Mei, X. Ling, M. Wondra, H. Su, and C Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In WWW ’07. ACM.
J. Neville and D. Jensen. 2000. Iterative classifica-
tion in relational data. In In Proc. AAAI-2000 Work-
shop on Learning Statistical Models from Relational
Data, pages 13–20. AAAI Press.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACl 2004.
R. Prasad, A. Lee, N. Dinesh, E. Miltsakaki, G. Cam-
pion, A. Joshi, and B. Webber. 2008. Penn dis-
course treebank version 2.0. Linguistic Data Con-
sortium.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Mach. Learn., 62(1-2):107–136.
K. Sadamitsu, S. Sekine, and M. Yamamoto. 2008.
Sentiment analysis based on probabilistic models us-
ing inter-sentence information. In LREC’08.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In HLT 2007:
NAACL.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In
SIGdial Workshop on Discourse and Dialogue 2007.
S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008.
Discourse level opinion interpretation. In Coling
2008.
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Coling 2008.
H. Takamura, T. Inui, and M. Okumura. 2007. Extract-
ing semantic orientations of phrases from dictionary.
In HLT-NAACL 2007.
B. Taskar, M. Wong, P. Abbeel, and D. Koller. 2004.
Link prediction in relational data. In Neural Infor-
mation Processing Systems.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from con-
gressional floor-debate transcripts. In EMNLP 2006.
I. Titov and R. McDonald. 2008. A joint model of text
and aspect ratings for sentiment summarization. In
ACL 2008.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In HLT-EMNLP 2005.
I. H. Witten and E. Frank. 2002. Data mining: practi-
cal machine learning tools and techniques with java
implementations. SIGMODRec., 31(1):76–77.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a
given topic using natural language processing tech-
niques. In ICDM-2003.
</reference>
<page confidence="0.999136">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.270346">
<title confidence="0.99683">Graphs for Polarity and Discourse Classification</title>
<author confidence="0.851523">Galileo Namata</author>
<affiliation confidence="0.99976">Univ. of Maryland</affiliation>
<address confidence="0.999915">College Park, MD 20742</address>
<email confidence="0.999639">namatag@cs.umd.edu</email>
<author confidence="0.786036">Swapna Somasundaran</author>
<affiliation confidence="0.999929">Univ. of Pittsburgh</affiliation>
<address confidence="0.999207">Pittsburgh, PA 15260</address>
<email confidence="0.846447">swapna@cs.pitt.edu</email>
<author confidence="0.814372">Lise Getoor</author>
<affiliation confidence="0.991753">Univ. of</affiliation>
<address confidence="0.982079">College Park, MD</address>
<email confidence="0.99921">getoor@cs.umd.edu</email>
<author confidence="0.701591">Janyce Wiebe</author>
<affiliation confidence="0.999915">Univ. of Pittsburgh</affiliation>
<address confidence="0.999269">Pittsburgh, PA 15260</address>
<email confidence="0.863225">wiebe@cs.pitt.edu</email>
<abstract confidence="0.998991">This work shows how to construct discourse-level opinion graphs to perform a joint interpretation of opinions and discourse relations. Specifically, our opinion graphs enable us to factor in discourse information for polarity classification, and polarity information for discourse-link classification. This inter-dependent framework can be used to augment and improve the performance of local polarity and discourse-link classifiers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Asher</author>
<author>F Benamara</author>
<author>Y Mathieu</author>
</authors>
<title>Distilling opinion in discourse: A preliminary study.</title>
<date>2008</date>
<contexts>
<context position="30871" citStr="Asher et al. (2008)" startWordPosition="5014" endWordPosition="5017">ssifiers, the performance of these classifiers improve considerably. This overall observed trend is similar to that observed with the polarity classifiers. 6 Related Work Previous work on polarity disambiguation has used contextual clues and reversal words (Wilson et al., 2005; Kennedy and Inkpen, 2006; Kanayama and Nasukawa, 2006; Devitt and Ahmad, 2007; Sadamitsu et al., 2008). However, these do not capture discourse-level relations. 72 Polanyi and Zaenen (2006) observe that a central topic may be divided into subtopics in order to perform evaluations. Similar to Somasundaran et al. (2008), Asher et al. (2008) advocate a discourse-level analysis in order to get a deeper understanding of contextual polarity and the strength of opinions. However, these works do not provide an implementation for their insights. In this work we demonstrate a concrete way that discourse-level interpretation can improve recognition of individual opinions and their polarities. Graph-based approaches for joint inference in sentiment analysis have been explored previously by many researchers. The biggest difference between this work and theirs is in what the links represent linguistically. Some of these are not related to d</context>
</contexts>
<marker>Asher, Benamara, Mathieu, 2008</marker>
<rawString>N. Asher, F. Benamara, and Y. Mathieu. 2008. Distilling opinion in discourse: A preliminary study.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bilgic</author>
<author>G M Namata</author>
<author>L Getoor</author>
</authors>
<title>Combining collective classification and link prediction.</title>
<date>2007</date>
<booktitle>In Workshop on Mining Graphs and Complex Structures at the IEEE International Conference on Data Mining.</booktitle>
<contexts>
<context position="1390" citStr="Bilgic et al., 2007" startWordPosition="190" endWordPosition="193">d discourse-link classifiers. 1 Introduction Much research in opinion analysis has focused on information from words, phrases and semantic orientation lexicons to perform sentiment classification. While these are vital for opinion analysis, they do not capture discourse-level associations that arise from relations between opinions. To capture this information, we propose discourse-level opinion graphs for classifying opinion polarity. In order to build our computational model, we combine a linguistic scheme opinion frames (Somasundaran et al., 2008) with a collective classification framework (Bilgic et al., 2007). According to this scheme, two opinions are related in the discourse when their targets (what they are about) are related. Further, these pair-wise discourse-level relations between opinions are either reinforcing or non-reinforcing frames. Reinforcing frames capture reinforcing discourse scenarios where the individual opinions reinforce one another, contributing to the same opinion polarity or stance. Non-reinforcing frames, on the other hand, capture discourse scenarios where the individual opinions do not support the same stance. The individual opinion polarities and the type of relation ∗</context>
<context position="12126" citStr="Bilgic et al (2007)" startWordPosition="1930" endWordPosition="1933">ative or neutral. The TLC determines if a given node pair has related targets and whether they are linked by a same or alternative relation. The FLC determines if a given node pair is related via frames, and whether it is a reinforcing or non-reinforcing link. As we saw in the example, there are local clues available for each unit to arrive at its classification. The discourse augments this information to aid in further disambiguation. 4 Collective Classification Framework For our collective classification framework, we use a variant of the iterative classification algorithm (ICA) proposed by Bilgic et al (2007). It combines several common prediction tasks in graphs: object classification (predicting the label of an object) and link prediction (predicting the existence and class of a link between objects). For our tasks, object classification directly corresponds to predicting opinion polarity and the link prediction corresponds to predicting the existence of a same or alternative target link or a reinforcing or non-reinforcing frame link between opinions. We note that given the nature of our problem formulation and approach, we use the terms link prediction and link classification interchangeably. I</context>
<context position="33832" citStr="Bilgic et al., 2007" startWordPosition="5479" endWordPosition="5482">l opinion polarities via target relations. The task of finding coreferent opinion topics by (Stoyanov and Cardie, 2008) is similar to our target link classification task, and we use somewhat similar features. Even though their genre is different, we plan to experiment with their full feature set for improving our TLC system. Turning to collective classification, there have been various collective classification frameworks proposed (for example, Neville and Jensen (2000), Lu and Getoor (2003), Taskar et al. (2004), Richardson and Domingos (2006)). In this paper, we use an approach proposed by (Bilgic et al., 2007) which iteratively predicts class and link existence using local classifiers. Other joint models used in sentiment classification include the spin model (Takamura et al., 2007), relaxation labeling (Popescu and Etzioni, 2005), and label propagation (Goldberg and Zhu, 2006). 7 Conclusion This work uses an opinion graph framework, DLOG, to create an interdependent classification of polarity and discourse relations. We employed this graph to augment lexicon-based methods to improve polarity classification. We found that polarity classification in multi-party conversations benefits from opinion le</context>
</contexts>
<marker>Bilgic, Namata, Getoor, 2007</marker>
<rawString>M. Bilgic, G. M. Namata, and L. Getoor. 2007. Combining collective classification and link prediction. In Workshop on Mining Graphs and Complex Structures at the IEEE International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>S Ashby</author>
<author>S Bourban</author>
<author>M Flynn</author>
<author>M Guillemot</author>
<author>T Hain</author>
<author>J Kadlec</author>
<author>V Karaiskos</author>
<author>W Kraaij</author>
<author>M Kronenthal</author>
<author>G Lathoud</author>
<author>M Lincoln</author>
<author>A Lisowska</author>
<author>I McCowan</author>
<author>W Post</author>
<author>D Reidsma</author>
<author>P Wellner</author>
</authors>
<title>The AMI Meetings Corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of the Measuring Behavior Symposium on ”Annotating and measuring Meeting Behavior”.</booktitle>
<contexts>
<context position="24020" citStr="Carletta et al., 2005" startWordPosition="3856" endWordPosition="3859">bout neighborhood target links; the FLC classifier uses true target-links and polarity information, and previous-stage classifications for information about neighborhood frame-links. 5.1 Data For our experiments, we use the opinion frame annotations from previous work (Somasundaran et al., 2008). These annotations consist of the opinion spans that reveal opinions, their targets, the polarity information for opinions, the labeled links between the targets and the frame links between the opinions. The annotated data consists of 7 scenario-based, multi-party meetings from the AMI meeting corpus (Carletta et al., 2005). The manual Dialog Act (DA) annotations, provided by AMI, segment the meeting transcription into separate dialog acts. We use these DAs as nodes or instances in our opinion graph. A DA is assigned the opinion orientation of the words it contains (for example, if a DA contains a positive opinion expression, then the DA assigned the positive opinion category). We filter out very small DAs (DAs with fewer than 3 tokens, punctuation included) in order to alleviate data skewness problem in the link classifiers. This gives us a total of 4606 DA instances, of which 1935 (42%) have opinions. Out of t</context>
</contexts>
<marker>Carletta, Ashby, Bourban, Flynn, Guillemot, Hain, Kadlec, Karaiskos, Kraaij, Kronenthal, Lathoud, Lincoln, Lisowska, McCowan, Post, Reidsma, Wellner, 2005</marker>
<rawString>J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner. 2005. The AMI Meetings Corpus. In Proceedings of the Measuring Behavior Symposium on ”Annotating and measuring Meeting Behavior”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Devitt</author>
<author>K Ahmad</author>
</authors>
<title>Sentiment polarity identification in financial news: A cohesion-based approach.</title>
<date>2007</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="30608" citStr="Devitt and Ahmad, 2007" startWordPosition="4969" endWordPosition="4973"> the data, accuracy of all classifiers is high; however, the macro Fmeasure, which depends on the F1 of the minority classes, is poor for the ICA-noInfo. Note, however, that when we provide some (Partial) or full (LinkNeigh) neighborhood information for the Link classifiers, the performance of these classifiers improve considerably. This overall observed trend is similar to that observed with the polarity classifiers. 6 Related Work Previous work on polarity disambiguation has used contextual clues and reversal words (Wilson et al., 2005; Kennedy and Inkpen, 2006; Kanayama and Nasukawa, 2006; Devitt and Ahmad, 2007; Sadamitsu et al., 2008). However, these do not capture discourse-level relations. 72 Polanyi and Zaenen (2006) observe that a central topic may be divided into subtopics in order to perform evaluations. Similar to Somasundaran et al. (2008), Asher et al. (2008) advocate a discourse-level analysis in order to get a deeper understanding of contextual polarity and the strength of opinions. However, these works do not provide an implementation for their insights. In this work we demonstrate a concrete way that discourse-level interpretation can improve recognition of individual opinions and thei</context>
</contexts>
<marker>Devitt, Ahmad, 2007</marker>
<rawString>A. Devitt and K. Ahmad. 2007. Sentiment polarity identification in financial news: A cohesion-based approach. In ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Goldberg</author>
<author>X Zhu</author>
</authors>
<title>Seeing stars when there aren’t many stars: Graph-based semisupervised learning for sentiment categorization.</title>
<date>2006</date>
<booktitle>In HLT-NAACL 2006 Workshop on Textgraphs: Graphbased Algorithms for Natural Language Processing.</booktitle>
<contexts>
<context position="31657" citStr="Goldberg and Zhu, 2006" startWordPosition="5132" endWordPosition="5135">e an implementation for their insights. In this work we demonstrate a concrete way that discourse-level interpretation can improve recognition of individual opinions and their polarities. Graph-based approaches for joint inference in sentiment analysis have been explored previously by many researchers. The biggest difference between this work and theirs is in what the links represent linguistically. Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007), morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006)). Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008). Our model is not based on sentence cohesion or structural adjacency. The relations due to the opinion frames are based on relationships between targets and discourse-level functions of opinions being mutually reinforcing or non-reinforcing. Adjacent instances need not be related via opinion frames, while long distant relations can be present if opinion targets are same or alternatives. Also, previous efforts in graph-based joint inference in opi</context>
<context position="34105" citStr="Goldberg and Zhu, 2006" startWordPosition="5519" endWordPosition="5522">ith their full feature set for improving our TLC system. Turning to collective classification, there have been various collective classification frameworks proposed (for example, Neville and Jensen (2000), Lu and Getoor (2003), Taskar et al. (2004), Richardson and Domingos (2006)). In this paper, we use an approach proposed by (Bilgic et al., 2007) which iteratively predicts class and link existence using local classifiers. Other joint models used in sentiment classification include the spin model (Takamura et al., 2007), relaxation labeling (Popescu and Etzioni, 2005), and label propagation (Goldberg and Zhu, 2006). 7 Conclusion This work uses an opinion graph framework, DLOG, to create an interdependent classification of polarity and discourse relations. We employed this graph to augment lexicon-based methods to improve polarity classification. We found that polarity classification in multi-party conversations benefits from opinion lexicons, unigram and dialog-act information. We found that the DLOGs are valuable for further improving polarity classification, even with partial neighborhood information. Our experiments showed three to five percentage points improvement in F-measure with link information</context>
</contexts>
<marker>Goldberg, Zhu, 2006</marker>
<rawString>A. B. Goldberg and X. Zhu. 2006. Seeing stars when there aren’t many stars: Graph-based semisupervised learning for sentiment categorization. In HLT-NAACL 2006 Workshop on Textgraphs: Graphbased Algorithms for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Opinion extraction and summarization on the Web.</title>
<date>2006</date>
<booktitle>In 21st National Conference on Artificial Intelligence (AAAI-2006).</booktitle>
<contexts>
<context position="32922" citStr="Hu and Liu (2006)" startWordPosition="5331" endWordPosition="5334"> is over multi-party conversations. McDonald et al. (2007) propose a joint model for sentiment classification based on relations defined by granularity (sentence and document). Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (topic) model. Their aspects would be related as same and their high contrast relations would correspond to (a subset of) the non-reinforcing frames. In the field of product review mining, sentiments and features (aspects or targets) have been mined (for example, Yi et al. (2003), Popescu and Etzioni (2005), and Hu and Liu (2006)). More recently there has been work on creating joint models of topic and sentiments (Mei et al., 2007; Titov and McDonald, 2008) to improve topic-sentiment summaries. We do not model topics; instead we directly model the relations between targets. The focus of our work is to jointly model opinion polarities via target relations. The task of finding coreferent opinion topics by (Stoyanov and Cardie, 2008) is similar to our target link classification task, and we use somewhat similar features. Even though their genre is different, we plan to experiment with their full feature set for improving</context>
</contexts>
<marker>Hu, Liu, 2006</marker>
<rawString>M. Hu and B. Liu. 2006. Opinion extraction and summarization on the Web. In 21st National Conference on Artificial Intelligence (AAAI-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kanayama</author>
<author>T Nasukawa</author>
</authors>
<title>Fully automatic lexicon expansion for domain-oriented sentiment analysis.</title>
<date>2006</date>
<booktitle>In EMNLP-2006,</booktitle>
<pages>355--363</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="16002" citStr="Kanayama and Nasukawa, 2006" startWordPosition="2574" endWordPosition="2577">* TLC, FLC Are both nodes from same speaker * TLC, FLC Bag of words for each node TLC, FLC Anaphoric indicator in the second node TLC Adjacency pair between the node pair FLC Discourse relation between node pair * FLC Table 1: Features and the classification task it is used for; TLC = target-link classification, FLC = Frame-link classification tures. We use lexicons that have been successfully used in previous work (the polarity lexicon from (Wilson et al., 2005) and the arguing lexicon (Somasundaran et al., 2007)). Previous work used features based on parse trees, e.g., (Wilson et al., 2005; Kanayama and Nasukawa, 2006), but our data has very different characteristics from monologic texts – the utterances and sentences are much shorter, and there are frequent disfluencies, restarts, hedging and repetitions. Because of this, we cannot rely on parsing features. On the other hand, in this data, we have dialog act information1 (Dialog Acts), which we can exploit. Note that the IPC uses only the Dialog Act tags (instance level tags like Inform, Suggest) and not the dialog structure information. Opinion frame detection between sentences has been previously attempted (Somasundaran et al., 2008) by using features th</context>
<context position="30584" citStr="Kanayama and Nasukawa, 2006" startWordPosition="4965" endWordPosition="4968">-M). Due to the heavy skew in the data, accuracy of all classifiers is high; however, the macro Fmeasure, which depends on the F1 of the minority classes, is poor for the ICA-noInfo. Note, however, that when we provide some (Partial) or full (LinkNeigh) neighborhood information for the Link classifiers, the performance of these classifiers improve considerably. This overall observed trend is similar to that observed with the polarity classifiers. 6 Related Work Previous work on polarity disambiguation has used contextual clues and reversal words (Wilson et al., 2005; Kennedy and Inkpen, 2006; Kanayama and Nasukawa, 2006; Devitt and Ahmad, 2007; Sadamitsu et al., 2008). However, these do not capture discourse-level relations. 72 Polanyi and Zaenen (2006) observe that a central topic may be divided into subtopics in order to perform evaluations. Similar to Somasundaran et al. (2008), Asher et al. (2008) advocate a discourse-level analysis in order to get a deeper understanding of contextual polarity and the strength of opinions. However, these works do not provide an implementation for their insights. In this work we demonstrate a concrete way that discourse-level interpretation can improve recognition of indi</context>
</contexts>
<marker>Kanayama, Nasukawa, 2006</marker>
<rawString>H. Kanayama and T. Nasukawa. 2006. Fully automatic lexicon expansion for domain-oriented sentiment analysis. In EMNLP-2006, pages 355–363, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kennedy</author>
<author>D Inkpen</author>
</authors>
<title>Sentiment classification of movie reviews using contextual valence shifters.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>125</pages>
<contexts>
<context position="30555" citStr="Kennedy and Inkpen, 2006" startWordPosition="4961" endWordPosition="4964">ll (R-M) and F-measure (F1-M). Due to the heavy skew in the data, accuracy of all classifiers is high; however, the macro Fmeasure, which depends on the F1 of the minority classes, is poor for the ICA-noInfo. Note, however, that when we provide some (Partial) or full (LinkNeigh) neighborhood information for the Link classifiers, the performance of these classifiers improve considerably. This overall observed trend is similar to that observed with the polarity classifiers. 6 Related Work Previous work on polarity disambiguation has used contextual clues and reversal words (Wilson et al., 2005; Kennedy and Inkpen, 2006; Kanayama and Nasukawa, 2006; Devitt and Ahmad, 2007; Sadamitsu et al., 2008). However, these do not capture discourse-level relations. 72 Polanyi and Zaenen (2006) observe that a central topic may be divided into subtopics in order to perform evaluations. Similar to Somasundaran et al. (2008), Asher et al. (2008) advocate a discourse-level analysis in order to get a deeper understanding of contextual polarity and the strength of opinions. However, these works do not provide an implementation for their insights. In this work we demonstrate a concrete way that discourse-level interpretation ca</context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>A. Kennedy and D. Inkpen. 2006. Sentiment classification of movie reviews using contextual valence shifters. Computational Intelligence, 22(2):110– 125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Lu</author>
<author>L Getoor</author>
</authors>
<title>Link-based classification.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="33708" citStr="Lu and Getoor (2003)" startWordPosition="5457" endWordPosition="5460">s. We do not model topics; instead we directly model the relations between targets. The focus of our work is to jointly model opinion polarities via target relations. The task of finding coreferent opinion topics by (Stoyanov and Cardie, 2008) is similar to our target link classification task, and we use somewhat similar features. Even though their genre is different, we plan to experiment with their full feature set for improving our TLC system. Turning to collective classification, there have been various collective classification frameworks proposed (for example, Neville and Jensen (2000), Lu and Getoor (2003), Taskar et al. (2004), Richardson and Domingos (2006)). In this paper, we use an approach proposed by (Bilgic et al., 2007) which iteratively predicts class and link existence using local classifiers. Other joint models used in sentiment classification include the spin model (Takamura et al., 2007), relaxation labeling (Popescu and Etzioni, 2005), and label propagation (Goldberg and Zhu, 2006). 7 Conclusion This work uses an opinion graph framework, DLOG, to create an interdependent classification of polarity and discourse relations. We employed this graph to augment lexicon-based methods to </context>
</contexts>
<marker>Lu, Getoor, 2003</marker>
<rawString>Q. Lu and L. Getoor. 2003. Link-based classification. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Hannan</author>
<author>T Neylon</author>
<author>M Wells</author>
<author>J Reynar</author>
</authors>
<title>Structured models for fine-tocoarse sentiment analysis.</title>
<date>2007</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="32363" citStr="McDonald et al. (2007)" startWordPosition="5241" endWordPosition="5244">ement between speakers (Thomas et al., 2006; Bansal et al., 2008). Our model is not based on sentence cohesion or structural adjacency. The relations due to the opinion frames are based on relationships between targets and discourse-level functions of opinions being mutually reinforcing or non-reinforcing. Adjacent instances need not be related via opinion frames, while long distant relations can be present if opinion targets are same or alternatives. Also, previous efforts in graph-based joint inference in opinion analysis has been textbased, while our work is over multi-party conversations. McDonald et al. (2007) propose a joint model for sentiment classification based on relations defined by granularity (sentence and document). Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (topic) model. Their aspects would be related as same and their high contrast relations would correspond to (a subset of) the non-reinforcing frames. In the field of product review mining, sentiments and features (aspects or targets) have been mined (for example, Yi et al. (2003), Popescu and Etzioni (2005), and Hu and Liu (2006)). More recently there has been work on c</context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>R. McDonald, K. Hannan, T. Neylon, M. Wells, and J. Reynar. 2007. Structured models for fine-tocoarse sentiment analysis. In ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Mei</author>
<author>X Ling</author>
<author>M Wondra</author>
<author>H Su</author>
<author>C Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In WWW ’07.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="33025" citStr="Mei et al., 2007" startWordPosition="5351" endWordPosition="5354">ation based on relations defined by granularity (sentence and document). Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (topic) model. Their aspects would be related as same and their high contrast relations would correspond to (a subset of) the non-reinforcing frames. In the field of product review mining, sentiments and features (aspects or targets) have been mined (for example, Yi et al. (2003), Popescu and Etzioni (2005), and Hu and Liu (2006)). More recently there has been work on creating joint models of topic and sentiments (Mei et al., 2007; Titov and McDonald, 2008) to improve topic-sentiment summaries. We do not model topics; instead we directly model the relations between targets. The focus of our work is to jointly model opinion polarities via target relations. The task of finding coreferent opinion topics by (Stoyanov and Cardie, 2008) is similar to our target link classification task, and we use somewhat similar features. Even though their genre is different, we plan to experiment with their full feature set for improving our TLC system. Turning to collective classification, there have been various collective classificatio</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Q. Mei, X. Ling, M. Wondra, H. Su, and C Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In WWW ’07. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Neville</author>
<author>D Jensen</author>
</authors>
<title>Iterative classification in relational data. In</title>
<date>2000</date>
<booktitle>In Proc. AAAI-2000 Workshop on Learning Statistical Models from Relational Data,</booktitle>
<pages>13--20</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="33686" citStr="Neville and Jensen (2000)" startWordPosition="5453" endWordPosition="5456">ve topic-sentiment summaries. We do not model topics; instead we directly model the relations between targets. The focus of our work is to jointly model opinion polarities via target relations. The task of finding coreferent opinion topics by (Stoyanov and Cardie, 2008) is similar to our target link classification task, and we use somewhat similar features. Even though their genre is different, we plan to experiment with their full feature set for improving our TLC system. Turning to collective classification, there have been various collective classification frameworks proposed (for example, Neville and Jensen (2000), Lu and Getoor (2003), Taskar et al. (2004), Richardson and Domingos (2006)). In this paper, we use an approach proposed by (Bilgic et al., 2007) which iteratively predicts class and link existence using local classifiers. Other joint models used in sentiment classification include the spin model (Takamura et al., 2007), relaxation labeling (Popescu and Etzioni, 2005), and label propagation (Goldberg and Zhu, 2006). 7 Conclusion This work uses an opinion graph framework, DLOG, to create an interdependent classification of polarity and discourse relations. We employed this graph to augment lex</context>
</contexts>
<marker>Neville, Jensen, 2000</marker>
<rawString>J. Neville and D. Jensen. 2000. Iterative classification in relational data. In In Proc. AAAI-2000 Workshop on Learning Statistical Models from Relational Data, pages 13–20. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In ACl</booktitle>
<contexts>
<context position="31720" citStr="Pang and Lee, 2004" startWordPosition="5143" endWordPosition="5146"> a concrete way that discourse-level interpretation can improve recognition of individual opinions and their polarities. Graph-based approaches for joint inference in sentiment analysis have been explored previously by many researchers. The biggest difference between this work and theirs is in what the links represent linguistically. Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007), morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006)). Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008). Our model is not based on sentence cohesion or structural adjacency. The relations due to the opinion frames are based on relationships between targets and discourse-level functions of opinions being mutually reinforcing or non-reinforcing. Adjacent instances need not be related via opinion frames, while long distant relations can be present if opinion targets are same or alternatives. Also, previous efforts in graph-based joint inference in opinion analysis has been textbased, while our work is over multi-</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In ACl 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>A Lee</author>
<author>N Dinesh</author>
<author>E Miltsakaki</author>
<author>G Campion</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>Penn discourse treebank version 2.0. Linguistic Data Consortium.</title>
<date>2008</date>
<contexts>
<context position="10431" citStr="Prasad et al., 2008" startWordPosition="1648" endWordPosition="1651"> where all the information is not available (between ergonomic and a bit different) there are multiple possibilities. Depending on the polarity, either a reinforcing frame (if a bit different has positive polarity) or a non-reinforcing frame (if a bit different has negative polarity) can exist. There are clues in the discourse that this passage represents a reinforcing scenario. For instance there are reinforcing frames between the first few opinions, the repeated use of “and” indicates a list, conjunction or expansion relation between clauses (according to the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008)), and there is a lack of contrastive clues that would indicate a change in the opinion. Thus the reinforcing frame link emerges as being the most likely candidate. This in turn disambiguates the polarity of a bit different. Thus, by establishing target links and frame links between the opinion instances, we are able to perform a joint interpretation of the opinions. The interdependent framework of this example is iterative and dynamic — the information in the nodes can be used to change the structure (i.e., establish new links), and the structure provides a framework to change node polarity. </context>
</contexts>
<marker>Prasad, Lee, Dinesh, Miltsakaki, Campion, Joshi, Webber, 2008</marker>
<rawString>R. Prasad, A. Lee, N. Dinesh, E. Miltsakaki, G. Campion, A. Joshi, and B. Webber. 2008. Penn discourse treebank version 2.0. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<pages>62--1</pages>
<location>Mach. Learn.,</location>
<contexts>
<context position="33762" citStr="Richardson and Domingos (2006)" startWordPosition="5465" endWordPosition="5468">ly model the relations between targets. The focus of our work is to jointly model opinion polarities via target relations. The task of finding coreferent opinion topics by (Stoyanov and Cardie, 2008) is similar to our target link classification task, and we use somewhat similar features. Even though their genre is different, we plan to experiment with their full feature set for improving our TLC system. Turning to collective classification, there have been various collective classification frameworks proposed (for example, Neville and Jensen (2000), Lu and Getoor (2003), Taskar et al. (2004), Richardson and Domingos (2006)). In this paper, we use an approach proposed by (Bilgic et al., 2007) which iteratively predicts class and link existence using local classifiers. Other joint models used in sentiment classification include the spin model (Takamura et al., 2007), relaxation labeling (Popescu and Etzioni, 2005), and label propagation (Goldberg and Zhu, 2006). 7 Conclusion This work uses an opinion graph framework, DLOG, to create an interdependent classification of polarity and discourse relations. We employed this graph to augment lexicon-based methods to improve polarity classification. We found that polarit</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>M. Richardson and P. Domingos. 2006. Markov logic networks. Mach. Learn., 62(1-2):107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sadamitsu</author>
<author>S Sekine</author>
<author>M Yamamoto</author>
</authors>
<title>Sentiment analysis based on probabilistic models using inter-sentence information.</title>
<date>2008</date>
<booktitle>In LREC’08.</booktitle>
<contexts>
<context position="30633" citStr="Sadamitsu et al., 2008" startWordPosition="4974" endWordPosition="4977">ll classifiers is high; however, the macro Fmeasure, which depends on the F1 of the minority classes, is poor for the ICA-noInfo. Note, however, that when we provide some (Partial) or full (LinkNeigh) neighborhood information for the Link classifiers, the performance of these classifiers improve considerably. This overall observed trend is similar to that observed with the polarity classifiers. 6 Related Work Previous work on polarity disambiguation has used contextual clues and reversal words (Wilson et al., 2005; Kennedy and Inkpen, 2006; Kanayama and Nasukawa, 2006; Devitt and Ahmad, 2007; Sadamitsu et al., 2008). However, these do not capture discourse-level relations. 72 Polanyi and Zaenen (2006) observe that a central topic may be divided into subtopics in order to perform evaluations. Similar to Somasundaran et al. (2008), Asher et al. (2008) advocate a discourse-level analysis in order to get a deeper understanding of contextual polarity and the strength of opinions. However, these works do not provide an implementation for their insights. In this work we demonstrate a concrete way that discourse-level interpretation can improve recognition of individual opinions and their polarities. Graph-based</context>
</contexts>
<marker>Sadamitsu, Sekine, Yamamoto, 2008</marker>
<rawString>K. Sadamitsu, S. Sekine, and M. Yamamoto. 2008. Sentiment analysis based on probabilistic models using inter-sentence information. In LREC’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>R Barzilay</author>
</authors>
<title>Multiple aspect ranking using the good grief algorithm.</title>
<date>2007</date>
<booktitle>In HLT 2007: NAACL.</booktitle>
<contexts>
<context position="32508" citStr="Snyder and Barzilay (2007)" startWordPosition="5262" endWordPosition="5265">elations due to the opinion frames are based on relationships between targets and discourse-level functions of opinions being mutually reinforcing or non-reinforcing. Adjacent instances need not be related via opinion frames, while long distant relations can be present if opinion targets are same or alternatives. Also, previous efforts in graph-based joint inference in opinion analysis has been textbased, while our work is over multi-party conversations. McDonald et al. (2007) propose a joint model for sentiment classification based on relations defined by granularity (sentence and document). Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (topic) model. Their aspects would be related as same and their high contrast relations would correspond to (a subset of) the non-reinforcing frames. In the field of product review mining, sentiments and features (aspects or targets) have been mined (for example, Yi et al. (2003), Popescu and Etzioni (2005), and Hu and Liu (2006)). More recently there has been work on creating joint models of topic and sentiments (Mei et al., 2007; Titov and McDonald, 2008) to improve topic-sentiment summaries. We do not model t</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>B. Snyder and R. Barzilay. 2007. Multiple aspect ranking using the good grief algorithm. In HLT 2007: NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Ruppenhofer</author>
<author>J Wiebe</author>
</authors>
<title>Detecting arguing and sentiment in meetings.</title>
<date>2007</date>
<booktitle>In SIGdial Workshop on Discourse and Dialogue</booktitle>
<contexts>
<context position="15893" citStr="Somasundaran et al., 2007" startWordPosition="2557" endWordPosition="2560"> node pair TLC,FLC Focus space overlap between the node pair TLC, FLC Bigram overlap between the node pair * TLC, FLC Are both nodes from same speaker * TLC, FLC Bag of words for each node TLC, FLC Anaphoric indicator in the second node TLC Adjacency pair between the node pair FLC Discourse relation between node pair * FLC Table 1: Features and the classification task it is used for; TLC = target-link classification, FLC = Frame-link classification tures. We use lexicons that have been successfully used in previous work (the polarity lexicon from (Wilson et al., 2005) and the arguing lexicon (Somasundaran et al., 2007)). Previous work used features based on parse trees, e.g., (Wilson et al., 2005; Kanayama and Nasukawa, 2006), but our data has very different characteristics from monologic texts – the utterances and sentences are much shorter, and there are frequent disfluencies, restarts, hedging and repetitions. Because of this, we cannot rely on parsing features. On the other hand, in this data, we have dialog act information1 (Dialog Acts), which we can exploit. Note that the IPC uses only the Dialog Act tags (instance level tags like Inform, Suggest) and not the dialog structure information. Opinion fra</context>
</contexts>
<marker>Somasundaran, Ruppenhofer, Wiebe, 2007</marker>
<rawString>S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007. Detecting arguing and sentiment in meetings. In SIGdial Workshop on Discourse and Dialogue 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
<author>J Ruppenhofer</author>
</authors>
<title>Discourse level opinion interpretation. In Coling</title>
<date>2008</date>
<contexts>
<context position="1325" citStr="Somasundaran et al., 2008" startWordPosition="179" endWordPosition="183">can be used to augment and improve the performance of local polarity and discourse-link classifiers. 1 Introduction Much research in opinion analysis has focused on information from words, phrases and semantic orientation lexicons to perform sentiment classification. While these are vital for opinion analysis, they do not capture discourse-level associations that arise from relations between opinions. To capture this information, we propose discourse-level opinion graphs for classifying opinion polarity. In order to build our computational model, we combine a linguistic scheme opinion frames (Somasundaran et al., 2008) with a collective classification framework (Bilgic et al., 2007). According to this scheme, two opinions are related in the discourse when their targets (what they are about) are related. Further, these pair-wise discourse-level relations between opinions are either reinforcing or non-reinforcing frames. Reinforcing frames capture reinforcing discourse scenarios where the individual opinions reinforce one another, contributing to the same opinion polarity or stance. Non-reinforcing frames, on the other hand, capture discourse scenarios where the individual opinions do not support the same sta</context>
<context position="5148" citStr="Somasundaran et al. (2008)" startWordPosition="775" endWordPosition="779">ype (sentiment or arguing). Sentiment opinions are evaluations, feelings or judgments about the target. Arguing opinions argue for or against something. Target links are labeled as either same or alternatives. Same links hold between targets that refer to the same entity or proposition, while alternative links hold between targets that are related by virtue of being opposing (mutually exclusive) options in the context of the discourse. The frame links correspond to the opinion frame relation between opinions. We illustrate the construction of the opinion graph with an example (Example 1, from Somasundaran et al. (2008)) from a multi-party meeting corpus where participants discuss and design a new TV remote control. The opinion expressions are in bold and their targets are in italics. Notice here that speaker D has a positive sentiment towards the rubbery material for the TV remote. (1) D::... this kind of rubbery material, it’s a bit more bouncy, like you said they get chucked around a lot. A bit more durable and that can also be ergonomic and it kind of feels a bit different from all the other remote controls. All the individual opinions in this example are essentially regarding the same thing – the rubber</context>
<context position="7276" citStr="Somasundaran et al. (2008)" startWordPosition="1132" endWordPosition="1136">rame type is derived from the details (type and polarity) of the opinions it relates and the target relation involved. Even though the different combinations of opinion type (sentiment and arguing), polarity (positive and negative) and target links (same and alternative) result in many distinct frames types (32 in total), they can be grouped, according to their discourse-level characteristics, into the two categories reinforcing and non-reinforcing. In this work, we only make this category distinction for opinion frames and the corresponding frame links. The next example (Example 2, also from Somasundaran et al. (2008)) illustrates an alternative target relation. In the domain of TV remote controls, the set of all shapes are alternatives to one another, since a remote control may have only one shape at a time. In such scenarios, a positive opinion regarding one choice may imply a negative opinion toward competing choices, and vice versa. In this passage, speaker C’s positive stance towards the curved shape is brought out even more strongly with his negative opinions toward the alternative, square-like, shapes. (2) C:: ... shapes should be curved, so round shapes. Nothing square-like. ... C:: ... So we shoul</context>
<context position="16581" citStr="Somasundaran et al., 2008" startWordPosition="2664" endWordPosition="2667">lson et al., 2005; Kanayama and Nasukawa, 2006), but our data has very different characteristics from monologic texts – the utterances and sentences are much shorter, and there are frequent disfluencies, restarts, hedging and repetitions. Because of this, we cannot rely on parsing features. On the other hand, in this data, we have dialog act information1 (Dialog Acts), which we can exploit. Note that the IPC uses only the Dialog Act tags (instance level tags like Inform, Suggest) and not the dialog structure information. Opinion frame detection between sentences has been previously attempted (Somasundaran et al., 2008) by using features that capture discourse and dialog continuity. Even though our link classification tasks are not directly comparable (the previous work performs binary classification of frame-present/frame-absent between opinion bearing sentences, while this work performs three-way classification: no-link/reinforcing/nonreinforcing between DA pairs), we adapt the features for the link classification tasks addressed here. These features depend on properties of the nodes that the link connects. We also create some new features that capture discourse relations and lexical overlap. Table 1 lists</context>
<context position="23694" citStr="Somasundaran et al., 2008" startWordPosition="3807" endWordPosition="3810">odes and other same-speaker nodes Presence of a target link y between the nodes Table 2: Relational features: x E {non-neutral (i.e., positive or negative), positive, negative}, y E {same, alt}, z E {reinforcing, non-reinforcing} uses true frame-links and polarity information, and previous-stage classifications for information about neighborhood target links; the FLC classifier uses true target-links and polarity information, and previous-stage classifications for information about neighborhood frame-links. 5.1 Data For our experiments, we use the opinion frame annotations from previous work (Somasundaran et al., 2008). These annotations consist of the opinion spans that reveal opinions, their targets, the polarity information for opinions, the labeled links between the targets and the frame links between the opinions. The annotated data consists of 7 scenario-based, multi-party meetings from the AMI meeting corpus (Carletta et al., 2005). The manual Dialog Act (DA) annotations, provided by AMI, segment the meeting transcription into separate dialog acts. We use these DAs as nodes or instances in our opinion graph. A DA is assigned the opinion orientation of the words it contains (for example, if a DA conta</context>
<context position="30850" citStr="Somasundaran et al. (2008)" startWordPosition="5009" endWordPosition="5013">information for the Link classifiers, the performance of these classifiers improve considerably. This overall observed trend is similar to that observed with the polarity classifiers. 6 Related Work Previous work on polarity disambiguation has used contextual clues and reversal words (Wilson et al., 2005; Kennedy and Inkpen, 2006; Kanayama and Nasukawa, 2006; Devitt and Ahmad, 2007; Sadamitsu et al., 2008). However, these do not capture discourse-level relations. 72 Polanyi and Zaenen (2006) observe that a central topic may be divided into subtopics in order to perform evaluations. Similar to Somasundaran et al. (2008), Asher et al. (2008) advocate a discourse-level analysis in order to get a deeper understanding of contextual polarity and the strength of opinions. However, these works do not provide an implementation for their insights. In this work we demonstrate a concrete way that discourse-level interpretation can improve recognition of individual opinions and their polarities. Graph-based approaches for joint inference in sentiment analysis have been explored previously by many researchers. The biggest difference between this work and theirs is in what the links represent linguistically. Some of these</context>
</contexts>
<marker>Somasundaran, Wiebe, Ruppenhofer, 2008</marker>
<rawString>S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008. Discourse level opinion interpretation. In Coling 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
</authors>
<title>Topic identification for fine-grained opinion analysis.</title>
<date>2008</date>
<booktitle>In Coling</booktitle>
<contexts>
<context position="33331" citStr="Stoyanov and Cardie, 2008" startWordPosition="5400" endWordPosition="5403">set of) the non-reinforcing frames. In the field of product review mining, sentiments and features (aspects or targets) have been mined (for example, Yi et al. (2003), Popescu and Etzioni (2005), and Hu and Liu (2006)). More recently there has been work on creating joint models of topic and sentiments (Mei et al., 2007; Titov and McDonald, 2008) to improve topic-sentiment summaries. We do not model topics; instead we directly model the relations between targets. The focus of our work is to jointly model opinion polarities via target relations. The task of finding coreferent opinion topics by (Stoyanov and Cardie, 2008) is similar to our target link classification task, and we use somewhat similar features. Even though their genre is different, we plan to experiment with their full feature set for improving our TLC system. Turning to collective classification, there have been various collective classification frameworks proposed (for example, Neville and Jensen (2000), Lu and Getoor (2003), Taskar et al. (2004), Richardson and Domingos (2006)). In this paper, we use an approach proposed by (Bilgic et al., 2007) which iteratively predicts class and link existence using local classifiers. Other joint models us</context>
</contexts>
<marker>Stoyanov, Cardie, 2008</marker>
<rawString>V. Stoyanov and C. Cardie. 2008. Topic identification for fine-grained opinion analysis. In Coling 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Takamura</author>
<author>T Inui</author>
<author>M Okumura</author>
</authors>
<title>Extracting semantic orientations of phrases from dictionary.</title>
<date>2007</date>
<booktitle>In HLT-NAACL</booktitle>
<contexts>
<context position="31538" citStr="Takamura et al., 2007" startWordPosition="5115" endWordPosition="5118"> to get a deeper understanding of contextual polarity and the strength of opinions. However, these works do not provide an implementation for their insights. In this work we demonstrate a concrete way that discourse-level interpretation can improve recognition of individual opinions and their polarities. Graph-based approaches for joint inference in sentiment analysis have been explored previously by many researchers. The biggest difference between this work and theirs is in what the links represent linguistically. Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007), morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006)). Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008). Our model is not based on sentence cohesion or structural adjacency. The relations due to the opinion frames are based on relationships between targets and discourse-level functions of opinions being mutually reinforcing or non-reinforcing. Adjacent instances need not be related via opinion frames, while long distant relations c</context>
<context position="34008" citStr="Takamura et al., 2007" startWordPosition="5505" endWordPosition="5508"> we use somewhat similar features. Even though their genre is different, we plan to experiment with their full feature set for improving our TLC system. Turning to collective classification, there have been various collective classification frameworks proposed (for example, Neville and Jensen (2000), Lu and Getoor (2003), Taskar et al. (2004), Richardson and Domingos (2006)). In this paper, we use an approach proposed by (Bilgic et al., 2007) which iteratively predicts class and link existence using local classifiers. Other joint models used in sentiment classification include the spin model (Takamura et al., 2007), relaxation labeling (Popescu and Etzioni, 2005), and label propagation (Goldberg and Zhu, 2006). 7 Conclusion This work uses an opinion graph framework, DLOG, to create an interdependent classification of polarity and discourse relations. We employed this graph to augment lexicon-based methods to improve polarity classification. We found that polarity classification in multi-party conversations benefits from opinion lexicons, unigram and dialog-act information. We found that the DLOGs are valuable for further improving polarity classification, even with partial neighborhood information. Our </context>
</contexts>
<marker>Takamura, Inui, Okumura, 2007</marker>
<rawString>H. Takamura, T. Inui, and M. Okumura. 2007. Extracting semantic orientations of phrases from dictionary. In HLT-NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>M Wong</author>
<author>P Abbeel</author>
<author>D Koller</author>
</authors>
<title>Link prediction in relational data.</title>
<date>2004</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="33730" citStr="Taskar et al. (2004)" startWordPosition="5461" endWordPosition="5464">ics; instead we directly model the relations between targets. The focus of our work is to jointly model opinion polarities via target relations. The task of finding coreferent opinion topics by (Stoyanov and Cardie, 2008) is similar to our target link classification task, and we use somewhat similar features. Even though their genre is different, we plan to experiment with their full feature set for improving our TLC system. Turning to collective classification, there have been various collective classification frameworks proposed (for example, Neville and Jensen (2000), Lu and Getoor (2003), Taskar et al. (2004), Richardson and Domingos (2006)). In this paper, we use an approach proposed by (Bilgic et al., 2007) which iteratively predicts class and link existence using local classifiers. Other joint models used in sentiment classification include the spin model (Takamura et al., 2007), relaxation labeling (Popescu and Etzioni, 2005), and label propagation (Goldberg and Zhu, 2006). 7 Conclusion This work uses an opinion graph framework, DLOG, to create an interdependent classification of polarity and discourse relations. We employed this graph to augment lexicon-based methods to improve polarity class</context>
</contexts>
<marker>Taskar, Wong, Abbeel, Koller, 2004</marker>
<rawString>B. Taskar, M. Wong, P. Abbeel, and D. Koller. 2004. Link prediction in relational data. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thomas</author>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="31784" citStr="Thomas et al., 2006" startWordPosition="5151" endWordPosition="5154">recognition of individual opinions and their polarities. Graph-based approaches for joint inference in sentiment analysis have been explored previously by many researchers. The biggest difference between this work and theirs is in what the links represent linguistically. Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007), morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006)). Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008). Our model is not based on sentence cohesion or structural adjacency. The relations due to the opinion frames are based on relationships between targets and discourse-level functions of opinions being mutually reinforcing or non-reinforcing. Adjacent instances need not be related via opinion frames, while long distant relations can be present if opinion targets are same or alternatives. Also, previous efforts in graph-based joint inference in opinion analysis has been textbased, while our work is over multi-party conversations. McDonald et al. (2007) propose a joint mode</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>M. Thomas, B. Pang, and L. Lee. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>R McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="33052" citStr="Titov and McDonald, 2008" startWordPosition="5355" endWordPosition="5358">ations defined by granularity (sentence and document). Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (topic) model. Their aspects would be related as same and their high contrast relations would correspond to (a subset of) the non-reinforcing frames. In the field of product review mining, sentiments and features (aspects or targets) have been mined (for example, Yi et al. (2003), Popescu and Etzioni (2005), and Hu and Liu (2006)). More recently there has been work on creating joint models of topic and sentiments (Mei et al., 2007; Titov and McDonald, 2008) to improve topic-sentiment summaries. We do not model topics; instead we directly model the relations between targets. The focus of our work is to jointly model opinion polarities via target relations. The task of finding coreferent opinion topics by (Stoyanov and Cardie, 2008) is similar to our target link classification task, and we use somewhat similar features. Even though their genre is different, we plan to experiment with their full feature set for improving our TLC system. Turning to collective classification, there have been various collective classification frameworks proposed (for </context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>I. Titov and R. McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP</booktitle>
<contexts>
<context position="15841" citStr="Wilson et al., 2005" startWordPosition="2548" endWordPosition="2551">nces TLC, FLC Content word overlap between the node pair TLC,FLC Focus space overlap between the node pair TLC, FLC Bigram overlap between the node pair * TLC, FLC Are both nodes from same speaker * TLC, FLC Bag of words for each node TLC, FLC Anaphoric indicator in the second node TLC Adjacency pair between the node pair FLC Discourse relation between node pair * FLC Table 1: Features and the classification task it is used for; TLC = target-link classification, FLC = Frame-link classification tures. We use lexicons that have been successfully used in previous work (the polarity lexicon from (Wilson et al., 2005) and the arguing lexicon (Somasundaran et al., 2007)). Previous work used features based on parse trees, e.g., (Wilson et al., 2005; Kanayama and Nasukawa, 2006), but our data has very different characteristics from monologic texts – the utterances and sentences are much shorter, and there are frequent disfluencies, restarts, hedging and repetitions. Because of this, we cannot rely on parsing features. On the other hand, in this data, we have dialog act information1 (Dialog Acts), which we can exploit. Note that the IPC uses only the Dialog Act tags (instance level tags like Inform, Suggest) a</context>
<context position="30529" citStr="Wilson et al., 2005" startWordPosition="4956" endWordPosition="4960">precision (P-M), recall (R-M) and F-measure (F1-M). Due to the heavy skew in the data, accuracy of all classifiers is high; however, the macro Fmeasure, which depends on the F1 of the minority classes, is poor for the ICA-noInfo. Note, however, that when we provide some (Partial) or full (LinkNeigh) neighborhood information for the Link classifiers, the performance of these classifiers improve considerably. This overall observed trend is similar to that observed with the polarity classifiers. 6 Related Work Previous work on polarity disambiguation has used contextual clues and reversal words (Wilson et al., 2005; Kennedy and Inkpen, 2006; Kanayama and Nasukawa, 2006; Devitt and Ahmad, 2007; Sadamitsu et al., 2008). However, these do not capture discourse-level relations. 72 Polanyi and Zaenen (2006) observe that a central topic may be divided into subtopics in order to perform evaluations. Similar to Somasundaran et al. (2008), Asher et al. (2008) advocate a discourse-level analysis in order to get a deeper understanding of contextual polarity and the strength of opinions. However, these works do not provide an implementation for their insights. In this work we demonstrate a concrete way that discour</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In HLT-EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data mining: practical machine learning tools and techniques with java implementations.</title>
<date>2002</date>
<journal>SIGMODRec.,</journal>
<pages>31--1</pages>
<contexts>
<context position="26208" citStr="Witten and Frank, 2002" startWordPosition="4238" endWordPosition="4241">(6%) pairs have target links and 1264 (5.5%) pairs have frame links. We perform 7-fold cross-validation experiments, using the 7 meetings. In each fold, 6 meetings are used for training and one meeting is used for testing. 5.2 Classifiers Our baseline (Base) classifies the test data based on the distribution of the classes in the training data. Note that due to the heavily skewed nature of our link data, this classifier performs very poorly for minority class prediction, even though it may achieve good overall accuracy. For our local classifiers, we used the classifiers from the Weka toolkit (Witten and Frank, 2002). For opinion polarity, we used the Weka’s SVM implementation. For the target link and frame link classes, the huge class skew caused SVM to learn a trivial model and always predict the majority class. To address this, we used a cost sensitive classifier in Weka where we set the cost of misclassifying a less frequent class, A, to a more frequent class, B, 71 Base Local ICA LinkNeigh LinkOnly noInfo Acc 45.9 68.7 78.8 72.9 68.4 Class: neutral (majority class) Prec 61.2 76.3 83.9 78.2 73.5 Rec 61.5 83.9 89.6 89.1 86.6 F1 61.1 79.6 86.6 83.2 79.3 Class: positive polarity Prec 26.3 56.2 70.9 63.3 </context>
</contexts>
<marker>Witten, Frank, 2002</marker>
<rawString>I. H. Witten and E. Frank. 2002. Data mining: practical machine learning tools and techniques with java implementations. SIGMODRec., 31(1):76–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yi</author>
<author>T Nasukawa</author>
<author>R Bunescu</author>
<author>W Niblack</author>
</authors>
<title>Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques.</title>
<date>2003</date>
<booktitle>In ICDM-2003.</booktitle>
<contexts>
<context position="32871" citStr="Yi et al. (2003)" startWordPosition="5322" endWordPosition="5325">pinion analysis has been textbased, while our work is over multi-party conversations. McDonald et al. (2007) propose a joint model for sentiment classification based on relations defined by granularity (sentence and document). Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (topic) model. Their aspects would be related as same and their high contrast relations would correspond to (a subset of) the non-reinforcing frames. In the field of product review mining, sentiments and features (aspects or targets) have been mined (for example, Yi et al. (2003), Popescu and Etzioni (2005), and Hu and Liu (2006)). More recently there has been work on creating joint models of topic and sentiments (Mei et al., 2007; Titov and McDonald, 2008) to improve topic-sentiment summaries. We do not model topics; instead we directly model the relations between targets. The focus of our work is to jointly model opinion polarities via target relations. The task of finding coreferent opinion topics by (Stoyanov and Cardie, 2008) is similar to our target link classification task, and we use somewhat similar features. Even though their genre is different, we plan to e</context>
</contexts>
<marker>Yi, Nasukawa, Bunescu, Niblack, 2003</marker>
<rawString>J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003. Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques. In ICDM-2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>