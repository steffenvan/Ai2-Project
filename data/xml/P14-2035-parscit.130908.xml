<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000730">
<title confidence="0.997593">
Resolving Lexical Ambiguity in
Tensor Regression Models of Meaning
</title>
<author confidence="0.998577">
Dimitri Kartsaklis
</author>
<affiliation confidence="0.985493333333333">
University of Oxford
Department of
Computer Science
</affiliation>
<address confidence="0.9647325">
Wolfson Bldg, Parks Road
Oxford, OX1 3QD, UK
</address>
<email confidence="0.999069">
dimitri.kartsaklis@cs.ox.ac.uk
</email>
<author confidence="0.985202">
Nal Kalchbrenner
</author>
<affiliation confidence="0.985422">
University of Oxford
Department of
Computer Science
</affiliation>
<address confidence="0.964729">
Wolfson Bldg, Parks Road
Oxford, OX1 3QD, UK
</address>
<email confidence="0.998998">
nkalch@cs.ox.ac.uk
</email>
<author confidence="0.811312">
Mehrnoosh Sadrzadeh
</author>
<affiliation confidence="0.729544">
Queen Mary Univ. of London
School of Electronic Engineering
and Computer Science
</affiliation>
<address confidence="0.856393">
Mile End Road
London, E1 4NS, UK
</address>
<email confidence="0.999153">
mehrnoosh.sadrzadeh@qmul.ac.uk
</email>
<sectionHeader confidence="0.997395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999717857142857">
This paper provides a method for improv-
ing tensor-based compositional distribu-
tional models of meaning by the addition
of an explicit disambiguation step prior to
composition. In contrast with previous re-
search where this hypothesis has been suc-
cessfully tested against relatively simple
compositional models, in our work we use
a robust model trained with linear regres-
sion. The results we get in two experi-
ments show the superiority of the prior dis-
ambiguation method and suggest that the
effectiveness of this approach is model-
independent.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999074">
The provision of compositionality in distributional
models of meaning, where a word is represented as
a vector of co-occurrence counts with every other
word in the vocabulary, offers a solution to the
fact that no text corpus, regardless of its size, is
capable of providing reliable co-occurrence statis-
tics for anything but very short text constituents.
By composing the vectors for the words within
a sentence, we are still able to create a vectorial
representation for that sentence that is very useful
in a variety of natural language processing tasks,
such as paraphrase detection, sentiment analysis
or machine translation. Hence, given a sentence
w1w2 ... wn, a compositional distributional model
provides a function f such that:
</bodyText>
<equation confidence="0.729905">
→− s = f(−→w1, −→w2, . . . , −→wn) (1)
</equation>
<bodyText confidence="0.99994665">
where wi−→ is the distributional vector of the ith
word in the sentence and →−s the resulting compos-
ite sentential vector.
An interesting question that has attracted the at-
tention of researchers lately refers to the way in
which these models affect ambiguous words; in
other words, given a sentence such as “a man was
waiting by the bank”, we are interested to know to
what extent a composite vector can appropriately
reflect the intended use of word ‘bank’ in that con-
text, and how such a vector would differ, for exam-
ple, from the vector of the sentence “a fisherman
was waiting by the bank”.
Recent experimental evidence (Reddy et al.,
2011; Kartsaklis et al., 2013; Kartsaklis and
Sadrzadeh, 2013) suggests that for a number of
compositional models the introduction of a dis-
ambiguation step prior to the actual composi-
tional process results in better composite represen-
tations. In other words, the suggestion is that Eq.
</bodyText>
<equation confidence="0.932783">
1 should be replaced by:
→− s = f(φ(−→w1), φ(−→ w2), ... ,φ(−→ wn)) (2)
</equation>
<bodyText confidence="0.999893">
where the purpose of function φ is to return a dis-
ambiguated version of each word vector given the
rest of the context (e.g. all the other words in the
sentence). The composition operation, whatever
that could be, is then applied on these unambigu-
ous representations of the words, instead of the
original distributional vectors.
Until now this idea has been verified on rela-
tively simple compositional functions, usually in-
volving some form of element-wise operation be-
tween the word vectors, such as addition or mul-
tiplication. An exception to this is the work of
Kartsaklis and Sadrzadeh (2013), who apply Eq.
2 on partial tensor-based compositional models.
In a tensor-based model, relational words such
as verbs and adjectives are represented by multi-
linear maps; composition takes place as the ap-
plication of those maps on vectors representing
the arguments (usually nouns). What makes the
models of the above work ‘partial’ is that the au-
thors used simplified versions of the linear maps,
projected onto spaces of order lower than that re-
quired by the theoretical framework. As a result,
a certain amount of transformational power was
traded off for efficiency.
A potential explanation then for the effective-
ness of the proposed prior disambiguation method
can be sought on the limitations imposed by the
compositional models under test. After all, the
idea of having disambiguation emerge as a direct
</bodyText>
<page confidence="0.975795">
212
</page>
<bodyText confidence="0.940927681818182">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 212–217,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
consequence of the compositional process, with-
out the introduction of any explicit step, seems
more natural and closer to the way the human
mind resolves lexical ambiguities.
The purpose of this paper is to investigate
the hypothesis whether prior disambiguation is
important in a pure tensor-based compositional
model, where no simplifying assumptions have
been made. We create such a model by using lin-
ear regression, and we explain how an explicit dis-
ambiguation step can be introduced to this model
prior to composition. We then proceed by com-
paring the composite vectors produced by this ap-
proach with those produced by the model alone in
a number of experiments. The results show a clear
superiority of the priorly disambiguated models
following Eq. 2, confirming previous research and
suggesting that the reasons behind the success of
this approach are more fundamental than the form
of the compositional function.
</bodyText>
<sectionHeader confidence="0.945649" genericHeader="introduction">
2 Composition in distributional models
</sectionHeader>
<bodyText confidence="0.997613896551724">
Compositional distributional models of meaning
vary in sophistication, from simple element-wise
operations between vectors such as addition and
multiplication (Mitchell and Lapata, 2008) to deep
learning techniques based on neural networks
(Socher et al., 2011; Socher et al., 2012; Kalch-
brenner and Blunsom, 2013a). Tensor-based mod-
els, formalized by Coecke et al. (2010), comprise
a third class of models lying somewhere in be-
tween these two extremes. Under this setting rela-
tional words such as verbs and adjectives are rep-
resented by multi-linear maps (tensors of various
orders) acting on a number of arguments. An ad-
jective for example is a linear map f : N —* N
(where N is our basic vector space for nouns),
which takes as input a noun and returns a mod-
ified version of it. Since every map of this sort
can be represented by a matrix living in the ten-
sor product space N ® N, we now see that the
meaning of a phrase such as ‘red car’ is given by
red x �—*
car, where red is an adjective matrix and
x indicates matrix multiplication. The same con-
cept applies for functions of higher order, such as
a transitive verb (a function of two arguments, so
a tensor of order 3). For these cases, matrix mul-
tiplication generalizes to the more generic notion
of tensor contraction. The meaning of a sentence
such as ‘kids play games’ is computed as:
</bodyText>
<equation confidence="0.98007">
kidsT x play x ����—*
��—* games (3)
</equation>
<bodyText confidence="0.99955625">
where play here is an order-3 tensor (a “cube”)
and x now represents tensor contraction. A con-
cise introduction to compositional distributional
models can be found in (Kartsaklis, 2014).
</bodyText>
<sectionHeader confidence="0.879912" genericHeader="method">
3 Disambiguation and composition
</sectionHeader>
<bodyText confidence="0.997657541666667">
The idea of separating disambiguation from com-
position first appears in a work of Reddy et al.
(2011), where the authors show that the intro-
duction of an explicit disambiguation step prior
to simple element-wise composition is beneficial
for noun-noun compounds. Subsequent work by
Kartsaklis et al. (2013) reports very similar find-
ings for verb-object structures, again on additive
and multiplicative models. Finally, in (Kartsaklis
and Sadrzadeh, 2013) these experiments were ex-
tended to include tensor-based models following
the categorical framework of Coecke et al. (2010),
where again all “unambiguous” models present
superior performance compared to their “ambigu-
ous” versions.
However, in this last work one of the dimen-
sions of the tensors was kept empty (filled in
with zeros). This simplified the calculations but
also weakened the effectiveness of the multi-linear
maps. If, for example, instead of using an order-3
tensor for a transitive verb, one uses some of the
matrix instantiations of Kartsaklis and Sadrzadeh,
Eq. 3 is reduced to one of the following forms:
play O (kids (9 �����
</bodyText>
<equation confidence="0.8523405">
� games) , kids O (play x �����
� games)
T x play) O �����
games
</equation>
<bodyText confidence="0.999947533333333">
where symbol O denotes element-wise multipli-
cation and play is a matrix. Here, the model does
not fully exploit the space provided by the theo-
retical framework (i.e. an order-3 tensor), which
has two disadvantages: firstly, we lose space that
could hold valuable information about the verb in
this case and relational words in general; secondly,
the generally non-commutative tensor contraction
operation is now partly relying on element-wise
multiplication, which is commutative, thus forgets
(part of the) order of composition.
In the next section we will see how to apply lin-
ear regression in order to create full tensors for
verbs and use them for a compositional model that
avoids these pitfalls.
</bodyText>
<sectionHeader confidence="0.930688" genericHeader="method">
4 Creating tensors for verbs
</sectionHeader>
<bodyText confidence="0.999903666666667">
The essence of any tensor-based compositional
model is the way we choose to create our sentence-
producing maps, i.e. the verbs. In this paper we
adopt a method proposed by Baroni and Zampar-
elli (2010) for building adjective matrices, which
can be generally applied to any relational word.
</bodyText>
<figure confidence="0.711811">
(kids
(4)
</figure>
<page confidence="0.995952">
213
</page>
<bodyText confidence="0.99161309375">
In order to create a matrix for, say, the intransi-
tive verb ‘play’, we first collect all instances of
the verb occurring with some subject in the train-
ing corpus, and then we create non-compositional
holistic vectors for these elementary sentences fol-
lowing exactly the same methodology as if they
were words. We now have a dataset with instances
of the form (−−−→
subji, −−−−−−→
subji play) (e.g. the vector of
‘kids’ paired with the holistic vector of ‘kids play’,
and so on), that can be used to train a linear regres-
sion model in order to produce an appropriate ma-
trix for verb ‘play’. The premise of a model like
this is that the multiplication of the verb matrix
with the vector of a new subject will produce a re-
sult that approximates the distributional behaviour
of all these elementary two-word exemplars used
in training.
We present examples and experiments based
on this method, constructing ambiguous and dis-
ambiguated tensors of order 2 (that is, matrices)
for verbs taking one argument. In principle, our
method is directly applicable to tensors of higher
order, following a multi-step process similar to
that of Grefenstette et al. (2013) who create order-
3 tensors for transitive verbs using similar means.
Instead of using subject-verb constructs as above
we concentrate on elementary verb phrases of the
form verb-object (e.g. ‘play football’, ‘admit stu-
dent’), since in general objects comprise stronger
contexts for disambiguating the usage of a verb.
</bodyText>
<sectionHeader confidence="0.998211" genericHeader="method">
5 Experimental setting
</sectionHeader>
<bodyText confidence="0.999991075">
Our basic vector space is trained from the ukWaC
corpus (Ferraresi et al., 2008), originally using as
a basis the 2,000 content words with the highest
frequency (but excluding a list of stop words as
well as the 50 most frequent content words since
they exhibit low information content). We cre-
ated vectors for all content words with at least
100 occurrences in the corpus. As context we
considered a 5-word window from either side of
the target word, while as our weighting scheme
we used local mutual information (i.e. point-wise
mutual information multiplied by raw counts).
This initial semantic space achieved a score of
0.77 Spearman’s ρ (and 0.71 Pearson’s r) on the
well-known benchmark dataset of Rubenstein and
Goodenough (1965). In order to reduce the time of
regression training, our vector space was normal-
ized and projected onto a 300-dimensional space
using singular value decomposition (SVD). The
performance of the reduced space on the R&amp;G
dataset was again very satisfying, specifically 0.73
Spearman’s ρ and 0.72 Pearson’s r.
In order to create the vector space of the holistic
verb phrase vectors, we first collected all instances
where a verb participating in the experiments ap-
peared at least 100 times in a verb-object relation-
ship with some noun in the corpus. As context of
a verb phrase we considered any content word that
falls into a 5-word window from either side of the
verb or the object. For the 68 verbs participating
in our experiments, this procedure resulted in 22k
verb phrases, a vector space that again was pro-
jected into 300 dimensions using SVD.
Linear regression For each verb we use simple
linear regression with gradient descent directly ap-
plied on matrices X and Y, where the rows of X
correspond to vectors of the nouns that appear as
objects for the given verb and the rows of Y to the
holistic vectors of the corresponding verb phrases.
Our objective function then becomes:
</bodyText>
<equation confidence="0.752281">
2m (111WXT − YT112 + λ11W112) (5)
</equation>
<bodyText confidence="0.999957666666667">
where m is the number of training examples and A
a regularization parameter. The matrix W is used
as the tensor for the specific verb.
</bodyText>
<sectionHeader confidence="0.994636" genericHeader="method">
6 Supervised disambiguation
</sectionHeader>
<bodyText confidence="0.999976666666667">
In our first experiment we test the effectiveness
of a prior disambiguation step for a tensor-based
model in a “sandbox” using supervised learning.
The goal is to create composite vectors for a num-
ber of elementary verb phrases of the form verb-
object with and without an explicit disambiguation
step, and evaluate which model approximates bet-
ter the holistic vectors of these verb phrases.
The verb phrases of our dataset are based on the
5 ambiguous verbs of Table 1. Each verb has been
combined with two different sets of nouns that ap-
pear in a verb-object relationship with that verb
in the corpus (a total of 343 verb phrases). The
nouns of each set have been manually selected in
order to explicitly represent a different meaning of
the verb. As an example, in the verb ‘play’ we im-
pose the two distinct meanings of using a musical
instrument and participating in a sport; so the first
</bodyText>
<subsectionHeader confidence="0.738287">
Verb Meaning 1 Meaning 2
</subsectionHeader>
<tableCaption confidence="0.856464">
break violate (56) break (22)
catch capture (28) be on time (21)
play musical instrument (47) sports (29)
admit permit to enter (12) acknowledge (25)
draw attract (64) sketch (39)
Table 1: Ambiguous verbs for the supervised task.
The numbers in parentheses refer to the collected
training examples for each case.
</tableCaption>
<equation confidence="0.821445">
Wˆ = arg min
w
</equation>
<page confidence="0.99351">
214
</page>
<bodyText confidence="0.999769814814815">
set of objects contains nouns such as ‘oboe’, ‘pi-
ano’, ‘guitar’, and so on, while in the second set
we see nouns such as ‘football’, ’baseball” etc.
In more detail, the creation of the dataset was
done in the following way: First, all verb entries
with more than one definition in the Oxford Junior
Dictionary (Sansome et al., 2000) were collected
into a list. Next, a linguist (native speaker of En-
glish) annotated the semantic difference between
the definitions of each verb in a scale from 1 (sim-
ilar) to 5 (distinct). Only verbs with definitions
exhibiting completely distinct meanings (marked
with 5) were kept for the next step. For each one
of these verbs, a list was constructed with all the
nouns that appear at least 50 times under a verb-
object relationship in the corpus with the specific
verb. Then, each object in the list was manually
annotated as exclusively belonging to one of the
two senses; so, an object could be selected only if
it was related to a single sense, but not both. For
example, ‘attention’ was a valid object for the at-
tract sense of verb ‘draw’, since it is unrelated to
the sketch sense of that verb. On the other hand,
‘car’ is not an appropriate object for either sense
of ‘draw’, since it could actually appear under both
of them in different contexts. The verbs of Table
1 were the ones with the highest numbers of ex-
emplars per sense, creating a dataset of significant
size for the intended task (each holistic vector is
compared with 343 composite vectors).
We proceed as follows: We apply linear regres-
sion in order to train verb matrices using jointly
the object sets for both meanings of each verb, as
well as separately—so in this latter case we get
two matrices for each verb, one for each sense. For
each verb phrase, we create a composite vector by
matrix-multiplying the verb matrix with the vector
of the specific object. Then we use 4-fold cross
validation to evaluate which version of composite
vectors (the one created by the ambiguous tensors
or the one created by the unambiguous ones) ap-
proximates better the holistic vectors of the verb
phrases in our test set. This is done by comparing
each holistic vector with all the composite ones,
and then evaluating the rank of the correct com-
posite vector within the list of results.
In order to get a proper mixing of objects from
both senses of a verb in training and testing sets,
we set the cross-validation process as follows: We
first split both sets of objects in 4 parts. For each
fold then, our training set is comprised by 43 of set
#1 plus 43of set #2, while the test set consists of
the remaining 41 of set #1 plus 41 of set #2. The
data points of the training set are presented in the
</bodyText>
<table confidence="0.997346142857143">
Accuracy MRR Avg Sim
Amb. Dis. Amb. Dis. Amb. Dis.
break 0.19 0.28 0.41 0.50 0.41 0.43
catch 0.35 0.37 0.58 0.61 0.51 0.57
play 0.20 0.28 0.41 0.49 0.60 0.68
admit 0.33 0.43 0.57 0.64 0.41 0.46
draw 0.24 0.29 0.45 0.51 0.40 0.44
</table>
<tableCaption confidence="0.9358055">
Table 2: Results for the supervised task. ‘Amb.’
refers to models without the explicit disambigua-
</tableCaption>
<bodyText confidence="0.9319511">
tion step, and ‘Dis.’ to models with that step.
learning algorithm in random order.
We measure approximation in three different
metrics. The first one, accuracy, is the strictest,
and evaluates in how many cases the composite
vector of a verb phrase is the closest one (the first
one in the result list) to the corresponding holistic
vector. A more relaxed and perhaps more repre-
sentative method is to calculate the mean recipro-
cal rank (MRR), which is given by:
</bodyText>
<equation confidence="0.910018">
1 (6)
rankz
</equation>
<bodyText confidence="0.999919454545454">
where m is the number of objects and rankz refers
to the rank of the correct composite vector for the
ith object.
Finally, a third way to evaluate the efficiency of
each model is to simply calculate the average co-
sine similarity between every holistic vector and
its corresponding composite vector. The results
are presented in Table 2, reflecting a clear supe-
riority (p &lt; 0.001 for average cosine similarity)
of the prior disambiguation method for every verb
and every metric.
</bodyText>
<sectionHeader confidence="0.967093" genericHeader="method">
7 Unsupervised disambiguation
</sectionHeader>
<bodyText confidence="0.999943473684211">
In Section 6 we used a controlled procedure to col-
lect genuinely ambiguous verbs and we trained our
models from manually annotated data. In this sec-
tion we briefly outline how the process of creat-
ing tensors for distinct senses of a verb can be au-
tomated, and we test this idea on a generic verb
phrase similarity task.
First, we use unsupervised learning in order to
detect the latent senses of each verb in the corpus,
following a procedure first described by Sch¨utze
(1998). For every occurrence of the verb, we cre-
ate a vector representing the surrounding context
by averaging the vectors of every other word in
the same sentence. Then, we apply hierarchical
agglomerative clustering (HAC) in order to cluster
these context vectors, hoping that different groups
of contexts will correspond to the different senses
under which the word has been used in the corpus.
The clustering algorithm uses Ward’s method as
</bodyText>
<equation confidence="0.994076">
MRR = 1 m
m z=1
</equation>
<page confidence="0.994722">
215
</page>
<bodyText confidence="0.999953192307692">
inter-cluster measure, and Pearson correlation for
measuring the distance of vectors within a clus-
ter. Since HAC returns a dendrogram embedding
all possible groupings, we measure the quality of
each partitioning by using the variance ratio crite-
rion (Cali´nski and Harabasz, 1974) and we select
the partitioning that achieves the best score (so the
number of senses varies from verb to verb).
The next step is to classify every noun that has
been used as an object with that verb to the most
probable verb sense, and then use these sets of
nouns as before for training tensors for the vari-
ous verb senses. Being equipped with a number of
sense clusters created as above for every verb, the
classification of each object to a relevant sense is
based on the cosine distance of the object vector
from the centroids of the clusters.1 Every sense
with less than 3 training exemplars is merged to
the dominant sense of the verb. The union of all
object sets is used for training a single unambigu-
ous tensor for the verb. As usual, data points are
presented to learning algorithm in random order.
No objects in our test set are used for training.
We test this system on a verb phase similarity
task introduced in (Mitchell and Lapata, 2010).
The goal is to assess the similarity between pairs
of short verb phrases (verb-object constructs) and
evaluate the results against human annotations.
The dataset consists of 72 verb phrases, paired
in three different ways to form groups of various
degrees of phrase similarity—a total of 108 verb
phrase pairs.
The experiment has the following form: For ev-
ery pair of verb phrases, we construct composite
vectors and then we evaluate their cosine similar-
ity. For the ambiguous regression model, the com-
position is done by matrix-multiplying the am-
biguous verb matrix (learned by the union of all
object sets) with the vector of the noun. For the
disambiguated version, we first detect the most
probable sense of the verb given the noun, again
by comparing the vector of the noun with the
centroids of the verb clusters; then, we matrix-
multiply the corresponding unambiguous tensor
created exclusively from objects that have been
classified as closer to this specific sense of the
verb with the noun. We also test a number
of baselines: the ‘verbs-only’ model is a non-
compositional baseline where only the two verbs
are compared; ‘additive’ and ‘multiplicative’ com-
pose the word vectors of each phrase by applying
simple element-wise operations.
</bodyText>
<footnote confidence="0.9711755">
1In general, our approach is quite close to the multi-
prototype models of Reisinger and Mooney (2010).
</footnote>
<table confidence="0.998811875">
Model Spearman’s ρ
Verbs-only 0.331
Additive 0.379
Multiplicative 0.301
Linear regression (ambiguous) 0.349
Linear regression (disamb.) 0.399
Holistic verb phrase vectors 0.403
Human agreement 0.550
</table>
<tableCaption confidence="0.907594333333333">
Table 3: Results for the phrase similarity task. The
difference between the ambiguous and the disam-
biguated version is s.s. with p &lt; 0.001.
</tableCaption>
<bodyText confidence="0.999592739130435">
The results are presented in Table 3, where
again the version with the prior disambiguation
step shows performance superior to that of the am-
biguous version. There are two interesting obser-
vations that can be made on the basis of Table
3. First of all, the regression model is based on
the assumption that the holistic vectors of the ex-
emplar verb phrases follow an ideal distributional
behaviour that the model aims to approximate as
close as possible. The results of Table 3 confirm
this: using just the holistic vectors of the corre-
sponding verb phrases (no composition is involved
here) returns the best correlation with human an-
notations (0.403), providing a proof that the holis-
tic vectors of the verb phrases are indeed reli-
able representations of each verb phrase’s mean-
ing. Next, observe that the prior disambiguation
model approximates this behaviour very closely
(0.399) on unseen data, with a difference not sta-
tistically significant. This is very important, since
a regression model can only perform as well as its
training dataset allows it; and in our case this is
achieved to a very satisfactory level.
</bodyText>
<sectionHeader confidence="0.978174" genericHeader="conclusions">
8 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.99993525">
This paper adds to existing evidence from previ-
ous research that the introduction of an explicit
disambiguation step before the composition im-
proves the quality of the produced composed rep-
resentations. The use of a robust regression model
rejects the hypothesis that the proposed methodol-
ogy is helpful only for relatively “weak” composi-
tional approaches. As for future work, an interest-
ing direction would be to see how a prior disam-
biguation step can affect deep learning composi-
tional settings similar to (Socher et al., 2012) and
(Kalchbrenner and Blunsom, 2013b).
</bodyText>
<sectionHeader confidence="0.997748" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99978075">
We would like to thank the three anonymous
reviewers for their fruitful comments. Support
by EPSRC grant EP/F042728/1 is gratefully ac-
knowledged by D. Kartsaklis and M. Sadrzadeh.
</bodyText>
<page confidence="0.99833">
216
</page>
<sectionHeader confidence="0.99642" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99977362962963">
M. Baroni and R. Zamparelli. 2010. Nouns are Vec-
tors, Adjectives are Matrices. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
T. Cali´nski and J. Harabasz. 1974. A Dendrite Method
for Cluster Analysis. Communications in Statistics-
Theory and Methods, 3(1):1–27.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical Foundations for Distributed Compositional
Model of Meaning. Lambek Festschrift. Linguistic
Analysis, 36:345–384.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4) Can we beat Google, pages 47–54.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
N. Kalchbrenner and P. Blunsom. 2013a. Recurrent
convolutional neural networks for discourse compo-
sitionality. In Proceedings of the 2013 Workshop on
Continuous Vector Space Models and their Compo-
sitionality, Sofia, Bulgaria, August.
Nal Kalchbrenner and Phil Blunsom. 2013b. Re-
current continuous translation models. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Seattle,
USA, October. Association for Computational Lin-
guistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Seattle, USA, October.
D. Kartsaklis, M. Sadrzadeh, and S. Pulman. 2013.
Separating Disambiguation from Composition in
Distributional Semantics. In Proceedings of 17th
Conference on Computational Natural Language
Learning (CoNLL-2013), Sofia, Bulgaria, August.
Dimitri Kartsaklis. 2014. Compositional operators in
distributional semantics. Springer Science Reviews,
April. DOI: 10.1007/s40362-014-0017-z.
J. Mitchell and M. Lapata. 2008. Vector-based Mod-
els of Semantic Composition. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics, pages 236–244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1439.
Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static pro-
totype vectors for semantic composition. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 705–713.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109–117. Association for Computational Lin-
guistics.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual Correlates of Synonymy. Communications of
the ACM, 8(10):627–633.
R. Sansome, D. Reid, and A. Spooner. 2000. The Ox-
ford Junior Dictionary. Oxford University Press.
H. Sch¨utze. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics, 24:97–123.
R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and
C.D. Manning. 2011. Dynamic Pooling and Un-
folding Recursive Autoencoders for Paraphrase De-
tection. Advances in Neural Information Processing
Systems, 24.
R. Socher, B. Huval, C. Manning, and Ng. A.
2012. Semantic compositionality through recursive
matrix-vector spaces. In Conference on Empirical
Methods in Natural Language Processing 2012.
</reference>
<page confidence="0.998367">
217
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.049945">
<title confidence="0.9992835">Resolving Lexical Ambiguity Tensor Regression Models of Meaning</title>
<author confidence="0.99669">Dimitri</author>
<affiliation confidence="0.995865">University of Department Computer</affiliation>
<address confidence="0.8997715">Wolfson Bldg, Parks Oxford, OX1 3QD, UK</address>
<email confidence="0.996604">dimitri.kartsaklis@cs.ox.ac.uk</email>
<affiliation confidence="0.9144865">Nal University of Department Computer</affiliation>
<address confidence="0.8993235">Wolfson Bldg, Parks Oxford, OX1 3QD, UK</address>
<email confidence="0.985649">nkalch@cs.ox.ac.uk</email>
<title confidence="0.557524">Mehrnoosh</title>
<author confidence="0.633803">of</author>
<affiliation confidence="0.917921">School of Electronic</affiliation>
<title confidence="0.403936">and Computer</title>
<author confidence="0.721049">Mile End</author>
<address confidence="0.99123">London, E1 4NS, UK</address>
<email confidence="0.997575">mehrnoosh.sadrzadeh@qmul.ac.uk</email>
<abstract confidence="0.9821914">This paper provides a method for improving tensor-based compositional distributional models of meaning by the addition of an explicit disambiguation step prior to composition. In contrast with previous research where this hypothesis has been successfully tested against relatively simple compositional models, in our work we use a robust model trained with linear regression. The results we get in two experiments show the superiority of the prior disambiguation method and suggest that the effectiveness of this approach is modelindependent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>R Zamparelli</author>
</authors>
<title>Nouns are Vectors, Adjectives are Matrices.</title>
<date>2010</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="9136" citStr="Baroni and Zamparelli (2010)" startWordPosition="1478" endWordPosition="1482"> and relational words in general; secondly, the generally non-commutative tensor contraction operation is now partly relying on element-wise multiplication, which is commutative, thus forgets (part of the) order of composition. In the next section we will see how to apply linear regression in order to create full tensors for verbs and use them for a compositional model that avoids these pitfalls. 4 Creating tensors for verbs The essence of any tensor-based compositional model is the way we choose to create our sentenceproducing maps, i.e. the verbs. In this paper we adopt a method proposed by Baroni and Zamparelli (2010) for building adjective matrices, which can be generally applied to any relational word. (kids (4) 213 In order to create a matrix for, say, the intransitive verb ‘play’, we first collect all instances of the verb occurring with some subject in the training corpus, and then we create non-compositional holistic vectors for these elementary sentences following exactly the same methodology as if they were words. We now have a dataset with instances of the form (−−−→ subji, −−−−−−→ subji play) (e.g. the vector of ‘kids’ paired with the holistic vector of ‘kids play’, and so on), that can be used t</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>M. Baroni and R. Zamparelli. 2010. Nouns are Vectors, Adjectives are Matrices. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cali´nski</author>
<author>J Harabasz</author>
</authors>
<title>A Dendrite Method for Cluster Analysis.</title>
<date>1974</date>
<booktitle>Communications in StatisticsTheory and Methods,</booktitle>
<pages>3--1</pages>
<marker>Cali´nski, Harabasz, 1974</marker>
<rawString>T. Cali´nski and J. Harabasz. 1974. A Dendrite Method for Cluster Analysis. Communications in StatisticsTheory and Methods, 3(1):1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coecke</author>
<author>M Sadrzadeh</author>
<author>S Clark</author>
</authors>
<title>Mathematical Foundations for Distributed Compositional Model of Meaning. Lambek Festschrift. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--345</pages>
<contexts>
<context position="5809" citStr="Coecke et al. (2010)" startWordPosition="913" endWordPosition="916">priorly disambiguated models following Eq. 2, confirming previous research and suggesting that the reasons behind the success of this approach are more fundamental than the form of the compositional function. 2 Composition in distributional models Compositional distributional models of meaning vary in sophistication, from simple element-wise operations between vectors such as addition and multiplication (Mitchell and Lapata, 2008) to deep learning techniques based on neural networks (Socher et al., 2011; Socher et al., 2012; Kalchbrenner and Blunsom, 2013a). Tensor-based models, formalized by Coecke et al. (2010), comprise a third class of models lying somewhere in between these two extremes. Under this setting relational words such as verbs and adjectives are represented by multi-linear maps (tensors of various orders) acting on a number of arguments. An adjective for example is a linear map f : N —* N (where N is our basic vector space for nouns), which takes as input a noun and returns a modified version of it. Since every map of this sort can be represented by a matrix living in the tensor product space N ® N, we now see that the meaning of a phrase such as ‘red car’ is given by red x �—* car, whe</context>
<context position="7618" citStr="Coecke et al. (2010)" startWordPosition="1225" endWordPosition="1228">s, 2014). 3 Disambiguation and composition The idea of separating disambiguation from composition first appears in a work of Reddy et al. (2011), where the authors show that the introduction of an explicit disambiguation step prior to simple element-wise composition is beneficial for noun-noun compounds. Subsequent work by Kartsaklis et al. (2013) reports very similar findings for verb-object structures, again on additive and multiplicative models. Finally, in (Kartsaklis and Sadrzadeh, 2013) these experiments were extended to include tensor-based models following the categorical framework of Coecke et al. (2010), where again all “unambiguous” models present superior performance compared to their “ambiguous” versions. However, in this last work one of the dimensions of the tensors was kept empty (filled in with zeros). This simplified the calculations but also weakened the effectiveness of the multi-linear maps. If, for example, instead of using an order-3 tensor for a transitive verb, one uses some of the matrix instantiations of Kartsaklis and Sadrzadeh, Eq. 3 is reduced to one of the following forms: play O (kids (9 ����� � games) , kids O (play x ����� � games) T x play) O ����� games where symbol</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Mathematical Foundations for Distributed Compositional Model of Meaning. Lambek Festschrift. Linguistic Analysis, 36:345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating ukWaC, a very large web-derived corpus of English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="10813" citStr="Ferraresi et al., 2008" startWordPosition="1756" endWordPosition="1759">that is, matrices) for verbs taking one argument. In principle, our method is directly applicable to tensors of higher order, following a multi-step process similar to that of Grefenstette et al. (2013) who create order3 tensors for transitive verbs using similar means. Instead of using subject-verb constructs as above we concentrate on elementary verb phrases of the form verb-object (e.g. ‘play football’, ‘admit student’), since in general objects comprise stronger contexts for disambiguating the usage of a verb. 5 Experimental setting Our basic vector space is trained from the ukWaC corpus (Ferraresi et al., 2008), originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content). We created vectors for all content words with at least 100 occurrences in the corpus. As context we considered a 5-word window from either side of the target word, while as our weighting scheme we used local mutual information (i.e. point-wise mutual information multiplied by raw counts). This initial semantic space achieved a score of 0.77 Spearman’s ρ (and 0.71 Pearson’s r) on the well-k</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google, pages 47–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Georgiana Dinu</author>
<author>Yao-Zhong Zhang</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Marco Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS</booktitle>
<contexts>
<context position="10392" citStr="Grefenstette et al. (2013)" startWordPosition="1691" endWordPosition="1694">del in order to produce an appropriate matrix for verb ‘play’. The premise of a model like this is that the multiplication of the verb matrix with the vector of a new subject will produce a result that approximates the distributional behaviour of all these elementary two-word exemplars used in training. We present examples and experiments based on this method, constructing ambiguous and disambiguated tensors of order 2 (that is, matrices) for verbs taking one argument. In principle, our method is directly applicable to tensors of higher order, following a multi-step process similar to that of Grefenstette et al. (2013) who create order3 tensors for transitive verbs using similar means. Instead of using subject-verb constructs as above we concentrate on elementary verb phrases of the form verb-object (e.g. ‘play football’, ‘admit student’), since in general objects comprise stronger contexts for disambiguating the usage of a verb. 5 Experimental setting Our basic vector space is trained from the ukWaC corpus (Ferraresi et al., 2008), originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhib</context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni. 2013. Multi-step regression learning for compositional distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kalchbrenner</author>
<author>P Blunsom</author>
</authors>
<title>Recurrent convolutional neural networks for discourse compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5750" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="903" endWordPosition="907">a number of experiments. The results show a clear superiority of the priorly disambiguated models following Eq. 2, confirming previous research and suggesting that the reasons behind the success of this approach are more fundamental than the form of the compositional function. 2 Composition in distributional models Compositional distributional models of meaning vary in sophistication, from simple element-wise operations between vectors such as addition and multiplication (Mitchell and Lapata, 2008) to deep learning techniques based on neural networks (Socher et al., 2011; Socher et al., 2012; Kalchbrenner and Blunsom, 2013a). Tensor-based models, formalized by Coecke et al. (2010), comprise a third class of models lying somewhere in between these two extremes. Under this setting relational words such as verbs and adjectives are represented by multi-linear maps (tensors of various orders) acting on a number of arguments. An adjective for example is a linear map f : N —* N (where N is our basic vector space for nouns), which takes as input a noun and returns a modified version of it. Since every map of this sort can be represented by a matrix living in the tensor product space N ® N, we now see that the meaning o</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>N. Kalchbrenner and P. Blunsom. 2013a. Recurrent convolutional neural networks for discourse compositionality. In Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, USA,</location>
<contexts>
<context position="5750" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="903" endWordPosition="907">a number of experiments. The results show a clear superiority of the priorly disambiguated models following Eq. 2, confirming previous research and suggesting that the reasons behind the success of this approach are more fundamental than the form of the compositional function. 2 Composition in distributional models Compositional distributional models of meaning vary in sophistication, from simple element-wise operations between vectors such as addition and multiplication (Mitchell and Lapata, 2008) to deep learning techniques based on neural networks (Socher et al., 2011; Socher et al., 2012; Kalchbrenner and Blunsom, 2013a). Tensor-based models, formalized by Coecke et al. (2010), comprise a third class of models lying somewhere in between these two extremes. Under this setting relational words such as verbs and adjectives are represented by multi-linear maps (tensors of various orders) acting on a number of arguments. An adjective for example is a linear map f : N —* N (where N is our basic vector space for nouns), which takes as input a noun and returns a modified version of it. Since every map of this sort can be represented by a matrix living in the tensor product space N ® N, we now see that the meaning o</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013b. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), Seattle, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Prior disambiguation of word tensors for constructing sentence vectors.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Seattle, USA,</location>
<contexts>
<context position="2557" citStr="Kartsaklis and Sadrzadeh, 2013" startWordPosition="400" endWordPosition="403">e and →−s the resulting composite sentential vector. An interesting question that has attracted the attention of researchers lately refers to the way in which these models affect ambiguous words; in other words, given a sentence such as “a man was waiting by the bank”, we are interested to know to what extent a composite vector can appropriately reflect the intended use of word ‘bank’ in that context, and how such a vector would differ, for example, from the vector of the sentence “a fisherman was waiting by the bank”. Recent experimental evidence (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013) suggests that for a number of compositional models the introduction of a disambiguation step prior to the actual compositional process results in better composite representations. In other words, the suggestion is that Eq. 1 should be replaced by: →− s = f(φ(−→w1), φ(−→ w2), ... ,φ(−→ wn)) (2) where the purpose of function φ is to return a disambiguated version of each word vector given the rest of the context (e.g. all the other words in the sentence). The composition operation, whatever that could be, is then applied on these unambiguous representations of the words, instead of the original</context>
<context position="7495" citStr="Kartsaklis and Sadrzadeh, 2013" startWordPosition="1207" endWordPosition="1210">be”) and x now represents tensor contraction. A concise introduction to compositional distributional models can be found in (Kartsaklis, 2014). 3 Disambiguation and composition The idea of separating disambiguation from composition first appears in a work of Reddy et al. (2011), where the authors show that the introduction of an explicit disambiguation step prior to simple element-wise composition is beneficial for noun-noun compounds. Subsequent work by Kartsaklis et al. (2013) reports very similar findings for verb-object structures, again on additive and multiplicative models. Finally, in (Kartsaklis and Sadrzadeh, 2013) these experiments were extended to include tensor-based models following the categorical framework of Coecke et al. (2010), where again all “unambiguous” models present superior performance compared to their “ambiguous” versions. However, in this last work one of the dimensions of the tensors was kept empty (filled in with zeros). This simplified the calculations but also weakened the effectiveness of the multi-linear maps. If, for example, instead of using an order-3 tensor for a transitive verb, one uses some of the matrix instantiations of Kartsaklis and Sadrzadeh, Eq. 3 is reduced to one </context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, 2013</marker>
<rawString>Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013. Prior disambiguation of word tensors for constructing sentence vectors. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), Seattle, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kartsaklis</author>
<author>M Sadrzadeh</author>
<author>S Pulman</author>
</authors>
<title>Separating Disambiguation from Composition in Distributional Semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of 17th Conference on Computational Natural Language Learning (CoNLL-2013),</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2524" citStr="Kartsaklis et al., 2013" startWordPosition="396" endWordPosition="399">e ith word in the sentence and →−s the resulting composite sentential vector. An interesting question that has attracted the attention of researchers lately refers to the way in which these models affect ambiguous words; in other words, given a sentence such as “a man was waiting by the bank”, we are interested to know to what extent a composite vector can appropriately reflect the intended use of word ‘bank’ in that context, and how such a vector would differ, for example, from the vector of the sentence “a fisherman was waiting by the bank”. Recent experimental evidence (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013) suggests that for a number of compositional models the introduction of a disambiguation step prior to the actual compositional process results in better composite representations. In other words, the suggestion is that Eq. 1 should be replaced by: →− s = f(φ(−→w1), φ(−→ w2), ... ,φ(−→ wn)) (2) where the purpose of function φ is to return a disambiguated version of each word vector given the rest of the context (e.g. all the other words in the sentence). The composition operation, whatever that could be, is then applied on these unambiguous representations of t</context>
<context position="7347" citStr="Kartsaklis et al. (2013)" startWordPosition="1187" endWordPosition="1190">ning of a sentence such as ‘kids play games’ is computed as: kidsT x play x ����—* ��—* games (3) where play here is an order-3 tensor (a “cube”) and x now represents tensor contraction. A concise introduction to compositional distributional models can be found in (Kartsaklis, 2014). 3 Disambiguation and composition The idea of separating disambiguation from composition first appears in a work of Reddy et al. (2011), where the authors show that the introduction of an explicit disambiguation step prior to simple element-wise composition is beneficial for noun-noun compounds. Subsequent work by Kartsaklis et al. (2013) reports very similar findings for verb-object structures, again on additive and multiplicative models. Finally, in (Kartsaklis and Sadrzadeh, 2013) these experiments were extended to include tensor-based models following the categorical framework of Coecke et al. (2010), where again all “unambiguous” models present superior performance compared to their “ambiguous” versions. However, in this last work one of the dimensions of the tensors was kept empty (filled in with zeros). This simplified the calculations but also weakened the effectiveness of the multi-linear maps. If, for example, instea</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, Pulman, 2013</marker>
<rawString>D. Kartsaklis, M. Sadrzadeh, and S. Pulman. 2013. Separating Disambiguation from Composition in Distributional Semantics. In Proceedings of 17th Conference on Computational Natural Language Learning (CoNLL-2013), Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
</authors>
<title>Compositional operators in distributional semantics.</title>
<date>2014</date>
<volume>DOI:</volume>
<pages>10--1007</pages>
<publisher>Springer</publisher>
<institution>Science Reviews,</institution>
<contexts>
<context position="7006" citStr="Kartsaklis, 2014" startWordPosition="1137" endWordPosition="1138">y red x �—* car, where red is an adjective matrix and x indicates matrix multiplication. The same concept applies for functions of higher order, such as a transitive verb (a function of two arguments, so a tensor of order 3). For these cases, matrix multiplication generalizes to the more generic notion of tensor contraction. The meaning of a sentence such as ‘kids play games’ is computed as: kidsT x play x ����—* ��—* games (3) where play here is an order-3 tensor (a “cube”) and x now represents tensor contraction. A concise introduction to compositional distributional models can be found in (Kartsaklis, 2014). 3 Disambiguation and composition The idea of separating disambiguation from composition first appears in a work of Reddy et al. (2011), where the authors show that the introduction of an explicit disambiguation step prior to simple element-wise composition is beneficial for noun-noun compounds. Subsequent work by Kartsaklis et al. (2013) reports very similar findings for verb-object structures, again on additive and multiplicative models. Finally, in (Kartsaklis and Sadrzadeh, 2013) these experiments were extended to include tensor-based models following the categorical framework of Coecke e</context>
</contexts>
<marker>Kartsaklis, 2014</marker>
<rawString>Dimitri Kartsaklis. 2014. Compositional operators in distributional semantics. Springer Science Reviews, April. DOI: 10.1007/s40362-014-0017-z.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Vector-based Models of Semantic Composition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="5623" citStr="Mitchell and Lapata, 2008" startWordPosition="883" endWordPosition="886">on. We then proceed by comparing the composite vectors produced by this approach with those produced by the model alone in a number of experiments. The results show a clear superiority of the priorly disambiguated models following Eq. 2, confirming previous research and suggesting that the reasons behind the success of this approach are more fundamental than the form of the compositional function. 2 Composition in distributional models Compositional distributional models of meaning vary in sophistication, from simple element-wise operations between vectors such as addition and multiplication (Mitchell and Lapata, 2008) to deep learning techniques based on neural networks (Socher et al., 2011; Socher et al., 2012; Kalchbrenner and Blunsom, 2013a). Tensor-based models, formalized by Coecke et al. (2010), comprise a third class of models lying somewhere in between these two extremes. Under this setting relational words such as verbs and adjectives are represented by multi-linear maps (tensors of various orders) acting on a number of arguments. An adjective for example is a linear map f : N —* N (where N is our basic vector space for nouns), which takes as input a noun and returns a modified version of it. Sinc</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>J. Mitchell and M. Lapata. 2008. Vector-based Models of Semantic Composition. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="20276" citStr="Mitchell and Lapata, 2010" startWordPosition="3411" endWordPosition="3414">eing equipped with a number of sense clusters created as above for every verb, the classification of each object to a relevant sense is based on the cosine distance of the object vector from the centroids of the clusters.1 Every sense with less than 3 training exemplars is merged to the dominant sense of the verb. The union of all object sets is used for training a single unambiguous tensor for the verb. As usual, data points are presented to learning algorithm in random order. No objects in our test set are used for training. We test this system on a verb phase similarity task introduced in (Mitchell and Lapata, 2010). The goal is to assess the similarity between pairs of short verb phrases (verb-object constructs) and evaluate the results against human annotations. The dataset consists of 72 verb phrases, paired in three different ways to form groups of various degrees of phrase similarity—a total of 108 verb phrase pairs. The experiment has the following form: For every pair of verb phrases, we construct composite vectors and then we evaluate their cosine similarity. For the ambiguous regression model, the composition is done by matrix-multiplying the ambiguous verb matrix (learned by the union of all ob</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Ioannis Klapaftis</author>
<author>Diana McCarthy</author>
<author>Suresh Manandhar</author>
</authors>
<title>Dynamic and static prototype vectors for semantic composition.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>705--713</pages>
<contexts>
<context position="2499" citStr="Reddy et al., 2011" startWordPosition="392" endWordPosition="395">utional vector of the ith word in the sentence and →−s the resulting composite sentential vector. An interesting question that has attracted the attention of researchers lately refers to the way in which these models affect ambiguous words; in other words, given a sentence such as “a man was waiting by the bank”, we are interested to know to what extent a composite vector can appropriately reflect the intended use of word ‘bank’ in that context, and how such a vector would differ, for example, from the vector of the sentence “a fisherman was waiting by the bank”. Recent experimental evidence (Reddy et al., 2011; Kartsaklis et al., 2013; Kartsaklis and Sadrzadeh, 2013) suggests that for a number of compositional models the introduction of a disambiguation step prior to the actual compositional process results in better composite representations. In other words, the suggestion is that Eq. 1 should be replaced by: →− s = f(φ(−→w1), φ(−→ w2), ... ,φ(−→ wn)) (2) where the purpose of function φ is to return a disambiguated version of each word vector given the rest of the context (e.g. all the other words in the sentence). The composition operation, whatever that could be, is then applied on these unambig</context>
<context position="7142" citStr="Reddy et al. (2011)" startWordPosition="1157" endWordPosition="1160">her order, such as a transitive verb (a function of two arguments, so a tensor of order 3). For these cases, matrix multiplication generalizes to the more generic notion of tensor contraction. The meaning of a sentence such as ‘kids play games’ is computed as: kidsT x play x ����—* ��—* games (3) where play here is an order-3 tensor (a “cube”) and x now represents tensor contraction. A concise introduction to compositional distributional models can be found in (Kartsaklis, 2014). 3 Disambiguation and composition The idea of separating disambiguation from composition first appears in a work of Reddy et al. (2011), where the authors show that the introduction of an explicit disambiguation step prior to simple element-wise composition is beneficial for noun-noun compounds. Subsequent work by Kartsaklis et al. (2013) reports very similar findings for verb-object structures, again on additive and multiplicative models. Finally, in (Kartsaklis and Sadrzadeh, 2013) these experiments were extended to include tensor-based models following the categorical framework of Coecke et al. (2010), where again all “unambiguous” models present superior performance compared to their “ambiguous” versions. However, in this</context>
</contexts>
<marker>Reddy, Klapaftis, McCarthy, Manandhar, 2011</marker>
<rawString>Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and Suresh Manandhar. 2011. Dynamic and static prototype vectors for semantic composition. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 705–713.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>109--117</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21623" citStr="Reisinger and Mooney (2010)" startWordPosition="3631" endWordPosition="3634">rb given the noun, again by comparing the vector of the noun with the centroids of the verb clusters; then, we matrixmultiply the corresponding unambiguous tensor created exclusively from objects that have been classified as closer to this specific sense of the verb with the noun. We also test a number of baselines: the ‘verbs-only’ model is a noncompositional baseline where only the two verbs are compared; ‘additive’ and ‘multiplicative’ compose the word vectors of each phrase by applying simple element-wise operations. 1In general, our approach is quite close to the multiprototype models of Reisinger and Mooney (2010). Model Spearman’s ρ Verbs-only 0.331 Additive 0.379 Multiplicative 0.301 Linear regression (ambiguous) 0.349 Linear regression (disamb.) 0.399 Holistic verb phrase vectors 0.403 Human agreement 0.550 Table 3: Results for the phrase similarity task. The difference between the ambiguous and the disambiguated version is s.s. with p &lt; 0.001. The results are presented in Table 3, where again the version with the prior disambiguation step shows performance superior to that of the ambiguous version. There are two interesting observations that can be made on the basis of Table 3. First of all, the re</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 109–117. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual Correlates of Synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="11471" citStr="Rubenstein and Goodenough (1965)" startWordPosition="1864" endWordPosition="1867">sis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content). We created vectors for all content words with at least 100 occurrences in the corpus. As context we considered a 5-word window from either side of the target word, while as our weighting scheme we used local mutual information (i.e. point-wise mutual information multiplied by raw counts). This initial semantic space achieved a score of 0.77 Spearman’s ρ (and 0.71 Pearson’s r) on the well-known benchmark dataset of Rubenstein and Goodenough (1965). In order to reduce the time of regression training, our vector space was normalized and projected onto a 300-dimensional space using singular value decomposition (SVD). The performance of the reduced space on the R&amp;G dataset was again very satisfying, specifically 0.73 Spearman’s ρ and 0.72 Pearson’s r. In order to create the vector space of the holistic verb phrase vectors, we first collected all instances where a verb participating in the experiments appeared at least 100 times in a verb-object relationship with some noun in the corpus. As context of a verb phrase we considered any content</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J.B. Goodenough. 1965. Contextual Correlates of Synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sansome</author>
<author>D Reid</author>
<author>A Spooner</author>
</authors>
<title>The Oxford Junior Dictionary.</title>
<date>2000</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="14432" citStr="Sansome et al., 2000" startWordPosition="2380" endWordPosition="2383"> (28) be on time (21) play musical instrument (47) sports (29) admit permit to enter (12) acknowledge (25) draw attract (64) sketch (39) Table 1: Ambiguous verbs for the supervised task. The numbers in parentheses refer to the collected training examples for each case. Wˆ = arg min w 214 set of objects contains nouns such as ‘oboe’, ‘piano’, ‘guitar’, and so on, while in the second set we see nouns such as ‘football’, ’baseball” etc. In more detail, the creation of the dataset was done in the following way: First, all verb entries with more than one definition in the Oxford Junior Dictionary (Sansome et al., 2000) were collected into a list. Next, a linguist (native speaker of English) annotated the semantic difference between the definitions of each verb in a scale from 1 (similar) to 5 (distinct). Only verbs with definitions exhibiting completely distinct meanings (marked with 5) were kept for the next step. For each one of these verbs, a list was constructed with all the nouns that appear at least 50 times under a verbobject relationship in the corpus with the specific verb. Then, each object in the list was manually annotated as exclusively belonging to one of the two senses; so, an object could be</context>
</contexts>
<marker>Sansome, Reid, Spooner, 2000</marker>
<rawString>R. Sansome, D. Reid, and A. Spooner. 2000. The Oxford Junior Dictionary. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<date>1998</date>
<booktitle>Automatic Word Sense Discrimination. Computational Linguistics,</booktitle>
<pages>24--97</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>H. Sch¨utze. 1998. Automatic Word Sense Discrimination. Computational Linguistics, 24:97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>E H Huang</author>
<author>J Pennington</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>24</pages>
<contexts>
<context position="5697" citStr="Socher et al., 2011" startWordPosition="895" endWordPosition="898">with those produced by the model alone in a number of experiments. The results show a clear superiority of the priorly disambiguated models following Eq. 2, confirming previous research and suggesting that the reasons behind the success of this approach are more fundamental than the form of the compositional function. 2 Composition in distributional models Compositional distributional models of meaning vary in sophistication, from simple element-wise operations between vectors such as addition and multiplication (Mitchell and Lapata, 2008) to deep learning techniques based on neural networks (Socher et al., 2011; Socher et al., 2012; Kalchbrenner and Blunsom, 2013a). Tensor-based models, formalized by Coecke et al. (2010), comprise a third class of models lying somewhere in between these two extremes. Under this setting relational words such as verbs and adjectives are represented by multi-linear maps (tensors of various orders) acting on a number of arguments. An adjective for example is a linear map f : N —* N (where N is our basic vector space for nouns), which takes as input a noun and returns a modified version of it. Since every map of this sort can be represented by a matrix living in the tens</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. 2011. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. Advances in Neural Information Processing Systems, 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing</booktitle>
<marker>A, 2012</marker>
<rawString>R. Socher, B. Huval, C. Manning, and Ng. A. 2012. Semantic compositionality through recursive matrix-vector spaces. In Conference on Empirical Methods in Natural Language Processing 2012.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>