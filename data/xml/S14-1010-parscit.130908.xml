<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002756">
<title confidence="0.990104">
Text Summarization through Entailment-based Minimum Vertex Cover
</title>
<author confidence="0.998632">
Anand Gupta&apos;, Manpreet Kaur2, Adarsh Singh2, Aseem Goel2, Shachar Mirkin3
</author>
<affiliation confidence="0.975239666666667">
&apos;Dept. of Information Technology, NSIT, New Delhi, India
2Dept. of Computer Engineering, NSIT, New Delhi, India
3Xerox Research Centre Europe, Meylan, France
</affiliation>
<sectionHeader confidence="0.988366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999666105263158">
Sentence Connectivity is a textual charac-
teristic that may be incorporated intelli-
gently for the selection of sentences of a
well meaning summary. However, the ex-
isting summarization methods do not uti-
lize its potential fully. The present pa-
per introduces a novel method for single-
document text summarization. It poses
the text summarization task as an opti-
mization problem, and attempts to solve
it using Weighted Minimum Vertex Cover
(WMVC), a graph-based algorithm. Tex-
tual entailment, an established indicator of
semantic relationships between text units,
is used to measure sentence connectivity
and construct the graph on which WMVC
operates. Experiments on a standard sum-
marization dataset show that the suggested
algorithm outperforms related methods.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981375">
In the present age of digital revolution with pro-
liferating numbers of internet-connected devices,
we are facing an exponential rise in the volume
of available information. Users are constantly fac-
ing the problem of deciding what to read and what
to skip. Text summarization provides a practical
solution to this problem, causing a resurgence in
research in this field.
Given a topic of interest, a standard search of-
ten yields a large number of documents. Many of
them are not of the user’s interest. Rather than go-
ing through the entire result-set, the reader may
read a gist of a document, produced via summa-
rization tools, and then decide whether to fully
read the document or not, thus saving a substan-
tial amount of time. According to Jones (2007),
a summary can be defined as “a reductive trans-
formation of source text to summary text through
content reduction by selection and/or generaliza-
tion on what is important in the source”. Summa-
rization based on content reduction by selection is
referred to as extraction (identifying and includ-
ing the important sentences in the final summary),
whereas a summary involving content reduction
by generalization is called abstraction (reproduc-
ing the most informative content in a new way).
The present paper focuses on extraction-based
single-document summarization. We formulate
the task as a graph-based optimization problem,
where vertices represent the sentences and edges
the connections between sentences. Textual en-
tailment (Giampiccolo et al., 2007) is employed to
estimate the degree of connectivity between sen-
tences, and subsequently to assign a weight to each
vertex of the graph. Then, the Weighted Mini-
mum Vertex Cover, a classical graph algorithm,
is used to find the minimal set of vertices (that is
– sentences) that forms a cover. The idea is that
such cover of well-connected vertices would cor-
respond to a cover of the salient content of the doc-
ument.
The rest of the paper is organized as follows: In
Section 2, we discuss related work and describe
the WMVC algorithm. In Section 3, we propose
a novel summarization method, and in Section 4,
experiments and results are presented. Finally, in
Section 5, we conclude and outline future research
directions.
</bodyText>
<sectionHeader confidence="0.987265" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999865888888889">
Extractive text summarization is the task of iden-
tifying those text segments which provide impor-
tant information about the gist of the document
– the salient units of the text. In (Marcu, 2008),
salient units are determined as the ones that con-
tain frequently-used words, contain words that are
within titles and headings, are located at the begin-
ning or at the end of sections, contain key phrases
and are the most highly connected to other parts
</bodyText>
<page confidence="0.990161">
75
</page>
<note confidence="0.760633">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 75–80,
Dublin, Ireland, August 23-24 2014.
</note>
<bodyText confidence="0.99933">
of the text. In this work we focus on the last of
the above criteria, connectivity, to find highly con-
nected sentences in a document. Such sentences
often contain information that is found in other
sentences, and are therefore natural candidates to
be included in the summary.
</bodyText>
<sectionHeader confidence="0.729309" genericHeader="related work">
2.1 Related Work
</sectionHeader>
<bodyText confidence="0.999695886363637">
The connectivity between sentences has been pre-
viously exploited for extraction-based summariza-
tion. Salton et al. (1997) generate intra-document
links between passages of a document using auto-
matic hypertext link generation algorithms. Mani
and Bloedorn (1997) use the number of shared
words, phrases and co-references to measure con-
nectedness among sentences. In (Barzilay and El-
hadad, 1999), lexical chains are constructed based
on words relatedness.
Textual entailment (TE) was exploited recently
for text summarization in order to find the highly
connected sentences in the document. Textual en-
tailment is an asymmetric relation between two
text fragments specifying whether one fragment
can be inferred from the other. Tatar et al. (2008)
have proposed a method called Logic Text Tiling
(LTT), which uses TE for sentence scoring that
is equal to the number of entailed sentences and
to form text segments comprising of highly con-
nected sentences. Another method called Ana-
log Textual Entailment and Spectral Clustering
(ATESC), suggested in (Gupta et al., 2012), also
uses TE for sentence scoring, using analog scores.
We use a graph-based algorithm to produce the
summary. Graph-based ranking algorithms have
been employed for text summarization in the past,
with similar representation to ours. Vertices rep-
resent text units (words, phrases or sentences) and
an edge between two vertices represent any kind
of relationship between two text units. Scores are
assigned to the vertices using some relevant crite-
ria to select the vertices with the highest scores.
In (Mihalcea and Tarau, 2004), content overlap
between sentences is used to add edges between
two vertices and Page Rank (Page et al., 1999) is
used for scoring the vertices. Erkan and Radev
(2004) use inter-sentence cosine similarity based
on word overlap and tf-idf weighting to identify
relations between sentences. In our paper, we use
TE to compute connectivity between nodes of the
graph and apply the weighted minimum vertex
cover (WMVC) algorithm on the graph to select
the sentences for the summary.
</bodyText>
<subsectionHeader confidence="0.973992">
2.2 Weighted MVC
</subsectionHeader>
<bodyText confidence="0.993634533333333">
WMVC is a combinatorial optimization problem
listed within the classical NP-complete problems
(Garey and Johnson, 1979; Cormen et al., 2001).
Over the years, it has caught the attention of many
researchers, due to its NP-completeness, and also
because its formulation complies with many real
world problems.
Weighted Minimum Vertex Cover Given a
weighted graph G = (V, E, w), such that w is
a positive weight (cost) function on the vertices,
w : V —* R, a weighted minimum vertex cover of
G is a subset of the vertices, C C_ V such that for
every edge (u, v) E E either u E C or v E C
(or both), and the total sum of the weights is min-
imized.
</bodyText>
<equation confidence="0.999026">
�C = argminC&apos; w(v) (1)
V∈ C&apos;
</equation>
<sectionHeader confidence="0.972564" genericHeader="method">
3 Weighted MVC for text summarization
</sectionHeader>
<bodyText confidence="0.999978375">
We formulate the text summarization task as a
WMVC problem. The input document to be sum-
marized is represented as a weighted graph G =
(V, E, w), where each of v E V corresponds to a
sentence in the document; an edge (u, v) E E ex-
ists if either u entails v or v entails u with a value
at least as high as an empirically-set threshold. A
weight w is then assigned to each sentence based
on (negated) TE values (see Section 3.2 for further
details). WMVC returns a cover C which is a sub-
set of the sentences with a minimum total weight,
corresponding to the best connected sentences in
the document. The cover is our output – the sum-
mary of the input document.
Our proposed method, shown in Figure 1, con-
sists of the following main steps.
</bodyText>
<listItem confidence="0.9996438">
1. Intra-sentence textual entailment score com-
putation
2. Entailment-based connectivity scoring
3. Entailment connectivity graph construction
4. Application of WMVC to the graph
</listItem>
<bodyText confidence="0.997317">
We elaborate on each of these steps in the fol-
lowing sections.
</bodyText>
<subsectionHeader confidence="0.999377">
3.1 Computing entailment scores
</subsectionHeader>
<bodyText confidence="0.99999">
Given a document d for which summary is to be
generated, we represent d as an array of sentences
</bodyText>
<page confidence="0.909281">
76
</page>
<bodyText confidence="0.994013238095238">
Id Sentence
S1 A representative of the African National Congress said Saturday the South African government may release black nationalist leader Nelson Mandela
as early as Tuesday.
S2 “There are very strong rumors in South Africa today that on Nov. 15 Nelson Mandela will be released,” said Yusef Saloojee, chief representative in
Canada for the ANC, which is fighting to end white-minority rule in South Africa.
S3 Mandela the 70-year-old leader of the ANC jailed 27 years ago, was sentenced to life in prison for conspiring to overthrow the South African
government.
S4 He was transferred from prison to a hospital in August for treatment of tuberculosis.
S5 Since then, it has been widely rumoured Mandela will be released by Christmas in a move to win strong international support for the South African
government.
S6 “It will be a victory for the people of South Africa and indeed a victory for the whole of Africa,” Saloojee told an audience at the University of
Toronto.
S7 A South African government source last week indicated recent rumours of Mandela’s impending release were orchestrated by members of the
anti-apartheid movement to pressure the government into taking some action.
S8 And a prominent anti-apartheid activist in South Africa said there has been “no indication (Mandela) would pe released today or in the near future.”
S9 Apartheid is South Africa’s policy of racial separation.
Summary “There are very strong rumors in South Africa today that on Nov.15 Nelson Mandela will pe released,” said Yusef Saloojee, chief representative
in Canada for the ANC, which is fighting to end white-minority rule in South Africa. He was transferred from prison to a hospital in August for
treatment of tuberculosis. A South African government source last week indicated recent rumours of Mandela’s impending release were orchestrated
by members of the anti-apartheid movement to pressure the government into taking some action. Apartheid is South Africa’s policy of racial
separation.
</bodyText>
<tableCaption confidence="0.961942">
Table 1: The sentence array of article AP881113-0007 of cluster do106 in the DUC’02 dataset.
</tableCaption>
<figureCaption confidence="0.995247">
Figure 1: Outline of the proposed method.
</figureCaption>
<bodyText confidence="0.999828909090909">
D1xN. An example article is shown in Table 1.
We use this article to demonstrate the steps of our
algorithm.
Then, we compute a TE score between every
possible pair of sentences in D using a textual en-
tailment tool. TE scores for all the pairs are stored
in a sentence entailment matrix, SENxN. An en-
try SE[i, j] in the matrix represents the extent by
which sentence i entails sentence j. The sentence
entailment matrix produced for our example doc-
ument is shown in Table 2.
</bodyText>
<table confidence="0.9464765">
S1 S2 S3 S4 S5 SB S7 S8 S9
S1 - 0 0 0.04 0 0 0.001 0.02 0.02
S2 0.02 - 0.01 0.04 0.06 0.01 0 0.01 0.04
S3 0 0 - 0.09 0 0 0 0 0.04
S4 0 0 0 - 0 0 0 0 0.01
S5 0 0 0 0.04 - 0 0.01 0.01 0.04
SB 0 0 0 0.04 0 - 0 0 0.02
S7 0 0 0 0.04 0.06 0 - 0.02 0.27
S8 0 0 0 0.04 0 0 0.01 - 0.02
S9 0 0 0 0.04 0 0 0 0 -
</table>
<tableCaption confidence="0.997089">
Table 2: The sentence entailment matrix of the ex-
ample article.
</tableCaption>
<table confidence="0.999767833333333">
Id ConnScore Id ConnScore
S1 0.08 S6 0.06
S2 0.19 S7 0.39
S3 0.13 S8 0.07
S4 0.01 S9 0.04
S5 0.1
</table>
<tableCaption confidence="0.9954765">
Table 3: Connectivity Scores of the sentences of
article AP881113-0007.
</tableCaption>
<subsectionHeader confidence="0.998961">
3.2 Connectivity scores
</subsectionHeader>
<bodyText confidence="0.999806416666667">
Our assumption is that entailment between sen-
tences indicates connectivity, that – as mentioned
above – is an indicator of sentence salience. More
specifically, salience of a sentence is determined
by the degree by which it entails other sentences
in the document. We thus use the sentence entail-
ment matrix to compute a connectivity score for
each sentence by summing the entailment scores
of the sentence with respect to the rest of the sen-
tences in the document, and denote this sum as
ConnScore. Formally, ConnScore for sentence
i is computed as follows.
</bodyText>
<equation confidence="0.9953825">
�ConnScore[i] = SE [i, j] (2)
i�= j
</equation>
<bodyText confidence="0.99958225">
Applying it to each sentence in the document,
we obtain the ConnScore1xN vector. The sen-
tence connectivity scores corresponding to Table 2
are shown in Table 3.
</bodyText>
<subsectionHeader confidence="0.740886">
3.3 Entailment connectivity graph
construction
</subsectionHeader>
<bodyText confidence="0.99900075">
The more a sentence is connected, the higher its
connectivity score. To adapt the scores to the
WMVC algorithm, that searches for a minimal so-
lution, we convert the scores into positive weights
</bodyText>
<page confidence="0.977632">
77
</page>
<bodyText confidence="0.850616">
in inverted order:
</bodyText>
<equation confidence="0.998124">
w[i] = −ConnScore[i] + Z (3)
</equation>
<bodyText confidence="0.989543578947368">
w[i] is the score that is assigned to the vertex of
sentence i; Z is a large constant, meant to keep
the scores positive. In this paper, Z has been as-
signed value = 100. Now, the better a sentence is
connected, the lower its weight.
Given the weights, we construct an undi-
rected weighted entailment connectivity graph,
G(V, E, w), for the document d. V consists of
vertices for the document’s sentences, and E are
edges that correspond to the entailment relations
between the sentences. w is the weight explained
above. We create an edge between two vertices as
explained below. Suppose that Si and Sj are two
sentences in d, with entailment scores SE[i, j] and
SE[j, i] between them. We set a threshold T for
the entailment scores as the mean of all entailment
values in the matrix SE. We add an edge (i, j) to
G if SE[i, j] &gt; T OR SE[j, i] &gt; T, i.e. if at least
one of them is as high as the threshold.
</bodyText>
<figureCaption confidence="0.8229732">
Figure 2 shows the connectivity graph con-
structed for the example in Table 1.
Figure 2: The Entailment connectivity graph of the
considered example with associated Score of each
node shown.
</figureCaption>
<subsectionHeader confidence="0.996405">
3.4 Applying WMVC
</subsectionHeader>
<bodyText confidence="0.999981222222222">
Finally, we apply the weighted minimum vertex
cover algorithm to find the minimal vertex cover,
which would be the document’s summary. We
use integer linear programming (ILP) for find-
ing a minimum cover. This algorithm is a 2-
approximation for the problem, meaning it is an
efficient (polynomial-time) algorithm, guaranteed
to find a solution that is no more than 2 times big-
ger than the optimal solution.1 The algorithm’s
</bodyText>
<footnote confidence="0.543173">
1We have used an implementation of ILP for WMVC in
MATLAB, grMinVerCover.
</footnote>
<bodyText confidence="0.997008307692308">
input is G = (V, E, w), a weighted graph where
each vertex vi E V (1 G i G n) has weight wi. Its
output is a minimal vertex cover C of G, contain-
ing a subset of the vertices V . We then list these
sentences as our summary, according to their orig-
inal order in the document.
After applying WMVC to the graph in Fig-
ure 2, the cover C returned by the algorithm is
{S2, S4, S7, S9} (highlighted in Figure 2).
Whenever a summary is required, a word-limit
on the summary is specified. We find the threshold
which results with a cover that matches the word
limit through binary search.
</bodyText>
<sectionHeader confidence="0.99808" genericHeader="evaluation">
4 Experiments and results
</sectionHeader>
<subsectionHeader confidence="0.97369">
4.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.999989916666667">
We have conducted experiments on the single-
document summarization task of the DUC 2002
dataset2, using a random sample that contains 60
news articles picked from each of the 60 clus-
ters available in the dataset. The target sum-
mary length limit has been set to 100 words. We
used version 2.1.1 of BIUTEE (Stern and Da-
gan, 2012), a transformation-based TE system to
compute textual entailment score between pairs of
sentences.3 BIUTEE was trained with 600 text-
hypothesis pairs of the RTE-5 dataset (Bentivogli
et al., 2009).
</bodyText>
<subsectionHeader confidence="0.776134">
4.1.1 Baselines
</subsectionHeader>
<bodyText confidence="0.999947">
We have compared our method’s performance
with the following re-implemented methods:
</bodyText>
<listItem confidence="0.8661501">
1. Sentence selection with tf-idf: In this base-
line, sentences are ranked based on the sum
of the If-idf scores of all the words except
stopwords they contain, where idf figures are
computed from the dataset of 60 documents.
Top ranking sentences are added to the sum-
mary one by one, until the word limit is
reached.
2. LTT: (see Section 2)
3. ATESC : (see Section 2)
</listItem>
<subsubsectionHeader confidence="0.446116">
4.1.2 Evaluation metrics
</subsubsectionHeader>
<bodyText confidence="0.9590975">
We have evaluated the method’s performance us-
ing ROUGE (Lin, 2004). ROUGE measures the
</bodyText>
<footnote confidence="0.99773375">
2http://www-nlpir.nist.gov/projects/
duc/data/2002_data.html
3Available at: http://www.cs.biu.ac.il/
˜nlp/downloads/biutee.
</footnote>
<page confidence="0.989911">
78
</page>
<table confidence="0.9996282">
Method P (%) R (%) Fl (%)
TF-IDF 13.3 17.6 15.1
LTT 39.9 34.6 37.1
ATESC 37.7 32.5 34.9
WMVC 39.8 38.8 39.2
</table>
<tableCaption confidence="0.948569">
Table 4: ROUGE-1 results.
</tableCaption>
<table confidence="0.9998602">
Method P (%) R (%) Fl (%)
TF-IDF 7.4 9.6 8.4
LTT 18.4 15.2 16.6
ATESC 16.3 11.7 13.6
WMVC 16.7 16.8 16.8
</table>
<tableCaption confidence="0.999409">
Table 5: ROUGE-2 results.
</tableCaption>
<bodyText confidence="0.999913">
quality of an automatically-generated summary by
comparing it to a “gold-standard”, typically a hu-
man generated summary. ROUGE-n measures n-
gram precision and recall of a candidate summary
with respect to a set of reference summaries. We
compare the system-generated summary with two
reference summaries for each article in the dataset,
and show the results for ROUGE-1, ROUGE-2 and
ROUGE-SU4 that allows skips within n-grams.
These metrics were shown to perform well for
single document text summarization, especially
for short summaries. Specifically, Lin and Hovy
(2003) showed that ROUGE-1 achieves high cor-
relation with human judgments.4
</bodyText>
<subsectionHeader confidence="0.600719">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999951428571429">
The results for ROUGE-1, ROUGE-2 and
ROUGE-SU4 are shown in Tables 4, 5 and 6, re-
spectively. For each, we show the precision (P),
recall (R) and F1 scores. Boldface marks the high-
est score in each table. As shown in the tables,
our method achieves the best score for each of the
three metrics.
</bodyText>
<subsectionHeader confidence="0.999677">
4.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999151272727273">
The entailment connectivity graph generated con-
veys information about the connectivity of sen-
tences in the document, an important parameter
for indicating the salience of a sentences.
The purpose of the WMVC is therefore to find
a subset of the sentences that are well-connected
and cover all the content of all the sentences. Note
that merely selecting the sentences on the basis
of a greedy approach, that picks the those sen-
tences with the highest connectivity score, does
not ensure that all edges of the graph are cov-
</bodyText>
<footnote confidence="0.795519">
4See (Lin, 2004) for formal definitions of these metrics.
</footnote>
<table confidence="0.9995376">
Method P (%) R (%) Fl (%)
TF-IDF 2.2 4.2 2.9
LTT 16 11.8 13.6
ATESC 15.5 11.1 12.9
WMVC 14.1 14.2 14.2
</table>
<tableCaption confidence="0.996566">
Table 6: ROUGE-SU4 results.
</tableCaption>
<bodyText confidence="0.995624733333333">
ered, i.e. it does not ensure that all the infor-
mation is covered in the summary. In Figure 3,
we illustrate the difference between WMVC (left)
and a greedy algorithm (right) over our example
document. The vertices selected by each algo-
rithm are highlighted. The selected set by WMVC,
{S2, S4, S7, S9}, covers all the edges in the graph.
In contrast, using the greedy algorithm, the subset
of vertices selected on the basis of highest scores
is {S2, S3, S7, S8}. There, several edges are not
covered (e.g. (S1 → S9)).
It is therefore much more in sync with the sum-
marization goal of finding a subset of sentences
that conveys the important information of the doc-
ument in a compressed manner.
</bodyText>
<figureCaption confidence="0.837972">
Figure 3: Minimum Vertex Cover vs. Greedy se-
lection of sentences.
</figureCaption>
<sectionHeader confidence="0.995355" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999946714285714">
The paper presents a novel method for single-
document extractive summarization. We formu-
late the summarization task as an optimization
problem and employ the weighted minimum ver-
tex cover algorithm on a graph based on textual en-
tailment relations between sentences. Our method
has outperformed previous methods that employed
TE for summarization as well as a frequency-
based baseline. For future work, we wish to ap-
ply our algorithm on smaller segments of the sen-
tences, using partial textual entailment Levy et al.
(2013), where we may obtain more reliable en-
tailment measurements, and to apply the same ap-
proach for multi-document summarization.
</bodyText>
<figure confidence="0.99430975">
S3
S5
S1
Weighted
Minimum Vertex Cover
S7 S9
S2
Sa
S6
Sg
S3
S5
S1
S7 S9
Greedy
vertex selection
S2
Sa
S6
Sg
</figure>
<page confidence="0.990316">
79
</page>
<sectionHeader confidence="0.995522" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995175324675325">
Regina Barzilay and Michael Elhadad. 1999. Using
lexical chains for text summanzauon. In In Inder-
jeet Mani and Mark T. Maybury, editors, Advances
in Automatic Text Summarization, pages 111–121,
The MIT Press, 1999.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The
fifth pascal recognizing textual entailment chal-
lenge. In Proceedings of Text Analysis Conference,
pages 14–24, Gaithersburg, Maryland USA.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to
Algorithms. McGraw-Hill, New York, 2nd edition.
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intellignece
Research(JAIR), 22(1):457–479.
Michael R. Garey and David S. Johnson. 1979. Com-
puters and Intractability: A Guide to the Theory of
NP-Completeness. FREEMAN, New York.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recognizing
textual entailment challenge. In Proceedings of the
Association for Computational Linguistics, ACL’07,
pages 1–9, Prague, Czech Republic.
Anand Gupta, Manpreet Kaur, Arjun Singh, Ashish
Sachdeva, and Shruti Bhati. 2012. Analog textual
entailment and spectral clustering (atesc) based sum-
marization. In Lecture Notes in Computer Science,
Springer, pages 101–110, New Delhi, India.
Karen Spark Jones. 2007. Automatic summarizing:
The state of the art. Information Processing and
Management, 43:1449–1481.
Omer Levy, Torsten Zesch, Ido Dagan, and Iryna
Gurevych. 2013. Recognizing partial textual entail-
ment. In Proceedings of the Association for Compu-
tational Linguistics, pages 17–23, Sofia, Bulgaria.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 71–78,
Edmonta, Canada, 27 May- June 1.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out,
pages 25–26, Barcelona, Spain.
Inderjeet Mani and Eric Bloedorn. 1997. Multi-
document summarization by graph search and
matching. In Proceedings of the Fourteenth Na-
tional Conference on Articial Intelligence (AAAI-
97), American Association for Articial Intelligence,
pages 622–628, Providence, Rhode Island.
Daniel Marcu. 2008. From discourse structure to text
summaries. In Proceedings of the ACL/EACL ’97,
Workshop on Intelligent Scalable Text Summariza-
tion, pages 82–88, Madrid, Spain.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Proceedings of
EMNLP, volume 4(4), page 275, Barcelona, Spain.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report.
Gerard Salton, Amit Singhal, Mandar Mitra, and Chris
Buckley. 1997. Automatic text structuring and sum-
marization. Information Processing and Manage-
ment, 33:193–207.
Asher Stern and Ido Dagan. 2012. BIUTEE: A mod-
ular open-source system for recognizing textual en-
tailment. In Proceedings of the ACL 2012 System
Demonstrations, pages 73–78, Jeju, Korea.
Doina Tatar, Emma Tamaianu Morita, Andreea Mihis,
and Dana Lupsa. 2008. Summarization by logic
seg-mentation and text entailment. In Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLing 08), pages 15–26, Haifa, Israel.
</reference>
<page confidence="0.997851">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.853270">
<title confidence="0.994322">Text Summarization through Entailment-based Minimum Vertex Cover</title>
<author confidence="0.996812">Manpreet Adarsh Aseem Shachar</author>
<affiliation confidence="0.949541">of Information Technology, NSIT, New Delhi, of Computer Engineering, NSIT, New Delhi,</affiliation>
<address confidence="0.966034">Research Centre Europe, Meylan, France</address>
<abstract confidence="0.999416">Connectivity a textual characteristic that may be incorporated intelligently for the selection of sentences of a well meaning summary. However, the existing summarization methods do not utilize its potential fully. The present paper introduces a novel method for singledocument text summarization. It poses the text summarization task as an optimization problem, and attempts to solve using Minimum Vertex Cover a graph-based algorithm. Textual entailment, an established indicator of semantic relationships between text units, is used to measure sentence connectivity construct the graph on which operates. Experiments on a standard summarization dataset show that the suggested algorithm outperforms related methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using lexical chains for text summanzauon.</title>
<date>1999</date>
<booktitle>In In Inderjeet Mani and</booktitle>
<pages>111--121</pages>
<editor>Mark T. Maybury, editors,</editor>
<publisher>The MIT Press,</publisher>
<contexts>
<context position="4602" citStr="Barzilay and Elhadad, 1999" startWordPosition="723" endWordPosition="727">criteria, connectivity, to find highly connected sentences in a document. Such sentences often contain information that is found in other sentences, and are therefore natural candidates to be included in the summary. 2.1 Related Work The connectivity between sentences has been previously exploited for extraction-based summarization. Salton et al. (1997) generate intra-document links between passages of a document using automatic hypertext link generation algorithms. Mani and Bloedorn (1997) use the number of shared words, phrases and co-references to measure connectedness among sentences. In (Barzilay and Elhadad, 1999), lexical chains are constructed based on words relatedness. Textual entailment (TE) was exploited recently for text summarization in order to find the highly connected sentences in the document. Textual entailment is an asymmetric relation between two text fragments specifying whether one fragment can be inferred from the other. Tatar et al. (2008) have proposed a method called Logic Text Tiling (LTT), which uses TE for sentence scoring that is equal to the number of entailed sentences and to form text segments comprising of highly connected sentences. Another method called Analog Textual Ent</context>
</contexts>
<marker>Barzilay, Elhadad, 1999</marker>
<rawString>Regina Barzilay and Michael Elhadad. 1999. Using lexical chains for text summanzauon. In In Inderjeet Mani and Mark T. Maybury, editors, Advances in Automatic Text Summarization, pages 111–121, The MIT Press, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
</authors>
<title>The fifth pascal recognizing textual entailment challenge.</title>
<date>2009</date>
<booktitle>In Proceedings of Text Analysis Conference,</booktitle>
<pages>14--24</pages>
<location>Gaithersburg, Maryland USA.</location>
<contexts>
<context position="15098" citStr="Bentivogli et al., 2009" startWordPosition="2576" endWordPosition="2579">that matches the word limit through binary search. 4 Experiments and results 4.1 Experimental settings We have conducted experiments on the singledocument summarization task of the DUC 2002 dataset2, using a random sample that contains 60 news articles picked from each of the 60 clusters available in the dataset. The target summary length limit has been set to 100 words. We used version 2.1.1 of BIUTEE (Stern and Dagan, 2012), a transformation-based TE system to compute textual entailment score between pairs of sentences.3 BIUTEE was trained with 600 texthypothesis pairs of the RTE-5 dataset (Bentivogli et al., 2009). 4.1.1 Baselines We have compared our method’s performance with the following re-implemented methods: 1. Sentence selection with tf-idf: In this baseline, sentences are ranked based on the sum of the If-idf scores of all the words except stopwords they contain, where idf figures are computed from the dataset of 60 documents. Top ranking sentences are added to the summary one by one, until the word limit is reached. 2. LTT: (see Section 2) 3. ATESC : (see Section 2) 4.1.2 Evaluation metrics We have evaluated the method’s performance using ROUGE (Lin, 2004). ROUGE measures the 2http://www-nlpir</context>
</contexts>
<marker>Bentivogli, Dagan, Dang, Giampiccolo, Magnini, 2009</marker>
<rawString>Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth pascal recognizing textual entailment challenge. In Proceedings of Text Analysis Conference, pages 14–24, Gaithersburg, Maryland USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
<author>Clifford Stein</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>2001</date>
<publisher>McGraw-Hill,</publisher>
<location>New York,</location>
<note>2nd edition.</note>
<contexts>
<context position="6443" citStr="Cormen et al., 2001" startWordPosition="1013" endWordPosition="1016"> between sentences is used to add edges between two vertices and Page Rank (Page et al., 1999) is used for scoring the vertices. Erkan and Radev (2004) use inter-sentence cosine similarity based on word overlap and tf-idf weighting to identify relations between sentences. In our paper, we use TE to compute connectivity between nodes of the graph and apply the weighted minimum vertex cover (WMVC) algorithm on the graph to select the sentences for the summary. 2.2 Weighted MVC WMVC is a combinatorial optimization problem listed within the classical NP-complete problems (Garey and Johnson, 1979; Cormen et al., 2001). Over the years, it has caught the attention of many researchers, due to its NP-completeness, and also because its formulation complies with many real world problems. Weighted Minimum Vertex Cover Given a weighted graph G = (V, E, w), such that w is a positive weight (cost) function on the vertices, w : V —* R, a weighted minimum vertex cover of G is a subset of the vertices, C C_ V such that for every edge (u, v) E E either u E C or v E C (or both), and the total sum of the weights is minimized. �C = argminC&apos; w(v) (1) V∈ C&apos; 3 Weighted MVC for text summarization We formulate the text summariz</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2001</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2001. Introduction to Algorithms. McGraw-Hill, New York, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intellignece Research(JAIR),</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="5974" citStr="Erkan and Radev (2004)" startWordPosition="941" endWordPosition="944">ed algorithm to produce the summary. Graph-based ranking algorithms have been employed for text summarization in the past, with similar representation to ours. Vertices represent text units (words, phrases or sentences) and an edge between two vertices represent any kind of relationship between two text units. Scores are assigned to the vertices using some relevant criteria to select the vertices with the highest scores. In (Mihalcea and Tarau, 2004), content overlap between sentences is used to add edges between two vertices and Page Rank (Page et al., 1999) is used for scoring the vertices. Erkan and Radev (2004) use inter-sentence cosine similarity based on word overlap and tf-idf weighting to identify relations between sentences. In our paper, we use TE to compute connectivity between nodes of the graph and apply the weighted minimum vertex cover (WMVC) algorithm on the graph to select the sentences for the summary. 2.2 Weighted MVC WMVC is a combinatorial optimization problem listed within the classical NP-complete problems (Garey and Johnson, 1979; Cormen et al., 2001). Over the years, it has caught the attention of many researchers, due to its NP-completeness, and also because its formulation com</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Gunes Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. Journal of Artificial Intellignece Research(JAIR), 22(1):457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Garey</author>
<author>David S Johnson</author>
</authors>
<title>Computers and Intractability: A Guide to the Theory of NP-Completeness.</title>
<date>1979</date>
<publisher>FREEMAN,</publisher>
<location>New York.</location>
<contexts>
<context position="6421" citStr="Garey and Johnson, 1979" startWordPosition="1009" endWordPosition="1012">u, 2004), content overlap between sentences is used to add edges between two vertices and Page Rank (Page et al., 1999) is used for scoring the vertices. Erkan and Radev (2004) use inter-sentence cosine similarity based on word overlap and tf-idf weighting to identify relations between sentences. In our paper, we use TE to compute connectivity between nodes of the graph and apply the weighted minimum vertex cover (WMVC) algorithm on the graph to select the sentences for the summary. 2.2 Weighted MVC WMVC is a combinatorial optimization problem listed within the classical NP-complete problems (Garey and Johnson, 1979; Cormen et al., 2001). Over the years, it has caught the attention of many researchers, due to its NP-completeness, and also because its formulation complies with many real world problems. Weighted Minimum Vertex Cover Given a weighted graph G = (V, E, w), such that w is a positive weight (cost) function on the vertices, w : V —* R, a weighted minimum vertex cover of G is a subset of the vertices, C C_ V such that for every edge (u, v) E E either u E C or v E C (or both), and the total sum of the weights is minimized. �C = argminC&apos; w(v) (1) V∈ C&apos; 3 Weighted MVC for text summarization We formu</context>
</contexts>
<marker>Garey, Johnson, 1979</marker>
<rawString>Michael R. Garey and David S. Johnson. 1979. Computers and Intractability: A Guide to the Theory of NP-Completeness. FREEMAN, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third pascal recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics, ACL’07,</booktitle>
<pages>1--9</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2588" citStr="Giampiccolo et al., 2007" startWordPosition="393" endWordPosition="396">nd/or generalization on what is important in the source”. Summarization based on content reduction by selection is referred to as extraction (identifying and including the important sentences in the final summary), whereas a summary involving content reduction by generalization is called abstraction (reproducing the most informative content in a new way). The present paper focuses on extraction-based single-document summarization. We formulate the task as a graph-based optimization problem, where vertices represent the sentences and edges the connections between sentences. Textual entailment (Giampiccolo et al., 2007) is employed to estimate the degree of connectivity between sentences, and subsequently to assign a weight to each vertex of the graph. Then, the Weighted Minimum Vertex Cover, a classical graph algorithm, is used to find the minimal set of vertices (that is – sentences) that forms a cover. The idea is that such cover of well-connected vertices would correspond to a cover of the salient content of the document. The rest of the paper is organized as follows: In Section 2, we discuss related work and describe the WMVC algorithm. In Section 3, we propose a novel summarization method, and in Secti</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the Association for Computational Linguistics, ACL’07, pages 1–9, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Gupta</author>
<author>Manpreet Kaur</author>
<author>Arjun Singh</author>
<author>Ashish Sachdeva</author>
<author>Shruti Bhati</author>
</authors>
<title>Analog textual entailment and spectral clustering (atesc) based summarization.</title>
<date>2012</date>
<booktitle>In Lecture Notes in Computer Science,</booktitle>
<pages>101--110</pages>
<publisher>Springer,</publisher>
<location>New Delhi, India.</location>
<contexts>
<context position="5276" citStr="Gupta et al., 2012" startWordPosition="829" endWordPosition="832">ness. Textual entailment (TE) was exploited recently for text summarization in order to find the highly connected sentences in the document. Textual entailment is an asymmetric relation between two text fragments specifying whether one fragment can be inferred from the other. Tatar et al. (2008) have proposed a method called Logic Text Tiling (LTT), which uses TE for sentence scoring that is equal to the number of entailed sentences and to form text segments comprising of highly connected sentences. Another method called Analog Textual Entailment and Spectral Clustering (ATESC), suggested in (Gupta et al., 2012), also uses TE for sentence scoring, using analog scores. We use a graph-based algorithm to produce the summary. Graph-based ranking algorithms have been employed for text summarization in the past, with similar representation to ours. Vertices represent text units (words, phrases or sentences) and an edge between two vertices represent any kind of relationship between two text units. Scores are assigned to the vertices using some relevant criteria to select the vertices with the highest scores. In (Mihalcea and Tarau, 2004), content overlap between sentences is used to add edges between two v</context>
</contexts>
<marker>Gupta, Kaur, Singh, Sachdeva, Bhati, 2012</marker>
<rawString>Anand Gupta, Manpreet Kaur, Arjun Singh, Ashish Sachdeva, and Shruti Bhati. 2012. Analog textual entailment and spectral clustering (atesc) based summarization. In Lecture Notes in Computer Science, Springer, pages 101–110, New Delhi, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Spark Jones</author>
</authors>
<title>Automatic summarizing: The state of the art.</title>
<date>2007</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>43--1449</pages>
<contexts>
<context position="1834" citStr="Jones (2007)" startWordPosition="284" endWordPosition="285">olume of available information. Users are constantly facing the problem of deciding what to read and what to skip. Text summarization provides a practical solution to this problem, causing a resurgence in research in this field. Given a topic of interest, a standard search often yields a large number of documents. Many of them are not of the user’s interest. Rather than going through the entire result-set, the reader may read a gist of a document, produced via summarization tools, and then decide whether to fully read the document or not, thus saving a substantial amount of time. According to Jones (2007), a summary can be defined as “a reductive transformation of source text to summary text through content reduction by selection and/or generalization on what is important in the source”. Summarization based on content reduction by selection is referred to as extraction (identifying and including the important sentences in the final summary), whereas a summary involving content reduction by generalization is called abstraction (reproducing the most informative content in a new way). The present paper focuses on extraction-based single-document summarization. We formulate the task as a graph-bas</context>
</contexts>
<marker>Jones, 2007</marker>
<rawString>Karen Spark Jones. 2007. Automatic summarizing: The state of the art. Information Processing and Management, 43:1449–1481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Torsten Zesch</author>
<author>Ido Dagan</author>
<author>Iryna Gurevych</author>
</authors>
<title>Recognizing partial textual entailment.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>17--23</pages>
<location>Sofia, Bulgaria.</location>
<marker>Levy, Zesch, Dagan, Gurevych, 2013</marker>
<rawString>Omer Levy, Torsten Zesch, Ido Dagan, and Iryna Gurevych. 2013. Recognizing partial textual entailment. In Proceedings of the Association for Computational Linguistics, pages 17–23, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram cooccurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>71--78</pages>
<location>Edmonta,</location>
<contexts>
<context position="16645" citStr="Lin and Hovy (2003)" startWordPosition="2821" endWordPosition="2824"> WMVC 16.7 16.8 16.8 Table 5: ROUGE-2 results. quality of an automatically-generated summary by comparing it to a “gold-standard”, typically a human generated summary. ROUGE-n measures ngram precision and recall of a candidate summary with respect to a set of reference summaries. We compare the system-generated summary with two reference summaries for each article in the dataset, and show the results for ROUGE-1, ROUGE-2 and ROUGE-SU4 that allows skips within n-grams. These metrics were shown to perform well for single document text summarization, especially for short summaries. Specifically, Lin and Hovy (2003) showed that ROUGE-1 achieves high correlation with human judgments.4 4.2 Results The results for ROUGE-1, ROUGE-2 and ROUGE-SU4 are shown in Tables 4, 5 and 6, respectively. For each, we show the precision (P), recall (R) and F1 scores. Boldface marks the highest score in each table. As shown in the tables, our method achieves the best score for each of the three metrics. 4.3 Analysis The entailment connectivity graph generated conveys information about the connectivity of sentences in the document, an important parameter for indicating the salience of a sentences. The purpose of the WMVC is </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 71–78, Edmonta, Canada, 27 May- June 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Summarization Branches Out,</booktitle>
<pages>25--26</pages>
<location>Barcelona,</location>
<contexts>
<context position="15660" citStr="Lin, 2004" startWordPosition="2673" endWordPosition="2674">s of the RTE-5 dataset (Bentivogli et al., 2009). 4.1.1 Baselines We have compared our method’s performance with the following re-implemented methods: 1. Sentence selection with tf-idf: In this baseline, sentences are ranked based on the sum of the If-idf scores of all the words except stopwords they contain, where idf figures are computed from the dataset of 60 documents. Top ranking sentences are added to the summary one by one, until the word limit is reached. 2. LTT: (see Section 2) 3. ATESC : (see Section 2) 4.1.2 Evaluation metrics We have evaluated the method’s performance using ROUGE (Lin, 2004). ROUGE measures the 2http://www-nlpir.nist.gov/projects/ duc/data/2002_data.html 3Available at: http://www.cs.biu.ac.il/ ˜nlp/downloads/biutee. 78 Method P (%) R (%) Fl (%) TF-IDF 13.3 17.6 15.1 LTT 39.9 34.6 37.1 ATESC 37.7 32.5 34.9 WMVC 39.8 38.8 39.2 Table 4: ROUGE-1 results. Method P (%) R (%) Fl (%) TF-IDF 7.4 9.6 8.4 LTT 18.4 15.2 16.6 ATESC 16.3 11.7 13.6 WMVC 16.7 16.8 16.8 Table 5: ROUGE-2 results. quality of an automatically-generated summary by comparing it to a “gold-standard”, typically a human generated summary. ROUGE-n measures ngram precision and recall of a candidate summary</context>
<context position="17572" citStr="Lin, 2004" startWordPosition="2983" endWordPosition="2984">hod achieves the best score for each of the three metrics. 4.3 Analysis The entailment connectivity graph generated conveys information about the connectivity of sentences in the document, an important parameter for indicating the salience of a sentences. The purpose of the WMVC is therefore to find a subset of the sentences that are well-connected and cover all the content of all the sentences. Note that merely selecting the sentences on the basis of a greedy approach, that picks the those sentences with the highest connectivity score, does not ensure that all edges of the graph are cov4See (Lin, 2004) for formal definitions of these metrics. Method P (%) R (%) Fl (%) TF-IDF 2.2 4.2 2.9 LTT 16 11.8 13.6 ATESC 15.5 11.1 12.9 WMVC 14.1 14.2 14.2 Table 6: ROUGE-SU4 results. ered, i.e. it does not ensure that all the information is covered in the summary. In Figure 3, we illustrate the difference between WMVC (left) and a greedy algorithm (right) over our example document. The vertices selected by each algorithm are highlighted. The selected set by WMVC, {S2, S4, S7, S9}, covers all the edges in the graph. In contrast, using the greedy algorithm, the subset of vertices selected on the basis of </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out, pages 25–26, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Eric Bloedorn</author>
</authors>
<title>Multidocument summarization by graph search and matching.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Articial Intelligence (AAAI97), American Association for Articial Intelligence,</booktitle>
<pages>622--628</pages>
<location>Providence, Rhode Island.</location>
<contexts>
<context position="4470" citStr="Mani and Bloedorn (1997)" startWordPosition="703" endWordPosition="706">antics (*SEM 2014), pages 75–80, Dublin, Ireland, August 23-24 2014. of the text. In this work we focus on the last of the above criteria, connectivity, to find highly connected sentences in a document. Such sentences often contain information that is found in other sentences, and are therefore natural candidates to be included in the summary. 2.1 Related Work The connectivity between sentences has been previously exploited for extraction-based summarization. Salton et al. (1997) generate intra-document links between passages of a document using automatic hypertext link generation algorithms. Mani and Bloedorn (1997) use the number of shared words, phrases and co-references to measure connectedness among sentences. In (Barzilay and Elhadad, 1999), lexical chains are constructed based on words relatedness. Textual entailment (TE) was exploited recently for text summarization in order to find the highly connected sentences in the document. Textual entailment is an asymmetric relation between two text fragments specifying whether one fragment can be inferred from the other. Tatar et al. (2008) have proposed a method called Logic Text Tiling (LTT), which uses TE for sentence scoring that is equal to the numbe</context>
</contexts>
<marker>Mani, Bloedorn, 1997</marker>
<rawString>Inderjeet Mani and Eric Bloedorn. 1997. Multidocument summarization by graph search and matching. In Proceedings of the Fourteenth National Conference on Articial Intelligence (AAAI97), American Association for Articial Intelligence, pages 622–628, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>From discourse structure to text summaries.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL/EACL ’97, Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>82--88</pages>
<location>Madrid,</location>
<contexts>
<context position="3514" citStr="Marcu, 2008" startWordPosition="554" endWordPosition="555">cover of well-connected vertices would correspond to a cover of the salient content of the document. The rest of the paper is organized as follows: In Section 2, we discuss related work and describe the WMVC algorithm. In Section 3, we propose a novel summarization method, and in Section 4, experiments and results are presented. Finally, in Section 5, we conclude and outline future research directions. 2 Background Extractive text summarization is the task of identifying those text segments which provide important information about the gist of the document – the salient units of the text. In (Marcu, 2008), salient units are determined as the ones that contain frequently-used words, contain words that are within titles and headings, are located at the beginning or at the end of sections, contain key phrases and are the most highly connected to other parts 75 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 75–80, Dublin, Ireland, August 23-24 2014. of the text. In this work we focus on the last of the above criteria, connectivity, to find highly connected sentences in a document. Such sentences often contain information that is found in other s</context>
</contexts>
<marker>Marcu, 2008</marker>
<rawString>Daniel Marcu. 2008. From discourse structure to text summaries. In Proceedings of the ACL/EACL ’97, Workshop on Intelligent Scalable Text Summarization, pages 82–88, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>4</volume>
<issue>4</issue>
<pages>275</pages>
<location>Barcelona,</location>
<contexts>
<context position="5806" citStr="Mihalcea and Tarau, 2004" startWordPosition="912" endWordPosition="915">ed Analog Textual Entailment and Spectral Clustering (ATESC), suggested in (Gupta et al., 2012), also uses TE for sentence scoring, using analog scores. We use a graph-based algorithm to produce the summary. Graph-based ranking algorithms have been employed for text summarization in the past, with similar representation to ours. Vertices represent text units (words, phrases or sentences) and an edge between two vertices represent any kind of relationship between two text units. Scores are assigned to the vertices using some relevant criteria to select the vertices with the highest scores. In (Mihalcea and Tarau, 2004), content overlap between sentences is used to add edges between two vertices and Page Rank (Page et al., 1999) is used for scoring the vertices. Erkan and Radev (2004) use inter-sentence cosine similarity based on word overlap and tf-idf weighting to identify relations between sentences. In our paper, we use TE to compute connectivity between nodes of the graph and apply the weighted minimum vertex cover (WMVC) algorithm on the graph to select the sentences for the summary. 2.2 Weighted MVC WMVC is a combinatorial optimization problem listed within the classical NP-complete problems (Garey an</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Proceedings of EMNLP, volume 4(4), page 275, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web.</title>
<date>1999</date>
<tech>Technical Report.</tech>
<contexts>
<context position="5917" citStr="Page et al., 1999" startWordPosition="931" endWordPosition="934">ence scoring, using analog scores. We use a graph-based algorithm to produce the summary. Graph-based ranking algorithms have been employed for text summarization in the past, with similar representation to ours. Vertices represent text units (words, phrases or sentences) and an edge between two vertices represent any kind of relationship between two text units. Scores are assigned to the vertices using some relevant criteria to select the vertices with the highest scores. In (Mihalcea and Tarau, 2004), content overlap between sentences is used to add edges between two vertices and Page Rank (Page et al., 1999) is used for scoring the vertices. Erkan and Radev (2004) use inter-sentence cosine similarity based on word overlap and tf-idf weighting to identify relations between sentences. In our paper, we use TE to compute connectivity between nodes of the graph and apply the weighted minimum vertex cover (WMVC) algorithm on the graph to select the sentences for the summary. 2.2 Weighted MVC WMVC is a combinatorial optimization problem listed within the classical NP-complete problems (Garey and Johnson, 1979; Cormen et al., 2001). Over the years, it has caught the attention of many researchers, due to </context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bringing order to the web. Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Amit Singhal</author>
<author>Mandar Mitra</author>
<author>Chris Buckley</author>
</authors>
<title>Automatic text structuring and summarization.</title>
<date>1997</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>33--193</pages>
<contexts>
<context position="4330" citStr="Salton et al. (1997)" startWordPosition="684" endWordPosition="687">y phrases and are the most highly connected to other parts 75 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 75–80, Dublin, Ireland, August 23-24 2014. of the text. In this work we focus on the last of the above criteria, connectivity, to find highly connected sentences in a document. Such sentences often contain information that is found in other sentences, and are therefore natural candidates to be included in the summary. 2.1 Related Work The connectivity between sentences has been previously exploited for extraction-based summarization. Salton et al. (1997) generate intra-document links between passages of a document using automatic hypertext link generation algorithms. Mani and Bloedorn (1997) use the number of shared words, phrases and co-references to measure connectedness among sentences. In (Barzilay and Elhadad, 1999), lexical chains are constructed based on words relatedness. Textual entailment (TE) was exploited recently for text summarization in order to find the highly connected sentences in the document. Textual entailment is an asymmetric relation between two text fragments specifying whether one fragment can be inferred from the oth</context>
</contexts>
<marker>Salton, Singhal, Mitra, Buckley, 1997</marker>
<rawString>Gerard Salton, Amit Singhal, Mandar Mitra, and Chris Buckley. 1997. Automatic text structuring and summarization. Information Processing and Management, 33:193–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asher Stern</author>
<author>Ido Dagan</author>
</authors>
<title>BIUTEE: A modular open-source system for recognizing textual entailment.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>73--78</pages>
<location>Jeju,</location>
<contexts>
<context position="14903" citStr="Stern and Dagan, 2012" startWordPosition="2546" endWordPosition="2550">urned by the algorithm is {S2, S4, S7, S9} (highlighted in Figure 2). Whenever a summary is required, a word-limit on the summary is specified. We find the threshold which results with a cover that matches the word limit through binary search. 4 Experiments and results 4.1 Experimental settings We have conducted experiments on the singledocument summarization task of the DUC 2002 dataset2, using a random sample that contains 60 news articles picked from each of the 60 clusters available in the dataset. The target summary length limit has been set to 100 words. We used version 2.1.1 of BIUTEE (Stern and Dagan, 2012), a transformation-based TE system to compute textual entailment score between pairs of sentences.3 BIUTEE was trained with 600 texthypothesis pairs of the RTE-5 dataset (Bentivogli et al., 2009). 4.1.1 Baselines We have compared our method’s performance with the following re-implemented methods: 1. Sentence selection with tf-idf: In this baseline, sentences are ranked based on the sum of the If-idf scores of all the words except stopwords they contain, where idf figures are computed from the dataset of 60 documents. Top ranking sentences are added to the summary one by one, until the word lim</context>
</contexts>
<marker>Stern, Dagan, 2012</marker>
<rawString>Asher Stern and Ido Dagan. 2012. BIUTEE: A modular open-source system for recognizing textual entailment. In Proceedings of the ACL 2012 System Demonstrations, pages 73–78, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doina Tatar</author>
<author>Emma Tamaianu Morita</author>
<author>Andreea Mihis</author>
<author>Dana Lupsa</author>
</authors>
<title>Summarization by logic seg-mentation and text entailment.</title>
<date>2008</date>
<booktitle>In Conference on Intelligent Text Processing and Computational Linguistics (CICLing 08),</booktitle>
<pages>15--26</pages>
<location>Haifa,</location>
<contexts>
<context position="4953" citStr="Tatar et al. (2008)" startWordPosition="777" endWordPosition="780">erate intra-document links between passages of a document using automatic hypertext link generation algorithms. Mani and Bloedorn (1997) use the number of shared words, phrases and co-references to measure connectedness among sentences. In (Barzilay and Elhadad, 1999), lexical chains are constructed based on words relatedness. Textual entailment (TE) was exploited recently for text summarization in order to find the highly connected sentences in the document. Textual entailment is an asymmetric relation between two text fragments specifying whether one fragment can be inferred from the other. Tatar et al. (2008) have proposed a method called Logic Text Tiling (LTT), which uses TE for sentence scoring that is equal to the number of entailed sentences and to form text segments comprising of highly connected sentences. Another method called Analog Textual Entailment and Spectral Clustering (ATESC), suggested in (Gupta et al., 2012), also uses TE for sentence scoring, using analog scores. We use a graph-based algorithm to produce the summary. Graph-based ranking algorithms have been employed for text summarization in the past, with similar representation to ours. Vertices represent text units (words, phr</context>
</contexts>
<marker>Tatar, Morita, Mihis, Lupsa, 2008</marker>
<rawString>Doina Tatar, Emma Tamaianu Morita, Andreea Mihis, and Dana Lupsa. 2008. Summarization by logic seg-mentation and text entailment. In Conference on Intelligent Text Processing and Computational Linguistics (CICLing 08), pages 15–26, Haifa, Israel.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>