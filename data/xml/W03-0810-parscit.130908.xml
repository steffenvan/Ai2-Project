<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012864">
<title confidence="0.998048">
Accelerating Corporate Research in the Development, Application
and Deployment of Human Language Technologies
</title>
<author confidence="0.985157">
David Ferrucci
</author>
<affiliation confidence="0.867593">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.814541">
Yorktown Heights, NY 10598
</address>
<email confidence="0.993337">
ferrucci@us.ibm.com
</email>
<author confidence="0.954418">
Adam Lally
</author>
<affiliation confidence="0.841644">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.810368">
Yorktown Heights, NY 10598
</address>
<email confidence="0.987608">
alally@us.ibm.com
</email>
<sectionHeader confidence="0.964413" genericHeader="method">
1 Architecture Goals
</sectionHeader>
<bodyText confidence="0.997615363636364">
In six major labs spread out over the globe, IBM Re-
search has over 200 people working on Unstructured
Information Management (UIM) technologies with a
significant focus on Human Language Technologies
(HLT). These researchers are engaged in activities rang-
ing from natural language dialog to machine translation
to bioinformatics to open-domain question answering.
Each group is developing different technical and engi-
neering approaches to process unstructured information
(e.g., natural language text, voice, audio and video) in
pursuit of specific research objectives and their applica-
tions.
The high-level objectives of IBM’s Unstructured In-
formation Management Architecture (UIMA) are two
fold:
1) Accelerate scientific advances by enabling the
rapid combination UIM technologies (e.g., natu-
ral language processing, video analysis, infor-
mation retrieval, etc.).
2) Accelerate transfer of UIM technologies to
product by providing a robust software frame-
work that promotes reuse and supports flexible
deployment options.
UIMA is a software architecture for developing ap-
plications which integrate search and analytics over a
combination of structured and unstructured information.
We define structured information as information whose
intended meaning is unambiguous and explicitly repre-
sented in the structure or format of the data. The ca-
nonical example is a database table. We define
unstructured information as information whose intended
meaning is only implied by its form. The canonical ex-
ample is a natural language document.
</bodyText>
<sectionHeader confidence="0.66242" genericHeader="method">
Abstract
</sectionHeader>
<bodyText confidence="0.9991043125">
IBM Research has over 200 people working
on Unstructured Information Management
(UIM) technologies with a strong focus on
HLT. Spread out over the globe they are en-
gaged in activities ranging from natural lan-
guage dialog to machine translation to
bioinformatics to open-domain question an-
swering. An analysis of these activities
strongly suggested that improving the organi-
zation’s ability to quickly discover each
other&apos;s results and rapidly combine different
technologies and approaches would accelerate
scientific advance. Furthermore, the ability to
reuse and combine results through a common
architecture and a robust software framework
would accelerate the transfer of research re-
sults in HLT into IBM’s product platforms.
Market analyses indicating a growing need to
process unstructured information, specifically
multi-lingual, natural language text, coupled
with IBM Research’s investment in HLT, led
to the development of middleware architecture
for processing unstructured information
dubbed UIMA. At the heart of UIMA are
powerful search capabilities and a data-driven
framework for the development, composition
and distributed deployment of analysis en-
gines. In this paper we give a general intro-
duction to UIMA focusing on the design
points of its analysis engine architecture and
we discuss how UIMA is helping to accelerate
research and technology transfer.
</bodyText>
<note confidence="0.485024333333333">
Semantic Search Engine
Query key words and concepts
Token and concept Indexing
</note>
<figure confidence="0.9832357">
Application
Logic
Unstructured
Information
Relevant
Knowledge
Collection
Analysis
Engine(s)
Collection
Processing
Manager
(CPM)
Document, Collection
&amp; Meta-data Store
(Text) Analysis Engines (TAEs)
Analysis engines employing a variety
of analytical techniques and
strategies for detecting semantic
content
</figure>
<subsectionHeader confidence="0.719859">
Structured Knowledge Access
</subsectionHeader>
<bodyText confidence="0.72165475">
Knowledge Source Adapters (KSAs)
integrate and deliver content from
many structured knowledge sources
according to central ontologies
</bodyText>
<subsectionHeader confidence="0.604478">
Structured
Information
</subsectionHeader>
<figureCaption confidence="0.998879">
Figure 1: UIMA High-Level Architecture
</figureCaption>
<bodyText confidence="0.992061043478261">
The UIMA high-level architecture, illustrated in
Figure 1, defines the roles, interfaces and communica-
tions of large-grained components essential for UIM
applications. These include components capable of ana-
lyzing unstructured artifacts, integrating and accessing
structured sources and storing, indexing and searching
for artifacts based on discovered semantic content.
As part of the UIMA project, IBM is developing dif-
ferent implementations of the architecture suitable for
different classes of deployment. These range from light-
weight and embeddable implementations to highly
scaleable implementations that are meant to exploit
clusters of machines and provide high throughput and
high availability.
While the architecture extends to a variety of un-
structured artifacts including voice, audio and video, a
primary analytic focus of current UIMA implementa-
tions is squarely on human language technologies.
In this paper we will refer to elements of unstruc-
tured information processing as documents admitting,
however, that an element may represent for the applica-
tion, a whole text document, a text document fragment
or even multiple documents.
</bodyText>
<sectionHeader confidence="0.989479" genericHeader="method">
2 Generalized Application Scenario and
</sectionHeader>
<subsectionHeader confidence="0.584707">
the High-Level Architecture
</subsectionHeader>
<bodyText confidence="0.999882727272727">
In this section we provide a high-level overview of the
UIMA architecture by describing its component roles in
a generalized application scenario.
The generalized scenario includes both analysis and
access functions. Analysis functions are divided into
two classes, namely document-level and collection-level
analysis. Access functions are divided into semantic
search and structured knowledge access.
We refer to the software program that employs
UIMA components to implement some end-user capa-
bility as the application or application program.
</bodyText>
<subsectionHeader confidence="0.983886">
2.1 Document-Level Analysis
</subsectionHeader>
<bodyText confidence="0.999920327586207">
Document-level analysis is performed by component
processing elements named Text Analysis Engines
(TAEs). These are extensions of the generic analysis
engine, specialized for text. They are analogous, for
example, to Processing Resources in the GATE archi-
tecture (Cunningham et al., 2000). In UIMA, a TAE is a
recursive structure which may be composed of sub or
component engines each performing a different stage of
the application’s analysis.
Examples of Text Analysis Engines include lan-
guage translators, document summarizers, document
classifiers, and named-entity detectors. Each TAE spe-
cializes in discovering specific concepts (or &amp;quot;semantic
entities&amp;quot;) otherwise unidentified or implicit in the
document text.
A TAE takes in a document and produces an analy-
sis. The original document and its analysis are repre-
sented in a common structure called the Common
Analysis System or CAS. The CAS is conceptually
analogous to the annotations in other architectures, be-
ginning with TIPSTER (Grishman, 1996).
In general, annotations associate some meta-data
with a region in the original artifact. Where the artifact
is a text document, for example, the annotation associ-
ates meta-data (e.g., a label) with a span of text in the
document by giving the span’s start and end positions.
Annotations in the CAS are stand-off, meaning that the
annotations are maintained separately from the docu-
ment itself; this is more flexible than inline markup
(Mardis and Burger, 2002). In UIMA, annotations are
not the only type of information stored in the CAS. The
CAS may be used to represent any class of meta-data
element associated with analysis of a document regard-
less of whether it is explicitly linked to some sub com-
ponent of the original document. The CAS also allows
for multiple definitions of this linkage, as is necessary
for the analysis of images, video or other modalities.
The analysis represented in the CAS may be thought
of as a collection of meta-data that is enriched as it
passes through successive stages of analysis. At a spe-
cific stage of analysis, for example, the CAS may in-
clude a deep parse. A named-entity detector receiving
this CAS may consider the deep parse to identify named
entities. The named entities may be input to an analysis
engine that produces summaries or classifications of the
document.
The UIMA CAS object provides general object-
based representation with a hierarchical type system
supporting single inheritance. It includes data creation,
access and serialization methods designed for the effi-
cient representation, access and transport of analysis
results among TAEs and between TAEs and other
UIMA components or applications. Elements in the
CAS may be indexed for fast access (Goetz et al.,
2001). The CAS has been implemented in C++ and
Java with serialization methods for binary as well as
XML formats for managing the tradeoff between effi-
ciency and interoperability.
</bodyText>
<subsectionHeader confidence="0.99795">
2.2 Collection-Level Analysis
</subsectionHeader>
<bodyText confidence="0.999966209302326">
Documents are gathered by the application and organ-
ized into collections. The architecture defines a Collec-
tion Reader interface. Implementations of the Collection
Reader provide access to collection elements, collection
meta-data and element meta-data. UIMA implementa-
tions include a document, collection and meta-data store
that implements the Collection Reader interface and
manages multiple collections and their elements. How-
ever, applications that need to manage their own collec-
tions can provide an implementation of a Collection
Reader to UIMA components that require access to col-
lection data.
Collections are analyzed to produce collection level
analysis results. These results represent aggregate infer-
ences computed over all or some subset of the docu-
ments in a collection. The component of an application
that analyzes an entire collection is considered a Collec-
tion Analysis Engine. These engines typically apply
element-level, or more specifically document-level
analysis, to elements of a collection and then consider-
ing the element analyses in performing aggregate com-
putations.
Examples of collection level analysis results include
sub collections where elements contain certain features,
glossaries of terms with their variants and frequencies,
taxonomies, feature vectors for statistical categorizers,
databases of extracted relations, and master indices of
tokens and other detected entities.
In support of Collection Analysis Engines, UIMA
defines the Collection Processing Manager (CPM)
component. The CPM’s primary responsibility is to
manage the application of a designated TAE to each
document accessible through a Collection Reader. A
Collection Analysis Engine may provide, as input to the
CPM, a TAE and a Collection Reader. The CPM applies
the TAE and returns the analysis, represented by a CAS,
for each element in the collection. To control the proc-
ess, the CPM provides administrative functions that
include failure reporting, pausing and restarting.
At the request of the application’s collection analy-
sis engine, the CPM may be optionally configured to
perform functions typical of UIM application scenarios.
Examples of these include:
</bodyText>
<listItem confidence="0.994193090909091">
1) Filtering - ensures that only certain elements
are processed based on meta-data constraints.
2) Persistence - stores element-level analysis re-
sults in a provided Collection Writer.
3) Indexing - indexes documents using a desig-
nated search engine indexing interface based
on meta-data extracted from the analysis.
4) Parallelization - manages the creation and exe-
cution of multiple instances of a TAE for proc-
essing multiple documents simultaneously
utilizing available computing resources.
</listItem>
<subsectionHeader confidence="0.996833">
2.3 Semantic Search
</subsectionHeader>
<bodyText confidence="0.999902642857143">
To support the concept of “semantic search” – the capa-
bility to find documents based on semantic content dis-
covered by document or collection level analysis and
represented as annotations – UIMA specifies search
engine indexing and query interfaces.
A key feature of the indexing interface is that it sup-
ports the indexing of tokens as well as annotations and
particularly cross-over annotations. Two or more anno-
tations cross-over one another if they are linked to inter-
secting regions of the document.
The key feature of the query interface is that it sup-
ports queries that may be predicated on nested structures
of annotations and tokens in addition to Boolean combi-
nations of tokens and annotations.
</bodyText>
<subsectionHeader confidence="0.998846">
2.4 Structured Knowledge Access
</subsectionHeader>
<bodyText confidence="0.9999795">
As analysis engines do their job they may consult a
wide variety of structured knowledge sources. To in-
crease reusability and facilitate integration, UIMA
specifies the Knowledge Source Adapter (KSA) inter-
face.
KSA objects provide a layer of uniform access to
disparate knowledge sources. They manage the techni-
cal communication, representation language and ontol-
ogy mapping necessary to deliver knowledge encoded
in databases, dictionaries, knowledge bases and other
structured sources in a uniform way. The primary inter-
face to a KSA presents structured knowledge as instan-
tiated predicates using the Knowledge Interchange
Format (KIF) encoded in XML.
A key aspect of the KSA architecture is the KSA
meta-data and related services supporting KSA registra-
tion and search. These services include the description
and registration of named ontologies. Ontologies are
described by the concepts and predicates they include.
The KSA is self-descriptive and among other meta-data
includes the predicate signatures belonging to registered
ontologies that the KSA can instantiate and the knowl-
edge sources it consults.
Application or analysis engine developers can con-
sult human browseable KSA directory services to search
for and find KSAs that instantiate predicates of a regis-
tered ontology. The service will deliver a handle to a
web service or an embeddable KSA component.
</bodyText>
<sectionHeader confidence="0.980087" genericHeader="method">
3 Analysis Engine Framework
</sectionHeader>
<bodyText confidence="0.9992604">
This section takes a closer look at the analysis engine
framework.
UIMA specifies an interface for an analysis engine;
roughly speaking it is “CAS in” and “CAS out”. There
are other operations used for filtering, administrative
</bodyText>
<figureCaption confidence="0.995297">
Figure 2: Primitive Analysis Engine
</figureCaption>
<bodyText confidence="0.99879">
and self-descriptive functions, but the main interface
takes a CAS as input and delivers a CAS as output.
Any program that implements this interface may be
plugged in as an analysis engine component in an im-
plementation of UIMA. However, as part of UIMA
tooling we have developed an analysis engine frame-
work to support the creation, composition and flexible
deployment of primitive and aggregate analysis engines
on a variety of different system middleware platforms.
The underlying design philosophy for the Analysis
Engine framework was driven by three primary princi-
ples:
</bodyText>
<listItem confidence="0.998034142857143">
1) Encourage and enable component reuse.
2) Support distinct development roles insulating
the algorithm developer from system and
deployment details.
3) Support a flexible variety of deployment op-
tions by insulating lower-level system middle-
ware APIs.
</listItem>
<subsectionHeader confidence="0.994428">
3.1 Encourage and Enable Component Reuse
</subsectionHeader>
<bodyText confidence="0.9996005">
With many HLT components being developed through-
out IBM Research by independent groups, encouraging
and enabling reuse is a critical design objective to
achieve expected efficiencies and cross-group collabo-
rations. Three characteristics of the analysis engine
framework address this objective:
</bodyText>
<figure confidence="0.970591918918919">
1) Recursive Structure
2) Data-Driven
3) Self-Descriptive
CAS
(part of framework)
9etCAS()
9etCAS()
Primitive Analysis Engine
Controller
(part of framework)
reads/writes
analysis data
process(Result Spec.)
process(Result Spec.)
Annotator
(Annotator Developer
Implements)
process(Result Spec.)
Aggregate Analysis Engine
getCAS()
CAS
Analysis Sequencer
init (Result Spec.)
Process(Result Spec.)
next()
read/write analysis data
Controller
Analysis
Engine 1
process(RS)
Analysis Structure Broker
process(Result Spec.)
Analysis
Engine 2
process(RS) process(RS)
Analysis
Engine 3
</figure>
<figureCaption confidence="0.999842">
Figure 3: Aggregate Analysis Engine
</figureCaption>
<bodyText confidence="0.999381712328767">
Recursive Structure. A primitive analysis engine, illus-
trated in Figure 2, is composed of an Annotator and a
CAS. The annotator is the object that implements the
analysis logic (e.g. tokenization, grammatical parsing,
entity detection). It reads the original document content
and meta-data from the CAS. It then computes and
writes new meta-data to the CAS. An aggregate analy-
sis engine, illustrated in Figure 3, is composed of two or
more component analysis engines, but implements ex-
actly the same external interface as the primitive engine.
At run-time an aggregate analysis engine is given a se-
quence in which to execute its component engines. A
component called the Analysis Structure Broker ensures
that each component engine has access to the CAS ac-
cording to the specified sequence. Like any nested pro-
gramming model, this recursive structure ensures that
components may be easily reused in combination with
one another while insulating their internal structure.
Data-Driven. An analysis engine’s processing
model is strictly data-driven. This means that an anno-
tator’s analysis logic may be predicated only on the con-
tent of its input and not on the specific analysis engines
it may be combined with or the control sequence in
which it may be embedded. This restriction ensures that
an analysis engine may be successfully reused in differ-
ent aggregate structures and different control environ-
ments as long as its input requirements are met.
The Analysis Sequencer is a component in the
framework responsible for dynamically determining the
next analysis engine to receive access to the CAS. The
Analysis Sequencer is distinct from the Analysis Struc-
ture Broker, whose responsibility is to deliver the CAS
to the next analysis engine whichever it is wherever it
may be located. The Analysis Sequencer’s control logic
is separate from the analysis logic embedded in an An-
notator and separate from the Analysis Structure Bro-
ker’s concerns related to ensuring and/or optimizing the
CAS transport. This separation of concerns allows for
the plug-n-play of different Analysis Sequencers. The
Analysis Sequencer is a pluggable range from provide
simple iteration over a declaratively specified static
flow to complex planning algorithms. Current imple-
mentations have been limited to simple linear flows
between analysis engines; however more advanced ap-
plications are generating requirements for dynamic and
adaptive sequencing. How much of the control specifi-
cation ends up in a declarative representation and how
much is implemented in the sequencer for these ad-
vanced requirements is currently being explored.
Self-Descriptive. Ensuring that analysis engines
may be easily composed to form aggregates and may be
reused in different control sequences is necessary for
technical reusability but not sufficient for enabling and
validating reuse within a broad community of develop-
ers. To promote reuse, analysis engine developers must
be able to discover which analysis engines are available
in terms of what they do – their capabilities.
Each analysis engine&apos;s data model is declared in
XML and then dynamically realized in the CAS at run-
time, an approach similar to MAIA (Laprun et al.,
2002). In UIMA, however, analysis engines publish
their input requirements and output specifications rela-
tive to this declared data model, and this information is
used to register the analysis engine in an analysis engine
directory service. This service includes a human-
oriented interface that allows application developers to
browse and/or search for analysis engines that meet
their needs.
While self-description and related directory services
will promote reuse, their value is still dependent on es-
tablishing common data models (or fragments thereof)
to which analysis engine capability descriptions sub-
scribe.
</bodyText>
<subsectionHeader confidence="0.990019">
3.2 Support Distinct Development Roles
</subsectionHeader>
<bodyText confidence="0.999973102564103">
Language technology researchers that specialize in, for
example, multi-lingual machine translation, may not be
highly trained software engineers nor be skilled in the
system technologies required for flexible and scaleable
deployments. Yet one of the primary objectives of the
UIMA project is to ensure that their work can be effi-
ciently deployed in robust and scaleable system archi-
tecture.
Along the same lines, researchers with ideas about
how to combine and orchestrate different components
may not themselves be algorithm developers or systems
engineers, yet we need to enable them to rapidly create
and validate ideas through combining existing compo-
nents.
Finally, deploying analysis engines as distributed,
highly available services or as collocated objects in an
aggregate system requires yet another skill.
As a result we have identified the following devel-
opment roles and have designed the architecture with
independent sets of interfaces in support of each of
these different skill sets. Our separation of development
roles is analogous to the separation of roles in Sun&apos;s
J2EE platform (Sun Microsystems, 2001).
Annotator Developer. The annotator developer role
is focused on developing core algorithms ranging from
statistical language recognizers to rule-based named-
entity detectors to document classifiers.
The framework design ensures that the annotator
developer need NOT develop code to address aggregate
system behavior or systems issues like interoperability,
recovery, remote communications, distributed deploy-
ment, etc., but instead allow them to focus squarely on
the algorithmic logic and the logical representation of
their results.
This was achieved through the analysis engine
framework by requiring the annotator developer to un-
derstand only three interfaces, namely the Annotator,
AnnotatorContext, and CAS interfaces. The annotator
developer performs the following steps:
</bodyText>
<listItem confidence="0.998280666666667">
1) Implement Annotator interface
2) Encode analysis algorithm using the CAS inter-
face to read input and write results and the An-
notatorContext interface to access resources
3) Write Analysis Engine Descriptor
4) Call Analysis Engine Factory
</listItem>
<bodyText confidence="0.99991554">
To embed an analysis algorithm in the framework,
the annotator developer implements the Annotator inter-
face. This interface is simple and requires the imple-
mentation of only two methods: one for initialization
and one to analyze a document.
It is only through the CAS that the annotator devel-
oper accesses input data and registers analysis results.
The CAS contains the original document (the subject of
analysis) plus the meta-data contributed by any analysis
engines that have run previously. This meta-data may
include annotations over elements of the original docu-
ment. The CAS input to an analysis engine may reside
in memory, be managed remotely, or shared by other
components. These issues are of concern to the analysis
engine deployer role, but the annotator developer is in-
sulated from these issues.
All external resources, such as dictionaries, that an
annotator needs to consult are accessed through the An-
notator Context interface. The exact physical manifes-
tation of the data can therefore be determined by the
deployer, as can decisions about whether and how to
cache the resource data.
The annotator developer completes an XML descrip-
tor that identifies the input requirements, output specifi-
cations, and external resource dependencies. Given the
annotator object and the descriptor, the framework’s
Analysis Engine Factory returns a complete analysis
engine.
Analysis Engine Assembler. The analysis engine
assembler creates aggregate analysis engines through
the declarative coordination of component engines. The
design objective is to allow the assembler to build an
aggregate engine without writing any code.
The analysis engine assembler considers available
engines in terms of their capabilities and declaratively
describes flow constraints. These constraints are cap-
tured in the aggregate engine’s XML descriptor along
with the identities of selected component engines. The
assembler inputs this descriptor in the framework’s
analysis engine factory object and an aggregate analysis
engine is created and returned.
Analysis Engine Deployer. The analysis engine de-
ployer decides how analysis engines and the resources
they require are deployed on particular hardware and
system middleware. UIMA does not provide its own
specification for how components are deployed, nor
does it mandate the use of a particular type of middle-
ware or middleware product. Instead, UIMA aims to
give deployers the flexibility to choose the middleware
that meets their needs.
</bodyText>
<subsectionHeader confidence="0.999775">
3.3 Insulate Lower-Level System Middleware
</subsectionHeader>
<bodyText confidence="0.999973882352941">
HLT applications can share many requirements with
other types of applications – for example, they may
need scalability, security, and transactions. Existing
middleware such as application servers can meet many
of these needs. On the other hand, HLT applications
may need to have a small footprint so they can be de-
ployed on a desktop computer or PDA or they may need
to be embeddable within other applications that use their
own middleware.
One design goal of UIMA is to support deployment
of analysis engines on any type of middleware, and to
insulate the annotator developer and analysis engine
assembler from these concerns. This is done through
the use of Service Wrappers and the Analysis Structure
Broker. The analysis engine interface specifies that
input and output are done via a CAS, but it does not
specify how that CAS is transported between compo-
nent analysis engines. A service wrapper implements
the CAS serialization and deserialization necessary for a
particular deployment. Within an aggregate Analysis
Engine, components may be deployed using different
service wrappers. The Analysis Structure Broker is the
component that transports the CAS between these com-
ponents regardless of how they are deployed.
To support a new type of middleware, a new service
wrapper and an extension to the Analysis Structure Bro-
ker must be developed and plugged into the framework.
The Analysis Engine itself does not need to be modified
in any way.
For example, we have implemented Service Wrap-
pers and Analysis Structure Broker on top of both a
web-services and a message queuing infrastructure.
Each implementation has different pros and cons for
deployment scenarios.
</bodyText>
<sectionHeader confidence="0.996429" genericHeader="method">
4 Measuring Success
</sectionHeader>
<bodyText confidence="0.986593">
We have considered four ways to evaluate UIMA in
meeting its intended objectives:
</bodyText>
<listItem confidence="0.99072125">
1) Combination Experiments
2) Compliant Components and their Reuse
3) System Performance Improvements
4) Product Integration
</listItem>
<subsectionHeader confidence="0.994943">
4.1 Combination Experiments
</subsectionHeader>
<bodyText confidence="0.9999693">
Our UIMA implementations and associated tooling have
led to the design of several combination experiments
that were previously unimagined or considered too
cumbersome to implement. This work has only just be-
gun but includes collaborative efforts combining rule-
based parsers with statistical machine translation en-
gines, statistical named-entity detectors in rule-based
question answering systems (Chu-Carroll et al., 2003),
and a host of independently developed analysis capabili-
ties in bioinformatics applications.
</bodyText>
<subsectionHeader confidence="0.999222">
4.2 Compliant Components and their Reuse
</subsectionHeader>
<bodyText confidence="0.999985294117647">
Another way to measure the impact of UIMA is to look
at its adoption by the target community. This may be
measured by the number of compliant components and
the frequency of their reuse by different projects/groups.
We have developed a registry and an integration test
bed where contributed components are tested, certified,
registered and made available to the community for
demonstration and download.
The integration test bed includes a web-based facil-
ity where users can select from a collection of corpora, a
collection of certified analysis engines and run the
analysis engine on the corpus. Performance statistics
breaking down the time spent in analysis, communica-
tions between components, and framework overhead are
computed and presented. The system generates analysis
results and stores them. Analysis results may be viewed
using any of a variety of CAS viewers. The results may
also be indexed by a search engine, which may then be
used to process queries.
While we are still instrumenting this site and gather-
ing reuse data, within six months of an internal distribu-
tion we will have over 15 UIMA-compliant analysis
engines, many of which are based on code developed as
part of multi-year HLT research efforts. Engines were
contributed from six independent and geographically
dispersed groups. They include several named-entity
detectors of both rule-based and statistical varieties,
several classifiers, a summarizer, deep and shallow
parsers, a semantic class detector, an Arabic to English
translator, an Arabic person detector, and several bio-
logical entity and relationship detectors. Several of these
engines have been assembled using component engines
that were previously inaccessible for reuse due to engi-
neering incompatibilities.
</bodyText>
<subsectionHeader confidence="0.999222">
4.3 System Performance Improvements
</subsectionHeader>
<bodyText confidence="0.99999447368421">
We expect the architecture’s support for modularization
and for the insulation of system deployment issues from
algorithm development to result in opportunities to
quickly deploy more robust and scaleable solutions.
For example, IBM&apos;s Question Answering system,
now under development for over three years, includes a
home-grown answer type detector to analyze large cor-
pora of millions of documents (Prager et al., 2002).
With a half day of training, an algorithm developer cast
the answer type detector as a UIMA Annotator and em-
bedded it in the UIMA Analysis Engine framework. The
framework provided the infrastructure necessary for
configuring an aggregate analysis engine with the an-
swer type detector down-stream of a tokenizer and
named-entity detector without additional programming.
Within a day, the framework was used to build the ag-
gregate analysis engine and to deploy multiple instances
of it on a host of machines. The result was a dramatic
improvement in overall throughput.
</bodyText>
<subsectionHeader confidence="0.983876">
4.4 Product Integration
</subsectionHeader>
<bodyText confidence="0.999974071428572">
IBM develops a variety of information management,
information integration, data mining, knowledge man-
agement and search-related products and services. Un-
structured information processing and language
technologies in particular represent an increasingly im-
portant capability that can enhance all of these products.
UIMA will be a business success for IBM if it plays
an important role in technology transfers. IBM Research
has engaged a number of product groups which are real-
izing the benefits of adopting a standard architectural
approach for integrating HLT that does not constrain
algorithm invention and that allows for easy extension
and integration and support for a wide variety of system
deployment options.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99996888">
The UIMA project at IBM has encouraged many groups
in six of the Research division’s labs to understand and
adopt the UIMA architecture as a common conceptual
foundation for classifying, describing, developing and
combining HLT components in aggregate applications
that integrate search and analytical functions.
While our measurements are only just beginning, the
adoption of UIMA has clearly improved knowledge
transfer throughout the organization. Implementations
of the architecture are advancing and are beginning to
demonstrate that HLT components developed within
Research can be quickly combined to explore hybrid
approaches as well as to rapidly transfer results into
IBM product and service offerings.
IBM product groups are encouraged by Research’s
effort and are committed to leverage UIMA as a vehicle
to embed HLT components. They are realizing the
benefits of having Research adopt a standard architec-
tural approach that does not constrain algorithm inven-
tion while allowing for a wide variety of system
deployment options. Product and service groups are
seeing an easier path to combine, integrate and deliver
these technologies into information integration, data
mining, knowledge management and search-related
products and services.
</bodyText>
<sectionHeader confidence="0.996547" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999954923076923">
We acknowledge the contributions of Dan Gruhl and the
WF project to the development of UIMA. In addition
we acknowledge David Johnson, Thomas Hampp, Thilo
Goetz and Oliver Suhre in the development of IBM’s
Text Analysis Framework and the work of Roy Byrd
and Mary Neff in the design of the Talent system. Their
work continues to influence the UIMA CAS and analy-
sis engine framework.
This work was supported in part by the Advanced
Research and Development Activity (ARDA)’s Ad-
vanced Question Answering for Intelligence
(AQUAINT) Program under contract number MDA904-
01-C-0988.
</bodyText>
<sectionHeader confidence="0.996426" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999510666666667">
Kaling Bontcheva, Hamish Cunningham, Valentin Tab-
lan, Diana Maynard, Horacio Saggion. 2002. “De-
veloping Reusable and Robust Language Processing
Components for Information Systems using GATE.”
3rd International Workshop on Natural Language and
Information Systems (NLIS&apos;2002), IEEE Computer
Society Press.
Jennifer Chu-Carroll, David Ferrucci, John Prager, and
Christopher Welty. 2003. &amp;quot;Hybridization in Question
Answering Systems.&amp;quot; Working Notes of the AAAI
Spring Symposium on New Directions in Question
Answering, to appear.
Hamish Cunningham, Kaling Bontcheva, Valentin Tab-
lan and Yorick Wilks. 2000. “Software Infrastructure
for Language Resources: a Taxonomy of Previous
Work and a Requirements Analysis.” Proceedings of
the Second Conference on Language Resources
Evaluation.
Ralph Grishman. 1996. “Tipster architecture design
document version 2.2.” Technical report, DARPA
TIPSTER.
Thilo Goetz, Robin Lougee-Heimer and Nicolas
Nicolov. 2001. “Efficient Indexing for Typed Fea-
ture Structures.” Proceedings of Recent Advances in
Natural Language Processing, Tzigov Chark, Bul-
garia.
Christophe Laprun, Johnathan Fiscus, John Garofolo,
and Sylvain Pajot. 2002. “A Practical Introduction to
ATLAS.” Proceedings of the Third International
Conference on Language Resources and Evaluation
(LREC).
John Prager, Jennifer Chu-Carroll, Eric Brown, and
Krzysztof Czuba. 2003. &amp;quot;Question Answering Using
Predictive Annotation.&amp;quot; Advances in Open-Domain
Question Answering, T. Strzalkowski &amp; S. Hara-
bagiu (eds.), Kluwer Academic Publishers, to appear.
John Prager, Eric Brown, Anni Coden and Dragomir
Radev. 2000. “Question-answering by Predictive
Annotation.” Proceedings of ACMSIGIR.
Scott Mardis and John Burger. 2002. “Qanda and the
Catalyst Architecture.” AAAI Spring Symposium on
Mining Answers from Text and Knowledge Bases.
Sun Microsystems, Inc. 2001. “JavaTM 2 Platform
Enterprise Edition Specification, v1.3.”
http://java.sun.com/j2ee/1.3/docs/.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999454">Accelerating Corporate Research in the Development, Application and Deployment of Human Language Technologies</title>
<author confidence="0.999996">David Ferrucci</author>
<affiliation confidence="0.999122">IBM T.J. Watson Research</affiliation>
<address confidence="0.821824">Yorktown Heights, NY 10598</address>
<email confidence="0.999418">ferrucci@us.ibm.com</email>
<author confidence="0.99993">Adam Lally</author>
<affiliation confidence="0.999117">IBM T.J. Watson Research</affiliation>
<address confidence="0.887358">Yorktown Heights, NY 10598</address>
<email confidence="0.916342">alally@us.ibm.com</email>
<abstract confidence="0.986857791044776">1 Architecture Goals In six major labs spread out over the globe, IBM Research has over 200 people working on Unstructured Information Management (UIM) technologies with a significant focus on Human Language Technologies (HLT). These researchers are engaged in activities ranging from natural language dialog to machine translation to bioinformatics to open-domain question answering. Each group is developing different technical and engineering approaches to process unstructured information (e.g., natural language text, voice, audio and video) in pursuit of specific research objectives and their applications. The high-level objectives of IBM’s Unstructured Information Management Architecture (UIMA) are two fold: 1) Accelerate scientific advances by enabling the rapid combination UIM technologies (e.g., natural language processing, video analysis, information retrieval, etc.). 2) Accelerate transfer of UIM technologies to product by providing a robust software framework that promotes reuse and supports flexible deployment options. UIMA is a software architecture for developing applications which integrate search and analytics over a combination of structured and unstructured information. define information information whose intended meaning is unambiguous and explicitly represented in the structure or format of the data. The canonical example is a database table. We define information information whose intended meaning is only implied by its form. The canonical example is a natural language document. Abstract IBM Research has over 200 people working on Unstructured Information Management (UIM) technologies with a strong focus on HLT. Spread out over the globe they are engaged in activities ranging from natural language dialog to machine translation to bioinformatics to open-domain question answering. An analysis of these activities strongly suggested that improving the organization’s ability to quickly discover each other&apos;s results and rapidly combine different technologies and approaches would accelerate scientific advance. Furthermore, the ability to reuse and combine results through a common architecture and a robust software framework would accelerate the transfer of research results in HLT into IBM’s product platforms. Market analyses indicating a growing need to process unstructured information, specifically multi-lingual, natural language text, coupled with IBM Research’s investment in HLT, led to the development of middleware architecture for processing unstructured information dubbed UIMA. At the heart of UIMA are powerful search capabilities and a data-driven framework for the development, composition distributed deployment of en- In this paper we give a general introduction to UIMA focusing on the design points of its analysis engine architecture and we discuss how UIMA is helping to accelerate research and technology transfer.</abstract>
<title confidence="0.858519066666667">Semantic Search Engine Query key words and Token and concept Indexing Application Logic Unstructured Information Relevant Knowledge Engine(s) Collection Processing Manager (CPM) Document, Collection</title>
<author confidence="0.527112">Meta-data Store</author>
<abstract confidence="0.989433320158102">(Text) Analysis Engines (TAEs) Analysis engines employing a variety of analytical techniques and strategies for detecting semantic content Structured Knowledge Access Knowledge Source Adapters (KSAs) integrate and deliver content from many structured knowledge sources according to central ontologies Structured Information Figure 1: UIMA High-Level Architecture The UIMA high-level architecture, illustrated in Figure 1, defines the roles, interfaces and communications of large-grained components essential for UIM applications. These include components capable of analyzing unstructured artifacts, integrating and accessing structured sources and storing, indexing and searching for artifacts based on discovered semantic content. As part of the UIMA project, IBM is developing different implementations of the architecture suitable for different classes of deployment. These range from lightweight and embeddable implementations to highly scaleable implementations that are meant to exploit clusters of machines and provide high throughput and high availability. While the architecture extends to a variety of unstructured artifacts including voice, audio and video, a primary analytic focus of current UIMA implementations is squarely on human language technologies. In this paper we will refer to elements of unstrucinformation processing as however, that an element may represent for the application, a whole text document, a text document fragment or even multiple documents. 2 Generalized Application Scenario and the High-Level Architecture In this section we provide a high-level overview of the UIMA architecture by describing its component roles in a generalized application scenario. The generalized scenario includes both analysis and access functions. Analysis functions are divided into two classes, namely document-level and collection-level analysis. Access functions are divided into semantic search and structured knowledge access. We refer to the software program that employs UIMA components to implement some end-user capaas the Analysis Document-level analysis is performed by component elements named Analysis Engines (TAEs). These are extensions of the generic analysis engine, specialized for text. They are analogous, for example, to Processing Resources in the GATE architecture (Cunningham et al., 2000). In UIMA, a TAE is a recursive structure which may be composed of sub or component engines each performing a different stage of the application’s analysis. Examples of Text Analysis Engines include language translators, document summarizers, document classifiers, and named-entity detectors. Each TAE specializes in discovering specific concepts (or &amp;quot;semantic entities&amp;quot;) otherwise unidentified or implicit in the document text. A TAE takes in a document and produces an analysis. The original document and its analysis are reprein a common structure called the System The CAS is conceptually to the other architectures, beginning with TIPSTER (Grishman, 1996). In general, annotations associate some meta-data with a region in the original artifact. Where the artifact is a text document, for example, the annotation associates meta-data (e.g., a label) with a span of text in the document by giving the span’s start and end positions. Annotations in the CAS are stand-off, meaning that the annotations are maintained separately from the document itself; this is more flexible than inline markup (Mardis and Burger, 2002). In UIMA, annotations are not the only type of information stored in the CAS. The CAS may be used to represent any class of meta-data element associated with analysis of a document regardless of whether it is explicitly linked to some sub component of the original document. The CAS also allows for multiple definitions of this linkage, as is necessary for the analysis of images, video or other modalities. The analysis represented in the CAS may be thought of as a collection of meta-data that is enriched as it passes through successive stages of analysis. At a specific stage of analysis, for example, the CAS may include a deep parse. A named-entity detector receiving this CAS may consider the deep parse to identify named entities. The named entities may be input to an analysis engine that produces summaries or classifications of the document. The UIMA CAS object provides general objectbased representation with a hierarchical type system supporting single inheritance. It includes data creation, access and serialization methods designed for the efficient representation, access and transport of analysis results among TAEs and between TAEs and other UIMA components or applications. Elements in the CAS may be indexed for fast access (Goetz et al., 2001). The CAS has been implemented in C++ and Java with serialization methods for binary as well as XML formats for managing the tradeoff between efficiency and interoperability. Analysis Documents are gathered by the application and organinto collections. The architecture defines a Collec- Reader Implementations of the Collection Reader provide access to collection elements, collection meta-data and element meta-data. UIMA implementations include a document, collection and meta-data store that implements the Collection Reader interface and manages multiple collections and their elements. However, applications that need to manage their own collections can provide an implementation of a Collection Reader to UIMA components that require access to collection data. Collections are analyzed to produce collection level analysis results. These results represent aggregate inferences computed over all or some subset of the documents in a collection. The component of an application analyzes an entire collection is considered a Collec- Analysis These engines typically apply element-level, or more specifically document-level analysis, to elements of a collection and then considering the element analyses in performing aggregate computations. Examples of collection level analysis results include sub collections where elements contain certain features, glossaries of terms with their variants and frequencies, taxonomies, feature vectors for statistical categorizers, databases of extracted relations, and master indices of tokens and other detected entities. In support of Collection Analysis Engines, UIMA the Processing Manager component. The CPM’s primary responsibility is to manage the application of a designated TAE to each document accessible through a Collection Reader. A Collection Analysis Engine may provide, as input to the CPM, a TAE and a Collection Reader. The CPM applies the TAE and returns the analysis, represented by a CAS, for each element in the collection. To control the process, the CPM provides administrative functions that include failure reporting, pausing and restarting. At the request of the application’s collection analysis engine, the CPM may be optionally configured to perform functions typical of UIM application scenarios. Examples of these include: Filtering ensures that only certain elements are processed based on meta-data constraints. Persistence stores element-level analysis rein a provided Writer. Indexing indexes documents using a designated search engine indexing interface based on meta-data extracted from the analysis. Parallelization manages the creation and execution of multiple instances of a TAE for processing multiple documents simultaneously utilizing available computing resources. Search To support the concept of “semantic search” – the capability to find documents based on semantic content discovered by document or collection level analysis and represented as annotations – UIMA specifies search engine indexing and query interfaces. A key feature of the indexing interface is that it supports the indexing of tokens as well as annotations and particularly cross-over annotations. Two or more annoanother if they are linked to intersecting regions of the document. The key feature of the query interface is that it supports queries that may be predicated on nested structures of annotations and tokens in addition to Boolean combinations of tokens and annotations. Knowledge Access As analysis engines do their job they may consult a wide variety of structured knowledge sources. To increase reusability and facilitate integration, UIMA the Source Adapter interface. KSA objects provide a layer of uniform access to disparate knowledge sources. They manage the technical communication, representation language and ontology mapping necessary to deliver knowledge encoded in databases, dictionaries, knowledge bases and other structured sources in a uniform way. The primary interface to a KSA presents structured knowledge as instantiated predicates using the Knowledge Interchange Format (KIF) encoded in XML. A key aspect of the KSA architecture is the KSA meta-data and related services supporting KSA registration and search. These services include the description and registration of named ontologies. Ontologies are described by the concepts and predicates they include. The KSA is self-descriptive and among other meta-data includes the predicate signatures belonging to registered ontologies that the KSA can instantiate and the knowledge sources it consults. Application or analysis engine developers can consult human browseable KSA directory services to search for and find KSAs that instantiate predicates of a registered ontology. The service will deliver a handle to a web service or an embeddable KSA component. 3 Analysis Engine Framework This section takes a closer look at the analysis engine framework. UIMA specifies an interface for an analysis engine; roughly speaking it is “CAS in” and “CAS out”. There are other operations used for filtering, administrative Figure 2: Primitive Analysis Engine and self-descriptive functions, but the main interface takes a CAS as input and delivers a CAS as output. Any program that implements this interface may be plugged in as an analysis engine component in an implementation of UIMA. However, as part of UIMA tooling we have developed an analysis engine framework to support the creation, composition and flexible deployment of primitive and aggregate analysis engines on a variety of different system middleware platforms. The underlying design philosophy for the Analysis Engine framework was driven by three primary principles: 1) Encourage and enable component reuse. 2) Support distinct development roles insulating the algorithm developer from system and deployment details. 3) Support a flexible variety of deployment options by insulating lower-level system middleware APIs. and Enable Component Reuse With many HLT components being developed throughout IBM Research by independent groups, encouraging and enabling reuse is a critical design objective to achieve expected efficiencies and cross-group collaborations. Three characteristics of the analysis engine framework address this objective: 1) Recursive Structure 2) Data-Driven 3) Self-Descriptive CAS (part of framework) 9etCAS() 9etCAS() Primitive Analysis Engine Controller (part of framework) reads/writes analysis data process(Result Spec.)</abstract>
<note confidence="0.644616142857143">process(Result Spec.) Annotator (Annotator Developer Implements) process(Result Spec.) Aggregate Analysis Engine getCAS(</note>
<title confidence="0.50971775">CAS Analysis Sequencer init (Result Spec.) Process(Result Spec.)</title>
<abstract confidence="0.989303727272728">next() read/write analysis data Controller Analysis Engine 1 process(RS) Analysis Structure Broker process(Result Spec.) Analysis Engine 2 process(RS) process(RS) Analysis Engine 3 Figure 3: Aggregate Analysis Engine A primitive analysis engine, illusin Figure 2, is composed of an a CAS. The annotator is the object that implements the analysis logic (e.g. tokenization, grammatical parsing, entity detection). It reads the original document content and meta-data from the CAS. It then computes and writes new meta-data to the CAS. An aggregate analysis engine, illustrated in Figure 3, is composed of two or more component analysis engines, but implements exactly the same external interface as the primitive engine. At run-time an aggregate analysis engine is given a sequence in which to execute its component engines. A called the Structure Broker that each component engine has access to the CAS according to the specified sequence. Like any nested programming model, this recursive structure ensures that components may be easily reused in combination with one another while insulating their internal structure. An analysis engine’s processing model is strictly data-driven. This means that an annotator’s analysis logic may be predicated only on the content of its input and not on the specific analysis engines it may be combined with or the control sequence in which it may be embedded. This restriction ensures that an analysis engine may be successfully reused in different aggregate structures and different control environments as long as its input requirements are met. Sequencer a component in the framework responsible for dynamically determining the next analysis engine to receive access to the CAS. The Analysis Sequencer is distinct from the Analysis Structure Broker, whose responsibility is to deliver the CAS to the next analysis engine whichever it is wherever it may be located. The Analysis Sequencer’s control logic is separate from the analysis logic embedded in an Annotator and separate from the Analysis Structure Broker’s concerns related to ensuring and/or optimizing the CAS transport. This separation of concerns allows for the plug-n-play of different Analysis Sequencers. The Analysis Sequencer is a pluggable range from provide simple iteration over a declaratively specified static flow to complex planning algorithms. Current implementations have been limited to simple linear flows between analysis engines; however more advanced applications are generating requirements for dynamic and adaptive sequencing. How much of the control specification ends up in a declarative representation and how much is implemented in the sequencer for these advanced requirements is currently being explored. that analysis engines may be easily composed to form aggregates and may be reused in different control sequences is necessary for technical reusability but not sufficient for enabling and validating reuse within a broad community of developers. To promote reuse, analysis engine developers must be able to discover which analysis engines are available in terms of what they do – their capabilities. Each analysis engine&apos;s data model is declared in XML and then dynamically realized in the CAS at runtime, an approach similar to MAIA (Laprun et al., 2002). In UIMA, however, analysis engines publish their input requirements and output specifications relative to this declared data model, and this information is used to register the analysis engine in an analysis engine directory service. This service includes a humanoriented interface that allows application developers to browse and/or search for analysis engines that meet their needs. While self-description and related directory services will promote reuse, their value is still dependent on establishing common data models (or fragments thereof) to which analysis engine capability descriptions subscribe. Distinct Development Roles Language technology researchers that specialize in, for example, multi-lingual machine translation, may not be highly trained software engineers nor be skilled in the system technologies required for flexible and scaleable deployments. Yet one of the primary objectives of the UIMA project is to ensure that their work can be efficiently deployed in robust and scaleable system architecture. Along the same lines, researchers with ideas about how to combine and orchestrate different components may not themselves be algorithm developers or systems engineers, yet we need to enable them to rapidly create and validate ideas through combining existing components. Finally, deploying analysis engines as distributed, highly available services or as collocated objects in an aggregate system requires yet another skill. As a result we have identified the following development roles and have designed the architecture with independent sets of interfaces in support of each of these different skill sets. Our separation of development roles is analogous to the separation of roles in Sun&apos;s J2EE platform (Sun Microsystems, 2001). Developer. annotator developer role is focused on developing core algorithms ranging from statistical language recognizers to rule-based namedentity detectors to document classifiers. The framework design ensures that the annotator developer need NOT develop code to address aggregate system behavior or systems issues like interoperability, recovery, remote communications, distributed deployment, etc., but instead allow them to focus squarely on the algorithmic logic and the logical representation of their results. This was achieved through the analysis engine framework by requiring the annotator developer to unonly three interfaces, namely the and The annotator developer performs the following steps: 1) Implement Annotator interface 2) Encode analysis algorithm using the CAS interface to read input and write results and the AnnotatorContext interface to access resources 3) Write Analysis Engine Descriptor 4) Call Analysis Engine Factory To embed an analysis algorithm in the framework, the annotator developer implements the Annotator interface. This interface is simple and requires the implementation of only two methods: one for initialization and one to analyze a document. It is only through the CAS that the annotator developer accesses input data and registers analysis results. The CAS contains the original document (the subject of analysis) plus the meta-data contributed by any analysis engines that have run previously. This meta-data may include annotations over elements of the original document. The CAS input to an analysis engine may reside in memory, be managed remotely, or shared by other components. These issues are of concern to the analysis engine deployer role, but the annotator developer is insulated from these issues. All external resources, such as dictionaries, that an annotator needs to consult are accessed through the Annotator Context interface. The exact physical manifestation of the data can therefore be determined by the deployer, as can decisions about whether and how to cache the resource data. The annotator developer completes an XML descriptor that identifies the input requirements, output specifications, and external resource dependencies. Given the annotator object and the descriptor, the framework’s Analysis Engine Factory returns a complete analysis engine. Engine Assembler. analysis engine assembler creates aggregate analysis engines through the declarative coordination of component engines. The design objective is to allow the assembler to build an aggregate engine without writing any code. The analysis engine assembler considers available engines in terms of their capabilities and declaratively describes flow constraints. These constraints are captured in the aggregate engine’s XML descriptor along with the identities of selected component engines. The assembler inputs this descriptor in the framework’s analysis engine factory object and an aggregate analysis engine is created and returned. Engine Deployer. analysis engine deployer decides how analysis engines and the resources they require are deployed on particular hardware and system middleware. UIMA does not provide its own specification for how components are deployed, nor does it mandate the use of a particular type of middleware or middleware product. Instead, UIMA aims to give deployers the flexibility to choose the middleware that meets their needs. Lower-Level System Middleware HLT applications can share many requirements with other types of applications – for example, they may need scalability, security, and transactions. Existing middleware such as application servers can meet many of these needs. On the other hand, HLT applications may need to have a small footprint so they can be deployed on a desktop computer or PDA or they may need to be embeddable within other applications that use their own middleware. One design goal of UIMA is to support deployment of analysis engines on any type of middleware, and to insulate the annotator developer and analysis engine assembler from these concerns. This is done through use of Wrappers the Analysis Structure Broker. The analysis engine interface specifies that input and output are done via a CAS, but it does not specify how that CAS is transported between component analysis engines. A service wrapper implements the CAS serialization and deserialization necessary for a particular deployment. Within an aggregate Analysis Engine, components may be deployed using different service wrappers. The Analysis Structure Broker is the component that transports the CAS between these components regardless of how they are deployed. To support a new type of middleware, a new service wrapper and an extension to the Analysis Structure Broker must be developed and plugged into the framework. The Analysis Engine itself does not need to be modified in any way. For example, we have implemented Service Wrappers and Analysis Structure Broker on top of both a web-services and a message queuing infrastructure. Each implementation has different pros and cons for deployment scenarios. 4 Measuring Success We have considered four ways to evaluate UIMA in meeting its intended objectives: 1) Combination Experiments 2) Compliant Components and their Reuse 3) System Performance Improvements 4) Product Integration Experiments Our UIMA implementations and associated tooling have led to the design of several combination experiments that were previously unimagined or considered too cumbersome to implement. This work has only just begun but includes collaborative efforts combining rulebased parsers with statistical machine translation engines, statistical named-entity detectors in rule-based question answering systems (Chu-Carroll et al., 2003), and a host of independently developed analysis capabilities in bioinformatics applications. Components and their Reuse Another way to measure the impact of UIMA is to look at its adoption by the target community. This may be measured by the number of compliant components and the frequency of their reuse by different projects/groups. We have developed a registry and an integration test bed where contributed components are tested, certified, registered and made available to the community for demonstration and download. The integration test bed includes a web-based facility where users can select from a collection of corpora, a collection of certified analysis engines and run the analysis engine on the corpus. Performance statistics breaking down the time spent in analysis, communications between components, and framework overhead are computed and presented. The system generates analysis results and stores them. Analysis results may be viewed using any of a variety of CAS viewers. The results may also be indexed by a search engine, which may then be used to process queries. While we are still instrumenting this site and gathering reuse data, within six months of an internal distribution we will have over 15 UIMA-compliant analysis engines, many of which are based on code developed as part of multi-year HLT research efforts. Engines were contributed from six independent and geographically dispersed groups. They include several named-entity detectors of both rule-based and statistical varieties, several classifiers, a summarizer, deep and shallow parsers, a semantic class detector, an Arabic to English translator, an Arabic person detector, and several biological entity and relationship detectors. Several of these engines have been assembled using component engines that were previously inaccessible for reuse due to engineering incompatibilities. Performance Improvements We expect the architecture’s support for modularization and for the insulation of system deployment issues from algorithm development to result in opportunities to quickly deploy more robust and scaleable solutions. For example, IBM&apos;s Question Answering system, now under development for over three years, includes a home-grown answer type detector to analyze large corpora of millions of documents (Prager et al., 2002). With a half day of training, an algorithm developer cast the answer type detector as a UIMA Annotator and embedded it in the UIMA Analysis Engine framework. The framework provided the infrastructure necessary for configuring an aggregate analysis engine with the answer type detector down-stream of a tokenizer and named-entity detector without additional programming. Within a day, the framework was used to build the aggregate analysis engine and to deploy multiple instances of it on a host of machines. The result was a dramatic improvement in overall throughput. Integration IBM develops a variety of information management, information integration, data mining, knowledge management and search-related products and services. Unstructured information processing and language technologies in particular represent an increasingly important capability that can enhance all of these products. UIMA will be a business success for IBM if it plays an important role in technology transfers. IBM Research has engaged a number of product groups which are realizing the benefits of adopting a standard architectural approach for integrating HLT that does not constrain algorithm invention and that allows for easy extension and integration and support for a wide variety of system deployment options. 5 Conclusion The UIMA project at IBM has encouraged many groups in six of the Research division’s labs to understand and adopt the UIMA architecture as a common conceptual foundation for classifying, describing, developing and combining HLT components in aggregate applications that integrate search and analytical functions. While our measurements are only just beginning, the adoption of UIMA has clearly improved knowledge transfer throughout the organization. Implementations of the architecture are advancing and are beginning to demonstrate that HLT components developed within Research can be quickly combined to explore hybrid approaches as well as to rapidly transfer results into IBM product and service offerings. IBM product groups are encouraged by Research’s effort and are committed to leverage UIMA as a vehicle to embed HLT components. They are realizing the benefits of having Research adopt a standard architectural approach that does not constrain algorithm invention while allowing for a wide variety of system deployment options. Product and service groups are seeing an easier path to combine, integrate and deliver these technologies into information integration, data mining, knowledge management and search-related products and services. Acknowledgements We acknowledge the contributions of Dan Gruhl and the WF project to the development of UIMA. In addition we acknowledge David Johnson, Thomas Hampp, Thilo Goetz and Oliver Suhre in the development of IBM’s Text Analysis Framework and the work of Roy Byrd and Mary Neff in the design of the Talent system. Their work continues to influence the UIMA CAS and analysis engine framework.</abstract>
<note confidence="0.798827875">This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program under contract number MDA904- 01-C-0988. References Kaling Bontcheva, Hamish Cunningham, Valentin Tablan, Diana Maynard, Horacio Saggion. 2002. “De-</note>
<title confidence="0.782007">veloping Reusable and Robust Language Processing</title>
<note confidence="0.699388416666667">Components for Information Systems using GATE.” 3rd International Workshop on Natural Language and Information Systems (NLIS&apos;2002), IEEE Computer Society Press. Jennifer Chu-Carroll, David Ferrucci, John Prager, and Christopher Welty. 2003. &amp;quot;Hybridization in Question Answering Systems.&amp;quot; Working Notes of the AAAI Spring Symposium on New Directions in Question Answering, to appear. Hamish Cunningham, Kaling Bontcheva, Valentin Tablan and Yorick Wilks. 2000. “Software Infrastructure for Language Resources: a Taxonomy of Previous Work and a Requirements Analysis.” Proceedings of the Second Conference on Language Resources Evaluation. Ralph Grishman. 1996. “Tipster architecture design document version 2.2.” Technical report, DARPA TIPSTER. Thilo Goetz, Robin Lougee-Heimer and Nicolas Nicolov. 2001. “Efficient Indexing for Typed Feature Structures.” Proceedings of Recent Advances in Natural Language Processing, Tzigov Chark, Bulgaria. Christophe Laprun, Johnathan Fiscus, John Garofolo,</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kaling Bontcheva</author>
<author>Hamish Cunningham</author>
<author>Valentin Tablan</author>
<author>Diana Maynard</author>
<author>Horacio Saggion</author>
</authors>
<title>Developing Reusable and Robust Language Processing Components for Information Systems using</title>
<date>2002</date>
<booktitle>GATE.” 3rd International Workshop on Natural Language and Information Systems (NLIS&apos;2002), IEEE</booktitle>
<publisher>Computer Society Press.</publisher>
<marker>Bontcheva, Cunningham, Tablan, Maynard, Saggion, 2002</marker>
<rawString>Kaling Bontcheva, Hamish Cunningham, Valentin Tablan, Diana Maynard, Horacio Saggion. 2002. “Developing Reusable and Robust Language Processing Components for Information Systems using GATE.” 3rd International Workshop on Natural Language and Information Systems (NLIS&apos;2002), IEEE Computer Society Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>David Ferrucci</author>
<author>John Prager</author>
<author>Christopher Welty</author>
</authors>
<title>Hybridization in Question Answering Systems.&amp;quot; Working Notes of the AAAI Spring Symposium on New Directions in Question Answering,</title>
<date>2003</date>
<note>to appear.</note>
<contexts>
<context position="26279" citStr="Chu-Carroll et al., 2003" startWordPosition="3904" endWordPosition="3907">evaluate UIMA in meeting its intended objectives: 1) Combination Experiments 2) Compliant Components and their Reuse 3) System Performance Improvements 4) Product Integration 4.1 Combination Experiments Our UIMA implementations and associated tooling have led to the design of several combination experiments that were previously unimagined or considered too cumbersome to implement. This work has only just begun but includes collaborative efforts combining rulebased parsers with statistical machine translation engines, statistical named-entity detectors in rule-based question answering systems (Chu-Carroll et al., 2003), and a host of independently developed analysis capabilities in bioinformatics applications. 4.2 Compliant Components and their Reuse Another way to measure the impact of UIMA is to look at its adoption by the target community. This may be measured by the number of compliant components and the frequency of their reuse by different projects/groups. We have developed a registry and an integration test bed where contributed components are tested, certified, registered and made available to the community for demonstration and download. The integration test bed includes a web-based facility where </context>
</contexts>
<marker>Chu-Carroll, Ferrucci, Prager, Welty, 2003</marker>
<rawString>Jennifer Chu-Carroll, David Ferrucci, John Prager, and Christopher Welty. 2003. &amp;quot;Hybridization in Question Answering Systems.&amp;quot; Working Notes of the AAAI Spring Symposium on New Directions in Question Answering, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
</authors>
<title>Kaling Bontcheva, Valentin Tablan and Yorick Wilks.</title>
<date>2000</date>
<booktitle>Proceedings of the Second Conference on Language Resources Evaluation.</booktitle>
<marker>Cunningham, 2000</marker>
<rawString>Hamish Cunningham, Kaling Bontcheva, Valentin Tablan and Yorick Wilks. 2000. “Software Infrastructure for Language Resources: a Taxonomy of Previous Work and a Requirements Analysis.” Proceedings of the Second Conference on Language Resources Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>Tipster architecture design document version 2.2.”</title>
<date>1996</date>
<tech>Technical report, DARPA TIPSTER.</tech>
<contexts>
<context position="6631" citStr="Grishman, 1996" startWordPosition="937" endWordPosition="938">orming a different stage of the application’s analysis. Examples of Text Analysis Engines include language translators, document summarizers, document classifiers, and named-entity detectors. Each TAE specializes in discovering specific concepts (or &amp;quot;semantic entities&amp;quot;) otherwise unidentified or implicit in the document text. A TAE takes in a document and produces an analysis. The original document and its analysis are represented in a common structure called the Common Analysis System or CAS. The CAS is conceptually analogous to the annotations in other architectures, beginning with TIPSTER (Grishman, 1996). In general, annotations associate some meta-data with a region in the original artifact. Where the artifact is a text document, for example, the annotation associates meta-data (e.g., a label) with a span of text in the document by giving the span’s start and end positions. Annotations in the CAS are stand-off, meaning that the annotations are maintained separately from the document itself; this is more flexible than inline markup (Mardis and Burger, 2002). In UIMA, annotations are not the only type of information stored in the CAS. The CAS may be used to represent any class of meta-data ele</context>
</contexts>
<marker>Grishman, 1996</marker>
<rawString>Ralph Grishman. 1996. “Tipster architecture design document version 2.2.” Technical report, DARPA TIPSTER.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thilo Goetz</author>
<author>Robin Lougee-Heimer</author>
<author>Nicolas Nicolov</author>
</authors>
<title>Efficient Indexing for Typed Feature Structures.”</title>
<date>2001</date>
<booktitle>Proceedings of Recent Advances in Natural Language Processing, Tzigov Chark,</booktitle>
<contexts>
<context position="8360" citStr="Goetz et al., 2001" startWordPosition="1219" endWordPosition="1222">-entity detector receiving this CAS may consider the deep parse to identify named entities. The named entities may be input to an analysis engine that produces summaries or classifications of the document. The UIMA CAS object provides general objectbased representation with a hierarchical type system supporting single inheritance. It includes data creation, access and serialization methods designed for the efficient representation, access and transport of analysis results among TAEs and between TAEs and other UIMA components or applications. Elements in the CAS may be indexed for fast access (Goetz et al., 2001). The CAS has been implemented in C++ and Java with serialization methods for binary as well as XML formats for managing the tradeoff between efficiency and interoperability. 2.2 Collection-Level Analysis Documents are gathered by the application and organized into collections. The architecture defines a Collection Reader interface. Implementations of the Collection Reader provide access to collection elements, collection meta-data and element meta-data. UIMA implementations include a document, collection and meta-data store that implements the Collection Reader interface and manages multiple </context>
</contexts>
<marker>Goetz, Lougee-Heimer, Nicolov, 2001</marker>
<rawString>Thilo Goetz, Robin Lougee-Heimer and Nicolas Nicolov. 2001. “Efficient Indexing for Typed Feature Structures.” Proceedings of Recent Advances in Natural Language Processing, Tzigov Chark, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christophe Laprun</author>
<author>Johnathan Fiscus</author>
<author>John Garofolo</author>
<author>Sylvain Pajot</author>
</authors>
<title>A Practical Introduction to ATLAS.”</title>
<date>2002</date>
<booktitle>Proceedings of the Third International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="18616" citStr="Laprun et al., 2002" startWordPosition="2758" endWordPosition="2761">uirements is currently being explored. Self-Descriptive. Ensuring that analysis engines may be easily composed to form aggregates and may be reused in different control sequences is necessary for technical reusability but not sufficient for enabling and validating reuse within a broad community of developers. To promote reuse, analysis engine developers must be able to discover which analysis engines are available in terms of what they do – their capabilities. Each analysis engine&apos;s data model is declared in XML and then dynamically realized in the CAS at runtime, an approach similar to MAIA (Laprun et al., 2002). In UIMA, however, analysis engines publish their input requirements and output specifications relative to this declared data model, and this information is used to register the analysis engine in an analysis engine directory service. This service includes a humanoriented interface that allows application developers to browse and/or search for analysis engines that meet their needs. While self-description and related directory services will promote reuse, their value is still dependent on establishing common data models (or fragments thereof) to which analysis engine capability descriptions s</context>
</contexts>
<marker>Laprun, Fiscus, Garofolo, Pajot, 2002</marker>
<rawString>Christophe Laprun, Johnathan Fiscus, John Garofolo, and Sylvain Pajot. 2002. “A Practical Introduction to ATLAS.” Proceedings of the Third International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Jennifer Chu-Carroll</author>
<author>Eric Brown</author>
<author>Krzysztof Czuba</author>
</authors>
<title>Question Answering Using Predictive Annotation.&amp;quot;</title>
<date>2003</date>
<booktitle>Advances in Open-Domain Question Answering,</booktitle>
<editor>T. Strzalkowski &amp; S. Harabagiu (eds.),</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<note>to appear.</note>
<marker>Prager, Chu-Carroll, Brown, Czuba, 2003</marker>
<rawString>John Prager, Jennifer Chu-Carroll, Eric Brown, and Krzysztof Czuba. 2003. &amp;quot;Question Answering Using Predictive Annotation.&amp;quot; Advances in Open-Domain Question Answering, T. Strzalkowski &amp; S. Harabagiu (eds.), Kluwer Academic Publishers, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
</authors>
<title>Eric Brown, Anni Coden and Dragomir Radev.</title>
<date>2000</date>
<booktitle>Proceedings of ACMSIGIR.</booktitle>
<marker>Prager, 2000</marker>
<rawString>John Prager, Eric Brown, Anni Coden and Dragomir Radev. 2000. “Question-answering by Predictive Annotation.” Proceedings of ACMSIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Mardis</author>
<author>John Burger</author>
</authors>
<date>2002</date>
<booktitle>Qanda and the Catalyst Architecture.” AAAI Spring Symposium on Mining Answers from Text and Knowledge Bases.</booktitle>
<contexts>
<context position="7093" citStr="Mardis and Burger, 2002" startWordPosition="1010" endWordPosition="1013">ure called the Common Analysis System or CAS. The CAS is conceptually analogous to the annotations in other architectures, beginning with TIPSTER (Grishman, 1996). In general, annotations associate some meta-data with a region in the original artifact. Where the artifact is a text document, for example, the annotation associates meta-data (e.g., a label) with a span of text in the document by giving the span’s start and end positions. Annotations in the CAS are stand-off, meaning that the annotations are maintained separately from the document itself; this is more flexible than inline markup (Mardis and Burger, 2002). In UIMA, annotations are not the only type of information stored in the CAS. The CAS may be used to represent any class of meta-data element associated with analysis of a document regardless of whether it is explicitly linked to some sub component of the original document. The CAS also allows for multiple definitions of this linkage, as is necessary for the analysis of images, video or other modalities. The analysis represented in the CAS may be thought of as a collection of meta-data that is enriched as it passes through successive stages of analysis. At a specific stage of analysis, for ex</context>
</contexts>
<marker>Mardis, Burger, 2002</marker>
<rawString>Scott Mardis and John Burger. 2002. “Qanda and the Catalyst Architecture.” AAAI Spring Symposium on Mining Answers from Text and Knowledge Bases.</rawString>
</citation>
<citation valid="true">
<date>2001</date>
<booktitle>JavaTM 2 Platform Enterprise Edition Specification, v1.3.” http://java.sun.com/j2ee/1.3/docs/.</booktitle>
<institution>Sun Microsystems, Inc.</institution>
<marker>2001</marker>
<rawString>Sun Microsystems, Inc. 2001. “JavaTM 2 Platform Enterprise Edition Specification, v1.3.” http://java.sun.com/j2ee/1.3/docs/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>