<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.683445">
ROBUST TEXT PROCESSING IN AUTOMATED INFORMATION RETRIEVAL
</title>
<author confidence="0.971357">
Tomek Strzalkowski
</author>
<affiliation confidence="0.977368">
Courant Institute of Mathematical Sciences
</affiliation>
<address confidence="0.952854666666667">
New York University
715 Broadway, rm. 704
New York, NY 10003
</address>
<email confidence="0.998224">
tomek@cs.nyu.edu
</email>
<sectionHeader confidence="0.959088" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999978392857143">
This paper outlines a prototype text retrieval system
which uses relatively advanced natural language pro-
cessing techniques in order to enhance the effective-
ness of statistical document retrieval. The backbone
of our system is a traditional retrieval engine which
builds inverted index files from pre-processed docu-
ments, and then searches and ranks the documents in
response to user queries. Natural language process-
ing is used to (1) preprocess the documents in order
to extract contents-carrying terms, (2) discover inter-
term dependencies and build a conceptual hierarchy
specific to the database domain, and (3) process
user&apos;s natural language requests into effective search
queries. The basic assumption of this design is that
term-based representation of contents is in principle
sufficient to build an effective if not optimal search
query out of any user&apos;s request. This has been
confirmed by an experiment that compared effective-
ness of expert-user prepared queries with those
derived automatically from an initial narrative infor-
mation request. In this paper we show that large-
scale natural language processing (hundreds of mil-
lions of words and more) is not only required for a
better retrieval, but it is also doable, given appropri-
ate resources. We report on selected preliminary
results of experiments with 500 MByte database of
Wall Street Journal articles, as well as some earlier
results with a smaller document collection.
</bodyText>
<sectionHeader confidence="0.988566" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999977444444445">
A typical information retrieval (IR) task is to
select documents from a database in response to a
user&apos;s query, and rank these documents according to
relevance. This has been usually accomplished using
statistical methods (often coupled with manual
encoding) that (a) select terms (words, phrases. and
other units) from documents that are deemed to best
represent their contents, and (b) create an inverted
index file (or files) that provide and easy access to
documents containing these terms. An important
issue here is that of finding an appropriate
combination of term weights which would reflect
each term&apos;s relative contribution to the information
contents of the document. Among many possible
weighting schemes the inverted document frequency
(idf) has come to be recognized as universally appli-
cable across variety of different text collections.
Once the index is created, the search process
will attempt to match a preprocessed user query (or
queries) against representations of documents in each
case determining a degree of relevance between the
two which depends upon the number and types of
matching terms. Although many sophisticated search
and matching methods are available, the crucial prob-
lem remains to be that of an adequate representation
of contents for both the documents and the queries.
The simplest word-based representations of
contents are usually inadequate since single words
are rarely specific enough for accurate discrimina-
tion, and their grouping is often accidental. A better
method is to identify groups of words that create
meaningful phrases, especially if these phrases
denote important concepts in database domain. For
example, joint venture is an important term in Wall
Street Journal (WSJ henceforth) database, while nei-
ther joint nor venture are important by themselves. In
the retrieval experiments with the WSJ database. we
noticed that both joint and venture were dropped
from the list of terms by the system because their idf
weights were too low. In large databases, such as
TIPSTER/TREC, the use of phrasal terms is not just
desirable, it becomes necessary.
The question thus becomes, how to identify the
correct phrases in the text? Both statistical and syn-
tactic methods were used before with only limited
success. Statistical methods based on word co-
occurrences and mutual information are prone to high
error rates. turning out many unwanted associations.
Syntactic methods suffered from low quality of gen-
erated parse structures that could be attributed to lim-
ited coverage grammars and the lack of adequate lex-
icons. In fact, the difficulties encountered in applying
computational linguistics technologies to text pro-
cessing have contributed to a wide-spread belief that
</bodyText>
<page confidence="0.995516">
9
</page>
<bodyText confidence="0.984663901098901">
automated natural language processing may not be
suitable in IR. These difficulties included
inefficiency, lack of robustness, and prohibitive cost
of manual effort required to build lexicons and
knowledge bases for each new text domain. On the
other hand, while numerous experiments did not
establish the usefulness of linguistic methods in IR,
they cannot be considered conclusive because of their
limited scale. 1
The rapid progress in Computational Linguis-
tics over the last few years has changed this equation
in various ways. First of all, large-scale resources
became available: on-line lexicons, including Oxford
Advanced Learner&apos;s Dictionary (OALD), Longman
Dictionary of Contemporary English (LDOCE),
Webster&apos;s Dictionary, Oxford English Dictionary,
Collins Dictionary, and others, as well as large text
corpora, many of which can now be obtained for
research purposes. Robust text-oriented software
tools have been built, including part of speech
taggers (stochastic and otherwise), and fast parsers
capable of processing text at speeds of 4200 words
per minute or more (e.g., TTP parser developed by
the author). While many of the fast parsers are not
very accurate (they are usually partial analyzers by
design),2 some, like &apos;ITP, perform in fact no worse
than standard full-analysis parsers which are many
times slower and far less robust. 3
An accurate syntactic analysis is an essential
prerequisite for term selection, but it is by no means
sufficient. Syntactic parsing of the database contents
is usually attempted in order to extract linguistically
motivated phrases, which presumably are better indi-
cators of contents than &amp;quot;statistical phrases&amp;quot; where
words are grouped solely on the basis of physical
proximity (e.g., &amp;quot;college junior&amp;quot; is not the same as
&amp;quot;junior college&amp;quot;). However, creation of such com-
pound terms makes term matching process more
complex since in addition to the usual problems of
synonymy and subsumption, one must deal with their
structure (e.g., &amp;quot;college junior&amp;quot; is the same as &amp;quot;junior
in college&amp;quot;). In order to deal with structure, parser&apos;s
StandardIR benciunark collections are statistically too
small and the experiments can easily produce counterintuitive
results. For example, Cranfield collection is only approx. 180,000
English words, while CACM-3204 collection is approx. 200,003
words.
2 Partial parsing is usually fast enough, but it also generates
noisy data: as many as SO% of all genenued phrases could be in-
correct (Lewis and Croft, 1990).
TTP has been shown to produce parse structures which are
no worse in recall, precision and crossing rate than those generated
by full-scale linguistic parsers when compared to band-coded
Treebank parse trees.
output needs to be &amp;quot;normalized&amp;quot; or &amp;quot;regularized&amp;quot; so
that complex terms with the same or closely related
meanings would indeed receive matching representa-
tions. This goal has been achieved to a certain extent
in the present work. As it will be discussed in more
detail below, indexing terms were selected from
among head-modifier pairs extracted from predicate-
argument representations of sentences.
The next important task is to achieve normali-
zation across diferent terms with close or related
meaning. This can be accomplished by discovering
various semantic relationships among words and
phrases, such as synonymy and subsumption. For
example, the term natural language can be con-
sidered, in certain domains at least; to subsume any
term denoting a specific human language. such as
English. Therefore, a query containing the former
may be expected to retrieve documents containing
the latter. The system presented here computes term
associations from text on word and fixed phrase level
and then uses these associations in query expansion.
A fairly primitive filter is employed to separate
synonymy and subsumption relationships from others
including antonymy and complementation, some of
which are strongly domain-dependent. This process
has led to an increased retrieval precision in experi-
ments with smaller and more cohesive collections
(CACM-3204).
In the following sections we present an over-
view of our system, with the emphasis on its text-
processing components. We would like to point out
here that the system is completely automated, i.e., all
the processing steps, those performed by the statisti-
cal core, and these performed by the natural language
processing components, are done automatically, and
no human intervention or manual encoding is
required.
</bodyText>
<sectionHeader confidence="0.937586" genericHeader="method">
OVERALL DESIGN
</sectionHeader>
<bodyText confidence="0.999897666666666">
Our information retrieval system consists of a
traditional statistical backbone (NIST&apos;s PRISE sys-
tem; Harman and Candela, 1989) augmented with
various natural language processing components that
assist the system in database processing (stemming,
indexing, word and phrase clustering, selectional res-
trictions), and translate a user&apos;s information request
into an effective query. This design is a careful
compromise between purely statistical non-linguistic
approaches and those requiring rather accomplished
(and expensive) semantic analysis of data, often
referred to as &apos;conceptual retrieval&apos;.
In our system the database text is first pro-
cessed with a fast syntactic parser. Subsequently cer-
tain types of phrases are extracted from the parse
</bodyText>
<page confidence="0.996951">
10
</page>
<bodyText confidence="0.999959387096774">
trees and used as compound indexing terms in addi-
tion to single-word terms. The extracted phrases are
statistically analyzed as syntactic contexts in order to
discover a variety of similarity links between smaller
subphrases and words occurring in them. A further
filtering process maps these similarity links onto
semantic relations (generalization, specialization,
synonymy, etc.) after which they are used to
transform user&apos;s request into a search query.
The user&apos;s natural language request is also
parsed, and all indexing terms occurring in them are
identified. Certain highly ambiguous, usually single-
word terms may be dropped, provided that they also
occur as elements in some compound terms. At the
same time, other terms may be added, namely those
which are linked to some query term through admis-
sible similarity relations. For example, &amp;quot;unlawful
activity&amp;quot; is added to a query containing the com-
pound term &amp;quot;illegal activity&amp;quot; via a synonymy link
between &amp;quot;illegal&amp;quot; and &amp;quot;unlawful&amp;quot;. After the final
query is constructed, the database search follows, and
a ranked list of documents is returned.
The purpose of this elaborate linguistic pro-
cessing is to create a better representation of docu-
ments and to generate best possible queries out of
user&apos;s initial requests. Despite limitations of term-
and-weight type representation (or boolean versions
thereof), very good queries can be produced by
human experts. In order to imitate an expert, the sys-
tem must be able to learn about its database, in par-
ticular about various correlations among index terms.
</bodyText>
<sectionHeader confidence="0.82363" genericHeader="method">
FAST PARSING WITH TTP PARSER
</sectionHeader>
<bodyText confidence="0.992192986111112">
TIP (Tagged Text Parser) is based on the
Linguistic String Grammar developed by Sager
(1981). The parser currently encompasses some 400
grammar productions, but it is by no means complete.
The parser&apos;s output is a regularized parse tree
representation of each sentence, that is, a representa-
tion that reflects the sentence&apos;s logical predicate-
argument structure. For example, logical subject and
logical object are identified in both passive and active
sentences, and noun phrases are organized around
their head elements. The significance of this
representation will be discussed below. The parser is
equipped with a powerful skip-and-fit recovery
mechanism that allows it to operate effectively in the
face of ill-formed input or under a severe time pres-
sure. In the runs with approximately 83 million words
of TREC&apos;s Wall Street Journal texts ,4 the parser&apos;s
Appmximately 0.5 GBytes of text, over 4 million sen-
tences.
speed averaged between 0.3 and 0.5 seconds per sen-
tence, or up to 4200 words per minute. on a Sun&apos;s
SparcStation-2.
TTP is a full grammar parser. and initially, it
attempts to generate a complete analysis for each
sentence. However, unlike an ordinary parser, it has a
built-in timer which regulates the amount of time
allowed for parsing any one sentence. If a parse is not
returned before the allotted time elapses, the parser
enters the skip-and-fit mode in which it will try to
&amp;quot;fit&amp;quot; the parse. While in the skip-and-fit mode, the
parser will attempt to forcibly reduce incomplete
constituents, possibly skipping portions of input in
order to restart processing at a next unattempted con-
stituent. In other words, the parser will favor reduc-
tion to backtracking while in the skip-and-fit mode.
The result of this strategy is an approximate parse.
partially fitted using top-down predictions. The frag-
ments skipped in the first pass are not thrown out,
instead they are analyzed by a simple phrasal parser
that looks for noun phrases and relative clauses and
then attaches the recovered material to the main parse
structure. As an illustration, consider the following
sentence taken from the CACM-3204 corpus:
The method is illustrated by the automatic con-
struction of both recursive and iterative pro-
grams operating on natural numbers, lists, and
trees, in order to construct a program satisfying
certain specifications a theorem induced by
those specifications is proved, and the desired
program is extracted from the proof.
The italicized fragment is likely to cause additional
complications in parsing this lengthy string, and the
parser may be better off ignoring this fragment alto-
gether. To do so successfully, the parser must close
the currently open constituent (i.e., reduce a program
satisfying certain specifications to NP), and possibly
a few of its parent constituents, removing
corresponding productions from further considera-
tion, until an appropriate production is reactivated.
In this case, TTP may force the following reductions:
51 —) to V NP; SA -4 SI; S —&gt; NP V NP SA, until the
production S S and S is reached. Next, the parser
skips input to find and, and resumes normal process-
ing.
As may be expected, the skip-and-fit strategy
will only be effective if the input skipping can be per-
formed with a degree of determinism. This means
that most of the lexical level ambiguity must be
removed from the input text, prior to parsing. We
achieve this using a stochastic parts of speech tagger
to preprocess the text. Full details of the parser can
be found in (Strzallcowski, 1992).
</bodyText>
<page confidence="0.997844">
11
</page>
<sectionHeader confidence="0.798973" genericHeader="method">
PART OF SPEECH TAGGER
</sectionHeader>
<bodyText confidence="0.999923666666667">
One way of dealing with lexical ambiguity is to
use a tagger to preprocess the input marking each
word with a tag that indicates its syntactic categoriza-
tion: a part of speech with selected morphological
features such as number, tense, mode, case and
degree. The following are tagged sentences from the
</bodyText>
<equation confidence="0.637664">
CACM-3204 collection: 5
</equation>
<bodyText confidence="0.96912578">
The/dr paper/nn presents/vbz a/dr proposal/nn
for/in structured/vbn representation/nn of/in
multiprogramming/vbg in/in aid: highlij level/nn
language/nn ./per
The/dr notation/nn used/vbn explicitly/rb
associates/vbz data/nns structure/nn
shared/vbn by/in concurrent/f] processes/nns
with/in operations/nns defined/vbn onlin it/pp
./per
The tags are understood as follows: dt - determiner,
nn - singular noun, nns - plural noun, in - preposition,
jj - adjective, vbz - verb in present tense third person
singular, to - particle &amp;quot;to&amp;quot;, vbg - present participle,
vbn - past participle, vbd - past tense verb, vb -
infinitive verb, cc - coordinate conjunction.
Tagging of the input text substantially reduces
the search space of a top-down parser since it
resolves most of the lexical level ambiguities. In the
examples above, tagging of presents as &amp;quot;vbz&amp;quot; in the
first sentence cuts off a potentially long and costly
&amp;quot;garden path&amp;quot; with presents as a plural noun followed
by a headless relative clause starting with (that) a
proposal .... In the second sentence, tagging resolves
ambiguity of used (vbn vs. vbd), and associates (vbz
vs. nns). Perhaps more importantly, elimination of
word-level lexical ambiguity allows the parser to
make projection about the input which is yet to be
parsed, using a simple loolcahead; in particular,
phrase boundaries can be determined with a degree
of confidence (Church, 1988). This latter property is
critical for implementing skip-and-fit recovery tech-
nique outlined in the previous section.
Tagging of input also helps to reduce the
number of parse structures that can be assigned to a
sentence, decreases the demand for consulting of the
dictionary, and simplifies dealing with unknown
words. Since every item in the sentence is assigned a
tag. so are the words for which we have no entry in
the lexicon. Many of these words will be tagged as
&amp;quot;np&amp;quot; (proper noun), however, the surrounding tags
may force other selections. In the following exam-
ple, chinese, which does not appear in the dictionary,
Tagged using the 35-tag Penn Treehank Tagset created at
the University of Pennsylvania.
is tagged as
this/di paper/nn dates/vbz back/rb theldi
genesis/nn of/in binary/jj conception/nn circa/in
5000/cd years/nns ago/rb ,/corn as/rb
derived/vbn by/in the/di chinese/jj ancients/nns
./per
</bodyText>
<sectionHeader confidence="0.939115" genericHeader="method">
WORD SUFFIX TRIMMER
</sectionHeader>
<bodyText confidence="0.985274886363636">
Word stemming has been an effective way of
improving document recall since it reduces words to
their common morphological root, thus allowing
more successful matches. On the other hand, stem-
ming tends to decrease retrieval precision, if care is
not taken to prevent situations where otherwise unre-
lated words are reduced to the same stem. In our sys-
tem we replaced a traditional morphological stemmer
with a conservative dictionary-assisted suffix trim-
mer. 7 The suffix trimmer performs essentially two
tasks: (1) it reduces inflected word forms to their root
forms as specified in the dictionary, and (2) it con-
verts nominalized verb forms (e.g., &amp;quot;implementa-
tion&amp;quot;, &amp;quot;storage&amp;quot;) to the root forms of corresponding
verbs (i.e.. &amp;quot;implement&amp;quot;, &amp;quot;store&amp;quot;). This is accom-
plished by removing a standard suffix. e.g..
&amp;quot;stor+age&amp;quot;, replacing it with a standard root ending
(&amp;quot;+e&amp;quot;), and checking the newly created word against
the dictionary, i.e., we check whether the new root
(&amp;quot;store&amp;quot;) is indeed a legal word, and whether the ori-
ginal root (&amp;quot;storage&amp;quot;) is defined using the new root
(&amp;quot;store&amp;quot;) or one of its standard inflectional forms
(e.g., &amp;quot;storing&amp;quot;). For example, the following
definitions are excerpted from the Oxford Advanced
Learner&apos;s Dictionary (OALD):
storage n RI] (space used for, money paid for)
the storing of goods ...
diversion n [U] diverting ...
procession n IC] number of persons, vehicles,
etc moving forward and following each other in
an orderly way.
Therefore, we can reduce &amp;quot;diversion&amp;quot; to &amp;quot;divert&amp;quot; by
removing the suffix &amp;quot;+sion&amp;quot; and adding root form
suffix &amp;quot;+t&amp;quot;. On the other hand, &amp;quot;process+ion&amp;quot; is not
reduced to &amp;quot;process&amp;quot;.
Earlier experiments with CACM-3204 collec-
tion showed an improvement in retrieval precision by
6% to 8% over the base system equipped with a stan-
dard morphological stemmer (the SMART stemmer).
We use the machine readable version of the Oxford Ad-
vanced Learner&apos;s Dictionary (OALD).
Dealing with prefixes is a more complicated matter, since
they may have quite strong effect upon the meaning of the result-
ing tenn, e.g., un- usually introduces explicit negation.
</bodyText>
<page confidence="0.995652">
12
</page>
<sectionHeader confidence="0.923024" genericHeader="method">
HEAD-MODIFIER STRUCTURES
</sectionHeader>
<bodyText confidence="0.999644608695652">
Syntactic phrases extracted from TTP parse
trees are head-modifier pairs. The head in such a pair
is a central element of a phrase (main verb, main
noun, etc.), while the modifier is one of the adjunct
arguments of the head. In the TREC experiments
reported here we extracted head-modifier word and
fixed-phrase pairs only. While TREC WSJ database
is large enough to warrant generation of larger com-
pounds, we were in no position to verify their effec-
tiveness in indexing. This was largely because of the
tight schedule, but also because of rapidly escalating
complexity of the indexing process: even with 2-
word phrases, compound terms accounted for nearly
96% of all index entries, in other words, including 2-
word phrases has increased the index size 25 times!
Let us consider a specific example from WSJ
database:
The former Soviet president has been a local
hero ever since a Russian tank invaded Wiscon-
sin.
The tagged sentence is given below, followed by the
regularized parse structure generated by TTP, given
in Figure 1.
</bodyText>
<table confidence="0.52416925">
The/di former/j/ Soviet/jj president/nn haslvbz
beenlvbn aid! local/jj hero/nn ever/rb since/in
aid: Russian/jj tank/nn invaded/vbd
Wisconsin/np ./per
</table>
<bodyText confidence="0.998409444444445">
It should be noted that the parser&apos;s output is a
predicate-argument structure centered around main
elements of various phrases. In Figure 1, BE is the
main predicate (modified by HAVE) with 2 argu-
ments (subject, object) and 2 adjuncts (adv, sub_ord).
INVADE is the predicate in the subordinate clause
with 2 arguments (subject, object). The subject of
BE is a noun phrase with PRESIDENT as the head
element, two modifiers (FORMER, SOVIET) and a
determiner (THE). From this structure, we extract
head-modifier pairs that become candidates for com-
pound terms. The following types of pairs are con-
sidered: (1) a head noun and its left adjective or noun
adjunct, (2) a head noun and the head of its right
adjunct, (3) the main verb of a clause and the head of
its object phrase, and (4) the head of the subject
phrase and the main verb. These types of pairs
account for most of the syntactic variants for relating
two words (or simple phrases) into pairs carrying
compatible semantic content. For example, the pair
retrieve+information will be extracted from any of
the following fragments: information retrieval sys-
tem; retrieval of information from databases; and
information that can be retrieved by a user-
controlled interactive search process. In the example
at hand, the following head-modifier pairs are
extracted (pairs containing low-contents elements,
</bodyText>
<figure confidence="0.997007846153846">
&apos;assert
Rperf [HAVE]]
[(verb [BE))
&apos;subject
Inp
In PRESIDENT]
ft_pos THE)
[adj [FORMER])
ladj [SOVIET])))
[object
Inp
In HERO)
It_pos A)
ladj [LOCALIIII
lady EVER)
Isub_ord
[SINCE
[(verb [INVADE]]
[subject
Inp
In TANK]
[t_pos A]
ladj [RUSSIAN]]]]
[object
Inp
[name [WISCONSINIIIIIIII))
</figure>
<figureCaption confidence="0.999985">
Figure 1. Predicate-argument parse structure.
</figureCaption>
<bodyText confidence="0.998694">
such as BE and FORMER, or names, such as
WISCONSIN, will be later discarded):
</bodyText>
<figure confidence="0.991386875">
[PRES1DENT.BE]
[PRESIDENT,FORMER]
[PRESIDENT,SOVIET)
[BE,HEROI
[HERO,LOCAL]
ITANIONVADEJ
[TANK.RUSSIAN]
fINVADE,WISCONSIN]
</figure>
<bodyText confidence="0.999039111111111">
We may note that the three-word phrase former
Soviet president has been broken into two pairs
former president and Soviet president, both of which
denote things that are potentially quite different from
what the original phrase refers to, and this fact may
have potentially negative effect on retrieval preci-
sion. This is one place where a longer phrase appears
more appropriate. The representation of this sentence
may therefore contain the following terms:
</bodyText>
<figureCaption confidence="0.38169025">
PRESIDENT. SOVIET, PRESIDENT+SOVIET.
PRESIDENT+FORMER. HERO. HERO+LOCAL,
INVADE. TANK. TANK+INVADE, TANK+RUSS1AN.
RUSSIAN. INVADE+W1SCONSK WISCONSIN.
</figureCaption>
<bodyText confidence="0.999298">
The particular way of interpreting syntactic
contexts was dictated, to some degree at least, by sta-
tistical considerations. Our original experiments
</bodyText>
<page confidence="0.995636">
13
</page>
<bodyText confidence="0.999989255813953">
were performed on a relatively small collection
(CACM-3204), and therefore we combined pairs
obtained from different syntactic relations (e.g.,
verb-object, subject-verb, noun-adjunct, etc.) in order
to increase frequencies of some associations. This
became largely unnecessary in a large collection such
as TIPSTER, but we had no means to test alternative
options, and thus decided to stay with the original. It
should not be difficult to see that this was a
compromise solution, since many important distinc-
tions were potentially lost, and strong associations
could be produced where there weren&apos;t any. A way to
improve things is to consider different syntactic rela-
tions independently, perhaps as independent sources
of evidence that could lend support (or not) to certain
term similarity predictions. We have already started
testing this option.
One difficulty in obtaining head-modifier pairs
of highest accuracy is the notorious ambiguity of
nominal compounds. For example, the phrase natural
language processing should generate
language+natural and processing+language, while
dynamic information processing is expected to yield
processing+dynamic and processing+information. A
still another case is executive vice president where
the association president+executive may be stretch-
ing things a bit too far. Since our parser has no
knowledge about the text domain, and uses no
semantic preferences, it does not attempt to guess any
internal associations within such phrases. Instead,
this task is passed to the pair extractor module which
processes ambiguous parse structures in two phases.
In phase one, all and only unambiguous head-
modifier pairs are extracted, and the frequencies of
their occurrences are recorded. In phase two, fre-
quency information about pairs generated in the first
pass is used to form associations from ambiguous
structures. For example, if language+natural has
occurred unambiguously a number times in contexts
such as parser for natural language, while
processing+natural has occurred significantly fewer
times or perhaps none at all, then we will prefer the
former association as valid.
</bodyText>
<sectionHeader confidence="0.998096" genericHeader="method">
TERM CORRELATIONS FROM TEXT
</sectionHeader>
<bodyText confidence="0.957412333333334">
Head-modifier pairs form compound terms
used in database indexing. They also serve as
occurrence contexts for smaller terms, including
single-word terms. If two terms tend to be modified
with a number of common modifiers and otherwise
appear in few distinct contexts, we assign them a
similarity coefficient, a real number between 0 and 1.
The similarity is determined by comparing distribu-
tion characteristics for both terms within the corpus:
how much information contents do they carry, do
their information contribution over contexts vary
greatly, are the common contexts in which these
terms occur specific enough? In general we will
credit high-contents terms appearing in identical con-
texts, especially if these contexts are not too com-
monplace.8 The relative similarity between two
words x1 and x2 can be obtained using the following
formula (a is a large constant): 9
,VM(XI,X2)= log (a I simy(x ,x2))
where
simy(x ,x2) = MIN (IC (x &apos;,[x 1.Y ]),IC 2,[x 2.y
* MIN (IC [x •Y])•IC Ex 2.y D)
and IC is the Information Contribution measure indi-
cating the strength of word pairings, and defined as
</bodyText>
<equation confidence="0.811231">
fx.y
(x,[x,y1)=
ni+dx-1
</equation>
<bodyText confidence="0.999940384615385">
where f,y is the absolute frequency of pair Ex,y] in
the corpus, nx is the frequency of term x at the head
position, and di is a dispersion parameter understood
as the number of distinct syntactic contexts in which
term x is found. The similarity function is further
normalized with respect to SIM (x 1,x 1). Example
similarities are listed in Table I.
We also considered a term clustering option
which, unlike the similarity formula above, produces
clusters of related words and phrases, but will not
generate uniform term similarity ranking across clus-
ters. We used a variant of weighted Tanimoto&apos;s
measure described in (Grefenstette, 1992):
</bodyText>
<equation confidence="0.968005615384615">
ZMIN (W ([x,att]),W ([y,att ])
an
SIM (xi,x2)=
IMAX (W ax,att)),W ay,attp
an
with
W ([x,y]) = GW (x)*log (fx,y)
[
,
GW (x)= 1 –1 ny
— * log[
fx.y .f.i.y
nr
</equation>
<bodyText confidence="0.956216333333333">
It would not be appropriate to predict similarity between
language and logarithm on the basis of their co-occurrence with
&apos;This was inspired by a formula used by Hindle (1990).
</bodyText>
<equation confidence="0.872576">
log (N)
</equation>
<page confidence="0.986391">
14
</page>
<bodyText confidence="0.9981435">
Sample clusters obtained from approx. 100 MByte
(17 million words) sample of WSJ are given in Table
2.
In order to generate better similarities and clus-
ters, we require that words x1 and x2 appear in at
least M distinct common contexts, where a common
context is a couple of pairs [x 1,y] and [x2,y], or
[y.xl] and [y,x2] such that they each occurred at least
twice. Thus, banana and Baltic will not be con-
sidered for similarity relation on the basis of their
occurrences in the common context of republic, no
matter how frequent, unless there is another such
common context comparably frequent (there wasn&apos;t
any in TREC WSJ database). For smaller or narrow
domain databases M=2 is usually sufficient. For large
databases covering rather diverse subject matter, like
TIPSTER or even WSJ, we used M&gt;3.I°
It may be worth pointing out that the similari-
ties are calculated using term co-occurrences in syn-
tactic rather than in document-size contexts, the latter
being the usual practice in non-linguistic clustering
(e.g., Sparck Jones and Barber, 1971; Crouch, 1988;
Lewis and Croft, 1990). Although the two methods of
term clustering may be considered mutually comple-
mentary in certain situations, we believe that more
and stronger associations can be obtained through
syntactic-context clustering, given sufficient amount
of data and a reasonably accurate syntactic parser.&amp;quot;
</bodyText>
<sectionHeader confidence="0.823759" genericHeader="method">
QUERY EXPANSION
</sectionHeader>
<bodyText confidence="0.997256101694915">
Similarity relations are used to expand user
queries with new terms, in an attempt to make the
final search query more comprehensive (adding
synonyms) and/or more pointed (adding specializa-
tions).12 It follows that not all similarity relations will
be equally useful in query expansion, for instance,
complementary and antonymous relations like the
I° For example banana and Dominican were found to have
two common contexts: republic and plant, although this second oc-
curred in apparently different senses in Dominican plant and bana-
na plant.
&apos;I Non-syntactic contexts cross sentence boundaries with no
fuss, which is helpful with short, succinct documents (such as
CACM abstracts), but less so with longer texts; see also (Grishman
et al.. 1986).
12 Query expansion (in the sense considered here, though not
quite in the same way) has been used in information retrieval
research before (e.g., Sparck Jones and Tait, 1984; Harman, 1988),
usually with mixed results. An alternative is to use tenn clusters to
create new terms, &amp;quot;metaterms&amp;quot;, and use them to index the database
instead (e.g.. Crouch. 1988; Lewis and Croft, 1990). We found that
the query expansion approach gives the system more flexibility, for
instance, by making room for hypertext-style topic exploration via
user feedback.
one between Australian and Canadian, or accept and
reject may actually harm system&apos;s performance.
since we may end up retrieving many irrelevant
documents. Similarly, the effectiveness of a query
containing vitamin is likely to diminish if we add a
similar but far more general term such as acid. On
the other hand, database search is likely to miss
relevant documents if we overlook the fact that for-
tran is a programming language, or that infant is a
baby and baby is a child. We noted that an average
set of similarities generated from a text corpus con-
tains about as many &amp;quot;good&amp;quot; relations (synonymy.
specialization) as &amp;quot;bad&amp;quot; relations (antonymy. comple-
mentation, generalization), as seen from the query
expansion viewpoint. Therefore any attempt to
separate these two classes and to increase the propor-
tion of &amp;quot;good&amp;quot; relations should result in improved
retrieval. This has indeed been confirmed in our
experiments where a relatively crude filter has visibly
increased retrieval precision.
In order to create an appropriate filter, we dev-
ised a global term specificity measure (GTS) which is
calculated for each term across all contexts in which
it occurs. The general philosophy here is that a more
specific word/phrase would have a more limited use,
i.e., a more specific term would appear in fewer dis-
tinct contexts. In this respect. GTS is similar to the
standard inverted document frequency (idf) measure
except that term frequency is measured over syntactic
units rather than document size units.13 Terms with
higher GTS values are generally considered more
specific, but the specificity comparison is only mean-
ingful for terms which are already known to be simi-
lar. The new function is calculated according to the
following formula:
</bodyText>
<equation confidence="0.871172142857143">
* /CR(w) if both exist
GTS (w)= !Claw) if only ICN(w) exists
0 otherwise
where (with nw, d, &gt; 0):
ICL(w)= ([w,_]) — di.(n+4-1)
n,„
ICN(w)= IC ([_,w]) —
</equation>
<bodyText confidence="0.800544428571429">
For any two terms wi and w2, and a constant 6&gt; 1,
if GTS (w 2) ?. 8 * GTS (w 1) then w2 is considered
more specific than w1. In addition, if
n We believe that measuring term specificity over
document-size contexts (e.g.. Sparck Jones, 1972) may not be ap-
propriate in this case. In particular, syntax-based contexts allow for
processing texts without any internal document structure.
</bodyText>
<page confidence="0.984986">
15
</page>
<bodyText confidence="0.930214">
SIM.,„.(w ,w2)= o &gt; 0,
where 0 is an empirically established threshold, then
w2 can be added to the query containing term w
with weight 0.14 For example, the following were
obtained from TREC WSJ training database:
</bodyText>
<equation confidence="0.995738714285714">
GTS (child) = 0.000001
GTS (baby) = 0.000013
GTS (infant) = 0.000055
with
SIM (child,infant) = 0.131381
SIM (baby,child) =0.183064
SIM (baby,infant) = 0.323121
</equation>
<bodyText confidence="0.999559833333333">
Therefore both baby and infant can be used to spe-
cialize child. With this filter, the relationship between
baby and infant had to be discarded, as we are unable
to tell synonymous or near synonymous relationships
from those which are primarily complementary, e.g.,
man and woman.
</bodyText>
<sectionHeader confidence="0.986872" genericHeader="method">
SUMMARY OF RESULTS
</sectionHeader>
<bodyText confidence="0.997711428571429">
We have processed the total of 500 MBytes of
articles from Wall Street Journal section of TREC
database. Retrieval experiments involved 50 user
information requests (topics) (TREC topics 51-100)
consisting of several fields that included both text and
user supplied keywords. A typical topic is shown
below:
</bodyText>
<table confidence="0.553492">
&lt;top&gt;
&lt;head&gt; Tipster Topic Description
&lt;num&gt; Number 059
&lt;dom&gt; Domain: Environment
&lt;title&gt; Topic: Weather Related Fatalities
&lt;desc&gt; Description:
</table>
<subsectionHeader confidence="0.646569">
Document will report a type of weather event which has
directly caused at least one fatality in some location.
</subsectionHeader>
<bodyText confidence="0.798705083333333">
&lt;narr&gt; Narrative:
A relevant document will include the number of people
killed and injured by the weather event, as well as
reporting the type of weather event and the location
of the event.
&lt;con&gt; Concept(s):
&amp;quot; For CACM-3204 collection the filter was most effective at
= 0.57. For 1REC-1 we changed the similarity formula slightly
in order to obtain correct normalizations in all cases. This however
lowered similarity coefficients in general and a new threshold had
to be selected. We used a = 0.1 in TREC-1 runs, although it turned
out to be a poor choice. In all cases b varied between 10 and 100.
</bodyText>
<reference confidence="0.821897857142857">
1. lightning, avalanche, tornado, typhoon, hurricane.
heat, heat wave, flood, snow, rain, downpour,
blizzard, storm, freezing temperatures
2. dead, killed, fatal, death, fatality, victim
3. NOT man-made disasters. NOT war-induced famine
4. NOT earthquakes. NOT volcanic eruptions
&lt;/top&gt;
</reference>
<bodyText confidence="0.99866644680851">
Note that this topic actually consists of two different
statements of the same query: the natural language
specification consisting of &lt;desc&gt; and &lt;narr&gt; fields.
and an expert-selected list of key terms which are
often far more informative than the narrative part.
Results obtained for queries using text fields only and
those involving both text and keyword fields are
reported separately. Further experiments have sug-
gested that natural language processing impact is
significant but may be severely limited by the expres-
siveness of the term-based representation. Since the
&lt;con&gt; field is considered the expert-user&apos;s rendering
of the &apos;optimal&apos; search query, our system is able to
discover much of it from a less complete
specification in the text section of the request via
query expansion. In fact, we noted that the
recall/precision gap between automatically generated
queries and those supplied by the user was largely
closed when NLP was used. Moreover, even with the
keyword field included in the query along with other
fields, NLP&apos;s impact on the system&apos;s performance is
still noticeable.
Other results on the impact of different fields in
TREC topics on the final recall/precision results were
reported by Broglio and Croft (1993) at the ARPA
HLT workshop, although text-only runs were not
included. One of the most striking observations they
have made is that the narrative field is entirely
disposable, and moreover that its inclusion in the
query actually hurts the system&apos;s performance. It has
to be pointed out, however, that they do little
language processing.15
Summary statistics for these runs are shown in
Table 4. These results are fairly tentative and should
be regarded with some caution. For one, the column
named txt reports performance of &lt;desc&gt; and &lt;narr&gt;
fields which have been processed with our suffix-
trimmer. This means some Ni.? has been done
already (tagging + lexicon), and therefore what we
see there is not the performance of &apos;pure&apos; statistical
system. The same applies to con column. (For
&amp;quot; Bruce Croft (personal communication. 1992) has suggest-
ed that excluding all expert-made fields (i.e.. &lt;con&gt; and &lt;fac&gt;)
would make the queries quite ineffective. Broglio (personal com-
munication, 1993) confirms this showing that text-only retrieval
(i.e.. with &lt;desc&gt; and •marr&gt;) shows an average precision at more
than 30% below that of &lt;con&gt;-based retrieval.
</bodyText>
<page confidence="0.995778">
16
</page>
<table confidence="0.999775666666667">
wordl word2 SIMnorm
abm *anti+ballistic 0.534894
absence *maternity 0.233082
accept acquire 0.179078
accord pact 0.492332
acquire purchase 0.449362
speech address 0.263789
adjustable one+year 0.824053
maxsaver *advance+purchase 0.734008
affair scandal 0.684877
affordable low+income 0.181795
disease *ailment 0.247382
medium+range *air+to+air 0.874508
aircraft *jetliner 0.166777
aircraft plane 0.423831
airline carrier 0.345490
alien immigrate 0.270412
anniversary *bicentennial 0.588210
anti+age anti+wrinkle 0.153918
anti+clot cholesterol+lower 0.856712
contra *anti+sandinista 0.294677
candidate *aspirant 0.116025
contend *aspirant 0.143459
property asset 0.285299
attempt bid 0.641592
await pend 0.572960
stealth *b+1 0.877582
child *baby 0.183064
baggage luggage 0.607333
ban restrict 0.321943
bearish bullish 0.847103
bee *honeybee 0.461023
roller+coast *bumpy 0.898278
two+income two+earner 0.293104
television iv 0.806018
soldier troop 0374410
treasury *short+term 0.661133
research study 0.209257
withdrawal *pullout 0.622558
</table>
<tableCaption confidence="0.946155">
Table 1. Selecte filtered word similarities (* indicates
the more specific term).
</tableCaption>
<table confidence="0.998409102564103">
word cluster
takeover merge, buy-out
acquisition, bid
stock share, issue, bond, price
staff personnel, employee, force
share stock, issue, fund
sensitive crucial, difficult, critical
rumor speculate
president director, executive
chairman, manage
outlook forecast, prospect
trend, picture
law rule, legislate
bill, regulate
earnings revenue, income
I
portfolio asset, invest, loan
property, hold
inflate growth, earnings, rise
industry business, company, market
help additional, support, involve
growth increase, rise, gain
decline, earnings, profit
firm bank, concern, group, unit
environ climate, condition
situation, trend
debt loan, secure, bond
custom(er) client, investor
buyer, consume(r)
counsel attorney
compute machine, software
competitor rival, partner, buyer
company business, firm, bank
market, industry, concern
big large, major, huge
base facile, source
reserve, support
asset property, loan, fund, invest
share, stock, money
</table>
<tableCaption confidence="0.9981815">
Table 2. Selected clusters obtained from approx. 107
words of text with weighted Tanimoto formula.
</tableCaption>
<page confidence="0.998684">
17
</page>
<bodyText confidence="0.999811625">
comparison, see Table 3 where runs with CACM-
3204 collection included &apos;pure&apos; statistics run (base),
and note the impact our suffix trimmer is having.)
Nonetheless, one may notice that automated NLP can
be very effective at discovering the right query from
an imprecise narrative specification: as much as 82%
of the effectiveness of the expert-generated query can
be attained.
</bodyText>
<sectionHeader confidence="0.823075" genericHeader="conclusions">
CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999939366666667">
We presented in some detail a natural language
information retrieval system consisting of an
advanced NLP module and a &apos;pure&apos; statistical core
engine. While many problems remain to be resolved,
including the question of adequacy of term-based
representation of document contents, we attempted to
demonstrate that the architecture described here is
nonetheless viable. In particular, we demonstrated
that natural language processing can now be done on
a fairly large scale and that its speed and robustness
can match those of traditional statistical programs
such as key-word indexing or statistical phrase
extraction. We suggest, with some caution until more
experiments are run, that natural language processing
can be very effective in creating appropriate search
queries out of user&apos;s initial specifications which can
be frequently imprecise or vague.
On the other hand, we must be aware of the
limits of NLP technologies at our disposal. While
part-of-speech tagging, lexicon-based stemming, and
parsing can be done on large amounts of text (hun-
dreds of millions of words and more), other, more
advanced processing involving conceptual structur-
ing, logical forms, etc., is still beyond reach, compu-
tationally. It may be assumed that these super-
advanced techniques will prove even more effective,
since they address the problem of representation-
level limits, however the experimental evidence is
sparse and necessarily limited to rather small scale
tests (e.g., Mauldin, 1991).
</bodyText>
<sectionHeader confidence="0.997843" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.99981625">
We would like to thank Donna Harman of
NIST for making her PRISE system available to us.
We would also like to thank Ralph Weischedel and
Heidi Fox of BBN for providing and assisting in the
use of the part of speech tagger. Jose Perez Carballo
has contributed a number of valuable observations
during the course of this work, and his assistance in
processing the TREC data was critical. This paper is
based upon work supported by the Defense
Advanced Research Project Agency under Contract
N00014-9044851 from the Office of Naval
Research, under Contract N00600-88-D-3717 from
PRC Inc., and the National Science Foundation under
Grant IR1-89-02304. We also acknowledge support
from Canadian Institute for Robotics and Intelligent
Systems (IRIS).
</bodyText>
<sectionHeader confidence="0.99958" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.998608361702128">
Broglio, John and W. Bruce Croft. 1993. &amp;quot;Query
Processing for Retrieval from Large Text Bases.&amp;quot;
Proceedings of ARPA HLT Workshop. March
21-24, Plainsboro, NJ.
Church, Kenneth Ward and Hanks, Patrick. 1990.
&amp;quot;Word association norms, mutual information.
and lexicography.&amp;quot; Computational Linguistics,
16(1), MIT Press. pp. 22-29.
Crouch, Carolyn J. 1988. &amp;quot;A cluster-based approach
to thesaurus construction.&amp;quot; Proceedings of ACM
SIGIR-88, pp. 309-320.
Grefenstette, Gregory. 1992. &amp;quot;Use of Syntactic Con-
text To Produce Term Association Lists for Text
Retrieval.&amp;quot; Proceedings of SIGIR-92,
Copenhagen, Denmark. pp. 89-97.
Grishman, Ralph, Lynette Hirschman, and Ngo T.
Nhan. 1986. &amp;quot;Discovery procedures for sub-
language selectional patterns: initial experi-
ments&amp;quot;. Computational Linguistics, 12(3), pp.
205-215.
Grishman, Ralph and Tomek Strzalkowski. 1991.
&amp;quot;Information Retrieval and Natural Language
Processing.&amp;quot; Position paper at the workshop on
Future Directions in Natural Language Processing
in Information Retrieval, Chicago.
Hannan, Donna. 1988. &amp;quot;Towards interactive query
expansion.&amp;quot; Proceedings of ACM SIGIR-88, pp.
321-331.
Harman, Donna and Gerald Candela. 1989.
&amp;quot;Retrieving Records from a Gigabyte of text on a
Minicomputer Using Statistical Ranking.&amp;quot; Jour-
nal of the American Society for Information Sci-
ence, 41(8), pp. 581-589.
Hindle, Donald. 1990. &amp;quot;Noun classification from
predicate-argument structures.&amp;quot; Proc. 28 Meet-
ing of the ACL. Pittsburgh, PA, pp. 268-275.
Lewis, David D. and W. Bruce Croft. 1990. &amp;quot;Term
Clustering of Syntactic Phrases&amp;quot;. Proceedings of
ACM SIGIR-90, pp. 385-405.
Mauldin, Michael. 1991. &amp;quot;Retrieval Performance in
Ferret A Conceptual Information Retrieval Sys-
tem.&amp;quot; Proceedings of ACM SIGIR-91, pp. 347-
355.
Meteer, Marie, Richard Schwartz. and Ralph
Weischedel. 1991. &amp;quot;Studies in Part of Speech
Labeling.&amp;quot; Proceedings of the 4th DARPA
Speech and Natural Language Workshop.
</reference>
<page confidence="0.985159">
18
</page>
<reference confidence="0.99935525">
Morgan-Kaufman, San Mateo, CA. pp. 331-336.
Sager, Naomi. 1981. Natural Language Information
Processing. Addison-Wesley.
Sparck Jones, Karen. 1972. &amp;quot;Statistical interpreta-
tion of term specificity and its application in
retrieval.&amp;quot; Journal of Documentation, 28(1), pp.
11-20.
Sparck Jones, K. and E. 0. Barber. 1971. &amp;quot;What
makes automatic keyword classification effec-
tive?&amp;quot; Journal of the American Society for Infor-
mation Science, May-June, pp. 166-175.
Sparck Jones, K. and J. I. Tait. 1984. &amp;quot;Automatic
search term variant generation.&amp;quot; Journal of
Documentation, 40(1), pp. 50-66.
Strzallcowski, Tomek and Barbara Vauthey. 1991.
&amp;quot;Fast Text Processing for Information
Retrieval.&amp;quot; Proceedings of the 4th DARPA
Speech and Natural Language Workshop,
Morgan-Kaufman, pp. 346-351.
Strzalkowslci, Tomek and Barbara Vauthey. 1992.
&amp;quot;Information Retrieval Using Robust Natural
Language Processing.&amp;quot; Proc. of the 30th ACL
Meeting, Newark, DE, June-July. pp. 104-111.
Strzallcowski, Tomek. 1992. &amp;quot;TTP: A Fast and
Robust Parser for Natural Language.&amp;quot; Proceed-
ings of the 14th International Conference on
Computational Linguistics (COLING), Names,
France, July 1992. pp. 198-204.
</reference>
<table confidence="0.998971466666667">
Runs base I suff trim I query exp.
Recall Precision Averages
0.00 0.764 0.775 0.793
0.10 0.674 0.688 0.700
0.20 0.547 0.547 0.573
0.30 0.449 0.479 0.486
0.40 0.387 0.421 0.421
0.50 0329 0.356 0372
0.60 0.273 0.280 0.304
0.70 0.198 0.222 0.226
0.80 0.146 0.170 0.174
0.90 0.093 0.112 0.114
1.00 0.079 0.087 0.090
3-pt Avg. 0328 0.356 0371
%chg +8.3 +13.1
</table>
<tableCaption confidence="0.961479333333333">
Table 3. Run statistics for CACM-3204 da-
tabase: with no NLP; with suffix trimmer,
and with both phrases and similarities.
</tableCaption>
<table confidence="0.9988007">
Run tzt tzt+nlp con con+nlp
Queries 50 50 50 50
Tot. number of docs over all queries
Ret 9980 9980 9788 9975
Rel 6228 6228 6228 6228
RelRet 1598 1835 1927 2062
%chg +14.8 +20.6 +29.0
Recall Precision Averages
0.00 0.6420 0.6917 0.7021 0.7539
0.10 0.3727 0.4194 0.4476 0.4848
0.20 0.2476 0.2959 0.3353 0.3641
0.30 0.1543 0.2150 0.2202 0.2674
0.40 0.1093 0.1513 0.1443 0.1735
0.50 0.0611 0.0959 0.0851 0.1001
0.60 0.0298 0.0396 0.0403 0.0665
0.70 0.0160 0.0175 0.0187 0.0103
0.80 0.0046 0.0047 0.0048 0.0024
0.90 0.0000 0.0027 0.0000 0.0010
1.00 0.0000 0.0000 0.0000 0.0010
Average Precisions
11-pt 0.1489 0.1758 0.1817 0.2023
%chg +18.0 +22.0 +35.8
3-pt 0.1044 0.1322 0.1417 0.1555
%chg +26.6 +35.7 +48.9
at 5 0.4360 0.5000 0.4680 0.4800
%chg +14.6 +7.3 +10.0
at 15 0.3453 0.3827 0.3880 0.4107
%chg +10.8 +12.3 +18.9
at 100 0.2108 0.2384 0.2498 0.2712
%chg +13.0 +18.5 +28.6
</table>
<tableCaption confidence="0.995973">
Table 4. Run statistics with TIPSTER WSJ database
</tableCaption>
<figureCaption confidence="0.317179875">
with top 200 documents considered per each query:
(1) txr - with &lt;narr&gt; and &lt;desc&gt; fields only; (2)
txt+nlp - with &lt;narr&gt; and &lt;desc&gt; only including syn-
tactic phrase terms and similarities; (3) con - with
&lt;desc&gt; and &lt;con&gt; fields only; and (4) con+nlp - with
&lt;desc&gt; and &lt;con&gt; fields including phrases and simi-
larities. In all cases documents preprocessed with
lexicon-based suffix-trimmer.
</figureCaption>
<page confidence="0.998502">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.655942">
<title confidence="0.993277">ROBUST TEXT PROCESSING IN AUTOMATED INFORMATION RETRIEVAL</title>
<author confidence="0.990925">Tomek</author>
<affiliation confidence="0.999695">Courant Institute of Mathematical</affiliation>
<address confidence="0.885316666666667">New York 715 Broadway, rm. New York, NY</address>
<email confidence="0.999066">tomek@cs.nyu.edu</email>
<abstract confidence="0.999566965517241">This paper outlines a prototype text retrieval system which uses relatively advanced natural language processing techniques in order to enhance the effectiveness of statistical document retrieval. The backbone of our system is a traditional retrieval engine which builds inverted index files from pre-processed documents, and then searches and ranks the documents in response to user queries. Natural language processing is used to (1) preprocess the documents in order to extract contents-carrying terms, (2) discover interterm dependencies and build a conceptual hierarchy specific to the database domain, and (3) process user&apos;s natural language requests into effective search queries. The basic assumption of this design is that term-based representation of contents is in principle sufficient to build an effective if not optimal search query out of any user&apos;s request. This has been confirmed by an experiment that compared effectiveness of expert-user prepared queries with those derived automatically from an initial narrative information request. In this paper we show that largescale natural language processing (hundreds of millions of words and more) is not only required for a better retrieval, but it is also doable, given appropriate resources. We report on selected preliminary results of experiments with 500 MByte database of Wall Street Journal articles, as well as some earlier results with a smaller document collection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>lightning</author>
</authors>
<title>avalanche, tornado, typhoon, hurricane. heat, heat wave, flood, snow, rain, downpour, blizzard, storm, freezing temperatures 2. dead, killed, fatal, death, fatality, victim 3. NOT man-made disasters. NOT war-induced famine 4. NOT earthquakes. NOT volcanic eruptions</title>
<marker>lightning, </marker>
<rawString>1. lightning, avalanche, tornado, typhoon, hurricane. heat, heat wave, flood, snow, rain, downpour, blizzard, storm, freezing temperatures 2. dead, killed, fatal, death, fatality, victim 3. NOT man-made disasters. NOT war-induced famine 4. NOT earthquakes. NOT volcanic eruptions &lt;/top&gt;</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Broglio</author>
<author>W Bruce Croft</author>
</authors>
<title>Query Processing for Retrieval from Large Text Bases.&amp;quot;</title>
<date>1993</date>
<booktitle>Proceedings of ARPA HLT Workshop.</booktitle>
<location>Plainsboro, NJ.</location>
<marker>Broglio, Croft, 1993</marker>
<rawString>Broglio, John and W. Bruce Croft. 1993. &amp;quot;Query Processing for Retrieval from Large Text Bases.&amp;quot; Proceedings of ARPA HLT Workshop. March 21-24, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information. and lexicography.&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>22--29</pages>
<publisher>MIT Press.</publisher>
<marker>Church, Hanks, 1990</marker>
<rawString>Church, Kenneth Ward and Hanks, Patrick. 1990. &amp;quot;Word association norms, mutual information. and lexicography.&amp;quot; Computational Linguistics, 16(1), MIT Press. pp. 22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn J Crouch</author>
</authors>
<title>A cluster-based approach to thesaurus construction.&amp;quot;</title>
<date>1988</date>
<booktitle>Proceedings of ACM SIGIR-88,</booktitle>
<pages>309--320</pages>
<contexts>
<context position="28700" citStr="Crouch, 1988" startWordPosition="4466" endWordPosition="4467"> of their occurrences in the common context of republic, no matter how frequent, unless there is another such common context comparably frequent (there wasn&apos;t any in TREC WSJ database). For smaller or narrow domain databases M=2 is usually sufficient. For large databases covering rather diverse subject matter, like TIPSTER or even WSJ, we used M&gt;3.I° It may be worth pointing out that the similarities are calculated using term co-occurrences in syntactic rather than in document-size contexts, the latter being the usual practice in non-linguistic clustering (e.g., Sparck Jones and Barber, 1971; Crouch, 1988; Lewis and Croft, 1990). Although the two methods of term clustering may be considered mutually complementary in certain situations, we believe that more and stronger associations can be obtained through syntactic-context clustering, given sufficient amount of data and a reasonably accurate syntactic parser.&amp;quot; QUERY EXPANSION Similarity relations are used to expand user queries with new terms, in an attempt to make the final search query more comprehensive (adding synonyms) and/or more pointed (adding specializations).12 It follows that not all similarity relations will be equally useful in qu</context>
</contexts>
<marker>Crouch, 1988</marker>
<rawString>Crouch, Carolyn J. 1988. &amp;quot;A cluster-based approach to thesaurus construction.&amp;quot; Proceedings of ACM SIGIR-88, pp. 309-320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Use of Syntactic Context To Produce Term Association Lists for Text Retrieval.&amp;quot;</title>
<date>1992</date>
<booktitle>Proceedings of SIGIR-92,</booktitle>
<pages>89--97</pages>
<location>Copenhagen,</location>
<contexts>
<context position="27292" citStr="Grefenstette, 1992" startWordPosition="4224" endWordPosition="4225">te frequency of pair Ex,y] in the corpus, nx is the frequency of term x at the head position, and di is a dispersion parameter understood as the number of distinct syntactic contexts in which term x is found. The similarity function is further normalized with respect to SIM (x 1,x 1). Example similarities are listed in Table I. We also considered a term clustering option which, unlike the similarity formula above, produces clusters of related words and phrases, but will not generate uniform term similarity ranking across clusters. We used a variant of weighted Tanimoto&apos;s measure described in (Grefenstette, 1992): ZMIN (W ([x,att]),W ([y,att ]) an SIM (xi,x2)= IMAX (W ax,att)),W ay,attp an with W ([x,y]) = GW (x)*log (fx,y) [ , GW (x)= 1 –1 ny — * log[ fx.y .f.i.y nr It would not be appropriate to predict similarity between language and logarithm on the basis of their co-occurrence with &apos;This was inspired by a formula used by Hindle (1990). log (N) 14 Sample clusters obtained from approx. 100 MByte (17 million words) sample of WSJ are given in Table 2. In order to generate better similarities and clusters, we require that words x1 and x2 appear in at least M distinct common contexts, where a common co</context>
</contexts>
<marker>Grefenstette, 1992</marker>
<rawString>Grefenstette, Gregory. 1992. &amp;quot;Use of Syntactic Context To Produce Term Association Lists for Text Retrieval.&amp;quot; Proceedings of SIGIR-92, Copenhagen, Denmark. pp. 89-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Lynette Hirschman</author>
<author>Ngo T Nhan</author>
</authors>
<title>Discovery procedures for sublanguage selectional patterns: initial experiments&amp;quot;.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<pages>205--215</pages>
<marker>Grishman, Hirschman, Nhan, 1986</marker>
<rawString>Grishman, Ralph, Lynette Hirschman, and Ngo T. Nhan. 1986. &amp;quot;Discovery procedures for sublanguage selectional patterns: initial experiments&amp;quot;. Computational Linguistics, 12(3), pp. 205-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Tomek Strzalkowski</author>
</authors>
<title>Information Retrieval and Natural Language Processing.&amp;quot; Position paper at the workshop on</title>
<date>1991</date>
<booktitle>Future Directions in Natural Language Processing in Information Retrieval,</booktitle>
<location>Chicago.</location>
<marker>Grishman, Strzalkowski, 1991</marker>
<rawString>Grishman, Ralph and Tomek Strzalkowski. 1991. &amp;quot;Information Retrieval and Natural Language Processing.&amp;quot; Position paper at the workshop on Future Directions in Natural Language Processing in Information Retrieval, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Hannan</author>
</authors>
<title>Towards interactive query expansion.&amp;quot;</title>
<date>1988</date>
<booktitle>Proceedings of ACM SIGIR-88,</booktitle>
<pages>321--331</pages>
<marker>Hannan, 1988</marker>
<rawString>Hannan, Donna. 1988. &amp;quot;Towards interactive query expansion.&amp;quot; Proceedings of ACM SIGIR-88, pp. 321-331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Harman</author>
<author>Gerald Candela</author>
</authors>
<title>Retrieving Records from a Gigabyte of text on a Minicomputer Using Statistical Ranking.&amp;quot;</title>
<date>1989</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>8</issue>
<pages>581--589</pages>
<contexts>
<context position="9001" citStr="Harman and Candela, 1989" startWordPosition="1370" endWordPosition="1373"> in experiments with smaller and more cohesive collections (CACM-3204). In the following sections we present an overview of our system, with the emphasis on its textprocessing components. We would like to point out here that the system is completely automated, i.e., all the processing steps, those performed by the statistical core, and these performed by the natural language processing components, are done automatically, and no human intervention or manual encoding is required. OVERALL DESIGN Our information retrieval system consists of a traditional statistical backbone (NIST&apos;s PRISE system; Harman and Candela, 1989) augmented with various natural language processing components that assist the system in database processing (stemming, indexing, word and phrase clustering, selectional restrictions), and translate a user&apos;s information request into an effective query. This design is a careful compromise between purely statistical non-linguistic approaches and those requiring rather accomplished (and expensive) semantic analysis of data, often referred to as &apos;conceptual retrieval&apos;. In our system the database text is first processed with a fast syntactic parser. Subsequently certain types of phrases are extract</context>
</contexts>
<marker>Harman, Candela, 1989</marker>
<rawString>Harman, Donna and Gerald Candela. 1989. &amp;quot;Retrieving Records from a Gigabyte of text on a Minicomputer Using Statistical Ranking.&amp;quot; Journal of the American Society for Information Science, 41(8), pp. 581-589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.&amp;quot;</title>
<date>1990</date>
<booktitle>Proc. 28 Meeting of the ACL.</booktitle>
<pages>268--275</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="27625" citStr="Hindle (1990)" startWordPosition="4286" endWordPosition="4287">onsidered a term clustering option which, unlike the similarity formula above, produces clusters of related words and phrases, but will not generate uniform term similarity ranking across clusters. We used a variant of weighted Tanimoto&apos;s measure described in (Grefenstette, 1992): ZMIN (W ([x,att]),W ([y,att ]) an SIM (xi,x2)= IMAX (W ax,att)),W ay,attp an with W ([x,y]) = GW (x)*log (fx,y) [ , GW (x)= 1 –1 ny — * log[ fx.y .f.i.y nr It would not be appropriate to predict similarity between language and logarithm on the basis of their co-occurrence with &apos;This was inspired by a formula used by Hindle (1990). log (N) 14 Sample clusters obtained from approx. 100 MByte (17 million words) sample of WSJ are given in Table 2. In order to generate better similarities and clusters, we require that words x1 and x2 appear in at least M distinct common contexts, where a common context is a couple of pairs [x 1,y] and [x2,y], or [y.xl] and [y,x2] such that they each occurred at least twice. Thus, banana and Baltic will not be considered for similarity relation on the basis of their occurrences in the common context of republic, no matter how frequent, unless there is another such common context comparably f</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle, Donald. 1990. &amp;quot;Noun classification from predicate-argument structures.&amp;quot; Proc. 28 Meeting of the ACL. Pittsburgh, PA, pp. 268-275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>W Bruce Croft</author>
</authors>
<title>Term Clustering of Syntactic Phrases&amp;quot;.</title>
<date>1990</date>
<booktitle>Proceedings of ACM SIGIR-90,</booktitle>
<pages>385--405</pages>
<contexts>
<context position="6881" citStr="Lewis and Croft, 1990" startWordPosition="1048" endWordPosition="1051">plex since in addition to the usual problems of synonymy and subsumption, one must deal with their structure (e.g., &amp;quot;college junior&amp;quot; is the same as &amp;quot;junior in college&amp;quot;). In order to deal with structure, parser&apos;s StandardIR benciunark collections are statistically too small and the experiments can easily produce counterintuitive results. For example, Cranfield collection is only approx. 180,000 English words, while CACM-3204 collection is approx. 200,003 words. 2 Partial parsing is usually fast enough, but it also generates noisy data: as many as SO% of all genenued phrases could be incorrect (Lewis and Croft, 1990). TTP has been shown to produce parse structures which are no worse in recall, precision and crossing rate than those generated by full-scale linguistic parsers when compared to band-coded Treebank parse trees. output needs to be &amp;quot;normalized&amp;quot; or &amp;quot;regularized&amp;quot; so that complex terms with the same or closely related meanings would indeed receive matching representations. This goal has been achieved to a certain extent in the present work. As it will be discussed in more detail below, indexing terms were selected from among head-modifier pairs extracted from predicateargument representations of se</context>
<context position="28724" citStr="Lewis and Croft, 1990" startWordPosition="4468" endWordPosition="4471">rrences in the common context of republic, no matter how frequent, unless there is another such common context comparably frequent (there wasn&apos;t any in TREC WSJ database). For smaller or narrow domain databases M=2 is usually sufficient. For large databases covering rather diverse subject matter, like TIPSTER or even WSJ, we used M&gt;3.I° It may be worth pointing out that the similarities are calculated using term co-occurrences in syntactic rather than in document-size contexts, the latter being the usual practice in non-linguistic clustering (e.g., Sparck Jones and Barber, 1971; Crouch, 1988; Lewis and Croft, 1990). Although the two methods of term clustering may be considered mutually complementary in certain situations, we believe that more and stronger associations can be obtained through syntactic-context clustering, given sufficient amount of data and a reasonably accurate syntactic parser.&amp;quot; QUERY EXPANSION Similarity relations are used to expand user queries with new terms, in an attempt to make the final search query more comprehensive (adding synonyms) and/or more pointed (adding specializations).12 It follows that not all similarity relations will be equally useful in query expansion, for insta</context>
<context position="30153" citStr="Lewis and Croft, 1990" startWordPosition="4688" endWordPosition="4691">es in Dominican plant and banana plant. &apos;I Non-syntactic contexts cross sentence boundaries with no fuss, which is helpful with short, succinct documents (such as CACM abstracts), but less so with longer texts; see also (Grishman et al.. 1986). 12 Query expansion (in the sense considered here, though not quite in the same way) has been used in information retrieval research before (e.g., Sparck Jones and Tait, 1984; Harman, 1988), usually with mixed results. An alternative is to use tenn clusters to create new terms, &amp;quot;metaterms&amp;quot;, and use them to index the database instead (e.g.. Crouch. 1988; Lewis and Croft, 1990). We found that the query expansion approach gives the system more flexibility, for instance, by making room for hypertext-style topic exploration via user feedback. one between Australian and Canadian, or accept and reject may actually harm system&apos;s performance. since we may end up retrieving many irrelevant documents. Similarly, the effectiveness of a query containing vitamin is likely to diminish if we add a similar but far more general term such as acid. On the other hand, database search is likely to miss relevant documents if we overlook the fact that fortran is a programming language, o</context>
</contexts>
<marker>Lewis, Croft, 1990</marker>
<rawString>Lewis, David D. and W. Bruce Croft. 1990. &amp;quot;Term Clustering of Syntactic Phrases&amp;quot;. Proceedings of ACM SIGIR-90, pp. 385-405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mauldin</author>
</authors>
<title>Retrieval Performance in Ferret A Conceptual Information Retrieval System.&amp;quot;</title>
<date>1991</date>
<booktitle>Proceedings of ACM SIGIR-91,</booktitle>
<pages>347--355</pages>
<marker>Mauldin, 1991</marker>
<rawString>Mauldin, Michael. 1991. &amp;quot;Retrieval Performance in Ferret A Conceptual Information Retrieval System.&amp;quot; Proceedings of ACM SIGIR-91, pp. 347-355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
</authors>
<title>Studies in Part of Speech Labeling.&amp;quot;</title>
<date>1991</date>
<booktitle>Proceedings of the 4th DARPA Speech and Natural Language Workshop. Morgan-Kaufman,</booktitle>
<pages>331--336</pages>
<location>San Mateo, CA.</location>
<marker>Weischedel, 1991</marker>
<rawString>Meteer, Marie, Richard Schwartz. and Ralph Weischedel. 1991. &amp;quot;Studies in Part of Speech Labeling.&amp;quot; Proceedings of the 4th DARPA Speech and Natural Language Workshop. Morgan-Kaufman, San Mateo, CA. pp. 331-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Sager</author>
</authors>
<title>Natural Language Information Processing.</title>
<date>1981</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="11300" citStr="Sager (1981)" startWordPosition="1724" endWordPosition="1725">ed list of documents is returned. The purpose of this elaborate linguistic processing is to create a better representation of documents and to generate best possible queries out of user&apos;s initial requests. Despite limitations of termand-weight type representation (or boolean versions thereof), very good queries can be produced by human experts. In order to imitate an expert, the system must be able to learn about its database, in particular about various correlations among index terms. FAST PARSING WITH TTP PARSER TIP (Tagged Text Parser) is based on the Linguistic String Grammar developed by Sager (1981). The parser currently encompasses some 400 grammar productions, but it is by no means complete. The parser&apos;s output is a regularized parse tree representation of each sentence, that is, a representation that reflects the sentence&apos;s logical predicateargument structure. For example, logical subject and logical object are identified in both passive and active sentences, and noun phrases are organized around their head elements. The significance of this representation will be discussed below. The parser is equipped with a powerful skip-and-fit recovery mechanism that allows it to operate effectiv</context>
</contexts>
<marker>Sager, 1981</marker>
<rawString>Sager, Naomi. 1981. Natural Language Information Processing. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>Karen</author>
</authors>
<title>Statistical interpretation of term specificity and its application in retrieval.&amp;quot;</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<volume>28</volume>
<issue>1</issue>
<pages>11--20</pages>
<marker>Jones, Karen, 1972</marker>
<rawString>Sparck Jones, Karen. 1972. &amp;quot;Statistical interpretation of term specificity and its application in retrieval.&amp;quot; Journal of Documentation, 28(1), pp. 11-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>K</author>
<author>E</author>
</authors>
<title>What makes automatic keyword classification effective?&amp;quot;</title>
<date>1971</date>
<journal>Journal of the American Society for Information Science, May-June,</journal>
<pages>166--175</pages>
<marker>Jones, K, E, 1971</marker>
<rawString>Sparck Jones, K. and E. 0. Barber. 1971. &amp;quot;What makes automatic keyword classification effective?&amp;quot; Journal of the American Society for Information Science, May-June, pp. 166-175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>K</author>
<author>J I Tait</author>
</authors>
<title>Automatic search term variant generation.&amp;quot;</title>
<date>1984</date>
<journal>Journal of Documentation,</journal>
<volume>40</volume>
<issue>1</issue>
<pages>50--66</pages>
<marker>Jones, K, Tait, 1984</marker>
<rawString>Sparck Jones, K. and J. I. Tait. 1984. &amp;quot;Automatic search term variant generation.&amp;quot; Journal of Documentation, 40(1), pp. 50-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzallcowski</author>
<author>Barbara Vauthey</author>
</authors>
<title>Fast Text Processing for Information Retrieval.&amp;quot;</title>
<date>1991</date>
<booktitle>Proceedings of the 4th DARPA Speech and Natural Language Workshop, Morgan-Kaufman,</booktitle>
<pages>346--351</pages>
<marker>Strzallcowski, Vauthey, 1991</marker>
<rawString>Strzallcowski, Tomek and Barbara Vauthey. 1991. &amp;quot;Fast Text Processing for Information Retrieval.&amp;quot; Proceedings of the 4th DARPA Speech and Natural Language Workshop, Morgan-Kaufman, pp. 346-351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowslci</author>
<author>Barbara Vauthey</author>
</authors>
<title>Information Retrieval Using Robust Natural Language Processing.&amp;quot;</title>
<date>1992</date>
<booktitle>Proc. of the 30th ACL Meeting,</booktitle>
<pages>104--111</pages>
<location>Newark, DE, June-July.</location>
<marker>Strzalkowslci, Vauthey, 1992</marker>
<rawString>Strzalkowslci, Tomek and Barbara Vauthey. 1992. &amp;quot;Information Retrieval Using Robust Natural Language Processing.&amp;quot; Proc. of the 30th ACL Meeting, Newark, DE, June-July. pp. 104-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzallcowski</author>
</authors>
<title>TTP: A Fast and Robust Parser for Natural Language.&amp;quot;</title>
<date>1992</date>
<booktitle>Proceedings of the 14th International Conference on Computational Linguistics (COLING), Names,</booktitle>
<pages>198--204</pages>
<location>France,</location>
<contexts>
<context position="14728" citStr="Strzallcowski, 1992" startWordPosition="2282" endWordPosition="2283"> is reactivated. In this case, TTP may force the following reductions: 51 —) to V NP; SA -4 SI; S —&gt; NP V NP SA, until the production S S and S is reached. Next, the parser skips input to find and, and resumes normal processing. As may be expected, the skip-and-fit strategy will only be effective if the input skipping can be performed with a degree of determinism. This means that most of the lexical level ambiguity must be removed from the input text, prior to parsing. We achieve this using a stochastic parts of speech tagger to preprocess the text. Full details of the parser can be found in (Strzallcowski, 1992). 11 PART OF SPEECH TAGGER One way of dealing with lexical ambiguity is to use a tagger to preprocess the input marking each word with a tag that indicates its syntactic categorization: a part of speech with selected morphological features such as number, tense, mode, case and degree. The following are tagged sentences from the CACM-3204 collection: 5 The/dr paper/nn presents/vbz a/dr proposal/nn for/in structured/vbn representation/nn of/in multiprogramming/vbg in/in aid: highlij level/nn language/nn ./per The/dr notation/nn used/vbn explicitly/rb associates/vbz data/nns structure/nn shared/v</context>
</contexts>
<marker>Strzallcowski, 1992</marker>
<rawString>Strzallcowski, Tomek. 1992. &amp;quot;TTP: A Fast and Robust Parser for Natural Language.&amp;quot; Proceedings of the 14th International Conference on Computational Linguistics (COLING), Names, France, July 1992. pp. 198-204.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>