<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000995">
<title confidence="0.998953">
Towards A Better Understanding of
Uncertainties and Speculations in Swedish Clinical Text
– Analysis of an Initial Annotation Trial
</title>
<author confidence="0.995627">
Sumithra Velupillai
</author>
<affiliation confidence="0.983821">
Department of Computer and Systems Sciences (DSV)
Stockholm University
</affiliation>
<address confidence="0.96933">
Forum 100
SE-164 40 Kista, Sweden
</address>
<email confidence="0.998452">
sumithra@dsv.su.se
</email>
<sectionHeader confidence="0.993795" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99952475862069">
Electronic Health Records (EHRs) contain
a large amount of free text documentation
which is potentially very useful for Infor-
mation Retrieval and Text Mining appli-
cations. We have, in an initial annotation
trial, annotated 6 739 sentences randomly
extracted from a corpus of Swedish EHRs
for sentence level (un)certainty, and token
level speculative keywords and negations.
This set is split into different clinical prac-
tices and analyzed by means of descrip-
tive statistics and pairwise Inter-Annotator
Agreement (IAA) measured by Fl-score.
We identify geriatrics as a clinical prac-
tice with a low average amount of uncer-
tain sentences and a high average IAA,
and neurology with a high average amount
of uncertain sentences. Speculative words
are often n-grams, and uncertain sentences
longer than average. The results of this
analysis is to be used in the creation of a
new annotated corpus where we will refine
and further develop the initial annotation
guidelines and introduce more levels of di-
mensionality. Once we have finalized our
guidelines and refined the annotations we
plan to release the corpus for further re-
search, after ensuring that no identifiable
information is included.
</bodyText>
<sectionHeader confidence="0.999322" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999707355555556">
Electronic Health Records (EHRs) contain a large
amount of free text documentation which is po-
tentially very useful for Information Retrieval and
Text Mining applications. Clinical documentation
is specific in many ways; there are many authors
in a document (e.g. physicians, nurses), there are
different situations that are documented (e.g. ad-
mission, current status). Moreover, they may often
be written under time pressure, resulting in frag-
mented, brief texts often containing spelling errors
and abbreviations. With access to EHR data, many
possibilities to exploit documented clinical knowl-
edge and experience arise.
One of the properties of EHRs is that they con-
tain reasoning about the status and diagnoses of
patients. Gathering such information for the use
in e.g. medical research in order to find rela-
tionships between diagnoses, treatments etc. has
great potential. However, in many situations, clin-
icians might describe uncertain or negated find-
ings, which is crucial to distinguish from positive
or asserted findings. Potential future applications
include search engines where medical researchers
can search for particular diseases where negated
or speculative contexts are separated from asserted
contexts, or text mining systems where e.g. dis-
eases that seem to occur often in speculative con-
texts are presented to the user, indicating that more
research is needed. Moreover, laymen may also
benefit from information retrieval systems that dis-
tinguish diseases or symptoms that are more or
less certain given current medical expertise and
knowledge.
We have, in an initial annotation trial, annotated
6 739 sentences randomly extracted from a corpus
of Swedish EHRs for sentence level (un)certainty,
and token level speculative keywords and nega-
tions1. In this paper, a deeper analysis of the re-
sulting annotations is performed. The aims are
to analyze the results split into different clinical
practices by means of descriptive statistics and
pairwise Inter-Annotator Agreement (IAA) mea-
sured by Fl-score, with the goal of identifying a)
whether specific clinical practices contain higher
or lower amounts of uncertain expressions, b)
</bodyText>
<footnote confidence="0.955462">
1This research has been carried out after approval
from the Regional Ethical Review Board in Stockholm
(Etikpr¨ovningsn¨amnden i Stockholm), permission number
2009/1742-31/5
</footnote>
<page confidence="0.992445">
14
</page>
<note confidence="0.640203">
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 14–22,
Uppsala, July 2010.
</note>
<bodyText confidence="0.9999654375">
whether specific clinical practices result in higher
or lower IAA - indicating a less or more difficult
clinical practice for judging uncertainties, and c)
identifying the characteristics of the entities anno-
tated as speculative words, are they highly lexi-
cal or is a deeper syntactic and/or semantic anal-
ysis required for modeling? From this analysis,
we plan to conduct a new annotation trial where
we will refine and further develop the annotation
guidelines and use domain experts for annotations
in order to be able to create a useful annotated cor-
pus modeling uncertainties, negations and specu-
lations in Swedish clinical text, which can be used
to develop tools for the automatic identification of
these phenomena in, for instance, Text Mining ap-
plications.
</bodyText>
<sectionHeader confidence="0.997685" genericHeader="introduction">
2 Related Research
</sectionHeader>
<bodyText confidence="0.999948671875">
In recent years, the interest for identifying and
modeling speculative language in natural language
text has grown. In particular, biomedical scien-
tific articles and abstracts have been the object of
several experiments. In Light et al. (2004), four
annotators annotated 891 sentences each as either
highly speculative, low speculative, or definite,
in biomedical scientific abstracts extracted from
Medline. In total, they found 11 percent specula-
tive sentences, resulting in IAA results, measured
with kappa, between 0.54 and 0.68. One of their
main findings was that the majority of the specu-
lative sentences appeared towards the end of the
abstract.
Vincze et al. (2008) describe the creation of the
BioScope corpus, where more than 20 000 sen-
tences from both medical (clinical) free texts (ra-
diology reports), biological full papers and biolog-
ical scientific abstracts have been annotated with
speculative and negation keywords along with
their scope. Over 10 percent of the sentences
were either speculative or negated. In the clinical
sub-corpus, 14 percent contained speculative key-
words. Three annotators annotated the corpus, and
the guidelines were modified several times during
the annotation process, in order to resolve prob-
lematic issues and refine definitions. The IAA
results, measured with Fl-score, in the clinical
sub-corpus for negation keywords ranged between
0.91 and 0.96, and for speculative keywords be-
tween 0.84 and 0.92. The BioScope corpus has
been used to train and evaluate automatic classi-
fiers (e.g. ¨Ozg¨ur and Radev (2009) and Morante
and Daelemans (2009)) with promising results.
Five qualitative dimensions for characterizing
scientific sentences are defined in Wilbur et al.
(2006), including levels of certainty. Here, guide-
lines are also developed over a long period of time
(more than a year), testing and revising the guide-
lines consecutively. Their final IAA results, mea-
sured with Fl-score, range between 0.70 and 0.80.
Different levels of dimensionality for categorizing
certainty (in newspaper articles) is also presented
in Rubin et al. (2006).
Expressions for communicating probabilities or
levels of certainty in clinical care may be inher-
ently difficult to judge. Eleven observers were
asked to indicate the level of probability of a dis-
ease implied by eighteen expressions in the work
presented by Hobby et al. (2000). They found
that expressions indicating intermediate probabili-
ties were much less consistently rated than those
indicating very high or low probabilities. Sim-
ilarly, Khorasani et al. (2003) performed a sur-
vey analyzing agreement between radiologists and
non-radiologists regarding phrases used to convey
degrees of certainty. In this study, they found lit-
tle or no agreement among the survey participants
regarding the diagnostic certainty associated with
these phrases. Although we do not have access to
radiology reports in our corpus, these findings in-
dicate that it is not trivial to classify uncertain lan-
guage in clinical documentation, even for domain
experts.
</bodyText>
<sectionHeader confidence="0.985482" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.994469375">
The annotation trial is based on sentences ran-
domly extracted from a corpus of Swedish EHRs
(see Dalianis and Velupillai (2010) for an initial
description and analysis). These records contain
both structured (e.g. measure values, gender in-
formation) and unstructured information (i.e. free
text). Each free text entry is written under a spe-
cific heading, e.g. Status, Current medication, So-
cial Background. For this corpus, sentences were
extracted only from the free text entry Assessment
(Bed¨omning), with the assumption that these en-
tries contain a substantial amount of reasoning re-
garding a patient’s diagnosis and situation. A sim-
ple sentence tokenizing strategy was employed,
based on heuristic regular expressions2. We have
used Knowtator (Ogren, 2006) for the annotation
</bodyText>
<footnote confidence="0.9992725">
2The performance of the sentence tokenizer has not been
evaluated in this work.
</footnote>
<page confidence="0.996508">
15
</page>
<bodyText confidence="0.993983555555556">
work.
One senior level student (SLS), one undergrad-
uate computer scientist (UCS), and one undergrad-
uate language consultant (ULC) annotated the sen-
tences into the following classes; on a sentence
level: certain, uncertain or undefined, and on a
token level: speculative words, negations, and un-
defined words.
The annotators are to be considered naive
coders, as they had no prior knowledge of the
task, nor any clinical background. The annota-
tion guidelines were inspired by those created for
the BioScope corpus (Vincze et al., 2008), with
some modifications (see Dalianis and Velupillai
(2010)). The annotators were allowed to break a
sentence into subclauses if they found that a sen-
tence contained conflicting levels of certainty, and
they were allowed to mark question marks as spec-
ulative words. They did not annotate the linguis-
tic scopes of each token level instance. The anno-
tators worked independently, and met for discus-
sions in even intervals (in total seven), in order to
resolve problematic issues. No information about
the clinic, patient gender, etc. was shown. The
annotation trial is considered as a first step in fur-
ther work of annotating Swedish clinical text for
speculative language.
</bodyText>
<table confidence="0.9998240625">
Clinical practice # sentences # tokens
hematology 140 1 494
surgery 295 3 269
neurology 351 4 098
geriatrics 142 1 568
orthopaedics 245 2 541
rheumatology 384 3 348
urology 120 1 393
cardiology 128 1 242
oncology 550 5 262
ENT 224 2 120
infection 107 1 228
emergency 717 6 755
paediatrics 935 8 926
total, clinical practice 4 338 43 244
total, full corpus 6 739 69 495
</table>
<tableCaption confidence="0.996774">
Table 1: Number of sentences and tokens per clin-
</tableCaption>
<bodyText confidence="0.6619245">
ical practice (#sentences &gt; 100), and in total. ENT
= Ear, Nose and Throat.
</bodyText>
<subsectionHeader confidence="0.999694">
3.1 Annotations and clinical practices
</subsectionHeader>
<bodyText confidence="0.999989183673469">
The resulting corpus consists of 6 739 sentences,
extracted from 485 unique clinics. In order to
be able to analyze possible similarities and dif-
ferences across clinical practices, sentences from
clinics belonging to a specific practice type were
grouped together. In Table 1, the resulting groups,
along with the total amount of sentences and to-
kens, are presented3. Only groups with a total
amount of sentences &gt; 100 were used in the anal-
ysis, resulting in 13 groups. A clinic was included
in a clinical practice group based on a priority
heuristics, e.g. the clinic ”Barnakuten-kir” (Pae-
diatric emergency surgery) was grouped into pae-
diatrics.
The average length (in tokens) per clinical prac-
tice and in total are given in Table 2. Clinical
documentation is often very brief and fragmented,
for most clinical practices (except urology and
cardiology) the minimum sentence length (in to-
kens) was one, e.g. ”basal”, ”terapisvikt” (ther-
apy failure), ”lymf¨odem” (lymphedema), ”viros”
(virosis), ”opanm¨ales” (reported to surgery, com-
pound with abbreviation). We see that the aver-
age sentence length is around ten for all practices,
where the shortest are found in rheumatology and
the longest in infection.
As the annotators were allowed to break up sen-
tences into subclauses, but not required to, this led
to a considerable difference in the total amount of
annotations per annotator. In order to be able to
analyze similarities and differences between the
resulting annotations, all sentence level annota-
tions were converted into one sentence class only,
the primary class (defined as the first sentence
level annotation class, i.e. if a sentence was bro-
ken into two clauses by an annotator, the first be-
ing certain and the second being uncertain, the
final sentence level annotation class will be cer-
tain). The sentence level annotation class certain
was in clear majority among all three annotators.
On both sentence and token level, the class unde-
fined (a sentence that could not be classified as
certain or uncertain, or a token which was not
clearly speculative) was rarely used. Therefore,
all sentence level annotations marked as undefined
are converted to the majority class, certain, result-
ing in two sentence level annotation classes (cer-
tain and uncertain) and two token level annotation
classes (speculative words and negations, i.e. to-
</bodyText>
<footnote confidence="0.868207">
3White space tokenization.
</footnote>
<page confidence="0.995234">
16
</page>
<bodyText confidence="0.992151">
kens annotated as undefined are ignored).
For the remaining analysis, we focus on the
distributions of the annotation classes uncertain
and speculative words, per annotator and annota-
tor pair, and per clinical practice.
</bodyText>
<table confidence="0.9998254">
Clinical practice Max Avg Stddev
hematology 40 10.67 7.97
surgery 57 11.08 8.29
neurology 105 11.67 10.30
geriatrics 58 11.04 9.29
orthopaedics 40 10.37 6.88
rheumatology 59 8.72 7.99
urology 46 11.61 7.86
cardiology 50 9.70 7.46
oncology 54 9.57 7.75
ENT 54 9.46 7.53
infection 37 11.48 7.76
emergency 55 9.42 6.88
paediatrics 68 9.55 7.24
total, full corpus 120 10.31 8.53
</table>
<tableCaption confidence="0.98559">
Table 2: Token statistics per sentence and clinical
</tableCaption>
<figureCaption confidence="0.762624">
practice. All clinic groups except urology (min =
2) and cardiology (min = 2) have a minimum sen-
tence length of one token.
Figure 1: Sentence level annotation: uncertain,
percentage per annotator and clinical practice.
</figureCaption>
<sectionHeader confidence="0.999924" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.998362428571429">
We have measured the proportions (in percent) per
annotator for each clinical practice and in total.
This enables an analysis of whether there are sub-
stantial individual differences in the distributions,
indicating that this annotation task is highly sub-
jective and/or difficult. Moreover, we measure
IAA by pairwise Fl-score. From this, we may
</bodyText>
<figureCaption confidence="0.993127857142857">
Figure 2: Pairwise Fl-score, sentence level anno-
tation class uncertain.
draw conclusions whether specific clinical prac-
tices are harder or easier to judge reliably (i.e. by
high IAA results).
Figure 3: Average length in tokens, per annotator
and sentence class.
</figureCaption>
<bodyText confidence="0.99985225">
In Figure 1, we see that the average amount of
uncertain sentences lies between 9 and 12 percent
for each annotator in the full corpus. In general,
UCS has annotated a larger proportion of uncer-
tain sentences compared to ULC and SLS.
The clinical discipline with the highest average
amount of uncertain sentences is neurology (13.7
percent), the lowest average amount is found in
cardiology (4.7 percent). Surgery and cardiology
show the largest individual differences in propor-
tions (from 9 percent (ULC) to 15 percent (UCS),
and from 2 percent (ULC) to 7 percent (UCS), re-
spectively).
However, in Figure 2, we see that the pairwise
IAA, measured by Fl-score, is relatively low, with
an average IAA of 0.58, ranging between 0.54
(UCS/SLS) and 0.65 (UCS/ULC), for the entire
corpus. In general, the annotator pair UCS/ULC
have higher IAA results, with the highest for geri-
atrics (0.78). The individual proportions for un-
</bodyText>
<page confidence="0.99558">
17
</page>
<bodyText confidence="0.99987275">
certain sentences in geriatrics is also lower for
all annotators (see Figure 1), indicating a clinical
practice with a low amount of uncertain sentences,
and a slightly higher average IAA (0.64 F1-score).
</bodyText>
<subsectionHeader confidence="0.997888">
4.1 Sentence lengths
</subsectionHeader>
<bodyText confidence="0.999652882352941">
As the focus lies on analyzing sentences annotated
as uncertain, one interesting property is to look at
sentence lengths (measured in tokens). One hy-
pothesis is that uncertain sentences are in general
longer. In Figure 3 we see that in general, for
all three annotators, uncertain sentences are longer
than certain sentences. This result is, of course,
highly influenced by the skewness of the data (i.e.
uncertain sentences are in minority), but it is clear
that uncertain sentences, in general, are longer on
average. It is interesting to note that the annota-
tor SLS has, in most cases, annotated longer sen-
tences as uncertain, compared to UCS and ULC.
Moreover, geriatrics, with relatively high IAA but
relatively low amounts of uncertain sentences, has
well above average sentence lengths in the uncer-
tain class.
</bodyText>
<subsectionHeader confidence="0.985133">
4.2 Token level annotations
</subsectionHeader>
<bodyText confidence="0.999527869565218">
When it comes to the token level annotations,
speculative words and negations, we observed
very high IAA for negations (0.95 F1-score (exact
match) on average in the full corpus, the lowest for
neurology, 0.94). These annotations were highly
lexical (13 unique tokens) and unambiguous, and
spread evenly across the two sentence level anno-
tation classes (ranging between 1 and 3 percent of
the total amount of tokens per class). Moreover,
all negations were unigrams.
On the other hand, we observed large variations
in IAA results for speculative words. In Figure
4, we see that there are considerable differences
between exact and partial matches4 between all
annotator pairs, indicating individual differences
in the interpretations of what constitutes a spec-
ulative word and how many tokens they cover,
and the lexicality is not as evident as for nega-
tions. The highest level of agreement we find be-
tween UCS/ULC in orthopaedics (0.65 F1-score,
partial match) and neurology (0.64 F1-score, par-
tial match), and the lowest in infection (UCS/SLS,
0.31 F1-score).
</bodyText>
<footnote confidence="0.910009">
4Partial matches are measured on a character level.
</footnote>
<figureCaption confidence="0.99852">
Figure 4: F1-score, speculative words, exact and
partial match.
</figureCaption>
<subsubsectionHeader confidence="0.598097">
4.2.1 Speculative words – most common
</subsubsectionHeader>
<bodyText confidence="0.999970352941176">
The low IAA results for speculative words invites
a deeper analysis for this class. How is this inter-
preted by the individual annotators? First, we look
at the most common tokens annotated as specu-
lative words, shared by the three annotators: ”?”,
”sannolikt” (likely), ”ev” (possibly, abbreviated),
”om” (if). The most common speculative words
are all unigrams, for all three annotators. These
tokens are similar to the most common specu-
lative words in the clinical BioScope subcorpus,
where if, may and likely are among the top five
most common. Those tokens that are most com-
mon per annotator and not shared by the other two
(among the five most frequent) include ”bed¨oms”
(judged), ”kan” (could), ”helt” (completely) and
”st¨allningstagande” (standpoint).
Looking at neurology and urology, with a higher
overall average amount of uncertain sentences, we
find that the most common words for neurology
are similar to those most common in total, while
for urology we find more n-grams. In Table 3, the
five most common speculative words per annotator
for neurology and urology are presented.
When it comes to the unigrams, many of these
are also not annotated as speculative words. For
instance, ”om” (if), is annotated as speculative in
only 9 percent on average of its occurrence in the
neurological data (the same distribution holds, on
average, in the total set). In Morante and Daele-
mans (2009), if is also one of the words that are
subject to the majority of false positives in their
automatic classifier. On the other hand, ”sanno-
likt” (likely) is almost always annotated as a spec-
ulative word (over 90 percent of the time).
</bodyText>
<page confidence="0.997162">
18
</page>
<table confidence="0.999545384615385">
UCS ULC SLS
neurology ? ? ?
sannolikt (likely) kan (could) sannolikt (likely)
kan (could) sannolikt (likely) ev (possibly, abbr)
om (if) om (if) om (if)
pr¨ova (try) verkar (seems) st¨allningstagande (standpoint)
ter (seem) ev (possibly, abbr) m¨ojligen (possibly)
urology kan vara (could be) mycket (very) tyder p˚a(indicates)
tyder p˚a(indicates) inga tecken (no signs) i f¨orsta hand (primarily)
ev (possibly, abbr) kan vara (could be) misst¨ankt (suspected)
misst¨ankt (suspected) kan (could) kanske (perhaps)
kanske (perhaps) tyder (indicates) skall vi f¨ors¨oka (should we try)
planeras tydligen (apparently planned) misst¨ankt (suspected) kan vara (could be)
</table>
<tableCaption confidence="0.999579">
Table 3: Most common speculative words per annotator for neurology and urology.
</tableCaption>
<subsubsectionHeader confidence="0.497142">
4.2.2 Speculative words – n-grams
</subsubsectionHeader>
<bodyText confidence="0.999905157894737">
Speculative words are, in Swedish clinical text,
clearly not simple lexical unigrams. In Figure 5
we see that the average length of tokens anno-
tated as speculative words is, on average, 1.34,
with the longest in orthopaedics (1.49) and urol-
ogy (1.46). We also see that SLS has, on aver-
age, annotated longer sequences of tokens as spec-
ulative words compared to UCS and ULC. The
longest n-grams range between three and six to-
kens, e.g. ”kan inte se n˚agra tydliga” (can’t see
any clear), ”kan r¨ora sig om” (could be about),
”inte helt har kunnat uteslutas” (has not been able
to completely exclude), ”i f¨orsta hand” (primarily).
In many of these cases, the strongest indicator is
actually a unigram (”kan” (could)), within a verb
phrase. Moreover, negations inside a speculative
word annotation, such as ”inga tecken” (no signs)
are annotated differently among the individual an-
notators.
</bodyText>
<figureCaption confidence="0.991257">
Figure 5: Average length, speculative words.
</figureCaption>
<subsectionHeader confidence="0.995187">
4.3 Examples
</subsectionHeader>
<bodyText confidence="0.999791304347826">
We have observed low average pairwise IAA for
sentence level annotations in the uncertain class,
with more or less large differences between the an-
notator pairs. Moreover, at the token level and for
the class speculative words, we also see low av-
erage agreement, and indications that speculative
words often are n-grams. We focus on the clinical
practices neurology, because of its average large
proportion of uncertain sentences, geriatrics for
its high IAA results for UCS/ULC and low aver-
age proportion of uncertain sentences, and finally
surgery, for its large discrepancy in proportions
and low average IAA results.
In Example 1 we see a sentence where two an-
notators (ULC, SLS) have marked the sentence
as uncertain, also marking a unigram (”ospecifik”
(unspecific) as a speculative word. This example
is interesting since the utterance is ambiguous, it
can be judged as certain as in the dizziness is con-
firmed to be of an unspecific type or uncertain as
in the type of dizziness is unclear, a type of ut-
terance which should be clearly addressed in the
guidelines.
</bodyText>
<figure confidence="0.952675333333333">
&lt;C&gt; Yrsel av ospecifik typ. &lt;/C&gt;
&lt;U&gt; Yrsel av &lt;S&gt; ospecifik &lt;/S&gt; typ.
&lt;/U&gt;
&lt;U&gt; Yrsel av &lt;S&gt; ospecifik &lt;/S&gt; typ.
&lt;/U&gt;
Dizziness of unspecific type
</figure>
<figureCaption confidence="0.5958355">
Example 1: Annotation example, neurology. Am-
biguous sentence, unspecific as a possible specu-
lation cue. C = Certain, U = Uncertain, S = Spec-
ulative words.
</figureCaption>
<bodyText confidence="0.9999456">
An example of different interpretations of the
minimum span a speculative word covers is given
in Example 2. Here, we see that ”inga egentliga
m¨arkbara” (no real apparent) has been annotated
in three different ways. It is also interesting to
</bodyText>
<page confidence="0.998171">
19
</page>
<bodyText confidence="0.999597772727273">
note the role of the negation as part of ampli-
fying speculation. Several such instances were
marked by the annotators (for further examples,
see Dalianis and Velupillai (2010)), which con-
forms well with the findings reported in Kilicoglu
and Bergler (2008), where it is showed that ex-
plicit certainty markers together with negation are
indicators of speculative language. In the Bio-
Scope corpus (Vincze et al., 2008), such instances
are marked as speculation cues. This example, as
well as Example 1, is also interesting as they both
clearly are part of a longer passage of reasoning of
a patient, with no particular diagnosis mentioned
in the current sentence. Instead of randomly ex-
tracting sentences from the free text entry Assess-
ment, one possibility would be to let the annotators
judge all sentences in an entry (or a full EHR). Do-
ing this, differences in where speculative language
often occur in an EHR (entry) might become ev-
ident, as for scientific writings, where it has been
showed that speculative sentences occur towards
the end of abstracts (Light et al., 2004).
</bodyText>
<figure confidence="0.878773444444444">
&lt;U&gt; &lt;S&gt;&lt;N&gt; Inga &lt;/N&gt; egentliga &lt;/S&gt;
&lt;S&gt; m¨arkbara&lt;/S&gt; minnessvarigheter under
samtal. &lt;/U&gt;.
&lt;U&gt; &lt;N&gt; Inga &lt;/N&gt; &lt;S&gt; egentliga &lt;/S&gt;
m¨arkbara minnessvarigheter under samtal. &lt;/U&gt;.
&lt;U&gt; &lt;S&gt;&lt;N&gt; Inga &lt;/N&gt; egentliga m¨arkbara
&lt;/S&gt; minnessvarigheter under samtal. &lt;/U&gt;.
No real apparent memory difficulties during
conversation
</figure>
<bodyText confidence="0.937633545454545">
Example 2: Annotation example, neurology. Dif-
ferent annotation coverage over negation and spec-
ulation. C = Certain, U = Uncertain, S = Specula-
tive words, N = Negation
In geriatrics, we have observed a lower than
average amount of uncertain sentences, and high
IAA between UCS and ULC. In Example 3 we see
a sentence where UCS and ULC have matching
annotations, whereas SLS has judged this sentence
as certain. This example shows the difficulty of
interpreting expressions indicating possible spec-
ulation – is ”ganska” (relatively) used here as a
marker of certainty (as certain as one gets when
diagnosing this type of illness)?
The word ”sannolikt” (likely) is one of the most
common words annotated as a speculative word
in the total corpus. In Example 4, we see a sen-
&lt;U&gt; Bade anamnestiskt och testm¨assigt &lt;S&gt;
ganska &lt;/S&gt; stabil vad det g¨aller Alzheimer
sjukdom. &lt;/U&gt;.
&lt;U&gt; Bade anamnestiskt och testm¨assigt &lt;S&gt;
ganska &lt;/S&gt; stabil vad det 0erAlzheimer
sjukdom. &lt;/U&gt;.
&lt;C&gt; Bade anamnestiskt och testm¨assigt ganska
stabil vad det g¨aller Alzheimer sjukdom. &lt;/C&gt;.
Both anamnesis and tests relatively stabile
when it comes to Alzheimer’s disease.
Example 3: Annotation example, geriatrics. Dif-
ferent judgements for the word ”ganska” (rela-
tively). C = Certain, U = Uncertain, S = Specu-
lative words.
tence where the annotators UCS and SLS have
judged it to be uncertain, while UCS and ULC
have marked the word ”sannolikt” (likely) as a
speculative word. This is an interesting exam-
ple, through informal discussions with clinicians
we were informed that this word might as well be
used as a marker of high certainty. Such instances
show the need for using domain experts in future
annotations of similar corpora.
&lt;C&gt;En 66-arig kvinna med &lt;S&gt;sannolikt&lt;/S&gt;
2 synkrona tum¨orer v¨anster colon/sigmoideum och
d¨ar till levermetastaser.&lt;/C&gt;.
&lt;U&gt;En 66-arig kvinna med &lt;S&gt;sannolikt&lt;/S&gt;
2 synkrona tum¨orer v¨anster colon/sigmoideum och
d¨ar till levermetastaser.&lt;/U&gt;.
&lt;C&gt;En 66-arig kvinna med sannolikt 2 synkrona
tum¨orer v¨anster colon/sigmoideum och d¨ar till
levermetastaser.&lt;/C&gt;.
A 66 year old woman likely with 2 synchronous
tumours left colon/sigmoideum in addition to liver
metastasis.
Example 4: Annotation example, surgery. Differ-
ent judgements for the word ”sannolikt” (likely). C
= Certain, U = Uncertain, S = Speculative words.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999997285714286">
We have presented an analysis of an initial anno-
tation trial for the identification of uncertain sen-
tences as well as for token level cues (specula-
tive words) across different clinical practices. Our
main findings are that IAA results for both sen-
tence level annotations of uncertainty and token
level annotations for speculative words are, on av-
</bodyText>
<page confidence="0.979913">
20
</page>
<bodyText confidence="0.999991294117647">
erage, fairly low, with higher average agreement
in geriatrics and rheumatology (see Figures 1 and
2). Moreover, by analyzing the individual distri-
butions for the classes uncertain and speculative
words, we find that neurology has the highest aver-
age amount of uncertain sentences, and cardiology
the lowest. On average, the amount of uncertain
sentences ranges between 9 and 12 percent, which
is in line with previous work on sentence level an-
notations of uncertainty (see Section 2).
We have also showed that the most common
speculative words are unigrams, but that a substan-
tial amount are n-grams. The n-grams are, how-
ever, often part of verb phrases, where the head is
often the speculation cue. However, it is evident
that speculative words are not always simple lex-
ical units, i.e. syntactic information is potentially
very useful. Question marks are the most common
entities annotated as speculative words. Although
these are not interesting indicators in themselves,
it is interesting to note that they are very common
in clinical documentation.
From the relatively low IAA results we draw the
conclusion that this task is difficult and requires
more clearly defined guidelines. Moreover, using
naive coders on clinical documentation is possibly
not very useful if the resulting annotations are to
be used in, e.g. a Text Mining application for med-
ical researchers. Clinical documentation is highly
domain-specific and contains a large amount of
internal jargon, which requires judgements from
clinicians. However, we find it interesting to note
that we have identified differences between dif-
ferent clinical practices. A consensus corpus has
been created from the resulting annotations, which
has been used in an experiment for automatic clas-
sification, see Dalianis and Skeppstedt (2010) for
initial results and evaluation.
During discussions among the annotators, some
specific problems were noted. For instance, the
extracted sentences were not always about the pa-
tient or the current status or diagnosis, and in many
cases an expression could describe (un)certainty of
someone other than the author (e.g. another physi-
cian or a family member), introducing aspects of
perspective. The sentences annotated as certain,
are difficult to interpret, as they are simply not un-
certain. We believe that it is important to intro-
duce further dimensions, e.g. explicit certainty,
and focus (what is (un)certain?), as well as time
(e.g. current or past).
</bodyText>
<sectionHeader confidence="0.999285" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999996104166667">
To our knowledge, there is no previous research on
annotating Swedish clinical text for sentence and
token level uncertainty together with an analysis
of the differences between different clinical prac-
tices. Although the initial IAA results are in gen-
eral relatively low for all clinical practice groups,
we have identified indications that neurology is a
practice which has an above average amount of
uncertain elements, and that geriatrics has a be-
low average amount, as well as higher IAA. Both
these disciplines would be interesting to continue
the work on identifying speculative language.
It is evident that clinical language contains a rel-
atively high amount of uncertain elements, but it
is also clear that naive coders are not optimal to
use for interpreting the contents of EHRs. More-
over, more care needs to be taken in the extrac-
tion of sentences to be annotated, in order to en-
sure that the sentences actually describe reason-
ing about the patient status and diagnosis. For in-
stance, instead of randomly extracting sentences
from within a free text entry, it might be better to
let the annotators judge all sentences within an en-
try. This would also enable an analysis of whether
speculative language is more or less frequent in
specific parts of EHRs.
From our findings, we plan to further develop
the guidelines and particularly focus on specify-
ing the minimal entities that should be annotated
as speculative words (e.g. ”kan” (could)). We
also plan to introduce further levels of dimension-
ality in the annotation task, e.g. cues that indi-
cate a high level of certainty, and to use domain
experts as annotators. Although there are prob-
lematic issues regarding the use of naive coders
for this task, we believe that our analysis has re-
vealed some properties of speculative language in
clinical text which enables us to develop a useful
resource for further research in the area of specula-
tive language. Judging an instance as being certain
or uncertain is, perhaps, a task which can never
exclude subjective interpretations. One interesting
way of exploiting this fact would be to exploit in-
dividual annotations similar to the work presented
in Reidsma and op den Akker (2008). Once we
have finalized the annotated set, and ensured that
no identifiable information is included, we plan to
make this resource available for further research.
</bodyText>
<page confidence="0.998737">
21
</page>
<sectionHeader confidence="0.995875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999855041666667">
Hercules Dalianis and Maria Skeppstedt. 2010. Cre-
ating and Evaluating a Consensus for Negated and
Speculative Words in a Swedish Clinical Corpus. To
be published in the proceedings of the Negation and
Speculation in Natural Language Processing Work-
shop, July 10, Uppsala, Sweden.
Hercules Dalianis and Sumithra Velupillai. 2010.
How Certain are Clinical Assessments? Annotat-
ing Swedish Clinical Text for (Un)certainties, Spec-
ulations and Negations. In Proceedings of the of
the Seventh International Conference on Language
Resources and Evaluation, LREC 2010, Valletta,
Malta, May 19-21.
J. L. Hobby, B. D. M. Tom, C. Todd, P. W. P. Bearcroft,
and A. K. Dixon. 2000. Communication of doubt
and certainty in radiological reports. The British
Journal of Radiology, 73:999–1001, September.
R. Khorasani, D. W. Bates, S. Teeger, J. M. Rotschild,
D. F. Adams, and S. E. Seltzer. 2003. Is terminol-
ogy used effectively to convey diagnostic certainty
in radiology reports? Academic Radiology, 10:685–
688.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9(S-11).
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, spec-
ulations, and statements in between. In Lynette
Hirschman and James Pustejovsky, editors, HLT-
NAACL 2004 Workshop: BioLINK 2004, Linking
Biological Literature, Ontologies and Databases,
pages 17–24, Boston, Massachusetts, USA, May 6.
Association for Computational Linguistics.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts.
In BioNLP ’09: Proceedings of the Workshop on
BioNLP, pages 28–36, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philip V. Ogren. 2006. Knowtator: a prot´eg´e plug-in
for annotated corpus construction. In Proceedings of
the 2006 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 273–275, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. De-
tecting speculations and their scopes in scientific
text. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1398–1407, Singapore, August. Association
for Computational Linguistics.
Dennis Reidsma and Rieks op den Akker. 2008. Ex-
ploiting ’subjective’ annotations. In HumanJudge
’08: Proceedings of the Workshop on Human Judge-
ments in Computational Linguistics, pages 8–16,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Victoria L. Rubin, Elizabeth D. Liddy, and Noriko
Kando. 2006. Certainty identification in texts: Cat-
egorization model and manual tagging results. In
Computing Affect and Attitutde in Text: Theory and
Applications. Springer.
Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas,
Gy¨orgy M´ora, and J´anos Csirik. 2008. The bio-
scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMCBioinformat-
ics, 9(S-11).
J. W. Wilbur, A. Rzhetsky, and H. Shatkay. 2006. New
directions in biomedical text annotation: definitions,
guidelines and corpus construction. BMC Bioinfor-
matics, 7:356+, July.
</reference>
<page confidence="0.99903">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.125523">
<title confidence="0.927099666666667">Towards A Better Understanding Uncertainties and Speculations in Swedish Clinical – Analysis of an Initial Annotation Trial</title>
<author confidence="0.732332">Sumithra</author>
<affiliation confidence="0.997528">Department of Computer and Systems Sciences</affiliation>
<address confidence="0.784333">Stockholm</address>
<email confidence="0.59565">Forum</email>
<phone confidence="0.275865">SE-164 40 Kista,</phone>
<email confidence="0.958846">sumithra@dsv.su.se</email>
<abstract confidence="0.999572">Electronic Health Records (EHRs) contain a large amount of free text documentation which is potentially very useful for Information Retrieval and Text Mining applications. We have, in an initial annotation trial, annotated 6 739 sentences randomly extracted from a corpus of Swedish EHRs for sentence level (un)certainty, and token level speculative keywords and negations. This set is split into different clinical practices and analyzed by means of descriptive statistics and pairwise Inter-Annotator (IAA) measured by identify a clinical practice with a low average amount of uncertain sentences and a high average IAA, a high average amount of uncertain sentences. Speculative words often and uncertain sentences longer than average. The results of this analysis is to be used in the creation of a new annotated corpus where we will refine and further develop the initial annotation guidelines and introduce more levels of dimensionality. Once we have finalized our guidelines and refined the annotations we plan to release the corpus for further research, after ensuring that no identifiable information is included.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hercules Dalianis</author>
<author>Maria Skeppstedt</author>
</authors>
<title>Creating and Evaluating a Consensus for Negated and Speculative Words in a Swedish Clinical Corpus.</title>
<date>2010</date>
<booktitle>the proceedings of the Negation and Speculation in Natural Language Processing Workshop,</booktitle>
<location>Uppsala,</location>
<note>To be published in</note>
<contexts>
<context position="28318" citStr="Dalianis and Skeppstedt (2010)" startWordPosition="4462" endWordPosition="4465">defined guidelines. Moreover, using naive coders on clinical documentation is possibly not very useful if the resulting annotations are to be used in, e.g. a Text Mining application for medical researchers. Clinical documentation is highly domain-specific and contains a large amount of internal jargon, which requires judgements from clinicians. However, we find it interesting to note that we have identified differences between different clinical practices. A consensus corpus has been created from the resulting annotations, which has been used in an experiment for automatic classification, see Dalianis and Skeppstedt (2010) for initial results and evaluation. During discussions among the annotators, some specific problems were noted. For instance, the extracted sentences were not always about the patient or the current status or diagnosis, and in many cases an expression could describe (un)certainty of someone other than the author (e.g. another physician or a family member), introducing aspects of perspective. The sentences annotated as certain, are difficult to interpret, as they are simply not uncertain. We believe that it is important to introduce further dimensions, e.g. explicit certainty, and focus (what </context>
</contexts>
<marker>Dalianis, Skeppstedt, 2010</marker>
<rawString>Hercules Dalianis and Maria Skeppstedt. 2010. Creating and Evaluating a Consensus for Negated and Speculative Words in a Swedish Clinical Corpus. To be published in the proceedings of the Negation and Speculation in Natural Language Processing Workshop, July 10, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hercules Dalianis</author>
<author>Sumithra Velupillai</author>
</authors>
<title>How Certain are Clinical Assessments? Annotating Swedish Clinical Text for (Un)certainties, Speculations and Negations.</title>
<date>2010</date>
<booktitle>In Proceedings of the of the Seventh International Conference on Language Resources and Evaluation, LREC 2010,</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="7899" citStr="Dalianis and Velupillai (2010)" startWordPosition="1198" endWordPosition="1201">i et al. (2003) performed a survey analyzing agreement between radiologists and non-radiologists regarding phrases used to convey degrees of certainty. In this study, they found little or no agreement among the survey participants regarding the diagnostic certainty associated with these phrases. Although we do not have access to radiology reports in our corpus, these findings indicate that it is not trivial to classify uncertain language in clinical documentation, even for domain experts. 3 Method The annotation trial is based on sentences randomly extracted from a corpus of Swedish EHRs (see Dalianis and Velupillai (2010) for an initial description and analysis). These records contain both structured (e.g. measure values, gender information) and unstructured information (i.e. free text). Each free text entry is written under a specific heading, e.g. Status, Current medication, Social Background. For this corpus, sentences were extracted only from the free text entry Assessment (Bed¨omning), with the assumption that these entries contain a substantial amount of reasoning regarding a patient’s diagnosis and situation. A simple sentence tokenizing strategy was employed, based on heuristic regular expressions2. We</context>
<context position="9231" citStr="Dalianis and Velupillai (2010)" startWordPosition="1402" endWordPosition="1405">ot been evaluated in this work. 15 work. One senior level student (SLS), one undergraduate computer scientist (UCS), and one undergraduate language consultant (ULC) annotated the sentences into the following classes; on a sentence level: certain, uncertain or undefined, and on a token level: speculative words, negations, and undefined words. The annotators are to be considered naive coders, as they had no prior knowledge of the task, nor any clinical background. The annotation guidelines were inspired by those created for the BioScope corpus (Vincze et al., 2008), with some modifications (see Dalianis and Velupillai (2010)). The annotators were allowed to break a sentence into subclauses if they found that a sentence contained conflicting levels of certainty, and they were allowed to mark question marks as speculative words. They did not annotate the linguistic scopes of each token level instance. The annotators worked independently, and met for discussions in even intervals (in total seven), in order to resolve problematic issues. No information about the clinic, patient gender, etc. was shown. The annotation trial is considered as a first step in further work of annotating Swedish clinical text for speculativ</context>
<context position="22605" citStr="Dalianis and Velupillai (2010)" startWordPosition="3567" endWordPosition="3570">ospecifik &lt;/S&gt; typ. &lt;/U&gt; Dizziness of unspecific type Example 1: Annotation example, neurology. Ambiguous sentence, unspecific as a possible speculation cue. C = Certain, U = Uncertain, S = Speculative words. An example of different interpretations of the minimum span a speculative word covers is given in Example 2. Here, we see that ”inga egentliga m¨arkbara” (no real apparent) has been annotated in three different ways. It is also interesting to 19 note the role of the negation as part of amplifying speculation. Several such instances were marked by the annotators (for further examples, see Dalianis and Velupillai (2010)), which conforms well with the findings reported in Kilicoglu and Bergler (2008), where it is showed that explicit certainty markers together with negation are indicators of speculative language. In the BioScope corpus (Vincze et al., 2008), such instances are marked as speculation cues. This example, as well as Example 1, is also interesting as they both clearly are part of a longer passage of reasoning of a patient, with no particular diagnosis mentioned in the current sentence. Instead of randomly extracting sentences from the free text entry Assessment, one possibility would be to let the</context>
</contexts>
<marker>Dalianis, Velupillai, 2010</marker>
<rawString>Hercules Dalianis and Sumithra Velupillai. 2010. How Certain are Clinical Assessments? Annotating Swedish Clinical Text for (Un)certainties, Speculations and Negations. In Proceedings of the of the Seventh International Conference on Language Resources and Evaluation, LREC 2010, Valletta, Malta, May 19-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Hobby</author>
<author>B D M Tom</author>
<author>C Todd</author>
<author>P W P Bearcroft</author>
<author>A K Dixon</author>
</authors>
<title>Communication of doubt and certainty in radiological reports.</title>
<date>2000</date>
<journal>The British Journal of Radiology,</journal>
<pages>73--999</pages>
<contexts>
<context position="7094" citStr="Hobby et al. (2000)" startWordPosition="1075" endWordPosition="1078">y. Here, guidelines are also developed over a long period of time (more than a year), testing and revising the guidelines consecutively. Their final IAA results, measured with Fl-score, range between 0.70 and 0.80. Different levels of dimensionality for categorizing certainty (in newspaper articles) is also presented in Rubin et al. (2006). Expressions for communicating probabilities or levels of certainty in clinical care may be inherently difficult to judge. Eleven observers were asked to indicate the level of probability of a disease implied by eighteen expressions in the work presented by Hobby et al. (2000). They found that expressions indicating intermediate probabilities were much less consistently rated than those indicating very high or low probabilities. Similarly, Khorasani et al. (2003) performed a survey analyzing agreement between radiologists and non-radiologists regarding phrases used to convey degrees of certainty. In this study, they found little or no agreement among the survey participants regarding the diagnostic certainty associated with these phrases. Although we do not have access to radiology reports in our corpus, these findings indicate that it is not trivial to classify un</context>
</contexts>
<marker>Hobby, Tom, Todd, Bearcroft, Dixon, 2000</marker>
<rawString>J. L. Hobby, B. D. M. Tom, C. Todd, P. W. P. Bearcroft, and A. K. Dixon. 2000. Communication of doubt and certainty in radiological reports. The British Journal of Radiology, 73:999–1001, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Khorasani</author>
<author>D W Bates</author>
<author>S Teeger</author>
<author>J M Rotschild</author>
<author>D F Adams</author>
<author>S E Seltzer</author>
</authors>
<title>Is terminology used effectively to convey diagnostic certainty in radiology reports? Academic Radiology,</title>
<date>2003</date>
<pages>10--685</pages>
<contexts>
<context position="7284" citStr="Khorasani et al. (2003)" startWordPosition="1102" endWordPosition="1105">range between 0.70 and 0.80. Different levels of dimensionality for categorizing certainty (in newspaper articles) is also presented in Rubin et al. (2006). Expressions for communicating probabilities or levels of certainty in clinical care may be inherently difficult to judge. Eleven observers were asked to indicate the level of probability of a disease implied by eighteen expressions in the work presented by Hobby et al. (2000). They found that expressions indicating intermediate probabilities were much less consistently rated than those indicating very high or low probabilities. Similarly, Khorasani et al. (2003) performed a survey analyzing agreement between radiologists and non-radiologists regarding phrases used to convey degrees of certainty. In this study, they found little or no agreement among the survey participants regarding the diagnostic certainty associated with these phrases. Although we do not have access to radiology reports in our corpus, these findings indicate that it is not trivial to classify uncertain language in clinical documentation, even for domain experts. 3 Method The annotation trial is based on sentences randomly extracted from a corpus of Swedish EHRs (see Dalianis and Ve</context>
</contexts>
<marker>Khorasani, Bates, Teeger, Rotschild, Adams, Seltzer, 2003</marker>
<rawString>R. Khorasani, D. W. Bates, S. Teeger, J. M. Rotschild, D. F. Adams, and S. E. Seltzer. 2003. Is terminology used effectively to convey diagnostic certainty in radiology reports? Academic Radiology, 10:685– 688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>Recognizing speculative language in biomedical research articles: a linguistically motivated perspective.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<contexts>
<context position="22686" citStr="Kilicoglu and Bergler (2008)" startWordPosition="3580" endWordPosition="3583">, neurology. Ambiguous sentence, unspecific as a possible speculation cue. C = Certain, U = Uncertain, S = Speculative words. An example of different interpretations of the minimum span a speculative word covers is given in Example 2. Here, we see that ”inga egentliga m¨arkbara” (no real apparent) has been annotated in three different ways. It is also interesting to 19 note the role of the negation as part of amplifying speculation. Several such instances were marked by the annotators (for further examples, see Dalianis and Velupillai (2010)), which conforms well with the findings reported in Kilicoglu and Bergler (2008), where it is showed that explicit certainty markers together with negation are indicators of speculative language. In the BioScope corpus (Vincze et al., 2008), such instances are marked as speculation cues. This example, as well as Example 1, is also interesting as they both clearly are part of a longer passage of reasoning of a patient, with no particular diagnosis mentioned in the current sentence. Instead of randomly extracting sentences from the free text entry Assessment, one possibility would be to let the annotators judge all sentences in an entry (or a full EHR). Doing this, differen</context>
</contexts>
<marker>Kilicoglu, Bergler, 2008</marker>
<rawString>Halil Kilicoglu and Sabine Bergler. 2008. Recognizing speculative language in biomedical research articles: a linguistically motivated perspective. BMC Bioinformatics, 9(S-11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
<author>Xin Ying Qiu</author>
<author>Padmini Srinivasan</author>
</authors>
<title>The language of bioscience: Facts, speculations, and statements in between.</title>
<date>2004</date>
<booktitle>In Lynette Hirschman and James Pustejovsky, editors, HLTNAACL 2004 Workshop: BioLINK 2004, Linking Biological Literature, Ontologies and Databases,</booktitle>
<pages>17--24</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="4969" citStr="Light et al. (2004)" startWordPosition="749" endWordPosition="752"> refine and further develop the annotation guidelines and use domain experts for annotations in order to be able to create a useful annotated corpus modeling uncertainties, negations and speculations in Swedish clinical text, which can be used to develop tools for the automatic identification of these phenomena in, for instance, Text Mining applications. 2 Related Research In recent years, the interest for identifying and modeling speculative language in natural language text has grown. In particular, biomedical scientific articles and abstracts have been the object of several experiments. In Light et al. (2004), four annotators annotated 891 sentences each as either highly speculative, low speculative, or definite, in biomedical scientific abstracts extracted from Medline. In total, they found 11 percent speculative sentences, resulting in IAA results, measured with kappa, between 0.54 and 0.68. One of their main findings was that the majority of the speculative sentences appeared towards the end of the abstract. Vincze et al. (2008) describe the creation of the BioScope corpus, where more than 20 000 sentences from both medical (clinical) free texts (radiology reports), biological full papers and b</context>
<context position="23507" citStr="Light et al., 2004" startWordPosition="3718" endWordPosition="3721"> cues. This example, as well as Example 1, is also interesting as they both clearly are part of a longer passage of reasoning of a patient, with no particular diagnosis mentioned in the current sentence. Instead of randomly extracting sentences from the free text entry Assessment, one possibility would be to let the annotators judge all sentences in an entry (or a full EHR). Doing this, differences in where speculative language often occur in an EHR (entry) might become evident, as for scientific writings, where it has been showed that speculative sentences occur towards the end of abstracts (Light et al., 2004). &lt;U&gt; &lt;S&gt;&lt;N&gt; Inga &lt;/N&gt; egentliga &lt;/S&gt; &lt;S&gt; m¨arkbara&lt;/S&gt; minnessvarigheter under samtal. &lt;/U&gt;. &lt;U&gt; &lt;N&gt; Inga &lt;/N&gt; &lt;S&gt; egentliga &lt;/S&gt; m¨arkbara minnessvarigheter under samtal. &lt;/U&gt;. &lt;U&gt; &lt;S&gt;&lt;N&gt; Inga &lt;/N&gt; egentliga m¨arkbara &lt;/S&gt; minnessvarigheter under samtal. &lt;/U&gt;. No real apparent memory difficulties during conversation Example 2: Annotation example, neurology. Different annotation coverage over negation and speculation. C = Certain, U = Uncertain, S = Speculative words, N = Negation In geriatrics, we have observed a lower than average amount of uncertain sentences, and high IAA between UCS and </context>
</contexts>
<marker>Light, Qiu, Srinivasan, 2004</marker>
<rawString>Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004. The language of bioscience: Facts, speculations, and statements in between. In Lynette Hirschman and James Pustejovsky, editors, HLTNAACL 2004 Workshop: BioLINK 2004, Linking Biological Literature, Ontologies and Databases, pages 17–24, Boston, Massachusetts, USA, May 6. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>Learning the scope of hedge cues in biomedical texts.</title>
<date>2009</date>
<booktitle>In BioNLP ’09: Proceedings of the Workshop on BioNLP,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6316" citStr="Morante and Daelemans (2009)" startWordPosition="955" endWordPosition="958">10 percent of the sentences were either speculative or negated. In the clinical sub-corpus, 14 percent contained speculative keywords. Three annotators annotated the corpus, and the guidelines were modified several times during the annotation process, in order to resolve problematic issues and refine definitions. The IAA results, measured with Fl-score, in the clinical sub-corpus for negation keywords ranged between 0.91 and 0.96, and for speculative keywords between 0.84 and 0.92. The BioScope corpus has been used to train and evaluate automatic classifiers (e.g. ¨Ozg¨ur and Radev (2009) and Morante and Daelemans (2009)) with promising results. Five qualitative dimensions for characterizing scientific sentences are defined in Wilbur et al. (2006), including levels of certainty. Here, guidelines are also developed over a long period of time (more than a year), testing and revising the guidelines consecutively. Their final IAA results, measured with Fl-score, range between 0.70 and 0.80. Different levels of dimensionality for categorizing certainty (in newspaper articles) is also presented in Rubin et al. (2006). Expressions for communicating probabilities or levels of certainty in clinical care may be inheren</context>
<context position="18849" citStr="Morante and Daelemans (2009)" startWordPosition="2959" endWordPosition="2963">eurology and urology, with a higher overall average amount of uncertain sentences, we find that the most common words for neurology are similar to those most common in total, while for urology we find more n-grams. In Table 3, the five most common speculative words per annotator for neurology and urology are presented. When it comes to the unigrams, many of these are also not annotated as speculative words. For instance, ”om” (if), is annotated as speculative in only 9 percent on average of its occurrence in the neurological data (the same distribution holds, on average, in the total set). In Morante and Daelemans (2009), if is also one of the words that are subject to the majority of false positives in their automatic classifier. On the other hand, ”sannolikt” (likely) is almost always annotated as a speculative word (over 90 percent of the time). 18 UCS ULC SLS neurology ? ? ? sannolikt (likely) kan (could) sannolikt (likely) kan (could) sannolikt (likely) ev (possibly, abbr) om (if) om (if) om (if) pr¨ova (try) verkar (seems) st¨allningstagande (standpoint) ter (seem) ev (possibly, abbr) m¨ojligen (possibly) urology kan vara (could be) mycket (very) tyder p˚a(indicates) tyder p˚a(indicates) inga tecken (no</context>
</contexts>
<marker>Morante, Daelemans, 2009</marker>
<rawString>Roser Morante and Walter Daelemans. 2009. Learning the scope of hedge cues in biomedical texts. In BioNLP ’09: Proceedings of the Workshop on BioNLP, pages 28–36, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip V Ogren</author>
</authors>
<title>Knowtator: a prot´eg´e plug-in for annotated corpus construction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>273--275</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8533" citStr="Ogren, 2006" startWordPosition="1294" endWordPosition="1295">ption and analysis). These records contain both structured (e.g. measure values, gender information) and unstructured information (i.e. free text). Each free text entry is written under a specific heading, e.g. Status, Current medication, Social Background. For this corpus, sentences were extracted only from the free text entry Assessment (Bed¨omning), with the assumption that these entries contain a substantial amount of reasoning regarding a patient’s diagnosis and situation. A simple sentence tokenizing strategy was employed, based on heuristic regular expressions2. We have used Knowtator (Ogren, 2006) for the annotation 2The performance of the sentence tokenizer has not been evaluated in this work. 15 work. One senior level student (SLS), one undergraduate computer scientist (UCS), and one undergraduate language consultant (ULC) annotated the sentences into the following classes; on a sentence level: certain, uncertain or undefined, and on a token level: speculative words, negations, and undefined words. The annotators are to be considered naive coders, as they had no prior knowledge of the task, nor any clinical background. The annotation guidelines were inspired by those created for the </context>
</contexts>
<marker>Ogren, 2006</marker>
<rawString>Philip V. Ogren. 2006. Knowtator: a prot´eg´e plug-in for annotated corpus construction. In Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 273–275, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arzucan ¨Ozg¨ur</author>
<author>Dragomir R Radev</author>
</authors>
<title>Detecting speculations and their scopes in scientific text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1398--1407</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>¨Ozg¨ur, Radev, 2009</marker>
<rawString>Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. Detecting speculations and their scopes in scientific text. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1398–1407, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Rieks op den Akker</author>
</authors>
<title>Exploiting ’subjective’ annotations.</title>
<date>2008</date>
<booktitle>In HumanJudge ’08: Proceedings of the Workshop on Human Judgements in Computational Linguistics,</booktitle>
<pages>8--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Reidsma, den Akker, 2008</marker>
<rawString>Dennis Reidsma and Rieks op den Akker. 2008. Exploiting ’subjective’ annotations. In HumanJudge ’08: Proceedings of the Workshop on Human Judgements in Computational Linguistics, pages 8–16, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria L Rubin</author>
<author>Elizabeth D Liddy</author>
<author>Noriko Kando</author>
</authors>
<title>Certainty identification in texts: Categorization model and manual tagging results.</title>
<date>2006</date>
<booktitle>In Computing Affect and Attitutde in Text: Theory and Applications.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="6816" citStr="Rubin et al. (2006)" startWordPosition="1030" endWordPosition="1033">s been used to train and evaluate automatic classifiers (e.g. ¨Ozg¨ur and Radev (2009) and Morante and Daelemans (2009)) with promising results. Five qualitative dimensions for characterizing scientific sentences are defined in Wilbur et al. (2006), including levels of certainty. Here, guidelines are also developed over a long period of time (more than a year), testing and revising the guidelines consecutively. Their final IAA results, measured with Fl-score, range between 0.70 and 0.80. Different levels of dimensionality for categorizing certainty (in newspaper articles) is also presented in Rubin et al. (2006). Expressions for communicating probabilities or levels of certainty in clinical care may be inherently difficult to judge. Eleven observers were asked to indicate the level of probability of a disease implied by eighteen expressions in the work presented by Hobby et al. (2000). They found that expressions indicating intermediate probabilities were much less consistently rated than those indicating very high or low probabilities. Similarly, Khorasani et al. (2003) performed a survey analyzing agreement between radiologists and non-radiologists regarding phrases used to convey degrees of certai</context>
</contexts>
<marker>Rubin, Liddy, Kando, 2006</marker>
<rawString>Victoria L. Rubin, Elizabeth D. Liddy, and Noriko Kando. 2006. Certainty identification in texts: Categorization model and manual tagging results. In Computing Affect and Attitutde in Text: Theory and Applications. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>Gy¨orgy Szarvas</author>
<author>Rich´ard Farkas</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
</authors>
<title>The bioscope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMCBioinformatics,</title>
<date>2008</date>
<pages>9--11</pages>
<marker>Vincze, Szarvas, Farkas, M´ora, Csirik, 2008</marker>
<rawString>Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas, Gy¨orgy M´ora, and J´anos Csirik. 2008. The bioscope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMCBioinformatics, 9(S-11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Wilbur</author>
<author>A Rzhetsky</author>
<author>H Shatkay</author>
</authors>
<title>New directions in biomedical text annotation: definitions, guidelines and corpus construction.</title>
<date>2006</date>
<journal>BMC Bioinformatics,</journal>
<pages>7--356</pages>
<contexts>
<context position="6445" citStr="Wilbur et al. (2006)" startWordPosition="972" endWordPosition="975">ree annotators annotated the corpus, and the guidelines were modified several times during the annotation process, in order to resolve problematic issues and refine definitions. The IAA results, measured with Fl-score, in the clinical sub-corpus for negation keywords ranged between 0.91 and 0.96, and for speculative keywords between 0.84 and 0.92. The BioScope corpus has been used to train and evaluate automatic classifiers (e.g. ¨Ozg¨ur and Radev (2009) and Morante and Daelemans (2009)) with promising results. Five qualitative dimensions for characterizing scientific sentences are defined in Wilbur et al. (2006), including levels of certainty. Here, guidelines are also developed over a long period of time (more than a year), testing and revising the guidelines consecutively. Their final IAA results, measured with Fl-score, range between 0.70 and 0.80. Different levels of dimensionality for categorizing certainty (in newspaper articles) is also presented in Rubin et al. (2006). Expressions for communicating probabilities or levels of certainty in clinical care may be inherently difficult to judge. Eleven observers were asked to indicate the level of probability of a disease implied by eighteen express</context>
</contexts>
<marker>Wilbur, Rzhetsky, Shatkay, 2006</marker>
<rawString>J. W. Wilbur, A. Rzhetsky, and H. Shatkay. 2006. New directions in biomedical text annotation: definitions, guidelines and corpus construction. BMC Bioinformatics, 7:356+, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>