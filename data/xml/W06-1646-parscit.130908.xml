<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004874">
<title confidence="0.986347">
Corrective Models for Speech Recognition of Inflected Languages
</title>
<author confidence="0.921403">
Izhak Shafran and Keith Hall
</author>
<affiliation confidence="0.869211">
Center for Language and Speech Processing
</affiliation>
<address confidence="0.69131">
Johns Hopkins University
Baltimore, MD 21218
</address>
<email confidence="0.997289">
{zakshafran,keith hall}@jhu.edu
</email>
<sectionHeader confidence="0.995587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935157894737">
This paper presents a corrective model
for speech recognition of inflected lan-
guages. The model, based on a discrim-
inative framework, incorporates word n-
grams features as well as factored mor-
phological features, providing error reduc-
tion over the model based solely on word
n-gram features. Experiments on a large
vocabulary task, namely the Czech portion
of the MALACH corpus, demonstrate per-
formance gain of about 1.1–1.5% absolute
in word error rate, wherein morphologi-
cal features contribute about a third of the
improvement. A simple feature selection
mechanism based on x2 statistics is shown
to be effective in reducing the number of
features by about 70% without any loss in
performance, making it feasible to explore
yet larger feature spaces.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948851851852">
N-gram models have long been the stronghold of
statistical language modeling approaches. Within
the n-gram paradigm, straightforward approaches
for increasing accuracy include using larger train-
ing sets and augmenting the contextual informa-
tion within the n-gram window. Incorporating
syntactic features into the context has been at the
forefront of recent research (Collins et al., 2005;
Rosenfeld et al., 2001; Chelba and Jelinek, 2000;
Hall and Johnson, 2004). However, much of the
previous work has focused on English language
syntax. This paper addresses syntax as captured
by the inflectional morphology of highly inflected
language.
High inflection in a language is generally cor-
related with some level of word-order flexibil-
ity. Morphological features either directly identify
or help disambiguate the syntactic participants of
a sentence. Inflectional morphology works as a
proxy for structured syntax in a language. Model-
ing morphological features in these languages not
only provides an additional source of information
but can also alleviate data sparsity problems.
Czech speech recognition needs to deal with
two sources of errors which are absent in En-
glish, namely, the inflectional morphology and the
differences in the formal (written) and colloquial
(spoken) forms. Table 1 presents an example out-
put of our speech recognizer on an utterance from
a Holocaust survivor, who is recounting General
Romel’s desert campaign during the Second World
War. In this example, the feminine past-tense
form of the Czech verb for to be is chosen mis-
takenly, which is followed by a sequence of in-
correct words chosen primarily to maintain agree-
ment with the feminine form of the verb. This is
an example of what we refer to as the morpho-
logical grouping effect. When the acoustic model
prefers a word with an incorrect inflection, the lan-
guage model effectively propagates the error to
later words. A language model based on word-
forms prefers sequences observed in the training
data, which will implicitly force an agreement
with the inflections of preceding words, making it
difficult to stop propagating errors. Although this
analysis is anecdotal in nature, the grouping effect
appears to be prevalent in the Czech dataset used
in this work. The proposed corrective model with
morphological features is expected to alleviate the
grouping effect as well as to improve the recogni-
tion of inflected languages in general.
In the following section, we present a brief
review of related work on morphological lan-
guage modeling and discriminative language mod-
</bodyText>
<page confidence="0.972012">
390
</page>
<note confidence="0.996925375">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 390–398,
Sydney, July 2006. c�2006 Association for Computational Linguistics
REF no Jeˇziˇs to uˇz byl Romel hnedle pied Alexandrii
gloss well Jesus by that time already was Romel just in front of Alexandria
translation oh Jesus, Romel was already just in front of Alexandria by that time
HYP no Jeˇziˇs to uˇz byla sama hned lepˇsi Alexandrie
gloss well Jesus by that time already (she) was herself just better Alexandria
translation oh Jesus, she was herself just better Alexandria by that time
</note>
<tableCaption confidence="0.987333">
Table 1: An example of the grouping effect. The incorrect form of the verb to be begins a group of
</tableCaption>
<bodyText confidence="0.978809875">
incorrect words in the hypothesis, but these words agree in their morphological inflection.
els. We begin the description of our work in sec-
tion 3 with the type of morphological features
modeled as well as their computation from the out-
put word-lattices of a speech recognizer. Section 4
presents the corrective model and the training ap-
proach explored in the current work. A simple and
effective feature selection mechanism is described
in section 5. In section 6, the proposed framework
is evaluated on a large vocabulary Czech speech
recognition task. Results show that the morpho-
logical features provide a significant improvement
over models lacking these features; subsequently,
two different analyses are provided to understand
the contribution of different morphological fea-
tures.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999899782608696">
It has long been assumed that incorporating mor-
phological features into a language models should
help improve the performance of speech recogni-
tion systems. Early models for German showed
little improvements over bigram language mod-
els and almost no improvement over trigram mod-
els (Geutner, 1995). More recently, morphology-
based models have been shown to help reduce er-
ror rate for out-of-vocabulary words (Carki et al.,
2000; Podvesky and Machek, 2005).
Much of the early work on morphological lan-
guage modeling was focused on utilizing compos-
ite morphological tags, largely due to the difficulty
in teasing apart the intricate interdependencies of
the morphological features. Apart from a few ex-
ceptions, there has been little work done in explor-
ing the morphological systems of highly inflected
languages.
Kirchhoff and colleagues (2004) successfully
incorporated morphological features for Arabic
using a factored language model. In their ap-
proach, morphological inflections are modeled in
a generative framework, and the space of factored
morphological tags is explored using a genetic al-
gorithm.
Adopting a different tactic, Choueiter and
colleagues (2006) exploited morphological con-
straints to prune illegal morpheme sequences from
ASR output. They noticed that the gains obtained
from the application of such constraints in Arabic
depends on the size of the vocabulary – an absolute
gain of 2.4% in word error rate (WER) reduced
to 0.2% when the size was increased from 64k to
800k.
Our approach to modeling morphology differs
from that of Vergyri et al. (2004) and Choueiter et
al. (2006). By choosing a discriminative frame-
work and maximum entropy based estimation, we
allow arbitrary features or constraints and their
combinations without the need for explicit elab-
oration of the factored space and its backoff ar-
chitecture. Thus, morphological features can be
incorporated in the absence of knowledge about
their interdependencies.
Several researchers have investigated tech-
niques for improving automatic speech recogni-
tion (ASR) results by modeling the errors (Collins
et al., 2005; Shafran and Byrne, 2004). Collins
et al. (2005) present a corrective language model
based on a discriminative framework. Initially, a
set of hypotheses is generated by a baseline de-
coder with standard acoustic and language models.
A corrective model is estimated such that it scores
desired or oracle hypotheses higher than compet-
ing hypotheses. The parameters are learned via
the perceptron algorithm which shifts weight away
from features associated with poor hypotheses and
towards those associated with better hypotheses.
By the appropriate choice of desired hypotheses,
the model parameters can be estimated to mini-
mize WER in speech recognition. During decod-
ing, the model can then be used to rerank a set
of hypotheses, and hence, it is also known as a
reranking framework. This paradigm allows mod-
eling arbitrary input features, even syntactic fea-
tures obtained from a parser. We adopt a vari-
ant of this framework where the corrective model
is based on a conditional model estimated by the
maximum entropy procedure (Charniak and John-
</bodyText>
<page confidence="0.997703">
391
</page>
<bodyText confidence="0.998340666666667">
son, 2005) and we investigate its effectiveness in
modeling morphological features for highly in-
flected languages, in particular, Czech.
</bodyText>
<sectionHeader confidence="0.998105" genericHeader="method">
3 Inflectional Morphology
</sectionHeader>
<bodyText confidence="0.999989625">
Inflectional abundance in a language generally
corresponds to some flexibility in word order. In
a free word-order language, the order of senten-
tial participants is relatively unconstrained. This
does not mean a speaker of the language can ar-
bitrarily choose an order. Word-order choice may
change the semantic and/or pragmatic interpreta-
tion of an utterance. Czech is known as a free
word-order language allowing for subject, object,
and verbal components to come in any order. Mor-
phological inflection in these languages must in-
clude a syntactic case marker to allow the determi-
nation of which participants are subjects (nomina-
tive case), objects (accusative or dative) and other
such entities. Additionally, morphological inflec-
tion encodes features such as gender and number.
The agreement of these features between senten-
tial components (adjectives with nouns, subjects
with verbs, etc.) may further disambiguate the tar-
get of a modifier (e.g., identifying the noun that is
modified by a particular adjective).
The increased flexibility in word order aggra-
vates the data sparsity of standard n-gram lan-
guage model for two reasons: first, the number of
valid configurations of a group of words increases
with the free order; and second, lexical items are
decorated with the inflectional morphemes, multi-
plying the number of word-forms that appear.
In addition to modeling sequences of word-
forms, we model sequences of morphologically
reduced lemmas, sequence of morphological tags
and sequences of various factored representations
of the morphological tags. Factoring a word
into the semantics-bearing lemma and syntax-
bearing morphological tag alleviates the data spar-
sity problem to some extent. However, the number
of possible factorizations of n-grams is large. The
approach adopted in this work is to provide a rich
class of features and defer the modeling of their
interaction to the learning procedure.
</bodyText>
<subsectionHeader confidence="0.999503">
3.1 Extracting Morphological Features
</subsectionHeader>
<bodyText confidence="0.99993275">
The extraction of reliable morphological features
critically effects further morphological modeling.
Here, we first select the most likely morphologi-
cal analysis for each word using a morphological
</bodyText>
<table confidence="0.999314857142857">
Label Description # Values
lemma Reduced lexeme &lt; Ivocabl
POS Coarse part-of-speech 12
D-POS Detailed part-of-speech 65
gen Grammatical Gender 10
num Grammatical Number 5
case Grammatical Case 8
</table>
<tableCaption confidence="0.98737">
Table 2: Czech morphological features used in the
</tableCaption>
<bodyText confidence="0.974706580645161">
current work. The # Values field indicates the size
of the closed set of possible values. Not all values
are used in the annotated data.
tagger. In particular, we use the Czech feature-
based tagger distributed with the Prague Depen-
dency Treebank (Hajiˇc et al., 2005). The tagger is
based on a morphological analyzer which uses a
lexicon and a rule-based tag guesser for words not
found in the lexicon. Trained by the maximum en-
tropy procedure, the tagger uses left and right con-
textual features from the input string. Currently,
this is the best available Czech-language tagger.
See Hajiˇc and Vidov´a-Hladk´a (1998) for further
details on the tagger.
A disadvantage of such an approach is that
the tagger works on strings rather than the word-
lattices that we expect from an ASR system.
Therefore, we must extract a set of strings from the
lattices prior to tagging. An alternative approach is
to hypothesize all morphological analyses for each
word in the lattice, thereby considering the entire
set of analyses as features in the model. In the cur-
rent implementation we have chosen to use a tag-
ger to reduce the complexity of the model by lim-
iting the number of active features while still ob-
taining relatively reliable features. Moreover, sys-
tematic errors in tagging can be potentially com-
pensated by the corrective model.
The initial stage of feature extraction begins
with an analysis of the data on which we train and
test our models. The process follows:
</bodyText>
<listItem confidence="0.95447325">
1. Extract the n-best hypotheses according to a
baseline model, where n varies from 50 to
1000 in the current work.
2. Tag each of the hypotheses with the morpho-
logical tagger.
3. Re-encode the original word strings along
with their tagged morphological analysis in
a weighted finite state transducer to allow
</listItem>
<page confidence="0.978444">
392
</page>
<table confidence="0.66659625">
Word-form to obdobi bylo pomˇern´e kr´atk´e
gloss that period was relatively short
lemma ten obdobi b´yt pomˇernˇe kr´atk´y
tag PDNS1 NNNS1 VpNS- Dg— AAFS2
</table>
<tableCaption confidence="0.991469">
Table 3: A morphological analysis of Czech. This analyses was generated by the Hajiˇc tagger.
</tableCaption>
<table confidence="0.999441071428571">
form to obdobi bylo pomˇern´e kr´atk´e
to obdobi obdobibylo bylo pomˇern´e pomˇern´e kr´atk´e
lemma ten obdobi b´yt pomˇernˇe kr´atk´y
ten obdobi obdobib´yt b´yt pomˇernˇe pomˇernˇe kr´atk´y
tag PDNS1 NNNS1 VpNS- Dg— AAFS2
PDNS1 NNNS1 NNNS1 VpNS- VpNS- Dg— Dg— AAFS2
POS P N V D A
P N N V V D D A
. . . 1 1 . . . - 2
case 1 1 1 - - - 2
- 0
num/case S1 S1 S- – S2
S1 S1 S1 S- S- – – S2
. . . . . .
</table>
<tableCaption confidence="0.979213">
Table 4: Examples of the n-grams extracted from the Czech sentence To obdobibylo pomˇernˇe kr´atk´e. A
</tableCaption>
<bodyText confidence="0.994077416666667">
subset of the feature classes is presented here. The morphological feature values are those assigned by
the Hajiˇc tagger.
an efficient means of projecting the hypothe-
ses from word-form to morphology and vice
versa.
4. Extract appropriately factored n-gram fea-
tures for each hypothesis as described below.
Each word state in the original lattice has an
associated lemma/tag from which a variety of n-
gram features can be extracted.
From the morphological features assigned by
the tagger, we chose to retain only a subset and dis-
card the less reliable features which are semantic
in nature. The basic morphological features used
are detailed in Table 2. In the tag-based model, a
string of 5 characters representing the 5 morpho-
logical fields is used as a unique identifier. The
derived features include n-grams of POS, D-POS,
gender (gen), number (num), and case features as
well as their combinations.
POS, D-POS Captures the sub-categorization of
the part-of-speech tags.
gen, num Captures complex gender-number
agreement features.
num, case Captures number agreement between
specific case markers.
POS, case Captures associated POS/Case fea-
tures (e.g., adjectives associated with nomi-
native elements).
The paired features allow for complex inflec-
tional interactions and are less sparse than the
composite 5-component morphological tags. Ad-
ditionally, the morphologically reduced lemma
and n-grams of lemmas are used as features in the
models.
Table 3 presents a morphological analysis of the
Czech sentence To obdobibylo pomˇernˇe kr´atk´e.
The encoded tags represent the first 5 fields of the
Prague Dependency Treebank morphological en-
coding and correspond to the last 5 rows of Ta-
ble 2. Features for this sentence include the word-
form, lemma, and composite tag features as well
as the components of each tag and the above men-
tioned concatenation of tag fields. Additionally,
n-grams of each of these features are included. Bi-
gram features extracted from an example sentence
are illustrated in Table 4.
The following section describes how the fea-
</bodyText>
<page confidence="0.996617">
393
</page>
<bodyText confidence="0.99179">
tures extracted above are modeled in a discrimi-
native framework to reduce word error rate.
</bodyText>
<sectionHeader confidence="0.989585" genericHeader="method">
4 Corrective Model and Estimation
</sectionHeader>
<bodyText confidence="0.999981454545455">
In this work, we adopt the reranking framework
of Charniak and Johnson (2005) for incorporating
morphological features. The model scores each
test hypothesis y using a linear function, vθ(y), of
features extracted from the hypothesis fj(y) and
model parameters θj, i.e., vθ(y) = Ej θjfj(y).
The hypothesis with the highest score is then cho-
sen as the output.
The model parameters, θ, are learned from a
training set by maximum entropy estimation of the
following conditional model:
</bodyText>
<equation confidence="0.997036">
H E Pθ(yi|Ys)
s yiEY3:g(yi)=maxjg(yj)
</equation>
<bodyText confidence="0.999900679245283">
Here, Ys = {yj} is the set of hypotheses for each
training utterance s and the function g returns an
extrinsic evaluation score, which in our case is
the WER of the hypothesis. Pθ(yi|Ys) is modeled
by a maximum entropy distribution of the form,
Pθ(yi|Ys) = expvθ(yi)/Ej expvθ(yj). This
choice simplifies the numerical estimation proce-
dure since the gradient of the log-likelihood with
respect to a parameter, say θj, reduces to differ-
ence in expected counts of the associated feature,
Eθ[fj|Ys] − Eθ[fj|yi E Ys : g(yi) = maxjg(yj)].
To allow good generalization properties, a Gaus-
sian regularization term is also included in the cost
function.
A set of hypotheses Ys is generated for each
training utterance using a baseline ASR system.
Care is taken to reduce the bias in decoding the
training set by following a jack-knife procedure.
The training set is divided into 20 subsets and each
subset is decoded after excluding the transcripts
of that subset from the language model of the de-
coder.
The model allows the exploration of a large fea-
ture space, including n-grams of words, morpho-
logical tags, and factored tags. In a large vocab-
ulary system, this could be an enormous space.
However, in a discriminative maximum entropy
framework, only the observed features are consid-
ered. Among the observed features, those associ-
ated with words that are correct in all hypotheses
do not provide any additional discrimination ca-
pability. Mathematically, the gradient of the log-
likelihood with respect to the parameters of these
features tends to zero and they may be discarded.
Additionally, the parameters associated with fea-
tures that are rarely observed in the training set are
difficult to learn reliably and may be discarded.
To avoid redundant features, we focus on words
which are frequently incorrect; this is the error re-
gion we aim to model. In the training utterance,
the error regions of a hypothesis are identified us-
ing the alignment corresponding to the minimum
edit distance from the reference, akin to comput-
ing word error rate. To mark all the error regions in
an ASR lattice, the minimum edit distance align-
ment is obtained using equivalent finite state ma-
chine operations (Mohri, 2002). From amongst all
the error regions in the training lattices, the most
frequent 12k words in error are shortlisted. Fea-
tures are computed in the corrective model only if
they involve words for the shortlist. The parame-
ters, θ, are estimated by numerical optimization as
in (Charniak and Johnson, 2005).
</bodyText>
<sectionHeader confidence="0.993775" genericHeader="method">
5 Feature Selection
</sectionHeader>
<bodyText confidence="0.999881944444444">
The space of features spanned by the cross-
product space of words, lemmas, tags, factored-
tags and their n-gram can potentially be over-
whelming. However, not all of these features
are equally important and many of the features
may not have a significant impact on the word
error rate. The maximum entropy framework af-
fords the luxury of discarding such irrelevant fea-
tures without much bookkeeping, unlike maxi-
mum likelihood models. In the context of mod-
eling morphological features, we investigate the
efficacy of simple feature selection based on the
χ2 statistics, which has been shown to effective
in certain text categorization problems. e.g. (Yang
and Pedersen, 1997).
The χ2 statistics measures the lack of indepen-
dence by computing the deviation of the observed
counts Oi from the expected counts Ei.
</bodyText>
<equation confidence="0.995162">
X2 �� (Oi − Ei)2/Ei
i
</equation>
<bodyText confidence="0.99964375">
In our case, there are two classes – oracle hy-
potheses c and competing hypotheses c. The
expected count is the count marginalized over
classes.
</bodyText>
<equation confidence="0.998163083333333">
X2(f, c) _ (P(f, c) − P(f))2 + (P(f, �c) − P(f))2
P(f) P(f)
+ (P(
f)
f,�c) − P( f))2
P(
f)
f,c)
−
P(
f))2
P( + (P(
</equation>
<page confidence="0.993358">
394
</page>
<bodyText confidence="0.999788277777778">
This can be simplified using a two-way contin-
gency table of feature and class, where A is the
number of times f and c co-occur, B is the num-
ber of times f occurs without c, C is the number
of times c occurs without f, and D is the number
of times neither f nor c occurs, and N is the total
number of examples. Then, the x2 is defined to
be:
The x2 statistics are computed for all the fea-
tures and the features with larger value are re-
tained. Alternatives feature selection mechanisms
such as those based on mutual information and in-
formation gain are less reliable than x2 statistics
for heavy-tailed distributions. More complex fea-
ture selection mechanism would entail computing
higher order interaction between features which is
computationally expensive and so is not explored
in this work.
</bodyText>
<sectionHeader confidence="0.99592" genericHeader="method">
6 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.9998916">
The corrective model presented in this work is
evaluated on a large vocabulary task consisting
of spontaneous spoken testimonies in Czech lan-
guage, which is a subset of the multilingual
MALACH corpus (Psutka et al., 2003).
</bodyText>
<subsectionHeader confidence="0.982786">
6.1 Task
</subsectionHeader>
<bodyText confidence="0.9999495">
For acoustic model training, transcripts are avail-
able for about 62 hours of speech from 336 speak-
ers, amounting to 507k spoken words from a vo-
cabulary of 79k. A portion of this data containing
speech from 44 speakers, about 21k words in all
is treated as development set (dev). The test set
(eval) consists of about 2 hours of speech from 10
new speakers and contains about 15k words.
</bodyText>
<subsectionHeader confidence="0.999545">
6.2 Baseline ASR System
</subsectionHeader>
<bodyText confidence="0.999950755555555">
The baseline ASR system uses perceptual linear
prediction (PLP) features which is computed on
44KHz input speech at the rate of 10 frames per
second, and is normalized to have zero mean and
unit variance per speaker. The acoustic models are
made of 3-state HMM triphones, whose observa-
tion distributions are clustered into about 4500 al-
lophonic (triphone) states. Each state is modeled
by a 16 component Gaussian mixture with diag-
onal covariances. The parameters of the acoustic
models are initially estimated by maximum likeli-
hood and then refined by five iterations of maxi-
mum mutual information estimation (MMI).
Unlike other comparable corpora, this corpus
contains a relatively high percentage of colloquial
words – about 9% of the vocabulary and 7% of the
tokens. For the sake of downstream application,
the colloquial variants are subsumed in the lexi-
con. As a result, common words contain several
pronunciation variants, and a few have as many as
14 variants.
For the first pass decoding, a language model
was created by interpolating the in-domain model
(weight=0.75), estimated from 600k words of
transcripts with an out-of-domain model, esti-
mated from 15M words of Czech National Cor-
pus (Psutka et al., 2003). Both models are param-
eterized by a trigram language model with Katz
back-off. The decoding graph was built by com-
posing the language model, the lexical transducer
and the context-dependent transducer (phones to
triphones) into a single compact finite state ma-
chine.
The baseline ASR system decodes test utter-
ance in two passes. A first pass decoding is per-
formed with MMIE acoustic models, whose out-
put transcripts are bootstrapped to estimate two
maximum likelihood linear regression transforms
for each speaker using five iterations. A second
pass decoding is then performed with the new
speaker adapted acoustic models. The resulting
performance is given in Table 5. The performance
reflects the difficulty of transcribing spontaneous
speech from the elderly speakers whose speech is
also heavily accented and emotional in this corpus.
</bodyText>
<table confidence="0.997822333333333">
1-best 1000-best
Dev 29.9 21.5
Eval 35.9 22.4
</table>
<tableCaption confidence="0.65093625">
Table 5: The performance of the baseline ASR
system is reported, showing the word error rate
of 1-best MAP hypothesis and the oracle in 1000-
best hypotheses for dev and eval sets.
</tableCaption>
<subsectionHeader confidence="0.995585">
6.3 Experiments With Morphology
</subsectionHeader>
<bodyText confidence="0.9999464">
We present a set of contrastive experiments to
gauge the performance of the corrective models
and the contribution of morphological features.
For training the corrective models, 50 best hy-
potheses are generated for each utterance using the
</bodyText>
<equation confidence="0.868314333333333">
N xχ2 (f, c) _ ( )
(A + C) x (B + D)x (A BB) x (C + D)
2
</equation>
<page confidence="0.994897">
395
</page>
<figure confidence="0.918927">
(a)Devel (b)Eval
</figure>
<figureCaption confidence="0.9715925">
Figure 1: Feature selection via x2 statistics helps reduce the number of parameters by 70% without any
loss in performance, as observed in dev (a) and eval (b) sets.
</figureCaption>
<bodyText confidence="0.9963544">
jack-knife procedure mentioned earlier. For each
hypothesis, bigram and unigram features are com-
puted which consist of word-forms, lemmas, mor-
phologoical tags, factored morphological tags, and
the likelihood from the baseline ASR system. For
testing, the baseline ASR system is used to gener-
ate 1000 best hypotheses for each utterance. These
are then evaluated using the corrective models and
the best scored hypothesis is chosen as the output.
Table 6 summarizes the results on two test sets
– the dev and the eval set. A corrective model with
word bigram features improve the word error rate
by about an absolute 1% over the baseline. Mor-
phological features provide a further gain on both
the test sets consistently.
</bodyText>
<table confidence="0.99431125">
Features Dev Eval
Baseline 29.9 35.9
Word bigram 29.0 34.8
+ Morph bigram 28.7 34.4
</table>
<tableCaption confidence="0.987992">
Table 6: The word error rate of the corrective
</tableCaption>
<bodyText confidence="0.994355346153846">
model is compared with that of the baseline ASR
system, illustrating the improvement in perfor-
mance with morphological features.
The gains on the dev set are significant at the
level of p &lt; 0.001 for three standard NIST tests,
namely, matched pair sentence segment, signed
pair comparison, and Wilcoxon signed rank tests.
For the smaller eval set the significant levels were
lower for morphological features. The relative
gains observed are consistent over a variety of con-
ditions that we have tested including the ones re-
ported below.
Subsequently, we investigated the impact of re-
ducing the number of features using x2 statistics,
as described in section 5. The experiments with
bigram features of word-forms and morphology
were repeated using reduced feature sets, and the
performance was measured at 10%, 30% and 60%
of their original features. The results, as illustrated
in Figure 1, show that the word error rate does not
change significantly even after the number of fea-
tures are reduced by 70%. We have also observed
that most of the gain can be achieved by evalu-
ating 200 best hypotheses from the baseline ASR
system, which could further reduce the computa-
tional cost for time-sensitive applications.
</bodyText>
<subsectionHeader confidence="0.999587">
6.4 Analysis of Feature Classes
</subsectionHeader>
<bodyText confidence="0.999942285714286">
The impact of feature classes can be analyzed by
excluding all features from a particular class and
evaluating the performance of the resulting model
without re-estimation. Figure 2 illustrates the ef-
fectiveness of different features class. The y-axis
shows the gain in F-score, which is monotonic
with the word error rate, on the entire develop-
ment dataset. In this analysis, the likelihood score
from the baseline ASR system was omitted since
our interest is in understanding the effectiveness
of categorical features such as words, lemmas and
tags.
The most independently influential feature class
is the factored tag features. This corresponds with
</bodyText>
<page confidence="0.994485">
396
</page>
<figure confidence="0.980886">
0.005
0.004
0.003
0.002
0.001
0
-0.001
</figure>
<figureCaption confidence="0.999602">
Figure 2: Analysis of features classes for a bigram
</figureCaption>
<bodyText confidence="0.9452696">
form, lemma, tag, and factored tag model. Y-axis
is the contribution of this feature if added to an
otherwise complete model. Feature classes are la-
beled: TNG – tag n-gram, LNG – lemma n-gram,
FNG – form n-gram and TFAC – factored tag n-
grams. The number following the # represents the
order of the n-gram.
our belief that modeling morphological features
requires detailed models of the morphology; in
this model the composite morphological tag n-
gram features (TNG) offer little contribution in the
presence of the factored features.
Analysis of feature reduction by the x2 statistics
reveals a similar story. When features are ranked
according to their x2 statistics, about 57% of the
factored tag n-grams occur in the top 10% while
only 7% of the word n-grams make it. The lemma
and composite tag n-grams give about 6.2% and
19.2% respectively. Once again, the factored tag
is the most influential feature class.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999985459459459">
We have proposed a corrective modeling frame-
work for incorporating inflectional morphology
into a discriminative language model. Empirical
results on a difficult Czech speech recognition task
support our claim that morphology can help im-
prove speech recognition results for these types of
languages. Additionally, we present a feature se-
lection method that effectively reduces the model
size by about 70% while having little or no im-
pact on recognition accuracy. Model size reduc-
tion greatly reduces training time which can often
be prohibitively expensive for maximum entropy
training.
Analysis of the models learned on our task show
that factored morphological tags along with word-
forms provide most of the discriminative power;
and, in the presence of these features, composite
morphological tags are of little use.
The corrective model outlined here operates on
the word lattices produced by an ASR system. The
morphological tags are inferred from the word se-
quences in the lattice. Alternatively, by employ-
ing an ASR system that models the morphological
constraints in the acoustics as in (Chung and Sen-
eff, 1999), the corrective model could be applied
directly to a lattice with morphological tags.
When dealing with ASR word lattices, the ef-
ficacy of the proposed feature selection mecha-
nism can be exploited to eliminate the intermedi-
ate tagger, a potential source of errors. Instead of
considering the best morphological analysis, the
model could consider all possible analyses of the
words. Further, the feature space could be en-
riched with syntactic features which are known to
be useful (Collins et al., 2005). The task of mod-
eling is then tackled by feature selection and the
maximum entropy training procedure.
</bodyText>
<sectionHeader confidence="0.998386" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.995571428571429">
The authors would like to thank William Byrne for
discussions on modeling aspects, and Jan Hajiˇc,
Petr Nˇemec, and Vaclav Nov´ak for discussions
regarding Czech morphology and tagging. This
work was supported by the NSF (U.S.A) under the
Information Technology Research (ITR) program,
NSF IIS Award No. 0122466.
</bodyText>
<sectionHeader confidence="0.999119" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9975995">
Kenan Carki, Petra Geutner, and Tanja Schultz. 2000.
Turkish LVCSR: towards better speech recognition
for agglutinative languages. In Proceedings of the
2000 IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 3688–3691.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling. Computer Speech and
Language, 14(4):283–332.
Ghinwa Choueiter, Daniel Povey, Stanley Chen, and
Geoffrey Zweig. 2006. Morpheme-based language
modeling for Arabic LVCSR. In Proceedings of the
2006 IEEE International Conference on Acoustics,
Speech, and Signal Processing, Toulouse, France.
Grace Chung and Stephanie Seneff. 1999. A hierar-
chical duration model for speech recognition based
</reference>
<figure confidence="0.7581335">
TNG#1 LNG#2 FNG#2 TFAC#1 LNG#1 FNG#1 TFAC#2
TNG#2
</figure>
<page confidence="0.970006">
397
</page>
<reference confidence="0.999803142857143">
on the ANGIE framework. Speech Communication,
27:113–134.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling
for speech recognition. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL’05), pages 507–514, Ann
Arbor, Michigan, June. Association for Computa-
tional Linguistics.
Petra Geutner. 1995. Using morphology towards bet-
ter large-vocabulary speech recognition systems. In
Proceedings of the 1995 IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
pages 445–448, Detroit, MI.
Jan Hajiˇc and Barbora Vidov´a-Hladk´a. 1998. Tagging
inflective languages: Prediction of morphological
categories for a rich, structured tagset. In Proceed-
ings of the COLING-ACL Conference, pages 483–
490, Montreal, Canada.
Jan Hajiˇc, Eva Hajiˇcov´a, Petr Pajas, Jarmila
Panevov´a, Petr Sgall, and Barbora Vidov´a Hladk´a.
2005. The prague dependency treebank 2.0.
http://ufal.mff.cuni.cz/pdt2.0.
Keith Hall and Mark Johnson. 2004. Attention shifting
for parsing speech. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics, pages 41–47, Barcelona.
Mehryar Mohri. 2002. Edit-distance of weighted
automata. In Proceedings of the 7th Interna-
tional Conference on Implementation and Applica-
tion ofAutomata, Jean-Marc Champarnaud andDe-
nis Maurel, Eds.
Petr Podvesky and Pavel Machek. 2005. Speech
recognition of Czech—inclusion of rare words
helps. In Proceedings of the ACL Student Research
Workshop, pages 121–126, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Josef Psutka, Pavel Ircing, Josef V. Psutka, Vlasta
Radovic, William Byrne, Jan Hajiˇc, Jiri Mirovsky,
and Samuel Gustman. 2003. Large vocabulary ASR
for spontaneous Czech in the MALACH project.
In Proceedings of the 8th European Conference on
Speech Communication and Technology, Geneva,
Switzerland.
Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: a vehicle for linguistic-statistical integration.
Computers Speech and Language, 15(1).
Izhak Shafran and William Byrne. 2004. Task-specific
minimum Bayes-risk decoding using learned edit
distance. In Proceedings of the 7th International
Conference on Spoken Language Processing, vol-
ume 3, pages 1945–48, Jeju Islands, Korea.
Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and An-
dreas Stolcke. 2004. Morphology-based language
modeling for arabic speech recognition. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing (ICSLP/Interspeech 2004).
Yiming Yang and Jan 0. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the 14th International Conference
on Machine Learning, pages 412 – 420, San Fran-
cisco, CA, USA.
</reference>
<page confidence="0.998275">
398
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.605228">
<title confidence="0.999866">Corrective Models for Speech Recognition of Inflected Languages</title>
<author confidence="0.993758">Shafran</author>
<affiliation confidence="0.8100045">Center for Language and Speech Johns Hopkins</affiliation>
<address confidence="0.97226">Baltimore, MD</address>
<abstract confidence="0.9996582">This paper presents a corrective model for speech recognition of inflected languages. The model, based on a discriminative framework, incorporates word ngrams features as well as factored morphological features, providing error reduction over the model based solely on word n-gram features. Experiments on a large vocabulary task, namely the Czech portion of the MALACH corpus, demonstrate performance gain of about 1.1–1.5% absolute in word error rate, wherein morphological features contribute about a third of the improvement. A simple feature selection based on statistics is shown to be effective in reducing the number of features by about 70% without any loss in performance, making it feasible to explore yet larger feature spaces.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenan Carki</author>
<author>Petra Geutner</author>
<author>Tanja Schultz</author>
</authors>
<title>Turkish LVCSR: towards better speech recognition for agglutinative languages.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>3688--3691</pages>
<contexts>
<context position="5483" citStr="Carki et al., 2000" startWordPosition="855" endWordPosition="858">ignificant improvement over models lacking these features; subsequently, two different analyses are provided to understand the contribution of different morphological features. 2 Related Work It has long been assumed that incorporating morphological features into a language models should help improve the performance of speech recognition systems. Early models for German showed little improvements over bigram language models and almost no improvement over trigram models (Geutner, 1995). More recently, morphologybased models have been shown to help reduce error rate for out-of-vocabulary words (Carki et al., 2000; Podvesky and Machek, 2005). Much of the early work on morphological language modeling was focused on utilizing composite morphological tags, largely due to the difficulty in teasing apart the intricate interdependencies of the morphological features. Apart from a few exceptions, there has been little work done in exploring the morphological systems of highly inflected languages. Kirchhoff and colleagues (2004) successfully incorporated morphological features for Arabic using a factored language model. In their approach, morphological inflections are modeled in a generative framework, and the</context>
</contexts>
<marker>Carki, Geutner, Schultz, 2000</marker>
<rawString>Kenan Carki, Petra Geutner, and Tanja Schultz. 2000. Turkish LVCSR: towards better speech recognition for agglutinative languages. In Proceedings of the 2000 IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 3688–3691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="15541" citStr="Charniak and Johnson (2005)" startWordPosition="2469" endWordPosition="2472">logical encoding and correspond to the last 5 rows of Table 2. Features for this sentence include the wordform, lemma, and composite tag features as well as the components of each tag and the above mentioned concatenation of tag fields. Additionally, n-grams of each of these features are included. Bigram features extracted from an example sentence are illustrated in Table 4. The following section describes how the fea393 tures extracted above are modeled in a discriminative framework to reduce word error rate. 4 Corrective Model and Estimation In this work, we adopt the reranking framework of Charniak and Johnson (2005) for incorporating morphological features. The model scores each test hypothesis y using a linear function, vθ(y), of features extracted from the hypothesis fj(y) and model parameters θj, i.e., vθ(y) = Ej θjfj(y). The hypothesis with the highest score is then chosen as the output. The model parameters, θ, are learned from a training set by maximum entropy estimation of the following conditional model: H E Pθ(yi|Ys) s yiEY3:g(yi)=maxjg(yj) Here, Ys = {yj} is the set of hypotheses for each training utterance s and the function g returns an extrinsic evaluation score, which in our case is the WER</context>
<context position="18484" citStr="Charniak and Johnson, 2005" startWordPosition="2953" endWordPosition="2956">he error regions of a hypothesis are identified using the alignment corresponding to the minimum edit distance from the reference, akin to computing word error rate. To mark all the error regions in an ASR lattice, the minimum edit distance alignment is obtained using equivalent finite state machine operations (Mohri, 2002). From amongst all the error regions in the training lattices, the most frequent 12k words in error are shortlisted. Features are computed in the corrective model only if they involve words for the shortlist. The parameters, θ, are estimated by numerical optimization as in (Charniak and Johnson, 2005). 5 Feature Selection The space of features spanned by the crossproduct space of words, lemmas, tags, factoredtags and their n-gram can potentially be overwhelming. However, not all of these features are equally important and many of the features may not have a significant impact on the word error rate. The maximum entropy framework affords the luxury of discarding such irrelevant features without much bookkeeping, unlike maximum likelihood models. In the context of modeling morphological features, we investigate the efficacy of simple feature selection based on the χ2 statistics, which has be</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="1426" citStr="Chelba and Jelinek, 2000" startWordPosition="209" endWordPosition="212">atistics is shown to be effective in reducing the number of features by about 70% without any loss in performance, making it feasible to explore yet larger feature spaces. 1 Introduction N-gram models have long been the stronghold of statistical language modeling approaches. Within the n-gram paradigm, straightforward approaches for increasing accuracy include using larger training sets and augmenting the contextual information within the n-gram window. Incorporating syntactic features into the context has been at the forefront of recent research (Collins et al., 2005; Rosenfeld et al., 2001; Chelba and Jelinek, 2000; Hall and Johnson, 2004). However, much of the previous work has focused on English language syntax. This paper addresses syntax as captured by the inflectional morphology of highly inflected language. High inflection in a language is generally correlated with some level of word-order flexibility. Morphological features either directly identify or help disambiguate the syntactic participants of a sentence. Inflectional morphology works as a proxy for structured syntax in a language. Modeling morphological features in these languages not only provides an additional source of information but ca</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ghinwa Choueiter</author>
<author>Daniel Povey</author>
<author>Stanley Chen</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Morpheme-based language modeling for Arabic LVCSR.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="6656" citStr="Choueiter et al. (2006)" startWordPosition="1037" endWordPosition="1040">ons are modeled in a generative framework, and the space of factored morphological tags is explored using a genetic algorithm. Adopting a different tactic, Choueiter and colleagues (2006) exploited morphological constraints to prune illegal morpheme sequences from ASR output. They noticed that the gains obtained from the application of such constraints in Arabic depends on the size of the vocabulary – an absolute gain of 2.4% in word error rate (WER) reduced to 0.2% when the size was increased from 64k to 800k. Our approach to modeling morphology differs from that of Vergyri et al. (2004) and Choueiter et al. (2006). By choosing a discriminative framework and maximum entropy based estimation, we allow arbitrary features or constraints and their combinations without the need for explicit elaboration of the factored space and its backoff architecture. Thus, morphological features can be incorporated in the absence of knowledge about their interdependencies. Several researchers have investigated techniques for improving automatic speech recognition (ASR) results by modeling the errors (Collins et al., 2005; Shafran and Byrne, 2004). Collins et al. (2005) present a corrective language model based on a discri</context>
</contexts>
<marker>Choueiter, Povey, Chen, Zweig, 2006</marker>
<rawString>Ghinwa Choueiter, Daniel Povey, Stanley Chen, and Geoffrey Zweig. 2006. Morpheme-based language modeling for Arabic LVCSR. In Proceedings of the 2006 IEEE International Conference on Acoustics, Speech, and Signal Processing, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Chung</author>
<author>Stephanie Seneff</author>
</authors>
<title>A hierarchical duration model for speech recognition based on the ANGIE framework.</title>
<date>1999</date>
<journal>Speech Communication,</journal>
<pages>27--113</pages>
<contexts>
<context position="28746" citStr="Chung and Seneff, 1999" startWordPosition="4659" endWordPosition="4663">uces training time which can often be prohibitively expensive for maximum entropy training. Analysis of the models learned on our task show that factored morphological tags along with wordforms provide most of the discriminative power; and, in the presence of these features, composite morphological tags are of little use. The corrective model outlined here operates on the word lattices produced by an ASR system. The morphological tags are inferred from the word sequences in the lattice. Alternatively, by employing an ASR system that models the morphological constraints in the acoustics as in (Chung and Seneff, 1999), the corrective model could be applied directly to a lattice with morphological tags. When dealing with ASR word lattices, the efficacy of the proposed feature selection mechanism can be exploited to eliminate the intermediate tagger, a potential source of errors. Instead of considering the best morphological analysis, the model could consider all possible analyses of the words. Further, the feature space could be enriched with syntactic features which are known to be useful (Collins et al., 2005). The task of modeling is then tackled by feature selection and the maximum entropy training proc</context>
</contexts>
<marker>Chung, Seneff, 1999</marker>
<rawString>Grace Chung and Stephanie Seneff. 1999. A hierarchical duration model for speech recognition based on the ANGIE framework. Speech Communication, 27:113–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
</authors>
<title>Discriminative syntactic language modeling for speech recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>507--514</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1376" citStr="Collins et al., 2005" startWordPosition="201" endWordPosition="204">ple feature selection mechanism based on x2 statistics is shown to be effective in reducing the number of features by about 70% without any loss in performance, making it feasible to explore yet larger feature spaces. 1 Introduction N-gram models have long been the stronghold of statistical language modeling approaches. Within the n-gram paradigm, straightforward approaches for increasing accuracy include using larger training sets and augmenting the contextual information within the n-gram window. Incorporating syntactic features into the context has been at the forefront of recent research (Collins et al., 2005; Rosenfeld et al., 2001; Chelba and Jelinek, 2000; Hall and Johnson, 2004). However, much of the previous work has focused on English language syntax. This paper addresses syntax as captured by the inflectional morphology of highly inflected language. High inflection in a language is generally correlated with some level of word-order flexibility. Morphological features either directly identify or help disambiguate the syntactic participants of a sentence. Inflectional morphology works as a proxy for structured syntax in a language. Modeling morphological features in these languages not only p</context>
<context position="7153" citStr="Collins et al., 2005" startWordPosition="1109" endWordPosition="1112">rom 64k to 800k. Our approach to modeling morphology differs from that of Vergyri et al. (2004) and Choueiter et al. (2006). By choosing a discriminative framework and maximum entropy based estimation, we allow arbitrary features or constraints and their combinations without the need for explicit elaboration of the factored space and its backoff architecture. Thus, morphological features can be incorporated in the absence of knowledge about their interdependencies. Several researchers have investigated techniques for improving automatic speech recognition (ASR) results by modeling the errors (Collins et al., 2005; Shafran and Byrne, 2004). Collins et al. (2005) present a corrective language model based on a discriminative framework. Initially, a set of hypotheses is generated by a baseline decoder with standard acoustic and language models. A corrective model is estimated such that it scores desired or oracle hypotheses higher than competing hypotheses. The parameters are learned via the perceptron algorithm which shifts weight away from features associated with poor hypotheses and towards those associated with better hypotheses. By the appropriate choice of desired hypotheses, the model parameters ca</context>
</contexts>
<marker>Collins, Roark, Saraclar, 2005</marker>
<rawString>Michael Collins, Brian Roark, and Murat Saraclar. 2005. Discriminative syntactic language modeling for speech recognition. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 507–514, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petra Geutner</author>
</authors>
<title>Using morphology towards better large-vocabulary speech recognition systems.</title>
<date>1995</date>
<booktitle>In Proceedings of the 1995 IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>445--448</pages>
<location>Detroit, MI.</location>
<contexts>
<context position="5354" citStr="Geutner, 1995" startWordPosition="836" endWordPosition="837">rk is evaluated on a large vocabulary Czech speech recognition task. Results show that the morphological features provide a significant improvement over models lacking these features; subsequently, two different analyses are provided to understand the contribution of different morphological features. 2 Related Work It has long been assumed that incorporating morphological features into a language models should help improve the performance of speech recognition systems. Early models for German showed little improvements over bigram language models and almost no improvement over trigram models (Geutner, 1995). More recently, morphologybased models have been shown to help reduce error rate for out-of-vocabulary words (Carki et al., 2000; Podvesky and Machek, 2005). Much of the early work on morphological language modeling was focused on utilizing composite morphological tags, largely due to the difficulty in teasing apart the intricate interdependencies of the morphological features. Apart from a few exceptions, there has been little work done in exploring the morphological systems of highly inflected languages. Kirchhoff and colleagues (2004) successfully incorporated morphological features for Ar</context>
</contexts>
<marker>Geutner, 1995</marker>
<rawString>Petra Geutner. 1995. Using morphology towards better large-vocabulary speech recognition systems. In Proceedings of the 1995 IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 445–448, Detroit, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Barbora Vidov´a-Hladk´a</author>
</authors>
<title>Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL Conference,</booktitle>
<pages>483--490</pages>
<location>Montreal, Canada.</location>
<marker>Hajiˇc, Vidov´a-Hladk´a, 1998</marker>
<rawString>Jan Hajiˇc and Barbora Vidov´a-Hladk´a. 1998. Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset. In Proceedings of the COLING-ACL Conference, pages 483– 490, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Eva Hajiˇcov´a, Petr Pajas, Jarmila Panevov´a, Petr Sgall, and Barbora Vidov´a Hladk´a.</title>
<date>2005</date>
<note>http://ufal.mff.cuni.cz/pdt2.0.</note>
<marker>Hajiˇc, 2005</marker>
<rawString>Jan Hajiˇc, Eva Hajiˇcov´a, Petr Pajas, Jarmila Panevov´a, Petr Sgall, and Barbora Vidov´a Hladk´a. 2005. The prague dependency treebank 2.0. http://ufal.mff.cuni.cz/pdt2.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Mark Johnson</author>
</authors>
<title>Attention shifting for parsing speech.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>41--47</pages>
<location>Barcelona.</location>
<contexts>
<context position="1451" citStr="Hall and Johnson, 2004" startWordPosition="213" endWordPosition="216">fective in reducing the number of features by about 70% without any loss in performance, making it feasible to explore yet larger feature spaces. 1 Introduction N-gram models have long been the stronghold of statistical language modeling approaches. Within the n-gram paradigm, straightforward approaches for increasing accuracy include using larger training sets and augmenting the contextual information within the n-gram window. Incorporating syntactic features into the context has been at the forefront of recent research (Collins et al., 2005; Rosenfeld et al., 2001; Chelba and Jelinek, 2000; Hall and Johnson, 2004). However, much of the previous work has focused on English language syntax. This paper addresses syntax as captured by the inflectional morphology of highly inflected language. High inflection in a language is generally correlated with some level of word-order flexibility. Morphological features either directly identify or help disambiguate the syntactic participants of a sentence. Inflectional morphology works as a proxy for structured syntax in a language. Modeling morphological features in these languages not only provides an additional source of information but can also alleviate data spa</context>
</contexts>
<marker>Hall, Johnson, 2004</marker>
<rawString>Keith Hall and Mark Johnson. 2004. Attention shifting for parsing speech. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 41–47, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Edit-distance of weighted automata.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Implementation and Application ofAutomata, Jean-Marc Champarnaud andDenis Maurel,</booktitle>
<location>Eds.</location>
<contexts>
<context position="18182" citStr="Mohri, 2002" startWordPosition="2905" endWordPosition="2906">he parameters associated with features that are rarely observed in the training set are difficult to learn reliably and may be discarded. To avoid redundant features, we focus on words which are frequently incorrect; this is the error region we aim to model. In the training utterance, the error regions of a hypothesis are identified using the alignment corresponding to the minimum edit distance from the reference, akin to computing word error rate. To mark all the error regions in an ASR lattice, the minimum edit distance alignment is obtained using equivalent finite state machine operations (Mohri, 2002). From amongst all the error regions in the training lattices, the most frequent 12k words in error are shortlisted. Features are computed in the corrective model only if they involve words for the shortlist. The parameters, θ, are estimated by numerical optimization as in (Charniak and Johnson, 2005). 5 Feature Selection The space of features spanned by the crossproduct space of words, lemmas, tags, factoredtags and their n-gram can potentially be overwhelming. However, not all of these features are equally important and many of the features may not have a significant impact on the word error</context>
</contexts>
<marker>Mohri, 2002</marker>
<rawString>Mehryar Mohri. 2002. Edit-distance of weighted automata. In Proceedings of the 7th International Conference on Implementation and Application ofAutomata, Jean-Marc Champarnaud andDenis Maurel, Eds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Podvesky</author>
<author>Pavel Machek</author>
</authors>
<title>Speech recognition of Czech—inclusion of rare words helps.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop,</booktitle>
<pages>121--126</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="5511" citStr="Podvesky and Machek, 2005" startWordPosition="859" endWordPosition="862">nt over models lacking these features; subsequently, two different analyses are provided to understand the contribution of different morphological features. 2 Related Work It has long been assumed that incorporating morphological features into a language models should help improve the performance of speech recognition systems. Early models for German showed little improvements over bigram language models and almost no improvement over trigram models (Geutner, 1995). More recently, morphologybased models have been shown to help reduce error rate for out-of-vocabulary words (Carki et al., 2000; Podvesky and Machek, 2005). Much of the early work on morphological language modeling was focused on utilizing composite morphological tags, largely due to the difficulty in teasing apart the intricate interdependencies of the morphological features. Apart from a few exceptions, there has been little work done in exploring the morphological systems of highly inflected languages. Kirchhoff and colleagues (2004) successfully incorporated morphological features for Arabic using a factored language model. In their approach, morphological inflections are modeled in a generative framework, and the space of factored morpholog</context>
</contexts>
<marker>Podvesky, Machek, 2005</marker>
<rawString>Petr Podvesky and Pavel Machek. 2005. Speech recognition of Czech—inclusion of rare words helps. In Proceedings of the ACL Student Research Workshop, pages 121–126, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Psutka</author>
<author>Pavel Ircing</author>
<author>Josef V Psutka</author>
<author>Vlasta Radovic</author>
<author>William Byrne</author>
<author>Jan Hajiˇc</author>
<author>Jiri Mirovsky</author>
<author>Samuel Gustman</author>
</authors>
<title>Large vocabulary ASR for spontaneous Czech in the MALACH project.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th European Conference on Speech Communication and Technology,</booktitle>
<location>Geneva, Switzerland.</location>
<marker>Psutka, Ircing, Psutka, Radovic, Byrne, Hajiˇc, Mirovsky, Gustman, 2003</marker>
<rawString>Josef Psutka, Pavel Ircing, Josef V. Psutka, Vlasta Radovic, William Byrne, Jan Hajiˇc, Jiri Mirovsky, and Samuel Gustman. 2003. Large vocabulary ASR for spontaneous Czech in the MALACH project. In Proceedings of the 8th European Conference on Speech Communication and Technology, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roni Rosenfeld</author>
<author>Stanley F Chen</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Whole-sentence exponential language models: a vehicle for linguistic-statistical integration.</title>
<date>2001</date>
<journal>Computers Speech and Language,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="1400" citStr="Rosenfeld et al., 2001" startWordPosition="205" endWordPosition="208">mechanism based on x2 statistics is shown to be effective in reducing the number of features by about 70% without any loss in performance, making it feasible to explore yet larger feature spaces. 1 Introduction N-gram models have long been the stronghold of statistical language modeling approaches. Within the n-gram paradigm, straightforward approaches for increasing accuracy include using larger training sets and augmenting the contextual information within the n-gram window. Incorporating syntactic features into the context has been at the forefront of recent research (Collins et al., 2005; Rosenfeld et al., 2001; Chelba and Jelinek, 2000; Hall and Johnson, 2004). However, much of the previous work has focused on English language syntax. This paper addresses syntax as captured by the inflectional morphology of highly inflected language. High inflection in a language is generally correlated with some level of word-order flexibility. Morphological features either directly identify or help disambiguate the syntactic participants of a sentence. Inflectional morphology works as a proxy for structured syntax in a language. Modeling morphological features in these languages not only provides an additional so</context>
</contexts>
<marker>Rosenfeld, Chen, Zhu, 2001</marker>
<rawString>Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001. Whole-sentence exponential language models: a vehicle for linguistic-statistical integration. Computers Speech and Language, 15(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Izhak Shafran</author>
<author>William Byrne</author>
</authors>
<title>Task-specific minimum Bayes-risk decoding using learned edit distance.</title>
<date>2004</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing,</booktitle>
<volume>3</volume>
<pages>1945--48</pages>
<location>Jeju Islands,</location>
<contexts>
<context position="7179" citStr="Shafran and Byrne, 2004" startWordPosition="1113" endWordPosition="1116">pproach to modeling morphology differs from that of Vergyri et al. (2004) and Choueiter et al. (2006). By choosing a discriminative framework and maximum entropy based estimation, we allow arbitrary features or constraints and their combinations without the need for explicit elaboration of the factored space and its backoff architecture. Thus, morphological features can be incorporated in the absence of knowledge about their interdependencies. Several researchers have investigated techniques for improving automatic speech recognition (ASR) results by modeling the errors (Collins et al., 2005; Shafran and Byrne, 2004). Collins et al. (2005) present a corrective language model based on a discriminative framework. Initially, a set of hypotheses is generated by a baseline decoder with standard acoustic and language models. A corrective model is estimated such that it scores desired or oracle hypotheses higher than competing hypotheses. The parameters are learned via the perceptron algorithm which shifts weight away from features associated with poor hypotheses and towards those associated with better hypotheses. By the appropriate choice of desired hypotheses, the model parameters can be estimated to minimize</context>
</contexts>
<marker>Shafran, Byrne, 2004</marker>
<rawString>Izhak Shafran and William Byrne. 2004. Task-specific minimum Bayes-risk decoding using learned edit distance. In Proceedings of the 7th International Conference on Spoken Language Processing, volume 3, pages 1945–48, Jeju Islands, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitra Vergyri</author>
<author>Katrin Kirchhoff</author>
<author>Kevin Duh</author>
<author>Andreas Stolcke</author>
</authors>
<title>Morphology-based language modeling for arabic speech recognition.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing</booktitle>
<contexts>
<context position="6628" citStr="Vergyri et al. (2004)" startWordPosition="1032" endWordPosition="1035">ch, morphological inflections are modeled in a generative framework, and the space of factored morphological tags is explored using a genetic algorithm. Adopting a different tactic, Choueiter and colleagues (2006) exploited morphological constraints to prune illegal morpheme sequences from ASR output. They noticed that the gains obtained from the application of such constraints in Arabic depends on the size of the vocabulary – an absolute gain of 2.4% in word error rate (WER) reduced to 0.2% when the size was increased from 64k to 800k. Our approach to modeling morphology differs from that of Vergyri et al. (2004) and Choueiter et al. (2006). By choosing a discriminative framework and maximum entropy based estimation, we allow arbitrary features or constraints and their combinations without the need for explicit elaboration of the factored space and its backoff architecture. Thus, morphological features can be incorporated in the absence of knowledge about their interdependencies. Several researchers have investigated techniques for improving automatic speech recognition (ASR) results by modeling the errors (Collins et al., 2005; Shafran and Byrne, 2004). Collins et al. (2005) present a corrective lang</context>
</contexts>
<marker>Vergyri, Kirchhoff, Duh, Stolcke, 2004</marker>
<rawString>Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and Andreas Stolcke. 2004. Morphology-based language modeling for arabic speech recognition. In Proceedings of the International Conference on Spoken Language Processing (ICSLP/Interspeech 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th International Conference on Machine Learning, pages 412 – 420,</booktitle>
<location>San Francisco, CA, USA.</location>
<marker>Yang, Jan, 1997</marker>
<rawString>Yiming Yang and Jan 0. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proceedings of the 14th International Conference on Machine Learning, pages 412 – 420, San Francisco, CA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>