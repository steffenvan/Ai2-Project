<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001070">
<title confidence="0.988223">
Using Large-scale Parser Output to Guide Grammar Development
</title>
<author confidence="0.942759">
Ascander Dost Tracy Holloway King
</author>
<affiliation confidence="0.896025">
Powerset, a Microsoft company Powerset, a Microsoft company
</affiliation>
<email confidence="0.987491">
adost@microsoft.com Tracy.King@microsoft.com
</email>
<sectionHeader confidence="0.993585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999831153846154">
This paper reports on guiding parser de-
velopment by extracting information from
output of a large-scale parser applied to
Wikipedia documents. Data-driven parser
improvement is especially important for
applications where the corpus may differ
from that originally used to develop the
core grammar and where efficiency con-
cerns affect whether a new construction
should be added, or existing analyses mod-
ified. The large size of the corpus in ques-
tion also brings scalability concerns to the
foreground.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998772">
Initial development of rule-based parsers1 is often
guided by the grammar writer’s knowledge of the
language and test suites that cover the “core” lin-
guistic phenomena of the language (Nerbonne et
al., 1988; Cooper et al., 1996; Lehmann et al.,
1996). Once the basic grammar is implemented,
including an appropriate lexicon, the direction of
grammar development becomes less clear. Inte-
gration of a grammar in a particular application
and the use of a particular corpus can guide gram-
mar development: the corpus and application will
require the implementation of specific construc-
tions and lexical items, as well as the reevalua-
tion of existing analyses. To streamline this sort
of output-driven development, tools to examine
parser output over large corpora are necessary, and
as corpus size increases, the efficiency and scal-
ability of those tools become crucial concerns.
Some immediate relevant questions for the gram-
mar writer include:
</bodyText>
<footnote confidence="0.899275">
1The techniques discussed here may also be relevant to
purely machine-learned parsers and are certainly applicable
to hybrid parsers.
</footnote>
<listItem confidence="0.956959333333333">
• What constructions and lexical items need to
be added for the application and corpus in
question?
• For any potential new construction or lexical
item, is it worth adding, or would it be better
to fall back to robust techniques?
• For existing analyses, are they applying cor-
rectly, or do they need to be restricted, or even
removed?
</listItem>
<bodyText confidence="0.9998239">
In the remainder of this section, we briefly dis-
cuss some existing techniques for guiding large-
scale grammar development and then introduce
the grammar being developed and the tool we use
in examining the grammar’s output. The remain-
der of the paper discusses development of lexical
resources and grammar rules, how overall progress
is tracked, and how analysis of the grammar output
can help development in other natural language
components.
</bodyText>
<subsectionHeader confidence="0.997583">
1.1 Current Techniques
</subsectionHeader>
<bodyText confidence="0.9998320625">
There are several techniques currently being used
by grammar engineers to guide large-scale gram-
mar development, including error mining to de-
tect gaps in grammar coverage, querying tools for
gold standard treebanks to determine frequency
of linguistic phenomena, and tools for querying
parser output to determine how linguistic phenom-
ena were analyzed in practice.
An error mining technique presented by van
Noord (2004) (henceforth: the van Noord Tool)
can reveal gaps in grammar coverage by compar-
ing the frequency of arbitrary n-grams of words
in unsuccessfully parsed sentences with the same
n-grams in unproblematic sentences, for large
unannotated corpora.2 A parser can be run over
new text, and a comparison of the in-domain and
</bodyText>
<footnote confidence="0.9944295">
2The suffix array error mining software is available at:
http://www.let.rug.nl/˜vannoord/SuffixArrays.tgz
</footnote>
<page confidence="0.984186">
63
</page>
<note confidence="0.99899">
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 63–70,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999851020408164">
out-of-domain sentences can determine, for in-
stance, that the grammar cannot parse adjective-
noun hyphenation correctly (e.g. an electrical-
switch cover). A different technique for error
mining that uses discriminative treebanking is de-
scribed in (Baldwin et al., 2005). This tech-
nique aims at determining issues with lexical cov-
erage, grammatical (rule) coverage, ungrammati-
cality within the corpus (e.g. misspelled words),
and extragrammaticality within the corpus (e.g.
bulleted lists).
A second approach involves querying gold-
standard treebanks such as the Penn Treebank
(Marcus et al., 1994) and Tiger Treebank (Brants
et al., 2004) to determine the frequency of cer-
tain phenomena. For example, Tiger Search (Lez-
ius, 2002) can be used to list and frequency-
sort stacked prepositions (e.g. up to the door) or
temporal noun/adverbs after prepositions (e.g. by
now). The search tools over these treebanks al-
low for complex searches involving specification
of lexical items, parts of speech, and tree config-
urations (see (Mirovsk´y, 2008) for discussion of
query requirements for searching tree and depen-
dency banks).
The third approach we discuss here differs from
querying gold-standard treebanks in that corpora
of actual parser output are queried to examine
how constructions are analyzed by the grammar.
For example, Bouma and Kloosterman (2002) use
XQuery (an XML query language) to mine parse
results stored as XML data.3 It is this sort of ex-
amination of parser output that is the focus of the
present paper, and specific examples of our expe-
riences follow in Section 2.2.
Use of such tools has proven vital to the devel-
opment of large-scale grammars. Based on our
experiences with them, we began extensively us-
ing a tool called Oceanography (Waterman, 2009)
to search parser output for very large (approxi-
mately 125 million sentence) parse runs stored on
a distributed file system. Oceanography queries
the parser output and returns counts of specific
constructions or properties, as well as the exam-
ple sentences they were extracted from. In the
subsequent sections we discuss how this tool (in
conjunction with existing ones like the van No-
ord Tool and Tiger Search) has enhanced grammar
development for an English-language Lexical-
</bodyText>
<footnote confidence="0.89364">
3See also (Bouma and Kloosterman, 2007) for further dis-
cussion of this technique.
</footnote>
<bodyText confidence="0.630032">
Functional Grammar used for a semantic search
application over Wikipedia.
</bodyText>
<subsectionHeader confidence="0.998706">
1.2 The Grammar and its Role
</subsectionHeader>
<bodyText confidence="0.971317621621621">
The grammar being developed is a Lexical-
Functional Grammar (LFG (Dalrymple, 2001))
that is part of the ParGram parallel grammar
project (Butt et al., 1999; Butt et al., 2002). It runs
on the XLE system (Crouch et al., 2009) and pro-
duces c(onstituent)-structures which are trees and
f(unctional)-structures which are attribute value
matrices recording grammatical functions and
other syntactic features such as tense and number,
as well as debugging features such as the source
of lexical items (e.g. from a named entity finder,
the morphology, or the guesser). There is a base
grammar which covers the constructions found in
standard written English, as well as three overlay
grammars: one for parsing Wikipedia sentences,
one for parsing Wikipedia headers, and one for
parsing queries (sentential, phrasal, and keyword).
The grammar is being used by Powerset (a Mi-
crosoft company) in a semantic consumer-search
reference vertical which allows people to search
Wikipedia using natural language queries as well
as traditional keyword queries. The system uses a
pipeline architecture which includes: text extrac-
tion, sentence breaking, named entity detection,
parsing (tokenization, morphological analysis, c-
structure, f-structure, ranking), semantic analysis,
and indexing of selected semantic facts (see Fig-
ure 1). A similar pipeline is used on the query
side except that the resulting semantic analysis is
turned into a query execution language which is
used to query the index.
text extraction
sentence breaker
named entity detection
LFG grammars
tokenizer
morphology
</bodyText>
<figure confidence="0.772657333333333">
XLE: parser
MaxEnt model
XLE: XFR
</figure>
<figureCaption confidence="0.999595">
Figure 1: NL Pipeline Components
</figureCaption>
<bodyText confidence="0.92149775">
The core idea behind using a deep parser in the
pipeline in conjunction with the semantic rules is
to localize role information as to who did what to
whom (i.e. undo long-distance dependencies and
grammar
ranking
semantics
script
finite state
MaxEnt model
finite state
finite state
</bodyText>
<page confidence="0.996205">
64
</page>
<bodyText confidence="0.999946666666667">
locate heads of arguments), to abstract away from
choice of particular lexical items (i.e. lemmatiza-
tion and detection of synonyms), and generally
provide a more normalized representation of the
natural language string to improve both precision
and recall.
</bodyText>
<subsectionHeader confidence="0.970139">
1.3 Oceanography
</subsectionHeader>
<bodyText confidence="0.999807045454546">
As a byproduct of the indexing pipeline, all of
the syntactic and semantic structures are stored
for later inspection as part of failure analysis.4
The files containing these structures are distributed
over several machines since ∼125 million sen-
tences are parsed for the analysis of Wikipedia.
For any given syntactic or semantic structure,
the XLE ordered rewrite system (XFR; (Crouch et
al., 2009)) can be used to extract information that
is of interest to the grammar engineer, by way of
“rules” or statements in the XFR language. As the
XFR ordered rewrite system is also used for the
semantics rules that turn f-structures into seman-
tic representations, the notation is familiar to the
grammar writers and is already designed for ma-
nipulating the syntactic f-structures.
However, the mechanics of accessing each file
on each machine and then assembling the results is
prohibitively complicated without a tool that pro-
vides a simple interface to the system. Oceanogra-
phy was designed to take a single specification file
stating:
</bodyText>
<listItem confidence="0.993931">
• which data to examine (which corpus ver-
sion; full Wikipedia build or fixed 10,000
document set);
• the XFR rules to be applied;
• what extracted data to count and report back.
</listItem>
<bodyText confidence="0.999704583333333">
Many concrete examples of Oceanography runs
will be discussed below. The basic idea is to
use the XFR rules to specify searches over lexi-
cal items, features, and constructions in a way that
is similar to that of Tiger Search and other facili-
ties. The Oceanography machinery enables these
searches over massive data and helps in compil-
ing the results for the grammar engineer to inspect.
We believe that similar approaches would be fea-
sible to implement in other grammar development
environments and, in fact, for some grammar out-
puts and applications, existing tools such as Tiger
</bodyText>
<footnote confidence="0.746906">
4The index is self-contained and does not need to refer-
</footnote>
<bodyText confidence="0.910814714285714">
ence the semantic, much less the earlier syntactic, structures
as part of the search application.
Search would be sufficient. By providing exam-
ples where such searches have aided our grammar
development, we hope to encourage other gram-
mar engineers to similarly extend their efforts to
use easy access to massive data to drive their work.
</bodyText>
<sectionHeader confidence="0.952702" genericHeader="method">
2 Grammar Development
</sectionHeader>
<bodyText confidence="0.9996794">
The ParGram English LFG grammar has been de-
veloped over many years. However, the focus of
development was on newspaper text and technical
manuals, although some adaptation was done for
new domains (King and Maxwell, 2007). When
moving to the Wikipedia domain, many new con-
structions and lexical items were encountered (see
(Baldwin et al., 2005) for a similar experience
with the BNC) and, at the same time, the require-
ments on parsing efficiency increased.
</bodyText>
<subsectionHeader confidence="0.981856">
2.1 Lexical Development
</subsectionHeader>
<bodyText confidence="0.999983129032258">
When first parsing a new corpus, the grammar en-
counters new words that were previously unknown
to the morphology. The morphology falls back to a
guesser that uses regular expressions to guess the
part of speech and other features associated with
an unknown form. For example, a novel word end-
ing in s might be a plural noun. The grammar
records a feature LEX-SOURCE with the value
guesser for all guessed words. Oceanography was
used to extract all guessed forms and their parts
of speech. In many cases, the guesser had cor-
rectly identified the word’s part of speech. How-
ever, words that occurred frequently were added to
the morphology to avoid the possibility that they
would be incorrectly guessed as a different part of
speech. The fact that Oceanography was able to
identify not just the word, but its posited part of
speech and frequency in the corpus greatly sped
lexical development.
Incorrect guessing of verbs was of particular
concern to the grammar writers, as misidentifica-
tion of verbs was almost always accompanied by
a bad parse. In addition, subcategorization frames
for guessed verbs were guessed as either transi-
tive or intransitive, which often proved to be in-
correct. As such, the guessed verbs extracted us-
ing Oceanography were hand curated: true verbs
were added to the morphology and their subcate-
gorization frames to the lexicon. Due to the high
rate of error with guessed verbs, once the correctly
guessed verbs were added to the morphology, this
</bodyText>
<page confidence="0.998223">
65
</page>
<bodyText confidence="0.999829911111111">
option was removed from the guesser.5
Overall, ∼4200 new stems were added to the
already substantial morphology, with correct in-
flection. Approximately ∼1300 of these were
verbs. The decision to eliminate verbs as possi-
ble guessed parts of speech was directly motivated
by data extracted using Oceanography.
Since the guesser works with regular expres-
sions (e.g. lowercase letters + s form plural
nouns), it is possible to encounter forms in
the corpus that neither the morphology nor the
guesser recognize. The grammar will fragment on
these sentences, creating well-formed f-structure
chunks but no single spanning parse, and the un-
recognized forms will be recorded as TOKENs
(Riezler et al., 2002). An Oceanography run ex-
tracting all TOKENs resulted in the addition of sev-
eral new patterns to the guesser as well as the addi-
tion of some of the frequent forms to the morphol-
ogy. For example, sequences of all upper case let-
ters followed by a hyphen and then by a sequence
of digits were added for forms like AK-47, F-22,
and V-1.
The guesser and TOKENs Oceanography runs
look for general problems with the morphology
and lexicon, and can be run for every new cor-
pus. More specific jobs are run when evaluating
whether to implement a new analysis, or when
evaluating whether a current analysis is function-
ing properly. For example, use of the van No-
ord tool indicated that the grammar had problems
with certain less common multiword prepositions
(e.g. pursuant to, in contrast with). Once these
multiword prepositions were added, the question
then arose as to whether more common preposi-
tions should be multiwords when stacked (e.g. up
to, along with). An Oceanography run was per-
formed to extract all occurrences of stacked prepo-
sitions from the corpus. Their frequency was tal-
lied in both the stacked formations and when used
as simple prepositions. With this information, we
determined which stacked configurations to add to
the lexicon as multiword prepositions, while main-
taining preposition stacking for less common com-
binations.
</bodyText>
<subsectionHeader confidence="0.964634">
2.2 Grammar Rule Development
</subsectionHeader>
<bodyText confidence="0.9967875">
In addition to using Oceanography to help develop
the morphology and lexicon, it has also proven ex-
</bodyText>
<footnote confidence="0.9966475">
5It is simple to turn the guessed verbs back on in order to
run the same Oceanography experiment with a new corpus.
</footnote>
<bodyText confidence="0.9996053">
tremely useful in grammar rule development. In
general, the issue is not in finding constructions
which the grammar does not cover correctly: a
quick investigation of sentences which fragment
can provide these and issues are identified and re-
ported by the semantics which uses the syntax out-
put as its input. Furthermore, the van Noord tool
can be used to effectively identify gaps in gram-
mar rule coverage.
Rather, the more pressing issues include
whether it is worthwhile adding a construction,
which possible solution to pick (when it is worth-
while), and whether an existing solution is ap-
plying correctly and efficiently. Being able to
look at the occurrence of a construction over large
amounts of data can help with all of these issues,
especially when combined with searching over
gold standard treebanks such as the Penn Tree-
bank.
Determining which constructions to examine
using Oceanography is often the result of failure
analysis findings on components outside the gram-
mar itself, but that build on the grammar’s output
later in the natural language processing pipeline.
The point we wish to emphasize here is that the
grammar engineer’s effectiveness can greatly ben-
efit from being able to take a set of problematic
data gathered from massive parser output and de-
termine from it that a particular construction mer-
its closer scrutiny.
</bodyText>
<subsectionHeader confidence="0.942436">
2.2.1 When relative/subordinate clauses
</subsectionHeader>
<bodyText confidence="0.9993462">
An observation that subordinate clauses contain-
ing when (e.g. Mary laughed when Ed tripped.)
were sometimes misanalyzed as relative clauses
attaching to a noun (e.g. the time when Ed tripped)
prompted a more directed analysis of whether
when relative clauses should be allowed to at-
tach to nouns that were not time-related expres-
sions (e.g. time, year, day). An Oceanography run
was performed to extract all when relative clauses,
the modified nominal head, and the sentence con-
taining the construction. A frequency-sorted list
of nouns taking when relative clause modifiers
helped to direct hand-examination of when relative
clauses for accuracy of the analysis. This yielded
some correct analyses:
</bodyText>
<listItem confidence="0.737905">
(1) There are times [when a Bigfoot sighting or
footprint is a hoax].
</listItem>
<bodyText confidence="0.8706435">
More importantly, however, the search revealed
many incorrect analyses of when subordinate
</bodyText>
<page confidence="0.979126">
66
</page>
<bodyText confidence="0.938260071428571">
clauses as relative clauses:
(2) He gets the last laugh [when he tows away
his boss’ car as well as everyone else’s].
By extracting all when relative clauses, and their
head nouns, it was determined that the construc-
tion was generally only correct for a small class of
time expression nominals. Comparatively, when
relative clause modification of other nominals was
rarely correct. The grammar was modified to dis-
prefer relative clause analyses of when clauses un-
less the head noun was an expression of time. As
a result, the overall quality of parses for all sen-
tences containing when subordinate clauses was
improved.
</bodyText>
<subsubsectionHeader confidence="0.582116">
2.2.2 Relative clauses modifying gerunds
</subsubsectionHeader>
<bodyText confidence="0.999296666666667">
Another example of an issue with the accuracy of a
grammatical analysis concerns gerund nouns mod-
ified by relative clauses without an overt relative
pronoun (e.g. the singing we liked). It was ob-
served that many strings were incorrectly analyzed
as a gerund and reduced relative clause modifier:
</bodyText>
<listItem confidence="0.9438965">
(3) She lost all of her powers, including [her
sonic screams].
</listItem>
<bodyText confidence="0.998825933333333">
Again, a frequency sorted list of gerunds modi-
fied by reduced relative clauses helped to guide
hand inspection of the instances of this construc-
tion. By extracting all of the gerunds with re-
duced relative clause modifiers, it was possible
to see which gerunds were appearing in this con-
struction (e.g. including occurred alarmingly fre-
quently) and how rarely the overall analysis was
correct. As a result of the data analysis, such rel-
ative clause modifiers are now dispreferred in the
grammar and certain verbs (e.g. include) are addi-
tionally dispreferred as gerunds in general. Note
that this type of failure analysis is not possible
with a tool (such as the van Noord tool) that only
points out gaps in grammar coverage.
</bodyText>
<subsectionHeader confidence="0.485236">
2.2.3 Noun-noun compounds
</subsectionHeader>
<bodyText confidence="0.999989315789474">
As part of the semantic search application,
argument-relation triples are extracted from the
corpus and presented to the user as a form of sum-
mary over what Wikipedia knows about a partic-
ular entity. These are referred to as Factz. For
example, a search on Noam Chomsky will find
Factz triples as in Figure 2. Such an application
highlights parse problems, since the predicate-
argument relations displayed are ultimately ex-
tracted from the syntactic parses themselves.
One class of problem arises when forms which
are ambiguous between nominal and verbal analy-
ses are erroneously analyzed as verbs and hence
show up as Factz relations. This is particularly
troublesome when the putative verb is part of a
noun-noun compound (e.g. ice cream, hot dog)
and the verb form is comparatively rare. A list
of potentially problematic noun-noun compounds
was extracted by using an independent part of
speech tagger over the sentences that generated the
Factz triples. If the relation in the triple was tagged
as a noun and was not a deverbal noun (e.g. de-
struction, writing), then the first argument of the
triple and the relation were tagged as potentially
problematic noun-noun compounds. Oceanogra-
phy was then used to determine the relative fre-
quency of whether the word pairs were analyzed as
noun-noun compounds, verb-argument relations,
or independent nouns and verbs.
This distributional information, in conjunction
with information about known noun-noun com-
pounds in WordNet (Fellbaum, 1998), is being
used to extract a set of ∼100,000 noun-noun com-
pounds whose analysis is extremely strongly pre-
ferred by the grammar. Currently, these are con-
strained via c-structure optimality marks6 but they
may eventually be allowed only as noun-noun
compounds if the list proves reliable enough.
</bodyText>
<sectionHeader confidence="0.860987" genericHeader="method">
3 Tracking Grammar Progress
</sectionHeader>
<bodyText confidence="0.9999856">
The grammar is used as part of a larger applica-
tion which is actively being developed and which
is regularly updated. As such, new versions of
the grammar are regularly released. Each release
includes a detailed list of improvements and bug
fixes, as well as requirements on other compo-
nents of the system (e.g. the grammar may require
a specific version of the XLE parser or of the mor-
phology). It is extremely important to be able to
confirm that the changes to the grammar are in
place and are functioning as expected when used
in the pipeline. Some changes can be confirmed
by browsing documents, finding a sentence likely
to contain the relevant lexical item or construction,
and then inspecting the syntactic structures for that
</bodyText>
<footnote confidence="0.959136428571429">
6See (Frank et al., 2001) and (Crouch et al., 2009) on the
use of Optimality Theory marks within XLE. C-structure op-
timality marks apply preferences to the context free backbone
before any constraints supplied by the f-structure annotations
are applied. This means that the noun-noun compounds will
be the only analysis possible if any tree can be constructed
with them.
</footnote>
<page confidence="0.99884">
67
</page>
<figureCaption confidence="0.999431">
Figure 2: Example Factz
</figureCaption>
<bodyText confidence="0.679459">
document.
</bodyText>
<subsectionHeader confidence="0.999942">
3.1 Confirming Grammar Changes
</subsectionHeader>
<bodyText confidence="0.99998671875">
However, some changes are more complicated to
confirm either because it is hard to determine from
a sentence whether the grammar change would ap-
ply or because the change is more frequency re-
lated. For these types of changes, Oceanogra-
phy runs can detect whether a rare change oc-
curred at all, alleviating the need to search through
documents by hand. For example, to determine
whether the currency symbols are being correctly
treated by the grammar, especially the ones that
are not standard ASCII (e.g. the euro and yen sym-
bols), two simple XFR rules can be written: one
that looks for the relevant c-structure leaf node and
counts up which symbols occur under this node
and one that looks for the known list of currency
symbols in the f-structure and counts up what part-
of-speech they were analyzed as.
To detect whether frequency related changes
to the grammar are behaving as expected, two
Oceanography runs can be compared, one with
the older grammar and one with the newer one.
For example, to determine whether relative clauses
headed by when were dispreferred relative to
subordinate clauses, the number of such relative
clauses and such subordinate clauses were counted
in two successive runs; the relative occurrence of
the types confirmed that the preference mecha-
nism was working correctly. In addition, a quick
examination of sentences containing each type
showed that the change was not over-applying
(e.g. incorrectly analyzing when relative clauses as
subordinate clauses).
</bodyText>
<subsectionHeader confidence="0.999418">
3.2 General Grammar Checking
</subsectionHeader>
<bodyText confidence="0.9999888">
In addition to Oceanography runs done to check
on specific changes to the grammar, a core set of
XFR rules extracts all of the features from the f-
structure and counts them. The resulting statistics
of features and counts are computed for each ma-
jor release and compared to that of the previous
release. This provides a list of new features which
subsequent components must be alerted to (e.g. a
feature added to indicate what type of punctua-
tion surrounded a parenthetical). It also provides a
quick check of whether some feature is no longer
occurring with the same frequency. In some cases
this is expected; once many guessed forms were
added to the lexicon, the feature indicating that
the guesser had applied dropped sharply. How-
ever, unexpected steep variations from previous
runs can be investigated to make sure that rules
were not inadvertently removed from the gram-
mar, and that rules added to the grammar are func-
tioning correctly.
</bodyText>
<sectionHeader confidence="0.914554" genericHeader="method">
4 Using Grammar Output to Develop
Other Components
</sectionHeader>
<bodyText confidence="0.999986217391304">
In addition to being used in development of the
grammar itself, examination of the grammar out-
put can be useful for engineering efforts on other
components. In addition to the examples cited
above concerning the development of the mor-
phology used by the grammar, we discuss one sim-
ple example here. The sentence breaker used in
the pipeline is designed for high precision; it only
breaks sentences when it is sure that there is a sen-
tence break. To make up for breaks that may have
been missed, the grammar contains a rule that al-
lows multiple sentences to be parsed as a single
string. The resulting f-structure has the final sen-
tence’s f-structure as the value of a feature, LAST,
and the remainder as the value of a feature, REST.
The grammar iteratively parses multiple sentences
into these LAST-REST structures. Because the fea-
ture LAST is only instantiated when parsing mul-
tiple sentences, input strings whose parses con-
tained a LAST component could be extracted to
determine whether the sentence breaker’s behavior
should be changed. An example of two sentences
which were not broken is:
</bodyText>
<page confidence="0.997924">
68
</page>
<bodyText confidence="0.9998298">
(4) The current air staff includes former CNN
Headline News gal Holly Firfer in the morn-
ings with co-host Orff. Mid-days is Mara
Davis, who does a theme lunch hour.
The relatively short unknown word Orff before the
period makes it unclear whether this is an abbrevi-
ation or not. Based on the Oceanography analysis,
the number of unbroken sentences which received
analyses was roughly halved and one bug concern-
ing footnote markers was discovered and fixed.
</bodyText>
<sectionHeader confidence="0.998864" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999974068965517">
Large-scale grammars are increasingly being used
in applications. In order to maximize their effec-
tiveness in terms of coverage, accuracy, and effi-
ciency for a given application, it is increasingly
important to examine the behavior of the grammar
on the relevant corpus and in the relevant applica-
tion.
Having good tools makes the grammar engi-
neer’s task of massive data driven grammar de-
velopment significantly easier. In this paper we
have discussed how such a tool, which can ap-
ply search patterns over the syntactic (and seman-
tic) representations of Wikipedia, is being used in
a semantic search research vertical. When used
in conjunction with existing tools for detecting
gaps in parser coverage (e.g. the van Noord tool),
Oceanography greatly aids in the evaluation of ex-
isting linguistic analyses from the parser. In ad-
dition, oceanography provides vital information to
determining whether or not to implement coverage
for a particular construction, based on efficiency
requirements. Thus, the grammar writer has a
suite of tools available to address the questions
raised in the introduction of this paper: what gaps
exist in parser coverage, how to best address those
gaps, and whether existing analyses are function-
ing appropriately. We hope that our experiences
encourage other grammar engineers to use similar
techniques in their grammar development efforts.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999709">
We would like to thank Scott Waterman for creat-
ing Oceanography and adapting it to our needs.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999637535714286">
Timothy Baldwin, John Beavers, Emily M. Bender,
Dan Flickinger, Ara Kim, and Stephan Oepen.
2005. Beauty and the beast: What running a
broad-coverage precision grammar over thee bnc
taught us about the grammar — and the corpus.
In Stephan Kepser and Marga Reis, editors, Lin-
guistic Evidence: Empirical, Theoretical, and Com-
putational Perspectives, pages 49–70. Mouton de
Gruyter, Berlin.
Gosse Bouma and Geert Kloosterman. 2002. Query-
ing dependency treebanks in XML. In Proceedings
of the Third international conference on Language
Resources and Evaluation (LREC), Gran Canaria.
Gosse Bouma and Geert Kloosterman. 2007. Mining
syntactically annotated corpora using XQuery. In
Proceedings of the Linguistic Annotation Workshop,
Prague, June. ACL.
Sabine Brants, Stefanie Dipper, Peter Eisenberg, Sil-
via Hansen, Esther K¨onig, Wolfgang Lezius, Chris-
tian Rohrer, George Smith, and Hans Uszkoreit.
2004. TIGER: Linguistic interpretation of a German
corpus. Research on Language and Computation,
2:597–620.
Miriam Butt, Tracy Holloway King, Mar´ıa-Eugenia
Ni˜no, and Fr´ed´erique Segond. 1999. A Grammar
Writer’s Cookbook. CSLI Publications.
Miram Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project. In COLING2002 Work-
shop on Grammar Engineering and Evaluation,
pages 1–7.
Robin Cooper, Dick Crouch, Jan van Eijck, Chris
Fox, Josef van Genabith, Jan Jaspars, Hans Kamp,
David Milward, Manfred Pinkal, Massimo Poesio,
and Steve Pulman. 1996. Using the framework.
FraCas: A Framework for Computational Semantics
(LRE 62-051).
Dick Crouch, Mary Dalrymple, Ronald Kaplan,
Tracy Holloway King, John T. Maxwell III, and
Paula Newman. 2009. XLE Documentation. On-
line.
Mary Dalrymple. 2001. Lexical Functional Grammar.
Syntax and Semantics. Academic Press.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John T. Maxwell III. 2001. Optimality theory style
constraint ranking in large-scale LFG grammars. In
Peter Sells, editor, Formal and Empirical Issues in
Optimality Theoretic Syntax, pages 367–397. CSLI
Publications.
Tracy Holloway King and John T. Maxwell, III. 2007.
Overlay mechanisms for multi-level deep processing
applications. In Proceedings of the Grammar En-
gineering Across Frameworks (GEAF07) Workshop.
CSLI Publications.
</reference>
<page confidence="0.983778">
69
</page>
<reference confidence="0.99952127027027">
Sabine Lehmann, Stephan Oepen, Sylvie Regnier-
Prost, Klaus Netter, Veronika Lux, Judith Klein,
Kirsten Falkedal, Frederik Fouvry, Dominique Esti-
val, Eva Dauphin, Herv´e Compagnion, Judith Baur,
Lorna Balkan, and Doug Arnold. 1996. TSNLP
— Test Suites for Natural Language Processing. In
Proceedings of COLING 1996.
Wolfgang Lezius. 2002. Ein Suchwerkzeug f¨ur syn-
taktisch annotierte Textkorpora (in German). Ph.D.
thesis, IMS, University of Stuttgart Arbeitspapiere
des Instituts f¨ur Maschinelle Sprachverarbeitung
(AIMS). volume 8, number 4.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn treebank: Annotating
predicate argument structure. In ARPA Human
Language Technology Workshop.
Jiˇr´ı M´ırovsk´y. 2008. PDT 2.0 requirements on a query
language. In Proceedings of ACL-08: HLT, pages
37–45. Association for Computational Linguistics.
John Nerbonne, Dan Flickinger, and Tom Wasow.
1988. The HP Labs natural language evaluation
tool. In Proceedings of the Workshop on Evaluation
of Natural Language Processing Systems.
Stefan Riezler, Tracy Holloway King, Ronald Kaplan,
Dick Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
lexical-functional grammar and discriminative esti-
mation techniques. In Proceedings of the ACL.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of
ACL.
Scott A. Waterman. 2009. Distributed parse mining.
In Proceedings of the NAACL Workshop on Soft-
ware Engineering, Testing, and Quality Assurance
for Natural Language Processing.
</reference>
<page confidence="0.998458">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.700325">
<title confidence="0.999892">Using Large-scale Parser Output to Guide Grammar Development</title>
<author confidence="0.869117">Ascander Dost Tracy Holloway King</author>
<affiliation confidence="0.845836">Powerset, a Microsoft company Powerset, a Microsoft company</affiliation>
<email confidence="0.934235">adost@microsoft.comTracy.King@microsoft.com</email>
<abstract confidence="0.992970857142857">This paper reports on guiding parser development by extracting information from output of a large-scale parser applied to Wikipedia documents. Data-driven parser improvement is especially important for applications where the corpus may differ from that originally used to develop the core grammar and where efficiency concerns affect whether a new construction should be added, or existing analyses modified. The large size of the corpus in question also brings scalability concerns to the foreground.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>John Beavers</author>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Ara Kim</author>
<author>Stephan Oepen</author>
</authors>
<title>Beauty and the beast: What running a broad-coverage precision grammar over thee bnc taught us about the grammar — and the corpus.</title>
<date>2005</date>
<booktitle>Linguistic Evidence: Empirical, Theoretical, and Computational Perspectives,</booktitle>
<pages>49--70</pages>
<editor>In Stephan Kepser and Marga Reis, editors,</editor>
<location>Berlin.</location>
<contexts>
<context position="3876" citStr="Baldwin et al., 2005" startWordPosition="585" endWordPosition="588">d corpora.2 A parser can be run over new text, and a comparison of the in-domain and 2The suffix array error mining software is available at: http://www.let.rug.nl/˜vannoord/SuffixArrays.tgz 63 Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 63–70, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP out-of-domain sentences can determine, for instance, that the grammar cannot parse adjectivenoun hyphenation correctly (e.g. an electricalswitch cover). A different technique for error mining that uses discriminative treebanking is described in (Baldwin et al., 2005). This technique aims at determining issues with lexical coverage, grammatical (rule) coverage, ungrammaticality within the corpus (e.g. misspelled words), and extragrammaticality within the corpus (e.g. bulleted lists). A second approach involves querying goldstandard treebanks such as the Penn Treebank (Marcus et al., 1994) and Tiger Treebank (Brants et al., 2004) to determine the frequency of certain phenomena. For example, Tiger Search (Lezius, 2002) can be used to list and frequencysort stacked prepositions (e.g. up to the door) or temporal noun/adverbs after prepositions (e.g. by now). T</context>
<context position="10784" citStr="Baldwin et al., 2005" startWordPosition="1682" endWordPosition="1685">application. Search would be sufficient. By providing examples where such searches have aided our grammar development, we hope to encourage other grammar engineers to similarly extend their efforts to use easy access to massive data to drive their work. 2 Grammar Development The ParGram English LFG grammar has been developed over many years. However, the focus of development was on newspaper text and technical manuals, although some adaptation was done for new domains (King and Maxwell, 2007). When moving to the Wikipedia domain, many new constructions and lexical items were encountered (see (Baldwin et al., 2005) for a similar experience with the BNC) and, at the same time, the requirements on parsing efficiency increased. 2.1 Lexical Development When first parsing a new corpus, the grammar encounters new words that were previously unknown to the morphology. The morphology falls back to a guesser that uses regular expressions to guess the part of speech and other features associated with an unknown form. For example, a novel word ending in s might be a plural noun. The grammar records a feature LEX-SOURCE with the value guesser for all guessed words. Oceanography was used to extract all guessed forms </context>
</contexts>
<marker>Baldwin, Beavers, Bender, Flickinger, Kim, Oepen, 2005</marker>
<rawString>Timothy Baldwin, John Beavers, Emily M. Bender, Dan Flickinger, Ara Kim, and Stephan Oepen. 2005. Beauty and the beast: What running a broad-coverage precision grammar over thee bnc taught us about the grammar — and the corpus. In Stephan Kepser and Marga Reis, editors, Linguistic Evidence: Empirical, Theoretical, and Computational Perspectives, pages 49–70. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
<author>Geert Kloosterman</author>
</authors>
<title>Querying dependency treebanks in XML.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third international conference on Language Resources and Evaluation (LREC), Gran Canaria.</booktitle>
<contexts>
<context position="4957" citStr="Bouma and Kloosterman (2002)" startWordPosition="750" endWordPosition="753">2002) can be used to list and frequencysort stacked prepositions (e.g. up to the door) or temporal noun/adverbs after prepositions (e.g. by now). The search tools over these treebanks allow for complex searches involving specification of lexical items, parts of speech, and tree configurations (see (Mirovsk´y, 2008) for discussion of query requirements for searching tree and dependency banks). The third approach we discuss here differs from querying gold-standard treebanks in that corpora of actual parser output are queried to examine how constructions are analyzed by the grammar. For example, Bouma and Kloosterman (2002) use XQuery (an XML query language) to mine parse results stored as XML data.3 It is this sort of examination of parser output that is the focus of the present paper, and specific examples of our experiences follow in Section 2.2. Use of such tools has proven vital to the development of large-scale grammars. Based on our experiences with them, we began extensively using a tool called Oceanography (Waterman, 2009) to search parser output for very large (approximately 125 million sentence) parse runs stored on a distributed file system. Oceanography queries the parser output and returns counts o</context>
</contexts>
<marker>Bouma, Kloosterman, 2002</marker>
<rawString>Gosse Bouma and Geert Kloosterman. 2002. Querying dependency treebanks in XML. In Proceedings of the Third international conference on Language Resources and Evaluation (LREC), Gran Canaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
<author>Geert Kloosterman</author>
</authors>
<title>Mining syntactically annotated corpora using XQuery.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop,</booktitle>
<publisher>ACL.</publisher>
<location>Prague,</location>
<contexts>
<context position="5888" citStr="Bouma and Kloosterman, 2007" startWordPosition="905" endWordPosition="908">mmars. Based on our experiences with them, we began extensively using a tool called Oceanography (Waterman, 2009) to search parser output for very large (approximately 125 million sentence) parse runs stored on a distributed file system. Oceanography queries the parser output and returns counts of specific constructions or properties, as well as the example sentences they were extracted from. In the subsequent sections we discuss how this tool (in conjunction with existing ones like the van Noord Tool and Tiger Search) has enhanced grammar development for an English-language Lexical3See also (Bouma and Kloosterman, 2007) for further discussion of this technique. Functional Grammar used for a semantic search application over Wikipedia. 1.2 The Grammar and its Role The grammar being developed is a LexicalFunctional Grammar (LFG (Dalrymple, 2001)) that is part of the ParGram parallel grammar project (Butt et al., 1999; Butt et al., 2002). It runs on the XLE system (Crouch et al., 2009) and produces c(onstituent)-structures which are trees and f(unctional)-structures which are attribute value matrices recording grammatical functions and other syntactic features such as tense and number, as well as debugging featu</context>
</contexts>
<marker>Bouma, Kloosterman, 2007</marker>
<rawString>Gosse Bouma and Geert Kloosterman. 2007. Mining syntactically annotated corpora using XQuery. In Proceedings of the Linguistic Annotation Workshop, Prague, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Peter Eisenberg</author>
<author>Silvia Hansen</author>
<author>Esther K¨onig</author>
<author>Wolfgang Lezius</author>
<author>Christian Rohrer</author>
<author>George Smith</author>
<author>Hans Uszkoreit</author>
</authors>
<title>TIGER: Linguistic interpretation of a German corpus.</title>
<date>2004</date>
<journal>Research on Language and Computation,</journal>
<pages>2--597</pages>
<marker>Brants, Dipper, Eisenberg, Hansen, K¨onig, Lezius, Rohrer, Smith, Uszkoreit, 2004</marker>
<rawString>Sabine Brants, Stefanie Dipper, Peter Eisenberg, Silvia Hansen, Esther K¨onig, Wolfgang Lezius, Christian Rohrer, George Smith, and Hans Uszkoreit. 2004. TIGER: Linguistic interpretation of a German corpus. Research on Language and Computation, 2:597–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
<author>Tracy Holloway King</author>
<author>Mar´ıa-Eugenia Ni˜no</author>
<author>Fr´ed´erique Segond</author>
</authors>
<title>A Grammar Writer’s Cookbook.</title>
<date>1999</date>
<publisher>CSLI Publications.</publisher>
<marker>Butt, King, Ni˜no, Segond, 1999</marker>
<rawString>Miriam Butt, Tracy Holloway King, Mar´ıa-Eugenia Ni˜no, and Fr´ed´erique Segond. 1999. A Grammar Writer’s Cookbook. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miram Butt</author>
<author>Helge Dyvik</author>
<author>Tracy Holloway King</author>
<author>Hiroshi Masuichi</author>
<author>Christian Rohrer</author>
</authors>
<title>The Parallel Grammar Project.</title>
<date>2002</date>
<booktitle>In COLING2002 Workshop on Grammar Engineering and Evaluation,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="6208" citStr="Butt et al., 2002" startWordPosition="957" endWordPosition="960">rties, as well as the example sentences they were extracted from. In the subsequent sections we discuss how this tool (in conjunction with existing ones like the van Noord Tool and Tiger Search) has enhanced grammar development for an English-language Lexical3See also (Bouma and Kloosterman, 2007) for further discussion of this technique. Functional Grammar used for a semantic search application over Wikipedia. 1.2 The Grammar and its Role The grammar being developed is a LexicalFunctional Grammar (LFG (Dalrymple, 2001)) that is part of the ParGram parallel grammar project (Butt et al., 1999; Butt et al., 2002). It runs on the XLE system (Crouch et al., 2009) and produces c(onstituent)-structures which are trees and f(unctional)-structures which are attribute value matrices recording grammatical functions and other syntactic features such as tense and number, as well as debugging features such as the source of lexical items (e.g. from a named entity finder, the morphology, or the guesser). There is a base grammar which covers the constructions found in standard written English, as well as three overlay grammars: one for parsing Wikipedia sentences, one for parsing Wikipedia headers, and one for pars</context>
</contexts>
<marker>Butt, Dyvik, King, Masuichi, Rohrer, 2002</marker>
<rawString>Miram Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Masuichi, and Christian Rohrer. 2002. The Parallel Grammar Project. In COLING2002 Workshop on Grammar Engineering and Evaluation, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cooper</author>
<author>Dick Crouch</author>
<author>Jan van Eijck</author>
<author>Chris Fox</author>
<author>Josef van Genabith</author>
<author>Jan Jaspars</author>
<author>Hans Kamp</author>
<author>David Milward</author>
<author>Manfred Pinkal</author>
<author>Massimo Poesio</author>
<author>Steve Pulman</author>
</authors>
<title>Using the framework. FraCas: A Framework for Computational Semantics</title>
<date>1996</date>
<journal>LRE</journal>
<pages>62--051</pages>
<marker>Cooper, Crouch, van Eijck, Fox, van Genabith, Jaspars, Kamp, Milward, Pinkal, Poesio, Pulman, 1996</marker>
<rawString>Robin Cooper, Dick Crouch, Jan van Eijck, Chris Fox, Josef van Genabith, Jan Jaspars, Hans Kamp, David Milward, Manfred Pinkal, Massimo Poesio, and Steve Pulman. 1996. Using the framework. FraCas: A Framework for Computational Semantics (LRE 62-051).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Mary Dalrymple</author>
<author>Ronald Kaplan</author>
<author>Tracy Holloway King</author>
<author>John T Maxwell</author>
<author>Paula Newman</author>
</authors>
<date>2009</date>
<publisher>XLE Documentation. Online.</publisher>
<contexts>
<context position="6257" citStr="Crouch et al., 2009" startWordPosition="967" endWordPosition="970">re extracted from. In the subsequent sections we discuss how this tool (in conjunction with existing ones like the van Noord Tool and Tiger Search) has enhanced grammar development for an English-language Lexical3See also (Bouma and Kloosterman, 2007) for further discussion of this technique. Functional Grammar used for a semantic search application over Wikipedia. 1.2 The Grammar and its Role The grammar being developed is a LexicalFunctional Grammar (LFG (Dalrymple, 2001)) that is part of the ParGram parallel grammar project (Butt et al., 1999; Butt et al., 2002). It runs on the XLE system (Crouch et al., 2009) and produces c(onstituent)-structures which are trees and f(unctional)-structures which are attribute value matrices recording grammatical functions and other syntactic features such as tense and number, as well as debugging features such as the source of lexical items (e.g. from a named entity finder, the morphology, or the guesser). There is a base grammar which covers the constructions found in standard written English, as well as three overlay grammars: one for parsing Wikipedia sentences, one for parsing Wikipedia headers, and one for parsing queries (sentential, phrasal, and keyword). T</context>
<context position="8630" citStr="Crouch et al., 2009" startWordPosition="1325" endWordPosition="1328">ce of particular lexical items (i.e. lemmatization and detection of synonyms), and generally provide a more normalized representation of the natural language string to improve both precision and recall. 1.3 Oceanography As a byproduct of the indexing pipeline, all of the syntactic and semantic structures are stored for later inspection as part of failure analysis.4 The files containing these structures are distributed over several machines since ∼125 million sentences are parsed for the analysis of Wikipedia. For any given syntactic or semantic structure, the XLE ordered rewrite system (XFR; (Crouch et al., 2009)) can be used to extract information that is of interest to the grammar engineer, by way of “rules” or statements in the XFR language. As the XFR ordered rewrite system is also used for the semantics rules that turn f-structures into semantic representations, the notation is familiar to the grammar writers and is already designed for manipulating the syntactic f-structures. However, the mechanics of accessing each file on each machine and then assembling the results is prohibitively complicated without a tool that provides a simple interface to the system. Oceanography was designed to take a s</context>
<context position="21295" citStr="Crouch et al., 2009" startWordPosition="3405" endWordPosition="3408">ly released. Each release includes a detailed list of improvements and bug fixes, as well as requirements on other components of the system (e.g. the grammar may require a specific version of the XLE parser or of the morphology). It is extremely important to be able to confirm that the changes to the grammar are in place and are functioning as expected when used in the pipeline. Some changes can be confirmed by browsing documents, finding a sentence likely to contain the relevant lexical item or construction, and then inspecting the syntactic structures for that 6See (Frank et al., 2001) and (Crouch et al., 2009) on the use of Optimality Theory marks within XLE. C-structure optimality marks apply preferences to the context free backbone before any constraints supplied by the f-structure annotations are applied. This means that the noun-noun compounds will be the only analysis possible if any tree can be constructed with them. 67 Figure 2: Example Factz document. 3.1 Confirming Grammar Changes However, some changes are more complicated to confirm either because it is hard to determine from a sentence whether the grammar change would apply or because the change is more frequency related. For these types</context>
</contexts>
<marker>Crouch, Dalrymple, Kaplan, King, Maxwell, Newman, 2009</marker>
<rawString>Dick Crouch, Mary Dalrymple, Ronald Kaplan, Tracy Holloway King, John T. Maxwell III, and Paula Newman. 2009. XLE Documentation. Online.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
</authors>
<title>Lexical Functional Grammar. Syntax and Semantics.</title>
<date>2001</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="6115" citStr="Dalrymple, 2001" startWordPosition="942" endWordPosition="943">ceanography queries the parser output and returns counts of specific constructions or properties, as well as the example sentences they were extracted from. In the subsequent sections we discuss how this tool (in conjunction with existing ones like the van Noord Tool and Tiger Search) has enhanced grammar development for an English-language Lexical3See also (Bouma and Kloosterman, 2007) for further discussion of this technique. Functional Grammar used for a semantic search application over Wikipedia. 1.2 The Grammar and its Role The grammar being developed is a LexicalFunctional Grammar (LFG (Dalrymple, 2001)) that is part of the ParGram parallel grammar project (Butt et al., 1999; Butt et al., 2002). It runs on the XLE system (Crouch et al., 2009) and produces c(onstituent)-structures which are trees and f(unctional)-structures which are attribute value matrices recording grammatical functions and other syntactic features such as tense and number, as well as debugging features such as the source of lexical items (e.g. from a named entity finder, the morphology, or the guesser). There is a base grammar which covers the constructions found in standard written English, as well as three overlay gramm</context>
</contexts>
<marker>Dalrymple, 2001</marker>
<rawString>Mary Dalrymple. 2001. Lexical Functional Grammar. Syntax and Semantics. Academic Press.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Frank</author>
<author>Tracy Holloway King</author>
<author>Jonas Kuhn</author>
<author>John T Maxwell</author>
</authors>
<title>Optimality theory style constraint ranking in large-scale LFG grammars.</title>
<date>2001</date>
<booktitle>Formal and Empirical Issues in Optimality Theoretic Syntax,</booktitle>
<pages>367--397</pages>
<editor>In Peter Sells, editor,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="21269" citStr="Frank et al., 2001" startWordPosition="3400" endWordPosition="3403">f the grammar are regularly released. Each release includes a detailed list of improvements and bug fixes, as well as requirements on other components of the system (e.g. the grammar may require a specific version of the XLE parser or of the morphology). It is extremely important to be able to confirm that the changes to the grammar are in place and are functioning as expected when used in the pipeline. Some changes can be confirmed by browsing documents, finding a sentence likely to contain the relevant lexical item or construction, and then inspecting the syntactic structures for that 6See (Frank et al., 2001) and (Crouch et al., 2009) on the use of Optimality Theory marks within XLE. C-structure optimality marks apply preferences to the context free backbone before any constraints supplied by the f-structure annotations are applied. This means that the noun-noun compounds will be the only analysis possible if any tree can be constructed with them. 67 Figure 2: Example Factz document. 3.1 Confirming Grammar Changes However, some changes are more complicated to confirm either because it is hard to determine from a sentence whether the grammar change would apply or because the change is more frequenc</context>
</contexts>
<marker>Frank, King, Kuhn, Maxwell, 2001</marker>
<rawString>Anette Frank, Tracy Holloway King, Jonas Kuhn, and John T. Maxwell III. 2001. Optimality theory style constraint ranking in large-scale LFG grammars. In Peter Sells, editor, Formal and Empirical Issues in Optimality Theoretic Syntax, pages 367–397. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy Holloway King</author>
<author>John T Maxwell</author>
</authors>
<title>Overlay mechanisms for multi-level deep processing applications.</title>
<date>2007</date>
<booktitle>In Proceedings of the Grammar Engineering Across Frameworks (GEAF07) Workshop.</booktitle>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="10660" citStr="King and Maxwell, 2007" startWordPosition="1662" endWordPosition="1665">self-contained and does not need to reference the semantic, much less the earlier syntactic, structures as part of the search application. Search would be sufficient. By providing examples where such searches have aided our grammar development, we hope to encourage other grammar engineers to similarly extend their efforts to use easy access to massive data to drive their work. 2 Grammar Development The ParGram English LFG grammar has been developed over many years. However, the focus of development was on newspaper text and technical manuals, although some adaptation was done for new domains (King and Maxwell, 2007). When moving to the Wikipedia domain, many new constructions and lexical items were encountered (see (Baldwin et al., 2005) for a similar experience with the BNC) and, at the same time, the requirements on parsing efficiency increased. 2.1 Lexical Development When first parsing a new corpus, the grammar encounters new words that were previously unknown to the morphology. The morphology falls back to a guesser that uses regular expressions to guess the part of speech and other features associated with an unknown form. For example, a novel word ending in s might be a plural noun. The grammar re</context>
</contexts>
<marker>King, Maxwell, 2007</marker>
<rawString>Tracy Holloway King and John T. Maxwell, III. 2007. Overlay mechanisms for multi-level deep processing applications. In Proceedings of the Grammar Engineering Across Frameworks (GEAF07) Workshop. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Lehmann</author>
<author>Stephan Oepen</author>
<author>Sylvie RegnierProst</author>
<author>Klaus Netter</author>
<author>Veronika Lux</author>
<author>Judith Klein</author>
<author>Kirsten Falkedal</author>
</authors>
<title>TSNLP — Test Suites for Natural Language Processing.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING</booktitle>
<institution>Frederik Fouvry, Dominique Estival, Eva Dauphin, Herv´e Compagnion, Judith Baur, Lorna Balkan, and</institution>
<contexts>
<context position="976" citStr="Lehmann et al., 1996" startWordPosition="140" endWordPosition="143">a-driven parser improvement is especially important for applications where the corpus may differ from that originally used to develop the core grammar and where efficiency concerns affect whether a new construction should be added, or existing analyses modified. The large size of the corpus in question also brings scalability concerns to the foreground. 1 Introduction Initial development of rule-based parsers1 is often guided by the grammar writer’s knowledge of the language and test suites that cover the “core” linguistic phenomena of the language (Nerbonne et al., 1988; Cooper et al., 1996; Lehmann et al., 1996). Once the basic grammar is implemented, including an appropriate lexicon, the direction of grammar development becomes less clear. Integration of a grammar in a particular application and the use of a particular corpus can guide grammar development: the corpus and application will require the implementation of specific constructions and lexical items, as well as the reevaluation of existing analyses. To streamline this sort of output-driven development, tools to examine parser output over large corpora are necessary, and as corpus size increases, the efficiency and scalability of those tools </context>
</contexts>
<marker>Lehmann, Oepen, RegnierProst, Netter, Lux, Klein, Falkedal, 1996</marker>
<rawString>Sabine Lehmann, Stephan Oepen, Sylvie RegnierProst, Klaus Netter, Veronika Lux, Judith Klein, Kirsten Falkedal, Frederik Fouvry, Dominique Estival, Eva Dauphin, Herv´e Compagnion, Judith Baur, Lorna Balkan, and Doug Arnold. 1996. TSNLP — Test Suites for Natural Language Processing. In Proceedings of COLING 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Lezius</author>
</authors>
<title>Ein Suchwerkzeug f¨ur syntaktisch annotierte Textkorpora (in German).</title>
<date>2002</date>
<booktitle>Arbeitspapiere des Instituts f¨ur Maschinelle Sprachverarbeitung (AIMS).</booktitle>
<tech>Ph.D. thesis,</tech>
<volume>8</volume>
<institution>IMS, University of Stuttgart</institution>
<contexts>
<context position="4334" citStr="Lezius, 2002" startWordPosition="655" endWordPosition="657">rectly (e.g. an electricalswitch cover). A different technique for error mining that uses discriminative treebanking is described in (Baldwin et al., 2005). This technique aims at determining issues with lexical coverage, grammatical (rule) coverage, ungrammaticality within the corpus (e.g. misspelled words), and extragrammaticality within the corpus (e.g. bulleted lists). A second approach involves querying goldstandard treebanks such as the Penn Treebank (Marcus et al., 1994) and Tiger Treebank (Brants et al., 2004) to determine the frequency of certain phenomena. For example, Tiger Search (Lezius, 2002) can be used to list and frequencysort stacked prepositions (e.g. up to the door) or temporal noun/adverbs after prepositions (e.g. by now). The search tools over these treebanks allow for complex searches involving specification of lexical items, parts of speech, and tree configurations (see (Mirovsk´y, 2008) for discussion of query requirements for searching tree and dependency banks). The third approach we discuss here differs from querying gold-standard treebanks in that corpora of actual parser output are queried to examine how constructions are analyzed by the grammar. For example, Bouma</context>
</contexts>
<marker>Lezius, 2002</marker>
<rawString>Wolfgang Lezius. 2002. Ein Suchwerkzeug f¨ur syntaktisch annotierte Textkorpora (in German). Ph.D. thesis, IMS, University of Stuttgart Arbeitspapiere des Instituts f¨ur Maschinelle Sprachverarbeitung (AIMS). volume 8, number 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="4203" citStr="Marcus et al., 1994" startWordPosition="632" endWordPosition="635">009. c�2009 ACL and AFNLP out-of-domain sentences can determine, for instance, that the grammar cannot parse adjectivenoun hyphenation correctly (e.g. an electricalswitch cover). A different technique for error mining that uses discriminative treebanking is described in (Baldwin et al., 2005). This technique aims at determining issues with lexical coverage, grammatical (rule) coverage, ungrammaticality within the corpus (e.g. misspelled words), and extragrammaticality within the corpus (e.g. bulleted lists). A second approach involves querying goldstandard treebanks such as the Penn Treebank (Marcus et al., 1994) and Tiger Treebank (Brants et al., 2004) to determine the frequency of certain phenomena. For example, Tiger Search (Lezius, 2002) can be used to list and frequencysort stacked prepositions (e.g. up to the door) or temporal noun/adverbs after prepositions (e.g. by now). The search tools over these treebanks allow for complex searches involving specification of lexical items, parts of speech, and tree configurations (see (Mirovsk´y, 2008) for discussion of query requirements for searching tree and dependency banks). The third approach we discuss here differs from querying gold-standard treeban</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn treebank: Annotating predicate argument structure. In ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiˇr´ı M´ırovsk´y</author>
</authors>
<title>PDT 2.0 requirements on a query language. In</title>
<date>2008</date>
<booktitle>Proceedings of ACL-08: HLT,</booktitle>
<pages>37--45</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>M´ırovsk´y, 2008</marker>
<rawString>Jiˇr´ı M´ırovsk´y. 2008. PDT 2.0 requirements on a query language. In Proceedings of ACL-08: HLT, pages 37–45. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Nerbonne</author>
<author>Dan Flickinger</author>
<author>Tom Wasow</author>
</authors>
<title>The HP Labs natural language evaluation tool.</title>
<date>1988</date>
<booktitle>In Proceedings of the Workshop on Evaluation of Natural Language Processing Systems.</booktitle>
<contexts>
<context position="932" citStr="Nerbonne et al., 1988" startWordPosition="132" endWordPosition="135">e parser applied to Wikipedia documents. Data-driven parser improvement is especially important for applications where the corpus may differ from that originally used to develop the core grammar and where efficiency concerns affect whether a new construction should be added, or existing analyses modified. The large size of the corpus in question also brings scalability concerns to the foreground. 1 Introduction Initial development of rule-based parsers1 is often guided by the grammar writer’s knowledge of the language and test suites that cover the “core” linguistic phenomena of the language (Nerbonne et al., 1988; Cooper et al., 1996; Lehmann et al., 1996). Once the basic grammar is implemented, including an appropriate lexicon, the direction of grammar development becomes less clear. Integration of a grammar in a particular application and the use of a particular corpus can guide grammar development: the corpus and application will require the implementation of specific constructions and lexical items, as well as the reevaluation of existing analyses. To streamline this sort of output-driven development, tools to examine parser output over large corpora are necessary, and as corpus size increases, th</context>
</contexts>
<marker>Nerbonne, Flickinger, Wasow, 1988</marker>
<rawString>John Nerbonne, Dan Flickinger, and Tom Wasow. 1988. The HP Labs natural language evaluation tool. In Proceedings of the Workshop on Evaluation of Natural Language Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy Holloway King</author>
<author>Ronald Kaplan</author>
<author>Dick Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="13098" citStr="Riezler et al., 2002" startWordPosition="2063" endWordPosition="2066">e already substantial morphology, with correct inflection. Approximately ∼1300 of these were verbs. The decision to eliminate verbs as possible guessed parts of speech was directly motivated by data extracted using Oceanography. Since the guesser works with regular expressions (e.g. lowercase letters + s form plural nouns), it is possible to encounter forms in the corpus that neither the morphology nor the guesser recognize. The grammar will fragment on these sentences, creating well-formed f-structure chunks but no single spanning parse, and the unrecognized forms will be recorded as TOKENs (Riezler et al., 2002). An Oceanography run extracting all TOKENs resulted in the addition of several new patterns to the guesser as well as the addition of some of the frequent forms to the morphology. For example, sequences of all upper case letters followed by a hyphen and then by a sequence of digits were added for forms like AK-47, F-22, and V-1. The guesser and TOKENs Oceanography runs look for general problems with the morphology and lexicon, and can be run for every new corpus. More specific jobs are run when evaluating whether to implement a new analysis, or when evaluating whether a current analysis is fu</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy Holloway King, Ronald Kaplan, Dick Crouch, John T. Maxwell III, and Mark Johnson. 2002. Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation techniques. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Error mining for widecoverage grammar engineering.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>van Noord, 2004</marker>
<rawString>Gertjan van Noord. 2004. Error mining for widecoverage grammar engineering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott A Waterman</author>
</authors>
<title>Distributed parse mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing.</booktitle>
<contexts>
<context position="5373" citStr="Waterman, 2009" startWordPosition="826" endWordPosition="827">e differs from querying gold-standard treebanks in that corpora of actual parser output are queried to examine how constructions are analyzed by the grammar. For example, Bouma and Kloosterman (2002) use XQuery (an XML query language) to mine parse results stored as XML data.3 It is this sort of examination of parser output that is the focus of the present paper, and specific examples of our experiences follow in Section 2.2. Use of such tools has proven vital to the development of large-scale grammars. Based on our experiences with them, we began extensively using a tool called Oceanography (Waterman, 2009) to search parser output for very large (approximately 125 million sentence) parse runs stored on a distributed file system. Oceanography queries the parser output and returns counts of specific constructions or properties, as well as the example sentences they were extracted from. In the subsequent sections we discuss how this tool (in conjunction with existing ones like the van Noord Tool and Tiger Search) has enhanced grammar development for an English-language Lexical3See also (Bouma and Kloosterman, 2007) for further discussion of this technique. Functional Grammar used for a semantic sea</context>
</contexts>
<marker>Waterman, 2009</marker>
<rawString>Scott A. Waterman. 2009. Distributed parse mining. In Proceedings of the NAACL Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>