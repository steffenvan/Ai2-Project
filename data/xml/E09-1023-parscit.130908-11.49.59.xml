<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000068">
<title confidence="0.994894">
Correcting Dependency Annotation Errors
</title>
<author confidence="0.994349">
Markus Dickinson
</author>
<affiliation confidence="0.998544">
Indiana University
</affiliation>
<address confidence="0.50173">
Bloomington, IN, USA
</address>
<email confidence="0.995574">
md7@indiana.edu
</email>
<sectionHeader confidence="0.99381" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999826928571429">
Building on work detecting errors in de-
pendency annotation, we set out to correct
local dependency errors. To do this, we
outline the properties of annotation errors
that make the task challenging and their
existence problematic for learning. For
the task, we define a feature-based model
that explicitly accounts for non-relations
between words, and then use ambiguities
from one model to constrain a second,
more relaxed model. In this way, we are
successfully able to correct many errors,
in a way which is potentially applicable to
dependency parsing more generally.
</bodyText>
<sectionHeader confidence="0.985969" genericHeader="keywords">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.999980093750001">
Annotation error detection has been explored for
part-of-speech (POS), syntactic constituency, se-
mantic role, and syntactic dependency annotation
(see Boyd et al., 2008, and references therein).
Such work is extremely useful, given the harm-
fulness of annotation errors for training, including
the learning of noise (e.g., Hogan, 2007; Habash
et al., 2007), and for evaluation (e.g., Padro and
Marquez, 1998). But little work has been done
to show the full impact of errors, or what types
of cases are the most damaging, important since
noise can sometimes be overcome (cf. Osborne,
2002). Likewise, it is not clear how to learn from
consistently misannotated data; studies often only
note the presence of errors or eliminate them from
evaluation (e.g., Hogan, 2007), and a previous at-
tempt at correction was limited to POS annotation
(Dickinson, 2006). By moving from annotation
error detection to error correction, we can more
fully elucidate ways in which noise can be over-
come and ways it cannot.
We thus explore annotation error correction and
its feasibility for dependency annotation, a form
of annotation that provides argument relations
among words and is useful for training and testing
dependency parsers (e.g., Nivre, 2006; McDonald
and Pereira, 2006). A recent innovation in depen-
dency parsing, relevant here, is to use the predic-
tions made by one model to refine another (Nivre
and McDonald, 2008; Torres Martins et al., 2008).
This general notion can be employed here, as dif-
ferent models of the data have different predictions
about whch parts are erroneous and can highlight
the contributions of different features. Using dif-
ferences that complement one another, we can be-
gin to sort accurate from inaccurate patterns, by
integrating models in such a way as to learn the
true patterns and not the errors. Although we focus
on dependency annotation, the methods are poten-
tially applicable for different types of annotation,
given that they are based on the similar data repre-
sentations (see sections 2.1 and 3.2).
In order to examine the effects of errors and
to refine one model with another’s information,
we need to isolate the problematic cases. The
data representation must therefore be such that it
clearly allows for the specific identification of er-
rors between words. Thus, we explore relatively
simple models of the data, emphasizing small sub-
structures (see section 3.2). This simple model-
ing is not always rich enough for full dependency
parsing, but different models can reveal conflict-
ing information and are generally useful as part of
a larger system. Graph-based models of depen-
dency parsing (e.g., McDonald et al., 2006), for
example, rely on breaking parsing down into deci-
sions about smaller substructures, and focusing on
pairs of words has been used for domain adapta-
tion (Chen et al., 2008) and in memory-based pars-
ing (Canisius et al., 2006). Exploring annotation
error correction in this way can provide insights
into more general uses of the annotation, just as
previous work on correction for POS annotation
(Dickinson, 2006) led to a way to improve POS
</bodyText>
<note confidence="0.9229695">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 193–201,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999035">
193
</page>
<bodyText confidence="0.998081727272727">
tagging (Dickinson, 2007).
After describing previous work on error detec-
tion and correction in section 2, we outline in sec-
tion 3 how we model the data, focusing on individ-
ual relations between pairs of words. In section 4,
we illustrate the difficulties of error correction and
show how simple combinations of local features
perform poorly. Based on the idea that ambigui-
ties from strict, lexical models can constrain more
general POS models, we see improvement in error
correction in section 5.
</bodyText>
<sectionHeader confidence="0.7785615" genericHeader="introduction">
2 Background
2.1 Error detection
</sectionHeader>
<bodyText confidence="0.999973510638298">
We base our method of error correction on a
form of error detection for dependency annota-
tion (Boyd et al., 2008). The variation n-gram ap-
proach was developed for constituency-based tree-
banks (Dickinson and Meurers, 2003, 2005) and
it detects strings which occur multiple times in
the corpus with varying annotation, the so-called
variation nuclei. For example, the variation nu-
cleus next Tuesday occurs three times in the Wall
Street Journal portion of the Penn Treebank (Tay-
lor et al., 2003), twice labeled as NP and once as
PP (Dickinson and Meurers, 2003).
Every variation detected in the annotation of a
nucleus is classified as either an annotation error
or as a genuine ambiguity. The basic heuristic
for detecting errors requires one word of recur-
ring context on each side of the nucleus. The nu-
cleus with its repeated surrounding context is re-
ferred to as a variation n-gram. While the original
proposal expanded the context as far as possible
given the repeated n-gram, using only the immedi-
ately surrounding words as context is sufficient for
detecting errors with high precision (Boyd et al.,
2008). This “shortest” context heuristic receives
some support from research on first language ac-
quisition (Mintz, 2006) and unsupervised gram-
mar induction (Klein and Manning, 2002).
The approach can detect both bracketing and la-
beling errors in constituency annotation, and we
already saw a labeling error for next Tuesday. As
an example of a bracketing error, the variation nu-
cleus last month occurs within the NP its biggest
jolt last month once with the label NP and once as
a non-constituent, which in the algorithm is han-
dled through a special label NIL.
The method for detecting annotation errors can
be extended to discontinuous constituency annota-
tion (Dickinson and Meurers, 2005), making it ap-
plicable to dependency annotation, where words
in a relation can be arbitrarily far apart. Specifi-
cally, Boyd et al. (2008) adapt the method by treat-
ing dependency pairs as variation nuclei, and they
include NIL elements for pairs of words not an-
notated as a relation. The method is successful
at detecting annotation errors in corpora for three
different languages, with precisions of 93% for
Swedish, 60% for Czech, and 48% for German.1
</bodyText>
<subsectionHeader confidence="0.987817">
2.2 Error correction
</subsectionHeader>
<bodyText confidence="0.999972818181818">
Correcting POS annotation errors can be done by
applying a POS tagger and altering the input POS
tags (Dickinson, 2006). Namely, ambiguity class
information (e.g., IN/RB/RP) is added to each cor-
pus position for training, creating complex ambi-
guity tags, such as &lt;IN/RB/RP,IN&gt;. While this
results in successful correction, it is not clear how
it applies to annotation which is not positional and
uses NIL labels. However, ambiguity class infor-
mation is relevant when there is a choice between
labels; we return to this in section 5.
</bodyText>
<sectionHeader confidence="0.706241" genericHeader="method">
3 Modeling the data
</sectionHeader>
<subsectionHeader confidence="0.999548">
3.1 The data
</subsectionHeader>
<bodyText confidence="0.999600631578947">
For our data set, we use the written portion (sec-
tions P and G) of the Swedish Talbanken05 tree-
bank (Nivre et al., 2006), a reconstruction of the
Talbanken76 corpus (Einarsson, 1976) The written
data of Talbanken05 consists of 11,431 sentences
with 197,123 tokens, annotated using 69 types of
dependency relations.
This is a small sample, but it matches the
data used for error detection, which results in
634 shortest non-fringe variation n-grams, corre-
sponding to 2490 tokens. From a subset of 210
nuclei (917 tokens), hand-evaluation reveals error
detection precision to be 93% (195/210), with 274
(of the 917) corpus positions in need of correction
(Boyd et al., 2008). This means that 643 positions
do not need to be corrected, setting a baseline of
70.1% (643/917) for error correction.2 Following
Dickinson (2006), we train our models on the en-
tire corpus, explicitly including NIL relations (see
</bodyText>
<footnote confidence="0.9974455">
1The German experiment uses a more relaxed heuristic;
precision is likely higher with the shortest context heuristic.
2Detection and correction precision are different measure-
ments: for detection, it is the percentage of variation nuclei
types where at least one is incorrect; for correction, it is the
percentage of corpus tokens with the true (corrected) label.
</footnote>
<page confidence="0.998333">
194
</page>
<bodyText confidence="0.9760065">
section 3.2); we train on the original annotation,
but not the corrections.
</bodyText>
<subsectionHeader confidence="0.998622">
3.2 Individual relations
</subsectionHeader>
<bodyText confidence="0.999994982142857">
Annotation error correction involves overcoming
noise in the corpus, in order to learn the true
patterns underlying the data. This is a slightly
different goal from that of general dependency
parsing methods, which often integrate a vari-
ety of features in making decisions about depen-
dency relations (cf., e.g., Nivre, 2006; McDon-
ald and Pereira, 2006). Instead of maximizing a
feature model to improve parsing, we isolate in-
dividual pieces of information (e.g., context POS
tags), thereby being able to pinpoint, for example,
when non-local information is needed for particu-
lar types of relations and pointing to cases where
pieces of information conflict (cf. also McDonald
and Nivre, 2007).
To support this isolation of information, we use
dependency pairs as the basic unit of analysis and
assign a dependency label to each word pair. Fol-
lowing Boyd et al. (2008), we add L or R to the
label to indicate which word is the head, the left
(L) or the right (R). This is tantamount to han-
dling pairs of words as single entries in a “lex-
icon” and provides a natural way to talk of am-
biguities. Breaking the representation down into
strings whch receive a label also makes the method
applicable to other annotation types (e.g., Dickin-
son and Meurers, 2005).
A major issue in generating a lexicon is how
to handle pairs of words which are not dependen-
cies. We follow Boyd et al. (2008) and generate
NIL labels for those pairs of words which also
occur as a true labeled relation. In other words,
only word pairs which can be relations can also be
NILs. For every sentence, then, when we produce
feature lists (see section 3.3), we produce them for
all word pairs that are related or could potentially
be related, but not those which have never been
observed as a dependency pair. This selection of
NIL items works because there are no unknown
words. We use the method in Dickinson and Meur-
ers (2005) to efficiently calculate the NIL tokens.
Focusing on word pairs and not attempting to
build a a whole dependency graph allows us to ex-
plore the relations between different kinds of fea-
tures, and it has the potential benefit of not rely-
ing on possibly erroneous sister relations. From
the perspective of error correction, we cannot as-
sume that information from the other relations in
the sentence is reliable.3 This representation also
fits nicely with previous work, both in error de-
tection (see section 2.1) and in dependency pars-
ing (e.g., Canisius et al., 2006; Chen et al., 2008).
Most directly, Canisius et al. (2006) integrate such
a representation into a memory-based dependency
parser, treating each pair individually, with words
and POS tags as features.
</bodyText>
<subsectionHeader confidence="0.999982">
3.3 Method of learning
</subsectionHeader>
<bodyText confidence="0.99295005">
We employ memory-based learning (MBL) for
correction. MBL stores all corpus instances as
vectors of features, and given a new instance, the
task of the classifier is to find the most similar
cases in memory to deduce the best class. Given
the previous discussion of the goals of correcting
errors, what seems to be needed is a way to find
patterns which do not fully generalize because of
noise appearing in very similar cases in the cor-
pus. As Zavrel et al. (1997, p. 137) state about the
advantages of MBL:
Because language-processing tasks typ-
ically can only be described as a com-
plex interaction of regularities, sub-
regularities and (families of) exceptions,
storing all empirical data as potentially
useful in analogical extrapolation works
better than extracting the main regulari-
ties and forgetting the individual exam-
ples (Daelemans, 1996).
By storing all corpus examples, as MBL does,
both correct and incorrect data is maintained, al-
lowing us to pinpoint the effect of errors on train-
ing. For our experiments, we use TiMBL, version
6.1 (Daelemans et al., 2007), with the default set-
tings. We use the default overlap metric, as this
maintains a direct connection to majority-based
correction. We could run TiMBL with different
values of k, as this should lead to better feature
integration. However, this is difficult to explore
without development data, and initial experiments
with higher k values were not promising (see sec-
tion 4.2).
To fully correct every error, one could also ex-
periment with a real dependency parser in the fu-
ture, in order to look beyond the immediate con-
text and to account for interactions between rela-
3We use POS information, which is also prone to errors,
but on a different level of annotation. Still, this has its prob-
lems, as discussed in section 4.1.
</bodyText>
<page confidence="0.988777">
195
</page>
<figure confidence="0.954938181818182">
v¨ag/NN
l˚ang
v¨ag
way
en
a
long
och
way and
¨ar/AV
is
</figure>
<bodyText confidence="0.999954666666667">
tions. The approach to correction pursued here,
however, isolates problems for assigning depen-
dency structures, highlighting the effectiveness of
different features within the same local domain.
Initial experiments with a dependency parser were
again not promising (see section 4.2).
</bodyText>
<subsectionHeader confidence="0.986049">
3.4 Integrating features
</subsectionHeader>
<bodyText confidence="0.9999848">
When using features for individual relations, we
have different options for integrating them. On
the one hand, one can simply additively combine
features into a larger vector for training, as de-
scribed in section 4.2. On the other hand, one can
use one set of features to constrain another set,
as described in section 5. Pulling apart the fea-
tures commonly employed in dependency parsing
can help indicate the contributions each has on the
classification.
This general idea is akin to the notion of clas-
sifier stacking, and in the realm of dependency
parsing, Nivre and McDonald (2008) successfully
stack classifiers to improve parsing by “allow[ing]
a model to learn relative to the predictions of the
other” (p. 951). The output from one classifier
is used as a feature in the next one (see also Tor-
res Martins et al., 2008). Nivre and McDonald
(2008) use different kinds of learning paradigms,
but the general idea can be carried over to a situ-
ation using the same learning mechanism. Instead
of focusing on what one learning algorithm in-
forms another about, we ask what one set of more
or less informative features can inform another set
about, as described in section 5.1.
</bodyText>
<sectionHeader confidence="0.941729" genericHeader="method">
4 Performing error correction
</sectionHeader>
<subsectionHeader confidence="0.924215">
4.1 Challenges
</subsectionHeader>
<bodyText confidence="0.999977325581395">
The task of automatic error correction in some
sense seems straightforward, in that there are no
unknown words. Furthermore, we are looking at
identical recurring words, which should for the
most part have consistent annotation. But it is pre-
cisely this similarity of local contexts that makes
the correction task challenging.
Given that variations contain sets of corpus po-
sitions with differing labels, it is tempting to take
the error detection output and use a heuristic of
“majority rules” for the correction cases, i.e., cor-
rect the cases to the majority label. When us-
ing only information from the word sequence, this
runs into problems quickly, however, in that there
are many non-majority labels which are correct.
Some of these non-majority cases pattern in uni-
form ways and are thus more correctable; oth-
ers are less tractable in being corrected, as they
behave in non-uniform and often non-local ways.
Exploring the differences will highlight what can
and cannot be easily corrected, underscoring the
difficulties in training from erroneous annotation.
Uniform non-majority cases The first problem
with correction to the majority label is an issue
of coverage: a large number of variations are ties
between two different labels. Out of 634 shortest
non-fringe variation nuclei, 342 (53.94%) have no
majority label; for the corresponding 2490 tokens,
749 (30.08%) have no majority tag.
The variation ¨ar v¨ag (’is way’), for example, ap-
pears twice with the same local context shown in
(1),4 once incorrectly labeled as OO-L (other ob-
ject [head on the left]) and once correctly as SP-
L (subjective predicative complement). To dis-
tinguish these two, more information is necessary
than the exact sequence of words. In this case, for
example, looking at the POS categories of the nu-
clei could potentially lead to accurate correction:
AV NN is SP-L 1032 times and OO-L 32 times
(AV = the verb “vara” (be), NN = other noun).
While some ties might require non-local informa-
tion, we can see that local—but more general—
information could accurately break this tie.
</bodyText>
<equation confidence="0.81063925">
(1) k¨arlekens
love’s
. . .
. . .
</equation>
<bodyText confidence="0.995043214285715">
Secondly, in a surprising number of cases where
there is a majority tag (122 out of the 917 tokens
we have a correction for), a non-majority label
is actually correct. For the example in (2), the
string institution kvarleva (‘institution remnant’)
varies between CC-L (sister of first conjunct in bi-
nary branching analysis of coordination) and AN-
L (apposition).5 CC-L appears 5 times and AN-L
3 times, but the CC-L cases are incorrect and need
to be changed to AN-L.
(2) en f¨or˚aldrad institution/NN
an obsolete institution
kvarleva/NN fr˚an 1800-talets
remnant from the 1800s
</bodyText>
<footnote confidence="0.99723">
4We put variation nuclei in bold and underline the imme-
diately surrounding context.
5Note that CC is a category introduced in the conversion
from the 1976 to the 2005 corpus.
</footnote>
<table confidence="0.469327">
,/IK en/EN
, a
</table>
<page confidence="0.994121">
196
</page>
<bodyText confidence="0.999601642857143">
Other cases with a non-majority label have
other problems. In example (3), for instance, the
string under h¨agnet (‘under protection’) varies in
this context between HD-L (other head, 3 cases)
and PA-L (complement of preposition, 5 cases),
where the PA-L cases need to be corrected to HD-
L. Both of these categories are new, so part of the
issue here could be in the consistency of the con-
version.
the other is idiomatic, despite having identical lo-
cal context. In these examples, at least the POS
labels are different. Note, though, that in (4) we
need to trust the POS labels to overcome the simi-
larity of text, and in (3) we need to distrust them.6
</bodyText>
<listItem confidence="0.571073">
(4) a. Med/PR andra ord/NN en
</listItem>
<bodyText confidence="0.848906333333333">
with other words an
¨andam˚alsenlig ...
appropriate
(3) fria liv under/PR h¨agnet/ID|NN b. Med/AB andra ord/ID en form av
free life under the protection with other words a form of
av/IDJPR ett en g˚ang givet l¨ofte prostitution .
of a one time given promise prostitution
The additional problem is that there are other,
correlated errors in the analysis, as shown in fig-
ure 1. In the case of the correct HD analysis, both
h¨agnet and av are POS-annotated as ID (part of id-
iom (multi-word unit)) and are HD dependents of
under, indicating that the three words make up an
idiom. The PA analysis is a non-idiomatic analy-
sis, with h¨agnet as NN.
Without non-local information, some legitimate
variations are virtually irresolvable. Consider (5),
for instance: here, we find variation between SS-R
(other subject), as in (5a), and FS-R (dummy sub-
ject), as in (5b). Crucially, the POS tags are the
same, and the context is the same. What differen-
tiates these cases is that g˚ar has a different set of
dependents in the two sentences, as shown in fig-
ure 2; to use this information would require us to
trust the rest of the dependency structure or to use
a dependency parser which accurately derives the
structural differences.
</bodyText>
<figure confidence="0.982265">
AT ET HD HD
fria liv under h¨agnet av ...
AJ NN PR ID ID
AT ET PA PA
fria liv under h¨agnet av ...
AJ NN PR NN PR
‘It just doesn’t add up.’
b. Det/PO
it
˚ar/VV inte
g bara
goes just
not
att
to
h˚alla
hold
(5) a. Det/PO
it
g bara
˚ar/VV inte
goes just not
ihop .
together
ihop ...
</figure>
<figureCaption confidence="0.999987">
Figure 1: Erroneous POS &amp; dependency variation together ...
</figureCaption>
<bodyText confidence="0.9998934">
Significantly, h¨agnet only appears 10 times in
the corpus, all with under as its head, 5 times HD-
L and 5 times PA-L. We will not focus explicitly
on correcting these types of cases, but the example
serves to emphasize the necessity of correction at
all levels of annotation.
Non-uniform non-majority cases All of the
above cases have in common that whatever change
is needed, it needs to be done for all positions in a
variation. But this is not sound, as error detection
precision is not 100%. Thus, there are variations
which clearly must not change.
For example, in (4), there is legitimate varia-
tion between PA-L (4a) and HD-L (4b), stemming
from the fact that one case is non-idiomatic, and
</bodyText>
<subsectionHeader confidence="0.999149">
4.2 Using local information
</subsectionHeader>
<bodyText confidence="0.999961083333333">
While some variations require non-local informa-
tion, we have seen that some cases are correctable
simply with different kinds of local information
(cf. (1)). In this paper, we will not attempt to
directly cover non-local cases or cases with POS
annotation problems, instead trying to improve the
integration of different pieces of local information.
In our experiments, we trained simple models of
the original corpus using TiMBL (see section 3.3)
and then tested on the same corpus. The models
we use include words (W) and/or tags (T) for nu-
cleus and/or context positions, where context here
</bodyText>
<footnote confidence="0.9690075">
6Rerunning the experiments in the paper by first running
a POS tagger showed slight degradations in precision.
</footnote>
<page confidence="0.994999">
197
</page>
<figureCaption confidence="0.999767">
Figure 2: Correct dependency variation
</figureCaption>
<bodyText confidence="0.953527375">
refers only to the immediately surrounding words.
These are outlined in table 1, for different mod-
els of the nucleus (Nuc.) and the context (Con.).
For instance, the model 6 representation of exam-
ple (6) (=(1)) consists of all the underlined words
and tags.
(6) k¨arlekens v¨ag/NN ¨ar/AV en/EN l˚ang/AJ
v¨ag/NN och/++ man g¨or oklokt ...
In table 1, we report the precision figures for
different models on the 917 positions we have
corrections for. We report the correction preci-
sion for positions the classifier changed the label
of (Changed), and the overall correction precision
(Overall). We also report the precision TiMBL has
for the whole corpus, with respect to the original
tags (instead of the corrected tags).
</bodyText>
<table confidence="0.997423111111111">
# Nuc. Con. TiMBL Changed Overall
1 W - 86.6% 34.0% 62.5%
2 W, T - 88.1% 35.9% 64.8%
3 W W 99.8% 50.3% 72.7%
4 W W, T 99.9% 52.6% 73.5%
5 W, T W 99.9% 50.8% 72.4%
6 W, T W, T 99.9% 51.2% 72.6%
7 T - 73.4% 20.1% 49.5%
8 T T 92.7% 50.2% 73.2%
</table>
<tableCaption confidence="0.999866">
Table 1: The models tested
</tableCaption>
<bodyText confidence="0.999939047619047">
We can draw a few conclusions from these re-
sults. First, all models using contexual informa-
tion perform essentially the same—approximately
50% on changed positions and 73% overall. When
not generalizing to new data, simply adding fea-
tures (i.e., words or tags) to the model is less im-
portant than the sheer presence of context. This
is true even for some higher values of k: model
6, for example, has only 73.2% and 72.1% overall
precision for k = 2 and k = 3, respectively.
Secondly, these results confirm that the task is
difficult, even for a corpus with relatively high er-
ror detection precision (see section 2.1). Despite
high similarity of context (e.g., model 6), the best
results are only around 73%, and this is given a
baseline (no changes) of 70%. While a more ex-
pansive set of features would help, there are other
problems here, as the method appears to be over-
training. There is no question that we are learning
the “correct” patterns, i.e., 99.9% similarity to the
benchmark in the best cases. The problem is that,
for error correction, we have to overcome noise in
the data. Training and testing with the dependency
parser MaltParser (Nivre et al., 2007, default set-
tings) is no better, with 72.1% overall precision
(despite a labeled attachment score of 98.3%).
Recall in this light that there are variations for
which the non-majority label is the correct one;
attempting to get a non-majority label correct us-
ing a strict lexical model does not work. To be
able not to learn the erroneous patterns requires
a more general model. Interestingly, a more gen-
eral model—e.g., treating the corpus as a sequence
of tags (model 8)—results in equally good correc-
tion, without being a good overall fit to the cor-
pus data (only 92.7%). This model, too, learns
noise, as it misses cases that the lexical models get
correct. Simply combining the features does not
help (cf. model 6); what we need is to use infor-
mation from both stricter and looser models in a
way that allows general patterns to emerge with-
out overgeneralizing.
</bodyText>
<sectionHeader confidence="0.920266" genericHeader="method">
5 Model combination
</sectionHeader>
<bodyText confidence="0.999710384615385">
Given the discussion in section 4.1 surrounding
examples (1)-(5), it is clear that the information
needed for correction is sometimes within the
immediate context, although that information is
needed, however, is often different. Consider the
more general models, 7 and 8, which only use POS
tag information. While sometimes this general in-
formation is effective, at times it is dramatically
incorrect. For example, for (7), the original (incor-
rect) relation between finna and erbjuda is CC-L;
the model 7 classifier selects OO-L as the correct
tag; model 8 selects NIL; and the correct label is
+F-L (coordination at main clause level).
</bodyText>
<table confidence="0.7244385">
SS MA NA PL
Det g˚ar bara inte ihop
PO VV AB AB AB
FS CA NA IM ES
Det g˚ar bara inte att h˚alla ...
PO VV AB AB IM VV
</table>
<page confidence="0.907592">
198
</page>
<bodyText confidence="0.945420928571428">
(7) f¨ors¨oker
try
¨oppna marknaden eller erbjuda/VV andra
open market or to offer other
arbetsm¨ojligheter .
work possibilities
The original variation for the nucleus finna erb-
juda (‘find offer’) is between CC-L and +F-L, but
when represented as the POS tags VV VV (other
verb), there are 42 possible labels, with OO-L be-
ing the most frequent. This allows for too much
confusion. If model 7 had more restrictions on the
set of allowable tags, it could make a more sensi-
ble choice and, in this case, select the correct label.
</bodyText>
<subsectionHeader confidence="0.999846">
5.1 Using ambiguity classes
</subsectionHeader>
<bodyText confidence="0.999795210526316">
Previous error correction work (Dickinson, 2006)
used ambiguity classes for POS annotation, and
this is precisely the type of information we need
to constrain the label to one which we know is rel-
evant to the current case. Here, we investigate am-
biguity class information derived from one model
integrated into another model.
There are at least two main ways we can use
ambiguity classes in our models. The first is what
we have just been describing: an ambiguity class
can serve as a constraint on the set of possible out-
comes for the system. If the correct label is in the
ambiguity class (as it usually is for error correc-
tion), this constraining can do no worse than the
original model. The other way to use an ambigu-
ity class is as a feature in the model. The success
of this approach depends on whether or not each
ambiguity class patterns in its own way, i.e., de-
fines a sub-regularity within a feature set.
</bodyText>
<subsectionHeader confidence="0.999468">
5.2 Experiment details
</subsectionHeader>
<bodyText confidence="0.9999979">
We consider two different feature models, those
containing only tags (models 7 and 8), and add
to these ambiguity classes derived from two other
models, those containing only words (models 1
and 3). To correct the labels, we need models
which do not strictly adhere to the corpus, and the
tag-based models are best at this (see the TiMBL
results in table 1). The ambiguity classes, how-
ever, must be fairly constrained, and the word-
based models do this best (cf. example (7)).
</bodyText>
<subsectionHeader confidence="0.708852">
5.2.1 Ambiguity classes as constraints
</subsectionHeader>
<bodyText confidence="0.999449045454546">
As described in section 5.1, we can use ambiguity
classes to constrain the output of a model. Specif-
ically, we take models 7 and 8 and constrain each
selected tag to be one which is within the ambi-
guity class of a lexical model, either 1 or 3. That
is, if the TiMBL-determined label is not in the am-
biguity class, we select the most likely tag of the
ones which are. If no majority label can be de-
cided from this restricted set, we fall back to the
TiMBL-selected tag. In (7), for instance, if we use
model 7, the TiMBL tag is OO-L, but model 3’s
ambiguity class restricts this to either CC-L or +F-
L. For the representation VV VV, the label CC-L
appears 315 times and +F-L 544 times, so +F-L is
correctly selected.7
The results are given in table 2, which can be
compared to the the original models 7 and 8 in ta-
ble 1, i.e., total precisions of 49.5% and 73.2%,
respectively. With these simple constraints, model
8 now outperforms any other model (75.5%), and
model 7 begins to approach all the models that use
contextual information (68.8%).
</bodyText>
<table confidence="0.995358">
# AC Changed Total
7 1 28.5% (114/400) 57.4% (526/917)
7 3 45.9% (138/301) 68.8% (631/917)
8 1 54.0% (142/263) 74.8% (686/917)
8 3 56.7% (144/254) 75.5% (692/917)
</table>
<tableCaption confidence="0.998176">
Table 2: Constraining TiMBL with ACs
</tableCaption>
<subsubsectionHeader confidence="0.68615">
5.2.2 Ambiguity classes as features
</subsubsectionHeader>
<bodyText confidence="0.999313">
Ambiguity classes from one model can also be
used as features for another (see section 5.1); in
this case, ambiguity class information from lexical
models (1 and 3) is used as a feature for POS tag
models (7 and 8). The results are given in table 3,
where we can see dramatically improved perfor-
mance from the original models (cf. table 1) and
generally improved performance over using ambi-
guity classes as constraints (cf. table 2).
</bodyText>
<table confidence="0.9986002">
# AC Changed Total
7 1 33.2% (122/368) 61.9% (568/917)
7 3 50.2% (131/261) 72.1% (661/917)
8 1 59.0% (148/251) 76.4% (701/917)
8 3 55.1% (130/236) 73.6% (675/917)
</table>
<tableCaption confidence="0.999956">
Table 3: TiMBL with ACs as features
</tableCaption>
<bodyText confidence="0.997034333333333">
If we compare the two results for model 7
(61.9% vs. 72.1%) and then the two results for
model 8 (76.4% vs. 73.6%), we observe that the
</bodyText>
<footnote confidence="0.9188475">
7Even if CC-L had been selected here, the choice is sig-
nificantly better than OO-L.
</footnote>
<table confidence="0.738549">
finna/VV ett l¨ampligt arbete i
to find a suitable job in
</table>
<page confidence="0.995644">
199
</page>
<bodyText confidence="0.9999159375">
better use of ambiguity classes integrates contex-
tual and non-contextual features. Model 7 (POS,
no context) with model 3 ambiguity classes (lex-
ical, with context) is better than using ambiguity
classes derived from a non-contextual model. For
model 8, on the other hand, which uses contextual
POS features, using the ambiguity class without
context (model 1) does better. In some ways, this
combination of model 8 with model 1 ambiguity
classes makes the most sense: ambiguity classes
are derived from a lexicon, and for dependency an-
notation, a lexicon can be treated as a set of pairs
of words. It is also noteworthy that model 7, de-
spite not using context directly, achieves compara-
ble results to all the previous models using context,
once appropriate ambiguity classes are employed.
</bodyText>
<sectionHeader confidence="0.708156" genericHeader="method">
5.2.3 Both methods
</sectionHeader>
<bodyText confidence="0.999831545454545">
Given that the results of ambiguity classes as fea-
tures are better than that of constraining, we can
now easily combine both methodologies, by con-
straining the output from section 5.2.2 with the
ambiguity class tags. The results are given in ta-
ble 4; as we can see, all results are a slight im-
provement over using ambiguity classes as fea-
tures without constraining the output (table 3). Us-
ing only local context, the best model here is 3.2%
points better than the best original model, repre-
senting an improvement in correction.
</bodyText>
<table confidence="0.991756">
# AC Changed Total
7 1 33.5% (123/367) 62.2% (570/917)
7 3 55.8% (139/249) 74.1% (679/917)
8 1 59.6% (149/250) 76.7% (703/917)
8 3 57.1% (133/233) 74.3% (681/917)
</table>
<tableCaption confidence="0.999408">
Table 4: TiMBL w/ ACs as features &amp; constraints
</tableCaption>
<sectionHeader confidence="0.965893" genericHeader="conclusions">
6 Summary and Outlook
</sectionHeader>
<bodyText confidence="0.999980111111111">
After outlining the challenges of error correction,
we have shown how to integrate information from
different models of dependency annotation in or-
der to perform annotation error correction. By us-
ing ambiguity classes from lexical models, both as
features and as constraints on the final output, we
saw improvements in POS models that were able
to overcome noise, without using non-local infor-
mation.
A first step in further validating these methods
is to correct other dependency corpora; this is lim-
ited, of course, by the amount of corpora with cor-
rected data available. Secondly, because this work
is based on features and using ambiguity classes, it
can in principle be applied to other types of anno-
tation, e.g., syntactic constituency annotation and
semantic role annotation. In this light, it is inter-
esting to note the connection to annotation error
detection: the work here is in some sense an ex-
tension of the variation n-gram method. Whether
it can be employed as an error detection system on
its own requires future work.
Another way in which this work can be ex-
tended is to explore how these representations and
integration of features can be used for dependency
parsing. There are several issues to work out, how-
ever, in making insights from this work more gen-
eral. First, it is not clear that pairs of words are suf-
ficiently general to treat them as a lexicon, when
one is parsing new data. Secondly, we have ex-
plicit representations for word pairs not annotated
as a dependency relation (i.e., NILs), and these are
constrained by looking at those which are the same
words as real relations. Again, one would have to
determine which pairs of words need NIL repre-
sentations in new data.
</bodyText>
<sectionHeader confidence="0.99473" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996014">
Thanks to Yvonne Samuelsson for help with the
Swedish examples; to Joakim Nivre, Mattias Nils-
son, and Eva Pettersson for the evaluation data for
Talbanken05; and to the three anonymous review-
ers for their insightful comments.
</bodyText>
<sectionHeader confidence="0.998599" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999702055555556">
Boyd, Adriane, Markus Dickinson and Detmar
Meurers (2008). On Detecting Errors in Depen-
dency Treebanks. Research on Language and
Computation 6(2), 113–137.
Canisius, Sander, Toine Bogers, Antal van den
Bosch, Jeroen Geertzen and Erik Tjong Kim
Sang (2006). Dependency parsing by infer-
ence over high-recall dependency predictions.
In Proceedings of CoNLL-X. New York.
Chen, Wenliang, Youzheng Wu and Hitoshi Isa-
hara (2008). Learning Reliable Information for
Dependency Parsing Adaptation. In Proceed-
ings of Coling 2008. Manchester.
Daelemans, Walter (1996). Abstraction Consid-
ered Harmful: Lazy Learning of Language Pro-
cessing. In Proceedings of the 6th Belgian-
Dutch Conference on Machine Learning. Maas-
tricht, The Netherlands.
</reference>
<page confidence="0.939713">
200
</page>
<reference confidence="0.999795224719101">
Daelemans, Walter, Jakub Zavrel, Ko Van der
Sloot and Antal Van den Bosch (2007). TiMBL:
Tilburg Memory Based Learner, version 6.1,
Reference Guide. Tech. rep., ILK Research
Group. ILK Research Group Technical Report
Series no. 07-07.
Dickinson, Markus (2006). From Detecting Errors
to Automatically Correcting Them. In Proceed-
ings of EACL-06. Trento, Italy.
Dickinson, Markus (2007). Determining Ambigu-
ity Classes for Part-of-Speech Tagging. In Pro-
ceedings of RANLP-07. Borovets, Bulgaria.
Dickinson, Markus and W. Detmar Meurers
(2003). Detecting Inconsistencies in Treebanks.
In Proceedings of TLT-03. V¨axj¨o, Sweden.
Dickinson, Markus and W. Detmar Meurers
(2005). Detecting Errors in Discontinuous
Structural Annotation. In Proceedings of ACL-
Einarsson, Jan (1976). Talbankens skrift-
sprøakskonkordans. Tech. rep., Lund Univer-
sity, Dept. of Scandinavian Languages.
Habash, Nizar, Ryan Gabbard, Owen Rambow,
Seth Kulick and Mitch Marcus (2007). Deter-
mining Case in Arabic: Learning Complex Lin-
guistic Behavior Requires Complex Linguistic
Features. In Proceedings of EMNLP-07.
Hogan, Deirdre (2007). Coordinate Noun Phrase
Disambiguation in a Generative Parsing Model.
In Proceedings of ACL-07. Prague.
Klein, Dan and Christopher D. Manning (2002). A
Generative Constituent-Context Model for Im-
proved Grammar Induction. In Proceedings of
ACL-02. Philadelphia, PA.
McDonald, Ryan, Kevin Lerman and Fernando
Pereira (2006). Multilingual Dependency Anal-
ysis with a Two-Stage Discriminative Parser. In
Proceedings of CoNLL-X. New York City.
McDonald, Ryan and Joakim Nivre (2007). Char-
acterizing the Errors of Data-Driven Depen-
dency Parsing Models. In Proceedings of
EMNLP-CoNLL-07. Prague, pp. 122–131.
McDonald, Ryan and Fernando Pereira (2006).
Online learning of approximate dependency
parsing algorithms. In Proceedings of EACL-
6. Trento.
Mintz, Toben H. (2006). Finding the verbs: dis-
tributional cues to categories available to young
learners. In K. Hirsh-Pasek and R. M. Golinkoff
(eds.), Action Meets Word: How Children Learn
Verbs, New York: Oxford University Press, pp.
31–63.
Nivre, Joakim (2006). Inductive Dependency
Parsing. Berlin: Springer.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, Gulsen Eryigit, Sandra Kubler, Sve-
toslav Marinov and Erwin Marsi (2007). Malt-
Parser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering 13(2), 95–135.
Nivre, Joakim and Ryan McDonald (2008). Inte-
grating Graph-Based and Transition-Based De-
pendency Parsers. In Proceedings of ACL-08:
HLT. Columbus, OH.
Nivre, Joakim, Jens Nilsson and Johan Hall
(2006). Talbanken05: A Swedish Treebank
with Phrase Structure and Dependency Annota-
tion. In Proceedings of LREC-06. Genoa, Italy.
Osborne, Miles (2002). Shallow Parsing using
Noisy and Non-Stationary Training Material. In
JMLR Special Issue on Machine Learning Ap-
proaches to Shallow Parsing, vol. 2, pp. 695–
719.
Padro, Lluis and Lluis Marquez (1998). On the
Evaluation and Comparison of Taggers: the Ef-
fect of Noise in Testing Corpora. In Proceed-
ings of ACL-COLING-98. San Francisco, CA.
Taylor, Ann, Mitchell Marcus and Beatrice San-
torini (2003). The Penn Treebank: An
Overview. In Anne Abeill´e (ed.), Treebanks:
Building and using syntactically annotated cor-
pora, Dordrecht: Kluwer, chap. 1, pp. 5–22.
Torres Martins, Andr´e Filipe, Dipanjan Das,
Noah A. Smith and Eric P. Xing (2008). Stack-
ing Dependency Parsers. In Proceedings of
EMNLP-08. Honolulu, Hawaii, pp. 157–166.
Zavrel, Jakub, Walter Daelemans and Jorn Veensta
(1997). Resolving PP attachment Ambiguities
with Memory-Based Learning. In Proceedings
of CoNLL-97. Madrid.
</reference>
<page confidence="0.998404">
201
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987553">
<title confidence="0.999843">Correcting Dependency Annotation Errors</title>
<author confidence="0.999652">Markus Dickinson</author>
<affiliation confidence="1">Indiana University</affiliation>
<address confidence="0.998761">Bloomington, IN, USA</address>
<email confidence="0.998215">md7@indiana.edu</email>
<abstract confidence="0.9993954">Building on work detecting errors in dependency annotation, we set out to correct local dependency errors. To do this, we outline the properties of annotation errors that make the task challenging and their existence problematic for learning. For the task, we define a feature-based model that explicitly accounts for non-relations between words, and then use ambiguities from one model to constrain a second, more relaxed model. In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adriane Boyd</author>
</authors>
<title>Markus Dickinson and Detmar Meurers</title>
<date>2008</date>
<journal>Research on Language and Computation</journal>
<volume>6</volume>
<issue>2</issue>
<pages>113--137</pages>
<marker>Boyd, 2008</marker>
<rawString>Boyd, Adriane, Markus Dickinson and Detmar Meurers (2008). On Detecting Errors in Dependency Treebanks. Research on Language and Computation 6(2), 113–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sander Canisius</author>
</authors>
<title>Toine Bogers, Antal van den Bosch, Jeroen Geertzen and Erik Tjong Kim Sang</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<location>New York.</location>
<marker>Canisius, 2006</marker>
<rawString>Canisius, Sander, Toine Bogers, Antal van den Bosch, Jeroen Geertzen and Erik Tjong Kim Sang (2006). Dependency parsing by inference over high-recall dependency predictions. In Proceedings of CoNLL-X. New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
</authors>
<title>Youzheng Wu and Hitoshi Isahara</title>
<date>2008</date>
<booktitle>In Proceedings of Coling</booktitle>
<location>Manchester.</location>
<marker>Chen, 2008</marker>
<rawString>Chen, Wenliang, Youzheng Wu and Hitoshi Isahara (2008). Learning Reliable Information for Dependency Parsing Adaptation. In Proceedings of Coling 2008. Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
</authors>
<title>Abstraction Considered Harmful: Lazy Learning of Language Processing.</title>
<date>1996</date>
<booktitle>In Proceedings of the 6th BelgianDutch Conference on Machine Learning.</booktitle>
<location>Maastricht, The Netherlands.</location>
<contexts>
<context position="12267" citStr="Daelemans, 1996" startWordPosition="2005" endWordPosition="2006">. Given the previous discussion of the goals of correcting errors, what seems to be needed is a way to find patterns which do not fully generalize because of noise appearing in very similar cases in the corpus. As Zavrel et al. (1997, p. 137) state about the advantages of MBL: Because language-processing tasks typically can only be described as a complex interaction of regularities, subregularities and (families of) exceptions, storing all empirical data as potentially useful in analogical extrapolation works better than extracting the main regularities and forgetting the individual examples (Daelemans, 1996). By storing all corpus examples, as MBL does, both correct and incorrect data is maintained, allowing us to pinpoint the effect of errors on training. For our experiments, we use TiMBL, version 6.1 (Daelemans et al., 2007), with the default settings. We use the default overlap metric, as this maintains a direct connection to majority-based correction. We could run TiMBL with different values of k, as this should lead to better feature integration. However, this is difficult to explore without development data, and initial experiments with higher k values were not promising (see section 4.2). </context>
</contexts>
<marker>Daelemans, 1996</marker>
<rawString>Daelemans, Walter (1996). Abstraction Considered Harmful: Lazy Learning of Language Processing. In Proceedings of the 6th BelgianDutch Conference on Machine Learning. Maastricht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko Van der Sloot</author>
<author>Antal Van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 6.1, Reference Guide.</title>
<date>2007</date>
<journal>Tech. rep., ILK Research Group. ILK Research Group</journal>
<tech>Technical Report Series no. 07-07.</tech>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2007</marker>
<rawString>Daelemans, Walter, Jakub Zavrel, Ko Van der Sloot and Antal Van den Bosch (2007). TiMBL: Tilburg Memory Based Learner, version 6.1, Reference Guide. Tech. rep., ILK Research Group. ILK Research Group Technical Report Series no. 07-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
</authors>
<title>From Detecting Errors to Automatically Correcting Them.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL-06.</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="1574" citStr="Dickinson, 2006" startWordPosition="239" endWordPosition="240"> harmfulness of annotation errors for training, including the learning of noise (e.g., Hogan, 2007; Habash et al., 2007), and for evaluation (e.g., Padro and Marquez, 1998). But little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf. Osborne, 2002). Likewise, it is not clear how to learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be overcome and ways it cannot. We thus explore annotation error correction and its feasibility for dependency annotation, a form of annotation that provides argument relations among words and is useful for training and testing dependency parsers (e.g., Nivre, 2006; McDonald and Pereira, 2006). A recent innovation in dependency parsing, relevant here, is to use the predictions made by one model to refine another (Nivre and McDonald, 2008; Torres Martins et al., 2008). This gener</context>
<context position="3790" citStr="Dickinson, 2006" startWordPosition="599" endWordPosition="600">ll dependency parsing, but different models can reveal conflicting information and are generally useful as part of a larger system. Graph-based models of dependency parsing (e.g., McDonald et al., 2006), for example, rely on breaking parsing down into decisions about smaller substructures, and focusing on pairs of words has been used for domain adaptation (Chen et al., 2008) and in memory-based parsing (Canisius et al., 2006). Exploring annotation error correction in this way can provide insights into more general uses of the annotation, just as previous work on correction for POS annotation (Dickinson, 2006) led to a way to improve POS Proceedings of the 12th Conference of the European Chapter of the ACL, pages 193–201, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 193 tagging (Dickinson, 2007). After describing previous work on error detection and correction in section 2, we outline in section 3 how we model the data, focusing on individual relations between pairs of words. In section 4, we illustrate the difficulties of error correction and show how simple combinations of local features perform poorly. Based on the idea that ambiguities from strict, l</context>
<context position="6917" citStr="Dickinson, 2006" startWordPosition="1115" endWordPosition="1116">nson and Meurers, 2005), making it applicable to dependency annotation, where words in a relation can be arbitrarily far apart. Specifically, Boyd et al. (2008) adapt the method by treating dependency pairs as variation nuclei, and they include NIL elements for pairs of words not annotated as a relation. The method is successful at detecting annotation errors in corpora for three different languages, with precisions of 93% for Swedish, 60% for Czech, and 48% for German.1 2.2 Error correction Correcting POS annotation errors can be done by applying a POS tagger and altering the input POS tags (Dickinson, 2006). Namely, ambiguity class information (e.g., IN/RB/RP) is added to each corpus position for training, creating complex ambiguity tags, such as &lt;IN/RB/RP,IN&gt;. While this results in successful correction, it is not clear how it applies to annotation which is not positional and uses NIL labels. However, ambiguity class information is relevant when there is a choice between labels; we return to this in section 5. 3 Modeling the data 3.1 The data For our data set, we use the written portion (sections P and G) of the Swedish Talbanken05 treebank (Nivre et al., 2006), a reconstruction of the Talbanke</context>
<context position="8183" citStr="Dickinson (2006)" startWordPosition="1322" endWordPosition="1323">albanken05 consists of 11,431 sentences with 197,123 tokens, annotated using 69 types of dependency relations. This is a small sample, but it matches the data used for error detection, which results in 634 shortest non-fringe variation n-grams, corresponding to 2490 tokens. From a subset of 210 nuclei (917 tokens), hand-evaluation reveals error detection precision to be 93% (195/210), with 274 (of the 917) corpus positions in need of correction (Boyd et al., 2008). This means that 643 positions do not need to be corrected, setting a baseline of 70.1% (643/917) for error correction.2 Following Dickinson (2006), we train our models on the entire corpus, explicitly including NIL relations (see 1The German experiment uses a more relaxed heuristic; precision is likely higher with the shortest context heuristic. 2Detection and correction precision are different measurements: for detection, it is the percentage of variation nuclei types where at least one is incorrect; for correction, it is the percentage of corpus tokens with the true (corrected) label. 194 section 3.2); we train on the original annotation, but not the corrections. 3.2 Individual relations Annotation error correction involves overcoming</context>
<context position="25796" citStr="Dickinson, 2006" startWordPosition="4327" endWordPosition="4328"> AB IM VV 198 (7) f¨ors¨oker try ¨oppna marknaden eller erbjuda/VV andra open market or to offer other arbetsm¨ojligheter . work possibilities The original variation for the nucleus finna erbjuda (‘find offer’) is between CC-L and +F-L, but when represented as the POS tags VV VV (other verb), there are 42 possible labels, with OO-L being the most frequent. This allows for too much confusion. If model 7 had more restrictions on the set of allowable tags, it could make a more sensible choice and, in this case, select the correct label. 5.1 Using ambiguity classes Previous error correction work (Dickinson, 2006) used ambiguity classes for POS annotation, and this is precisely the type of information we need to constrain the label to one which we know is relevant to the current case. Here, we investigate ambiguity class information derived from one model integrated into another model. There are at least two main ways we can use ambiguity classes in our models. The first is what we have just been describing: an ambiguity class can serve as a constraint on the set of possible outcomes for the system. If the correct label is in the ambiguity class (as it usually is for error correction), this constrainin</context>
</contexts>
<marker>Dickinson, 2006</marker>
<rawString>Dickinson, Markus (2006). From Detecting Errors to Automatically Correcting Them. In Proceedings of EACL-06. Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
</authors>
<title>Determining Ambiguity Classes for Part-of-Speech Tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP-07. Borovets,</booktitle>
<contexts>
<context position="4024" citStr="Dickinson, 2007" startWordPosition="637" endWordPosition="638">parsing down into decisions about smaller substructures, and focusing on pairs of words has been used for domain adaptation (Chen et al., 2008) and in memory-based parsing (Canisius et al., 2006). Exploring annotation error correction in this way can provide insights into more general uses of the annotation, just as previous work on correction for POS annotation (Dickinson, 2006) led to a way to improve POS Proceedings of the 12th Conference of the European Chapter of the ACL, pages 193–201, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 193 tagging (Dickinson, 2007). After describing previous work on error detection and correction in section 2, we outline in section 3 how we model the data, focusing on individual relations between pairs of words. In section 4, we illustrate the difficulties of error correction and show how simple combinations of local features perform poorly. Based on the idea that ambiguities from strict, lexical models can constrain more general POS models, we see improvement in error correction in section 5. 2 Background 2.1 Error detection We base our method of error correction on a form of error detection for dependency annotation (</context>
</contexts>
<marker>Dickinson, 2007</marker>
<rawString>Dickinson, Markus (2007). Determining Ambiguity Classes for Part-of-Speech Tagging. In Proceedings of RANLP-07. Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>W Detmar Meurers</author>
</authors>
<title>Detecting Inconsistencies in Treebanks.</title>
<date>2003</date>
<booktitle>In Proceedings of TLT-03. V¨axj¨o,</booktitle>
<contexts>
<context position="4749" citStr="Dickinson and Meurers, 2003" startWordPosition="755" endWordPosition="758">tion 3 how we model the data, focusing on individual relations between pairs of words. In section 4, we illustrate the difficulties of error correction and show how simple combinations of local features perform poorly. Based on the idea that ambiguities from strict, lexical models can constrain more general POS models, we see improvement in error correction in section 5. 2 Background 2.1 Error detection We base our method of error correction on a form of error detection for dependency annotation (Boyd et al., 2008). The variation n-gram approach was developed for constituency-based treebanks (Dickinson and Meurers, 2003, 2005) and it detects strings which occur multiple times in the corpus with varying annotation, the so-called variation nuclei. For example, the variation nucleus next Tuesday occurs three times in the Wall Street Journal portion of the Penn Treebank (Taylor et al., 2003), twice labeled as NP and once as PP (Dickinson and Meurers, 2003). Every variation detected in the annotation of a nucleus is classified as either an annotation error or as a genuine ambiguity. The basic heuristic for detecting errors requires one word of recurring context on each side of the nucleus. The nucleus with its re</context>
</contexts>
<marker>Dickinson, Meurers, 2003</marker>
<rawString>Dickinson, Markus and W. Detmar Meurers (2003). Detecting Inconsistencies in Treebanks. In Proceedings of TLT-03. V¨axj¨o, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>W Detmar Meurers</author>
</authors>
<title>Detecting Errors in Discontinuous Structural Annotation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-</booktitle>
<contexts>
<context position="6324" citStr="Dickinson and Meurers, 2005" startWordPosition="1014" endWordPosition="1017"> from research on first language acquisition (Mintz, 2006) and unsupervised grammar induction (Klein and Manning, 2002). The approach can detect both bracketing and labeling errors in constituency annotation, and we already saw a labeling error for next Tuesday. As an example of a bracketing error, the variation nucleus last month occurs within the NP its biggest jolt last month once with the label NP and once as a non-constituent, which in the algorithm is handled through a special label NIL. The method for detecting annotation errors can be extended to discontinuous constituency annotation (Dickinson and Meurers, 2005), making it applicable to dependency annotation, where words in a relation can be arbitrarily far apart. Specifically, Boyd et al. (2008) adapt the method by treating dependency pairs as variation nuclei, and they include NIL elements for pairs of words not annotated as a relation. The method is successful at detecting annotation errors in corpora for three different languages, with precisions of 93% for Swedish, 60% for Czech, and 48% for German.1 2.2 Error correction Correcting POS annotation errors can be done by applying a POS tagger and altering the input POS tags (Dickinson, 2006). Namel</context>
<context position="9990" citStr="Dickinson and Meurers, 2005" startWordPosition="1617" endWordPosition="1621">pieces of information conflict (cf. also McDonald and Nivre, 2007). To support this isolation of information, we use dependency pairs as the basic unit of analysis and assign a dependency label to each word pair. Following Boyd et al. (2008), we add L or R to the label to indicate which word is the head, the left (L) or the right (R). This is tantamount to handling pairs of words as single entries in a “lexicon” and provides a natural way to talk of ambiguities. Breaking the representation down into strings whch receive a label also makes the method applicable to other annotation types (e.g., Dickinson and Meurers, 2005). A major issue in generating a lexicon is how to handle pairs of words which are not dependencies. We follow Boyd et al. (2008) and generate NIL labels for those pairs of words which also occur as a true labeled relation. In other words, only word pairs which can be relations can also be NILs. For every sentence, then, when we produce feature lists (see section 3.3), we produce them for all word pairs that are related or could potentially be related, but not those which have never been observed as a dependency pair. This selection of NIL items works because there are no unknown words. We use </context>
</contexts>
<marker>Dickinson, Meurers, 2005</marker>
<rawString>Dickinson, Markus and W. Detmar Meurers (2005). Detecting Errors in Discontinuous Structural Annotation. In Proceedings of ACL-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einarsson</author>
</authors>
<title>Talbankens skriftsprøakskonkordans.</title>
<date>1976</date>
<tech>Tech. rep.,</tech>
<institution>Lund University, Dept. of Scandinavian Languages.</institution>
<contexts>
<context position="7545" citStr="Einarsson, 1976" startWordPosition="1221" endWordPosition="1222">biguity class information (e.g., IN/RB/RP) is added to each corpus position for training, creating complex ambiguity tags, such as &lt;IN/RB/RP,IN&gt;. While this results in successful correction, it is not clear how it applies to annotation which is not positional and uses NIL labels. However, ambiguity class information is relevant when there is a choice between labels; we return to this in section 5. 3 Modeling the data 3.1 The data For our data set, we use the written portion (sections P and G) of the Swedish Talbanken05 treebank (Nivre et al., 2006), a reconstruction of the Talbanken76 corpus (Einarsson, 1976) The written data of Talbanken05 consists of 11,431 sentences with 197,123 tokens, annotated using 69 types of dependency relations. This is a small sample, but it matches the data used for error detection, which results in 634 shortest non-fringe variation n-grams, corresponding to 2490 tokens. From a subset of 210 nuclei (917 tokens), hand-evaluation reveals error detection precision to be 93% (195/210), with 274 (of the 917) corpus positions in need of correction (Boyd et al., 2008). This means that 643 positions do not need to be corrected, setting a baseline of 70.1% (643/917) for error c</context>
</contexts>
<marker>Einarsson, 1976</marker>
<rawString>Einarsson, Jan (1976). Talbankens skriftsprøakskonkordans. Tech. rep., Lund University, Dept. of Scandinavian Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Ryan Gabbard</author>
<author>Owen Rambow</author>
<author>Seth Kulick</author>
<author>Mitch Marcus</author>
</authors>
<title>Determining Case in Arabic: Learning Complex Linguistic Behavior Requires Complex Linguistic Features.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-07.</booktitle>
<contexts>
<context position="1078" citStr="Habash et al., 2007" startWordPosition="155" endWordPosition="158">een words, and then use ambiguities from one model to constrain a second, more relaxed model. In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally. 1 Introduction and Motivation Annotation error detection has been explored for part-of-speech (POS), syntactic constituency, semantic role, and syntactic dependency annotation (see Boyd et al., 2008, and references therein). Such work is extremely useful, given the harmfulness of annotation errors for training, including the learning of noise (e.g., Hogan, 2007; Habash et al., 2007), and for evaluation (e.g., Padro and Marquez, 1998). But little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf. Osborne, 2002). Likewise, it is not clear how to learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully elucidate ways in whi</context>
</contexts>
<marker>Habash, Gabbard, Rambow, Kulick, Marcus, 2007</marker>
<rawString>Habash, Nizar, Ryan Gabbard, Owen Rambow, Seth Kulick and Mitch Marcus (2007). Determining Case in Arabic: Learning Complex Linguistic Behavior Requires Complex Linguistic Features. In Proceedings of EMNLP-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deirdre Hogan</author>
</authors>
<title>Coordinate Noun Phrase Disambiguation in a Generative Parsing Model.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07.</booktitle>
<location>Prague.</location>
<contexts>
<context position="1056" citStr="Hogan, 2007" startWordPosition="153" endWordPosition="154">elations between words, and then use ambiguities from one model to constrain a second, more relaxed model. In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally. 1 Introduction and Motivation Annotation error detection has been explored for part-of-speech (POS), syntactic constituency, semantic role, and syntactic dependency annotation (see Boyd et al., 2008, and references therein). Such work is extremely useful, given the harmfulness of annotation errors for training, including the learning of noise (e.g., Hogan, 2007; Habash et al., 2007), and for evaluation (e.g., Padro and Marquez, 1998). But little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf. Osborne, 2002). Likewise, it is not clear how to learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully</context>
</contexts>
<marker>Hogan, 2007</marker>
<rawString>Hogan, Deirdre (2007). Coordinate Noun Phrase Disambiguation in a Generative Parsing Model. In Proceedings of ACL-07. Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A Generative Constituent-Context Model for Improved Grammar Induction.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-02.</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5815" citStr="Klein and Manning, 2002" startWordPosition="929" endWordPosition="932">r as a genuine ambiguity. The basic heuristic for detecting errors requires one word of recurring context on each side of the nucleus. The nucleus with its repeated surrounding context is referred to as a variation n-gram. While the original proposal expanded the context as far as possible given the repeated n-gram, using only the immediately surrounding words as context is sufficient for detecting errors with high precision (Boyd et al., 2008). This “shortest” context heuristic receives some support from research on first language acquisition (Mintz, 2006) and unsupervised grammar induction (Klein and Manning, 2002). The approach can detect both bracketing and labeling errors in constituency annotation, and we already saw a labeling error for next Tuesday. As an example of a bracketing error, the variation nucleus last month occurs within the NP its biggest jolt last month once with the label NP and once as a non-constituent, which in the algorithm is handled through a special label NIL. The method for detecting annotation errors can be extended to discontinuous constituency annotation (Dickinson and Meurers, 2005), making it applicable to dependency annotation, where words in a relation can be arbitrari</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Klein, Dan and Christopher D. Manning (2002). A Generative Constituent-Context Model for Improved Grammar Induction. In Proceedings of ACL-02. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual Dependency Analysis with a Two-Stage Discriminative Parser.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<location>New York City.</location>
<contexts>
<context position="3376" citStr="McDonald et al., 2006" startWordPosition="530" endWordPosition="533"> order to examine the effects of errors and to refine one model with another’s information, we need to isolate the problematic cases. The data representation must therefore be such that it clearly allows for the specific identification of errors between words. Thus, we explore relatively simple models of the data, emphasizing small substructures (see section 3.2). This simple modeling is not always rich enough for full dependency parsing, but different models can reveal conflicting information and are generally useful as part of a larger system. Graph-based models of dependency parsing (e.g., McDonald et al., 2006), for example, rely on breaking parsing down into decisions about smaller substructures, and focusing on pairs of words has been used for domain adaptation (Chen et al., 2008) and in memory-based parsing (Canisius et al., 2006). Exploring annotation error correction in this way can provide insights into more general uses of the annotation, just as previous work on correction for POS annotation (Dickinson, 2006) led to a way to improve POS Proceedings of the 12th Conference of the European Chapter of the ACL, pages 193–201, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computa</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>McDonald, Ryan, Kevin Lerman and Fernando Pereira (2006). Multilingual Dependency Analysis with a Two-Stage Discriminative Parser. In Proceedings of CoNLL-X. New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the Errors of Data-Driven Dependency Parsing Models.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL-07. Prague,</booktitle>
<pages>122--131</pages>
<contexts>
<context position="9428" citStr="McDonald and Nivre, 2007" startWordPosition="1514" endWordPosition="1517">, in order to learn the true patterns underlying the data. This is a slightly different goal from that of general dependency parsing methods, which often integrate a variety of features in making decisions about dependency relations (cf., e.g., Nivre, 2006; McDonald and Pereira, 2006). Instead of maximizing a feature model to improve parsing, we isolate individual pieces of information (e.g., context POS tags), thereby being able to pinpoint, for example, when non-local information is needed for particular types of relations and pointing to cases where pieces of information conflict (cf. also McDonald and Nivre, 2007). To support this isolation of information, we use dependency pairs as the basic unit of analysis and assign a dependency label to each word pair. Following Boyd et al. (2008), we add L or R to the label to indicate which word is the head, the left (L) or the right (R). This is tantamount to handling pairs of words as single entries in a “lexicon” and provides a natural way to talk of ambiguities. Breaking the representation down into strings whch receive a label also makes the method applicable to other annotation types (e.g., Dickinson and Meurers, 2005). A major issue in generating a lexico</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>McDonald, Ryan and Joakim Nivre (2007). Characterizing the Errors of Data-Driven Dependency Parsing Models. In Proceedings of EMNLP-CoNLL-07. Prague, pp. 122–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL6.</booktitle>
<location>Trento.</location>
<contexts>
<context position="1985" citStr="McDonald and Pereira, 2006" startWordPosition="301" endWordPosition="304">nsistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be overcome and ways it cannot. We thus explore annotation error correction and its feasibility for dependency annotation, a form of annotation that provides argument relations among words and is useful for training and testing dependency parsers (e.g., Nivre, 2006; McDonald and Pereira, 2006). A recent innovation in dependency parsing, relevant here, is to use the predictions made by one model to refine another (Nivre and McDonald, 2008; Torres Martins et al., 2008). This general notion can be employed here, as different models of the data have different predictions about whch parts are erroneous and can highlight the contributions of different features. Using differences that complement one another, we can begin to sort accurate from inaccurate patterns, by integrating models in such a way as to learn the true patterns and not the errors. Although we focus on dependency annotatio</context>
<context position="9088" citStr="McDonald and Pereira, 2006" startWordPosition="1460" endWordPosition="1464">n, it is the percentage of variation nuclei types where at least one is incorrect; for correction, it is the percentage of corpus tokens with the true (corrected) label. 194 section 3.2); we train on the original annotation, but not the corrections. 3.2 Individual relations Annotation error correction involves overcoming noise in the corpus, in order to learn the true patterns underlying the data. This is a slightly different goal from that of general dependency parsing methods, which often integrate a variety of features in making decisions about dependency relations (cf., e.g., Nivre, 2006; McDonald and Pereira, 2006). Instead of maximizing a feature model to improve parsing, we isolate individual pieces of information (e.g., context POS tags), thereby being able to pinpoint, for example, when non-local information is needed for particular types of relations and pointing to cases where pieces of information conflict (cf. also McDonald and Nivre, 2007). To support this isolation of information, we use dependency pairs as the basic unit of analysis and assign a dependency label to each word pair. Following Boyd et al. (2008), we add L or R to the label to indicate which word is the head, the left (L) or the </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>McDonald, Ryan and Fernando Pereira (2006). Online learning of approximate dependency parsing algorithms. In Proceedings of EACL6. Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toben H Mintz</author>
</authors>
<title>Finding the verbs: distributional cues to categories available to young learners.</title>
<date>2006</date>
<booktitle>Action Meets Word: How Children Learn Verbs,</booktitle>
<pages>31--63</pages>
<editor>In K. Hirsh-Pasek and R. M. Golinkoff (eds.),</editor>
<publisher>Oxford University Press,</publisher>
<location>New York:</location>
<contexts>
<context position="5754" citStr="Mintz, 2006" startWordPosition="922" endWordPosition="923">eus is classified as either an annotation error or as a genuine ambiguity. The basic heuristic for detecting errors requires one word of recurring context on each side of the nucleus. The nucleus with its repeated surrounding context is referred to as a variation n-gram. While the original proposal expanded the context as far as possible given the repeated n-gram, using only the immediately surrounding words as context is sufficient for detecting errors with high precision (Boyd et al., 2008). This “shortest” context heuristic receives some support from research on first language acquisition (Mintz, 2006) and unsupervised grammar induction (Klein and Manning, 2002). The approach can detect both bracketing and labeling errors in constituency annotation, and we already saw a labeling error for next Tuesday. As an example of a bracketing error, the variation nucleus last month occurs within the NP its biggest jolt last month once with the label NP and once as a non-constituent, which in the algorithm is handled through a special label NIL. The method for detecting annotation errors can be extended to discontinuous constituency annotation (Dickinson and Meurers, 2005), making it applicable to depe</context>
</contexts>
<marker>Mintz, 2006</marker>
<rawString>Mintz, Toben H. (2006). Finding the verbs: distributional cues to categories available to young learners. In K. Hirsh-Pasek and R. M. Golinkoff (eds.), Action Meets Word: How Children Learn Verbs, New York: Oxford University Press, pp. 31–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive Dependency Parsing.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<location>Berlin:</location>
<contexts>
<context position="1956" citStr="Nivre, 2006" startWordPosition="299" endWordPosition="300">learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be overcome and ways it cannot. We thus explore annotation error correction and its feasibility for dependency annotation, a form of annotation that provides argument relations among words and is useful for training and testing dependency parsers (e.g., Nivre, 2006; McDonald and Pereira, 2006). A recent innovation in dependency parsing, relevant here, is to use the predictions made by one model to refine another (Nivre and McDonald, 2008; Torres Martins et al., 2008). This general notion can be employed here, as different models of the data have different predictions about whch parts are erroneous and can highlight the contributions of different features. Using differences that complement one another, we can begin to sort accurate from inaccurate patterns, by integrating models in such a way as to learn the true patterns and not the errors. Although we </context>
<context position="9059" citStr="Nivre, 2006" startWordPosition="1458" endWordPosition="1459"> for detection, it is the percentage of variation nuclei types where at least one is incorrect; for correction, it is the percentage of corpus tokens with the true (corrected) label. 194 section 3.2); we train on the original annotation, but not the corrections. 3.2 Individual relations Annotation error correction involves overcoming noise in the corpus, in order to learn the true patterns underlying the data. This is a slightly different goal from that of general dependency parsing methods, which often integrate a variety of features in making decisions about dependency relations (cf., e.g., Nivre, 2006; McDonald and Pereira, 2006). Instead of maximizing a feature model to improve parsing, we isolate individual pieces of information (e.g., context POS tags), thereby being able to pinpoint, for example, when non-local information is needed for particular types of relations and pointing to cases where pieces of information conflict (cf. also McDonald and Nivre, 2007). To support this isolation of information, we use dependency pairs as the basic unit of analysis and assign a dependency label to each word pair. Following Boyd et al. (2008), we add L or R to the label to indicate which word is t</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Nivre, Joakim (2006). Inductive Dependency Parsing. Berlin: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov and Erwin Marsi</title>
<date>2007</date>
<journal>Natural Language Engineering</journal>
<volume>13</volume>
<issue>2</issue>
<pages>95--135</pages>
<contexts>
<context position="23547" citStr="Nivre et al., 2007" startWordPosition="3938" endWordPosition="3941">us with relatively high error detection precision (see section 2.1). Despite high similarity of context (e.g., model 6), the best results are only around 73%, and this is given a baseline (no changes) of 70%. While a more expansive set of features would help, there are other problems here, as the method appears to be overtraining. There is no question that we are learning the “correct” patterns, i.e., 99.9% similarity to the benchmark in the best cases. The problem is that, for error correction, we have to overcome noise in the data. Training and testing with the dependency parser MaltParser (Nivre et al., 2007, default settings) is no better, with 72.1% overall precision (despite a labeled attachment score of 98.3%). Recall in this light that there are variations for which the non-majority label is the correct one; attempting to get a non-majority label correct using a strict lexical model does not work. To be able not to learn the erroneous patterns requires a more general model. Interestingly, a more general model—e.g., treating the corpus as a sequence of tags (model 8)—results in equally good correction, without being a good overall fit to the corpus data (only 92.7%). This model, too, learns n</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev, Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov and Erwin Marsi (2007). MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering 13(2), 95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating Graph-Based and Transition-Based Dependency Parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="2132" citStr="Nivre and McDonald, 2008" startWordPosition="327" endWordPosition="330">empt at correction was limited to POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be overcome and ways it cannot. We thus explore annotation error correction and its feasibility for dependency annotation, a form of annotation that provides argument relations among words and is useful for training and testing dependency parsers (e.g., Nivre, 2006; McDonald and Pereira, 2006). A recent innovation in dependency parsing, relevant here, is to use the predictions made by one model to refine another (Nivre and McDonald, 2008; Torres Martins et al., 2008). This general notion can be employed here, as different models of the data have different predictions about whch parts are erroneous and can highlight the contributions of different features. Using differences that complement one another, we can begin to sort accurate from inaccurate patterns, by integrating models in such a way as to learn the true patterns and not the errors. Although we focus on dependency annotation, the methods are potentially applicable for different types of annotation, given that they are based on the similar data representations (see sec</context>
<context position="14167" citStr="Nivre and McDonald (2008)" startWordPosition="2318" endWordPosition="2321">section 4.2). 3.4 Integrating features When using features for individual relations, we have different options for integrating them. On the one hand, one can simply additively combine features into a larger vector for training, as described in section 4.2. On the other hand, one can use one set of features to constrain another set, as described in section 5. Pulling apart the features commonly employed in dependency parsing can help indicate the contributions each has on the classification. This general idea is akin to the notion of classifier stacking, and in the realm of dependency parsing, Nivre and McDonald (2008) successfully stack classifiers to improve parsing by “allow[ing] a model to learn relative to the predictions of the other” (p. 951). The output from one classifier is used as a feature in the next one (see also Torres Martins et al., 2008). Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism. Instead of focusing on what one learning algorithm informs another about, we ask what one set of more or less informative features can inform another set about, as described in section 5.1. 4 Perfo</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Nivre, Joakim and Ryan McDonald (2008). Integrating Graph-Based and Transition-Based Dependency Parsers. In Proceedings of ACL-08: HLT. Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
<author>Johan Hall</author>
</authors>
<title>Talbanken05: A Swedish Treebank with Phrase Structure and Dependency Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-06.</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="7483" citStr="Nivre et al., 2006" startWordPosition="1211" endWordPosition="1214">ger and altering the input POS tags (Dickinson, 2006). Namely, ambiguity class information (e.g., IN/RB/RP) is added to each corpus position for training, creating complex ambiguity tags, such as &lt;IN/RB/RP,IN&gt;. While this results in successful correction, it is not clear how it applies to annotation which is not positional and uses NIL labels. However, ambiguity class information is relevant when there is a choice between labels; we return to this in section 5. 3 Modeling the data 3.1 The data For our data set, we use the written portion (sections P and G) of the Swedish Talbanken05 treebank (Nivre et al., 2006), a reconstruction of the Talbanken76 corpus (Einarsson, 1976) The written data of Talbanken05 consists of 11,431 sentences with 197,123 tokens, annotated using 69 types of dependency relations. This is a small sample, but it matches the data used for error detection, which results in 634 shortest non-fringe variation n-grams, corresponding to 2490 tokens. From a subset of 210 nuclei (917 tokens), hand-evaluation reveals error detection precision to be 93% (195/210), with 274 (of the 917) corpus positions in need of correction (Boyd et al., 2008). This means that 643 positions do not need to b</context>
</contexts>
<marker>Nivre, Nilsson, Hall, 2006</marker>
<rawString>Nivre, Joakim, Jens Nilsson and Johan Hall (2006). Talbanken05: A Swedish Treebank with Phrase Structure and Dependency Annotation. In Proceedings of LREC-06. Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
</authors>
<title>Shallow Parsing using Noisy and Non-Stationary Training Material.</title>
<date>2002</date>
<booktitle>In JMLR Special Issue on Machine Learning Approaches to Shallow Parsing,</booktitle>
<volume>2</volume>
<pages>695--719</pages>
<contexts>
<context position="1310" citStr="Osborne, 2002" startWordPosition="197" endWordPosition="198">ntroduction and Motivation Annotation error detection has been explored for part-of-speech (POS), syntactic constituency, semantic role, and syntactic dependency annotation (see Boyd et al., 2008, and references therein). Such work is extremely useful, given the harmfulness of annotation errors for training, including the learning of noise (e.g., Hogan, 2007; Habash et al., 2007), and for evaluation (e.g., Padro and Marquez, 1998). But little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf. Osborne, 2002). Likewise, it is not clear how to learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be overcome and ways it cannot. We thus explore annotation error correction and its feasibility for dependency annotation, a form of annotation that provides argument relations among words and is useful for training and</context>
</contexts>
<marker>Osborne, 2002</marker>
<rawString>Osborne, Miles (2002). Shallow Parsing using Noisy and Non-Stationary Training Material. In JMLR Special Issue on Machine Learning Approaches to Shallow Parsing, vol. 2, pp. 695– 719.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis Padro</author>
<author>Lluis Marquez</author>
</authors>
<title>On the Evaluation and Comparison of Taggers: the Effect of Noise in Testing Corpora.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL-COLING-98.</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="1130" citStr="Padro and Marquez, 1998" startWordPosition="163" endWordPosition="166">el to constrain a second, more relaxed model. In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally. 1 Introduction and Motivation Annotation error detection has been explored for part-of-speech (POS), syntactic constituency, semantic role, and syntactic dependency annotation (see Boyd et al., 2008, and references therein). Such work is extremely useful, given the harmfulness of annotation errors for training, including the learning of noise (e.g., Hogan, 2007; Habash et al., 2007), and for evaluation (e.g., Padro and Marquez, 1998). But little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf. Osborne, 2002). Likewise, it is not clear how to learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be overcome and ways it cannot. We thus</context>
</contexts>
<marker>Padro, Marquez, 1998</marker>
<rawString>Padro, Lluis and Lluis Marquez (1998). On the Evaluation and Comparison of Taggers: the Effect of Noise in Testing Corpora. In Proceedings of ACL-COLING-98. San Francisco, CA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ann Taylor</author>
</authors>
<title>Mitchell Marcus and Beatrice Santorini (2003). The Penn Treebank: An Overview.</title>
<volume>1</volume>
<pages>5--22</pages>
<editor>In Anne Abeill´e (ed.),</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht:</location>
<marker>Taylor, </marker>
<rawString>Taylor, Ann, Mitchell Marcus and Beatrice Santorini (2003). The Penn Treebank: An Overview. In Anne Abeill´e (ed.), Treebanks: Building and using syntactically annotated corpora, Dordrecht: Kluwer, chap. 1, pp. 5–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torres Martins</author>
<author>Andr´e Filipe</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking Dependency Parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP-08.</booktitle>
<pages>157--166</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2162" citStr="Martins et al., 2008" startWordPosition="332" endWordPosition="335"> POS annotation (Dickinson, 2006). By moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be overcome and ways it cannot. We thus explore annotation error correction and its feasibility for dependency annotation, a form of annotation that provides argument relations among words and is useful for training and testing dependency parsers (e.g., Nivre, 2006; McDonald and Pereira, 2006). A recent innovation in dependency parsing, relevant here, is to use the predictions made by one model to refine another (Nivre and McDonald, 2008; Torres Martins et al., 2008). This general notion can be employed here, as different models of the data have different predictions about whch parts are erroneous and can highlight the contributions of different features. Using differences that complement one another, we can begin to sort accurate from inaccurate patterns, by integrating models in such a way as to learn the true patterns and not the errors. Although we focus on dependency annotation, the methods are potentially applicable for different types of annotation, given that they are based on the similar data representations (see sections 2.1 and 3.2). In order t</context>
<context position="14408" citStr="Martins et al., 2008" startWordPosition="2361" endWordPosition="2364">ection 4.2. On the other hand, one can use one set of features to constrain another set, as described in section 5. Pulling apart the features commonly employed in dependency parsing can help indicate the contributions each has on the classification. This general idea is akin to the notion of classifier stacking, and in the realm of dependency parsing, Nivre and McDonald (2008) successfully stack classifiers to improve parsing by “allow[ing] a model to learn relative to the predictions of the other” (p. 951). The output from one classifier is used as a feature in the next one (see also Torres Martins et al., 2008). Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism. Instead of focusing on what one learning algorithm informs another about, we ask what one set of more or less informative features can inform another set about, as described in section 5.1. 4 Performing error correction 4.1 Challenges The task of automatic error correction in some sense seems straightforward, in that there are no unknown words. Furthermore, we are looking at identical recurring words, which should for the most part ha</context>
</contexts>
<marker>Martins, Filipe, Das, Smith, Xing, 2008</marker>
<rawString>Torres Martins, Andr´e Filipe, Dipanjan Das, Noah A. Smith and Eric P. Xing (2008). Stacking Dependency Parsers. In Proceedings of EMNLP-08. Honolulu, Hawaii, pp. 157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Zavrel</author>
</authors>
<title>Walter Daelemans and Jorn Veensta</title>
<date>1997</date>
<booktitle>In Proceedings of CoNLL-97.</booktitle>
<location>Madrid.</location>
<marker>Zavrel, 1997</marker>
<rawString>Zavrel, Jakub, Walter Daelemans and Jorn Veensta (1997). Resolving PP attachment Ambiguities with Memory-Based Learning. In Proceedings of CoNLL-97. Madrid.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>