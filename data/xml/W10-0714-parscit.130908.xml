<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.356571">
<title confidence="0.9896305">
MTurk Crowdsourcing:
A Viable Method for Rapid Discovery of Arabic Nicknames?
</title>
<author confidence="0.998477">
Chiara Higgins Elizabeth McGrath Lailla Moretto
</author>
<affiliation confidence="0.8293315">
George Mason University MITRE MITRE
Fairfax, VA. 22030, USA McLean, VA. 20112, USA McLean, VA. 20112, USA
</affiliation>
<email confidence="0.991138">
chiara.higgins@gmail.com emcgrath@mitre.org lmoretto@mitre.org
</email>
<sectionHeader confidence="0.995548" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999075">
This paper presents findings on using
crowdsourcing via Amazon Mechanical
Turk (MTurk) to obtain Arabic nicknames
as a contribution to exiting Named Entity
(NE) lexicons. It demonstrates a strategy
for increasing MTurk participation from
Arab countries. The researchers validate
the nicknames using experts, MTurk
workers, and Google search and then
compare them against the Database of
Arabic Names (DAN). Additionally, the
experiment looks at the effect of pay rate
on speed of nickname collection and doc-
uments an advertising effect where
MTurk workers respond to existing work
batches, called Human Intelligence Tasks
(HITs), more quickly once similar higher
paying HITs are posted.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999740875">
The question this experiment investigates is: can
MTurk crowdsourcing add undocumented nick-
names to existing Named Entity (NE) lexicons?
This experiment seeks to produce nicknames
to add to DAN Version 1.1, which contains
147,739 lines of names. While DAN does not list
nicknames as a metadata type, it does include some
commonly known nicknames.
</bodyText>
<subsectionHeader confidence="0.98312">
1.1 Traditional collection methods are costly
</subsectionHeader>
<bodyText confidence="0.9999172">
According to DAN’s website, administrators col-
lect nicknames using a team of software engineers
and native speakers. They also draw on a “large
variety of sources including websites, corpora,
books, phone directories, dictionaries, encyclope-
dias, and university rosters” (Halpern, 2009). Col-
lecting names by searching various media sources
or employing linguists and native speakers is a
massive effort requiring significant expenditure of
time and money.
</bodyText>
<subsectionHeader confidence="0.997331">
1.2 Crowdsourcing might work better
</subsectionHeader>
<bodyText confidence="0.996467166666667">
The experiment uses crowdsourcing via MTurk
since it offers a web-based problem-solving model
and quickly engages a large number of internation-
al workers at low cost. Furthermore, previous re-
search shows the effectiveness of crowdsourcing as
a method of accomplishing labor intensive natural
language processing tasks (Callison-Burch, 2009)
and the effectiveness of using MTurk for a variety
of natural language automation tasks (Snow,
Jurafsy, &amp; O&apos;Connor, 2008).
The experiment answers the following ques-
tions:
</bodyText>
<listItem confidence="0.992497714285714">
• Can we discover valid nicknames not cur-
rently in DAN?
• What do we need to pay workers to gather
nicknames rapidly?
• How do we convey the task to guide non-
experts and increase participation from
Arab countries?
</listItem>
<page confidence="0.988978">
89
</page>
<note confidence="0.8644305">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 89–92,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.955044" genericHeader="method">
2 Experiment Design
</sectionHeader>
<bodyText confidence="0.999930833333333">
The experiment contains three main phases. First,
nicknames are gathered from MTurk workers.
Second, the collected names are validated via
MTurk, internet searches, and expert opinion. Fi-
nally, the verified names are compared against the
available list of names in the DAN.
</bodyText>
<subsectionHeader confidence="0.999157">
2.1 Collecting nicknames on MTurk
</subsectionHeader>
<bodyText confidence="0.999955384615385">
In this phase, we open HITs on MTurk requesting
workers to enter an Arabic nickname they have
heard. In addition to writing a nickname, the
workers input where they heard the name and their
country of residence.
HIT instructions are kept simple and writ-
ten in short sentences to guide non-experts and
include a basic definition of a nickname. To en-
courage participation of native Arabic speakers,
the instructions and search words are in Arabic as
well as English. Workers are asked to input names
in the Arabic alphabet, thus eliminating any worker
who does not use Arabic often enough to warrant
having an Arabic keyboard. Further clarifying the
task, words highlighted in red, “Arabic alphabet”,
emphasize what the worker needs to do.
While seeking to encourage participation
from Arab countries, we choose not to block par-
ticipation from other countries since there are
Arabic speakers and immigrants in many countries
where Arabic is not the main language.
To evaluate the effect of pay rate on nick-
name collection rate, HITs have a variety of pay
rates. HITs paying $0.03 per HIT are kept up
throughout the experiment, while HITs paying
$0.05 and finally $0.25 are added later.
</bodyText>
<subsectionHeader confidence="0.982322">
2.2 Nickname validation phase
</subsectionHeader>
<bodyText confidence="0.999344066666667">
Vetting the nicknames, involves a Google check
and asking 3 experts and 5 MTurk workers to rate
each name that is submitted in a valid format.
would occur in the Arab world on a Likert scale
(Strongly Agree, Agree, Neither Agree nor Disag-
ree, Disagree, Strongly Disagree).
The entire validation process is completed
twice, once paying the workers $.01 per validation
and once paying $.05 per validation to allow us to
further research the effect of pay on HIT collection
rate.
The Google check vets the names to see if
they occur on the web thus eliminating, any nick-
names that are nowhere in print and therefore not
currently necessary additions to NE lexicons.
</bodyText>
<subsectionHeader confidence="0.965837">
2.3 Compare data to ground truth in DAN
</subsectionHeader>
<bodyText confidence="0.999961666666667">
The third phase is a search for exact matches for
the validated nicknames in DAN to determine if
they represent new additions to the lexicon.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999908583333333">
MTurk workers generated 332 nicknames during
the course of this experiment. Because the initial
collection rate was slower than expected, we vali-
dated and compared only the first 108 names to
report results related to the usefulness of MTurk in
nickname collection. Results involving pay and
collection rate draw on the full data.
Based on self-reported data, approximately
35% of the respondents came from the Arabic
speaking countries of Morocco, Egypt, Lebanon,
Jordan, UAE, and Dubai. 46% were submitted
from India, 13% from the U.S. and 5% elsewhere.
</bodyText>
<figure confidence="0.987534833333333">
Nicknames by Nation
38 50
14 6
Arabic India USA Other
Speaking
Countries
</figure>
<figureCaption confidence="0.999742">
Figure 1. Nicknames by nation
</figureCaption>
<bodyText confidence="0.994521">
Each expert and MTurk worker has the
opportunity to rate the likelihood the nickname
</bodyText>
<page confidence="0.994492">
90
</page>
<subsectionHeader confidence="0.998563">
3.1 Validation results
</subsectionHeader>
<bodyText confidence="0.999977384615384">
Each of the nicknames was verified by MTurk
workers and three experts. On a five-point Likert
scale with 1 representing strong disagreement and
5 showing strong agreement, we accepted 51 of the
names as valid because the majority (3 of 5 MTurk
workers and 2 of 3 experts) scored the name as 3
or higher.
One of the 51 names accepted by other
means could not be found in a Google search leav-
ing us with 50 valid nicknames.
Comparing the 50 remaining names to
DAN we found that 11 of the valid names were
already in the lexicon.
</bodyText>
<subsectionHeader confidence="0.99974">
3.2 Effect of increased pay on responses
</subsectionHeader>
<bodyText confidence="0.999828285714286">
Holding everything else constant, we increased the
worker’s pay during nickname collection. On aver-
age, $0.03 delivered 9.8 names a day, for $0.05 we
collected 25 names a day and for $0.25 we col-
lected 100 names in a day.
We also posted one of our MTurk verifica-
tion files two times, once at $0.01 per HIT and
once at $0.05 per HIT, holding everything constant
except the pay. Figure 2 shows the speed with
which the two batches of HITs were completed.
The results show not only an increased collection
speed for the higher paying HITs, but also an in-
creased collection speed for the existing lower pay-
ing HIT once the higher paying HITs were posted.
</bodyText>
<figureCaption confidence="0.965366">
Figure 2. HITS by payment amount over time
</figureCaption>
<sectionHeader confidence="0.992837" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999956947368421">
As our most significant goal, we sought to investi-
gate whether MTurk crowdsourcing could success-
fully collect undiscovered nicknames to add to an
existing NE lexicon.
The results indicate that MTurk is a viable
method for collecting nicknames; in the course of
the experiment, we successfully produced 39 veri-
fied nicknames that we recommend adding to the
DAN.
Another goal was to explore the effect of
worker pay on HIT completion rate. Our initial
collection rate, at $0.03 per HIT, was only 9.8
names per day. By increasing pay, we were able to
speed up the process. At $0.05 per name, we in-
creased the daily collection rate from 9.8 to 25, and
by making the pay rate $0.25 we collected 100
names in a day. So increasing pay significantly
improved collection speed.
While working with pricing for the verifi-
cation HITs, we were able to quantify an “advertis-
ing effect” we had noticed previously where the
posting of a higher paying HIT causes existing
similar lower paying HITs to be completed more
quickly as well. Further research could be con-
ducted to determine a mix of pay rates that max-
imizes collection rate while minimizing cost.
Furthermore, the experiment shows that by
using bilingual directions and requiring typing in
Arabic, we were able to increase the participation
from Arabic speaking countries. Based on our pre-
vious experience where we posted Arabic language
related HITs in English only, Arab country partici-
pation on MTurk is minimal. Other researchers
have also found little MTurk participation from
Arabic speaking countries (Ross, Zaldivar, Irani, &amp;
Tomlinson, 2009). In this experiment, however,
we received more than 35% participation from
workers in Arabic speaking countries.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997895">
Thanks to the MITRE Corporation for providing
the ground truth DAN data under their research
</bodyText>
<figure confidence="0.991957538461538">
Cumulative HITs per Day
500 $.01 per HIT
400 $.05 per HIT
300
200
100
0
Wed Feb ...
Thur Feb ...
Thur Feb ...
Fri Feb 26 ...
Fri Feb 26 ...
Sat Feb 27 ...
</figure>
<page confidence="0.99526">
91
</page>
<bodyText confidence="0.9997545">
license agreement with the CJK Dictionary Insti-
tute. Also thanks to Trent Rockwood of MITRE
for providing expert assistance in the Arabic lan-
guage and on some technical issues.
</bodyText>
<sectionHeader confidence="0.996438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999765611111111">
Callison-Burch, C. (2009). Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazon’s Me-
chanical Turk. Proceedings of the EMNLP. Singa-
pore.
Halpern, J. (2009). Lexicon-Driven Approach to the
Recognition of Arabic Named Entities. Second In-
ternational Conference on Arabic Language Re-
sources and Tools. Cairo.
Ross, J., Zaldivar, A., Irani, L., &amp; Tomlinson, B. (2009).
Who are the Turkers? Worker Demographics in
Amazon. Department of Informatics, University of
California, Irvine.
Snow, R., Jurafsy, D., &amp; O’Connor, B. (2008). Cheap
and fast – but is it good?: Evaluating Non-Expert
Annotations for Natural Language Tasks. Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, (pp. 254-263). Hono-
lulu.
</reference>
<page confidence="0.995943">
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.920185">
<title confidence="0.9993985">MTurk Crowdsourcing: A Viable Method for Rapid Discovery of Arabic Nicknames?</title>
<author confidence="0.989203">Chiara Higgins Elizabeth McGrath Lailla Moretto</author>
<affiliation confidence="0.952268">George Mason University MITRE MITRE</affiliation>
<address confidence="0.999882">Fairfax, VA. 22030, USA McLean, VA. 20112, USA McLean, VA. 20112, USA</address>
<email confidence="0.98297">emcgrath@mitre.orglmoretto@mitre.org</email>
<abstract confidence="0.999414263157895">This paper presents findings on using crowdsourcing via Amazon Mechanical Turk (MTurk) to obtain Arabic nicknames as a contribution to exiting Named Entity (NE) lexicons. It demonstrates a strategy for increasing MTurk participation from Arab countries. The researchers validate the nicknames using experts, MTurk workers, and Google search and then compare them against the Database of Arabic Names (DAN). Additionally, the experiment looks at the effect of pay rate on speed of nickname collection and documents an advertising effect where MTurk workers respond to existing work batches, called Human Intelligence Tasks (HITs), more quickly once similar higher paying HITs are posted.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
</authors>
<title>Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>Proceedings of the EMNLP.</booktitle>
<publisher>Singapore.</publisher>
<contexts>
<context position="2231" citStr="Callison-Burch, 2009" startWordPosition="318" endWordPosition="319">ories, dictionaries, encyclopedias, and university rosters” (Halpern, 2009). Collecting names by searching various media sources or employing linguists and native speakers is a massive effort requiring significant expenditure of time and money. 1.2 Crowdsourcing might work better The experiment uses crowdsourcing via MTurk since it offers a web-based problem-solving model and quickly engages a large number of international workers at low cost. Furthermore, previous research shows the effectiveness of crowdsourcing as a method of accomplishing labor intensive natural language processing tasks (Callison-Burch, 2009) and the effectiveness of using MTurk for a variety of natural language automation tasks (Snow, Jurafsy, &amp; O&apos;Connor, 2008). The experiment answers the following questions: • Can we discover valid nicknames not currently in DAN? • What do we need to pay workers to gather nicknames rapidly? • How do we convey the task to guide nonexperts and increase participation from Arab countries? 89 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 89–92, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics 2 </context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Callison-Burch, C. (2009). Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk. Proceedings of the EMNLP. Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Halpern</author>
</authors>
<title>Lexicon-Driven Approach to the Recognition of Arabic Named Entities.</title>
<date>2009</date>
<booktitle>Second International Conference on Arabic Language Resources and Tools.</booktitle>
<location>Cairo.</location>
<contexts>
<context position="1685" citStr="Halpern, 2009" startWordPosition="240" endWordPosition="241">cing add undocumented nicknames to existing Named Entity (NE) lexicons? This experiment seeks to produce nicknames to add to DAN Version 1.1, which contains 147,739 lines of names. While DAN does not list nicknames as a metadata type, it does include some commonly known nicknames. 1.1 Traditional collection methods are costly According to DAN’s website, administrators collect nicknames using a team of software engineers and native speakers. They also draw on a “large variety of sources including websites, corpora, books, phone directories, dictionaries, encyclopedias, and university rosters” (Halpern, 2009). Collecting names by searching various media sources or employing linguists and native speakers is a massive effort requiring significant expenditure of time and money. 1.2 Crowdsourcing might work better The experiment uses crowdsourcing via MTurk since it offers a web-based problem-solving model and quickly engages a large number of international workers at low cost. Furthermore, previous research shows the effectiveness of crowdsourcing as a method of accomplishing labor intensive natural language processing tasks (Callison-Burch, 2009) and the effectiveness of using MTurk for a variety of</context>
</contexts>
<marker>Halpern, 2009</marker>
<rawString>Halpern, J. (2009). Lexicon-Driven Approach to the Recognition of Arabic Named Entities. Second International Conference on Arabic Language Resources and Tools. Cairo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross</author>
<author>A Zaldivar</author>
<author>L Irani</author>
<author>B Tomlinson</author>
</authors>
<title>Who are the Turkers? Worker Demographics in Amazon.</title>
<date>2009</date>
<institution>Department of Informatics, University of California, Irvine.</institution>
<marker>Ross, Zaldivar, Irani, Tomlinson, 2009</marker>
<rawString>Ross, J., Zaldivar, A., Irani, L., &amp; Tomlinson, B. (2009). Who are the Turkers? Worker Demographics in Amazon. Department of Informatics, University of California, Irvine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsy</author>
<author>B O’Connor</author>
</authors>
<title>Cheap and fast – but is it good?: Evaluating Non-Expert Annotations for Natural Language Tasks.</title>
<date>2008</date>
<booktitle>Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>254--263</pages>
<location>Honolulu.</location>
<marker>Snow, Jurafsy, O’Connor, 2008</marker>
<rawString>Snow, R., Jurafsy, D., &amp; O’Connor, B. (2008). Cheap and fast – but is it good?: Evaluating Non-Expert Annotations for Natural Language Tasks. Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, (pp. 254-263). Honolulu.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>