<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.961937">
DLS@CU: Sentence Similarity from Word Alignment
</title>
<author confidence="0.962628">
Md Arafat Sultan†, Steven Bethard‡, Tamara Sumner††Institute of Cognitive Science and Department of Computer Science
</author>
<affiliation confidence="0.998278">
University of Colorado Boulder
‡Department of Computer and Information Sciences
University of Alabama at Birmingham
</affiliation>
<email confidence="0.983002">
arafat.sultan@colorado.edu, bethard@cis.uab.edu, sumner@colorado.edu
</email>
<sectionHeader confidence="0.997366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999722941176471">
We present an algorithm for computing
the semantic similarity between two sen-
tences. It adopts the hypothesis that se-
mantic similarity is a monotonically in-
creasing function of the degree to which
(1) the two sentences contain similar se-
mantic units, and (2) such units occur in
similar semantic contexts. With a simplis-
tic operationalization of the notion of se-
mantic units with individual words, we ex-
perimentally show that this hypothesis can
lead to state-of-the-art results for sentence-
level semantic similarity. At the Sem-
Eval 2014 STS task (task 10), our system
demonstrated the best performance (mea-
sured by correlation with human annota-
tions) among 38 system runs.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.90627425">
Semantic textual similarity (STS), in the context
of short text fragments, has drawn considerable
attention in recent times. Its application spans a
multitude of areas, including natural language pro-
cessing, information retrieval and digital learning.
Examples of tasks that benefit from STS include
text summarization, machine translation, question
answering, short answer scoring, and so on.
The annual series of SemEval STS tasks (Agirre
et al., 2012; Agirre et al., 2013; Agirre et al., 2014)
is an important platform where STS systems are
evaluated on common data and evaluation criteria.
In this article, we describe an STS system which
participated and outperformed all other systems at
SemEval 2014.
The algorithm is a straightforward application
of the monolingual word aligner presented in (Sul-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
tan et al., 2014). This aligner aligns related words
in two sentences based on the following properties
of the words:
</bodyText>
<listItem confidence="0.992210333333333">
1. They are semantically similar.
2. They occur in similar semantic contexts in
the respective sentences.
</listItem>
<bodyText confidence="0.999803785714286">
The output of the word aligner for a sentence
pair can be used to predict the pair’s semantic sim-
ilarity by taking the proportion of their aligned
content words. Intuitively, the more semantic
components in the sentences we can meaningfully
align, the higher their semantic similarity should
be. In experiments on STS 2013 data reported
by Sultan et al. (2014), this approach was found
highly effective. We also adopt this hypothesis of
semantic compositionality for STS 2014.
We implement an STS algorithm that is only
slightly different from the algorithm in (Sultan et
al., 2014). The approach remains equally success-
ful on STS 2014 data.
</bodyText>
<sectionHeader confidence="0.995479" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999654">
We focus on two relevant topics in this section:
the state of the art of STS research, and the word
aligner presented in (Sultan et al., 2014).
</bodyText>
<subsectionHeader confidence="0.988653">
2.1 Semantic Textual Similarity
</subsectionHeader>
<bodyText confidence="0.999959166666667">
Since the inception of textual similarity research
for short text, perhaps with the studies reported
by Mihalcea et al. (2006) and Li et al. (2006),
the topic has spawned significant research inter-
est. The majority of systems have been reported
as part of the SemEval 2012 and *SEM 2013 STS
tasks (Agirre et al., 2012; Agirre et al., 2013).
Here we confine our discussion to systems that
participated in these tasks.
With designated training data for several test
sets, supervised systems were the most successful
in STS 2012 (B¨ar et al., 2012; ˇSari´c et al., 2012;
</bodyText>
<page confidence="0.975497">
241
</page>
<note confidence="0.7343575">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246,
Dublin, Ireland, August 23-24, 2014.
</note>
<figure confidence="0.994854210526316">
Align
identical
word
sequences
Align
named
entities
Align
content
words
using
depen-
dencies
Align
content
words
using sur-
rounding
words
</figure>
<bodyText confidence="0.997769666666667">
Jimenez et al., 2012). Such systems typically ap-
ply a regression algorithm on a large number of
STS features (e.g., string similarity, syntactic sim-
ilarity and word or phrase-level semantic similar-
ity) to generate a final similarity score. This ap-
proach continued to do well in 2013 (Han et al.,
2013; Wu et al., 2013; Shareghi and Bergler, 2013)
even without domain-specific training data, but the
best results were demonstrated by an unsupervised
system (Han et al., 2013). This has important im-
plications for STS since extraction of each feature
adds to the latency of a supervised system. STS
systems are typically important in the context of
a larger system rather than on their own, so high
latency is an obvious drawback for such systems.
We present an STS system that has simplicity,
high accuracy and speed as its design goals, can
be deployed without any supervision, operates in
a linguistically principled manner with purely se-
mantic sentence properties, and was the top sys-
tem at SemEval STS 2014.
</bodyText>
<subsectionHeader confidence="0.996595">
2.2 The Sultan et al. (2014) Aligner
</subsectionHeader>
<bodyText confidence="0.969372363636364">
The word aligner presented in (Sultan et al., 2014)
has been used unchanged in this work and plays a
central role in our STS algorithm. We give only an
overview here; for the full details, see the original
article.
We will denote the sentences being aligned (and
are subsequently input to the STS algorithm) as
5(1) and 5(2). We describe only content word
alignment here; stop words are not used in our STS
computation.
The aligner first identifies word pairs w(1) iE
</bodyText>
<equation confidence="0.937492">
5(1) and w(2)
j E 5(2) such that:
1. w(1) iand w(2)
</equation>
<bodyText confidence="0.683976333333333">
j have non-zero semantic simi-
larity, simWij. The calculation of simWij is
described in Section 2.2.1.
</bodyText>
<listItem confidence="0.706975470588235">
2. The semantic contexts of w(1)
i and w(2)
j have
some similarity, simCij. We define the se-
mantic context of a word w in a sentence
5 as a set of words in 5, and the seman-
tic context of the word pair (w(1)
i ,w(2)
j ), de-
noted by contextij, as the Cartesian product
of the context of w(1) iin 5(1) and the con-
text of w(2)
j in 5(2). We define several types
of context (i.e., several selections of words)
and describe the corresponding calculations
of simCij in Section 2.2.2.
3. There are no competing pairs scoring higher
</listItem>
<figureCaption confidence="0.992281">
Figure 1: The alignment pipeline.
</figureCaption>
<equation confidence="0.996997166666667">
than (w(1)
i , w(2)
j ) under f(simW, simC) =
0.9 x simW + 0.1 x simC. That is,
there are no pairs (w(1)
k , w(2)
</equation>
<bodyText confidence="0.617949333333333">
j ) such that
f(simWkj, simCkj) &gt; f(simWij, simCij),
and there are no pairs (w(1)
</bodyText>
<equation confidence="0.758445">
i , w(2)
</equation>
<bodyText confidence="0.990136142857143">
l ) such that
f(simWil, simCil) &gt; f(simWij, simCij).
The weights 0.9 and 0.1 were derived empiri-
cally via a grid search in the range [0, 1] (with
a step size of 0.1) to maximize alignment per-
formance on the training set of the (Brockett,
2007) alignment corpus. This set contains
800 human-aligned sentence pairs collected
from a textual entailment corpus (Bar-Haim
et al., 2006).
The aligner then performs one-to-one word align-
ments in decreasing order of the f value.
This alignment process is applied in four steps
as shown in Figure 1; each step applies the above
process to a particular type of context: identi-
cal words, dependencies and surrounding content
words. Additionally, named entities are aligned in
a separate step (details in Section 2.2.2).
Words that are aligned by an earlier module of
the pipeline are not allowed to be re-aligned by
downstream modules.
</bodyText>
<subsectionHeader confidence="0.623107">
2.2.1 Word Similarity
</subsectionHeader>
<bodyText confidence="0.944488">
Word similarity (simW) is computed as follows:
</bodyText>
<listItem confidence="0.884138">
1. If the two words or their lemmas are identi-
cal, then simW = 1.
2. If the two words are present as a pair
in the lexical XXXL corpus of the Para-
phrase Database1 (PPDB) (Ganitkevitch et
al., 2013), then simW = 0.9.2 For this
step, PPDB was augmented with lemmatized
forms of the already existing word pairs.3
</listItem>
<footnote confidence="0.773765">
1PPDB is a large database of lexical, phrasal and syntactic
paraphrases.
2Again, the value 0.9 was derived empirically via a grid
search in [0, 1] (step size = 0.1) to maximize alignment per-
formance on the (Brockett, 2007) training data.
3The Python NLTK WordNetLemmatizer was used to
lemmatize the original PPDB words.
</footnote>
<page confidence="0.990043">
242
</page>
<listItem confidence="0.929903">
3. For any other word pair, simW = 0.
</listItem>
<subsubsectionHeader confidence="0.740199">
2.2.2 Contextual Similarity
</subsubsectionHeader>
<bodyText confidence="0.972198">
Contextual similarity (simC) for a word pair
</bodyText>
<equation confidence="0.997618">
(w(1)
i , w(2)
</equation>
<bodyText confidence="0.7859035">
j ) is computed as the sum of the word
similarities for each pair of words in the context of
</bodyText>
<equation confidence="0.671997133333333">
(w(1)
i ,w(2)
j ). That is:
dobj
nsubj
5(1): He wrote a book .
rcmod
nsubj
5(2): I read the book he wrote .
det
nsubj
dobj
det
simCij = simWkl Figure 2: Example of dependency equivalence.
(wk1),wl2)) ∈ contextij
</equation>
<bodyText confidence="0.988420461538462">
Each of the stages in Figure 1 employs a specific
type of context.
Identical Word Sequences. Contextual sim-
ilarity for identical word sequences (a word se-
quence W which is present in both 5(1) and 5(2)
and contains at least one content word) defines the
context by pairing up each word in the instance of
W in 5(1) with its occurrence in the instance of
W in 5(2). All such sequences with length ≥ 2
are aligned; longer sequences are aligned before
shorter ones. This simple step was found to be of
very high precision in (Sultan et al., 2014) and re-
duces the overall computational cost of alignment.
Named Entities. Named entities are a special
case in the alignment pipeline. Even though the
context for a named entity is defined in the same
way as it is defined for any other content word
(as described below), named entities are aligned
in a separate step before other content words be-
cause they have special properties such as corefer-
ring mentions of different lengths (e.g. Smith and
John Smith, BBC and British Broadcasting Cor-
poration). The head word of the named entity is
used in dependency calculations.
Dependencies. Dependency-based contex-
tual similarity defines the context for the pair
</bodyText>
<equation confidence="0.992027142857143">
(w(1)
i , w(2)
j ) using the syntactic dependencies of
wi and w(2)
(1) j . The context is the set of all word
pairs (w(1)
k , w(2)
</equation>
<bodyText confidence="0.537296">
l ) such that:
</bodyText>
<listItem confidence="0.941945333333333">
• wk is a dependency of w(1)
(1) i ,
• wl is a dependency of w(2)
(2) j ,
• wi and w(2)
(1) j have the same lexical category,
• wk and w(2)
(1) l have the same lexical category,
and,
• The two dependencies are either identical or
semantically “equivalent” according to the
equivalence table provided by Sultan et al.
</listItem>
<bodyText confidence="0.70730425">
(2014). We explain semantic equivalence of
dependencies using an example below.
Equivalence of Dependency Structures. Con-
sider 5(1) and 5(2) in Figure 2. Note that w(1)
</bodyText>
<equation confidence="0.998349888888889">
2 =
w6 = ‘wrote’ and w(1)
(2) 4 = w(2) 4= ‘book’ in
this pair. Now, each of the two following typed
dependencies: dobj(w(1)
2 ,w(1)
4 ) in 5(1) and rc-
mod(w(2)
4 , w(2)
</equation>
<bodyText confidence="0.9661286">
6 ) in 5(2), represents the relation
“thing that was written” between the verb ‘wrote’
and its argument ‘book’. Thus, to summarize,
an instance of contextual evidence for a possible
alignment between the pair (w(1)
</bodyText>
<equation confidence="0.92705125">
2 , w(2)
6 ) (‘wrote’)
lies in the pair (w(1)
4 , w(2)
</equation>
<bodyText confidence="0.984953142857143">
4 ) (‘book’) and the equiv-
alence of the two dependency types dobj and rc-
mod.
The equivalence table of Sultan et al. (2014) is
a list of all such possible equivalences among dif-
ferent dependency types (given that w(1)
i has the
</bodyText>
<equation confidence="0.6710094">
same lexical category as w(2)
j and w(1)
k has the
same lexical category as w(2)
l ).
</equation>
<bodyText confidence="0.9503568">
If there are no word pairs with identical or
equivalent dependencies as defined above, i.e. if
simCij = 0, then w(1) iand w(2)
j will not be
aligned by this module.
</bodyText>
<subsectionHeader confidence="0.521913">
Surrounding Content Words. Surrounding-
</subsectionHeader>
<bodyText confidence="0.95371">
word-based contextual similarity defines the con-
text of a word in a sentence as a fixed window of
3 words to its left and 3 words to its right. Only
content words in the window are considered. (As
explained in the beginning of this section, the con-
text of the pair (w(1)i,wj(2)) is then the Cartesian
product of the context of w(1) iin 5(1) and w(2)
j in
5(2).) Note that w(1)
</bodyText>
<equation confidence="0.586227">
i and w(2)
j can be of different
</equation>
<bodyText confidence="0.969094">
lexical categories here.
A content word can often be surrounded by
stop words which provide almost no information
about its semantic context. The chosen window
size is assumed, on average, to effectively make
</bodyText>
<page confidence="0.998447">
243
</page>
<table confidence="0.985818857142857">
Data Set Source of Text # of Pairs
deft-forum discussion forums 450
deft-news news articles 300
headlines news headlines 750
images image descriptions 750
OnWN word sense definitions 750
tweet-news news articles and tweets 750
</table>
<tableCaption confidence="0.998887">
Table 1: Test sets for SemEval STS 2014.
</tableCaption>
<bodyText confidence="0.852412">
sufficient contextual information available while
avoiding the inclusion of contextually unrelated
words. But further experiments are necessary to
determine the best span in the context of align-
ment.
Unlike dependency-based alignment, even if
there are no similar words in the context, i.e. if
simCij = 0, w(1)
</bodyText>
<equation confidence="0.9901882">
i may still be aligned to w(2)
j if
simWij &gt; 0 and no alignments for w(1)
i or w(2)
j
</equation>
<bodyText confidence="0.864027">
have been found by earlier stages of the pipeline.
</bodyText>
<subsubsectionHeader confidence="0.836249">
2.2.3 The Alignment Sequence
</subsubsectionHeader>
<bodyText confidence="0.999957333333333">
The rationale behind the specific sequence of
alignment steps (Figure 1) was explained in (Sul-
tan et al., 2014): (1) Identical word sequence
alignment was found to be the step with the
highest precision in experiments on the (Brock-
ett, 2007) training data, (2) It is convenient to
align named entities before other content words
to enable alignment of entity mentions of differ-
ent lengths, (3) Dependency-based evidence was
observed to be more reliable (i.e. of higher preci-
sion) than textual evidence on the (Brockett, 2007)
training data.
</bodyText>
<sectionHeader confidence="0.993644" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.99996125">
Our STS score is a function of the proportions of
aligned content words in the two input sentences.
The proportion of content words in S(1) that are
aligned to some word in S(2) is:
</bodyText>
<equation confidence="0.9950564">
(1) = Ili : [Ij : (i, j) E Al] and w E C}I
o
Al
Ili : w(1)
i EC}1
</equation>
<bodyText confidence="0.907949272727273">
where C is the set of all content words in En-
glish and Al are the predicted word alignments. A
word alignment is a pair of indices (i, j) indicating
that word w(1)
i is aligned to w(2)
j . The proportion
of aligned content words in S(2), prop(2), can be
Al
computed in a similar way.
We posit that a simple yet sensible way to obtain
an STS estimate for S(1) and S(2) is to take a mean
</bodyText>
<table confidence="0.99987475">
Data Set Run 1 Run 2
deft-forum 0.4828 0.4828
deft-news 0.7657 0.7657
headlines 0.7646 0.7646
images 0.8214 0.8214
OnWN 0.7227 0.8589
tweet-news 0.7639 0.7639
Weighted Mean 0.7337 0.7610
</table>
<tableCaption confidence="0.996561">
Table 2: Results of evaluation on SemEval STS
</tableCaption>
<bodyText confidence="0.756558142857143">
2014 data. Each value on columns 2 and 3 is the
correlation between system output and human an-
notations for the corresponding data set. The last
row shows the value of the final evaluation metric.
of prop(1) Aland prop(2)
Al . Our two submitted runs
use the harmonic mean:
</bodyText>
<equation confidence="0.989620666666667">
sim(S(1), S(2)) = 2 x prop(1)
Al x prop(2)
Al
prop(1)
Al + prop(2)
Al
</equation>
<bodyText confidence="0.999865">
It is a more conservative estimate than the arith-
metic mean, and penalizes sentence pairs with a
large disparity between the values of prop(1) and
</bodyText>
<equation confidence="0.581027666666667">
Al
propAl . Experiments on STS 2012 and 2013 data
(2)
</equation>
<bodyText confidence="0.998426">
revealed the harmonic mean of the two propor-
tions to be a better STS estimate than the arith-
metic mean.
</bodyText>
<sectionHeader confidence="0.99581" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999774111111111">
STS systems at SemEval 2014 were evaluated on
six data sets. Each test set consists of a number
of sentence pairs; each pair has a human-assigned
similarity score in the range [0, 5] which increases
with similarity. Every score is the mean of five
scores crowdsourced using the Amazon Mechan-
ical Turk. The sentences were collected from a
variety of sources. In Table 1, we provide a brief
description of each test set.
</bodyText>
<sectionHeader confidence="0.996352" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999810857142857">
We submitted the results of two system runs at
SemEval 2014 based on the idea presented in Sec-
tion 3. The two runs were identical, except for the
fact that for the OnWN test set, we specified the
following words as additional stop words during
run 2 (but not during run 1): something, someone,
somebody, act, activity, some, state.4 For both
</bodyText>
<footnote confidence="0.999459">
4OnWN has many sentence pairs where each sentence
is of the form “the act/activity/state of verb+ing some-
thing/somebody”. The selected words act merely as fillers
in such pairs and consequently do not typically contribute to
the similarity scores.
</footnote>
<page confidence="0.989858">
244
</page>
<table confidence="0.999936">
Data Set Run 1 Run 2
FNWN 0.4686 0.4686
headlines 0.7797 0.7797
OnWN 0.6083 0.8197
SMT 0.3837 0.3837
Weighted Mean 0.5788 0.6315
</table>
<tableCaption confidence="0.998552">
Table 3: Results of evaluation on *SEM STS 2013
</tableCaption>
<bodyText confidence="0.98227235">
data.
runs, the tweet-news sentences were preprocessed
by separating the hashtag from the word for each
hashtagged word.
Table 2 shows the performance of each run.
Rows 1 through 6 show the Pearson correlation
coefficients between the system scores and human
annotations for all test sets. The last row shows
the value of the final evaluation metric, which is a
weighted sum of all correlations in rows 1–6. The
weight assigned to a data set is proportional to its
number of pairs. Our run 1 ranked 7th and run 2
ranked 1st among 38 submitted system runs.
An important implication of these results is the
fact that knowledge of domain-specific stop words
can be beneficial for an STS system. Even though
we imparted this knowledge to our system during
run 2 via a manually constructed set of additional
stop words, simple measures like TF-IDF can be
used to automate the process.
</bodyText>
<subsectionHeader confidence="0.995657">
5.1 Performance on STS 2012 and 2013 Data
</subsectionHeader>
<bodyText confidence="0.999782090909091">
We applied our algorithm on the 2012 and 2013
STS test sets to examine its general utility. Note
that the STS 2013 setup was similar to STS 2014
with no domain-dependent training data, whereas
several of the 2012 test sets had designated train-
ing data.
Over all the 2013 test sets, our two runs demon-
strated weighted correlations of 0.5788 (rank: 4)
and 0.6315 (rank: 1), respectively. Table 3 shows
performances on individual test sets. (Descrip-
tions of the test sets can be found in (Agirre et
al., 2013).) Again, run 2 outperformed run 1 on
OnWN by a large margin.
On the 2012 test sets, however, the performance
was worse (relative to other systems), with respec-
tive weighted correlations of 0.6476 (rank: 8) and
0.6423 (rank: 9). Table 4 shows performances on
individual test sets. (Descriptions of the test sets
can be found in (Agirre et al., 2012).)
This performance drop seems to be an obvious
consequence of the fact that our algorithm was
not trained on domain-specific data: on STS 2013
</bodyText>
<table confidence="0.999879285714286">
Data Set Run 1 Run 2
MSRpar 0.6413 0.6413
MSRvid 0.8200 0.8200
OnWN 0.7227 0.7004
SMTeuroparl 0.4267 0.4267
SMTnews 0.4486 0.4486
Weighted Mean 0.6476 0.6423
</table>
<tableCaption confidence="0.978775">
Table 4: Results of evaluation on SemEval STS
2012 data.
</tableCaption>
<bodyText confidence="0.999872894736842">
data, the top two STS 2012 systems, with respec-
tive weighted correlations of 0.5652 and 0.5221
(Agirre et al., 2013), were outperformed by our
system by a large margin.
In contrast to the other two years, our run 1
outperformed run 2 on the 2012 OnWN test set
by a very small margin. A closer inspection
revealed that the previously mentioned sentence
structure “the act/activity/state of verb+ing some-
thing/somebody” is much less common in this set,
and as a result, our additional stop words tend to
play more salient semantic roles in this set than in
the other two OnWN sets (i.e. they act relatively
more as content words than stop words). The drop
in correlation with human annotations is a con-
sequence of this role reversal. This result again
shows the importance of a proper selection of stop
words for STS and also points to the challenges
associated with making such a selection.
</bodyText>
<sectionHeader confidence="0.999137" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999960294117647">
We show that alignment of related words in two
sentences, if carried out in a principled and accu-
rate manner, can yield state-of-the-art results for
sentence-level semantic similarity. Our system has
the desired quality of being both accurate and fast.
Evaluation on test data from different STS years
demonstrates its general applicability as well.
The idea of STS from alignment is worth inves-
tigating with larger semantic units (i.e. phrases)
in the two sentences. Another possible research
direction is to investigate whether the alignment
proportions observed for the two sentences can be
used as features to improve performance in a su-
pervised setup (even though this scenario is ar-
guably less common in practice because of un-
availability of domain or situation-specific train-
ing data).
</bodyText>
<sectionHeader confidence="0.999222" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.915502">
This material is based in part upon work supported
by the National Science Foundation under Grant
</bodyText>
<page confidence="0.994574">
245
</page>
<bodyText confidence="0.999746">
Numbers EHR/0835393 and EHR/0835381. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
author(s) and do not necessarily reflect the views
of the National Science Foundation.
</bodyText>
<sectionHeader confidence="0.99593" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999824397849462">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: A
pilot on semantic textual similarity. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, Volume 2: Proceedings
of the Sixth International Workshop on Semantic
Evaluation, SemEval ’12, pages 385-393, Montreal,
Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic Textual Similarity. In Proceedings of
the Second Joint Conference on Lexical and Compu-
tational Semantics, *SEM ’13, pages 32-43, Atlanta,
Georgia, USA.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilin-
gual semantic textual similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation, SemEval ’14, Dublin, Ireland.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The Second PASCAL Recognising
Textual Entailment Challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation, held in
conjunction with the 1st Joint Conference on Lexical
and Computational Semantics, SemEval ’12, pages
435-440, Montreal, Canada.
Chris Brockett. 2007. Aligning the RTE 2006 Corpus.
Technical Report MSR-TR-2007-77, Microsoft Re-
search.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL-HLT ’13,
pages 758-764, Atlanta, Georgia, USA.
Lushan Han, Abhay Kashyap, Tim Finin, James
Mayfield, and Jonathan Weese. 2013. UMBC
EBIQUITY-CORE: Semantic Textual Similarity
Systems. In Proceedings of the Second Joint Con-
ference on Lexical and Computational Semantics,
*SEM ’13, pages 44-52, Atlanta, Georgia, USA.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft Cardinality: a parameterized sim-
ilarity function for text comparison. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation, held in conjunction with the 1st Joint Con-
ference on Lexical and Computational Semantics,
SemEval ’12, pages 449-453, Montreal, Canada.
Yuhua Li, David Mclean, Zuhair A. Bandar, James D.
O’Shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
IEEE Transactions on Knowledge and Data Engi-
neering, vol.18, no.8. 1138-1150.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of the 21st national conference on Artificial in-
telligence, AAAI ’06, pages 775-780, Boston, Mas-
sachusetts, USA.
Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder,
and Bojana Dalbelo Ba&amp;quot;si´c. 2012. TakeLab: sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the 6th International Workshop on Se-
mantic Evaluation, held in conjunction with the 1st
Joint Conference on Lexical and Computational Se-
mantics, SemEval ’12, pages 441-448, Montreal,
Canada.
Ehsan Shareghi and Sabine Bergler. 2013. CLaC-
CORE: Exhaustive Feature Combination for Mea-
suring Textual Similarity. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics, *SEM ’13, pages 202-206, At-
lanta, Georgia, USA.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014. Back to Basics for Monolingual Align-
ment: Exploiting Word Similarity and Contextual
Evidence. Transactions of the Association for Com-
putational Linguistics, 2 (May), pages 219-230.
Stephen Wu, Dongqing Zhu, Ben Carterette, and Hong-
fang Liu. 2013. MayoClinicNLP-CORE: Semantic
representations for textual similarity. In Proceed-
ings of the Second Joint Conference on Lexical and
Computational Semantics, *SEM ’13, pages 148-
154, Atlanta, Georgia, USA.
</reference>
<page confidence="0.998728">
246
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.693924">
<title confidence="0.983729">DLS@CU: Sentence Similarity from Word Alignment</title>
<author confidence="0.735152">Arafat Steven Tamara of Cognitive Science</author>
<author confidence="0.735152">Department of Computer</author>
<affiliation confidence="0.992822666666667">University of Colorado of Computer and Information University of Alabama at</affiliation>
<email confidence="0.997444">arafat.sultan@colorado.edu,bethard@cis.uab.edu,sumner@colorado.edu</email>
<abstract confidence="0.997957">We present an algorithm for computing the semantic similarity between two sentences. It adopts the hypothesis that semantic similarity is a monotonically increasing function of the degree to which (1) the two sentences contain similar semantic units, and (2) such units occur in similar semantic contexts. With a simplistic operationalization of the notion of semantic units with individual words, we experimentally show that this hypothesis can lead to state-of-the-art results for sentencelevel semantic similarity. At the Sem- Eval 2014 STS task (task 10), our system demonstrated the best performance (measured by correlation with human annotations) among 38 system runs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics, Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12,</booktitle>
<pages>385--393</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="1501" citStr="Agirre et al., 2012" startWordPosition="213" endWordPosition="216">e SemEval 2014 STS task (task 10), our system demonstrated the best performance (measured by correlation with human annotations) among 38 system runs. 1 Introduction Semantic textual similarity (STS), in the context of short text fragments, has drawn considerable attention in recent times. Its application spans a multitude of areas, including natural language processing, information retrieval and digital learning. Examples of tasks that benefit from STS include text summarization, machine translation, question answering, short answer scoring, and so on. The annual series of SemEval STS tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014) is an important platform where STS systems are evaluated on common data and evaluation criteria. In this article, we describe an STS system which participated and outperformed all other systems at SemEval 2014. The algorithm is a straightforward application of the monolingual word aligner presented in (SulThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ tan et al., 2014). This aligner align</context>
<context position="3436" citStr="Agirre et al., 2012" startWordPosition="524" endWordPosition="527">different from the algorithm in (Sultan et al., 2014). The approach remains equally successful on STS 2014 data. 2 Background We focus on two relevant topics in this section: the state of the art of STS research, and the word aligner presented in (Sultan et al., 2014). 2.1 Semantic Textual Similarity Since the inception of textual similarity research for short text, perhaps with the studies reported by Mihalcea et al. (2006) and Li et al. (2006), the topic has spawned significant research interest. The majority of systems have been reported as part of the SemEval 2012 and *SEM 2013 STS tasks (Agirre et al., 2012; Agirre et al., 2013). Here we confine our discussion to systems that participated in these tasks. With designated training data for several test sets, supervised systems were the most successful in STS 2012 (B¨ar et al., 2012; ˇSari´c et al., 2012; 241 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland, August 23-24, 2014. Align identical word sequences Align named entities Align content words using dependencies Align content words using surrounding words Jimenez et al., 2012). Such systems typically apply a regression algorith</context>
<context position="17611" citStr="Agirre et al., 2012" startWordPosition="3041" endWordPosition="3044">ed training data. Over all the 2013 test sets, our two runs demonstrated weighted correlations of 0.5788 (rank: 4) and 0.6315 (rank: 1), respectively. Table 3 shows performances on individual test sets. (Descriptions of the test sets can be found in (Agirre et al., 2013).) Again, run 2 outperformed run 1 on OnWN by a large margin. On the 2012 test sets, however, the performance was worse (relative to other systems), with respective weighted correlations of 0.6476 (rank: 8) and 0.6423 (rank: 9). Table 4 shows performances on individual test sets. (Descriptions of the test sets can be found in (Agirre et al., 2012).) This performance drop seems to be an obvious consequence of the fact that our algorithm was not trained on domain-specific data: on STS 2013 Data Set Run 1 Run 2 MSRpar 0.6413 0.6413 MSRvid 0.8200 0.8200 OnWN 0.7227 0.7004 SMTeuroparl 0.4267 0.4267 SMTnews 0.4486 0.4486 Weighted Mean 0.6476 0.6423 Table 4: Results of evaluation on SemEval STS 2012 data. data, the top two STS 2012 systems, with respective weighted correlations of 0.5652 and 0.5221 (Agirre et al., 2013), were outperformed by our system by a large margin. In contrast to the other two years, our run 1 outperformed run 2 on the </context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12, pages 385-393, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>shared task: Semantic Textual Similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM ’13,</booktitle>
<pages>32--43</pages>
<publisher>SEM</publisher>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="1522" citStr="Agirre et al., 2013" startWordPosition="217" endWordPosition="220">sk (task 10), our system demonstrated the best performance (measured by correlation with human annotations) among 38 system runs. 1 Introduction Semantic textual similarity (STS), in the context of short text fragments, has drawn considerable attention in recent times. Its application spans a multitude of areas, including natural language processing, information retrieval and digital learning. Examples of tasks that benefit from STS include text summarization, machine translation, question answering, short answer scoring, and so on. The annual series of SemEval STS tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014) is an important platform where STS systems are evaluated on common data and evaluation criteria. In this article, we describe an STS system which participated and outperformed all other systems at SemEval 2014. The algorithm is a straightforward application of the monolingual word aligner presented in (SulThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ tan et al., 2014). This aligner aligns related words in tw</context>
<context position="3458" citStr="Agirre et al., 2013" startWordPosition="528" endWordPosition="531">gorithm in (Sultan et al., 2014). The approach remains equally successful on STS 2014 data. 2 Background We focus on two relevant topics in this section: the state of the art of STS research, and the word aligner presented in (Sultan et al., 2014). 2.1 Semantic Textual Similarity Since the inception of textual similarity research for short text, perhaps with the studies reported by Mihalcea et al. (2006) and Li et al. (2006), the topic has spawned significant research interest. The majority of systems have been reported as part of the SemEval 2012 and *SEM 2013 STS tasks (Agirre et al., 2012; Agirre et al., 2013). Here we confine our discussion to systems that participated in these tasks. With designated training data for several test sets, supervised systems were the most successful in STS 2012 (B¨ar et al., 2012; ˇSari´c et al., 2012; 241 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland, August 23-24, 2014. Align identical word sequences Align named entities Align content words using dependencies Align content words using surrounding words Jimenez et al., 2012). Such systems typically apply a regression algorithm on a large number of</context>
<context position="17262" citStr="Agirre et al., 2013" startWordPosition="2981" endWordPosition="2984">words, simple measures like TF-IDF can be used to automate the process. 5.1 Performance on STS 2012 and 2013 Data We applied our algorithm on the 2012 and 2013 STS test sets to examine its general utility. Note that the STS 2013 setup was similar to STS 2014 with no domain-dependent training data, whereas several of the 2012 test sets had designated training data. Over all the 2013 test sets, our two runs demonstrated weighted correlations of 0.5788 (rank: 4) and 0.6315 (rank: 1), respectively. Table 3 shows performances on individual test sets. (Descriptions of the test sets can be found in (Agirre et al., 2013).) Again, run 2 outperformed run 1 on OnWN by a large margin. On the 2012 test sets, however, the performance was worse (relative to other systems), with respective weighted correlations of 0.6476 (rank: 8) and 0.6423 (rank: 9). Table 4 shows performances on individual test sets. (Descriptions of the test sets can be found in (Agirre et al., 2012).) This performance drop seems to be an obvious consequence of the fact that our algorithm was not trained on domain-specific data: on STS 2013 Data Set Run 1 Run 2 MSRpar 0.6413 0.6413 MSRvid 0.8200 0.8200 OnWN 0.7227 0.7004 SMTeuroparl 0.4267 0.4267</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic Textual Similarity. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM ’13, pages 32-43, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<title>SemEval-2014 Task 10: Multilingual semantic textual similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval ’14,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1544" citStr="Agirre et al., 2014" startWordPosition="221" endWordPosition="224">tem demonstrated the best performance (measured by correlation with human annotations) among 38 system runs. 1 Introduction Semantic textual similarity (STS), in the context of short text fragments, has drawn considerable attention in recent times. Its application spans a multitude of areas, including natural language processing, information retrieval and digital learning. Examples of tasks that benefit from STS include text summarization, machine translation, question answering, short answer scoring, and so on. The annual series of SemEval STS tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014) is an important platform where STS systems are evaluated on common data and evaluation criteria. In this article, we describe an STS system which participated and outperformed all other systems at SemEval 2014. The algorithm is a straightforward application of the monolingual word aligner presented in (SulThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ tan et al., 2014). This aligner aligns related words in two sentences based on t</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 Task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval ’14, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The Second PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="6775" citStr="Bar-Haim et al., 2006" startWordPosition="1108" endWordPosition="1111">her Figure 1: The alignment pipeline. than (w(1) i , w(2) j ) under f(simW, simC) = 0.9 x simW + 0.1 x simC. That is, there are no pairs (w(1) k , w(2) j ) such that f(simWkj, simCkj) &gt; f(simWij, simCij), and there are no pairs (w(1) i , w(2) l ) such that f(simWil, simCil) &gt; f(simWij, simCij). The weights 0.9 and 0.1 were derived empirically via a grid search in the range [0, 1] (with a step size of 0.1) to maximize alignment performance on the training set of the (Brockett, 2007) alignment corpus. This set contains 800 human-aligned sentence pairs collected from a textual entailment corpus (Bar-Haim et al., 2006). The aligner then performs one-to-one word alignments in decreasing order of the f value. This alignment process is applied in four steps as shown in Figure 1; each step applies the above process to a particular type of context: identical words, dependencies and surrounding content words. Additionally, named entities are aligned in a separate step (details in Section 2.2.2). Words that are aligned by an earlier module of the pipeline are not allowed to be re-aligned by downstream modules. 2.2.1 Word Similarity Word similarity (simW) is computed as follows: 1. If the two words or their lemmas </context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The Second PASCAL Recognising Textual Entailment Challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics, SemEval ’12,</booktitle>
<pages>435--440</pages>
<location>Montreal, Canada.</location>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. UKP: computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics, SemEval ’12, pages 435-440, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
</authors>
<title>Aligning the RTE</title>
<date>2007</date>
<tech>Technical Report MSR-TR-2007-77, Microsoft Research.</tech>
<contexts>
<context position="6639" citStr="Brockett, 2007" startWordPosition="1091" endWordPosition="1092">ons of words) and describe the corresponding calculations of simCij in Section 2.2.2. 3. There are no competing pairs scoring higher Figure 1: The alignment pipeline. than (w(1) i , w(2) j ) under f(simW, simC) = 0.9 x simW + 0.1 x simC. That is, there are no pairs (w(1) k , w(2) j ) such that f(simWkj, simCkj) &gt; f(simWij, simCij), and there are no pairs (w(1) i , w(2) l ) such that f(simWil, simCil) &gt; f(simWij, simCij). The weights 0.9 and 0.1 were derived empirically via a grid search in the range [0, 1] (with a step size of 0.1) to maximize alignment performance on the training set of the (Brockett, 2007) alignment corpus. This set contains 800 human-aligned sentence pairs collected from a textual entailment corpus (Bar-Haim et al., 2006). The aligner then performs one-to-one word alignments in decreasing order of the f value. This alignment process is applied in four steps as shown in Figure 1; each step applies the above process to a particular type of context: identical words, dependencies and surrounding content words. Additionally, named entities are aligned in a separate step (details in Section 2.2.2). Words that are aligned by an earlier module of the pipeline are not allowed to be re-</context>
<context position="7871" citStr="Brockett, 2007" startWordPosition="1298" endWordPosition="1299">ream modules. 2.2.1 Word Similarity Word similarity (simW) is computed as follows: 1. If the two words or their lemmas are identical, then simW = 1. 2. If the two words are present as a pair in the lexical XXXL corpus of the Paraphrase Database1 (PPDB) (Ganitkevitch et al., 2013), then simW = 0.9.2 For this step, PPDB was augmented with lemmatized forms of the already existing word pairs.3 1PPDB is a large database of lexical, phrasal and syntactic paraphrases. 2Again, the value 0.9 was derived empirically via a grid search in [0, 1] (step size = 0.1) to maximize alignment performance on the (Brockett, 2007) training data. 3The Python NLTK WordNetLemmatizer was used to lemmatize the original PPDB words. 242 3. For any other word pair, simW = 0. 2.2.2 Contextual Similarity Contextual similarity (simC) for a word pair (w(1) i , w(2) j ) is computed as the sum of the word similarities for each pair of words in the context of (w(1) i ,w(2) j ). That is: dobj nsubj 5(1): He wrote a book . rcmod nsubj 5(2): I read the book he wrote . det nsubj dobj det simCij = simWkl Figure 2: Example of dependency equivalence. (wk1),wl2)) ∈ contextij Each of the stages in Figure 1 employs a specific type of context. </context>
<context position="12800" citStr="Brockett, 2007" startWordPosition="2187" endWordPosition="2189">s. But further experiments are necessary to determine the best span in the context of alignment. Unlike dependency-based alignment, even if there are no similar words in the context, i.e. if simCij = 0, w(1) i may still be aligned to w(2) j if simWij &gt; 0 and no alignments for w(1) i or w(2) j have been found by earlier stages of the pipeline. 2.2.3 The Alignment Sequence The rationale behind the specific sequence of alignment steps (Figure 1) was explained in (Sultan et al., 2014): (1) Identical word sequence alignment was found to be the step with the highest precision in experiments on the (Brockett, 2007) training data, (2) It is convenient to align named entities before other content words to enable alignment of entity mentions of different lengths, (3) Dependency-based evidence was observed to be more reliable (i.e. of higher precision) than textual evidence on the (Brockett, 2007) training data. 3 Method Our STS score is a function of the proportions of aligned content words in the two input sentences. The proportion of content words in S(1) that are aligned to some word in S(2) is: (1) = Ili : [Ij : (i, j) E Al] and w E C}I o Al Ili : w(1) i EC}1 where C is the set of all content words in </context>
</contexts>
<marker>Brockett, 2007</marker>
<rawString>Chris Brockett. 2007. Aligning the RTE 2006 Corpus. Technical Report MSR-TR-2007-77, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The Paraphrase Database.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT ’13,</booktitle>
<pages>758--764</pages>
<location>Atlanta, Georgia, USA.</location>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The Paraphrase Database. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT ’13, pages 758-764, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Jonathan Weese</author>
</authors>
<title>UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM ’13,</booktitle>
<pages>44--52</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="4263" citStr="Han et al., 2013" startWordPosition="655" endWordPosition="658">¨ar et al., 2012; ˇSari´c et al., 2012; 241 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland, August 23-24, 2014. Align identical word sequences Align named entities Align content words using dependencies Align content words using surrounding words Jimenez et al., 2012). Such systems typically apply a regression algorithm on a large number of STS features (e.g., string similarity, syntactic similarity and word or phrase-level semantic similarity) to generate a final similarity score. This approach continued to do well in 2013 (Han et al., 2013; Wu et al., 2013; Shareghi and Bergler, 2013) even without domain-specific training data, but the best results were demonstrated by an unsupervised system (Han et al., 2013). This has important implications for STS since extraction of each feature adds to the latency of a supervised system. STS systems are typically important in the context of a larger system rather than on their own, so high latency is an obvious drawback for such systems. We present an STS system that has simplicity, high accuracy and speed as its design goals, can be deployed without any supervision, operates in a linguist</context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay Kashyap, Tim Finin, James Mayfield, and Jonathan Weese. 2013. UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM ’13, pages 44-52, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Soft Cardinality: a parameterized similarity function for text comparison.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics, SemEval ’12,</booktitle>
<pages>449--453</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="3984" citStr="Jimenez et al., 2012" startWordPosition="608" endWordPosition="611"> as part of the SemEval 2012 and *SEM 2013 STS tasks (Agirre et al., 2012; Agirre et al., 2013). Here we confine our discussion to systems that participated in these tasks. With designated training data for several test sets, supervised systems were the most successful in STS 2012 (B¨ar et al., 2012; ˇSari´c et al., 2012; 241 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland, August 23-24, 2014. Align identical word sequences Align named entities Align content words using dependencies Align content words using surrounding words Jimenez et al., 2012). Such systems typically apply a regression algorithm on a large number of STS features (e.g., string similarity, syntactic similarity and word or phrase-level semantic similarity) to generate a final similarity score. This approach continued to do well in 2013 (Han et al., 2013; Wu et al., 2013; Shareghi and Bergler, 2013) even without domain-specific training data, but the best results were demonstrated by an unsupervised system (Han et al., 2013). This has important implications for STS since extraction of each feature adds to the latency of a supervised system. STS systems are typically im</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2012</marker>
<rawString>Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh. 2012. Soft Cardinality: a parameterized similarity function for text comparison. In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics, SemEval ’12, pages 449-453, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David Mclean</author>
<author>Zuhair A Bandar</author>
<author>James D O’Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence similarity based on semantic nets and corpus statistics.</title>
<date>2006</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<pages>1138--1150</pages>
<marker>Li, Mclean, Bandar, O’Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David Mclean, Zuhair A. Bandar, James D. O’Shea, and Keeley Crockett. 2006. Sentence similarity based on semantic nets and corpus statistics. IEEE Transactions on Knowledge and Data Engineering, vol.18, no.8. 1138-1150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st national conference on Artificial intelligence, AAAI ’06,</booktitle>
<pages>775--780</pages>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="3245" citStr="Mihalcea et al. (2006)" startWordPosition="489" endWordPosition="492">d by Sultan et al. (2014), this approach was found highly effective. We also adopt this hypothesis of semantic compositionality for STS 2014. We implement an STS algorithm that is only slightly different from the algorithm in (Sultan et al., 2014). The approach remains equally successful on STS 2014 data. 2 Background We focus on two relevant topics in this section: the state of the art of STS research, and the word aligner presented in (Sultan et al., 2014). 2.1 Semantic Textual Similarity Since the inception of textual similarity research for short text, perhaps with the studies reported by Mihalcea et al. (2006) and Li et al. (2006), the topic has spawned significant research interest. The majority of systems have been reported as part of the SemEval 2012 and *SEM 2013 STS tasks (Agirre et al., 2012; Agirre et al., 2013). Here we confine our discussion to systems that participated in these tasks. With designated training data for several test sets, supervised systems were the most successful in STS 2012 (B¨ar et al., 2012; ˇSari´c et al., 2012; 241 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland, August 23-24, 2014. Align identical w</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st national conference on Artificial intelligence, AAAI ’06, pages 775-780, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane Sari´c</author>
<author>Goran Glavas</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Ba&amp;quot;si´c.</title>
<date></date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics, SemEval ’12,</booktitle>
<pages>441--448</pages>
<location>Montreal, Canada.</location>
<marker>Sari´c, Glavas, Karan, </marker>
<rawString>Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder, and Bojana Dalbelo Ba&amp;quot;si´c. 2012. TakeLab: systems for measuring semantic text similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics, SemEval ’12, pages 441-448, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehsan Shareghi</author>
<author>Sabine Bergler</author>
</authors>
<title>CLaCCORE: Exhaustive Feature Combination for Measuring Textual Similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM ’13,</booktitle>
<pages>202--206</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="4309" citStr="Shareghi and Bergler, 2013" startWordPosition="663" endWordPosition="666">012; 241 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland, August 23-24, 2014. Align identical word sequences Align named entities Align content words using dependencies Align content words using surrounding words Jimenez et al., 2012). Such systems typically apply a regression algorithm on a large number of STS features (e.g., string similarity, syntactic similarity and word or phrase-level semantic similarity) to generate a final similarity score. This approach continued to do well in 2013 (Han et al., 2013; Wu et al., 2013; Shareghi and Bergler, 2013) even without domain-specific training data, but the best results were demonstrated by an unsupervised system (Han et al., 2013). This has important implications for STS since extraction of each feature adds to the latency of a supervised system. STS systems are typically important in the context of a larger system rather than on their own, so high latency is an obvious drawback for such systems. We present an STS system that has simplicity, high accuracy and speed as its design goals, can be deployed without any supervision, operates in a linguistically principled manner with purely semantic </context>
</contexts>
<marker>Shareghi, Bergler, 2013</marker>
<rawString>Ehsan Shareghi and Sabine Bergler. 2013. CLaCCORE: Exhaustive Feature Combination for Measuring Textual Similarity. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM ’13, pages 202-206, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Md Arafat Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<title>Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<pages>219--230</pages>
<contexts>
<context position="2648" citStr="Sultan et al. (2014)" startWordPosition="391" endWordPosition="394">reativecommons.org/licenses/by/4.0/ tan et al., 2014). This aligner aligns related words in two sentences based on the following properties of the words: 1. They are semantically similar. 2. They occur in similar semantic contexts in the respective sentences. The output of the word aligner for a sentence pair can be used to predict the pair’s semantic similarity by taking the proportion of their aligned content words. Intuitively, the more semantic components in the sentences we can meaningfully align, the higher their semantic similarity should be. In experiments on STS 2013 data reported by Sultan et al. (2014), this approach was found highly effective. We also adopt this hypothesis of semantic compositionality for STS 2014. We implement an STS algorithm that is only slightly different from the algorithm in (Sultan et al., 2014). The approach remains equally successful on STS 2014 data. 2 Background We focus on two relevant topics in this section: the state of the art of STS research, and the word aligner presented in (Sultan et al., 2014). 2.1 Semantic Textual Similarity Since the inception of textual similarity research for short text, perhaps with the studies reported by Mihalcea et al. (2006) an</context>
<context position="5002" citStr="Sultan et al. (2014)" startWordPosition="779" endWordPosition="782">monstrated by an unsupervised system (Han et al., 2013). This has important implications for STS since extraction of each feature adds to the latency of a supervised system. STS systems are typically important in the context of a larger system rather than on their own, so high latency is an obvious drawback for such systems. We present an STS system that has simplicity, high accuracy and speed as its design goals, can be deployed without any supervision, operates in a linguistically principled manner with purely semantic sentence properties, and was the top system at SemEval STS 2014. 2.2 The Sultan et al. (2014) Aligner The word aligner presented in (Sultan et al., 2014) has been used unchanged in this work and plays a central role in our STS algorithm. We give only an overview here; for the full details, see the original article. We will denote the sentences being aligned (and are subsequently input to the STS algorithm) as 5(1) and 5(2). We describe only content word alignment here; stop words are not used in our STS computation. The aligner first identifies word pairs w(1) iE 5(1) and w(2) j E 5(2) such that: 1. w(1) iand w(2) j have non-zero semantic similarity, simWij. The calculation of simWij </context>
<context position="8947" citStr="Sultan et al., 2014" startWordPosition="1495" endWordPosition="1498"> simWkl Figure 2: Example of dependency equivalence. (wk1),wl2)) ∈ contextij Each of the stages in Figure 1 employs a specific type of context. Identical Word Sequences. Contextual similarity for identical word sequences (a word sequence W which is present in both 5(1) and 5(2) and contains at least one content word) defines the context by pairing up each word in the instance of W in 5(1) with its occurrence in the instance of W in 5(2). All such sequences with length ≥ 2 are aligned; longer sequences are aligned before shorter ones. This simple step was found to be of very high precision in (Sultan et al., 2014) and reduces the overall computational cost of alignment. Named Entities. Named entities are a special case in the alignment pipeline. Even though the context for a named entity is defined in the same way as it is defined for any other content word (as described below), named entities are aligned in a separate step before other content words because they have special properties such as coreferring mentions of different lengths (e.g. Smith and John Smith, BBC and British Broadcasting Corporation). The head word of the named entity is used in dependency calculations. Dependencies. Dependency-bas</context>
<context position="10797" citStr="Sultan et al. (2014)" startWordPosition="1835" endWordPosition="1838">uctures. Consider 5(1) and 5(2) in Figure 2. Note that w(1) 2 = w6 = ‘wrote’ and w(1) (2) 4 = w(2) 4= ‘book’ in this pair. Now, each of the two following typed dependencies: dobj(w(1) 2 ,w(1) 4 ) in 5(1) and rcmod(w(2) 4 , w(2) 6 ) in 5(2), represents the relation “thing that was written” between the verb ‘wrote’ and its argument ‘book’. Thus, to summarize, an instance of contextual evidence for a possible alignment between the pair (w(1) 2 , w(2) 6 ) (‘wrote’) lies in the pair (w(1) 4 , w(2) 4 ) (‘book’) and the equivalence of the two dependency types dobj and rcmod. The equivalence table of Sultan et al. (2014) is a list of all such possible equivalences among different dependency types (given that w(1) i has the same lexical category as w(2) j and w(1) k has the same lexical category as w(2) l ). If there are no word pairs with identical or equivalent dependencies as defined above, i.e. if simCij = 0, then w(1) iand w(2) j will not be aligned by this module. Surrounding Content Words. Surroundingword-based contextual similarity defines the context of a word in a sentence as a fixed window of 3 words to its left and 3 words to its right. Only content words in the window are considered. (As explained</context>
<context position="12670" citStr="Sultan et al., 2014" startWordPosition="2163" endWordPosition="2167">Test sets for SemEval STS 2014. sufficient contextual information available while avoiding the inclusion of contextually unrelated words. But further experiments are necessary to determine the best span in the context of alignment. Unlike dependency-based alignment, even if there are no similar words in the context, i.e. if simCij = 0, w(1) i may still be aligned to w(2) j if simWij &gt; 0 and no alignments for w(1) i or w(2) j have been found by earlier stages of the pipeline. 2.2.3 The Alignment Sequence The rationale behind the specific sequence of alignment steps (Figure 1) was explained in (Sultan et al., 2014): (1) Identical word sequence alignment was found to be the step with the highest precision in experiments on the (Brockett, 2007) training data, (2) It is convenient to align named entities before other content words to enable alignment of entity mentions of different lengths, (3) Dependency-based evidence was observed to be more reliable (i.e. of higher precision) than textual evidence on the (Brockett, 2007) training data. 3 Method Our STS score is a function of the proportions of aligned content words in the two input sentences. The proportion of content words in S(1) that are aligned to s</context>
</contexts>
<marker>Sultan, Bethard, Sumner, 2014</marker>
<rawString>Md Arafat Sultan, Steven Bethard, and Tamara Sumner. 2014. Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence. Transactions of the Association for Computational Linguistics, 2 (May), pages 219-230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wu</author>
<author>Dongqing Zhu</author>
<author>Ben Carterette</author>
<author>Hongfang Liu</author>
</authors>
<title>MayoClinicNLP-CORE: Semantic representations for textual similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM ’13,</booktitle>
<pages>148--154</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="4280" citStr="Wu et al., 2013" startWordPosition="659" endWordPosition="662">ˇSari´c et al., 2012; 241 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland, August 23-24, 2014. Align identical word sequences Align named entities Align content words using dependencies Align content words using surrounding words Jimenez et al., 2012). Such systems typically apply a regression algorithm on a large number of STS features (e.g., string similarity, syntactic similarity and word or phrase-level semantic similarity) to generate a final similarity score. This approach continued to do well in 2013 (Han et al., 2013; Wu et al., 2013; Shareghi and Bergler, 2013) even without domain-specific training data, but the best results were demonstrated by an unsupervised system (Han et al., 2013). This has important implications for STS since extraction of each feature adds to the latency of a supervised system. STS systems are typically important in the context of a larger system rather than on their own, so high latency is an obvious drawback for such systems. We present an STS system that has simplicity, high accuracy and speed as its design goals, can be deployed without any supervision, operates in a linguistically principled</context>
</contexts>
<marker>Wu, Zhu, Carterette, Liu, 2013</marker>
<rawString>Stephen Wu, Dongqing Zhu, Ben Carterette, and Hongfang Liu. 2013. MayoClinicNLP-CORE: Semantic representations for textual similarity. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, *SEM ’13, pages 148-154, Atlanta, Georgia, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>