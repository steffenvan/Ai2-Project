<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.033515">
<title confidence="0.983944">
TJP: Using Twitter to Analyze the Polarity of Contexts
</title>
<author confidence="0.994727">
Tawunrat Chalothorn Jeremy Ellman
</author>
<affiliation confidence="0.846403333333333">
University of Northumbria at Newcastle University of Northumbria at Newcastle
Pandon Building, Camden Street Pandon Building, Camden Street
Newcastle Upon Tyne, NE2 1XE, UK Newcastle Upon Tyne, NE2 1XE, UK
</affiliation>
<email confidence="0.964007">
Tawunrat.chalothorn@unn.ac.uk Jeremy.ellman@unn.ac.uk
</email>
<sectionHeader confidence="0.996197" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999731777777778">
This paper presents our system, TJP, which
participated in SemEval 2013 Task 2 part A:
Contextual Polarity Disambiguation. The goal
of this task is to predict whether marked con-
texts are positive, neutral or negative. Howev-
er, only the scores of positive and negative
class will be used to calculate the evaluation
result using F-score. We chose to work as
‘constrained’, which used only the provided
training and development data without addi-
tional sentiment annotated resources. Our ap-
proach considered unigram, bigram and
trigram using Naïve Bayes training model
with the objective of establishing a simple-
approach baseline. Our system achieved F-
score 81.23% and F-score 78.16% in the re-
sults for SMS messages and Tweets respec-
tively.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994670461538462">
Natural language processing (NLP) is a research
area comprising various tasks; one of which is sen-
timent analysis. The main goal of sentiment analy-
sis is to identify the polarity of natural language
text (Shaikh et al., 2007). Sentiment analysis can
be referred to as opinion mining, as study peoples’
opinions, appraisals and emotions towards entities
and events and their attributes (Pang and Lee,
2008). Sentiment analysis has become a popular
research area in NLP with the purpose of identify-
ing opinions or attitudes in terms of polarity.
This paper presents TJP, a system submitted to
SemEval 2013 for Task 2 part A: Contextual Polar-
</bodyText>
<page confidence="0.987156">
375
</page>
<bodyText confidence="0.99968875">
ity Disambiguation (Wilson et al., 2013). TJP was
focused on the ‘constrained’ task, which used only
training and development data provided. This
avoided both resource implications and potential
advantages implied by the use of additional data
containing sentiment annotations. The objective
was to explore the relative success of a simple ap-
proach that could be implemented easily with
open-source software.
The TJP system was implemented using the Py-
thon Natural Language Toolkit (NLTK, Bird et al.,
2009). We considered several basic approaches.
These used a preprocessing phase to expand con-
tractions, eliminate stopwords, and identify emoti-
cons. The next phase used supervised machine
learning and n-gram features. Although we had
two approaches that both used n-gram features, we
were limited to submitting just one result. Conse-
quently, we chose to submit a unigram based ap-
proach followed by naive Bayes since this
performed better on the data.
The remainder of this paper is structured as fol-
lows: section 2 provides some discussion on the
related work. The methodology of corpus collec-
tion and data classification are provided in section
3. Section 4 outlines details of the experiment and
results, followed by the conclusion and ideas for
future work in section 5.
</bodyText>
<sectionHeader confidence="0.999806" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996478888888889">
The micro-blogging tool Twitter is well-known
and increasingly popular. Twitter allows its users
to post messages, or ‘Tweets’ of up to 140 charac-
ters each time, which are available for immediate
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 375–379, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
download over the Internet. Tweets are extremely
interesting to marketing since their rapid public
interaction can either indicate customer success or
presage public relations disasters far more quickly
than web pages or traditional media. Consequently,
the content of tweets and identifying their senti-
ment polarity as positive or negative is a current
active research topic.
Emoticons are features of both SMS texts, and
tweets. Emoticons such as :) to represent a smile,
allow emotions to augment the limited text in SMS
messages using few characters. Read (2005) used
emoticons from a training set that was downloaded
from Usenet newsgroups as annotations (positive
and negative). Using the machine learning tech-
niques of Naïve Bayes and Support Vector Ma-
chines Read (2005) achieved up to 70 % accuracy
in determining text polarity from the emoticons
used.
Go et al. (2009) used distant supervision to clas-
sify sentiment of Twitter, as similar as in (Read,
2005). Emoticons have been used as noisy labels in
training data to perform distant supervised learning
(positive and negative). Three classifiers were
used: Naïve Bayes, Maximum Entropy and Sup-
port Vector Machine, and they were able to obtain
more than 80% accuracy on their testing data.
Aisopos et al. (2011) divided tweets in to three
groups using emoticons for classification. If tweets
contain positive emoticons, they will be classified
as positive and vice versa. Tweets without posi-
tive/negative emoticons will be classified as neu-
tral. However, tweets that contain both positive
and negative emoticons are ignored in their study.
Their task focused on analyzing the contents of
social media by using n-gram graphs, and the re-
sults showed that n-gram yielded high accuracy
when tested with C4.5, but low accuracy with Na-
ïve Bayes Multinomial (NBM).
</bodyText>
<sectionHeader confidence="0.994905" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.993987">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.99798675">
The training data set for SemEval was built using
Twitter messages training and development data.
There are more than 7000 pieces of context. Users
usually use emoticons in their tweets; therefore,
emoticons have been manually collected and la-
beled as positive and negative to provide some
context (Table 1), which is the same idea as in Ai-
sopos et al. (2011).
</bodyText>
<table confidence="0.96048">
Negative emoticons :( :-( :d :&lt; D: :\ /: etc.
Positive emoticons :) ;) :-) ;-) :P ;P (: (; :D
;D etc.
</table>
<tableCaption confidence="0.999916">
Table 1: Emoticon labels as negative and positive
</tableCaption>
<bodyText confidence="0.99913275">
Furthermore, there are often features that have
been used in tweets, such as hashtags, URL links,
etc. To extract those features, the following pro-
cesses have been applied to the data.
</bodyText>
<listItem confidence="0.922848083333333">
1. Retweet (RT), twitter username (@panda),
URL links (e.g. y2u.be/fiKKzdLQvFo),
and special punctuation were removed.
2. Hashtags have been replaced by the fol-
lowing word (e.g. # love was replaced by
love, # exciting was replaced by exciting).
3. English contraction of ‘not’ was converted
to full form (e.g. don’t -&gt; do not).
4. Repeated letters have been reduced and re-
placed by 2 of the same character (e.g.
happpppppy will be replaced by happy,
coollllll will be replaced by cooll).
</listItem>
<subsectionHeader confidence="0.993443">
3.2 Classifier
</subsectionHeader>
<bodyText confidence="0.999592909090909">
Our system used the NLTK Naïve Bayes classifier
module. This is a classification based on Bayes’s
rule and also known as the state-of-art of the Bayes
rules (Cufoglu et al., 2008). The Naïve Bayes
model follows the assumption that attributes within
the same case are independent given the class label
(Hope and Korb, 2004).
Tang et al. (2009) considered that Naïve Bayes
assigns a context (represented by a vector ) to
the class that maximizes by applying
Bayes’s rule, as in (1).
</bodyText>
<equation confidence="0.999047">
(  |) (1)
</equation>
<bodyText confidence="0.9959752">
where is a randomly selected context . The
representation of vector is . is the random
select context that is assigned to class .
To classify the term , features in
were assumed as from as in (2).
</bodyText>
<page confidence="0.957797">
376
</page>
<equation confidence="0.7569655">
∏
(  |) (2)
</equation>
<bodyText confidence="0.999896363636364">
There are many different approaches to lan-
guage analysis using syntax, semantics, and se-
mantic resources such as WordNet. That may be
exploited using the NLTK (Bird et al. 2009). How-
ever, for simplicity we opted here for the n-gram
approach where texts are decomposed into term
sequences. A set of single sequences is a unigram.
The set of two word sequences (with overlapping)
are bigrams, whilst the set of overlapping three
term sequences are trigrams. The relative ad-
vantage of the bi-and trigram approaches are that
coordinates terms effectively disambiguate senses
and focus content retrieval and recognition.
N-grams have been used many times in contents
classification. For example, Pang et al. (2002) used
unigram and bigram to classify movie reviews. The
results showed that unigram gave better results
than bigram. Conversely, Dave et al. (2003) re-
ported gaining better results from trigrams rather
than bigram in classifying product reviews. Conse-
quently, we chose to evaluate unigrams, bigrams
and trigrams to see which will give the best results
</bodyText>
<figureCaption confidence="0.9999975">
Figure 1: Comparison of Twitter messages from two approaches
Figure 2: Comparison of SMS messages from two approaches
</figureCaption>
<figure confidence="0.994741694444444">
90
85
80
75
70
65
60
55
50 Unigram Bigram Trigram
Pos 1 84.46 82.09 80.8
Neg 1 71.08 59.53 52.91
Pos 2 84.62 83.31 83.25
Neg 2 71.70 65.00 64.34
F-score (%)
Pos 1
Neg 1
Pos 2
Neg 2
90
50 Unigram Bigram Trigram
Pos 1 76.23 73.89 72.02
Neg 1 82.61 76.04 71.19
Pos 2 77.81 75.69 75.42
Neg 2 84.66 79.94 79.37
F-score (%)
75
70
65
60
55
85
80
Pos 1
Neg 1
Pos 2
Neg 2
</figure>
<page confidence="0.773843">
377
</page>
<bodyText confidence="0.7091635">
in the polarity classification. Our results are de-
scribed in the next section.
</bodyText>
<figure confidence="0.780426142857143">
4 Experiment and Results
gonna miss (some of) my classes.
Unigram Bigram Trigram
gonna gonna miss gonna miss my
miss miss my miss my classes
my my classes
classes
</figure>
<bodyText confidence="0.992174266666667">
In this experiment, we used the distributed data
from Twitter messages and the F-measure for sys-
tem evaluation. As at first approach, the corpora
were trained directly in the system, while stop-
words (e.g. a, an, the) were removed before train-
ing using the python NLTK for the second
approach. The approaches are demonstrated on a
sample context in Table 2 and 3.
After comparing both approaches (Figure 1), we
were able to obtain an F-score 84.62% of positive
and 71.70% of negative after removing stopwords.
Then, the average F-score is 78.16%, which was
increased from the first approach by 0.50%. The
results from both approaches showed that, unigram
achieved higher scores than either bigrams or tri-
grams.
Moreover, these experiments have been tested
with a set of SMS messages to assess how well our
system trained on Twitter data can be generalized
to other types of message data. The second ap-
proach still achieved the better scores (Figure 2),
where we were able to obtain an F-score of 77.81%
of positive and 84.66% of negative; thus, the aver-
age F-score is 81.23%.
The results of unigram from the second ap-
proach submitted to SemEval 2013 can be found in
Figure 3. After comparing them using the average
F-score from positive and negative class, the re-
sults showed that our system works better for SMS
messaging than for Twitter.
</bodyText>
<table confidence="0.904450125">
gonna miss some of my classes.
Unigram Bigram Trigram
gonna gonna miss gonna miss some
miss miss some miss some of
some some of some of my
of of my of my classes
my my classes
classes
</table>
<tableCaption confidence="0.961196">
Table 2: Example of context from first approach
Table 3: Example of context from second approach.
Note ‘some’ and ‘of’ are listed in NLTK stopwords.
</tableCaption>
<figureCaption confidence="0.9102005">
Figure 3: Results of unigram of Twitter and SMS in the
second approach
</figureCaption>
<sectionHeader confidence="0.939784" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.998755">
A system, TJP, has been described that participated
in SemEval 2013 Task 2 part A: Contextual Polari-
ty Disambiguation (Wilson et al., 2013). The sys-
tem used the Python NLTK (Bird et al 2009) Naive
Bayes classifier trained on Twitter data. Further-
more, emoticons were collected and labeled as pos-
itive and negative in order to classify contexts with
emoticons. After analyzing the Twitter message
and SMS messages, we were able to obtain an av-
erage F-score of 78.16% and 81.23% respectively
during the SemEval 2013 task. The reason that, our
system achieved better scores with SMS message
then Twitter message might be due to our use of
Twitter messages as training data. However this is
still to be verified experimentally.
The experimental performance on the tasks
demonstrates the advantages of simple approaches.
This provides a baseline performance set to which
more sophisticated or resource intensive tech-
niques may be compared.
</bodyText>
<figure confidence="0.998005277777778">
F-score (%)
80
75
70
65
Pos
84.62
Neg
71.70
Average
78.16
85
90
77.81
81.23
84.66
Twitter
SMS
</figure>
<page confidence="0.993402">
378
</page>
<bodyText confidence="0.9999594">
For future work, we intend to trace back to the
root words and work with the suffix and prefix that
imply negative semantics, such as ‘dis-’, ‘un-’, ‘-
ness’ and ‘-less’. Moreover, we would like to col-
lect more shorthand texts than that used commonly
in microblogs, such as gr8 (great), btw (by the
way), pov (point of view), gd (good) and ne1 (any-
one). We believe these could help to improve our
system and achieve better accuracy when classify-
ing the sentiment of context from microblogs.
</bodyText>
<sectionHeader confidence="0.997804" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999665107142857">
Alec Go, Richa Bhayani and Lei Huang. 2009. Twitter
sentiment classification using distant supervision.
CS224N Project Report, Stanford, 1-12.
Ayse Cufoglu, Mahi Lohi and Kambiz Madani. 2008.
Classification accuracy performance of Naive Bayes-
ian (NB), Bayesian Networks (BN), Lazy Learning of
Bayesian Rules (LBR) and Instance-Based Learner
(IB1) - comparative study. Paper presented at the
Computer Engineering &amp; Systems, 2008. ICCES
2008. International Conference on.
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. Paper presented at the
Proceedings of the ACL-02 conference on Empirical
methods in natural language processing - Volume 10.
Fotis Aisopos, George Papadakis and Theodora
Varvarigou. 2011. Sentiment analysis of social media
content using N-Gram graphs. Paper presented at the
Proceedings of the 3rd ACM SIGMM international
workshop on Social media, Scottsdale, Arizona,
USA.
Huifeng Tang, Songbo Tan and Xueqi Cheng. 2009. A
survey on sentiment detection of reviews. Expert Sys-
tems with Applications, 36(7), 10760-10773.
Jonathon. Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. Paper presented at the
Proceedings of the ACL Student Research Work-
shop, Ann Arbor, Michigan.
Kushal Dave, Steve Lawrence and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. Paper
presented at the Proceedings of the 12th international
conference on World Wide Web, Budapest, Hungary.
Lucas R. Hope and Kevin B. Korb. 2004. A bayesian
metric for evaluating machine learning algorithms.
Paper presented at the Proceedings of the 17th Aus-
tralian joint conference on Advances in Artificial In-
telligence, Cairns, Australia.
Mostafa Al Shaikh, Helmut Prendinger and Ishizuka
Mitsuru. 2007. Assessing Sentiment of Text by Se-
mantic Dependency and Contextual Valence Analy-
sis. Paper presented at the Proceedings of the 2nd in-
ternational conference on Affective Computing and
Intelligent Interaction, Lisbon, Portugal.
Pang Bo and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Found. Trends Inf. Retr., 2(1-2),
1-135.
Steven Bird, Ewan Klein and Edward Loper. 2009. Nat-
ural language processing with Python: O&apos;Reilly.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov and Alan Ritter.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter Proceedings of the 7th International Work-
shop on Semantic Evaluation: Association for Com-
putational Linguistics.
</reference>
<page confidence="0.999181">
379
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.372468">
<title confidence="0.999333">TJP: Using Twitter to Analyze the Polarity of Contexts</title>
<author confidence="0.999847">Tawunrat Chalothorn Jeremy Ellman</author>
<affiliation confidence="0.999882">University of Northumbria at Newcastle University of Northumbria at Newcastle</affiliation>
<address confidence="0.7336885">Pandon Building, Camden Street Pandon Building, Camden Street Newcastle Upon Tyne, NE2 1XE, UK Newcastle Upon Tyne, NE2 1XE, UK</address>
<author confidence="0.896185">Tawunrat chalothornunn ac uk Jeremy ellmanunn ac uk</author>
<abstract confidence="0.990840315789474">This paper presents our system, TJP, which participated in SemEval 2013 Task 2 part A: Contextual Polarity Disambiguation. The goal of this task is to predict whether marked contexts are positive, neutral or negative. However, only the scores of positive and negative class will be used to calculate the evaluation result using F-score. We chose to work as which used only the provided training and development data without additional sentiment annotated resources. Our approach considered unigram, bigram and trigram using Naïve Bayes training model with the objective of establishing a simpleapproach baseline. Our system achieved Fscore 81.23% and F-score 78.16% in the results for SMS messages and Tweets respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N Project Report,</title>
<date>2009</date>
<pages>1--12</pages>
<location>Stanford,</location>
<contexts>
<context position="4369" citStr="Go et al. (2009)" startWordPosition="672" endWordPosition="675">tent of tweets and identifying their sentiment polarity as positive or negative is a current active research topic. Emoticons are features of both SMS texts, and tweets. Emoticons such as :) to represent a smile, allow emotions to augment the limited text in SMS messages using few characters. Read (2005) used emoticons from a training set that was downloaded from Usenet newsgroups as annotations (positive and negative). Using the machine learning techniques of Naïve Bayes and Support Vector Machines Read (2005) achieved up to 70 % accuracy in determining text polarity from the emoticons used. Go et al. (2009) used distant supervision to classify sentiment of Twitter, as similar as in (Read, 2005). Emoticons have been used as noisy labels in training data to perform distant supervised learning (positive and negative). Three classifiers were used: Naïve Bayes, Maximum Entropy and Support Vector Machine, and they were able to obtain more than 80% accuracy on their testing data. Aisopos et al. (2011) divided tweets in to three groups using emoticons for classification. If tweets contain positive emoticons, they will be classified as positive and vice versa. Tweets without positive/negative emoticons w</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani and Lei Huang. 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ayse Cufoglu</author>
<author>Mahi Lohi</author>
<author>Kambiz Madani</author>
</authors>
<title>Classification accuracy performance of Naive Bayesian</title>
<date>2008</date>
<booktitle>(NB), Bayesian Networks (BN), Lazy Learning of Bayesian Rules (LBR) and Instance-Based Learner (IB1) - comparative study. Paper presented at the Computer Engineering &amp; Systems,</booktitle>
<contexts>
<context position="6716" citStr="Cufoglu et al., 2008" startWordPosition="1063" endWordPosition="1066">2u.be/fiKKzdLQvFo), and special punctuation were removed. 2. Hashtags have been replaced by the following word (e.g. # love was replaced by love, # exciting was replaced by exciting). 3. English contraction of ‘not’ was converted to full form (e.g. don’t -&gt; do not). 4. Repeated letters have been reduced and replaced by 2 of the same character (e.g. happpppppy will be replaced by happy, coollllll will be replaced by cooll). 3.2 Classifier Our system used the NLTK Naïve Bayes classifier module. This is a classification based on Bayes’s rule and also known as the state-of-art of the Bayes rules (Cufoglu et al., 2008). The Naïve Bayes model follows the assumption that attributes within the same case are independent given the class label (Hope and Korb, 2004). Tang et al. (2009) considered that Naïve Bayes assigns a context (represented by a vector ) to the class that maximizes by applying Bayes’s rule, as in (1). ( |) (1) where is a randomly selected context . The representation of vector is . is the random select context that is assigned to class . To classify the term , features in were assumed as from as in (2). 376 ∏ ( |) (2) There are many different approaches to language analysis using syntax, semant</context>
</contexts>
<marker>Cufoglu, Lohi, Madani, 2008</marker>
<rawString>Ayse Cufoglu, Mahi Lohi and Kambiz Madani. 2008. Classification accuracy performance of Naive Bayesian (NB), Bayesian Networks (BN), Lazy Learning of Bayesian Rules (LBR) and Instance-Based Learner (IB1) - comparative study. Paper presented at the Computer Engineering &amp; Systems, 2008. ICCES 2008. International Conference on.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>Paper presented at the Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume</booktitle>
<volume>10</volume>
<contexts>
<context position="7948" citStr="Pang et al. (2002)" startWordPosition="1273" endWordPosition="1276">tic resources such as WordNet. That may be exploited using the NLTK (Bird et al. 2009). However, for simplicity we opted here for the n-gram approach where texts are decomposed into term sequences. A set of single sequences is a unigram. The set of two word sequences (with overlapping) are bigrams, whilst the set of overlapping three term sequences are trigrams. The relative advantage of the bi-and trigram approaches are that coordinates terms effectively disambiguate senses and focus content retrieval and recognition. N-grams have been used many times in contents classification. For example, Pang et al. (2002) used unigram and bigram to classify movie reviews. The results showed that unigram gave better results than bigram. Conversely, Dave et al. (2003) reported gaining better results from trigrams rather than bigram in classifying product reviews. Consequently, we chose to evaluate unigrams, bigrams and trigrams to see which will give the best results Figure 1: Comparison of Twitter messages from two approaches Figure 2: Comparison of SMS messages from two approaches 90 85 80 75 70 65 60 55 50 Unigram Bigram Trigram Pos 1 84.46 82.09 80.8 Neg 1 71.08 59.53 52.91 Pos 2 84.62 83.31 83.25 Neg 2 71.7</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. Paper presented at the Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fotis Aisopos</author>
<author>George Papadakis</author>
<author>Theodora Varvarigou</author>
</authors>
<title>Sentiment analysis of social media content using N-Gram graphs.</title>
<date>2011</date>
<booktitle>Paper presented at the Proceedings of the 3rd ACM SIGMM international workshop on Social media,</booktitle>
<location>Scottsdale, Arizona, USA.</location>
<contexts>
<context position="4764" citStr="Aisopos et al. (2011)" startWordPosition="736" endWordPosition="739">otations (positive and negative). Using the machine learning techniques of Naïve Bayes and Support Vector Machines Read (2005) achieved up to 70 % accuracy in determining text polarity from the emoticons used. Go et al. (2009) used distant supervision to classify sentiment of Twitter, as similar as in (Read, 2005). Emoticons have been used as noisy labels in training data to perform distant supervised learning (positive and negative). Three classifiers were used: Naïve Bayes, Maximum Entropy and Support Vector Machine, and they were able to obtain more than 80% accuracy on their testing data. Aisopos et al. (2011) divided tweets in to three groups using emoticons for classification. If tweets contain positive emoticons, they will be classified as positive and vice versa. Tweets without positive/negative emoticons will be classified as neutral. However, tweets that contain both positive and negative emoticons are ignored in their study. Their task focused on analyzing the contents of social media by using n-gram graphs, and the results showed that n-gram yielded high accuracy when tested with C4.5, but low accuracy with Naïve Bayes Multinomial (NBM). 3 Methodology 3.1 Corpus The training data set for Se</context>
</contexts>
<marker>Aisopos, Papadakis, Varvarigou, 2011</marker>
<rawString>Fotis Aisopos, George Papadakis and Theodora Varvarigou. 2011. Sentiment analysis of social media content using N-Gram graphs. Paper presented at the Proceedings of the 3rd ACM SIGMM international workshop on Social media, Scottsdale, Arizona, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huifeng Tang</author>
<author>Songbo Tan</author>
<author>Xueqi Cheng</author>
</authors>
<title>A survey on sentiment detection of reviews.</title>
<date>2009</date>
<journal>Expert Systems with Applications,</journal>
<volume>36</volume>
<issue>7</issue>
<pages>10760--10773</pages>
<contexts>
<context position="6879" citStr="Tang et al. (2009)" startWordPosition="1090" endWordPosition="1093">ced by exciting). 3. English contraction of ‘not’ was converted to full form (e.g. don’t -&gt; do not). 4. Repeated letters have been reduced and replaced by 2 of the same character (e.g. happpppppy will be replaced by happy, coollllll will be replaced by cooll). 3.2 Classifier Our system used the NLTK Naïve Bayes classifier module. This is a classification based on Bayes’s rule and also known as the state-of-art of the Bayes rules (Cufoglu et al., 2008). The Naïve Bayes model follows the assumption that attributes within the same case are independent given the class label (Hope and Korb, 2004). Tang et al. (2009) considered that Naïve Bayes assigns a context (represented by a vector ) to the class that maximizes by applying Bayes’s rule, as in (1). ( |) (1) where is a randomly selected context . The representation of vector is . is the random select context that is assigned to class . To classify the term , features in were assumed as from as in (2). 376 ∏ ( |) (2) There are many different approaches to language analysis using syntax, semantics, and semantic resources such as WordNet. That may be exploited using the NLTK (Bird et al. 2009). However, for simplicity we opted here for the n-gram approach</context>
</contexts>
<marker>Tang, Tan, Cheng, 2009</marker>
<rawString>Huifeng Tang, Songbo Tan and Xueqi Cheng. 2009. A survey on sentiment detection of reviews. Expert Systems with Applications, 36(7), 10760-10773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Read</author>
</authors>
<title>Using emoticons to reduce dependency in machine learning techniques for sentiment classification.</title>
<date>2005</date>
<booktitle>Paper presented at the Proceedings of the ACL Student Research Workshop,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="4058" citStr="Read (2005)" startWordPosition="623" endWordPosition="624">13 Association for Computational Linguistics download over the Internet. Tweets are extremely interesting to marketing since their rapid public interaction can either indicate customer success or presage public relations disasters far more quickly than web pages or traditional media. Consequently, the content of tweets and identifying their sentiment polarity as positive or negative is a current active research topic. Emoticons are features of both SMS texts, and tweets. Emoticons such as :) to represent a smile, allow emotions to augment the limited text in SMS messages using few characters. Read (2005) used emoticons from a training set that was downloaded from Usenet newsgroups as annotations (positive and negative). Using the machine learning techniques of Naïve Bayes and Support Vector Machines Read (2005) achieved up to 70 % accuracy in determining text polarity from the emoticons used. Go et al. (2009) used distant supervision to classify sentiment of Twitter, as similar as in (Read, 2005). Emoticons have been used as noisy labels in training data to perform distant supervised learning (positive and negative). Three classifiers were used: Naïve Bayes, Maximum Entropy and Support Vector</context>
</contexts>
<marker>Read, 2005</marker>
<rawString>Jonathon. Read. 2005. Using emoticons to reduce dependency in machine learning techniques for sentiment classification. Paper presented at the Proceedings of the ACL Student Research Workshop, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>Paper presented at the Proceedings of the 12th international conference on World Wide Web,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="8095" citStr="Dave et al. (2003)" startWordPosition="1296" endWordPosition="1299">ch where texts are decomposed into term sequences. A set of single sequences is a unigram. The set of two word sequences (with overlapping) are bigrams, whilst the set of overlapping three term sequences are trigrams. The relative advantage of the bi-and trigram approaches are that coordinates terms effectively disambiguate senses and focus content retrieval and recognition. N-grams have been used many times in contents classification. For example, Pang et al. (2002) used unigram and bigram to classify movie reviews. The results showed that unigram gave better results than bigram. Conversely, Dave et al. (2003) reported gaining better results from trigrams rather than bigram in classifying product reviews. Consequently, we chose to evaluate unigrams, bigrams and trigrams to see which will give the best results Figure 1: Comparison of Twitter messages from two approaches Figure 2: Comparison of SMS messages from two approaches 90 85 80 75 70 65 60 55 50 Unigram Bigram Trigram Pos 1 84.46 82.09 80.8 Neg 1 71.08 59.53 52.91 Pos 2 84.62 83.31 83.25 Neg 2 71.70 65.00 64.34 F-score (%) Pos 1 Neg 1 Pos 2 Neg 2 90 50 Unigram Bigram Trigram Pos 1 76.23 73.89 72.02 Neg 1 82.61 76.04 71.19 Pos 2 77.81 75.69 75</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence and David M. Pennock. 2003. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. Paper presented at the Proceedings of the 12th international conference on World Wide Web, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucas R Hope</author>
<author>Kevin B Korb</author>
</authors>
<title>A bayesian metric for evaluating machine learning algorithms.</title>
<date>2004</date>
<booktitle>Paper presented at the Proceedings of the 17th Australian joint conference on Advances in Artificial Intelligence,</booktitle>
<location>Cairns, Australia.</location>
<contexts>
<context position="6859" citStr="Hope and Korb, 2004" startWordPosition="1086" endWordPosition="1089">, # exciting was replaced by exciting). 3. English contraction of ‘not’ was converted to full form (e.g. don’t -&gt; do not). 4. Repeated letters have been reduced and replaced by 2 of the same character (e.g. happpppppy will be replaced by happy, coollllll will be replaced by cooll). 3.2 Classifier Our system used the NLTK Naïve Bayes classifier module. This is a classification based on Bayes’s rule and also known as the state-of-art of the Bayes rules (Cufoglu et al., 2008). The Naïve Bayes model follows the assumption that attributes within the same case are independent given the class label (Hope and Korb, 2004). Tang et al. (2009) considered that Naïve Bayes assigns a context (represented by a vector ) to the class that maximizes by applying Bayes’s rule, as in (1). ( |) (1) where is a randomly selected context . The representation of vector is . is the random select context that is assigned to class . To classify the term , features in were assumed as from as in (2). 376 ∏ ( |) (2) There are many different approaches to language analysis using syntax, semantics, and semantic resources such as WordNet. That may be exploited using the NLTK (Bird et al. 2009). However, for simplicity we opted here for</context>
</contexts>
<marker>Hope, Korb, 2004</marker>
<rawString>Lucas R. Hope and Kevin B. Korb. 2004. A bayesian metric for evaluating machine learning algorithms. Paper presented at the Proceedings of the 17th Australian joint conference on Advances in Artificial Intelligence, Cairns, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mostafa Al Shaikh</author>
<author>Helmut Prendinger</author>
<author>Ishizuka Mitsuru</author>
</authors>
<title>Assessing Sentiment of Text by Semantic Dependency and Contextual Valence Analysis.</title>
<date>2007</date>
<booktitle>Paper presented at the Proceedings of the 2nd international conference on Affective Computing and Intelligent Interaction,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="1336" citStr="Shaikh et al., 2007" startWordPosition="200" endWordPosition="203">constrained’, which used only the provided training and development data without additional sentiment annotated resources. Our approach considered unigram, bigram and trigram using Naïve Bayes training model with the objective of establishing a simpleapproach baseline. Our system achieved Fscore 81.23% and F-score 78.16% in the results for SMS messages and Tweets respectively. 1 Introduction Natural language processing (NLP) is a research area comprising various tasks; one of which is sentiment analysis. The main goal of sentiment analysis is to identify the polarity of natural language text (Shaikh et al., 2007). Sentiment analysis can be referred to as opinion mining, as study peoples’ opinions, appraisals and emotions towards entities and events and their attributes (Pang and Lee, 2008). Sentiment analysis has become a popular research area in NLP with the purpose of identifying opinions or attitudes in terms of polarity. This paper presents TJP, a system submitted to SemEval 2013 for Task 2 part A: Contextual Polar375 ity Disambiguation (Wilson et al., 2013). TJP was focused on the ‘constrained’ task, which used only training and development data provided. This avoided both resource implications a</context>
</contexts>
<marker>Shaikh, Prendinger, Mitsuru, 2007</marker>
<rawString>Mostafa Al Shaikh, Helmut Prendinger and Ishizuka Mitsuru. 2007. Assessing Sentiment of Text by Semantic Dependency and Contextual Valence Analysis. Paper presented at the Proceedings of the 2nd international conference on Affective Computing and Intelligent Interaction, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pang Bo</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis.</title>
<date>2008</date>
<journal>Found. Trends Inf. Retr.,</journal>
<volume>2</volume>
<issue>1</issue>
<pages>1--135</pages>
<marker>Bo, Lee, 2008</marker>
<rawString>Pang Bo and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Found. Trends Inf. Retr., 2(1-2), 1-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural language processing with Python: O&apos;Reilly.</title>
<date>2009</date>
<contexts>
<context position="2261" citStr="Bird et al., 2009" startWordPosition="345" endWordPosition="348">f polarity. This paper presents TJP, a system submitted to SemEval 2013 for Task 2 part A: Contextual Polar375 ity Disambiguation (Wilson et al., 2013). TJP was focused on the ‘constrained’ task, which used only training and development data provided. This avoided both resource implications and potential advantages implied by the use of additional data containing sentiment annotations. The objective was to explore the relative success of a simple approach that could be implemented easily with open-source software. The TJP system was implemented using the Python Natural Language Toolkit (NLTK, Bird et al., 2009). We considered several basic approaches. These used a preprocessing phase to expand contractions, eliminate stopwords, and identify emoticons. The next phase used supervised machine learning and n-gram features. Although we had two approaches that both used n-gram features, we were limited to submitting just one result. Consequently, we chose to submit a unigram based approach followed by naive Bayes since this performed better on the data. The remainder of this paper is structured as follows: section 2 provides some discussion on the related work. The methodology of corpus collection and dat</context>
<context position="7416" citStr="Bird et al. 2009" startWordPosition="1190" endWordPosition="1193"> are independent given the class label (Hope and Korb, 2004). Tang et al. (2009) considered that Naïve Bayes assigns a context (represented by a vector ) to the class that maximizes by applying Bayes’s rule, as in (1). ( |) (1) where is a randomly selected context . The representation of vector is . is the random select context that is assigned to class . To classify the term , features in were assumed as from as in (2). 376 ∏ ( |) (2) There are many different approaches to language analysis using syntax, semantics, and semantic resources such as WordNet. That may be exploited using the NLTK (Bird et al. 2009). However, for simplicity we opted here for the n-gram approach where texts are decomposed into term sequences. A set of single sequences is a unigram. The set of two word sequences (with overlapping) are bigrams, whilst the set of overlapping three term sequences are trigrams. The relative advantage of the bi-and trigram approaches are that coordinates terms effectively disambiguate senses and focus content retrieval and recognition. N-grams have been used many times in contents classification. For example, Pang et al. (2002) used unigram and bigram to classify movie reviews. The results show</context>
<context position="10989" citStr="Bird et al 2009" startWordPosition="1816" endWordPosition="1819">iss some of my classes. Unigram Bigram Trigram gonna gonna miss gonna miss some miss miss some miss some of some some of some of my of of my of my classes my my classes classes Table 2: Example of context from first approach Table 3: Example of context from second approach. Note ‘some’ and ‘of’ are listed in NLTK stopwords. Figure 3: Results of unigram of Twitter and SMS in the second approach 5 Conclusion and Future Work A system, TJP, has been described that participated in SemEval 2013 Task 2 part A: Contextual Polarity Disambiguation (Wilson et al., 2013). The system used the Python NLTK (Bird et al 2009) Naive Bayes classifier trained on Twitter data. Furthermore, emoticons were collected and labeled as positive and negative in order to classify contexts with emoticons. After analyzing the Twitter message and SMS messages, we were able to obtain an average F-score of 78.16% and 81.23% respectively during the SemEval 2013 task. The reason that, our system achieved better scores with SMS message then Twitter message might be due to our use of Twitter messages as training data. However this is still to be verified experimentally. The experimental performance on the tasks demonstrates the advanta</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein and Edward Loper. 2009. Natural language processing with Python: O&apos;Reilly.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
</authors>
<date>2013</date>
<booktitle>SemEval-2013 Task 2: Sentiment Analysis in Twitter Proceedings of the 7th International Workshop on Semantic Evaluation: Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1794" citStr="Wilson et al., 2013" startWordPosition="274" endWordPosition="277"> various tasks; one of which is sentiment analysis. The main goal of sentiment analysis is to identify the polarity of natural language text (Shaikh et al., 2007). Sentiment analysis can be referred to as opinion mining, as study peoples’ opinions, appraisals and emotions towards entities and events and their attributes (Pang and Lee, 2008). Sentiment analysis has become a popular research area in NLP with the purpose of identifying opinions or attitudes in terms of polarity. This paper presents TJP, a system submitted to SemEval 2013 for Task 2 part A: Contextual Polar375 ity Disambiguation (Wilson et al., 2013). TJP was focused on the ‘constrained’ task, which used only training and development data provided. This avoided both resource implications and potential advantages implied by the use of additional data containing sentiment annotations. The objective was to explore the relative success of a simple approach that could be implemented easily with open-source software. The TJP system was implemented using the Python Natural Language Toolkit (NLTK, Bird et al., 2009). We considered several basic approaches. These used a preprocessing phase to expand contractions, eliminate stopwords, and identify </context>
<context position="10938" citStr="Wilson et al., 2013" startWordPosition="1805" endWordPosition="1808">orks better for SMS messaging than for Twitter. gonna miss some of my classes. Unigram Bigram Trigram gonna gonna miss gonna miss some miss miss some miss some of some some of some of my of of my of my classes my my classes classes Table 2: Example of context from first approach Table 3: Example of context from second approach. Note ‘some’ and ‘of’ are listed in NLTK stopwords. Figure 3: Results of unigram of Twitter and SMS in the second approach 5 Conclusion and Future Work A system, TJP, has been described that participated in SemEval 2013 Task 2 part A: Contextual Polarity Disambiguation (Wilson et al., 2013). The system used the Python NLTK (Bird et al 2009) Naive Bayes classifier trained on Twitter data. Furthermore, emoticons were collected and labeled as positive and negative in order to classify contexts with emoticons. After analyzing the Twitter message and SMS messages, we were able to obtain an average F-score of 78.16% and 81.23% respectively during the SemEval 2013 task. The reason that, our system achieved better scores with SMS message then Twitter message might be due to our use of Twitter messages as training data. However this is still to be verified experimentally. The experimenta</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, Ritter, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov and Alan Ritter. 2013. SemEval-2013 Task 2: Sentiment Analysis in Twitter Proceedings of the 7th International Workshop on Semantic Evaluation: Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>