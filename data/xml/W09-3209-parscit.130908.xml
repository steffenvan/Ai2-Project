<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.9934665">
Ranking and Semi-supervised Classification
on Large Scale Graphs Using Map-Reduce
</title>
<author confidence="0.996569">
Delip Rao David Yarowsky
</author>
<affiliation confidence="0.914339">
Dept. of Computer Science Dept. of Computer Science
Johns Hopkins University Johns Hopkins University
</affiliation>
<email confidence="0.99915">
delip@cs.jhu.edu yarowsky@cs.jhu.edu
</email>
<sectionHeader confidence="0.997391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999789869565218">
Label Propagation, a standard algorithm
for semi-supervised classification, suffers
from scalability issues involving memory
and computation when used with large-
scale graphs from real-world datasets. In
this paper we approach Label Propagation
as solution to a system of linear equations
which can be implemented as a scalable
parallel algorithm using the map-reduce
framework. In addition to semi-supervised
classification, this approach to Label Prop-
agation allows us to adapt the algorithm to
make it usable for ranking on graphs and
derive the theoretical connection between
Label Propagation and PageRank. We pro-
vide empirical evidence to that effect using
two natural language tasks – lexical relat-
edness and polarity induction. The version
of the Label Propagation algorithm pre-
sented here scales linearly in the size of
the data with a constant main memory re-
quirement, in contrast to the quadratic cost
of both in traditional approaches.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999822636363636">
Natural language data often lend themselves to a
graph-based representation. Words can be linked
by explicit relations as in WordNet (Fellbaum,
1989), and documents can be linked to one an-
other via hyperlinks. Even in the absence of such a
straightforward representation it is possible to de-
rive meaningful graphs such as the nearest neigh-
bor graphs, as done in certain manifold learning
methods, e.g. Roweis and Saul (2000); Belkin and
Niyogi (2001). Typically, these graphs share the
following properties:
</bodyText>
<listItem confidence="0.971648">
• They are edge-weighted.
• The edge weight encodes some notion of re-
latedness between the vertices.
• The relation represented by edges is at least
weakly transitive. Examples of such rela-
tions include, “is similar to”, “is more gen-
eral than”, and so on. It is important that the
relations selected are transitive for the graph-
based learning methods using random walks.
</listItem>
<bodyText confidence="0.999744814814815">
Such graphs present several possibilities for
solving natural language problems involving rank-
ing, classification, and clustering. Graphs have
been successfully employed in machine learning
in a variety of supervised, unsupervised, and semi-
supervised tasks. Graph based algorithms perform
better than their counterparts as they capture the
latent structure of the problem. Further, their ele-
gant mathematical framework allows simpler anal-
ysis to gain a deeper understanding of the prob-
lem. Despite these advantages, implementations
of most graph-based learning algorithms do not
scale well on large datasets from real world prob-
lems in natural language processing. With large
amounts of unlabeled data available, the graphs
can easily grow to millions of nodes and most ex-
isting non-parallel methods either fail to work due
to resource constraints or find the computation in-
tractable.
In this paper we describe a scalable implemen-
tation of Label Propagation, a popular random
walk based semi-supervised classification method.
We show that our framework can also be used for
ranking on graphs. Our parallel formulation shows
a theoretical connection between Label Propaga-
tion and PageRank. We also confirm this em-
pirically using the lexical relatedness task. The
</bodyText>
<page confidence="0.98449">
58
</page>
<note confidence="0.999718">
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 58–65,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.997251695652174">
proposed Parallel Label Propagation scales up lin-
early in the data and the number of processing ele-
ments available. Also, the main memory required
by the method does not grow with the size of the
graph.
The outline of this paper is as follows: Section 2
introduces the manifold assumption and explains
why graph-based learning algorithms perform bet-
ter than their counterparts. Section 3 motivates
the random walk based approach for learning on
graphs. Section 4 introduces the Label Propaga-
tion method by Zhu et al. (2003). In Section 5 we
describe a method to scale up Label Propagation
using Map-Reduce. Section 6 shows how Label
Propagation could be used for ranking on graphs
and derives the relation between Label Propaga-
tion and PageRank. Parallel Label Propagation is
evaluated on ranking and semi-supervised classifi-
cation problems in natural language processing in
Section 8. We study scalability of this algorithm in
Section 9 and describe related work in the area of
parallel algorithms and machine learning in Sec-
tion 10.
</bodyText>
<sectionHeader confidence="0.973109" genericHeader="introduction">
2 Manifold Assumption
</sectionHeader>
<bodyText confidence="0.999989529411765">
The training data D can be considered as a collec-
tion of tuples D = (X, Y) where Y are the labels
and X are the features, and the learned model M
is a surrogate for an underlying physical process
which generates the data D. The data D can be
considered as a sampling from a smooth surface or
a manifold which represents the physical process.
This is known as the manifold assumption (Belkin
et al., 2005). Observe that even in the simple case
of Euclidean data (X = {x : x E Rd}) as shown
in Figure 1, points that lie close in the Euclidean
space might actually be far off on the manifold.
A graph, as shown in Figure 1c, approximates the
structure of the manifold which was lost in vector-
ized algorithms operating in the Euclidean space.
This explains the better performance of graph al-
gorithms for learning as seen in the literature.
</bodyText>
<sectionHeader confidence="0.99101" genericHeader="method">
3 Distance measures on graphs
</sectionHeader>
<bodyText confidence="0.999333">
Most learning tasks on graphs require some notion
of distance or similarity to be defined between the
vertices of a graph. The most obvious measure of
distance in a graph is the shortest path between the
vertices, which is defined as the minimum number
of intervening edges between two vertices. This is
also known as the geodesic distance. To convert
this distance measure to a similarity measure, we
take the reciprocal of the shortest-path length. We
refer to this as the geodesic similarity.
</bodyText>
<figureCaption confidence="0.9409325">
Figure 2: Shortest path distances on graphs ignore
the connectivity structure of the graph.
</figureCaption>
<bodyText confidence="0.999699117647059">
While shortest-path distances are useful in
many applications, it fails to capture the following
observation. Consider the subgraph of WordNet
shown in Figure 2. The term moon is con-
nected to the terms religious leader
and satellite.1 Observe that both
religious leader and satellite are
at the same shortest path distance from moon.
However, the connectivity structure of the graph
would suggest satellite to be more similar
than religious leader as there are multiple
senses, and hence multiple paths, connecting
satellite and moon.
Thus it is desirable to have a measure that cap-
tures not only path lengths but also the connectiv-
ity structure of the graph. This notion is elegantly
captured using random walks on graphs.
</bodyText>
<sectionHeader confidence="0.80188" genericHeader="method">
4 Label Propagation: Random Walk on
Manifold Graphs
</sectionHeader>
<bodyText confidence="0.996112153846154">
An efficient way to combine labeled and unla-
beled data involves construction of a graph from
the data and performing a Markov random walk
on the graph. This has been utilized in Szummer
and Jaakkola (2001), Zhu et. al. (2003), and Azran
(2007). The general idea of Label Propagation in-
volves defining a probability distribution F over
the labels for each node in the graph. For labeled
nodes, this distribution reflects the true labels and
the aim is to recover this distribution for the unla-
beled nodes in the graph.
Consider a graph G(V, E, W) with vertices V ,
edges E, and an n x n edge weight matrix W =
</bodyText>
<footnote confidence="0.9946345">
1The religious leader sense of moon is due to Sun
Myung Moon, a US religious leader.
</footnote>
<page confidence="0.996318">
59
</page>
<figure confidence="0.999121">
(a) (b) (c)
</figure>
<figureCaption confidence="0.990989">
Figure 1: Manifold Assumption [Belkin et al., 2005]: Data lies on a manifold (a) and points along the
manifold are locally similar (b).
</figureCaption>
<bodyText confidence="0.99642">
[wij], where n = |V |. The Label Propagation al-
gorithm minimizes a quadratic energy function
</bodyText>
<equation confidence="0.9992275">
£ =2 wij(Fi − Fj)2 (1)
(i,j)EE
� �wijFi = wijFj (3)
(i, j) E E (i,j)EE
� Fi(c) = 1 bi, j E V.
c E classes(i)
</equation>
<bodyText confidence="0.999869666666667">
The general recipe for using random walks
for classification involves constructing the graph
Laplacian and using the pseudo-inverse of the
Laplacian as a kernel (Xiao and Gutman, 2003).
Given a weighted undirected graph, G(V, E, W),
the Laplacian is defined as follows:
</bodyText>
<equation confidence="0.8970906">
di if i = j
−wij if i is adjacent to j (2)
0 otherwise
�where di = wij.
j
</equation>
<bodyText confidence="0.999898375">
It has been shown that the pseudo-inverse of the
Laplacian L is a kernel (Xiao and Gutman, 2003),
i.e., it satisfies the Mercer conditions. However,
there is a practical limitation to this approach. For
very large graphs, even if the graph Laplacians are
sparse, their pseudo-inverses are dense matrices
requiring O(n2) space. This can be prohibitive in
most computing environments.
</bodyText>
<sectionHeader confidence="0.959156" genericHeader="method">
5 Parallel Label Propagation
</sectionHeader>
<bodyText confidence="0.999735125">
In developing a parallel algorithm for Label
Propagation we instead take an alternate approach
and completely avoid the use of inverse Lapla-
cians for the reasons stated above. Our approach
follows from the observation made from Zhu et
al.’s (2003) Label Propagation algorithm:
Observation: In a weighted graph G(V, E, W)
with n = |V  |vertices, minimization of Equation
(1) is equivalent to solving the following system
of linear equations.
We use this observation to derive an iterative
Label Propagation algorithm that we will later par-
allelize. Consider a weighted undirected graph
G(V, E, W) with the vertex set partitioned into VL
and VU (i.e., V = VLUVU) such that all vertices in
VL are labeled and all vertices in VU are unlabeled.
Typically only a small set of vertices are labeled,
i.e., |VU |» |VL|. Let Fu denote the probability
distribution over the labels associated with vertex
u E V . For v E VL, Fv is known, and we also
add a “dummy vertex” v� to the graph G such that
wvv, = 1 and Fv, = Fv. This is equivalent to the
“clamping” done in (Zhu et al., 2003). Let VD be
the set of dummy vertices.
</bodyText>
<equation confidence="0.687118285714286">
Algorithm 1: Iterative Label Propogation
repeat
forall v E (V U VD) do
�Fv = wuvFv
(v,u)∈E
Row normalize Fv.
end
</equation>
<bodyText confidence="0.987610833333333">
until convergence or maxIterations
Observe that every iteration of Algorithm 1 per-
forms certain operations on each vertex of the
graph. Further, these operations only rely on
local information (from neighboring vertices of
the graph). This leads to the parallel algorithm
(Algorithm 2) implemented using the map-reduce
model. Map-Reduce (Dean and Ghemawat, 2004)
is a paradigm for implementing distributed algo-
rithms with two user supplied functions “map” and
“reduce”. The map function processes the input
key/value pairs with the key being a unique iden-
</bodyText>
<equation confidence="0.55659975">
⎧
⎨
⎩
Lij =
</equation>
<page confidence="0.942463">
60
</page>
<bodyText confidence="0.9933676">
tifier for a node in the graph and the value corre-
sponds to the data associated with the node. The
mappers run on different machines operating on
different parts of the data and the reduce function
aggregates results from various mappers.
</bodyText>
<equation confidence="0.895630571428571">
Algorithm 2: Parallel Label Propagation
map(key, value):
begin
d = 0
neighbors = getNeighbors(value);
foreach n E neighbors do
w = n.weight();
d += w * n.getDistribution();
end
normalize(d);
value.setDistribution(d);
Emit(key, value);
end
reduce(key, values): Identity Reducer
</equation>
<bodyText confidence="0.9392042">
Algorithm 2 represents one iteration of Algo-
rithm 1. This is run repeatedly until convergence
or for a specified number of iterations. The al-
gorithm is considered to have converged if the la-
bel distributions associated with each node do not
</bodyText>
<equation confidence="0.936967">
~~~~2 &lt; �
</equation>
<bodyText confidence="0.959127">
change significantly, i.e., ����F(i+1) − F(i)
for a fixed E &gt; 0.
</bodyText>
<sectionHeader confidence="0.993793" genericHeader="method">
6 Label Propagation for Ranking
</sectionHeader>
<bodyText confidence="0.999929578947368">
Graph ranking is applicable in a variety of prob-
lems in natural language processing and informa-
tion retrieval. Given a graph, we would like to
rank the vertices of a graph with respect to a node,
called the pivot node or query node. Label Prop-
agation and its variants (Szummer and Jaakkola,
2001; Zhu et al., 2003; Azran, 2007) have been
traditionally used for semi-supervised classifica-
tion. Our view of Label Propagation (via Algo-
rithm 1) suggests a way to perform ranking on
graphs.
Ranking on graphs can be performed in the Par-
allel Label Propagation framework by associating
a single point distribution with all vertices. The
pivot node has a mass fixed to the value 1 at all it-
erations. In addition, the normalization step in Al-
gorithm 2 is omitted. At the end of the algorithm,
the mass associated with each node determines its
rank.
</bodyText>
<subsectionHeader confidence="0.997347">
6.1 Connection to PageRank
</subsectionHeader>
<bodyText confidence="0.999442666666667">
It is interesting to note that Algorithm 1 brings
out a connection between Label Propagation and
PageRank (Page et al., 1998). PageRank is a ran-
dom walk model that allows the random walk to
“jump” to its initial state with a nonzero proba-
bility (α). Given the probability transition matrix
P = [Prs], where Prs is the probability of jumping
from node r to node s, the weight update for any
vertex (say v) is derived as follows
</bodyText>
<equation confidence="0.997967">
vt+1 = αvtP + (1 − α)v0 (4)
</equation>
<bodyText confidence="0.973053857142857">
Notice that when α = 0.5, PageRank is reduced
to Algorithm 1, by a constant factor, with the ad-
ditional (1 − α)v0 term corresponding to the con-
tribution from the “dummy vertices” VD in Algo-
rithm 1.
We can in fact show that Algorithm 1 reduces to
PageRank as follows:
</bodyText>
<equation confidence="0.999718666666667">
vt+1 = αvtP + (1 − α)v0
(1 − α)
a vtP +
α
= vtP + Qv0
whereQ = (1−α)
</equation>
<bodyText confidence="0.959442833333333">
α . Thus by setting the edge
weights to the dummy vertices to Q, i.e., b(z, z&apos;) E
E and z&apos; E VD, wzz, = Q, Algorithm 1, and hence
Algorithm 2, reduces to PageRank. Observe that
when Q = 1 we get the original Algorithm 1.
We’ll refer to this as the “Q-correction”.
</bodyText>
<sectionHeader confidence="0.992317" genericHeader="method">
7 Graph Representation
</sectionHeader>
<bodyText confidence="0.998640384615385">
Since Parallel Label Propagation algorithm uses
only local information, we use the adjacency list
representation (which is same as the sparse adja-
cency matrix representation) for the graph. This
representation is important for the algorithm to
have a constant main memory requirement as no
further lookups need to be done while comput-
ing the label distribution at a node. The interface
definition for the graph is listed in Appendix A.
Often graph data is available in an edge format,
as &lt;source, destination, weight&gt; triples. We use
another map-reduce step (Algorithm 3) to convert
that data to the form shown in Appendix A.
</bodyText>
<sectionHeader confidence="0.99841" genericHeader="method">
8 Evaluation
</sectionHeader>
<bodyText confidence="0.9999845">
We evaluate the Parallel Label Propagation algo-
rithm for both ranking and semi-supervised clas-
sification. In ranking our goal is to rank the ver-
tices of a graph with respect to a given node called
the pivot/query node. In semi-supervised classi-
fication, we are given a graph with some vertices
</bodyText>
<equation confidence="0.933274">
v0 (5)
</equation>
<page confidence="0.962331">
61
</page>
<figure confidence="0.87003475">
Algorithm 3: Graph Construction
map(key, value):
begin
edgeEntry = value;
Node n(edgeEntry);
Emit(n.id, n);
end
bel Propagation algorithm. The results are seen in
</figure>
<tableCaption confidence="0.824278">
Table 2.
</tableCaption>
<table confidence="0.826733111111111">
Method Spearman
Correlation
PageRank (α = 0.1) 0.39
Parallel Label 0.39
Propagation (0 = 9)
reduce(key, values):
begin
Emit(key, serialize(values));
end
</table>
<tableCaption confidence="0.547013">
Table 2: Lexical-relatedness results: Comparision
of PageRank and 0-corrected Parallel Label Prop-
agation
</tableCaption>
<bodyText confidence="0.690109">
labeled and would like to predict labels for the re-
maining vertices.
</bodyText>
<subsectionHeader confidence="0.981115">
8.1 Ranking
</subsectionHeader>
<bodyText confidence="0.9997319">
To evaluate ranking, we consider the problem
of deriving lexical relatedness between terms.
This has been a topic of interest with applica-
tions in word sense disambiguation (Patwardhan
et al., 2005), paraphrasing (Kauchak and Barzilay,
2006), question answering (Prager et al., 2001),
and machine translation (Blatz et al., 2004), to
name a few. Following the tradition in pre-
vious literature we evaluate on the Miller and
Charles (1991) dataset. We compare our rankings
with the human judegments using the Spearman
rank correlation coefficient. The graph for this
task is derived from WordNet, an electronic lex-
ical database. We compare Algorithm 2 with re-
sults from using geodesic similarity as a baseline.
As observed in Table 1, the parallel implemen-
tation in Algorithm 2 performs better than rank-
ing using geodesic similarity derived from short-
est path lengths. This reinforces the motivation of
using random walks as described in Section 3.
</bodyText>
<table confidence="0.998788">
Method Spearman
Correlation
Geodesic (baseline) 0.28
Parallel Label 0.36
Propagation
</table>
<tableCaption confidence="0.942023">
Table 1: Lexical-relatedness results: Comparison
with geodesic similarity.
</tableCaption>
<bodyText confidence="0.9624175">
We now empirically verify the equivalence of
the 0-corrected Parallel Label Propagation and
PageRank established in Equation 4. To do this,
we use α = 0.1 in the PageRank algorithm and
set 0 = (1−α)
α = 9 in the 0-corrected Parallel La-
</bodyText>
<subsectionHeader confidence="0.965577">
8.2 Semi-supervised Classification
</subsectionHeader>
<bodyText confidence="0.999332333333333">
Label Propagation was originally developed as a
semi-supervised classification method. Hence Al-
gorithm 2 can be applied without modification.
After execution of Algorithm 2, every node v in
the graph will have a distribution over the labels
Fv. The predicted label is set to arg max Fv(c).
</bodyText>
<equation confidence="0.545765">
c∈classes(v)
</equation>
<bodyText confidence="0.999897310344828">
To evaluate semi-supervised classification we
consider the problem of learning sentiment polar-
ity lexicons. We consider the polarity of a word to
be either positive or negative. For example, words
such as good, beautiful, and wonderful are consid-
ered as positive sentiment words; whereas words
such as bad, ugly, and sad are considered negative
sentiment words. Learning such lexicons has ap-
plications in sentiment detection and opinion min-
ing. We treat sentiment polarity detection as a
semi-supervised Label Propagation problem in a
graph. In the graph, each node represents a word
whose polarity is to be determined. Each weighted
edge encodes a relation that exists between two
words. Each node (word) can have two labels:
positive or negative. It is important to note that La-
bel Propagation, and hence Algorithms 1&amp;2, sup-
port multi-class classification but for the purpose
of this task we have two labels. The graph for the
task is derived from WordNet. We use the Gen-
eral Inquirer (GI)2 data for evaluation. General
Inquirer is lexicon of English words hand-labeled
with categorical information along several dimen-
sions. One such dimension is called valence, with
1915 words labeled “Positiv” (sic) and 2291 words
labeled “Negativ” for words with positive and neg-
ative sentiments respectively. We used a random
20% of the data as our seed labels and the rest
as our unlabeled data. We compare our results
</bodyText>
<footnote confidence="0.977502">
2http://www.wjh.harvard.edu/∼inquirer/
</footnote>
<page confidence="0.995421">
62
</page>
<figure confidence="0.999528">
(a) (b)
</figure>
<figureCaption confidence="0.999954">
Figure 3: Scalability results: (a) Scaleup (b) Speedup
</figureCaption>
<bodyText confidence="0.998143111111111">
(F-scores) with another scalable previous work by
Kim and Hovy (Kim and Hovy, 2006) in Table 2
for the same seed set. Their approach starts with a
few seeds of positive and negative terms and boot-
straps the list by considering all synonyms of pos-
itive word as positive and antonyms of positive
words as negative. This procedure is repeated mu-
tatis mutandis for negative words in the seed list
until there are no more words to add.
</bodyText>
<table confidence="0.9991945">
Method Nouns Verbs Adjectives
Kim &amp; Hovy 34.80 53.36 47.28
Parallel Label 58.53 83.40 72.95
Propagation
</table>
<tableCaption confidence="0.999676">
Table 3: Polarity induction results (F-scores)
</tableCaption>
<bodyText confidence="0.99904475">
The performance gains seen in Table 3 should
be attributed to the Label Propagation in general
as the previous work (Kim and Hovy, 2006) did
not utilize a graph based method.
</bodyText>
<sectionHeader confidence="0.802465" genericHeader="method">
9 Scalability experiments
</sectionHeader>
<bodyText confidence="0.999946954545454">
We present some experiments to study the scala-
bility of the algorithm presented. All our experi-
ments were performed on an experimental cluster
of four machines to test the concept. The machines
were Intel Xeon 2.4 GHz with 1Gb main memory.
All performance measures were averaged over 20
runs.
Figure 3a shows scaleup of the algorithm which
measures how well the algorithm handles increas-
ing data sizes. For this experiment, we used all
nodes in the cluster. As observed, the increase in
time is at most linear in the size of the data. Fig-
ure 3b shows speedup of the algorithm. Speedup
shows how well the algorithm performs with in-
crease in resources for a fixed input size. In
this case, we progressively increase the number of
nodes in the cluster. Again, the speedup achieved
is linear in the number of processing elements
(CPUs). An appealing factor of Algorithm 2 is that
the memory used by each mapper process is fixed
regardless of the size of the graph. This makes the
algorithm feasible for use with large-scale graphs.
</bodyText>
<sectionHeader confidence="0.999487" genericHeader="method">
10 Related Work
</sectionHeader>
<bodyText confidence="0.999831148148148">
Historically, there is an abundance of work in par-
allel and distributed algorithms for graphs. See
Grama et al. (2003) for survey chapters on the
topic. In addition, the emergence of open-source
implementations of Google’s map-reduce (Dean
and Ghemawat, 2004) such as Hadoop3 has made
parallel implementations more accessible.
Recent literature shows tremendous interest in
application of distributed computing to scale up
machine learning algorithms. Chu et al. (2006)
describe a family of learning algorithms that fit
the Statistical Query Model (Kearns, 1993). These
algorithms can be written in a special summation
form that is amenable to parallel speed-up. Exam-
ples of such algorithms include Naive Bayes, Lo-
gistic Regression, backpropagation in Neural Net-
works, Expectation Maximization (EM), Princi-
pal Component Analysis, and Support Vector Ma-
chines to name a few. The summation form can be
easily decomposed so that the mapper can com-
pute the partial sums that are then aggregated by a
reducer. Wolfe et al. (2008) describe an approach
to estimate parameters via the EM algorithm in a
setup aimed to minimize communication latency.
The k-means clustering algorithm has been an
archetype of the map-reduce framework with sev-
eral implementations available on the web. In
</bodyText>
<footnote confidence="0.975955">
3http://hadoop.apache.org/core
</footnote>
<page confidence="0.999204">
63
</page>
<bodyText confidence="0.99917975">
addition, the Netflix Million Dollar Challenge4
generated sufficient interest in large scale cluster-
ing algorithms. (McCallum et al., 2000), describe
algorithmic improvements to the k-means algo-
rithm, called canopy clustering, to enable efficient
parallel clustering of data.
While there is earlier work on scalable map-
reduce implementations of PageRank (E.g., Gle-
ich and Zhukov (2005)) there is no existing liter-
ature on parallel algorithms for graph-based semi-
supervised learning or the relationship between
PageRank and Label Propagation.
</bodyText>
<sectionHeader confidence="0.983721" genericHeader="method">
11 Conclusion
</sectionHeader>
<bodyText confidence="0.9999834">
In this paper, we have described a parallel algo-
rithm for graph ranking and semi-supervised clas-
sification. We derived this by first observing that
the Label Propagation algorithm can be expressed
as a solution to a set of linear equations. This is
easily expressed as an iterative algorithm that can
be cast into the map-reduce framework. This al-
gorithm uses fixed main memory regardless of the
size of the graph. Further, our scalability study re-
veals that the algorithm scales linearly in the size
of the data and the number of processing elements
in the cluster. We also showed how Label Prop-
agation can be used for ranking on graphs and
the conditions under which it reduces to PageR-
ank. We evaluated our implementation on two
learning tasks – ranking and semi-supervised clas-
sification – using examples from natural language
processing including lexical-relatedness and senti-
ment polarity lexicon induction with a substantial
gain in performance.
</bodyText>
<sectionHeader confidence="0.5874" genericHeader="method">
A Appendix A: Interface definition for
Undirected Graphs
</sectionHeader>
<bodyText confidence="0.997125727272727">
In order to guarantee the constant main memory
requirement of Algorithm 2, the graph represen-
tation should encode for each node, the complete
information about it’s neighbors. We represent
our undirected graphs in the Google’s Protocol
Buffer format.5 Protocol Buffers allow a compact,
portable on-disk representation that is easily ex-
tensible. This definition can be compiled into effi-
cient Java/C++ classes.
The interface definition for undirected graphs is
listed below:
</bodyText>
<footnote confidence="0.994034333333333">
4http://www.netflixprize.com
5Implementation available at
http://code.google.com/p/protobuf/
</footnote>
<figure confidence="0.469662785714286">
package graph;
message NodeNeighbor {
required string id = 1;
required double edgeWeight = 2;
repeated double labelDistribution = 3;
}
message UndirectedGraphNode {
required string id = 1;
repeated NodeNeighbor neighbors = 2;
repeated double labelDistribution = 3;
}
message UndirectedGraph {
repeated UndirectedGraphNode nodes = 1;
}
</figure>
<sectionHeader confidence="0.988776" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999461194444444">
Arik Azran. 2007. The rendezvous algorithm: Multi-
class semi-supervised learning with markov random
walks. In Proceedings of the International Confer-
ence on Machine Learning (ICML).
Micheal. Belkin, Partha Niyogi, and Vikas Sindhwani.
2005. On manifold regularization. In Proceedings
ofAISTATS.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceeding of
COLING.
Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu,
Gary R. Bradski, Andrew Y. Ng, and Kunle Oluko-
tun. 2006. Map-reduce for machine learning on
multicore. In Proceedings of Neural Information
Processing Systems.
Jeffrey Dean and Sanjay Ghemawat. 2004. Map-
reduce: Simplified data processing on large clusters.
In Proceedings of the symposium on Operating sys-
tems design and implementation (OSDI).
Christaine Fellbaum, editor. 1989. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
D. Gleich and L. Zhukov. 2005. Scalable comput-
ing for power law graphs: Experience with parallel
pagerank. In Proceedings of SuperComputing.
Ananth Grama, George Karypis, Vipin Kumar, and An-
shul Gupta. 2003. Introduction to Parallel Comput-
ing (2nd Edition). Addison-Wesley, January.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of HLT-NAACL.
Michael Kearns. 1993. Efficient noise-tolerant learn-
ing from statistical queries. In Proceedings of the
Twenty-Fifth Annual ACM Symposium on Theory of
Computing (STOC).
</reference>
<page confidence="0.986511">
64
</page>
<reference confidence="0.998604289473684">
Soo-Min Kim and Eduard H. Hovy. 2006. Identifying
and analyzing judgment opinions. In Proceedings of
HLT-NAACL.
Andrew McCallum, Kamal Nigam, and Lyle H. Un-
gar. 2000. Efficient clustering of high-dimensional
data sets with application to reference matching.
In Knowledge Discovery and Data Mining (KDD),
pages 169–178.
G. Miller and W. Charles. 1991. Contextual correlates
of semantic similarity. In Language and Cognitive
Process.
Larry Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1998. The pagerank citation ranking:
Bringing order to the web. Technical report, Stan-
ford University, Stanford, CA.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2005. Senserelate::targetword - A gen-
eralized framework for word sense disambiguation.
In Proceedings of ACL.
John M. Prager, Jennifer Chu-Carroll, and Krzysztof
Czuba. 2001. Use of wordnet hypernyms for an-
swering what-is questions. In Proceedings of the
Text REtrieval Conference.
M. Szummer and T. Jaakkola. 2001. Clustering and
efficient use of unlabeled examples. In Proceedings
of Neural Information Processing Systems (NIPS).
Jason Wolfe, Aria Haghighi, and Dan Klein. 2008.
Fully distributed EM for very large datasets. In Pro-
ceedings of the International Conference in Machine
Learning.
W. Xiao and I. Gutman. 2003. Resistance distance and
laplacian spectrum. Theoretical Chemistry Associa-
tion, 110:284–289.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using Gaussian
fields and harmonic functions. In Proceedings of
the International Conference on Machine Learning
(ICML).
</reference>
<page confidence="0.999597">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981310">
<title confidence="0.9971525">Ranking and Semi-supervised on Large Scale Graphs Using Map-Reduce</title>
<author confidence="0.999488">Delip Rao David Yarowsky</author>
<affiliation confidence="0.999486">Dept. of Computer Science Dept. of Computer Science Johns Hopkins University Johns Hopkins University</affiliation>
<email confidence="0.999021">delip@cs.jhu.eduyarowsky@cs.jhu.edu</email>
<abstract confidence="0.999527958333333">Label Propagation, a standard algorithm for semi-supervised classification, suffers from scalability issues involving memory and computation when used with largescale graphs from real-world datasets. In this paper we approach Label Propagation as solution to a system of linear equations which can be implemented as a scalable parallel algorithm using the map-reduce framework. In addition to semi-supervised classification, this approach to Label Propagation allows us to adapt the algorithm to make it usable for ranking on graphs and derive the theoretical connection between Label Propagation and PageRank. We provide empirical evidence to that effect using two natural language tasks – lexical relatedness and polarity induction. The version of the Label Propagation algorithm presented here scales linearly in the size of the data with a constant main memory requirement, in contrast to the quadratic cost of both in traditional approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arik Azran</author>
</authors>
<title>The rendezvous algorithm: Multiclass semi-supervised learning with markov random walks.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="7065" citStr="Azran (2007)" startWordPosition="1131" endWordPosition="1132">to be more similar than religious leader as there are multiple senses, and hence multiple paths, connecting satellite and moon. Thus it is desirable to have a measure that captures not only path lengths but also the connectivity structure of the graph. This notion is elegantly captured using random walks on graphs. 4 Label Propagation: Random Walk on Manifold Graphs An efficient way to combine labeled and unlabeled data involves construction of a graph from the data and performing a Markov random walk on the graph. This has been utilized in Szummer and Jaakkola (2001), Zhu et. al. (2003), and Azran (2007). The general idea of Label Propagation involves defining a probability distribution F over the labels for each node in the graph. For labeled nodes, this distribution reflects the true labels and the aim is to recover this distribution for the unlabeled nodes in the graph. Consider a graph G(V, E, W) with vertices V , edges E, and an n x n edge weight matrix W = 1The religious leader sense of moon is due to Sun Myung Moon, a US religious leader. 59 (a) (b) (c) Figure 1: Manifold Assumption [Belkin et al., 2005]: Data lies on a manifold (a) and points along the manifold are locally similar (b)</context>
<context position="11610" citStr="Azran, 2007" startWordPosition="1922" endWordPosition="1923">n repeatedly until convergence or for a specified number of iterations. The algorithm is considered to have converged if the label distributions associated with each node do not ~~~~2 &lt; � change significantly, i.e., ����F(i+1) − F(i) for a fixed E &gt; 0. 6 Label Propagation for Ranking Graph ranking is applicable in a variety of problems in natural language processing and information retrieval. Given a graph, we would like to rank the vertices of a graph with respect to a node, called the pivot node or query node. Label Propagation and its variants (Szummer and Jaakkola, 2001; Zhu et al., 2003; Azran, 2007) have been traditionally used for semi-supervised classification. Our view of Label Propagation (via Algorithm 1) suggests a way to perform ranking on graphs. Ranking on graphs can be performed in the Parallel Label Propagation framework by associating a single point distribution with all vertices. The pivot node has a mass fixed to the value 1 at all iterations. In addition, the normalization step in Algorithm 2 is omitted. At the end of the algorithm, the mass associated with each node determines its rank. 6.1 Connection to PageRank It is interesting to note that Algorithm 1 brings out a con</context>
</contexts>
<marker>Azran, 2007</marker>
<rawString>Arik Azran. 2007. The rendezvous algorithm: Multiclass semi-supervised learning with markov random walks. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Niyogi Belkin</author>
<author>Vikas Sindhwani</author>
</authors>
<title>On manifold regularization.</title>
<date>2005</date>
<booktitle>In Proceedings ofAISTATS.</booktitle>
<marker>Belkin, Sindhwani, 2005</marker>
<rawString>Micheal. Belkin, Partha Niyogi, and Vikas Sindhwani. 2005. On manifold regularization. In Proceedings ofAISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In Proceeding of COLING.</booktitle>
<contexts>
<context position="15012" citStr="Blatz et al., 2004" startWordPosition="2506" endWordPosition="2509">39 Parallel Label 0.39 Propagation (0 = 9) reduce(key, values): begin Emit(key, serialize(values)); end Table 2: Lexical-relatedness results: Comparision of PageRank and 0-corrected Parallel Label Propagation labeled and would like to predict labels for the remaining vertices. 8.1 Ranking To evaluate ranking, we consider the problem of deriving lexical relatedness between terms. This has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004), to name a few. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. We compare our rankings with the human judegments using the Spearman rank correlation coefficient. The graph for this task is derived from WordNet, an electronic lexical database. We compare Algorithm 2 with results from using geodesic similarity as a baseline. As observed in Table 1, the parallel implementation in Algorithm 2 performs better than ranking using geodesic similarity derived from shortest path lengths. This reinforces the motivation of using random walks as descri</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence estimation for machine translation. In Proceeding of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheng T Chu</author>
<author>Sang K Kim</author>
<author>Yi A Lin</author>
<author>Yuanyuan Yu</author>
<author>Gary R Bradski</author>
<author>Andrew Y Ng</author>
<author>Kunle Olukotun</author>
</authors>
<title>Map-reduce for machine learning on multicore.</title>
<date>2006</date>
<booktitle>In Proceedings of Neural Information Processing Systems.</booktitle>
<contexts>
<context position="20175" citStr="Chu et al. (2006)" startWordPosition="3344" endWordPosition="3347">ach mapper process is fixed regardless of the size of the graph. This makes the algorithm feasible for use with large-scale graphs. 10 Related Work Historically, there is an abundance of work in parallel and distributed algorithms for graphs. See Grama et al. (2003) for survey chapters on the topic. In addition, the emergence of open-source implementations of Google’s map-reduce (Dean and Ghemawat, 2004) such as Hadoop3 has made parallel implementations more accessible. Recent literature shows tremendous interest in application of distributed computing to scale up machine learning algorithms. Chu et al. (2006) describe a family of learning algorithms that fit the Statistical Query Model (Kearns, 1993). These algorithms can be written in a special summation form that is amenable to parallel speed-up. Examples of such algorithms include Naive Bayes, Logistic Regression, backpropagation in Neural Networks, Expectation Maximization (EM), Principal Component Analysis, and Support Vector Machines to name a few. The summation form can be easily decomposed so that the mapper can compute the partial sums that are then aggregated by a reducer. Wolfe et al. (2008) describe an approach to estimate parameters v</context>
</contexts>
<marker>Chu, Kim, Lin, Yu, Bradski, Ng, Olukotun, 2006</marker>
<rawString>Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu, Gary R. Bradski, Andrew Y. Ng, and Kunle Olukotun. 2006. Map-reduce for machine learning on multicore. In Proceedings of Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>Mapreduce: Simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>In Proceedings of the symposium on Operating systems design and implementation (OSDI).</booktitle>
<contexts>
<context position="10211" citStr="Dean and Ghemawat, 2004" startWordPosition="1685" endWordPosition="1688">o the graph G such that wvv, = 1 and Fv, = Fv. This is equivalent to the “clamping” done in (Zhu et al., 2003). Let VD be the set of dummy vertices. Algorithm 1: Iterative Label Propogation repeat forall v E (V U VD) do �Fv = wuvFv (v,u)∈E Row normalize Fv. end until convergence or maxIterations Observe that every iteration of Algorithm 1 performs certain operations on each vertex of the graph. Further, these operations only rely on local information (from neighboring vertices of the graph). This leads to the parallel algorithm (Algorithm 2) implemented using the map-reduce model. Map-Reduce (Dean and Ghemawat, 2004) is a paradigm for implementing distributed algorithms with two user supplied functions “map” and “reduce”. The map function processes the input key/value pairs with the key being a unique iden⎧ ⎨ ⎩ Lij = 60 tifier for a node in the graph and the value corresponds to the data associated with the node. The mappers run on different machines operating on different parts of the data and the reduce function aggregates results from various mappers. Algorithm 2: Parallel Label Propagation map(key, value): begin d = 0 neighbors = getNeighbors(value); foreach n E neighbors do w = n.weight(); d += w * n</context>
<context position="19965" citStr="Dean and Ghemawat, 2004" startWordPosition="3315" endWordPosition="3318">is case, we progressively increase the number of nodes in the cluster. Again, the speedup achieved is linear in the number of processing elements (CPUs). An appealing factor of Algorithm 2 is that the memory used by each mapper process is fixed regardless of the size of the graph. This makes the algorithm feasible for use with large-scale graphs. 10 Related Work Historically, there is an abundance of work in parallel and distributed algorithms for graphs. See Grama et al. (2003) for survey chapters on the topic. In addition, the emergence of open-source implementations of Google’s map-reduce (Dean and Ghemawat, 2004) such as Hadoop3 has made parallel implementations more accessible. Recent literature shows tremendous interest in application of distributed computing to scale up machine learning algorithms. Chu et al. (2006) describe a family of learning algorithms that fit the Statistical Query Model (Kearns, 1993). These algorithms can be written in a special summation form that is amenable to parallel speed-up. Examples of such algorithms include Naive Bayes, Logistic Regression, backpropagation in Neural Networks, Expectation Maximization (EM), Principal Component Analysis, and Support Vector Machines t</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce: Simplified data processing on large clusters. In Proceedings of the symposium on Operating systems design and implementation (OSDI).</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1989</date>
<editor>Christaine Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1989</marker>
<rawString>Christaine Fellbaum, editor. 1989. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gleich</author>
<author>L Zhukov</author>
</authors>
<title>Scalable computing for power law graphs: Experience with parallel pagerank.</title>
<date>2005</date>
<booktitle>In Proceedings of SuperComputing.</booktitle>
<contexts>
<context position="21403" citStr="Gleich and Zhukov (2005)" startWordPosition="3529" endWordPosition="3533">he EM algorithm in a setup aimed to minimize communication latency. The k-means clustering algorithm has been an archetype of the map-reduce framework with several implementations available on the web. In 3http://hadoop.apache.org/core 63 addition, the Netflix Million Dollar Challenge4 generated sufficient interest in large scale clustering algorithms. (McCallum et al., 2000), describe algorithmic improvements to the k-means algorithm, called canopy clustering, to enable efficient parallel clustering of data. While there is earlier work on scalable mapreduce implementations of PageRank (E.g., Gleich and Zhukov (2005)) there is no existing literature on parallel algorithms for graph-based semisupervised learning or the relationship between PageRank and Label Propagation. 11 Conclusion In this paper, we have described a parallel algorithm for graph ranking and semi-supervised classification. We derived this by first observing that the Label Propagation algorithm can be expressed as a solution to a set of linear equations. This is easily expressed as an iterative algorithm that can be cast into the map-reduce framework. This algorithm uses fixed main memory regardless of the size of the graph. Further, our s</context>
</contexts>
<marker>Gleich, Zhukov, 2005</marker>
<rawString>D. Gleich and L. Zhukov. 2005. Scalable computing for power law graphs: Experience with parallel pagerank. In Proceedings of SuperComputing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananth Grama</author>
<author>George Karypis</author>
<author>Vipin Kumar</author>
<author>Anshul Gupta</author>
</authors>
<date>2003</date>
<booktitle>Introduction to Parallel Computing (2nd Edition).</booktitle>
<publisher>Addison-Wesley,</publisher>
<contexts>
<context position="19824" citStr="Grama et al. (2003)" startWordPosition="3295" endWordPosition="3298">b shows speedup of the algorithm. Speedup shows how well the algorithm performs with increase in resources for a fixed input size. In this case, we progressively increase the number of nodes in the cluster. Again, the speedup achieved is linear in the number of processing elements (CPUs). An appealing factor of Algorithm 2 is that the memory used by each mapper process is fixed regardless of the size of the graph. This makes the algorithm feasible for use with large-scale graphs. 10 Related Work Historically, there is an abundance of work in parallel and distributed algorithms for graphs. See Grama et al. (2003) for survey chapters on the topic. In addition, the emergence of open-source implementations of Google’s map-reduce (Dean and Ghemawat, 2004) such as Hadoop3 has made parallel implementations more accessible. Recent literature shows tremendous interest in application of distributed computing to scale up machine learning algorithms. Chu et al. (2006) describe a family of learning algorithms that fit the Statistical Query Model (Kearns, 1993). These algorithms can be written in a special summation form that is amenable to parallel speed-up. Examples of such algorithms include Naive Bayes, Logist</context>
</contexts>
<marker>Grama, Karypis, Kumar, Gupta, 2003</marker>
<rawString>Ananth Grama, George Karypis, Vipin Kumar, and Anshul Gupta. 2003. Introduction to Parallel Computing (2nd Edition). Addison-Wesley, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="14924" citStr="Kauchak and Barzilay, 2006" startWordPosition="2493" endWordPosition="2496">on algorithm. The results are seen in Table 2. Method Spearman Correlation PageRank (α = 0.1) 0.39 Parallel Label 0.39 Propagation (0 = 9) reduce(key, values): begin Emit(key, serialize(values)); end Table 2: Lexical-relatedness results: Comparision of PageRank and 0-corrected Parallel Label Propagation labeled and would like to predict labels for the remaining vertices. 8.1 Ranking To evaluate ranking, we consider the problem of deriving lexical relatedness between terms. This has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004), to name a few. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. We compare our rankings with the human judegments using the Spearman rank correlation coefficient. The graph for this task is derived from WordNet, an electronic lexical database. We compare Algorithm 2 with results from using geodesic similarity as a baseline. As observed in Table 1, the parallel implementation in Algorithm 2 performs better than ranking using geodesic similarity derived fr</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kearns</author>
</authors>
<title>Efficient noise-tolerant learning from statistical queries.</title>
<date>1993</date>
<booktitle>In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing (STOC).</booktitle>
<contexts>
<context position="20268" citStr="Kearns, 1993" startWordPosition="3360" endWordPosition="3361">e for use with large-scale graphs. 10 Related Work Historically, there is an abundance of work in parallel and distributed algorithms for graphs. See Grama et al. (2003) for survey chapters on the topic. In addition, the emergence of open-source implementations of Google’s map-reduce (Dean and Ghemawat, 2004) such as Hadoop3 has made parallel implementations more accessible. Recent literature shows tremendous interest in application of distributed computing to scale up machine learning algorithms. Chu et al. (2006) describe a family of learning algorithms that fit the Statistical Query Model (Kearns, 1993). These algorithms can be written in a special summation form that is amenable to parallel speed-up. Examples of such algorithms include Naive Bayes, Logistic Regression, backpropagation in Neural Networks, Expectation Maximization (EM), Principal Component Analysis, and Support Vector Machines to name a few. The summation form can be easily decomposed so that the mapper can compute the partial sums that are then aggregated by a reducer. Wolfe et al. (2008) describe an approach to estimate parameters via the EM algorithm in a setup aimed to minimize communication latency. The k-means clusterin</context>
</contexts>
<marker>Kearns, 1993</marker>
<rawString>Michael Kearns. 1993. Efficient noise-tolerant learning from statistical queries. In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing (STOC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard H Hovy</author>
</authors>
<title>Identifying and analyzing judgment opinions.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="17962" citStr="Kim and Hovy, 2006" startWordPosition="2974" endWordPosition="2977">quirer (GI)2 data for evaluation. General Inquirer is lexicon of English words hand-labeled with categorical information along several dimensions. One such dimension is called valence, with 1915 words labeled “Positiv” (sic) and 2291 words labeled “Negativ” for words with positive and negative sentiments respectively. We used a random 20% of the data as our seed labels and the rest as our unlabeled data. We compare our results 2http://www.wjh.harvard.edu/∼inquirer/ 62 (a) (b) Figure 3: Scalability results: (a) Scaleup (b) Speedup (F-scores) with another scalable previous work by Kim and Hovy (Kim and Hovy, 2006) in Table 2 for the same seed set. Their approach starts with a few seeds of positive and negative terms and bootstraps the list by considering all synonyms of positive word as positive and antonyms of positive words as negative. This procedure is repeated mutatis mutandis for negative words in the seed list until there are no more words to add. Method Nouns Verbs Adjectives Kim &amp; Hovy 34.80 53.36 47.28 Parallel Label 58.53 83.40 72.95 Propagation Table 3: Polarity induction results (F-scores) The performance gains seen in Table 3 should be attributed to the Label Propagation in general as the</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard H. Hovy. 2006. Identifying and analyzing judgment opinions. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
<author>Lyle H Ungar</author>
</authors>
<title>Efficient clustering of high-dimensional data sets with application to reference matching.</title>
<date>2000</date>
<booktitle>In Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>169--178</pages>
<contexts>
<context position="21157" citStr="McCallum et al., 2000" startWordPosition="3494" endWordPosition="3497"> and Support Vector Machines to name a few. The summation form can be easily decomposed so that the mapper can compute the partial sums that are then aggregated by a reducer. Wolfe et al. (2008) describe an approach to estimate parameters via the EM algorithm in a setup aimed to minimize communication latency. The k-means clustering algorithm has been an archetype of the map-reduce framework with several implementations available on the web. In 3http://hadoop.apache.org/core 63 addition, the Netflix Million Dollar Challenge4 generated sufficient interest in large scale clustering algorithms. (McCallum et al., 2000), describe algorithmic improvements to the k-means algorithm, called canopy clustering, to enable efficient parallel clustering of data. While there is earlier work on scalable mapreduce implementations of PageRank (E.g., Gleich and Zhukov (2005)) there is no existing literature on parallel algorithms for graph-based semisupervised learning or the relationship between PageRank and Label Propagation. 11 Conclusion In this paper, we have described a parallel algorithm for graph ranking and semi-supervised classification. We derived this by first observing that the Label Propagation algorithm can</context>
</contexts>
<marker>McCallum, Nigam, Ungar, 2000</marker>
<rawString>Andrew McCallum, Kamal Nigam, and Lyle H. Ungar. 2000. Efficient clustering of high-dimensional data sets with application to reference matching. In Knowledge Discovery and Data Mining (KDD), pages 169–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>W Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>In Language and Cognitive Process.</booktitle>
<contexts>
<context position="15120" citStr="Miller and Charles (1991)" startWordPosition="2525" endWordPosition="2528">d Table 2: Lexical-relatedness results: Comparision of PageRank and 0-corrected Parallel Label Propagation labeled and would like to predict labels for the remaining vertices. 8.1 Ranking To evaluate ranking, we consider the problem of deriving lexical relatedness between terms. This has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004), to name a few. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. We compare our rankings with the human judegments using the Spearman rank correlation coefficient. The graph for this task is derived from WordNet, an electronic lexical database. We compare Algorithm 2 with results from using geodesic similarity as a baseline. As observed in Table 1, the parallel implementation in Algorithm 2 performs better than ranking using geodesic similarity derived from shortest path lengths. This reinforces the motivation of using random walks as described in Section 3. Method Spearman Correlation Geodesic (baseline) 0.28 Parallel Label 0.36 Propagation Table</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>G. Miller and W. Charles. 1991. Contextual correlates of semantic similarity. In Language and Cognitive Process.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Stanford University,</institution>
<location>Stanford, CA.</location>
<contexts>
<context position="12276" citStr="Page et al., 1998" startWordPosition="2033" endWordPosition="2036">d classification. Our view of Label Propagation (via Algorithm 1) suggests a way to perform ranking on graphs. Ranking on graphs can be performed in the Parallel Label Propagation framework by associating a single point distribution with all vertices. The pivot node has a mass fixed to the value 1 at all iterations. In addition, the normalization step in Algorithm 2 is omitted. At the end of the algorithm, the mass associated with each node determines its rank. 6.1 Connection to PageRank It is interesting to note that Algorithm 1 brings out a connection between Label Propagation and PageRank (Page et al., 1998). PageRank is a random walk model that allows the random walk to “jump” to its initial state with a nonzero probability (α). Given the probability transition matrix P = [Prs], where Prs is the probability of jumping from node r to node s, the weight update for any vertex (say v) is derived as follows vt+1 = αvtP + (1 − α)v0 (4) Notice that when α = 0.5, PageRank is reduced to Algorithm 1, by a constant factor, with the additional (1 − α)v0 term corresponding to the contribution from the “dummy vertices” VD in Algorithm 1. We can in fact show that Algorithm 1 reduces to PageRank as follows: vt+</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>Larry Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1998. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford University, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Senserelate::targetword - A generalized framework for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="14881" citStr="Patwardhan et al., 2005" startWordPosition="2488" endWordPosition="2491">Entry); Emit(n.id, n); end bel Propagation algorithm. The results are seen in Table 2. Method Spearman Correlation PageRank (α = 0.1) 0.39 Parallel Label 0.39 Propagation (0 = 9) reduce(key, values): begin Emit(key, serialize(values)); end Table 2: Lexical-relatedness results: Comparision of PageRank and 0-corrected Parallel Label Propagation labeled and would like to predict labels for the remaining vertices. 8.1 Ranking To evaluate ranking, we consider the problem of deriving lexical relatedness between terms. This has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004), to name a few. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. We compare our rankings with the human judegments using the Spearman rank correlation coefficient. The graph for this task is derived from WordNet, an electronic lexical database. We compare Algorithm 2 with results from using geodesic similarity as a baseline. As observed in Table 1, the parallel implementation in Algorithm 2 performs better than r</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2005</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2005. Senserelate::targetword - A generalized framework for word sense disambiguation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Prager</author>
<author>Jennifer Chu-Carroll</author>
<author>Krzysztof Czuba</author>
</authors>
<title>Use of wordnet hypernyms for answering what-is questions.</title>
<date>2001</date>
<booktitle>In Proceedings of the Text REtrieval Conference.</booktitle>
<contexts>
<context position="14966" citStr="Prager et al., 2001" startWordPosition="2499" endWordPosition="2502">thod Spearman Correlation PageRank (α = 0.1) 0.39 Parallel Label 0.39 Propagation (0 = 9) reduce(key, values): begin Emit(key, serialize(values)); end Table 2: Lexical-relatedness results: Comparision of PageRank and 0-corrected Parallel Label Propagation labeled and would like to predict labels for the remaining vertices. 8.1 Ranking To evaluate ranking, we consider the problem of deriving lexical relatedness between terms. This has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004), to name a few. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. We compare our rankings with the human judegments using the Spearman rank correlation coefficient. The graph for this task is derived from WordNet, an electronic lexical database. We compare Algorithm 2 with results from using geodesic similarity as a baseline. As observed in Table 1, the parallel implementation in Algorithm 2 performs better than ranking using geodesic similarity derived from shortest path lengths. This reinforces </context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, 2001</marker>
<rawString>John M. Prager, Jennifer Chu-Carroll, and Krzysztof Czuba. 2001. Use of wordnet hypernyms for answering what-is questions. In Proceedings of the Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Szummer</author>
<author>T Jaakkola</author>
</authors>
<title>Clustering and efficient use of unlabeled examples.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="7027" citStr="Szummer and Jaakkola (2001)" startWordPosition="1122" endWordPosition="1125">ivity structure of the graph would suggest satellite to be more similar than religious leader as there are multiple senses, and hence multiple paths, connecting satellite and moon. Thus it is desirable to have a measure that captures not only path lengths but also the connectivity structure of the graph. This notion is elegantly captured using random walks on graphs. 4 Label Propagation: Random Walk on Manifold Graphs An efficient way to combine labeled and unlabeled data involves construction of a graph from the data and performing a Markov random walk on the graph. This has been utilized in Szummer and Jaakkola (2001), Zhu et. al. (2003), and Azran (2007). The general idea of Label Propagation involves defining a probability distribution F over the labels for each node in the graph. For labeled nodes, this distribution reflects the true labels and the aim is to recover this distribution for the unlabeled nodes in the graph. Consider a graph G(V, E, W) with vertices V , edges E, and an n x n edge weight matrix W = 1The religious leader sense of moon is due to Sun Myung Moon, a US religious leader. 59 (a) (b) (c) Figure 1: Manifold Assumption [Belkin et al., 2005]: Data lies on a manifold (a) and points alon</context>
<context position="11578" citStr="Szummer and Jaakkola, 2001" startWordPosition="1914" endWordPosition="1917">sents one iteration of Algorithm 1. This is run repeatedly until convergence or for a specified number of iterations. The algorithm is considered to have converged if the label distributions associated with each node do not ~~~~2 &lt; � change significantly, i.e., ����F(i+1) − F(i) for a fixed E &gt; 0. 6 Label Propagation for Ranking Graph ranking is applicable in a variety of problems in natural language processing and information retrieval. Given a graph, we would like to rank the vertices of a graph with respect to a node, called the pivot node or query node. Label Propagation and its variants (Szummer and Jaakkola, 2001; Zhu et al., 2003; Azran, 2007) have been traditionally used for semi-supervised classification. Our view of Label Propagation (via Algorithm 1) suggests a way to perform ranking on graphs. Ranking on graphs can be performed in the Parallel Label Propagation framework by associating a single point distribution with all vertices. The pivot node has a mass fixed to the value 1 at all iterations. In addition, the normalization step in Algorithm 2 is omitted. At the end of the algorithm, the mass associated with each node determines its rank. 6.1 Connection to PageRank It is interesting to note t</context>
</contexts>
<marker>Szummer, Jaakkola, 2001</marker>
<rawString>M. Szummer and T. Jaakkola. 2001. Clustering and efficient use of unlabeled examples. In Proceedings of Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Wolfe</author>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Fully distributed EM for very large datasets.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference in Machine Learning.</booktitle>
<contexts>
<context position="20729" citStr="Wolfe et al. (2008)" startWordPosition="3434" endWordPosition="3437">puting to scale up machine learning algorithms. Chu et al. (2006) describe a family of learning algorithms that fit the Statistical Query Model (Kearns, 1993). These algorithms can be written in a special summation form that is amenable to parallel speed-up. Examples of such algorithms include Naive Bayes, Logistic Regression, backpropagation in Neural Networks, Expectation Maximization (EM), Principal Component Analysis, and Support Vector Machines to name a few. The summation form can be easily decomposed so that the mapper can compute the partial sums that are then aggregated by a reducer. Wolfe et al. (2008) describe an approach to estimate parameters via the EM algorithm in a setup aimed to minimize communication latency. The k-means clustering algorithm has been an archetype of the map-reduce framework with several implementations available on the web. In 3http://hadoop.apache.org/core 63 addition, the Netflix Million Dollar Challenge4 generated sufficient interest in large scale clustering algorithms. (McCallum et al., 2000), describe algorithmic improvements to the k-means algorithm, called canopy clustering, to enable efficient parallel clustering of data. While there is earlier work on scal</context>
</contexts>
<marker>Wolfe, Haghighi, Klein, 2008</marker>
<rawString>Jason Wolfe, Aria Haghighi, and Dan Klein. 2008. Fully distributed EM for very large datasets. In Proceedings of the International Conference in Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Xiao</author>
<author>I Gutman</author>
</authors>
<title>Resistance distance and laplacian spectrum. Theoretical Chemistry Association,</title>
<date>2003</date>
<pages>110--284</pages>
<contexts>
<context position="8053" citStr="Xiao and Gutman, 2003" startWordPosition="1313" endWordPosition="1316">1The religious leader sense of moon is due to Sun Myung Moon, a US religious leader. 59 (a) (b) (c) Figure 1: Manifold Assumption [Belkin et al., 2005]: Data lies on a manifold (a) and points along the manifold are locally similar (b). [wij], where n = |V |. The Label Propagation algorithm minimizes a quadratic energy function £ =2 wij(Fi − Fj)2 (1) (i,j)EE � �wijFi = wijFj (3) (i, j) E E (i,j)EE � Fi(c) = 1 bi, j E V. c E classes(i) The general recipe for using random walks for classification involves constructing the graph Laplacian and using the pseudo-inverse of the Laplacian as a kernel (Xiao and Gutman, 2003). Given a weighted undirected graph, G(V, E, W), the Laplacian is defined as follows: di if i = j −wij if i is adjacent to j (2) 0 otherwise �where di = wij. j It has been shown that the pseudo-inverse of the Laplacian L is a kernel (Xiao and Gutman, 2003), i.e., it satisfies the Mercer conditions. However, there is a practical limitation to this approach. For very large graphs, even if the graph Laplacians are sparse, their pseudo-inverses are dense matrices requiring O(n2) space. This can be prohibitive in most computing environments. 5 Parallel Label Propagation In developing a parallel alg</context>
</contexts>
<marker>Xiao, Gutman, 2003</marker>
<rawString>W. Xiao and I. Gutman. 2003. Resistance distance and laplacian spectrum. Theoretical Chemistry Association, 110:284–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="4058" citStr="Zhu et al. (2003)" startWordPosition="618" endWordPosition="621">cessing, ACL-IJCNLP 2009, pages 58–65, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP proposed Parallel Label Propagation scales up linearly in the data and the number of processing elements available. Also, the main memory required by the method does not grow with the size of the graph. The outline of this paper is as follows: Section 2 introduces the manifold assumption and explains why graph-based learning algorithms perform better than their counterparts. Section 3 motivates the random walk based approach for learning on graphs. Section 4 introduces the Label Propagation method by Zhu et al. (2003). In Section 5 we describe a method to scale up Label Propagation using Map-Reduce. Section 6 shows how Label Propagation could be used for ranking on graphs and derives the relation between Label Propagation and PageRank. Parallel Label Propagation is evaluated on ranking and semi-supervised classification problems in natural language processing in Section 8. We study scalability of this algorithm in Section 9 and describe related work in the area of parallel algorithms and machine learning in Section 10. 2 Manifold Assumption The training data D can be considered as a collection of tuples D </context>
<context position="9697" citStr="Zhu et al., 2003" startWordPosition="1603" endWordPosition="1606">ervation to derive an iterative Label Propagation algorithm that we will later parallelize. Consider a weighted undirected graph G(V, E, W) with the vertex set partitioned into VL and VU (i.e., V = VLUVU) such that all vertices in VL are labeled and all vertices in VU are unlabeled. Typically only a small set of vertices are labeled, i.e., |VU |» |VL|. Let Fu denote the probability distribution over the labels associated with vertex u E V . For v E VL, Fv is known, and we also add a “dummy vertex” v� to the graph G such that wvv, = 1 and Fv, = Fv. This is equivalent to the “clamping” done in (Zhu et al., 2003). Let VD be the set of dummy vertices. Algorithm 1: Iterative Label Propogation repeat forall v E (V U VD) do �Fv = wuvFv (v,u)∈E Row normalize Fv. end until convergence or maxIterations Observe that every iteration of Algorithm 1 performs certain operations on each vertex of the graph. Further, these operations only rely on local information (from neighboring vertices of the graph). This leads to the parallel algorithm (Algorithm 2) implemented using the map-reduce model. Map-Reduce (Dean and Ghemawat, 2004) is a paradigm for implementing distributed algorithms with two user supplied function</context>
<context position="11596" citStr="Zhu et al., 2003" startWordPosition="1918" endWordPosition="1921">ithm 1. This is run repeatedly until convergence or for a specified number of iterations. The algorithm is considered to have converged if the label distributions associated with each node do not ~~~~2 &lt; � change significantly, i.e., ����F(i+1) − F(i) for a fixed E &gt; 0. 6 Label Propagation for Ranking Graph ranking is applicable in a variety of problems in natural language processing and information retrieval. Given a graph, we would like to rank the vertices of a graph with respect to a node, called the pivot node or query node. Label Propagation and its variants (Szummer and Jaakkola, 2001; Zhu et al., 2003; Azran, 2007) have been traditionally used for semi-supervised classification. Our view of Label Propagation (via Algorithm 1) suggests a way to perform ranking on graphs. Ranking on graphs can be performed in the Parallel Label Propagation framework by associating a single point distribution with all vertices. The pivot node has a mass fixed to the value 1 at all iterations. In addition, the normalization step in Algorithm 2 is omitted. At the end of the algorithm, the mass associated with each node determines its rank. 6.1 Connection to PageRank It is interesting to note that Algorithm 1 br</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 2003. Semi-supervised learning using Gaussian fields and harmonic functions. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>