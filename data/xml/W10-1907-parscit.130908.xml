<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010274">
<title confidence="0.997754">
Improving Summarization of Biomedical Documents
using Word Sense Disambiguation
</title>
<author confidence="0.981359">
Laura Plaza† Mark Stevenson* Alberto Diaz†
</author>
<email confidence="0.614208">
lplazam@fdi.ucm.es m.stevenson@dcs.shef.ac.uk albertodiaz@fdi.ucm.es
</email>
<affiliation confidence="0.792305">
† Universidad Complutense de Madrid, C/Prof. Jos´e Garc´ıa Santesmases, 28040 Madrid, Spain
* University of Sheffield, Regent Court, 211 Portobello St., Sheffield, S1 4DP, UK
</affiliation>
<sectionHeader confidence="0.989046" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999881352941176">
We describe a concept-based summariza-
tion system for biomedical documents and
show that its performance can be improved
using Word Sense Disambiguation. The
system represents the documents as graphs
formed from concepts and relations from
the UMLS. A degree-based clustering al-
gorithm is applied to these graphs to dis-
cover different themes or topics within
the document. To create the graphs, the
MetaMap program is used to map the
text onto concepts in the UMLS Metathe-
saurus. This paper shows that applying a
graph-based Word Sense Disambiguation
algorithm to the output of MetaMap im-
proves the quality of the summaries that
are generated.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957065573771">
Extractive text summarization can be defined as
the process of determining salient sentences in a
text. These sentences are expected to condense
the relevant information regarding the main topic
covered in the text. Automatic summarization of
biomedical texts may benefit both health-care ser-
vices and biomedical research (Reeve et al., 2007;
Hunter and Cohen, 2006). Providing physicians
with summaries of their patient records can help
to reduce the diagnosis time. Researchers can use
summaries to quickly determine whether a docu-
ment is of interest without having to read it all.
Summarization systems usually work with a
representation of the document consisting of in-
formation that can be directly extracted from the
document itself (Erkan and Radev, 2004; Mihalcea
and Tarau, 2004). However, recent studies have
demonstrated the benefit of summarization based
on richer representations that make use of external
knowledge sources (Plaza et al., 2008; Fiszman et
al., 2004). These approaches can represent seman-
tic associations between the words and terms in the
document (i.e. synonymy, hypernymy, homonymy
or co-occurrence) and use this information to im-
prove the quality of the summaries. In the biomed-
ical domain the Unified Medical Language Sys-
tem (UMLS) (Nelson et al., 2002) has proved to
be a useful knowledge source for summarization
(Fiszman et al., 2004; Reeve et al., 2007; Plaza et
al., 2008). In order to access the information con-
tained in the UMLS, the vocabulary of the doc-
ument being summarized has to be mapped onto
it. However, ambiguity is common in biomedi-
cal documents (Weeber et al., 2001). For exam-
ple, the string “cold” is associated with seven pos-
sible meanings in the UMLS Metathesuarus in-
cluding “common cold”, “cold sensation” , “cold
temperature” and “Chronic Obstructive Airway
Disease”. The majority of summarization sys-
tems in the biomedical domain rely on MetaMap
(Aronson, 2001) to map the text onto concepts
from the UMLS Metathesaurus (Fiszman et al.,
2004; Reeve et al., 2007). However, MetaMap fre-
quently fails to identify a unique mapping and, as
a result, various concepts with the same score are
returned. For instance, for the phrase “tissues are
often cold” MetaMap returns three equally scored
concepts for the word ‘‘cold”: “common cold”,
“cold sensation” and ”cold temperature”.
The purpose of this paper is to study the ef-
fect of lexical ambiguity in the knowledge source
on semantic approaches to biomedical summariza-
tion. To this end, the paper describes a concept-
based summarization system for biomedical doc-
uments that uses the UMLS as an external knowl-
edge source. To address the word ambiguity prob-
lem, we have adapted an existing WSD system
(Agirre and Soroa, 2009) to assign concepts from
the UMLS. The system is applied to the summa-
rization of 150 biomedical scientific articles from
the BioMed Central corpus and it is found that
</bodyText>
<page confidence="0.987063">
55
</page>
<note confidence="0.9792385">
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 55–63,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999661470588235">
WSD improves the quality of the summaries. This
paper is, to our knowledge, the first to apply WSD
to the summarization of biomedical documents
and also demonstrates that this leads to an im-
provement in performance.
The next section describes related work on sum-
marization and WSD. Section 3 introduces the
UMLS resources used in the WSD and sum-
marization systems. Section 4 describes our
concept-based summarization algorithm. Section
5 presents a graph-based WSD algorithm which
has been adapted to assign concepts from the
UMLS. Section 6 describes the experiments car-
ried out to evaluate the impact of WSD and dis-
cusses the results. The final section provides
concluding remarks and suggests future lines of
work.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999957469879518">
Summarization has been an active area within
NLP research since the 1950s and a variety of ap-
proaches have been proposed (Mani, 2001; Afan-
tenos et al., 2005). Our focus is on graph-based
summarization methods. Graph-based approaches
typically represent the document as a graph, where
the nodes represent text units (i.e. words, sen-
tences or paragraphs), and the links represent co-
hesion relations or similarity measures between
these units. The best-known work in the area is
LexRank (Erkan and Radev, 2004). It assumes a
fully connected and undirected graph, where each
node corresponds to a sentence, represented by
its TF-IDF vector, and the edges are labeled with
the cosine similarity between the sentences. Mi-
halcea and Tarau (2004) present a similar method
where the similarity among sentences is measured
in terms of word overlaps.
However, methods based on term frequencies
and syntactic representations do not exploit the se-
mantic relations among the words in the text (i.e.
synonymy, homonymy or co-occurrence). They
cannot realize, for instance, that the phrases my-
ocardial infarction and heart attack refer to the
same concepts, or that pneumococcal pneumonia
and mycoplasma pneumonia are two similar dis-
eases that differ in the type of bacteria that causes
them. This problem can be partially solved by
dealing with concepts and semantic relations from
domain-specific resources, rather than terms and
lexical or syntactic relations. Consequently, some
recent approaches have adapted existing methods
to represent the document at a conceptual level. In
particular, in the biomedical domain Reeve et al.
(2007) adapt the lexical chaining approach (Barzi-
lay and Elhadad, 1997) to work with UMLS con-
cepts, using the MetaMap Transfer Tool to anno-
tate these concepts. Yoo et al. (2007) represent a
corpus of documents as a graph, where the nodes
are the MeSH descriptors found in the corpus, and
the edges represent hypernymy and co-occurrence
relations between them. They cluster the MeSH
concepts in the corpus to identify sets of docu-
ments dealing with the same topic and then gen-
erate a summary from each document cluster.
Word sense disambiguation attempts to solve
lexical ambiguities by identifying the correct
meaning of a word based on its context. Super-
vised approaches have been shown to perform bet-
ter than unsupervised ones (Agirre and Edmonds,
2006) but need large amounts of manually-tagged
data, which are often unavailable or impractical to
create. Knowledge-based approaches are a good
alternative that do not require manually-tagged
data.
Graph-based methods have recently been shown
to be an effective approach for knowledge-based
WSD. They typically build a graph for the text in
which the nodes represent all possible senses of
the words and the edges represent different kinds
of relations between them (e.g. lexico-semantic,
co-occurrence). Some algorithm for analyzing
these graphs is then applied from which a rank-
ing of the senses of each word in the context is
obtained and the highest-ranking one is chosen
(Mihalcea and Tarau, 2004; Navigli and Velardi,
2005; Agirre and Soroa, 2009). These methods
find globally optimal solutions and are suitable for
disambiguating all words in a text.
One such method is Personalized PageRank
(Agirre and Soroa, 2009) which makes use of
the PageRank algorithm used by internet search
engines (Brin and Page, 1998). PageRank as-
signs weight to each node in a graph by analyz-
ing its structure and prefers ones that are linked to
by other nodes that are highly weighted. Agirre
and Soroa (2009) used WordNet as the lexical
knowledge base and creates graphs using the en-
tire WordNet hierarchy. The ambiguous words in
the document are added as nodes to this graph and
directed links are created from them to each of
their possible meanings. These nodes are assigned
weight in the graph and the PageRank algorithm is
</bodyText>
<page confidence="0.994377">
56
</page>
<bodyText confidence="0.999989">
applied to distribute this information through the
graph. The meaning of each word with the high-
est weight is chosen. We refer to this approach
as ppr. It is efficient since it allows all ambigu-
ous words in a document to be disambiguated si-
multaneously using the whole lexical knowledge
base, but can be misled when two of the possible
senses for an ambiguous word are related to each
other in WordNet since the PageRank algorithm
assigns weight to these senses rather than transfer-
ring it to related words. Agirre and Soroa (2009)
also describe a variant of the approach, referred
to as “word to word” (ppr w2w), in which a sep-
arate graph is created for each ambiguous word.
In these graphs no weight is assigned to the word
being disambiguated so that all of the information
used to assign weights to the possible senses of the
word is obtained from the other words in the doc-
ument. The ppr w2w is more accurate but less
efficient due to the number of graphs that have to
be created and analyzed. Agirre and Soroa (2009)
show that the Personalized PageRank approach
performs well in comparison to other knowledge-
based approaches to WSD and report an accuracy
of around 58% on standard evaluation data sets.
</bodyText>
<sectionHeader confidence="0.99967" genericHeader="method">
3 UMLS
</sectionHeader>
<bodyText confidence="0.999926078431373">
The Unified Medical Language System (UMLS)
(Humphreys et al., 1998) is a collection of con-
trolled vocabularies related to biomedicine and
contains a wide range of information that can
be used for Natural Language Processing. The
UMLS comprises of three parts: the Specialist
Lexicon, the Semantic Network and the Metathe-
saurus.
The Metathesaurus forms the backbone of the
UMLS and is created by unifying over 100 con-
trolled vocabularies and classification systems. It
is organized around concepts, each of which repre-
sents a meaning and is assigned a Concept Unique
Identifier (CUI). For example, the following CUIs
are all associated with the term “cold”: C0009443
‘Common Cold’, C0009264 ‘Cold Temperature’
and C0234192 ‘Cold Sensation’.
The MRREL table in the Metathesaurus lists re-
lations between CUIs found in the various sources
that are used to form the Metathesaurus. This ta-
ble lists a range of different types of relations, in-
cluding CHD (“child”), PAR (“parent”), QB (“can
be qualified by”), RQ (“related and possibly syn-
onymous”) and RO (“other related”). For exam-
ple, the MRREL table states that C0009443 ‘Com-
mon Cold’ and C0027442 ‘Nasopharynx’ are con-
nected via the RO relation.
The MRHIER table in the Metathesaurus lists
the hierarchies in which each CUI appears, and
presents the whole path to the top or root of
each hierarchy for the CUI. For example, the
MRHIER table states that C0035243 ‘Respiratory
Tract Infections’ is a parent of C0009443 ‘Com-
mon Cold’.
The Semantic Network consists of a set of cat-
egories (or semantic types) that provides a consis-
tent categorization of the concepts in the Metathe-
saurus, along with a set of relationships (or seman-
tic relations) that exist between the semantic types.
For example, the CUI C0009443 ‘Common Cold’
is classified in the semantic type ‘Disease or Syn-
drome’.
The SRSTR table in the Semantic Network de-
scribes the structure of the network. This table
lists a range of different relations between seman-
tic types, including hierarchical relations (is a)
and non hierarchical relations (e.g. result of,
associated with and co-occurs with).
For example, the semantic types ‘Disease or Syn-
drome’ and ‘Pathologic Function’ are connected
via the is a relation in this table.
</bodyText>
<sectionHeader confidence="0.945907" genericHeader="method">
4 Summarization system
</sectionHeader>
<bodyText confidence="0.999935666666667">
The method presented in this paper consists of 4
main steps: (1) concept identification, (2) doc-
ument representation, (3) concept clustering and
topic recognition, and (4) sentence selection. Each
step is discussed in detail in the following subsec-
tions.
</bodyText>
<subsectionHeader confidence="0.996101">
4.1 Concept identification
</subsectionHeader>
<bodyText confidence="0.9999745">
The first stage of our process is to map the doc-
ument to concepts from the UMLS Metathesaurus
and semantic types from the UMLS Semantic Net-
work.
We first run the MetaMap program over the text
in the body section of the document1 MetaMap
(Aronson, 2001) identifies all the phrases that
could be mapped onto a UMLS CUI, retrieves
and scores all possible CUI mappings for each
phrase, and returns all the candidates along with
</bodyText>
<footnote confidence="0.9254788">
1We do not make use of the disambiguation algorithm
provided by MetaMap, which is invoked using the -y flag
(Aronson, 2006), since our aim is to compare the effect of
WSD on the performance of our summarization system rather
than comparing WSD algorithms.
</footnote>
<page confidence="0.998284">
57
</page>
<bodyText confidence="0.999597">
their score. The semantic type for each concept
mapping is also returned. Table 1 shows this map-
ping for the phrase tissues are often cold. This ex-
ample shows that MetaMap returns a single CUI
for two words (tissues and often) but also returns
three equally scored CUIs for cold (C0234192,
C0009443 and C0009264). Section 5 describes
how concepts are selected when MetaMap is un-
able to return a single CUI for a word.
</bodyText>
<figure confidence="0.993867285714286">
Phrase: “Tissues”
Meta Mapping (1000)
1000 C0040300:Tissues (Body tissue)
Phrase: “are”
Phrase: “often cold”
MetaMapping (888)
694 C0332183:Often (Frequent)
861 C0234192:Cold (Cold Sensation)
MetaMapping (888)
694 C0332183:Often (Frequent)
861 C0009443:Cold (Common Cold)
MetaMapping (888)
694 C0332183:Often (Frequent)
861 C0009264:Cold (cold temperature)
</figure>
<tableCaption confidence="0.981243">
Table 1: An example of MetaMap mapping for the
</tableCaption>
<bodyText confidence="0.917845833333333">
phrase Tissues are often cold
UMLS concepts belonging to very general se-
mantic types are discarded, since they have been
found to be excessively broad or unrelated to the
main topic of the document. These types are
Quantitative Concept, Qualitative Concept, Tem-
poral Concept, Functional Concept, Idea or Con-
cept, Intellectual Product, Mental Process, Spatial
Concept and Language. Therefore, the concept
C0332183 ‘Often’ in the previous example, which
belongs to the semantic type Temporal Concept, is
discarded.
</bodyText>
<subsectionHeader confidence="0.991167">
4.2 Document representation
</subsectionHeader>
<bodyText confidence="0.964669024390244">
The next step is to construct a graph-based repre-
sentation of the document. To this end, we first ex-
tend the disambiguated UMLS concepts with their
complete hierarchy of hypernyms and merge the
hierarchies of all the concepts in the same sentence
to construct a graph representing it. The two upper
levels of these hierarchies are removed, since they
represent concepts with excessively broad mean-
ings and may introduce noise to later processing.
Next, all the sentence graphs are merged into
a single document graph. This graph is extended
with more semantic relations to obtain a more
complete representation of the document. Vari-
ous types of information from the UMLS can be
used to extend the graph. We experimented us-
ing different sets of relations and finally used the
hypernymy and other related relations between
concepts from the Metathesaurus, and the asso-
ciated with relation between semantic types from
the Semantic Network. Hypernyms are extracted
from the MRHIER table, RO (“other related”) re-
lations are extracted from the MRREL table, and
associated with relations are extracted from
the SRSTR table (see Section 3). Finally, each
edge is assigned a weight in [0, 1]. This weight
is calculated as the ratio between the relative posi-
tions in their corresponding hierarchies of the con-
cepts linked by the edge.
Figure 1 shows an example graph for a sim-
plified document consisting of the two sentences
below. Continuous lines represent hypernymy re-
lations, dashed lines represent other related rela-
tions and dotted lines represent associated with re-
lations.
1. The goal of the trial was to assess cardiovascular
mortality and morbidity for stroke, coronary heart
disease and congestive heart failure, as an evidence-
based guide for clinicians who treat hypertension.
2. The trial was carried out in two groups: the first
group taking doxazosin, and the second group tak-
ing chlorthalidone.
</bodyText>
<subsectionHeader confidence="0.99892">
4.3 Concept clustering and topic recognition
</subsectionHeader>
<bodyText confidence="0.9997584">
Our next step consists of clustering the UMLS
concepts in the document graph using a degree-
based clustering method (Erkan and Radev, 2004).
The aim is to construct sets of concepts strongly
related in meaning, based on the assumption that
each of these sets represents a different topic in the
document.
We assume that the document graph is an in-
stance of a scale-free network (Barabasi and Al-
bert, 1999). A scale-free network is a complex net-
work that (among other characteristics) presents a
particular type of node which are highly connected
to other nodes in the network, while the remain-
ing nodes are quite unconnected. These highest-
degree nodes are often called hubs. This scale-
free power-law distribution has been empirically
observed in many large networks, including lin-
guistic and semantic ones.
To discover these prominent or hub nodes, we
compute the salience or prestige of each vertex
</bodyText>
<page confidence="0.99753">
58
</page>
<figureCaption confidence="0.999949">
Figure 1: Example of a simplified document graph
</figureCaption>
<bodyText confidence="0.999965166666667">
in the graph (Yoo et al., 2007), as shown in (1).
Whenever an edge from vi to vj exists, a vote from
node i to node j is added with the strength of this
vote depending on the weight of the edge. This
ranks the nodes according to their structural im-
portance in the graph.
</bodyText>
<equation confidence="0.99133">
weight(ej) (1)
bej|Dvk/fiejconnect(vi,vk)
</equation>
<bodyText confidence="0.999991277777778">
The n vertices with a highest salience are
named Hub Vertices. The clustering algorithm
first groups the hub vertices into Hub Vertices
Sets (HVS). These can be seen as set of concepts
strongly related in meaning, and will represent the
centroids of the clusters. To construct these HVS,
the clustering algorithm first searches, iteratively
and for each hub vertex, the hub vertex most con-
nected to it, and merges them into a single HVS.
Second, the algorithm checks, for every pair of
HVS, if their internal connectivity is lower than
the connectivity between them. If so, both HVS
are merged. The remaining vertices (i.e. those
not included in the HVS) are iteratively assigned
to the cluster to which they are more connected.
This connectivity is computed as the sum of the
weights of the edges that connect the target vertex
to the other vertices in the cluster.
</bodyText>
<subsectionHeader confidence="0.998953">
4.4 Sentence selection
</subsectionHeader>
<bodyText confidence="0.999995642857143">
The last step of the summarization process con-
sists of computing the similarity between all sen-
tences in the document and each of the clusters,
and selecting the sentences for the summary based
on these similarities. To compute the similarity be-
tween a sentence graph and a cluster, we use a non-
democratic vote mechanism (Yoo et al., 2007), so
that each vertex of a sentence assigns a vote to
a cluster if the vertex belongs to its HVS, half a
vote if the vertex belongs to it but not to its HVS,
and no votes otherwise. Finally, the similarity be-
tween the sentence and the cluster is computed as
the sum of the votes assigned by all the vertices in
the sentence to the cluster, as expressed in (2).
</bodyText>
<equation confidence="0.902880833333333">
similarity(Ci, Sj� = � wkJ (2)
vk|vkESj
�
wk,j=0 if vkOCi
where wk,j=1 if vkEHVS(Ci)
wk,j=0.5 if vkOHVS(Ci)
</equation>
<bodyText confidence="0.999327428571429">
Finally, we select the sentences for the sum-
mary based on the similarity between them and
the clusters as defined above. In previous work
(blind reference), we experimented with different
heuristics for sentence selection. In this paper, we
just present the one that reported the best results.
For each sentence, we compute a single score, as
</bodyText>
<equation confidence="0.931447">
salience(vi) =
</equation>
<page confidence="0.985548">
59
</page>
<bodyText confidence="0.9934245">
the sum of its similarity to each cluster adjusted
to the cluster’s size (expression 3). Then, the N
sentences with higher scores are selected for the
summary.
</bodyText>
<equation confidence="0.938452">
�Score(Sj) _ similarity(Ci, Sj) (3)
Ci |Ci|
</equation>
<bodyText confidence="0.999820684210527">
In addition to semantic-graph similarity
(SemGr) we have also tested two further features
for computing the salience of sentences: sentence
location (Location) and similarity with the title
section (Title). The sentence location feature
assigns higher scores to the sentences close to the
beginning and the end of the document, while
the similarity with the title feature assigns higher
scores as the proportion of common concepts be-
tween the title and the target sentence is increased.
Despite their simplicity, these are well accepted
summarization heuristics that are commonly used
(Bawakid and Oussalah, 2008; Bossard et al.,
2008).
The final selection of the sentences for the sum-
mary is based on the weighted sum of these feature
values, as stated in (4). The values for the param-
eters A, 0 and X have been empirically set to 0.8,
0.1, and 0.1 respectively.
</bodyText>
<equation confidence="0.787132">
Score(Sj) _ A x SemGr(Sj) +
0 x Location(Sj) + X x Title(Sj) (4)
</equation>
<sectionHeader confidence="0.994529" genericHeader="method">
5 WSD for concept identification
</sectionHeader>
<bodyText confidence="0.999486411764706">
Since our summarization system is based on the
UMLS it is important to be able to accurately map
the documents onto CUIs. The example in Section
4.1 shows that MetaMap does not always select a
single CUI and it is therefore necessary to have
some method for choosing between the ones that
are returned. Summarization systems typically
take the first mapping as returned by MetaMap,
and no attempt is made to solve this ambiguity
(Plaza et al., 2008). This paper reports an alter-
native approach that uses a WSD algorithm that
makes use of the entire UMLS Metathesaurus.
The Personalized PageRank algorithm (see Sec-
tion 2) was adapted to use the UMLS Metathe-
saurus and used to select a CUI from the MetaMap
output2. The UMLS is converted into a graph
in which the CUIs are the nodes and the edges
</bodyText>
<footnote confidence="0.889685333333333">
2We use a publicly available implementation of the Per-
sonalized Page Rank algorithm (http://ixa2.si.ehu.
es/ukb/) for the experiments described here.
</footnote>
<bodyText confidence="0.999963423076923">
are derived from the MRREL table. All possible
relations in this table are included. The output
from MetaMap is used to provide the list of pos-
sible CUIs for each term in a document and these
are passed to the disambiguation algorithm. We
use both the standard (ppr) and “word to word”
(ppr w2w) variants of the Personalized PageRank
approach.
It is difficult to evaluate how well the Person-
alized PageRank approach performs when used
in this way due to a lack of suitable data. The
NLM-WSD corpus (Weeber et al., 2001) con-
tains manually labeled examples of ambiguous
terms in biomedical text but only provides exam-
ples for 50 terms that were specifically chosen be-
cause of their ambiguity. To evaluate an approach
such as Personalized PageRank we require doc-
uments in which the sense of every ambiguous
word has been identified. Unfortunately no such
resource is available and creating one would be
prohibitively expensive. However, our main in-
terest is in whether WSD can be used to improve
the summaries generated by our system rather than
its own performance and, consequently, decided to
evaluate the WSD by comparing the output of the
summarization system with and without WSD.
</bodyText>
<sectionHeader confidence="0.999418" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.986665">
6.1 Setup
</subsectionHeader>
<bodyText confidence="0.999985454545455">
The ROUGE metrics (Lin, 2004) are used to eval-
uate the system. ROUGE compares automati-
cally generated summaries (called peers) against
human-created summaries (called models), and
calculates a set of measures to estimate the con-
tent quality of the summaries. Results are re-
ported for the ROUGE-1 (R-1), ROUGE-2 (R-
2), ROUGE-SU4 (R-SU) and ROUGE-W (R-W)
metrics. ROUGE-N (e.g. ROUGE-1 and ROUGE-
2) evaluates n-gram co-occurrences among the
peer and models summaries, where N stands for
the length of the n-grams. ROUGE-SU4 allows
bi-gram to have intervening word gaps no longer
than four words. Finally, ROUGE-W computes
the union of the longest common subsequences be-
tween the candidate and the reference summaries
taking into account the presence of consecutive
matches.
To the authors’ knowledge, no specific corpus
for biomedical summarization exists. To evalu-
ate our approach we use a collection of 150 doc-
uments randomly selected from the BioMed Cen-
</bodyText>
<page confidence="0.99637">
60
</page>
<bodyText confidence="0.99997115625">
tral corpus3 for text mining research. This collec-
tion is large enough to ensure significant results in
the ROUGE evaluation (Lin, 2004) and allows us
to work with the ppr w2w disambiguation soft-
ware, which is quite time consuming. We generate
automatic summaries by selecting sentences until
the summary reaches a length of the 30% over the
original document size. The abstract of the papers
(i.e. the authors’ summaries) are removed from
the documents and used as model summaries.
A separate development set was used to deter-
mine the optimal values for the parameters in-
volved in the algorithm. This set consists of 10
documents from the BioMed Central corpus. The
model summaries for these documents were man-
ually created by medical students by selecting be-
tween 20-30% of the sentences within the paper.
The parameters to be estimated include the per-
centage of vertices considered as hub vertices by
the clustering method (see Section 4.3) and the
combination of summarization features used to
sentence selection (see Section 4.4). As a result,
the percentage of hub vertices was set to 15%, and
no additional summarization features (apart from
the semantic-graph similarity) were used.
Two baselines were also implemented. The
first, lead baseline, generate summaries by select-
ing the first n sentences from each document. The
second, random baseline, randomly selects n sen-
tences from the document. The n parameter is
based on the desired compression rate (i.e. 30%
of the document size).
</bodyText>
<subsectionHeader confidence="0.965413">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.9988714375">
Various summarizers were created and evaluated.
First, we generated summaries using our method
without performing word sense disambiguation
(SemGr), but selecting the first CUI returned by
MetaMap. Second, we repeated these experiments
using the Personalized Page Rank disambigua-
tion algorithm (ppr) to disambiguate the CUIs re-
turned by MetaMap (SemGr + ppr). Finally, we
use the “word to word” variant of the Personalized
Page Rank algorithm (ppr w2w) to perform the
disambiguation (SemGr + ppr w2w).
Table 2 shows ROUGE scores for the different
configurations of our system together with the two
baselines. All configurations significantly outper-
form both baselines (Wilcoxon Signed Ranks Test,
p &lt; 0.01).
</bodyText>
<footnote confidence="0.945116">
3http://www.biomedcentral.com/info/about/datamining/
</footnote>
<table confidence="0.9994615">
Summarizer R-1 R-2 R-W R-SU
random .5089 .1879 .1473 .2349
lead .6483 .2566 .1621 .2646
SemGr .7504 .3283 .1915 .3117
SemGr+ppr .7737 .3419 .1937 .3178
SemGr+ppr w2w .7804 .3530 .1966 .3262
</table>
<tableCaption confidence="0.722928">
Table 2: ROUGE scores for two baselines and
SemGr (with and without WSD). Significant dif-
ferences among the three versions of SemGr are
indicated in bold font.
</tableCaption>
<bodyText confidence="0.999239571428571">
The use of WSD improves the average ROUGE
score for the summarizer. The “standard” (i.e.
ppr) version of the WSD algorithm signifi-
cantly improves ROUGE-1 and ROUGE-2 metrics
(Wilcoxon Signed Ranks Test, p &lt; 0.01), com-
pared with no WSD (i.e. SemGr). The “word to
word” variant (ppr w2w) significantly improves
all ROUGE metrics. Performance using the “word
to word” variant is also higher than standard ppr
in all ROUGE scores.
These results demonstrate that employing a
state of the art WSD algorithm that has been
adapted to use the UMLS Metathesaurus improves
the quality of the summaries generated by a sum-
marization system. To our knowledge this is
the first result to demonstrate that WSD can im-
prove summarization systems. However, this im-
provement is less than expected and this is prob-
ably due to errors made by the WSD system.
The Personalized PageRank algorithms (ppr and
ppr w2w) have been reported to correctly dis-
ambiguate around 58% of words in general text
(see Section 2) and, although we were unable to
quantify their performance when adapted for the
biomedical domain (see Section 5), it is highly
likely that they will still make errors. However, the
WSD performance they do achieve is good enough
to improve the summarization process.
</bodyText>
<subsectionHeader confidence="0.998439">
6.3 Analysis
</subsectionHeader>
<bodyText confidence="0.9998398">
The results presented above demonstrate that us-
ing WSD improves the performance of our sum-
marizer. The reason seems to be that, since the ac-
curacy in the concept identification step increases,
the document graph built in the following steps is
a better approximation of the structure of the doc-
ument, both in terms of concepts and relations. As
a result, the clustering method succeeds in finding
the topics covered in the document, and the infor-
mation in the sentences selected for the summary
</bodyText>
<page confidence="0.997514">
61
</page>
<bodyText confidence="0.99821493442623">
is closer to that presented in the model summaries.
We have observed that the clustering method
usually produces one big cluster along with a vari-
able number of small clusters. As a consequence,
though the heuristic for sentence selection was de-
signed to select sentences from all the clusters in
the document, the fact is that most of the sentences
are extracted from this single large cluster. This
allows our system to identify sentences that cover
the main topic of the document, while it occasion-
ally fails to extract other “satellite” information.
We have also observed that the ROUGE scores
differ considerably from one document to others.
To understand the reasons of these differences we
examined the two documents with the highest and
lowest ROUGE scores respectively. The best case
is one of the largest document in the corpus, while
the worst case is one of the shortest (6 versus 3
pages). This was expected, since according to our
hypothesis that the document graph is an instance
of a scale-free network (see Section 4.3), the sum-
marization algorithm works better with larger doc-
uments. Both documents also differ in their under-
lying subject matter. The best case concerns the
reactions of some kind of proteins over the brain
synaptic membranes; while the worst case regards
the use of pattern matching for database searching.
We have verified that UMLS covers the vocabu-
lary contained in the first document better than in
the second one. We have also observed that the use
in the abstract of synonyms of terms presented in
the document body is quite frequent. In particular
the worst case document uses different terms in the
abstract and the body, for example “pattern match-
ing” and “string searching”. Since the ROUGE
metrics rely on evaluating summaries based on the
number of strings they have in common with the
model summaries the system’s output is unreason-
ably penalised.
Another problem is related to the use of
acronyms and abbreviations. Most papers in the
corpus do not include an Abbreviations section but
define them ad hoc in the document body. These
contracted forms are usually non-standard and do
not exist in the UMLS Metathesaurus. This seri-
ously affects the performance of both the disam-
biguation and the summarization algorithms, es-
pecially considering that it has been observed that
the terms (or phrases) represented in an abbrevi-
ated form frequently correspond to central con-
cepts in the document. For example, in a pa-
per from the corpus that presents an analysis tool
for simple sequence repeat tracts in DNA, only
the first occurrence of ‘simple sequence repeat’
is presented in its expanded form. In the re-
maining of the document, this phrase is named
by its acronym ‘SSR’. The same occurs in a pa-
per that investigates the developmental expression
of survivin during embryonic submandibular sali-
vary gland development, where ‘embryonic sub-
mandibular gland’ is always referred as ‘SMG’.
</bodyText>
<sectionHeader confidence="0.988313" genericHeader="conclusions">
7 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999588451612903">
In this paper we propose a graph-based approach
to biomedical summarization. Our algorithm rep-
resents the document as a semantic graph, where
the nodes are concepts from the UMLS Metathe-
saurus and the links are different kinds of seman-
tic relations between them. This produces a richer
representation than the one provided by traditional
models based on terms.
This approach relies on accurate mapping of
the document being summarized into the concepts
in the UMLS Metathesaurus. Three methods for
doing this were compared and evaluated. The
first was to select the first mapping generated by
MetaMap while the other two used a state of the
art WSD algorithm. This WSD algorithm was
adapted for the biomedical domain by using the
UMLS Metathesaurus as a knowledge based and
MetaMap as a pre-processor to identify the pos-
sible CUIs for each term. Results show that the
system performs better when WSD is used.
In future work we plan to make use of the dif-
ferent types of information within the UMLS to
create different configurations of the Personalized
PageRank WSD algorithm and explore their ef-
fect on the summarization system (i.e. consider-
ing different UMLS relations and assigning differ-
ent weights to different relations). It would also
be interesting to test the system with other disam-
biguation algorithms and use a state of the art al-
gorithm for identifying and expanding acronyms
and abbreviations.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.961506">
This research is funded by the Spanish Govern-
ment through the FPU program and the projects
TIN2009-14659-C03-01 and TSI 020312-2009-
44. Mark Stevenson acknowledges the support of
the Engineering and Physical Sciences Research
Council (grant EP/D069548/1).
</bodyText>
<page confidence="0.999025">
62
</page>
<sectionHeader confidence="0.998348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999825511363637">
S.D. Afantenos, V. Karkaletsis, and P. Stamatopou-
los. 2005. Summarization from medical docu-
ments: a survey. Artificial Intelligence in Medicine,
33(2):157–177.
E. Agirre and P. Edmonds, editors, 2006. Word
Sense Disambiguation: Algorithms and Applica-
tions. Springer.
E. Agirre and A. Soroa. 2009. Personalizing PageRank
for Word Sense Disambiguation. In Proceedings of
EACL-09, pages 33–41, Athens, Greece.
A. Aronson. 2001. Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: the MetaMap
program. In Proceedings of the AMIA Symposium,
pages 17–21.
A. Aronson. 2006. MetaMap: Mapping text to the
UMLS Metathesaurus. Technical report, U.S. Na-
tional Library of Medicine.
A.L. Barabasi and R. Albert. 1999. Emergence of scal-
ing in random networks. Science, 268:509–512.
R. Barzilay and M. Elhadad. 1997. Using lexical
chains for text summarization. In Proceedings of the
ACL Workshop on Intelligent Scalable Text Summa-
rization, pages 10–17.
A. Bawakid and M. Oussalah. 2008. A semantic
summarization system: University of Birmingham
at TAC 2008. In Proceedings of the First Text Anal-
ysis Conference (TAC 2008).
A. Bossard, M. Gnreux, and T. Poibeau. 2008. De-
scription of the LIPN systems at TAC 2008: sum-
marizing information and opinions. In Proceedings
of the First Text Analysis Conference (TAC 2008).
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30:1–7.
G. Erkan and D. R. Radev. 2004. LexRank: Graph-
based lexical centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR), 22:457–479.
M. Fiszman, T. C. Rindflesch, and H. Kilicoglu.
2004. Abstraction summarization for managing the
biomedical research literature. In Proceedings of
the HLT-NAACL Workshop on Computational Lex-
ical Semantics, pages 76–83.
L. Humphreys, D. Lindberg, H. Schoolman, and
G. Barnett. 1998. The Unified Medical Lan-
guage System: An informatics research collabora-
tion. Journal of the American Medical Informatics
Association, 1(5):1–11.
L. Hunter and K. B. Cohen. 2006. Biomedical
Language Processing: Perspective Whats Beyond
PubMed? Mol Cell., 21(5):589–594.
C.-Y. Lin. 2004. Rouge: A package for automatic eval-
uation of summaries. In Proceedings of the ACL-
04 Workshop: Text Summarization Branches Out.,
pages 74–81, Barcelona, Spain.
I. Mani. 2001. Automatic summarization. Jonh Ben-
jamins Publishing Company.
R. Mihalcea and P. Tarau. 2004. TextRank - Bringing
order into text. In Proceedings of the Conference
EMNLP 2004, pages 404–411.
R. Navigli and P. Velardi. 2005. Structural seman-
tic interconnections: A knowledge-based approach
to word sense disambiguation. IEEE Trans. Pattern
Anal. Mach. Intell., 27(7):1075–1086.
S. Nelson, T. Powell, and B. Humphreys. 2002. The
Unified Medical Language System (UMLS) Project.
In Allen Kent and Carolyn M. Hall, editors, Ency-
clopedia of Library and Information Science. Mar-
cel Dekker, Inc.
L. Plaza, A. D´ıaz, and P. Gerv´as. 2008. Concept-
graph based biomedical automatic summarization
using ontologies. In TextGraphs ’08: Proceedings
of the 3rd Textgraphs Workshop on Graph-Based Al-
gorithms for Natural Language Processing, pages
53–56.
L.H. Reeve, H. Han, and A.D. Brooks. 2007. The
use of domain-specific concepts in biomedical text
summarization. Information Processing and Man-
agement, 43:1765–1776.
M. Weeber, J. Mork, and A. Aronson. 2001. Devel-
oping a Test Collection for Biomedical Word Sense
Disambiguation. In Proceedings of AMIA Sympo-
sium, pages 746–50, Washington, DC.
I. Yoo, X. Hu, and I-Y. Song. 2007. A coherent
graph-based semantic clustering and summarization
approach for biomedical literature and a new sum-
marization evaluation method. BMC Bioinformat-
ics, 8(9).
</reference>
<page confidence="0.999463">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476021">
<title confidence="0.936632">Improving Summarization of Biomedical using Word Sense Disambiguation lplazam@fdi.ucm.es m.stevenson@dcs.shef.ac.uk albertodiaz@fdi.ucm.es</title>
<address confidence="0.766816">Complutense de Madrid, C/Prof. Jos´e Garc´ıa Santesmases, 28040 Madrid, Spain of Sheffield, Regent Court, 211 Portobello St., Sheffield, S1 4DP, UK</address>
<abstract confidence="0.999683833333333">We describe a concept-based summarization system for biomedical documents and show that its performance can be improved using Word Sense Disambiguation. The system represents the documents as graphs formed from concepts and relations from the UMLS. A degree-based clustering algorithm is applied to these graphs to discover different themes or topics within the document. To create the graphs, the MetaMap program is used to map the text onto concepts in the UMLS Metathesaurus. This paper shows that applying a graph-based Word Sense Disambiguation algorithm to the output of MetaMap improves the quality of the summaries that are generated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S D Afantenos</author>
<author>V Karkaletsis</author>
<author>P Stamatopoulos</author>
</authors>
<title>Summarization from medical documents: a survey.</title>
<date>2005</date>
<journal>Artificial Intelligence in Medicine,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="5001" citStr="Afantenos et al., 2005" startWordPosition="782" endWordPosition="786">ation and WSD. Section 3 introduces the UMLS resources used in the WSD and summarization systems. Section 4 describes our concept-based summarization algorithm. Section 5 presents a graph-based WSD algorithm which has been adapted to assign concepts from the UMLS. Section 6 describes the experiments carried out to evaluate the impact of WSD and discusses the results. The final section provides concluding remarks and suggests future lines of work. 2 Related work Summarization has been an active area within NLP research since the 1950s and a variety of approaches have been proposed (Mani, 2001; Afantenos et al., 2005). Our focus is on graph-based summarization methods. Graph-based approaches typically represent the document as a graph, where the nodes represent text units (i.e. words, sentences or paragraphs), and the links represent cohesion relations or similarity measures between these units. The best-known work in the area is LexRank (Erkan and Radev, 2004). It assumes a fully connected and undirected graph, where each node corresponds to a sentence, represented by its TF-IDF vector, and the edges are labeled with the cosine similarity between the sentences. Mihalcea and Tarau (2004) present a similar </context>
</contexts>
<marker>Afantenos, Karkaletsis, Stamatopoulos, 2005</marker>
<rawString>S.D. Afantenos, V. Karkaletsis, and P. Stamatopoulos. 2005. Summarization from medical documents: a survey. Artificial Intelligence in Medicine, 33(2):157–177.</rawString>
</citation>
<citation valid="true">
<title>Word Sense Disambiguation: Algorithms and Applications.</title>
<date>2006</date>
<editor>E. Agirre and P. Edmonds, editors,</editor>
<publisher>Springer.</publisher>
<marker>2006</marker>
<rawString>E. Agirre and P. Edmonds, editors, 2006. Word Sense Disambiguation: Algorithms and Applications. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>Personalizing PageRank for Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL-09,</booktitle>
<pages>33--41</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="3761" citStr="Agirre and Soroa, 2009" startWordPosition="582" endWordPosition="585">various concepts with the same score are returned. For instance, for the phrase “tissues are often cold” MetaMap returns three equally scored concepts for the word ‘‘cold”: “common cold”, “cold sensation” and ”cold temperature”. The purpose of this paper is to study the effect of lexical ambiguity in the knowledge source on semantic approaches to biomedical summarization. To this end, the paper describes a conceptbased summarization system for biomedical documents that uses the UMLS as an external knowledge source. To address the word ambiguity problem, we have adapted an existing WSD system (Agirre and Soroa, 2009) to assign concepts from the UMLS. The system is applied to the summarization of 150 biomedical scientific articles from the BioMed Central corpus and it is found that 55 Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 55–63, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics WSD improves the quality of the summaries. This paper is, to our knowledge, the first to apply WSD to the summarization of biomedical documents and also demonstrates that this leads to an improvement in performance. The next section describes related</context>
<context position="7966" citStr="Agirre and Soroa, 2009" startWordPosition="1248" endWordPosition="1251">es are a good alternative that do not require manually-tagged data. Graph-based methods have recently been shown to be an effective approach for knowledge-based WSD. They typically build a graph for the text in which the nodes represent all possible senses of the words and the edges represent different kinds of relations between them (e.g. lexico-semantic, co-occurrence). Some algorithm for analyzing these graphs is then applied from which a ranking of the senses of each word in the context is obtained and the highest-ranking one is chosen (Mihalcea and Tarau, 2004; Navigli and Velardi, 2005; Agirre and Soroa, 2009). These methods find globally optimal solutions and are suitable for disambiguating all words in a text. One such method is Personalized PageRank (Agirre and Soroa, 2009) which makes use of the PageRank algorithm used by internet search engines (Brin and Page, 1998). PageRank assigns weight to each node in a graph by analyzing its structure and prefers ones that are linked to by other nodes that are highly weighted. Agirre and Soroa (2009) used WordNet as the lexical knowledge base and creates graphs using the entire WordNet hierarchy. The ambiguous words in the document are added as nodes to </context>
<context position="9263" citStr="Agirre and Soroa (2009)" startWordPosition="1472" endWordPosition="1475">ble meanings. These nodes are assigned weight in the graph and the PageRank algorithm is 56 applied to distribute this information through the graph. The meaning of each word with the highest weight is chosen. We refer to this approach as ppr. It is efficient since it allows all ambiguous words in a document to be disambiguated simultaneously using the whole lexical knowledge base, but can be misled when two of the possible senses for an ambiguous word are related to each other in WordNet since the PageRank algorithm assigns weight to these senses rather than transferring it to related words. Agirre and Soroa (2009) also describe a variant of the approach, referred to as “word to word” (ppr w2w), in which a separate graph is created for each ambiguous word. In these graphs no weight is assigned to the word being disambiguated so that all of the information used to assign weights to the possible senses of the word is obtained from the other words in the document. The ppr w2w is more accurate but less efficient due to the number of graphs that have to be created and analyzed. Agirre and Soroa (2009) show that the Personalized PageRank approach performs well in comparison to other knowledgebased approaches </context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>E. Agirre and A. Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proceedings of EACL-09, pages 33–41, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Aronson</author>
</authors>
<title>Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program.</title>
<date>2001</date>
<booktitle>In Proceedings of the AMIA Symposium,</booktitle>
<pages>17--21</pages>
<contexts>
<context position="2954" citStr="Aronson, 2001" startWordPosition="450" endWordPosition="451"> useful knowledge source for summarization (Fiszman et al., 2004; Reeve et al., 2007; Plaza et al., 2008). In order to access the information contained in the UMLS, the vocabulary of the document being summarized has to be mapped onto it. However, ambiguity is common in biomedical documents (Weeber et al., 2001). For example, the string “cold” is associated with seven possible meanings in the UMLS Metathesuarus including “common cold”, “cold sensation” , “cold temperature” and “Chronic Obstructive Airway Disease”. The majority of summarization systems in the biomedical domain rely on MetaMap (Aronson, 2001) to map the text onto concepts from the UMLS Metathesaurus (Fiszman et al., 2004; Reeve et al., 2007). However, MetaMap frequently fails to identify a unique mapping and, as a result, various concepts with the same score are returned. For instance, for the phrase “tissues are often cold” MetaMap returns three equally scored concepts for the word ‘‘cold”: “common cold”, “cold sensation” and ”cold temperature”. The purpose of this paper is to study the effect of lexical ambiguity in the knowledge source on semantic approaches to biomedical summarization. To this end, the paper describes a concep</context>
<context position="12745" citStr="Aronson, 2001" startWordPosition="2052" endWordPosition="2053">tion’ are connected via the is a relation in this table. 4 Summarization system The method presented in this paper consists of 4 main steps: (1) concept identification, (2) document representation, (3) concept clustering and topic recognition, and (4) sentence selection. Each step is discussed in detail in the following subsections. 4.1 Concept identification The first stage of our process is to map the document to concepts from the UMLS Metathesaurus and semantic types from the UMLS Semantic Network. We first run the MetaMap program over the text in the body section of the document1 MetaMap (Aronson, 2001) identifies all the phrases that could be mapped onto a UMLS CUI, retrieves and scores all possible CUI mappings for each phrase, and returns all the candidates along with 1We do not make use of the disambiguation algorithm provided by MetaMap, which is invoked using the -y flag (Aronson, 2006), since our aim is to compare the effect of WSD on the performance of our summarization system rather than comparing WSD algorithms. 57 their score. The semantic type for each concept mapping is also returned. Table 1 shows this mapping for the phrase tissues are often cold. This example shows that MetaM</context>
</contexts>
<marker>Aronson, 2001</marker>
<rawString>A. Aronson. 2001. Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program. In Proceedings of the AMIA Symposium, pages 17–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Aronson</author>
</authors>
<title>MetaMap: Mapping text to the UMLS Metathesaurus.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>U.S. National Library of Medicine.</institution>
<contexts>
<context position="13040" citStr="Aronson, 2006" startWordPosition="2102" endWordPosition="2103">in detail in the following subsections. 4.1 Concept identification The first stage of our process is to map the document to concepts from the UMLS Metathesaurus and semantic types from the UMLS Semantic Network. We first run the MetaMap program over the text in the body section of the document1 MetaMap (Aronson, 2001) identifies all the phrases that could be mapped onto a UMLS CUI, retrieves and scores all possible CUI mappings for each phrase, and returns all the candidates along with 1We do not make use of the disambiguation algorithm provided by MetaMap, which is invoked using the -y flag (Aronson, 2006), since our aim is to compare the effect of WSD on the performance of our summarization system rather than comparing WSD algorithms. 57 their score. The semantic type for each concept mapping is also returned. Table 1 shows this mapping for the phrase tissues are often cold. This example shows that MetaMap returns a single CUI for two words (tissues and often) but also returns three equally scored CUIs for cold (C0234192, C0009443 and C0009264). Section 5 describes how concepts are selected when MetaMap is unable to return a single CUI for a word. Phrase: “Tissues” Meta Mapping (1000) 1000 C00</context>
</contexts>
<marker>Aronson, 2006</marker>
<rawString>A. Aronson. 2006. MetaMap: Mapping text to the UMLS Metathesaurus. Technical report, U.S. National Library of Medicine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Barabasi</author>
<author>R Albert</author>
</authors>
<title>Emergence of scaling in random networks.</title>
<date>1999</date>
<journal>Science,</journal>
<pages>268--509</pages>
<contexts>
<context position="16889" citStr="Barabasi and Albert, 1999" startWordPosition="2710" endWordPosition="2714">encebased guide for clinicians who treat hypertension. 2. The trial was carried out in two groups: the first group taking doxazosin, and the second group taking chlorthalidone. 4.3 Concept clustering and topic recognition Our next step consists of clustering the UMLS concepts in the document graph using a degreebased clustering method (Erkan and Radev, 2004). The aim is to construct sets of concepts strongly related in meaning, based on the assumption that each of these sets represents a different topic in the document. We assume that the document graph is an instance of a scale-free network (Barabasi and Albert, 1999). A scale-free network is a complex network that (among other characteristics) presents a particular type of node which are highly connected to other nodes in the network, while the remaining nodes are quite unconnected. These highestdegree nodes are often called hubs. This scalefree power-law distribution has been empirically observed in many large networks, including linguistic and semantic ones. To discover these prominent or hub nodes, we compute the salience or prestige of each vertex 58 Figure 1: Example of a simplified document graph in the graph (Yoo et al., 2007), as shown in (1). Whe</context>
</contexts>
<marker>Barabasi, Albert, 1999</marker>
<rawString>A.L. Barabasi and R. Albert. 1999. Emergence of scaling in random networks. Science, 268:509–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<contexts>
<context position="6532" citStr="Barzilay and Elhadad, 1997" startWordPosition="1016" endWordPosition="1020">t the phrases myocardial infarction and heart attack refer to the same concepts, or that pneumococcal pneumonia and mycoplasma pneumonia are two similar diseases that differ in the type of bacteria that causes them. This problem can be partially solved by dealing with concepts and semantic relations from domain-specific resources, rather than terms and lexical or syntactic relations. Consequently, some recent approaches have adapted existing methods to represent the document at a conceptual level. In particular, in the biomedical domain Reeve et al. (2007) adapt the lexical chaining approach (Barzilay and Elhadad, 1997) to work with UMLS concepts, using the MetaMap Transfer Tool to annotate these concepts. Yoo et al. (2007) represent a corpus of documents as a graph, where the nodes are the MeSH descriptors found in the corpus, and the edges represent hypernymy and co-occurrence relations between them. They cluster the MeSH concepts in the corpus to identify sets of documents dealing with the same topic and then generate a summary from each document cluster. Word sense disambiguation attempts to solve lexical ambiguities by identifying the correct meaning of a word based on its context. Supervised approaches</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>R. Barzilay and M. Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization, pages 10–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bawakid</author>
<author>M Oussalah</author>
</authors>
<title>A semantic summarization system: University of Birmingham at TAC</title>
<date>2008</date>
<booktitle>In Proceedings of the First Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="20624" citStr="Bawakid and Oussalah, 2008" startWordPosition="3340" endWordPosition="3343">) (3) Ci |Ci| In addition to semantic-graph similarity (SemGr) we have also tested two further features for computing the salience of sentences: sentence location (Location) and similarity with the title section (Title). The sentence location feature assigns higher scores to the sentences close to the beginning and the end of the document, while the similarity with the title feature assigns higher scores as the proportion of common concepts between the title and the target sentence is increased. Despite their simplicity, these are well accepted summarization heuristics that are commonly used (Bawakid and Oussalah, 2008; Bossard et al., 2008). The final selection of the sentences for the summary is based on the weighted sum of these feature values, as stated in (4). The values for the parameters A, 0 and X have been empirically set to 0.8, 0.1, and 0.1 respectively. Score(Sj) _ A x SemGr(Sj) + 0 x Location(Sj) + X x Title(Sj) (4) 5 WSD for concept identification Since our summarization system is based on the UMLS it is important to be able to accurately map the documents onto CUIs. The example in Section 4.1 shows that MetaMap does not always select a single CUI and it is therefore necessary to have some met</context>
</contexts>
<marker>Bawakid, Oussalah, 2008</marker>
<rawString>A. Bawakid and M. Oussalah. 2008. A semantic summarization system: University of Birmingham at TAC 2008. In Proceedings of the First Text Analysis Conference (TAC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bossard</author>
<author>M Gnreux</author>
<author>T Poibeau</author>
</authors>
<title>Description of the LIPN systems at TAC 2008: summarizing information and opinions.</title>
<date>2008</date>
<booktitle>In Proceedings of the First Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="20647" citStr="Bossard et al., 2008" startWordPosition="3344" endWordPosition="3347"> semantic-graph similarity (SemGr) we have also tested two further features for computing the salience of sentences: sentence location (Location) and similarity with the title section (Title). The sentence location feature assigns higher scores to the sentences close to the beginning and the end of the document, while the similarity with the title feature assigns higher scores as the proportion of common concepts between the title and the target sentence is increased. Despite their simplicity, these are well accepted summarization heuristics that are commonly used (Bawakid and Oussalah, 2008; Bossard et al., 2008). The final selection of the sentences for the summary is based on the weighted sum of these feature values, as stated in (4). The values for the parameters A, 0 and X have been empirically set to 0.8, 0.1, and 0.1 respectively. Score(Sj) _ A x SemGr(Sj) + 0 x Location(Sj) + X x Title(Sj) (4) 5 WSD for concept identification Since our summarization system is based on the UMLS it is important to be able to accurately map the documents onto CUIs. The example in Section 4.1 shows that MetaMap does not always select a single CUI and it is therefore necessary to have some method for choosing betwee</context>
</contexts>
<marker>Bossard, Gnreux, Poibeau, 2008</marker>
<rawString>A. Bossard, M. Gnreux, and T. Poibeau. 2008. Description of the LIPN systems at TAC 2008: summarizing information and opinions. In Proceedings of the First Text Analysis Conference (TAC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a largescale hypertextual web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<pages>30--1</pages>
<contexts>
<context position="8232" citStr="Brin and Page, 1998" startWordPosition="1290" endWordPosition="1293"> and the edges represent different kinds of relations between them (e.g. lexico-semantic, co-occurrence). Some algorithm for analyzing these graphs is then applied from which a ranking of the senses of each word in the context is obtained and the highest-ranking one is chosen (Mihalcea and Tarau, 2004; Navigli and Velardi, 2005; Agirre and Soroa, 2009). These methods find globally optimal solutions and are suitable for disambiguating all words in a text. One such method is Personalized PageRank (Agirre and Soroa, 2009) which makes use of the PageRank algorithm used by internet search engines (Brin and Page, 1998). PageRank assigns weight to each node in a graph by analyzing its structure and prefers ones that are linked to by other nodes that are highly weighted. Agirre and Soroa (2009) used WordNet as the lexical knowledge base and creates graphs using the entire WordNet hierarchy. The ambiguous words in the document are added as nodes to this graph and directed links are created from them to each of their possible meanings. These nodes are assigned weight in the graph and the PageRank algorithm is 56 applied to distribute this information through the graph. The meaning of each word with the highest </context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a largescale hypertextual web search engine. Computer Networks and ISDN Systems, 30:1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D R Radev</author>
</authors>
<title>LexRank: Graphbased lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>22--457</pages>
<contexts>
<context position="1795" citStr="Erkan and Radev, 2004" startWordPosition="262" endWordPosition="265">ense the relevant information regarding the main topic covered in the text. Automatic summarization of biomedical texts may benefit both health-care services and biomedical research (Reeve et al., 2007; Hunter and Cohen, 2006). Providing physicians with summaries of their patient records can help to reduce the diagnosis time. Researchers can use summaries to quickly determine whether a document is of interest without having to read it all. Summarization systems usually work with a representation of the document consisting of information that can be directly extracted from the document itself (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). However, recent studies have demonstrated the benefit of summarization based on richer representations that make use of external knowledge sources (Plaza et al., 2008; Fiszman et al., 2004). These approaches can represent semantic associations between the words and terms in the document (i.e. synonymy, hypernymy, homonymy or co-occurrence) and use this information to improve the quality of the summaries. In the biomedical domain the Unified Medical Language System (UMLS) (Nelson et al., 2002) has proved to be a useful knowledge source for summarization (Fiszman et </context>
<context position="5351" citStr="Erkan and Radev, 2004" startWordPosition="836" endWordPosition="839">usses the results. The final section provides concluding remarks and suggests future lines of work. 2 Related work Summarization has been an active area within NLP research since the 1950s and a variety of approaches have been proposed (Mani, 2001; Afantenos et al., 2005). Our focus is on graph-based summarization methods. Graph-based approaches typically represent the document as a graph, where the nodes represent text units (i.e. words, sentences or paragraphs), and the links represent cohesion relations or similarity measures between these units. The best-known work in the area is LexRank (Erkan and Radev, 2004). It assumes a fully connected and undirected graph, where each node corresponds to a sentence, represented by its TF-IDF vector, and the edges are labeled with the cosine similarity between the sentences. Mihalcea and Tarau (2004) present a similar method where the similarity among sentences is measured in terms of word overlaps. However, methods based on term frequencies and syntactic representations do not exploit the semantic relations among the words in the text (i.e. synonymy, homonymy or co-occurrence). They cannot realize, for instance, that the phrases myocardial infarction and heart </context>
<context position="16623" citStr="Erkan and Radev, 2004" startWordPosition="2664" endWordPosition="2667">ations, dashed lines represent other related relations and dotted lines represent associated with relations. 1. The goal of the trial was to assess cardiovascular mortality and morbidity for stroke, coronary heart disease and congestive heart failure, as an evidencebased guide for clinicians who treat hypertension. 2. The trial was carried out in two groups: the first group taking doxazosin, and the second group taking chlorthalidone. 4.3 Concept clustering and topic recognition Our next step consists of clustering the UMLS concepts in the document graph using a degreebased clustering method (Erkan and Radev, 2004). The aim is to construct sets of concepts strongly related in meaning, based on the assumption that each of these sets represents a different topic in the document. We assume that the document graph is an instance of a scale-free network (Barabasi and Albert, 1999). A scale-free network is a complex network that (among other characteristics) presents a particular type of node which are highly connected to other nodes in the network, while the remaining nodes are quite unconnected. These highestdegree nodes are often called hubs. This scalefree power-law distribution has been empirically obser</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G. Erkan and D. R. Radev. 2004. LexRank: Graphbased lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR), 22:457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fiszman</author>
<author>T C Rindflesch</author>
<author>H Kilicoglu</author>
</authors>
<title>Abstraction summarization for managing the biomedical research literature.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL Workshop on Computational Lexical Semantics,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="2013" citStr="Fiszman et al., 2004" startWordPosition="294" endWordPosition="297">n, 2006). Providing physicians with summaries of their patient records can help to reduce the diagnosis time. Researchers can use summaries to quickly determine whether a document is of interest without having to read it all. Summarization systems usually work with a representation of the document consisting of information that can be directly extracted from the document itself (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). However, recent studies have demonstrated the benefit of summarization based on richer representations that make use of external knowledge sources (Plaza et al., 2008; Fiszman et al., 2004). These approaches can represent semantic associations between the words and terms in the document (i.e. synonymy, hypernymy, homonymy or co-occurrence) and use this information to improve the quality of the summaries. In the biomedical domain the Unified Medical Language System (UMLS) (Nelson et al., 2002) has proved to be a useful knowledge source for summarization (Fiszman et al., 2004; Reeve et al., 2007; Plaza et al., 2008). In order to access the information contained in the UMLS, the vocabulary of the document being summarized has to be mapped onto it. However, ambiguity is common in bi</context>
</contexts>
<marker>Fiszman, Rindflesch, Kilicoglu, 2004</marker>
<rawString>M. Fiszman, T. C. Rindflesch, and H. Kilicoglu. 2004. Abstraction summarization for managing the biomedical research literature. In Proceedings of the HLT-NAACL Workshop on Computational Lexical Semantics, pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Humphreys</author>
<author>D Lindberg</author>
<author>H Schoolman</author>
<author>G Barnett</author>
</authors>
<title>The Unified Medical Language System: An informatics research collaboration.</title>
<date>1998</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>1</volume>
<issue>5</issue>
<contexts>
<context position="10015" citStr="Humphreys et al., 1998" startWordPosition="1605" endWordPosition="1608">ambiguous word. In these graphs no weight is assigned to the word being disambiguated so that all of the information used to assign weights to the possible senses of the word is obtained from the other words in the document. The ppr w2w is more accurate but less efficient due to the number of graphs that have to be created and analyzed. Agirre and Soroa (2009) show that the Personalized PageRank approach performs well in comparison to other knowledgebased approaches to WSD and report an accuracy of around 58% on standard evaluation data sets. 3 UMLS The Unified Medical Language System (UMLS) (Humphreys et al., 1998) is a collection of controlled vocabularies related to biomedicine and contains a wide range of information that can be used for Natural Language Processing. The UMLS comprises of three parts: the Specialist Lexicon, the Semantic Network and the Metathesaurus. The Metathesaurus forms the backbone of the UMLS and is created by unifying over 100 controlled vocabularies and classification systems. It is organized around concepts, each of which represents a meaning and is assigned a Concept Unique Identifier (CUI). For example, the following CUIs are all associated with the term “cold”: C0009443 ‘</context>
</contexts>
<marker>Humphreys, Lindberg, Schoolman, Barnett, 1998</marker>
<rawString>L. Humphreys, D. Lindberg, H. Schoolman, and G. Barnett. 1998. The Unified Medical Language System: An informatics research collaboration. Journal of the American Medical Informatics Association, 1(5):1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hunter</author>
<author>K B Cohen</author>
</authors>
<title>Biomedical Language Processing: Perspective Whats Beyond PubMed? Mol Cell.,</title>
<date>2006</date>
<contexts>
<context position="1400" citStr="Hunter and Cohen, 2006" startWordPosition="200" endWordPosition="203">aMap program is used to map the text onto concepts in the UMLS Metathesaurus. This paper shows that applying a graph-based Word Sense Disambiguation algorithm to the output of MetaMap improves the quality of the summaries that are generated. 1 Introduction Extractive text summarization can be defined as the process of determining salient sentences in a text. These sentences are expected to condense the relevant information regarding the main topic covered in the text. Automatic summarization of biomedical texts may benefit both health-care services and biomedical research (Reeve et al., 2007; Hunter and Cohen, 2006). Providing physicians with summaries of their patient records can help to reduce the diagnosis time. Researchers can use summaries to quickly determine whether a document is of interest without having to read it all. Summarization systems usually work with a representation of the document consisting of information that can be directly extracted from the document itself (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). However, recent studies have demonstrated the benefit of summarization based on richer representations that make use of external knowledge sources (Plaza et al., 2008; Fiszman </context>
</contexts>
<marker>Hunter, Cohen, 2006</marker>
<rawString>L. Hunter and K. B. Cohen. 2006. Biomedical Language Processing: Perspective Whats Beyond PubMed? Mol Cell., 21(5):589–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL04 Workshop: Text Summarization Branches Out.,</booktitle>
<pages>74--81</pages>
<location>Barcelona,</location>
<contexts>
<context position="23156" citStr="Lin, 2004" startWordPosition="3779" endWordPosition="3780">terms that were specifically chosen because of their ambiguity. To evaluate an approach such as Personalized PageRank we require documents in which the sense of every ambiguous word has been identified. Unfortunately no such resource is available and creating one would be prohibitively expensive. However, our main interest is in whether WSD can be used to improve the summaries generated by our system rather than its own performance and, consequently, decided to evaluate the WSD by comparing the output of the summarization system with and without WSD. 6 Experiments 6.1 Setup The ROUGE metrics (Lin, 2004) are used to evaluate the system. ROUGE compares automatically generated summaries (called peers) against human-created summaries (called models), and calculates a set of measures to estimate the content quality of the summaries. Results are reported for the ROUGE-1 (R-1), ROUGE-2 (R2), ROUGE-SU4 (R-SU) and ROUGE-W (R-W) metrics. ROUGE-N (e.g. ROUGE-1 and ROUGE2) evaluates n-gram co-occurrences among the peer and models summaries, where N stands for the length of the n-grams. ROUGE-SU4 allows bi-gram to have intervening word gaps no longer than four words. Finally, ROUGE-W computes the union o</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C.-Y. Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of the ACL04 Workshop: Text Summarization Branches Out., pages 74–81, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
</authors>
<title>Automatic summarization.</title>
<date>2001</date>
<publisher>Jonh Benjamins Publishing Company.</publisher>
<contexts>
<context position="4976" citStr="Mani, 2001" startWordPosition="780" endWordPosition="781"> on summarization and WSD. Section 3 introduces the UMLS resources used in the WSD and summarization systems. Section 4 describes our concept-based summarization algorithm. Section 5 presents a graph-based WSD algorithm which has been adapted to assign concepts from the UMLS. Section 6 describes the experiments carried out to evaluate the impact of WSD and discusses the results. The final section provides concluding remarks and suggests future lines of work. 2 Related work Summarization has been an active area within NLP research since the 1950s and a variety of approaches have been proposed (Mani, 2001; Afantenos et al., 2005). Our focus is on graph-based summarization methods. Graph-based approaches typically represent the document as a graph, where the nodes represent text units (i.e. words, sentences or paragraphs), and the links represent cohesion relations or similarity measures between these units. The best-known work in the area is LexRank (Erkan and Radev, 2004). It assumes a fully connected and undirected graph, where each node corresponds to a sentence, represented by its TF-IDF vector, and the edges are labeled with the cosine similarity between the sentences. Mihalcea and Tarau </context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>I. Mani. 2001. Automatic summarization. Jonh Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>TextRank - Bringing order into text.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference EMNLP</booktitle>
<pages>404--411</pages>
<contexts>
<context position="1822" citStr="Mihalcea and Tarau, 2004" startWordPosition="266" endWordPosition="269">mation regarding the main topic covered in the text. Automatic summarization of biomedical texts may benefit both health-care services and biomedical research (Reeve et al., 2007; Hunter and Cohen, 2006). Providing physicians with summaries of their patient records can help to reduce the diagnosis time. Researchers can use summaries to quickly determine whether a document is of interest without having to read it all. Summarization systems usually work with a representation of the document consisting of information that can be directly extracted from the document itself (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). However, recent studies have demonstrated the benefit of summarization based on richer representations that make use of external knowledge sources (Plaza et al., 2008; Fiszman et al., 2004). These approaches can represent semantic associations between the words and terms in the document (i.e. synonymy, hypernymy, homonymy or co-occurrence) and use this information to improve the quality of the summaries. In the biomedical domain the Unified Medical Language System (UMLS) (Nelson et al., 2002) has proved to be a useful knowledge source for summarization (Fiszman et al., 2004; Reeve et al., 20</context>
<context position="5582" citStr="Mihalcea and Tarau (2004)" startWordPosition="872" endWordPosition="876">roposed (Mani, 2001; Afantenos et al., 2005). Our focus is on graph-based summarization methods. Graph-based approaches typically represent the document as a graph, where the nodes represent text units (i.e. words, sentences or paragraphs), and the links represent cohesion relations or similarity measures between these units. The best-known work in the area is LexRank (Erkan and Radev, 2004). It assumes a fully connected and undirected graph, where each node corresponds to a sentence, represented by its TF-IDF vector, and the edges are labeled with the cosine similarity between the sentences. Mihalcea and Tarau (2004) present a similar method where the similarity among sentences is measured in terms of word overlaps. However, methods based on term frequencies and syntactic representations do not exploit the semantic relations among the words in the text (i.e. synonymy, homonymy or co-occurrence). They cannot realize, for instance, that the phrases myocardial infarction and heart attack refer to the same concepts, or that pneumococcal pneumonia and mycoplasma pneumonia are two similar diseases that differ in the type of bacteria that causes them. This problem can be partially solved by dealing with concepts</context>
<context position="7914" citStr="Mihalcea and Tarau, 2004" startWordPosition="1240" endWordPosition="1243">le or impractical to create. Knowledge-based approaches are a good alternative that do not require manually-tagged data. Graph-based methods have recently been shown to be an effective approach for knowledge-based WSD. They typically build a graph for the text in which the nodes represent all possible senses of the words and the edges represent different kinds of relations between them (e.g. lexico-semantic, co-occurrence). Some algorithm for analyzing these graphs is then applied from which a ranking of the senses of each word in the context is obtained and the highest-ranking one is chosen (Mihalcea and Tarau, 2004; Navigli and Velardi, 2005; Agirre and Soroa, 2009). These methods find globally optimal solutions and are suitable for disambiguating all words in a text. One such method is Personalized PageRank (Agirre and Soroa, 2009) which makes use of the PageRank algorithm used by internet search engines (Brin and Page, 1998). PageRank assigns weight to each node in a graph by analyzing its structure and prefers ones that are linked to by other nodes that are highly weighted. Agirre and Soroa (2009) used WordNet as the lexical knowledge base and creates graphs using the entire WordNet hierarchy. The am</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>R. Mihalcea and P. Tarau. 2004. TextRank - Bringing order into text. In Proceedings of the Conference EMNLP 2004, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>Structural semantic interconnections: A knowledge-based approach to word sense disambiguation.</title>
<date>2005</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="7941" citStr="Navigli and Velardi, 2005" startWordPosition="1244" endWordPosition="1247">e. Knowledge-based approaches are a good alternative that do not require manually-tagged data. Graph-based methods have recently been shown to be an effective approach for knowledge-based WSD. They typically build a graph for the text in which the nodes represent all possible senses of the words and the edges represent different kinds of relations between them (e.g. lexico-semantic, co-occurrence). Some algorithm for analyzing these graphs is then applied from which a ranking of the senses of each word in the context is obtained and the highest-ranking one is chosen (Mihalcea and Tarau, 2004; Navigli and Velardi, 2005; Agirre and Soroa, 2009). These methods find globally optimal solutions and are suitable for disambiguating all words in a text. One such method is Personalized PageRank (Agirre and Soroa, 2009) which makes use of the PageRank algorithm used by internet search engines (Brin and Page, 1998). PageRank assigns weight to each node in a graph by analyzing its structure and prefers ones that are linked to by other nodes that are highly weighted. Agirre and Soroa (2009) used WordNet as the lexical knowledge base and creates graphs using the entire WordNet hierarchy. The ambiguous words in the docume</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>R. Navigli and P. Velardi. 2005. Structural semantic interconnections: A knowledge-based approach to word sense disambiguation. IEEE Trans. Pattern Anal. Mach. Intell., 27(7):1075–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nelson</author>
<author>T Powell</author>
<author>B Humphreys</author>
</authors>
<title>The Unified Medical Language System</title>
<date>2002</date>
<booktitle>Encyclopedia of Library and Information Science.</booktitle>
<editor>(UMLS) Project. In Allen Kent and Carolyn M. Hall, editors,</editor>
<publisher>Marcel Dekker, Inc.</publisher>
<contexts>
<context position="2321" citStr="Nelson et al., 2002" startWordPosition="343" endWordPosition="346"> of information that can be directly extracted from the document itself (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). However, recent studies have demonstrated the benefit of summarization based on richer representations that make use of external knowledge sources (Plaza et al., 2008; Fiszman et al., 2004). These approaches can represent semantic associations between the words and terms in the document (i.e. synonymy, hypernymy, homonymy or co-occurrence) and use this information to improve the quality of the summaries. In the biomedical domain the Unified Medical Language System (UMLS) (Nelson et al., 2002) has proved to be a useful knowledge source for summarization (Fiszman et al., 2004; Reeve et al., 2007; Plaza et al., 2008). In order to access the information contained in the UMLS, the vocabulary of the document being summarized has to be mapped onto it. However, ambiguity is common in biomedical documents (Weeber et al., 2001). For example, the string “cold” is associated with seven possible meanings in the UMLS Metathesuarus including “common cold”, “cold sensation” , “cold temperature” and “Chronic Obstructive Airway Disease”. The majority of summarization systems in the biomedical domai</context>
</contexts>
<marker>Nelson, Powell, Humphreys, 2002</marker>
<rawString>S. Nelson, T. Powell, and B. Humphreys. 2002. The Unified Medical Language System (UMLS) Project. In Allen Kent and Carolyn M. Hall, editors, Encyclopedia of Library and Information Science. Marcel Dekker, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Plaza</author>
<author>A D´ıaz</author>
<author>P Gerv´as</author>
</authors>
<title>Conceptgraph based biomedical automatic summarization using ontologies.</title>
<date>2008</date>
<booktitle>In TextGraphs ’08: Proceedings of the 3rd Textgraphs Workshop on Graph-Based Algorithms for Natural Language Processing,</booktitle>
<pages>53--56</pages>
<marker>Plaza, D´ıaz, Gerv´as, 2008</marker>
<rawString>L. Plaza, A. D´ıaz, and P. Gerv´as. 2008. Conceptgraph based biomedical automatic summarization using ontologies. In TextGraphs ’08: Proceedings of the 3rd Textgraphs Workshop on Graph-Based Algorithms for Natural Language Processing, pages 53–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L H Reeve</author>
<author>H Han</author>
<author>A D Brooks</author>
</authors>
<title>The use of domain-specific concepts in biomedical text summarization.</title>
<date>2007</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>43--1765</pages>
<contexts>
<context position="1375" citStr="Reeve et al., 2007" startWordPosition="196" endWordPosition="199"> the graphs, the MetaMap program is used to map the text onto concepts in the UMLS Metathesaurus. This paper shows that applying a graph-based Word Sense Disambiguation algorithm to the output of MetaMap improves the quality of the summaries that are generated. 1 Introduction Extractive text summarization can be defined as the process of determining salient sentences in a text. These sentences are expected to condense the relevant information regarding the main topic covered in the text. Automatic summarization of biomedical texts may benefit both health-care services and biomedical research (Reeve et al., 2007; Hunter and Cohen, 2006). Providing physicians with summaries of their patient records can help to reduce the diagnosis time. Researchers can use summaries to quickly determine whether a document is of interest without having to read it all. Summarization systems usually work with a representation of the document consisting of information that can be directly extracted from the document itself (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). However, recent studies have demonstrated the benefit of summarization based on richer representations that make use of external knowledge sources (Pla</context>
<context position="3055" citStr="Reeve et al., 2007" startWordPosition="466" endWordPosition="469">l., 2008). In order to access the information contained in the UMLS, the vocabulary of the document being summarized has to be mapped onto it. However, ambiguity is common in biomedical documents (Weeber et al., 2001). For example, the string “cold” is associated with seven possible meanings in the UMLS Metathesuarus including “common cold”, “cold sensation” , “cold temperature” and “Chronic Obstructive Airway Disease”. The majority of summarization systems in the biomedical domain rely on MetaMap (Aronson, 2001) to map the text onto concepts from the UMLS Metathesaurus (Fiszman et al., 2004; Reeve et al., 2007). However, MetaMap frequently fails to identify a unique mapping and, as a result, various concepts with the same score are returned. For instance, for the phrase “tissues are often cold” MetaMap returns three equally scored concepts for the word ‘‘cold”: “common cold”, “cold sensation” and ”cold temperature”. The purpose of this paper is to study the effect of lexical ambiguity in the knowledge source on semantic approaches to biomedical summarization. To this end, the paper describes a conceptbased summarization system for biomedical documents that uses the UMLS as an external knowledge sour</context>
<context position="6467" citStr="Reeve et al. (2007)" startWordPosition="1007" endWordPosition="1010">or co-occurrence). They cannot realize, for instance, that the phrases myocardial infarction and heart attack refer to the same concepts, or that pneumococcal pneumonia and mycoplasma pneumonia are two similar diseases that differ in the type of bacteria that causes them. This problem can be partially solved by dealing with concepts and semantic relations from domain-specific resources, rather than terms and lexical or syntactic relations. Consequently, some recent approaches have adapted existing methods to represent the document at a conceptual level. In particular, in the biomedical domain Reeve et al. (2007) adapt the lexical chaining approach (Barzilay and Elhadad, 1997) to work with UMLS concepts, using the MetaMap Transfer Tool to annotate these concepts. Yoo et al. (2007) represent a corpus of documents as a graph, where the nodes are the MeSH descriptors found in the corpus, and the edges represent hypernymy and co-occurrence relations between them. They cluster the MeSH concepts in the corpus to identify sets of documents dealing with the same topic and then generate a summary from each document cluster. Word sense disambiguation attempts to solve lexical ambiguities by identifying the corr</context>
</contexts>
<marker>Reeve, Han, Brooks, 2007</marker>
<rawString>L.H. Reeve, H. Han, and A.D. Brooks. 2007. The use of domain-specific concepts in biomedical text summarization. Information Processing and Management, 43:1765–1776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Weeber</author>
<author>J Mork</author>
<author>A Aronson</author>
</authors>
<title>Developing a Test Collection for Biomedical Word Sense Disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of AMIA Symposium,</booktitle>
<pages>746--50</pages>
<location>Washington, DC.</location>
<contexts>
<context position="2653" citStr="Weeber et al., 2001" startWordPosition="402" endWordPosition="405">an represent semantic associations between the words and terms in the document (i.e. synonymy, hypernymy, homonymy or co-occurrence) and use this information to improve the quality of the summaries. In the biomedical domain the Unified Medical Language System (UMLS) (Nelson et al., 2002) has proved to be a useful knowledge source for summarization (Fiszman et al., 2004; Reeve et al., 2007; Plaza et al., 2008). In order to access the information contained in the UMLS, the vocabulary of the document being summarized has to be mapped onto it. However, ambiguity is common in biomedical documents (Weeber et al., 2001). For example, the string “cold” is associated with seven possible meanings in the UMLS Metathesuarus including “common cold”, “cold sensation” , “cold temperature” and “Chronic Obstructive Airway Disease”. The majority of summarization systems in the biomedical domain rely on MetaMap (Aronson, 2001) to map the text onto concepts from the UMLS Metathesaurus (Fiszman et al., 2004; Reeve et al., 2007). However, MetaMap frequently fails to identify a unique mapping and, as a result, various concepts with the same score are returned. For instance, for the phrase “tissues are often cold” MetaMap re</context>
<context position="22438" citStr="Weeber et al., 2001" startWordPosition="3660" endWordPosition="3663">e Personalized Page Rank algorithm (http://ixa2.si.ehu. es/ukb/) for the experiments described here. are derived from the MRREL table. All possible relations in this table are included. The output from MetaMap is used to provide the list of possible CUIs for each term in a document and these are passed to the disambiguation algorithm. We use both the standard (ppr) and “word to word” (ppr w2w) variants of the Personalized PageRank approach. It is difficult to evaluate how well the Personalized PageRank approach performs when used in this way due to a lack of suitable data. The NLM-WSD corpus (Weeber et al., 2001) contains manually labeled examples of ambiguous terms in biomedical text but only provides examples for 50 terms that were specifically chosen because of their ambiguity. To evaluate an approach such as Personalized PageRank we require documents in which the sense of every ambiguous word has been identified. Unfortunately no such resource is available and creating one would be prohibitively expensive. However, our main interest is in whether WSD can be used to improve the summaries generated by our system rather than its own performance and, consequently, decided to evaluate the WSD by compar</context>
</contexts>
<marker>Weeber, Mork, Aronson, 2001</marker>
<rawString>M. Weeber, J. Mork, and A. Aronson. 2001. Developing a Test Collection for Biomedical Word Sense Disambiguation. In Proceedings of AMIA Symposium, pages 746–50, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Yoo</author>
<author>X Hu</author>
<author>I-Y Song</author>
</authors>
<title>A coherent graph-based semantic clustering and summarization approach for biomedical literature and a new summarization evaluation method.</title>
<date>2007</date>
<journal>BMC Bioinformatics,</journal>
<volume>8</volume>
<issue>9</issue>
<contexts>
<context position="6638" citStr="Yoo et al. (2007)" startWordPosition="1037" endWordPosition="1040">mycoplasma pneumonia are two similar diseases that differ in the type of bacteria that causes them. This problem can be partially solved by dealing with concepts and semantic relations from domain-specific resources, rather than terms and lexical or syntactic relations. Consequently, some recent approaches have adapted existing methods to represent the document at a conceptual level. In particular, in the biomedical domain Reeve et al. (2007) adapt the lexical chaining approach (Barzilay and Elhadad, 1997) to work with UMLS concepts, using the MetaMap Transfer Tool to annotate these concepts. Yoo et al. (2007) represent a corpus of documents as a graph, where the nodes are the MeSH descriptors found in the corpus, and the edges represent hypernymy and co-occurrence relations between them. They cluster the MeSH concepts in the corpus to identify sets of documents dealing with the same topic and then generate a summary from each document cluster. Word sense disambiguation attempts to solve lexical ambiguities by identifying the correct meaning of a word based on its context. Supervised approaches have been shown to perform better than unsupervised ones (Agirre and Edmonds, 2006) but need large amount</context>
<context position="17467" citStr="Yoo et al., 2007" startWordPosition="2806" endWordPosition="2809">ree network (Barabasi and Albert, 1999). A scale-free network is a complex network that (among other characteristics) presents a particular type of node which are highly connected to other nodes in the network, while the remaining nodes are quite unconnected. These highestdegree nodes are often called hubs. This scalefree power-law distribution has been empirically observed in many large networks, including linguistic and semantic ones. To discover these prominent or hub nodes, we compute the salience or prestige of each vertex 58 Figure 1: Example of a simplified document graph in the graph (Yoo et al., 2007), as shown in (1). Whenever an edge from vi to vj exists, a vote from node i to node j is added with the strength of this vote depending on the weight of the edge. This ranks the nodes according to their structural importance in the graph. weight(ej) (1) bej|Dvk/fiejconnect(vi,vk) The n vertices with a highest salience are named Hub Vertices. The clustering algorithm first groups the hub vertices into Hub Vertices Sets (HVS). These can be seen as set of concepts strongly related in meaning, and will represent the centroids of the clusters. To construct these HVS, the clustering algorithm first</context>
<context position="18978" citStr="Yoo et al., 2007" startWordPosition="3066" endWordPosition="3069">tices (i.e. those not included in the HVS) are iteratively assigned to the cluster to which they are more connected. This connectivity is computed as the sum of the weights of the edges that connect the target vertex to the other vertices in the cluster. 4.4 Sentence selection The last step of the summarization process consists of computing the similarity between all sentences in the document and each of the clusters, and selecting the sentences for the summary based on these similarities. To compute the similarity between a sentence graph and a cluster, we use a nondemocratic vote mechanism (Yoo et al., 2007), so that each vertex of a sentence assigns a vote to a cluster if the vertex belongs to its HVS, half a vote if the vertex belongs to it but not to its HVS, and no votes otherwise. Finally, the similarity between the sentence and the cluster is computed as the sum of the votes assigned by all the vertices in the sentence to the cluster, as expressed in (2). similarity(Ci, Sj� = � wkJ (2) vk|vkESj � wk,j=0 if vkOCi where wk,j=1 if vkEHVS(Ci) wk,j=0.5 if vkOHVS(Ci) Finally, we select the sentences for the summary based on the similarity between them and the clusters as defined above. In previou</context>
</contexts>
<marker>Yoo, Hu, Song, 2007</marker>
<rawString>I. Yoo, X. Hu, and I-Y. Song. 2007. A coherent graph-based semantic clustering and summarization approach for biomedical literature and a new summarization evaluation method. BMC Bioinformatics, 8(9).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>