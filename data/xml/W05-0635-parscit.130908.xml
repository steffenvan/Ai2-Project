<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.047523">
<title confidence="0.987489">
Semantic Role Labeling Using Complete Syntactic Analysis
</title>
<author confidence="0.994443">
Mihai Surdeanu
</author>
<affiliation confidence="0.99736">
Technical University of Catalunya
</affiliation>
<email confidence="0.99687">
surdeanu@lsi.upc.edu
</email>
<author confidence="0.989512">
Jordi Turmo
</author>
<affiliation confidence="0.995302">
Technical University of Catalunya
</affiliation>
<email confidence="0.998323">
turmo@lsi.upc.edu
</email>
<sectionHeader confidence="0.995656" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999517846153846">
In this paper we introduce a semantic role
labeling system constructed on top of the
full syntactic analysis of text. The la-
beling problem is modeled using a rich
set of lexical, syntactic, and semantic at-
tributes and learned using one-versus-all
AdaBoost classifiers.
Our results indicate that even a simple ap-
proach that assumes that each semantic ar-
gument maps into exactly one syntactic
phrase obtains encouraging performance,
surpassing the best system that uses par-
tial syntax by almost 6%.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999818523809524">
Most current semantic role labeling (SRL) ap-
proaches can be classified in one of two classes:
approaches that take advantage of complete syntac-
tic analysis of text, pioneered by (Gildea and Juraf-
sky, 2002), and approaches that use partial syntac-
tic analysis, championed by the previous CoNLL
shared task evaluations (Carreras and M`arquez,
2004).
However, to the authors’ knowledge, a clear anal-
ysis of the benefits of using full syntactic analysis
versus partial analysis is not yet available. On one
hand, the additional information provided by com-
plete syntax should intuitively be useful. But, on
the other hand, the state-of-the-art of full parsing
is known to be less robust and perform worse than
the tools used for partial syntactic analysis, which
would decrease the quality of the information pro-
vided. The work presented in this paper contributes
to this analysis by introducing a model that is en-
tirely based on the full syntactic analysis of text,
generated by a real-world parser.
</bodyText>
<sectionHeader confidence="0.98788" genericHeader="method">
2 System Description
</sectionHeader>
<subsectionHeader confidence="0.7984255">
2.1 Mapping Arguments to Syntactic
Constituents
</subsectionHeader>
<bodyText confidence="0.999911833333334">
Our approach maps each argument label to one syn-
tactic constituent, using a strategy similar to (Sur-
deanu et al., 2003). Using a bottom-up approach,
we map each argument to the first phrase that has the
exact same boundaries and climb as high as possible
in the syntactic tree across unary production chains.
Unfortunately, this one-to-one mapping between
semantic arguments and syntactic constituents is not
always possible. One semantic argument may be
mapped to many syntactic constituents due to: (a)
intrinsic differences between the syntactic and se-
mantic representations, and (b) incorrect syntactic
structure. Figure 1 illustrates each one of these sit-
uations: Figure 1 (a) shows a sentence where each
semantic argument correctly maps to one syntac-
tic constituent; Figure 1 (b) illustrates the situation
where one semantic argument correctly maps to two
syntactic constituents; and Figure 1 (c) shows a one-
to-many mapping caused by an incorrect syntactic
structure: argument A0 maps to two phrases, the ter-
minal “by” and the noun phrase “Robert Goldberg”,
due to the incorrect attachment of the last preposi-
tional phrase, “at the University of California”.
Using the above observations, we separate one-
</bodyText>
<page confidence="0.977492">
221
</page>
<note confidence="0.5039955">
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 221–224, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<figure confidence="0.993642">
(a) (b) (c)
</figure>
<figureCaption confidence="0.9976545">
Figure 1: Mapping semantic arguments to syntactic constituents: (a) correct one–to-one mapping; (b) correct
one-to-many mapping; (c) one-to-many mapping due to incorrect syntax.
</figureCaption>
<figure confidence="0.9991197">
S
VP
developed by Robert Goldberg at the University of California
The luxury auto maker last year sold 1,214 cars in the U.S.
rising consumer prices
A0 AM−TMP P A1 AM−LOC
P A1
P A0 AM−LOC
NP
NP
VP
PP
NP
PP
NP
VBG NN NNS
NP
NP PP
NP
(a) (b) (c)
</figure>
<table confidence="0.488288">
Training 96.06% 2.49% 1.45%
Development 91.36% 4.83% 3.81%
</table>
<tableCaption confidence="0.558216">
Table 1: Distribution of semantic arguments accord-
</tableCaption>
<bodyText confidence="0.898600681818182">
ing to their mapping to syntactic constituents ob-
tained with the Charniak parser: (a) one-to-one, (b)
one-to-many, all syntactic constituents have same
parent, (c) one-to-many, syntactic constituents have
different parents.
to-many mappings in two classes: (a) when the syn-
tactic constituents mapped to the semantic argument
have the same parent (Figure 1 (b)) the mapping is
correct and/or could theoretically be learned by a
sequential SRL strategy, and (b) when the syntac-
tic constituents mapped to the same argument have
different parents, the mapping is generally caused
by incorrect syntax. Such cases are very hard to be
learned due to the irregularities of the parser errors.
Table 1 shows the distribution of semantic argu-
ments into one of the above classes, using the syn-
tactic trees provided by the Charniak parser. For the
results reported in this paper, we model only one-
to-one mappings between semantic arguments and
syntactic constituents. A subset of the one-to-many
mappings are addressed with a simple heuristic, de-
scribed in Section 2.4.
</bodyText>
<subsectionHeader confidence="0.960046">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.962118208333333">
The features incorporated in the proposed model
are inspired from the work of (Gildea and Juraf-
sky, 2002; Surdeanu et al., 2003; Pradhan et al.,
2005; Collins, 1999) and can be classified into five
classes: (a) features that capture the internal struc-
ture of the candidate argument, (b) features extracted
The syntactic label of the candidate constituent.
The constituent head word, suffixes of length 2, 3, and 4,
lemma, and POS tag.
The constituent content word, suffixes of length 2, 3, and
4, lemma, POS tag, and NE label. Content words, which
add informative lexicalized information different from
the head word, were detected using the heuristics
of (Surdeanu et al., 2003).
The first and last constituent words and their POS tags.
NE labels included in the candidate phrase.
Binary features to indicate the presence of temporal cue
words, i.e. words that appear often in AM-TMP phrases
in training.
For each TreeBank syntactic label we added a feature to
indicate the number of such labels included in the
candidate phrase.
The sequence ofsyntactic labels of the constituent
immediate children.
</bodyText>
<tableCaption confidence="0.97211975">
Table 2: Argument structure features
The phrase label, head word and POS tag of the
constituent parent, left sibling, and right sibling.
Table 3: Argument context features
</tableCaption>
<bodyText confidence="0.99906">
from the argument context, (c) features that describe
properties of the target predicate, (d) features gener-
ated from the predicate context, and (e) features that
model the distance between the predicate and the ar-
gument. These five feature sets are listed in Tables 2,
3, 4, 5, and 6.
</bodyText>
<subsectionHeader confidence="0.974594">
2.3 Classifier
</subsectionHeader>
<bodyText confidence="0.999848444444445">
The classifiers used in this paper were devel-
oped using AdaBoost with confidence rated predic-
tions (Schapire and Singer, 1999). AdaBoost com-
bines many simple base classifiers or rules (in our
case decision trees of depth 3) into a single strong
classifier using a weighted-voted scheme. Each base
classifier is learned sequentially from weighted ex-
amples and the weights are dynamically adjusted ev-
ery learning iteration based on the behavior of the
</bodyText>
<page confidence="0.9875">
222
</page>
<bodyText confidence="0.9318094">
The predicate word and lemma.
The predicate voice. We currently distinguish five voice
types: active, passive, copulative, infinitive, and progressive.
A binary feature to indicate if the predicate is frequent - i.e.
it appears more than twice in the training partition - or not.
</bodyText>
<tableCaption confidence="0.994079">
Table 4: Predicate structure features
</tableCaption>
<table confidence="0.870773333333333">
Sub-categorization rule, i.e. the phrase structure rule that
expands the predicate immediate parent, e.g.
NP --+ VBG NN NNS for the predicate in Figure 1 (b).
</table>
<tableCaption confidence="0.998842">
Table 5: Predicate context features
</tableCaption>
<bodyText confidence="0.986494741935484">
The path in the syntactic tree between the argument phrase
and the predicate as a chain of syntactic labels along with
the traversal direction (up or down).
The length of the above syntactic path.
The number of clauses (S* phrases) in the path.
The number of verb phrases (VP) in the path.
The subsumption count, i.e. the difference between the
depths in the syntactic tree of the argument and predicate
constituents. This value is 0 if the two phrases share the
same parent.
The governing category, which indicates if NP
arguments are dominated by a sentence (typical for
subjects) or a verb phrase (typical for objects).
We generalize syntactic paths with more than 3
elements using two templates:
(a) Arg I Ancestor 1 Ni 1 Pred, where Arg is the
argument label, Pred is the predicate label, Ancestor
is the label of the common ancestor, and Ni is instantiated
with all the labels between Pred and Ancestor in
the full path; and
(b) Arg I Ni I Ancestor 1 Pred, where Ni is
instantiated with all the labels between Arg and
Ancestor in the full path.
The surface distance between the predicate and the
argument phrases encoded as: the number of tokens, verb
terminals (VB*), commas, and coordinations (CC) between
the argument and predicate phrases, and a binary feature to
indicate if the two constituents are adjacent.
A binary feature to indicate if the argument starts with a
predicate particle, i.e. a token seen with the RP* POS
tag and directly attached to the predicate in training.
</bodyText>
<tableCaption confidence="0.970742">
Table 6: Predicate-argument distance features
</tableCaption>
<bodyText confidence="0.998314555555555">
previously learned rules.
We trained one-vs-all classifiers for the top 24
most common arguments in training (including
R-A* and C-A*). For simplicity we do not la-
bel predicates. Following the strategy proposed
by (Carreras et al., 2004) we select training exam-
ples (both positive and negative) only from: (a) the
first S* phrase that includes the predicate, or (b)
from phrases that appear to the left of the predicate
in the sentence. More than 98% of the arguments
fall into one of these classes.
At prediction time the classifiers are combined us-
ing a simple greedy technique that iteratively assigns
to each predicate the argument classified with the
highest confidence. For each predicate we consider
as candidates all AM attributes, but only numbered
attributes indicated in the corresponding PropBank
frame.
</bodyText>
<subsectionHeader confidence="0.996437">
2.4 Argument Expansion Heuristics
</subsectionHeader>
<bodyText confidence="0.9999935625">
We address arguments that should map to more
than one terminal phrase with the following post-
processing heuristic: if an argument is mapped to
one terminal phrase, its boundaries are extended
to the right to include all terminal phrases that are
not already labeled as other arguments for the same
predicate. For example, after the system tags “con-
sumer” as the beginning of an A1 argument in Fig-
ure 1, this heuristic extends the right boundary of
the A1 argument to include the following terminal,
“prices”.
To handle inconsistencies in the treatment of
quotes in parsing we added a second heuristic: argu-
ments are expanded to include preceding/following
quotes if the corresponding pairing quote is already
included in the argument constituent.
</bodyText>
<sectionHeader confidence="0.994809" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.993803">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999941666666667">
We trained our system using positive examples ex-
tracted from all training data available. Due to mem-
ory limitations on our development machines we
used only the first 500,000 negative examples. In the
experiments reported in this paper we used the syn-
tactic trees generated by the Charniak parser. The
results were evaluated for precision, recall, and F1
using the scoring script provided by the task orga-
nizers.
</bodyText>
<subsectionHeader confidence="0.990124">
3.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.997484888888889">
Table 7 presents the results obtained by our system.
On the WSJ data, our results surpass with almost 6%
the results obtained by the best SRL system that used
partial syntax in the CoNLL 2004 shared task eval-
uation (Hacioglu et al., 2004). Even though these
numbers are not directly comparable (this year’s
shared task offers more training data), we consider
these results encouraging given the simplicity of
our system (we essentially model only one-to-one
</bodyText>
<page confidence="0.996896">
223
</page>
<table confidence="0.999997236842105">
Precision Recall Fp=1
Development 79.14% 71.57% 75.17
Test WSJ 80.32% 72.95% 76.46
Test Brown 72.41% 59.67% 65.42
Test WSJ+Brown 79.35% 71.17% 75.04
Test WSJ Precision Recall Fp=1
Overall 80.32% 72.95% 76.46
A0 87.09% 85.21% 86.14
A1 79.80% 72.23% 75.83
A2 74.74% 58.38% 65.55
A3 83.04% 53.76% 65.26
A4 77.42% 70.59% 73.85
A5 0.00% 0.00% 0.00
AM-ADV 57.82% 46.05% 51.27
AM-CAU 49.38% 54.79% 51.95
AM-DIR 62.96% 40.00% 48.92
AM-DIS 72.19% 76.25% 74.16
AM-EXT 60.87% 43.75% 50.91
AM-LOC 64.19% 52.34% 57.66
AM-MNR 63.90% 44.77% 52.65
AM-MOD 98.09% 93.28% 95.63
AM-NEG 96.15% 97.83% 96.98
AM-PNC 55.22% 32.17% 40.66
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 79.17% 73.41% 76.18
R-A0 84.85% 87.50% 86.15
R-A1 75.00% 71.15% 73.03
R-A2 60.00% 37.50% 46.15
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 68.00% 80.95% 73.91
R-AM-MNR 30.00% 50.00% 37.50
R-AM-TMP 60.81% 86.54% 71.43
V 0.00% 0.00% 0.00
</table>
<tableCaption confidence="0.9437745">
Table 7: Overall results (top) and detailed results on
the WSJ test (bottom).
</tableCaption>
<bodyText confidence="0.9639436">
mappings between semantic arguments and syntac-
tic constituents). Only 0.14% out of the 75.17% F
measure obtained on the development partition are
attributed to the argument expansion heuristics in-
troduced in Section 2.4.
</bodyText>
<sectionHeader confidence="0.999723" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999962166666667">
This paper describes a semantic role labeling sys-
tem constructed on top of the complete syntactic
analysis of text. We model semantic arguments that
map into exactly one syntactic phrase (about 90%
of all semantic arguments in the development set)
using a rich set of lexical, syntactic, and semantic
attributes. We trained AdaBoost one-versus-all clas-
sifiers for the 24 most common argument types. Ar-
guments that map to more than one syntactic con-
stituent are expanded with a simple heuristic in a
post-processing step.
Our results surpass with almost 6% the results ob-
tained by best SRL system that used partial syntax in
the CoNLL 2004 shared task evaluation. Although
the two evaluations are not directly comparable due
to differences in training set size, the current results
are encouraging given the simplicity of our proposed
system.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999565555555556">
This research has been partially funded by the Euro-
pean Union project “Computers in the Human Inter-
action Loop” (CHIL - IP506909). Mihai Surdeanu is
a research fellow within the Ram´on y Cajal program
of the Spanish Ministry of Education and Science.
We would also like to thank Lluis M`arquez and
Xavi Carreras for the help with the AdaBoost classi-
fier, for providing the set of temporal cue words, and
for the many motivating discussions.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999865">
X. Carreras and L. M`arquez. 2004. Introduction to the CoNLL-
2004 shared task: Semantic role labeling. In Proceedings of
CoNLL 2004 Shared Task.
X. Carreras, L. M`arquez, and G. Chrupała. 2004. Hierarchical
recognition of propositional arguments with perceptrons. In
Proceedings of CoNLL 2004 Shared Task.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. PhD Dissertation, University of Penn-
sylvania.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3).
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and D. Ju-
rafsky. 2004. Semantic role labeling by tagging syntactic
chunks. In Proceedings of CoNLL 2004 Shared Task.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin,
and D. Jurafsky. 2005. Support vector learning for semantic
argument classification. To appear in Journal of Machine
Learning.
R. E. Schapire and Y. Singer. 1999. Improved boosting algo-
rithms using confidence-rated predictions. Machine Learn-
ing, 37(3).
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003.
Using predicate-argument structures for information extrac-
tion. In Proceedings ofACL 2003.
</reference>
<page confidence="0.99871">
224
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.865108">
<title confidence="0.999775">Semantic Role Labeling Using Complete Syntactic Analysis</title>
<author confidence="0.944566">Mihai</author>
<affiliation confidence="0.995846">Technical University of</affiliation>
<email confidence="0.997681">surdeanu@lsi.upc.edu</email>
<author confidence="0.983654">Jordi Turmo</author>
<affiliation confidence="0.998366">Technical University of Catalunya</affiliation>
<email confidence="0.999266">turmo@lsi.upc.edu</email>
<abstract confidence="0.995466642857143">In this paper we introduce a semantic role labeling system constructed on top of the full syntactic analysis of text. The labeling problem is modeled using a rich set of lexical, syntactic, and semantic attributes and learned using one-versus-all AdaBoost classifiers. Our results indicate that even a simple approach that assumes that each semantic argument maps into exactly one syntactic phrase obtains encouraging performance, surpassing the best system that uses partial syntax by almost 6%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the CoNLL2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<note>Shared Task.</note>
<marker>Carreras, M`arquez, 2004</marker>
<rawString>X. Carreras and L. M`arquez. 2004. Introduction to the CoNLL2004 shared task: Semantic role labeling. In Proceedings of CoNLL 2004 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
<author>G Chrupała</author>
</authors>
<title>Hierarchical recognition of propositional arguments with perceptrons.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<note>Shared Task.</note>
<marker>Carreras, M`arquez, Chrupała, 2004</marker>
<rawString>X. Carreras, L. M`arquez, and G. Chrupała. 2004. Hierarchical recognition of propositional arguments with perceptrons. In Proceedings of CoNLL 2004 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>PhD</tech>
<institution>Dissertation, University of Pennsylvania.</institution>
<contexts>
<context position="4940" citStr="Collins, 1999" startWordPosition="776" endWordPosition="777"> to be learned due to the irregularities of the parser errors. Table 1 shows the distribution of semantic arguments into one of the above classes, using the syntactic trees provided by the Charniak parser. For the results reported in this paper, we model only oneto-one mappings between semantic arguments and syntactic constituents. A subset of the one-to-many mappings are addressed with a simple heuristic, described in Section 2.4. 2.2 Features The features incorporated in the proposed model are inspired from the work of (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2005; Collins, 1999) and can be classified into five classes: (a) features that capture the internal structure of the candidate argument, (b) features extracted The syntactic label of the candidate constituent. The constituent head word, suffixes of length 2, 3, and 4, lemma, and POS tag. The constituent content word, suffixes of length 2, 3, and 4, lemma, POS tag, and NE label. Content words, which add informative lexicalized information different from the head word, were detected using the heuristics of (Surdeanu et al., 2003). The first and last constituent words and their POS tags. NE labels included in the c</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. PhD Dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="917" citStr="Gildea and Jurafsky, 2002" startWordPosition="133" endWordPosition="137">yntactic analysis of text. The labeling problem is modeled using a rich set of lexical, syntactic, and semantic attributes and learned using one-versus-all AdaBoost classifiers. Our results indicate that even a simple approach that assumes that each semantic argument maps into exactly one syntactic phrase obtains encouraging performance, surpassing the best system that uses partial syntax by almost 6%. 1 Introduction Most current semantic role labeling (SRL) approaches can be classified in one of two classes: approaches that take advantage of complete syntactic analysis of text, pioneered by (Gildea and Jurafsky, 2002), and approaches that use partial syntactic analysis, championed by the previous CoNLL shared task evaluations (Carreras and M`arquez, 2004). However, to the authors’ knowledge, a clear analysis of the benefits of using full syntactic analysis versus partial analysis is not yet available. On one hand, the additional information provided by complete syntax should intuitively be useful. But, on the other hand, the state-of-the-art of full parsing is known to be less robust and perform worse than the tools used for partial syntactic analysis, which would decrease the quality of the information pr</context>
<context position="4879" citStr="Gildea and Jurafsky, 2002" startWordPosition="763" endWordPosition="767">apping is generally caused by incorrect syntax. Such cases are very hard to be learned due to the irregularities of the parser errors. Table 1 shows the distribution of semantic arguments into one of the above classes, using the syntactic trees provided by the Charniak parser. For the results reported in this paper, we model only oneto-one mappings between semantic arguments and syntactic constituents. A subset of the one-to-many mappings are addressed with a simple heuristic, described in Section 2.4. 2.2 Features The features incorporated in the proposed model are inspired from the work of (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2005; Collins, 1999) and can be classified into five classes: (a) features that capture the internal structure of the candidate argument, (b) features extracted The syntactic label of the candidate constituent. The constituent head word, suffixes of length 2, 3, and 4, lemma, and POS tag. The constituent content word, suffixes of length 2, 3, and 4, lemma, POS tag, and NE label. Content words, which add informative lexicalized information different from the head word, were detected using the heuristics of (Surdeanu et al., 2003). The first and last cons</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hacioglu</author>
<author>S Pradhan</author>
<author>W Ward</author>
<author>J H Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Semantic role labeling by tagging syntactic chunks.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<note>Shared Task.</note>
<contexts>
<context position="11151" citStr="Hacioglu et al., 2004" startWordPosition="1791" endWordPosition="1794">from all training data available. Due to memory limitations on our development machines we used only the first 500,000 negative examples. In the experiments reported in this paper we used the syntactic trees generated by the Charniak parser. The results were evaluated for precision, recall, and F1 using the scoring script provided by the task organizers. 3.2 Results and Discussion Table 7 presents the results obtained by our system. On the WSJ data, our results surpass with almost 6% the results obtained by the best SRL system that used partial syntax in the CoNLL 2004 shared task evaluation (Hacioglu et al., 2004). Even though these numbers are not directly comparable (this year’s shared task offers more training data), we consider these results encouraging given the simplicity of our system (we essentially model only one-to-one 223 Precision Recall Fp=1 Development 79.14% 71.57% 75.17 Test WSJ 80.32% 72.95% 76.46 Test Brown 72.41% 59.67% 65.42 Test WSJ+Brown 79.35% 71.17% 75.04 Test WSJ Precision Recall Fp=1 Overall 80.32% 72.95% 76.46 A0 87.09% 85.21% 86.14 A1 79.80% 72.23% 75.83 A2 74.74% 58.38% 65.55 A3 83.04% 53.76% 65.26 A4 77.42% 70.59% 73.85 A5 0.00% 0.00% 0.00 AM-ADV 57.82% 46.05% 51.27 AM-CAU</context>
</contexts>
<marker>Hacioglu, Pradhan, Ward, Martin, Jurafsky, 2004</marker>
<rawString>K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and D. Jurafsky. 2004. Semantic role labeling by tagging syntactic chunks. In Proceedings of CoNLL 2004 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>K Hacioglu</author>
<author>V Krugler</author>
<author>W Ward</author>
<author>J H Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<journal>Journal of Machine Learning.</journal>
<note>To appear in</note>
<contexts>
<context position="4924" citStr="Pradhan et al., 2005" startWordPosition="772" endWordPosition="775">ch cases are very hard to be learned due to the irregularities of the parser errors. Table 1 shows the distribution of semantic arguments into one of the above classes, using the syntactic trees provided by the Charniak parser. For the results reported in this paper, we model only oneto-one mappings between semantic arguments and syntactic constituents. A subset of the one-to-many mappings are addressed with a simple heuristic, described in Section 2.4. 2.2 Features The features incorporated in the proposed model are inspired from the work of (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2005; Collins, 1999) and can be classified into five classes: (a) features that capture the internal structure of the candidate argument, (b) features extracted The syntactic label of the candidate constituent. The constituent head word, suffixes of length 2, 3, and 4, lemma, and POS tag. The constituent content word, suffixes of length 2, 3, and 4, lemma, POS tag, and NE label. Content words, which add informative lexicalized information different from the head word, were detected using the heuristics of (Surdeanu et al., 2003). The first and last constituent words and their POS tags. NE labels i</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin, and D. Jurafsky. 2005. Support vector learning for semantic argument classification. To appear in Journal of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved boosting algorithms using confidence-rated predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="6476" citStr="Schapire and Singer, 1999" startWordPosition="1022" endWordPosition="1025">ituent immediate children. Table 2: Argument structure features The phrase label, head word and POS tag of the constituent parent, left sibling, and right sibling. Table 3: Argument context features from the argument context, (c) features that describe properties of the target predicate, (d) features generated from the predicate context, and (e) features that model the distance between the predicate and the argument. These five feature sets are listed in Tables 2, 3, 4, 5, and 6. 2.3 Classifier The classifiers used in this paper were developed using AdaBoost with confidence rated predictions (Schapire and Singer, 1999). AdaBoost combines many simple base classifiers or rules (in our case decision trees of depth 3) into a single strong classifier using a weighted-voted scheme. Each base classifier is learned sequentially from weighted examples and the weights are dynamically adjusted every learning iteration based on the behavior of the 222 The predicate word and lemma. The predicate voice. We currently distinguish five voice types: active, passive, copulative, infinitive, and progressive. A binary feature to indicate if the predicate is frequent - i.e. it appears more than twice in the training partition - </context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>R. E. Schapire and Y. Singer. 1999. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>S Harabagiu</author>
<author>J Williams</author>
<author>P Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="1895" citStr="Surdeanu et al., 2003" startWordPosition="290" endWordPosition="294">x should intuitively be useful. But, on the other hand, the state-of-the-art of full parsing is known to be less robust and perform worse than the tools used for partial syntactic analysis, which would decrease the quality of the information provided. The work presented in this paper contributes to this analysis by introducing a model that is entirely based on the full syntactic analysis of text, generated by a real-world parser. 2 System Description 2.1 Mapping Arguments to Syntactic Constituents Our approach maps each argument label to one syntactic constituent, using a strategy similar to (Surdeanu et al., 2003). Using a bottom-up approach, we map each argument to the first phrase that has the exact same boundaries and climb as high as possible in the syntactic tree across unary production chains. Unfortunately, this one-to-one mapping between semantic arguments and syntactic constituents is not always possible. One semantic argument may be mapped to many syntactic constituents due to: (a) intrinsic differences between the syntactic and semantic representations, and (b) incorrect syntactic structure. Figure 1 illustrates each one of these situations: Figure 1 (a) shows a sentence where each semantic </context>
<context position="4902" citStr="Surdeanu et al., 2003" startWordPosition="768" endWordPosition="771">by incorrect syntax. Such cases are very hard to be learned due to the irregularities of the parser errors. Table 1 shows the distribution of semantic arguments into one of the above classes, using the syntactic trees provided by the Charniak parser. For the results reported in this paper, we model only oneto-one mappings between semantic arguments and syntactic constituents. A subset of the one-to-many mappings are addressed with a simple heuristic, described in Section 2.4. 2.2 Features The features incorporated in the proposed model are inspired from the work of (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2005; Collins, 1999) and can be classified into five classes: (a) features that capture the internal structure of the candidate argument, (b) features extracted The syntactic label of the candidate constituent. The constituent head word, suffixes of length 2, 3, and 4, lemma, and POS tag. The constituent content word, suffixes of length 2, 3, and 4, lemma, POS tag, and NE label. Content words, which add informative lexicalized information different from the head word, were detected using the heuristics of (Surdeanu et al., 2003). The first and last constituent words and their</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceedings ofACL 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>