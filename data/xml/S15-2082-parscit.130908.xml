<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000058">
<title confidence="0.856089">
SemEval-2015 Task 12: Aspect Based Sentiment Analysis
</title>
<author confidence="0.9371075">
Maria Pontiki*, Dimitrios Galanis*, Haris Papageorgiou*,
Suresh Manandhar±, Ion Androutsopoulos°*
</author>
<affiliation confidence="0.866588">
*Institute for Language and Speech Processing, Athena R.C., Athens, Greece
± Dept. of Computer Science, University of York, UK
° Dept. of Informatics, Athens University of Economics and Business, Greece
</affiliation>
<email confidence="0.925943333333333">
{mpontiki, galanisd, xaris} @ilsp.gr
suresh@cs.york.ac.uk
ion@aueb.gr
</email>
<sectionHeader confidence="0.994909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997052363636364">
SemEval-2015 Task 12, a continuation of
SemEval-2014 Task 4, aimed to foster re-
search beyond sentence- or text-level senti-
ment classification towards Aspect Based
Sentiment Analysis. The goal is to identify
opinions expressed about specific entities
(e.g., laptops) and their aspects (e.g., price).
The task provided manually annotated reviews
in three domains (restaurants, laptops and ho-
tels), and a common evaluation procedure. It
attracted 93 submissions from 16 teams.
</bodyText>
<sectionHeader confidence="0.969636" genericHeader="categories and subject descriptors">
1 Introduction and Related Work
</sectionHeader>
<bodyText confidence="0.999594714285714">
The rise of e-commerce, as a new shopping and
marketing channel, has led to an upsurge of review
sites for a variety of services and products. In this
context, Aspect Based Sentiment Analysis (ABSA)
-i.e., mining opinions from text about specific enti-
ties and their aspects- can help consumers decide
what to purchase and businesses to better monitor
their reputation and understand the needs of the
market (Pavlopoulos 2014). Given a target of in-
terest (e.g., Apple Mac mini), an ABSA method
can summarize the content of the respective re-
views in an aspect-sentiment table like the one in
Fig 1. Some review sites also generate such tables
based on customer ratings, but usually only for a
limited set of predefined aspects and not from free-
text reviews.
Several ABSA methods have been proposed for
various domains, like consumer electronics (Hu
and Liu {2004a, 2004b}), restaurants (Ganu et al.,
2009) and movies (Thet et al., 2010). The available
methods can be divided into those that adopt do-
main-independent solutions (Lin and He, 2009),
and those that use domain-specific knowledge to
improve their results (Thet et al., 2010). Typically,
most methods treat aspect extraction and sentiment
classification separately (Brody and Elhadad,
2010), but there are also approaches that model the
two problems jointly (Jo and Oh, 2011).
</bodyText>
<figureCaption confidence="0.9832435">
Figure 1. Table summarizing the average sentiment for
each aspect of an entity.
</figureCaption>
<bodyText confidence="0.998027625">
Publicly available ABSA datasets adopt differ-
ent annotation schemes for different subtasks and
languages (Pavlopoulos 2014). For example, the
datasets of McAuley et al. (2012) provide aspects
and respective ratings at the review level (i.e., as-
pects and ratings associated with entire reviews,
not particular sentences)1 about Beers, Pubs, Toys
and Games, and Audiobooks. The reviews are ob-
tained from sites that allow users to evaluate a
product not only in terms of its overall quality, but
also focusing on specific predefined aspects (e.g.
“smell” and “taste” for Beers, “fun” and “educa-
tional value” for Toys and Games). The IGGSA
Shared Tasks on German Sentiment Analysis
(Ruppenhofer et al., 2014) provided human anno-
tated datasets of political speeches (STEPS task)
</bodyText>
<footnote confidence="0.925594">
1 A subset of the datasets has been annotated with aspects at
the sentence level.
</footnote>
<page confidence="0.950178">
486
</page>
<note confidence="0.9539765">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 486–495,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999944617021276">
and reviews about products (StAR task) like coffee
machines and washers. The StAR task focused on
the extraction of evaluative phrases (e.g., “bad”)
and aspect expressions (e.g., “washer”). The
STEPS dataset includes annotations for evaluative
phrases, opinion targets, and the corresponding
sources (opinion holders). The extraction of opin-
ion targets and holders has also been addressed in
the context of the Multilingual Opinion Analysis
Task (Seki et al., 2007; Seki et al., 2008; Seki et
al., 2010) and the Sentiment Slot Filling2 Task of
the Knowledge Base Population Track (Mitchell,
2013). However, these tasks deal with the identifi-
cation of opinion targets in general, not in the con-
text of ABSA.
SemEval-2014 Task 4 (SE-ABSA14) provided
datasets annotated with aspect terms (e.g., “hard
disk”, “pizza”) and their polarity for laptop and
restaurant reviews, as well as coarser aspect cate-
gories (e.g., PRICE) and their polarity only for res-
taurants3 (Pontiki et al., 2014). The task attracted
165 submissions from 32 teams that experimented
with a variety of features (e.g., based on n-grams,
parse trees, named entities, word clusters), tech-
niques (e.g., rule-based, supervised and unsuper-
vised learning), and resources (e.g., sentiment
lexica, Wikipedia, WordNet). The participants ob-
tained higher scores in the restaurants domain. The
laptops domain proved to be harder involving more
entities (e.g., hardware and software components)
and complex concepts (e.g., usability, portability)
that are often discussed implicitly in the text. The
SE-ABSA14 task set-up has been adopted for the
creation of aspect-level sentiment datasets in other
languages, like Czech (Steinberger et al., 2014).
SemEval-2015 Task 12 (SE-ABSA15) built up-
on SE-ABSA14 and consolidated its subtasks (as-
pect category extraction, aspect term extraction,
polarity classification) into a principled unified
framework (described in Section 2). In addition,
SE-ABSA15 included an aspect level polarity clas-
sification subtask for the hotels domain in which
no training data were provided (out-of-domain
ABSA). The annotation schema and the provided
datasets are described in Section 3. The evaluation
measures and the baseline methods are described
in Section 4, while the evaluation scores and the
</bodyText>
<footnote confidence="0.980355">
2 http://www.nist.gov/tac/2014/KBP/Sentiment/index.html
3 The SE-ABSA14 inventory of categories for the restaurants
domain is similar to the one of Ganu et al. (2009).
</footnote>
<bodyText confidence="0.997019666666667">
main characteristics of the developed systems are
presented in Section 5. The paper concludes with a
general assessment of the task.
</bodyText>
<sectionHeader confidence="0.996816" genericHeader="general terms">
2 Task Set-Up
</sectionHeader>
<subsectionHeader confidence="0.96328">
2.1 ABSA Framework: From SE-ABSA14 to
SE-ABSA15
</subsectionHeader>
<bodyText confidence="0.999880642857143">
In SE-ABSA14, given a sentence from a user re-
view about a target entity e (e.g., a laptop), the goal
was to identify all aspects (explicit terms or cate-
gories) and the corresponding polarities. Following
Liu (2006) &amp; Zhang and Liu (2014), an aspect
(term or category) indicated: (a) a part/component
of e (e.g., battery), (b) an attribute of e (e.g., price),
or (c) an attribute of a part/component of e (e.g.,
battery life). In SE-ABSA15, an aspect category is
defined as a combination of an entity type E and an
attribute type A. This definition of aspect makes
more explicit the difference between entities and
the particular facets that are being evaluated. E can
be the reviewed entity e itself (e.g., laptop), a
part/component of it (e.g., battery or customer sup-
port), or another relevant entity (e.g., the manufac-
turer of e), while A is a particular attribute (e.g.,
durability, quality) of E. E and A are concept names
(classes) from a given domain ontology and do not
necessarily occur as terms in a sentence. For ex-
ample, in “They sent it back with a huge crack in it
and it still didn&apos;t work; and that was the fourth
time I’ve sent it to them to get fixed” the reviewer
is evaluating the quality (A) of the customer sup-
port (E) without explicitly mentioning it.
In contrast to SE-ABSA14, in the current
framework aspect terms correspond to explicit
mentions of the entities E (e.g., service, pizza) or
attributes A (e.g., price, quality). However, only the
extraction of the explicit mentions of E is required
(see Section 2.2). Another difference is that the
datasets of SE-ABSA15 consist of whole reviews,
not isolated sentences. Correctly identifying the E,
A pairs of a sentence and their polarities often re-
quires examining a wider part or the whole review.
In this setting, the ABSA problem has been for-
malized into a principled unified framework in
which all the identified constituents of the ex-
pressed opinions (i.e., opinion target expressions,
aspects and sentiment polarities) meet a set of
guidelines/specifications and are linked to each
other within tuples. The extracted tuples directly
</bodyText>
<page confidence="0.997449">
487
</page>
<bodyText confidence="0.99759525">
reflect the intended meaning of the texts and, thus,
can be used to generate structured aspect-based
opinion summaries from user reviews in realistic
applications (e.g., review sites).
</bodyText>
<subsectionHeader confidence="0.998564">
2.2 Task Description
</subsectionHeader>
<bodyText confidence="0.967018515151515">
SE-ABSA15 consisted of the following subtasks.
Participants were free to choose the subtasks, slots
and domains they wished to participate in.
Subtask 1: In-domain ABSA. Given a review
text about a laptop or restaurant, identify all the
opinion tuples with the following types (tuple
slots) of information:
Slot 1: Aspect Category. The goal is to identify
every entity E and attribute A pair towards which
an opinion is expressed in the given text. E and A
should be chosen from predefined inventories of
entity types (e.g., LAPTOP, MOUSE, RESTAURANT,
FOOD) and attribute labels (e.g., DESIGN, PRICE,
QUALITY). The E, A inventories for each domain
are described in section 3.
Slot 2: Opinion Target Expression (OTE).
The task is to extract the OTE, i.e., the linguistic
expression used in the given text to refer to the
reviewed entity E of each E#A pair. The OTE is
defined by its starting and ending offsets. When
there is no explicit mention of the entity, the slot
takes the value “NULL”. The identification of Slot 2
values was required only in the restaurants domain.
Slot 3: Sentiment Polarity. Each identified
E#A pair has to be assigned one of the following
polarity labels: positive, negative, neutral (mildly
positive or mildly negative sentiment).
Two examples of opinion tuples with Slot 1-3
values from the restaurants domain are shown be-
low. Such tuples can be used to generate aspect-
sentiment tables like the one of Fig 1.
a. The food was delicious but do not come here
on an empty stomach. →
</bodyText>
<construct confidence="0.544493">
{category= “FOOD#QUALITY”, target= “food”,
from: “4”, to: “8”, polarity= “positive”},
{category= “FOOD#STYLE_OPTIONS”4, target =
“food”, from: “4”, to: “8”, polarity= “negative”}
b. Prices are in line. →
{category: “RESTAURANT#PRICES”, target= “NULL”,
from: “-”, to: “-”, polarity: “neutral”}
</construct>
<footnote confidence="0.862736">
4 Opinions evaluating the food quantity (e.g. portions size) are
assigned the label “FOOD#STYLE_OPTIONS”.
</footnote>
<bodyText confidence="0.985252857142857">
Subtask 2: Out-of-domain ABSA. In this sub-
task, participants had the opportunity to test their
systems in a previously unseen domain (hotel re-
views) for which no training data was made avail-
able. The gold annotations for Slots 1 and 2 were
provided and the teams had to return the sentiment
polarity values (Slot 3).
</bodyText>
<sectionHeader confidence="0.980116" genericHeader="keywords">
3 Datasets and Annotation
</sectionHeader>
<subsectionHeader confidence="0.997627">
3.1 Data Collection
</subsectionHeader>
<bodyText confidence="0.99769">
Datasets for three domains (laptops, restaurants,
hotels) were provided; consult Table 1 for more
information.
</bodyText>
<table confidence="0.990089285714286">
Laptops Restaurants Hotels
Training data
Review texts 277 254 -
Sentences 1739 1315 -
Test data
Review texts 173 96 30
Sentences 761 685 266
</table>
<tableCaption confidence="0.999888">
Table 1. Datasets provided for ABSA.
</tableCaption>
<bodyText confidence="0.993488">
Note that in the domain of hotels no training da-
ta were provided (Out-of-Domain ABSA).
</bodyText>
<subsectionHeader confidence="0.999982">
3.2 Annotation Schema and Guidelines
</subsectionHeader>
<bodyText confidence="0.999988">
Given a review text about a laptop, a restaurant or
a hotel, the task of the annotators was to identify
opinions expressed towards specific entities and
their attributes and to assign the respective aspect
category (Slot 1) and polarity (Slot 3) labels. The
category (E#A) values had to be chosen from pre-
defined inventories of entities and attributes for
each domain; the inventories were described in
detail in the respective annotation guidelines5. In
particular, the entity E could be assigned 22 possi-
ble labels for the laptops domain (e.g., LAPTOP,
SOFTWARE, SUPPORT), 6 labels for the restaurants
domain (e.g., RESTAURANT, FOOD), and 7 labels
for the hotels domain (e.g., HOTEL, ROOMS). The
attribute A could be assigned 9 possible labels for
the laptops domain (e.g., USABILITY), 5 labels for
the restaurants domain (e.g., QUALITY), and 8 la-
bels for the hotels domain (e.g., COMFORT). The
</bodyText>
<footnote confidence="0.463480333333333">
5 The detailed annotation guidelines are available at:
http://alt.qcri.org/semeval2015/task12/index.php?id=data-and-
tools
</footnote>
<page confidence="0.9954">
488
</page>
<bodyText confidence="0.999946426470588">
full inventories of the aspect category labels for
each domain are provided below in appendices A-
C. Quite often reviews contain opinions towards
entities that are not directly related to the entity
being reviewed, for example, restaurants/hotels
that the reviewer has visited in the past, other lap-
tops or products (and their components) of the
same or a competitive brand. Such entities as well
as comparative opinions are considered to be out of
the scope of SE-ABSA15. In these cases, no opin-
ion annotations were provided.
The {E#A, polarity} annotations had to be as-
signed at the sentence level taking into account the
context of the whole review. For example, “Laptop
still did not work, blue screen within a week...”
(Previous sentence: “Horrible customer support-
they lost my laptop for a month-got it back 3
months later”) had to be assigned a negative opin-
ion about the customer support, not about the oper-
ation of the laptop, as implied by the previous
sentence. Similarly, in “I was so happy with my
new Mac.” (Next sentences: “For two months...
Then the hard drive failed.”), even though the re-
viewer says how happy he/she was with the laptop,
he/she is expressing a negative opinion.
For the polarity slot the possible values were:
positive, negative, and neutral. Contrary to SE-
ABSA14, the “neutral” label applies only to mildly
positive or mildly negative sentiment, thus it does
not indicate objectivity (e.g., “Food was okay,
nothing great.” — {FOOD#QUALITY, “Food”, neu-
tral}). Another difference is that this year the “con-
flict” label was not used, since –due to the adopted
fine-grained aspect classification schema– it is
very rare to encounter (in a sentence) both a posi-
tive and a negative opinion about the same attrib-
ute A of an entity E. In the few cases where this
happened, the dominant sentiment was chosen
(e.g., “The OS takes some getting used to but the
learning curve is so worth it!” — {OS#USABILITY,
positive}).
For the restaurants and the hotels domain the
annotators also had to tag the OTE (explicit men-
tion) for each identified entity E (Slot 2). Such
mentions can be named entities (e.g., “The Four
Seasons”), common nouns (e.g., “place”, “steak”,
“bed”) or multi-word terms (e.g., “vitello alla mar-
sala”, “conference/banquet room”). Similarly to
SE-ABSA14, the identified OTEs were annotated
as they appeared, even if misspelled. When an
evaluated entity E was only implicitly inferred or
referred to (e.g., through pronouns), the OTE slot
was assigned the value “NULL” (e.g. “Everything
was wonderful.” — {RESTAURANT#GENERAL,
NULL, positive}).
In the laptops domain we did not provide OTE
annotations, since most entities are instantiated
through a limited set of expressions (e.g.,
MEMORY: “memory”, “ram”, CPU: “processing
power”, “processor”, “cpu”) as opposed to the res-
taurants domain, where for example, the entity
“FOOD” is instantiated through a variety of food
types and dishes (e.g. “pizza”, “Lobster Cobb Sal-
ad”). Furthermore, LAPTOP, which is the majority
category label in laptops (see Section 3.3), is in-
stantiated mostly through pronominal mentions,
while the explicit mentions are limited to nouns
like laptop, computer, product, etc.
</bodyText>
<subsectionHeader confidence="0.999215">
3.3 Annotation Process and Statistics
</subsectionHeader>
<bodyText confidence="0.999416888888889">
Each dataset was annotated by a linguist (annotator
A) using BRAT (Stenetorp et al., 2012), a web-
based annotation tool, which was configured ap-
propriately for the needs of the task. Then, one of
the organizers (annotator B) validated/inspected
the resulting annotations. When B was not confi-
dent or disagreed with A, a decision was made col-
laboratively between them and a third annotator.
The main disagreements encountered during the
annotation process are summarized below:
Slot 1. In the laptops domain the main difficulty
was that in some negative evaluations the annota-
tors were unsure about the actual problem/target.
For example, in “Sometimes the screen even goes
black on this computer”, the black screen may be
related to the graphics, the laptop operation (e.g.,
motherboard issue) or the screen itself. The deci-
sion for such cases was to assign the E#A pair that
reflected what the reviewer is saying and not the
possible interpretations that a technician would
give. So, if someone reports screen issues without
providing further details, then the opinion is con-
sidered to be about the screen6. Another issue was
when an attribute could be inferred from an explic-
itly evaluated attribute. For example, DESIGN af-
fects USABILITY (e.g., “With the switch being at
the top you need to memorize the key combination
</bodyText>
<footnote confidence="0.764256">
6 “Blue screen” is an exception since it is well-known that it
refers to the laptop operation.
</footnote>
<page confidence="0.994626">
489
</page>
<figure confidence="0.7170575">
35.13%
32.07% 17.40% 20.71%
16.26% 16.20%
9.11%
11.06%
4.73%
5.62%
3.75% 3.26% 2.90%
4.50% 3.67% 4.14%
Train Test
1.42% 0.71% 0.95% 0.59%
2.06% 1.57% 1.21% 0.91%
</figure>
<figureCaption confidence="0.9882475">
Figure 2. Aspect category (E#A) distribution in the restaurants domain. REST = restaurant, SERV = service,
AMB = ambience, LOC = location, GEN=general, PRIC = price, S&amp;O = style&amp;options, MISC= miscellaneous
Figure 3. LAPTOP#ATTRIBUTE categories distribution in the laptops domain. LP= laptop, O&amp;P= operation
&amp;performance, QUAL= quality, D&amp;F= design &amp;features, USAB=usability, CONN=connectivity, PORT=portability.
</figureCaption>
<figure confidence="0.998090785714286">
7.70% 5.27%
7.59% 4.11%
6.74% 6.32%
4.61% 3.85%
22.55%
LP#GEN LP#D&amp;F LP#O&amp;P LP#QUAL LP#USAB LP#MISC LP#PRIC LP#PORT LP#CONN
Train Test
2.23%
2.53%
1.62%
0.84%
20.92%
9.59% 12.54%
8.21% 7.95%
</figure>
<bodyText confidence="0.964118804878049">
rather than just flicking a switch”). In such cases
annotators assigned both attribute labels. The an-
notation in the restaurants domain was easier, due
to the less fine-grained schema. A common prob-
lem was that (as in SE-ABSA14) the distinction
between the GENERAL and MISCELLANEOUS and
between the RESTAURANT and AMBIENCE labels
was not always clear.
Slot 2. The annotators found it easier to identify
explicit references to the target entities as opposed
to the more general aspect terms of SE-ABSA14.
However, the problem of distinguishing aspect
terms when they appear in conjunctions or disjunc-
tions remains. In this case the maximal phrase (e.g.
the entire conjunction or disjunction) is annotated
(e.g. “Greek or Cypriot dishes” instead of “Greek
dishes”, “Cypriot dishes”).
Slot 3. Most cases in which the annotators had
difficulty deciding the correct polarity label fall
into one of the following categories: (a) Change of
sentiment over time. Some reviewers tend to start
their review by saying how excited they were at
first (e.g., with the laptop) and continue by report-
ing problems or negative evaluations. (b) Negative
fact vs. positive opinion. Some reviewers do men-
tion particular deficiencies of a laptop or a restau-
rant saying, however, at the same time that they do
not bother (e.g., “Overheats but put a pillow and
problem solved!”). (c) Mildly positive and negative
sentiments are both denoted by the “neutral” la-
bel. In some cases the annotators reported that it
would be helpful to have a more fine-grained
schema (e.g., “negative”, “somewhat negative”,
“neutral”, “somewhat positive”, “positive”). Final-
ly, in some cases it is difficult to decide a polarity
label without knowing the reviewer’s intention
(e.g., “50% of the food was very good”).
The annotation process resulted in 5,761 opinion
tuples in total that correspond to more than 15,000
label assignments (E, A, OTE, polarity); consult
Table 2 for more information.
</bodyText>
<table confidence="0.998889666666667">
Laptops
training test total
{E#A, polarity} 1974 949 2923
Restaurants
training test total
{E#A, OTE, polarity} 1654 845 2499
Hotels
training test total
{E#A, OTE, polarity} - 339 339
</table>
<tableCaption confidence="0.999808">
Table 2. Number of tuples annotated per dataset.
</tableCaption>
<page confidence="0.995887">
490
</page>
<bodyText confidence="0.999960619047619">
The distribution of the category annotations in
the restaurants domain (Fig. 2) is similar across the
training and test set. In the laptops domain, 81 E,A
combinations (different pairs) were annotated in
the training set and 58 in the test set. LAPTOP is the
majority entity class in both sets; 62.36% in train-
ing, 72.81% in test data. Figure 3 presents the dis-
tribution for all the attributes of the LAPTOP entity
in the training and test sets. Again, the category
distributions are similar. The remaining 37.64% of
the annotations in the laptops training data corre-
spond to 72 categories with frequencies ranging
from 6.53% to 0.05%. In the test set, the remaining
27.19% of the annotations correspond to 49 cate-
gories with frequencies from 2.32 % to 0.11%.
Regarding polarity, positive is the majority class
in all domains (Table 3). The polarity distribution
is balanced in the laptops domain, while in the res-
taurants domain there is a significant imbalance
between the positive and negative classes across
the training and the test sets.
</bodyText>
<table confidence="0.999350666666667">
positive negative neutral
RS-TR 72.43% 24.36% 3.20%
RS-TE 53.72% 40.96% 5.32%
LP-TR 55.87% 38.75% 5.36%
LP-TE 57% 34.66% 8.32%
HT-TE 71.68% 24.77% 3.53%
</table>
<tableCaption confidence="0.969346">
Table 3. Polarity distribution per domain (RS-
restaurants, LP-laptops, HT-hotels). TR and TE indicate
the training and test sets.
</tableCaption>
<subsectionHeader confidence="0.992962">
3.4 Datasets Format and Availability
</subsectionHeader>
<bodyText confidence="0.999596833333333">
The datasets7 of the SE-ABSA15 task were pro-
vided in an XML format. They are available under
a non-commercial, no redistribution license
through META-SHARE8, a repository devoted to
the sharing and dissemination of language re-
sources (Piperidis, 2012).
</bodyText>
<sectionHeader confidence="0.976653" genericHeader="introduction">
4 Evaluation Measures and Baselines
</sectionHeader>
<bodyText confidence="0.9999426">
Similarly to SE-ABSA14, the evaluation ran in two
phases. In Phase A, the participants were asked to
return the {category, OTE} tuples for the restau-
rants domain and only the category slot (Slot1) for
the laptops domain. Subsequently, in Phase B, the
</bodyText>
<footnote confidence="0.962603">
7 The data are available at http://metashare.ilsp.gr:8080/.
8 META-SHARE (http: //www.metashare.org/) was
implemented in the framework of the META-NET Network of
Excellence (http://www.meta-net.eu/).
</footnote>
<bodyText confidence="0.99997325">
participants were given the gold annotations for the
reviews of Phase A and they were asked to return
the polarity (Slot3). Each participating team was
allowed to submit up to two runs per slot and do-
main in each phase; one constrained (C), where
only the provided training data could be used, and
one unconstrained (U), where other resources (e.g.,
publicly available lexica) and additional data of
any kind could be used for training. In the latter
case, the teams had to report the resources they
used. To evaluate aspect category (Slot1) and OTE
extraction (Slot2) in Phase A, we used the F-1
measure. To evaluate sentiment polarity (Slot 3) in
Phase B, we used accuracy. Furthermore, we im-
plemented and provided three baselines (see be-
low) for the respective slots.
</bodyText>
<subsectionHeader confidence="0.95428">
4.1 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.988776083333333">
Slot 1: F-1 scores are calculated by comparing
the category annotations that a system returned (for
all the sentences) to the gold category annotations
(using micro-averaging). These category annota-
tions are extracted from the values of Slot 1 (cate-
gory). Duplicate occurrences of categories (for the
same sentence) are ignored.
Slot 2: F-1 scores are calculated by comparing
the targets that a system returned (for all the sen-
tences) to the corresponding gold targets (using
micro-averaging). The targets are extracted using
their starting and ending offsets. The calculation
for each sentence considers only distinct targets
and discards NULL targets, since they do not cor-
respond to explicit mentions.
Slot 1&amp;2 (jointly): Again F-1 scores are calcu-
lated by comparing the {category, OTE} tuples of
a system to the gold ones (using micro-averaging).
Slot 3: To evaluate sentiment polarity detection
in Phase B, we calculated the accuracy of each sys-
tem, defined as the number of correctly predicted
polarity labels of aspect categories, divided by the
total number of aspect categories. Recall that we
use the gold aspect categories in Phase B.
</bodyText>
<subsectionHeader confidence="0.980541">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.996339333333333">
Slot 1: For category (E#A) extraction, a Support
Vector Machine (SVM) with a linear kernel was
trained. In particular, n unigram features are ex-
tracted from the respective sentence of each tuple
that is encountered in the training data. The catego-
ry value (e.g., SERVICE#GENERAL) of the tuple is
</bodyText>
<page confidence="0.997695">
491
</page>
<bodyText confidence="0.999873071428571">
used as the correct label of the feature vector.
Similarly, for each test sentence s, a feature vector
is built and the trained SVM is used to predict the
probabilities of assigning each possible category to
s (e.g., {SERVICE#GENERAL, 0.2}, {RESTAURANT#
GENERAL, 0.4}. Then, a threshold9 t is used to de-
cide which of the categories will be assigned10 to s.
As features, we use the 1,000 most frequent uni-
grams of the training data excluding stop-words.
Slot 2: The baseline uses the training reviews to
create for each category c (e.g., SERVICE#
GENERAL) a list of OTEs (e.g., SERVICE#GENERAL
→ {“staff”, “waiter”}). These are extracted from
the (training) opinion tuples whose category value
is c. Then, given a test sentence s and an assigned
category c, the baseline finds in s the first occur-
rence of each OTE of c’s list. The OTE slot is
filled with the first of the target occurrences found
in s. If no target occurrences are found, the slot is
assigned the value NULL.
Slot 3: For polarity prediction we trained a
SVM classifier with a linear kernel. Again, as in
Slot 1, n unigram features are extracted from the
respective sentence of each tuple of the training
data. In addition, an integer-valued feature11 that
indicates the category of the tuple is used. The cor-
rect label for the extracted training feature vector is
the corresponding polarity value (e.g., positive).
Then, for each tuple {category, OTE} of a test sen-
tence s, a feature vector is built and it is classified
using the trained SVM. Furthermore, for Slot 3 we
also used a majority baseline that assigns the most
frequent polarity (in the training data) to all test
tuples.
The baseline systems and evaluation scripts are
available for download as a single zip from the SE-
ABSA15 website12. They are implemented in Java
and can be used via a Linux shell script. The base-
lines use the LibSVM package13 (Chang and Lin,
2011) for SVM training and prediction. The scores
of the baselines in the test datasets are presented in
Tables 4–8 along with the system scores.
</bodyText>
<footnote confidence="0.87154425">
9 The threshold t was tuned on a subset of the training data (for
each domain) using a trial and error approach.
10We use the –b 1 option of LibSVM to obtain probabilities.
11 Each category (E#A pair) has been assigned a distinct inte-
ger value.
12http://alt.qcri.org/semeval2015/task12/index.php?id=data-
and-tools
13http://www.csie.ntu.edu.tw/~cjlin/libsvm/
</footnote>
<sectionHeader confidence="0.947476" genericHeader="method">
5 Evaluation Results
</sectionHeader>
<bodyText confidence="0.999984">
In total, the task attracted 92 submissions from 16
teams. The evaluation results per phase and slot are
presented below. For the teams that submitted
more than one unconstrained runs per slot and do-
main, we included in the tables only the run with
the highest score.
</bodyText>
<subsectionHeader confidence="0.995019">
5.1 Results of Phase A
</subsectionHeader>
<bodyText confidence="0.99967028">
The aspect category identification slot attracted 6
teams for the laptops dataset and 9 teams for the
restaurants dataset (consult Table 4). As expected,
the systems achieved significantly higher scores
(+12%) in the restaurants domain since in this do-
main the classification schema is less fine-grained;
it contains 6 entity types and 5 attribute classes that
result in 12 possible combinations, as opposed to
the laptops domain where the 22 entities and 9 at-
tribute labels give rise to more than 80 combina-
tions. The best F-1 scores in both domains, 50.86%
for laptops and 62.68% for restaurants, were
achieved by the unconstrained submission of the
NLANGP team, which modeled aspect category
extraction as a multiclass classification problem
with features based on n-grams, parsing, and word
clusters learnt from Amazon and Yelp data (for
laptops and restaurants, respectively). The system
of Sentiue (scores: 50% on laptops, 54.10% on
restaurants) used a separate MaxEnt classifier with
bag-of-word-like features (e.g. words, lemmas) for
each entity and for each attribute. Subsequently,
heuristics are applied to the output of the classifiers
to determine which categories will be assigned to
each sentence.
</bodyText>
<table confidence="0.972498470588235">
Laptops Restaurants
Team F1 Team F1
NLANGP 50.86* NLANGP 62.68*
Sentiue 50.00* NLANGP 61.94
IHS-RD. 49.59 UMDuluthC 57.19
NLANGP 49.06 UMDuluthT 57.19
TJUdeM 46.49 SIEL 57.14*
UFRGS 44.95 Sentiue 54.10*
UFRGS 44.73* LT3 53.67*
V3 24.94* TJUdeM 52.44*
UFRGS 52.09*
UFRGS 51.88
IHS-RD. 49.87
IHS-RD. 49.16
V3 41.85*
492
Baseline 48.06 Baseline 51.32 5.2 Results of Phase B
</table>
<tableCaption confidence="0.9951595">
Table 4. F-1 scores for aspect category extraction (slot
1). * indicate unconstrained systems.
</tableCaption>
<bodyText confidence="0.998653866666667">
The OTE slot, which was used only in the res-
taurants domain, attracted 14 teams; consult Table
5. The best F1 score (70.05%) was achieved by the
unconstrained submission of EliXa that addressed
the problem using an averaged perceptron with a
BIO tagging scheme. The features EliXa used in-
cluded n-grams, token classes, n-gram prefixes and
suffixes, and word clusters learnt from additional
data (Yelp for Brown and Clark clusters; Wikipe-
dia for word2vec clusters). Similarly, NLANGP
(67.11%) was based on a Conditional Random
Fields (CRF) model with features based on word
strings, head words (obtained from parse trees),
name lists (e.g. extracted using frequency), and
Brown clusters.
</bodyText>
<table confidence="0.998482545454546">
Restaurants
Team F1 Team F1
EliXa 70.05* UMDuluthC 50.36
NLANGP 67.11* UMDuluthT 50.36
IHS-RD. 63.12 LT3 49.97*
Lsislif 62.22 UFRGS 49.32*
NLANGP 61.49 V3 45.67*
wnlp 57.63 Sentiue 39.82*
SIEL 53.38* CU-BDDA 36.01
TJUdeM 52.44* CU-BDDA 33.86*
Baseline 48.06
</table>
<tableCaption confidence="0.9949975">
Table 5. Results for OTE extraction (slot 2). * indicate
unconstrained systems.
</tableCaption>
<bodyText confidence="0.999673142857143">
Finally, as expected, the scores are significantly
lower when systems have to link the extracted
OTEs to the relevant aspect categories (Slot1&amp;2
jointly). As shown in Table 6, the best F-1 score
(42.90%) was achieved by the NLANGP team that
simply combined the output for each slot to con-
struct the corresponding tuples.
</bodyText>
<table confidence="0.995149">
Restaurants
Team F1 Team F1
NLANGP 42.90* LT3 35.50*
IHS-RD. 42.72 UFRGS 34.87*
IHS-RD. 41.96 UMDuluthC 32.59
NLANGP 39.81 UMDuluthT 32.59
TJUdeM 37.15* Sentiue 31.20*
Baseline 34.44
</table>
<tableCaption confidence="0.875987">
Table 6. Results for Slot1&amp;2. * indicate unconstrained
systems.
</tableCaption>
<bodyText confidence="0.997951117647059">
The sentiment polarity slot attracted 10 teams for
the laptops and 12 teams for the restaurants domain
(see Table 7). The best accuracy scores in both
domains, 79.34% for laptops and 78.69% for res-
taurants, were achieved by Sentiue with a MaxEnt
classifier along with features based on n-grams,
POS tagging, lemmatization, negation words and
publicly available sentiment lexica (MPQA, Bing
Liu’s lexicon, AFINN). The system of ECNU
(scores: 78.29% laptops, 78.10% restaurants) used
features based on n-grams, PMI scores, POS tags,
parse trees, negation words and scores based on 7
sentiment lexica. The lsislif team (77.87% laptops,
75.50% restaurants) relied on a logistic regression
model (Liblinear) with various features: syntactic
(e.g., unigrams, negation), semantic (Brown dic-
tionary), sentiment (e.g., MPQA, SentiWordnet).
</bodyText>
<table confidence="0.9985835">
Laptops Restaurants
Team Acc. Team Acc.
Sentiue 79.34* Sentiue 78.69*
ECNU 78.29 ECNU 78.10*
Lsislif 77.87 Lsislif 75.50
ECNU 74.49* LT3 75.02*
LT3 73.76* UFRGS 71.71
TJUdeM 73.23* Wnlp 71.36
EliXa 72.91* UMDuluthC 71.12
Wnlp 72.07 EliXa 70.05*
EliXa 71.54 ECNU 69.82
V3 68.38* V3 69.46*
UFRGS 67.33 TJUdeM 68.87*
SINAI 65.85 EliXa 67.33
SINAI 51.84* SINAI 60.71*
SIEL 70.76*
SVM+ BOW 69.96 SVM+ BOW 63.55
Baseline Baseline
Majority Base- 57.00 Majority Base- 53.72
line line
</table>
<tableCaption confidence="0.92367275">
Table 7. Accuracy scores for slot 3 (polarity extraction).
* indicate unconstrained systems. The evaluated run of
SIEL team was submitted after the deadline had ex-
pired, but before the release of the gold polarity labels.
</tableCaption>
<bodyText confidence="0.999878285714286">
Most teams performed (slightly) better in the
laptops domain. This is probably due to the fact
that in the restaurants domain the positive polarity
is significantly more frequent in the training than
in the test data, which may have led to biased
models. Nevertheless, most system scores indicate
robustness across the two domains, with Sentiue
</bodyText>
<page confidence="0.997913">
493
</page>
<bodyText confidence="0.999954136363637">
achieving the most stable performance: 79.34% in
laptops and 78.69% in restaurants.
A similar score was obtained also by Sentiue in
the hidden domain (78.76%). The (hidden) hotels
domain (subtask 2) attracted 9 teams. Lsislif
achieved the best score based on a Liblinear model
developed for the restaurants domain. LT3
achieved the second best score (80.53%) with an
SVM model trained on the restaurants training da-
ta. The model used features based on unigrams,
sentiment lexica (by Bing Liu, General Inquirer)
and PMI scores learnt from TripAdvisor data. The
team of EliXa (79.64%) used a multiclass SVM
and features based on word clusters, lemmas, n-
grams, POS tagging, and well known sentiment
lexica. The system of Sentiue (78.76%) is some-
what similar; it uses BOW, POS tags, lemmas, and
sentiment lexica. The results of some systems
(LT3, EliXa, V3) suggest that the hidden domain
was easier, but other systems (e.g., ECNU, wnlp)
achieved significantly lower scores in the hidden
domain, compared to the in-domain ABSA scores.
</bodyText>
<table confidence="0.997744625">
Hotels
Team Acc. Team Acc.
lsislif 85.84 V3 71.09*
LT3 80.53* UFRGS 65.78
EliXa 79.64* SINAI 63.71*
sentiue 78.76* Wnlp 55.45
EliXa 74.92 UMDuluthC 71.38
Majority Baseline 71.68
</table>
<tableCaption confidence="0.94116475">
Table 8. Accuracy scores for slot 3 (polarity extraction).
* indicate unconstrained systems. The evaluated run of
UMDuluthC team was submitted after the deadline had
expired but before the release of the gold polarity labels.
</tableCaption>
<sectionHeader confidence="0.999035" genericHeader="method">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999803">
The SE-ABSA15 task is a continuation of SE-
ABSA14 task. The SE-ABSA15 task provided a
new definition of aspect –that makes explicit the
difference between entities and the particular facets
that are being evaluated- within a new principled,
unified ABSA framework and output representa-
tion, which may be used in realistic applications
(e.g., review sites). We also provided benchmark
datasets containing manually annotated reviews
from three domains (restaurants, laptops, hotels)
and baselines for the respective SE-ABSA15 slots.
The task attracted 93 submissions from 16 teams
that were evaluated in three slots: aspect categories,
opinion target expressions, and polarity classifica-
tion. Future work includes applying the new
framework and annotation schema to other lan-
guages (e.g., Spanish, Greek) and enhancing it with
information about topics or events, opinion holders,
and annotations for linguistic phenomena like met-
aphor and irony.
</bodyText>
<sectionHeader confidence="0.997384" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997919">
We thank Konstantina Papanikolaou, who carried
out a critical part of the annotation process, Thom-
as Keefe for his help during the initial phases of
the annotation process, Juli Bakagianni for her
support on the META-SHARE platform and John
Pavlopoulos for his valuable contribution in shap-
ing the SE-ABSA tasks. Maria Pontiki &amp; Haris
Papageorgiou were supported by the
POLYTROPON (KRIPIS-GSRT, MIS: 448306)
project.
</bodyText>
<sectionHeader confidence="0.998907" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.9996806875">
Bing Liu. 2006. Web Data Mining: Exploring Hyper-
links, Contents, and Usage Data, 2006 and 2011:
Springer.
Samuel Brody and Noemie Elhadad. 2010. An unsuper-
vised aspect-sentiment model for online reviews. In
Proceedings of NAACL, pages 804–812, Los Angeles,
California.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
a library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1--
27:27.
Gayatree Ganu, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predictions
using review text content. In Proceedings of WebDB,
Providence, Rhode Island, USA.
Minqing Hu and Bing Liu. 2004a. Mining and summa-
rizing customer reviews. In Proceedings of KDD, pag-
es 168–177, Seattle, WA, USA.
Minqing Hu and Bing Liu. 2004b. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI,
pages 755–760, San Jose, California.
Yohan Jo, and Alice H Oh. 2011. Aspect and sentiment
unification model for online review analysis. In Pro-
ceedings of the fourth ACM international conference
on Web search and data mining, pages 815-824.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Proceed-
ings of the 18th ACM Conference on Information and
Knowledge Management, pages 375–384.
Julian McAuley, Jure Leskovec, and Dan Jurafsky.
2012. Learning attitudes and attributes from multi-
aspect reviews. In Proceedings of the 12th IEEE Inter-
</reference>
<page confidence="0.988832">
494
</page>
<reference confidence="0.997124321428572">
national Conference on Data Mining, ICDM ’12, pag-
es 1020–1025, Brussels, Belgium.
Margaret Mitchell. 2013. Overview of the TAC2013
Knowledge Base Population Evaluation: English Sen-
timent Slot Filling. In Proceedings of the Text Analy-
sis Conference (TAC), Gaithersburg, MD, USA.
John Pavlopoulos. 2014. Aspect based sentiment analy-
sis. PhD thesis, Dept. of Informatics, Athens Universi-
ty of Economics and Business, Greece.
Stelios Piperidis. 2012. The META-SHARE language
resources sharing infrastructure: Principles, challeng-
es, solutions. In Proceedings of LREC-2012, pages
36–42, Istanbul, Turkey.
Maria Pontiki, Dimitrios Galanis, John Pavlopoulos,
Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4: As-
pect based sentiment analysis. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), pages 27–35, Dublin, Ireland.
Josef Ruppenhofer, Roman Klinger, Julia Maria Struß,
Jonathan Sonntag, and Michael Wiegand. 2014.
IGGSA Shared Tasks on German Sentiment Analysis
(GESTALT). In Workshop Proceedings of the 12th
Edition of the KONVENS Conference, pages 164-
173.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi.
Chen, Noriko Kando, and Chin-Yew Lin. 2007. Over-
view of opinion analysis pilot task at ntcir-6. In Pro-
ceedings of NTCIR-6 Workshop Meeting, pages 265–
278.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, and Noriko Kando. 2008. Overview of
multilingual opinion analysis task at NTCIR-7. In
Proceedings of the 7th NTCIR Workshop Meeting on
Evaluation of Information Access Technologies: In-
formation Retrieval, Question Answering, and Cross-
Lingual Information Access, pages 185–203.
Yohei Seki, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, and
Noriko Kando. 2010. Overview of Multilingual Opin-
ion Analysis Task at NTCIR-8: A Step Toward Cross
Lingual Opinion Analysis. In Proceedings of the 8th
NTCIR Workshop Meeting on Evaluation of Infor-
mation Access Technologies: Information Retrieval,
Question Answering and Cross-Lingual Information
Access, pages 209–220.
Josef Steinberger, Tomáš Brychcín and Michal Konkol.
2014. Aspect-Level Sentiment Analysis in Czech. In
Proceedings of the 5th workshop on computational
approaches to subjectivity, sentiment and social media
analysis, Association for Computational Linguistics,
pages 24–30, Baltimore, Maryland.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c,
Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsujii.
2012. BRAT: a web-based tool for NLP-assisted text
annotation. In Proceedings of EACL, pages 102–107,
Avignon, France.
Tun Thura Thet, Jin-Cheon Na, and Christopher S. G.
Khoo. 2010. Aspect-based sentiment analysis of mov-
ie reviews on discussion boards. J. Information Sci-
ence, 36(6):823–848.
Lei Zhang and Bing Liu. 2014. Aspect and Entity Ex-
traction for Opinion Mining&amp;quot;, book chapter in Data
Mining and Knowledge Discovery for Big Data:
Methodologies, Challenges, and Opportunities,
Springer, 2014.
Appendix A. Laptop Aspect Categories
Entity Labels
1. LAPTOP 13. BATTERY
2. DISPLAY 14. GRAPHICS
3. KEYBOARD 15. HARD DISK
4. MOUSE 16. MULTIMEDIA DEVICES
5. MOTHERBOARD 17. HARDWARE
6. CPU 18. SOFTWARE
7. FANS&amp; COOLING 19. OS
8. PORTS 20. WARRANTY
9. MEMORY 21. SHIPPING
10. POWER SUPPLY 22. SUPPORT
11. OPTICAL DRIVES 23. COMPANY
Attribute Labels
A. GENERAL E. USABILITY
B. PRICE F. DESIGN&amp; FEATURES
C. QUALITY G. PORTABILITY
D. OPERATION&amp; H. CONNECTIVITY
PERFORMANCE I. MISCELLANEOUS
</reference>
<sectionHeader confidence="0.883313777777778" genericHeader="method">
Appendix B. Restaurant Aspect Categories
Entity Labels Attribute Labels
1. RESTAURANT A. GENERAL
2. FOOD B. PRICES
3. DRINKS C. QUALITY
4. AMBIENCE D. STYLE &amp; OPTIONS
5. SERVICE E. MISCELLANEOUS
6. LOCATION
Appendix C. Hotel Aspect Categories
Entity Labels Attribute Labels
1. HOTEL A. GENERAL
2. ROOMS B. PRICE
3. FACILITIES C. COMFORT
4. ROOM AMENITIES D. CLEANLINESS
5. SERVICE E. QUALITY
6. LOCATION F. DESIGN &amp; FEATURES
7. FOOD &amp; DRINKS G. STYLE &amp; OPTIONS
H. MISCELLANEOUS
</sectionHeader>
<page confidence="0.998951">
495
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324759">
<title confidence="0.981258">SemEval-2015 Task 12: Aspect Based Sentiment Analysis</title>
<author confidence="0.994697">Maria Pontiki</author>
<author confidence="0.994697">Dimitrios Galanis</author>
<author confidence="0.994697">Haris</author>
<affiliation confidence="0.945501">Ion *Institute for Language and Speech Processing, Athena R.C., Athens, of Computer Science, University of York, of Informatics, Athens University of Economics and Business,</affiliation>
<email confidence="0.7175115">mpontikiion@aueb.gr</email>
<email confidence="0.7175115">galanisdion@aueb.gr</email>
<email confidence="0.7175115">xarision@aueb.gr</email>
<abstract confidence="0.993258166666667">SemEval-2015 Task 12, a continuation of SemEval-2014 Task 4, aimed to foster research beyond sentenceor text-level sentiment classification towards Aspect Based Sentiment Analysis. The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price). The task provided manually annotated reviews in three domains (restaurants, laptops and hotels), and a common evaluation procedure. It attracted 93 submissions from 16 teams.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data,</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="6255" citStr="Liu (2006)" startWordPosition="944" endWordPosition="945">le the evaluation scores and the 2 http://www.nist.gov/tac/2014/KBP/Sentiment/index.html 3 The SE-ABSA14 inventory of categories for the restaurants domain is similar to the one of Ganu et al. (2009). main characteristics of the developed systems are presented in Section 5. The paper concludes with a general assessment of the task. 2 Task Set-Up 2.1 ABSA Framework: From SE-ABSA14 to SE-ABSA15 In SE-ABSA14, given a sentence from a user review about a target entity e (e.g., a laptop), the goal was to identify all aspects (explicit terms or categories) and the corresponding polarities. Following Liu (2006) &amp; Zhang and Liu (2014), an aspect (term or category) indicated: (a) a part/component of e (e.g., battery), (b) an attribute of e (e.g., price), or (c) an attribute of a part/component of e (e.g., battery life). In SE-ABSA15, an aspect category is defined as a combination of an entity type E and an attribute type A. This definition of aspect makes more explicit the difference between entities and the particular facets that are being evaluated. E can be the reviewed entity e itself (e.g., laptop), a part/component of it (e.g., battery or customer support), or another relevant entity (e.g., the </context>
</contexts>
<marker>Liu, 2006</marker>
<rawString>Bing Liu. 2006. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data, 2006 and 2011: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Noemie Elhadad</author>
</authors>
<title>An unsupervised aspect-sentiment model for online reviews.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>804--812</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="2188" citStr="Brody and Elhadad, 2010" startWordPosition="325" endWordPosition="328">rate such tables based on customer ratings, but usually only for a limited set of predefined aspects and not from freetext reviews. Several ABSA methods have been proposed for various domains, like consumer electronics (Hu and Liu {2004a, 2004b}), restaurants (Ganu et al., 2009) and movies (Thet et al., 2010). The available methods can be divided into those that adopt domain-independent solutions (Lin and He, 2009), and those that use domain-specific knowledge to improve their results (Thet et al., 2010). Typically, most methods treat aspect extraction and sentiment classification separately (Brody and Elhadad, 2010), but there are also approaches that model the two problems jointly (Jo and Oh, 2011). Figure 1. Table summarizing the average sentiment for each aspect of an entity. Publicly available ABSA datasets adopt different annotation schemes for different subtasks and languages (Pavlopoulos 2014). For example, the datasets of McAuley et al. (2012) provide aspects and respective ratings at the review level (i.e., aspects and ratings associated with entire reviews, not particular sentences)1 about Beers, Pubs, Toys and Games, and Audiobooks. The reviews are obtained from sites that allow users to evalu</context>
</contexts>
<marker>Brody, Elhadad, 2010</marker>
<rawString>Samuel Brody and Noemie Elhadad. 2010. An unsupervised aspect-sentiment model for online reviews. In Proceedings of NAACL, pages 804–812, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="25931" citStr="Chang and Lin, 2011" startWordPosition="4119" endWordPosition="4122">t label for the extracted training feature vector is the corresponding polarity value (e.g., positive). Then, for each tuple {category, OTE} of a test sentence s, a feature vector is built and it is classified using the trained SVM. Furthermore, for Slot 3 we also used a majority baseline that assigns the most frequent polarity (in the training data) to all test tuples. The baseline systems and evaluation scripts are available for download as a single zip from the SEABSA15 website12. They are implemented in Java and can be used via a Linux shell script. The baselines use the LibSVM package13 (Chang and Lin, 2011) for SVM training and prediction. The scores of the baselines in the test datasets are presented in Tables 4–8 along with the system scores. 9 The threshold t was tuned on a subset of the training data (for each domain) using a trial and error approach. 10We use the –b 1 option of LibSVM to obtain probabilities. 11 Each category (E#A pair) has been assigned a distinct integer value. 12http://alt.qcri.org/semeval2015/task12/index.php?id=dataand-tools 13http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 5 Evaluation Results In total, the task attracted 92 submissions from 16 teams. The evaluation results</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gayatree Ganu</author>
<author>Noemie Elhadad</author>
<author>Amelie Marian</author>
</authors>
<title>Beyond the stars: Improving rating predictions using review text content.</title>
<date>2009</date>
<booktitle>In Proceedings of WebDB,</booktitle>
<location>Providence, Rhode Island, USA.</location>
<contexts>
<context position="1843" citStr="Ganu et al., 2009" startWordPosition="274" endWordPosition="277">onsumers decide what to purchase and businesses to better monitor their reputation and understand the needs of the market (Pavlopoulos 2014). Given a target of interest (e.g., Apple Mac mini), an ABSA method can summarize the content of the respective reviews in an aspect-sentiment table like the one in Fig 1. Some review sites also generate such tables based on customer ratings, but usually only for a limited set of predefined aspects and not from freetext reviews. Several ABSA methods have been proposed for various domains, like consumer electronics (Hu and Liu {2004a, 2004b}), restaurants (Ganu et al., 2009) and movies (Thet et al., 2010). The available methods can be divided into those that adopt domain-independent solutions (Lin and He, 2009), and those that use domain-specific knowledge to improve their results (Thet et al., 2010). Typically, most methods treat aspect extraction and sentiment classification separately (Brody and Elhadad, 2010), but there are also approaches that model the two problems jointly (Jo and Oh, 2011). Figure 1. Table summarizing the average sentiment for each aspect of an entity. Publicly available ABSA datasets adopt different annotation schemes for different subtas</context>
<context position="5844" citStr="Ganu et al. (2009)" startWordPosition="874" endWordPosition="877">extraction, polarity classification) into a principled unified framework (described in Section 2). In addition, SE-ABSA15 included an aspect level polarity classification subtask for the hotels domain in which no training data were provided (out-of-domain ABSA). The annotation schema and the provided datasets are described in Section 3. The evaluation measures and the baseline methods are described in Section 4, while the evaluation scores and the 2 http://www.nist.gov/tac/2014/KBP/Sentiment/index.html 3 The SE-ABSA14 inventory of categories for the restaurants domain is similar to the one of Ganu et al. (2009). main characteristics of the developed systems are presented in Section 5. The paper concludes with a general assessment of the task. 2 Task Set-Up 2.1 ABSA Framework: From SE-ABSA14 to SE-ABSA15 In SE-ABSA14, given a sentence from a user review about a target entity e (e.g., a laptop), the goal was to identify all aspects (explicit terms or categories) and the corresponding polarities. Following Liu (2006) &amp; Zhang and Liu (2014), an aspect (term or category) indicated: (a) a part/component of e (e.g., battery), (b) an attribute of e (e.g., price), or (c) an attribute of a part/component of e</context>
</contexts>
<marker>Ganu, Elhadad, Marian, 2009</marker>
<rawString>Gayatree Ganu, Noemie Elhadad, and Amelie Marian. 2009. Beyond the stars: Improving rating predictions using review text content. In Proceedings of WebDB, Providence, Rhode Island, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of KDD,</booktitle>
<pages>168--177</pages>
<location>Seattle, WA, USA.</location>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004a. Mining and summarizing customer reviews. In Proceedings of KDD, pages 168–177, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>755--760</pages>
<location>San Jose, California.</location>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004b. Mining opinion features in customer reviews. In Proceedings of AAAI, pages 755–760, San Jose, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohan Jo</author>
<author>Alice H Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the fourth ACM international conference on Web search and data mining,</booktitle>
<pages>815--824</pages>
<contexts>
<context position="2273" citStr="Jo and Oh, 2011" startWordPosition="340" endWordPosition="343">aspects and not from freetext reviews. Several ABSA methods have been proposed for various domains, like consumer electronics (Hu and Liu {2004a, 2004b}), restaurants (Ganu et al., 2009) and movies (Thet et al., 2010). The available methods can be divided into those that adopt domain-independent solutions (Lin and He, 2009), and those that use domain-specific knowledge to improve their results (Thet et al., 2010). Typically, most methods treat aspect extraction and sentiment classification separately (Brody and Elhadad, 2010), but there are also approaches that model the two problems jointly (Jo and Oh, 2011). Figure 1. Table summarizing the average sentiment for each aspect of an entity. Publicly available ABSA datasets adopt different annotation schemes for different subtasks and languages (Pavlopoulos 2014). For example, the datasets of McAuley et al. (2012) provide aspects and respective ratings at the review level (i.e., aspects and ratings associated with entire reviews, not particular sentences)1 about Beers, Pubs, Toys and Games, and Audiobooks. The reviews are obtained from sites that allow users to evaluate a product not only in terms of its overall quality, but also focusing on specific</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Yohan Jo, and Alice H Oh. 2011. Aspect and sentiment unification model for online review analysis. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 815-824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenghua Lin</author>
<author>Yulan He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>375--384</pages>
<contexts>
<context position="1982" citStr="Lin and He, 2009" startWordPosition="297" endWordPosition="300">. Given a target of interest (e.g., Apple Mac mini), an ABSA method can summarize the content of the respective reviews in an aspect-sentiment table like the one in Fig 1. Some review sites also generate such tables based on customer ratings, but usually only for a limited set of predefined aspects and not from freetext reviews. Several ABSA methods have been proposed for various domains, like consumer electronics (Hu and Liu {2004a, 2004b}), restaurants (Ganu et al., 2009) and movies (Thet et al., 2010). The available methods can be divided into those that adopt domain-independent solutions (Lin and He, 2009), and those that use domain-specific knowledge to improve their results (Thet et al., 2010). Typically, most methods treat aspect extraction and sentiment classification separately (Brody and Elhadad, 2010), but there are also approaches that model the two problems jointly (Jo and Oh, 2011). Figure 1. Table summarizing the average sentiment for each aspect of an entity. Publicly available ABSA datasets adopt different annotation schemes for different subtasks and languages (Pavlopoulos 2014). For example, the datasets of McAuley et al. (2012) provide aspects and respective ratings at the revie</context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, pages 375–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian McAuley</author>
<author>Jure Leskovec</author>
<author>Dan Jurafsky</author>
</authors>
<title>Learning attitudes and attributes from multiaspect reviews.</title>
<date>2012</date>
<booktitle>In Proceedings of the 12th IEEE International Conference on Data Mining, ICDM ’12,</booktitle>
<pages>1020--1025</pages>
<location>Brussels, Belgium.</location>
<contexts>
<context position="2530" citStr="McAuley et al. (2012)" startWordPosition="378" endWordPosition="381">ded into those that adopt domain-independent solutions (Lin and He, 2009), and those that use domain-specific knowledge to improve their results (Thet et al., 2010). Typically, most methods treat aspect extraction and sentiment classification separately (Brody and Elhadad, 2010), but there are also approaches that model the two problems jointly (Jo and Oh, 2011). Figure 1. Table summarizing the average sentiment for each aspect of an entity. Publicly available ABSA datasets adopt different annotation schemes for different subtasks and languages (Pavlopoulos 2014). For example, the datasets of McAuley et al. (2012) provide aspects and respective ratings at the review level (i.e., aspects and ratings associated with entire reviews, not particular sentences)1 about Beers, Pubs, Toys and Games, and Audiobooks. The reviews are obtained from sites that allow users to evaluate a product not only in terms of its overall quality, but also focusing on specific predefined aspects (e.g. “smell” and “taste” for Beers, “fun” and “educational value” for Toys and Games). The IGGSA Shared Tasks on German Sentiment Analysis (Ruppenhofer et al., 2014) provided human annotated datasets of political speeches (STEPS task) 1</context>
</contexts>
<marker>McAuley, Leskovec, Jurafsky, 2012</marker>
<rawString>Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012. Learning attitudes and attributes from multiaspect reviews. In Proceedings of the 12th IEEE International Conference on Data Mining, ICDM ’12, pages 1020–1025, Brussels, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
</authors>
<title>Overview of the TAC2013 Knowledge Base Population Evaluation: English Sentiment Slot Filling.</title>
<date>2013</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC),</booktitle>
<location>Gaithersburg, MD, USA.</location>
<contexts>
<context position="3994" citStr="Mitchell, 2013" startWordPosition="605" endWordPosition="606">tional Linguistics and reviews about products (StAR task) like coffee machines and washers. The StAR task focused on the extraction of evaluative phrases (e.g., “bad”) and aspect expressions (e.g., “washer”). The STEPS dataset includes annotations for evaluative phrases, opinion targets, and the corresponding sources (opinion holders). The extraction of opinion targets and holders has also been addressed in the context of the Multilingual Opinion Analysis Task (Seki et al., 2007; Seki et al., 2008; Seki et al., 2010) and the Sentiment Slot Filling2 Task of the Knowledge Base Population Track (Mitchell, 2013). However, these tasks deal with the identification of opinion targets in general, not in the context of ABSA. SemEval-2014 Task 4 (SE-ABSA14) provided datasets annotated with aspect terms (e.g., “hard disk”, “pizza”) and their polarity for laptop and restaurant reviews, as well as coarser aspect categories (e.g., PRICE) and their polarity only for restaurants3 (Pontiki et al., 2014). The task attracted 165 submissions from 32 teams that experimented with a variety of features (e.g., based on n-grams, parse trees, named entities, word clusters), techniques (e.g., rule-based, supervised and uns</context>
</contexts>
<marker>Mitchell, 2013</marker>
<rawString>Margaret Mitchell. 2013. Overview of the TAC2013 Knowledge Base Population Evaluation: English Sentiment Slot Filling. In Proceedings of the Text Analysis Conference (TAC), Gaithersburg, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Pavlopoulos</author>
</authors>
<title>Aspect based sentiment analysis.</title>
<date>2014</date>
<tech>PhD thesis,</tech>
<institution>Dept. of Informatics, Athens University of Economics and Business, Greece.</institution>
<contexts>
<context position="1365" citStr="Pavlopoulos 2014" startWordPosition="195" endWordPosition="196">ed manually annotated reviews in three domains (restaurants, laptops and hotels), and a common evaluation procedure. It attracted 93 submissions from 16 teams. 1 Introduction and Related Work The rise of e-commerce, as a new shopping and marketing channel, has led to an upsurge of review sites for a variety of services and products. In this context, Aspect Based Sentiment Analysis (ABSA) -i.e., mining opinions from text about specific entities and their aspects- can help consumers decide what to purchase and businesses to better monitor their reputation and understand the needs of the market (Pavlopoulos 2014). Given a target of interest (e.g., Apple Mac mini), an ABSA method can summarize the content of the respective reviews in an aspect-sentiment table like the one in Fig 1. Some review sites also generate such tables based on customer ratings, but usually only for a limited set of predefined aspects and not from freetext reviews. Several ABSA methods have been proposed for various domains, like consumer electronics (Hu and Liu {2004a, 2004b}), restaurants (Ganu et al., 2009) and movies (Thet et al., 2010). The available methods can be divided into those that adopt domain-independent solutions (</context>
</contexts>
<marker>Pavlopoulos, 2014</marker>
<rawString>John Pavlopoulos. 2014. Aspect based sentiment analysis. PhD thesis, Dept. of Informatics, Athens University of Economics and Business, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stelios Piperidis</author>
</authors>
<title>The META-SHARE language resources sharing infrastructure: Principles, challenges, solutions.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC-2012,</booktitle>
<pages>36--42</pages>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="21289" citStr="Piperidis, 2012" startWordPosition="3355" endWordPosition="3356">lasses across the training and the test sets. positive negative neutral RS-TR 72.43% 24.36% 3.20% RS-TE 53.72% 40.96% 5.32% LP-TR 55.87% 38.75% 5.36% LP-TE 57% 34.66% 8.32% HT-TE 71.68% 24.77% 3.53% Table 3. Polarity distribution per domain (RSrestaurants, LP-laptops, HT-hotels). TR and TE indicate the training and test sets. 3.4 Datasets Format and Availability The datasets7 of the SE-ABSA15 task were provided in an XML format. They are available under a non-commercial, no redistribution license through META-SHARE8, a repository devoted to the sharing and dissemination of language resources (Piperidis, 2012). 4 Evaluation Measures and Baselines Similarly to SE-ABSA14, the evaluation ran in two phases. In Phase A, the participants were asked to return the {category, OTE} tuples for the restaurants domain and only the category slot (Slot1) for the laptops domain. Subsequently, in Phase B, the 7 The data are available at http://metashare.ilsp.gr:8080/. 8 META-SHARE (http: //www.metashare.org/) was implemented in the framework of the META-NET Network of Excellence (http://www.meta-net.eu/). participants were given the gold annotations for the reviews of Phase A and they were asked to return the polar</context>
</contexts>
<marker>Piperidis, 2012</marker>
<rawString>Stelios Piperidis. 2012. The META-SHARE language resources sharing infrastructure: Principles, challenges, solutions. In Proceedings of LREC-2012, pages 36–42, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitrios Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Semeval-2014 task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>27--35</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="4380" citStr="Pontiki et al., 2014" startWordPosition="665" endWordPosition="668">ers has also been addressed in the context of the Multilingual Opinion Analysis Task (Seki et al., 2007; Seki et al., 2008; Seki et al., 2010) and the Sentiment Slot Filling2 Task of the Knowledge Base Population Track (Mitchell, 2013). However, these tasks deal with the identification of opinion targets in general, not in the context of ABSA. SemEval-2014 Task 4 (SE-ABSA14) provided datasets annotated with aspect terms (e.g., “hard disk”, “pizza”) and their polarity for laptop and restaurant reviews, as well as coarser aspect categories (e.g., PRICE) and their polarity only for restaurants3 (Pontiki et al., 2014). The task attracted 165 submissions from 32 teams that experimented with a variety of features (e.g., based on n-grams, parse trees, named entities, word clusters), techniques (e.g., rule-based, supervised and unsupervised learning), and resources (e.g., sentiment lexica, Wikipedia, WordNet). The participants obtained higher scores in the restaurants domain. The laptops domain proved to be harder involving more entities (e.g., hardware and software components) and complex concepts (e.g., usability, portability) that are often discussed implicitly in the text. The SE-ABSA14 task set-up has bee</context>
</contexts>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Roman Klinger</author>
<author>Julia Maria Struß</author>
<author>Jonathan Sonntag</author>
<author>Michael Wiegand</author>
</authors>
<title>IGGSA Shared Tasks on German Sentiment Analysis (GESTALT).</title>
<date>2014</date>
<booktitle>In Workshop Proceedings of the 12th Edition of the KONVENS Conference,</booktitle>
<pages>164--173</pages>
<contexts>
<context position="3059" citStr="Ruppenhofer et al., 2014" startWordPosition="463" endWordPosition="466">nt subtasks and languages (Pavlopoulos 2014). For example, the datasets of McAuley et al. (2012) provide aspects and respective ratings at the review level (i.e., aspects and ratings associated with entire reviews, not particular sentences)1 about Beers, Pubs, Toys and Games, and Audiobooks. The reviews are obtained from sites that allow users to evaluate a product not only in terms of its overall quality, but also focusing on specific predefined aspects (e.g. “smell” and “taste” for Beers, “fun” and “educational value” for Toys and Games). The IGGSA Shared Tasks on German Sentiment Analysis (Ruppenhofer et al., 2014) provided human annotated datasets of political speeches (STEPS task) 1 A subset of the datasets has been annotated with aspects at the sentence level. 486 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 486–495, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics and reviews about products (StAR task) like coffee machines and washers. The StAR task focused on the extraction of evaluative phrases (e.g., “bad”) and aspect expressions (e.g., “washer”). The STEPS dataset includes annotations for evaluative phrases, opinion </context>
</contexts>
<marker>Ruppenhofer, Klinger, Struß, Sonntag, Wiegand, 2014</marker>
<rawString>Josef Ruppenhofer, Roman Klinger, Julia Maria Struß, Jonathan Sonntag, and Michael Wiegand. 2014. IGGSA Shared Tasks on German Sentiment Analysis (GESTALT). In Workshop Proceedings of the 12th Edition of the KONVENS Conference, pages 164-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Kando Chen</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Overview of opinion analysis pilot task at ntcir-6.</title>
<date>2007</date>
<booktitle>In Proceedings of NTCIR-6 Workshop Meeting,</booktitle>
<pages>265--278</pages>
<marker>Chen, Lin, 2007</marker>
<rawString>Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi. Chen, Noriko Kando, and Chin-Yew Lin. 2007. Overview of opinion analysis pilot task at ntcir-6. In Proceedings of NTCIR-6 Workshop Meeting, pages 265– 278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>David Kirk Evans</author>
<author>Lun-Wei Ku</author>
<author>Hsin-Hsi Chen Le Sun</author>
<author>Noriko Kando</author>
</authors>
<title>Overview of multilingual opinion analysis task at NTCIR-7.</title>
<date>2008</date>
<booktitle>In Proceedings of the 7th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering, and CrossLingual Information Access,</booktitle>
<pages>185--203</pages>
<marker>Seki, Evans, Ku, Le Sun, Kando, 2008</marker>
<rawString>Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, and Noriko Kando. 2008. Overview of multilingual opinion analysis task at NTCIR-7. In Proceedings of the 7th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering, and CrossLingual Information Access, pages 185–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>Lun-Wei Ku</author>
<author>Hsin-Hsi Chen Le Sun</author>
<author>Noriko Kando</author>
</authors>
<title>Overview of Multilingual Opinion Analysis Task at NTCIR-8: A Step Toward Cross Lingual Opinion Analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access,</booktitle>
<pages>209--220</pages>
<marker>Seki, Ku, Le Sun, Kando, 2010</marker>
<rawString>Yohei Seki, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, and Noriko Kando. 2010. Overview of Multilingual Opinion Analysis Task at NTCIR-8: A Step Toward Cross Lingual Opinion Analysis. In Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access, pages 209–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Steinberger</author>
<author>Tomáš Brychcín</author>
<author>Michal Konkol</author>
</authors>
<title>Aspect-Level Sentiment Analysis in Czech.</title>
<date>2014</date>
<booktitle>In Proceedings of the 5th</booktitle>
<pages>24--30</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="5099" citStr="Steinberger et al., 2014" startWordPosition="768" endWordPosition="771">s (e.g., based on n-grams, parse trees, named entities, word clusters), techniques (e.g., rule-based, supervised and unsupervised learning), and resources (e.g., sentiment lexica, Wikipedia, WordNet). The participants obtained higher scores in the restaurants domain. The laptops domain proved to be harder involving more entities (e.g., hardware and software components) and complex concepts (e.g., usability, portability) that are often discussed implicitly in the text. The SE-ABSA14 task set-up has been adopted for the creation of aspect-level sentiment datasets in other languages, like Czech (Steinberger et al., 2014). SemEval-2015 Task 12 (SE-ABSA15) built upon SE-ABSA14 and consolidated its subtasks (aspect category extraction, aspect term extraction, polarity classification) into a principled unified framework (described in Section 2). In addition, SE-ABSA15 included an aspect level polarity classification subtask for the hotels domain in which no training data were provided (out-of-domain ABSA). The annotation schema and the provided datasets are described in Section 3. The evaluation measures and the baseline methods are described in Section 4, while the evaluation scores and the 2 http://www.nist.gov</context>
</contexts>
<marker>Steinberger, Brychcín, Konkol, 2014</marker>
<rawString>Josef Steinberger, Tomáš Brychcín and Michal Konkol. 2014. Aspect-Level Sentiment Analysis in Czech. In Proceedings of the 5th workshop on computational approaches to subjectivity, sentiment and social media analysis, Association for Computational Linguistics, pages 24–30, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
<author>Sampo Pyysalo</author>
<author>Goran Topi´c</author>
<author>Tomoko Ohta</author>
<author>Sophia Ananiadou</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>BRAT: a web-based tool for NLP-assisted text annotation.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>102--107</pages>
<location>Avignon, France.</location>
<marker>Stenetorp, Pyysalo, Topi´c, Ohta, Ananiadou, Tsujii, 2012</marker>
<rawString>Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c, Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsujii. 2012. BRAT: a web-based tool for NLP-assisted text annotation. In Proceedings of EACL, pages 102–107, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tun Thura Thet</author>
<author>Jin-Cheon Na</author>
<author>Christopher S G Khoo</author>
</authors>
<title>Aspect-based sentiment analysis of movie reviews on discussion boards.</title>
<date>2010</date>
<journal>J. Information Science,</journal>
<volume>36</volume>
<issue>6</issue>
<contexts>
<context position="1874" citStr="Thet et al., 2010" startWordPosition="280" endWordPosition="283">e and businesses to better monitor their reputation and understand the needs of the market (Pavlopoulos 2014). Given a target of interest (e.g., Apple Mac mini), an ABSA method can summarize the content of the respective reviews in an aspect-sentiment table like the one in Fig 1. Some review sites also generate such tables based on customer ratings, but usually only for a limited set of predefined aspects and not from freetext reviews. Several ABSA methods have been proposed for various domains, like consumer electronics (Hu and Liu {2004a, 2004b}), restaurants (Ganu et al., 2009) and movies (Thet et al., 2010). The available methods can be divided into those that adopt domain-independent solutions (Lin and He, 2009), and those that use domain-specific knowledge to improve their results (Thet et al., 2010). Typically, most methods treat aspect extraction and sentiment classification separately (Brody and Elhadad, 2010), but there are also approaches that model the two problems jointly (Jo and Oh, 2011). Figure 1. Table summarizing the average sentiment for each aspect of an entity. Publicly available ABSA datasets adopt different annotation schemes for different subtasks and languages (Pavlopoulos 2</context>
</contexts>
<marker>Thet, Na, Khoo, 2010</marker>
<rawString>Tun Thura Thet, Jin-Cheon Na, and Christopher S. G. Khoo. 2010. Aspect-based sentiment analysis of movie reviews on discussion boards. J. Information Science, 36(6):823–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Zhang</author>
<author>Bing Liu</author>
</authors>
<title>Aspect and Entity Extraction for Opinion Mining&amp;quot;, book chapter in Data Mining and Knowledge Discovery for Big Data: Methodologies, Challenges, and Opportunities,</title>
<date>2014</date>
<publisher>Springer,</publisher>
<contexts>
<context position="6278" citStr="Zhang and Liu (2014)" startWordPosition="947" endWordPosition="950">tion scores and the 2 http://www.nist.gov/tac/2014/KBP/Sentiment/index.html 3 The SE-ABSA14 inventory of categories for the restaurants domain is similar to the one of Ganu et al. (2009). main characteristics of the developed systems are presented in Section 5. The paper concludes with a general assessment of the task. 2 Task Set-Up 2.1 ABSA Framework: From SE-ABSA14 to SE-ABSA15 In SE-ABSA14, given a sentence from a user review about a target entity e (e.g., a laptop), the goal was to identify all aspects (explicit terms or categories) and the corresponding polarities. Following Liu (2006) &amp; Zhang and Liu (2014), an aspect (term or category) indicated: (a) a part/component of e (e.g., battery), (b) an attribute of e (e.g., price), or (c) an attribute of a part/component of e (e.g., battery life). In SE-ABSA15, an aspect category is defined as a combination of an entity type E and an attribute type A. This definition of aspect makes more explicit the difference between entities and the particular facets that are being evaluated. E can be the reviewed entity e itself (e.g., laptop), a part/component of it (e.g., battery or customer support), or another relevant entity (e.g., the manufacturer of e), whi</context>
</contexts>
<marker>Zhang, Liu, 2014</marker>
<rawString>Lei Zhang and Bing Liu. 2014. Aspect and Entity Extraction for Opinion Mining&amp;quot;, book chapter in Data Mining and Knowledge Discovery for Big Data: Methodologies, Challenges, and Opportunities, Springer, 2014.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Appendix</author>
</authors>
<title>Laptop Aspect Categories Entity Labels 1.</title>
<booktitle>LAPTOP 2. DISPLAY 3. KEYBOARD 4. MOUSE 5. MOTHERBOARD 6. CPU 7. FANS&amp; COOLING 8. PORTS 9. MEMORY 10. POWER SUPPLY 11. OPTICAL DRIVES 13. BATTERY 14. GRAPHICS 15. HARD DISK 16. MULTIMEDIA DEVICES 17. HARDWARE 18. SOFTWARE 19. OS 20. WARRANTY 21. SHIPPING 22. SUPPORT 23. COMPANY Attribute Labels</booktitle>
<publisher>MISCELLANEOUS</publisher>
<marker>Appendix, </marker>
<rawString>Appendix A. Laptop Aspect Categories Entity Labels 1. LAPTOP 2. DISPLAY 3. KEYBOARD 4. MOUSE 5. MOTHERBOARD 6. CPU 7. FANS&amp; COOLING 8. PORTS 9. MEMORY 10. POWER SUPPLY 11. OPTICAL DRIVES 13. BATTERY 14. GRAPHICS 15. HARD DISK 16. MULTIMEDIA DEVICES 17. HARDWARE 18. SOFTWARE 19. OS 20. WARRANTY 21. SHIPPING 22. SUPPORT 23. COMPANY Attribute Labels A. GENERAL B. PRICE C. QUALITY D. OPERATION&amp; PERFORMANCE E. USABILITY F. DESIGN&amp; FEATURES G. PORTABILITY H. CONNECTIVITY I. MISCELLANEOUS</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>