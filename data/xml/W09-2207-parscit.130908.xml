<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.607085">
An Analysis of Bootstrapping for the Recognition of Temporal Expressions
</title>
<author confidence="0.548457">
Mihai Surdeanu
</author>
<affiliation confidence="0.586213">
NLP Group
Stanford University
</affiliation>
<address confidence="0.791954">
Stanford, CA
</address>
<email confidence="0.983487">
mihais@stanford.edu
</email>
<author confidence="0.977693">
Jordi Poveda
</author>
<affiliation confidence="0.9919345">
TALP Research Center
Technical University of Catalonia (UPC)
</affiliation>
<address confidence="0.887934">
Barcelona, Spain
</address>
<email confidence="0.998279">
jpoveda@lsi.upc.edu
</email>
<author confidence="0.979207">
Jordi Turmo
</author>
<affiliation confidence="0.9922275">
TALP Research Center
Technical University of Catalonia (UPC)
</affiliation>
<address confidence="0.888186">
Barcelona, Spain
</address>
<email confidence="0.998814">
turmo@lsi.upc.edu
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914208333334">
We present a semi-supervised (bootstrapping)
approach to the extraction of time expression
mentions in large unlabelled corpora. Because
the only supervision is in the form of seed
examples, it becomes necessary to resort to
heuristics to rank and filter out spurious pat-
terns and candidate time expressions. The
application of bootstrapping to time expres-
sion recognition is, to the best of our knowl-
edge, novel. In this paper, we describe one
such architecture for bootstrapping Informa-
tion Extraction (IE) patterns —suited to the
extraction of entities, as opposed to events or
relations— and summarize our experimental
findings. These point out to the fact that a
pattern set with a good increase in recall with
respect to the seeds is achievable within our
framework while, on the other side, the de-
crease in precision in successive iterations is
succesfully controlled through the use of rank-
ing and selection heuristics. Experiments are
still underway to achieve the best use of these
heuristics and other parameters of the boot-
strapping algorithm.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957714285714">
The problem of time expression recognition refers
to the identification in free-format natural language
text of the occurrences of expressions that denote
time. Time-denoting expressions appear in a great
diversity of forms, beyond the most obvious ab-
solute time or date references (e.g. 11pm, Febru-
ary 14th, 2005): time references that anchor on an-
other time (three hours after midnight, two weeks be-
fore Christmas), expressions denoting durations (a
few months), expressions denoting recurring times
(every third month, twice in the hour), context-
dependent times (today, last year), vague references
(somewhere in the middle of June, the near future)
or times that are indicated by an event (the day G.
Bush was reelected). This problem is a subpart of
a task called TERN (Temporal Expression Recog-
nition and Normalization), where temporal expres-
sions are first identified in text and then its intended
temporal meaning is represented in a canonical for-
mat. TERN was first proposed as an independent
task in the 2004 edition of the ACE conferences1.
The most widely used standard for the annotation of
temporal expressions is TIMEX (Ferro et al., 2005).
The most common approach to temporal expres-
sion recognition in the past has been the use of
hand-made grammars to capture the expressions (see
(Wiebe et al., 1998; Filatova and Hovy, 2001; Sa-
quete et al., 2004) for examples), which can then
be easily expanded with additional attributes for the
normalization task, based on computing distance
and direction (past or future) with respect to a ref-
erence time. This approach achieves an F1-measure
of approximately 85% for recognition and normal-
ization. The use of machine learning techniques —
mainly statistical— for this task is a more recent
development, either alongside the traditional hand-
grammar approach to learn to distinguish specific
difficult cases (Mani and Wilson, 2000), or on its
own (Hacioglu et al., 2005). The latter apply SVMs
to the recognition task alone, using the output of sev-
eral human-made taggers as additional features for
the classifier, and report an F1-measure of 87.8%.
</bodyText>
<footnote confidence="0.984209">
1http://www.nist.gov/speech/tests/ace/
</footnote>
<page confidence="0.987469">
49
</page>
<note confidence="0.992181">
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999913043478261">
Bootstrapping techniques have been used for such
diverse NLP problems as: word sense disambigua-
tion (Yarowsky, 1995), named entity classification
(Collins and Singer, 1999), IE pattern acquisition
(Riloff, 1996; Yangarber et al., 2000; Yangarber,
2003; Stevenson and Greenwood, 2005), document
classification (Surdeanu et al., 2006), fact extraction
from the web (Pas¸ca et al., 2006) and hyponymy re-
lation extraction (Kozareva et al., 2008).
(Yarowsky, 1995) used bootstrapping to train de-
cision list classifiers to disambiguate between two
senses of a word, achieving impressive classification
accuracy. (Collins and Singer, 1999) applied boot-
strapping to extract rules for named entity (NE) clas-
sification, seeding the sytem with a few handcrafted
rules. Their main innovation was to split training
in two alternate stages: during one step, only con-
textual rules are sought; during the second step, the
new contextual rules are used to tag further NEs and
these are used to produce new spelling rules.
Bootstrapping approaches are employed in
(Riloff, 1996), (Yangarber et al., 2000), (Yangarber,
2003), and (Stevenson and Greenwood, 2005)
in order to find IE patterns for domain-specific
event extraction. (Pas¸ca et al., 2006) employ a
bootstrapping process to extract general facts from
the Web, viewed as two-term relationships (e.g
[Donald Knuth, 1938] could be an instance of
a “born in year” relationship). (Surdeanu et al.,
2006) used bootstrapping co-trained with an EM
classifier in order to perform topic classification
of documents based on the presence of certain
learned syntactic-semantic patterns. In (Kozareva
et al., 2008), bootstrapping is applied to finding
new members of certain class of objects (i.e. an
“is-a” relationship), by providing a member of the
required class as seed and using a “such as” type of
textual pattern to locate new instances.
The recognition of temporal expressions is cru-
cial for many applications in NLP, among them: IE,
Question Answering (QA) and Automatic Summa-
rization (for the temporal ordering of events). Work
on slightly supervised approaches such as bootstrap-
ping is justified by the large availability of unla-
belled corpora, as opposed to tagged ones, from
which to learn models for recognition.
</bodyText>
<sectionHeader confidence="0.962478" genericHeader="introduction">
2 Architecture
</sectionHeader>
<bodyText confidence="0.997077285714286">
Figure 1 illustrates the building blocks of the algo-
rithm and their interactions, along with input and
output data.
The inputs to the bootstrapping algorithm are the
unlabelled training corpus and a file of seed ex-
amples. The unlabelled corpus is a large collec-
tion of documents which has been tokenized, POS
tagged, lemmatized, and syntactically analyzed for
basic syntactic constituents (shallow parsing) and
headwords. The second input is a set of seed exam-
ples, consisting of a series of token sequences which
we assume to be correct time expressions. The seeds
are supplied without additional features, and without
context information.
Our bootstrapping algorithm works with two al-
ternative views of the same target data (time expres-
sions), that is: patterns and examples (i.e. an in-
stance of a pattern in the corpus). A pattern is a gen-
eralized representation that can match any sequence
of tokens meeting the conditions expressed in the
pattern (these can be morphological, semantic, syn-
tactic and contextual). An example is an actual can-
didate occurrence of a time expression. Patterns are
generated from examples found in the corpus and,
in its turn, new examples are found by searching
for matches of new patterns. Both patterns and ex-
amples may carry contextual information, that is, a
window of tokens left and right of the candidate time
expression.
Output examples and output patterns are the out-
puts of the bootstrapping process. Both the set of
output examples and the set of output patterns are
increased with each new iteration, by adding the new
candidate examples (respectively, patterns) that have
been “accepted” during the last iteration (i.e. those
that have passed the ranking and selection step).
Initially, a single pass through the corpus is per-
formed in order to find occurrences of the seeds in
the text. Thus, we bootstrap an initial set of exam-
ples. From then on, the bootstrapping process con-
sists of a succession of iterations with the following
steps:
</bodyText>
<listItem confidence="0.856388">
1. Ranking and selection of examples: Each ex-
</listItem>
<bodyText confidence="0.953277">
ample produced during any of the previous it-
erations, 0 to i − 1, is assigned a score (rank-
ing). The top n examples are selected to grow
the set of output examples (selection) and will
</bodyText>
<page confidence="0.997881">
50
</page>
<figureCaption confidence="0.99907">
Figure 1: Block diagram of bootstrapping algorithm
</figureCaption>
<bodyText confidence="0.5994495">
be used for the next step. The details are given
in Section 4.2.
</bodyText>
<listItem confidence="0.9592854">
2. Generation of candidate patterns: Candidate
patterns for the current iteration are generated
from the selected examples of the previous step
(discussed in Section 3).
3. Ranking and selection of candidate patterns:
</listItem>
<bodyText confidence="0.966115333333333">
Each pattern from the current iteration is as-
signed a score and the top m patterns are se-
lected to grow the set of output patterns and to
be used in the next step (discussed in Section
4.1). This step also involves a process of analy-
sis of subsumptions, performed simultaneously
with selection, in which the set of selected pat-
terns is examined and those that are subsumed
by other patterns are discarded.
4. Search for instances of the selected patterns:
The training corpus is traversed, in order to
search for instances (matches) of the selected
patterns, which, together with the accepted ex-
amples from all previous iterations, will form
the set of candidate examples for iteration i+1.
Also, in order to relax the matching of pat-
terns to corpus tokens and of token forms among
themselves, the matching of token forms is case-
insensitive, and all the digits in a token are gen-
eralized to a generic digit marker (for instance,
“12-23-2006” is internally rewritten as “@@-@@-
@@@@”).
Even though our architecture is built on a tradi-
tional boostrapping approach, there are several ele-
ments that are novel, at least in the context of tem-
poral expression recognition: a) our pattern repre-
sentation incorporates full syntax and distributional
semantics in a unified model (see Section 3); b) our
pattern ranking/selection approach includes a sub-
sumption model to limit redundancy; c) the formu-
lae in our example ranking/selection approach are
designed to work with variable-length expressions
that incorporate a context.
</bodyText>
<sectionHeader confidence="0.992675" genericHeader="method">
3 Pattern representation
</sectionHeader>
<bodyText confidence="0.999938153846154">
Patterns capture both the sequence of tokens that
integrate a potential time expression (i.e. a time
expression mention), and information from the left
and right context where it occurs (up to a bounded
length). Let us call prefix the part of the pattern that
represents the left context, infix the part that repre-
sents a potential time expression mention and postfix
the part that represents the right context.
The EBNF grammar that encodes our pattern rep-
resentation is given in Figure 2. Patterns are com-
posed of multiple pattern elements (PEs). A pattern
element is the minimal unit that is matched against
the tokens in the text, and a single pattern element
can match to one or several tokens, depending on
the pattern element type. A pattern is considered to
match a sequence of tokens in the text when: first,
all the PEs from the infix are matched (this gives the
potential time expression mention) and, second, all
the PEs from the prefix and the postfix are matched
(this gives the left and right context information for
the new candidate example, respectively). There-
fore, patterns with a larger context window are more
restrictive, because all of the PEs in the prefix and
the postfix have to be matched (on top of the infix)
for the pattern to yield a match.
We distinguish among token-level generalizations
</bodyText>
<page confidence="0.907113">
51
</page>
<equation confidence="0.982275454545454">
pattern ::= prefix SEP infix SEP postfix SEP
(modifiers)*
prefix ::= (pattern-elem)*
infix ::= (pattern-elem)+
postfix ::= (pattern-elem)*
pattern-elem ::= FORM &amp;quot;(&amp;quot; token-form &amp;quot;)&amp;quot; |
SEMCLASS &amp;quot;(&amp;quot; token-form &amp;quot;)&amp;quot; |
POS &amp;quot;(&amp;quot; pos-tag &amp;quot;)&amp;quot;  |LEMMA &amp;quot;(&amp;quot; lemma-form &amp;quot;)&amp;quot; |
SYN &amp;quot;(&amp;quot; syn-type &amp;quot;,&amp;quot; head &amp;quot;)&amp;quot; |
SYN-SEM &amp;quot;(&amp;quot; syn-type &amp;quot;,&amp;quot; head &amp;quot;)&amp;quot;
modifiers ::= COMPLETE-PHRASE
</equation>
<figureCaption confidence="0.999412">
Figure 2: The EBNF Grammar for Patterns
</figureCaption>
<bodyText confidence="0.92625125">
(i.e. PEs) and chunk-level generalizations. The for-
mer have been generated from the features of a sin-
gle token and will match to a single token in the text.
The latter have been generated from and match to a
sequence of tokens in the text (e.g. a basic syntactic
chunk). Patterns are built from the following types
of PEs (which can be seen in the grammar from Fig-
ure 2):
</bodyText>
<listItem confidence="0.9612145">
1. Token form PEs: The more restrictive, only
match a given token form.
2. Semantic class PEs: Match tokens (sometimes
multiwords) that belong to a given semantic
similarity class. This concept is defined below.
3. POS tag PEs: Match tokens with a given POS.
4. Lemma PEs: Match tokens with a given
lemma.
5. Syntactic chunk PEs: Match a sequence of to-
kens that is a syntactic chunk of a given type
(e.g. NP) and whose headword has the same
lemma as indicated.
6. Generalized syntactic PEs: Same as the previ-
ous, but the lemma of the headword may be any
</listItem>
<bodyText confidence="0.995756958333333">
in a given semantic similarity class.
The semantic similarity class of a word is defined
as the word itself plus a group of other semanti-
cally similar words. For computing these, we em-
ploy Lin’s corpus of pairwise distributional similari-
ties among words (nouns, verbs and adjectives) (Lin,
1998), filtered to include only those words whose
similarity value is above both an absolute (highest
n) and relative (to the highest similarity value in the
class) threshold. Even after filtering, Lin’s similari-
ties can be “noisy”, since the corpus has been con-
structed relying on purely statistical means. There-
fore, we are employing in addition a set of manu-
ally defined semantic classes (hardcoded lists) sen-
sitive to our domain of temporal expressions, such
that these lists “override” the Lin’s similarity cor-
pus whenever the semantic class of a word present
in them is involved. The manually defined semantic
classes include: the written form of cardinals; ordi-
nals; days of the week (plus today, tomorrow and
yesterday); months of the year; date trigger words
(e.g. day, week); time trigger words (e.g. hour, sec-
ond); frequency adverbs (e.g. hourly, monthly); date
adjectives (e.g. two- day, @@-week-long); and time
adjectives (e.g. three-hour, @@-minute-long).
We use a dynamic window for the amount of con-
text that is encoded into a pattern, that is, we gen-
erate all the possible patterns with the same infix,
and anything between 0 and the specified length of
the context window PEs in the prefix and the postfix,
and let the selection step decide which variations get
accepted into the next iteration.
The modifiers field in the pattern representa-
tion has been devised as an extension mecha-
nism. Currently the only implemented mod-
ifier is COMPLETE-PHRASE, which when at-
tached to a pattern, “rounds” the instance (i.e.
candidate time expression) captured by its infix
to include the closest complete basic syntactic
chunk (e.g. “LEMMA(end) LEMMA(of) SEM-
CLASS(January)” would match “the end of De-
cember 2009” instead of only “end of December”
against the text “... By the end of December 2009,
... ”). This modifier was implemented in view of the
fact that most temporal expressions correspond with
whole noun phrases or adverbial phrases.
From the above types of PEs, we have built the
following types of patterns:
</bodyText>
<listItem confidence="0.9995">
1. All-lemma patterns (including the prefix and
postfix).
2. All-semantic class patterns.
3. Combinations of token form with sem. class.
4. Combinations of lemma with sem. class.
5. All-POS tag patterns.
6. Combinations of token form with POS tag.
7. Combinations of lemma with POS tag.
8. All-syntactic chunk patterns.
9. All-generalized syntactic patterns.
</listItem>
<sectionHeader confidence="0.7838755" genericHeader="method">
4 Ranking and selection of patterns and
learning examples
</sectionHeader>
<subsectionHeader confidence="0.996825">
4.1 Patterns
</subsectionHeader>
<bodyText confidence="0.9998305">
For the purposes of this section, let us define the
control set C as being formed by the seed examples
plus all the selected examples over the previous it-
erations (only the infix considered, not the context).
</bodyText>
<page confidence="0.992573">
52
</page>
<bodyText confidence="0.99695125">
Note that, except for the seed examples, this is only
assumed correct, but cannot be guaranteed to be cor-
rect (unsupervised). In addition, let us define the in-
stance set Ip of a candidate pattern p as the set of
all the instances of the pattern found in a fraction of
the unlabelled corpus (only infix of the instance con-
sidered). Each candidate pattern pat is assigned two
partial scores:
</bodyText>
<listItem confidence="0.994305">
1. A frequency-based score freq sc(p) that mea-
sures the coverage of the pattern in (a section
of) the unsupervised corpus:
freq sc(p) = Card(Ip fl C)
2. A precision score prec sc(p) that evaluates the
precision of the pattern in (a section of) the un-
supervised corpus, measured against the con-
trol set:
</listItem>
<equation confidence="0.97897">
Card(-T nC)
prec sc (p) = Card(-T,)
</equation>
<bodyText confidence="0.999718090909091">
These two scores are computed only against a
fraction of the unlabelled corpus for time effi-
ciency. There remains an issue with whether multi-
sets (counting each repeated instance several times)
or normal sets (counting them only once) should be
used for the instance sets Ip. Our experiments indi-
cate that the best results are obtained by employing
multisets for the frequency-based score and normal
sets for the precision score.
Given the two partial scores above, we have tried
three different strategies for combining them:
</bodyText>
<listItem confidence="0.9995079">
• Multiplicative combination: A1 log(61 +
freq sc(p)) + A2 log(62 + prec sc(p))
• The strategy suggested in (Collins and Singer,
1999): Patterns are first filtered by imposing
a threshold on their precision score. Only for
those patterns that pass this first filter, their final
score is considered to be their frequency-based
score.
• The strategy suggested in (Riloff, 1996):
r prec sc(p) · log(freq sc(p)) if prec sc(p) ≥ thr
</listItem>
<footnote confidence="0.715956">
5l 0 otherwise
</footnote>
<bodyText confidence="0.959376928571429">
among them are selected, or the list of candidate pat-
terns is exhausted, whichever happens first. The pur-
pose of this analysis of subsumptions is twofold: on
the one hand, it results in a cleaner output pattern
set by getting rid of redundant patterns; on the other
hand, it improves temporal efficiency by reducing
the number of patterns being handled in the last step
of the algorithm (i.e. searching for new candidate
examples).
In our scenario, a pattern p1 with instance set Ip1
is subsumed by a pattern p2 with instance set Ip,,
if Ip1 ⊂ Ip,,. We make a distinction among “theo-
retical” and “empirical” subsumptions. Theoretical
subsumptions are those that can be justified based on
theoretical grounds alone, from observing the form
of the patterns. Empirical subsumptions are those
cases where in fact one pattern subsumes another ac-
cording to the former definition, but this could only
be detected by having calculated their respective in-
stance sets a priori, which beats one of the purposes
of the analysis of subsumptions —namely, tempo-
ral efficiency—. We are only dealing with theoreti-
cal subsumptions here. A pattern theoretically sub-
sumes another pattern when either of these condi-
tions occur:
• The first pattern is identical to the second, ex-
cept that the first has fewer contextual PEs in
the prefix and/or the postfix.
</bodyText>
<listItem confidence="0.775017666666667">
• Part or all of the PEs of the first pattern are
identical to the corresponding PEs in the sec-
ond pattern, except for the fact that they are
of a more general type (element-wise); the re-
maining PEs are identical. To this end, we have
defined a partial order of generality in the PE
types (see section 3), as follows:
FORM � LEMMA � SEMCLASS; FORM � POS;
SYN � SYN-SEMC
• Both the above conditions (fewer contextual
PEs and of a more general type) happen at the
same time.
</listItem>
<subsectionHeader confidence="0.979774">
4.1.1 Analysis of subsumptions
</subsectionHeader>
<bodyText confidence="0.9998002">
Intertwined with the selection step, an analysis of
subsumptions is performed among the selected pat-
terns, and the patterns found to be subsumed by oth-
ers in the set are discarded. This is repeated until ei-
ther a maximum of m patterns with no subsumptions
</bodyText>
<subsectionHeader confidence="0.997849">
4.2 Learning Examples
</subsectionHeader>
<bodyText confidence="0.9999828">
An example is composed of the tokens which have
been identified as a potential time expression (which
we shall call the infix) plus a certain amount of left
and right context (from now on, the context) en-
coded alongside the infix. For ranking and selecting
</bodyText>
<page confidence="0.997265">
53
</page>
<bodyText confidence="0.999076">
examples, we first assign a score and select a num-
ber n of distinct infixes and, in a second stage, we
assign a score to each context of appearance of an
infix and select (at most) m contexts per infix. Our
scoring system for the infixes is adapted from (Pas¸ca
et al., 2006). Each distinct infix receives three par-
tial scores and the final score for the infix is a linear
combination of these, with the Az being parameters:
A1sim sc(ex) + A2pc sc(ex) + A3ctxt sc(ex)
</bodyText>
<listItem confidence="0.95276575">
1. A similarity-based score (sim sc(ex)), which
measures the semantic similarity (as per the
Lin’s similarity corpus (Lin, 1998)) of the
infix with respect to set of “accepted” output
examples from all previous iterations plus the
initial seeds. If w1, ... , wn are the tokens in
the infix (excluding stopwords); ej,1, ... , ej,mj
are the tokens in the j-th example of the set
E of seed plus output examples; and sv(x, y)
represents a similarity value, the similarity
Sim(wz) of the i-th word of the infix wrt
the seeds and output is given by Sim(wz) =
</listItem>
<equation confidence="0.784667">
E|E|
j=1 max(sv(wz, ej,1), ... , sv(wz, ej,mj)),
</equation>
<bodyText confidence="0.5459675">
and the similarity-based score of an in-
fix containing n words is given by
</bodyText>
<equation confidence="0.966653666666667">
En
i=1 log(1+Sim(wi)) .
n
</equation>
<listItem confidence="0.886068">
2. A phrase-completeness score (pc sc(ex)),
which measures the likelihood that the infix
is a complete time expression and not merely
a part of one, over the entire set of candidate
example: count(INFIX)
count(*INFIX*)
3. A context-based score (ctxt sc(ex)), intended
as a measure of the infix’s relevance. For each
context (up to a length) where this infix appears
in the corpus, the frequency of the word with
maximum relative frequency (over the words
in all the infix’s contexts) is taken. The sum
is then scaled by the relative frequency of this
particular infix.
</listItem>
<bodyText confidence="0.999487555555556">
Apart from the score associated with the infix,
each example (i.e. infix plus a context) receives
two additional frequency scores for the left and right
context part of the example respectively. Each of
these is given by the relative frequency of the token
with maximum frequency of that context, computed
over all the tokens that appear in all the contexts of
all the candidate examples. For each selected infix,
the m contexts with best score are selected.
</bodyText>
<sectionHeader confidence="0.999797" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996482">
5.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999956866666667">
As unsupervised data for our experiments, we use
the NW (newswire) category of LDC’s ACE 2005
Unsupervised Data Pool, containing 456 Mbytes of
data in 204K documents for a total of over 82 mil-
lion tokens. Simultaneously, we use a much smaller
labelled corpus (where the correct time expressions
are tagged) to measure the precision, recall and F1-
measure of the pattern set learned by the bootstrap-
ping process. This is the ACE 2005 corpus, contain-
ing 550 documents with 257K tokens and approx.
4650 time expression mentions. The labelled corpus
is split in two halves: one half is used to obtain the
initial seed examples from among the time expres-
sions found therein; the other half is used for eval-
uation. We are requiring that a pattern captures the
target time expression mention exactly (no misalign-
ment allowed at the boundaries), in order to count it
as a precision or recall hit.
We will also be interested in measuring the gain
in recall, that is, the difference between the recall
in the best iteration and the initial recall given by
the seeds. Also important is the number of iter-
ations after which the bootstrapping process con-
verges. In the case where the same F1- measure
mark is achieved in two experimental settings, ear-
lier convergence of the algorithm will be prefered.
Otherwise, better F1 and gain in recall are the pri-
mary goals.
In order to start with a set of seeds with high pre-
cision, we select them automatically, imposing that
a seed time expression must have precision above a
certain value (understood as the percentage, of all
the appearances of the sequence of tokens in the su-
pervised corpus, those in which it is tagged as a cor-
rect time expression). In the experiments presented
below, this threshold for precision of the seeds is
90% —in the half of the supervised corpus reserved
for extraction of seeds—. From those that pass this
filter, the ones that appear with greater frequency are
selected. For time expressions that have an identi-
cal digit pattern (e.g. two dates “@@ December”
or two years “@@@@”, where @ stands for any
digit), only one seed is taken. This approach sim-
ulates the human domain expert, which typically is
the first step in bootstrapping IE models
</bodyText>
<page confidence="0.995976">
54
</page>
<bodyText confidence="0.999089">
Unless specifically stated otherwise, all the exper-
iments presented below share the following default
</bodyText>
<listItem confidence="0.553267">
settings:
• Only the first 2.36 Mbytes of the unsupervised
</listItem>
<bodyText confidence="0.9998382">
corpus are used (10 Mbytes after tokenization
and feature extraction), that is 0.5% of the
available data. This is to keep the execution
time of experiments low, where multiple exper-
iments need to be run to optimize a certain pa-
</bodyText>
<listItem confidence="0.584542">
rameter.
• We use the Collins and Singer strategy (see
</listItem>
<bodyText confidence="0.858316428571429">
section 4.1) with a precision threshold of 0.50
for sub-score combination in pattern selection.
This strategy favours patterns with slightly
higher precision.
• The maximum length of prefix and postfix is 1
and 0 elements, respectively. This was deter-
mined experimentally.
</bodyText>
<listItem confidence="0.993052">
• 100 seed examples are used (out of a maximum
of 605 available).
• In the ranking of examples, the λi weights for
</listItem>
<bodyText confidence="0.878299666666667">
the three sub- scores for infixes are 0.5 for
the “similarity-based score”, 0.25 for “phrase-
completeness” and 0.25 for “context-based
</bodyText>
<listItem confidence="0.8114095">
score”.
• In the selection of examples, the maximum
</listItem>
<bodyText confidence="0.962625333333333">
number of new infixes accepted per iteration is
200, with a maximum of 50 different contexts
per infix. In the selection of patterns, the max-
imum number of new accepted patterns per it-
eration is 5000 (although this number is never
reached due to the analysis of subsumptions).
</bodyText>
<listItem confidence="0.989034">
• In the selection of patterns, multisets are used
</listItem>
<bodyText confidence="0.9298535">
for computing the instance set of a pattern
for the frequency-based score and normal sets
for the precision score (determined experimen-
tally).
</bodyText>
<listItem confidence="0.811248">
• The POS tag type of generalization (pattern el-
</listItem>
<bodyText confidence="0.998187857142857">
ement) has been deactivated, that is, neither all-
POS patterns, nor patterns that are combina-
tions of POS PEs with another are generated.
After an analysis of errors, it was observed that
POS generalizations (because of the fact that
they are not lexicalized like, for instance, the
syntactic PEs with a given headword) give rise
</bodyText>
<listItem confidence="0.874679">
to a considerable number of precision errors.
• All patterns are generated with COMPLETE-
</listItem>
<bodyText confidence="0.985193">
PHRASE modifier automatically attached. It
was determined experimentally that it was best
to use this heuristic in all cases (see section 3).
</bodyText>
<subsectionHeader confidence="0.99855">
5.2 Variation of the number of seeds
</subsectionHeader>
<bodyText confidence="0.99993784375">
We have performed experiments using 1, 5, 10, 20,
50, 100, 200 and 500 seeds. The general trends ob-
served were as follows. The final precision (when
the bootstrapping converges) decreases more or less
monotonically as the number of seeds increases, al-
though there are slight fluctuations; besides, the dif-
ference in this respect between using few seeds (20
to 50) or more (100 to 200) is of only around 3%.
However, a big leap can be observed in moving from
200 to 500 seeds, where both the initial precision
(of the seeds) and final precision (at point of con-
vergence) drop by 10% wrt to using 200 seeds. The
final recall increases monotonically as the number
of seeds increases —since more supervised informa-
tion is provided—. The final F1-measure first in-
creases and then decreases with an increasing num-
ber of seeds, with an optimum value being reached
somewhere between the 50 and 100 seeds.
The largest gain in recall (difference between re-
call of the seeds and recall at the point of con-
vergence) is achieved with 20 seeds, for a gain
of 16.38% (initial recall is 20.08% and final is
36.46%). The best mark in F1-measure is achieved
with 100 seeds, after 6 iterations: 60.43% (the final
precision is 69.29% and the final recall is 53.58%;
the drop in precision is 6.5% and the gain in recall is
14.28%). Figure 3 shows a line plot of precision vs
recall for these experiments. This experiment sug-
gests that the problem of temporal expression recog-
nition can be captured with minimal supervised in-
formation (100 seeds) and larger amounts of unsu-
pervised information.
</bodyText>
<figureCaption confidence="0.995531">
Figure 3: Effect of varying the number of seeds
</figureCaption>
<page confidence="0.992084">
55
</page>
<subsectionHeader confidence="0.6201875">
5.3 Variation of the type of generalizations
used in patterns
</subsectionHeader>
<bodyText confidence="0.999949946428572">
In these experiments, we have defined four differ-
ents sets of generalizations (i.e. types of pattern ele-
ments among those specified in section 3) to evalu-
ate how semantic and syntactic generalizations con-
tribute to performance of the algorithm. These four
experiments are labelled as follows: NONE includes
only PEs of the LEMMA type; SYN includes PEs
of the lemma type and of the not-generalized syn-
tactic chunk (SYN) type; SEM includes PEs of the
lemma type and of the semantic class (SEMCLASS)
type, as well as combinations of lemma with SEM-
CLASS PEs; and lastly, SYN+SEM includes every-
thing that both SYN and SEM experiments include,
plus PEs of the generalized syntactic chunk (SYN-
SEMC) type.
One can observe than neither type of generaliza-
tion, syntactic or semantic, is specially “effective”
when used in isolation (only a 3.5% gain in recall in
both cases). It is only the combination of both types
that gives a good gain in recall (14.28% in the case
of this experiment). Figure 4 shows a line plot of this
experiment. The figure indicates that the problem of
temporal expression recognition, even though appar-
ently simple, requires both syntactic and semantic
information for efficient modeling.
roughly a fifth part, respectively. The objective
of these experiments is to determine whether
performance improves as the amount of training
data is increased. The number of seeds passed to
the bootstrapping is 68. The maximum number of
new infixes (the part of an example that contains a
candidate time expression) accepted per iteration
has been increased from 200 to 1000, because it
was observed that larger amounts of unsupervised
training data need a greater number of selection
“slots” in order to render an improvement (that is, a
more “reckless” bootstrapping), otherwise they will
fill up all the allowed selection slots.
The observed effect is that both the drop in preci-
sion (from the initial iteration to the point of conver-
gence) and the gain in recall improve more or less
consistently as a larger amount of training data is
taken, or otherwise the same recall point is achieved
in an earlier iteration. These improvements are nev-
ertheless slight, in the order of between 0.5% and
2%. The biggest improvement is observed in the 100
Mbytes experiment, where recall after 5 iterations is
6% better than in the 50 Mbytes experiment after 7
iterations. The drop in precision in the 100 Mbytes
experiment is 13.05%, for a gain in recall of 21.36%
(final precision is 71.02%, final recall 52.84% and
final F1 60.59%). Figure 5 shows a line plot of this
experiment. This experiment indicates that increas-
ing amounts of unsupervised data can be used to im-
prove the performance of our model, but the task is
not trivial.
</bodyText>
<figureCaption confidence="0.91734">
Figure 4: Effect of using syntactic and/or semantic gen-
eralizations
5.4 Variation of the size of unsupervised data
used
</figureCaption>
<bodyText confidence="0.8041288">
We performed experiments using increasing
amounts of unsupervised data for training in the
bootstrapping: 1, 5, 10, 50 and 100 Mbytes of
preprocessed corpus (tokenized and with feature
extraction). The amounts of plain text data are
</bodyText>
<figureCaption confidence="0.9730495">
Figure 5: Effect of varying the amount of unsupervised
training data
</figureCaption>
<sectionHeader confidence="0.987269" genericHeader="conclusions">
6 Conclusions and future research
</sectionHeader>
<bodyText confidence="0.9998825">
We have presented a slightly supervised algorithm
for the extraction of IE patterns for the recognition
</bodyText>
<page confidence="0.985991">
56
</page>
<bodyText confidence="0.999974136363636">
of time expressions, based on bootstrapping, which
introduces a novel representation of patterns suited
to this task. Our experiments show that with a rel-
atively small amount of supervision (50 to 100 ini-
tial correct examples or seeds) and using a combina-
tion of syntactic and semantic generalizations, it is
possible to obtain an improvement of around 15%-
20% in recall (with regard to the seeds) and F1-
measure over 60% learning exclusively from unla-
belled data. Furthermore, using increasing amounts
of unlabelled training data (of which there is plenty
available) is a workable way to obtain small im-
provements in performance, at the expense of train-
ing time. Our current focus is on addressing specific
problems that appear on inspection of the precision
errors in test, which can improve both precision and
recall to a degree. Future planned lines of research
include using WordNet for improving the semantic
aspects of the algorithm (semantic classes and simi-
larity), and studying forms of combining the patterns
obtained in this semi-supervised approach with su-
pervised learning.
</bodyText>
<sectionHeader confidence="0.999455" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999858613333333">
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proceedings of
the Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, pages 100–110, College Park, MD. ACL.
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. Tides 2005 standard for the annotation of
temporal expressions. Technical report, MITRE Cor-
poration.
E. Filatova and E. Hovy. 2001. Assigning time-stamps to
event-clauses. In Proceedings of the 2001 ACL Work-
shop on Temporal and Spatial Information Processing,
pages 88–95.
K. Hacioglu, Y. Chen, and B. Douglas. 2005. Automatic
time expression labelling for english and chinese text.
In Proc. of the 6th International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing), pages 548–559. Springer.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Seman-
tic class learning from the web with hyponym pattern
linkage graphs. In Proc. of the Association for Com-
putational Linguistics 2008 (ACL-2008:HLT), pages
1048–1056.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th International
Conference on Computational Linguistics and the 36th
Annual Meeting of the Association for Computational
Linguistics (COLING-ACL-98), pages 768–774, Mon-
treal, Quebec. ACL.
I. Mani and G. Wilson. 2000. Robust temporal process-
ing of news. In Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguistics,
pages 69–76, Morristown, NJ, USA. ACL.
M. Pas¸ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Proceedings of the 21th In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the ACL, pages 809–816.
ACL.
E. Riloff. 1996. Automatically generating extraction pat-
terns from untagged text. In Proceedings of the Thir-
teenth National Conference on Artificial Intelligence
(AAAI-96), pages 1044–1049. AAAI/MIT Press.
E. Saquete, R. Mu˜noz, and P. Martinez-Barco. 2004.
Event ordering using terseo system. In Proc. of the
9th International Conference on Application of Natu-
ral Language to Information Systems (NLDB), pages
39–50. Springer.
M. Stevenson and M. Greenwood. 2005. A semantic
approach to IE pattern induction. In Proceedings of
the 43rd Meeting of the Association for Computational
Linguistics, pages 379–386. ACL.
M. Surdeanu, J. Turmo, and A. Ageno. 2006. A hybrid
approach for the acquisition of information extraction
patterns. In Proceedings of the EACL 2006 Workshop
on Adaptive Text Extraction and Mining (ATEM 2006).
ACL.
J. M. Wiebe, T. P. O’Hara, T. Ohrstrom-Sandgren, and
K. J. McKeever. 1998. An empirical approach to tem-
poral reference resolution. Journal ofArtificial Intelli-
gence Research, 9:247–293.
R. Yangarber, R. Grishman, P. Tapanainen, and
S. Hutunen. 2000. Automatic acquisition of domain
knowledge for information extraction. In Proceedings
of the 18th International Conference of Computational
Linguistics, pages 940–946.
R. Yangarber. 2003. Counter-training in discovery of
semantic patterns. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics. ACL.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189–196, Cam-
bridge, MA. ACL.
</reference>
<page confidence="0.999144">
57
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.123735">
<title confidence="0.999022">An Analysis of Bootstrapping for the Recognition of Temporal Expressions</title>
<author confidence="0.873356">Mihai</author>
<affiliation confidence="0.654685333333333">NLP Stanford Stanford,</affiliation>
<email confidence="0.999911">mihais@stanford.edu</email>
<author confidence="0.906493">Jordi</author>
<affiliation confidence="0.998376">TALP Research Technical University of Catalonia</affiliation>
<address confidence="0.73302">Barcelona,</address>
<email confidence="0.999909">jpoveda@lsi.upc.edu</email>
<author confidence="0.911688">Jordi</author>
<affiliation confidence="0.998394">TALP Research Technical University of Catalonia</affiliation>
<address confidence="0.660779">Barcelona,</address>
<email confidence="0.999931">turmo@lsi.upc.edu</email>
<abstract confidence="0.99516836">We present a semi-supervised (bootstrapping) approach to the extraction of time expression mentions in large unlabelled corpora. Because the only supervision is in the form of seed examples, it becomes necessary to resort to heuristics to rank and filter out spurious patterns and candidate time expressions. The application of bootstrapping to time expression recognition is, to the best of our knowledge, novel. In this paper, we describe one such architecture for bootstrapping Information Extraction (IE) patterns —suited to the extraction of entities, as opposed to events or relations— and summarize our experimental findings. These point out to the fact that a pattern set with a good increase in recall with respect to the seeds is achievable within our framework while, on the other side, the decrease in precision in successive iterations is succesfully controlled through the use of ranking and selection heuristics. Experiments are still underway to achieve the best use of these heuristics and other parameters of the bootstrapping algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>100--110</pages>
<publisher>ACL.</publisher>
<location>College Park, MD.</location>
<contexts>
<context position="3962" citStr="Collins and Singer, 1999" startWordPosition="599" endWordPosition="602">), or on its own (Hacioglu et al., 2005). The latter apply SVMs to the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an F1-measure of 87.8%. 1http://www.nist.gov/speech/tests/ace/ 49 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Bootstrapping techniques have been used for such diverse NLP problems as: word sense disambiguation (Yarowsky, 1995), named entity classification (Collins and Singer, 1999), IE pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005), document classification (Surdeanu et al., 2006), fact extraction from the web (Pas¸ca et al., 2006) and hyponymy relation extraction (Kozareva et al., 2008). (Yarowsky, 1995) used bootstrapping to train decision list classifiers to disambiguate between two senses of a word, achieving impressive classification accuracy. (Collins and Singer, 1999) applied bootstrapping to extract rules for named entity (NE) classification, seeding the sytem with a few handcrafted rules. Their main inn</context>
<context position="17154" citStr="Collins and Singer, 1999" startWordPosition="2778" endWordPosition="2781">gainst a fraction of the unlabelled corpus for time efficiency. There remains an issue with whether multisets (counting each repeated instance several times) or normal sets (counting them only once) should be used for the instance sets Ip. Our experiments indicate that the best results are obtained by employing multisets for the frequency-based score and normal sets for the precision score. Given the two partial scores above, we have tried three different strategies for combining them: • Multiplicative combination: A1 log(61 + freq sc(p)) + A2 log(62 + prec sc(p)) • The strategy suggested in (Collins and Singer, 1999): Patterns are first filtered by imposing a threshold on their precision score. Only for those patterns that pass this first filter, their final score is considered to be their frequency-based score. • The strategy suggested in (Riloff, 1996): r prec sc(p) · log(freq sc(p)) if prec sc(p) ≥ thr 5l 0 otherwise among them are selected, or the list of candidate patterns is exhausted, whichever happens first. The purpose of this analysis of subsumptions is twofold: on the one hand, it results in a cleaner output pattern set by getting rid of redundant patterns; on the other hand, it improves tempor</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 100–110, College Park, MD. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ferro</author>
<author>L Gerber</author>
<author>I Mani</author>
<author>B Sundheim</author>
<author>G Wilson</author>
</authors>
<title>Tides</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>MITRE Corporation.</institution>
<contexts>
<context position="2604" citStr="Ferro et al., 2005" startWordPosition="396" endWordPosition="399">hour), contextdependent times (today, last year), vague references (somewhere in the middle of June, the near future) or times that are indicated by an event (the day G. Bush was reelected). This problem is a subpart of a task called TERN (Temporal Expression Recognition and Normalization), where temporal expressions are first identified in text and then its intended temporal meaning is represented in a canonical format. TERN was first proposed as an independent task in the 2004 edition of the ACE conferences1. The most widely used standard for the annotation of temporal expressions is TIMEX (Ferro et al., 2005). The most common approach to temporal expression recognition in the past has been the use of hand-made grammars to capture the expressions (see (Wiebe et al., 1998; Filatova and Hovy, 2001; Saquete et al., 2004) for examples), which can then be easily expanded with additional attributes for the normalization task, based on computing distance and direction (past or future) with respect to a reference time. This approach achieves an F1-measure of approximately 85% for recognition and normalization. The use of machine learning techniques — mainly statistical— for this task is a more recent devel</context>
</contexts>
<marker>Ferro, Gerber, Mani, Sundheim, Wilson, 2005</marker>
<rawString>L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wilson. 2005. Tides 2005 standard for the annotation of temporal expressions. Technical report, MITRE Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Filatova</author>
<author>E Hovy</author>
</authors>
<title>Assigning time-stamps to event-clauses.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 ACL Workshop on Temporal and Spatial Information Processing,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="2793" citStr="Filatova and Hovy, 2001" startWordPosition="428" endWordPosition="431">ted). This problem is a subpart of a task called TERN (Temporal Expression Recognition and Normalization), where temporal expressions are first identified in text and then its intended temporal meaning is represented in a canonical format. TERN was first proposed as an independent task in the 2004 edition of the ACE conferences1. The most widely used standard for the annotation of temporal expressions is TIMEX (Ferro et al., 2005). The most common approach to temporal expression recognition in the past has been the use of hand-made grammars to capture the expressions (see (Wiebe et al., 1998; Filatova and Hovy, 2001; Saquete et al., 2004) for examples), which can then be easily expanded with additional attributes for the normalization task, based on computing distance and direction (past or future) with respect to a reference time. This approach achieves an F1-measure of approximately 85% for recognition and normalization. The use of machine learning techniques — mainly statistical— for this task is a more recent development, either alongside the traditional handgrammar approach to learn to distinguish specific difficult cases (Mani and Wilson, 2000), or on its own (Hacioglu et al., 2005). The latter app</context>
</contexts>
<marker>Filatova, Hovy, 2001</marker>
<rawString>E. Filatova and E. Hovy. 2001. Assigning time-stamps to event-clauses. In Proceedings of the 2001 ACL Workshop on Temporal and Spatial Information Processing, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hacioglu</author>
<author>Y Chen</author>
<author>B Douglas</author>
</authors>
<title>Automatic time expression labelling for english and chinese text.</title>
<date>2005</date>
<booktitle>In Proc. of the 6th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing),</booktitle>
<pages>548--559</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3377" citStr="Hacioglu et al., 2005" startWordPosition="520" endWordPosition="523"> et al., 1998; Filatova and Hovy, 2001; Saquete et al., 2004) for examples), which can then be easily expanded with additional attributes for the normalization task, based on computing distance and direction (past or future) with respect to a reference time. This approach achieves an F1-measure of approximately 85% for recognition and normalization. The use of machine learning techniques — mainly statistical— for this task is a more recent development, either alongside the traditional handgrammar approach to learn to distinguish specific difficult cases (Mani and Wilson, 2000), or on its own (Hacioglu et al., 2005). The latter apply SVMs to the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an F1-measure of 87.8%. 1http://www.nist.gov/speech/tests/ace/ 49 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Bootstrapping techniques have been used for such diverse NLP problems as: word sense disambiguation (Yarowsky, 1995), named entity classification (Collins and Singer, 1999), IE pattern ac</context>
</contexts>
<marker>Hacioglu, Chen, Douglas, 2005</marker>
<rawString>K. Hacioglu, Y. Chen, and B. Douglas. 2005. Automatic time expression labelling for english and chinese text. In Proc. of the 6th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing), pages 548–559. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Riloff</author>
<author>E Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proc. of the Association for Computational Linguistics</booktitle>
<pages>1048--1056</pages>
<contexts>
<context position="4231" citStr="Kozareva et al., 2008" startWordPosition="638" endWordPosition="641">s of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Bootstrapping techniques have been used for such diverse NLP problems as: word sense disambiguation (Yarowsky, 1995), named entity classification (Collins and Singer, 1999), IE pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005), document classification (Surdeanu et al., 2006), fact extraction from the web (Pas¸ca et al., 2006) and hyponymy relation extraction (Kozareva et al., 2008). (Yarowsky, 1995) used bootstrapping to train decision list classifiers to disambiguate between two senses of a word, achieving impressive classification accuracy. (Collins and Singer, 1999) applied bootstrapping to extract rules for named entity (NE) classification, seeding the sytem with a few handcrafted rules. Their main innovation was to split training in two alternate stages: during one step, only contextual rules are sought; during the second step, the new contextual rules are used to tag further NEs and these are used to produce new spelling rules. Bootstrapping approaches are employe</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In Proc. of the Association for Computational Linguistics 2008 (ACL-2008:HLT), pages 1048–1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics (COLING-ACL-98),</booktitle>
<pages>768--774</pages>
<publisher>ACL.</publisher>
<location>Montreal, Quebec.</location>
<contexts>
<context position="13101" citStr="Lin, 1998" startWordPosition="2108" endWordPosition="2109">s with a given POS. 4. Lemma PEs: Match tokens with a given lemma. 5. Syntactic chunk PEs: Match a sequence of tokens that is a syntactic chunk of a given type (e.g. NP) and whose headword has the same lemma as indicated. 6. Generalized syntactic PEs: Same as the previous, but the lemma of the headword may be any in a given semantic similarity class. The semantic similarity class of a word is defined as the word itself plus a group of other semantically similar words. For computing these, we employ Lin’s corpus of pairwise distributional similarities among words (nouns, verbs and adjectives) (Lin, 1998), filtered to include only those words whose similarity value is above both an absolute (highest n) and relative (to the highest similarity value in the class) threshold. Even after filtering, Lin’s similarities can be “noisy”, since the corpus has been constructed relying on purely statistical means. Therefore, we are employing in addition a set of manually defined semantic classes (hardcoded lists) sensitive to our domain of temporal expressions, such that these lists “override” the Lin’s similarity corpus whenever the semantic class of a word present in them is involved. The manually define</context>
<context position="20438" citStr="Lin, 1998" startWordPosition="3350" endWordPosition="3351"> selecting 53 examples, we first assign a score and select a number n of distinct infixes and, in a second stage, we assign a score to each context of appearance of an infix and select (at most) m contexts per infix. Our scoring system for the infixes is adapted from (Pas¸ca et al., 2006). Each distinct infix receives three partial scores and the final score for the infix is a linear combination of these, with the Az being parameters: A1sim sc(ex) + A2pc sc(ex) + A3ctxt sc(ex) 1. A similarity-based score (sim sc(ex)), which measures the semantic similarity (as per the Lin’s similarity corpus (Lin, 1998)) of the infix with respect to set of “accepted” output examples from all previous iterations plus the initial seeds. If w1, ... , wn are the tokens in the infix (excluding stopwords); ej,1, ... , ej,mj are the tokens in the j-th example of the set E of seed plus output examples; and sv(x, y) represents a similarity value, the similarity Sim(wz) of the i-th word of the infix wrt the seeds and output is given by Sim(wz) = E|E| j=1 max(sv(wz, ej,1), ... , sv(wz, ej,mj)), and the similarity-based score of an infix containing n words is given by En i=1 log(1+Sim(wi)) . n 2. A phrase-completeness s</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics (COLING-ACL-98), pages 768–774, Montreal, Quebec. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>G Wilson</author>
</authors>
<title>Robust temporal processing of news.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>69--76</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3338" citStr="Mani and Wilson, 2000" startWordPosition="512" endWordPosition="515"> to capture the expressions (see (Wiebe et al., 1998; Filatova and Hovy, 2001; Saquete et al., 2004) for examples), which can then be easily expanded with additional attributes for the normalization task, based on computing distance and direction (past or future) with respect to a reference time. This approach achieves an F1-measure of approximately 85% for recognition and normalization. The use of machine learning techniques — mainly statistical— for this task is a more recent development, either alongside the traditional handgrammar approach to learn to distinguish specific difficult cases (Mani and Wilson, 2000), or on its own (Hacioglu et al., 2005). The latter apply SVMs to the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an F1-measure of 87.8%. 1http://www.nist.gov/speech/tests/ace/ 49 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Bootstrapping techniques have been used for such diverse NLP problems as: word sense disambiguation (Yarowsky, 1995), named entity classification (C</context>
</contexts>
<marker>Mani, Wilson, 2000</marker>
<rawString>I. Mani and G. Wilson. 2000. Robust temporal processing of news. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 69–76, Morristown, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Names and similarities on the web: Fact extraction in the fast lane.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21th International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>809--816</pages>
<publisher>ACL.</publisher>
<marker>Pas¸ca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>M. Pas¸ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006. Names and similarities on the web: Fact extraction in the fast lane. In Proceedings of the 21th International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 809–816. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96),</booktitle>
<pages>1044--1049</pages>
<publisher>AAAI/MIT Press.</publisher>
<contexts>
<context position="4000" citStr="Riloff, 1996" startWordPosition="606" endWordPosition="607">r apply SVMs to the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an F1-measure of 87.8%. 1http://www.nist.gov/speech/tests/ace/ 49 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Bootstrapping techniques have been used for such diverse NLP problems as: word sense disambiguation (Yarowsky, 1995), named entity classification (Collins and Singer, 1999), IE pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005), document classification (Surdeanu et al., 2006), fact extraction from the web (Pas¸ca et al., 2006) and hyponymy relation extraction (Kozareva et al., 2008). (Yarowsky, 1995) used bootstrapping to train decision list classifiers to disambiguate between two senses of a word, achieving impressive classification accuracy. (Collins and Singer, 1999) applied bootstrapping to extract rules for named entity (NE) classification, seeding the sytem with a few handcrafted rules. Their main innovation was to split training in two a</context>
<context position="17396" citStr="Riloff, 1996" startWordPosition="2818" endWordPosition="2819">s indicate that the best results are obtained by employing multisets for the frequency-based score and normal sets for the precision score. Given the two partial scores above, we have tried three different strategies for combining them: • Multiplicative combination: A1 log(61 + freq sc(p)) + A2 log(62 + prec sc(p)) • The strategy suggested in (Collins and Singer, 1999): Patterns are first filtered by imposing a threshold on their precision score. Only for those patterns that pass this first filter, their final score is considered to be their frequency-based score. • The strategy suggested in (Riloff, 1996): r prec sc(p) · log(freq sc(p)) if prec sc(p) ≥ thr 5l 0 otherwise among them are selected, or the list of candidate patterns is exhausted, whichever happens first. The purpose of this analysis of subsumptions is twofold: on the one hand, it results in a cleaner output pattern set by getting rid of redundant patterns; on the other hand, it improves temporal efficiency by reducing the number of patterns being handled in the last step of the algorithm (i.e. searching for new candidate examples). In our scenario, a pattern p1 with instance set Ip1 is subsumed by a pattern p2 with instance set Ip</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), pages 1044–1049. AAAI/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Saquete</author>
<author>R Mu˜noz</author>
<author>P Martinez-Barco</author>
</authors>
<title>Event ordering using terseo system.</title>
<date>2004</date>
<booktitle>In Proc. of the 9th International Conference on Application of Natural Language to Information Systems (NLDB),</booktitle>
<pages>39--50</pages>
<publisher>Springer.</publisher>
<marker>Saquete, Mu˜noz, Martinez-Barco, 2004</marker>
<rawString>E. Saquete, R. Mu˜noz, and P. Martinez-Barco. 2004. Event ordering using terseo system. In Proc. of the 9th International Conference on Application of Natural Language to Information Systems (NLDB), pages 39–50. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>M Greenwood</author>
</authors>
<title>A semantic approach to IE pattern induction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>379--386</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4073" citStr="Stevenson and Greenwood, 2005" startWordPosition="614" endWordPosition="617">utput of several human-made taggers as additional features for the classifier, and report an F1-measure of 87.8%. 1http://www.nist.gov/speech/tests/ace/ 49 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Bootstrapping techniques have been used for such diverse NLP problems as: word sense disambiguation (Yarowsky, 1995), named entity classification (Collins and Singer, 1999), IE pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005), document classification (Surdeanu et al., 2006), fact extraction from the web (Pas¸ca et al., 2006) and hyponymy relation extraction (Kozareva et al., 2008). (Yarowsky, 1995) used bootstrapping to train decision list classifiers to disambiguate between two senses of a word, achieving impressive classification accuracy. (Collins and Singer, 1999) applied bootstrapping to extract rules for named entity (NE) classification, seeding the sytem with a few handcrafted rules. Their main innovation was to split training in two alternate stages: during one step, only contextual rules are sought; durin</context>
</contexts>
<marker>Stevenson, Greenwood, 2005</marker>
<rawString>M. Stevenson and M. Greenwood. 2005. A semantic approach to IE pattern induction. In Proceedings of the 43rd Meeting of the Association for Computational Linguistics, pages 379–386. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>J Turmo</author>
<author>A Ageno</author>
</authors>
<title>A hybrid approach for the acquisition of information extraction patterns.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 Workshop on Adaptive Text Extraction and Mining (ATEM</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="4122" citStr="Surdeanu et al., 2006" startWordPosition="620" endWordPosition="623">s for the classifier, and report an F1-measure of 87.8%. 1http://www.nist.gov/speech/tests/ace/ 49 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Bootstrapping techniques have been used for such diverse NLP problems as: word sense disambiguation (Yarowsky, 1995), named entity classification (Collins and Singer, 1999), IE pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005), document classification (Surdeanu et al., 2006), fact extraction from the web (Pas¸ca et al., 2006) and hyponymy relation extraction (Kozareva et al., 2008). (Yarowsky, 1995) used bootstrapping to train decision list classifiers to disambiguate between two senses of a word, achieving impressive classification accuracy. (Collins and Singer, 1999) applied bootstrapping to extract rules for named entity (NE) classification, seeding the sytem with a few handcrafted rules. Their main innovation was to split training in two alternate stages: during one step, only contextual rules are sought; during the second step, the new contextual rules are u</context>
</contexts>
<marker>Surdeanu, Turmo, Ageno, 2006</marker>
<rawString>M. Surdeanu, J. Turmo, and A. Ageno. 2006. A hybrid approach for the acquisition of information extraction patterns. In Proceedings of the EACL 2006 Workshop on Adaptive Text Extraction and Mining (ATEM 2006). ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Wiebe</author>
<author>T P O’Hara</author>
<author>T Ohrstrom-Sandgren</author>
<author>K J McKeever</author>
</authors>
<title>An empirical approach to temporal reference resolution.</title>
<date>1998</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>9--247</pages>
<marker>Wiebe, O’Hara, Ohrstrom-Sandgren, McKeever, 1998</marker>
<rawString>J. M. Wiebe, T. P. O’Hara, T. Ohrstrom-Sandgren, and K. J. McKeever. 1998. An empirical approach to temporal reference resolution. Journal ofArtificial Intelligence Research, 9:247–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
<author>R Grishman</author>
<author>P Tapanainen</author>
<author>S Hutunen</author>
</authors>
<title>Automatic acquisition of domain knowledge for information extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference of Computational Linguistics,</booktitle>
<pages>940--946</pages>
<contexts>
<context position="4024" citStr="Yangarber et al., 2000" startWordPosition="608" endWordPosition="611">o the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an F1-measure of 87.8%. 1http://www.nist.gov/speech/tests/ace/ 49 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Bootstrapping techniques have been used for such diverse NLP problems as: word sense disambiguation (Yarowsky, 1995), named entity classification (Collins and Singer, 1999), IE pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005), document classification (Surdeanu et al., 2006), fact extraction from the web (Pas¸ca et al., 2006) and hyponymy relation extraction (Kozareva et al., 2008). (Yarowsky, 1995) used bootstrapping to train decision list classifiers to disambiguate between two senses of a word, achieving impressive classification accuracy. (Collins and Singer, 1999) applied bootstrapping to extract rules for named entity (NE) classification, seeding the sytem with a few handcrafted rules. Their main innovation was to split training in two alternate stages: during </context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Hutunen, 2000</marker>
<rawString>R. Yangarber, R. Grishman, P. Tapanainen, and S. Hutunen. 2000. Automatic acquisition of domain knowledge for information extraction. In Proceedings of the 18th International Conference of Computational Linguistics, pages 940–946.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
</authors>
<title>Counter-training in discovery of semantic patterns.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics. ACL.</booktitle>
<contexts>
<context position="4041" citStr="Yangarber, 2003" startWordPosition="612" endWordPosition="613">lone, using the output of several human-made taggers as additional features for the classifier, and report an F1-measure of 87.8%. 1http://www.nist.gov/speech/tests/ace/ 49 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Bootstrapping techniques have been used for such diverse NLP problems as: word sense disambiguation (Yarowsky, 1995), named entity classification (Collins and Singer, 1999), IE pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005), document classification (Surdeanu et al., 2006), fact extraction from the web (Pas¸ca et al., 2006) and hyponymy relation extraction (Kozareva et al., 2008). (Yarowsky, 1995) used bootstrapping to train decision list classifiers to disambiguate between two senses of a word, achieving impressive classification accuracy. (Collins and Singer, 1999) applied bootstrapping to extract rules for named entity (NE) classification, seeding the sytem with a few handcrafted rules. Their main innovation was to split training in two alternate stages: during one step, only co</context>
</contexts>
<marker>Yangarber, 2003</marker>
<rawString>R. Yangarber. 2003. Counter-training in discovery of semantic patterns. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<publisher>ACL.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3906" citStr="Yarowsky, 1995" startWordPosition="594" endWordPosition="595">pecific difficult cases (Mani and Wilson, 2000), or on its own (Hacioglu et al., 2005). The latter apply SVMs to the recognition task alone, using the output of several human-made taggers as additional features for the classifier, and report an F1-measure of 87.8%. 1http://www.nist.gov/speech/tests/ace/ 49 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Bootstrapping techniques have been used for such diverse NLP problems as: word sense disambiguation (Yarowsky, 1995), named entity classification (Collins and Singer, 1999), IE pattern acquisition (Riloff, 1996; Yangarber et al., 2000; Yangarber, 2003; Stevenson and Greenwood, 2005), document classification (Surdeanu et al., 2006), fact extraction from the web (Pas¸ca et al., 2006) and hyponymy relation extraction (Kozareva et al., 2008). (Yarowsky, 1995) used bootstrapping to train decision list classifiers to disambiguate between two senses of a word, achieving impressive classification accuracy. (Collins and Singer, 1999) applied bootstrapping to extract rules for named entity (NE) classification, seedin</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, MA. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>