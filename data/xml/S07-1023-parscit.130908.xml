<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008002">
<title confidence="0.980476">
CMU-AT: Semantic Distance and Background Knowledge for Identify-
ing Semantic Relations
</title>
<author confidence="0.992004">
Alicia Tribble
</author>
<affiliation confidence="0.981337">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.688027">
Pittsburgh, PA, USA
</address>
<email confidence="0.999478">
atribble@cs.cmu.edu
</email>
<author confidence="0.954066">
Scott E. Fahlman
</author>
<affiliation confidence="0.968792">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.688566">
Pittsburgh, PA, USA
</address>
<email confidence="0.999614">
sef@cs.cmu.edu
</email>
<sectionHeader confidence="0.995654" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999939785714286">
This system uses a background knowledge
base to identify semantic relations between
base noun phrases in English text, as eva-
luated in SemEval 2007, Task 4. Training
data for each relation is converted to state-
ments in the Scone Knowledge Representa-
tion Language. At testing time a new
Scone statement is created for the sentence
under scrutiny, and presence or absence of
a relation is calculated by comparing the
total semantic distance between the new
statement and all positive examples to the
total distance between the new statement
and all negative examples.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896166666666">
This paper introduces a knowledge-based approach
to the task of semantic relation classification, as
evaluated in SemEval 2007, Task 4: “Classifying
Relations Between Nominals”. In Task 4, a full
sentence is presented to the system, along with the
WordNet sense keys for two noun phrases which
appear there and the name of a semantic relation
(e.g. “cause-effect”). The system should return
“true” if a person reading the sentence would con-
clude that the relation holds between the two la-
beled noun phrases.
Our system represents a test sentence with a se-
mantic graph, including the relation being tested
and both of its proposed arguments. Semantic dis-
tance is calculated between this graph and a set of
graphs representing the training examples relevant
to the test sentence. A near-match between a test
sentence and a positive training example is evi-
dence that the same relation which holds in the
example also holds in the test. We compute se-
mantic distances to negative training examples as
well, comparing the total positive and negative
scores in order to decide whether a relation is true
or false in the test sentence.
</bodyText>
<sectionHeader confidence="0.975962" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999897846153846">
Many systems which perform well on related tasks
use syntactic features of the input sentence,
coupled with classification by machine learning.
This approach has been applied to problems like
compound noun interpretation (Rosario and Hearst
2001) and semantic role labeling (Gildea and Ju-
rafsky 2002).
In preparing our system for Task 4, we started
by applying a similar syntax-based feature analysis
to the trial data: 140 labeled examples of the rela-
tion “content-container”. In 10-fold cross-
validation with this data we achieved an average f-
score of 70.6, based on features similar to the sub-
set trees used for semantic role labeling in (Mo-
schitti 2004). For classification we applied the up-
dated tree-kernel package (Moschitti 2006), distri-
buted with the svm-light tool (Joachims 1999) for
learning Support Vector Machines (SVMs).
Training data for Task 4 is small, compared to
other tasks where machine learning is commonly
applied. We had difficulty finding a combination
of features which gave good performance in cross-
validation, but which did not result in a separate
support vector being stored for every training sen-
tence – a possible indicator of overfitting. As an
example, the ratio of support vectors to training
</bodyText>
<page confidence="0.973382">
121
</page>
<bodyText confidence="0.98489905">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 121–124,
Prague, June 2007. c�2007 Association for Computational Linguistics
examples for the experiment described above was
.97, nearly 1-to-1.
As a result of this analysis we started work on
our knowledge-based system, with the goal of us-
ing the two approaches together. We were also
motivated by an interest in using relation defini-
tions and background knowledge from WordNet to
greater advantage. The algorithm we used in our
final submission is similar to recent systems which
discover textual entailment relationships (Haghig-
hi, Ng et al. 2005; Zanzotto and Moschitti 2006).
It gives us a way to encode information from the
relation definitions directly, in the form of state-
ments in a knowledge representation language.
The inference rules that are learned by this system
from training examples are also easier to interpret
than the models generated by an SVM. In small-
data applications this can be an advantage.
</bodyText>
<sectionHeader confidence="0.996" genericHeader="method">
3 System Description: A Walk-Through
</sectionHeader>
<bodyText confidence="0.991803947368421">
The example sentence below is taken (in abbre-
viated form) from the training data for Task 4, Re-
lation 7 “Content-Container” (Girju, Hearst et al.
2007):
The kitchen holds a cooker.
We convert this positive example into a semantic
graph by creating a new instance of the relation
Contains and linking that instance to the WordNet
term for each labeled argument (&amp;quot;kitch-
en%1:06:00::&amp;quot;, &amp;quot;cooker%1:06:00::&amp;quot;). The result is
shown in Figure 1. WordNet sense keys (Fellbaum
1998) have been mapped to a term, a part of
speech (pos), and a sense number.
Figure 1. Semantic graph for the training example
&amp;quot;The kitchen holds a cooker&amp;quot;. Arguments are
represented by a WordNet term, part of speech,
and sense number.
This graph is instantiated as a statement using
the Scone Knowledge Representation System, or
</bodyText>
<equation confidence="0.9930085">
(new-statement {kitchen_n_1} {contains} {cooker_n_1})
n
(new-statement {artifact__1} {contains} {artifact_n_1})
(new-statement {whole_n_1} {contains} {whole_n_1})
</equation>
<figureCaption confidence="0.72936">
Figure 2. Statements in Scone KR syntax, based
</figureCaption>
<bodyText confidence="0.987280386363636">
on generalizing the training example &amp;quot;The kitchen
holds a cooker&amp;quot;.
“Scone” (Fahlman 2005). Scone gives us a way to
store, search, and perform inference on graphs like
the one shown above. After instantiating the graph
we generalize it using hypernym information from
WordNet. This generates additional Scone state-
ments which are stored in a knowledge base (KB),
shown in Figure 2. The first statement in the fig-
ure was generated verbatim from our training sen-
tence. The remaining statements contain hyper-
nyms of the original arguments.
For each argument seen in training, we also ex-
tract hypernyms and siblings from WordNet. For
the argument kitchen, we extract 101 ancestors
(artifact, whole, object, etc.) and siblings (struc-
ture, excavation, facility, etc.). A similar set of
WordNet entities is extracted for the argument
cooker. These entities, with repetitions removed,
are encoded in a second Scone knowledge base,
preserving the hierarchical (IS-A) links that come
from WordNet. The hierarchy is manually linked
at the top level into an existing background Scone
KB where entities like animate, inanimate, person,
location, and quantity are already defined.
After using the training data to create these two
KBs, the system is ready for a test sentence. The
following example is also adapted from SemEval
Task 4 training data:
Equipment was carried in a box.
First we convert the sentence to a semantic
graph, using the same technique as the one de-
scribed above. The graph is implemented as a new
Scone statement which includes the WordNet pos
and sense number for each of the arguments:
“box_n_1 contains equipment_n_1”.
Next, using inference operations in Scone, the
system verifies that the statement conforms to
high-level constraints imposed by the relation defi-
nition. If it does, we calculate semantic distances
between the argument nodes of our test statement
and the analogous nodes in relevant training state-
ments. A training statement is relevant if both of
its arguments are ancestors of the appropriate ar-
</bodyText>
<figure confidence="0.9673936">
kitchen_n_1
container content
Contains
{relation}
cooker_n_1
</figure>
<page confidence="0.976879">
122
</page>
<bodyText confidence="0.942925833333333">
guments of the test sentence. In our example, only
two of the three KB statements from Figure 2 are
relevant to the test statement “box contains equip-
ment”: “whole contains whole” and “artifact con-
tains artifact”. The first statement, “kitchen con-
tains cooker” fails to apply because kitchen is not
an ancestor of box, and also because cooker is not
an ancestor of equipment.
Figure 3 illustrates the distance from “box con-
tains equipment” to “whole contains whole”, calcu-
lated as the sum of the distances between box-
whole and equipment-whole.
</bodyText>
<figureCaption confidence="0.660847">
Figure 3. Calculating the distance through the
knowledge base between &amp;quot;equipment contains box&amp;quot;
and “whole contains whole”. Dashed lines indicate
IS-A links in the knowledge base.
</figureCaption>
<bodyText confidence="0.95737794117647">
Support = 1/2
The total number of these relevant, positive
training statements is an indicator of “support” for
the test sentence throughout the training data. The
distance between one such statement and the test
sentence is a measure of the strength of support.
To reach a verdict, we sum over the inverse dis-
tances to all arguments from positive relevant ex-
amples: in Figure 3, the test statement “box con-
tains equipment” receives a support score of (%2 +
%2 + 1 + 1), or 3.
Counter-evidence for a test sentence can be cal-
culated in the same way, using relevant negative
statements. In our example there are no negative
training statements, so the total positive support
score (3) is greater than the counter-evidence score
(0), and the system verdict is “true”.
</bodyText>
<sectionHeader confidence="0.98005" genericHeader="method">
4 System Components in Detail
</sectionHeader>
<bodyText confidence="0.9987052">
As the detailed example above shows, this system
is designed around its knowledge bases. The KBs
provide a consistent framework for representing
knowledge from a variety of sources as well as for
calculating semantic distance.
</bodyText>
<subsectionHeader confidence="0.997265">
4.1 Background knowledge
</subsectionHeader>
<bodyText confidence="0.999537181818182">
WordNet-extracted knowledge bases of the type
described in Section 3 are generated separately for
each relation. Average depth of these hierarchies
is 4; we store only hypernyms of WordNet depth 7
and above, based on experiments in the literature
by Nastase, et al. (2003; 2006).
Relation-specific and task-specific knowledge is
encoded by hand. For each relation, we examine
the relation definition and create a set of con-
straints in Scone formalism. For example, the de-
finition of “container-contains” includes the fol-
lowing restriction (taken from training data for
Task 4): There is strong preference against treat-
ing legal entities (people and institutions) as con-
tent.
In Scone, we encode this preference as a type
restriction on the container role of any Contains
relation: (new-is-not-a {container} {potential
agent})
During testing, before calculating semantic dis-
tances, the system checks whether the test state-
ment conforms to all such constraints.
</bodyText>
<subsectionHeader confidence="0.999645">
4.2 Calculating semantic distance
</subsectionHeader>
<bodyText confidence="0.999927">
Semantic distances are calculated between con-
cepts in the knowledge base, rather than through
WordNet directly. Distance between two KB en-
tites is calculated by counting the edges along the
shortest path between them, as illustrated in Figure
3. In the current implementation, only ancestors in
the IS-A hierarchy are considered relevant, so this
calculation amounts to counting the number of an-
cestors between an argument from the test sentence
and an argument from a training example. Quick
type-checking features which are built into Scone
allow us to skip the distance calculation for non-
relevant training examples.
</bodyText>
<sectionHeader confidence="0.99905" genericHeader="evaluation">
5 Results &amp; Conclusions
</sectionHeader>
<bodyText confidence="0.999892714285714">
This system performed reasonably well for relation
3, Product-Producer, outperforming the baseline
(baseline guesses “true” for every test sentence).
Performance for this relation was also higher than
the average F-score for all comparable groups in
Task 4 (all groups in class “B4”). Average recall
for this system over all relations was mid-range,
</bodyText>
<figure confidence="0.998968666666667">
Distance = 2
Support = 1/2
Distance = 2
Contains
{relation}
container content
whole whole
artifact artifact
Contains
{relation}
container content
box equipment
</figure>
<page confidence="0.997261">
123
</page>
<bodyText confidence="0.99920125">
compared to other participating groups. Average
precision and average f-score fell below the base-
line and below the average for all comparable
groups. These scores are given in Table 1.
</bodyText>
<table confidence="0.999729214285714">
Relation R P F
1. Cause-Effect 73.2 54.5 62.5
2. Instrument-Agency 76.3 50.9 61.1
3. Product-Producer 79.0 71.0 74.8
4. Origin-Entity 63.9 54.8 59.0
5. Theme-Tool 48.3 53.8 50.9
6. Part-Whole 57.7 45.5 50.8
7. Content-Container 68.4 59.1 63.4
Whole test set, not 57.1 68.9 62.4
divided by relation
Average for CMU-AT 66.7 55.7 60.4
Average for all B4 64.4 65.3 63.6
systems
Baseline: “alltrue” 100.0 48.5 64.8
</table>
<tableCaption confidence="0.755421">
Table 1. Recall, Precision, and F-scores, separated
by relation type. Baseline score is calculated by
guessing &amp;quot;true&amp;quot; for all test setences.
</tableCaption>
<bodyText confidence="0.999825272727273">
Analysis of the training data reveals that relation
3 is the class where target nouns occur most often
together in nominal compounds and base NPs, with
little additional syntax to connect them. While
other relations included sentences where the targets
were covered by a single VP, Product-Producer did
not. It seems that background knowledge plays a
larger role in identifying the Producer-Produces
relationship than it does for other relations. How-
ever this conclusion is softened by the fact that we
also spent more time in development and cross-
evaluation for relations 3 and 7, our two best per-
forming relations.
This system demonstrates a knowledge-based
framework that performs very well for certain re-
lations. Importantly, the system we submitted for
evaluation did not make use of syntactic features,
which are almost certainly relevant to this task.
We are already exploring methods for combining
the knowledge-based decision process with one
that uses syntactic evidence as well as corpus sta-
tistics, described in Section 2.
</bodyText>
<sectionHeader confidence="0.970028" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9853335">
This work was supported by a generous research
grant from Cisco Systems, and by the Defense Ad-
vanced Research Projects Agency (DARPA) under
contract number NBCHD030010.
</bodyText>
<sectionHeader confidence="0.988395" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999919409090909">
Fahlman, S. E. (2005). Scone User&apos;s Manual.
Fellbaum, C. (1998). WordNet An Electronic Lexical
Database, Bradford Books.
Gildea, D. and D. Jurafsky (2002). &amp;quot;Automatic labeling
of semantic roles.&amp;quot; Computational Linguistics 28(3):
245-288.
Girju, R., M. Hearst, et al. (2007). Classification of Se-
mantic Relations between Nominals: Dataset for
Task 4. SemEval 2007, 4th International Workshop
on Semantic Evaluations, Prague, Czech Republic.
Haghighi, A., A. Ng, et al. (2005). Robust Textual Infe-
rence via Graph Matching. Human Language Tech-
nology Conference and Conference on Empirical
Methods in Natural Language Processing, Vancou-
ver, British Columbia, Canada.
Joachims, T. (1999). Making large-scale SVM learning
practical. Advances in Kernel Methods - Support
Vector Learning. B. Schölkopf, C. Burges and A.
Smola.
Moschitti, A. (2004). A study on Convolution Kernel
for Shallow Semantic Parsing. proceedings of the
42nd Conference of the Association for Computa-
tional Linguistics (ACL-2004). Barcelona, Spain.
Moschitti, A. (2006). Making tree kernels practical for
natural language learning. Eleventh International
Conference on European Association for Computa-
tional Linguistics, Trento, Italy.
Nastase, V., J. S. Shirabad, et al. (2006). Learning noun-
modifier semantic relations with corpus-based and
Wordnet-based features. 21st National Conference on
Artificial Intelligence (AAAI-06), Boston, Massa-
chusetts.
Nastase, V. and S. Szpakowicz (2003). Exploring noun-
modifier semantic relations. IWCS 2003.
Rosario, B. and M. Hearst (2001). Classifying the se-
mantic relations in Noun Compounds. 2001 Confe-
rence on Empirical Methods in Natural Language
Processing.
Zanzotto, F. M. and A. Moschitti (2006). Automatic
Learning of Textual Entailments with Cross-Pair Si-
milarities. the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics
(ACL), Sydney, Austrailia.
</reference>
<page confidence="0.998319">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978603">
<title confidence="0.999183">CMU-AT: Semantic Distance and Background Knowledge for Identifying Semantic Relations</title>
<author confidence="0.999764">Alicia Tribble</author>
<affiliation confidence="0.9983885">Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.999702">Pittsburgh, PA, USA</address>
<email confidence="0.999621">atribble@cs.cmu.edu</email>
<author confidence="0.999772">Scott E Fahlman</author>
<affiliation confidence="0.995179">Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.999642">Pittsburgh, PA, USA</address>
<email confidence="0.999854">sef@cs.cmu.edu</email>
<abstract confidence="0.9996316">This system uses a background knowledge base to identify semantic relations between base noun phrases in English text, as evaluated in SemEval 2007, Task 4. Training data for each relation is converted to statements in the Scone Knowledge Representation Language. At testing time a new Scone statement is created for the sentence under scrutiny, and presence or absence of a relation is calculated by comparing the total semantic distance between the new statement and all positive examples to the total distance between the new statement and all negative examples.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S E Fahlman</author>
</authors>
<date>2005</date>
<journal>Scone User&apos;s Manual.</journal>
<contexts>
<context position="5418" citStr="Fahlman 2005" startWordPosition="843" endWordPosition="844">apped to a term, a part of speech (pos), and a sense number. Figure 1. Semantic graph for the training example &amp;quot;The kitchen holds a cooker&amp;quot;. Arguments are represented by a WordNet term, part of speech, and sense number. This graph is instantiated as a statement using the Scone Knowledge Representation System, or (new-statement {kitchen_n_1} {contains} {cooker_n_1}) n (new-statement {artifact__1} {contains} {artifact_n_1}) (new-statement {whole_n_1} {contains} {whole_n_1}) Figure 2. Statements in Scone KR syntax, based on generalizing the training example &amp;quot;The kitchen holds a cooker&amp;quot;. “Scone” (Fahlman 2005). Scone gives us a way to store, search, and perform inference on graphs like the one shown above. After instantiating the graph we generalize it using hypernym information from WordNet. This generates additional Scone statements which are stored in a knowledge base (KB), shown in Figure 2. The first statement in the figure was generated verbatim from our training sentence. The remaining statements contain hypernyms of the original arguments. For each argument seen in training, we also extract hypernyms and siblings from WordNet. For the argument kitchen, we extract 101 ancestors (artifact, wh</context>
</contexts>
<marker>Fahlman, 2005</marker>
<rawString>Fahlman, S. E. (2005). Scone User&apos;s Manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet An Electronic Lexical Database,</title>
<date>1998</date>
<location>Bradford Books.</location>
<contexts>
<context position="4793" citStr="Fellbaum 1998" startWordPosition="754" endWordPosition="755">sier to interpret than the models generated by an SVM. In smalldata applications this can be an advantage. 3 System Description: A Walk-Through The example sentence below is taken (in abbreviated form) from the training data for Task 4, Relation 7 “Content-Container” (Girju, Hearst et al. 2007): The kitchen holds a cooker. We convert this positive example into a semantic graph by creating a new instance of the relation Contains and linking that instance to the WordNet term for each labeled argument (&amp;quot;kitchen%1:06:00::&amp;quot;, &amp;quot;cooker%1:06:00::&amp;quot;). The result is shown in Figure 1. WordNet sense keys (Fellbaum 1998) have been mapped to a term, a part of speech (pos), and a sense number. Figure 1. Semantic graph for the training example &amp;quot;The kitchen holds a cooker&amp;quot;. Arguments are represented by a WordNet term, part of speech, and sense number. This graph is instantiated as a statement using the Scone Knowledge Representation System, or (new-statement {kitchen_n_1} {contains} {cooker_n_1}) n (new-statement {artifact__1} {contains} {artifact_n_1}) (new-statement {whole_n_1} {contains} {whole_n_1}) Figure 2. Statements in Scone KR syntax, based on generalizing the training example &amp;quot;The kitchen holds a cooker</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. (1998). WordNet An Electronic Lexical Database, Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.&amp;quot;</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<issue>3</issue>
<pages>245--288</pages>
<contexts>
<context position="2342" citStr="Gildea and Jurafsky 2002" startWordPosition="360" endWordPosition="364">nd a positive training example is evidence that the same relation which holds in the example also holds in the test. We compute semantic distances to negative training examples as well, comparing the total positive and negative scores in order to decide whether a relation is true or false in the test sentence. 2 Motivation Many systems which perform well on related tasks use syntactic features of the input sentence, coupled with classification by machine learning. This approach has been applied to problems like compound noun interpretation (Rosario and Hearst 2001) and semantic role labeling (Gildea and Jurafsky 2002). In preparing our system for Task 4, we started by applying a similar syntax-based feature analysis to the trial data: 140 labeled examples of the relation “content-container”. In 10-fold crossvalidation with this data we achieved an average fscore of 70.6, based on features similar to the subset trees used for semantic role labeling in (Moschitti 2004). For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). Training data for Task 4 is small, compared to other tasks where </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D. and D. Jurafsky (2002). &amp;quot;Automatic labeling of semantic roles.&amp;quot; Computational Linguistics 28(3): 245-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>M Hearst</author>
</authors>
<date>2007</date>
<booktitle>Classification of Semantic Relations between Nominals: Dataset for Task 4. SemEval 2007, 4th International Workshop on Semantic Evaluations,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Girju, Hearst, 2007</marker>
<rawString>Girju, R., M. Hearst, et al. (2007). Classification of Semantic Relations between Nominals: Dataset for Task 4. SemEval 2007, 4th International Workshop on Semantic Evaluations, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>A Ng</author>
</authors>
<title>Robust Textual Inference via Graph Matching.</title>
<date>2005</date>
<booktitle>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, British Columbia, Canada.</location>
<marker>Haghighi, Ng, 2005</marker>
<rawString>Haghighi, A., A. Ng, et al. (2005). Robust Textual Inference via Graph Matching. Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<contexts>
<context position="2831" citStr="Joachims 1999" startWordPosition="443" endWordPosition="444">to problems like compound noun interpretation (Rosario and Hearst 2001) and semantic role labeling (Gildea and Jurafsky 2002). In preparing our system for Task 4, we started by applying a similar syntax-based feature analysis to the trial data: 140 labeled examples of the relation “content-container”. In 10-fold crossvalidation with this data we achieved an average fscore of 70.6, based on features similar to the subset trees used for semantic role labeling in (Moschitti 2004). For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). Training data for Task 4 is small, compared to other tasks where machine learning is commonly applied. We had difficulty finding a combination of features which gave good performance in crossvalidation, but which did not result in a separate support vector being stored for every training sentence – a possible indicator of overfitting. As an example, the ratio of support vectors to training 121 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 121–124, Prague, June 2007. c�2007 Association for Computational </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Joachims, T. (1999). Making large-scale SVM learning practical. Advances in Kernel Methods - Support Vector Learning. B. Schölkopf, C. Burges and A. Smola.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>A study on Convolution Kernel for Shallow Semantic Parsing.</title>
<date>2004</date>
<booktitle>proceedings of the 42nd Conference of the Association for Computational Linguistics (ACL-2004).</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2698" citStr="Moschitti 2004" startWordPosition="423" endWordPosition="425">d tasks use syntactic features of the input sentence, coupled with classification by machine learning. This approach has been applied to problems like compound noun interpretation (Rosario and Hearst 2001) and semantic role labeling (Gildea and Jurafsky 2002). In preparing our system for Task 4, we started by applying a similar syntax-based feature analysis to the trial data: 140 labeled examples of the relation “content-container”. In 10-fold crossvalidation with this data we achieved an average fscore of 70.6, based on features similar to the subset trees used for semantic role labeling in (Moschitti 2004). For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). Training data for Task 4 is small, compared to other tasks where machine learning is commonly applied. We had difficulty finding a combination of features which gave good performance in crossvalidation, but which did not result in a separate support vector being stored for every training sentence – a possible indicator of overfitting. As an example, the ratio of support vectors to training 121 Proceedings of the 4th I</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Moschitti, A. (2004). A study on Convolution Kernel for Shallow Semantic Parsing. proceedings of the 42nd Conference of the Association for Computational Linguistics (ACL-2004). Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning.</title>
<date>2006</date>
<booktitle>Eleventh International Conference on European Association for Computational Linguistics,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="2778" citStr="Moschitti 2006" startWordPosition="435" endWordPosition="436">n by machine learning. This approach has been applied to problems like compound noun interpretation (Rosario and Hearst 2001) and semantic role labeling (Gildea and Jurafsky 2002). In preparing our system for Task 4, we started by applying a similar syntax-based feature analysis to the trial data: 140 labeled examples of the relation “content-container”. In 10-fold crossvalidation with this data we achieved an average fscore of 70.6, based on features similar to the subset trees used for semantic role labeling in (Moschitti 2004). For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). Training data for Task 4 is small, compared to other tasks where machine learning is commonly applied. We had difficulty finding a combination of features which gave good performance in crossvalidation, but which did not result in a separate support vector being stored for every training sentence – a possible indicator of overfitting. As an example, the ratio of support vectors to training 121 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 121–124, Pra</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Moschitti, A. (2006). Making tree kernels practical for natural language learning. Eleventh International Conference on European Association for Computational Linguistics, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Nastase</author>
<author>J S Shirabad</author>
</authors>
<title>Learning nounmodifier semantic relations with corpus-based and Wordnet-based features.</title>
<date>2006</date>
<booktitle>21st National Conference on Artificial Intelligence (AAAI-06),</booktitle>
<location>Boston, Massachusetts.</location>
<marker>Nastase, Shirabad, 2006</marker>
<rawString>Nastase, V., J. S. Shirabad, et al. (2006). Learning nounmodifier semantic relations with corpus-based and Wordnet-based features. 21st National Conference on Artificial Intelligence (AAAI-06), Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Nastase</author>
<author>S Szpakowicz</author>
</authors>
<title>Exploring nounmodifier semantic relations. IWCS</title>
<date>2003</date>
<marker>Nastase, Szpakowicz, 2003</marker>
<rawString>Nastase, V. and S. Szpakowicz (2003). Exploring nounmodifier semantic relations. IWCS 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rosario</author>
<author>M Hearst</author>
</authors>
<title>Classifying the semantic relations in Noun Compounds.</title>
<date>2001</date>
<booktitle>Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2288" citStr="Rosario and Hearst 2001" startWordPosition="352" endWordPosition="355">test sentence. A near-match between a test sentence and a positive training example is evidence that the same relation which holds in the example also holds in the test. We compute semantic distances to negative training examples as well, comparing the total positive and negative scores in order to decide whether a relation is true or false in the test sentence. 2 Motivation Many systems which perform well on related tasks use syntactic features of the input sentence, coupled with classification by machine learning. This approach has been applied to problems like compound noun interpretation (Rosario and Hearst 2001) and semantic role labeling (Gildea and Jurafsky 2002). In preparing our system for Task 4, we started by applying a similar syntax-based feature analysis to the trial data: 140 labeled examples of the relation “content-container”. In 10-fold crossvalidation with this data we achieved an average fscore of 70.6, based on features similar to the subset trees used for semantic role labeling in (Moschitti 2004). For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). Training da</context>
</contexts>
<marker>Rosario, Hearst, 2001</marker>
<rawString>Rosario, B. and M. Hearst (2001). Classifying the semantic relations in Noun Compounds. 2001 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>A Moschitti</author>
</authors>
<title>Automatic Learning of Textual Entailments with Cross-Pair Similarities.</title>
<date>2006</date>
<booktitle>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sydney, Austrailia.</location>
<contexts>
<context position="3945" citStr="Zanzotto and Moschitti 2006" startWordPosition="614" endWordPosition="617">op on Semantic Evaluations (SemEval-2007), pages 121–124, Prague, June 2007. c�2007 Association for Computational Linguistics examples for the experiment described above was .97, nearly 1-to-1. As a result of this analysis we started work on our knowledge-based system, with the goal of using the two approaches together. We were also motivated by an interest in using relation definitions and background knowledge from WordNet to greater advantage. The algorithm we used in our final submission is similar to recent systems which discover textual entailment relationships (Haghighi, Ng et al. 2005; Zanzotto and Moschitti 2006). It gives us a way to encode information from the relation definitions directly, in the form of statements in a knowledge representation language. The inference rules that are learned by this system from training examples are also easier to interpret than the models generated by an SVM. In smalldata applications this can be an advantage. 3 System Description: A Walk-Through The example sentence below is taken (in abbreviated form) from the training data for Task 4, Relation 7 “Content-Container” (Girju, Hearst et al. 2007): The kitchen holds a cooker. We convert this positive example into a s</context>
</contexts>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>Zanzotto, F. M. and A. Moschitti (2006). Automatic Learning of Textual Entailments with Cross-Pair Similarities. the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL), Sydney, Austrailia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>