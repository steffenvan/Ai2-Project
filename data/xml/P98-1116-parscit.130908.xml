<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.99535">
Generation that Exploits Corpus-Based Statistical Knowledge
</title>
<author confidence="0.990908">
Irene Langkilde and Kevin Knight
</author>
<affiliation confidence="0.825879666666667">
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
</affiliation>
<bodyText confidence="0.537565">
ilangk ile is i edu and knight° isi edu
</bodyText>
<sectionHeader confidence="0.961492" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999660181818182">
We describe novel aspects of a new natural lan-
guage generator called Nitrogen. This generator has
a highly flexible input representation that allows a
spectrum of input from syntactic to semantic depth,
and shifts the burden of many linguistic decisions
to the statistical post-processor. The generation al-
gorithm is compositional, making it efficient, yet it
also handles non-compositional aspects of language.
Nitrogen&apos;s design makes it robust and scalable, op-
erating with lexicons and knowledge bases of one
hundred thousand entities.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950894736842">
Language generation is an important subtask
of applications like machine translation, human-
computer dialogue, explanation, and summariza-
tion. The recurring need for generation suggests the
usefulness of a general-purpose, domain-independent
natural language generator (NLG). However, &amp;quot;plug-
in&amp;quot; generators available today, such as FUF/SURGE
(Elhadad and Robin, 1998), MUMBLE (Meteer et
al., 1987), KPML (Bateman, 1996), and CoGen-
Tex&apos;s RealPro (Lavoie and Rambow, 1997), require
inputs with a daunting amount of linguistic detail.
As a result, many client applications resort instead
to simpler template-based methods.
An important advantage of templates is that they
sidestep linguistic decision-making, and avoid the
need for large complex knowledge resources and pro-
cessing. For example, the following structure could
be a typical result from a database query on the type
of food a venue serves:
</bodyText>
<subsectionHeader confidence="0.365225">
((:obj-type venue)(:obj-name Top_of_the_Mark)
(:attribute food-type)(:attrib-value American))
</subsectionHeader>
<bodyText confidence="0.98511238">
By using a template like
&lt;obj-name&gt; &apos;s &lt;attribute&gt; is &lt;attrib-value&gt;.
the structure could produce the sentence, &amp;quot;Top of
the Mark&apos;s food type is American.&amp;quot;
Templates avoid the need for detailed linguistic
information about lexical items, part-of-speech tags,
number, gender, definiteness, tense, sentence organi-
zation, sub-categorization structure, semantic rela-
tions, etc., that more general NLG methods need to
have specified in the input (or supply defaults for).
Such information is usually not readily inferrable
from an application&apos;s database, nor is it always read-
ily available from other sources, with the breadth of
coverage or level of detail that is needed. Thus, using
a general-purpose generator can be formidable (Re-
iter, 1995). However, templates only work in very
controlled or limited situations. They cannot pro-
vide the expressiveness, flexibility or scalability that
many real domains need.
A desirable solution is a generator that abstracts
away from templates enough to provide the needed
flexibility and scalability, and yet still requires only
minimal semantic input (and maintains reasonable
efficiency). This generator would take on the re-
sponsibility of finding an appropriate linguistic re-
alization for an underspecified semantic input. This
solution is especially important in the context of ma-
chine translation, where the surface syntactic orga-
nization of the source text is usually different from
that of the target language, and the deep semantics
are often difficult to obtain or represent completely
as well. In Japanese to English translation, for ex-
ample, it is often hard to determine from a Japanese
text the number or gender of a noun phrase, the En-
glish equivalent of a verb tense, or the deep semantic
meaning of sentential arguments. There are many
other obvious syntactic divergences as well.
Thus, shifting such linguistic decisions to the gen-
erator is significantly helpful for client applications.
However, at the same time, it imposes enormous
needs for knowledge on the generator program. Tra-
ditional large-scale NLG already requires immense
amounts of knowledge, as does any large-scale Al
enterprise. NLG operating on a scale of 200,000 en-
tities (concepts, relations, and words) requires large
and sophisticated lexicons, grammars, ontologies,
collocation lists, and morphological tables. Acquir-
ing and applying accurate, detailed knowledge of
this breadth poses difficult problems.
(Knight and Hatzivassiloglou, 1995) suggested
</bodyText>
<page confidence="0.994855">
704
</page>
<figure confidence="0.9870278">
meaning
4.
symbolic generator &lt;— lexicon
&lt;— grammar
word lattice of
possible renderings
4.
statistical extractor 4— corpus
4.
English string
</figure>
<figureCaption confidence="0.713445666666667">
Figure 1: Combining Symbolic and Statisti-
cal Knowledge in a Natural Language Generator
(Knight and Hatzivassiloglou, 1995).
</figureCaption>
<bodyText confidence="0.999815119047619">
overcoming this knowledge acquisition bottleneck in
NLG by tapping the vast knowledge inherent in En-
glish text corpora. Experiments showed that corpus-
based knowledge greatly reduced the need for deep,
hand-crafted knowledge. This knowledge, in the
form of n-gram (word-pair) frequencies, could be ap-
plied to a set of semantically related sentences to
help sort good ones from bad ones. A corpus-based
statistical ranker takes a set of sentences packed ef-
ficiently into a word lattice, (a state transition di-
agram with links labeled by English words), and
extracts the best path from the lattice as output,
preferring fluent sentences over contorted ones. A
generator can take advantage of this by producing a
lattice that encodes various alternative possibilities
when the information needed to make a linguistic
decision is not available.
Such a system organization shown in Figure 1, is
robust against underspecified and even ambiguous
input meaning structures. Traditionally, underspec-
ification is handled with rigid defaults (e.g., assume
present tense, use the alphabetically-first synonyms,
use nominal arguments, etc.). However, the word
lattice structure permits all the different possibili-
ties to be encoded as different phrasings, and the
corpus-based statistical extractor can select a good
sentence from these possibilities.
The questions that still remain are: What kind of
input representation is minimally necessary? What
kinds of linguistic decisions can the statistics reliably
make, and which instead need to be made symbol-
ically? How should symbolic knowledge be applied
to the input to efficiently produce word lattices from
the input?
This paper describes Nitrogen, a generation sys-
tem that computes word lattices from a meaning rep-
resentation to take advantage of corpus-based sta-
tistical knowledge. Nitrogen performs sentence real-
ization and some components of sentence planning—
namely, mapping domain concepts to content words,
and to some extent, mapping semantic relations to
grammatical ones. It contributes:
</bodyText>
<listItem confidence="0.945017333333333">
• A flexible input representation based on concep-
tual meanings and the relations between them.
• A new grammar formalism for defining the map-
ping of meanings onto word lattices.
• A new efficient algorithm to do this mapping.
• A large grammar, lexicon, and morphology of
English, addressing linguistic phenomena such
as knowledge acquisition bottlenecks and under-
specified/ambiguous input.
</listItem>
<bodyText confidence="0.999333181818182">
This paper is organized as follows. First, we de-
scribe our Abstract Meaning Representation lan-
guage (AMR). Then we outline the generation algo-
rithm and describe how various knowledge sources
apply to render an AMR into English, including
lexical, morphological, and grammatical knowledge
bases. We describe the structure of these knowl-
edge bases and give examples. We also present a
technique that adds powerful flexibility to the gram-
mar formalism. We finish with a discussion of the
strengths and weaknesses of our generation system.
</bodyText>
<sectionHeader confidence="0.968572" genericHeader="introduction">
2 Abstract Meaning Representation
</sectionHeader>
<bodyText confidence="0.999988875">
The AMR language is composed of concepts from
the SENSUS knowledge base (Knight and Luk,
1994), including all of WordNet 1.5 (Miller, 1990),
and keywords relating these concepts to each other.1
An AMR is a labeled directed graph, or feature
structure, derived from the PENMAN Sentence Plan
Language (Penman, 1989). The most basic AMR is
of the form (label / concept), e.g.:2
</bodyText>
<equation confidence="0.523302">
(ml / Idog&lt;canidI)
</equation>
<bodyText confidence="0.9948404">
The slash is shorthand for a type (or instance) fea-
ture, and in logic notation this AMR might be writ-
ten as instance (ml, dog). This AMR can represent
&amp;quot;the dog,&amp;quot; &amp;quot;the dogs,&amp;quot; &amp;quot;a dog,&amp;quot; or &amp;quot;dog,&amp;quot; etc.
A concept can be modified using keywords:
</bodyText>
<equation confidence="0.9479605">
(m2 / Idog&lt;canidI
:quant plural)
</equation>
<bodyText confidence="0.988341636363636">
&apos;Strings can be used in place of concepts. If the string
is not a recognized word/phrase, then the generator will add
this ambiguity to the word lattice for the statistical extrac-
tor to resolve by proposing all possible part-of-speech tags.
We prefer to use concepts because they make the AMR more
language-independent, and enable semantic reasoning and
inference.
2Concept names appear between vertical bars. We use a
set of short, unique concept names derived from the struc-
ture of WordNet by Jonathan Graehl, and available from
http://www.isi.edu/natural-language/GAZELLE.html
</bodyText>
<page confidence="0.990962">
705
</page>
<bodyText confidence="0.999897913043478">
This narrows the meaning to &amp;quot;the dogs,&amp;quot; or &amp;quot;dogs.&amp;quot;
Concepts can be associated with each other in
a nested fashion to form more complex mean-
ings. These relations between conceptual mean-
ings are also expressed through keywords. It is
through them that our formalism exhibits an ap-
pealing flexibility. A client has the freedom to ex-
press the relations at various semantic and syn-
tactic levels, using whichever level of representa-
tion is most convenient.3 We have currently im-
plemented shallow semantic versions of roles such
as :agent, :patient, :sayer, :sensor, etc., as well as
deep syntactic roles such as :obliquel, :oblique2, and
:oblique3 (which correspond to deep subject, ob-
ject, and indirect object respectively, and serve as
an abstraction for passive versus active voice) and
the straightforward syntactic roles :subject, :direct-
object, :indirect-object, etc. We explain further how
this is implemented later in the paper.
Below is an example of a slightly more complex
meaning. The root concept is eating, and it has an
agent and a patient, which are dogs and a bone (or
bones), respectively.
</bodyText>
<equation confidence="0.85069775">
(m3 / leat,take ml
:agent (m4 / Idog&lt;canidl
:quant plural)
:patient (m5 / los,bonel))
</equation>
<bodyText confidence="0.998022333333333">
Possible output includes &amp;quot;The dogs ate the bone,&amp;quot;
&amp;quot;Dogs will eat a bone,&amp;quot; &amp;quot;The dogs eat bones,&amp;quot; &amp;quot;Dogs
eat bone,&amp;quot; and &amp;quot;The bones were eaten by dogs.&amp;quot;
</bodyText>
<sectionHeader confidence="0.984939" genericHeader="method">
3 Lexical Knowledge
</sectionHeader>
<bodyText confidence="0.991911842105263">
The Sensus concept ontology is mapped to an En-
glish lexicon that is consulted to find words for ex-
pressing the concepts in an AMR. The lexicon is a
list of 110,000 tuples of the form:
(&lt;word&gt; &lt;part-of-speech&gt; &lt;rank&gt; &lt;concept&gt;)
Examples:
((&amp;quot;eat&amp;quot; VERB 1 leat,take ml)
(&amp;quot;eat&amp;quot; VERB 2 leat&gt;eat lunch)
The &lt;rank&gt; field orders the concepts by sense fre-
quency for the given word, with a lower number sig-
nifying a more frequent sense.
Like other types of knowledge used in Nitro-
gen, the lexicon is very simple. It contains no
3This flexibility has another advantage from a research
point of view. We consider the appropriate level of abstrac-
tion an important problem in interlingua-style machine trans-
lation. The flexibility of this representation allows us to ex-
periment with various levels of abstraction without changing
the underlying system. Further, it has opened up to us the
possibility of implementing interlingua-based semantic trans-
fer, where the interlingua serves as the transfer mechanism,
rather than being a single, fixed peak point of abstraction.
information about features like transitivity, sub-
categorization, gradability (for adjectives), or count-
ability (for nouns), etc. Such features are needed
in other generators to produce correct grammatical
constructions. Our statistical post-processor instead
more softly (and robustly) ranks different grammat-
ical realizations according to their likelihood.
At the lexical level, several important issues in
word choice arise. WordNet maps a concept to one
or more synonyms. However, some words may be
less appropriate than others, or may actually be
misleading in certain contexts. An example is the
concept I sell&lt;cozen1 to which the lexicon maps the
words &amp;quot;betray&amp;quot; and &amp;quot;sell.&amp;quot; However, it is not very
common to use the word &amp;quot;sell&amp;quot; in the sense of &amp;quot;A
traitor sells out on his friends.&amp;quot; In the sentence &amp;quot;I
cannot I sen&lt;cozen I their trust&amp;quot; the word &amp;quot;sell&amp;quot; is
misleading, or at least sounds very strange; &amp;quot;betray&amp;quot;
is more appropriate.
This word choice problem occurs frequently, and
we deal with it by taking advantage of the word-
sense rankings that the lexicon offers. According to
the lexicon, the concept I sen&lt;cozen1 expresses the
second most frequent sense of the word &amp;quot;betray,&amp;quot;
but only the sixth most frequent sense of the word
&amp;quot;sell.&amp;quot; To minimize the lexical choice problem, we
have adopted a policy of rejecting words whose pri-
mary sense is not the given concept when better
words are available.4
Another issue in word choice relates to the broader
issue of preserving ambiguities in MT. In source
language analysis, it is often difficult to determine
which concept is intended by a certain word. The
AMR allows several concepts to be listed together
in a disjunction. For example,
</bodyText>
<sectionHeader confidence="0.344204" genericHeader="method">
(m6 / (*OR* Isell&lt;cozenI Icheat onl lbewrayl
</sectionHeader>
<subsectionHeader confidence="0.783387">
lbetray,faill Irat onl))
</subsectionHeader>
<bodyText confidence="0.999947">
The lexical lookup will attempt to preserve the
ambiguity of this *OR*. If it happens that several or
all of the concepts in a disjunction can be expressed
using the same word, then the lookup will return
only that word or words in preference to the other
possibilities. For the example above, the lookup re-
turns only the word &amp;quot;betray.&amp;quot; This also reduces the
complexity of the final sentence lattices.
</bodyText>
<sectionHeader confidence="0.9965" genericHeader="method">
4 Morphological Knowledge
</sectionHeader>
<bodyText confidence="0.9998745">
The lexicon contains words in their root form,
so morphological inflections must be generated.
The system also performs derivational morphol-
ogy, such as adjective—*noun and noun—*verb (ex:
</bodyText>
<footnote confidence="0.982600666666667">
4A better &amp;quot;soft&amp;quot; technique would be to accept all words
returned by the lexicon for a given concept, but associate
with each word a preference score using a method such as
Bayes&apos; Rule and probabilities computed from a corpus such
as SEMCOR, allowing the statistical extractor to choose the
best alternative. We plan to implement this in the future.
</footnote>
<page confidence="0.995559">
706
</page>
<bodyText confidence="0.9997466">
&amp;quot;translation&amp;quot; -+ &amp;quot;translate&amp;quot;) to give the generator
more syntactic flexibility in expressing complex
AMR&apos;s. This flexibility ensures that the generator
can find a way to express a complex meaning repre-
sented by nested AMRs, but is also useful for solving
problems of syntactic divergence in MT.
Both kinds of morphology are handled the same
way. Rules and exception tables are merged into a
single, concise knowledge base. Here, for example,
is a portion of the table for pluralizing nouns:
</bodyText>
<table confidence="0.420454">
(&amp;quot;-child&amp;quot; &amp;quot;children&amp;quot;)
(&amp;quot;-person&amp;quot; &amp;quot;people&amp;quot; &amp;quot;persons&amp;quot;)
(&amp;quot;-a&amp;quot; &amp;quot;as&amp;quot; &amp;quot;ae&amp;quot;) ; formulas/formulae
(&amp;quot;-x&amp;quot; &amp;quot;xes&amp;quot; &amp;quot;xen&amp;quot;) ; boxes/oxen
(&amp;quot;-man&amp;quot; &amp;quot;mans&amp;quot; &amp;quot;men&amp;quot;) ; humans/footmen
(&amp;quot;-Co&amp;quot; &amp;quot;os&amp;quot; noes&amp;quot;)
</table>
<bodyText confidence="0.999940235294117">
The last line means: if a noun ends in a conso-
nant followed by &amp;quot;-o,&amp;quot; then we compute two plural
forms, one ending in &amp;quot;-os&amp;quot; and one ending in &amp;quot;-oes,&amp;quot;
and put both possibilities in the word lattice for
the post-generation statistical extractor to choose
between later. Deciding between these usually re-
quires a large word list. However, the statistical
extractor already has a strong preference for &amp;quot;pho-
tos&amp;quot; and &amp;quot;potatoes&amp;quot; over &amp;quot;photoes&amp;quot; and &amp;quot;potatos,&amp;quot;
so we do not need to create such a list. Here again
corpus-based statistical knowledge greatly simplifies
the task of symbolic generation.
Derivational morphology raises the issue of mean-
ing shift between different part-of-speech forms
(such as &amp;quot;depart&amp;quot; —&gt; &amp;quot;departure&amp;quot; /&amp;quot;department&amp;quot;).
Errors of this kind are infrequent, and are corrected
in the morphology tables.
</bodyText>
<sectionHeader confidence="0.998046" genericHeader="method">
5 Generation Algorithm
</sectionHeader>
<bodyText confidence="0.9999708">
An AMR is transformed into word lattices by
keyword-based grammar rules described in Section
7. By contrast, other generators organize their
grammar rules around syntactic categories. A
keyword-based organization helps achieve simplicity
in the input specification, since syntactic informa-
tion is not required from a client. This simplification
can make Nitrogen more readily usable by client ap-
plications that are not inherently linguistically ori-
ented. The decisions about how to syntactically re-
alize a given meaning can be left largely up to the
generator.
The top-level keywords of an AMR are used to
match it with a rule (or rules). The algorithm
is compositional, avoiding a combinatorial explo-
sion in the number of rules needed for the various
keyword combinations. A matching rule splits the
AMR apart, associating a sub-AMR with each key-
word, and lumping the relations left over into a sub-
AMR under the :rest role using the same root as the
original AMR. Each sub-AMR is itself recursively
matched against the keyword rules, until the recur-
sion bottoms out at a basic AMR which matches
with the instance rule.
Lexical and morphological knowledge is used to
build the initial word lattices associated with a con-
cept when the recursion bottoms out. Then the in-
stance rule builds basic noun and verb groups from
these, as well as basic word lattices for other syn-
tactic categories. As the algorithm climbs out of the
recursion, each rule concatenates together lattices
for each of the sub-AMR&apos;s to form longer phrases.
The rhs specifies the needed syntactic category for
each sub-lattice and the surface order of the concate-
nation, as well as the syntactic category for the new
resulting lattice. Concatenation is performed by at-
taching the end state of one sub-lattice to the start
state of the next. Upon emerging from the top-level
rule, the lattice with the desired syntactic category,
by default S (sentence), is selected and handed to
the statistical extractor for ranking.
The next sections describe further how lexical and
morphological knowledge are used to build the initial
word lattices, how underspecification is handled, and
how the grammar is encoded.
</bodyText>
<sectionHeader confidence="0.995838" genericHeader="method">
6 The Instance Rule
</sectionHeader>
<bodyText confidence="0.999989741935484">
The instance rule is the most basic rule since it is ap-
plied to every concept in the AMR. This rule builds
the initial word lattices for each lexical item and
for basic noun and verb groups. Each concept in
the AMR is eventually handed to the instance rule,
where word lattices are constructed for all available
parts of speech.
The relational keywords that apply at the instance
level are :polarity, :quant, :tense, and :modal. In
cases where a meaning is underspecified and does
not include these keywords, the instance rule uses a
recasting mechanism (described below) to add some
of them. If not specified, the system assumes posi-
tive polarity, both singular and plural quantities, all
possible time frames, and no modality.
Japanese nouns are often ambiguous with respect
to number, so generating both singular and plural
possibilities and allowing the statistical extractor to
choose the best one results in better translation qual-
ity than rigidly choosing a single default as tradi-
tional generation systems do. Allowing number to
be unspecified in the input is also useful for gen-
eral English generation as well. There are many in-
stances when the number of a noun is dictated more
by usage convention or grammatical constraint than
by semantic content. For example, &amp;quot;The company
has (a plan/plans) to establish itself in February,&amp;quot; or
&amp;quot;This child won&apos;t eat any carrots,&amp;quot; (&amp;quot;carrots&amp;quot; must
be plural by grammatical constraint). It is easier
for a client program if the input is not required to
specify number in these cases, but is allowed to rely
</bodyText>
<page confidence="0.993415">
707
</page>
<bodyText confidence="0.999810869565217">
on the statistical extractor to supply the best one.
In translation, there is frequently no direct corre-
spondence between tenses of different languages, so
in Nitrogen, tense can be coarsely specified as either
past, present, or future, but need not be specified
at all. If not specified, Nitrogen generates lattices
for the most common English tenses, and allows the
statistical extractor to choose the most likely one.
The instance rule is factored into several sub-
instance rules with three main categories: nouns,
verbs, and miscellaneous. The noun instance rules
are further subdivided into two rules, one for plu-
ral noun phrases, and the other for singular. The
verb instance rules are factored into two categories
relating to modality and tense.
Polarity can apply across all three main instance
categories (noun, verb, and other), but only affects
the level it appears in. When applied to nouns or ad-
jectives, the result is &amp;quot;non-&amp;quot; prepended to the word,
which conveys the general intention, but is not usu-
ally very grammatical. Negative polarity is usually
most fluently expressed in the verb rules with the
word &amp;quot;not,&amp;quot; e.g., &amp;quot;does not eat.&amp;quot; 5
</bodyText>
<sectionHeader confidence="0.991334" genericHeader="method">
7 Grammar Formalism
</sectionHeader>
<bodyText confidence="0.9997955">
The grammatical specifications in the keyword rules
constitute the main formalism of the generation sys-
tem. The rules map semantic and syntactic roles to
grammatical word lattices. These roles include:
</bodyText>
<construct confidence="0.8339902">
:agent, :patient, :domain, :range, :source,
:destination, :spatial-locating,
:temporal-locating, :accompanier;
:obliquel, :oblique2, :oblique3;
:subject, :object, :mod, etc.
</construct>
<bodyText confidence="0.9681715">
A simplified version of the rule that applies to an
AMR with :agent and :patient roles is:
</bodyText>
<figure confidence="0.970492083333333">
((xl :agent)
(x2 :patient)
(x3 :rest)
-&gt;
(s (seq (xl np nom-pro) (x3 v-tensed)
(x2 np acc-pro)))
(s (seq (x2 np nom-pro) (x3 v-passive)
(wrd &amp;quot;by&amp;quot;) (xl np acc-pro)))
(np (seq (x3 np acc-pro nom-pro) (wrd &amp;quot;of&amp;quot;)
(x2 np acc-pro) (wrd &amp;quot;by&amp;quot;) (xl np acc-pro)))
(s-ger (seq ...))
(inf (seq ...)))
</figure>
<bodyText confidence="0.9643293">
The left-hand side is used to match an AMR with
agent and patient roles at the top level. The :rest
keyword serves as a catch-all for other roles that ap-
pear at the top level. Note that the rule specifies
two ways to build a sentence, one an active voice
5 We plan to generate more fluent expressions for negative
polarity on nouns and adjectives, for example, &amp;quot;unhappy&amp;quot;
instead of &amp;quot;non-happy.&amp;quot;
version and the other passive. Since at this level the
input may be underspecified regarding which voice
to use, the statistical extractor is expected to choose
later the most fluent version. Note also that this rule
builds lattices for other parts of speech, in addition
to sentences (ex: &amp;quot;the consumption of the bone by
the dogs&amp;quot;). In this way the generation algorithm
works bottom-up, building lattices for the leaves (in-
nermost nested levels of the input) first, to be com-
bined at outer levels according the relations between
the leaves. For example, the AMR below will match
this rule:
</bodyText>
<figure confidence="0.8860125">
(m7 / Ieat,take ml
:time present
:agent (d / Idog,canidl
:quant plural)
:patient (b / los,bonel
:quant sing))
</figure>
<bodyText confidence="0.9383505">
Below are some sample lattices that result from
applying the rule above to this AMR:6
</bodyText>
<table confidence="0.963363307692308">
(S (or (seq (or (wrd &amp;quot;the&amp;quot;) (wrd &amp;quot;*empty*&amp;quot;))
(wrd &amp;quot;dog&amp;quot;) (wrd &amp;quot;+plural&amp;quot;)
(wrd &amp;quot;may&amp;quot;) (wrd &amp;quot;eat&amp;quot;)
(or (wrd &amp;quot;the&amp;quot;) (wrd &amp;quot;a&amp;quot;)
(wrd &amp;quot;an&amp;quot;) (wrd &amp;quot;*empty*&amp;quot;))
(wrd &amp;quot;bone&amp;quot;))
(seq (or (wrd &amp;quot;the&amp;quot;) (wrd &amp;quot;a&amp;quot;)
(wrd &amp;quot;an&amp;quot;) (wrd &amp;quot;*empty*&amp;quot;))
(wrd &amp;quot;bone&amp;quot;) (wrd &amp;quot;may&amp;quot;) (wrd &amp;quot;be&amp;quot;)
(or (wrd &amp;quot;being&amp;quot;) (wrd &amp;quot;*empty*&amp;quot;))
(wrd &amp;quot;eat&amp;quot;) (wrd &amp;quot;+pastp&amp;quot;) (wrd &amp;quot;by&amp;quot;)
(or (wrd &amp;quot;the&amp;quot;) (wrd &amp;quot;*empty*&amp;quot;))
(wrd &amp;quot;dog&amp;quot;) (wrd &amp;quot;+plural&amp;quot;))))
(NP (seq (or (wrd &amp;quot;the&amp;quot;) (wrd &amp;quot;a&amp;quot;)
(wrd &amp;quot;an&amp;quot;) (wrd &amp;quot;*empty*&amp;quot;))
(wrd &amp;quot;possibility&amp;quot;) (wrd &amp;quot;of&amp;quot;)
(or (wrd &amp;quot;the&amp;quot;) (wrd &amp;quot;a&amp;quot;)
(wrd &amp;quot;an&amp;quot;) (wrd &amp;quot;*empty*&amp;quot;))
(wrd &amp;quot;consumption&amp;quot;) (wrd &amp;quot;of&amp;quot;)
(or (wrd &amp;quot;the&amp;quot;) (wrd &amp;quot;a&amp;quot;)
(wrd &amp;quot;an&amp;quot;) (wrd &amp;quot;*empty*&amp;quot;))
(wrd &amp;quot;bone&amp;quot;) (wrd &amp;quot;by&amp;quot;)
(or (wrd &amp;quot;the&amp;quot;) (wrd &amp;quot;*empty*&amp;quot;))
(wrd &amp;quot;dog&amp;quot;) (wrd &amp;quot;+plural&amp;quot;))))
(S-GER ...)
(INF ...)
</table>
<bodyText confidence="0.997758">
Note the variety of symbolic output that is pro-
duced with these excessively simple rules. Each re-
lation is mapped not to one but to many different
realizations, covering regular and irregular behav-
ior exhibited in natural language. Purposeful over-
generation becomes a strength.
</bodyText>
<footnote confidence="0.999473166666667">
6The grammar rules can insert the special token *empty*,
here indicating an option for the null determiner. Before run-
ning, the statistical extractor removes all *empty* transitions
by determinizing the word lattice. Note also the insertion of
morphological tokens like +plural. Inflectional morphology
rules also apply during this determinizing stage.
</footnote>
<page confidence="0.994759">
708
</page>
<bodyText confidence="0.999935285714286">
The :rest keyword in the rule head provides
a handy mechanism for decoupling the possible
keyword combinations. By means of this mecha-
nism, keywords which generate relatively indepen-
dent word lattices can be organized into separate
rules, avoiding combinatorial explosion in the num-
ber of rules which need to be written.
</bodyText>
<subsectionHeader confidence="0.999555">
7.1 Recasting Mechanism
</subsectionHeader>
<bodyText confidence="0.999720714285714">
The recasting mechanism that is used in the gram-
mar formalism gives it unique power and flexibil-
ity. The recasting mechanism enables the generator
to transform one semantic representation into an-
other one (such as deep to shallow, or instance to
sub-instance) and to accept as input a specification
anywhere along this spectrum, permitting meaning
to be encoded at whatever level is most convenient.
The recasting mechanism also makes it possible to
handle non-compositional aspects of language.
One area in which we use this mechanism is in the
:domain rule. Take for example the sentence, &amp;quot;It is
necessary that the dog eat.&amp;quot; It is sometimes most
convenient to represent this as:
</bodyText>
<figure confidence="0.628553">
(m8 / lobligatory&lt;necessaryl
:domain (m9 / Ieat,take ml
:agent (m10 / Idog,canidl)))
and at other times as:
(m11 / Ihave the quality of beingl
:domain (m12 / leat,take ml
:agent (d / Idog,canidI))
:range (m13 / lobligatory&lt;necessaryI))
</figure>
<bodyText confidence="0.999262454545455">
but we can define them to be semantically equiva-
lent. In our system, both are accepted, and the first
is automatically transformed into the second.
Other ways to say this sentence include &amp;quot;The dog
is required to eat,&amp;quot; or &amp;quot;The dog must eat.&amp;quot; How-
ever, the grammar formalism cannot express this,
because it would require inserting the word lattice
for I obligatory&lt;necessary I within the lattice for m9
or m12—but the formalism can only concatenate lat-
tices. The recasting mechanism solves this problem,
by recasting the above AMR as:
</bodyText>
<construct confidence="0.691849666666667">
(m14 / Ieat,take ml
:modal (m15 / lobligatory&lt;necessaryl)
:agent (m16 / Idog,canidI))
</construct>
<bodyText confidence="0.966459">
which makes it possible to form these sentences. The
syntax for recasting the first AMR to the second is:
</bodyText>
<table confidence="0.627277066666666">
((xl :rest)
(x2 :domain)
-&gt;
(? (xl (:new (/ Ihave the quality of beingl)
(:domain x2) (:range x1)) ?))
and for recasting the second into the third:
((x1 :rest)
(x2 :domain)
(x3 :range)
-&gt;
(? (x2 (:add (:modal (x3 (:add (:extra x1))))) ?))
(s (seq (x2 np nom-pro) (xl v-tensed)
(x3 adj np acc-pro)))
(s (seq (wrd &amp;quot;it&amp;quot;) (xl v-tensed)
(x3 adj np acc-pro) (wrd &amp;quot;that&amp;quot;) (x2 s)))
</table>
<bodyText confidence="0.999950129032258">
The :new and :add keywords signal an AMR re-
cast. The list after the keyword contains the in-
structions for doing the recast. In the first case,
the :new keyword means: build an AMR with a
new root, Ihave the quality of beingl , and two
roles, one labeled :domain and assigned sub-AMR
x2; the other labeled :range and assigned sub-AMR
xi. The question mark causes a direct splice of the
results from the recast.
In the second case, the : add keyword means: in-
sert into the sub-AMR of x2 a role labeled :modal
and assign to it the sub-AMR of x3 which itself is
recast to include the roles in the sub-AMR of xi but
not its root. (This is in case there are other roles
such as polarity or time which need to be included
in the new AMR.)
In fact, recasting makes it possible to nest modals
within modals to any desired depth, and even to at-
tach polarity and tense at any level. For example,
&amp;quot;It is not possible that it is required that you are per-
mitted to go,&amp;quot; can be also (more concisely) stated
as &amp;quot;It cannot be required that you be permitted to
go,&amp;quot; or &amp;quot;It is not possible that you must be permit-
ted to go,&amp;quot; or &amp;quot;You cannot have to be permitted
to go.&amp;quot; This is done by a grammar rule express-
ing the most nested modal concept as a modal verb
and the remaining modal concepts as a combination
of regular verbs or adjective phrases. Our grammar
includes a fairly complete model of obligation, pos-
sibility, permission, negation, tense, and all of their
possible interactions.
</bodyText>
<sectionHeader confidence="0.999571" genericHeader="conclusions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.996809857142857">
We have presented a new generation grammar for-
malism capable of mapping meanings onto word lat-
tices. It includes novel mechanisms for construct-
ing and combining word lattices, and for re-writing
meaning representations to handle a broad range of
linguistic phenomena. The grammar accepts inputs
along a continuum of semantic depth, requiring only
a minimal amount of syntactic detail, making it at-
tractive for a variety of purposes.
Nitrogen&apos;s grammar is organized around seman-
tic input patterns rather than the syntax of English.
This distinguishes it from both unification grammar
(Elhadad, 1993a; Shieber et al., 1989) and systemic-
network grammar (Penman, 1989). Meanings can
</bodyText>
<page confidence="0.994736">
709
</page>
<bodyText confidence="0.999975865384615">
be expressed directly, or else be recast and recy-
cled back through the generator. This recycling ul-
timately allows syntactic constraints to be localized,
even though the grammar is not organized around
English syntax.
Nitrogen&apos;s algorithm operates bottom-up, effi-
ciently encoding multiple analyses in a lattice data
structure to allow structure sharing, analogous to
the way a chart is used in bottom-up parsing. In
contrast, traditional generation control mechanisms
work top-down, either deterministically (Meteer et
al., 1987; Penman, 1989) or by backtracking to pre-
vious choice points (Elhadad, 1993b). This unnec-
essarily duplicates work at run time, unless sophis-
ticated control directives are included in the search
engine (Elhadad and Robin, 1992). Recently, (Kay,
1996) has explored a bottom-up approach to genera-
tion as well, using a chart rather than a word lattice.
Nitrogen&apos;s generation is robust and scalable. It
can generate output even for unexpected or incom-
plete input, and is designed for broad coverage.
It does not require the detailed, difficult-to-obtain
knowledge bases that other NLG systems require,
since it relies instead on corpus-based statistics to
make a wide variety of linguistic decisions. Cur-
rently the quality of the output is limited by the use
of only word bigram statistical information, which
cannot handle long-distance agreement, or distin-
guish likely collocations from unlikely grammatical
structure. However, we plan to remedy these prob-
lems by using statistical information extracted from
the Penn Treebank corpus (Marcus et al., 1994) to
rank tagged lattices and parse forests.
Nitrogen&apos;s rule matching is much less expensive
than graph unification, and lattices generated for
sub-AMRs are cached and reused in subsequent ref-
erences. The semantic roles used in the grammar
formalism cover most common syntactic phenomena,
though our grammar does not yet generate ques-
tions, or infer pronouns from explicit coreference.
Nitrogen has been used extensively as part of
a semantics-based Japanese-English MT system
(Knight et al., 1995). Japanese analysis provides
AMR&apos;s, which Nitrogen transforms into word lat-
tices on the order of hundreds of nodes and thou-
sands of arcs. These lattices compactly encode a
number of syntactic variants that usually reach into
the trillions and beyond. Most of these are some-
what ungrammatical or awkward, yet the statistical
extractor rather successfully narrows them down to
the top N best paths. An online demo is available at
http://www.isi.edu/natural-language/mt/nitrogen/
</bodyText>
<sectionHeader confidence="0.999505" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997035226415094">
J. Bateman. 1996. KPML development environ-
ment — multilingual linguistic resource devel-
opment and sentence generation. Technical re-
port, German Centre for Information Technology
(GMD).
M. Elhadad and J. Robin. 1992. Controlling content
realization with functional unification grammars.
In R. Dale, E. Hovy, D. Roesner, and 0. Stock,
editors, Aspects of Automated Natural Language
Generation. Springler Verlag.
M. Elhadad and J. Robin. 1998. Surge: a
comprehensive plug-in syntactic realiza-
tion component for text generation. In
http://www.cs.bgu.ac.il/research/projects/surge/.
M. Elhadad. 1993a. FUF: The universal unifier—
user manual, version 5.2. Technical Report
CUCS-038-91, Columbia University.
M. Elhadad. 1993b. Using Argumentation to Con-
trol Lexical Choice: A Unification-Based Imple-
mentation. Ph.D. thesis, Columbia University.
M. Kay. 1996. Chart generation. In Proc. ACL.
K. Knight and V. Hatzivassiloglou. 1995. Two-level,
many-paths generation. In Proc. ACL.
K. Knight and S. Luk. 1994. Building a large-scale
knowledge base for machine translation. In Proc.
AAAI.
K. Knight, I. Chander, M. Haines, V. Hatzivas-
siloglou, E. Hovy, M. Iida, S. K. Luk, R. Whitney,
and K. Yamada. 1995. Filling knowledge gaps in
a broad-coverage MT system. In Proc. IJCAI.
Benoit Lavoie and Owen Rambow. 1997. RealPro –
a fast, portable sentence realizer. In ANLP&apos;97.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger. 1994. The Penn treebank: Annotating
predicate argument structure. In ARPA Human
Language Technology Workshop.
M. Meteer, D. McDonald, S. Anderson, D. Forster,
L. Gay, A. Iluettner, and P. Sibun. 1987.
Mumble-86: Design and implementation. Tech-
nical Report COINS 87-87, U. of Massachussets
at Amherst, Amherst, MA.
G. Miller. 1990. Wordnet: An on-line lexical
database. International Journal of Lexicography,
3(4). (Special Issue).
Penman. 1989. The Penman documentation. Tech-
nical report, USC/Information Sciences Institute.
Ehud Reiter. 1995. NLG vs. templates. In Proc.
ENLGW &apos;95.
S. Shieber, G. van Noord, R. Moore, and F. Pereira.
1989. A semantic-head-driven generation algo-
rithm for unification based formalisms. In Proc.
ACL.
</reference>
<page confidence="0.996916">
710
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.773073">
<title confidence="0.996769">Generation that Exploits Corpus-Based Statistical Knowledge</title>
<author confidence="0.974677">Langkilde Knight</author>
<affiliation confidence="0.9996005">Information Sciences Institute University of Southern California</affiliation>
<address confidence="0.999719">Marina del Rey, CA 90292</address>
<email confidence="0.805591">ilangkileisieduandknight°isiedu</email>
<abstract confidence="0.998907416666667">We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen&apos;s design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bateman</author>
</authors>
<title>KPML development environment — multilingual linguistic resource development and sentence generation.</title>
<date>1996</date>
<tech>Technical report,</tech>
<institution>German Centre for Information Technology (GMD).</institution>
<contexts>
<context position="1201" citStr="Bateman, 1996" startWordPosition="168" endWordPosition="169">t it also handles non-compositional aspects of language. Nitrogen&apos;s design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities. 1 Introduction Language generation is an important subtask of applications like machine translation, humancomputer dialogue, explanation, and summarization. The recurring need for generation suggests the usefulness of a general-purpose, domain-independent natural language generator (NLG). However, &amp;quot;plugin&amp;quot; generators available today, such as FUF/SURGE (Elhadad and Robin, 1998), MUMBLE (Meteer et al., 1987), KPML (Bateman, 1996), and CoGenTex&apos;s RealPro (Lavoie and Rambow, 1997), require inputs with a daunting amount of linguistic detail. As a result, many client applications resort instead to simpler template-based methods. An important advantage of templates is that they sidestep linguistic decision-making, and avoid the need for large complex knowledge resources and processing. For example, the following structure could be a typical result from a database query on the type of food a venue serves: ((:obj-type venue)(:obj-name Top_of_the_Mark) (:attribute food-type)(:attrib-value American)) By using a template like &lt;</context>
</contexts>
<marker>Bateman, 1996</marker>
<rawString>J. Bateman. 1996. KPML development environment — multilingual linguistic resource development and sentence generation. Technical report, German Centre for Information Technology (GMD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
<author>J Robin</author>
</authors>
<title>Controlling content realization with functional unification grammars. In</title>
<date>1992</date>
<booktitle>Aspects of Automated Natural Language Generation.</booktitle>
<editor>R. Dale, E. Hovy, D. Roesner, and 0. Stock, editors,</editor>
<publisher>Springler Verlag.</publisher>
<contexts>
<context position="28937" citStr="Elhadad and Robin, 1992" startWordPosition="4603" endWordPosition="4606">raints to be localized, even though the grammar is not organized around English syntax. Nitrogen&apos;s algorithm operates bottom-up, efficiently encoding multiple analyses in a lattice data structure to allow structure sharing, analogous to the way a chart is used in bottom-up parsing. In contrast, traditional generation control mechanisms work top-down, either deterministically (Meteer et al., 1987; Penman, 1989) or by backtracking to previous choice points (Elhadad, 1993b). This unnecessarily duplicates work at run time, unless sophisticated control directives are included in the search engine (Elhadad and Robin, 1992). Recently, (Kay, 1996) has explored a bottom-up approach to generation as well, using a chart rather than a word lattice. Nitrogen&apos;s generation is robust and scalable. It can generate output even for unexpected or incomplete input, and is designed for broad coverage. It does not require the detailed, difficult-to-obtain knowledge bases that other NLG systems require, since it relies instead on corpus-based statistics to make a wide variety of linguistic decisions. Currently the quality of the output is limited by the use of only word bigram statistical information, which cannot handle long-di</context>
</contexts>
<marker>Elhadad, Robin, 1992</marker>
<rawString>M. Elhadad and J. Robin. 1992. Controlling content realization with functional unification grammars. In R. Dale, E. Hovy, D. Roesner, and 0. Stock, editors, Aspects of Automated Natural Language Generation. Springler Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
<author>J Robin</author>
</authors>
<title>Surge: a comprehensive plug-in syntactic realization component for text generation.</title>
<date>1998</date>
<note>In http://www.cs.bgu.ac.il/research/projects/surge/.</note>
<contexts>
<context position="1149" citStr="Elhadad and Robin, 1998" startWordPosition="158" endWordPosition="161">generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen&apos;s design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities. 1 Introduction Language generation is an important subtask of applications like machine translation, humancomputer dialogue, explanation, and summarization. The recurring need for generation suggests the usefulness of a general-purpose, domain-independent natural language generator (NLG). However, &amp;quot;plugin&amp;quot; generators available today, such as FUF/SURGE (Elhadad and Robin, 1998), MUMBLE (Meteer et al., 1987), KPML (Bateman, 1996), and CoGenTex&apos;s RealPro (Lavoie and Rambow, 1997), require inputs with a daunting amount of linguistic detail. As a result, many client applications resort instead to simpler template-based methods. An important advantage of templates is that they sidestep linguistic decision-making, and avoid the need for large complex knowledge resources and processing. For example, the following structure could be a typical result from a database query on the type of food a venue serves: ((:obj-type venue)(:obj-name Top_of_the_Mark) (:attribute food-type)</context>
</contexts>
<marker>Elhadad, Robin, 1998</marker>
<rawString>M. Elhadad and J. Robin. 1998. Surge: a comprehensive plug-in syntactic realization component for text generation. In http://www.cs.bgu.ac.il/research/projects/surge/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
</authors>
<title>FUF: The universal unifier— user manual, version 5.2.</title>
<date>1993</date>
<booktitle>In Proc. ACL.</booktitle>
<tech>Technical Report CUCS-038-91,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="28097" citStr="Elhadad, 1993" startWordPosition="4480" endWordPosition="4481">s. 8 Discussion We have presented a new generation grammar formalism capable of mapping meanings onto word lattices. It includes novel mechanisms for constructing and combining word lattices, and for re-writing meaning representations to handle a broad range of linguistic phenomena. The grammar accepts inputs along a continuum of semantic depth, requiring only a minimal amount of syntactic detail, making it attractive for a variety of purposes. Nitrogen&apos;s grammar is organized around semantic input patterns rather than the syntax of English. This distinguishes it from both unification grammar (Elhadad, 1993a; Shieber et al., 1989) and systemicnetwork grammar (Penman, 1989). Meanings can 709 be expressed directly, or else be recast and recycled back through the generator. This recycling ultimately allows syntactic constraints to be localized, even though the grammar is not organized around English syntax. Nitrogen&apos;s algorithm operates bottom-up, efficiently encoding multiple analyses in a lattice data structure to allow structure sharing, analogous to the way a chart is used in bottom-up parsing. In contrast, traditional generation control mechanisms work top-down, either deterministically (Metee</context>
</contexts>
<marker>Elhadad, 1993</marker>
<rawString>M. Elhadad. 1993a. FUF: The universal unifier— user manual, version 5.2. Technical Report CUCS-038-91, Columbia University. M. Elhadad. 1993b. Using Argumentation to Control Lexical Choice: A Unification-Based Implementation. Ph.D. thesis, Columbia University. M. Kay. 1996. Chart generation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Two-level, many-paths generation.</title>
<date>1995</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="4241" citStr="Knight and Hatzivassiloglou, 1995" startWordPosition="615" endWordPosition="618">. Thus, shifting such linguistic decisions to the generator is significantly helpful for client applications. However, at the same time, it imposes enormous needs for knowledge on the generator program. Traditional large-scale NLG already requires immense amounts of knowledge, as does any large-scale Al enterprise. NLG operating on a scale of 200,000 entities (concepts, relations, and words) requires large and sophisticated lexicons, grammars, ontologies, collocation lists, and morphological tables. Acquiring and applying accurate, detailed knowledge of this breadth poses difficult problems. (Knight and Hatzivassiloglou, 1995) suggested 704 meaning 4. symbolic generator &lt;— lexicon &lt;— grammar word lattice of possible renderings 4. statistical extractor 4— corpus 4. English string Figure 1: Combining Symbolic and Statistical Knowledge in a Natural Language Generator (Knight and Hatzivassiloglou, 1995). overcoming this knowledge acquisition bottleneck in NLG by tapping the vast knowledge inherent in English text corpora. Experiments showed that corpusbased knowledge greatly reduced the need for deep, hand-crafted knowledge. This knowledge, in the form of n-gram (word-pair) frequencies, could be applied to a set of sem</context>
</contexts>
<marker>Knight, Hatzivassiloglou, 1995</marker>
<rawString>K. Knight and V. Hatzivassiloglou. 1995. Two-level, many-paths generation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>S Luk</author>
</authors>
<title>Building a large-scale knowledge base for machine translation. In</title>
<date>1994</date>
<booktitle>Proc. AAAI.</booktitle>
<contexts>
<context position="7613" citStr="Knight and Luk, 1994" startWordPosition="1123" endWordPosition="1126">we describe our Abstract Meaning Representation language (AMR). Then we outline the generation algorithm and describe how various knowledge sources apply to render an AMR into English, including lexical, morphological, and grammatical knowledge bases. We describe the structure of these knowledge bases and give examples. We also present a technique that adds powerful flexibility to the grammar formalism. We finish with a discussion of the strengths and weaknesses of our generation system. 2 Abstract Meaning Representation The AMR language is composed of concepts from the SENSUS knowledge base (Knight and Luk, 1994), including all of WordNet 1.5 (Miller, 1990), and keywords relating these concepts to each other.1 An AMR is a labeled directed graph, or feature structure, derived from the PENMAN Sentence Plan Language (Penman, 1989). The most basic AMR is of the form (label / concept), e.g.:2 (ml / Idog&lt;canidI) The slash is shorthand for a type (or instance) feature, and in logic notation this AMR might be written as instance (ml, dog). This AMR can represent &amp;quot;the dog,&amp;quot; &amp;quot;the dogs,&amp;quot; &amp;quot;a dog,&amp;quot; or &amp;quot;dog,&amp;quot; etc. A concept can be modified using keywords: (m2 / Idog&lt;canidI :quant plural) &apos;Strings can be used in pla</context>
</contexts>
<marker>Knight, Luk, 1994</marker>
<rawString>K. Knight and S. Luk. 1994. Building a large-scale knowledge base for machine translation. In Proc. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
<author>M Haines</author>
<author>V Hatzivassiloglou</author>
<author>E Hovy</author>
<author>M Iida</author>
<author>S K Luk</author>
<author>R Whitney</author>
<author>K Yamada</author>
</authors>
<title>Filling knowledge gaps in a broad-coverage MT system.</title>
<date>1995</date>
<booktitle>In Proc. IJCAI.</booktitle>
<marker>Knight, Chander, Haines, Hatzivassiloglou, Hovy, Iida, Luk, Whitney, Yamada, 1995</marker>
<rawString>K. Knight, I. Chander, M. Haines, V. Hatzivassiloglou, E. Hovy, M. Iida, S. K. Luk, R. Whitney, and K. Yamada. 1995. Filling knowledge gaps in a broad-coverage MT system. In Proc. IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Lavoie</author>
<author>Owen Rambow</author>
</authors>
<title>RealPro – a fast, portable sentence realizer.</title>
<date>1997</date>
<booktitle>In ANLP&apos;97.</booktitle>
<contexts>
<context position="1251" citStr="Lavoie and Rambow, 1997" startWordPosition="174" endWordPosition="177">cts of language. Nitrogen&apos;s design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities. 1 Introduction Language generation is an important subtask of applications like machine translation, humancomputer dialogue, explanation, and summarization. The recurring need for generation suggests the usefulness of a general-purpose, domain-independent natural language generator (NLG). However, &amp;quot;plugin&amp;quot; generators available today, such as FUF/SURGE (Elhadad and Robin, 1998), MUMBLE (Meteer et al., 1987), KPML (Bateman, 1996), and CoGenTex&apos;s RealPro (Lavoie and Rambow, 1997), require inputs with a daunting amount of linguistic detail. As a result, many client applications resort instead to simpler template-based methods. An important advantage of templates is that they sidestep linguistic decision-making, and avoid the need for large complex knowledge resources and processing. For example, the following structure could be a typical result from a database query on the type of food a venue serves: ((:obj-type venue)(:obj-name Top_of_the_Mark) (:attribute food-type)(:attrib-value American)) By using a template like &lt;obj-name&gt; &apos;s &lt;attribute&gt; is &lt;attrib-value&gt;. the st</context>
</contexts>
<marker>Lavoie, Rambow, 1997</marker>
<rawString>Benoit Lavoie and Owen Rambow. 1997. RealPro – a fast, portable sentence realizer. In ANLP&apos;97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>G Kim</author>
<author>M Marcinkiewicz</author>
<author>R MacIntyre</author>
<author>A Bies</author>
<author>M Ferguson</author>
<author>K Katz</author>
<author>B Schasberger</author>
</authors>
<title>The Penn treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="29763" citStr="Marcus et al., 1994" startWordPosition="4730" endWordPosition="4733">ected or incomplete input, and is designed for broad coverage. It does not require the detailed, difficult-to-obtain knowledge bases that other NLG systems require, since it relies instead on corpus-based statistics to make a wide variety of linguistic decisions. Currently the quality of the output is limited by the use of only word bigram statistical information, which cannot handle long-distance agreement, or distinguish likely collocations from unlikely grammatical structure. However, we plan to remedy these problems by using statistical information extracted from the Penn Treebank corpus (Marcus et al., 1994) to rank tagged lattices and parse forests. Nitrogen&apos;s rule matching is much less expensive than graph unification, and lattices generated for sub-AMRs are cached and reused in subsequent references. The semantic roles used in the grammar formalism cover most common syntactic phenomena, though our grammar does not yet generate questions, or infer pronouns from explicit coreference. Nitrogen has been used extensively as part of a semantics-based Japanese-English MT system (Knight et al., 1995). Japanese analysis provides AMR&apos;s, which Nitrogen transforms into word lattices on the order of hundre</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn treebank: Annotating predicate argument structure. In ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meteer</author>
<author>D McDonald</author>
<author>S Anderson</author>
<author>D Forster</author>
<author>L Gay</author>
<author>A Iluettner</author>
<author>P Sibun</author>
</authors>
<title>Mumble-86: Design and implementation.</title>
<date>1987</date>
<tech>Technical Report COINS 87-87,</tech>
<institution>U. of Massachussets at Amherst,</institution>
<location>Amherst, MA.</location>
<contexts>
<context position="1179" citStr="Meteer et al., 1987" startWordPosition="163" endWordPosition="166">nal, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen&apos;s design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities. 1 Introduction Language generation is an important subtask of applications like machine translation, humancomputer dialogue, explanation, and summarization. The recurring need for generation suggests the usefulness of a general-purpose, domain-independent natural language generator (NLG). However, &amp;quot;plugin&amp;quot; generators available today, such as FUF/SURGE (Elhadad and Robin, 1998), MUMBLE (Meteer et al., 1987), KPML (Bateman, 1996), and CoGenTex&apos;s RealPro (Lavoie and Rambow, 1997), require inputs with a daunting amount of linguistic detail. As a result, many client applications resort instead to simpler template-based methods. An important advantage of templates is that they sidestep linguistic decision-making, and avoid the need for large complex knowledge resources and processing. For example, the following structure could be a typical result from a database query on the type of food a venue serves: ((:obj-type venue)(:obj-name Top_of_the_Mark) (:attribute food-type)(:attrib-value American)) By u</context>
<context position="28711" citStr="Meteer et al., 1987" startWordPosition="4568" endWordPosition="4571"> 1993a; Shieber et al., 1989) and systemicnetwork grammar (Penman, 1989). Meanings can 709 be expressed directly, or else be recast and recycled back through the generator. This recycling ultimately allows syntactic constraints to be localized, even though the grammar is not organized around English syntax. Nitrogen&apos;s algorithm operates bottom-up, efficiently encoding multiple analyses in a lattice data structure to allow structure sharing, analogous to the way a chart is used in bottom-up parsing. In contrast, traditional generation control mechanisms work top-down, either deterministically (Meteer et al., 1987; Penman, 1989) or by backtracking to previous choice points (Elhadad, 1993b). This unnecessarily duplicates work at run time, unless sophisticated control directives are included in the search engine (Elhadad and Robin, 1992). Recently, (Kay, 1996) has explored a bottom-up approach to generation as well, using a chart rather than a word lattice. Nitrogen&apos;s generation is robust and scalable. It can generate output even for unexpected or incomplete input, and is designed for broad coverage. It does not require the detailed, difficult-to-obtain knowledge bases that other NLG systems require, sin</context>
</contexts>
<marker>Meteer, McDonald, Anderson, Forster, Gay, Iluettner, Sibun, 1987</marker>
<rawString>M. Meteer, D. McDonald, S. Anderson, D. Forster, L. Gay, A. Iluettner, and P. Sibun. 1987. Mumble-86: Design and implementation. Technical Report COINS 87-87, U. of Massachussets at Amherst, Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<note>(Special Issue).</note>
<contexts>
<context position="7658" citStr="Miller, 1990" startWordPosition="1132" endWordPosition="1133">age (AMR). Then we outline the generation algorithm and describe how various knowledge sources apply to render an AMR into English, including lexical, morphological, and grammatical knowledge bases. We describe the structure of these knowledge bases and give examples. We also present a technique that adds powerful flexibility to the grammar formalism. We finish with a discussion of the strengths and weaknesses of our generation system. 2 Abstract Meaning Representation The AMR language is composed of concepts from the SENSUS knowledge base (Knight and Luk, 1994), including all of WordNet 1.5 (Miller, 1990), and keywords relating these concepts to each other.1 An AMR is a labeled directed graph, or feature structure, derived from the PENMAN Sentence Plan Language (Penman, 1989). The most basic AMR is of the form (label / concept), e.g.:2 (ml / Idog&lt;canidI) The slash is shorthand for a type (or instance) feature, and in logic notation this AMR might be written as instance (ml, dog). This AMR can represent &amp;quot;the dog,&amp;quot; &amp;quot;the dogs,&amp;quot; &amp;quot;a dog,&amp;quot; or &amp;quot;dog,&amp;quot; etc. A concept can be modified using keywords: (m2 / Idog&lt;canidI :quant plural) &apos;Strings can be used in place of concepts. If the string is not a recogn</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. Miller. 1990. Wordnet: An on-line lexical database. International Journal of Lexicography, 3(4). (Special Issue).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penman</author>
</authors>
<title>The Penman documentation. Technical report, USC/Information Sciences Institute. Ehud Reiter.</title>
<date>1989</date>
<booktitle>In Proc. ENLGW &apos;95.</booktitle>
<contexts>
<context position="7832" citStr="Penman, 1989" startWordPosition="1159" endWordPosition="1160">matical knowledge bases. We describe the structure of these knowledge bases and give examples. We also present a technique that adds powerful flexibility to the grammar formalism. We finish with a discussion of the strengths and weaknesses of our generation system. 2 Abstract Meaning Representation The AMR language is composed of concepts from the SENSUS knowledge base (Knight and Luk, 1994), including all of WordNet 1.5 (Miller, 1990), and keywords relating these concepts to each other.1 An AMR is a labeled directed graph, or feature structure, derived from the PENMAN Sentence Plan Language (Penman, 1989). The most basic AMR is of the form (label / concept), e.g.:2 (ml / Idog&lt;canidI) The slash is shorthand for a type (or instance) feature, and in logic notation this AMR might be written as instance (ml, dog). This AMR can represent &amp;quot;the dog,&amp;quot; &amp;quot;the dogs,&amp;quot; &amp;quot;a dog,&amp;quot; or &amp;quot;dog,&amp;quot; etc. A concept can be modified using keywords: (m2 / Idog&lt;canidI :quant plural) &apos;Strings can be used in place of concepts. If the string is not a recognized word/phrase, then the generator will add this ambiguity to the word lattice for the statistical extractor to resolve by proposing all possible part-of-speech tags. We pr</context>
<context position="28164" citStr="Penman, 1989" startWordPosition="4490" endWordPosition="4491">m capable of mapping meanings onto word lattices. It includes novel mechanisms for constructing and combining word lattices, and for re-writing meaning representations to handle a broad range of linguistic phenomena. The grammar accepts inputs along a continuum of semantic depth, requiring only a minimal amount of syntactic detail, making it attractive for a variety of purposes. Nitrogen&apos;s grammar is organized around semantic input patterns rather than the syntax of English. This distinguishes it from both unification grammar (Elhadad, 1993a; Shieber et al., 1989) and systemicnetwork grammar (Penman, 1989). Meanings can 709 be expressed directly, or else be recast and recycled back through the generator. This recycling ultimately allows syntactic constraints to be localized, even though the grammar is not organized around English syntax. Nitrogen&apos;s algorithm operates bottom-up, efficiently encoding multiple analyses in a lattice data structure to allow structure sharing, analogous to the way a chart is used in bottom-up parsing. In contrast, traditional generation control mechanisms work top-down, either deterministically (Meteer et al., 1987; Penman, 1989) or by backtracking to previous choice</context>
</contexts>
<marker>Penman, 1989</marker>
<rawString>Penman. 1989. The Penman documentation. Technical report, USC/Information Sciences Institute. Ehud Reiter. 1995. NLG vs. templates. In Proc. ENLGW &apos;95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
<author>G van Noord</author>
<author>R Moore</author>
<author>F Pereira</author>
</authors>
<title>A semantic-head-driven generation algorithm for unification based formalisms.</title>
<date>1989</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>Shieber, van Noord, Moore, Pereira, 1989</marker>
<rawString>S. Shieber, G. van Noord, R. Moore, and F. Pereira. 1989. A semantic-head-driven generation algorithm for unification based formalisms. In Proc. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>