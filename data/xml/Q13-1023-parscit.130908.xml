<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.721203333333333">
Good, Great, Excellent:
Global Inference of Semantic Intensities
Gerard de Melo
</note>
<address confidence="0.405447">
ICSI, Berkeley
</address>
<email confidence="0.996112">
demelo@icsi.berkeley.edu
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999793708333334">
Adjectives like good, great, and excellent are
similar in meaning, but differ in intensity. In-
tensity order information is very useful for
language learners as well as in several NLP
tasks, but is missing in most lexical resources
(dictionaries, WordNet, and thesauri). In this
paper, we present a primarily unsupervised
approach that uses semantics from Web-scale
data (e.g., phrases like good but not excel-
lent) to rank words by assigning them posi-
tions on a continuous scale. We rely on Mixed
Integer Linear Programming to jointly deter-
mine the ranks, such that individual decisions
benefit from global information. When rank-
ing English adjectives, our global algorithm
achieves substantial improvements over pre-
vious work on both pairwise and rank corre-
lation metrics (specifically, 70% pairwise ac-
curacy as compared to only 56% by previous
work). Moreover, our approach can incorpo-
rate external synonymy information (increas-
ing its pairwise accuracy to 78%) and extends
easily to new languages. We also make our
code and data freely available.1
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999890909090909">
Current lexical resources such as dictionaries and
thesauri do not provide information about the in-
tensity order of words. For example, both WordNet
(Miller, 1995) and Roget’s 21st Century Thesaurus
(thesaurus.com) present acceptable, great, and su-
perb as synonyms of the adjective good. However,
a native speaker knows that these words represent
varying intensity and can in fact generally be ranked
by intensity as acceptable &lt; good &lt; great &lt; superb.
Similarly, warm &lt; hot &lt; scorching are identified as
synonyms in these resources. Ranking information,
</bodyText>
<footnote confidence="0.970404">
1http://demelo.org/gdm/intensity/
</footnote>
<note confidence="0.377736">
Mohit Bansal
CS Division, UC Berkeley
</note>
<email confidence="0.985079">
mbansal@cs.berkeley.edu
</email>
<bodyText confidence="0.998388157894737">
however, is crucial because it allows us to differen-
tiate e.g. between various intensities of an emotion,
and is hence very useful for humans when learning a
language or judging product reviews, as well as for
automatic text understanding and generation tasks
such as sentiment and subjectivity analysis, recog-
nizing textual entailment, question answering, sum-
marization, and coreference and discourse analysis.
In this work, we attempt to automatically rank
sets of related words by intensity, focusing in par-
ticular on adjectives. This is made possible by the
vast amounts of world knowledge that are now avail-
able. We use lexico-semantic information extracted
from a Web-scale corpus in conjunction with an al-
gorithm based on a Mixed Integer Linear Program
(MILP). Linguistic analyses have identified phrases
such as good but not great or hot and almost scorch-
ing in a text corpus as sources of evidence about the
relative intensities of words. However, pure infor-
mation extraction approaches often fail to provide
enough coverage for real-world downstream appli-
cations (Tandon and de Melo, 2010), unless some
form of advanced inference is used (Snow et al.,
2006; Suchanek et al., 2009).
In our work, we address this sparsity problem by
relying on Web-scale data and using an MILP model
that extends the pairwise scores to a more com-
plete joint ranking of words on a continuous scale,
while maintaining global constraints such as transi-
tivity and giving more weight to the order of word
pairs with higher corpus evidence scores. Instead
of considering intensity ranking as a pairwise deci-
sion process, we thus exploit the fact that individual
decisions may benefit from global information, e.g.
about how two words relate to some third word.
Previous work (Sheinman and Tokunaga, 2009;
Schulam and Fellbaum, 2010; Sheinman et al.,
2012) has also used lexico-semantic patterns to or-
</bodyText>
<page confidence="0.990362">
279
</page>
<bodyText confidence="0.954915857142857">
Transactions of the Association for Computational Linguistics, 1 (2013) 279–290. Action Editor: Lillian Lee.
Submitted 11/2012; Revised 1/2013; Published 7/2013. c�2013 Association for Computational Linguistics.
der adjectives. They mainly evaluate their algorithm
on a set of pairwise decisions, but also present a par-
titioning approach that attempts to form scales by
placing each adjective to the left or right of pivot
words. Unfortunately, this approach often fails be-
cause many pairs lack order-based evidence even on
the Web, as explained in more detail in Section 3.
In contrast, our MILP jointly uses information
from all relevant word pairs and captures com-
plex interactions and inferences to produce inten-
sity scales. We can thus obtain an order between
two adjectives even when there is no explicit evi-
dence in the corpus (using evidence for related pairs
and transitive inference). Our global MILP is flex-
ible and can also incorporate additional synonymy
information if available (which helps the MILP find
an even better ranking solution). Our approach also
extends easily to new languages. We describe two
approaches for this multilingual extension: pattern
projection and cross-lingual MILPs.
We evaluate our predicted intensity rankings us-
ing both pairwise classification accuracy and rank-
ing correlation coefficients, achieving strong results,
significantly better than the previous approach by
Sheinman &amp; Tokunaga (32% relative error reduc-
tion) and quite close to human-level performance.
</bodyText>
<sectionHeader confidence="0.991276" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.99978825">
In this section, we describe each step of our ap-
proach to ordering adjectives on a single, relative
scale. Our method can also be applied to other word
classes and to languages other than English.
</bodyText>
<subsectionHeader confidence="0.876515">
2.1 Web-based Scoring Model
2.1.1 Intensity Scales
</subsectionHeader>
<bodyText confidence="0.999466307692308">
Near-synonyms may differ in intensity, e.g. joy
vs. euphoria, or drizzle vs. rain. This is particu-
larly true of adjectives, which can represent different
degrees of a given quality or attribute such as size
or age. Many adjectives are gradable and thus al-
low for grading adverbial modifiers to express such
intensity degrees, e.g., a house can be very big or
extremely big. Often, however, completely differ-
ent adjectives refer to varying degrees on the same
scale, e.g., huge, gigantic, gargantuan. Even adjec-
tives like enormous (or superb, impossible) that are
considered non-gradable from a syntactic perspec-
tive can be placed on a such a scale.
</bodyText>
<table confidence="0.997825555555556">
Weak-Strong Patterns Strong-Weak Patterns
? (,) but not ? not ? (,) just ?
? (,) if not ? not ? (,) but just ?
? (,) although not ? not ? (,) still ?
? (,) though not ? not ? (,) but still ?
? (,) (and/or) even ? not ? (,) although still ?
? (,) (and/or) almost ? not ? (,) though still ?
not only ? but ? ? (,) or very ?
not just ? but ?
</table>
<tableCaption confidence="0.796568833333333">
Table 1: Ranking patterns used in this work. Among the
patterns represented by the regular expressions above, we
use only those that capture less than or equal to five words
(to fit in the Google n-grams, see Section 2.1.2). Articles
(a, an, the) are allowed to appear before the wildcards
wherever possible.
</tableCaption>
<subsubsectionHeader confidence="0.550476">
2.1.2 Intensity Patterns
</subsubsectionHeader>
<bodyText confidence="0.999983294117647">
Linguistic studies have found lexical patterns like
‘? but not ?’ (e.g. good but not great) to reveal order
information between a pair of adjectives (Sheinman
and Tokunaga, 2009). We assume that we have two
sets of lexical patterns that allow us to infer the most
likely ordering between two words when encoun-
tered in a corpus. A first pattern set, Pws, contains
patterns that reflect a weak-strong order between a
pair of word (the first word is weaker than the sec-
ond), and a second pattern set, Psw, captures the
strong-weak order. See Table 1 for the adjective pat-
terns that we used in this work (and see Section 4.1
for implementation details regarding our pattern col-
lection). Many of these patterns also apply to other
parts of speech (e.g. ‘drizzle but not rain’, ‘running
or even sprinting’), with significant discrimination
on the Web in the right direction.
</bodyText>
<subsectionHeader confidence="0.924868">
2.1.3 Pairwise Scores
</subsectionHeader>
<bodyText confidence="0.999877416666667">
Given an input set of words to be placed on a
scale, we first collect evidence of their intensity or-
der by using the above-mentioned intensity patterns
and a large, Web-scale text corpus.
Previous work on information extraction from
limited-sized raw text corpora revealed that cover-
age is often limited (Hearst, 1992; Hatzivassiloglou
and McKeown, 1993). Some studies (Chklovski
and Pantel, 2004; Sheinman and Tokunaga, 2009)
used hit counts from an online search engine, but
this is unstable and irreproducible (Kilgarriff, 2007).
To avoid these issues, we use the largest available
</bodyText>
<page confidence="0.896402">
280
</page>
<bodyText confidence="0.988775714285714">
(good, great) (great, good) (small, minute)
good, but not great -+ 24492.0 not great, just good -+ 248.0 small, almost minute -+ 97.0
good, if not great -+ 1912.0 great or very good -+ 89.0 small, even minute -+ 41.0
good, though not great -+ 504.0 not great but still good -+ 47.0
good, or even great -+ 338.0
not just good but great -+ 181.0
good, almost great -+ 156.0
</bodyText>
<tableCaption confidence="0.978407">
Table 2: Some examples from the Web-scale corpus of useful intensity-based phrases on adjective pairs.
</tableCaption>
<bodyText confidence="0.9989198">
static corpus of counts, the Google n-grams corpus
(Brants and Franz, 2006), which contains English
n-grams (n = 1 to 5) and their observed frequency
counts, generated from nearly 1 trillion word tokens
and 95 billion sentences.
We consider each pair of words (a1, a2) in the in-
put set in turn. For each pattern p in the two pattern
sets (weak-strong Pw3 and strong-weak P3w), we in-
sert the word pair into the pattern as p(a1, a2) to get
a phrasal query like “big but not huge”. This is done
by replacing the two wildcards in the pattern by the
two words in order. Finally, we scan the Web n-
grams corpus in a batch approach similar to Bansal
and Klein (2011) and collect frequencies of all our
phrase queries. Table 2 depicts some examples of
useful intensity-based phrase queries and their fre-
quencies in the Web-scale corpus. We also collect
frequencies for the input word unigrams and the pat-
terns for normalization purposes. Given a word pair
(a1, a2) and a corpus count function cnt, we define
</bodyText>
<equation confidence="0.995506692307692">
1
W1 = P
1
p1XEPws
1XS1 = P
2 p2EPsw
1 X W2 = P1
p1EPws
1X
S2 = P2 p2EPsw
with
XP1 = cnt(p1)
p1EPws
</equation>
<bodyText confidence="0.672575">
such that the final overall weak-strong score is
</bodyText>
<equation confidence="0.9999415">
score(a1, a2) = (W1 − S1) − (W2 − S2) (3)
cnt(a1) · cnt(a2)
</equation>
<bodyText confidence="0.999655066666667">
Here W1 and S1 represent Web evidence of a1
and a2 being in the weak-strong and strong-weak
relation, respectively. W2 and S2 fit the reverse
pair (a2, a1) in the patterns and hence represent
the strong-weak and weak-strong relations, respec-
tively, in the opposite direction. Hence, overall,
(W1 − S1) − (W2 − S2) represents the total weak-
strong score of the pair (a1, a2), i.e. the score of a1
being on the left of a2 on a relative intensity scale,
such that score(a1, a2) = −score(a2, a1). The raw
frequencies in the score are divided by counts of the
patterns and by individual word unigram counts to
obtain a pointwise mutual information (PMI) style
normalization and hence avoid any bias in the score
due to high-frequency patterns or word unigrams.2
</bodyText>
<subsectionHeader confidence="0.9926255">
2.2 Global Ordering with an MILP
2.2.1 Objective and Constraints
</subsectionHeader>
<bodyText confidence="0.999993285714286">
Given pairwise scores, we now aim at producing a
global ranking of the input words that is much more
informative than the original pairwise scores. Joint
inference from multiple word pairs allows us to ben-
efit from global information: Due to the sparsity of
the pattern evidence, determining how two adjec-
tives relate to each other can sometimes e.g. only
be inferred by observing how each of them relate to
some third adjective.
We assume that we are given N input words A =
a1, ... , aN that we wish to place on a linear scale,
say [0, 1]. Thus each word ai is to be assigned a
position xi E [0, 1] based on the pairwise weak-
strong weights score(ai, aj). A positive value for
</bodyText>
<equation confidence="0.9952764">
cnt(p1(a1,a2))
cnt(p2(a1, a2))
cnt(p1(a2,a1))
cnt(p2(a2,a1)) (1)
XP2 = cnt(p2), (2)
</equation>
<footnote confidence="0.982841666666667">
p2EPsw
2In preliminary experiments on a development set, we also
evaluated other intuitive forms of normalization.
</footnote>
<page confidence="0.972378">
281
</page>
<figureCaption confidence="0.748936555555555">
be placed on (almost) the same position in the inten-
sity scale. If (i, j) ∈ E, we can safely assume that
ai, aj have near-equivalent intensity, so we should
encourage xi, xj to remain close to each other. The
MILP is defined as follows:
Figure 1: The input weak-strong data may contain one
or more cycles, e.g. due to noisy patterns, so the final
ranking will have to choose which input scores to honor
and which to remove.
</figureCaption>
<figure confidence="0.731626666666667">
maximize
� (wij − sij) · score(ai, aj)
(i,j)�∈E
</figure>
<bodyText confidence="0.983506833333333">
score(ai, aj) means that ai is supposedly weaker
than aj and hence we would like to obtain xi &lt; xj.
A negative value for score(ai, aj) means that ai is
assumed to be stronger than aj, so we would want
to obtain xi &gt; xj. Therefore, intuitively, our goal
corresponds to maximizing the objective
</bodyText>
<equation confidence="0.90586">
E sgn(xj − xi) · score(ai, aj) (4)
i,j
</equation>
<bodyText confidence="0.999851925925926">
Note that it is important to use the signum func-
tion sgn() here, because we only care about the rel-
ative order of xi and xj. Maximizing Eij(xj − xi) ·
score(ai, aj) would lead to all words being placed
at the edges of the scale, because the highest scores
would dominate over all other ones. We do include
the score magnitudes in the objective, because they
help resolve contradictions in the pairwise scores
(e.g., see Figure 1). This is discussed in more de-
tail in Section 2.2.2.
In order to maximize this non-differentiable ob-
jective, we use Mixed Integer Linear Programming
(MILP), a variant of linear programming in which
some but not all of the variables are constrained to
be integers. Using an MILP formalization, we can
find a globally optimal solution in the joint deci-
sion space, and unlike previous work, we jointly ex-
ploit global information rather than just individual
local (pairwise) scores. To encode the objective in a
MILP, we need to introduce additional variables dij,
wij, sij to capture the effect of the signum function,
as explained below.
We additionally also enable our MILP to make
use of any external equivalence (synonymy) infor-
mation E ⊆ {1, ... , N} × {1, ... , N} that may be
available. In this context, two words are considered
synonymous if they are close enough in meaning to
</bodyText>
<equation confidence="0.987454545454545">
� (wij + sij) C
(i,j)∈E
subject to
dij = xj − xi ∀i, j ∈ {1, ... , N}
dij − wijC ≤ 0 ∀i, j ∈ {1, ... , N}
dij + (1 − wij)C &gt; 0 ∀i,j ∈ {1, ... , N}
dij + sijC ≥ 0 ∀i, j ∈ {1, ... , N}
dij − (1 − sij)C &lt; 0 ∀i, j ∈ {1, ... , N}
xi ∈ [0, 1] ∀i ∈ {1, ... , N}
wij ∈ {0, 1} ∀i, j ∈ {1, ... , N}
sij ∈ {0, 1} ∀i, j ∈ {1, ... , N}
</equation>
<bodyText confidence="0.999973866666667">
The difference variables dij simply capture differ-
ences between xi, xj. C is any very large constant
greater than Ei,j |score(ai, aj)|; the exact value is
irrelevant. The indicator variables wij and sij are
jointly used to determine the value of the signum
function sgn(dij) = sgn(xj − xi). Variables wij
become 1 if and only if dij &gt; 0 and hence serve
as indicator variables for weak-strong relationships
in the output. Variables sij become 1 if and only if
dij &lt; 0 and hence serve as indicator variables for
a strong-weak relationship in the output. The ob-
jective encourages wij = 1 for score(ai, aj) &gt; 0
and sij = 1 for score(ai, aj) &lt; 0.3 When equiva-
lence (synonymy) information is available, then for
(i, j) ∈ E both sij = 0 and wij = 0 are encouraged.
</bodyText>
<subsectionHeader confidence="0.638076">
2.2.2 Discussion
</subsectionHeader>
<bodyText confidence="0.833800333333333">
Our MILP uses intensity evidence of all input
pairs together and assimilates all the scores via
global transitivity constraints to determine the posi-
tions of the input words on a continuous real-valued
scale. Hence, our approach addresses drawbacks
3In order to avoid numeric instability issues due to very
small score(ai, aj) values after frequency normalization, in
practice we have found it necessary to rescale them by a fac-
tor of 1 over the smallest |score(ai, aj) |&gt; 0.
</bodyText>
<page confidence="0.993653">
282
</page>
<figureCaption confidence="0.977774333333333">
Figure 2: Equivalence Information: Knowing that a.,,,,, a2
are synonyms gives the MILP an indication of where to
place a,,, on the scale with respect to a1, a2, a3
</figureCaption>
<bodyText confidence="0.99987125">
of local or divide-and-conquer approaches, where
adjectives are scored with respect to selected pivot
words, and hence many adjectives that lack pairwise
evidence with the pivots are not properly classified,
although they may have order evidence with some
third adjective that could help establish the ranking.
Optional synonymy information can further help, as
shown in Figure 2.
Moreover, our MILP also gives higher weight
to pairs with higher scores, which is useful when
breaking global constraint cycles as in the simple
example in Figure 1. If we need to break a con-
straint violating triangle or cycle, we would have to
make arbitrary choices if we were ranking based on
sgn(score(a, b)) alone. Instead, we can choose a
better ranking based on the magnitude of the pair-
wise scores. A stronger score between an adjective
pair doesn’t necessarily mean that they should be
further apart in the ranking. It means that these two
words are attested together on the Web with respect
to the intensity patterns more than with other candi-
date words. Therefore, we try to respect the order of
such word pairs more in the final ranking when we
are breaking constraint-violating cycles.
</bodyText>
<sectionHeader confidence="0.999957" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999926210526316">
Hatzivassiloglou and McKeown (1993) presented
the first step towards automatic identification of ad-
jective scales, thoroughly discussing the background
of adjective semantics and a means of discovering
clusters of adjectives that belong on the same scale,
thus providing one way of creating the input for our
ranking algorithm.
Inkpen and Hirst (2006) study near-synonyms and
nuances of meaning differentiation (such as stylistic,
attitudinal, etc.). They attempt to automatically ac-
quire a knowledge base of near-synonym differences
via an unsupervised decision-list algorithm. How-
ever, their method depends on a special dictionary
of synonym differences to learn the extraction pat-
terns, while we use only a raw Web-scale corpus.
Mohammad et al. (2013) proposed a method of
identifying whether two adjectives are antonymous.
This problem is related but distinct, because the de-
gree of antonymy does not necessarily determine
their position on an intensity scale. Antonyms (e.g.,
little, big) are not necessarily on the extreme ends of
scales.
Sheinman and Tokunaga (2009) and Sheinman et
al. (2012) present the most closely related previous
work on adjective intensities. They collect lexico-
semantic patterns via bootstrapping from seed adjec-
tive pairs to obtain pairwise intensities, albeit using
search engine ‘hits’, which are unstable and prob-
lematic (Kilgarriff, 2007). While their approach
is primarily evaluated in terms of a local pairwise
classification task, they also suggest the possibil-
ity of ordering adjectives on a scale using a pivot-
based partitioning approach. Although intuitive in
theory, the extracted pairwise scores are frequently
too sparse for this to work. Thus, many adjec-
tives have no score with a particular headword. In
our experiments, we reimplemented this approach
and show that our MILP method improves over it
by allowing individual pairwise decisions to benefit
more from global information. Schulam and Fell-
baum (2010) apply the approach of Sheinman and
Tokunaga (2009) to German adjectives. Our method
extends easily to various foreign languages as de-
scribed in Section 5.
Another related task is the extraction of lexico-
syntactic and lexico-semantic intensity-order pat-
terns from large text corpora (Hearst, 1992;
Chklovski and Pantel, 2004; Tandon and de Melo,
2010). Sheinman and Tokunaga (2009) follows
Davidov and Rappoport (2008) to automatically
bootstrap adjective scaling patterns using seed ad-
jectives and Web hits. These methods thus can be
used to provide the input patterns for our algorithm.
VerbOcean by Chklovski and Pantel (2004) ex-
tracts various fine-grained semantic relations (in-
cluding the stronger-than relation) between pairs of
verbs, using lexico-syntactic patterns over the Web.
</bodyText>
<page confidence="0.992011">
283
</page>
<bodyText confidence="0.999968809523809">
Our approach of jointly ranking a set of words using
pairwise evidence is also applicable to the VerbO-
cean pairs, and should help address similar sparsity
issues of local pairwise decisions. Such scales will
again be quite useful for language learners and lan-
guage understanding tools.
de Marneffe et al. (2010) infer yes-or-no answers
to questions with responses involving scalar adjec-
tives in a dialogue corpus. They correlate adjectives
with ratings in a movie review corpus to find that
good appears in lower-rated reviews than excellent.
Finally, there has been a lot of work on measuring
the general sentiment polarity of words (Hatzivas-
siloglou and McKeown, 1997; Hatzivassiloglou and
Wiebe, 2000; Turney and Littman, 2003; Liu and
Seneff, 2009; Taboada et al., 2011; Yessenalina and
Cardie, 2011; Pang and Lee, 2008). Our work in-
stead aims at producing a large, unrestricted number
of individual intensity scales for different qualities
and hence can help in fine-grained sentiment analy-
sis with respect to very particular content aspects.
</bodyText>
<sectionHeader confidence="0.999703" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.959647">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999934380952381">
Input Clusters In order to obtain input clusters for
evaluation, we started out with the satellite cluster or
‘dumbbell’ structure of adjectives in WordNet 3.0,
which consists of two direct antonyms as the poles
and a number of other satellite adjectives that are se-
mantically similar to each of the poles (Gross and
Miller, 1990). For each antonymy pair, we deter-
mined an extended dumbbell set by looking up syn-
onyms and words in related (satellite adjective and
‘see-also’) synonym sets. We cut such an extended
dumbbell into two antonymous halves and treated
each of these halves as a potential input adjective
cluster.
Most of these WordNet clusters are noisy for the
purpose of our task, i.e. they contain adjectives that
appear unrelatable on a single scale due to polysemy
and semantic drift, e.g. violent with respect to super-
natural and affected. Motivated by Sheinman and
Tokunaga (2009), we split such hard-to-relate ad-
jectives into smaller scale-specific subgroups using
the corpus evidence4. For this, we consider an undi-
</bodyText>
<footnote confidence="0.559808">
4Note that we do not use the WordNet dataset of Sheinman
and Tokunaga (2009) for evaluation, as it does not provide full
</footnote>
<figure confidence="0.7733405">
3 4 5 6 7 8
Length of chain
</figure>
<figureCaption confidence="0.999916">
Figure 4: The histogram of cluster sizes in the test set.
</figureCaption>
<bodyText confidence="0.999785260869565">
rected edge between each pair of adjectives that has
a non-zero intensity score (based on the Web-scale
scoring procedure described in Section 2.1.3). The
resulting graph is then partitioned into connected
components such that any adjectives in a subgraph
are at least indirectly connected via some path and
thus much more likely to belong to the same inten-
sity scale. While this does break up partitions when-
ever there is no corpus evidence connecting them,
ordering the adjectives within each such partition re-
mains a challenging task. This is because the Web
evidence will still not necessarily directly relate all
adjectives (in a partition) to each other. Addition-
ally, the Web evidence may still indicate the wrong
direction. Figure 3 shows the size distribution of the
resulting partitions.
Patterns To construct our intensity pattern set, we
started with a couple of common rankable adjective
seed pairs such as (good, great) and (hot, boiling)
and used the Web-scale n-grams corpus (Brants and
Franz, 2006) to collect the few most frequent pat-
terns between and around these seed-pairs (in both
directions). Among these, we manually chose a
</bodyText>
<figureCaption confidence="0.581203">
scales. Instead, their annotators only made pairwise compar-
isons with select words, using a 5-way classification scheme
(neutral, mild, very mild, intense, very intense).
</figureCaption>
<figure confidence="0.996436916666667">
500
438
115
60 35 19 12 14 5 4 3
# of chains
400
300
200
100
0
2 3 4 5 6 7 8 9 10-14 15-17
Length of chain
</figure>
<figureCaption confidence="0.995823">
Figure 3: The histogram of cluster sizes after partitioning.
</figureCaption>
<figure confidence="0.9978218">
3 3 2
# of chains
41
12
27
0
40
30
20
10
</figure>
<page confidence="0.992413">
284
</page>
<bodyText confidence="0.999958695652174">
small set of intuitive patterns that are linguistically
useful for ordering adjectives, several of which had
not been discovered in previous work. These are
shown in Table 1. Note that we only collected pat-
terns that were not ambiguous in the two orders, for
example the pattern ’* , not *’ is ambiguous be-
cause it can be used as both ’good, not great’ and
’great, not good’. Alternatively, one can easily also
use fully-automatic bootstrapping techniques based
on seed word pairs (Hearst, 1992; Chklovski and
Pantel, 2004; Yang and Su, 2007; Turney, 2008;
Davidov and Rappoport, 2008). However, our semi-
automatic approach is a simple and fast process that
extracts a small set of high-quality and very gen-
eral adjective-scaling patterns. This process can
quickly be repeated from scratch in any other lan-
guage. Moreover, as described in Section 5.1, the
English patterns can also be projected automatically
to patterns in other languages.
Development and Test Sets Section 2.1 describes
the method for collecting the intensity scores for ad-
jective pairs, using Web-scale n-grams (Brants and
Franz, 2006). We relied on a small development
set to test the MILP structure and the pairwise score
setup. For this, we manually chose 5 representative
adjective clusters from the full set of clusters.
The final test set, distinct from this development
set, consists of 569 word pairs in 88 clusters, each
annotated by two native speakers of English. Both
the gold test data (and our code) are freely avail-
able.5 To arrive at this data, we randomly drew 30
clusters each for cluster sizes 3, 4, and 5+ from the
histogram of partitioned adjective clusters in Fig-
ure 3. While labeling a cluster, annotators could ex-
clude words that they deemed unsuitable to fit on
a single shared intensity scale with the rest of the
cluster. Fortunately, the partitioning described ear-
lier had already separated most such cases into dis-
tinct clusters. The annotators ordered the remaining
words on a scale. Words that seemed indistinguish-
able in strength could share positions in their anno-
tation.
As our goal is to compare scale formation algo-
rithms, we did not include trivial clusters of size 2.
On such trivial clusters, the Web evidence alone de-
termines the output and hence all algorithms, includ-
</bodyText>
<footnote confidence="0.836974">
5http://demelo.org/gdm/intensity/
</footnote>
<bodyText confidence="0.997289666666667">
ing the baseline, obtain the same pairwise accuracy
(defined below) of 93.3% on a separate set of 30 ran-
dom clusters of size 2.
Figure 4 shows the distribution of cluster sizes in
our main gold set. The inter-annotator agreement in
terms of Cohen’s κ (Cohen, 1960) on the pairwise
classification task with 3 labels (weaker, stronger,
or equal/unknown) was 0.64. In terms of pairwise
accuracy, the agreement was 78.0%.
</bodyText>
<subsectionHeader confidence="0.974648">
4.2 Metrics
</subsectionHeader>
<bodyText confidence="0.999790333333333">
In order to thoroughly evaluate the performance of
our adjective ordering procedure, we rely on both
pairwise and ranking-correlation evaluation metrics.
Consider a set of input words A = {a1, a2, ... , an}
and two rankings for this set – a gold-standard rank-
ing rG(A) and a predicted ranking rP (A).
</bodyText>
<subsubsectionHeader confidence="0.658684">
4.2.1 Pairwise Accuracy
</subsubsectionHeader>
<bodyText confidence="0.9976686">
For a pair of words ai, aj, we may consider the
classification task of choosing one of three labels (&lt;,
&gt;, =?) for the case of ai being weaker, stronger, and
equal (or unknown) in intensity, respectively, com-
pared to a2:
</bodyText>
<equation confidence="0.999248666666667">
&lt; if r(ai) &lt; r(aj)
&gt; if r(ai) &gt; r(aj)
=? if r(ai) = r(aj)
</equation>
<bodyText confidence="0.8760645">
For each pair (a1, a2), we compute gold-standard
labels LG(a1, a2) and predicted labels LP(a1, a2) as
above, and then the pairwise accuracy PW (A) for
a particular ordering on A is simply the fraction of
pairs that are correctly classified, i.e. for which the
predicted label is same as the gold-standard label:
</bodyText>
<equation confidence="0.998834">
E 1{LG(ai, aj) = LP(ai, aj)}
PW (A) = i&lt;j
</equation>
<subsubsectionHeader confidence="0.471341">
4.2.2 Ranking Correlation Coefficients
</subsubsectionHeader>
<bodyText confidence="0.999773571428571">
Our second type of evaluation assesses the
rank correlation between two ranking permutations
(gold-standard and predicted). Many studies use
Kendall’s tau (Kendall, 1938), which measures the
total number of pairwise inversions, while others
prefer Spearman’s rho (Spearman, 1904), which
measures the L1 distance between ranks.
</bodyText>
<equation confidence="0.5564865">
⎧
⎨
⎩
L(a1, a2) =
E 1
i&lt;j
</equation>
<page confidence="0.994615">
285
</page>
<bodyText confidence="0.97140975">
Kendall’s tau correlation coefficient We use the
Tb version of Kendall’s correlation metric, as it in-
corporates a correction for ties (Kruskal, 1958; Dou
et al., 2008):
</bodyText>
<equation confidence="0.998748333333333">
P − Q
1/
(P + Q + X0) · (P + Q + Y0)
</equation>
<bodyText confidence="0.998534714285714">
where P is the number of concordant pairs, Q is
the number of discordant pairs, X0 is the number
of pairs tied in the first ranking, Y0 is the number of
pairs tied in the second ranking. Given the two rank-
ings of an adjective set A, the gold-standard ranking
rG(A) and the predicted ranking rP (A), two words
ai, aj are:
</bodyText>
<listItem confidence="0.999751333333333">
• concordant iff both rankings have the same strict
order of the two elements, i.e., rG(ai) &gt; rG(aj)
and rP(ai) &gt; rP(aj), or rG(ai) &lt; rG(aj) and
rP(ai) &lt; rP(aj).
• discordant iff the two rankings have an inverted
strict order of the two elements, i.e., rG(ai) &gt;
rG(aj) and rP(ai) &lt; rP(aj), or rG(ai) &lt;
rG(aj) and rP(ai) &gt; rP(aj).
• tied iff rG(ai) = rG(aj) or rP(ai) = rP(aj).
</listItem>
<bodyText confidence="0.9980445">
Spearman’s rho correlation coefficient For two
n-sized ranked lists {xi} and {yi}, the Spearman
correlation coefficient is defined as the Pearson cor-
relation coefficient between the ranks of variables:
</bodyText>
<equation confidence="0.993268333333333">
E (xi − ¯x) · (yi − ¯y)
i
ρ=
</equation>
<bodyText confidence="0.9998458125">
Here, x¯ and y¯ denote the means of the values in the
respective lists. We use the standard procedure for
handling ties correctly. Tied values are assigned the
average of all ranks of items sharing the same value
in the ranked list sorted in ascending order of the
values.
Handling Inversions While annotating, we some-
times observed that the ordering itself was very clear
but the annotators disagreed about which end of a
particular scale was to count as the strong one, e.g.
when transitioning from soft to hard or from alpha
to beta. We thus also report average absolute values
of both correlation coefficients, as these properly ac-
count for anticorrelations. Our test set only contains
clusters of size 3 or larger, so there is no need to
account for inversions in clusters of size 2.
</bodyText>
<subsectionHeader confidence="0.846137">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999783428571429">
In Table 3, we use the evaluation metrics mentioned
above to compare several different approaches.
Web Baseline The first baseline simply reflects
the original pairwise Web-based intensity scores.
We classify (with one of 3 labels) a given pair of
adjectives using the Web-based intensity scores (as
described in Section 2.1.3) as follows:
</bodyText>
<equation confidence="0.998948666666667">
Lbaseline(a1, a2) = { &lt; if score(ai,aj) &gt; 0
=? if score(ai,aj) = 0
&gt; if score(ai,aj) &lt; 0
</equation>
<bodyText confidence="0.996422088235294">
Since score(ai, aj) represents the weak-strong
score of the two adjectives, a more positive value
means a higher likelihood of ai being weaker (&lt;, on
the left) in intensity than aj.
In Table 3, we observe that the (micro-averaged)
pairwise accuracy, as defined earlier, for the origi-
nal Web baseline is 48.2%, while the ranking mea-
sures are undefined because the individual pairs do
not lead to a coherent scale.
Divide-and-Conquer The divide-and-conquer
baseline recursively splits a set of words into three
subgroups, placed to the left (weaker), on the same
position (no evidence), or to the right (stronger) of a
given randomly chosen pivot word.
While this approach shows only a minor improve-
ment in terms of the pairwise accuracy (50.6%), its
main benefit is that one obtains well-defined inten-
sity scales rather than just a collection of pairwise
scores.
Sheinman and Tokunaga The approach by
Sheinman and Tokunaga (2009) involves a simi-
lar divide-and-conquer based partitioning in the first
phase, except that their method makes use of syn-
onymy information from WordNet and uses all syn-
onyms in WordNet’s synset for the headword as
neutral pivot elements (if the headword is not in
WordNet, then the word with the maximal unigram
frequency is chosen). In the second phase, their
method performs pairwise comparisons within the
more intense and less intense subgroups. We reim-
plement their approach here, using the Google N-
Grams dataset instead of online Web search engine
hits. We observe a small improvement over the Web
baseline in terms of pairwise accuracy. Note that the
</bodyText>
<equation confidence="0.996851333333333">
Tb =
�E(xi − ¯x)2 · E(yi − ¯y)2
i i
</equation>
<page confidence="0.996769">
286
</page>
<table confidence="0.999242571428571">
Method Pairwise Accuracy Avg. r Avg. JrJ Avg. p Avg. JpJ
Web Baseline 48.2% N/A N/A N/A N/A
Divide-and-Conquer 50.6% 0.45 0.53 0.52 0.62
Sheinman and Tokunaga (2009) 55.5% N/A N/A N/A N/A
MILP 69.6% 0.57 0.65 0.64 0.73
MILP with synonymy 78.2% 0.57 0.66 0.67 0.80
Inter-Annotator Agreement 78.0% 0.67 0.76 0.75 0.86
</table>
<tableCaption confidence="0.998831">
Table 3: Main test results
</tableCaption>
<table confidence="0.9998564">
Weaker Predicted Class
Tie Stronger
Weaker 117 127 15
True Class Tie 5 42 15
Stronger 11 122 115
</table>
<tableCaption confidence="0.997811">
Table 4: Confusion matrix (Web baseline)
</tableCaption>
<table confidence="0.9998768">
Weaker Predicted Class
Tie Stronger
Weaker 177 29 53
True Class Tie 9 24 29
Stronger 15 38 195
</table>
<tableCaption confidence="0.9998">
Table 5: Confusion matrix (MILP)
</tableCaption>
<bodyText confidence="0.999910291666667">
rank correlation measure scores are undefined for
their approach. This is because in some cases their
method placed all words on the same position in the
scale, which these measures cannot handle even in
their tie-corrected versions. Overall, the Sheinman
and Tokunaga approach does not aggregate informa-
tion sufficiently well at the global level and often
fails to make use of transitive inference.
MILP Our MILP exploits the same pairwise
scores to induce significantly more accurate pair-
wise labels with 69.6% accuracy, a 41% relative
error reduction over the Web baseline, 38% over
Divide-and-Conquer, and 32% over Sheinman and
Tokunaga (2009). We further see that our MILP
method is able to exploit external synonymy (equiv-
alence) information (using synonyms marked by the
annotators). The accuracy of the pairwise scores as
well as the quality of the overall ranking increase
even further to 78.2%, approaching the human inter-
annotator agreement. In terms of average correlation
coefficients, we observe similar improvement trends
from the MILP, but of different magnitudes, because
these averages give small clusters the same weight
as larger ones.
</bodyText>
<subsectionHeader confidence="0.99877">
4.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999918875">
Confusion Matrices For a given approach, we
can study the confusion matrix obtained by cross-
tabulating the gold classification with the predicted
classification of every unique pair of adjectives in
the ground truth data. Table 4 shows the confusion
matrix for the Web baseline. We observe that due to
the sparsity of pairwise intensity order evidence, the
baseline method predicts too many ties.
Table 5 provides the confusion matrix for the
MILP (without external equivalence information)
for comparison. Although the middle column still
shows that the MILP predicts more ties than humans
annotators, we find that a clear majority of all unique
pairs are now correctly placed along the diagonal.
This confirms that our MILP successfully infers new
ordering decisions, although it uses the same input
(corpus evidence) as the baseline. The remaining
ties are mostly just the result of pairs for which there
simply is no evidence at all in the input Web counts.
Note that this problem could for instance be circum-
vented by relying on a crowdsourcing approach: A
few dispersed tie-breakers are enough to allow our
MILP to correct many other predictions.
Predicted Examples Finally, in Table 6, we pro-
vide a selection of real results obtained by our algo-
rithm. For instance, it correctly inferred that terri-
fying is more intense than creepy or scary, although
the Web pattern counts did not provide any explicit
information about these words pairs. In some cases,
however, the Web evidence did not suffice to draw
the right conclusions, or it was misleading due to is-
sues like polysemy (as for the word funny).
</bodyText>
<page confidence="0.990978">
287
</page>
<table confidence="0.99991312">
Accuracy Prediction Gold Standard
Good hard hard
&lt; painful &lt; painful
&lt; hopeless &lt; hopeless
full full
&lt; stuffed &lt; stuffed
&lt; (overflowing, &lt; overflowing
overloaded) &lt; overloaded
unusual uncommon
&lt; uncommon &lt; unusual
&lt; rare &lt; rare
&lt; exceptional &lt; extraordinary
&lt; extraordinary &lt; exceptional
Average creepy creepy
&lt; scary &lt; (scary, frightening)
&lt; sinister &lt; terrifying
&lt; frightening &lt; sinister
&lt; terrifying
Bad (awake, conscious) alive
&lt; alive &lt; awake
&lt; aware &lt; (aware, conscious)
strange (strange, funny)
&lt; (unusual, weird) &lt; unusual
&lt; (funny, eerie) &lt; weird
&lt; eerie
</table>
<tableCaption confidence="0.908184666666667">
Table 6: Some examples (of bad, average and good accu-
racy) of our MILP predictions (without synonymy infor-
mation) and the corresponding gold-standard annotation.
</tableCaption>
<bodyText confidence="0.999965428571429">
While we show results on gold-standard chains
here for evaluation purposes, in practice one can also
recombine two [0, 1] chains for a pair of antonymic
clusters to form a single scale from [−1, 1] that visu-
alizes the full spectrum of available adjectives along
a dimension, from adjacent all the way to removed,
or from black to glaring.
</bodyText>
<sectionHeader confidence="0.995236" genericHeader="method">
5 Extension to Multilingual Ordering
</sectionHeader>
<bodyText confidence="0.999989083333333">
Our method for globally ordering words on a scale
can easily be applied to languages other than En-
glish. The entire process is language-independent
as long as the required resources are available and a
small number of patterns are chosen. For morpho-
logically rich languages, the information extraction
step of course may require additional morphologi-
cal analysis tools for stemming and aggregating fre-
quencies across different forms.
Alternatively, a cross-lingual projection approach
is possible at multiple levels, utilizing information
from the English data and ranking. As the first step,
the set of words in the target language that we wish
to rank can be projected from the English word set if
necessary – e.g., as shown in de Melo and Weikum
(2009). Next, we outline two projection methods for
the ordering step. The first method is based on pro-
jection of the English intensity-ordering patterns to
the new language, and then using the same MILP
as described in Section 2.2. In the second method,
we also change the MILP and add cross-lingual con-
straints to better inform the target language’s ad-
jective ranking. A detailed empirical evaluation of
these approaches remains future work.
</bodyText>
<subsectionHeader confidence="0.991689">
5.1 Cross-Lingual Pattern Projection
</subsectionHeader>
<bodyText confidence="0.999984428571429">
Instead of creating new patterns, in many cases
we obtain quite adequate intensity patterns by us-
ing cross-lingual projection. We simply take sev-
eral adjective pairs, instantiate the English patterns
with them, and obtain new patterns using a machine
translation system. Filling the wildcards in a pat-
tern, say ‘* but not *’, with good/excellent results in
‘good but not excellent’. This phrase is then trans-
lated into the target language using the translation
system, say into German ‘gut aber nicht ausgezeich-
net’. Finally, put back the wildcards in the place of
the translations of the adjective words, here gut and
ausgezeichnet, to get the corresponding German pat-
tern ‘* aber nicht *’. Table 7 shows various German
intensity patterns that we obtain by projecting from
the English patterns as described. The process is re-
peated with multiple adjective pairs in case different
variants are returned, e.g. due to morphology. Most
of these translations deliver useful results.
Now that we have the target language adjectives
and the ranking patterns, we can compute the pair-
wise intensity scores using large-scale data in that
language. We can use the Google n-grams cor-
pora for 10 European languages (Brants and Franz,
2009), and also for Chinese (LDC2010T02) and
Japanese (LDC2009T08). For other languages, one
can use available large raw-text corpora or Web
crawling tools.
</bodyText>
<subsectionHeader confidence="0.991336">
5.2 Crosslingual MILP
</subsectionHeader>
<bodyText confidence="0.9998745">
To improve the rankings for lesser-resourced lan-
guages, we can further use a joint MILP approach
for the new language we want to transfer this pro-
cess to. Additional constraints between the English
</bodyText>
<page confidence="0.991269">
288
</page>
<table confidence="0.972713666666667">
Weak-Strong Patterns Strong-Weak Patterns
English German English German
* but not * * aber nicht * not *just * nicht * gerade *
* if not * * wenn nicht * not * but just * nicht * aber nur *
* and almost * * und fast * not * though still * nicht * aber immer noch *
not just * but * nicht nur * sondern * * or very * * oder sehr *
</table>
<tableCaption confidence="0.999612">
Table 7: Examples of German intensity patterns projected (translated) directly from the English patterns.
</tableCaption>
<bodyText confidence="0.9989195625">
words and their corresponding target language trans-
lations, in combination with the English ranking in-
formation, allow the algorithm to obtain better rank-
ings for the target words whenever the non-English
target language corpus does not provide sufficient
intensity order evidence.
In this case, the input set A contains words
in multiple languages. The Web intensity scores
score(ai, aj) should be set to zero when comparing
words across languages. We instead link them using
a translation table T C {1, ... , N} x {1,... , N}
from a translation dictionary or phrase table. Here,
(i, j) E T signifies that ai is a translation of aj. We
do not require a bijective relationship between them
(i.e., translations needn’t be unique). The objective
function is augmented by adding the new term
</bodyText>
<equation confidence="0.981897">
� (w0ij + s0 ij)CT (5)
(i,j)∈T
</equation>
<bodyText confidence="0.9999365">
for a constant CT &gt; 0 that determines how much
weight we assign to translations as opposed to the
corpus count scores. The MILP is extended by
adding the following extra constraints.
</bodyText>
<equation confidence="0.904572666666667">
dij − w0ijCT &lt; −dmax bi, j E {1, ... , N}
dij + (1 − w0ij)CT &gt; −dmax bi, j E {1, ... , N}
dij + s0ijCT &gt; dmax bi, j E {1, ... , N}
dij − (1 − s0ij)CT &lt; dmax bi, j E {1, ... , N}
w0ij E {0, 1} bi, j E T
s0ij E {0, 1} bi, j E T
</equation>
<bodyText confidence="0.999973117647059">
The variables di,j, as before, encode distances be-
tween positions of words on the scale, but now also
include cross-lingual pairs of words in different lan-
guages. The new constraints encourage translational
equivalents to remain close to each other, preferably
within a desired (but not strictly enforced) maximum
distance dmax. The new variables w0ij, s0ij are sim-
ilar to wij, sij in the standard MILP. However, the
w0ij become 1 if and only if dij &gt; −dmax and the s0ij
become 1 if and only if dij &lt; dmax. If both w0ij and
s0ij are 1, then the two words have a small distance
−dmax &lt; dij &lt; dmax. The augmented objective
function explicitly encourages this for translational
equivalents. Overall, this approach thus allows evi-
dence from a language with more Web evidence to
improve the process of adjective ordering in lesser-
resourced languages.
</bodyText>
<sectionHeader confidence="0.999668" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999990090909091">
In this work, we have presented an approach to the
challenging and little-studied task of ranking words
in terms of their intensity on a continuous scale. We
address the issue of sparsity of the intensity order ev-
idence in two ways. First, pairwise intensity scores
are computed using linguistically intuitive patterns
in a very large, Web-scale corpus. Next, a Mixed
Integer Linear Program (MILP) expands on this fur-
ther by inferring new relative relationships. Instead
of making ordering decisions about word pairs in-
dependently, our MILP considers the joint decision
space and factors in e.g. how two adjectives relate
to some third adjective, thus enforcing global con-
straints such as transitivity.
Our approach is general enough to allow addi-
tional evidence such as synonymy in the MILP,
and can straightforwardly be applied to other word
classes (such as verbs), and to other languages
(monolingually as well as cross-lingually). The
overall results across multiple metrics are substan-
tially better than previous approaches, and fairly
close to human agreement on this challenging task.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9993145">
We would like to thank the editor and the anony-
mous reviewers for their helpful feedback.
</bodyText>
<page confidence="0.997103">
289
</page>
<sectionHeader confidence="0.998326" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999863884615385">
Mohit Bansal and Dan Klein. 2011. Web-scale features
for full-scale parsing. In Proceedings of ACL 2011.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram corpus version 1.1. LDC2006T13.
Thorsten Brants and Alex Franz. 2009. Web 1T 5-gram,
10 European languages, version 1. LDC2009T25.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP 2004.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37–46.
Dmitry Davidov and Ari Rappoport. 2008. Unsuper-
vised discovery of generic relationships using pattern
clusters and its evaluation by automatically generated
sat analogy questions. In Proceedings ofACL 2008.
Marie-Catherine de Marneffe, Christopher D. Manning,
and Christopher Potts. 2010. Was it good? it was
provocative. learning the meaning of scalar adjectives.
In Proceedings of ACL 2010.
Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined evi-
dence. In Proceedings of CIKM 2009.
Zhicheng Dou, Ruihua Song, Xiaojie Yuan, and Ji-Rong
Wen. 2008. Are click-through data adequate for learn-
ing web search rankings? In Proc. of CIKM 2008.
Derek Gross and Katherine J. Miller. 1990. Adjectives
in WordNet. International Journal of Lexicography,
3(4):265–277.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1993. Towards the automatic identification of adjecti-
val scales: Clustering adjectives according to meaning.
In Proceedings of ACL 1993.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL 1997.
Vasileios Hatzivassiloglou and Janyce M. Wiebe. 2000.
Effects of adjective orientation and gradability on sen-
tence subjectivity. In Proceedings of COLING 2000.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING
1992.
Diana Inkpen and Graeme Hirst. 2006. Building and
using a lexical knowledge base of near-synonym dif-
ferences. Computational Linguistics, 32(2):223–262.
Maurice G. Kendall. 1938. A new measure of rank cor-
relation. Biometrika, 30(1/2):81–93.
Adam Kilgarriff. 2007. Googleology is bad science.
Computational Linguistics, 33(1).
William H. Kruskal. 1958. Ordinal measures of associa-
tion. Journal of the American Statistical Association,
53(284):814–861.
Jingjing Liu and Stephanie Seneff. 2009. Review senti-
ment scoring via a parse-and-paraphrase paradigm. In
Proceedings of EMNLP 2009.
George A. Miller. 1995. WordNet: A lexical database for
english. Communications of the ACM, 38(11):39–41.
Said M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2013. Computing lexical contrast.
Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135, January.
Peter F. Schulam and Christiane Fellbaum. 2010. Au-
tomatically determining the semantic gradation of ger-
man adjectives. In Proceedings of KONVENS 2010.
Vera Sheinman and Takenobu Tokunaga. 2009. AdjS-
cales: Visualizing differences between adjectives for
language learners. IEICE Transactions on Information
and Systems, 92(8):1542–1550.
Vera Sheinman, Takenobu Tokunaga, I. Julien, P. Schu-
lam, and C. Fellbaum. 2012. Refining WordNet adjec-
tive dumbbells using intensity relations. In Proceed-
ings of Global WordNet Conference 2012.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of COLING/ACL 2006.
Charles Spearman. 1904. The proof and measurement of
association between two things. The American journal
ofpsychology, 15(1):72–101.
Fabian M. Suchanek, Mauro Sozio, and Gerhard
Weikum. 2009. SOFIE: a self-organizing framework
for information extraction. In Proceedings of WWW
2009.
Maite Taboada, Julian Brooke, Milan Tofiloskiy, and
Kimberly Vollz. 2011. Lexicon-based methods for
sentiment analysis. Computational Linguistics.
Niket Tandon and Gerard de Melo. 2010. Information
extraction from web-scale n-gram data. In Proceed-
ings of the SIGIR 2010 Web N-gram Workshop.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Trans. Inf. Syst.,
21(4):315–346, October.
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of COLING 2008.
Xiaofeng Yang and Jian Su. 2007. Coreference resolu-
tion using semantic relatedness information from auto-
matically discovered patterns. In Proceedings of ACL
2007.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of EMNLP 2011.
</reference>
<page confidence="0.997124">
290
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.418953">
<title confidence="0.9640465">Good, Great, Excellent: Global Inference of Semantic Intensities</title>
<author confidence="0.968247">Gerard de_Melo</author>
<affiliation confidence="0.79831">ICSI, Berkeley</affiliation>
<email confidence="0.999833">demelo@icsi.berkeley.edu</email>
<abstract confidence="0.98271752">like and similar in meaning, but differ in intensity. Intensity order information is very useful for language learners as well as in several NLP tasks, but is missing in most lexical resources (dictionaries, WordNet, and thesauri). In this paper, we present a primarily unsupervised approach that uses semantics from Web-scale (e.g., phrases like but not excelto rank words by assigning them positions on a continuous scale. We rely on Mixed Integer Linear Programming to jointly determine the ranks, such that individual decisions benefit from global information. When ranking English adjectives, our global algorithm achieves substantial improvements over previous work on both pairwise and rank correlation metrics (specifically, 70% pairwise accuracy as compared to only 56% by previous work). Moreover, our approach can incorporate external synonymy information (increasing its pairwise accuracy to 78%) and extends easily to new languages. We also make our and data freely</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Web-scale features for full-scale parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="9434" citStr="Bansal and Klein (2011)" startWordPosition="1542" endWordPosition="1545"> n-grams corpus (Brants and Franz, 2006), which contains English n-grams (n = 1 to 5) and their observed frequency counts, generated from nearly 1 trillion word tokens and 95 billion sentences. We consider each pair of words (a1, a2) in the input set in turn. For each pattern p in the two pattern sets (weak-strong Pw3 and strong-weak P3w), we insert the word pair into the pattern as p(a1, a2) to get a phrasal query like “big but not huge”. This is done by replacing the two wildcards in the pattern by the two words in order. Finally, we scan the Web ngrams corpus in a batch approach similar to Bansal and Klein (2011) and collect frequencies of all our phrase queries. Table 2 depicts some examples of useful intensity-based phrase queries and their frequencies in the Web-scale corpus. We also collect frequencies for the input word unigrams and the patterns for normalization purposes. Given a word pair (a1, a2) and a corpus count function cnt, we define 1 W1 = P 1 p1XEPws 1XS1 = P 2 p2EPsw 1 X W2 = P1 p1EPws 1X S2 = P2 p2EPsw with XP1 = cnt(p1) p1EPws such that the final overall weak-strong score is score(a1, a2) = (W1 − S1) − (W2 − S2) (3) cnt(a1) · cnt(a2) Here W1 and S1 represent Web evidence of a1 and a2</context>
</contexts>
<marker>Bansal, Klein, 2011</marker>
<rawString>Mohit Bansal and Dan Klein. 2011. Web-scale features for full-scale parsing. In Proceedings of ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>The Google Web 1T 5-gram corpus version 1.1.</title>
<date>2006</date>
<pages>2006--13</pages>
<contexts>
<context position="8851" citStr="Brants and Franz, 2006" startWordPosition="1431" endWordPosition="1434">lgarriff, 2007). To avoid these issues, we use the largest available 280 (good, great) (great, good) (small, minute) good, but not great -+ 24492.0 not great, just good -+ 248.0 small, almost minute -+ 97.0 good, if not great -+ 1912.0 great or very good -+ 89.0 small, even minute -+ 41.0 good, though not great -+ 504.0 not great but still good -+ 47.0 good, or even great -+ 338.0 not just good but great -+ 181.0 good, almost great -+ 156.0 Table 2: Some examples from the Web-scale corpus of useful intensity-based phrases on adjective pairs. static corpus of counts, the Google n-grams corpus (Brants and Franz, 2006), which contains English n-grams (n = 1 to 5) and their observed frequency counts, generated from nearly 1 trillion word tokens and 95 billion sentences. We consider each pair of words (a1, a2) in the input set in turn. For each pattern p in the two pattern sets (weak-strong Pw3 and strong-weak P3w), we insert the word pair into the pattern as p(a1, a2) to get a phrasal query like “big but not huge”. This is done by replacing the two wildcards in the pattern by the two words in order. Finally, we scan the Web ngrams corpus in a batch approach similar to Bansal and Klein (2011) and collect freq</context>
<context position="22815" citStr="Brants and Franz, 2006" startWordPosition="3829" endWordPosition="3832">p partitions whenever there is no corpus evidence connecting them, ordering the adjectives within each such partition remains a challenging task. This is because the Web evidence will still not necessarily directly relate all adjectives (in a partition) to each other. Additionally, the Web evidence may still indicate the wrong direction. Figure 3 shows the size distribution of the resulting partitions. Patterns To construct our intensity pattern set, we started with a couple of common rankable adjective seed pairs such as (good, great) and (hot, boiling) and used the Web-scale n-grams corpus (Brants and Franz, 2006) to collect the few most frequent patterns between and around these seed-pairs (in both directions). Among these, we manually chose a scales. Instead, their annotators only made pairwise comparisons with select words, using a 5-way classification scheme (neutral, mild, very mild, intense, very intense). 500 438 115 60 35 19 12 14 5 4 3 # of chains 400 300 200 100 0 2 3 4 5 6 7 8 9 10-14 15-17 Length of chain Figure 3: The histogram of cluster sizes after partitioning. 3 3 2 # of chains 41 12 27 0 40 30 20 10 284 small set of intuitive patterns that are linguistically useful for ordering adject</context>
<context position="24437" citStr="Brants and Franz, 2006" startWordPosition="4110" endWordPosition="4113">(Hearst, 1992; Chklovski and Pantel, 2004; Yang and Su, 2007; Turney, 2008; Davidov and Rappoport, 2008). However, our semiautomatic approach is a simple and fast process that extracts a small set of high-quality and very general adjective-scaling patterns. This process can quickly be repeated from scratch in any other language. Moreover, as described in Section 5.1, the English patterns can also be projected automatically to patterns in other languages. Development and Test Sets Section 2.1 describes the method for collecting the intensity scores for adjective pairs, using Web-scale n-grams (Brants and Franz, 2006). We relied on a small development set to test the MILP structure and the pairwise score setup. For this, we manually chose 5 representative adjective clusters from the full set of clusters. The final test set, distinct from this development set, consists of 569 word pairs in 88 clusters, each annotated by two native speakers of English. Both the gold test data (and our code) are freely available.5 To arrive at this data, we randomly drew 30 clusters each for cluster sizes 3, 4, and 5+ from the histogram of partitioned adjective clusters in Figure 3. While labeling a cluster, annotators could </context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. The Google Web 1T 5-gram corpus version 1.1. LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram, 10 European languages, version 1.</title>
<date>2009</date>
<pages>2009--25</pages>
<contexts>
<context position="38368" citStr="Brants and Franz, 2009" startWordPosition="6442" endWordPosition="6445">, here gut and ausgezeichnet, to get the corresponding German pattern ‘* aber nicht *’. Table 7 shows various German intensity patterns that we obtain by projecting from the English patterns as described. The process is repeated with multiple adjective pairs in case different variants are returned, e.g. due to morphology. Most of these translations deliver useful results. Now that we have the target language adjectives and the ranking patterns, we can compute the pairwise intensity scores using large-scale data in that language. We can use the Google n-grams corpora for 10 European languages (Brants and Franz, 2009), and also for Chinese (LDC2010T02) and Japanese (LDC2009T08). For other languages, one can use available large raw-text corpora or Web crawling tools. 5.2 Crosslingual MILP To improve the rankings for lesser-resourced languages, we can further use a joint MILP approach for the new language we want to transfer this process to. Additional constraints between the English 288 Weak-Strong Patterns Strong-Weak Patterns English German English German * but not * * aber nicht * not *just * nicht * gerade * * if not * * wenn nicht * not * but just * nicht * aber nur * * and almost * * und fast * not * </context>
</contexts>
<marker>Brants, Franz, 2009</marker>
<rawString>Thorsten Brants and Alex Franz. 2009. Web 1T 5-gram, 10 European languages, version 1. LDC2009T25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="8108" citStr="Chklovski and Pantel, 2004" startWordPosition="1303" endWordPosition="1306"> pattern collection). Many of these patterns also apply to other parts of speech (e.g. ‘drizzle but not rain’, ‘running or even sprinting’), with significant discrimination on the Web in the right direction. 2.1.3 Pairwise Scores Given an input set of words to be placed on a scale, we first collect evidence of their intensity order by using the above-mentioned intensity patterns and a large, Web-scale text corpus. Previous work on information extraction from limited-sized raw text corpora revealed that coverage is often limited (Hearst, 1992; Hatzivassiloglou and McKeown, 1993). Some studies (Chklovski and Pantel, 2004; Sheinman and Tokunaga, 2009) used hit counts from an online search engine, but this is unstable and irreproducible (Kilgarriff, 2007). To avoid these issues, we use the largest available 280 (good, great) (great, good) (small, minute) good, but not great -+ 24492.0 not great, just good -+ 248.0 small, almost minute -+ 97.0 good, if not great -+ 1912.0 great or very good -+ 89.0 small, even minute -+ 41.0 good, though not great -+ 504.0 not great but still good -+ 47.0 good, or even great -+ 338.0 not just good but great -+ 181.0 good, almost great -+ 156.0 Table 2: Some examples from the Web</context>
<context position="19020" citStr="Chklovski and Pantel, 2004" startWordPosition="3218" endWordPosition="3221"> too sparse for this to work. Thus, many adjectives have no score with a particular headword. In our experiments, we reimplemented this approach and show that our MILP method improves over it by allowing individual pairwise decisions to benefit more from global information. Schulam and Fellbaum (2010) apply the approach of Sheinman and Tokunaga (2009) to German adjectives. Our method extends easily to various foreign languages as described in Section 5. Another related task is the extraction of lexicosyntactic and lexico-semantic intensity-order patterns from large text corpora (Hearst, 1992; Chklovski and Pantel, 2004; Tandon and de Melo, 2010). Sheinman and Tokunaga (2009) follows Davidov and Rappoport (2008) to automatically bootstrap adjective scaling patterns using seed adjectives and Web hits. These methods thus can be used to provide the input patterns for our algorithm. VerbOcean by Chklovski and Pantel (2004) extracts various fine-grained semantic relations (including the stronger-than relation) between pairs of verbs, using lexico-syntactic patterns over the Web. 283 Our approach of jointly ranking a set of words using pairwise evidence is also applicable to the VerbOcean pairs, and should help ad</context>
<context position="23855" citStr="Chklovski and Pantel, 2004" startWordPosition="4019" endWordPosition="4022">gure 3: The histogram of cluster sizes after partitioning. 3 3 2 # of chains 41 12 27 0 40 30 20 10 284 small set of intuitive patterns that are linguistically useful for ordering adjectives, several of which had not been discovered in previous work. These are shown in Table 1. Note that we only collected patterns that were not ambiguous in the two orders, for example the pattern ’* , not *’ is ambiguous because it can be used as both ’good, not great’ and ’great, not good’. Alternatively, one can easily also use fully-automatic bootstrapping techniques based on seed word pairs (Hearst, 1992; Chklovski and Pantel, 2004; Yang and Su, 2007; Turney, 2008; Davidov and Rappoport, 2008). However, our semiautomatic approach is a simple and fast process that extracts a small set of high-quality and very general adjective-scaling patterns. This process can quickly be repeated from scratch in any other language. Moreover, as described in Section 5.1, the English patterns can also be projected automatically to patterns in other languages. Development and Test Sets Section 2.1 describes the method for collecting the intensity scores for adjective pairs, using Web-scale n-grams (Brants and Franz, 2006). We relied on a s</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the web for fine-grained semantic verb relations. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="25909" citStr="Cohen, 1960" startWordPosition="4360" endWordPosition="4361"> words on a scale. Words that seemed indistinguishable in strength could share positions in their annotation. As our goal is to compare scale formation algorithms, we did not include trivial clusters of size 2. On such trivial clusters, the Web evidence alone determines the output and hence all algorithms, includ5http://demelo.org/gdm/intensity/ ing the baseline, obtain the same pairwise accuracy (defined below) of 93.3% on a separate set of 30 random clusters of size 2. Figure 4 shows the distribution of cluster sizes in our main gold set. The inter-annotator agreement in terms of Cohen’s κ (Cohen, 1960) on the pairwise classification task with 3 labels (weaker, stronger, or equal/unknown) was 0.64. In terms of pairwise accuracy, the agreement was 78.0%. 4.2 Metrics In order to thoroughly evaluate the performance of our adjective ordering procedure, we rely on both pairwise and ranking-correlation evaluation metrics. Consider a set of input words A = {a1, a2, ... , an} and two rankings for this set – a gold-standard ranking rG(A) and a predicted ranking rP (A). 4.2.1 Pairwise Accuracy For a pair of words ai, aj, we may consider the classification task of choosing one of three labels (&lt;, &gt;, =?</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated sat analogy questions.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="19114" citStr="Davidov and Rappoport (2008)" startWordPosition="3232" endWordPosition="3235">. In our experiments, we reimplemented this approach and show that our MILP method improves over it by allowing individual pairwise decisions to benefit more from global information. Schulam and Fellbaum (2010) apply the approach of Sheinman and Tokunaga (2009) to German adjectives. Our method extends easily to various foreign languages as described in Section 5. Another related task is the extraction of lexicosyntactic and lexico-semantic intensity-order patterns from large text corpora (Hearst, 1992; Chklovski and Pantel, 2004; Tandon and de Melo, 2010). Sheinman and Tokunaga (2009) follows Davidov and Rappoport (2008) to automatically bootstrap adjective scaling patterns using seed adjectives and Web hits. These methods thus can be used to provide the input patterns for our algorithm. VerbOcean by Chklovski and Pantel (2004) extracts various fine-grained semantic relations (including the stronger-than relation) between pairs of verbs, using lexico-syntactic patterns over the Web. 283 Our approach of jointly ranking a set of words using pairwise evidence is also applicable to the VerbOcean pairs, and should help address similar sparsity issues of local pairwise decisions. Such scales will again be quite use</context>
<context position="23918" citStr="Davidov and Rappoport, 2008" startWordPosition="4029" endWordPosition="4032">3 2 # of chains 41 12 27 0 40 30 20 10 284 small set of intuitive patterns that are linguistically useful for ordering adjectives, several of which had not been discovered in previous work. These are shown in Table 1. Note that we only collected patterns that were not ambiguous in the two orders, for example the pattern ’* , not *’ is ambiguous because it can be used as both ’good, not great’ and ’great, not good’. Alternatively, one can easily also use fully-automatic bootstrapping techniques based on seed word pairs (Hearst, 1992; Chklovski and Pantel, 2004; Yang and Su, 2007; Turney, 2008; Davidov and Rappoport, 2008). However, our semiautomatic approach is a simple and fast process that extracts a small set of high-quality and very general adjective-scaling patterns. This process can quickly be repeated from scratch in any other language. Moreover, as described in Section 5.1, the English patterns can also be projected automatically to patterns in other languages. Development and Test Sets Section 2.1 describes the method for collecting the intensity scores for adjective pairs, using Web-scale n-grams (Brants and Franz, 2006). We relied on a small development set to test the MILP structure and the pairwis</context>
</contexts>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2008. Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated sat analogy questions. In Proceedings ofACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
<author>Christopher Potts</author>
</authors>
<title>Was it good? it was provocative. learning the meaning of scalar adjectives.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>de Marneffe, Manning, Potts, 2010</marker>
<rawString>Marie-Catherine de Marneffe, Christopher D. Manning, and Christopher Potts. 2010. Was it good? it was provocative. learning the meaning of scalar adjectives. In Proceedings of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard de Melo</author>
<author>Gerhard Weikum</author>
</authors>
<title>Towards a universal wordnet by learning from combined evidence.</title>
<date>2009</date>
<booktitle>In Proceedings of CIKM</booktitle>
<marker>de Melo, Weikum, 2009</marker>
<rawString>Gerard de Melo and Gerhard Weikum. 2009. Towards a universal wordnet by learning from combined evidence. In Proceedings of CIKM 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhicheng Dou</author>
<author>Ruihua Song</author>
<author>Xiaojie Yuan</author>
<author>Ji-Rong Wen</author>
</authors>
<title>Are click-through data adequate for learning web search rankings?</title>
<date>2008</date>
<booktitle>In Proc. of CIKM</booktitle>
<contexts>
<context position="27596" citStr="Dou et al., 2008" startWordPosition="4643" endWordPosition="4646">dard label: E 1{LG(ai, aj) = LP(ai, aj)} PW (A) = i&lt;j 4.2.2 Ranking Correlation Coefficients Our second type of evaluation assesses the rank correlation between two ranking permutations (gold-standard and predicted). Many studies use Kendall’s tau (Kendall, 1938), which measures the total number of pairwise inversions, while others prefer Spearman’s rho (Spearman, 1904), which measures the L1 distance between ranks. ⎧ ⎨ ⎩ L(a1, a2) = E 1 i&lt;j 285 Kendall’s tau correlation coefficient We use the Tb version of Kendall’s correlation metric, as it incorporates a correction for ties (Kruskal, 1958; Dou et al., 2008): P − Q 1/ (P + Q + X0) · (P + Q + Y0) where P is the number of concordant pairs, Q is the number of discordant pairs, X0 is the number of pairs tied in the first ranking, Y0 is the number of pairs tied in the second ranking. Given the two rankings of an adjective set A, the gold-standard ranking rG(A) and the predicted ranking rP (A), two words ai, aj are: • concordant iff both rankings have the same strict order of the two elements, i.e., rG(ai) &gt; rG(aj) and rP(ai) &gt; rP(aj), or rG(ai) &lt; rG(aj) and rP(ai) &lt; rP(aj). • discordant iff the two rankings have an inverted strict order of the two ele</context>
</contexts>
<marker>Dou, Song, Yuan, Wen, 2008</marker>
<rawString>Zhicheng Dou, Ruihua Song, Xiaojie Yuan, and Ji-Rong Wen. 2008. Are click-through data adequate for learning web search rankings? In Proc. of CIKM 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>Adjectives in WordNet.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="20889" citStr="Gross and Miller, 1990" startWordPosition="3511" endWordPosition="3514">senalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 4 Experiments 4.1 Data Input Clusters In order to obtain input clusters for evaluation, we started out with the satellite cluster or ‘dumbbell’ structure of adjectives in WordNet 3.0, which consists of two direct antonyms as the poles and a number of other satellite adjectives that are semantically similar to each of the poles (Gross and Miller, 1990). For each antonymy pair, we determined an extended dumbbell set by looking up synonyms and words in related (satellite adjective and ‘see-also’) synonym sets. We cut such an extended dumbbell into two antonymous halves and treated each of these halves as a potential input adjective cluster. Most of these WordNet clusters are noisy for the purpose of our task, i.e. they contain adjectives that appear unrelatable on a single scale due to polysemy and semantic drift, e.g. violent with respect to supernatural and affected. Motivated by Sheinman and Tokunaga (2009), we split such hard-to-relate ad</context>
</contexts>
<marker>Gross, Miller, 1990</marker>
<rawString>Derek Gross and Katherine J. Miller. 1990. Adjectives in WordNet. International Journal of Lexicography, 3(4):265–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="8066" citStr="Hatzivassiloglou and McKeown, 1993" startWordPosition="1297" endWordPosition="1300">ection 4.1 for implementation details regarding our pattern collection). Many of these patterns also apply to other parts of speech (e.g. ‘drizzle but not rain’, ‘running or even sprinting’), with significant discrimination on the Web in the right direction. 2.1.3 Pairwise Scores Given an input set of words to be placed on a scale, we first collect evidence of their intensity order by using the above-mentioned intensity patterns and a large, Web-scale text corpus. Previous work on information extraction from limited-sized raw text corpora revealed that coverage is often limited (Hearst, 1992; Hatzivassiloglou and McKeown, 1993). Some studies (Chklovski and Pantel, 2004; Sheinman and Tokunaga, 2009) used hit counts from an online search engine, but this is unstable and irreproducible (Kilgarriff, 2007). To avoid these issues, we use the largest available 280 (good, great) (great, good) (small, minute) good, but not great -+ 24492.0 not great, just good -+ 248.0 small, almost minute -+ 97.0 good, if not great -+ 1912.0 great or very good -+ 89.0 small, even minute -+ 41.0 good, though not great -+ 504.0 not great but still good -+ 47.0 good, or even great -+ 338.0 not just good but great -+ 181.0 good, almost great -+</context>
<context position="16774" citStr="Hatzivassiloglou and McKeown (1993)" startWordPosition="2878" endWordPosition="2881">gle or cycle, we would have to make arbitrary choices if we were ranking based on sgn(score(a, b)) alone. Instead, we can choose a better ranking based on the magnitude of the pairwise scores. A stronger score between an adjective pair doesn’t necessarily mean that they should be further apart in the ranking. It means that these two words are attested together on the Web with respect to the intensity patterns more than with other candidate words. Therefore, we try to respect the order of such word pairs more in the final ranking when we are breaking constraint-violating cycles. 3 Related Work Hatzivassiloglou and McKeown (1993) presented the first step towards automatic identification of adjective scales, thoroughly discussing the background of adjective semantics and a means of discovering clusters of adjectives that belong on the same scale, thus providing one way of creating the input for our ranking algorithm. Inkpen and Hirst (2006) study near-synonyms and nuances of meaning differentiation (such as stylistic, attitudinal, etc.). They attempt to automatically acquire a knowledge base of near-synonym differences via an unsupervised decision-list algorithm. However, their method depends on a special dictionary of</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1993</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1993. Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning. In Proceedings of ACL 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="20157" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="3393" endWordPosition="3397">et of words using pairwise evidence is also applicable to the VerbOcean pairs, and should help address similar sparsity issues of local pairwise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 4 Experiments 4.1 Data Input Clusters In order to obtain input clusters for evaluation, we started out with the satellite cluster or ‘dumbbell’ structure of adjectives in WordNet 3.0, which consists of two direct antonyms</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of ACL 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Janyce M Wiebe</author>
</authors>
<title>Effects of adjective orientation and gradability on sentence subjectivity.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="20191" citStr="Hatzivassiloglou and Wiebe, 2000" startWordPosition="3398" endWordPosition="3401">is also applicable to the VerbOcean pairs, and should help address similar sparsity issues of local pairwise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 4 Experiments 4.1 Data Input Clusters In order to obtain input clusters for evaluation, we started out with the satellite cluster or ‘dumbbell’ structure of adjectives in WordNet 3.0, which consists of two direct antonyms as the poles and a number of othe</context>
</contexts>
<marker>Hatzivassiloglou, Wiebe, 2000</marker>
<rawString>Vasileios Hatzivassiloglou and Janyce M. Wiebe. 2000. Effects of adjective orientation and gradability on sentence subjectivity. In Proceedings of COLING 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="8029" citStr="Hearst, 1992" startWordPosition="1295" endWordPosition="1296">ork (and see Section 4.1 for implementation details regarding our pattern collection). Many of these patterns also apply to other parts of speech (e.g. ‘drizzle but not rain’, ‘running or even sprinting’), with significant discrimination on the Web in the right direction. 2.1.3 Pairwise Scores Given an input set of words to be placed on a scale, we first collect evidence of their intensity order by using the above-mentioned intensity patterns and a large, Web-scale text corpus. Previous work on information extraction from limited-sized raw text corpora revealed that coverage is often limited (Hearst, 1992; Hatzivassiloglou and McKeown, 1993). Some studies (Chklovski and Pantel, 2004; Sheinman and Tokunaga, 2009) used hit counts from an online search engine, but this is unstable and irreproducible (Kilgarriff, 2007). To avoid these issues, we use the largest available 280 (good, great) (great, good) (small, minute) good, but not great -+ 24492.0 not great, just good -+ 248.0 small, almost minute -+ 97.0 good, if not great -+ 1912.0 great or very good -+ 89.0 small, even minute -+ 41.0 good, though not great -+ 504.0 not great but still good -+ 47.0 good, or even great -+ 338.0 not just good but</context>
<context position="18992" citStr="Hearst, 1992" startWordPosition="3216" endWordPosition="3217">are frequently too sparse for this to work. Thus, many adjectives have no score with a particular headword. In our experiments, we reimplemented this approach and show that our MILP method improves over it by allowing individual pairwise decisions to benefit more from global information. Schulam and Fellbaum (2010) apply the approach of Sheinman and Tokunaga (2009) to German adjectives. Our method extends easily to various foreign languages as described in Section 5. Another related task is the extraction of lexicosyntactic and lexico-semantic intensity-order patterns from large text corpora (Hearst, 1992; Chklovski and Pantel, 2004; Tandon and de Melo, 2010). Sheinman and Tokunaga (2009) follows Davidov and Rappoport (2008) to automatically bootstrap adjective scaling patterns using seed adjectives and Web hits. These methods thus can be used to provide the input patterns for our algorithm. VerbOcean by Chklovski and Pantel (2004) extracts various fine-grained semantic relations (including the stronger-than relation) between pairs of verbs, using lexico-syntactic patterns over the Web. 283 Our approach of jointly ranking a set of words using pairwise evidence is also applicable to the VerbOce</context>
<context position="23827" citStr="Hearst, 1992" startWordPosition="4017" endWordPosition="4018">th of chain Figure 3: The histogram of cluster sizes after partitioning. 3 3 2 # of chains 41 12 27 0 40 30 20 10 284 small set of intuitive patterns that are linguistically useful for ordering adjectives, several of which had not been discovered in previous work. These are shown in Table 1. Note that we only collected patterns that were not ambiguous in the two orders, for example the pattern ’* , not *’ is ambiguous because it can be used as both ’good, not great’ and ’great, not good’. Alternatively, one can easily also use fully-automatic bootstrapping techniques based on seed word pairs (Hearst, 1992; Chklovski and Pantel, 2004; Yang and Su, 2007; Turney, 2008; Davidov and Rappoport, 2008). However, our semiautomatic approach is a simple and fast process that extracts a small set of high-quality and very general adjective-scaling patterns. This process can quickly be repeated from scratch in any other language. Moreover, as described in Section 5.1, the English patterns can also be projected automatically to patterns in other languages. Development and Test Sets Section 2.1 describes the method for collecting the intensity scores for adjective pairs, using Web-scale n-grams (Brants and Fr</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Inkpen</author>
<author>Graeme Hirst</author>
</authors>
<title>Building and using a lexical knowledge base of near-synonym differences.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="17090" citStr="Inkpen and Hirst (2006)" startWordPosition="2926" endWordPosition="2929">se two words are attested together on the Web with respect to the intensity patterns more than with other candidate words. Therefore, we try to respect the order of such word pairs more in the final ranking when we are breaking constraint-violating cycles. 3 Related Work Hatzivassiloglou and McKeown (1993) presented the first step towards automatic identification of adjective scales, thoroughly discussing the background of adjective semantics and a means of discovering clusters of adjectives that belong on the same scale, thus providing one way of creating the input for our ranking algorithm. Inkpen and Hirst (2006) study near-synonyms and nuances of meaning differentiation (such as stylistic, attitudinal, etc.). They attempt to automatically acquire a knowledge base of near-synonym differences via an unsupervised decision-list algorithm. However, their method depends on a special dictionary of synonym differences to learn the extraction patterns, while we use only a raw Web-scale corpus. Mohammad et al. (2013) proposed a method of identifying whether two adjectives are antonymous. This problem is related but distinct, because the degree of antonymy does not necessarily determine their position on an int</context>
</contexts>
<marker>Inkpen, Hirst, 2006</marker>
<rawString>Diana Inkpen and Graeme Hirst. 2006. Building and using a lexical knowledge base of near-synonym differences. Computational Linguistics, 32(2):223–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice G Kendall</author>
</authors>
<title>A new measure of rank correlation.</title>
<date>1938</date>
<journal>Biometrika,</journal>
<pages>30--1</pages>
<contexts>
<context position="27242" citStr="Kendall, 1938" startWordPosition="4586" endWordPosition="4587">ai) &lt; r(aj) &gt; if r(ai) &gt; r(aj) =? if r(ai) = r(aj) For each pair (a1, a2), we compute gold-standard labels LG(a1, a2) and predicted labels LP(a1, a2) as above, and then the pairwise accuracy PW (A) for a particular ordering on A is simply the fraction of pairs that are correctly classified, i.e. for which the predicted label is same as the gold-standard label: E 1{LG(ai, aj) = LP(ai, aj)} PW (A) = i&lt;j 4.2.2 Ranking Correlation Coefficients Our second type of evaluation assesses the rank correlation between two ranking permutations (gold-standard and predicted). Many studies use Kendall’s tau (Kendall, 1938), which measures the total number of pairwise inversions, while others prefer Spearman’s rho (Spearman, 1904), which measures the L1 distance between ranks. ⎧ ⎨ ⎩ L(a1, a2) = E 1 i&lt;j 285 Kendall’s tau correlation coefficient We use the Tb version of Kendall’s correlation metric, as it incorporates a correction for ties (Kruskal, 1958; Dou et al., 2008): P − Q 1/ (P + Q + X0) · (P + Q + Y0) where P is the number of concordant pairs, Q is the number of discordant pairs, X0 is the number of pairs tied in the first ranking, Y0 is the number of pairs tied in the second ranking. Given the two rankin</context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>Maurice G. Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Googleology is bad science.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="8243" citStr="Kilgarriff, 2007" startWordPosition="1325" endWordPosition="1326">h significant discrimination on the Web in the right direction. 2.1.3 Pairwise Scores Given an input set of words to be placed on a scale, we first collect evidence of their intensity order by using the above-mentioned intensity patterns and a large, Web-scale text corpus. Previous work on information extraction from limited-sized raw text corpora revealed that coverage is often limited (Hearst, 1992; Hatzivassiloglou and McKeown, 1993). Some studies (Chklovski and Pantel, 2004; Sheinman and Tokunaga, 2009) used hit counts from an online search engine, but this is unstable and irreproducible (Kilgarriff, 2007). To avoid these issues, we use the largest available 280 (good, great) (great, good) (small, minute) good, but not great -+ 24492.0 not great, just good -+ 248.0 small, almost minute -+ 97.0 good, if not great -+ 1912.0 great or very good -+ 89.0 small, even minute -+ 41.0 good, though not great -+ 504.0 not great but still good -+ 47.0 good, or even great -+ 338.0 not just good but great -+ 181.0 good, almost great -+ 156.0 Table 2: Some examples from the Web-scale corpus of useful intensity-based phrases on adjective pairs. static corpus of counts, the Google n-grams corpus (Brants and Fran</context>
<context position="18114" citStr="Kilgarriff, 2007" startWordPosition="3080" endWordPosition="3081">posed a method of identifying whether two adjectives are antonymous. This problem is related but distinct, because the degree of antonymy does not necessarily determine their position on an intensity scale. Antonyms (e.g., little, big) are not necessarily on the extreme ends of scales. Sheinman and Tokunaga (2009) and Sheinman et al. (2012) present the most closely related previous work on adjective intensities. They collect lexicosemantic patterns via bootstrapping from seed adjective pairs to obtain pairwise intensities, albeit using search engine ‘hits’, which are unstable and problematic (Kilgarriff, 2007). While their approach is primarily evaluated in terms of a local pairwise classification task, they also suggest the possibility of ordering adjectives on a scale using a pivotbased partitioning approach. Although intuitive in theory, the extracted pairwise scores are frequently too sparse for this to work. Thus, many adjectives have no score with a particular headword. In our experiments, we reimplemented this approach and show that our MILP method improves over it by allowing individual pairwise decisions to benefit more from global information. Schulam and Fellbaum (2010) apply the approac</context>
</contexts>
<marker>Kilgarriff, 2007</marker>
<rawString>Adam Kilgarriff. 2007. Googleology is bad science. Computational Linguistics, 33(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Kruskal</author>
</authors>
<title>Ordinal measures of association.</title>
<date>1958</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>53</volume>
<issue>284</issue>
<contexts>
<context position="27577" citStr="Kruskal, 1958" startWordPosition="4641" endWordPosition="4642">s the gold-standard label: E 1{LG(ai, aj) = LP(ai, aj)} PW (A) = i&lt;j 4.2.2 Ranking Correlation Coefficients Our second type of evaluation assesses the rank correlation between two ranking permutations (gold-standard and predicted). Many studies use Kendall’s tau (Kendall, 1938), which measures the total number of pairwise inversions, while others prefer Spearman’s rho (Spearman, 1904), which measures the L1 distance between ranks. ⎧ ⎨ ⎩ L(a1, a2) = E 1 i&lt;j 285 Kendall’s tau correlation coefficient We use the Tb version of Kendall’s correlation metric, as it incorporates a correction for ties (Kruskal, 1958; Dou et al., 2008): P − Q 1/ (P + Q + X0) · (P + Q + Y0) where P is the number of concordant pairs, Q is the number of discordant pairs, X0 is the number of pairs tied in the first ranking, Y0 is the number of pairs tied in the second ranking. Given the two rankings of an adjective set A, the gold-standard ranking rG(A) and the predicted ranking rP (A), two words ai, aj are: • concordant iff both rankings have the same strict order of the two elements, i.e., rG(ai) &gt; rG(aj) and rP(ai) &gt; rP(aj), or rG(ai) &lt; rG(aj) and rP(ai) &lt; rP(aj). • discordant iff the two rankings have an inverted strict o</context>
</contexts>
<marker>Kruskal, 1958</marker>
<rawString>William H. Kruskal. 1958. Ordinal measures of association. Journal of the American Statistical Association, 53(284):814–861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingjing Liu</author>
<author>Stephanie Seneff</author>
</authors>
<title>Review sentiment scoring via a parse-and-paraphrase paradigm.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="20239" citStr="Liu and Seneff, 2009" startWordPosition="3406" endWordPosition="3409">ddress similar sparsity issues of local pairwise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 4 Experiments 4.1 Data Input Clusters In order to obtain input clusters for evaluation, we started out with the satellite cluster or ‘dumbbell’ structure of adjectives in WordNet 3.0, which consists of two direct antonyms as the poles and a number of other satellite adjectives that are semantically sim</context>
</contexts>
<marker>Liu, Seneff, 2009</marker>
<rawString>Jingjing Liu and Stephanie Seneff. 2009. Review sentiment scoring via a parse-and-paraphrase paradigm. In Proceedings of EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1357" citStr="Miller, 1995" startWordPosition="205" endWordPosition="206">on. When ranking English adjectives, our global algorithm achieves substantial improvements over previous work on both pairwise and rank correlation metrics (specifically, 70% pairwise accuracy as compared to only 56% by previous work). Moreover, our approach can incorporate external synonymy information (increasing its pairwise accuracy to 78%) and extends easily to new languages. We also make our code and data freely available.1 1 Introduction Current lexical resources such as dictionaries and thesauri do not provide information about the intensity order of words. For example, both WordNet (Miller, 1995) and Roget’s 21st Century Thesaurus (thesaurus.com) present acceptable, great, and superb as synonyms of the adjective good. However, a native speaker knows that these words represent varying intensity and can in fact generally be ranked by intensity as acceptable &lt; good &lt; great &lt; superb. Similarly, warm &lt; hot &lt; scorching are identified as synonyms in these resources. Ranking information, 1http://demelo.org/gdm/intensity/ Mohit Bansal CS Division, UC Berkeley mbansal@cs.berkeley.edu however, is crucial because it allows us to differentiate e.g. between various intensities of an emotion, and is</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Said M Mohammad</author>
<author>Bonnie J Dorr</author>
<author>Graeme Hirst</author>
<author>Peter D Turney</author>
</authors>
<title>Computing lexical contrast. Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="17493" citStr="Mohammad et al. (2013)" startWordPosition="2985" endWordPosition="2988">sing the background of adjective semantics and a means of discovering clusters of adjectives that belong on the same scale, thus providing one way of creating the input for our ranking algorithm. Inkpen and Hirst (2006) study near-synonyms and nuances of meaning differentiation (such as stylistic, attitudinal, etc.). They attempt to automatically acquire a knowledge base of near-synonym differences via an unsupervised decision-list algorithm. However, their method depends on a special dictionary of synonym differences to learn the extraction patterns, while we use only a raw Web-scale corpus. Mohammad et al. (2013) proposed a method of identifying whether two adjectives are antonymous. This problem is related but distinct, because the degree of antonymy does not necessarily determine their position on an intensity scale. Antonyms (e.g., little, big) are not necessarily on the extreme ends of scales. Sheinman and Tokunaga (2009) and Sheinman et al. (2012) present the most closely related previous work on adjective intensities. They collect lexicosemantic patterns via bootstrapping from seed adjective pairs to obtain pairwise intensities, albeit using search engine ‘hits’, which are unstable and problemat</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, Turney, 2013</marker>
<rawString>Said M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and Peter D. Turney. 2013. Computing lexical contrast. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="20312" citStr="Pang and Lee, 2008" startWordPosition="3418" endWordPosition="3421">ll again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 4 Experiments 4.1 Data Input Clusters In order to obtain input clusters for evaluation, we started out with the satellite cluster or ‘dumbbell’ structure of adjectives in WordNet 3.0, which consists of two direct antonyms as the poles and a number of other satellite adjectives that are semantically similar to each of the poles (Gross and Miller, 1990). For each antonymy pai</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Schulam</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Automatically determining the semantic gradation of german adjectives.</title>
<date>2010</date>
<booktitle>In Proceedings of KONVENS</booktitle>
<contexts>
<context position="3661" citStr="Schulam and Fellbaum, 2010" startWordPosition="564" endWordPosition="567">. In our work, we address this sparsity problem by relying on Web-scale data and using an MILP model that extends the pairwise scores to a more complete joint ranking of words on a continuous scale, while maintaining global constraints such as transitivity and giving more weight to the order of word pairs with higher corpus evidence scores. Instead of considering intensity ranking as a pairwise decision process, we thus exploit the fact that individual decisions may benefit from global information, e.g. about how two words relate to some third word. Previous work (Sheinman and Tokunaga, 2009; Schulam and Fellbaum, 2010; Sheinman et al., 2012) has also used lexico-semantic patterns to or279 Transactions of the Association for Computational Linguistics, 1 (2013) 279–290. Action Editor: Lillian Lee. Submitted 11/2012; Revised 1/2013; Published 7/2013. c�2013 Association for Computational Linguistics. der adjectives. They mainly evaluate their algorithm on a set of pairwise decisions, but also present a partitioning approach that attempts to form scales by placing each adjective to the left or right of pivot words. Unfortunately, this approach often fails because many pairs lack order-based evidence even on the</context>
<context position="18696" citStr="Schulam and Fellbaum (2010)" startWordPosition="3168" endWordPosition="3172">e unstable and problematic (Kilgarriff, 2007). While their approach is primarily evaluated in terms of a local pairwise classification task, they also suggest the possibility of ordering adjectives on a scale using a pivotbased partitioning approach. Although intuitive in theory, the extracted pairwise scores are frequently too sparse for this to work. Thus, many adjectives have no score with a particular headword. In our experiments, we reimplemented this approach and show that our MILP method improves over it by allowing individual pairwise decisions to benefit more from global information. Schulam and Fellbaum (2010) apply the approach of Sheinman and Tokunaga (2009) to German adjectives. Our method extends easily to various foreign languages as described in Section 5. Another related task is the extraction of lexicosyntactic and lexico-semantic intensity-order patterns from large text corpora (Hearst, 1992; Chklovski and Pantel, 2004; Tandon and de Melo, 2010). Sheinman and Tokunaga (2009) follows Davidov and Rappoport (2008) to automatically bootstrap adjective scaling patterns using seed adjectives and Web hits. These methods thus can be used to provide the input patterns for our algorithm. VerbOcean b</context>
</contexts>
<marker>Schulam, Fellbaum, 2010</marker>
<rawString>Peter F. Schulam and Christiane Fellbaum. 2010. Automatically determining the semantic gradation of german adjectives. In Proceedings of KONVENS 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Sheinman</author>
<author>Takenobu Tokunaga</author>
</authors>
<title>AdjScales: Visualizing differences between adjectives for language learners.</title>
<date>2009</date>
<journal>IEICE Transactions on Information and Systems,</journal>
<volume>92</volume>
<issue>8</issue>
<contexts>
<context position="3633" citStr="Sheinman and Tokunaga, 2009" startWordPosition="560" endWordPosition="563"> 2006; Suchanek et al., 2009). In our work, we address this sparsity problem by relying on Web-scale data and using an MILP model that extends the pairwise scores to a more complete joint ranking of words on a continuous scale, while maintaining global constraints such as transitivity and giving more weight to the order of word pairs with higher corpus evidence scores. Instead of considering intensity ranking as a pairwise decision process, we thus exploit the fact that individual decisions may benefit from global information, e.g. about how two words relate to some third word. Previous work (Sheinman and Tokunaga, 2009; Schulam and Fellbaum, 2010; Sheinman et al., 2012) has also used lexico-semantic patterns to or279 Transactions of the Association for Computational Linguistics, 1 (2013) 279–290. Action Editor: Lillian Lee. Submitted 11/2012; Revised 1/2013; Published 7/2013. c�2013 Association for Computational Linguistics. der adjectives. They mainly evaluate their algorithm on a set of pairwise decisions, but also present a partitioning approach that attempts to form scales by placing each adjective to the left or right of pivot words. Unfortunately, this approach often fails because many pairs lack orde</context>
<context position="6999" citStr="Sheinman and Tokunaga, 2009" startWordPosition="1118" endWordPosition="1121">although still ? ? (,) (and/or) almost ? not ? (,) though still ? not only ? but ? ? (,) or very ? not just ? but ? Table 1: Ranking patterns used in this work. Among the patterns represented by the regular expressions above, we use only those that capture less than or equal to five words (to fit in the Google n-grams, see Section 2.1.2). Articles (a, an, the) are allowed to appear before the wildcards wherever possible. 2.1.2 Intensity Patterns Linguistic studies have found lexical patterns like ‘? but not ?’ (e.g. good but not great) to reveal order information between a pair of adjectives (Sheinman and Tokunaga, 2009). We assume that we have two sets of lexical patterns that allow us to infer the most likely ordering between two words when encountered in a corpus. A first pattern set, Pws, contains patterns that reflect a weak-strong order between a pair of word (the first word is weaker than the second), and a second pattern set, Psw, captures the strong-weak order. See Table 1 for the adjective patterns that we used in this work (and see Section 4.1 for implementation details regarding our pattern collection). Many of these patterns also apply to other parts of speech (e.g. ‘drizzle but not rain’, ‘runni</context>
<context position="17812" citStr="Sheinman and Tokunaga (2009)" startWordPosition="3034" endWordPosition="3037">tc.). They attempt to automatically acquire a knowledge base of near-synonym differences via an unsupervised decision-list algorithm. However, their method depends on a special dictionary of synonym differences to learn the extraction patterns, while we use only a raw Web-scale corpus. Mohammad et al. (2013) proposed a method of identifying whether two adjectives are antonymous. This problem is related but distinct, because the degree of antonymy does not necessarily determine their position on an intensity scale. Antonyms (e.g., little, big) are not necessarily on the extreme ends of scales. Sheinman and Tokunaga (2009) and Sheinman et al. (2012) present the most closely related previous work on adjective intensities. They collect lexicosemantic patterns via bootstrapping from seed adjective pairs to obtain pairwise intensities, albeit using search engine ‘hits’, which are unstable and problematic (Kilgarriff, 2007). While their approach is primarily evaluated in terms of a local pairwise classification task, they also suggest the possibility of ordering adjectives on a scale using a pivotbased partitioning approach. Although intuitive in theory, the extracted pairwise scores are frequently too sparse for th</context>
<context position="19077" citStr="Sheinman and Tokunaga (2009)" startWordPosition="3227" endWordPosition="3230">e no score with a particular headword. In our experiments, we reimplemented this approach and show that our MILP method improves over it by allowing individual pairwise decisions to benefit more from global information. Schulam and Fellbaum (2010) apply the approach of Sheinman and Tokunaga (2009) to German adjectives. Our method extends easily to various foreign languages as described in Section 5. Another related task is the extraction of lexicosyntactic and lexico-semantic intensity-order patterns from large text corpora (Hearst, 1992; Chklovski and Pantel, 2004; Tandon and de Melo, 2010). Sheinman and Tokunaga (2009) follows Davidov and Rappoport (2008) to automatically bootstrap adjective scaling patterns using seed adjectives and Web hits. These methods thus can be used to provide the input patterns for our algorithm. VerbOcean by Chklovski and Pantel (2004) extracts various fine-grained semantic relations (including the stronger-than relation) between pairs of verbs, using lexico-syntactic patterns over the Web. 283 Our approach of jointly ranking a set of words using pairwise evidence is also applicable to the VerbOcean pairs, and should help address similar sparsity issues of local pairwise decisions</context>
<context position="21456" citStr="Sheinman and Tokunaga (2009)" startWordPosition="3604" endWordPosition="3607">tically similar to each of the poles (Gross and Miller, 1990). For each antonymy pair, we determined an extended dumbbell set by looking up synonyms and words in related (satellite adjective and ‘see-also’) synonym sets. We cut such an extended dumbbell into two antonymous halves and treated each of these halves as a potential input adjective cluster. Most of these WordNet clusters are noisy for the purpose of our task, i.e. they contain adjectives that appear unrelatable on a single scale due to polysemy and semantic drift, e.g. violent with respect to supernatural and affected. Motivated by Sheinman and Tokunaga (2009), we split such hard-to-relate adjectives into smaller scale-specific subgroups using the corpus evidence4. For this, we consider an undi4Note that we do not use the WordNet dataset of Sheinman and Tokunaga (2009) for evaluation, as it does not provide full 3 4 5 6 7 8 Length of chain Figure 4: The histogram of cluster sizes in the test set. rected edge between each pair of adjectives that has a non-zero intensity score (based on the Web-scale scoring procedure described in Section 2.1.3). The resulting graph is then partitioned into connected components such that any adjectives in a subgraph </context>
<context position="30722" citStr="Sheinman and Tokunaga (2009)" startWordPosition="5191" endWordPosition="5194">is 48.2%, while the ranking measures are undefined because the individual pairs do not lead to a coherent scale. Divide-and-Conquer The divide-and-conquer baseline recursively splits a set of words into three subgroups, placed to the left (weaker), on the same position (no evidence), or to the right (stronger) of a given randomly chosen pivot word. While this approach shows only a minor improvement in terms of the pairwise accuracy (50.6%), its main benefit is that one obtains well-defined intensity scales rather than just a collection of pairwise scores. Sheinman and Tokunaga The approach by Sheinman and Tokunaga (2009) involves a similar divide-and-conquer based partitioning in the first phase, except that their method makes use of synonymy information from WordNet and uses all synonyms in WordNet’s synset for the headword as neutral pivot elements (if the headword is not in WordNet, then the word with the maximal unigram frequency is chosen). In the second phase, their method performs pairwise comparisons within the more intense and less intense subgroups. We reimplement their approach here, using the Google NGrams dataset instead of online Web search engine hits. We observe a small improvement over the We</context>
<context position="32674" citStr="Sheinman and Tokunaga (2009)" startWordPosition="5520" endWordPosition="5523">asure scores are undefined for their approach. This is because in some cases their method placed all words on the same position in the scale, which these measures cannot handle even in their tie-corrected versions. Overall, the Sheinman and Tokunaga approach does not aggregate information sufficiently well at the global level and often fails to make use of transitive inference. MILP Our MILP exploits the same pairwise scores to induce significantly more accurate pairwise labels with 69.6% accuracy, a 41% relative error reduction over the Web baseline, 38% over Divide-and-Conquer, and 32% over Sheinman and Tokunaga (2009). We further see that our MILP method is able to exploit external synonymy (equivalence) information (using synonyms marked by the annotators). The accuracy of the pairwise scores as well as the quality of the overall ranking increase even further to 78.2%, approaching the human interannotator agreement. In terms of average correlation coefficients, we observe similar improvement trends from the MILP, but of different magnitudes, because these averages give small clusters the same weight as larger ones. 4.4 Analysis Confusion Matrices For a given approach, we can study the confusion matrix obt</context>
</contexts>
<marker>Sheinman, Tokunaga, 2009</marker>
<rawString>Vera Sheinman and Takenobu Tokunaga. 2009. AdjScales: Visualizing differences between adjectives for language learners. IEICE Transactions on Information and Systems, 92(8):1542–1550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Sheinman</author>
<author>Takenobu Tokunaga</author>
<author>I Julien</author>
<author>P Schulam</author>
<author>C Fellbaum</author>
</authors>
<title>Refining WordNet adjective dumbbells using intensity relations.</title>
<date>2012</date>
<booktitle>In Proceedings of Global WordNet Conference</booktitle>
<contexts>
<context position="3685" citStr="Sheinman et al., 2012" startWordPosition="568" endWordPosition="571">is sparsity problem by relying on Web-scale data and using an MILP model that extends the pairwise scores to a more complete joint ranking of words on a continuous scale, while maintaining global constraints such as transitivity and giving more weight to the order of word pairs with higher corpus evidence scores. Instead of considering intensity ranking as a pairwise decision process, we thus exploit the fact that individual decisions may benefit from global information, e.g. about how two words relate to some third word. Previous work (Sheinman and Tokunaga, 2009; Schulam and Fellbaum, 2010; Sheinman et al., 2012) has also used lexico-semantic patterns to or279 Transactions of the Association for Computational Linguistics, 1 (2013) 279–290. Action Editor: Lillian Lee. Submitted 11/2012; Revised 1/2013; Published 7/2013. c�2013 Association for Computational Linguistics. der adjectives. They mainly evaluate their algorithm on a set of pairwise decisions, but also present a partitioning approach that attempts to form scales by placing each adjective to the left or right of pivot words. Unfortunately, this approach often fails because many pairs lack order-based evidence even on the Web, as explained in mo</context>
<context position="17839" citStr="Sheinman et al. (2012)" startWordPosition="3039" endWordPosition="3042">ly acquire a knowledge base of near-synonym differences via an unsupervised decision-list algorithm. However, their method depends on a special dictionary of synonym differences to learn the extraction patterns, while we use only a raw Web-scale corpus. Mohammad et al. (2013) proposed a method of identifying whether two adjectives are antonymous. This problem is related but distinct, because the degree of antonymy does not necessarily determine their position on an intensity scale. Antonyms (e.g., little, big) are not necessarily on the extreme ends of scales. Sheinman and Tokunaga (2009) and Sheinman et al. (2012) present the most closely related previous work on adjective intensities. They collect lexicosemantic patterns via bootstrapping from seed adjective pairs to obtain pairwise intensities, albeit using search engine ‘hits’, which are unstable and problematic (Kilgarriff, 2007). While their approach is primarily evaluated in terms of a local pairwise classification task, they also suggest the possibility of ordering adjectives on a scale using a pivotbased partitioning approach. Although intuitive in theory, the extracted pairwise scores are frequently too sparse for this to work. Thus, many adje</context>
</contexts>
<marker>Sheinman, Tokunaga, Julien, Schulam, Fellbaum, 2012</marker>
<rawString>Vera Sheinman, Takenobu Tokunaga, I. Julien, P. Schulam, and C. Fellbaum. 2012. Refining WordNet adjective dumbbells using intensity relations. In Proceedings of Global WordNet Conference 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL</booktitle>
<contexts>
<context position="3011" citStr="Snow et al., 2006" startWordPosition="457" endWordPosition="460">sible by the vast amounts of world knowledge that are now available. We use lexico-semantic information extracted from a Web-scale corpus in conjunction with an algorithm based on a Mixed Integer Linear Program (MILP). Linguistic analyses have identified phrases such as good but not great or hot and almost scorching in a text corpus as sources of evidence about the relative intensities of words. However, pure information extraction approaches often fail to provide enough coverage for real-world downstream applications (Tandon and de Melo, 2010), unless some form of advanced inference is used (Snow et al., 2006; Suchanek et al., 2009). In our work, we address this sparsity problem by relying on Web-scale data and using an MILP model that extends the pairwise scores to a more complete joint ranking of words on a continuous scale, while maintaining global constraints such as transitivity and giving more weight to the order of word pairs with higher corpus evidence scores. Instead of considering intensity ranking as a pairwise decision process, we thus exploit the fact that individual decisions may benefit from global information, e.g. about how two words relate to some third word. Previous work (Shein</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of COLING/ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Spearman</author>
</authors>
<title>The proof and measurement of association between two things. The American journal ofpsychology,</title>
<date>1904</date>
<pages>15--1</pages>
<contexts>
<context position="27351" citStr="Spearman, 1904" startWordPosition="4601" endWordPosition="4602">G(a1, a2) and predicted labels LP(a1, a2) as above, and then the pairwise accuracy PW (A) for a particular ordering on A is simply the fraction of pairs that are correctly classified, i.e. for which the predicted label is same as the gold-standard label: E 1{LG(ai, aj) = LP(ai, aj)} PW (A) = i&lt;j 4.2.2 Ranking Correlation Coefficients Our second type of evaluation assesses the rank correlation between two ranking permutations (gold-standard and predicted). Many studies use Kendall’s tau (Kendall, 1938), which measures the total number of pairwise inversions, while others prefer Spearman’s rho (Spearman, 1904), which measures the L1 distance between ranks. ⎧ ⎨ ⎩ L(a1, a2) = E 1 i&lt;j 285 Kendall’s tau correlation coefficient We use the Tb version of Kendall’s correlation metric, as it incorporates a correction for ties (Kruskal, 1958; Dou et al., 2008): P − Q 1/ (P + Q + X0) · (P + Q + Y0) where P is the number of concordant pairs, Q is the number of discordant pairs, X0 is the number of pairs tied in the first ranking, Y0 is the number of pairs tied in the second ranking. Given the two rankings of an adjective set A, the gold-standard ranking rG(A) and the predicted ranking rP (A), two words ai, aj </context>
</contexts>
<marker>Spearman, 1904</marker>
<rawString>Charles Spearman. 1904. The proof and measurement of association between two things. The American journal ofpsychology, 15(1):72–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Mauro Sozio</author>
<author>Gerhard Weikum</author>
</authors>
<title>SOFIE: a self-organizing framework for information extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW</booktitle>
<contexts>
<context position="3035" citStr="Suchanek et al., 2009" startWordPosition="461" endWordPosition="464">mounts of world knowledge that are now available. We use lexico-semantic information extracted from a Web-scale corpus in conjunction with an algorithm based on a Mixed Integer Linear Program (MILP). Linguistic analyses have identified phrases such as good but not great or hot and almost scorching in a text corpus as sources of evidence about the relative intensities of words. However, pure information extraction approaches often fail to provide enough coverage for real-world downstream applications (Tandon and de Melo, 2010), unless some form of advanced inference is used (Snow et al., 2006; Suchanek et al., 2009). In our work, we address this sparsity problem by relying on Web-scale data and using an MILP model that extends the pairwise scores to a more complete joint ranking of words on a continuous scale, while maintaining global constraints such as transitivity and giving more weight to the order of word pairs with higher corpus evidence scores. Instead of considering intensity ranking as a pairwise decision process, we thus exploit the fact that individual decisions may benefit from global information, e.g. about how two words relate to some third word. Previous work (Sheinman and Tokunaga, 2009; </context>
</contexts>
<marker>Suchanek, Sozio, Weikum, 2009</marker>
<rawString>Fabian M. Suchanek, Mauro Sozio, and Gerhard Weikum. 2009. SOFIE: a self-organizing framework for information extraction. In Proceedings of WWW 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloskiy</author>
<author>Kimberly Vollz</author>
</authors>
<title>Lexicon-based methods for sentiment analysis. Computational Linguistics.</title>
<date>2011</date>
<contexts>
<context position="20261" citStr="Taboada et al., 2011" startWordPosition="3410" endWordPosition="3413">y issues of local pairwise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 4 Experiments 4.1 Data Input Clusters In order to obtain input clusters for evaluation, we started out with the satellite cluster or ‘dumbbell’ structure of adjectives in WordNet 3.0, which consists of two direct antonyms as the poles and a number of other satellite adjectives that are semantically similar to each of the po</context>
</contexts>
<marker>Taboada, Brooke, Tofiloskiy, Vollz, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloskiy, and Kimberly Vollz. 2011. Lexicon-based methods for sentiment analysis. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niket Tandon</author>
<author>Gerard de Melo</author>
</authors>
<title>Information extraction from web-scale n-gram data.</title>
<date>2010</date>
<booktitle>In Proceedings of the SIGIR 2010 Web N-gram Workshop.</booktitle>
<marker>Tandon, de Melo, 2010</marker>
<rawString>Niket Tandon and Gerard de Melo. 2010. Information extraction from web-scale n-gram data. In Proceedings of the SIGIR 2010 Web N-gram Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="20217" citStr="Turney and Littman, 2003" startWordPosition="3402" endWordPosition="3405">n pairs, and should help address similar sparsity issues of local pairwise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 4 Experiments 4.1 Data Input Clusters In order to obtain input clusters for evaluation, we started out with the satellite cluster or ‘dumbbell’ structure of adjectives in WordNet 3.0, which consists of two direct antonyms as the poles and a number of other satellite adjectives tha</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Trans. Inf. Syst., 21(4):315–346, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="23888" citStr="Turney, 2008" startWordPosition="4027" endWordPosition="4028">rtitioning. 3 3 2 # of chains 41 12 27 0 40 30 20 10 284 small set of intuitive patterns that are linguistically useful for ordering adjectives, several of which had not been discovered in previous work. These are shown in Table 1. Note that we only collected patterns that were not ambiguous in the two orders, for example the pattern ’* , not *’ is ambiguous because it can be used as both ’good, not great’ and ’great, not good’. Alternatively, one can easily also use fully-automatic bootstrapping techniques based on seed word pairs (Hearst, 1992; Chklovski and Pantel, 2004; Yang and Su, 2007; Turney, 2008; Davidov and Rappoport, 2008). However, our semiautomatic approach is a simple and fast process that extracts a small set of high-quality and very general adjective-scaling patterns. This process can quickly be repeated from scratch in any other language. Moreover, as described in Section 5.1, the English patterns can also be projected automatically to patterns in other languages. Development and Test Sets Section 2.1 describes the method for collecting the intensity scores for adjective pairs, using Web-scale n-grams (Brants and Franz, 2006). We relied on a small development set to test the </context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of COLING 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
</authors>
<title>Coreference resolution using semantic relatedness information from automatically discovered patterns.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="23874" citStr="Yang and Su, 2007" startWordPosition="4023" endWordPosition="4026">ster sizes after partitioning. 3 3 2 # of chains 41 12 27 0 40 30 20 10 284 small set of intuitive patterns that are linguistically useful for ordering adjectives, several of which had not been discovered in previous work. These are shown in Table 1. Note that we only collected patterns that were not ambiguous in the two orders, for example the pattern ’* , not *’ is ambiguous because it can be used as both ’good, not great’ and ’great, not good’. Alternatively, one can easily also use fully-automatic bootstrapping techniques based on seed word pairs (Hearst, 1992; Chklovski and Pantel, 2004; Yang and Su, 2007; Turney, 2008; Davidov and Rappoport, 2008). However, our semiautomatic approach is a simple and fast process that extracts a small set of high-quality and very general adjective-scaling patterns. This process can quickly be repeated from scratch in any other language. Moreover, as described in Section 5.1, the English patterns can also be projected automatically to patterns in other languages. Development and Test Sets Section 2.1 describes the method for collecting the intensity scores for adjective pairs, using Web-scale n-grams (Brants and Franz, 2006). We relied on a small development se</context>
</contexts>
<marker>Yang, Su, 2007</marker>
<rawString>Xiaofeng Yang and Jian Su. 2007. Coreference resolution using semantic relatedness information from automatically discovered patterns. In Proceedings of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="20291" citStr="Yessenalina and Cardie, 2011" startWordPosition="3414" endWordPosition="3417">wise decisions. Such scales will again be quite useful for language learners and language understanding tools. de Marneffe et al. (2010) infer yes-or-no answers to questions with responses involving scalar adjectives in a dialogue corpus. They correlate adjectives with ratings in a movie review corpus to find that good appears in lower-rated reviews than excellent. Finally, there has been a lot of work on measuring the general sentiment polarity of words (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000; Turney and Littman, 2003; Liu and Seneff, 2009; Taboada et al., 2011; Yessenalina and Cardie, 2011; Pang and Lee, 2008). Our work instead aims at producing a large, unrestricted number of individual intensity scales for different qualities and hence can help in fine-grained sentiment analysis with respect to very particular content aspects. 4 Experiments 4.1 Data Input Clusters In order to obtain input clusters for evaluation, we started out with the satellite cluster or ‘dumbbell’ structure of adjectives in WordNet 3.0, which consists of two direct antonyms as the poles and a number of other satellite adjectives that are semantically similar to each of the poles (Gross and Miller, 1990). </context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In Proceedings of EMNLP 2011.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>