<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001971">
<title confidence="0.997587">
Swiss-Chocolate: Sentiment Detection
using Sparse SVMs and Part-Of-Speech n-Grams
</title>
<author confidence="0.983712">
Martin Jaggi Fatih Uzdilli and Mark Cieliebak
</author>
<affiliation confidence="0.775683">
ETH Zurich Zurich University of Applied Sciences
Z¨urich, Switzerland Winterthur, Switzerland
</affiliation>
<email confidence="0.986884">
jaggi@inf.ethz.ch { uzdi, ciel } @zhaw.ch
</email>
<sectionHeader confidence="0.993497" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999985285714286">
We describe a classifier to predict the
message-level sentiment of English micro-
blog messages from Twitter. This pa-
per describes the classifier submitted to
the SemEval-2014 competition (Task 9B).
Our approach was to build up on the sys-
tem of the last year’s winning approach
by NRC Canada 2013 (Mohammad et al.,
2013), with some modifications and addi-
tions of features, and additional sentiment
lexicons. Furthermore, we used a sparse
(`1-regularized) SVM, instead of the more
commonly used `2-regularization, result-
ing in a very sparse linear classifier.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998681157894737">
With the immense growth of user generated text
online, the interest in automatic sentiment analy-
sis of text has greatly increased recently in both
academia and industry.
In this paper, we describe our approach for a
modified SVM based classifier for short text as in
Twitter messages. Our system has participated in
the SemEval-2014 Task 9 competition, “Sentiment
Analysis in Twitter, Subtask–B Message Polarity
Classification” (Rosenthal et al., 2014). The goal
is to classify a tweet (on the full message level)
into the three classes positive, negative, and neu-
tral. An almost identical competition was already
run in 2013.
Our Results in the Competition. Our approach
was ranked on the 8th place out of the 50 partici-
pating submissions, with an F1-score of 67.54 on
the Twitter-2014 test set. The 2014 winning team
obtained an average F1-score of 70.96.
</bodyText>
<footnote confidence="0.90784975">
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.997737357142857">
(The more detailed rankings of our approach
were 4th rank on the LiveJournal data, 5th on the
SMS data (2013), 18th on Twitter-2013, and 16th
on Twitter Sarcasm, see (Rosenthal et al., 2014)
for full details and all results).
Data. In the competition, the tweets for training
and development were only provided as tweet IDs.
A fraction (10-15%) of the tweets were no longer
available on twitter, which makes the results of the
competition not fully comparable. For testing, in
addition to last years data (tweets and SMS), new
tweets and data from a surprise domain were pro-
vided. An overview of the data, which we were
able to download, is shown in Table 1.
</bodyText>
<tableCaption confidence="0.795173">
Table 1: Overview of the data we found available
for training, development and testing.
</tableCaption>
<table confidence="0.99968625">
Dataset Total Positive Negative Neutral
Train (Tweets) 8224 3058 1210 3956
Dev (Tweets) 1417 494 286 637
Test: Twitter2014 1853 982 202 669
Test: Twitter2013 3813 1572 601 1640
Test: SMS2013 2093 492 394 1207
Test: Tw2014Sarcasm 86 33 40 13
Test: LiveJournal2014 1142 427 304 411
</table>
<sectionHeader confidence="0.790575" genericHeader="method">
2 Description of Our Approach
</sectionHeader>
<bodyText confidence="0.999901615384615">
Compared to the previous NRC Canada 2013
approach (Mohammad et al., 2013), our main
changes are the following three: First we use
sparse linear classifiers instead of classical dense
ones. Secondly, we drop n-gram features com-
pletely, in favor of what we call part-of-speech
n-grams, which are n-grams where up to two to-
kens are the original ones, and the rest of the to-
kens is replaced by their corresponding POS tag
(noun, verb, punctuation etc). Third, we added
two new sentiment lexicons, containing numerical
scores associated for all 3 classes (positive, neu-
tral, negative), instead of just 2 as in classical po-
</bodyText>
<page confidence="0.978727">
601
</page>
<note confidence="0.730699">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 601–604,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.984979142857143">
larity lexicons. All changes are described in more
detail in Sections 4 and 3 below.
Performance. We tried to reproduce the same
classifier as in (Mohammad et al., 2013) as a base-
line for comparison.
Trying to quantify our contributions, when
adding all our additional features and tricks de-
scribed below, the score of our method increases
from the baseline of 63.25 to 64.81 (on the Twitter-
2013 test set), which is a gain of 1.56 points in F1.
Baseline Approach by NRC Canada 2013.
Unfortunately our replica system of Mohammad
et al. (2013) only achieved an F1-score of 63.25 on
the Twitter-2013 test set, while their score in the
2013 competition on the same test set was 69.02,
nearly 6 points higher in F1.
Part of this big difference might be explained
by the fact that the exact same training sets are
not available anymore. Other possibly more im-
portant differences are the SVM classifier variant
used and class weighting (described in Section 4).
Furthermore, we didn’t implement all features in
the exactly same way, see the more detailed de-
scription in Section 3.1.2 below. Although we had
the impression that these changes individually had
only a relatively minor effect, it might be that the
changes together with the different training set add
up to the difference in score.
</bodyText>
<sectionHeader confidence="0.999458" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999984">
Before we describe the linear classifier in Sec-
tion 4, we detail the used features for each tweet
message. On average, we generated 843 features
per tweet. For comparison, the average in our
NRC Canada 2013 replica system was only 285.
Most of the increase in features comes from the
fact that we allowed for slightly longer n-grams
(6 instead of 4), and substrings (length 6 instead
of 5).
</bodyText>
<subsectionHeader confidence="0.9983935">
3.1 New Features
3.1.1 Part of Speech n-grams
</subsectionHeader>
<bodyText confidence="0.999980842105263">
We used the ArkTweetNLP structured prediction
POS tagger provided by Owoputi et al. (2013)
together with their provided standard model
(model.20120919) suitable for twitter data.
Part of speech n-grams are n-grams where up
to two tokens are kept as the original ones, and all
other tokens are replaced by their corresponding
POS tag (noun, verb, punctuation etc). We gener-
ated these modified n-grams for all possible posi-
tions of the one or two original tokens within the n
positions, for 3 &lt; n &lt; 6.
As features for a classifier, we found POS n-
grams at least as useful (with some more robust-
ness) as the n-grams themselves. In our final
approach, we dropped the use of n-grams com-
pletely, and only used POS n-grams instead. The
idea of replacing some of the tokens by their POS
tag is also investigated by Joshi and Penstein-Ros´e
(2009), where the authors used n &lt; 3.
</bodyText>
<subsectionHeader confidence="0.4976515">
3.1.2 Various Changes Compared to NRC
Canada 2013
</subsectionHeader>
<listItem confidence="0.969946777777778">
• We do not allow n-grams (or POS n-grams)
to span over sentence boundaries.
• Substrings of length up to 6 (instead of 5).
• Substring features are weighted in-
creasingly by their length (weights
0.7 · {1.0,1.1,1.2,1.4,1.6,1.9} for lengths
3,4,... )
• Instead of the score itself, we used the sig-
moid value s(t) = 1/(1 + e−t)) of each lexi-
</listItem>
<bodyText confidence="0.962031153846154">
con score. For each lexicon, the 4 scores were
the same as in (Mohammad et al., 2013), i.e.
per tweet, we use the number of tokens ap-
pearing in the lexicon, the sum and the max
of the scores, and the last non-zero score.
We skipped some features from the baseline ap-
proach (because their effect was not significant in
our setting): Elongated words (number of words
with one character repeated more than two times),
and word clustering. Also, we had a slightly sim-
plified variant of how to use the lexicon scores.
We didn’t count the lexicon scores separately per
emotion (pos and neg), but only altogether.
</bodyText>
<subsectionHeader confidence="0.999573">
3.2 Existing Features
</subsectionHeader>
<bodyText confidence="0.9980346">
Text Preprocessing. A good tokenization seems
very important for twitter data. We used the pop-
ular tokenizer ArkTweetNLP (Owoputi et al., 2013)
which is suitable for tweets. All text was trans-
formed to lowercase (except for those features in
(Mohammad et al., 2013) which use case infor-
mation). As usual, URLs were normalized to
http://someurl and twitter user IDs to @someuser.
We also employed the usual marking of negated
contexts of a sentence as in (Pang et al., 2002),
</bodyText>
<page confidence="0.993202">
602
</page>
<bodyText confidence="0.6177054">
Our final classifier was trained on 9641 tweets,
which are all we could download from the IDs
given in this years train and dev set.
using the list of negation words from Christopher
Potts’ sentiment tutorial1.
</bodyText>
<sectionHeader confidence="0.993754" genericHeader="method">
4 Classifier
</sectionHeader>
<bodyText confidence="0.992309238095238">
We used a linear support vector machine (SVM)
classifier, which is standard for text data. The Lib-
Linear package (Fan et al., 2008) was employed for
training the multi-class classifier.
Multi-Class Formulation, and Class Weights.
We found significant performance changes de-
pending on which type of multi-class SVM, and
also which regularizer (f1- or `2-norm) is used.
For the multi-class variant, we found the one-
against-all models to perform slightly better than
the Crammer and Singer (2001) formulation.
More importantly, since the 3 classes (posi-
tive, negative and neutral) are quite unbalanced
in size in the training set, it is crucial to set a
good weight for each class in the SVM. We used
(4.52, 1.38, 1.80), which corresponds to the twice
the ratio of each class compared to the average
class size.
Sparse Linear Classifiers. In our setting, an
`1-regularized squared loss SVM (one-against-all)
performed best (this is mode L1R L2LOSS SVC in
LibLinear), despite the fact that `2-regularization is
generally more commonly used in text applica-
tions. We used C = 0.055 for the regularization
parameter, and e = 0.003 as the optimization stop-
ping criterion. We did not employ any kernel, but
always used linear classifiers.
Another benefit of the `1-regularization is that
the resulting classifier is extremely sparse and
compact, which significantly accelerates the eval-
uation of the classifier on large amounts of text,
e.g. for testing. Our final classifier only uses
1985 non-zero features (1427 unigram/substrings,
and 558 other features, such as lexicon scores, n-
grams, POS n-grams, as explained in the previous
Section 3).
As the resulting classifier is so small, it is also
relatively easy to read and interpret. We have
made our final classifier weights publicly available
for download as a text file2 . Every line contains
the feature description followed by the 3 weights
corresponding to the 3 sentiment classes.
</bodyText>
<footnote confidence="0.994857333333333">
1 http://sentiment.christopherpotts.
net/lingstruc.html
2http://www.mBj.net/sentiment/
</footnote>
<sectionHeader confidence="0.982725" genericHeader="evaluation">
5 Lexicons
</sectionHeader>
<bodyText confidence="0.999981625">
A sentiment lexicon is a mapping from words (or
n-grams) to an association score corresponding to
positive or negative sentiment. Such lists can be
constructed either from manually labeled data (su-
pervised), or automatically labeled data (unsuper-
vised) as for example tweets with a positive or
negative smiley. We used the same set of lexicons
as in (Mohammad et al., 2013), with one addition:
</bodyText>
<subsectionHeader confidence="0.995318">
5.1 A Lexicon for 3-Class Classification
</subsectionHeader>
<bodyText confidence="0.999975032258065">
Our main new addition was another type of lexi-
con, which not only provides one score per word,
but 3 of them, (being the association to positive,
negative and neutral). The idea here is to improve
on the discrimination quality, especially for neu-
tral text, and treat all 3 labels in this multi-class
task the same way, instead of just 2 as in the pre-
vious approaches.
Data. We found it challenging to find good
datasets to build such a lexicon. We again used the
Sentiment140 corpus (Go et al., 2009) (containing
tweets with positive or negative emoticons). Using
a subset of 100k positive and 100k negative ones,
we added a set of 100k arbitrary (hopefully neu-
tral) tweets. The neutral set was chosen randomly
from the thinknook.com dataset3 of 1.5mio tweets
(from which we ignored the provided labels, and
counted the tweets as neutral).
We did the same with the movie reviews from
the recent kaggle competition on annotated re-
views from the rotten-tomatoes website4. We au-
tomatically built a lexicon from 100k texts in this
dataset, with the data balanced equally for the
three classes.
Features Used in the Lexicon. To construct the
lexicon, we extracted the POS n-grams (as we de-
scribed in Section 3.1.1 above) from all texts. In
comparison, Mohammad et al. (2013) used non-
contiguous n-grams (unigram–unigram, unigram–
bigram, and bigram–bigram pairs). We only used
POS n-grams with 2 tokens kept original, and the
</bodyText>
<footnote confidence="0.699618166666667">
3 http://thinknook.com/twitter-
sentiment-analysis-training-corpus-
dataset-2012-09-22/
4
http://www.kaggle.com/c/
sentiment-analysis-on-movie-reviews/data
</footnote>
<page confidence="0.998709">
603
</page>
<bodyText confidence="0.999942941176471">
remaining ones replaced by their POS tag, with n
ranging from 3 to 6.
Building the Lexicon. While in (Mohammad
et al., 2013), the score for each n-gram was com-
puted using point-wise mutual information (PMI)
with the labels, we trained a linear classifier on
the same labels instead. The lexicon weights are
set as the resulting classifier weights for our (POS)
n-grams. We used the same type of sparse SVM
trained with LibLinear, for 3 classes, as in the final
classifier.
Download of the Lexicons. We built 4 lexicons
as described above. Thanks to the sparsity of
the linear weights from the SVM, they are again
relatively small, analogous to the final classifier.
We also provide the lexicons for download as text
files5.
</bodyText>
<subsectionHeader confidence="0.996483">
5.2 Existing Lexicons
</subsectionHeader>
<bodyText confidence="0.997989318181818">
Lexicons from Manually Labeled Data. We
used the same 3 existing sentiment lexicons as in
(Mohammad et al., 2013). All lexicons give a sin-
gle score for each word (if present in the lexicon).
Those existing lexicons are: NRC Emotion Lexi-
con (about 14k words), the MPQA Lexicon (about
8k words), and the Bing Liu Lexicon (about 7k
words).
Lexicons from Automatically Labeled Data.
The NRC hashtag sentiment lexicon was gen-
erated automatically from a set of 775k tweets
containing a hashtag of a small predefined list
of positive and negative hashtags (Mohammad
et al., 2013). Lexicon scores were trained via
PMI (point-wise mutual information). Scores are
not only available for words, but also unigram–
unigram, unigram–bigram, and bigram–bigram
pairs (that can be non-contiguous in the text).
The Sentiment140 lexicon (Go et al., 2009) was
generated automatically from a set of 1.6 million
tweets containing a positive or negative emoticon.
This uses the same features and scoring as above.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9997192">
We have described an SVM classifier to detect the
sentiment of short texts such as tweets. Our sys-
tem is built up on the approach of NRC Canada
(Mohammad et al., 2013), with several modifica-
tions and extensions (e.g. sparse linear classifiers,
</bodyText>
<footnote confidence="0.883092">
5http://www.mBj.net/sentiment/
</footnote>
<bodyText confidence="0.999722272727273">
POS-n-grams, new lexicons). We have seen that
our system significantly improves the baseline ap-
proach, achieving a gain of 1.56 points in F1 score.
We participated in the SemEval-2014 competi-
tion for Twitter polarity classification, and our sys-
tem was among the top ten out of 50 submissions,
with an F1-score of 67.54 on tweets.
For future work, it would be interesting to in-
corporate our improvements into the most recent
version of NRC Canada or similar systems, to see
how much one could gain there.
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999777297297297">
Crammer, K. and Singer, Y. (2001). On the Algo-
rithmic Implementation of Multiclass Kernel-
based Vector Machines. JMLR, 2:265–292.
Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-
R., and Lin, C.-J. (2008). LIBLINEAR: A Li-
brary for Large Linear Classification. JMLR,
9:1871–1874.
Go, A., Bhayani, R., and Huang, L. (2009). Twit-
ter Sentiment Classification using Distant Su-
pervision. Technical report, The Stanford Natu-
ral Language Processing Group.
Joshi, M. and Penstein-Ros´e, C. (2009). General-
izing dependency features for opinion mining.
In Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, p 313–316, Singapore.
Association for Computational Linguistics.
Mohammad, S. M., Kiritchenko, S., and Zhu, X.
(2013). NRC-Canada: Building the State-of-
the-Art in Sentiment Analysis of Tweets. In
SemEval-2013 - Proceedings of the Interna-
tional Workshop on Semantic Evaluation, pages
321–327, Atlanta, Georgia, USA.
Owoputi, O., O’Connor, B., Dyer, C., Gimpel,
K., Schneider, N., and Smith, N. A. (2013).
Improved Part-Of-Speech Tagging for Online
Conversational Text with Word Clusters. In
Proceedings of NAACL-HLT, pages 380–390.
Pang, B., Lee, L., and Vaithyanathan, S. (2002).
Thumbs up? Sentiment Classification using
Machine Learning Techniques. In ACL-02 con-
ference, pages 79–86, Morristown, NJ, USA.
Association for Computational Linguistics.
Rosenthal, S., Ritter, A., Nakov, P., and Stoyanov,
V. (2014). SemEval-2014 Task 9: Sentiment
Analysis in Twitter. In SemEval 2014 - Pro-
ceedings of the Eighth International Workshop
on Semantic Evaluation, Dublin, Ireland.
</reference>
<page confidence="0.998718">
604
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.874119">
<title confidence="0.9982645">Swiss-Chocolate: Sentiment Sparse SVMs and Part-Of-Speech</title>
<author confidence="0.997858">Martin Jaggi Fatih Uzdilli Cieliebak</author>
<affiliation confidence="0.985891">ETH Zurich Zurich University of Applied Sciences</affiliation>
<address confidence="0.988653">Z¨urich, Switzerland Winterthur, Switzerland</address>
<email confidence="0.984911">uzdi@zhaw.ch</email>
<email confidence="0.984911">ciel@zhaw.ch</email>
<abstract confidence="0.9942198">We describe a classifier to predict the message-level sentiment of English microblog messages from Twitter. This paper describes the classifier submitted to the SemEval-2014 competition (Task 9B). Our approach was to build up on the system of the last year’s winning approach NRC Canada 2013(Mohammad et al., 2013),with some modifications and additions of features, and additional sentiment lexicons. Furthermore, we used a sparse SVM, instead of the more used resulting in a very sparse linear classifier.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>On the Algorithmic Implementation of Multiclass Kernelbased Vector Machines.</title>
<date>2001</date>
<pages>2--265</pages>
<publisher>JMLR,</publisher>
<contexts>
<context position="8550" citStr="Crammer and Singer (2001)" startWordPosition="1415" endWordPosition="1418">s given in this years train and dev set. using the list of negation words from Christopher Potts’ sentiment tutorial1. 4 Classifier We used a linear support vector machine (SVM) classifier, which is standard for text data. The LibLinear package (Fan et al., 2008) was employed for training the multi-class classifier. Multi-Class Formulation, and Class Weights. We found significant performance changes depending on which type of multi-class SVM, and also which regularizer (f1- or `2-norm) is used. For the multi-class variant, we found the oneagainst-all models to perform slightly better than the Crammer and Singer (2001) formulation. More importantly, since the 3 classes (positive, negative and neutral) are quite unbalanced in size in the training set, it is crucial to set a good weight for each class in the SVM. We used (4.52, 1.38, 1.80), which corresponds to the twice the ratio of each class compared to the average class size. Sparse Linear Classifiers. In our setting, an `1-regularized squared loss SVM (one-against-all) performed best (this is mode L1R L2LOSS SVC in LibLinear), despite the fact that `2-regularization is generally more commonly used in text applications. We used C = 0.055 for the regulariz</context>
</contexts>
<marker>Crammer, Singer, 2001</marker>
<rawString>Crammer, K. and Singer, Y. (2001). On the Algorithmic Implementation of Multiclass Kernelbased Vector Machines. JMLR, 2:265–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<pages>9--1871</pages>
<publisher>JMLR,</publisher>
<contexts>
<context position="8188" citStr="Fan et al., 2008" startWordPosition="1361" endWordPosition="1364">pt for those features in (Mohammad et al., 2013) which use case information). As usual, URLs were normalized to http://someurl and twitter user IDs to @someuser. We also employed the usual marking of negated contexts of a sentence as in (Pang et al., 2002), 602 Our final classifier was trained on 9641 tweets, which are all we could download from the IDs given in this years train and dev set. using the list of negation words from Christopher Potts’ sentiment tutorial1. 4 Classifier We used a linear support vector machine (SVM) classifier, which is standard for text data. The LibLinear package (Fan et al., 2008) was employed for training the multi-class classifier. Multi-Class Formulation, and Class Weights. We found significant performance changes depending on which type of multi-class SVM, and also which regularizer (f1- or `2-norm) is used. For the multi-class variant, we found the oneagainst-all models to perform slightly better than the Crammer and Singer (2001) formulation. More importantly, since the 3 classes (positive, negative and neutral) are quite unbalanced in size in the training set, it is crucial to set a good weight for each class in the SVM. We used (4.52, 1.38, 1.80), which corresp</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. (2008). LIBLINEAR: A Library for Large Linear Classification. JMLR, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Go</author>
<author>R Bhayani</author>
<author>L Huang</author>
</authors>
<title>Twitter Sentiment Classification using Distant Supervision. Technical report, The Stanford Natural Language Processing Group.</title>
<date>2009</date>
<contexts>
<context position="11021" citStr="Go et al., 2009" startWordPosition="1814" endWordPosition="1817">the same set of lexicons as in (Mohammad et al., 2013), with one addition: 5.1 A Lexicon for 3-Class Classification Our main new addition was another type of lexicon, which not only provides one score per word, but 3 of them, (being the association to positive, negative and neutral). The idea here is to improve on the discrimination quality, especially for neutral text, and treat all 3 labels in this multi-class task the same way, instead of just 2 as in the previous approaches. Data. We found it challenging to find good datasets to build such a lexicon. We again used the Sentiment140 corpus (Go et al., 2009) (containing tweets with positive or negative emoticons). Using a subset of 100k positive and 100k negative ones, we added a set of 100k arbitrary (hopefully neutral) tweets. The neutral set was chosen randomly from the thinknook.com dataset3 of 1.5mio tweets (from which we ignored the provided labels, and counted the tweets as neutral). We did the same with the movie reviews from the recent kaggle competition on annotated reviews from the rotten-tomatoes website4. We automatically built a lexicon from 100k texts in this dataset, with the data balanced equally for the three classes. Features U</context>
<context position="13678" citStr="Go et al., 2009" startWordPosition="2232" endWordPosition="2235"> NRC Emotion Lexicon (about 14k words), the MPQA Lexicon (about 8k words), and the Bing Liu Lexicon (about 7k words). Lexicons from Automatically Labeled Data. The NRC hashtag sentiment lexicon was generated automatically from a set of 775k tweets containing a hashtag of a small predefined list of positive and negative hashtags (Mohammad et al., 2013). Lexicon scores were trained via PMI (point-wise mutual information). Scores are not only available for words, but also unigram– unigram, unigram–bigram, and bigram–bigram pairs (that can be non-contiguous in the text). The Sentiment140 lexicon (Go et al., 2009) was generated automatically from a set of 1.6 million tweets containing a positive or negative emoticon. This uses the same features and scoring as above. 6 Conclusion We have described an SVM classifier to detect the sentiment of short texts such as tweets. Our system is built up on the approach of NRC Canada (Mohammad et al., 2013), with several modifications and extensions (e.g. sparse linear classifiers, 5http://www.mBj.net/sentiment/ POS-n-grams, new lexicons). We have seen that our system significantly improves the baseline approach, achieving a gain of 1.56 points in F1 score. We parti</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Go, A., Bhayani, R., and Huang, L. (2009). Twitter Sentiment Classification using Distant Supervision. Technical report, The Stanford Natural Language Processing Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Joshi</author>
<author>C Penstein-Ros´e</author>
</authors>
<title>Generalizing dependency features for opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, p 313–316, Singapore. Association for Computational Linguistics.</booktitle>
<marker>Joshi, Penstein-Ros´e, 2009</marker>
<rawString>Joshi, M. and Penstein-Ros´e, C. (2009). Generalizing dependency features for opinion mining. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, p 313–316, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Mohammad</author>
<author>S Kiritchenko</author>
<author>X Zhu</author>
</authors>
<title>NRC-Canada: Building the State-ofthe-Art in Sentiment Analysis of Tweets. In</title>
<date>2013</date>
<booktitle>SemEval-2013 - Proceedings of the International Workshop on Semantic Evaluation,</booktitle>
<pages>321--327</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="3046" citStr="Mohammad et al., 2013" startWordPosition="477" endWordPosition="480">eets and SMS), new tweets and data from a surprise domain were provided. An overview of the data, which we were able to download, is shown in Table 1. Table 1: Overview of the data we found available for training, development and testing. Dataset Total Positive Negative Neutral Train (Tweets) 8224 3058 1210 3956 Dev (Tweets) 1417 494 286 637 Test: Twitter2014 1853 982 202 669 Test: Twitter2013 3813 1572 601 1640 Test: SMS2013 2093 492 394 1207 Test: Tw2014Sarcasm 86 33 40 13 Test: LiveJournal2014 1142 427 304 411 2 Description of Our Approach Compared to the previous NRC Canada 2013 approach (Mohammad et al., 2013), our main changes are the following three: First we use sparse linear classifiers instead of classical dense ones. Secondly, we drop n-gram features completely, in favor of what we call part-of-speech n-grams, which are n-grams where up to two tokens are the original ones, and the rest of the tokens is replaced by their corresponding POS tag (noun, verb, punctuation etc). Third, we added two new sentiment lexicons, containing numerical scores associated for all 3 classes (positive, neutral, negative), instead of just 2 as in classical po601 Proceedings of the 8th International Workshop on Sem</context>
<context position="4272" citStr="Mohammad et al. (2013)" startWordPosition="681" endWordPosition="684">c Evaluation (SemEval 2014), pages 601–604, Dublin, Ireland, August 23-24, 2014. larity lexicons. All changes are described in more detail in Sections 4 and 3 below. Performance. We tried to reproduce the same classifier as in (Mohammad et al., 2013) as a baseline for comparison. Trying to quantify our contributions, when adding all our additional features and tricks described below, the score of our method increases from the baseline of 63.25 to 64.81 (on the Twitter2013 test set), which is a gain of 1.56 points in F1. Baseline Approach by NRC Canada 2013. Unfortunately our replica system of Mohammad et al. (2013) only achieved an F1-score of 63.25 on the Twitter-2013 test set, while their score in the 2013 competition on the same test set was 69.02, nearly 6 points higher in F1. Part of this big difference might be explained by the fact that the exact same training sets are not available anymore. Other possibly more important differences are the SVM classifier variant used and class weighting (described in Section 4). Furthermore, we didn’t implement all features in the exactly same way, see the more detailed description in Section 3.1.2 below. Although we had the impression that these changes individ</context>
<context position="6810" citStr="Mohammad et al., 2013" startWordPosition="1126" endWordPosition="1129">d. The idea of replacing some of the tokens by their POS tag is also investigated by Joshi and Penstein-Ros´e (2009), where the authors used n &lt; 3. 3.1.2 Various Changes Compared to NRC Canada 2013 • We do not allow n-grams (or POS n-grams) to span over sentence boundaries. • Substrings of length up to 6 (instead of 5). • Substring features are weighted increasingly by their length (weights 0.7 · {1.0,1.1,1.2,1.4,1.6,1.9} for lengths 3,4,... ) • Instead of the score itself, we used the sigmoid value s(t) = 1/(1 + e−t)) of each lexicon score. For each lexicon, the 4 scores were the same as in (Mohammad et al., 2013), i.e. per tweet, we use the number of tokens appearing in the lexicon, the sum and the max of the scores, and the last non-zero score. We skipped some features from the baseline approach (because their effect was not significant in our setting): Elongated words (number of words with one character repeated more than two times), and word clustering. Also, we had a slightly simplified variant of how to use the lexicon scores. We didn’t count the lexicon scores separately per emotion (pos and neg), but only altogether. 3.2 Existing Features Text Preprocessing. A good tokenization seems very impor</context>
<context position="10459" citStr="Mohammad et al., 2013" startWordPosition="1714" endWordPosition="1717">ailable for download as a text file2 . Every line contains the feature description followed by the 3 weights corresponding to the 3 sentiment classes. 1 http://sentiment.christopherpotts. net/lingstruc.html 2http://www.mBj.net/sentiment/ 5 Lexicons A sentiment lexicon is a mapping from words (or n-grams) to an association score corresponding to positive or negative sentiment. Such lists can be constructed either from manually labeled data (supervised), or automatically labeled data (unsupervised) as for example tweets with a positive or negative smiley. We used the same set of lexicons as in (Mohammad et al., 2013), with one addition: 5.1 A Lexicon for 3-Class Classification Our main new addition was another type of lexicon, which not only provides one score per word, but 3 of them, (being the association to positive, negative and neutral). The idea here is to improve on the discrimination quality, especially for neutral text, and treat all 3 labels in this multi-class task the same way, instead of just 2 as in the previous approaches. Data. We found it challenging to find good datasets to build such a lexicon. We again used the Sentiment140 corpus (Go et al., 2009) (containing tweets with positive or n</context>
<context position="11790" citStr="Mohammad et al. (2013)" startWordPosition="1941" endWordPosition="1944">(hopefully neutral) tweets. The neutral set was chosen randomly from the thinknook.com dataset3 of 1.5mio tweets (from which we ignored the provided labels, and counted the tweets as neutral). We did the same with the movie reviews from the recent kaggle competition on annotated reviews from the rotten-tomatoes website4. We automatically built a lexicon from 100k texts in this dataset, with the data balanced equally for the three classes. Features Used in the Lexicon. To construct the lexicon, we extracted the POS n-grams (as we described in Section 3.1.1 above) from all texts. In comparison, Mohammad et al. (2013) used noncontiguous n-grams (unigram–unigram, unigram– bigram, and bigram–bigram pairs). We only used POS n-grams with 2 tokens kept original, and the 3 http://thinknook.com/twittersentiment-analysis-training-corpusdataset-2012-09-22/ 4 http://www.kaggle.com/c/ sentiment-analysis-on-movie-reviews/data 603 remaining ones replaced by their POS tag, with n ranging from 3 to 6. Building the Lexicon. While in (Mohammad et al., 2013), the score for each n-gram was computed using point-wise mutual information (PMI) with the labels, we trained a linear classifier on the same labels instead. The lexico</context>
<context position="13415" citStr="Mohammad et al., 2013" startWordPosition="2194" endWordPosition="2197">or download as text files5. 5.2 Existing Lexicons Lexicons from Manually Labeled Data. We used the same 3 existing sentiment lexicons as in (Mohammad et al., 2013). All lexicons give a single score for each word (if present in the lexicon). Those existing lexicons are: NRC Emotion Lexicon (about 14k words), the MPQA Lexicon (about 8k words), and the Bing Liu Lexicon (about 7k words). Lexicons from Automatically Labeled Data. The NRC hashtag sentiment lexicon was generated automatically from a set of 775k tweets containing a hashtag of a small predefined list of positive and negative hashtags (Mohammad et al., 2013). Lexicon scores were trained via PMI (point-wise mutual information). Scores are not only available for words, but also unigram– unigram, unigram–bigram, and bigram–bigram pairs (that can be non-contiguous in the text). The Sentiment140 lexicon (Go et al., 2009) was generated automatically from a set of 1.6 million tweets containing a positive or negative emoticon. This uses the same features and scoring as above. 6 Conclusion We have described an SVM classifier to detect the sentiment of short texts such as tweets. Our system is built up on the approach of NRC Canada (Mohammad et al., 2013),</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Mohammad, S. M., Kiritchenko, S., and Zhu, X. (2013). NRC-Canada: Building the State-ofthe-Art in Sentiment Analysis of Tweets. In SemEval-2013 - Proceedings of the International Workshop on Semantic Evaluation, pages 321–327, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Owoputi</author>
<author>B O’Connor</author>
<author>C Dyer</author>
<author>K Gimpel</author>
<author>N Schneider</author>
<author>N A Smith</author>
</authors>
<title>Improved Part-Of-Speech Tagging for Online Conversational Text with Word Clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Owoputi, O., O’Connor, B., Dyer, C., Gimpel, K., Schneider, N., and Smith, N. A. (2013). Improved Part-Of-Speech Tagging for Online Conversational Text with Word Clusters. In Proceedings of NAACL-HLT, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In ACL-02 conference,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7827" citStr="Pang et al., 2002" startWordPosition="1299" endWordPosition="1302">w to use the lexicon scores. We didn’t count the lexicon scores separately per emotion (pos and neg), but only altogether. 3.2 Existing Features Text Preprocessing. A good tokenization seems very important for twitter data. We used the popular tokenizer ArkTweetNLP (Owoputi et al., 2013) which is suitable for tweets. All text was transformed to lowercase (except for those features in (Mohammad et al., 2013) which use case information). As usual, URLs were normalized to http://someurl and twitter user IDs to @someuser. We also employed the usual marking of negated contexts of a sentence as in (Pang et al., 2002), 602 Our final classifier was trained on 9641 tweets, which are all we could download from the IDs given in this years train and dev set. using the list of negation words from Christopher Potts’ sentiment tutorial1. 4 Classifier We used a linear support vector machine (SVM) classifier, which is standard for text data. The LibLinear package (Fan et al., 2008) was employed for training the multi-class classifier. Multi-Class Formulation, and Class Weights. We found significant performance changes depending on which type of multi-class SVM, and also which regularizer (f1- or `2-norm) is used. Fo</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang, B., Lee, L., and Vaithyanathan, S. (2002). Thumbs up? Sentiment Classification using Machine Learning Techniques. In ACL-02 conference, pages 79–86, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Rosenthal</author>
<author>A Ritter</author>
<author>P Nakov</author>
<author>V Stoyanov</author>
</authors>
<title>SemEval-2014 Task 9: Sentiment Analysis in Twitter.</title>
<date>2014</date>
<booktitle>In SemEval 2014 - Proceedings of the Eighth International Workshop on Semantic Evaluation,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1297" citStr="Rosenthal et al., 2014" startWordPosition="188" endWordPosition="191">ons. Furthermore, we used a sparse (`1-regularized) SVM, instead of the more commonly used `2-regularization, resulting in a very sparse linear classifier. 1 Introduction With the immense growth of user generated text online, the interest in automatic sentiment analysis of text has greatly increased recently in both academia and industry. In this paper, we describe our approach for a modified SVM based classifier for short text as in Twitter messages. Our system has participated in the SemEval-2014 Task 9 competition, “Sentiment Analysis in Twitter, Subtask–B Message Polarity Classification” (Rosenthal et al., 2014). The goal is to classify a tweet (on the full message level) into the three classes positive, negative, and neutral. An almost identical competition was already run in 2013. Our Results in the Competition. Our approach was ranked on the 8th place out of the 50 participating submissions, with an F1-score of 67.54 on the Twitter-2014 test set. The 2014 winning team obtained an average F1-score of 70.96. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.or</context>
</contexts>
<marker>Rosenthal, Ritter, Nakov, Stoyanov, 2014</marker>
<rawString>Rosenthal, S., Ritter, A., Nakov, P., and Stoyanov, V. (2014). SemEval-2014 Task 9: Sentiment Analysis in Twitter. In SemEval 2014 - Proceedings of the Eighth International Workshop on Semantic Evaluation, Dublin, Ireland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>