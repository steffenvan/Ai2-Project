<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.983709">
Logical Structures in the Lexicon
</title>
<note confidence="0.7602265">
John F. Sowa
IBM Systems Research Institute
500 Columbus Avenue
Thornwood, NY 10594
</note>
<email confidence="0.666169">
email: sowa@watson.ibm.com
</email>
<sectionHeader confidence="0.760217" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999891307692308">
The lexical entry for a word must contain all the information needed to construct a se-
mantic representation for sentences that contain the word. Because of that requirement,
the formats for lexical representations must be as detailed as the semantic forms. Simple
representations, such as features and frames, are adequate for resolving many syntactic
ambiguities. But since those notations cannot represent all of logic, they are incapable
of supporting all the function needed for semantics. Richer semantic-based approaches
have been developed in both the model-theoretic tradition and the more computational
AI tradition. Although superficially in conflict, these two traditions have a great deal in
common at a deeper level. Both of them have developed semantic structures that are ca-
pable of representing a wide range of linguistic phenomena. This paper compares these
approaches and evaluates their adequacy for various kinds of semantic information that
must be stored in the lexicon. It presents conceptual graphs as a synthesis of the logicist
and Al representations designed to support the requirements of both.
</bodyText>
<sectionHeader confidence="0.657839" genericHeader="method">
1 Semantics from the Point of View of the Lexicon
</sectionHeader>
<bodyText confidence="0.999794111111111">
To understand a semantic theory, start by looking at what goes into the lexicon. In one
of the early semantic theories in the Chomskyan tradition, Katz and Fodor (1963) did in
fact start with the lexicon. More recent theories, however, almost treat the lexicon as an
afterthought. Yet the essence of the theory is still in the lexicon: every element of the
semantic representation of a sentence ultimately derives from something in the lexicon.
That principle is just as true for Richard Montague&apos;s highly formalized grammar as for
Roger Schank&apos;s &amp;quot;scruffy&amp;quot; conceptual dependencies, scripts, and MOPs.
Context and background knowledge are also important, since most sentences cannot
be understood in isolation. This fact contradicts Frege&apos;s principle of compositionality,
which says that the meaning of a sentence is derived from the meanings of the words it
contains. Yet context can also be stated in words and sentences. Even when nonlinguistic
surroundings are necessary for understanding a sentence, every significant feature could
be encoded in a sentence; people normally encode such information when they tell a story.
More general encyclopedic knowledge is contextual information that was learned at an
earlier time; it too could be stated in sentences. An extended Fregean principle should
therefore say that the meaning of a sentence must be derivable from the meanings of the
words in the sentence together with the meanings of the words in the sentences that de-
scribe the relevant context and background knowledge.
</bodyText>
<page confidence="0.998088">
38
</page>
<bodyText confidence="0.999904483870968">
Besides the meanings of words, grammar and logic are necessary to combine those
meanings into a complete semantic representation. But there are competing theories
about how much grammar and logic is necessary, how much is expressed in the lexicon,
and how much is expressed in the linguistic system outside the lexicon. Lexically based
theories suggest that the grammar rules should be simple and that most of the syntactic
complexity should be encoded in the lexicon. One might even go further and say that
most of the syntactic complexity isn&apos;t syntactic at all. It is the result of interactions among
the logical structures of the underlying concepts. In his work on semantically based syn-
tax, Dixon (1991) maintained that syntactic irregularities and idiosyncrasies are not acci-
dental. Instead, he showed that many of them can be predicted from the semantics of the
words. Such theories imply that a system would only need a simple grammar to represent
a language if it had sufficiently rich semantic structures.
A rich theory of semantics in the lexicon must also explain how the semantics got into
the lexicon. A child could learn an initial stock of meanings by associating prelinguistic
structures with words. But even those prelinguistic structures are shaped, polished, and
refined by long usage in the context of sentences. They are combined with the structures
learned from other words, and they are molded into patterns that are traditional in the
language and culture. More complex, abstract, and sophisticated concepts are either
learned exclusively through language or through experiences that are highly colored and
shaped by language. For these reasons, the meaning representations in the lexicon should
be derivable from the semantic representations for sentences. As a working hypothesis,
the two should be identical: the same knowledge representation language should be used
for representing meanings in the lexicon and for representing the semantics of sentences
and extended discourse structures.
This paper explores the implications of that hypothesis. Section 2 reviews several
different lexical representations and their implications. Section 3 compares the underlying
assumptions of the model-theoretic and Al traditions and shows that they are computa-
tionally more compatible than their metaphysics would suggest. Section 4 illustrates the
use of conceptual graphs for representing the semantic content of lexical entries. Section
5 shows how such representations can be used to handle aspects of language that require
both logic and background knowledge.
</bodyText>
<sectionHeader confidence="0.937347" genericHeader="method">
2 Review of Lexical Representations
</sectionHeader>
<bodyText confidence="0.9996295">
Features are one of the oldest and simplest semantic representations. In his Universal
Characteristic, Leibniz (1679) assigned prime numbers to semantic primitives and repres-
ented compound concepts by products of the primes. If RATIONAL were represented
by 2 and ANIMAL by 3, then their product 6 would represent RATIONAL ANIMAL
or HUMAN. That representation generates a lattice: concept A is a supertype of B if the
number for A divides the number for B; the minimal common supertype of A and B
corresponds to their greatest common divisor; and the maximal common subtype of A
and B corresponds to their least common multiple. Leibniz tried to use his system to
mechanize Aristotle&apos;s syllogisms, but a feature-based representation is too limited. It
cannot distinguish different quantifiers or show how primitive concepts are related to one
another in compounds. Modem notations use bit strings instead of products of primes,
but their logical power is just as limited as Leibniz&apos;s system of 1679.
</bodyText>
<page confidence="0.998773">
39
</page>
<bodyText confidence="0.967121636363636">
In their feature-based system, Katz and Fodor (1963) factored the meaning of a word
into a string of features and an undigested lump called a distinguisher. Following is their
representation for one sense of bachelor:
bachelor 4. noun 4- (Animal) 4- (Male) 4- (Young)
4. [fur seal when without a mate during the breeding time].
In this definition, noun is the syntactic marker; the features (Animal), (Male), and
(Young) are the semantic markers that contain the theoretically significant information;
and the phrase in brackets is the unanalyzed distinguisher. Shortly after it appeared, the
Katz-Fodor theory was subjected to devastating criticisms. Although no one today uses
the theory in its original form, those criticisms are worth mentioning because many of the
more modern approaches suffer from the same limitations:
</bodyText>
<listItem confidence="0.954701851851852">
• The sharp distinction between semantic features and the distinguisher is so funda-
mental to the theory that it should have an enormous impact on the structures of
language and the normal use of language. Yet there is no linguistic evidence from
syntax or cooccurrence patterns to indicate that it has any effect whatever.
• The distinguisher is made up of words, each of which has its own meaning. A com-
plete semantic theory should explain how the meanings of the words in the
distinguisher contribute to the meaning of the whole. But such an analysis would
imply a deeper representation that underlies both the features and the distinguisher.
• The Katz-Fodor theory treats different word senses as if they were purely accidental
groupings that have no relationship to one another. Yet the four senses of bachelor
do have something in common. They all represent an immature or transitional stage
that leads to some further goal: a student who has completed an academic step on
the way to becoming a master or doctor; a young knight who is still an apprentice to
another; a seal on its way to full maturity as a patriarch of the herd; or an unmarried
man who has not yet started to form his own family. The feature-distinguisher theory
does not show the commonality; it cannot explain how these meanings developed
from a common root or why they remain associated with the same word form.
• If the features had no deeper structure, there would be nothing to constrain their
possible combinations. Yet certain combinations, such as (Abstract)+ (Color) or
(Action)+ (Weight), never occur. More structure is needed in the theory to explain
why such combinations are impossible.
• Finally, many features cannot be named with a single word. Certain Mexican dialects,
for example, make a distinction between a difunto, a deceased person who was married
at the time of death, and an angelito, a deceased person who was not married at the
time of death (El Guindi 1986). A feature such as (Married-at-the-time-of-death) is
so blatantly nonprimitive that it cries out for a theory that represents deeper struc-
tures.
</listItem>
<bodyText confidence="0.992164833333333">
These criticisms do not imply that features are useless. But they indicate that features are
derived from some deeper, more fundamental representation.
Despite their limitations, features are attractive because they are easy to program, and
they generate convenient computational structures, such as Leibniz&apos;s lattice. Yet a lattice
generated by features permits too many impossible combinations. A well-structured
definitional mechanism is necessary to generate lattices without those undesirable nodes.
</bodyText>
<page confidence="0.996874">
40
</page>
<bodyText confidence="0.987886888888889">
The prototype for all definitions is Aristotle&apos;s method of genus plus differentiae: a new
term is defined by specifying its genus or supertype plus the differentiae that distinguish
it from other subtypes of the same genus. The result of such definitions is a tree. The tree
becomes a lattice when the differentiae are specified in a system of logic that can determine
when one definition is implied by another. Such a lattice, variously known as a type,
taxonomic, or subsumption hierarchy, forms the basis for many systems of frames and
semantic networks. These lattices have several advantages over feature lattices: only the
desired nodes are ever defmed; most, if not all, of the impossible combinations can be
proved to be contradictory in the underlying logic; and the logic can specify relationships
that a simple concatenation of features can never represent.
A serious computational problem arises with definitional systems that are based on
logic. If the logic is rich enough to express anything in first-order predicate calculus, the
proof procedures can become intractable. To simplify the computations, many frame
systems adopt a restricted logic, usually without negations or disjunctions. Yet those re-
stricted logics cannot express all the definitions used in language. The definition of pen-
niless, for example, requires a negation to express &amp;quot;not having a penny.&amp;quot; The word
bachelor in Katz and Fodor&apos;s example requires temporal logic to express &amp;quot;without a mate
during the breeding time.&amp;quot; In general, every logical quantifier, Boolean operator, and
modal operator occurs in dictionary definitions. Although tractability is an important
feature in a computational system, a logic that is suitable for natural language semantics
must be able to represent anything that people might say. A problem solver or reasoner
might have to simplify the statements in order to improve efficiency, but those simplifi-
cations are likely to be highly domain-dependent. A general language handler must rep-
resent every statement in as rich a form as it was originally expressed.
In several articles written shortly before his death, Richard Montague (1974) began
the tradition of applying model-theoretic techniques to natural language semantics. He
started with Carnap&apos;s intuition (1947) that the intension of a sentence is a function from
possible worlds to truth values. He combined that notion with Frege&apos;s principle of
compositionality to develop a systematic way of deriving the function that represents the
meaning of a sentence. The intension of each word in each lexical category would corre-
spond to a functional form of some sort. The intension of the noun unicorn, for example,
would be a function that applies to entities in the world and generates the value true for
each unicorn and false for each nonunicorn. Other lexical categories would be represented
by 1-expressions that would combine with the functional forms that happened to occur
to their right or left. As an example, Montague&apos;s lexical entry for the word be is a function
that checks whether the predicate is true of the subject. The idea is straightforward, but
his implementation uses a rather heavy notation with many subtle distinctions:
Ax (?(y) where ext(x) = ext(y)).
This A-expression defines the intension of be as a function with two arguments: .9 is the
intension of the phrase that follows the word be, and x is the intension of the subject of
be. The body of the expression applies the function to some y whose extension is equal
to the extension of x. Montague&apos;s lexicon consists of such constructions that apply
functions of functions to generate other functions of functions of functions.&apos;
I To improve readability, this example modifies Montague&apos;s notation by adding the keyword &amp;quot;where&amp;quot; and
using the function ext(x) for extension instead of Montague&apos;s cryptic marks.
</bodyText>
<page confidence="0.99899">
41
</page>
<bodyText confidence="0.999831933333333">
Besides constructing functions, Montague used them to solve certain logical puzzles.
One of them is Barbara Partee&apos;s example: The temperature is ninety, and it is rising.
Therefore, ninety is rising. To avoid the conclusion that a constant like ninety could
change, Montague drew some subtle distinctions. He treated temperature as an &amp;quot;extraor-
dinary noun&amp;quot; that denoted &amp;quot;an individual concept, not an individual&amp;quot;. He also gave spe-
cial treatment to rise, which &amp;quot;unlike most verbs, depends for its applicability on the full
behavior of individual concepts, not just on their extensions.&amp;quot; As a result, he claimed that
The temperature is ninety asserted the equality of extensions, but that The temperature is
rising applied the verb rise to the intension. Consequently, the conclusion that ninety is
rising would be blocked, since rise would not be applied to the extension. To linguists,
Montague&apos;s distinction between words whose semantics depend on intensions and those
whose semantics depend on extensions seemed like an ad hoc contrivance; he never gave
any linguistic evidence to support it. To psychologists, the complex manipulations re-
quired for processing the A-expressions seemed unlikely to have any psychological reality.
And to programmers, the infinities of possible worlds seemed computationally intractable.
Yet for all its unnaturalness, Montague&apos;s system was an impressive achievement: it
showed that formal methods of logic could be applied to natural languages, that they
could define the semantics of a significant subset of English, and that they could represent
logical aspects of natural language with the depth and precision usually attained only in
artificial systems of logic.
At the opposite extreme from Montague&apos;s logical rigor are Roger Schank&apos;s informal
diagrams and quasi-psychological theories that were never tested in controlled psycho-
logical experiments. Yet they led his students to build impressive demos that exhibited
interesting language behavior. As an example, the Integrated Partial Parser (Schank,
Lebowitz, &amp; Birnbaum 1980) represents a fairly mature stage of Schank&apos;s theories. IPP
would analyze newspaper stories about international terrorism, search for words that
represent concepts in that domain, and apply scripts that relate those concepts to one
another. In one example, IPP processed the sentence, About 20 persons occupied the office
of Amnesty International seeking better jail conditions for three alleged terrorists. To in-
terpret that sentence, it used the following dictionary entry for the word occupied:
</bodyText>
<figure confidence="0.979488125">
(WORD-DEF OCCUPIED
INTEREST 5
TYPE EB
SUBCLASS SEB
TEMPLATE (SCRIPT $DEMONSTRATE
ACTOR NIL
OBJECT NIL
DEMANDS NIL
METHOD (SCENE $OCCUPY
ACTOR NIL
LOCATION NIL))
FILL (((ACTOR) (TOP-OF *ACTOR-STACK*))
((METHOD ACTOR) (TOP-OF *ACTOR-STACK*)))
REQS (FIND-DEMON-OBJECT
FIND-OCCUPY-LOC
RECOGNIZE-DEMANDS))
</figure>
<bodyText confidence="0.684945">
This entry says that occupied has interest level 5 (on a scale from 0 to 10) and it is an event
builder (EB) of subclass scene event builder (SEB). The template is a script of type
</bodyText>
<page confidence="0.995711">
42
</page>
<bodyText confidence="0.999913833333333">
$DEMONSTRATE with an unknown actor, object, and demands. As its method, the
demonstration has a scene of type $OCCUPY with an unknown actor and location. At
the end of the entry are fill and request slots that give procedural hints for finding the ac-
tor, object, location, and demands. In analyzing the sample sentence, IPP identified the
20 persons as the actors, the office as the location, and the better jail conditions as the
demand.
The fill and request slots implement the Schanlcian &amp;quot;expectations.&amp;quot; A fill slot is filled
with something previously found in the sentence, and a request slot waits for something
still to come. They serve the same purpose as Montague&apos;s left and right cancellation rules
for categorial grammar. The act of filling the slots corresponds to the A rules for ex-
panding a function that is applied to a list of arguments. Their differences in style are
more significant than their differences in computational mechanisms:
</bodyText>
<listItem confidence="0.886375375">
• Schank&apos;s antiformalist stance is irrelevant, since anything that can be programmed on
a digital computer could be formalized. One Prolog programmer, in fact, showed that
most of the slot filling in Schank&apos;s parsers and script handlers could be done directly
by Prolog&apos;s unification algorithm. Techniques such as unification and graph gram-
mars could be used to formalize Schank&apos;s systems while making major improvements
in clarity, robustness, and generality.
• Montague&apos;s appearance of rigor results from his use of Greek letters and logical sym-
bols. Yet some constructions, such as his solution to Partee&apos;s puzzle, are contrivances
that programmers would call &amp;quot;hacks&amp;quot; (if they ever took the trouble to work their way
through his notation).
• Schank and Montague had different attitudes about what aspects of language were
most important. Schank believed that the ability to represent and use world know-
ledge is the essence of language understanding, and Montague believed that the ability
to handle scope of quantifiers and modalities was the most significant. They were
both right in believing that their favorite aspects were important, but they were both
wrong in ignoring the others.
</listItem>
<bodyText confidence="0.998475">
Schank and Montague represented different aspects of language with different methodol-
ogies, but they are complementary rather than conflicting. Yorick Wilks (1991) observed
that Montague&apos;s lexical entries are most complex for words like the, for which Schank&apos;s
entries are trivial. Conversely, Schank&apos;s entries are richest for content words, which
Montague simply put in one of his categories, while ignoring their connotations. Both
logic and background knowledge are important, and the lexicon must include both kinds
of information.
</bodyText>
<sectionHeader confidence="0.705956" genericHeader="method">
3 Metaphysical Baggage and Observable Results
</sectionHeader>
<bodyText confidence="0.99984675">
Linguistic theories are usually packaged in metaphysical terms that go far beyond the
available evidence. Chomsky&apos;s metaphysics may be summarized in a single sentence from
Syntactic Structures: &amp;quot;Grammar is best formulated as a self-contained study independent
of semantics.&amp;quot; For Montague, the title and opening sentence of &amp;quot;English as a Formal
Language&amp;quot; express his point of view: &amp;quot;I reject the contention that an important theore-
tical difference exists between formal and natural languages.&amp;quot; Schank&apos;s outlook is sum-
marized in the following sentence from Conceptual Information Processing: &amp;quot;Conceptual
Dependency Theory was always intended to be a theory of how humans process natural
</bodyText>
<page confidence="0.998713">
43
</page>
<bodyText confidence="0.999015380952381">
language that was explicit enough to allow for programming it on a computer.&amp;quot; These
characteristic sentences provide a key to understanding their authors&apos; motivation. Yet
their actual achievements are easier to understand when the metaphysics is ignored. Look
at what they do, not at what they say.
In their basic attitudes and metaphysics, Schank and Montague are irreconcilable.
Montague is the epitome of the kind of logician that Schank has always denounced as
misguided or at least irrelevant. Montague stated every detail of his theory in a precise
formalism, while Schank made sweeping generalizations and left the detailed programming
to his students. For Montague, the meaning of a sentence is a function from possible
worlds to truth values; for Schank, it is a diagram that represents human conceptuali-
zations. On the surface, their only point of agreement is their implacable opposition to
Chomsky and &amp;quot;the developments emanating from the Massachusetts Institute of Tech-
nology&amp;quot; (Montague 1970). Yet in their reaction against Chomsky, both Montague and
Schank evolved positions that are remarkably similar, although their terminology hides the
resemblance. What Chomsky called a noun, Schank called a picture producer, and
Montague called a function from entities to truth values. But those terms are irrelevant
to anything that they ever did: Schank never produced a single picture or even stated a
plausible hypothesis about how one might be produced from his diagrams; Montague
never applied any of his functions to the real world, let alone the infinity of possible worlds
he so freely assumed. In neutral terms, what Montague and Schank did could be de-
scribed in a way that makes the logicist and AI points of view nearly indistinguishable:
</bodyText>
<listItem confidence="0.930227133333333">
1. Semantics, not syntax, is the key to understanding sentence structure. The traditional
grammatical categories are surface manifestations of the fundamental semantic cate-
gories.
2. Associated with each word is a characteristic semantic structure that determines how
it combines with other words in a sentence.
3. The grammar of a language can be reduced to relatively simple rules that show what
categories of words may occur on the right or the left of a given word (the cancellation
rules of categorial grammar or the SchanIcian expectations). The variety of sentence
patterns is not the result of a complex grammar, but of the complex interactions be-
tween a simple grammar and the underlying semantic structures.
4. The meaning of a sentence is derived by combining the semantic structures for each
of the words it contains. The combining operations are primarily semantic, although
they are guided by word order and inflections.
5. The truth of a sentence in a possible world is computed by evaluating its meaning
representation in terms of a model of that world. Although Schank never used logical
</listItem>
<bodyText confidence="0.985397">
terms like denotation, his question-answering systems embodied effective procedures
for computing denotations, while Montague&apos;s infinities were computationally
intractable.
Terms like picture producer or function from entities to truth values engender heated argu-
ments, but they have no effect on the application of the theory to language or its imple-
mentation in a computer program. Without the metaphysical baggage, both theories
incorporate a semantic-based approach that is widely accepted in Al and computational
linguistics.
</bodyText>
<page confidence="0.996027">
44
</page>
<bodyText confidence="0.999971137931034">
At the level of data structures and operations, there are significant differences between
Montague and Schank. Montague&apos;s representations were A-expressions, which have the
associated operations of functional application, A-expansion, and 2-contraction. His
metaphysics gave him a rigorous methodology for assigning each word to one of his cat-
egories of functions (even though he never actually applied those functions to the real
world or any possible world). And his concerns about logic led him to a careful treatment
of quantifiers, modalities, and their scope. Schank&apos;s representations are graphs on paper
and LISP structures of various kinds in his students&apos; programs. The permissible oper-
ations include any manipulations of those structures that could be performed in LISP.
Schank&apos;s lack of a precise formalism gave his students the freedom and flexibility to invent
novel solutions to problems that Montague&apos;s followers never attempted to address, such
as the use of world knowledge in language understanding. Yet that lack of formalism led
to ad hoc accretions in the programs that made them unmaintainable. Many of Schank&apos;s
students found it easier to start from scratch and write a new parser than to modify one
that was written by an earlier generation of students. Montague and Schank have com-
plementary strengths: rigor vs. flexibility; logical precision vs. open-ended access to
background knowledge; exhaustive analysis of a tiny fragment of English vs. a broad-brush
sketch of a wide range of language use.
Montague and Schank represent two extremes on the semantic-based spectrum, which
is broad enough to encompass most AI work on language. Since the extremes are more
complementary than conflicting, it is possible to formulate approaches that combine the
strengths of both: a precise formalism, the expressive power of intensional logic, and the
ability to use background knowledge in language understanding. To allow greater flexi-
bility, some of Montague&apos;s rigid constraints must be relaxed: his requirement of a strict
one-to-one mapping between syntactic rules and semantic rules; his use of A-expressions
as the meaning representation; and his inability to handle ellipses, metaphor, metonymy,
anaphora, and anything requiring background knowledge. With a well-designed
formalism, these constraints could be relaxed while still allowing formal definitions of the
permissible operations.
</bodyText>
<sectionHeader confidence="0.965398" genericHeader="method">
4 Lexical Representations in Conceptual Graphs
</sectionHeader>
<bodyText confidence="0.9993154">
Without a formal system of logic, the issues that a lexical theory must address can only
be discussed at an anecdotal level. A formal system can clarify the issues, make them
precise, and lead the discussion to a deeper level of detail. This paper uses the theory of
conceptual graphs: a system of logic with a graph notation designed for a direct mapping
to and from the semantic structures of natural language. They have a graph structure
based on the semantic networks of Al and C. S. Peirce&apos;s existential graphs, which form a
complete system of logic. They are as formal and expressive as Montague&apos;s intensional
logic, but they permit a broader range of operations on the formalism. The theory has
been presented in book form (Sowa 1984) and in a recent summary (Sowa 1991); see those
sources for more detail. An earlier paper on lexical issues with conceptual graphs (Sowa
1988) did not discuss logic explicitly. This section will give some examples to illustrate
the kind of logical structures that are needed in the lexicon.
A basic feature of conceptual graph theory is the use of canonical graphs to represent
the expected roles associated with each concept type. Canonical graphs are similar to the
case frames used in many systems, but they are richer in the kinds of structures and logical
</bodyText>
<page confidence="0.996677">
45
</page>
<bodyText confidence="0.999713166666667">
operators they support. As an example, Figure 1 shows a canonical graph that represents
the lexical pattern associated with the verb support, It shows that every instance of the
concept type SUPPORT has four expected participants: an animate agent, some entity
as patient, some entity as instrument, and a purpose, which is represented by a nested
context. That context, which might represent something at a different time and place from
the outer context, shows that the entity is in some state.
</bodyText>
<figureCaption confidence="0.993962">
Figure 1. Canonical graph for the lexical type SUPPORT
</figureCaption>
<bodyText confidence="0.9991133">
Whereas case frames merely show the thematic roles for a verb and the expected
concept types that can fill those roles, conceptual graphs can grow arbitrarily large: they
can show long-range dependencies far removed from the central concept; and they may
contain nested contexts that show situations at different times and in different modalities.
The dotted line in Figure 1 is a coreference link that crosses context boundaries; it shows
that the entity that is the patient of SUPPORT is coreferent with the thing in the nested
context.
Conceptual graphs are a complete system of logic with their own model-theoretic se-
mantics, but there is also a formula operator ck that maps conceptual graphs into predicate
calculus. Following is the result of applying ci) to Figure 1:
</bodyText>
<equation confidence="0.97826425">
(Vx) (3y) (3z) (3u) (3v) (support(x) D
(animate(y) A entity(z) A entity(u) A situation(v) A
agnt(x,y) A ptnt(x,z) A inst(x,u) A purp(x,v) A
description(v, (3w)(state(w) A stat(z,w))))).
</equation>
<bodyText confidence="0.955091">
This formula and the graph in Figure 1 express exactly the same information with identical
ontological presuppositions. The first three lines of the formula represent the standard
</bodyText>
<figure confidence="0.932455">
STATE
ANIMATE
ENTITY
</figure>
<page confidence="0.998788">
46
</page>
<bodyText confidence="0.998055190476191">
information that is typical of case frames. The fourth line, however, represents structures
typical of situation semantics. It says that the situation v is described by a formula, which
says that there exists a state w and that the entity z is in state w; i.e. the purpose of sup-
porting is to maintain an entity in some state. The predicate description(s,p) means that
a situation s is described by a proposition p. This is one of the predicates that result from
the context-creating boxes in conceptual graphs, which are necessary for representing a
variety of structures in language.
Besides being more readable, the graph contains structures that are not supported in
predicate calculus, such as the context box that encloses the purpose of the supporting.
That box itself represents a concept to which relations can be attached to express purpose,
causality, time sequence, and other intersentential connectives. The description predicate
in the formula requires a version of higher order logic with nested propositions and
quantification over the situations described by those propositions. Although the de-
scription predicate allows some contexts to be translated into predicate calculus by 4),
there are other features associated with contexts that cannot be translated. One such fea-
ture is the indexical referent #, which is used to represent unresolved anaphoric references,
tenses, and other context-dependent phenomena. The formula operator 45 is undefmed
for graphs containing # until those references have been resolved.
As another example, Figure 2 shows the canonical graphs for the concepts EASY and
EAGER, which are used for the adjectives easy and eager as well as the adverbs easily and
eagerly.
</bodyText>
<figureCaption confidence="0.988565">
Figure 2. Canonical graphs for EASY and EAGER
</figureCaption>
<bodyText confidence="0.9973167">
The first graph says that every instance of EASY is an attribute of some ENTITY, and
it is also the manner of some ACT. That ENTITY also happens to be the patient of the
same ACT. The graph for EAGER has the same shape as the graph for EASY, but
EAGER is an attribute of some ANIMATE being that is the agent of some ACT. These
graphs illustrate the point that many syntactic features result from deeper logical proper-
ties. In this case, the AGNT and PTNT relations have different preferences for expression
as subject or object. As a result, the canonical graphs permit sentences of the following
form:
John easily does the homework.
John eagerly does the homework.
</bodyText>
<page confidence="0.998492">
47
</page>
<bodyText confidence="0.951709636363636">
The homework is easy for John to do.
But they rule out the following sentences:
* The homework is eager for John to do.
* John is easy to do the homework.
Sowa and Way (1986) showed how such graphs could be used in a semantic interpreter.
The grammar would only require general rules for adjectives and adverbs; no features
would be required to distinguish special properties of easy and eager or their adverbial
forms. Instead, the correct options would be selected and the incorrect ones would be
blocked by the graph unification operations (the maximal joins of conceptual graphs).
Besides the display form for conceptual graphs (Figures 1 and 2), there is also a more
compact linear form. Following are the linear representations for the graphs in Figure 2:
</bodyText>
<equation confidence="0.991407333333333">
[EASY: V]-
(ATTR)+[ENTITY]4-(PINT)+[ACT: *x]
(MANR)+[*x].
[EAGER: V]-
(ATTR)4-[ANIMATE]÷(AGNT)÷[ACT: *x]
(MANR)÷[*x].
</equation>
<bodyText confidence="0.949473411764706">
In the linear notation, concepts are enclosed in square brackets, and conceptual relations
are enclosed in parentheses. The hyphens show that additional relations attached to a
node are continued on separate lines. The variable *x indicates that the concept [*x] re-
presents the same node as the concept [ACT: *x]. Converting from the box and circle
notation to the linear notation causes cycles be broken, and variables like *x are needed
to show cross-references. The point at which the cycle is broken is purely arbitrary; dif-
ferent linearizations represent exactly the same graph.
The examples cited in Section 2 — penniless, difunto, and angelito — can also be de-
fined in conceptual graphs. The definition of PENNILESS, for example, requires a ne-
gation:
PENNILESS = (Ax) [STATE: *x]-4-(STAT)-(-[PER5ON: *y]
-[ [*y]-*(POSS)-&gt;[PENNY]].
This definition says that PENNILESS is a state x of a person y, where it is false that y
has possession (POSS) of a penny. Systems that do not allow negations in definitions
may make the reasoning process faster, but they make it impossible to define many kinds
of concepts. The definition for the concept type DIFUNTO would require a relation
WHEN linking two situations:
</bodyText>
<equation confidence="0.7262355">
DIFUNTO = (Ax) [PERSON: *x]-)-(STAT)÷[DEAD]
[SITUATION: Vx]-)-(STAT)÷[MARRIED]]÷(WHEN)-+ISITUATION: [*x]4-(PINT)4-[DIE]].
</equation>
<bodyText confidence="0.981264">
By this definition, a difunto is a person x in state dead, where x was in a situation of being
married when a situation occurred in which x died. The definition of angelito is similar,
but with a negation inside the first situation. In reasoning by inheritance, both
DIFUNTO and ANGELITO could be treated as simple subtypes of DEAD-PERSON,
and the details inside the definition could be ignored. But to determine whether a partic-
</bodyText>
<page confidence="0.998144">
48
</page>
<bodyText confidence="0.99901725">
ular individual was a difunto or an angelito, the details could be recovered by expanding
the A-expression.
New types of conceptual relations can also be defined by A-abstraction. The relation
WHEN in the previous examples could be defined by the following graph:
</bodyText>
<equation confidence="0.90289">
WHEN = (Ax,y)
[SITUATION: *x]÷(PTIM)-0-[TIME]-4-(PTIM)•-[SITUATION: .
</equation>
<bodyText confidence="0.978079945945946">
In this defmition, WHEN has two formal parameters x and y; each of them refers to a
situation that occurs at the same point in time (PTIM). Any occurrence of the relation
WHEN in a graph could be replaced by the sequence of concepts and relations between
[sx] and [*y]. The graph in the definition of DIFUNTO could be expanded to the fol-
lowing:
[SITUATION: [*x]÷(STAT)+[MARRIED]]-)-(PTIM)-[TIME]-
(PTIM)÷[SITUATION: Pcx]-4-(PINT)&lt;-[DIE]].
This graph says that x is in the state married at some point in time, which is the same time
that x dies. Since the graph is too long to fit on one line, the hyphen shows that the re-
lation attached to [TIME] is continued on the next line.
In Section 2, one criticism of Katz and Fodor&apos;s theory was its inability to show the
commonality among different senses of the same word. Some linguists, such as Ruhl
(1989), even maintain a principle of monosemy: each word has a single very abstract
meaning, and the multiple senses that appear in dictionary definitions are the result of
applying the word in different domains. For Katz and Fodor&apos;s example of bachelor, there
is a such a unifying meaning: &amp;quot;an animate being x preparing for a situation in which x is
in a mature state.&amp;quot; That definition could also be expressed in a conceptual graph:
BACHELOR = (Ax) [ANIMATE: *4÷(AGNT)+[PREPARE]-
(PURP)+ESITUATION: [*x]-&gt;(STAT)-0.[STATE]-)-(ATTR)-qMATURE]].
This central meaning for the concept type BACHELOR explains why Pope John Paul
II, who is an unmarried man, would not be called a bachelor: he has already achieved the
ultimate stage in his profession, which has celibacy as a precondition.
Partee&apos;s puzzle about the temperature may be represented in conceptual graphs by
distinguishing temperature as a state from its measure as a number. The sentence The
temperature is 90 would therefore be treated as an abbreviation for the sentence The
measure of the temperature is 90. Such abbreviations are common in ordinary language,
and words such as measure and amount are evidence for a distinction that is familiar to
most speakers. The following graph shows that the temperature is in the state of RISE
and its current measure happens to be 90°F.
[TEMP-MEASURE: 90F]÷(MEAS)4-[TEMPERATURE: 10-)-(STAT)-■[RISE].
In this graph, the temperature is in a state of rising. Since its measure of 90°F is not di-
rectly attached to [RISE], the value of 90 will not change. Instead, the temperature at a
later time will have a different measure. Unlike Montague&apos;s subtle distinctions, this sol-
ution to Partee&apos;s puzzle is based on concepts derived from the words and phrases used
by people who talk about temperature.
As an example of a complex sentence containing nested contexts and quantification,
Figure 3 shows the graph for a sentence that defines Prix Goncourt as &amp;quot;an institution
</bodyText>
<page confidence="0.998782">
49
</page>
<bodyText confidence="0.8644945">
comprising a panel of judges who each year award a prize of money to an author who
published an outstanding literary work.&amp;quot;
</bodyText>
<table confidence="0.998524772727273">
I INSTITUTION: Prix Goncourt
COMPRISE PANEL JUDGE: (*)
I
/
i
YEAR: V 411D ii
/
i
i
i
i
SITUATION: i
i
i
MONEY ID PRIZE 1441:10 AWARD 0 T
, -1 AUTHOR 41;)
....&apos; &apos;
(44) ...-&apos;-&apos;-°&apos;.
,,,, -
. _
e
-&apos; 6 PUBLISH (41) LITERARY-WORK OUTSTANDING
</table>
<page confidence="0.9152">
7
</page>
<figureCaption confidence="0.998643">
Figure 3. Conceptual graph that defines the Prix Goncourt
</figureCaption>
<bodyText confidence="0.9999415">
In Figure 3, the quantifier V permits the concept [YEAR] to be instantiated with dif-
ferent years, in each of which a separate author is awarded a separate instance of the prize.
The relation (PAST) shows the past tense of published; that relation is not a primitive,
since it may be expanded according to the following definition:
</bodyText>
<equation confidence="0.941639">
PAST = (Ax)[SITUATION: *x]-0.(PTIM)-P[TIME]-(-(SUCC)-4-[TIME: #].
</equation>
<bodyText confidence="0.9998388">
This defines (PAST) as a monadic relation with a formal parameter x. It applies to a
situation whose point in time (PTIM) is a successor to some contextually defined time.
The marker # indicates a reference to be resolved to the point in time of the containing
context, in this case the year of the award — i.e. the publishing occurred before the
awarding.
</bodyText>
<page confidence="0.995301">
50
</page>
<sectionHeader confidence="0.798248" genericHeader="method">
5 Operations on Knowledge in the Lexicon
</sectionHeader>
<bodyText confidence="0.997513428571429">
The purpose of a rich semantic representation is to support a rich set of operations on the
representation. Of the various reasoning systems that have been implemented for con-
ceptual graphs, some are theorem provers that are based on the logical structure of the
graphs; others emphasize heuristic techniques based on semantic distance measures; and
others combine logic and heuristics to speed up the proofs. Fargues et al. (1986) imple-
mented a Prolog-like theorem prover with conceptual graphs as the replacement for the
usual predicates. It incorporated several advances over standard Prolog: arbitrarily large
graphs as the unit of inference instead of single predicates; semantic unification that derives
maximal common supertypes when joining graphs; and the ability to do 2-expansion and
contraction of types. Garner and Tsui (1988) implemented an inference engine that used
heuristics and semantic distance measures to guide the proofs. They demonstrated that
it could handle the kinds of scripts processed by the Schankian systems, but with a more
robust, formally defmed representation. Hartley and Coombs (1991) showed how con-
ceptual graphs could be used in abductive reasoning for generating models that satisfied
given constraints.
Interpreting nonliteral language is an application where background knowledge is es-
sential. Way (1991) presented a book-length study of metaphor using conceptual graphs.
According to her hypothesis, the purpose of a good metaphor is to refine the concept hi-
erarchy by creating a new type. As a result of interpreting a metaphor, a word is gener-
alind to a concept type that includes the original meaning plus a more abstract meaning
that can be transferred to a new domain. Her approach takes advantage of the operations
for joining and projecting graphs and the 2-expressions for defining new concept types.
Metonymy is another kind of nonliteral language that requires detailed semantic
structures in the lexicon and detailed operators for processing them. As an example of
metonymy, consider the sentence The White House announced the budget. Syntactically,
White House is the subject of announce, but semantic constraints rule out the building as
a possible agent of the concept ANNOUNCE. Therefore, a semantic interpreter might
construct a graph with &amp;quot;subj&amp;quot; as a syntactic annotation on the relation, but with the type
of relation unspecified:
[BUILDING: White House]-4-(; subj)+[ANNOUNCE]-0-(PINT)÷[BUDGET: tt].
After constructing this graph in the parsing stage, the semantic interpreter must determine
the unknown relation type and insert it in front of the semicolon. It could search for
background knowledge about the White House, discovering that people work there who
make announcements. From the graphs that state that knowledge, it could abstract the
following 2-expression to define a possible relation between an act and a building:
</bodyText>
<equation confidence="0.592811">
(Ax,y) [ACT: *x14-(AGNT)+[PERSON]÷(LOC)+[BUILDING: *y].
</equation>
<bodyText confidence="0.760893857142857">
This definition relates an act x whose agent is a person located in a building y. The entire
2-expression could then be inserted just before the semicolon of the undefined relation:
[BUILDING: White House]÷(
(Ax,y) [ACT: *x]÷(AGNT)-1-[PERSON]-)-(LOC)÷[BUILDING: *A;
subj)÷[ANNOUNCE]÷(PINT)÷[BUDGET: #].
When the 2-expression is expanded, the concept marked by x (the first parameter) is
joined to the concept with the arrow pointing towards the relation; and the concept
</bodyText>
<page confidence="0.994888">
51
</page>
<bodyText confidence="0.9582848">
marked by y (the second parameter) is joined to the concept with the arrow pointing away
from the relation. The syntactic annotation &amp;quot;subj&amp;quot; must be dropped, since no single re-
lation in the expansion exactly corresponds to the original subject of the sentence.
[BUILDING: White House]÷(LOC)&lt;-[PERSON]&lt;-(AGNT)+.[ANNOUNCE]-
(PINT)+[BUDGET: #].
This graph represents the expanded sentence A person at the White House announced the
budget. The option of omitting the conceptual relation in cases of metonymy or ellipsis
allows certain decisions to be deferred until additional information is obtained from the
context or from background knowledge.
As another example of metonymy, consider the problem of multiple meanings of the
term Prix Goncourt, discussed by Kayser (1988). He found seven metonyms for that term:
a literary prize, the money awarded as the prize, the person who received the prize, the
panel that awards the prize, the book that won the prize, the time that the prize was won,
or the institution that grants a new instance of the prize each year. Following are his
sample sentences and their English translations:
</bodyText>
<listItem confidence="0.999967666666667">
• Prize: Le PG a ete attribue a X. [The PG was awarded to X.]
• Money: X a verse son PG ci la Croix Rouge. [X turned over his PG to the Red Cross.]
• Person: Le PG a ete filicite par le President. [The PG was congratulated by the
President.]
• Panel: Le PG a admis un nouveau jure. [The PG admitted a new judge.]
• Book: Peux-tu aller m&apos;acheter le PG a la librairie X? [Could you go buy the PG for
me at bookstore X?]
• Time: Depuis son PG, il est devenu arrogant. [Since his PG, he has become arrogant.]
• Institution: Le PG pervertit la vie litteraire. [The PG perverts the literary life.]
</listItem>
<bodyText confidence="0.999855272727273">
Each of these metonyms could be interpreted by the method of constructing a
2-expression for the unknown relation. The graph in Figure 3 provides the basic back-
ground knowledge for constructing those relations.
With Figure 3 as background knowledge, each metonym of Prix Goncourt can be
determined by finding a suitable path through the graph and mapping it into a
2-expression that defines the unknown relation. The verbs in the input sentences impose
selectional constraints that determine the direction the path may take. When the con-
straints imposed by the verb are not strong enough, additional background knowledge
derived from other words in the input sentence may be needed; that knowledge could be
represented in other conceptual graphs that would also be joined to the input graph.
Following is a sketch of how such a system could interpret each metonym:
</bodyText>
<listItem confidence="0.977007714285714">
• Prize: The verb attribue in the input maps to the concept [AWARD]. The corre-
sponding concept in the background graph is linked to [PRIZE] by the relation
(PTNT). A maximal join of the input graph to the background graph starting with
the two concepts of type AWARD would automatically associate PG with the node
[PRIZE].
• Money: X could present either the prize itself or the money of the prize to the Red
Cross. Background knowledge that people give money to charitable organizations
</listItem>
<page confidence="0.996788">
52
</page>
<bodyText confidence="0.992616">
would lead to a preference for [MONEY]. That knowledge could be represented in
a separate conceptual graph triggered by the term Red Cross.
</bodyText>
<listItem confidence="0.940833857142857">
• Person: The verb felicite maps to [CONGRATULATE], with its selectional con-
straints for PERSON or a subtype such as AUTHOR. Judges are also persons, but
the node [JUDGE: {*}] is marked as plural by the symbol {*} and is therefore unlikely
to be indicated as the Prix Goncourt. Information about salience should also be
marked on the graph: [AUTHOR], [PRIZE], and [LITERARY-WORK] are more
salient and hence more likely to be selected.
• Panel: The verb admis maps to the concept [ADMIT], which would select an agent
</listItem>
<bodyText confidence="0.885127833333333">
of type PERSON or a collection of persons, such as a panel. But that constraint
would permit either [AUTHOR], [JUDGE], or [PANEL] as the agent. The remainder
of the sentence un nouveau jure introduces the concept [JUDGE], which would unify
with the set of judges linked by the member relation to [PANEL]. The preference rule
for increased connectivity would select the concept [PANEL], especially since one
sense of ADMIT would include the admission of a member to a set.
</bodyText>
<listItem confidence="0.933953166666667">
• Book: The verb acheter maps to [BUY], which would prefer a nonhuman physical
entity as patient. Buying a prize is possible, but that might suggest bribing the panel.
The background knowledge that bookstores sell books would give a strong preference
for BOOK, which would unify with [LITERARY-WORK] (although this is another
example of metonymy, since book could refer to the literary work or to a physical
object in which the work is printed).
• Time: The preposition depuis requires a point in time as its object. The concept
[YEAR: V] indicates an entire series of possible times. The possessive pronoun son,
coreferent with il, indicates a particular person, which would most likely select the
node [AUTHOR], which would occur in one instance of a year, which would then
be the correct time.
• Institution: The verb pervertit maps to [PERVERT], which could have a human as
</listItem>
<bodyText confidence="0.8901084">
agent or almost anything as instrument, either of which might occur in subject posi-
tion. But the present tense of the verb suggests a continuing influence; therefore, the
subject must be something outside the scope of the quantifier on [YEAR: V]. Since
[AUTHOR], [PRIZE], and [MONEY] are all inside that scope, there would be a
separate instance of them for each year. A continuing perversion could only be ex-
erted by something outside that scope, such as the institution or the panel; when ei-
ther is permissible, salience might prefer the node [INSTITUTION].
Once a concept node has been selected by one of these mechanisms, the correct metonym
could then be defined by a A-abstraction over the graph with that node marked as the
formal parameter. As these examples illustrate, the process of interpretation is complex:
it requires a great deal of domain-dependent knowledge; and it must be sensitive to many
syntactic and semantic features, including verb tenses, definite and indefmite articles, and
quantifier scopes. Yet the kind of analysis required, although complex, is within the realm
of what is computable — but only if the background knowledge and lexical entries are
encoded in a suitably rich knowledge representation language.
</bodyText>
<page confidence="0.998143">
53
</page>
<sectionHeader confidence="0.963524" genericHeader="conclusions">
6 Towards a Synthesis of the Logicist and Al Traditions
</sectionHeader>
<bodyText confidence="0.999980631578947">
Linguists who work in the tradition of Noam Chomsky are fond of saying that semantic
theory is not as well developed as syntax. That may be true of their work, but it is not
true of the model-theoretic tradition that follows from the work of Richard Montague.
Nor is it true of the computational work in the Al tradition. These two approaches,
which are often considered diametrically opposed, have complementary strengths and
weaknesses. With a suitable knowledge representation, it is possible to have the best of
both: a formal system of logic that can accommodate background knowledge as used in
AI systems. Features and frames are too weak to serve as a complete system of logic.
Graphs are potentially much more powerful, but they must be formalind in order to
support all logical operators. The inventor of the modem linear notation for predicate
calculus was C. S. Pierce, who later abandoned the linear form in favor of his existential
graphs, which he called &amp;quot;the logic of the future.&amp;quot; Remarkably, Peirce&apos;s graphs have a
context structure that is isomorphic to Kamp&apos;s Discourse Representation Structures
(1981). As a synthesis of Peirce&apos;s graphs with the AI work on semantic networks, con-
ceptual graphs benefitted from a stroke of serendipity: their contexts can directly support
Kamp&apos;s rules for resolving anaphora, even though that was not one of their original design
criteria. A great deal of research is undoubtedly necessary to support all the semantic
structures of language, but these felicitous convergences give hope that such a synthesis
of the logicist and Al traditions is proceeding in the right direction.
</bodyText>
<sectionHeader confidence="0.999115" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999900578947368">
Dixon, R. M. W. (1991) A New Approach to English Grammar on Semantic Principles,
Oxford University Press, New York.
Fargues, Jean, Marie Claude Landau, Anne Dugourd, &amp; Laurent Catach, (1986) &amp;quot;Con-
ceptual graphs for semantics and knowledge processing,&amp;quot; IBM Journal of Research and
Development 30:1, 70-79.
Garner, B.J., &amp; Tsui, E. (1988) &amp;quot;General purpose inference engine for canonical graph
models,&amp;quot; Knowledge Based Systems 1:5, pp. 266-278.
Hartley, Roger T., &amp; Michael J. Coombs (1991) &amp;quot;Reasoning with graph operations,&amp;quot; in
J. F. Sowa, ed., Principles of Semantic Networks: Explorations in the Representation of
Knowledge, Morgan Kaufmann Publishers, San Mateo, CA, pp. 487-505.
Kamp, Hans (1981) &amp;quot;Events, discourse representations, and temporal references,&amp;quot;
Langages 64, 39-64.
Katz, Jerrold J., &amp; Jerry A. Fodor (1963) &amp;quot;The structure of a semantic theory,&amp;quot; Language
39, 170-210. Reprinted in J. A. Fodor &amp; J. J. Katz, eds. (1964) The Structure of
Language, Prentice-Hall, Englewood Cliffs, NJ, pp. 479-518.
Kayser, Daniel (1988) &amp;quot;What kind of thing is a concept?&amp;quot; Computational Intelligence 4:2,
158-165.
Montague, Richard (1970) &amp;quot;English as a formal language,&amp;quot; reprinted in Montague (1974),
pp. 188-221.
</reference>
<page confidence="0.978229">
54
</page>
<reference confidence="0.997395388888889">
Montague, Richard (1974) Formal Philosophy, Yale University Press, New Haven.
Ruhl, Charles (1989) On Monosemy: A Study in Linguistic Semantics, State University
of New York Press, Albany.
Schank, Roger C., ed. (1975) Conceptual Information Processing, North-Holland Pub-
lishing Co., Amsterdam.
Schank, Roger C., Michael Lebowitz, &amp; Lawrence Birnbaum (1980) &amp;quot;An integrated
understander,&amp;quot; American Journal of Computational Linguistics 6, 13-30.
Sowa, John F. (1976) &amp;quot;Conceptual graphs for a database interface,&amp;quot; IBM Journal of Re-
search and Development 20:4, pp. 336-357.
Sowa, John F. (1984) Conceptual Structures: Information Processing in Mind and Ma-
chine, Addison-Wesley, Reading, MA.
Sowa, John F. (1988) &amp;quot;Using a lexicon of canonical graphs in a semantic interpreter,&amp;quot; in
M. Evens, ed., Relational Models of the Lexicon, Cambridge University Press, pp. 73-97.
Sowa, John F. (1991) &amp;quot;Towards the expressive power of natural language,&amp;quot; in J. F. Sowa,
ed., Principles of Semantic Networks: Explorations in the Representation of Knowledge,
Morgan Kaufmann Publishers, San Mateo, CA, pp. 157-189.
Way, Eileen C. (1991) Dynamic Type Hierarchies, Kluwer Academic Publishers.
Wilks, Yorick A. (1991) Personal communication.
</reference>
<page confidence="0.999048">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001175">
<title confidence="0.99737">Logical Structures in the Lexicon</title>
<author confidence="0.99748">F John</author>
<affiliation confidence="0.999871">IBM Systems Research</affiliation>
<address confidence="0.9964475">500 Columbus Thornwood, NY 10594</address>
<email confidence="0.992904"></email>
<abstract confidence="0.995146905940594">The lexical entry for a word must contain all the information needed to construct a semantic representation for sentences that contain the word. Because of that requirement, the formats for lexical representations must be as detailed as the semantic forms. Simple representations, such as features and frames, are adequate for resolving many syntactic ambiguities. But since those notations cannot represent all of logic, they are incapable of supporting all the function needed for semantics. Richer semantic-based approaches have been developed in both the model-theoretic tradition and the more computational AI tradition. Although superficially in conflict, these two traditions have a great deal in common at a deeper level. Both of them have developed semantic structures that are capable of representing a wide range of linguistic phenomena. This paper compares these approaches and evaluates their adequacy for various kinds of semantic information that must be stored in the lexicon. It presents conceptual graphs as a synthesis of the logicist and Al representations designed to support the requirements of both. 1 Semantics from the Point of View of the Lexicon To understand a semantic theory, start by looking at what goes into the lexicon. In one of the early semantic theories in the Chomskyan tradition, Katz and Fodor (1963) did in fact start with the lexicon. More recent theories, however, almost treat the lexicon as an afterthought. Yet the essence of the theory is still in the lexicon: every element of the semantic representation of a sentence ultimately derives from something in the lexicon. That principle is just as true for Richard Montague&apos;s highly formalized grammar as for Roger Schank&apos;s &amp;quot;scruffy&amp;quot; conceptual dependencies, scripts, and MOPs. Context and background knowledge are also important, since most sentences cannot be understood in isolation. This fact contradicts Frege&apos;s principle of compositionality, which says that the meaning of a sentence is derived from the meanings of the words it contains. Yet context can also be stated in words and sentences. Even when nonlinguistic surroundings are necessary for understanding a sentence, every significant feature could be encoded in a sentence; people normally encode such information when they tell a story. More general encyclopedic knowledge is contextual information that was learned at an earlier time; it too could be stated in sentences. An extended Fregean principle should therefore say that the meaning of a sentence must be derivable from the meanings of the words in the sentence together with the meanings of the words in the sentences that describe the relevant context and background knowledge. 38 Besides the meanings of words, grammar and logic are necessary to combine those meanings into a complete semantic representation. But there are competing theories about how much grammar and logic is necessary, how much is expressed in the lexicon, and how much is expressed in the linguistic system outside the lexicon. Lexically based theories suggest that the grammar rules should be simple and that most of the syntactic complexity should be encoded in the lexicon. One might even go further and say that most of the syntactic complexity isn&apos;t syntactic at all. It is the result of interactions among the logical structures of the underlying concepts. In his work on semantically based syntax, Dixon (1991) maintained that syntactic irregularities and idiosyncrasies are not accidental. Instead, he showed that many of them can be predicted from the semantics of the words. Such theories imply that a system would only need a simple grammar to represent a language if it had sufficiently rich semantic structures. A rich theory of semantics in the lexicon must also explain how the semantics got into the lexicon. A child could learn an initial stock of meanings by associating prelinguistic structures with words. But even those prelinguistic structures are shaped, polished, and refined by long usage in the context of sentences. They are combined with the structures learned from other words, and they are molded into patterns that are traditional in the language and culture. More complex, abstract, and sophisticated concepts are either learned exclusively through language or through experiences that are highly colored and shaped by language. For these reasons, the meaning representations in the lexicon should be derivable from the semantic representations for sentences. As a working hypothesis, the two should be identical: the same knowledge representation language should be used for representing meanings in the lexicon and for representing the semantics of sentences and extended discourse structures. This paper explores the implications of that hypothesis. Section 2 reviews several different lexical representations and their implications. Section 3 compares the underlying assumptions of the model-theoretic and Al traditions and shows that they are computationally more compatible than their metaphysics would suggest. Section 4 illustrates the use of conceptual graphs for representing the semantic content of lexical entries. Section 5 shows how such representations can be used to handle aspects of language that require both logic and background knowledge. 2 Review of Lexical Representations are one of the oldest and simplest semantic representations. In his (1679) assigned prime numbers to semantic primitives and represented compound concepts by products of the primes. If RATIONAL were represented by 2 and ANIMAL by 3, then their product 6 would represent RATIONAL ANIMAL or HUMAN. That representation generates a lattice: concept A is a supertype of B if the number for A divides the number for B; the minimal common supertype of A and B corresponds to their greatest common divisor; and the maximal common subtype of A and B corresponds to their least common multiple. Leibniz tried to use his system to mechanize Aristotle&apos;s syllogisms, but a feature-based representation is too limited. It cannot distinguish different quantifiers or show how primitive concepts are related to one another in compounds. Modem notations use bit strings instead of products of primes, but their logical power is just as limited as Leibniz&apos;s system of 1679. 39 In their feature-based system, Katz and Fodor (1963) factored the meaning of a word a string of features and an undigested lump called a is their for one sense of bachelor 4. noun 4- (Animal) 4- (Male) 4- (Young) 4. [fur seal when without a mate during the breeding time]. this definition, the syntactic marker; the features (Male), the semantic markers that contain the theoretically significant information; and the phrase in brackets is the unanalyzed distinguisher. Shortly after it appeared, the Katz-Fodor theory was subjected to devastating criticisms. Although no one today uses the theory in its original form, those criticisms are worth mentioning because many of the more modern approaches suffer from the same limitations: • The sharp distinction between semantic features and the distinguisher is so fundamental to the theory that it should have an enormous impact on the structures of language and the normal use of language. Yet there is no linguistic evidence from syntax or cooccurrence patterns to indicate that it has any effect whatever. • The distinguisher is made up of words, each of which has its own meaning. A complete semantic theory should explain how the meanings of the words in the distinguisher contribute to the meaning of the whole. But such an analysis would imply a deeper representation that underlies both the features and the distinguisher. • The Katz-Fodor theory treats different word senses as if they were purely accidental that have no relationship to one another. Yet the four senses of do have something in common. They all represent an immature or transitional stage that leads to some further goal: a student who has completed an academic step on the way to becoming a master or doctor; a young knight who is still an apprentice to another; a seal on its way to full maturity as a patriarch of the herd; or an unmarried man who has not yet started to form his own family. The feature-distinguisher theory does not show the commonality; it cannot explain how these meanings developed from a common root or why they remain associated with the same word form. • If the features had no deeper structure, there would be nothing to constrain their combinations. Yet certain combinations, such as (Color) (Weight), occur. More structure is needed in the theory to explain why such combinations are impossible. • Finally, many features cannot be named with a single word. Certain Mexican dialects, example, make a distinction between a deceased person who was married the time of death, and an deceased person who was not married at the of death (El Guindi 1986). A feature such as is so blatantly nonprimitive that it cries out for a theory that represents deeper structures. These criticisms do not imply that features are useless. But they indicate that features are derived from some deeper, more fundamental representation. Despite their limitations, features are attractive because they are easy to program, and they generate convenient computational structures, such as Leibniz&apos;s lattice. Yet a lattice generated by features permits too many impossible combinations. A well-structured definitional mechanism is necessary to generate lattices without those undesirable nodes. 40 The prototype for all definitions is Aristotle&apos;s method of genus plus differentiae: a new term is defined by specifying its genus or supertype plus the differentiae that distinguish it from other subtypes of the same genus. The result of such definitions is a tree. The tree becomes a lattice when the differentiae are specified in a system of logic that can determine when one definition is implied by another. Such a lattice, variously known as a type, taxonomic, or subsumption hierarchy, forms the basis for many systems of frames and semantic networks. These lattices have several advantages over feature lattices: only the desired nodes are ever defmed; most, if not all, of the impossible combinations can be proved to be contradictory in the underlying logic; and the logic can specify relationships that a simple concatenation of features can never represent. A serious computational problem arises with definitional systems that are based on logic. If the logic is rich enough to express anything in first-order predicate calculus, the proof procedures can become intractable. To simplify the computations, many frame systems adopt a restricted logic, usually without negations or disjunctions. Yet those relogics cannot express all the definitions used in language. The definition of penexample, requires a negation to express &amp;quot;not having a penny.&amp;quot; The word Katz and Fodor&apos;s example requires temporal logic to express &amp;quot;without a mate during the breeding time.&amp;quot; In general, every logical quantifier, Boolean operator, and modal operator occurs in dictionary definitions. Although tractability is an important feature in a computational system, a logic that is suitable for natural language semantics must be able to represent anything that people might say. A problem solver or reasoner might have to simplify the statements in order to improve efficiency, but those simplifications are likely to be highly domain-dependent. A general language handler must represent every statement in as rich a form as it was originally expressed. In several articles written shortly before his death, Richard Montague (1974) began the tradition of applying model-theoretic techniques to natural language semantics. He started with Carnap&apos;s intuition (1947) that the intension of a sentence is a function from possible worlds to truth values. He combined that notion with Frege&apos;s principle of compositionality to develop a systematic way of deriving the function that represents the meaning of a sentence. The intension of each word in each lexical category would correto a functional form of some sort. The intension of the noun example, be a function that applies to entities in the world and generates the value unicorn and each nonunicorn. Other lexical categories would be represented by 1-expressions that would combine with the functional forms that happened to occur their right or left. As an example, Montague&apos;s lexical entry for the word a function that checks whether the predicate is true of the subject. The idea is straightforward, but his implementation uses a rather heavy notation with many subtle distinctions: (?(y) ext(x) = ext(y)). A-expression defines the intension of a function with two arguments: .9 is the of the phrase that follows the word the intension of the subject of body of the expression applies the function to some extension is equal the extension of lexicon consists of such constructions that apply functions of functions to generate other functions of functions of functions.&apos; I To improve readability, this example modifies Montague&apos;s notation by adding the keyword &amp;quot;where&amp;quot; and using the function ext(x) for extension instead of Montague&apos;s cryptic marks. 41 Besides constructing functions, Montague used them to solve certain logical puzzles. of them is Barbara Partee&apos;s example: temperature is ninety, and it is rising. ninety is rising. avoid the conclusion that a constant like ninety could Montague drew some subtle distinctions. He treated an &amp;quot;extraordinary noun&amp;quot; that denoted &amp;quot;an individual concept, not an individual&amp;quot;. He also gave spetreatment to &amp;quot;unlike most verbs, depends for its applicability on the full behavior of individual concepts, not just on their extensions.&amp;quot; As a result, he claimed that temperature is ninety the equality of extensions, but that temperature is the verb the intension. Consequently, the conclusion that ninety is would be blocked, since not be applied to the extension. To linguists, Montague&apos;s distinction between words whose semantics depend on intensions and those semantics depend on extensions seemed like an hoc he never gave any linguistic evidence to support it. To psychologists, the complex manipulations required for processing the A-expressions seemed unlikely to have any psychological reality. And to programmers, the infinities of possible worlds seemed computationally intractable. Yet for all its unnaturalness, Montague&apos;s system was an impressive achievement: it showed that formal methods of logic could be applied to natural languages, that they could define the semantics of a significant subset of English, and that they could represent logical aspects of natural language with the depth and precision usually attained only in artificial systems of logic. At the opposite extreme from Montague&apos;s logical rigor are Roger Schank&apos;s informal diagrams and quasi-psychological theories that were never tested in controlled psychological experiments. Yet they led his students to build impressive demos that exhibited interesting language behavior. As an example, the Integrated Partial Parser (Schank, Lebowitz, &amp; Birnbaum 1980) represents a fairly mature stage of Schank&apos;s theories. IPP would analyze newspaper stories about international terrorism, search for words that concepts in domain, and apply scripts that relate those concepts to one In one IPP processed the sentence, 20 persons occupied the office Amnesty International seeking better jail conditions for three alleged terrorists. inthat sentence, it used the following dictionary entry for the word (WORD-DEF OCCUPIED INTEREST 5</abstract>
<title confidence="0.591294416666667">TYPE EB SUBCLASS SEB TEMPLATE (SCRIPT $DEMONSTRATE ACTOR NIL OBJECT NIL DEMANDS NIL METHOD (SCENE $OCCUPY ACTOR LOCATION NIL)) FILL (((ACTOR) (TOP-OF ((METHOD ACTOR) (TOP-OF *ACTOR-STACK*))) REQS</title>
<abstract confidence="0.988936074152542">FIND-OCCUPY-LOC RECOGNIZE-DEMANDS)) entry says that interest level 5 (on a scale from 0 to 10) and it is an event builder (EB) of subclass scene event builder (SEB). The template is a script of type 42 $DEMONSTRATE with an unknown actor, object, and demands. As its method, the demonstration has a scene of type $OCCUPY with an unknown actor and location. At the end of the entry are fill and request slots that give procedural hints for finding the actor, object, location, and demands. In analyzing the sample sentence, IPP identified the 20 persons as the actors, the office as the location, and the better jail conditions as the demand. The fill and request slots implement the Schanlcian &amp;quot;expectations.&amp;quot; A fill slot is filled with something previously found in the sentence, and a request slot waits for something still to come. They serve the same purpose as Montague&apos;s left and right cancellation rules categorial grammar. The act of filling the slots corresponds to the for expanding a function that is applied to a list of arguments. Their differences in style are more significant than their differences in computational mechanisms: • Schank&apos;s antiformalist stance is irrelevant, since anything that can be programmed on a digital computer could be formalized. One Prolog programmer, in fact, showed that most of the slot filling in Schank&apos;s parsers and script handlers could be done directly by Prolog&apos;s unification algorithm. Techniques such as unification and graph grammars could be used to formalize Schank&apos;s systems while making major improvements in clarity, robustness, and generality. • Montague&apos;s appearance of rigor results from his use of Greek letters and logical symbols. Yet some constructions, such as his solution to Partee&apos;s puzzle, are contrivances that programmers would call &amp;quot;hacks&amp;quot; (if they ever took the trouble to work their way through his notation). • Schank and Montague had different attitudes about what aspects of language were most important. Schank believed that the ability to represent and use world knowledge is the essence of language understanding, and Montague believed that the ability to handle scope of quantifiers and modalities was the most significant. They were both right in believing that their favorite aspects were important, but they were both wrong in ignoring the others. Schank and Montague represented different aspects of language with different methodologies, but they are complementary rather than conflicting. Yorick Wilks (1991) observed Montague&apos;s lexical entries are most complex for words like which Schank&apos;s entries are trivial. Conversely, Schank&apos;s entries are richest for content words, which Montague simply put in one of his categories, while ignoring their connotations. Both logic and background knowledge are important, and the lexicon must include both kinds of information. 3 Metaphysical Baggage and Observable Results Linguistic theories are usually packaged in metaphysical terms that go far beyond the available evidence. Chomsky&apos;s metaphysics may be summarized in a single sentence from Structures: is best formulated as a self-contained study independent of semantics.&amp;quot; For Montague, the title and opening sentence of &amp;quot;English as a Formal Language&amp;quot; express his point of view: &amp;quot;I reject the contention that an important theoretical difference exists between formal and natural languages.&amp;quot; Schank&apos;s outlook is sumin the following sentence from Information Processing: Dependency Theory was always intended to be a theory of how humans process natural 43 language that was explicit enough to allow for programming it on a computer.&amp;quot; These characteristic sentences provide a key to understanding their authors&apos; motivation. Yet their actual achievements are easier to understand when the metaphysics is ignored. Look at what they do, not at what they say. In their basic attitudes and metaphysics, Schank and Montague are irreconcilable. Montague is the epitome of the kind of logician that Schank has always denounced as misguided or at least irrelevant. Montague stated every detail of his theory in a precise formalism, while Schank made sweeping generalizations and left the detailed programming to his students. For Montague, the meaning of a sentence is a function from possible worlds to truth values; for Schank, it is a diagram that represents human conceptualizations. On the surface, their only point of agreement is their implacable opposition to Chomsky and &amp;quot;the developments emanating from the Massachusetts Institute of Technology&amp;quot; (Montague 1970). Yet in their reaction against Chomsky, both Montague and Schank evolved positions that are remarkably similar, although their terminology hides the resemblance. What Chomsky called a noun, Schank called a picture producer, and Montague called a function from entities to truth values. But those terms are irrelevant to anything that they ever did: Schank never produced a single picture or even stated a plausible hypothesis about how one might be produced from his diagrams; Montague never applied any of his functions to the real world, let alone the infinity of possible worlds he so freely assumed. In neutral terms, what Montague and Schank did could be dein a way that makes the logicist and of view nearly indistinguishable: 1. Semantics, not syntax, is the key to understanding sentence structure. The traditional grammatical categories are surface manifestations of the fundamental semantic categories. 2. Associated with each word is a characteristic semantic structure that determines how it combines with other words in a sentence. 3. The grammar of a language can be reduced to relatively simple rules that show what categories of words may occur on the right or the left of a given word (the cancellation rules of categorial grammar or the SchanIcian expectations). The variety of sentence patterns is not the result of a complex grammar, but of the complex interactions between a simple grammar and the underlying semantic structures. 4. The meaning of a sentence is derived by combining the semantic structures for each of the words it contains. The combining operations are primarily semantic, although they are guided by word order and inflections. 5. The truth of a sentence in a possible world is computed by evaluating its meaning representation in terms of a model of that world. Although Schank never used logical like question-answering systems embodied effective procedures for computing denotations, while Montague&apos;s infinities were computationally intractable. like producer from entities to truth values heated arguments, but they have no effect on the application of the theory to language or its implementation in a computer program. Without the metaphysical baggage, both theories a semantic-based approach that is widely accepted in and computational linguistics. 44 At the level of data structures and operations, there are significant differences between Montague and Schank. Montague&apos;s representations were A-expressions, which have the associated operations of functional application, A-expansion, and 2-contraction. His metaphysics gave him a rigorous methodology for assigning each word to one of his categories of functions (even though he never actually applied those functions to the real world or any possible world). And his concerns about logic led him to a careful treatment of quantifiers, modalities, and their scope. Schank&apos;s representations are graphs on paper and LISP structures of various kinds in his students&apos; programs. The permissible operations include any manipulations of those structures that could be performed in LISP. Schank&apos;s lack of a precise formalism gave his students the freedom and flexibility to invent novel solutions to problems that Montague&apos;s followers never attempted to address, such as the use of world knowledge in language understanding. Yet that lack of formalism led hoc in the programs that made them unmaintainable. Many of Schank&apos;s students found it easier to start from scratch and write a new parser than to modify one that was written by an earlier generation of students. Montague and Schank have complementary strengths: rigor vs. flexibility; logical precision vs. open-ended access to background knowledge; exhaustive analysis of a tiny fragment of English vs. a broad-brush sketch of a wide range of language use. Montague and Schank represent two extremes on the semantic-based spectrum, which is broad enough to encompass most AI work on language. Since the extremes are more complementary than conflicting, it is possible to formulate approaches that combine the strengths of both: a precise formalism, the expressive power of intensional logic, and the ability to use background knowledge in language understanding. To allow greater flexibility, some of Montague&apos;s rigid constraints must be relaxed: his requirement of a strict one-to-one mapping between syntactic rules and semantic rules; his use of A-expressions as the meaning representation; and his inability to handle ellipses, metaphor, metonymy, anaphora, and anything requiring background knowledge. With a well-designed formalism, these constraints could be relaxed while still allowing formal definitions of the permissible operations. 4 Lexical Representations in Conceptual Graphs Without a formal system of logic, the issues that a lexical theory must address can only be discussed at an anecdotal level. A formal system can clarify the issues, make them precise, and lead the discussion to a deeper level of detail. This paper uses the theory of conceptual graphs: a system of logic with a graph notation designed for a direct mapping to and from the semantic structures of natural language. They have a graph structure based on the semantic networks of Al and C. S. Peirce&apos;s existential graphs, which form a complete system of logic. They are as formal and expressive as Montague&apos;s intensional logic, but they permit a broader range of operations on the formalism. The theory has been presented in book form (Sowa 1984) and in a recent summary (Sowa 1991); see those sources for more detail. An earlier paper on lexical issues with conceptual graphs (Sowa 1988) did not discuss logic explicitly. This section will give some examples to illustrate the kind of logical structures that are needed in the lexicon. basic feature of conceptual graph theory is the use of graphs represent the expected roles associated with each concept type. Canonical graphs are similar to the case frames used in many systems, but they are richer in the kinds of structures and logical 45 operators they support. As an example, Figure 1 shows a canonical graph that represents lexical pattern associated with the verb shows that every instance of the concept type SUPPORT has four expected participants: an animate agent, some entity as patient, some entity as instrument, and a purpose, which is represented by a nested context. That context, which might represent something at a different time and place from the outer context, shows that the entity is in some state. Figure 1. Canonical graph for the lexical type SUPPORT Whereas case frames merely show the thematic roles for a verb and the expected concept types that can fill those roles, conceptual graphs can grow arbitrarily large: they can show long-range dependencies far removed from the central concept; and they may contain nested contexts that show situations at different times and in different modalities. dotted line in Figure 1 is a link crosses context boundaries; it shows that the entity that is the patient of SUPPORT is coreferent with the thing in the nested context. Conceptual graphs are a complete system of logic with their own model-theoretic sebut there is also a operator ck maps conceptual graphs into predicate calculus. Following is the result of applying ci) to Figure 1: (3z) (3u) (3v) (support(x) situation(v) A (3w)(state(w) This formula and the graph in Figure 1 express exactly the same information with identical ontological presuppositions. The first three lines of the formula represent the standard STATE ANIMATE ENTITY 46 information that is typical of case frames. The fourth line, however, represents structures typical of situation semantics. It says that the situation v is described by a formula, which says that there exists a state w and that the entity z is in state w; i.e. the purpose of supporting is to maintain an entity in some state. The predicate description(s,p) means that situation described by a proposition is one of the predicates that result from the context-creating boxes in conceptual graphs, which are necessary for representing a variety of structures in language. Besides being more readable, the graph contains structures that are not supported in predicate calculus, such as the context box that encloses the purpose of the supporting. That box itself represents a concept to which relations can be attached to express purpose, causality, time sequence, and other intersentential connectives. The description predicate in the formula requires a version of higher order logic with nested propositions and quantification over the situations described by those propositions. Although the description predicate allows some contexts to be translated into predicate calculus by 4), there are other features associated with contexts that cannot be translated. One such feais the referent #, is used to represent unresolved anaphoric references, and other context-dependent phenomena. The formula operator undefmed for graphs containing # until those references have been resolved. As another example, Figure 2 shows the canonical graphs for the concepts EASY and which are used for the adjectives well as the adverbs eagerly. Figure 2. Canonical graphs for EASY and EAGER The first graph says that every instance of EASY is an attribute of some ENTITY, and it is also the manner of some ACT. That ENTITY also happens to be the patient of the same ACT. The graph for EAGER has the same shape as the graph for EASY, but EAGER is an attribute of some ANIMATE being that is the agent of some ACT. These graphs illustrate the point that many syntactic features result from deeper logical properties. In this case, the AGNT and PTNT relations have different preferences for expression as subject or object. As a result, the canonical graphs permit sentences of the following form: John easily does the homework. John eagerly does the homework. 47 The homework is easy for John to do. But they rule out the following sentences: * The homework is eager for John to do. * John is easy to do the homework. Sowa and Way (1986) showed how such graphs could be used in a semantic interpreter. The grammar would only require general rules for adjectives and adverbs; no features be required to distinguish special properties of their adverbial forms. Instead, the correct options would be selected and the incorrect ones would be blocked by the graph unification operations (the maximal joins of conceptual graphs). Besides the display form for conceptual graphs (Figures 1 and 2), there is also a more compact linear form. Following are the linear representations for the graphs in Figure 2: [EASY: V]- (ATTR)+[ENTITY]4-(PINT)+[ACT: *x] (MANR)+[*x]. [EAGER: V]- (ATTR)4-[ANIMATE]÷(AGNT)÷[ACT: *x] (MANR)÷[*x]. In the linear notation, concepts are enclosed in square brackets, and conceptual relations are enclosed in parentheses. The hyphens show that additional relations attached to a node are continued on separate lines. The variable *x indicates that the concept [*x] represents the same node as the concept [ACT: *x]. Converting from the box and circle notation to the linear notation causes cycles be broken, and variables like *x are needed to show cross-references. The point at which the cycle is broken is purely arbitrary; different linearizations represent exactly the same graph. examples cited in Section 2 — difunto, — also be defined in conceptual graphs. The definition of PENNILESS, for example, requires a negation: PENNILESS = (Ax) [STATE: *x]-4-(STAT)-(-[PER5ON: *y] -[ [*y]-*(POSS)-&gt;[PENNY]]. This definition says that PENNILESS is a state x of a person y, where it is false that y has possession (POSS) of a penny. Systems that do not allow negations in definitions may make the reasoning process faster, but they make it impossible to define many kinds of concepts. The definition for the concept type DIFUNTO would require a relation WHEN linking two situations: DIFUNTO = (Ax) [PERSON: *x]-)-(STAT)÷[DEAD] [SITUATION: Vx]-)-(STAT)÷[MARRIED]]÷(WHEN)-+ISITUATION: [*x]4-(PINT)4-[DIE]]. By this definition, a difunto is a person x in state dead, where x was in a situation of being married when a situation occurred in which x died. The definition of angelito is similar, but with a negation inside the first situation. In reasoning by inheritance, both DIFUNTO and ANGELITO could be treated as simple subtypes of DEAD-PERSON, the details inside the definition could be ignored. But to determine whether a partic- 48 ular individual was a difunto or an angelito, the details could be recovered by expanding the A-expression. New types of conceptual relations can also be defined by A-abstraction. The relation WHEN in the previous examples could be defined by the following graph: WHEN = (Ax,y) *x]÷(PTIM)-0-[TIME]-4-(PTIM)•-[SITUATION: In this defmition, WHEN has two formal parameters x and y; each of them refers to a situation that occurs at the same point in time (PTIM). Any occurrence of the relation WHEN in a graph could be replaced by the sequence of concepts and relations between [sx] and [*y]. The graph in the definition of DIFUNTO could be expanded to the following: [SITUATION: [*x]÷(STAT)+[MARRIED]]-)-(PTIM)-[TIME]- (PTIM)÷[SITUATION: Pcx]-4-(PINT)&lt;-[DIE]]. This graph says that x is in the state married at some point in time, which is the same time that x dies. Since the graph is too long to fit on one line, the hyphen shows that the relation attached to [TIME] is continued on the next line. In Section 2, one criticism of Katz and Fodor&apos;s theory was its inability to show the commonality among different senses of the same word. Some linguists, such as Ruhl even maintain a principle of word has a single very abstract meaning, and the multiple senses that appear in dictionary definitions are the result of the word in different domains. For Katz and Fodor&apos;s example of is a such a unifying meaning: &amp;quot;an animate being x preparing for a situation in which x is in a mature state.&amp;quot; That definition could also be expressed in a conceptual graph: BACHELOR = (Ax) [ANIMATE: *4÷(AGNT)+[PREPARE]- (PURP)+ESITUATION: [*x]-&gt;(STAT)-0.[STATE]-)-(ATTR)-qMATURE]]. This central meaning for the concept type BACHELOR explains why Pope John Paul II, who is an unmarried man, would not be called a bachelor: he has already achieved the ultimate stage in his profession, which has celibacy as a precondition. Partee&apos;s puzzle about the temperature may be represented in conceptual graphs by temperature as a state from its measure as a number. The sentence is 90 therefore be treated as an abbreviation for the sentence of the temperature is 90. abbreviations are common in ordinary language, words such as evidence for a distinction that is familiar to most speakers. The following graph shows that the temperature is in the state of RISE and its current measure happens to be 90°F. [TEMP-MEASURE: 90F]÷(MEAS)4-[TEMPERATURE: 10-)-(STAT)-■[RISE]. In this graph, the temperature is in a state of rising. Since its measure of 90°F is not directly attached to [RISE], the value of 90 will not change. Instead, the temperature at a later time will have a different measure. Unlike Montague&apos;s subtle distinctions, this solution to Partee&apos;s puzzle is based on concepts derived from the words and phrases used by people who talk about temperature. As an example of a complex sentence containing nested contexts and quantification, 3 shows the graph for a sentence that defines Goncourt &amp;quot;an institution 49 comprising a panel of judges who each year award a prize of money to an author who published an outstanding literary work.&amp;quot; IINSTITUTION: Prix Goncourt COMPRISE PANEL JUDGE: (*) I / i YEAR: V / i i i i SITUATION: i i i MONEY ID PRIZE AWARD 0 T , -1 AUTHOR 41;) ....&apos; &apos; ...-&apos;-&apos;-°&apos;. ,,,, - . _ e -&apos; 6 PUBLISH LITERARY-WORK OUTSTANDING 7 Figure 3. Conceptual graph that defines the Prix Goncourt In Figure 3, the quantifier V permits the concept [YEAR] to be instantiated with different years, in each of which a separate author is awarded a separate instance of the prize. relation (PAST) shows the past tense of relation is not a primitive, since it may be expanded according to the following definition: = (Ax)[SITUATION: This defines (PAST) as a monadic relation with a formal parameter x. It applies to a situation whose point in time (PTIM) is a successor to some contextually defined time. The marker # indicates a reference to be resolved to the point in time of the containing context, in this case the year of the award — i.e. the publishing occurred before the awarding. 50 5 Operations on Knowledge in the Lexicon The purpose of a rich semantic representation is to support a rich set of operations on the representation. Of the various reasoning systems that have been implemented for conceptual graphs, some are theorem provers that are based on the logical structure of the graphs; others emphasize heuristic techniques based on semantic distance measures; and others combine logic and heuristics to speed up the proofs. Fargues et al. (1986) implemented a Prolog-like theorem prover with conceptual graphs as the replacement for the usual predicates. It incorporated several advances over standard Prolog: arbitrarily large graphs as the unit of inference instead of single predicates; semantic unification that derives maximal common supertypes when joining graphs; and the ability to do 2-expansion and contraction of types. Garner and Tsui (1988) implemented an inference engine that used heuristics and semantic distance measures to guide the proofs. They demonstrated that it could handle the kinds of scripts processed by the Schankian systems, but with a more robust, formally defmed representation. Hartley and Coombs (1991) showed how conceptual graphs could be used in abductive reasoning for generating models that satisfied given constraints. Interpreting nonliteral language is an application where background knowledge is essential. Way (1991) presented a book-length study of metaphor using conceptual graphs. According to her hypothesis, the purpose of a good metaphor is to refine the concept hierarchy by creating a new type. As a result of interpreting a metaphor, a word is generalind to a concept type that includes the original meaning plus a more abstract meaning that can be transferred to a new domain. Her approach takes advantage of the operations for joining and projecting graphs and the 2-expressions for defining new concept types. Metonymy is another kind of nonliteral language that requires detailed semantic structures in the lexicon and detailed operators for processing them. As an example of consider the sentence White House announced the budget. House the subject of semantic constraints rule out the building as a possible agent of the concept ANNOUNCE. Therefore, a semantic interpreter might construct a graph with &amp;quot;subj&amp;quot; as a syntactic annotation on the relation, but with the type of relation unspecified: White House]-4-(; subj)+[ANNOUNCE]-0-(PINT)÷[BUDGET: After constructing this graph in the parsing stage, the semantic interpreter must determine the unknown relation type and insert it in front of the semicolon. It could search for background knowledge about the White House, discovering that people work there who make announcements. From the graphs that state that knowledge, it could abstract the following 2-expression to define a possible relation between an act and a building: (Ax,y) [ACT: *x14-(AGNT)+[PERSON]÷(LOC)+[BUILDING: *y]. This definition relates an act x whose agent is a person located in a building y. The entire 2-expression could then be inserted just before the semicolon of the undefined relation: [BUILDING: White House]÷( (Ax,y) [ACT: *x]÷(AGNT)-1-[PERSON]-)-(LOC)÷[BUILDING: *A; When the 2-expression is expanded, the concept marked by x (the first parameter) is to the concept the arrow pointing towards the relation; and the concept 51 marked by y (the second parameter) is joined to the concept with the arrow pointing away from the relation. The syntactic annotation &amp;quot;subj&amp;quot; must be dropped, since no single relation in the expansion exactly corresponds to the original subject of the sentence. [BUILDING: White House]÷(LOC)&lt;-[PERSON]&lt;-(AGNT)+.[ANNOUNCE]- (PINT)+[BUDGET: #]. graph represents the expanded sentence person at the White House announced the option of omitting the conceptual relation in cases of metonymy or ellipsis allows certain decisions to be deferred until additional information is obtained from the context or from background knowledge. As another example of metonymy, consider the problem of multiple meanings of the Goncourt, by Kayser (1988). He found seven metonyms for that term: a literary prize, the money awarded as the prize, the person who received the prize, the panel that awards the prize, the book that won the prize, the time that the prize was won, or the institution that grants a new instance of the prize each year. Following are his sample sentences and their English translations: Prize: PG a ete attribue PG was awarded to X.] Money: son PG ci la Croix Rouge. turned over his PG to the Red Cross.] Person: PG a ete filicite par le President. PG was congratulated by the President.] Panel: PG a admis un nouveau jure. PG admitted a new judge.] Book: aller m&apos;acheter le PG librairie X? you go buy the PG for me at bookstore X?] Time: son PG, il est devenu arrogant. his PG, he has become arrogant.] Institution: PG pervertit la vie litteraire. PG perverts the literary life.] Each of these metonyms could be interpreted by the method of constructing a 2-expression for the unknown relation. The graph in Figure 3 provides the basic background knowledge for constructing those relations. Figure 3 as background knowledge, each metonym of Goncourt be determined by finding a suitable path through the graph and mapping it into a 2-expression that defines the unknown relation. The verbs in the input sentences impose selectional constraints that determine the direction the path may take. When the constraints imposed by the verb are not strong enough, additional background knowledge derived from other words in the input sentence may be needed; that knowledge could be represented in other conceptual graphs that would also be joined to the input graph. Following is a sketch of how such a system could interpret each metonym: Prize: The verb the input maps to the concept [AWARD]. The corresponding concept in the background graph is linked to [PRIZE] by the relation (PTNT). A maximal join of the input graph to the background graph starting with the two concepts of type AWARD would automatically associate PG with the node [PRIZE]. • Money: X could present either the prize itself or the money of the prize to the Red Cross. Background knowledge that people give money to charitable organizations 52 would lead to a preference for [MONEY]. That knowledge could be represented in separate conceptual graph triggered by the term Cross. Person: The verb to [CONGRATULATE], with its selectional constraints for PERSON or a subtype such as AUTHOR. Judges are also persons, but the node [JUDGE: {*}] is marked as plural by the symbol {*} and is therefore unlikely be indicated as Goncourt. Information about salience should also be marked on the graph: [AUTHOR], [PRIZE], and [LITERARY-WORK] are more salient and hence more likely to be selected. Panel: The verb to the concept [ADMIT], which would select an agent of type PERSON or a collection of persons, such as a panel. But that constraint would permit either [AUTHOR], [JUDGE], or [PANEL] as the agent. The remainder the sentence nouveau jure the concept [JUDGE], which would unify with the set of judges linked by the member relation to [PANEL]. The preference rule for increased connectivity would select the concept [PANEL], especially since one sense of ADMIT would include the admission of a member to a set. Book: The verb to [BUY], which would prefer a nonhuman physical entity as patient. Buying a prize is possible, but that might suggest bribing the panel. The background knowledge that bookstores sell books would give a strong preference for BOOK, which would unify with [LITERARY-WORK] (although this is another of metonymy, since refer to the literary work or to a physical object in which the work is printed). Time: The preposition a point in time as its object. The concept V] indicates an entire series of possible times. The possessive pronoun with a particular person, which would most likely select the node [AUTHOR], which would occur in one instance of a year, which would then be the correct time. Institution: The verb to [PERVERT], which could have a human as agent or almost anything as instrument, either of which might occur in subject position. But the present tense of the verb suggests a continuing influence; therefore, the subject must be something outside the scope of the quantifier on [YEAR: V]. Since [AUTHOR], [PRIZE], and [MONEY] are all inside that scope, there would be a separate instance of them for each year. A continuing perversion could only be exerted by something outside that scope, such as the institution or the panel; when either is permissible, salience might prefer the node [INSTITUTION]. Once a concept node has been selected by one of these mechanisms, the correct metonym could then be defined by a A-abstraction over the graph with that node marked as the formal parameter. As these examples illustrate, the process of interpretation is complex: it requires a great deal of domain-dependent knowledge; and it must be sensitive to many syntactic and semantic features, including verb tenses, definite and indefmite articles, and quantifier scopes. Yet the kind of analysis required, although complex, is within the realm of what is computable — but only if the background knowledge and lexical entries are encoded in a suitably rich knowledge representation language. 53 6 Towards a Synthesis of the Logicist and Al Traditions Linguists who work in the tradition of Noam Chomsky are fond of saying that semantic theory is not as well developed as syntax. That may be true of their work, but it is not true of the model-theoretic tradition that follows from the work of Richard Montague. Nor is it true of the computational work in the Al tradition. These two approaches, which are often considered diametrically opposed, have complementary strengths and weaknesses. With a suitable knowledge representation, it is possible to have the best of both: a formal system of logic that can accommodate background knowledge as used in AI systems. Features and frames are too weak to serve as a complete system of logic. Graphs are potentially much more powerful, but they must be formalind in order to support all logical operators. The inventor of the modem linear notation for predicate was C. S. Pierce, who later abandoned the linear form in favor of his he called &amp;quot;the logic of the future.&amp;quot; Remarkably, Peirce&apos;s graphs have a context structure that is isomorphic to Kamp&apos;s Discourse Representation Structures (1981). As a synthesis of Peirce&apos;s graphs with the AI work on semantic networks, conceptual graphs benefitted from a stroke of serendipity: their contexts can directly support Kamp&apos;s rules for resolving anaphora, even though that was not one of their original design criteria. A great deal of research is undoubtedly necessary to support all the semantic structures of language, but these felicitous convergences give hope that such a synthesis of the logicist and Al traditions is proceeding in the right direction.</abstract>
<title confidence="0.784445">References</title>
<author confidence="0.829622">R M W New Approach to English Grammar on Semantic Principles</author>
<affiliation confidence="0.891109">Oxford University Press, New York.</affiliation>
<address confidence="0.687912">Fargues, Jean, Marie Claude Landau, Anne Dugourd, &amp; Laurent Catach, (1986) &amp;quot;Con-</address>
<note confidence="0.982134857142857">graphs for semantics and knowledge processing,&amp;quot; Journal of Research and 70-79. Garner, B.J., &amp; Tsui, E. (1988) &amp;quot;General purpose inference engine for canonical graph Based Systems 266-278. Hartley, Roger T., &amp; Michael J. Coombs (1991) &amp;quot;Reasoning with graph operations,&amp;quot; in F. Sowa, ed., of Semantic Networks: Explorations in the Representation of Kaufmann Publishers, San Mateo, CA, pp. 487-505. Kamp, Hans (1981) &amp;quot;Events, discourse representations, and temporal references,&amp;quot; Jerrold J., &amp; Jerry A. Fodor (1963) &amp;quot;The structure of a semantic theory,&amp;quot; Reprinted in J. A. Fodor &amp; J. J. Katz, eds. (1964) Structure of Englewood Cliffs, NJ, pp. 479-518. Daniel (1988) &amp;quot;What kind of thing is a concept?&amp;quot; Intelligence 158-165. Montague, Richard (1970) &amp;quot;English as a formal language,&amp;quot; reprinted in Montague (1974), pp. 188-221. 54 Richard (1974) Philosophy, University Press, New Haven. Charles (1989) Monosemy: A Study in Linguistic Semantics, University of New York Press, Albany. Roger C., ed. (1975) Information Processing, Publishing Co., Amsterdam. Schank, Roger C., Michael Lebowitz, &amp; Lawrence Birnbaum (1980) &amp;quot;An integrated Journal of Computational Linguistics John F. (1976) &amp;quot;Conceptual graphs for a database interface,&amp;quot; Journal of Reand Development 336-357. John F. (1984) Structures: Information Processing in Mind and Ma- Reading, MA. Sowa, John F. (1988) &amp;quot;Using a lexicon of canonical graphs in a semantic interpreter,&amp;quot; in Evens, ed., Models of the Lexicon, University Press, pp. 73-97. John F. (1991) &amp;quot;Towards the expressive power of natural language,&amp;quot; in Sowa, of Semantic Networks: Explorations in the Representation of Knowledge, Morgan Kaufmann Publishers, San Mateo, CA, pp. 157-189. Eileen C. (1991) Type Hierarchies, Academic Publishers. Wilks, Yorick A. (1991) Personal communication. 55</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R M W Dixon</author>
</authors>
<title>A New Approach to English Grammar on Semantic Principles,</title>
<date>1991</date>
<publisher>University Press,</publisher>
<location>Oxford</location>
<contexts>
<context position="3566" citStr="Dixon (1991)" startWordPosition="556" endWordPosition="557">omplete semantic representation. But there are competing theories about how much grammar and logic is necessary, how much is expressed in the lexicon, and how much is expressed in the linguistic system outside the lexicon. Lexically based theories suggest that the grammar rules should be simple and that most of the syntactic complexity should be encoded in the lexicon. One might even go further and say that most of the syntactic complexity isn&apos;t syntactic at all. It is the result of interactions among the logical structures of the underlying concepts. In his work on semantically based syntax, Dixon (1991) maintained that syntactic irregularities and idiosyncrasies are not accidental. Instead, he showed that many of them can be predicted from the semantics of the words. Such theories imply that a system would only need a simple grammar to represent a language if it had sufficiently rich semantic structures. A rich theory of semantics in the lexicon must also explain how the semantics got into the lexicon. A child could learn an initial stock of meanings by associating prelinguistic structures with words. But even those prelinguistic structures are shaped, polished, and refined by long usage in </context>
</contexts>
<marker>Dixon, 1991</marker>
<rawString>Dixon, R. M. W. (1991) A New Approach to English Grammar on Semantic Principles, Oxford University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Fargues</author>
<author>Marie Claude Landau</author>
<author>Anne Dugourd</author>
<author>Laurent Catach</author>
</authors>
<title>Conceptual graphs for semantics and knowledge processing,&amp;quot;</title>
<date>1986</date>
<journal>IBM Journal of Research and Development</journal>
<volume>30</volume>
<pages>70--79</pages>
<contexts>
<context position="38994" citStr="Fargues et al. (1986)" startWordPosition="6156" endWordPosition="6159">e resolved to the point in time of the containing context, in this case the year of the award — i.e. the publishing occurred before the awarding. 50 5 Operations on Knowledge in the Lexicon The purpose of a rich semantic representation is to support a rich set of operations on the representation. Of the various reasoning systems that have been implemented for conceptual graphs, some are theorem provers that are based on the logical structure of the graphs; others emphasize heuristic techniques based on semantic distance measures; and others combine logic and heuristics to speed up the proofs. Fargues et al. (1986) implemented a Prolog-like theorem prover with conceptual graphs as the replacement for the usual predicates. It incorporated several advances over standard Prolog: arbitrarily large graphs as the unit of inference instead of single predicates; semantic unification that derives maximal common supertypes when joining graphs; and the ability to do 2-expansion and contraction of types. Garner and Tsui (1988) implemented an inference engine that used heuristics and semantic distance measures to guide the proofs. They demonstrated that it could handle the kinds of scripts processed by the Schankian</context>
</contexts>
<marker>Fargues, Landau, Dugourd, Catach, 1986</marker>
<rawString>Fargues, Jean, Marie Claude Landau, Anne Dugourd, &amp; Laurent Catach, (1986) &amp;quot;Conceptual graphs for semantics and knowledge processing,&amp;quot; IBM Journal of Research and Development 30:1, 70-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Garner</author>
<author>E Tsui</author>
</authors>
<title>General purpose inference engine for canonical graph models,&amp;quot;</title>
<date>1988</date>
<journal>Knowledge Based Systems</journal>
<volume>1</volume>
<pages>266--278</pages>
<contexts>
<context position="39402" citStr="Garner and Tsui (1988)" startWordPosition="6215" endWordPosition="6218">hat are based on the logical structure of the graphs; others emphasize heuristic techniques based on semantic distance measures; and others combine logic and heuristics to speed up the proofs. Fargues et al. (1986) implemented a Prolog-like theorem prover with conceptual graphs as the replacement for the usual predicates. It incorporated several advances over standard Prolog: arbitrarily large graphs as the unit of inference instead of single predicates; semantic unification that derives maximal common supertypes when joining graphs; and the ability to do 2-expansion and contraction of types. Garner and Tsui (1988) implemented an inference engine that used heuristics and semantic distance measures to guide the proofs. They demonstrated that it could handle the kinds of scripts processed by the Schankian systems, but with a more robust, formally defmed representation. Hartley and Coombs (1991) showed how conceptual graphs could be used in abductive reasoning for generating models that satisfied given constraints. Interpreting nonliteral language is an application where background knowledge is essential. Way (1991) presented a book-length study of metaphor using conceptual graphs. According to her hypothe</context>
</contexts>
<marker>Garner, Tsui, 1988</marker>
<rawString>Garner, B.J., &amp; Tsui, E. (1988) &amp;quot;General purpose inference engine for canonical graph models,&amp;quot; Knowledge Based Systems 1:5, pp. 266-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger T Hartley</author>
<author>Michael J Coombs</author>
</authors>
<title>Reasoning with graph operations,&amp;quot;</title>
<date>1991</date>
<booktitle>Principles of Semantic Networks: Explorations in the Representation of Knowledge,</booktitle>
<pages>487--505</pages>
<editor>in J. F. Sowa, ed.,</editor>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Mateo, CA,</location>
<contexts>
<context position="39685" citStr="Hartley and Coombs (1991)" startWordPosition="6257" endWordPosition="6260">s the replacement for the usual predicates. It incorporated several advances over standard Prolog: arbitrarily large graphs as the unit of inference instead of single predicates; semantic unification that derives maximal common supertypes when joining graphs; and the ability to do 2-expansion and contraction of types. Garner and Tsui (1988) implemented an inference engine that used heuristics and semantic distance measures to guide the proofs. They demonstrated that it could handle the kinds of scripts processed by the Schankian systems, but with a more robust, formally defmed representation. Hartley and Coombs (1991) showed how conceptual graphs could be used in abductive reasoning for generating models that satisfied given constraints. Interpreting nonliteral language is an application where background knowledge is essential. Way (1991) presented a book-length study of metaphor using conceptual graphs. According to her hypothesis, the purpose of a good metaphor is to refine the concept hierarchy by creating a new type. As a result of interpreting a metaphor, a word is generalind to a concept type that includes the original meaning plus a more abstract meaning that can be transferred to a new domain. Her </context>
</contexts>
<marker>Hartley, Coombs, 1991</marker>
<rawString>Hartley, Roger T., &amp; Michael J. Coombs (1991) &amp;quot;Reasoning with graph operations,&amp;quot; in J. F. Sowa, ed., Principles of Semantic Networks: Explorations in the Representation of Knowledge, Morgan Kaufmann Publishers, San Mateo, CA, pp. 487-505.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
</authors>
<title>Events, discourse representations, and temporal references,&amp;quot;</title>
<date>1981</date>
<journal>Langages</journal>
<volume>64</volume>
<pages>39--64</pages>
<marker>Kamp, 1981</marker>
<rawString>Kamp, Hans (1981) &amp;quot;Events, discourse representations, and temporal references,&amp;quot; Langages 64, 39-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold J Katz</author>
<author>Jerry A Fodor</author>
</authors>
<title>The structure of a semantic theory,&amp;quot;</title>
<date>1963</date>
<journal>Language</journal>
<booktitle>The Structure of Language, Prentice-Hall, Englewood Cliffs, NJ,</booktitle>
<volume>39</volume>
<pages>170--210</pages>
<editor>J. A. Fodor &amp; J. J. Katz, eds.</editor>
<note>Reprinted in</note>
<contexts>
<context position="1495" citStr="Katz and Fodor (1963)" startWordPosition="226" endWordPosition="229">er level. Both of them have developed semantic structures that are capable of representing a wide range of linguistic phenomena. This paper compares these approaches and evaluates their adequacy for various kinds of semantic information that must be stored in the lexicon. It presents conceptual graphs as a synthesis of the logicist and Al representations designed to support the requirements of both. 1 Semantics from the Point of View of the Lexicon To understand a semantic theory, start by looking at what goes into the lexicon. In one of the early semantic theories in the Chomskyan tradition, Katz and Fodor (1963) did in fact start with the lexicon. More recent theories, however, almost treat the lexicon as an afterthought. Yet the essence of the theory is still in the lexicon: every element of the semantic representation of a sentence ultimately derives from something in the lexicon. That principle is just as true for Richard Montague&apos;s highly formalized grammar as for Roger Schank&apos;s &amp;quot;scruffy&amp;quot; conceptual dependencies, scripts, and MOPs. Context and background knowledge are also important, since most sentences cannot be understood in isolation. This fact contradicts Frege&apos;s principle of compositionalit</context>
<context position="6531" citStr="Katz and Fodor (1963)" startWordPosition="1008" endWordPosition="1011">ides the number for B; the minimal common supertype of A and B corresponds to their greatest common divisor; and the maximal common subtype of A and B corresponds to their least common multiple. Leibniz tried to use his system to mechanize Aristotle&apos;s syllogisms, but a feature-based representation is too limited. It cannot distinguish different quantifiers or show how primitive concepts are related to one another in compounds. Modem notations use bit strings instead of products of primes, but their logical power is just as limited as Leibniz&apos;s system of 1679. 39 In their feature-based system, Katz and Fodor (1963) factored the meaning of a word into a string of features and an undigested lump called a distinguisher. Following is their representation for one sense of bachelor: bachelor 4. noun 4- (Animal) 4- (Male) 4- (Young) 4. [fur seal when without a mate during the breeding time]. In this definition, noun is the syntactic marker; the features (Animal), (Male), and (Young) are the semantic markers that contain the theoretically significant information; and the phrase in brackets is the unanalyzed distinguisher. Shortly after it appeared, the Katz-Fodor theory was subjected to devastating criticisms. </context>
</contexts>
<marker>Katz, Fodor, 1963</marker>
<rawString>Katz, Jerrold J., &amp; Jerry A. Fodor (1963) &amp;quot;The structure of a semantic theory,&amp;quot; Language 39, 170-210. Reprinted in J. A. Fodor &amp; J. J. Katz, eds. (1964) The Structure of Language, Prentice-Hall, Englewood Cliffs, NJ, pp. 479-518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Kayser</author>
</authors>
<title>What kind of thing is a concept?&amp;quot;</title>
<date>1988</date>
<journal>Computational Intelligence</journal>
<volume>4</volume>
<pages>158--165</pages>
<contexts>
<context position="42756" citStr="Kayser (1988)" startWordPosition="6717" endWordPosition="6718">since no single relation in the expansion exactly corresponds to the original subject of the sentence. [BUILDING: White House]÷(LOC)&lt;-[PERSON]&lt;-(AGNT)+.[ANNOUNCE]- (PINT)+[BUDGET: #]. This graph represents the expanded sentence A person at the White House announced the budget. The option of omitting the conceptual relation in cases of metonymy or ellipsis allows certain decisions to be deferred until additional information is obtained from the context or from background knowledge. As another example of metonymy, consider the problem of multiple meanings of the term Prix Goncourt, discussed by Kayser (1988). He found seven metonyms for that term: a literary prize, the money awarded as the prize, the person who received the prize, the panel that awards the prize, the book that won the prize, the time that the prize was won, or the institution that grants a new instance of the prize each year. Following are his sample sentences and their English translations: • Prize: Le PG a ete attribue a X. [The PG was awarded to X.] • Money: X a verse son PG ci la Croix Rouge. [X turned over his PG to the Red Cross.] • Person: Le PG a ete filicite par le President. [The PG was congratulated by the President.] </context>
</contexts>
<marker>Kayser, 1988</marker>
<rawString>Kayser, Daniel (1988) &amp;quot;What kind of thing is a concept?&amp;quot; Computational Intelligence 4:2, 158-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>English as a formal language,&amp;quot; reprinted in</title>
<date>1970</date>
<pages>188--221</pages>
<location>Montague</location>
<contexts>
<context position="21252" citStr="Montague 1970" startWordPosition="3308" endWordPosition="3309">s the epitome of the kind of logician that Schank has always denounced as misguided or at least irrelevant. Montague stated every detail of his theory in a precise formalism, while Schank made sweeping generalizations and left the detailed programming to his students. For Montague, the meaning of a sentence is a function from possible worlds to truth values; for Schank, it is a diagram that represents human conceptualizations. On the surface, their only point of agreement is their implacable opposition to Chomsky and &amp;quot;the developments emanating from the Massachusetts Institute of Technology&amp;quot; (Montague 1970). Yet in their reaction against Chomsky, both Montague and Schank evolved positions that are remarkably similar, although their terminology hides the resemblance. What Chomsky called a noun, Schank called a picture producer, and Montague called a function from entities to truth values. But those terms are irrelevant to anything that they ever did: Schank never produced a single picture or even stated a plausible hypothesis about how one might be produced from his diagrams; Montague never applied any of his functions to the real world, let alone the infinity of possible worlds he so freely assu</context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>Montague, Richard (1970) &amp;quot;English as a formal language,&amp;quot; reprinted in Montague (1974), pp. 188-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>Formal Philosophy,</title>
<date>1974</date>
<publisher>Yale University Press,</publisher>
<location>New Haven.</location>
<contexts>
<context position="12074" citStr="Montague (1974)" startWordPosition="1890" endWordPosition="1891">ier, Boolean operator, and modal operator occurs in dictionary definitions. Although tractability is an important feature in a computational system, a logic that is suitable for natural language semantics must be able to represent anything that people might say. A problem solver or reasoner might have to simplify the statements in order to improve efficiency, but those simplifications are likely to be highly domain-dependent. A general language handler must represent every statement in as rich a form as it was originally expressed. In several articles written shortly before his death, Richard Montague (1974) began the tradition of applying model-theoretic techniques to natural language semantics. He started with Carnap&apos;s intuition (1947) that the intension of a sentence is a function from possible worlds to truth values. He combined that notion with Frege&apos;s principle of compositionality to develop a systematic way of deriving the function that represents the meaning of a sentence. The intension of each word in each lexical category would correspond to a functional form of some sort. The intension of the noun unicorn, for example, would be a function that applies to entities in the world and gener</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>Montague, Richard (1974) Formal Philosophy, Yale University Press, New Haven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Ruhl</author>
</authors>
<title>On Monosemy: A Study in Linguistic Semantics,</title>
<date>1989</date>
<publisher>Press,</publisher>
<institution>State University of New York</institution>
<location>Albany.</location>
<contexts>
<context position="35384" citStr="Ruhl (1989)" startWordPosition="5560" endWordPosition="5561">ations between [sx] and [*y]. The graph in the definition of DIFUNTO could be expanded to the following: [SITUATION: [*x]÷(STAT)+[MARRIED]]-)-(PTIM)-[TIME]- (PTIM)÷[SITUATION: Pcx]-4-(PINT)&lt;-[DIE]]. This graph says that x is in the state married at some point in time, which is the same time that x dies. Since the graph is too long to fit on one line, the hyphen shows that the relation attached to [TIME] is continued on the next line. In Section 2, one criticism of Katz and Fodor&apos;s theory was its inability to show the commonality among different senses of the same word. Some linguists, such as Ruhl (1989), even maintain a principle of monosemy: each word has a single very abstract meaning, and the multiple senses that appear in dictionary definitions are the result of applying the word in different domains. For Katz and Fodor&apos;s example of bachelor, there is a such a unifying meaning: &amp;quot;an animate being x preparing for a situation in which x is in a mature state.&amp;quot; That definition could also be expressed in a conceptual graph: BACHELOR = (Ax) [ANIMATE: *4÷(AGNT)+[PREPARE]- (PURP)+ESITUATION: [*x]-&gt;(STAT)-0.[STATE]-)-(ATTR)-qMATURE]]. This central meaning for the concept type BACHELOR explains why</context>
</contexts>
<marker>Ruhl, 1989</marker>
<rawString>Ruhl, Charles (1989) On Monosemy: A Study in Linguistic Semantics, State University of New York Press, Albany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>ed</author>
</authors>
<date>1975</date>
<booktitle>Conceptual Information Processing,</booktitle>
<publisher>North-Holland Publishing Co.,</publisher>
<location>Amsterdam.</location>
<marker>Schank, ed, 1975</marker>
<rawString>Schank, Roger C., ed. (1975) Conceptual Information Processing, North-Holland Publishing Co., Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Michael Lebowitz</author>
<author>Lawrence Birnbaum</author>
</authors>
<title>An integrated understander,&amp;quot;</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>6</volume>
<pages>13--30</pages>
<marker>Schank, Lebowitz, Birnbaum, 1980</marker>
<rawString>Schank, Roger C., Michael Lebowitz, &amp; Lawrence Birnbaum (1980) &amp;quot;An integrated understander,&amp;quot; American Journal of Computational Linguistics 6, 13-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John F Sowa</author>
</authors>
<title>Conceptual graphs for a database interface,&amp;quot;</title>
<date>1976</date>
<journal>IBM Journal of Research and Development</journal>
<volume>20</volume>
<pages>336--357</pages>
<marker>Sowa, 1976</marker>
<rawString>Sowa, John F. (1976) &amp;quot;Conceptual graphs for a database interface,&amp;quot; IBM Journal of Research and Development 20:4, pp. 336-357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John F Sowa</author>
</authors>
<date>1984</date>
<booktitle>Conceptual Structures: Information Processing in Mind and Machine,</booktitle>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="26843" citStr="Sowa 1984" startWordPosition="4177" endWordPosition="4178">al system can clarify the issues, make them precise, and lead the discussion to a deeper level of detail. This paper uses the theory of conceptual graphs: a system of logic with a graph notation designed for a direct mapping to and from the semantic structures of natural language. They have a graph structure based on the semantic networks of Al and C. S. Peirce&apos;s existential graphs, which form a complete system of logic. They are as formal and expressive as Montague&apos;s intensional logic, but they permit a broader range of operations on the formalism. The theory has been presented in book form (Sowa 1984) and in a recent summary (Sowa 1991); see those sources for more detail. An earlier paper on lexical issues with conceptual graphs (Sowa 1988) did not discuss logic explicitly. This section will give some examples to illustrate the kind of logical structures that are needed in the lexicon. A basic feature of conceptual graph theory is the use of canonical graphs to represent the expected roles associated with each concept type. Canonical graphs are similar to the case frames used in many systems, but they are richer in the kinds of structures and logical 45 operators they support. As an exampl</context>
</contexts>
<marker>Sowa, 1984</marker>
<rawString>Sowa, John F. (1984) Conceptual Structures: Information Processing in Mind and Machine, Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John F Sowa</author>
</authors>
<title>Using a lexicon of canonical graphs in a semantic interpreter,&amp;quot;</title>
<date>1988</date>
<booktitle>Relational Models of the Lexicon,</booktitle>
<pages>73--97</pages>
<editor>in M. Evens, ed.,</editor>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="26985" citStr="Sowa 1988" startWordPosition="4201" endWordPosition="4202">tual graphs: a system of logic with a graph notation designed for a direct mapping to and from the semantic structures of natural language. They have a graph structure based on the semantic networks of Al and C. S. Peirce&apos;s existential graphs, which form a complete system of logic. They are as formal and expressive as Montague&apos;s intensional logic, but they permit a broader range of operations on the formalism. The theory has been presented in book form (Sowa 1984) and in a recent summary (Sowa 1991); see those sources for more detail. An earlier paper on lexical issues with conceptual graphs (Sowa 1988) did not discuss logic explicitly. This section will give some examples to illustrate the kind of logical structures that are needed in the lexicon. A basic feature of conceptual graph theory is the use of canonical graphs to represent the expected roles associated with each concept type. Canonical graphs are similar to the case frames used in many systems, but they are richer in the kinds of structures and logical 45 operators they support. As an example, Figure 1 shows a canonical graph that represents the lexical pattern associated with the verb support, It shows that every instance of the </context>
</contexts>
<marker>Sowa, 1988</marker>
<rawString>Sowa, John F. (1988) &amp;quot;Using a lexicon of canonical graphs in a semantic interpreter,&amp;quot; in M. Evens, ed., Relational Models of the Lexicon, Cambridge University Press, pp. 73-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John F Sowa</author>
</authors>
<title>Towards the expressive power of natural language,&amp;quot;</title>
<date>1991</date>
<booktitle>Principles of Semantic Networks: Explorations in the Representation of Knowledge,</booktitle>
<pages>157--189</pages>
<editor>in J. F. Sowa, ed.,</editor>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Mateo, CA,</location>
<contexts>
<context position="26879" citStr="Sowa 1991" startWordPosition="4184" endWordPosition="4185">ke them precise, and lead the discussion to a deeper level of detail. This paper uses the theory of conceptual graphs: a system of logic with a graph notation designed for a direct mapping to and from the semantic structures of natural language. They have a graph structure based on the semantic networks of Al and C. S. Peirce&apos;s existential graphs, which form a complete system of logic. They are as formal and expressive as Montague&apos;s intensional logic, but they permit a broader range of operations on the formalism. The theory has been presented in book form (Sowa 1984) and in a recent summary (Sowa 1991); see those sources for more detail. An earlier paper on lexical issues with conceptual graphs (Sowa 1988) did not discuss logic explicitly. This section will give some examples to illustrate the kind of logical structures that are needed in the lexicon. A basic feature of conceptual graph theory is the use of canonical graphs to represent the expected roles associated with each concept type. Canonical graphs are similar to the case frames used in many systems, but they are richer in the kinds of structures and logical 45 operators they support. As an example, Figure 1 shows a canonical graph </context>
</contexts>
<marker>Sowa, 1991</marker>
<rawString>Sowa, John F. (1991) &amp;quot;Towards the expressive power of natural language,&amp;quot; in J. F. Sowa, ed., Principles of Semantic Networks: Explorations in the Representation of Knowledge, Morgan Kaufmann Publishers, San Mateo, CA, pp. 157-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eileen C Way</author>
</authors>
<title>Dynamic Type Hierarchies,</title>
<date>1991</date>
<tech>Personal communication.</tech>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Wilks, Yorick</location>
<contexts>
<context position="39910" citStr="Way (1991)" startWordPosition="6291" endWordPosition="6292">hen joining graphs; and the ability to do 2-expansion and contraction of types. Garner and Tsui (1988) implemented an inference engine that used heuristics and semantic distance measures to guide the proofs. They demonstrated that it could handle the kinds of scripts processed by the Schankian systems, but with a more robust, formally defmed representation. Hartley and Coombs (1991) showed how conceptual graphs could be used in abductive reasoning for generating models that satisfied given constraints. Interpreting nonliteral language is an application where background knowledge is essential. Way (1991) presented a book-length study of metaphor using conceptual graphs. According to her hypothesis, the purpose of a good metaphor is to refine the concept hierarchy by creating a new type. As a result of interpreting a metaphor, a word is generalind to a concept type that includes the original meaning plus a more abstract meaning that can be transferred to a new domain. Her approach takes advantage of the operations for joining and projecting graphs and the 2-expressions for defining new concept types. Metonymy is another kind of nonliteral language that requires detailed semantic structures in </context>
</contexts>
<marker>Way, 1991</marker>
<rawString>Way, Eileen C. (1991) Dynamic Type Hierarchies, Kluwer Academic Publishers. Wilks, Yorick A. (1991) Personal communication.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>