<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015657">
<title confidence="0.90188">
On the Mathematical Properties of Linguistic Theories
</title>
<author confidence="0.850884">
C. Raymond Perrault
</author>
<affiliation confidence="0.935051666666667">
Dept. of Computer Science
University of Toronto
Toronto, Ontario, Canada M5S 1A4
</affiliation>
<sectionHeader confidence="0.732369" genericHeader="abstract">
AlISTRACT
</sectionHeader>
<bodyText confidence="0.9998475">
Meta-theoretical results on the decidability, genera-
tive capacity, and recognition complexity of several syn-
tactic theories are surveyed. These include context-free
grammars, transformational grammars, lexical func-
tional grammars, generalized phrase structure gram-
mars, and tree adjunct grammars.
</bodyText>
<sectionHeader confidence="0.727823" genericHeader="keywords">
1. introduction.
</sectionHeader>
<bodyText confidence="0.9999634">
The development of new formalisms in which to
express linguistic theories has been accompanied, at
least since Chomsky and Miller&apos;s early work on context-
free languages, by the study of their meta-theory. In par-
ticular, numerous results on the ciecidability, generative
capacity, and more recently the complexity of recogni-
tion of these formalisms have been published (and
rumoured!). Strangely enough, much less attention
seems to have been devoted to a discussion of the
significance of these mathematical results. As a prelim-
inary to the panel on formal properties which will address
the significance issue, it seemed appropriate to survey
the existing results. Such is the modest goal of this
paper.
We will consider context-free Languages, transforma-
tional grammars, lexical functional grammars, general-
ized phrase structure grammars, and tree adjunct gram-
mars. Although we will not examine them here, formal
studies of other syntactic theories have been under-
taken: e.g. Warren [51] for Montague&apos;s PTQ 30], and Bor-
gida 1_71 for the stratificational grammars of Lamb ;251.
There follows a brief summary of some comments in the
literature about related empirical issues, but we avoid
entirely the issue of whether one theory is more descrip-
tively adequate than another.
</bodyText>
<sectionHeader confidence="0.976458" genericHeader="introduction">
2. Preliminary Definitions
</sectionHeader>
<bodyText confidence="0.999431391304348">
We assume the reader is familiar with the basic
definitions of regular, context-free (CF), context-sensitive
(CS), recursive, and recursively enumerable (re.)
languages and with their acceptors as can be found in
141
Some elementary definitions from complexity theory
may be useful. Further details may be found in :2].
Complexity theory is the study of the resources required
of algorithms, usually space and time. Let fez) be a func-
tion, say the recognition function for a language L The
most interest-Ig results we could obtain about f would be
a lower bound on the resources needed to compute f on a
machine of a given architecture, say a von Neumann
---------
This research was sponsored by the National Science and
Engineering Research Council of Canada under Grant
A921&apos;.5.
computer or a parallel array of neurons. These results
over whole classes of machines are very difficult to
obtain, and none of any significance exist for parsing
problems.
Restricting ourselves to a specific machine model
and an algorithm M for f, we can ask about the cost (e.g
time or space) c(x) of executing M on a .specific input x.
Typically c is too fine-grained to be useful: what one stu-
dies Instead is a function cw whose argument is an
integer it denoting the size of the input to M. and which
gives some measure of the cost of processing inputs of
length it. Complexity theorists have been most interested
in the asymptotic behaviour of cw, i.e. the behaviour of
cw as it gets Large.
If one is interested in tipper bounds on the behavioi:-
of M. one usually defines cw(n) as the rruzrimum of cez,
over all inputs x of size n.-This is called the worst-case
complexity function for M. Notice that other definition:
are possible: one could. define the expected complexity
function c(n) for 14 as the average of c(z) over all
of Length it. c. might be more useful than cw if one had
an idea of what the distribution of inputs to M could be.
Unfortunately, the introduction of probabilistic con-
siderations makes the study of expected complexity
technically more difficult that of worst case complexity
For a given problem, expected and worst case measures
may be quite different.
it is quite difficult to get detailed descriptions of cw
and for many purposes a cruder estimate is sufficient.
The next abstraction involves &amp;quot;Lumping&amp;quot; classes of cw
functions into simpler ones that more clearly demon-
strate their asymptotic behaviour and are easier to mani-
pulate. This is the purpose of 0-notation. Let f(rt) and
g(n) be two functions. f is said to be 0(g) if a constant
multiple of g is an upper bound for!, for all but, a finite
number of values of it. More precisely, f is 0(g) if there ,s
are constants K and no such that for all n&gt;no f(n) &lt;
107(n).
Given an algorithm M, we will say that its worst-case
time complexity is 0(g) if the worst-case time cost func-
tion cu(n) for M is 0(g). Notice that this merely says that
almost all riputs to M of size it can be processed in time
at most a constant times g(mn). It does not say that all
inputs require g(n) time, or even that any do even on M,
let alone on any other machine that implements f. Also.
if two algorithms A, and A2 are available for a function f,
and if their worst-case complexity can be given respec-
tively as O(g1) and 0(g). and g1 &lt; g2. it may stall.be the
case that for a large number of cases (maybe even for all
cases one is likely to encounter in practice) that A2 will
be the preferable algorithm, simply because the constant
K1 for gi may be much smaller than K2 for g 2.
</bodyText>
<page confidence="0.984305">
98
</page>
<bodyText confidence="0.999910704545455">
In examining known results about the recognition
complexity of various theories, it is useful to consider
how &amp;quot;robust&amp;quot; they are in the face of changes in the
machine model from which they were derived. These
models can be divided into two classes: sequential models
and parallel models. Sequential models [2] include the
familiar single- and multi-tape Turing Machines (TMs) as
well as Random Access Machines (RAMs) and Random
Access Stored Program. Machines (RASPs). A RAM is like a
TM except that its working menory is random access
rather than sequential. A RASP is like a RAM but stores
its program in its memory. Of all these models, it is most
Like a von Neumann computer.
All these sequential models can simulate each other
in ways that do not cause great changes in time complex-
ity. For example, a k-tape Turing Machine that runs in
time 0(t) can be simulated by a RAM in time 0(t). and
conversely, a RAM running in 0(t) can be simulated by a
k-tape TM in time 0(t2). In fact, all familiar sequential
models are polynomially related: they can simulate each
other with at most a polynomial loss in efficiency.
Thus if a syntactic model is known to have a difficult
recognition problem on one sequential model, then it will
not have a much easier one on another.
Transforming a sequential algorithm to one on a
parallel machine with a fixed number Kof processors pro-
vides at most a factor K improvement in speed. More
interesting results are obtained when the number of pro-
cessors is allowed to grow with the size of the problem,
e.g. with the Length of the string to be parsed. If we view
these processors as connected together in a circuit, with
inputs values entering at one end and outputs being pro-
duced at the other, then a problem that has a solution on
a sequential machine in polynomial time and in space s
will have a solution on a parallel machine with a polyno-
mial number of processors and circuit depth (or max-
imum number of processors data must be passed through
from input to output) 0(r). Since the depth of a parallel
circuit corresponds to the (parallel) time required to
complete the computation, this means that algorithms
with sequential solutions requiring small space (such as
deterministic CSLs) have fast parallel solutions. For a
comprehensive survey of parallel computation, see
Cook[9].
</bodyText>
<sectionHeader confidence="0.952229" genericHeader="method">
3. Context-Free Languages.
</sectionHeader>
<bodyText confidence="0.999920290322581">
Recognition techniques for context-free languages
are well-known 7,31. The so-called &amp;quot;CKY&apos; or &amp;quot;dynamic pro-
gramming&amp;quot; method is attributed by Hays [151 to J. Cocke,
and it was discovered independently by Kasami 54] and
Younger :531 who showed it to be 0(n.j). It requires the
grammar to be in Chomsky Normal Form, and putting an
arbitrary grammar in CNF may square the size of the
grammar.
Earley&apos;s algorithm recognizes strings in arbitrary
CFCs in time 0(n3) and space 0(n2), and in time 0(n2) for
unambiguous CFGs. Graham, Harrison and Ruzzo 1131
give an algorithm that unifies CKY and Earley&apos;s &apos;L101 algo-
rithm, and discuss implementation details.
Valiant [50] showed how to interpret the CKY algo-
rithm as the finding of the transitive closure of a matrix
and thus reduced CF recognition to matrix multiplica-
tion, for which sub-cubic algorithms exist. Because of
the enormous constants of proportionality associated
with this method, it is not likely to be of much practical
use, either an implementation method or as a descrip-
tion of the function of the brain.
Ruzzo [55] has shown how CFLs can be recognized by
boolean circuits of depth 0(log(n)2), and thus that paral-
lel recognition can be done in time 0(log(n)2). The
required circuit has size polynomial inn.
So as not to get mystified by the upper bounds on CP
recognition, it is useful to remember that no known CFL
requires more than linear time, nor is there a (non-
constructive) proof of the existence of such a
For an empirical comparison of various parsing
methods, see Slocum [441.
</bodyText>
<sectionHeader confidence="0.934702" genericHeader="method">
4. Transformational Grammar.
</sectionHeader>
<bodyText confidence="0.999910724137931">
From its earliest days, discussions of transforma-
tional grammar (TG) have included mention of matters
computational.
Peters and Ritchie [33] provided the first non-trivial
results on the generative power of TOs. Their model
reflects the &amp;quot;Aspects&amp;quot; version quite closely, including
transformations that could move and add constituents,
and delete them subject to recoverability. All transforma-
tions are obligatory, and applied cyclically from the bot-
tom up. They show that every recursively enumerable
(re.) set can be generated by a TG using a context-
sensitive base. The proof is quite simple: the right-hand
sides of the type-0 rules that genet-ate the re. set are
padded with a new &amp;quot;blank&amp;quot; symbol to make them at least
as long as their left-hand sides. Rules are added to allow
the blank symbols to commute with all others. These
context-sensitive rules are then used as the base of a TO
whose only transformation deletes the blank symbols.
Thus if the transformational formalism itself is sup-
posed to characterize the grammatical strings of possible
natural languages, then the only languages being
excluded are those which are not enumerable under any
model of computation.
At the expense of a considerably more intricate
argument, the previous result can be strengthened ,:321
to show that every re. set can be generated by a
context-free based TG, as long as a filter (intersection
with a regular set) can be applied to the phrase-markers
output by the transformations. In fact, the base gram-
mar can be independent of the language being generated.
The proof involves simulating a TM by a TG. The transfor-
mations first generate an &amp;quot;input tape&amp;quot; for the TM being
simulated, and then apply the TM productions, one per
cycle of the grammar. The filter insures that the base
grammar generated just as many S nodes as necessary to
generate the input string and do the simulation. Again, if
the transformational formalism is supposed to character-
ize the possible natural languages, then the Universal
Base Hypothesis [31] according to which all natural
languages can be generated from the same base gram-
mar is empirically vacuous: any recursively enumerable
language can.
Several attempts were then made to find a restricted
form of the transformational model that was descrip-
tively adequate and yet whose generated languages are
recursive (see e.g. [27]). Since a key part of the proof in
321 involves the use of a filter on the final derivation
trees, Peters and Ritchie examined the consequences of
forbidding final filtering [35]. They show that if 5&apos; is the
only recursive symbol in the CF base then the generated
language L is predictably enurn.erable and exponentially
bounded. A language L is predictably enumerable if there
is an &amp;quot;easily&amp;quot; computable function t(n) that gives an
upper bound on the number of tape squares needed by its
enumerating TM to enumerate the first n elements of L.
L is exponentially bounded if there is a constant K such
that for every string x in L there is another string x&apos; in L
whose length is at most K times the length of z.
</bodyText>
<page confidence="0.992762">
99
</page>
<bodyText confidence="0.99517808974359">
The class of non-filtering languages is quite unusual,
including all the CFLs (obviously), but also some (but not
all) CSLs, some (but not all) recursive languages, and
some (but not all) r.e. languages.
The source of non-recursivity in transformationally
generated languages is that transformations can delete
arbitrarily large parts of the tree, thus producing surface
trees arbitrarily smaller than the deep structure trees
they were derived from. This is what Chomsky&apos;s recover-
ability of deletions condition was meant to avoid. In his
thesis, Petrick [36] defines the following terminal-
length-increasing condition on transformational deriva-
tions: consider the following two p-markers from a
derivation, where the right one is derived from the left
one by applying the cycle of transformations to subtree c
producing the subtree u.
a&gt;.
cycle I
Continuing the derivation., apply the cycle to tree t yield-
ing tree u.
.&gt;•
cycle 2
A derivation satisfies the terminal-length-increasing con-
dition it the yield of iL is always longer than the yield of
Petrick shows that if all recursion in the base
&amp;quot;passes through 5&amp;quot; and if all derivations satisfy the
terminal-length-increasing condition, then the generated
language is recursive. Using a slightly more restricted
model of transformations, Rounds [42] strengthens this
result by showing that the resulting languages are in fact
context-sensitive.
In an unpublished paper. Myhill shows that if the
condition is weakened to terminal-length-non-decreasing,
then the resulting languages can be recognized in space
at most exprrn.entia/ in the length of the input. This
implies that the recognition can be done in at most
double-exponential time, but Rounds :43] shows that not
only can recognition be done in exponential time, but
that every language recognizable in exponential time can
be generated by a TO satisfying the terminal-length-non-
decreasing condition and recoverability of deletions.
This is a very strong result, because of the closure
properties of the class of exponential-time languages. To
see why this is so requires a few more definitions.
Let P be the class of all languages that can be recog-
razed in polynomial time on a deterministic TM, and NP
the class of all Languages that can be recognized in poly-
nomial time on a non-deterministic TM. P is obviously
contained in NP, but the converse is not known, although
there is much evidence that is false.
There is a class of problems. the so-called NP-
complete problems. which are in NP and &amp;quot;as difficult&amp;quot; as
any prr,blem in NY&apos; in the following sense if any of them
could be shown to be in P, then ail the problems in NP
would also be in P. One way to show that a language L is
NP-complete is to show that L is in NP and that every
other language 1,, in NP can be polynomially transformed
into L, i.e. that there is a deterministic TM, operating in
polynomial time., that will transform an input /I, to L into
an input wo to Lo such that w is in L if and only wo is in
Lo. In practice, to show that a language is NP-complete,
one shows that it is in NP, and that some already-known
NP-complete language can be polynomially transformed
to it.
All the known NP-complete languages can be recog-
nized in exponential time on a deterministic machine,
and none are known to have sub-exponential solutions.
Thus sinee the restricted transformational languages of
Rounds characterize the exponential languages. then if
all of them were to be in P, then P would be equal to NP.
Putting it another way, if P is not equal to NP, then some
transformational languages (even those satisfying the
terminal-length-non-increasing condition) have n
&amp;quot;tractable&amp;quot; (i.e. polynomial time) recognition pro:.
on any deterministic TM. Note that this result also holds
for all the other known sequential models of computa-
tion, and even for parallel machines with as many as a
polynomial number of processors.
</bodyText>
<sectionHeader confidence="0.94904" genericHeader="method">
5. Lexical Functional Grammar.
</sectionHeader>
<bodyText confidence="0.999985675675676">
In part, transformational grammar seeks to account
for a range of constraints or dependencies within sen-
tences. Of particular interest are subcategorization
dependencies and predicate-argument dependencies.
These dependencies can hold over arbitrarily large dis-
tances. Several recent theories suggest different ways of
accounting for these dependencies, but without making
use of transformations. We will examine three of these,
Lexical Functional Grammar, Generalized Phrase Struc-
ture Grammar. and Tree Adjunct Grammars, in tne next
few sections.
Lexical Functional Grammar (LEG) of Kaplan and
Bresnan [241 aims to provide a descriptively adequate
syntactic formalism without transformations. All the
work done by transformations is instead encoded in
structures in the lexicon and in links established
between nodes in the constituent structure
LEG languages are CS and properly include the CFLs
:241 Berwick :5] shows that a set of strings whose recog-
nition problem is known to be NP-complete, namely the
set of satisfiable boolean formulas, is an LEG language.
Therefore, as was the case for Rounds&apos;s restricted class of
TGs, if P is not equal to NP, then some languages gen-
erated by LFGs do not have polynomial time recognition
algorithms. Indeed only quite &amp;quot;basic&amp;quot; parts of the LEG
mechanism are necessary to the reduction. This
includes mechanisms necessary for feature agreement,
for forcing verbs to take certain cases, and lexical ambi-
guity. Thus no simple change to the formalism is likely
to avoid the combinatorial consequences of the full
mechanism.
Berwick has also examined the relation between LEG
and the class of languages generated by indexed gram-
mars [11, a class known to be a proper subset of the CSLs,
but including some NP-complete languages :42]. He
claims (personal communication) that the indexed
languages are a proper subse of the LEG languages.
</bodyText>
<sectionHeader confidence="0.988279" genericHeader="method">
6. Generalized Phrase Structure Grammar.
</sectionHeader>
<bodyText confidence="0.991725">
In a series of papers, Gerald Gazdar and his col-
leagues I] have argued for a joint account of the syntax
and semantics of English like LEG in eschewing the use of
transformations but unlike it in positing only one level of
</bodyText>
<page confidence="0.964543">
100
</page>
<bodyText confidence="0.9919845">
syntactic description. The syntactic apparatus is based
on a non-standard interpretation of phrase-structure
rules and on the use of meta-rules. The formal conse-
quences of both these moves have been investigated.
</bodyText>
<subsectionHeader confidence="0.975734">
6.1. Node Admissibility
</subsectionHeader>
<bodyText confidence="0.985198108695652">
There are two ways of interpreting the function of CF
rules. The first, and most usual, is as rules for rewriting
strings. Derivation trees can then be seen as canonical
representatives of classes of derivations producing the
s?rne string, and differing only in the order of application
o!&apos; the same productions.
The second interpretation of CF rules is as con-
straints on derivation trees: a legal derivation tree is
r..rie where each node is &amp;quot;admitted&amp;quot; by a rule, i.e. each
node dominates a sequence of nodes in a way sanctioned
by a rule. For CF rules, the two interpretations obviously
generate the same strings aiid the same set of trees.
Following a suggestion of McCawley&apos;s. Peters and
Ritchie [341 showed that if one considered context-
sensitive rules from the node-admissibility point of view,
the languages defined were still CF. Thus the use of CS
rules in the base to impose sub-categorization restric-
tions. for example, does not increase the weak generative
capacity of the base component. (For some different res-
trictions of context-sensitive rules that guarantee that
only CFLs will be generated. see Baker [41.)
Rounds [40] gives a simpler proof of Peters and
Ritchie&apos;s node-admissibility result using the techniques
from tree-automata theory, a generalization to trees of
finite state automata theory for strings. Just as a finite
state automaton (FSA) accepts a string by reading it one
character at a time, changing its state at each transi-
tion, a finite state tree automaton (Fh-IA) traverses trees,
propagating states. The top-down FSTA &amp;quot;attaches&amp;quot; a start-
ing state (from a finite set) to the root of the tree. Tran-
sitions are allowed by productions of the form
(q, a, it) --&gt; (q1,..„qn)
such that if state q is being applied to a node Labelled a
and dominating n descendants, then state gi should be
applied to its tth descendant. Acceptance occurs if all
leaves of the tree end up labelled with states in the
accepting subset. The bottom-up FM is similar: start-
ing stE.tes are attached to the leaves of the tree and the
productions are of the form
(a, n, q q
indicating that if a node labelled a dominating n descen-
dants each labelled with states q to gn, then node a gets
labelled with state q. Acceptance occurs when the root is
labelled by a state from the subset of accepting states.
As is the case with PSAs, 1•&amp;quot;SIAs of both flavours can
be either deterministic or non-deterministic. A set of
trees k said to be recognizable if it is accepted by a non-
deterministic bottom-up 1.1A. Again as with FSA.s. any
set of trees accepted by a non-deterministic bottom-up
F5TA I.i accepted by a deterministic bottom-up FSTA, but
the re:eitt does not hold for top-down A. although the
recognizable sets arc exactly the Languages retognized
by non-deterministic top-down FSTAs.
A set of trees is local if it is the set of derivation
trees of a CF grammar Clearly, every local set !s recog-
nizablr! by a one-state bottom-up FSTA that checks at
each node that it satisfies a CF production. Also, the
yield or a recognizable set of trees (the set of strings it
generetes) is CF. Although not all recognizable sets are
local, hey can all be mapped into local sets by a simple
(homomorphic) mapping.
Rounds&apos;s proof 41] that CS rules under node-
admissibility generate only CFLs involves showing that
the set of trees accepted by the rules is recognizable,
i.e. that there is a non-deterministic bottom-up FSrA that
can check at each node that some node-admissibility
condition holds there. This requires checking that the
&amp;quot;strictly context-free&amp;quot; part of the rule holds, and that
some proper analysis of the tree passing
node satisfies the &amp;quot;context-sensitive&amp;quot; part of the rule.
The difficulty comes from the fact that the bottom-
up automaton cannot generate the set of proper ana-
lyses, but must instead propagate (in its state set) the
proper analysis conditions necessary to &amp;quot;admit&amp;quot; the
nodes of its subtrees. It must, of course, also check that
those rules get satisfied.
A more intuitive proof using tree transducers as well
as }1AS s sketched inthe Appendix.
Joshi and Levy [21] strengthened Peters and
Ritchie&apos;s result by showing that the node admissibility
conditions could also include arbitrary Boolean combina-
tions of dominance conditions: a node could specify a
bounded set of labels that must occur immediately above
it along a path to the root, or immediately below it on a
path to the frontier.
In general the CF grammars constructed in •the
proof of weak equivalence to the CS grammars under
node admissibility are much larger than the original, and
not useful for practical recognition. Joshi, Levy and Yueh
[221, however, show how Earley&apos;s algorithm can be
extended to a parser that uses the local constraints
directly.
</bodyText>
<subsectionHeader confidence="0.805448">
8.2. Metarules.
</subsectionHeader>
<bodyText confidence="0.98480525">
The second important mechanism used by Gazdar
[11] is rnetarules. or rules that apply to rules to produce
other rules. Using standard notation for CF rules, one
example of a metarule that could replace the transforma-
tion known as &amp;quot;particle movement&amp;quot; is:
V--&gt; V N Pt X ==&gt; V--&gt; VPt IV[-PRO] X
X here is a variable behaving like variables in structural
analyses of transformations. If such variables are res-
tricted to being used as abbreviations, that is if they are
only allowed to range over a finite subset of strings over
the vocabulary, then closing the grammar under the
metarules produces only a finite set of derived rules, and
thus the generative power of the formalism is not
increased. If, on the other hand, X is allowed to range
over strings of unbounded length, as are the essential
variables of transformational theory, then the conse-
quences are less clear. It is well known, for example, that
if the right-hand sides of phrase structure rules are
allowed to be arbitrary regular expressions, then the gen-
erated languages are still context-free. Might something
like this not be happening with essential variables in
metarules? It turns out not.
The formal consequences of the presence of essen-
tial. variables in metarules depends on the presence of
another device, the so-called phantom categories. It may
be convenient in formulating rnetarules to allow, in the
left-hand sides of rules, occurrences of syntactic
categories that are never introduced by the grammar,
i.e. that never appear in the right-hand sides of rules. In
standard CFLs, these are called &apos;useless categories, and
rules containing them can simply be dropped, with no
change in generative capacity. Not so with metarules: it
is possible for metarules to rewrite rules containing
phantom categories into rules without them. Such a dev-
ice was proposed at one time as a way to implement pas-
sives in the GPSG framework.
</bodyText>
<page confidence="0.997844">
101
</page>
<bodyText confidence="0.999520909090909">
Uszkoreit and Peters [49] have shown that essential
variables in metarules are powerful devices indeed: CF
grammars ,with metarules that use at most one essential
variable and allow phantom categories can generate all
recursively enumerable sets. Even if phantom categories
are banned, as long as the use of at least one essential
variables is allowed, then some non-recursive sets can be
generated.
Possible restrictions on the use of metarules are
suggested in Gazdar and Pullum [12]. ,Shieber et al.[45]
discuss some empirical consequences of these moves.
</bodyText>
<sectionHeader confidence="0.912283" genericHeader="method">
7. Tree Adjunct Grammar
</sectionHeader>
<bodyText confidence="0.999329828571429">
The Tree Adjunct Grammars (TAGs) of Joshi and his
colleagues presents a different way of accounting for syn-
tactic dependencies ([17], [19]). A TAG consists of two
(finite) sets of (finite) trees, the centre trees and the
adjunct trees.
The centre trees correspond to the surface struc-
tures of the &amp;quot;kernel&amp;quot; sentences of the languages. The
root of the adjunct trees is labelled with a non-terminal
symbol which also appears exactly once on the frontier of
the tree. All other frontier nodes are labelled with termi-
nal symbols. Derivations in TAGs are defined by repeated
application of the operation of adjunction. If c is a centre
tree containing an occurrence of a non-terminal A. and if
a is an adjunct tree whose root (and one node n on the
frontier) is labelled A. then the adjunction of a to c is per-
formed by &amp;quot;detaching&amp;quot; from c the subtree t rooted at A.
attaching a in its place, and reattaching t at node it.
Adjunction may then be seen as a tree analogue of a
context-free derivation for strings [40]. The string
languagea obtained by taking the yields of the tree
languages generated by TAGs are called Tree Adjunct
languages, or TALs.
In TAGs all long-distance dependencies are the result
of adjunctions separating nodes that at one point in the
derivation were &amp;quot;close&amp;quot;. Both crossing and non-crossing
dependencies can be represented [18]. The formai pro-
perties of TAGs are fully discussed in [20]. [52], [23]. Of
particular interest are the following.
TALs properly contain the CFLs and are properly con-
tained in the indexed languages, which in turn are prop-
erly contained in the Gas. Although the indexed
languages contain NP-complete languages, TALs are
much better behaved: Joshi and Yokomori report [per-
sonal communication] an 0(n4) recognition algorithm
and conjecture that an 0(n3) bound may be possible.
</bodyText>
<subsectionHeader confidence="0.900319">
B. A Pointer to Empirical Discussions
</subsectionHeader>
<bodyText confidence="0.998874307692308">
The literature on the empirical issues underlying
the formal results reported here is not extensive.
Chomsky argues convincingly (81 that there is no
argument for natural languages necesscry-ity being recur-
sive. This, or course, is different from the possibility that
languages are contingently recursive. Putnam 39] gives
three reasons he claims &amp;quot;point in this direction&amp;quot;: (1)
-speakers can presumably classify sentences as accept-
able or unacceptable, deviant or non-deviant, at cetera,
without reliance on extra-linguistic contexts. There are
of course exceptions to this rule &apos;&apos;, (2) grammaticality
judgements can be made for nonsense sentences, and (3)
grammars can be learned. (2) and (3) are irrelevant and
(1) contains its own counter-argument.
Peters and Ritchie [33] contains a suggestive but
hardly open-and-shut case for contingent recursivity (1)
every TG has an exponentially bounded cycling function,
and thus generates only recursive languages, (2) every
natural language has a descriptively adequate TG, and (3)
the complexity of languages investigated so far is typical
of the class.
Hintikka[16] presents a very different argument
against the recursivity of English based on the distribu-
tion of the words any and every. His account of why John
knows everything is grammatical while John knows any-
thing is not is that any can appear only in contexts where
replacing it by every changes the meaning. Taking mean-
ing to be logical equivalence, this means that grammati-
cality is dependent on the determination of logical
equivalence of logical formulas, an undecidable problem.
Chomsky [8] argues that a simpler solution is available,
namely one that replaces logical equivalence by syntac-
tic identity of some kind of logical form.
Pullum and Gazdar [38] is a thorough survey of, and
argument against, published claims (mainly the &amp;quot;respec-
tively&amp;quot; examples (26], Dutch cross-serial dependencies,
and nominalization in Mohawk [37]) that some natural
Languages cannot be weakly generated by CF grammars.
No claims are made about the strong adequacy of CFGs.
</bodyText>
<sectionHeader confidence="0.546345" genericHeader="method">
9. Seeking Significance.
</sectionHeader>
<bodyText confidence="0.999740574468085">
When can the supporter of a weak (syntactic) formal-
ism (i.e. low recognition complexity, low generative capa-
city) claim that it superior to a competing more powerful
formalism?
Linguistic theories can differ along several dimen-
sions, with generative capacity and recognition capacity
being only two (albeit related) ones. The evaluation must
take into consideration at least the following others:
Coverage. Do the theories make the same grammat-
ical predictions?
Extensibility. The linguistic theory of which the syn-
tactic theory is a part will want to express well-
tormedness constraints other than syntactic ones These
constraints may be expressed over syntactic representa-
tions, or over different representations, presumably
related to the syntactic ones. One theory may make this
connection possible when another does not. This of
course underlies the arguments for strong descriptive
adequacy.
Also relevant here is how the linguistic theory as a
whole is decomposed. The syntactic theory can obviously
be made simpler by transferring some of the explanatory
burden to another constituent. The classic example in
programming languages is the constraint that all vari-
ables must be declared before they are used. This con-
straint cannot be imposed by a CFG but can be by an
indexed grammar, at the cost of a dramatic increase in
recognition complexity. Typically, however, the require-
ment is simply not considered part of &apos;syntax&amp;quot;, which
thus remains CF, and imposed separately. In this case,
the overall recognition complexity remains some low-
order polynomial, Sortie arguments of this kind can be
found in (36]
Separating the constraints into different sub-
theories will riot in general make the problem of recog-
nizing strings that satisfy all the constraints any more
efficient, but it may allow limiting the power of each con-
stituent. To take an extreme example, every r.e. set 3
the homomorphic image of the intersection of to.)
context-free Languages.
Implementation. This is probably the most subtle set
of issues determining the significance of the form,I
results, and I don&apos;t claim to understand them.
Comparison between theories requires agreement
between the machine models used to derive the complex-
ity results. As mentioned above, the sequential models
are all polynornially related, and no problem not having a
</bodyText>
<page confidence="0.997539">
102
</page>
<bodyText confidence="0.999978983870968">
polynomial time solution on a sequential machine is
likely to have one on a parallel machine limited to at
most a polynomial number of processors, at least if P is
not equal to NP. Both these results restrict the improve-
ment one can obtain by changing implementation. but
are of little use in comparing algorithms of Low complex-
ity. Berwick and Weinberg [6] give examples of how algo-
rithms of low complexity may have different implementa-
tions differing by large constant factors. In particular,
changes in the form of the grammar and in its represen-
tation may have this effect.
But of more interest I believe is the fact that imple-
mentation is often accompanied by some form of
resource limitation that has two effects. First it is also a
change in specification. A context-free parser imple-
mented with a bounded stack recognizes only a finite-
state Language.
Second. very special implementations can be used if
one is willing to restrict the size of the problem. to be
solved, or even use special-purpose methods for limited
problems. Marcus&apos;s parser [28] with its bounded look-
ahead is another good example. Sentences parsable
within the allowed look-ahead have &amp;quot;quick&amp;quot; parses, but
some grammatical sentences, such as &amp;quot;garden path&amp;quot; sen-
tences cannot be recognized without an extension to the
mechanism that would distort the complexity measures.
There is obviously much more of this story to be
told. Allow me to speculate as to how it might go. We may
end up with a space of linguistic theories, differing in the
idealization of the data they assume, in the way they
decompose constraints, and in the procedural
specifications they postulate (I take it that two theories
may differ in that the second simply provides more detail
than the first as to how constraints specified by the first
are to be used.) Our observations, in particular our meas-
urements of necessary resources, are drawn from the
&amp;quot;ultimate implementation&amp;quot;, but this does not mean that
the &amp;quot;ultimately Low-level theory&amp;quot; is necessarily the most
informative, witness many examples in the physical sci-
ences, or that less procedural theories are not useful
stepping stones to more procedural ones.
It is also not clear that theories of different compu-
tational power may not be useful as descriptions of
different parts of the syntactic apparatus. For example,
it may be easier to learn statements of constraints
within the framework of a general machine. The con-
straints once learned might then be subjected to
transformation to produce more efficient special-purpose
processors also imposing resource limitations. Indeed,
the &amp;quot;possible languages&amp;quot; of the future may be more com-
plex than the present ones, just as earlier ones may have
been syntactically simpler. Were ancient languages reg-
ular?
Whatever we decide to make of existing formal
results, it is clear that continuing contact with the com-
plexity community is important. The driving problems
there are the P= NP question. the determination of lower
bounds, the study of time-space tradeoffs, and the com-
plexity of parallel computations. We still have some
methodological house-cleaning to do, but I don&apos;t see how
we can avoid being affected by the outcome of their
investigations.
</bodyText>
<sectionHeader confidence="0.984895" genericHeader="method">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.8616925">
Thanks to Bob Berwick, Aravind Joshi, Jim Hoover,
and Stan Peters for their suggestions.
</bodyText>
<sectionHeader confidence="0.980337" genericHeader="conclusions">
APPENDIX
</sectionHeader>
<bodyText confidence="0.994894111111111">
Rounds [41] proves that context-sensitive rules
under node-admissibility generate only context-free
languages by constructing a non-deterministic bottom-up
tree automaton to recognize the accepted trees. We
sketch here a proof that makes use of several determinis-
tic transducers instead.
FSTAs can be generalized so that instead of simply
accepting or rejecting trees, they transform them, by
adding constant trees, and deleting or duplicating sub-
trees. Such devices are called finite state tree transduc-
ers (ISM, and like the FSTA they can be top-down or
bottom-up. First motivated as models of syntax-directed
translations for compilers, they have been extensively
studied (e.g. [V], [48], [40]) but a simple subset is
sufficient here.
The idea is this. Let Tbe the set of trees accepted by
the CS-based grammar. Let t be in T. Mils can be used
to label each node it of t with the set of all proper ana-
lyses passing through n. It will then be simple to check
that each node satisfies one of the node admissibility
conditions by sweeping through the labelled tree with a
bottom-up FSTA.
The node labelling is done by two FblTs r, and r2. Let
in. be the maximum length of any left or right-context of
any node admissibility condition. Thus we need only label
nodes with sets of strings of length at most in, and over a
finite alphabet there are only a finite number of such
strings.
7-1 operates bottom-up on a tree t, and labels each
node it of t with three sets Prefiz(n), Suffix(n). and
Yield(n) of proper analyses: if P is the set of all proper
analyses of the subtree rooted at it, then Prefix(n) is the
set of all substrings of length at most m that are prefixes
of strings of P. Similarly, 5Uffix(n) is the set of all
suffixes of length at. most in, and Yield(n) is the set of all
strings of P of length at most in. It can easily be shown
that for any set of trees T T is recognizable if and only if
(T) is.
Applying to the output of r1, the second transducer
r2, operating top-down, labels each node it with all the
proper analyses going through it, i.e. with a pair of sets
of strings. The first set will contain all left-contexts of
node n and the second all right-contexts. -,-2 also
preserves recognizability. A bottom-up FSTA can now be
defined to check at each node that both the context-free
part of a rule as well as its context conditions are
satisfied.
This argument also extends easily to cover the domi-
nance predicates of Joshi and Levy: transducers can be
added to label each node with all its -top contexts&amp;quot; and
all its &amp;quot;bottom- contexts&amp;quot; The final FSTA must. then
check that the nodes satisfy whatever Boolean combina-
tion of dominance and proper analysis predicates are
required by the node admissibility rules.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.99983625">
[1] Aho IV., Indexed grammars: an extension of the
context-free grammars, [ACM 15, 647-671, 1968.
[2] Aho Hoperoft J.E. and Ullman J.D., The Design and
Analysis of Computer Algorithms, Addison-Wesley, Reading
Mass, 1974.
[3] Aho A.V., and Ullman J.D., The Theory of Parsing.
Translation, and Compiling, vol 1: Parsing. Prentice Hall,
Englewood Cliffs N.J., 1972.
</reference>
<page confidence="0.995012">
103
</page>
<reference confidence="0.9998074921875">
[4] Baker B.S., Arbitrary grammars generating context-
free languages, TR 11-72, Center for Research in Comput-
ing Technology, Harvard Univ., 1972.
[5] Berwick R. C. Computatiomil complexity and lexical
functiona/ grammar, 19th ACL. 1981.
[6] Berwick R. C. and Weinberg A., Parsing efficiency,
computational complexity, and the evaluation of gram-
matical theories, Ling. Inci. 13, 165-191. 1982.
[7] Eiorgida A. T., Formal Studies of Stratificational Gram-
mars. PhD Thesis, University of Toronto, 1977.
[8] Chomsky N., Rules and Representations. Columbia
University Press, 1980.
[9] Cook S. A., 7bwards a complexity theory of synchro-
nous parallel computation, L&apos;Enseignement
Mathematique 27, 99-124, 1981.
[10] Earley J., An efficient context-free parsing algo-
rithm, Comm. of ACM 13.2. 94-102, 1970.
[11] Gazdar G., Phrase structure grammar, in Jacobson P.
and Pullum G. (eds.), The Nature of Syntactic Represen-
tation. Reidel, 1982.
[12] Gazdar G. and Pullum G., Generalized Phrase Struc-
ture Grammar: A Theoretical Synopsis, Indiana Univ. Ling.
Club, 1982.
[13] Graham S. L., Harrison M. A., and Ruzzo W.L., An
improved context-free recognizer, ACM Trans. on Frog.
Lang. and Systems, 2, 3, 415-462, 1980.
[14] Hoperoft J.E. and Ullman J., Introduction to Auto-
mata Theory. Languages and Computation. Addison Wes-
ley, 1979.
[15] Hays D.G., Automatic language data processing, In
Computer Applications in the Behavioral Sciences, H.
Borko (ed.), Prentice Hall, Englewood Cliffs N.J., 1962.
[16] Hintikka. J.K.K., Quantifiers in natural languages:
some logical problems 2, Ling. and Phil., 153-172, 1977.
[17] Joshi A. K.. How much context-sensitivity is required
to provide reasonable Structural descriptions: tree
adjoining grammars, to appear in Dowty D., Karttunen L.
and Zwicky A. (eds.), Natural Language Processing:
Psycholinguistic, Computational and Theoretical Proper-
ties, Cambridge Univ. Press.
[18] Joshi A.K., Factoring recursio? -znd dependencies: an
aspect of Tree Adjoining Grammars and a comparison of
some forma/ properties of TAG&apos;s, GPSG&apos;s, PLC&apos;s and LFG&apos;s,
these Proceedings, 1983.
[19i Joshi A.K. and Levy L.S., Phrase structure trees bear
mare fruit than you would have thought, 18th ACL. 1980.
[20] Joshi A.K., Levy L.S. and Takahashi M., Tree adjunct
grammars. J. of Comp. and Sys. Sc. 10, 1, 136-163, 1975.
F211 Joshi A.K., Levy L.S., Constraints on structural
&apos;descriptions: local transformations, SIAM J. on Comput-
ing, :977.
[22] Joshi AK.. Levy L.S. and Yueh K., Local constraints on
programming languages, Part 1: Syntax, Th. Comp. Sc.
12. 265-290, 1980.
[23] Joshi A. K. and Yokomori T, Some characterization
theorems for tree adjunct languages and recognizable
sets, forthcoming.
124] Kaplan R. and Bresnan J., Lexical-F&apos;unctional Gram-
mar: a formal system for grammatical representation, in
BreFnan J (ed.), The Mental Representation of Grammati-
cal Relations, MIT Press, 1982.
[25] Lamb S., Outline of Stratificational Grammar, George-
town University Press, Washington. 1966.
[26] Langendoen DI., On the inadequacy of 7ype- 2 and
Type-3 grammars for human languages, in P.J. Hopper
(ed.) Studies in Historical Linguistics: festschrift for
Winfred P. Lehman. John Benjamin, Amsterdam, 156-171,
1977.
[27] LaPointe S., Recursiveness and deletion, Ling. Anal.
3, 227-285. 1976.
[28] Marcus M.P., A Theory of Syntactic Recognition for
Natural Language, MIT Press, 1980.
[29] Matthews R., Are the grammatical sentences of a
language a recursive set?, Synthese 40, 206-224, 1979.
[30] Montague R., The proper treatment of quantification
in ordinary English, in Hintikka, J., Moravcsik J., and
Suppes P.(eds.), Approaches to Natural Language, Reidel,
Dordrecht. 1973.
[31] Peters P.S. and Ritchie R.W., A note on the universal
base hypothesis. J. of Linguistics, 5, 150-2, 1969.
[32] Peters P.S. and Ritchie R.W., On restricting the base
component of transformational grammars, Ird. and Con-
trol, 18, 483-5-1, 1971.
[33] Peters P.S. and Ritchie RW., On the generative power
of transformational grammars, Inf. Sc. 6, 49-83, 1973.
[34] Peters P.S. and Ritchie R.W., Context-sensitive
immediate constituent analysis - context-free languages -
revisited, Math. Sys. Theory 6, 324-333, 1973.
[35] Peters P.S. and Ritchie R.W., Nan-filtering and local
filtering grammars, in J.K.K. Hintikka, J.M.E. Moravcsik,
and P. Suppes (eds.) Approaches to Natural Language,
Reidel, 180-194, 1973.
[36] Petrick S. R., A Recognition Procedure for Transfor-
mational Grammars. PhD Thesis, MIT, 1965.
[37] Postal P.M., Limitations of phrase-structure gram-
mars, in J.A. Fodor and J.J. Katz (eds.), The structure of
language: Readings in the philosophy of language. Engle- -
wood Cliffs: Prentice Hall, 137-151, 1964.
[38] Pullum G.K. and Gazdar G., Natural and context-free
languages, Ling. and Phil., 4, 471-504, 1982.
[39] Putnam H., Some issues in the theory of grammar, in
Proc. of Symposia in Applied Mathematics. American
Math. Soc., 1961.
[40] Rounds W. C., Mappings and grammars on trees,
Math. Sys. Th.,4,3, 257-287, 1970.
[41] Rounds W. C., Tree-oriented proofs of some theorems
on context-free and indexed languages, 2nd ACM Symp. on
Th. Comp. Sc., 109-116, 1970.
[42] Rounds W. C., Complexity of recognition in
intermediate-level languages, 14th Symp. on Sw. and Aut.
Th. 1973.
[43] Rounds W. C., A grammatical characterization of
exponential-time languages, 1EZP. Symp. on &apos;Found. cf
Comp. Sc., 135-143, 1975.
[44] Slocum J., A practical comparison of parsing stra-
tegies, 19th ACL, 1981.
[45] Shieber S.M., Stucky S. U., Uszkoreit H. and Robinson
J. J., frbrrhai constraints on metarutes, these Proceed-
ings, 1983.
[46] Thatcher J.W., Characterising derivation trees of
context-free grammars through a generalization of finite
automata theory, J. of Comp. and Sys. Sc. 1. 317-322.
197]67
[4 Thatcher J.W., Gertara/ized2 sequential machine
mnps. J. of Comp. and Sys. Sc. 4, 339-67, 1.970
[48] Thatcher J W., Tree automata: an informal survey, in
A. Aho (ed.), Currents in the theory of computing. Pren-
tice Hail, 143-172, 1973.
</reference>
<page confidence="0.98352">
104
</page>
<reference confidence="0.999905142857143">
[49] Uszkoreit H. and Peters P. S asential variables in
rnetarules, forthcoming.
[50] Valiant L. General context-free recognition in less
than cubic time, J. of Comp. and Sp. Sc. 10, 308-315,
1975.
[51] Warren D. S., Syntax and Semantics of Parsing: An
Application to Montague Grammar. PhD Thesis, University
of Michigan, 1979.
[52] Yokomori T. and Joshi A. K. .mi-linearity, Parikh-
boundedness and tree adjunct languages, to appear in Inf.
Pr. Letters, 1983.
[53] Younger D. H. Recognition and parsing of context-
free languages in time nu, Inf. and Control, 10, 2, 189-208,
1967.
[54] Kasarni T., An efficient recognition and syntax algo-
rithm for context-free languages, Air Force Cambridge
Research Laboratory report AF-CRL-65-758, Bedford MA,
1965.
[55] Ruzzo W. L, On uniform circuit comp/ezity (extended
abstract), Proc. of 20th Annual Symp. on Found. of Coro.
Sc., 312-318, 1979.
</reference>
<page confidence="0.999017">
105
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.680482">
<title confidence="0.99999">On the Mathematical Properties of Linguistic Theories</title>
<author confidence="0.982404">Raymond</author>
<affiliation confidence="0.9999685">Dept. of Computer Science University of Toronto</affiliation>
<address confidence="0.999842">Toronto, Ontario, Canada M5S 1A4</address>
<abstract confidence="0.950397571428571">AlISTRACT Meta-theoretical results on the decidability, generative capacity, and recognition complexity of several syntactic theories are surveyed. These include context-free grammars, transformational grammars, lexical functional grammars, generalized phrase structure grammars, and tree adjunct grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aho</author>
</authors>
<title>Indexed grammars: an extension of the context-free grammars,</title>
<date>1968</date>
<journal>ACM</journal>
<volume>15</volume>
<pages>647--671</pages>
<marker>[1]</marker>
<rawString>Aho IV., Indexed grammars: an extension of the context-free grammars, [ACM 15, 647-671, 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aho Hoperoft J E</author>
<author>J D Ullman</author>
</authors>
<title>The Design and Analysis of Computer Algorithms,</title>
<date>1974</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading Mass,</location>
<contexts>
<context position="5623" citStr="[2]" startWordPosition="946" endWordPosition="946">given respectively as O(g1) and 0(g). and g1 &lt; g2. it may stall.be the case that for a large number of cases (maybe even for all cases one is likely to encounter in practice) that A2 will be the preferable algorithm, simply because the constant K1 for gi may be much smaller than K2 for g 2. 98 In examining known results about the recognition complexity of various theories, it is useful to consider how &amp;quot;robust&amp;quot; they are in the face of changes in the machine model from which they were derived. These models can be divided into two classes: sequential models and parallel models. Sequential models [2] include the familiar single- and multi-tape Turing Machines (TMs) as well as Random Access Machines (RAMs) and Random Access Stored Program. Machines (RASPs). A RAM is like a TM except that its working menory is random access rather than sequential. A RASP is like a RAM but stores its program in its memory. Of all these models, it is most Like a von Neumann computer. All these sequential models can simulate each other in ways that do not cause great changes in time complexity. For example, a k-tape Turing Machine that runs in time 0(t) can be simulated by a RAM in time 0(t). and conversely, a</context>
</contexts>
<marker>[2]</marker>
<rawString>Aho Hoperoft J.E. and Ullman J.D., The Design and Analysis of Computer Algorithms, Addison-Wesley, Reading Mass, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<title>The Theory of Parsing.</title>
<date>1972</date>
<journal>Translation, and Compiling,</journal>
<volume>1</volume>
<publisher>Parsing. Prentice Hall,</publisher>
<location>Englewood Cliffs N.J.,</location>
<marker>[3]</marker>
<rawString>Aho A.V., and Ullman J.D., The Theory of Parsing. Translation, and Compiling, vol 1: Parsing. Prentice Hall, Englewood Cliffs N.J., 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B S Baker</author>
</authors>
<title>Arbitrary grammars generating contextfree languages,</title>
<date>1972</date>
<tech>TR 11-72,</tech>
<institution>Center for Research in Computing Technology, Harvard Univ.,</institution>
<marker>[4]</marker>
<rawString>Baker B.S., Arbitrary grammars generating contextfree languages, TR 11-72, Center for Research in Computing Technology, Harvard Univ., 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Berwick</author>
</authors>
<title>Computatiomil complexity and lexical functiona/ grammar, 19th ACL.</title>
<date>1981</date>
<marker>[5]</marker>
<rawString>Berwick R. C. Computatiomil complexity and lexical functiona/ grammar, 19th ACL. 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Berwick</author>
<author>A Weinberg</author>
</authors>
<title>Parsing efficiency, computational complexity, and the evaluation of grammatical theories,</title>
<date>1982</date>
<journal>Ling. Inci.</journal>
<volume>13</volume>
<pages>165--191</pages>
<contexts>
<context position="32484" citStr="[6]" startWordPosition="5364" endWordPosition="5364">t claim to understand them. Comparison between theories requires agreement between the machine models used to derive the complexity results. As mentioned above, the sequential models are all polynornially related, and no problem not having a 102 polynomial time solution on a sequential machine is likely to have one on a parallel machine limited to at most a polynomial number of processors, at least if P is not equal to NP. Both these results restrict the improvement one can obtain by changing implementation. but are of little use in comparing algorithms of Low complexity. Berwick and Weinberg [6] give examples of how algorithms of low complexity may have different implementations differing by large constant factors. In particular, changes in the form of the grammar and in its representation may have this effect. But of more interest I believe is the fact that implementation is often accompanied by some form of resource limitation that has two effects. First it is also a change in specification. A context-free parser implemented with a bounded stack recognizes only a finitestate Language. Second. very special implementations can be used if one is willing to restrict the size of the pro</context>
</contexts>
<marker>[6]</marker>
<rawString>Berwick R. C. and Weinberg A., Parsing efficiency, computational complexity, and the evaluation of grammatical theories, Ling. Inci. 13, 165-191. 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A T Eiorgida</author>
</authors>
<title>Formal Studies of Stratificational Grammars.</title>
<date>1977</date>
<tech>PhD Thesis,</tech>
<institution>University of Toronto,</institution>
<marker>[7]</marker>
<rawString>Eiorgida A. T., Formal Studies of Stratificational Grammars. PhD Thesis, University of Toronto, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Rules and Representations.</title>
<date>1980</date>
<publisher>Columbia University Press,</publisher>
<contexts>
<context position="29309" citStr="[8]" startWordPosition="4862" endWordPosition="4862">ptively adequate TG, and (3) the complexity of languages investigated so far is typical of the class. Hintikka[16] presents a very different argument against the recursivity of English based on the distribution of the words any and every. His account of why John knows everything is grammatical while John knows anything is not is that any can appear only in contexts where replacing it by every changes the meaning. Taking meaning to be logical equivalence, this means that grammaticality is dependent on the determination of logical equivalence of logical formulas, an undecidable problem. Chomsky [8] argues that a simpler solution is available, namely one that replaces logical equivalence by syntactic identity of some kind of logical form. Pullum and Gazdar [38] is a thorough survey of, and argument against, published claims (mainly the &amp;quot;respectively&amp;quot; examples (26], Dutch cross-serial dependencies, and nominalization in Mohawk [37]) that some natural Languages cannot be weakly generated by CF grammars. No claims are made about the strong adequacy of CFGs. 9. Seeking Significance. When can the supporter of a weak (syntactic) formalism (i.e. low recognition complexity, low generative capaci</context>
</contexts>
<marker>[8]</marker>
<rawString>Chomsky N., Rules and Representations. Columbia University Press, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Cook</author>
</authors>
<title>7bwards a complexity theory of synchronous parallel computation,</title>
<date>1981</date>
<journal>L&apos;Enseignement Mathematique</journal>
<volume>27</volume>
<pages>99--124</pages>
<contexts>
<context position="7634" citStr="[9]" startWordPosition="1297" endWordPosition="1297">t the other, then a problem that has a solution on a sequential machine in polynomial time and in space s will have a solution on a parallel machine with a polynomial number of processors and circuit depth (or maximum number of processors data must be passed through from input to output) 0(r). Since the depth of a parallel circuit corresponds to the (parallel) time required to complete the computation, this means that algorithms with sequential solutions requiring small space (such as deterministic CSLs) have fast parallel solutions. For a comprehensive survey of parallel computation, see Cook[9]. 3. Context-Free Languages. Recognition techniques for context-free languages are well-known 7,31. The so-called &amp;quot;CKY&apos; or &amp;quot;dynamic programming&amp;quot; method is attributed by Hays [151 to J. Cocke, and it was discovered independently by Kasami 54] and Younger :531 who showed it to be 0(n.j). It requires the grammar to be in Chomsky Normal Form, and putting an arbitrary grammar in CNF may square the size of the grammar. Earley&apos;s algorithm recognizes strings in arbitrary CFCs in time 0(n3) and space 0(n2), and in time 0(n2) for unambiguous CFGs. Graham, Harrison and Ruzzo 1131 give an algorithm that u</context>
</contexts>
<marker>[9]</marker>
<rawString>Cook S. A., 7bwards a complexity theory of synchronous parallel computation, L&apos;Enseignement Mathematique 27, 99-124, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm,</title>
<date>1970</date>
<journal>Comm. of ACM</journal>
<volume>13</volume>
<pages>94--102</pages>
<marker>[10]</marker>
<rawString>Earley J., An efficient context-free parsing algorithm, Comm. of ACM 13.2. 94-102, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Phrase structure grammar,</title>
<date>1982</date>
<booktitle>The Nature of Syntactic Representation. Reidel,</booktitle>
<editor>in Jacobson P. and Pullum G. (eds.),</editor>
<contexts>
<context position="23411" citStr="[11]" startWordPosition="3905" endWordPosition="3905">rary Boolean combinations of dominance conditions: a node could specify a bounded set of labels that must occur immediately above it along a path to the root, or immediately below it on a path to the frontier. In general the CF grammars constructed in •the proof of weak equivalence to the CS grammars under node admissibility are much larger than the original, and not useful for practical recognition. Joshi, Levy and Yueh [221, however, show how Earley&apos;s algorithm can be extended to a parser that uses the local constraints directly. 8.2. Metarules. The second important mechanism used by Gazdar [11] is rnetarules. or rules that apply to rules to produce other rules. Using standard notation for CF rules, one example of a metarule that could replace the transformation known as &amp;quot;particle movement&amp;quot; is: V--&gt; V N Pt X ==&gt; V--&gt; VPt IV[-PRO] X X here is a variable behaving like variables in structural analyses of transformations. If such variables are restricted to being used as abbreviations, that is if they are only allowed to range over a finite subset of strings over the vocabulary, then closing the grammar under the metarules produces only a finite set of derived rules, and thus the generat</context>
</contexts>
<marker>[11]</marker>
<rawString>Gazdar G., Phrase structure grammar, in Jacobson P. and Pullum G. (eds.), The Nature of Syntactic Representation. Reidel, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>G Pullum</author>
</authors>
<title>Generalized Phrase Structure Grammar: A Theoretical Synopsis, Indiana Univ.</title>
<date>1982</date>
<publisher>Ling. Club,</publisher>
<contexts>
<context position="25749" citStr="[12]" startWordPosition="4290" endWordPosition="4290">o rules without them. Such a device was proposed at one time as a way to implement passives in the GPSG framework. 101 Uszkoreit and Peters [49] have shown that essential variables in metarules are powerful devices indeed: CF grammars ,with metarules that use at most one essential variable and allow phantom categories can generate all recursively enumerable sets. Even if phantom categories are banned, as long as the use of at least one essential variables is allowed, then some non-recursive sets can be generated. Possible restrictions on the use of metarules are suggested in Gazdar and Pullum [12]. ,Shieber et al.[45] discuss some empirical consequences of these moves. 7. Tree Adjunct Grammar The Tree Adjunct Grammars (TAGs) of Joshi and his colleagues presents a different way of accounting for syntactic dependencies ([17], [19]). A TAG consists of two (finite) sets of (finite) trees, the centre trees and the adjunct trees. The centre trees correspond to the surface structures of the &amp;quot;kernel&amp;quot; sentences of the languages. The root of the adjunct trees is labelled with a non-terminal symbol which also appears exactly once on the frontier of the tree. All other frontier nodes are labelled </context>
</contexts>
<marker>[12]</marker>
<rawString>Gazdar G. and Pullum G., Generalized Phrase Structure Grammar: A Theoretical Synopsis, Indiana Univ. Ling. Club, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Graham</author>
<author>M A Harrison</author>
<author>W L Ruzzo</author>
</authors>
<title>An improved context-free recognizer,</title>
<date>1980</date>
<journal>ACM Trans. on Frog. Lang. and Systems,</journal>
<volume>2</volume>
<pages>415--462</pages>
<marker>[13]</marker>
<rawString>Graham S. L., Harrison M. A., and Ruzzo W.L., An improved context-free recognizer, ACM Trans. on Frog. Lang. and Systems, 2, 3, 415-462, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hoperoft</author>
<author>J Ullman</author>
</authors>
<title>Introduction to Automata Theory. Languages and Computation.</title>
<date>1979</date>
<publisher>Addison Wesley,</publisher>
<marker>[14]</marker>
<rawString>Hoperoft J.E. and Ullman J., Introduction to Automata Theory. Languages and Computation. Addison Wesley, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Hays</author>
</authors>
<title>Automatic language data processing,</title>
<date>1962</date>
<booktitle>In Computer Applications in the Behavioral Sciences,</booktitle>
<editor>H. Borko (ed.),</editor>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs N.J.,</location>
<marker>[15]</marker>
<rawString>Hays D.G., Automatic language data processing, In Computer Applications in the Behavioral Sciences, H. Borko (ed.), Prentice Hall, Englewood Cliffs N.J., 1962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K K</author>
</authors>
<title>Quantifiers in natural languages: some logical problems 2, Ling.</title>
<date>1977</date>
<journal>and Phil.,</journal>
<pages>153--172</pages>
<contexts>
<context position="28820" citStr="[16]" startWordPosition="4781" endWordPosition="4781">xtra-linguistic contexts. There are of course exceptions to this rule &apos;&apos;, (2) grammaticality judgements can be made for nonsense sentences, and (3) grammars can be learned. (2) and (3) are irrelevant and (1) contains its own counter-argument. Peters and Ritchie [33] contains a suggestive but hardly open-and-shut case for contingent recursivity (1) every TG has an exponentially bounded cycling function, and thus generates only recursive languages, (2) every natural language has a descriptively adequate TG, and (3) the complexity of languages investigated so far is typical of the class. Hintikka[16] presents a very different argument against the recursivity of English based on the distribution of the words any and every. His account of why John knows everything is grammatical while John knows anything is not is that any can appear only in contexts where replacing it by every changes the meaning. Taking meaning to be logical equivalence, this means that grammaticality is dependent on the determination of logical equivalence of logical formulas, an undecidable problem. Chomsky [8] argues that a simpler solution is available, namely one that replaces logical equivalence by syntactic identit</context>
</contexts>
<marker>[16]</marker>
<rawString>Hintikka. J.K.K., Quantifiers in natural languages: some logical problems 2, Ling. and Phil., 153-172, 1977.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A K Joshi</author>
</authors>
<title>How much context-sensitivity is required to provide reasonable Structural descriptions: tree adjoining grammars, to appear in</title>
<booktitle>Natural Language Processing: Psycholinguistic, Computational and Theoretical Properties,</booktitle>
<editor>Dowty D., Karttunen L. and Zwicky A. (eds.),</editor>
<publisher>Univ. Press.</publisher>
<location>Cambridge</location>
<contexts>
<context position="25979" citStr="[17]" startWordPosition="4325" endWordPosition="4325">mars ,with metarules that use at most one essential variable and allow phantom categories can generate all recursively enumerable sets. Even if phantom categories are banned, as long as the use of at least one essential variables is allowed, then some non-recursive sets can be generated. Possible restrictions on the use of metarules are suggested in Gazdar and Pullum [12]. ,Shieber et al.[45] discuss some empirical consequences of these moves. 7. Tree Adjunct Grammar The Tree Adjunct Grammars (TAGs) of Joshi and his colleagues presents a different way of accounting for syntactic dependencies ([17], [19]). A TAG consists of two (finite) sets of (finite) trees, the centre trees and the adjunct trees. The centre trees correspond to the surface structures of the &amp;quot;kernel&amp;quot; sentences of the languages. The root of the adjunct trees is labelled with a non-terminal symbol which also appears exactly once on the frontier of the tree. All other frontier nodes are labelled with terminal symbols. Derivations in TAGs are defined by repeated application of the operation of adjunction. If c is a centre tree containing an occurrence of a non-terminal A. and if a is an adjunct tree whose root (and one nod</context>
</contexts>
<marker>[17]</marker>
<rawString>Joshi A. K.. How much context-sensitivity is required to provide reasonable Structural descriptions: tree adjoining grammars, to appear in Dowty D., Karttunen L. and Zwicky A. (eds.), Natural Language Processing: Psycholinguistic, Computational and Theoretical Properties, Cambridge Univ. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
</authors>
<title>Factoring recursio? -znd dependencies: an aspect of Tree Adjoining Grammars and a comparison of some forma/ properties of TAG&apos;s, GPSG&apos;s, PLC&apos;s and LFG&apos;s, these Proceedings,</title>
<date>1983</date>
<note>19i</note>
<contexts>
<context position="27195" citStr="[18]" startWordPosition="4536" endWordPosition="4536">rontier) is labelled A. then the adjunction of a to c is performed by &amp;quot;detaching&amp;quot; from c the subtree t rooted at A. attaching a in its place, and reattaching t at node it. Adjunction may then be seen as a tree analogue of a context-free derivation for strings [40]. The string languagea obtained by taking the yields of the tree languages generated by TAGs are called Tree Adjunct languages, or TALs. In TAGs all long-distance dependencies are the result of adjunctions separating nodes that at one point in the derivation were &amp;quot;close&amp;quot;. Both crossing and non-crossing dependencies can be represented [18]. The formai properties of TAGs are fully discussed in [20]. [52], [23]. Of particular interest are the following. TALs properly contain the CFLs and are properly contained in the indexed languages, which in turn are properly contained in the Gas. Although the indexed languages contain NP-complete languages, TALs are much better behaved: Joshi and Yokomori report [personal communication] an 0(n4) recognition algorithm and conjecture that an 0(n3) bound may be possible. B. A Pointer to Empirical Discussions The literature on the empirical issues underlying the formal results reported here is no</context>
</contexts>
<marker>[18]</marker>
<rawString>Joshi A.K., Factoring recursio? -znd dependencies: an aspect of Tree Adjoining Grammars and a comparison of some forma/ properties of TAG&apos;s, GPSG&apos;s, PLC&apos;s and LFG&apos;s, these Proceedings, 1983. [19i Joshi A.K. and Levy L.S., Phrase structure trees bear mare fruit than you would have thought, 18th ACL. 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>J. of Comp. and Sys. Sc.</journal>
<volume>10</volume>
<pages>136--163</pages>
<contexts>
<context position="27254" citStr="[20]" startWordPosition="4547" endWordPosition="4547">rformed by &amp;quot;detaching&amp;quot; from c the subtree t rooted at A. attaching a in its place, and reattaching t at node it. Adjunction may then be seen as a tree analogue of a context-free derivation for strings [40]. The string languagea obtained by taking the yields of the tree languages generated by TAGs are called Tree Adjunct languages, or TALs. In TAGs all long-distance dependencies are the result of adjunctions separating nodes that at one point in the derivation were &amp;quot;close&amp;quot;. Both crossing and non-crossing dependencies can be represented [18]. The formai properties of TAGs are fully discussed in [20]. [52], [23]. Of particular interest are the following. TALs properly contain the CFLs and are properly contained in the indexed languages, which in turn are properly contained in the Gas. Although the indexed languages contain NP-complete languages, TALs are much better behaved: Joshi and Yokomori report [personal communication] an 0(n4) recognition algorithm and conjecture that an 0(n3) bound may be possible. B. A Pointer to Empirical Discussions The literature on the empirical issues underlying the formal results reported here is not extensive. Chomsky argues convincingly (81 that there is </context>
</contexts>
<marker>[20]</marker>
<rawString>Joshi A.K., Levy L.S. and Takahashi M., Tree adjunct grammars. J. of Comp. and Sys. Sc. 10, 1, 136-163, 1975. F211 Joshi A.K., Levy L.S., Constraints on structural &apos;descriptions: local transformations, SIAM J. on Computing, :977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshi AK Levy L S</author>
<author>K Yueh</author>
</authors>
<title>Local constraints on programming languages, Part 1: Syntax,</title>
<date>1980</date>
<journal>Th. Comp. Sc.</journal>
<volume>12</volume>
<pages>265--290</pages>
<marker>[22]</marker>
<rawString>Joshi AK.. Levy L.S. and Yueh K., Local constraints on programming languages, Part 1: Syntax, Th. Comp. Sc. 12. 265-290, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>T Yokomori</author>
</authors>
<title>Some characterization theorems for tree adjunct languages and recognizable sets, forthcoming. 124</title>
<date>1982</date>
<editor>in BreFnan J (ed.),</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="27266" citStr="[23]" startWordPosition="4549" endWordPosition="4549">detaching&amp;quot; from c the subtree t rooted at A. attaching a in its place, and reattaching t at node it. Adjunction may then be seen as a tree analogue of a context-free derivation for strings [40]. The string languagea obtained by taking the yields of the tree languages generated by TAGs are called Tree Adjunct languages, or TALs. In TAGs all long-distance dependencies are the result of adjunctions separating nodes that at one point in the derivation were &amp;quot;close&amp;quot;. Both crossing and non-crossing dependencies can be represented [18]. The formai properties of TAGs are fully discussed in [20]. [52], [23]. Of particular interest are the following. TALs properly contain the CFLs and are properly contained in the indexed languages, which in turn are properly contained in the Gas. Although the indexed languages contain NP-complete languages, TALs are much better behaved: Joshi and Yokomori report [personal communication] an 0(n4) recognition algorithm and conjecture that an 0(n3) bound may be possible. B. A Pointer to Empirical Discussions The literature on the empirical issues underlying the formal results reported here is not extensive. Chomsky argues convincingly (81 that there is no argument </context>
</contexts>
<marker>[23]</marker>
<rawString>Joshi A. K. and Yokomori T, Some characterization theorems for tree adjunct languages and recognizable sets, forthcoming. 124] Kaplan R. and Bresnan J., Lexical-F&apos;unctional Grammar: a formal system for grammatical representation, in BreFnan J (ed.), The Mental Representation of Grammatical Relations, MIT Press, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lamb</author>
</authors>
<title>Outline of Stratificational Grammar,</title>
<date>1966</date>
<publisher>University Press,</publisher>
<location>Georgetown</location>
<marker>[25]</marker>
<rawString>Lamb S., Outline of Stratificational Grammar, Georgetown University Press, Washington. 1966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Langendoen DI</author>
</authors>
<title>On the inadequacy of 7ype- 2 and Type-3 grammars for human languages,</title>
<date>1977</date>
<booktitle>Studies in Historical Linguistics: festschrift for</booktitle>
<pages>156--171</pages>
<editor>in P.J. Hopper (ed.)</editor>
<location>Amsterdam,</location>
<marker>[26]</marker>
<rawString>Langendoen DI., On the inadequacy of 7ype- 2 and Type-3 grammars for human languages, in P.J. Hopper (ed.) Studies in Historical Linguistics: festschrift for Winfred P. Lehman. John Benjamin, Amsterdam, 156-171, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S LaPointe</author>
</authors>
<title>Recursiveness and deletion,</title>
<date>1976</date>
<journal>Ling. Anal.</journal>
<volume>3</volume>
<pages>227--285</pages>
<contexts>
<context position="11587" citStr="[27]" startWordPosition="1944" endWordPosition="1944">lter insures that the base grammar generated just as many S nodes as necessary to generate the input string and do the simulation. Again, if the transformational formalism is supposed to characterize the possible natural languages, then the Universal Base Hypothesis [31] according to which all natural languages can be generated from the same base grammar is empirically vacuous: any recursively enumerable language can. Several attempts were then made to find a restricted form of the transformational model that was descriptively adequate and yet whose generated languages are recursive (see e.g. [27]). Since a key part of the proof in 321 involves the use of a filter on the final derivation trees, Peters and Ritchie examined the consequences of forbidding final filtering [35]. They show that if 5&apos; is the only recursive symbol in the CF base then the generated language L is predictably enurn.erable and exponentially bounded. A language L is predictably enumerable if there is an &amp;quot;easily&amp;quot; computable function t(n) that gives an upper bound on the number of tape squares needed by its enumerating TM to enumerate the first n elements of L. L is exponentially bounded if there is a constant K such</context>
</contexts>
<marker>[27]</marker>
<rawString>LaPointe S., Recursiveness and deletion, Ling. Anal. 3, 227-285. 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language,</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<contexts>
<context position="33182" citStr="[28]" startWordPosition="5481" endWordPosition="5481">g by large constant factors. In particular, changes in the form of the grammar and in its representation may have this effect. But of more interest I believe is the fact that implementation is often accompanied by some form of resource limitation that has two effects. First it is also a change in specification. A context-free parser implemented with a bounded stack recognizes only a finitestate Language. Second. very special implementations can be used if one is willing to restrict the size of the problem. to be solved, or even use special-purpose methods for limited problems. Marcus&apos;s parser [28] with its bounded lookahead is another good example. Sentences parsable within the allowed look-ahead have &amp;quot;quick&amp;quot; parses, but some grammatical sentences, such as &amp;quot;garden path&amp;quot; sentences cannot be recognized without an extension to the mechanism that would distort the complexity measures. There is obviously much more of this story to be told. Allow me to speculate as to how it might go. We may end up with a space of linguistic theories, differing in the idealization of the data they assume, in the way they decompose constraints, and in the procedural specifications they postulate (I take it th</context>
</contexts>
<marker>[28]</marker>
<rawString>Marcus M.P., A Theory of Syntactic Recognition for Natural Language, MIT Press, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Matthews</author>
</authors>
<title>Are the grammatical sentences of a language a recursive set?,</title>
<date>1979</date>
<journal>Synthese</journal>
<volume>40</volume>
<pages>206--224</pages>
<marker>[29]</marker>
<rawString>Matthews R., Are the grammatical sentences of a language a recursive set?, Synthese 40, 206-224, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>The proper treatment of quantification in ordinary English,</title>
<date>1973</date>
<booktitle>Approaches to Natural Language,</booktitle>
<editor>in Hintikka, J., Moravcsik J., and Suppes P.(eds.),</editor>
<location>Reidel, Dordrecht.</location>
<marker>[30]</marker>
<rawString>Montague R., The proper treatment of quantification in ordinary English, in Hintikka, J., Moravcsik J., and Suppes P.(eds.), Approaches to Natural Language, Reidel, Dordrecht. 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Peters</author>
<author>R W Ritchie</author>
</authors>
<title>A note on the universal base hypothesis.</title>
<date>1969</date>
<journal>J. of Linguistics,</journal>
<volume>5</volume>
<pages>150--2</pages>
<contexts>
<context position="11254" citStr="[31]" startWordPosition="1892" endWordPosition="1892">applied to the phrase-markers output by the transformations. In fact, the base grammar can be independent of the language being generated. The proof involves simulating a TM by a TG. The transformations first generate an &amp;quot;input tape&amp;quot; for the TM being simulated, and then apply the TM productions, one per cycle of the grammar. The filter insures that the base grammar generated just as many S nodes as necessary to generate the input string and do the simulation. Again, if the transformational formalism is supposed to characterize the possible natural languages, then the Universal Base Hypothesis [31] according to which all natural languages can be generated from the same base grammar is empirically vacuous: any recursively enumerable language can. Several attempts were then made to find a restricted form of the transformational model that was descriptively adequate and yet whose generated languages are recursive (see e.g. [27]). Since a key part of the proof in 321 involves the use of a filter on the final derivation trees, Peters and Ritchie examined the consequences of forbidding final filtering [35]. They show that if 5&apos; is the only recursive symbol in the CF base then the generated la</context>
</contexts>
<marker>[31]</marker>
<rawString>Peters P.S. and Ritchie R.W., A note on the universal base hypothesis. J. of Linguistics, 5, 150-2, 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Peters</author>
<author>R W Ritchie</author>
</authors>
<title>On restricting the base component of transformational grammars,</title>
<date>1971</date>
<journal>Ird. and Control,</journal>
<volume>18</volume>
<pages>483--5</pages>
<marker>[32]</marker>
<rawString>Peters P.S. and Ritchie R.W., On restricting the base component of transformational grammars, Ird. and Control, 18, 483-5-1, 1971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Peters</author>
<author>Ritchie RW</author>
</authors>
<title>On the generative power of transformational grammars,</title>
<date>1973</date>
<journal>Inf. Sc.</journal>
<volume>6</volume>
<pages>49--83</pages>
<contexts>
<context position="9376" citStr="[33]" startWordPosition="1583" endWordPosition="1583"> by boolean circuits of depth 0(log(n)2), and thus that parallel recognition can be done in time 0(log(n)2). The required circuit has size polynomial inn. So as not to get mystified by the upper bounds on CP recognition, it is useful to remember that no known CFL requires more than linear time, nor is there a (nonconstructive) proof of the existence of such a For an empirical comparison of various parsing methods, see Slocum [441. 4. Transformational Grammar. From its earliest days, discussions of transformational grammar (TG) have included mention of matters computational. Peters and Ritchie [33] provided the first non-trivial results on the generative power of TOs. Their model reflects the &amp;quot;Aspects&amp;quot; version quite closely, including transformations that could move and add constituents, and delete them subject to recoverability. All transformations are obligatory, and applied cyclically from the bottom up. They show that every recursively enumerable (re.) set can be generated by a TG using a contextsensitive base. The proof is quite simple: the right-hand sides of the type-0 rules that genet-ate the re. set are padded with a new &amp;quot;blank&amp;quot; symbol to make them at least as long as their lef</context>
<context position="28482" citStr="[33]" startWordPosition="4732" endWordPosition="4732">languages necesscry-ity being recursive. This, or course, is different from the possibility that languages are contingently recursive. Putnam 39] gives three reasons he claims &amp;quot;point in this direction&amp;quot;: (1) -speakers can presumably classify sentences as acceptable or unacceptable, deviant or non-deviant, at cetera, without reliance on extra-linguistic contexts. There are of course exceptions to this rule &apos;&apos;, (2) grammaticality judgements can be made for nonsense sentences, and (3) grammars can be learned. (2) and (3) are irrelevant and (1) contains its own counter-argument. Peters and Ritchie [33] contains a suggestive but hardly open-and-shut case for contingent recursivity (1) every TG has an exponentially bounded cycling function, and thus generates only recursive languages, (2) every natural language has a descriptively adequate TG, and (3) the complexity of languages investigated so far is typical of the class. Hintikka[16] presents a very different argument against the recursivity of English based on the distribution of the words any and every. His account of why John knows everything is grammatical while John knows anything is not is that any can appear only in contexts where re</context>
</contexts>
<marker>[33]</marker>
<rawString>Peters P.S. and Ritchie RW., On the generative power of transformational grammars, Inf. Sc. 6, 49-83, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Peters</author>
<author>R W Ritchie</author>
</authors>
<title>Context-sensitive immediate constituent analysis - context-free languages -revisited,</title>
<date>1973</date>
<journal>Math. Sys. Theory</journal>
<volume>6</volume>
<pages>324--333</pages>
<marker>[34]</marker>
<rawString>Peters P.S. and Ritchie R.W., Context-sensitive immediate constituent analysis - context-free languages -revisited, Math. Sys. Theory 6, 324-333, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Peters</author>
<author>R W Ritchie</author>
</authors>
<title>Nan-filtering and local filtering grammars,</title>
<date>1973</date>
<booktitle>Approaches to Natural Language, Reidel,</booktitle>
<pages>180--194</pages>
<editor>in J.K.K. Hintikka, J.M.E. Moravcsik, and P. Suppes (eds.)</editor>
<contexts>
<context position="11766" citStr="[35]" startWordPosition="1975" endWordPosition="1975">osed to characterize the possible natural languages, then the Universal Base Hypothesis [31] according to which all natural languages can be generated from the same base grammar is empirically vacuous: any recursively enumerable language can. Several attempts were then made to find a restricted form of the transformational model that was descriptively adequate and yet whose generated languages are recursive (see e.g. [27]). Since a key part of the proof in 321 involves the use of a filter on the final derivation trees, Peters and Ritchie examined the consequences of forbidding final filtering [35]. They show that if 5&apos; is the only recursive symbol in the CF base then the generated language L is predictably enurn.erable and exponentially bounded. A language L is predictably enumerable if there is an &amp;quot;easily&amp;quot; computable function t(n) that gives an upper bound on the number of tape squares needed by its enumerating TM to enumerate the first n elements of L. L is exponentially bounded if there is a constant K such that for every string x in L there is another string x&apos; in L whose length is at most K times the length of z. 99 The class of non-filtering languages is quite unusual, including </context>
</contexts>
<marker>[35]</marker>
<rawString>Peters P.S. and Ritchie R.W., Nan-filtering and local filtering grammars, in J.K.K. Hintikka, J.M.E. Moravcsik, and P. Suppes (eds.) Approaches to Natural Language, Reidel, 180-194, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Petrick</author>
</authors>
<title>A Recognition Procedure for Transformational Grammars. PhD Thesis,</title>
<date>1965</date>
<location>MIT,</location>
<contexts>
<context position="12860" citStr="[36]" startWordPosition="2160" endWordPosition="2160">ength is at most K times the length of z. 99 The class of non-filtering languages is quite unusual, including all the CFLs (obviously), but also some (but not all) CSLs, some (but not all) recursive languages, and some (but not all) r.e. languages. The source of non-recursivity in transformationally generated languages is that transformations can delete arbitrarily large parts of the tree, thus producing surface trees arbitrarily smaller than the deep structure trees they were derived from. This is what Chomsky&apos;s recoverability of deletions condition was meant to avoid. In his thesis, Petrick [36] defines the following terminallength-increasing condition on transformational derivations: consider the following two p-markers from a derivation, where the right one is derived from the left one by applying the cycle of transformations to subtree c producing the subtree u. a&gt;. cycle I Continuing the derivation., apply the cycle to tree t yielding tree u. .&gt;• cycle 2 A derivation satisfies the terminal-length-increasing condition it the yield of iL is always longer than the yield of Petrick shows that if all recursion in the base &amp;quot;passes through 5&amp;quot; and if all derivations satisfy the terminal-</context>
</contexts>
<marker>[36]</marker>
<rawString>Petrick S. R., A Recognition Procedure for Transformational Grammars. PhD Thesis, MIT, 1965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Postal</author>
</authors>
<title>Limitations of phrase-structure grammars,</title>
<date>1964</date>
<booktitle>The structure of language: Readings in the philosophy of language. Engle- -wood Cliffs:</booktitle>
<pages>137--151</pages>
<editor>in J.A. Fodor and J.J. Katz (eds.),</editor>
<publisher>Prentice Hall,</publisher>
<contexts>
<context position="29647" citStr="[37]" startWordPosition="4913" endWordPosition="4913">ny can appear only in contexts where replacing it by every changes the meaning. Taking meaning to be logical equivalence, this means that grammaticality is dependent on the determination of logical equivalence of logical formulas, an undecidable problem. Chomsky [8] argues that a simpler solution is available, namely one that replaces logical equivalence by syntactic identity of some kind of logical form. Pullum and Gazdar [38] is a thorough survey of, and argument against, published claims (mainly the &amp;quot;respectively&amp;quot; examples (26], Dutch cross-serial dependencies, and nominalization in Mohawk [37]) that some natural Languages cannot be weakly generated by CF grammars. No claims are made about the strong adequacy of CFGs. 9. Seeking Significance. When can the supporter of a weak (syntactic) formalism (i.e. low recognition complexity, low generative capacity) claim that it superior to a competing more powerful formalism? Linguistic theories can differ along several dimensions, with generative capacity and recognition capacity being only two (albeit related) ones. The evaluation must take into consideration at least the following others: Coverage. Do the theories make the same grammatical</context>
</contexts>
<marker>[37]</marker>
<rawString>Postal P.M., Limitations of phrase-structure grammars, in J.A. Fodor and J.J. Katz (eds.), The structure of language: Readings in the philosophy of language. Engle- -wood Cliffs: Prentice Hall, 137-151, 1964.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Pullum</author>
<author>G Gazdar</author>
</authors>
<title>Natural and context-free languages,</title>
<date>1982</date>
<journal>Ling. and Phil.,</journal>
<volume>4</volume>
<pages>471--504</pages>
<contexts>
<context position="29474" citStr="[38]" startWordPosition="4889" endWordPosition="4889">cursivity of English based on the distribution of the words any and every. His account of why John knows everything is grammatical while John knows anything is not is that any can appear only in contexts where replacing it by every changes the meaning. Taking meaning to be logical equivalence, this means that grammaticality is dependent on the determination of logical equivalence of logical formulas, an undecidable problem. Chomsky [8] argues that a simpler solution is available, namely one that replaces logical equivalence by syntactic identity of some kind of logical form. Pullum and Gazdar [38] is a thorough survey of, and argument against, published claims (mainly the &amp;quot;respectively&amp;quot; examples (26], Dutch cross-serial dependencies, and nominalization in Mohawk [37]) that some natural Languages cannot be weakly generated by CF grammars. No claims are made about the strong adequacy of CFGs. 9. Seeking Significance. When can the supporter of a weak (syntactic) formalism (i.e. low recognition complexity, low generative capacity) claim that it superior to a competing more powerful formalism? Linguistic theories can differ along several dimensions, with generative capacity and recognition </context>
</contexts>
<marker>[38]</marker>
<rawString>Pullum G.K. and Gazdar G., Natural and context-free languages, Ling. and Phil., 4, 471-504, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Putnam</author>
</authors>
<title>Some issues in the theory of grammar, in</title>
<date>1961</date>
<booktitle>Proc. of Symposia in Applied Mathematics.</booktitle>
<publisher>American Math. Soc.,</publisher>
<marker>[39]</marker>
<rawString>Putnam H., Some issues in the theory of grammar, in Proc. of Symposia in Applied Mathematics. American Math. Soc., 1961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Rounds</author>
</authors>
<title>Mappings and grammars on trees,</title>
<date>1970</date>
<journal>Math. Sys. Th.,4,3,</journal>
<pages>257--287</pages>
<contexts>
<context position="19744" citStr="[40]" startWordPosition="3283" endWordPosition="3283">a rule. For CF rules, the two interpretations obviously generate the same strings aiid the same set of trees. Following a suggestion of McCawley&apos;s. Peters and Ritchie [341 showed that if one considered contextsensitive rules from the node-admissibility point of view, the languages defined were still CF. Thus the use of CS rules in the base to impose sub-categorization restrictions. for example, does not increase the weak generative capacity of the base component. (For some different restrictions of context-sensitive rules that guarantee that only CFLs will be generated. see Baker [41.) Rounds [40] gives a simpler proof of Peters and Ritchie&apos;s node-admissibility result using the techniques from tree-automata theory, a generalization to trees of finite state automata theory for strings. Just as a finite state automaton (FSA) accepts a string by reading it one character at a time, changing its state at each transition, a finite state tree automaton (Fh-IA) traverses trees, propagating states. The top-down FSTA &amp;quot;attaches&amp;quot; a starting state (from a finite set) to the root of the tree. Transitions are allowed by productions of the form (q, a, it) --&gt; (q1,..„qn) such that if state q is being a</context>
<context position="26855" citStr="[40]" startWordPosition="4484" endWordPosition="4484">hich also appears exactly once on the frontier of the tree. All other frontier nodes are labelled with terminal symbols. Derivations in TAGs are defined by repeated application of the operation of adjunction. If c is a centre tree containing an occurrence of a non-terminal A. and if a is an adjunct tree whose root (and one node n on the frontier) is labelled A. then the adjunction of a to c is performed by &amp;quot;detaching&amp;quot; from c the subtree t rooted at A. attaching a in its place, and reattaching t at node it. Adjunction may then be seen as a tree analogue of a context-free derivation for strings [40]. The string languagea obtained by taking the yields of the tree languages generated by TAGs are called Tree Adjunct languages, or TALs. In TAGs all long-distance dependencies are the result of adjunctions separating nodes that at one point in the derivation were &amp;quot;close&amp;quot;. Both crossing and non-crossing dependencies can be represented [18]. The formai properties of TAGs are fully discussed in [20]. [52], [23]. Of particular interest are the following. TALs properly contain the CFLs and are properly contained in the indexed languages, which in turn are properly contained in the Gas. Although the</context>
<context position="36141" citStr="[40]" startWordPosition="5945" endWordPosition="5945">xt-free languages by constructing a non-deterministic bottom-up tree automaton to recognize the accepted trees. We sketch here a proof that makes use of several deterministic transducers instead. FSTAs can be generalized so that instead of simply accepting or rejecting trees, they transform them, by adding constant trees, and deleting or duplicating subtrees. Such devices are called finite state tree transducers (ISM, and like the FSTA they can be top-down or bottom-up. First motivated as models of syntax-directed translations for compilers, they have been extensively studied (e.g. [V], [48], [40]) but a simple subset is sufficient here. The idea is this. Let Tbe the set of trees accepted by the CS-based grammar. Let t be in T. Mils can be used to label each node it of t with the set of all proper analyses passing through n. It will then be simple to check that each node satisfies one of the node admissibility conditions by sweeping through the labelled tree with a bottom-up FSTA. The node labelling is done by two FblTs r, and r2. Let in. be the maximum length of any left or right-context of any node admissibility condition. Thus we need only label nodes with sets of strings of length </context>
</contexts>
<marker>[40]</marker>
<rawString>Rounds W. C., Mappings and grammars on trees, Math. Sys. Th.,4,3, 257-287, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Rounds</author>
</authors>
<title>Tree-oriented proofs of some theorems on context-free and indexed languages,</title>
<date>1970</date>
<booktitle>2nd ACM Symp. on Th. Comp. Sc.,</booktitle>
<pages>109--116</pages>
<contexts>
<context position="35456" citStr="[41]" startWordPosition="5845" endWordPosition="5845"> Were ancient languages regular? Whatever we decide to make of existing formal results, it is clear that continuing contact with the complexity community is important. The driving problems there are the P= NP question. the determination of lower bounds, the study of time-space tradeoffs, and the complexity of parallel computations. We still have some methodological house-cleaning to do, but I don&apos;t see how we can avoid being affected by the outcome of their investigations. ACKNOWLEDGEMENTS Thanks to Bob Berwick, Aravind Joshi, Jim Hoover, and Stan Peters for their suggestions. APPENDIX Rounds [41] proves that context-sensitive rules under node-admissibility generate only context-free languages by constructing a non-deterministic bottom-up tree automaton to recognize the accepted trees. We sketch here a proof that makes use of several deterministic transducers instead. FSTAs can be generalized so that instead of simply accepting or rejecting trees, they transform them, by adding constant trees, and deleting or duplicating subtrees. Such devices are called finite state tree transducers (ISM, and like the FSTA they can be top-down or bottom-up. First motivated as models of syntax-directed</context>
</contexts>
<marker>[41]</marker>
<rawString>Rounds W. C., Tree-oriented proofs of some theorems on context-free and indexed languages, 2nd ACM Symp. on Th. Comp. Sc., 109-116, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Rounds</author>
</authors>
<title>Complexity of recognition</title>
<date>1973</date>
<booktitle>in intermediate-level languages, 14th Symp. on Sw. and Aut. Th.</booktitle>
<contexts>
<context position="13601" citStr="[42]" startWordPosition="2275" endWordPosition="2275">erivation, where the right one is derived from the left one by applying the cycle of transformations to subtree c producing the subtree u. a&gt;. cycle I Continuing the derivation., apply the cycle to tree t yielding tree u. .&gt;• cycle 2 A derivation satisfies the terminal-length-increasing condition it the yield of iL is always longer than the yield of Petrick shows that if all recursion in the base &amp;quot;passes through 5&amp;quot; and if all derivations satisfy the terminal-length-increasing condition, then the generated language is recursive. Using a slightly more restricted model of transformations, Rounds [42] strengthens this result by showing that the resulting languages are in fact context-sensitive. In an unpublished paper. Myhill shows that if the condition is weakened to terminal-length-non-decreasing, then the resulting languages can be recognized in space at most exprrn.entia/ in the length of the input. This implies that the recognition can be done in at most double-exponential time, but Rounds :43] shows that not only can recognition be done in exponential time, but that every language recognizable in exponential time can be generated by a TO satisfying the terminal-length-nondecreasing c</context>
</contexts>
<marker>[42]</marker>
<rawString>Rounds W. C., Complexity of recognition in intermediate-level languages, 14th Symp. on Sw. and Aut. Th. 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Rounds</author>
</authors>
<title>A grammatical characterization of exponential-time languages,</title>
<date>1975</date>
<booktitle>1EZP. Symp. on &apos;Found. cf Comp. Sc.,</booktitle>
<pages>135--143</pages>
<marker>[43]</marker>
<rawString>Rounds W. C., A grammatical characterization of exponential-time languages, 1EZP. Symp. on &apos;Found. cf Comp. Sc., 135-143, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Slocum</author>
</authors>
<title>A practical comparison of parsing strategies, 19th ACL,</title>
<date>1981</date>
<marker>[44]</marker>
<rawString>Slocum J., A practical comparison of parsing strategies, 19th ACL, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
<author>S U Stucky</author>
<author>H Uszkoreit</author>
<author>J J Robinson</author>
</authors>
<title>frbrrhai constraints on metarutes, these Proceedings,</title>
<date>1983</date>
<contexts>
<context position="25770" citStr="[45]" startWordPosition="4293" endWordPosition="4293"> Such a device was proposed at one time as a way to implement passives in the GPSG framework. 101 Uszkoreit and Peters [49] have shown that essential variables in metarules are powerful devices indeed: CF grammars ,with metarules that use at most one essential variable and allow phantom categories can generate all recursively enumerable sets. Even if phantom categories are banned, as long as the use of at least one essential variables is allowed, then some non-recursive sets can be generated. Possible restrictions on the use of metarules are suggested in Gazdar and Pullum [12]. ,Shieber et al.[45] discuss some empirical consequences of these moves. 7. Tree Adjunct Grammar The Tree Adjunct Grammars (TAGs) of Joshi and his colleagues presents a different way of accounting for syntactic dependencies ([17], [19]). A TAG consists of two (finite) sets of (finite) trees, the centre trees and the adjunct trees. The centre trees correspond to the surface structures of the &amp;quot;kernel&amp;quot; sentences of the languages. The root of the adjunct trees is labelled with a non-terminal symbol which also appears exactly once on the frontier of the tree. All other frontier nodes are labelled with terminal symbols</context>
</contexts>
<marker>[45]</marker>
<rawString>Shieber S.M., Stucky S. U., Uszkoreit H. and Robinson J. J., frbrrhai constraints on metarutes, these Proceedings, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Thatcher</author>
</authors>
<title>Characterising derivation trees of context-free grammars through a generalization of finite automata theory,</title>
<date></date>
<journal>J. of Comp. and Sys. Sc.</journal>
<booktitle>4 Thatcher J.W., Gertara/ized2 sequential machine mnps. J. of Comp. and Sys. Sc.</booktitle>
<volume>1</volume>
<pages>317--322</pages>
<marker>[46]</marker>
<rawString>Thatcher J.W., Characterising derivation trees of context-free grammars through a generalization of finite automata theory, J. of Comp. and Sys. Sc. 1. 317-322. 197]67 [4 Thatcher J.W., Gertara/ized2 sequential machine mnps. J. of Comp. and Sys. Sc. 4, 339-67, 1.970</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Thatcher</author>
</authors>
<title>Tree automata: an informal survey,</title>
<date>1973</date>
<booktitle>Currents in the theory of computing.</booktitle>
<pages>143--172</pages>
<editor>in A. Aho (ed.),</editor>
<publisher>Prentice Hail,</publisher>
<contexts>
<context position="36135" citStr="[48]" startWordPosition="5944" endWordPosition="5944"> context-free languages by constructing a non-deterministic bottom-up tree automaton to recognize the accepted trees. We sketch here a proof that makes use of several deterministic transducers instead. FSTAs can be generalized so that instead of simply accepting or rejecting trees, they transform them, by adding constant trees, and deleting or duplicating subtrees. Such devices are called finite state tree transducers (ISM, and like the FSTA they can be top-down or bottom-up. First motivated as models of syntax-directed translations for compilers, they have been extensively studied (e.g. [V], [48], [40]) but a simple subset is sufficient here. The idea is this. Let Tbe the set of trees accepted by the CS-based grammar. Let t be in T. Mils can be used to label each node it of t with the set of all proper analyses passing through n. It will then be simple to check that each node satisfies one of the node admissibility conditions by sweeping through the labelled tree with a bottom-up FSTA. The node labelling is done by two FblTs r, and r2. Let in. be the maximum length of any left or right-context of any node admissibility condition. Thus we need only label nodes with sets of strings of l</context>
</contexts>
<marker>[48]</marker>
<rawString>Thatcher J W., Tree automata: an informal survey, in A. Aho (ed.), Currents in the theory of computing. Prentice Hail, 143-172, 1973.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Uszkoreit</author>
<author>P Peters</author>
</authors>
<note>S asential variables in rnetarules, forthcoming.</note>
<contexts>
<context position="25289" citStr="[49]" startWordPosition="4218" endWordPosition="4218">onvenient in formulating rnetarules to allow, in the left-hand sides of rules, occurrences of syntactic categories that are never introduced by the grammar, i.e. that never appear in the right-hand sides of rules. In standard CFLs, these are called &apos;useless categories, and rules containing them can simply be dropped, with no change in generative capacity. Not so with metarules: it is possible for metarules to rewrite rules containing phantom categories into rules without them. Such a device was proposed at one time as a way to implement passives in the GPSG framework. 101 Uszkoreit and Peters [49] have shown that essential variables in metarules are powerful devices indeed: CF grammars ,with metarules that use at most one essential variable and allow phantom categories can generate all recursively enumerable sets. Even if phantom categories are banned, as long as the use of at least one essential variables is allowed, then some non-recursive sets can be generated. Possible restrictions on the use of metarules are suggested in Gazdar and Pullum [12]. ,Shieber et al.[45] discuss some empirical consequences of these moves. 7. Tree Adjunct Grammar The Tree Adjunct Grammars (TAGs) of Joshi </context>
</contexts>
<marker>[49]</marker>
<rawString>Uszkoreit H. and Peters P. S asential variables in rnetarules, forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Valiant</author>
</authors>
<title>General context-free recognition in less than cubic time,</title>
<date>1975</date>
<journal>J. of Comp. and Sp. Sc.</journal>
<volume>10</volume>
<pages>308--315</pages>
<contexts>
<context position="8323" citStr="[50]" startWordPosition="1407" endWordPosition="1407">ll-known 7,31. The so-called &amp;quot;CKY&apos; or &amp;quot;dynamic programming&amp;quot; method is attributed by Hays [151 to J. Cocke, and it was discovered independently by Kasami 54] and Younger :531 who showed it to be 0(n.j). It requires the grammar to be in Chomsky Normal Form, and putting an arbitrary grammar in CNF may square the size of the grammar. Earley&apos;s algorithm recognizes strings in arbitrary CFCs in time 0(n3) and space 0(n2), and in time 0(n2) for unambiguous CFGs. Graham, Harrison and Ruzzo 1131 give an algorithm that unifies CKY and Earley&apos;s &apos;L101 algorithm, and discuss implementation details. Valiant [50] showed how to interpret the CKY algorithm as the finding of the transitive closure of a matrix and thus reduced CF recognition to matrix multiplication, for which sub-cubic algorithms exist. Because of the enormous constants of proportionality associated with this method, it is not likely to be of much practical use, either an implementation method or as a description of the function of the brain. Ruzzo [55] has shown how CFLs can be recognized by boolean circuits of depth 0(log(n)2), and thus that parallel recognition can be done in time 0(log(n)2). The required circuit has size polynomial i</context>
</contexts>
<marker>[50]</marker>
<rawString>Valiant L. General context-free recognition in less than cubic time, J. of Comp. and Sp. Sc. 10, 308-315, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Warren</author>
</authors>
<title>Syntax and Semantics of Parsing: An Application to Montague Grammar.</title>
<date>1979</date>
<tech>PhD Thesis,</tech>
<institution>University of Michigan,</institution>
<contexts>
<context position="1466" citStr="[51]" startWordPosition="212" endWordPosition="212"> Strangely enough, much less attention seems to have been devoted to a discussion of the significance of these mathematical results. As a preliminary to the panel on formal properties which will address the significance issue, it seemed appropriate to survey the existing results. Such is the modest goal of this paper. We will consider context-free Languages, transformational grammars, lexical functional grammars, generalized phrase structure grammars, and tree adjunct grammars. Although we will not examine them here, formal studies of other syntactic theories have been undertaken: e.g. Warren [51] for Montague&apos;s PTQ 30], and Borgida 1_71 for the stratificational grammars of Lamb ;251. There follows a brief summary of some comments in the literature about related empirical issues, but we avoid entirely the issue of whether one theory is more descriptively adequate than another. 2. Preliminary Definitions We assume the reader is familiar with the basic definitions of regular, context-free (CF), context-sensitive (CS), recursive, and recursively enumerable (re.) languages and with their acceptors as can be found in 141 Some elementary definitions from complexity theory may be useful. Furt</context>
</contexts>
<marker>[51]</marker>
<rawString>Warren D. S., Syntax and Semantics of Parsing: An Application to Montague Grammar. PhD Thesis, University of Michigan, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Yokomori</author>
<author>Joshi A K mi-linearity</author>
</authors>
<title>Parikhboundedness and tree adjunct languages, to appear in Inf.</title>
<date>1983</date>
<publisher>Pr. Letters,</publisher>
<contexts>
<context position="27260" citStr="[52]" startWordPosition="4548" endWordPosition="4548">d by &amp;quot;detaching&amp;quot; from c the subtree t rooted at A. attaching a in its place, and reattaching t at node it. Adjunction may then be seen as a tree analogue of a context-free derivation for strings [40]. The string languagea obtained by taking the yields of the tree languages generated by TAGs are called Tree Adjunct languages, or TALs. In TAGs all long-distance dependencies are the result of adjunctions separating nodes that at one point in the derivation were &amp;quot;close&amp;quot;. Both crossing and non-crossing dependencies can be represented [18]. The formai properties of TAGs are fully discussed in [20]. [52], [23]. Of particular interest are the following. TALs properly contain the CFLs and are properly contained in the indexed languages, which in turn are properly contained in the Gas. Although the indexed languages contain NP-complete languages, TALs are much better behaved: Joshi and Yokomori report [personal communication] an 0(n4) recognition algorithm and conjecture that an 0(n3) bound may be possible. B. A Pointer to Empirical Discussions The literature on the empirical issues underlying the formal results reported here is not extensive. Chomsky argues convincingly (81 that there is no arg</context>
</contexts>
<marker>[52]</marker>
<rawString>Yokomori T. and Joshi A. K. .mi-linearity, Parikhboundedness and tree adjunct languages, to appear in Inf. Pr. Letters, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of contextfree languages in time nu,</title>
<date>1967</date>
<journal>Inf. and Control,</journal>
<volume>10</volume>
<pages>189--208</pages>
<marker>[53]</marker>
<rawString>Younger D. H. Recognition and parsing of contextfree languages in time nu, Inf. and Control, 10, 2, 189-208, 1967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasarni</author>
</authors>
<title>An efficient recognition and syntax algorithm for context-free languages, Air Force Cambridge Research Laboratory report AF-CRL-65-758,</title>
<date>1965</date>
<location>Bedford MA,</location>
<marker>[54]</marker>
<rawString>Kasarni T., An efficient recognition and syntax algorithm for context-free languages, Air Force Cambridge Research Laboratory report AF-CRL-65-758, Bedford MA, 1965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W L Ruzzo</author>
</authors>
<title>On uniform circuit comp/ezity (extended abstract),</title>
<date>1979</date>
<booktitle>Proc. of 20th Annual Symp. on Found. of Coro. Sc.,</booktitle>
<pages>312--318</pages>
<contexts>
<context position="8735" citStr="[55]" startWordPosition="1477" endWordPosition="1477">0(n2), and in time 0(n2) for unambiguous CFGs. Graham, Harrison and Ruzzo 1131 give an algorithm that unifies CKY and Earley&apos;s &apos;L101 algorithm, and discuss implementation details. Valiant [50] showed how to interpret the CKY algorithm as the finding of the transitive closure of a matrix and thus reduced CF recognition to matrix multiplication, for which sub-cubic algorithms exist. Because of the enormous constants of proportionality associated with this method, it is not likely to be of much practical use, either an implementation method or as a description of the function of the brain. Ruzzo [55] has shown how CFLs can be recognized by boolean circuits of depth 0(log(n)2), and thus that parallel recognition can be done in time 0(log(n)2). The required circuit has size polynomial inn. So as not to get mystified by the upper bounds on CP recognition, it is useful to remember that no known CFL requires more than linear time, nor is there a (nonconstructive) proof of the existence of such a For an empirical comparison of various parsing methods, see Slocum [441. 4. Transformational Grammar. From its earliest days, discussions of transformational grammar (TG) have included mention of matte</context>
</contexts>
<marker>[55]</marker>
<rawString>Ruzzo W. L, On uniform circuit comp/ezity (extended abstract), Proc. of 20th Annual Symp. on Found. of Coro. Sc., 312-318, 1979.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>