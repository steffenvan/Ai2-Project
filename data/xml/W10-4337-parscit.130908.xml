<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.210380">
<title confidence="0.998383">
Investigating Clarification Strategies in a
Hybrid POMDP Dialog Manager
</title>
<author confidence="0.9964">
Sebastian Varges and Silvia Quarteroni and Giuseppe Riccardi and Alexei V. Ivanov
</author>
<affiliation confidence="0.9924875">
Department of Information Engineering and Computer Science
University of Trento, 38050 Povo di Trento, Italy
</affiliation>
<email confidence="0.998119">
{varges|silviaq|riccardi|ivanov}@disi.unitn.it
</email>
<sectionHeader confidence="0.997376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947578947368">
We investigate the clarification strategies
exhibited by a hybrid POMDP dialog
manager based on data obtained from a
phone-based user study. The dialog man-
ager combines task structures with a num-
ber of POMDP policies each optimized for
obtaining an individual concept. We in-
vestigate the relationship between dialog
length and task completion. In order to
measure the effectiveness of the clarifica-
tion strategies, we compute concept pre-
cisions for two different mentions of the
concept in the dialog: first mentions and
final values after clarifications and simi-
lar strategies, and compare this to a rule-
based system on the same task. We ob-
serve an improvement in concept precision
of 12.1% for the hybrid POMDP com-
pared to 5.2% for the rule-based system.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999836796296296">
In recent years, probabilistic models of dialog
have been introduced into dialog management, the
part of the spoken dialog system that takes the ac-
tion decision. A major motivation is to improve
robustness in the face of uncertainty, in particu-
lar due to speech recognition errors. The inter-
action is characterized as a dynamic system that
manipulates its environment by performing dialog
actions and perceives feedback from the environ-
ment through its sensors. The original sensory in-
formation is obtained from the speech recognition
(ASR) results which are typically processed by a
spoken language understanding module (SLU) be-
fore being passed on to the dialog manager (DM).
The seminal work of (Levin et al., 2000) mod-
eled dialog management as a Markov Decision
Process (MDP). Using reinforcement learning as
the general learning paradigm, an MDP-based di-
alog manager incrementally acquires a policy by
obtaining rewards about actions it performed in
specific dialog states. As we found in earlier ex-
periments, an MDP can learn to gradually drop the
use of clarification questions if there is no noise.
This is due to the fact that clarifications do not
improve the outcome of the dialog, i.e. the re-
ward. However, with extremely high levels of
noise, the learner prefers to end the dialog imme-
diately (Varges et al., 2009). In contrast to deliber-
ate decision making in the pragmatist tradition of
dialog processing, reinforcement learning can be
regarded as low-level decision making.
MDPs do not account for the observational un-
certainty of the speech recognition results, a key
challenge in spoken dialog systems. Partially Ob-
servable Markov Decision Process (POMDPs) ad-
dress this issue by explicitly modeling how the dis-
tribution of observations is governed by states and
actions.
In this work, we describe the evaluation of a
divide-and-conquer approach to dialog manage-
ment with POMDPs that optimizes policies for
acquiring individual concepts separately. This
makes optimization much easier and allows us to
model the confusability of concrete concept values
explicitly. This also means that different clarifica-
tion strategies are learned for individual concepts
and even individual concept values. The use of the
POMDP policies is orchestrated by an explicit task
structure, resulting in a hybrid approach to dialog
management. The evaluation involved a user study
of 20 subjects in a tourist information domain. The
system is compared against a rule-based baseline
system in the same domain that was also evaluated
with 20 subjects.
</bodyText>
<sectionHeader confidence="0.965769" genericHeader="method">
2 Hybrid POMDP dialog management
</sectionHeader>
<bodyText confidence="0.8850235">
In this section we introduce the hybrid POMDP di-
alog manager that was used in the data collection.
</bodyText>
<subsubsectionHeader confidence="0.62402">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 213–216,
</subsubsectionHeader>
<affiliation confidence="0.874451">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.999667">
213
</page>
<subsectionHeader confidence="0.952737">
2.1 Concept-level POMDPs
</subsectionHeader>
<bodyText confidence="0.999909421052632">
The domain is a tourist information system that
uses 5 different policies that can be used in 8
different task roles (see below). For each con-
cept we optimized an individual policy. The
number of states of the POMDP can be lim-
ited to the concept values, for example a loca-
tion name such as trento. The set of ac-
tions consists of a question to obtain the concept
(e.g. question-location), a set of clari-
fication actions (e.g. verify-trento) and a
set of submit actions (e.g. submit-trento).
POMDP modeling including a heuristically set re-
ward structure follows the (simpler) ‘tiger prob-
lem’ that is well-known in the AI community
(Kaelbling et al., 1998): the system has a num-
ber of actions to obtain further information which
it can try and repeat in any order until it is ready
to commit to a concept value. For optimization we
used the APPL solver (Kurniawati et al., 2008).
</bodyText>
<subsectionHeader confidence="0.99997">
2.2 Task structure and dialog management
</subsectionHeader>
<bodyText confidence="0.999918925925926">
The use of individual policies is orchestrated by
an explicit task structure that activates and de-
activates them. The task structure is essentially
a directed AND-OR graph with a common root
node. The dialog manager maintains a separate be-
lief distribution for each concept. Figure 1 shows
the general system architecture with a schematic
view of the task structure, and additionally a more
detailed view of an active location node. In the
example, the root node has already finished and
the system is currently obtaining the location for a
lodging task. The term ‘role’ refers to a concept’s
part in the task, for example a month may be the
check-in or check-out month for accommodation
booking.
At the beginning of a dialog, the task structure is
initialized by activating the root node. A top level
function activates nodes of the task structure and
passes control to that node. Each node maintains
a belief bc for a concept c, which is used to rank
the available actions by computing the inner prod-
uct of policy vectors and belief. The top-ranked
action am is selected by the system, i.e. it is ex-
ploiting the policy, and passed to the natural lan-
guage generator (NLG). Next, the top-ranked SLU
results for the active node and concept are used as
observation zu,c to update the belief to b0c, which
</bodyText>
<figureCaption confidence="0.984348">
Figure 1: System architecture with Task Structure
(task node example in detailed view)
</figureCaption>
<bodyText confidence="0.972816">
follows the standard method for POMDPs:
</bodyText>
<equation confidence="0.988945">
b0c(s0) _ E bc(s) T(s, am, s0) O(a, s0, zu,c)/pzu,,
s∈S
(1)
</equation>
<bodyText confidence="0.999843">
where probability b0 c(s0) is the updated belief of
being in state s0, which is computed as the sum of
the probabilities of transitioning from all previous
belief points s to s0 by taking machine action am
with probability T (s, am, s0) and observing zu,c
with (smoothed) probability O(am, s0, zu,c). Nor-
malization to obtain a valid probability distribu-
tion is performed by dividing by the probability of
the observation pzu,,.
A concept remains active until a submit action
is selected. At that point, the next active node is
retrieved from the task structure and immediately
used for action selection with an initially uniform
belief. Submit actions are not communicated to
the user but collected and used for the database
query at the end of the dialog.
Overanswering, i.e. the user providing more in-
formation than directly asked for, is handled by de-
layed belief updating: the SLU results are stored
until the first concept of a matching type becomes
active. This is a heuristic rule designed to ensure
that a concept is interpreted in its correct role. Op-
erationally, unused SLU results zu,d (where con-
cept d 7� c) are passed on to the next activated
task node (see also figure 1).
</bodyText>
<sectionHeader confidence="0.997554" genericHeader="method">
3 Experiments and data analysis
</sectionHeader>
<bodyText confidence="0.99924925">
We conducted user studies with two systems in-
volving 20 subjects and 8 tasks in each study.
The systems use a Voice XML platform to drive
ASR and TTS components. Speech recognition is
</bodyText>
<figure confidence="0.92449852173913">
User
ASR
TTS
SLU
NLG
PASSIVE
am
zu,c
zu,d
DM
ACTIVE
BLOCKED
BLOCKED
BLOCKED BLOCKED
OPEN OPEN
Condition: (activity=
lodging-enquiry ∨
lodging-reservation),
ConceptName location,
ConceptRole: location-lodging,
Status: ACTIVE,
Action: question-location.
Belief:
</figure>
<page confidence="0.841228">
214
</page>
<table confidence="0.999877666666667">
Lodging Task Event Enquiry
TCR #turns TCR #turns
Rule-based DM 75.5% 13.7 66.7% 8.7
(40/53) (v=4.8) (28/42) (v=3.3)
POMDP-DM 78.1% 23.0 84.3% 14.4
(50/64) (v=8.8) (27/32) (v=4.5)
</table>
<tableCaption confidence="0.999886">
Table 1: Task completion and length metrics
</tableCaption>
<bodyText confidence="0.999818625">
based on statistical language models for the open-
ing prompt, and is grammar-based otherwise. One
system used the hybrid POMDP-DM, the other
is a rule-based dialog manager that uses explicit,
heuristically set confidence thresholds to trigger
the use of clarification questions (Varges et al.,
2008).
Dialog length and task completion Table 1
shows task completion rates (‘TCR’) and dura-
tions (‘#turns’) for the POMDP and rule-based
systems. Task completion in this metric is defined
as the number of tasks of a certain type that were
successfully concluded. Duration is measured in
the number of turn pairs consisting of a system
action followed by a user action. We combine
the counts for two closely related lodging tasks.
The number of tasks is shown in brackets. Table
1 shows that the POMDP-DM successfully con-
cludes more and longer lodging tasks and almost
as many event tasks. In general, the POMDP poli-
cies can be described as more cautious although
obviously the dialog length of the rule system de-
pends on the chosen thresholds.
Concept precision at the value level In order
to measure the effect of the clarification strategies
in both systems, we computed concept precisions
for two different mentions of a concept in a dialog
(table 2): first mentions and final values after clar-
ifications and similar strategies. The rationale for
this metric is that the last mentioned concept value
is the value that the system ultimately obtains from
the user, which is used in the database query:
</bodyText>
<listItem confidence="0.98230425">
• if the system decides not to use clarifications,
the only mentioned value is the accepted one,
• if the system verifies and obtains a positive
answer, the last mentioned value is the ac-
cepted one,
• if the system verifies and obtains a negative
answer, the user will mention a new value
(which may or may not be accepted).
</listItem>
<bodyText confidence="0.9841375">
Thus, this metric is a uniform way of capturing
the obtained values from systems that internally
</bodyText>
<table confidence="0.998666666666667">
Rule -based DM first POMDP -DM
first final A% final A%
a) activity 0.78 0.74 -4.1 0.83 0.88 5.0
b) location 0.64 0.74 15.8 0.69 0.73 6.3
c) starrating 0.67 0.70 3.4 0.90 0.97 7.7
d) month 0.85 0.89 4.3 0.76 0.86 12.7
e) day 0.70 0.76 8.3 0.61 0.76 25.3
ALL (a-e) 0.74 0.78 5.2 0.74 0.83 12.1
Clarifications 0.84 0.85 1.5 0.96 0.87 -8.8
</table>
<tableCaption confidence="0.999164">
Table 2: Concept precision of first vs final value
</tableCaption>
<bodyText confidence="0.9955234">
use very different dialog managers and representa-
tions. The actual precision of a concept C is calcu-
lated by comparing SLU results to annotations and
counting true positives (matches M) and false pos-
itives (separated into mismatches N and entirely
</bodyText>
<equation confidence="0.7893505">
un-annotated concepts U): Prec(C) = M
M+�+U .
</equation>
<bodyText confidence="0.994454424242424">
Unrecognized concepts, on the other hand, are re-
call related and not counted since they cannot be
part of any system belief.
As table 2 clearly shows, the use of clarification
strategies has a positive effect on concept preci-
sion in both systems. The exception is the preci-
sion of concept activity in the rule-based system
for which the system reprompted rather than ver-
ified.1 In table 2, row ‘All’ refers to the average
weighted precision of the five concepts. Both sys-
tems start from a similar level of overall precision.
The relative improvement of the POMDP-DM for
all concepts is 12.1%, compared to 5.2% of the
rule-based DM.
We conducted a statistical significance test by
computing the delta in the form of three values for
individual data points, i.e. dialogs, and assigned
+1 for all changes from non-match to match, -1
for a change in the opposite direction and 0 for ev-
erything else (e.g. from mismatch to mismatch).
We found that, although there is a tendency for
the POMDP-DM to perform better, the difference
is not statistically significant at p=0.05 (a possi-
ble explanation is the data size since we are using
human subjects).
We furthermore measured the precision of rec-
ognizing ’yes/no’ answers to clarification ques-
tions. In contrast to actual concepts, there is no be-
lief distribution for these in the DM since clarifica-
tion actions are part of the concept POMDP mod-
els. We are thus dealing with individual one-off
recognition results that should be entirely indepen-
dent of each other. However, as table 2 (bottom)
</bodyText>
<footnote confidence="0.987109333333333">
1The second value obtained may be incorrect but above
the confidence threshold; note that the rule system does not
maintain a belief distribution over values.
</footnote>
<page confidence="0.998669">
215
</page>
<bodyText confidence="0.997838833333333">
shows, the precision of verifications decreases for
the hybrid POMDP system. A plausible expla-
nation for this is the increasing impatience of the
users due to the longer dialog duration.
Characterization of dialog strategies For
some concepts, the best policy is to ask the
concept question once and then verify once before
committing to the value (assuming the answer is
positive). Other policies verify the same value
twice. Another learned strategy is to ask the orig-
inal concept question twice and then only verify
the value once (assuming that the understood
value was the same in both concept questions). In
other words, the individual concept policies show
different types of strategies regarding uncertainty
handling. This is in marked contrast to the
manually programmed DM that always asks the
concept question once and verifies it if needed
(concept activity being the exception).
HCI and language generation The domain is
sufficiently simple to use template-based genera-
tion techniques to produce the surface forms of
the responses. However, the experiments with the
POMDP-DM highlight some new challenges re-
garding HCI aspects of spoken dialog systems: the
choice of actions may not be ‘natural’ from the
user’s perspective, for example if the system asks
for a concept twice. However, it should be possi-
ble to better communicate the (change in the) be-
lief to the user.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.9999784">
The pragmatist tradition of dialog processing uses
explicit representations of dialog structure to take
decisions about clarification actions. These mod-
els are more fine-grained and often deal with writ-
ten text, e.g. (Purver, 2006), whereas in spo-
ken dialog systems a major challenge is managing
the uncertainty of the recognition. Reinforcement
learning approaches to dialog management learn
decisions from (often simulated) dialog data in a
less deliberative way. For example, the Hidden In-
formation State model (Young et al., 2010) uses a
reduced summary space that abstracts away many
of the details of observations and dialog state, and
mainly looks at the confidence scores of the hy-
potheses. This seems to imply that clarification
strategies are not tailored toward individual con-
cepts and their values. (Bui et al., 2009) uses fac-
tored POMDP representations that seem closest to
our approach. However, the effect of clarifications
does not seem to have been investigated.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999295">
We presented evaluation results for a hybrid
POMDP system and compared it to a rule-based
one. The POMDP system achieves higher con-
cept precision albeit at the cost of longer dialogs,
i.e. there is an empirically measurable trade-off
between concept precision and dialog length.
</bodyText>
<sectionHeader confidence="0.998936" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998938">
This work was partially supported by the Eu-
ropean Commission Marie Curie Excellence
Grant for the ADAMACH project (contract No.
022593).
</bodyText>
<sectionHeader confidence="0.99935" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993860666666667">
T.H. Bui, M. Poel, A. Nijholt, and J. Zwiers. 2009.
A tractable hybrid DDN-POMDP approach to affec-
tive dialogue modeling for probabilistic frame-based
dialogue systems. Natural Language Engineering,
15(2):273–307.
Leslie Pack Kaelbling, Michael L. Littman, and An-
thony R. Cassandra. 1998. Planning and acting in
partially observable stochastic domains. Artificial
Intelligence, 101:99–134.
H. Kurniawati, D. Hsu, and W.S. Lee. 2008. SARSOP:
Efficient point-based POMDP planning by approxi-
mating optimally reachable belief spaces. In Proc.
Robotics: Science and Systems.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1).
Matthew Purver. 2006. CLARIE: Handling clarifica-
tion requests in a dialogue system. Research on Lan-
guage and Computation, 4(2-3):259–288, October.
Sebastian Varges, Giuseppe Riccardi, and Silvia Quar-
teroni. 2008. Persistent information state in a data-
centric architecture. In Proceedings of the 9th SIG-
dial Workshop on Discourse and Dialogue, Colum-
bus, Ohio.
Sebastian Varges, Giuseppe Riccardi, Silvia Quar-
teroni, and Alexei V. Ivanov. 2009. The explo-
ration/exploitation trade-off in reinforcement learn-
ing for dialogue management. In Proceedings of
IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU).
S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The Hid-
den Information State Model: a practical framework
for POMDP-based spoken dialogue management.
Computer Speech and Language, 24:150–174.
</reference>
<page confidence="0.999142">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.339656">
<title confidence="0.938507">Investigating Clarification Strategies in Hybrid POMDP Dialog Manager</title>
<author confidence="0.995388">Varges Quarteroni Riccardi V</author>
<affiliation confidence="0.9965255">Department of Information Engineering and Computer University of Trento, 38050 Povo di Trento,</affiliation>
<abstract confidence="0.958884">We investigate the clarification strategies exhibited by a hybrid POMDP dialog manager based on data obtained from a phone-based user study. The dialog manager combines task structures with a number of POMDP policies each optimized for obtaining an individual concept. We investigate the relationship between dialog length and task completion. In order to measure the effectiveness of the clarification strategies, we compute concept precisions for two different mentions of the concept in the dialog: first mentions and final values after clarifications and similar strategies, and compare this to a rulebased system on the same task. We observe an improvement in concept precision of 12.1% for the hybrid POMDP compared to 5.2% for the rule-based system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T H Bui</author>
<author>M Poel</author>
<author>A Nijholt</author>
<author>J Zwiers</author>
</authors>
<title>A tractable hybrid DDN-POMDP approach to affective dialogue modeling for probabilistic frame-based dialogue systems.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="14845" citStr="Bui et al., 2009" startWordPosition="2432" endWordPosition="2435">e.g. (Purver, 2006), whereas in spoken dialog systems a major challenge is managing the uncertainty of the recognition. Reinforcement learning approaches to dialog management learn decisions from (often simulated) dialog data in a less deliberative way. For example, the Hidden Information State model (Young et al., 2010) uses a reduced summary space that abstracts away many of the details of observations and dialog state, and mainly looks at the confidence scores of the hypotheses. This seems to imply that clarification strategies are not tailored toward individual concepts and their values. (Bui et al., 2009) uses factored POMDP representations that seem closest to our approach. However, the effect of clarifications does not seem to have been investigated. 5 Conclusions We presented evaluation results for a hybrid POMDP system and compared it to a rule-based one. The POMDP system achieves higher concept precision albeit at the cost of longer dialogs, i.e. there is an empirically measurable trade-off between concept precision and dialog length. Acknowledgments This work was partially supported by the European Commission Marie Curie Excellence Grant for the ADAMACH project (contract No. 022593). Ref</context>
</contexts>
<marker>Bui, Poel, Nijholt, Zwiers, 2009</marker>
<rawString>T.H. Bui, M. Poel, A. Nijholt, and J. Zwiers. 2009. A tractable hybrid DDN-POMDP approach to affective dialogue modeling for probabilistic frame-based dialogue systems. Natural Language Engineering, 15(2):273–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie Pack Kaelbling</author>
<author>Michael L Littman</author>
<author>Anthony R Cassandra</author>
</authors>
<title>Planning and acting in partially observable stochastic domains.</title>
<date>1998</date>
<journal>Artificial Intelligence,</journal>
<pages>101--99</pages>
<contexts>
<context position="4673" citStr="Kaelbling et al., 1998" startWordPosition="733" endWordPosition="736">ation system that uses 5 different policies that can be used in 8 different task roles (see below). For each concept we optimized an individual policy. The number of states of the POMDP can be limited to the concept values, for example a location name such as trento. The set of actions consists of a question to obtain the concept (e.g. question-location), a set of clarification actions (e.g. verify-trento) and a set of submit actions (e.g. submit-trento). POMDP modeling including a heuristically set reward structure follows the (simpler) ‘tiger problem’ that is well-known in the AI community (Kaelbling et al., 1998): the system has a number of actions to obtain further information which it can try and repeat in any order until it is ready to commit to a concept value. For optimization we used the APPL solver (Kurniawati et al., 2008). 2.2 Task structure and dialog management The use of individual policies is orchestrated by an explicit task structure that activates and deactivates them. The task structure is essentially a directed AND-OR graph with a common root node. The dialog manager maintains a separate belief distribution for each concept. Figure 1 shows the general system architecture with a schema</context>
</contexts>
<marker>Kaelbling, Littman, Cassandra, 1998</marker>
<rawString>Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. 1998. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101:99–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kurniawati</author>
<author>D Hsu</author>
<author>W S Lee</author>
</authors>
<title>SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces.</title>
<date>2008</date>
<booktitle>In Proc. Robotics: Science and Systems.</booktitle>
<contexts>
<context position="4895" citStr="Kurniawati et al., 2008" startWordPosition="775" endWordPosition="778">s, for example a location name such as trento. The set of actions consists of a question to obtain the concept (e.g. question-location), a set of clarification actions (e.g. verify-trento) and a set of submit actions (e.g. submit-trento). POMDP modeling including a heuristically set reward structure follows the (simpler) ‘tiger problem’ that is well-known in the AI community (Kaelbling et al., 1998): the system has a number of actions to obtain further information which it can try and repeat in any order until it is ready to commit to a concept value. For optimization we used the APPL solver (Kurniawati et al., 2008). 2.2 Task structure and dialog management The use of individual policies is orchestrated by an explicit task structure that activates and deactivates them. The task structure is essentially a directed AND-OR graph with a common root node. The dialog manager maintains a separate belief distribution for each concept. Figure 1 shows the general system architecture with a schematic view of the task structure, and additionally a more detailed view of an active location node. In the example, the root node has already finished and the system is currently obtaining the location for a lodging task. Th</context>
</contexts>
<marker>Kurniawati, Hsu, Lee, 2008</marker>
<rawString>H. Kurniawati, D. Hsu, and W.S. Lee. 2008. SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces. In Proc. Robotics: Science and Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
<author>W Eckert</author>
</authors>
<title>A stochastic model of human-machine interaction for learning dialog strategies.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1809" citStr="Levin et al., 2000" startWordPosition="275" endWordPosition="278">rt of the spoken dialog system that takes the action decision. A major motivation is to improve robustness in the face of uncertainty, in particular due to speech recognition errors. The interaction is characterized as a dynamic system that manipulates its environment by performing dialog actions and perceives feedback from the environment through its sensors. The original sensory information is obtained from the speech recognition (ASR) results which are typically processed by a spoken language understanding module (SLU) before being passed on to the dialog manager (DM). The seminal work of (Levin et al., 2000) modeled dialog management as a Markov Decision Process (MDP). Using reinforcement learning as the general learning paradigm, an MDP-based dialog manager incrementally acquires a policy by obtaining rewards about actions it performed in specific dialog states. As we found in earlier experiments, an MDP can learn to gradually drop the use of clarification questions if there is no noise. This is due to the fact that clarifications do not improve the outcome of the dialog, i.e. the reward. However, with extremely high levels of noise, the learner prefers to end the dialog immediately (Varges et a</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
</authors>
<title>CLARIE: Handling clarification requests in a dialogue system.</title>
<date>2006</date>
<journal>Research on Language and Computation,</journal>
<pages>4--2</pages>
<contexts>
<context position="14247" citStr="Purver, 2006" startWordPosition="2339" endWordPosition="2340">face forms of the responses. However, the experiments with the POMDP-DM highlight some new challenges regarding HCI aspects of spoken dialog systems: the choice of actions may not be ‘natural’ from the user’s perspective, for example if the system asks for a concept twice. However, it should be possible to better communicate the (change in the) belief to the user. 4 Related work The pragmatist tradition of dialog processing uses explicit representations of dialog structure to take decisions about clarification actions. These models are more fine-grained and often deal with written text, e.g. (Purver, 2006), whereas in spoken dialog systems a major challenge is managing the uncertainty of the recognition. Reinforcement learning approaches to dialog management learn decisions from (often simulated) dialog data in a less deliberative way. For example, the Hidden Information State model (Young et al., 2010) uses a reduced summary space that abstracts away many of the details of observations and dialog state, and mainly looks at the confidence scores of the hypotheses. This seems to imply that clarification strategies are not tailored toward individual concepts and their values. (Bui et al., 2009) u</context>
</contexts>
<marker>Purver, 2006</marker>
<rawString>Matthew Purver. 2006. CLARIE: Handling clarification requests in a dialogue system. Research on Language and Computation, 4(2-3):259–288, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Varges</author>
<author>Giuseppe Riccardi</author>
<author>Silvia Quarteroni</author>
</authors>
<title>Persistent information state in a datacentric architecture.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="8616" citStr="Varges et al., 2008" startWordPosition="1390" endWordPosition="1393">me location, ConceptRole: location-lodging, Status: ACTIVE, Action: question-location. Belief: 214 Lodging Task Event Enquiry TCR #turns TCR #turns Rule-based DM 75.5% 13.7 66.7% 8.7 (40/53) (v=4.8) (28/42) (v=3.3) POMDP-DM 78.1% 23.0 84.3% 14.4 (50/64) (v=8.8) (27/32) (v=4.5) Table 1: Task completion and length metrics based on statistical language models for the opening prompt, and is grammar-based otherwise. One system used the hybrid POMDP-DM, the other is a rule-based dialog manager that uses explicit, heuristically set confidence thresholds to trigger the use of clarification questions (Varges et al., 2008). Dialog length and task completion Table 1 shows task completion rates (‘TCR’) and durations (‘#turns’) for the POMDP and rule-based systems. Task completion in this metric is defined as the number of tasks of a certain type that were successfully concluded. Duration is measured in the number of turn pairs consisting of a system action followed by a user action. We combine the counts for two closely related lodging tasks. The number of tasks is shown in brackets. Table 1 shows that the POMDP-DM successfully concludes more and longer lodging tasks and almost as many event tasks. In general, th</context>
</contexts>
<marker>Varges, Riccardi, Quarteroni, 2008</marker>
<rawString>Sebastian Varges, Giuseppe Riccardi, and Silvia Quarteroni. 2008. Persistent information state in a datacentric architecture. In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Varges</author>
<author>Giuseppe Riccardi</author>
<author>Silvia Quarteroni</author>
<author>Alexei V Ivanov</author>
</authors>
<title>The exploration/exploitation trade-off in reinforcement learning for dialogue management.</title>
<date>2009</date>
<booktitle>In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).</booktitle>
<contexts>
<context position="2418" citStr="Varges et al., 2009" startWordPosition="377" endWordPosition="380"> al., 2000) modeled dialog management as a Markov Decision Process (MDP). Using reinforcement learning as the general learning paradigm, an MDP-based dialog manager incrementally acquires a policy by obtaining rewards about actions it performed in specific dialog states. As we found in earlier experiments, an MDP can learn to gradually drop the use of clarification questions if there is no noise. This is due to the fact that clarifications do not improve the outcome of the dialog, i.e. the reward. However, with extremely high levels of noise, the learner prefers to end the dialog immediately (Varges et al., 2009). In contrast to deliberate decision making in the pragmatist tradition of dialog processing, reinforcement learning can be regarded as low-level decision making. MDPs do not account for the observational uncertainty of the speech recognition results, a key challenge in spoken dialog systems. Partially Observable Markov Decision Process (POMDPs) address this issue by explicitly modeling how the distribution of observations is governed by states and actions. In this work, we describe the evaluation of a divide-and-conquer approach to dialog management with POMDPs that optimizes policies for acq</context>
</contexts>
<marker>Varges, Riccardi, Quarteroni, Ivanov, 2009</marker>
<rawString>Sebastian Varges, Giuseppe Riccardi, Silvia Quarteroni, and Alexei V. Ivanov. 2009. The exploration/exploitation trade-off in reinforcement learning for dialogue management. In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>M Gasic</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Yu</author>
</authors>
<title>The Hidden Information State Model: a practical framework for POMDP-based spoken dialogue management. Computer Speech and Language,</title>
<date>2010</date>
<pages>24--150</pages>
<contexts>
<context position="14550" citStr="Young et al., 2010" startWordPosition="2383" endWordPosition="2386">ossible to better communicate the (change in the) belief to the user. 4 Related work The pragmatist tradition of dialog processing uses explicit representations of dialog structure to take decisions about clarification actions. These models are more fine-grained and often deal with written text, e.g. (Purver, 2006), whereas in spoken dialog systems a major challenge is managing the uncertainty of the recognition. Reinforcement learning approaches to dialog management learn decisions from (often simulated) dialog data in a less deliberative way. For example, the Hidden Information State model (Young et al., 2010) uses a reduced summary space that abstracts away many of the details of observations and dialog state, and mainly looks at the confidence scores of the hypotheses. This seems to imply that clarification strategies are not tailored toward individual concepts and their values. (Bui et al., 2009) uses factored POMDP representations that seem closest to our approach. However, the effect of clarifications does not seem to have been investigated. 5 Conclusions We presented evaluation results for a hybrid POMDP system and compared it to a rule-based one. The POMDP system achieves higher concept prec</context>
</contexts>
<marker>Young, Gasic, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, and K. Yu. 2010. The Hidden Information State Model: a practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24:150–174.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>