<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009181">
<title confidence="0.953973">
SZTE-NLP: Aspect Level Opinion Mining Exploiting Syntactic Cues
</title>
<author confidence="0.999548">
Viktor Hangya1, G´abor Berend1, Istv´an Varga2∗, Rich´ard Farkas1
</author>
<affiliation confidence="0.9978825">
1University of Szeged
Department of Informatics
</affiliation>
<email confidence="0.965536">
{hangyav,berendg,rfarkas}@inf.u-szeged.hu
</email>
<note confidence="0.517737">
2NEC Corporation, Japan
Knowledge Discovery Research Laboratories
</note>
<email confidence="0.994057">
vistvan@az.jp.nec.com
</email>
<sectionHeader confidence="0.993791" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999821615384616">
In this paper, we introduce our contribu-
tions to the SemEval-2014 Task 4 – As-
pect Based Sentiment Analysis (Pontiki et
al., 2014) challenge. We participated in
the aspect term polarity subtask where
the goal was to classify opinions related
to a given aspect into positive, negative,
neutral or conflict classes. To solve this
problem, we employed supervised ma-
chine learning techniques exploiting a rich
feature set. Our feature templates ex-
ploited both phrase structure and depen-
dency parses.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993835">
The booming volume of user-generated content
and the consequent popularity growth of online re-
view sites has led to vast amount of user reviews
that are becoming increasingly difficult to grasp.
There is desperate need for tools that can automat-
ically process and organize information that might
be useful for both users and commercial agents.
Such early approaches have focused on deter-
mining the overall polarity (e.g., positive, nega-
tive, neutral, conflict) or sentiment rating (e.g.,
star rating) of various entities (e.g., restaurants,
movies, etc.) cf. (Ganu et al., 2009). While the
overall polarity rating regarding a certain entity
is, without question, extremely valuable, it fails
to distinguish between various crucial dimensions
based on which an entity can be evaluated. Evalu-
ations targeting distinct key aspects (i.e., function-
ality, price, design, etc) provide important clues
that may be targeted by users with different priori-
ties concerning the entity in question, thus holding
</bodyText>
<footnote confidence="0.633777833333333">
∗The work was done while this author was working as a
guest researcher at the University of Szeged
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999194066666667">
much greater value in one’s decision making pro-
cess.
In this paper, we introduce our contribution to
the SemEval-2014 Task 4 – Aspect Based Sen-
timent Analysis (Pontiki et al., 2014) challenge.
We participated in the aspect term polarity sub-
task where the goal was to classify opinions which
are related to a given aspect into positive, nega-
tive, neutral or conflict classes. We employed su-
pervised machine learning techniques exploiting a
rich feature set for target polarity detection, with
a special emphasis on features that deal with the
detection of aspect scopes. Our system achieved
an accuracy of 0.752 and 0.669 for the restaurant
and laptop domains, respectively.
</bodyText>
<sectionHeader confidence="0.982321" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999974307692308">
We employed a four-class supervised (positive,
negative, neutral and conflict) classifier here. As
a normalization step, we converted the given texts
into their lowercased forms. Bag-of-words fea-
tures comprised the basic feature set for our max-
imum entropy classifier, which was shown to be
helpful in polarity detection (Hangya and Farkas,
2013).
In the case of aspect-oriented sentiment detec-
tion, we found it important to locate text parts
that refer to particular aspects. For this, we used
several syntactic parsing methods and introduced
parse tree based features.
</bodyText>
<subsectionHeader confidence="0.890824">
2.1 Distance-weighted Bag-of-words Features
</subsectionHeader>
<bodyText confidence="0.999983166666667">
Initially, we used n-gram token features (unigrams
and bigrams). It could be helpful to take into con-
sideration the distance between the token in ques-
tion and the mention of the target aspect. The
closer a token is to an entity the more plausible
that the given token is related to the aspect.
</bodyText>
<page confidence="0.926636">
610
</page>
<note confidence="0.723466">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 610–614,
Dublin, Ireland, August 23-24, 2014.
</note>
<figure confidence="0.997426">
NMOD
ROOT
SBJ
PRD
COORD
NMOD
CONJ
P
SBJ PRD
&lt;ROOT&gt; The food was great but the service was awful .
DT NN VBD JJ CC DT NN VBD JJ .
</figure>
<figureCaption confidence="0.999985">
Figure 1: Dependency parse tree (MATE parser).
</figureCaption>
<bodyText confidence="0.998060333333333">
For this we used weighted feature vectors, and
weighted each n-gram feature by its distance in to-
kens from the mention of the given aspect:
</bodyText>
<equation confidence="0.6563885">
1
|i−j|,
</equation>
<bodyText confidence="0.999943666666667">
where n is the length of the review and the values
i, j are the positions of the actual word and the
mentioned aspect.
</bodyText>
<subsectionHeader confidence="0.99963">
2.2 Polarity Lexicon
</subsectionHeader>
<bodyText confidence="0.999993256410256">
To examine the polarity of the words comprising
a review, we incorporated the SentiWordNet sen-
timent lexicon (Baccianella et al., 2010) into our
feature set.
In this resource, synsets – i.e. sets of word
forms sharing some common meaning – are as-
signed positivity, negativity and objectivity scores.
These scores can be interpreted as the probabilities
of seeing some representatives of the synsets in
a positive, negative and neutral meaning, respec-
tively. However, it is not unequivocal to deter-
mine automatically which particular synset a given
word belongs to with respect its context. Consider
the word form great for instance, which might
have multiple, fundamentally different sentiment
connotations in different contexts, e.g. in expres-
sions such as “great food” and “great crisis”.
We determined the most likely synset a particu-
lar word form belonged to based on its contexts by
selecting the synset, the members of which were
the most appropriate for the lexical substitution
of the target word. The extent of the appropri-
ateness of a word being a substitute for another
word was measured relying on Google’s N-Gram
Corpus, using the indexing framework described
in (Ceylan and Mihalcea, 2011).
We look up the frequencies of the n-grams that
we derive from the context by replacing the tar-
get words with its synonyms(great) from various
synsets, e.g. good versus big. We count down the
frequency of the phrases food is good and food is
big in a huge set of in-domain documents (Cey-
lan and Mihalcea, 2011). Than we choose the
meaning which has the highest probability, good
in this case. This way we assign a polarity value
for each word in a text and created three new fea-
tures for the machine learning algorithm, which
are the number of positive, negative and objective
words in the given document.
</bodyText>
<subsectionHeader confidence="0.998251">
2.3 Negation Scope Detection
</subsectionHeader>
<bodyText confidence="0.9999595">
Since negations are quite frequent in user reviews
and have the tendency to flip polarities, we took
special care of negation expressions. We collected
a set of negation expressions, like not, don’t, etc.
and a set of delimiters and, or, etc. It is reasonable
to think that the scope of a negation starts when
we detect a negation word in the sentence and it
lasts until the next delimiter. If an n-gram was in
a negation scope we added a NOT prefix to that
feature.
</bodyText>
<subsectionHeader confidence="0.983507">
2.4 Syntax-based Features
</subsectionHeader>
<bodyText confidence="0.999978375">
It is very important to discriminate between text
fragments that are referring to the given aspect and
the fragments that do not, within the same sen-
tence. To detect the relevant text fragments, we
used dependency and constituency parsers. Since
adjectives are good indicators of opinion polarity,
we add the ones to our feature set which are in
close proximity with the given aspect term. We
define proximity between an adjective and an as-
pect term as the length of the non-directional path
between them in the dependency tree. We gather
adjectives in proximity less than 6.
Another feature, which is not aspect specific but
can indicate the polarity of an opinion, is the polar-
ity of words’ modifiers. We defined a feature tem-
plate for tokens whose syntactic head is present in
</bodyText>
<figure confidence="0.8812621">
1
en
611
ROOT
S
.
S
.
VP
ADJ
</figure>
<page confidence="0.990272">
612
</page>
<sectionHeader confidence="0.999804" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.999386">
In this section, we will report our results on the
shared task database which consists of English
product reviews. There are 3, 000 laptop and
restaurant related sentences, respectively. Aspects
were annotated in these sentences, resulting in a
total of 6,051 annotated aspects. In our experi-
ments, we used maximum entropy classifier with
the default parameter settings of the Java-based
machine learning framework MALLET (McCal-
lum, ).
</bodyText>
<figureCaption confidence="0.999519">
Figure 3: Accuracy on the restaurant test data.
</figureCaption>
<bodyText confidence="0.999798869565217">
Our accuracy measured on the restaurant and
laptop test databases can be seen on figures 3 and
4. On the x-axis the accuracy loss can be seen
comparing to our baseline (n-gram features only)
and full-system, while turning off various sets of
features. Firstly, the weighting of n-gram features
are absent, then features based on aspect clustering
and words which indicate polarity in texts. After-
wards, features that are created using dependency
and constituency parsing are turned off and lastly
sentiment features based on the SentiWordNet lex-
icon are ignored. It can be seen that omitting the
features based on parsing results in the most seri-
ous drop in performance. We achieved 1.1 and 2.6
error reduction on the restaurant and laptop test
data using these features, respectively.
In Table 1 the results of several other participat-
ing teams can be seen on the restaurant and laptop
test data. There were more than 30 submissions,
from which we achieved the sixth and third best
results on the restaurants and laptop domains, re-
spectively. At the bottom of the table the official
baselines for each domain can be seen.
</bodyText>
<table confidence="0.987748625">
Team restaurant laptop
DCU 0.809 0.704
NRC-Canada 0.801 0.704
SZTE-NLP 0.752 0.669
UBham 0.746 0.666
USF 0.731 0.645
ECNU 0.707 0.611
baseline 0.642 0.510
</table>
<figure confidence="0.9896059375">
0.78
0.76
0.74
0.72
0.7
systems
full-system
baseline
0.68
0.66
0.64
0.62
0.7
systems
full-system
baseline
</figure>
<figureCaption confidence="0.999805">
Figure 4: Accuracy on the laptop test data.
</figureCaption>
<tableCaption confidence="0.913082">
Table 1: Accuracy results of several other partici-
pants. Our system is named SZTE-NLP.
</tableCaption>
<sectionHeader confidence="0.999089" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999947166666667">
In this paper, we presented our contribution to the
aspect term polarity subtask of the SemEval-2014
Task 4 – Aspect Based Sentiment Analysis chal-
lenge. We proposed a supervised machine learn-
ing technique that employs a rich feature set tar-
geting aspect term polarity detection. Among the
features designed here, the syntax-based feature
group for the determination of the scopes of the as-
pect terms showed the highest contribution. In the
end, our system was ranked as 6th and 3rd, achiev-
ing an 0.752 and 0.669 accuracies for the restau-
rant and laptop domains, respectively.
</bodyText>
<page confidence="0.998913">
613
</page>
<sectionHeader confidence="0.998328" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9993955">
Viktor Hangya and Istv´an Varga were funded in
part by the European Union and the European
Social Fund through the project FuturICT.hu
(T´AMOP-4.2.2.C-11/1/KONV-2012-0013).
G´abor Berend and Rich´ard Farkas was partially
funded by the ”Hungarian National Excellence
Program“ (T´AMOP 4.2.4.A/2-11-1-2012-0001),
co-financed by the European Social Fund.
</bodyText>
<sectionHeader confidence="0.998455" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999768666666667">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10).
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89–97, Bei-
jing, China, August. Coling 2010 Organizing Com-
mittee.
Hakan Ceylan and Rada Mihalcea. 2011. An efficient
indexer for large n-gram corpora. In ACL (System
Demonstrations), pages 103–108. The Association
for Computer Linguistics.
Gayatree Ganu, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predic-
tions using review text content. In WebDB.
Viktor Hangya and Richard Farkas. 2013. Target-
oriented opinion mining from tweets. In Cognitive
Infocommunications (CogInfoCom), 2013 IEEE 4th
International Conference on, pages 251–254. IEEE.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st ACL, pages 423–430.
Andrew Kachites McCallum. Mallet: A machine
learning for language toolkit.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation,
SemEval ’14.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, October.
</reference>
<page confidence="0.998146">
614
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.553939">
<title confidence="0.999668">SZTE-NLP: Aspect Level Opinion Mining Exploiting Syntactic Cues</title>
<author confidence="0.966556">G´abor Istv´an Rich´ard</author>
<email confidence="0.605946">of</email>
<affiliation confidence="0.984283666666667">Department of Corporation, Knowledge Discovery Research</affiliation>
<email confidence="0.998213">vistvan@az.jp.nec.com</email>
<abstract confidence="0.995618857142857">In this paper, we introduce our contributo the SemEval-2014 Task 4 – As- Based Sentiment Analysis et al., 2014) challenge. We participated in term polarity where the goal was to classify opinions related to a given aspect into positive, negative, neutral or conflict classes. To solve this problem, we employed supervised machine learning techniques exploiting a rich feature set. Our feature templates exploited both phrase structure and dependency parses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10).</booktitle>
<contexts>
<context position="4471" citStr="Baccianella et al., 2010" startWordPosition="693" endWordPosition="696">and, August 23-24, 2014. NMOD ROOT SBJ PRD COORD NMOD CONJ P SBJ PRD &lt;ROOT&gt; The food was great but the service was awful . DT NN VBD JJ CC DT NN VBD JJ . Figure 1: Dependency parse tree (MATE parser). For this we used weighted feature vectors, and weighted each n-gram feature by its distance in tokens from the mention of the given aspect: 1 |i−j|, where n is the length of the review and the values i, j are the positions of the actual word and the mentioned aspect. 2.2 Polarity Lexicon To examine the polarity of the words comprising a review, we incorporated the SentiWordNet sentiment lexicon (Baccianella et al., 2010) into our feature set. In this resource, synsets – i.e. sets of word forms sharing some common meaning – are assigned positivity, negativity and objectivity scores. These scores can be interpreted as the probabilities of seeing some representatives of the synsets in a positive, negative and neutral meaning, respectively. However, it is not unequivocal to determine automatically which particular synset a given word belongs to with respect its context. Consider the word form great for instance, which might have multiple, fundamentally different sentiment connotations in different contexts, e.g. </context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>89--97</pages>
<location>Beijing, China,</location>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89–97, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hakan Ceylan</author>
<author>Rada Mihalcea</author>
</authors>
<title>An efficient indexer for large n-gram corpora.</title>
<date>2011</date>
<booktitle>In ACL (System Demonstrations),</booktitle>
<pages>103--108</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="5539" citStr="Ceylan and Mihalcea, 2011" startWordPosition="863" endWordPosition="866">its context. Consider the word form great for instance, which might have multiple, fundamentally different sentiment connotations in different contexts, e.g. in expressions such as “great food” and “great crisis”. We determined the most likely synset a particular word form belonged to based on its contexts by selecting the synset, the members of which were the most appropriate for the lexical substitution of the target word. The extent of the appropriateness of a word being a substitute for another word was measured relying on Google’s N-Gram Corpus, using the indexing framework described in (Ceylan and Mihalcea, 2011). We look up the frequencies of the n-grams that we derive from the context by replacing the target words with its synonyms(great) from various synsets, e.g. good versus big. We count down the frequency of the phrases food is good and food is big in a huge set of in-domain documents (Ceylan and Mihalcea, 2011). Than we choose the meaning which has the highest probability, good in this case. This way we assign a polarity value for each word in a text and created three new features for the machine learning algorithm, which are the number of positive, negative and objective words in the given doc</context>
</contexts>
<marker>Ceylan, Mihalcea, 2011</marker>
<rawString>Hakan Ceylan and Rada Mihalcea. 2011. An efficient indexer for large n-gram corpora. In ACL (System Demonstrations), pages 103–108. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gayatree Ganu</author>
<author>Noemie Elhadad</author>
<author>Amelie Marian</author>
</authors>
<title>Beyond the stars: Improving rating predictions using review text content.</title>
<date>2009</date>
<booktitle>In WebDB.</booktitle>
<contexts>
<context position="1405" citStr="Ganu et al., 2009" startWordPosition="199" endWordPosition="202">e and dependency parses. 1 Introduction The booming volume of user-generated content and the consequent popularity growth of online review sites has led to vast amount of user reviews that are becoming increasingly difficult to grasp. There is desperate need for tools that can automatically process and organize information that might be useful for both users and commercial agents. Such early approaches have focused on determining the overall polarity (e.g., positive, negative, neutral, conflict) or sentiment rating (e.g., star rating) of various entities (e.g., restaurants, movies, etc.) cf. (Ganu et al., 2009). While the overall polarity rating regarding a certain entity is, without question, extremely valuable, it fails to distinguish between various crucial dimensions based on which an entity can be evaluated. Evaluations targeting distinct key aspects (i.e., functionality, price, design, etc) provide important clues that may be targeted by users with different priorities concerning the entity in question, thus holding ∗The work was done while this author was working as a guest researcher at the University of Szeged This work is licensed under a Creative Commons Attribution 4.0 International Lice</context>
</contexts>
<marker>Ganu, Elhadad, Marian, 2009</marker>
<rawString>Gayatree Ganu, Noemie Elhadad, and Amelie Marian. 2009. Beyond the stars: Improving rating predictions using review text content. In WebDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viktor Hangya</author>
<author>Richard Farkas</author>
</authors>
<title>Targetoriented opinion mining from tweets.</title>
<date>2013</date>
<booktitle>In Cognitive Infocommunications (CogInfoCom), 2013 IEEE 4th International Conference on,</booktitle>
<pages>251--254</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3167" citStr="Hangya and Farkas, 2013" startWordPosition="469" endWordPosition="472">earning techniques exploiting a rich feature set for target polarity detection, with a special emphasis on features that deal with the detection of aspect scopes. Our system achieved an accuracy of 0.752 and 0.669 for the restaurant and laptop domains, respectively. 2 Approach We employed a four-class supervised (positive, negative, neutral and conflict) classifier here. As a normalization step, we converted the given texts into their lowercased forms. Bag-of-words features comprised the basic feature set for our maximum entropy classifier, which was shown to be helpful in polarity detection (Hangya and Farkas, 2013). In the case of aspect-oriented sentiment detection, we found it important to locate text parts that refer to particular aspects. For this, we used several syntactic parsing methods and introduced parse tree based features. 2.1 Distance-weighted Bag-of-words Features Initially, we used n-gram token features (unigrams and bigrams). It could be helpful to take into consideration the distance between the token in question and the mention of the target aspect. The closer a token is to an entity the more plausible that the given token is related to the aspect. 610 Proceedings of the 8th Internatio</context>
</contexts>
<marker>Hangya, Farkas, 2013</marker>
<rawString>Viktor Hangya and Richard Farkas. 2013. Targetoriented opinion mining from tweets. In Cognitive Infocommunications (CogInfoCom), 2013 IEEE 4th International Conference on, pages 251–254. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st ACL,</booktitle>
<pages>423--430</pages>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st ACL, pages 423–430.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<marker>McCallum, </marker>
<rawString>Andrew Kachites McCallum. Mallet: A machine learning for language toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitrios Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Semeval-2014 task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’14.</booktitle>
<contexts>
<context position="2317" citStr="Pontiki et al., 2014" startWordPosition="337" endWordPosition="340"> provide important clues that may be targeted by users with different priorities concerning the entity in question, thus holding ∗The work was done while this author was working as a guest researcher at the University of Szeged This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ much greater value in one’s decision making process. In this paper, we introduce our contribution to the SemEval-2014 Task 4 – Aspect Based Sentiment Analysis (Pontiki et al., 2014) challenge. We participated in the aspect term polarity subtask where the goal was to classify opinions which are related to a given aspect into positive, negative, neutral or conflict classes. We employed supervised machine learning techniques exploiting a rich feature set for target polarity detection, with a special emphasis on features that deal with the detection of aspect scopes. Our system achieved an accuracy of 0.752 and 0.669 for the restaurant and laptop domains, respectively. 2 Approach We employed a four-class supervised (positive, negative, neutral and conflict) classifier here. </context>
</contexts>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<date></date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, </marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>