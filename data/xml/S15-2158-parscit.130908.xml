<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018399">
<title confidence="0.9991725">
TALN-UPF: Taxonomy Learning Exploiting CRF-Based Hypernym
Extraction on Encyclopedic Definitions
</title>
<author confidence="0.71273">
Luis Espinosa-Anke and Horacio Saggion and Francesco Ronzano
</author>
<affiliation confidence="0.733520333333333">
Tractament Autom`atic del Llenguatge Natural (TALN)
Department of Information and Communication Technologies
Universitat Pompeu Fabra
</affiliation>
<address confidence="0.8768405">
Carrer T`anger, 122-140
08018 Barcelona, Spain
</address>
<email confidence="0.998604">
{luis.espinosa, horacio.saggion, francesco.ronzano}@upf.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999508133333333">
This paper describes the system submitted
by the TALN-UPF team to SEMEVAL Task
17 (Taxonomy Extraction Evaluation). We
present a method for automatically learning
a taxonomy from a flat terminology, which
benefits from a definition corpus obtained
by querying the BabelNet semantic network.
Then, we combine a machine-learning al-
gorithm for term-hypernym extraction with
linguistically-motivated heuristics for hyper-
nym decomposition. Our approach performs
well in terms of vertex coverage and newly
added vertices, while it shows room for im-
provement in terms of graph topology, edge
coverage and precision of novel edges.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9986829">
Learning semantic relations out of flat terminologies
is an appealing task due to its potential application
in tasks like Question Answering (Cui et al., 2005;
Boella et al., 2014), automatic glossary construction
(Muresan and Klavans, 2002), Ontology Learning
(Navigli et al., 2011) or Textual Entailment (Roller
et al., 2014). Today, in the context of massive web-
enabled data, hypernym (is-a) relations are the focus
of much research, as they constitute the backbone of
ontologies (Navigli et al., 2011). However, one chal-
lenge remains open in the automatic construction of
knowledge bases that exploit this type of relation. It
is unfeasible to have up-to-date semantic resources
for each domain, as they are limited in scope and
domain, and their manual construction is knowledge
intensive and time consuming (Fu et al., 2014).
Given this rationale, Task 17 (Bordea et al., 2015)
in the SEMEVAL 2015 set of shared tasks focuses
on Taxonomy Extraction Evaluation, i.e. the con-
struction of a taxonomy out of a flat set of terms be-
longing to one of the four domains of choice (food,
chemical, equipment and science). These terms have
to be hierarchically organized, and new terms are al-
lowed to be included in the taxonomy. As for eval-
uation, for each domain, two taxonomies were used
as gold standard: One created by domain experts;
and one derived from the WordNet taxonomy rooted
at the domain node, e.g. food1. Finally, evaluation
is carried out from two standpoints: (1) The taxon-
omy topology and the rate of replicated nodes and
edges are taken into account when compared to a
gold standard taxonomy; and (2) Human experts val-
idated as correct or incorrect a subset of the newly
added edges.
In this paper we describe our contribution to
this shared task. Our approach relies on a set of
definitional sentences for each term, from which
term—*hypernym relations are extracted using a
machine-learning classifier. In a second step,
linguistically-motivated rules are applied in order to
</bodyText>
<listItem confidence="0.993033833333333">
(1) extract a hypernym candidate when the confi-
dence of the classifier was below a threshold, and
(2) decompose multiword hypernyms in more gen-
eral concepts (e.g. from coca-cola—*carbonated soft
drink to carbonated soft drink—*soft drink and soft
drink—*drink).
</listItem>
<footnote confidence="0.99862525">
1For our domain notation we simply use the name of the
domain for manually constructed taxonomies (e.g. “food”),
and add the prefix wn for the WordNet taxonomies (e.g.
“wn food”).
</footnote>
<page confidence="0.970799">
949
</page>
<bodyText confidence="0.867136333333333">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 949–954,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
The remainder of the paper is structured as fol-
lows: Section 3 describes the modules of our ap-
proach, Section 4 presents and discusses the evalua-
tion procedure as well as results, and finally Section
5 analyzes the performance of our system as well as
the difficulties encountered, and suggests potential
avenues for future work.
</bodyText>
<sectionHeader confidence="0.974489" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999859">
Generally, taxonomy learning from text has been
carried out either following rule-based or distribu-
tional approaches. In terms of rule-based meth-
ods reported in the literature, (Hearst, 1992) in-
troduced lexico-syntactic patterns, which were ex-
ploited in subsequent work (Berland and Charniak,
1999; Kozareva et al., 2008; Widdows and Dorow,
2002; Girju et al., 2003). Distributional approaches,
on the other hand, have become increasingly pop-
ular due to the availability of large corpora. Sys-
tems aimed at extracting hypernym relations from
text have exploited hybrid patterns as word-class lat-
tices (Navigli and Velardi, 2010), syntactic relations
as features for an SVM classifier (Boella et al., 2014)
or word-embedding-based semantic projections (Fu
et al., 2014; Roller et al., 2014). Inspired by the re-
ported success in the latter methods, we opted for
combining syntactic patterns with machine learning
to extract hypernyms from domain sentences.
</bodyText>
<sectionHeader confidence="0.985481" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.9998285">
This section describes the main modules that consti-
tute our taxonomy learning system.
</bodyText>
<subsectionHeader confidence="0.999394">
3.1 Definition corpus compilation
</subsectionHeader>
<bodyText confidence="0.999022136363636">
We benefit from BabelNet, a very large multilin-
gual semantic network that combines, among other
resources, Wikipedia and WordNet (Navigli and
Ponzetto, 2010). We get a set of BabelNet synsets
associated to each term and for each synset, we ex-
tract its definition. In this step we assume that a
term’s definition appears in the first sentence of its
Wikipedia article, which is a regular practice in the
literature (see (Navigli and Velardi, 2010) or (Boella
et al., 2014)). This step allowed us to compile a
domain corpus of definitional knowledge, and thus
maximizing the number of relevant terms defini-
tions. However, noise is also introduced in our cor-
pus. For example, given the term botifarra (a Cata-
lan type of sausage), we add two definitions to our
corpus:
Relevant: Botifarra is a type of sausage and
one of the most important dishes of the Catalan
cuisine.
Noisy: Botifarra is a point trick-taking card
game for four players in fixed partnerships
played in Catalonia.
</bodyText>
<subsectionHeader confidence="0.999769">
3.2 Hypernym Extraction
</subsectionHeader>
<bodyText confidence="0.999789444444445">
Given a set of definitional text fragments where the
definiendum2 term is known, i.e. can be extracted
from the url of the Wikipedia page, our goal is to tag
the tokens of the definition that correspond to one or
more hypernyms. To this end, we train a Conditional
Random Fields (Lafferty et al., 2001) classifier3 with
the WCL Dataset (Navigli and Velardi, 2010). We
argue that CRFs are a valid approach for sequential
classification, and particularly for this task, due to
their potential to capture prior and posterior token
features on the current iteration. The WCL dataset
includes near 2000 definitional sentences with terms
and hypernyms manually annotated. We preprocess
and parse the WCL dataset with a dependency parser
(Bohnet, 2010), and then train our classifier with the
following set of features.
surface: A word’s surface form.
lemma: The lemma of the word.
pos: The word’s part-of-speech.
head id: The id of the word to which the cur-
rent token depends in a dependency syntactic
tree.
deprel: Syntactic function of the current word
in relation to its head.
def—nodef: Whether the current token ap-
pears before or after the first verb of the sen-
tence.
</bodyText>
<footnote confidence="0.594361666666667">
2The classic components lexicographic genus-et-differentia
definition are (1) Definiendum (concept being defined); (2)
genus (hypernym or immediate superordinate that describes the
definiendum); and (3) definiens or cluster of words that differ-
entiate a definiendum from others of its kind.
3https://code.google.com/p/crfpp/
</footnote>
<page confidence="0.994682">
950
</page>
<bodyText confidence="0.999489483870968">
term—noterm: Whether the token is part of
the definiendum term or not.
Our CRF classifier learns the above word-level
features in a word window of [−2,2]. The predic-
tion the classifier must learn follows the classic BIO
format, i.e. whether a word is at the beginning of
a hypernym phrase, inside or outside. We evalu-
ate this hypernym extraction module on the WCL
dataset (Navigli and Velardi, 2010) performing 10-
fold cross-validation. It achieves and F-score of
79.86, outperforming existing state-of-the-art sys-
tems described in the literature (Navigli and Velardi,
2010; Boella et al., 2014).
Despite the good performance of this module,
we observe two potential drawbacks in terms of
its fitness for the taxonomy learning task. Firstly,
we aim at recovering hypernym candidates even in
cases in which they are predicted with low confi-
dence at the classification step. We build on the as-
sumption that all encyclopedic definitions are very
likely to include a hypernym, and hypothesize that
it will help increasing recall while keeping preci-
sion at a reasonable rate. Secondly, when a mul-
tiword hypernym is retrieved by our module, it
might not match exactly a term from the seed ter-
minology (e.g. original term—*soft drink, and re-
trieved term—*carbonated soft drink). Therefore,
we aim at decomposing it by dropping one modifier
at a time and creating new arcs recursively. These
two steps are described in more detail in the follow-
ing subsection.
</bodyText>
<subsectionHeader confidence="0.753216">
Post-classification Heuristic
</subsectionHeader>
<bodyText confidence="0.999952916666667">
Our recall-enhancing strategy consists in a post-
classification heuristic inspired by Flati et al. (2014):
(1) We exploit the tree-like dependency structure of
a parsed sentence in order to find the most likely to-
ken to be the head of a hypernymic phrase. We look
for definitions where no hypernym was identified.
Then, we find the node with the Predicative Com-
plement (PRD) syntactic function. If such node is
not a stop-hypernym (such as type, class, family or
kind), we consider it a valid head of a hypernymic-
phrase4. Then, we collect all its noun and adjec-
tive children with the syntactic function Modifier of
</bodyText>
<footnote confidence="0.9989535">
4The full list of stop-hypernyms is available at
www.wibitaxonomy.org
</footnote>
<bodyText confidence="0.999109533333333">
Nominal (NMOD). If, however, such node is a stop-
hypernym, we go down the syntactic tree one level
and look for a direct Preposition node with syntactic
function NMOD. Then, we extract this preposition’s
adjective and noun children if they have the syntac-
tic function Modifier of Prepositional (PMOD).
For example, consider the following sentence:
“Whisky or whiskey is a type of distilled alcoholic
beverage made from fermented grain mash”. Here,
type is the Predicative Complement node but it is an
uninformative word for describing the term whisky.
Therefore, our algorithm goes one level down the
syntactic tree and identifies the token beverage as
the direct child of the preposition and therefore ex-
tracts this token as hypernym.
</bodyText>
<subsectionHeader confidence="0.996679">
3.3 Hypernym Decomposition
</subsectionHeader>
<bodyText confidence="0.999944588235294">
This step is aimed at generating deeper paths from
a term and its hypernym by recursively decompos-
ing a candidate hypernym. For example, consider
the previous example’s term—*hypernym relation if
the hypernym’s modifiers are taken into account:
whisky—*distilled alcoholic beverage. Our objective
is to generate the following set of relations: distilled
alcoholic beverage—*alcoholic beverage and alco-
holic beverage—*beverage. In this way, we improve
the taxonomy since, in taxonomy learning, longer
hypernymy paths should be preferred (Navigli et al.,
2011), and we enable other potential distilled alco-
holic beverages to be connected with alcoholic bev-
erage rather than the more generic term beverage.
We achieve this by performing a similar algorithm
as in the post-classification heuristic, i.e. exploiting
head and modifier relations in a dependency tree.
</bodyText>
<subsectionHeader confidence="0.991947">
3.4 Graph Generation
</subsectionHeader>
<bodyText confidence="0.999970083333333">
At this stage, we have a dataset of term—*hypernym
pairs, and from here populating the taxonomy is a
trivial task. For each pair, if neither term nor hy-
pernym exist in the graph, add both nodes and con-
nect them. If term exists in the graph, only add the
hypernym and connect the existing term node with
it. If on the contrary, only the hypernym is found in
the graph, connect the term to the existing hypernym
node. Finally, we go back to the initial flat terminol-
ogy and, if no path is found between a term node and
the root node, add a direct edge between them. This
last step guarantees that the taxonomy will preserve
</bodyText>
<page confidence="0.995553">
951
</page>
<table confidence="0.999795333333333">
chem wn chem equip wn equip food wn food sci wn sci
VC 1 0.997 1 1 0.8695 1 0.9977 0.8624
EC 0.0004 0.093 0.1577 0.0453 0.0359 0.0782 0.0172 0.1111
RNE 0.7089 0.9531 0.9235 6.903 0.9527 0.9315 3.4731 0.78
F&amp;M 0.2225 0.2787 0.4482 0.0901 0.3267 0.3091 0.2202 0.2126
Cycles no yes yes yes no yes yes no
Precision 0.0006 0.0889 0.1458 0.0287 0.0363 0.0775 0.0733 0.1246
Recall 0.0004 0.093 0.1577 0.2 0.0359 0.0782 0.2559 0.1111
F-Score 0.0005 0.0909 0.1515 0.0503 0.0361 0.0778 0.1139 0.1175
</table>
<tableCaption confidence="0.898523">
Table 1: Summary of the results obtained with our approach in the structural evaluation in terms of vertex coverage
(VC), edge coverage (EC), ratio of novel edges (RNE), cumulative Fowlkes and Mallows Measure (F&amp;M), whether
the taxonomy contains cycles (Cycles), and Precision, Recall and F-Score against gold standard taxonomies.
</tableCaption>
<bodyText confidence="0.89476">
the vast majority of the initial terms (if not all, as can
be seen in Table 1).
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999965717391304">
Evaluation is carried out considering the structural
properties of the taxonomy, as well as its qual-
ity when compared to gold-standard (see Table 1).
These gold taxonomies can be either the subgraphs
rooted at one relevant WordNet term (chemical,
food, equipment or science), or taxonomies manu-
ally crafted by domain experts.
These results suggest that the approach described
in this paper can be safely followed to construct a
taxonomy from a flat terminology as input, provided
major issues like domain-specificity or WSD are ad-
dressed. Our approach strongly depends of available
definitions of terms in Wikipedia, which was not the
case in very specific domains (such as the chemical
terminology). On the other hand, however, the hy-
pernym extraction pass worked well and thus we are
encouraged to work in this direction, stressing the
importance of an appropriate domain dataset from
which definitional knowledge can be extracted.
In order to compare the system and reference tax-
onomies, the evaluation consists in computing node
and edge coverage by taking into account the num-
ber of nodes and edges in common and the sizes of
the taxonomies. In addition, the results of a struc-
tural metric are also provided, such metric being the
Fowlkes&amp;Mallows measure (Fowlkes and Mallows,
1983), a method for comparing hierarchical clusters.
The results show poor performance of our system in
inferring relations among concepts at deeper levels
in the taxonomy. One of the reasons this might be
due to is the fact that the lexicalization of a term does
not necessarily have to be exact between a BabelNet
synset and an associated Wikipedia definition.
Regarding the manual evaluation of the quality of
newly acquired edges, our system is unsurprisingly
weak (P=10.2%)5 due to the inherent term ambigu-
ity which makes our system retrieve noisy defini-
tions at each step. We hypothesize that our results
might be higher in the chemical domain, since termi-
nology would be less prone to be polysemous. How-
ever, this domain was not considered for this evalua-
tion measure. These negative results together with
the good performance of the hypernym-extraction
module stress the need to retrieve valid domain spe-
cific definitional sentences for our approach to work
well.
</bodyText>
<sectionHeader confidence="0.987737" genericHeader="conclusions">
5 Conclusions and Discussion
</sectionHeader>
<bodyText confidence="0.99994275">
We have described a system designed for construct-
ing a taxonomy from a flat list of terms. It is based
on a module that queries BabelNet for Wikipedia
definitions in order to obtain definitional knowledge
</bodyText>
<footnote confidence="0.999487">
5Full results for all systems are reported in
http://alt.qcri.org/semeval2015/task17/index.php?id=evaluation.
</footnote>
<page confidence="0.993312">
952
</page>
<bodyText confidence="0.999977733333333">
for each term. Then, a machine-learning algorithm
is trained with a manually annotated dataset with
hypernym relations in definitional sentences, and
applied to our definition dataset. Different post-
classification heuristics are afterwards incorporated
to the pipeline with a two-fold objective: (1) Extract
a candidate hypernym in cases where the classifier
lacked the confidence to tag one or more tokens as
possible hypernyms, and (2) Decompose candidate
hypernyms exploiting the syntactic relation between
their head and its modifiers in a syntactic depen-
dency tree. Finally, with a set of term→hypernym
pairs we populate a domain taxonomy by connect-
ing terms and hypernyms, and finally by fixing dis-
connected nodes from the root.
We have demonstrated that our approach has very
high vertex coverage, and on the other hand is flawed
in capturing deep taxonomic relations among en-
tities. The hypernym extraction module achieves
state-of-the-art performance and due to the simplic-
ity of the features used is open for improvements,
either by incorporating semantic similarity among
tokens, frequencies in domain corpora, or a token’s
position in the syntactic tree.
We observe a clear room for improvement in the
domain corpus compilation part, and for the future
we are investigating the potential of the Wikipedia
Categories Graph in order to gather domain defini-
tions from pages that are in recurrent categories in
the BabelNet synset list.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999859142857143">
We would like to express our gratitude to the anony-
mous reviewers for their helpful comments. This
work is partially funded by the SKATER project,
TIN2012-38584-C06-03, Ministerio de Economia
y Competitividad, Secretar´ıa de Estado de Inves-
tigaci´on, Desarrollo e Innovaci´on, Espa˜na; and
project Dr. Inventor (FP7-ICT-2013.8.1 611383).
</bodyText>
<sectionHeader confidence="0.998821" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999161824561404">
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, pages 57–
64. Association for Computational Linguistics.
Guido Boella, Luigi Di Caro, Alice Ruggeri, and Livio
Robaldo. 2014. Learning from syntax generalizations
for automatic semantic annotation. Journal of Intelli-
gent Information Systems, pages 1–16.
Bernd Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, COLING ’10, pages 89–97, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Georgeta Bordea, Paul Buitelaar, Stefano Faralli, and
Roberto Navigli. 2015. Semeval-2015 task 17: Tax-
onomy extraction evaluation (texeval). In Proceedings
of the 9th International Workshop on Semantic Evalu-
ation. Association for Computational Linguistics.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2005.
Generic soft pattern models for definitional question
answering. In Proceedings of the 28th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 384–391.
ACM.
Edward B. Fowlkes and Colin L. Mallows. 1983.
A method for comparing two hierarchical cluster-
ings. Journal of the American statistical association,
78(383):553–569.
Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng
Wang, and Ting Liu. 2014. Learning semantic hier-
archies via word embeddings. In Proceedings of the
52th Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers, volume 1.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 1–8. As-
sociation for Computational Linguistics.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics-Volume
2, pages 539–545. Association for Computational Lin-
guistics.
Zornitsa Kozareva, Ellen Riloff, and Eduard H. Hovy.
2008. Semantic class learning from the web with hy-
ponym pattern linkage graphs. In ACL, volume 8,
pages 1048–1056. Citeseer.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ’01, pages
282–289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
</reference>
<page confidence="0.98976">
953
</page>
<reference confidence="0.99568775">
A Muresan and Judith Klavans. 2002. A method for
automatically building and evaluating dictionary re-
sources. In Proceedings of the Language Resources
and Evaluation Conference (LREC.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belnet: Building a very large multilingual semantic
network. In Proceedings of the 48th annual meeting
of the association for computational linguistics, pages
216–225. Association for Computational Linguistics.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym extrac-
tion. In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, ACL ’10,
pages 1318–1327, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexical
taxonomies from scratch. In IJCAI, pages 1872–1877.
Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of the Twenty Fifth
International Conference on Computational Linguis-
tics (COLING-14), Dublin, Ireland.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In Pro-
ceedings of the 19th international conference on Com-
putational linguistics-Volume 1, pages 1–7. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.998841">
954
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.630869">
<title confidence="0.985251">TALN-UPF: Taxonomy Learning Exploiting CRF-Based Extraction on Encyclopedic Definitions Espinosa-Anke Saggion</title>
<author confidence="0.888818">Tractament Autom`atic del Llenguatge Natural</author>
<affiliation confidence="0.919244">Department of Information and Communication Universitat Pompeu Carrer T`anger,</affiliation>
<address confidence="0.999673">08018 Barcelona,</address>
<email confidence="0.992006">horacio.saggion,</email>
<abstract confidence="0.9987745">This paper describes the system submitted by the TALN-UPF team to SEMEVAL Task 17 (Taxonomy Extraction Evaluation). We present a method for automatically learning a taxonomy from a flat terminology, which benefits from a definition corpus obtained by querying the BabelNet semantic network. Then, we combine a machine-learning algorithm for term-hypernym extraction with linguistically-motivated heuristics for hypernym decomposition. Our approach performs well in terms of vertex coverage and newly added vertices, while it shows room for improvement in terms of graph topology, edge coverage and precision of novel edges.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4309" citStr="Berland and Charniak, 1999" startWordPosition="652" endWordPosition="655">of the paper is structured as follows: Section 3 describes the modules of our approach, Section 4 presents and discusses the evaluation procedure as well as results, and finally Section 5 analyzes the performance of our system as well as the difficulties encountered, and suggests potential avenues for future work. 2 Background Generally, taxonomy learning from text has been carried out either following rule-based or distributional approaches. In terms of rule-based methods reported in the literature, (Hearst, 1992) introduced lexico-syntactic patterns, which were exploited in subsequent work (Berland and Charniak, 1999; Kozareva et al., 2008; Widdows and Dorow, 2002; Girju et al., 2003). Distributional approaches, on the other hand, have become increasingly popular due to the availability of large corpora. Systems aimed at extracting hypernym relations from text have exploited hybrid patterns as word-class lattices (Navigli and Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). Inspired by the reported success in the latter methods, we opted for combining syntactic patterns with mach</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 57– 64. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Boella</author>
<author>Luigi Di Caro</author>
<author>Alice Ruggeri</author>
<author>Livio Robaldo</author>
</authors>
<title>Learning from syntax generalizations for automatic semantic annotation.</title>
<date>2014</date>
<journal>Journal of Intelligent Information Systems,</journal>
<pages>1--16</pages>
<marker>Boella, Di Caro, Ruggeri, Robaldo, 2014</marker>
<rawString>Guido Boella, Luigi Di Caro, Alice Ruggeri, and Livio Robaldo. 2014. Learning from syntax generalizations for automatic semantic annotation. Journal of Intelligent Information Systems, pages 1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Very high accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>89--97</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6842" citStr="Bohnet, 2010" startWordPosition="1061" endWordPosition="1062"> page, our goal is to tag the tokens of the definition that correspond to one or more hypernyms. To this end, we train a Conditional Random Fields (Lafferty et al., 2001) classifier3 with the WCL Dataset (Navigli and Velardi, 2010). We argue that CRFs are a valid approach for sequential classification, and particularly for this task, due to their potential to capture prior and posterior token features on the current iteration. The WCL dataset includes near 2000 definitional sentences with terms and hypernyms manually annotated. We preprocess and parse the WCL dataset with a dependency parser (Bohnet, 2010), and then train our classifier with the following set of features. surface: A word’s surface form. lemma: The lemma of the word. pos: The word’s part-of-speech. head id: The id of the word to which the current token depends in a dependency syntactic tree. deprel: Syntactic function of the current word in relation to its head. def—nodef: Whether the current token appears before or after the first verb of the sentence. 2The classic components lexicographic genus-et-differentia definition are (1) Definiendum (concept being defined); (2) genus (hypernym or immediate superordinate that describes t</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 89–97, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgeta Bordea</author>
<author>Paul Buitelaar</author>
<author>Stefano Faralli</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval-2015 task 17: Taxonomy extraction evaluation (texeval).</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1930" citStr="Bordea et al., 2015" startWordPosition="272" endWordPosition="275">avigli et al., 2011) or Textual Entailment (Roller et al., 2014). Today, in the context of massive webenabled data, hypernym (is-a) relations are the focus of much research, as they constitute the backbone of ontologies (Navigli et al., 2011). However, one challenge remains open in the automatic construction of knowledge bases that exploit this type of relation. It is unfeasible to have up-to-date semantic resources for each domain, as they are limited in scope and domain, and their manual construction is knowledge intensive and time consuming (Fu et al., 2014). Given this rationale, Task 17 (Bordea et al., 2015) in the SEMEVAL 2015 set of shared tasks focuses on Taxonomy Extraction Evaluation, i.e. the construction of a taxonomy out of a flat set of terms belonging to one of the four domains of choice (food, chemical, equipment and science). These terms have to be hierarchically organized, and new terms are allowed to be included in the taxonomy. As for evaluation, for each domain, two taxonomies were used as gold standard: One created by domain experts; and one derived from the WordNet taxonomy rooted at the domain node, e.g. food1. Finally, evaluation is carried out from two standpoints: (1) The ta</context>
</contexts>
<marker>Bordea, Buitelaar, Faralli, Navigli, 2015</marker>
<rawString>Georgeta Bordea, Paul Buitelaar, Stefano Faralli, and Roberto Navigli. 2015. Semeval-2015 task 17: Taxonomy extraction evaluation (texeval). In Proceedings of the 9th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Generic soft pattern models for definitional question answering.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>384--391</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1205" citStr="Cui et al., 2005" startWordPosition="158" endWordPosition="161">flat terminology, which benefits from a definition corpus obtained by querying the BabelNet semantic network. Then, we combine a machine-learning algorithm for term-hypernym extraction with linguistically-motivated heuristics for hypernym decomposition. Our approach performs well in terms of vertex coverage and newly added vertices, while it shows room for improvement in terms of graph topology, edge coverage and precision of novel edges. 1 Introduction Learning semantic relations out of flat terminologies is an appealing task due to its potential application in tasks like Question Answering (Cui et al., 2005; Boella et al., 2014), automatic glossary construction (Muresan and Klavans, 2002), Ontology Learning (Navigli et al., 2011) or Textual Entailment (Roller et al., 2014). Today, in the context of massive webenabled data, hypernym (is-a) relations are the focus of much research, as they constitute the backbone of ontologies (Navigli et al., 2011). However, one challenge remains open in the automatic construction of knowledge bases that exploit this type of relation. It is unfeasible to have up-to-date semantic resources for each domain, as they are limited in scope and domain, and their manual </context>
</contexts>
<marker>Cui, Kan, Chua, 2005</marker>
<rawString>Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2005. Generic soft pattern models for definitional question answering. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384–391. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward B Fowlkes</author>
<author>Colin L Mallows</author>
</authors>
<title>A method for comparing two hierarchical clusterings.</title>
<date>1983</date>
<journal>Journal of the American statistical association,</journal>
<volume>78</volume>
<issue>383</issue>
<contexts>
<context position="14231" citStr="Fowlkes and Mallows, 1983" startWordPosition="2253" endWordPosition="2256">ins (such as the chemical terminology). On the other hand, however, the hypernym extraction pass worked well and thus we are encouraged to work in this direction, stressing the importance of an appropriate domain dataset from which definitional knowledge can be extracted. In order to compare the system and reference taxonomies, the evaluation consists in computing node and edge coverage by taking into account the number of nodes and edges in common and the sizes of the taxonomies. In addition, the results of a structural metric are also provided, such metric being the Fowlkes&amp;Mallows measure (Fowlkes and Mallows, 1983), a method for comparing hierarchical clusters. The results show poor performance of our system in inferring relations among concepts at deeper levels in the taxonomy. One of the reasons this might be due to is the fact that the lexicalization of a term does not necessarily have to be exact between a BabelNet synset and an associated Wikipedia definition. Regarding the manual evaluation of the quality of newly acquired edges, our system is unsurprisingly weak (P=10.2%)5 due to the inherent term ambiguity which makes our system retrieve noisy definitions at each step. We hypothesize that our re</context>
</contexts>
<marker>Fowlkes, Mallows, 1983</marker>
<rawString>Edward B. Fowlkes and Colin L. Mallows. 1983. A method for comparing two hierarchical clusterings. Journal of the American statistical association, 78(383):553–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiji Fu</author>
<author>Jiang Guo</author>
<author>Bing Qin</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Learning semantic hierarchies via word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics: Long Papers,</booktitle>
<volume>1</volume>
<contexts>
<context position="1877" citStr="Fu et al., 2014" startWordPosition="263" endWordPosition="266">(Muresan and Klavans, 2002), Ontology Learning (Navigli et al., 2011) or Textual Entailment (Roller et al., 2014). Today, in the context of massive webenabled data, hypernym (is-a) relations are the focus of much research, as they constitute the backbone of ontologies (Navigli et al., 2011). However, one challenge remains open in the automatic construction of knowledge bases that exploit this type of relation. It is unfeasible to have up-to-date semantic resources for each domain, as they are limited in scope and domain, and their manual construction is knowledge intensive and time consuming (Fu et al., 2014). Given this rationale, Task 17 (Bordea et al., 2015) in the SEMEVAL 2015 set of shared tasks focuses on Taxonomy Extraction Evaluation, i.e. the construction of a taxonomy out of a flat set of terms belonging to one of the four domains of choice (food, chemical, equipment and science). These terms have to be hierarchically organized, and new terms are allowed to be included in the taxonomy. As for evaluation, for each domain, two taxonomies were used as gold standard: One created by domain experts; and one derived from the WordNet taxonomy rooted at the domain node, e.g. food1. Finally, evalu</context>
<context position="4778" citStr="Fu et al., 2014" startWordPosition="723" endWordPosition="726">ported in the literature, (Hearst, 1992) introduced lexico-syntactic patterns, which were exploited in subsequent work (Berland and Charniak, 1999; Kozareva et al., 2008; Widdows and Dorow, 2002; Girju et al., 2003). Distributional approaches, on the other hand, have become increasingly popular due to the availability of large corpora. Systems aimed at extracting hypernym relations from text have exploited hybrid patterns as word-class lattices (Navigli and Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). Inspired by the reported success in the latter methods, we opted for combining syntactic patterns with machine learning to extract hypernyms from domain sentences. 3 Method This section describes the main modules that constitute our taxonomy learning system. 3.1 Definition corpus compilation We benefit from BabelNet, a very large multilingual semantic network that combines, among other resources, Wikipedia and WordNet (Navigli and Ponzetto, 2010). We get a set of BabelNet synsets associated to each term and for each synset, we extract its definition. In this step we ass</context>
</contexts>
<marker>Fu, Guo, Qin, Che, Wang, Liu, 2014</marker>
<rawString>Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning semantic hierarchies via word embeddings. In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics: Long Papers, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of part-whole relations.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4378" citStr="Girju et al., 2003" startWordPosition="664" endWordPosition="667">r approach, Section 4 presents and discusses the evaluation procedure as well as results, and finally Section 5 analyzes the performance of our system as well as the difficulties encountered, and suggests potential avenues for future work. 2 Background Generally, taxonomy learning from text has been carried out either following rule-based or distributional approaches. In terms of rule-based methods reported in the literature, (Hearst, 1992) introduced lexico-syntactic patterns, which were exploited in subsequent work (Berland and Charniak, 1999; Kozareva et al., 2008; Widdows and Dorow, 2002; Girju et al., 2003). Distributional approaches, on the other hand, have become increasingly popular due to the availability of large corpora. Systems aimed at extracting hypernym relations from text have exploited hybrid patterns as word-class lattices (Navigli and Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). Inspired by the reported success in the latter methods, we opted for combining syntactic patterns with machine learning to extract hypernyms from domain sentences. 3 Method Thi</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automatic discovery of part-whole relations. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics-Volume 2,</booktitle>
<pages>539--545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4203" citStr="Hearst, 1992" startWordPosition="639" endWordPosition="640">er, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics The remainder of the paper is structured as follows: Section 3 describes the modules of our approach, Section 4 presents and discusses the evaluation procedure as well as results, and finally Section 5 analyzes the performance of our system as well as the difficulties encountered, and suggests potential avenues for future work. 2 Background Generally, taxonomy learning from text has been carried out either following rule-based or distributional approaches. In terms of rule-based methods reported in the literature, (Hearst, 1992) introduced lexico-syntactic patterns, which were exploited in subsequent work (Berland and Charniak, 1999; Kozareva et al., 2008; Widdows and Dorow, 2002; Girju et al., 2003). Distributional approaches, on the other hand, have become increasingly popular due to the availability of large corpora. Systems aimed at extracting hypernym relations from text have exploited hybrid patterns as word-class lattices (Navigli and Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). I</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguistics-Volume 2, pages 539–545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard H Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<volume>8</volume>
<pages>1048--1056</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="4332" citStr="Kozareva et al., 2008" startWordPosition="656" endWordPosition="659">s follows: Section 3 describes the modules of our approach, Section 4 presents and discusses the evaluation procedure as well as results, and finally Section 5 analyzes the performance of our system as well as the difficulties encountered, and suggests potential avenues for future work. 2 Background Generally, taxonomy learning from text has been carried out either following rule-based or distributional approaches. In terms of rule-based methods reported in the literature, (Hearst, 1992) introduced lexico-syntactic patterns, which were exploited in subsequent work (Berland and Charniak, 1999; Kozareva et al., 2008; Widdows and Dorow, 2002; Girju et al., 2003). Distributional approaches, on the other hand, have become increasingly popular due to the availability of large corpora. Systems aimed at extracting hypernym relations from text have exploited hybrid patterns as word-class lattices (Navigli and Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). Inspired by the reported success in the latter methods, we opted for combining syntactic patterns with machine learning to extract</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard H. Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In ACL, volume 8, pages 1048–1056. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="6399" citStr="Lafferty et al., 2001" startWordPosition="992" endWordPosition="995"> term botifarra (a Catalan type of sausage), we add two definitions to our corpus: Relevant: Botifarra is a type of sausage and one of the most important dishes of the Catalan cuisine. Noisy: Botifarra is a point trick-taking card game for four players in fixed partnerships played in Catalonia. 3.2 Hypernym Extraction Given a set of definitional text fragments where the definiendum2 term is known, i.e. can be extracted from the url of the Wikipedia page, our goal is to tag the tokens of the definition that correspond to one or more hypernyms. To this end, we train a Conditional Random Fields (Lafferty et al., 2001) classifier3 with the WCL Dataset (Navigli and Velardi, 2010). We argue that CRFs are a valid approach for sequential classification, and particularly for this task, due to their potential to capture prior and posterior token features on the current iteration. The WCL dataset includes near 2000 definitional sentences with terms and hypernyms manually annotated. We preprocess and parse the WCL dataset with a dependency parser (Bohnet, 2010), and then train our classifier with the following set of features. surface: A word’s surface form. lemma: The lemma of the word. pos: The word’s part-of-spe</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Muresan</author>
<author>Judith Klavans</author>
</authors>
<title>A method for automatically building and evaluating dictionary resources.</title>
<date>2002</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC.</booktitle>
<contexts>
<context position="1288" citStr="Muresan and Klavans, 2002" startWordPosition="169" endWordPosition="172">rying the BabelNet semantic network. Then, we combine a machine-learning algorithm for term-hypernym extraction with linguistically-motivated heuristics for hypernym decomposition. Our approach performs well in terms of vertex coverage and newly added vertices, while it shows room for improvement in terms of graph topology, edge coverage and precision of novel edges. 1 Introduction Learning semantic relations out of flat terminologies is an appealing task due to its potential application in tasks like Question Answering (Cui et al., 2005; Boella et al., 2014), automatic glossary construction (Muresan and Klavans, 2002), Ontology Learning (Navigli et al., 2011) or Textual Entailment (Roller et al., 2014). Today, in the context of massive webenabled data, hypernym (is-a) relations are the focus of much research, as they constitute the backbone of ontologies (Navigli et al., 2011). However, one challenge remains open in the automatic construction of knowledge bases that exploit this type of relation. It is unfeasible to have up-to-date semantic resources for each domain, as they are limited in scope and domain, and their manual construction is knowledge intensive and time consuming (Fu et al., 2014). Given thi</context>
</contexts>
<marker>Muresan, Klavans, 2002</marker>
<rawString>A Muresan and Judith Klavans. 2002. A method for automatically building and evaluating dictionary resources. In Proceedings of the Language Resources and Evaluation Conference (LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Babelnet: Building a very large multilingual semantic network.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>216--225</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5252" citStr="Navigli and Ponzetto, 2010" startWordPosition="794" endWordPosition="797">Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). Inspired by the reported success in the latter methods, we opted for combining syntactic patterns with machine learning to extract hypernyms from domain sentences. 3 Method This section describes the main modules that constitute our taxonomy learning system. 3.1 Definition corpus compilation We benefit from BabelNet, a very large multilingual semantic network that combines, among other resources, Wikipedia and WordNet (Navigli and Ponzetto, 2010). We get a set of BabelNet synsets associated to each term and for each synset, we extract its definition. In this step we assume that a term’s definition appears in the first sentence of its Wikipedia article, which is a regular practice in the literature (see (Navigli and Velardi, 2010) or (Boella et al., 2014)). This step allowed us to compile a domain corpus of definitional knowledge, and thus maximizing the number of relevant terms definitions. However, noise is also introduced in our corpus. For example, given the term botifarra (a Catalan type of sausage), we add two definitions to our </context>
</contexts>
<marker>Navigli, Ponzetto, 2010</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2010. Babelnet: Building a very large multilingual semantic network. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 216–225. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Learning word-class lattices for definition and hypernym extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1318--1327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4639" citStr="Navigli and Velardi, 2010" startWordPosition="703" endWordPosition="706">enerally, taxonomy learning from text has been carried out either following rule-based or distributional approaches. In terms of rule-based methods reported in the literature, (Hearst, 1992) introduced lexico-syntactic patterns, which were exploited in subsequent work (Berland and Charniak, 1999; Kozareva et al., 2008; Widdows and Dorow, 2002; Girju et al., 2003). Distributional approaches, on the other hand, have become increasingly popular due to the availability of large corpora. Systems aimed at extracting hypernym relations from text have exploited hybrid patterns as word-class lattices (Navigli and Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). Inspired by the reported success in the latter methods, we opted for combining syntactic patterns with machine learning to extract hypernyms from domain sentences. 3 Method This section describes the main modules that constitute our taxonomy learning system. 3.1 Definition corpus compilation We benefit from BabelNet, a very large multilingual semantic network that combines, among other resources, Wikipedia and WordNet (Navigli and Po</context>
<context position="6460" citStr="Navigli and Velardi, 2010" startWordPosition="1001" endWordPosition="1004">efinitions to our corpus: Relevant: Botifarra is a type of sausage and one of the most important dishes of the Catalan cuisine. Noisy: Botifarra is a point trick-taking card game for four players in fixed partnerships played in Catalonia. 3.2 Hypernym Extraction Given a set of definitional text fragments where the definiendum2 term is known, i.e. can be extracted from the url of the Wikipedia page, our goal is to tag the tokens of the definition that correspond to one or more hypernyms. To this end, we train a Conditional Random Fields (Lafferty et al., 2001) classifier3 with the WCL Dataset (Navigli and Velardi, 2010). We argue that CRFs are a valid approach for sequential classification, and particularly for this task, due to their potential to capture prior and posterior token features on the current iteration. The WCL dataset includes near 2000 definitional sentences with terms and hypernyms manually annotated. We preprocess and parse the WCL dataset with a dependency parser (Bohnet, 2010), and then train our classifier with the following set of features. surface: A word’s surface form. lemma: The lemma of the word. pos: The word’s part-of-speech. head id: The id of the word to which the current token d</context>
<context position="7992" citStr="Navigli and Velardi, 2010" startWordPosition="1242" endWordPosition="1245">ng defined); (2) genus (hypernym or immediate superordinate that describes the definiendum); and (3) definiens or cluster of words that differentiate a definiendum from others of its kind. 3https://code.google.com/p/crfpp/ 950 term—noterm: Whether the token is part of the definiendum term or not. Our CRF classifier learns the above word-level features in a word window of [−2,2]. The prediction the classifier must learn follows the classic BIO format, i.e. whether a word is at the beginning of a hypernym phrase, inside or outside. We evaluate this hypernym extraction module on the WCL dataset (Navigli and Velardi, 2010) performing 10- fold cross-validation. It achieves and F-score of 79.86, outperforming existing state-of-the-art systems described in the literature (Navigli and Velardi, 2010; Boella et al., 2014). Despite the good performance of this module, we observe two potential drawbacks in terms of its fitness for the taxonomy learning task. Firstly, we aim at recovering hypernym candidates even in cases in which they are predicted with low confidence at the classification step. We build on the assumption that all encyclopedic definitions are very likely to include a hypernym, and hypothesize that it w</context>
</contexts>
<marker>Navigli, Velardi, 2010</marker>
<rawString>Roberto Navigli and Paola Velardi. 2010. Learning word-class lattices for definition and hypernym extraction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1318–1327, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
<author>Stefano Faralli</author>
</authors>
<title>A graph-based algorithm for inducing lexical taxonomies from scratch.</title>
<date>2011</date>
<booktitle>In IJCAI,</booktitle>
<pages>1872--1877</pages>
<contexts>
<context position="1330" citStr="Navigli et al., 2011" startWordPosition="175" endWordPosition="178">ombine a machine-learning algorithm for term-hypernym extraction with linguistically-motivated heuristics for hypernym decomposition. Our approach performs well in terms of vertex coverage and newly added vertices, while it shows room for improvement in terms of graph topology, edge coverage and precision of novel edges. 1 Introduction Learning semantic relations out of flat terminologies is an appealing task due to its potential application in tasks like Question Answering (Cui et al., 2005; Boella et al., 2014), automatic glossary construction (Muresan and Klavans, 2002), Ontology Learning (Navigli et al., 2011) or Textual Entailment (Roller et al., 2014). Today, in the context of massive webenabled data, hypernym (is-a) relations are the focus of much research, as they constitute the backbone of ontologies (Navigli et al., 2011). However, one challenge remains open in the automatic construction of knowledge bases that exploit this type of relation. It is unfeasible to have up-to-date semantic resources for each domain, as they are limited in scope and domain, and their manual construction is knowledge intensive and time consuming (Fu et al., 2014). Given this rationale, Task 17 (Bordea et al., 2015)</context>
<context position="11080" citStr="Navigli et al., 2011" startWordPosition="1727" endWordPosition="1730">re extracts this token as hypernym. 3.3 Hypernym Decomposition This step is aimed at generating deeper paths from a term and its hypernym by recursively decomposing a candidate hypernym. For example, consider the previous example’s term—*hypernym relation if the hypernym’s modifiers are taken into account: whisky—*distilled alcoholic beverage. Our objective is to generate the following set of relations: distilled alcoholic beverage—*alcoholic beverage and alcoholic beverage—*beverage. In this way, we improve the taxonomy since, in taxonomy learning, longer hypernymy paths should be preferred (Navigli et al., 2011), and we enable other potential distilled alcoholic beverages to be connected with alcoholic beverage rather than the more generic term beverage. We achieve this by performing a similar algorithm as in the post-classification heuristic, i.e. exploiting head and modifier relations in a dependency tree. 3.4 Graph Generation At this stage, we have a dataset of term—*hypernym pairs, and from here populating the taxonomy is a trivial task. For each pair, if neither term nor hypernym exist in the graph, add both nodes and connect them. If term exists in the graph, only add the hypernym and connect t</context>
</contexts>
<marker>Navigli, Velardi, Faralli, 2011</marker>
<rawString>Roberto Navigli, Paola Velardi, and Stefano Faralli. 2011. A graph-based algorithm for inducing lexical taxonomies from scratch. In IJCAI, pages 1872–1877.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Katrin Erk</author>
<author>Gemma Boleda</author>
</authors>
<title>Inclusive yet selective: Supervised distributional hypernymy detection.</title>
<date>2014</date>
<booktitle>In Proceedings of the Twenty Fifth International Conference on Computational Linguistics (COLING-14),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1374" citStr="Roller et al., 2014" startWordPosition="182" endWordPosition="185">hypernym extraction with linguistically-motivated heuristics for hypernym decomposition. Our approach performs well in terms of vertex coverage and newly added vertices, while it shows room for improvement in terms of graph topology, edge coverage and precision of novel edges. 1 Introduction Learning semantic relations out of flat terminologies is an appealing task due to its potential application in tasks like Question Answering (Cui et al., 2005; Boella et al., 2014), automatic glossary construction (Muresan and Klavans, 2002), Ontology Learning (Navigli et al., 2011) or Textual Entailment (Roller et al., 2014). Today, in the context of massive webenabled data, hypernym (is-a) relations are the focus of much research, as they constitute the backbone of ontologies (Navigli et al., 2011). However, one challenge remains open in the automatic construction of knowledge bases that exploit this type of relation. It is unfeasible to have up-to-date semantic resources for each domain, as they are limited in scope and domain, and their manual construction is knowledge intensive and time consuming (Fu et al., 2014). Given this rationale, Task 17 (Bordea et al., 2015) in the SEMEVAL 2015 set of shared tasks foc</context>
<context position="4800" citStr="Roller et al., 2014" startWordPosition="727" endWordPosition="730">erature, (Hearst, 1992) introduced lexico-syntactic patterns, which were exploited in subsequent work (Berland and Charniak, 1999; Kozareva et al., 2008; Widdows and Dorow, 2002; Girju et al., 2003). Distributional approaches, on the other hand, have become increasingly popular due to the availability of large corpora. Systems aimed at extracting hypernym relations from text have exploited hybrid patterns as word-class lattices (Navigli and Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). Inspired by the reported success in the latter methods, we opted for combining syntactic patterns with machine learning to extract hypernyms from domain sentences. 3 Method This section describes the main modules that constitute our taxonomy learning system. 3.1 Definition corpus compilation We benefit from BabelNet, a very large multilingual semantic network that combines, among other resources, Wikipedia and WordNet (Navigli and Ponzetto, 2010). We get a set of BabelNet synsets associated to each term and for each synset, we extract its definition. In this step we assume that a term’s defi</context>
</contexts>
<marker>Roller, Erk, Boleda, 2014</marker>
<rawString>Stephen Roller, Katrin Erk, and Gemma Boleda. 2014. Inclusive yet selective: Supervised distributional hypernymy detection. In Proceedings of the Twenty Fifth International Conference on Computational Linguistics (COLING-14), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Beate Dorow</author>
</authors>
<title>A graph model for unsupervised lexical acquisition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4357" citStr="Widdows and Dorow, 2002" startWordPosition="660" endWordPosition="663">scribes the modules of our approach, Section 4 presents and discusses the evaluation procedure as well as results, and finally Section 5 analyzes the performance of our system as well as the difficulties encountered, and suggests potential avenues for future work. 2 Background Generally, taxonomy learning from text has been carried out either following rule-based or distributional approaches. In terms of rule-based methods reported in the literature, (Hearst, 1992) introduced lexico-syntactic patterns, which were exploited in subsequent work (Berland and Charniak, 1999; Kozareva et al., 2008; Widdows and Dorow, 2002; Girju et al., 2003). Distributional approaches, on the other hand, have become increasingly popular due to the availability of large corpora. Systems aimed at extracting hypernym relations from text have exploited hybrid patterns as word-class lattices (Navigli and Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). Inspired by the reported success in the latter methods, we opted for combining syntactic patterns with machine learning to extract hypernyms from domain se</context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>Dominic Widdows and Beate Dorow. 2002. A graph model for unsupervised lexical acquisition. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>