<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.98031">
A Knowledge-based Approach to Text Classification
</title>
<author confidence="0.97874">
Zhu Jingbo Yao Tianshun
</author>
<affiliation confidence="0.8107755">
Institute of Computer Software &amp; Theory Institute of Computer Software &amp; Theory
Northeastern University, Shenyang Liaoning,
P.R.China 110006
Northeastern University, Shenyang Liaoning,
</affiliation>
<address confidence="0.788449">
P.R.China 110006
</address>
<email confidence="0.980701">
zhujingbo@yahoo.com tsyao@china.com
</email>
<sectionHeader confidence="0.991818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9876213">
The paper presents a simple and effective
knowledge-based approach for the task of text
classification. The approach uses topic
identification algorithm named FIFA to text
classification. In this paper the basic process of
text classification task and FIFA algorithm are
described in detail. At last some results of
experiment and evaluations are discussed.
Keywords: FIFA algorithm, topic identification,
text classification, natural language processing
</bodyText>
<sectionHeader confidence="0.96038" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.988521928571429">
The text automatic classification method is based
on the content analysis automatically to allocate
the text into pre-determined catalogue. The
methods of text automatic classification mainly
use information retrieval techniques. Traditional
information retrieval mainly retrieves relevant
documents by using keyword-based or
statistic-based techniques (Salton.G1989).
Generally, three famous models are used: vector
space model, Boolean model and probability
model, based on the three models, some
researchers brought forward extended models such
as John M.Picrrc(2001), Thomas Bayer, Ingrid
Renz,Michael Stein(1996), Antal van den Bosch,
Walter Daelemans, Ton Weijters(1996), Manuel de
Buenaga Rodriguez, Jose Maria Gomez-llidalgo,
Belen Diaz-agudo(1997), Ellen Riloff and Wendy
Lehnert(1994).
One central step in automatic text classification
is to identify the major topics of the texts. We
present a simple and effective knowledge-based
approach to text automatic classification. The
approach uses topic identification algorithm
named FIFA to text classification. In this paper the
basic process of text classification task and FIFA
algorithm are described in detail. At last some
results of experiment and evaluations are
discussed.
</bodyText>
<sectionHeader confidence="0.88348" genericHeader="method">
1 Knowledge-based text classification
</sectionHeader>
<bodyText confidence="0.92438">
The principal process for the Knowledge -based
text classification is illustrated as following:
</bodyText>
<figure confidence="0.996745818181818">
Text set
Y
NULL? End
N
Select a text for analysis
Topic identification
Topic feature aggregation
formula library
Rule base
Output topic tagging of the text
Dictionary
</figure>
<figureCaption confidence="0.999909">
Figure 1 The principal process for the knowledge-based text classification
</figureCaption>
<bodyText confidence="0.9990055">
From the figure 1 we can know that the crucial
technique of the text classification is the topic
identification parser. The topic tagging of the text
is identified as its catalogue.
</bodyText>
<sectionHeader confidence="0.976363" genericHeader="method">
2 Automatic Topic Identification
</sectionHeader>
<subsectionHeader confidence="0.998842">
2.1 Feature Dictionary
</subsectionHeader>
<bodyText confidence="0.99987">
The feature dictionary is mainly used to store
some terms that can illustrate the topic feature
concept, and we call these terms as “feature
terms”. The data structure of the feature
dictionary is consist of word, POS, semantic,
location and field attribute. Some examples of
feature dictionary are described as following:
</bodyText>
<table confidence="0.998763076923077">
Feature terms Attributes
îà“ S(),, á,itC(îà“),,Ñá(+Ñ)VÖ(AM)it�(��),Z3(itJT)
HuiTong County S(),,LOCATION,LOCATION(HuitongCounty),,COUNTRY(China)PRO
VINCE (HuNan)CITY(HuaiHua),FIELD(geography)
+ÑT1 fT N(),,T, fT,fAfi,(11&apos;A1T, fT),,Ñá(+Ñ),Z3(T, fT)Z3(��)
Bank of China N(),,BANK,ORGANIZATION(bank),,COUNTRY(China),FIELD(bank)
FIELD(finance)
ITAthTRlp,C, N(),,ff�jIR1P,C,,fAfi,(IT-&amp;J�IRIP ),,,Z3(ff�jIR)Z3(+_)
battle information N(),,INFORMATION-CENTER,ORGANIZATION(battle information
center ceter),,,FIELD(information)FIELD(military)
Ý 1ß N(),,1�,1�(� 1 ),,Ñá(+Ñ),Z3(*�)
Sanxia Project N(),,PROJECT,PROJECT(Sanxia Project),,COUNTRY(China), FIELD
(irrigation)
</table>
<tableCaption confidence="0.999887">
Table 1: Some examples in the feature dictionary
</tableCaption>
<bodyText confidence="0.999780857142857">
Since 1996 we employed a semi-automatic
method to acquire feature terms from pre-
categorized corpus, and developed a feature
dictionary including about 300,000 feature terms.
There are about 1,500 kinds of semantic features,
and about 1,000 kinds of field attributes to tag
feature terms in this dictionary.
</bodyText>
<subsectionHeader confidence="0.9948725">
2.2 Topic Feature Distribution Computing
Formula
</subsectionHeader>
<bodyText confidence="0.99531444">
According to the field attributes, frequencies
and positions of feature terms, we could compute
topic feature distribution. The computing steps
are described as following:
1) According to the frequency and position of a
feature term fti, we could compute the weight of
the term fti. The computing formula is described
as following:
the different influence abilities on the topic
features. So we take into account of this factor
and use different experience coefficient in the
weight computing formula of feature terms. In
the formula (1), the coefficient of Ntitle is 1.0, the
coefficient of Nbegin is 0.5, and the coefficient of
Nend is 0.5.
2) From the attribute of a feature term in the
dictionary we could acquire its field attribute, in
fact this field attribute is the topic feature
illustrated by the feature term. The weight of a
topic feature could be gotten by adding all
weights of feature terms that illustrate the same
topic feature. The more the feature terms
illustrates the same topic feature, the higher the
weight of the topic feature. The weight of a topic
feature expresses its abilities illustrating the topic
</bodyText>
<equation confidence="0.560968222222222">
freq ft N
( ) + + ×
0. 5 N + ×
0. 5 N
i title begin end (1)
pft( ) =
i
∑)
i
</equation>
<bodyText confidence="0.885491916666667">
freq (ft
Where
is the weight of the feature term
is frequency of the feature term
is times of the feature term
occurring in the
title.
is times of the feature term
occurring in the first sentence of a paragraph.
Nend is times of the feature term
occurring in
the end sentence of a paragraph.
</bodyText>
<equation confidence="0.870517857142857">
p(fti)
fti.freq(fti)
fti.Ntitle
fti
Nbegin
fti
fti
</equation>
<bodyText confidence="0.9798175">
Efreq(fti) is
total frequency of the all feature terms in the text.
In the experiment we discovered that the
feature terms in different position of a text have
of the text. The weight p(fi) computing formula
of a topic feature fi is described as following:
</bodyText>
<equation confidence="0.824831">
p ( f i ) p ( ft j )
= ∑ (2)
ft j f i
∈
</equation>
<bodyText confidence="0.9993905">
Where p(fi) is the weight of the topic feature fi.
p(ftj) is the weight of the feature term ftj. ftjEfi
shows feature term set illustrating the same topic
feature fi.
</bodyText>
<subsectionHeader confidence="0.998025">
2.3 Topic Feature Aggregation Formula
</subsectionHeader>
<bodyText confidence="0.999177">
The topic feature aggregation formula is
described as following:
</bodyText>
<equation confidence="0.995532">
n
ξ :β( )
t � t = I pffi)×µ(4) (3)
i=1
</equation>
<bodyText confidence="0.998795">
Where B(ti) is the weight of the topic ti. fj is the
topic feature illustrating the topic ti. p(fj) is the
weight of the topic feature fj. u (fj) is the
coefficient of the topic feature fj.
In the application system, we used automatic
construction technique to construct a library,
which called topic feature aggregation formula
library that includes 105 topic feature aggregation
formulas.
</bodyText>
<subsectionHeader confidence="0.92345">
2.4 FIFA algorithm
</subsectionHeader>
<bodyText confidence="0.999985392857143">
Most of automatic text processing techniques
uses topic identification as part of a specific task.
The approaches to topic identification taken in
these techniques can be summarized in three
group: statistical, knowledge-based, and hybrid.
The statistical approach (H.P.Luhn 1957 ,
H.P.Edmundson 1969 , Gerard Salton, James
Allan, Chris Buckley, and Amit Singhal 1994)
infers topics of texts from term frequency, term
location, term co-occurrence, etc, without using
external knowledge bases such as machine
readable dictionaries. The knowledge-based
approach (Wendy Lehnert and C. Loiselle 1989,
Lin Hongfei 2000) relies on a syntactic or
semantic parser, knowledge bases such as scripts
or machine readable dictionaries, etc., without
using any corpus statistics. The hybrid approach
(Elizabeth D. Liddy and Sung H. Myaeng 1992,
Marti A. Hearst 1994) combines the statistical
and knowledge-based approaches to take
advantage of the strengths of both approaches
and thereby to improve the overall system
performance.
This paper presents a simple and effective
approach named FIFA (feature identification and
feature aggregation) to text automatic topic
identification. The core of algorithm FIFA is
based on the equation:
</bodyText>
<sectionHeader confidence="0.7156805" genericHeader="method">
Topic Identification = Topic Feature
Identification + Topic Feature Aggregation.
</sectionHeader>
<bodyText confidence="0.943977">
Topic identification (TI) can be divided into
two phases: topic feature identification (FI) and
topic feature aggregation (FA).
</bodyText>
<sectionHeader confidence="0.479651" genericHeader="method">
1) Topic Feature Identification (FI): We use
</sectionHeader>
<bodyText confidence="0.943382095238095">
the term ‘topic feature’ to name the sub-topic in a
text. In this phase algorithm FIFA identifies
feature terms1 in a text by dictionary-based and
rule-based methods. The distribution of a topic
feature is computed by attributes, frequencies and
positions of topic feature terms.
2) Topic Feature Aggregation (FA):
According to distribution of topic features, in this
phase we use topic feature aggregation formulas
to compute the weights of topics of a text, then
the topic of a text could be determined by the
weights computed. Topic feature aggregation
formula will be introduced detailedly in the
following chapters. Using machine-learning
method, the topic feature aggregation formulas
could be acquired automatically from
pre-classified training corpus.
The topic identification algorithm FIFA could
be described as following:
Step1: Text segmentation and POS tagging
Input: a raw text
</bodyText>
<listItem confidence="0.935322833333333">
1. Preprocessing phase: One major function is
to recognize sentence boundaries, paragraph
breaks, abbreviations, numbers, and other
special tokens.
2. Segmentation phase: Employing maximal
matching algorithm to segment a sentences
into some words, and setting a word’s POS
set in machine readable dictionary as its
POS tagging.
3. Disambiguation phase: Employing a
technology based on ambiguous
segmentation dictionary 2 to resolve the
problem of word ambiguous segmentation,
and base on rules to recognize the unknown
words, such as name, location, company,
organization noun etc.
4. POS tagging phase: Employing tri-gram
based technology to POS tagging.
</listItem>
<bodyText confidence="0.7788765">
Output: a text with formats, segmentation
and POS tagging
</bodyText>
<subsectionHeader confidence="0.47751">
Step2: Topic feature identification
</subsectionHeader>
<bodyText confidence="0.902605444444445">
Input: a text with formats, segmentation and
POS tagging
1. Feature-dictionary-based feature terms
identification and tagging
The core of the method is to use feature
dictionary to realize the feature terms
identification and tagging. If a term in the
text is found in the dictionary, then we call
this term as a feature term of the text and its
</bodyText>
<footnote confidence="0.9822615">
1 Perhaps is a word, phrase, string, etc. According to
need of the application system to determine the type of
the feature term.
2 Is a machine readable man-made dictionary which
includes examples of ambiguous segmentation and its
correct segmentation
</footnote>
<bodyText confidence="0.9705185">
field attribute in the dictionary is tagged as
the topic attribute of the feature term.
2. Rule-based feature terms identification and
tagging
Because of the limitation of the feature
dictionary, we could not identify all feature
terms by feature-dictionary-based technique.
To resolve the problem of the unknown
feature terms, we use the technique of
rule-based feature terms identification and
tagging. There are two steps for the
identification and tagging:
</bodyText>
<listItem confidence="0.992704428571429">
1) We employ statistics-based approach to
acquire some high-frequency terms from the
text as analysis objects which length is
composed of two or more words, and the
frequency in the text should exceed two
times.
2) We employ rule-based technique to
analyze the grammatical structure of the
high frequency terms, and according to the
grammatical structure of the terms and the
attribute of the central word to estimate the
field attribute of the term, which is tagged as
the topic feature attribute of the term.
3. According to attributes, frequencies and
</listItem>
<bodyText confidence="0.972999166666667">
positions of the feature terms to calculate the
distribution of the topic feature of the text.
Output: topic feature set¸(={(fi,Iii)})of the
text. Where fi is the ith text topic feature; Iii is the
weight of the ith text topic feature fi subjected to
IiiE(0,1).
</bodyText>
<subsectionHeader confidence="0.649908">
Step3: Topic feature aggregation
</subsectionHeader>
<bodyText confidence="0.987874">
Input: The topic feature set¸ of the text.
</bodyText>
<listItem confidence="0.992959285714286">
1. Reading a formula ¯i from the topic feature
aggregation formula library, where the
formula ¯i is the aggregation formula of
the topic ti.
2. According to parameters in the topic feature
set ¸, the weight of the topic ti could be
computed by the formula¯i.
3. If there are some other aggregation formulas
in the library, then go to Step1, otherwise go
to the next step.
4. Supposing the topic feature fi in the set¸
(={(fi,Iii)})as a topic, and Iii is the weight
of the topic fi. We sort the topic ti and fi by
weight.
</listItem>
<bodyText confidence="0.599743666666667">
Output: Selecting the topic with maximal
weight as topic tagging of the text.
Algorithm1: Topic identification algorithm FIFA
</bodyText>
<sectionHeader confidence="0.984504" genericHeader="evaluation">
3. Experiment
</sectionHeader>
<bodyText confidence="0.999825666666667">
To test the efficiency of the FIFA-based text
automatic classification, and according to the
pre-determined 10 topics we constructed a test
corpus, which includes 1000 articles downloaded
from the Internet. The composing of the test
corpus is described as following:
</bodyText>
<table confidence="0.999515083333334">
Topic (abbreviation) Number of articles
Sex (SEX) 100
Sex Healthy (SHE) 100
Fa Lun Gong (FLG) 100
Critical of Fa Lun Gong (CFLG) 100
Physical (PHY) 100
Military affairs (MIA) 100
Finance and economics (FAE) 100
Education (EDU) 100
Entertainment (ENT) 100
Computer (COM) 100
Total 1000
</table>
<tableCaption confidence="0.998116">
Table 2 The composing of the test corpus
</tableCaption>
<figureCaption confidence="0.432449666666667">
Experiment 1: By classifying the test corpus, we
could value the effect of the FIFA-based text
automatic classification. The following figure 2
</figureCaption>
<bodyText confidence="0.998901666666667">
shows the results of text classification. Line 1
represents precision percent while line ■
represents recall percent.
</bodyText>
<table confidence="0.932695857142857">
150
100
50
0
SEX SHE FLG CFLG PHY MIA FAE EDU ENT COM
precision% 71 75 68 74 94 83 73 81 85 96
recall% 64 61 58 72 95 81 76 83 74 94
</table>
<figureCaption confidence="0.974139">
Figure 2 The results of FIFA-based text classification
</figureCaption>
<sectionHeader confidence="0.995853" genericHeader="conclusions">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.999975272727273">
This paper presented a simple and effective
approach to topic automatic identification. We
use the topic identification approach for the task
of text classification. The results of experiment
show that a good precise and recall percent are
achieved. In fact the topic identification approach
called FIFA could be used not only as a
stand-alone topic identification unit, but also in
other text processing tasks such as text
summarization, information retrieval, information
routing etc.
</bodyText>
<sectionHeader confidence="0.997754" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994978442622951">
Salton.G(1989), Automatic Text Processing : The
Transformation : Analysis and Retrieval of
Information by Computer, Addison-Wesley,
Reading, Mass
John M.Picrrc(2001), On the automated
classification of web sites, Linkoping Electronic
Articles in Computer and Information Science,
Vol.6, 2001, Sweden,
Thomas Bayer, Ingrid Renz,Michael Stein, Ulrich
Kressel(1996), Domain and language
independent feature extraction for statistical text
categorization, proceedings of workshop on
language engineering for document analysis and
recognition - ed. by L. Evett and T. Rose, part of
the AISB 1996 Workshop Series, April 96,
Sussex University, England, 21-32 (ISBN 0 905
488628)
Antal van den Bosch, Walter Daelemans, Ton
Weijters(1996), Morphological analysis as
classification: an inductive-learning approach,
Proceedings of NEMLAP-2, 2, July, 1996
Manuel de Buenaga Rodriguez, Jose Maria
Gomez-llidalgo, Belen Diaz-agudo(1997), Using
WORDNET to complement training information
in text categorization, Second International
Conference on Recent Advances in Natural
Language Processing, 1997
Ellen Riloff and Wendy Lehnert(1994),
Information Extraction as Basis for
High-precision Text Classification, ACM
Transactions on Information System, Vol12, No.3,
July 1994
H.P.Luhn(1957). A statistical approach to
mechanized encoding and searching of literary
information. IBM Journal, p309-17, October
1957
H.P.Edmundson(1969). New methods in
automatic extracting. Journal of the ACM,
16(2):264-85,1969
Gerard Salton, James Allan, Chris Buckley, and
Amit Singhal(1994). Automatic analysis, theme
generation, and summarization of
machine-readable texts. Science, 264:1421-26,
June 1994
Wendy Lehnert and C. Loiselle(1989). An
introduction to plot unit. In David Waltz, editor,
Semantic Structures-Advances in Natural
Language Processing, p88-111, Lawrence
Erlbaum Associates, Hillsdale, New Jersey, 1989
Lin Hongfei(2000), Logic Model for Chinese Text
Filtering, Ph.D dissertation, Northeastern
University, 2000.3
Elizabeth D. Liddy and Sung H. Myaeng(1992).
DR-LINK’s linguistic- conceptual approach to
document detection. In Proceedings of the First
Text Retrieval Conferece (TREC-1), p113-29,
1992
Marti A. Hearst(1994). Context and Structure in
Automated Full-Text Information Access. PhD
thesis, Computer Science Division, University of
California at Berkeley, California, April 1994
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.486552">
<title confidence="0.999933">A Knowledge-based Approach to Text Classification</title>
<author confidence="0.997909">Zhu Jingbo Yao Tianshun</author>
<affiliation confidence="0.9997305">Institute of Computer Software &amp; Theory Institute of Computer Software &amp; Theory Northeastern University, Shenyang Liaoning,</affiliation>
<address confidence="0.946255">P.R.China 110006</address>
<affiliation confidence="0.996792">Northeastern University, Shenyang Liaoning,</affiliation>
<address confidence="0.884797">P.R.China 110006</address>
<email confidence="0.995541">zhujingbo@yahoo.comtsyao@china.com</email>
<abstract confidence="0.999937111111111">The paper presents a simple and effective knowledge-based approach for the task of text classification. The approach uses topic identification algorithm named FIFA to text classification. In this paper the basic process of text classification task and FIFA algorithm are described in detail. At last some results of experiment and evaluations are discussed.</abstract>
<keyword confidence="0.7931375">Keywords: FIFA algorithm, topic identification, text classification, natural language processing</keyword>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Text Processing : The Transformation : Analysis and Retrieval of Information by Computer,</title>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Mass</location>
<marker>Salton, </marker>
<rawString>Salton.G(1989), Automatic Text Processing : The Transformation : Analysis and Retrieval of Information by Computer, Addison-Wesley, Reading, Mass</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Picrrc</author>
</authors>
<title>On the automated classification of web sites,</title>
<date>2001</date>
<booktitle>Linkoping Electronic Articles in Computer and Information Science, Vol.6,</booktitle>
<marker>Picrrc, 2001</marker>
<rawString>John M.Picrrc(2001), On the automated classification of web sites, Linkoping Electronic Articles in Computer and Information Science, Vol.6, 2001, Sweden,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Bayer</author>
</authors>
<title>Ingrid Renz,Michael Stein, Ulrich Kressel(1996), Domain and language independent feature extraction for statistical text categorization, proceedings of workshop on language engineering for document analysis and recognition -</title>
<date></date>
<journal>ISBN</journal>
<booktitle>of the AISB 1996 Workshop Series, April 96,</booktitle>
<volume>0</volume>
<pages>905--488628</pages>
<editor>ed. by L. Evett and T. Rose, part</editor>
<institution>Sussex University,</institution>
<location>England,</location>
<marker>Bayer, </marker>
<rawString>Thomas Bayer, Ingrid Renz,Michael Stein, Ulrich Kressel(1996), Domain and language independent feature extraction for statistical text categorization, proceedings of workshop on language engineering for document analysis and recognition - ed. by L. Evett and T. Rose, part of the AISB 1996 Workshop Series, April 96, Sussex University, England, 21-32 (ISBN 0 905 488628)</rawString>
</citation>
<citation valid="false">
<authors>
<author>Antal van den Bosch</author>
<author>Walter Daelemans</author>
</authors>
<title>Ton Weijters(1996), Morphological analysis as classification: an inductive-learning approach,</title>
<date>1996</date>
<booktitle>Proceedings of NEMLAP-2, 2,</booktitle>
<location>Rodriguez, Jose Maria Gomez-llidalgo, Belen</location>
<marker>van den Bosch, Daelemans, 1996</marker>
<rawString>Antal van den Bosch, Walter Daelemans, Ton Weijters(1996), Morphological analysis as classification: an inductive-learning approach, Proceedings of NEMLAP-2, 2, July, 1996 Manuel de Buenaga Rodriguez, Jose Maria Gomez-llidalgo, Belen Diaz-agudo(1997), Using WORDNET to complement training information in text categorization, Second International Conference on Recent Advances in Natural Language Processing, 1997</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Wendy Lehnert</author>
</authors>
<title>Information Extraction as Basis for High-precision Text Classification,</title>
<date>1994</date>
<journal>ACM Transactions on Information System,</journal>
<location>Vol12, No.3,</location>
<marker>Riloff, Lehnert, 1994</marker>
<rawString>Ellen Riloff and Wendy Lehnert(1994), Information Extraction as Basis for High-precision Text Classification, ACM Transactions on Information System, Vol12, No.3, July 1994</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>A statistical approach to mechanized encoding and searching of literary information.</title>
<date>1957</date>
<journal>IBM Journal,</journal>
<pages>309--17</pages>
<contexts>
<context position="6872" citStr="Luhn 1957" startWordPosition="1012" endWordPosition="1013">the topic ti. p(fj) is the weight of the topic feature fj. u (fj) is the coefficient of the topic feature fj. In the application system, we used automatic construction technique to construct a library, which called topic feature aggregation formula library that includes 105 topic feature aggregation formulas. 2.4 FIFA algorithm Most of automatic text processing techniques uses topic identification as part of a specific task. The approaches to topic identification taken in these techniques can be summarized in three group: statistical, knowledge-based, and hybrid. The statistical approach (H.P.Luhn 1957 , H.P.Edmundson 1969 , Gerard Salton, James Allan, Chris Buckley, and Amit Singhal 1994) infers topics of texts from term frequency, term location, term co-occurrence, etc, without using external knowledge bases such as machine readable dictionaries. The knowledge-based approach (Wendy Lehnert and C. Loiselle 1989, Lin Hongfei 2000) relies on a syntactic or semantic parser, knowledge bases such as scripts or machine readable dictionaries, etc., without using any corpus statistics. The hybrid approach (Elizabeth D. Liddy and Sung H. Myaeng 1992, Marti A. Hearst 1994) combines the statistical a</context>
</contexts>
<marker>Luhn, 1957</marker>
<rawString>H.P.Luhn(1957). A statistical approach to mechanized encoding and searching of literary information. IBM Journal, p309-17, October 1957</rawString>
</citation>
<citation valid="false">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<journal>Journal of the ACM,</journal>
<pages>16--2</pages>
<marker>Edmundson, </marker>
<rawString>H.P.Edmundson(1969). New methods in automatic extracting. Journal of the ACM, 16(2):264-85,1969</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>James Allan</author>
<author>Chris Buckley</author>
<author>Amit Singhal</author>
</authors>
<title>Automatic analysis, theme generation, and summarization of machine-readable texts.</title>
<date>1994</date>
<journal>Science,</journal>
<pages>264--1421</pages>
<marker>Salton, Allan, Buckley, Singhal, 1994</marker>
<rawString>Gerard Salton, James Allan, Chris Buckley, and Amit Singhal(1994). Automatic analysis, theme generation, and summarization of machine-readable texts. Science, 264:1421-26, June 1994</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy Lehnert</author>
<author>C Loiselle</author>
</authors>
<title>An introduction to plot unit. In</title>
<date>1989</date>
<booktitle>Semantic Structures-Advances in Natural Language Processing, p88-111, Lawrence Erlbaum Associates,</booktitle>
<pages>2000--3</pages>
<editor>David Waltz, editor,</editor>
<institution>Northeastern University,</institution>
<location>Hillsdale, New Jersey,</location>
<marker>Lehnert, Loiselle, 1989</marker>
<rawString>Wendy Lehnert and C. Loiselle(1989). An introduction to plot unit. In David Waltz, editor, Semantic Structures-Advances in Natural Language Processing, p88-111, Lawrence Erlbaum Associates, Hillsdale, New Jersey, 1989 Lin Hongfei(2000), Logic Model for Chinese Text Filtering, Ph.D dissertation, Northeastern University, 2000.3</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth D Liddy</author>
<author>Sung H Myaeng</author>
</authors>
<title>DR-LINK’s linguistic- conceptual approach to document detection.</title>
<date>1992</date>
<booktitle>In Proceedings of the First Text Retrieval Conferece (TREC-1),</booktitle>
<pages>113--29</pages>
<marker>Liddy, Myaeng, 1992</marker>
<rawString>Elizabeth D. Liddy and Sung H. Myaeng(1992). DR-LINK’s linguistic- conceptual approach to document detection. In Proceedings of the First Text Retrieval Conferece (TREC-1), p113-29, 1992</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Context and Structure in Automated Full-Text Information Access.</title>
<date>1994</date>
<tech>PhD thesis,</tech>
<institution>Computer Science Division, University of California at Berkeley,</institution>
<location>California,</location>
<contexts>
<context position="7445" citStr="Hearst 1994" startWordPosition="1097" endWordPosition="1098">The statistical approach (H.P.Luhn 1957 , H.P.Edmundson 1969 , Gerard Salton, James Allan, Chris Buckley, and Amit Singhal 1994) infers topics of texts from term frequency, term location, term co-occurrence, etc, without using external knowledge bases such as machine readable dictionaries. The knowledge-based approach (Wendy Lehnert and C. Loiselle 1989, Lin Hongfei 2000) relies on a syntactic or semantic parser, knowledge bases such as scripts or machine readable dictionaries, etc., without using any corpus statistics. The hybrid approach (Elizabeth D. Liddy and Sung H. Myaeng 1992, Marti A. Hearst 1994) combines the statistical and knowledge-based approaches to take advantage of the strengths of both approaches and thereby to improve the overall system performance. This paper presents a simple and effective approach named FIFA (feature identification and feature aggregation) to text automatic topic identification. The core of algorithm FIFA is based on the equation: Topic Identification = Topic Feature Identification + Topic Feature Aggregation. Topic identification (TI) can be divided into two phases: topic feature identification (FI) and topic feature aggregation (FA). 1) Topic Feature Ide</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti A. Hearst(1994). Context and Structure in Automated Full-Text Information Access. PhD thesis, Computer Science Division, University of California at Berkeley, California, April 1994</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>