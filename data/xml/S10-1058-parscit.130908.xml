<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032808">
<title confidence="0.988968">
UvT: Memory-based pairwise ranking of paraphrasing verbs
</title>
<author confidence="0.998095">
Sander Wubben
</author>
<affiliation confidence="0.888101333333333">
Tilburg centre for Cognition and Communication
Tilburg University
The Netherlands
</affiliation>
<email confidence="0.985068">
s.wubben@uvt.nl
</email>
<sectionHeader confidence="0.993441" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998516272727273">
In this paper we describe Mephisto, our
system for Task 9 of the SemEval-2 work-
shop. Our approach to this task is to de-
velop a machine learning classifier which
determines for each verb pair describing
a noun compound which verb should be
ranked higher. These classifications are
then combined into one ranking. Our clas-
sifier uses features from the Google N-
gram Corpus, WordNet and the provided
training data.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999758555555555">
We interpret the task of ranking a set of given
paraphrasing verbs as described by Butnariu et
al (2010) as a competition between these verbs.
Each verb competes with every other verb in the
set and receives a positive score if it is more likely
to describe the given noun compound (NC) than
the other verb and a negative score if it is less
likely to describe the NC. In line with this ap-
proach we regard the task as a classification prob-
lem where for each comparison our classification
algorithm picks the paraphrasing verb that is more
likely to describe the NC. This brings the clas-
sification problem down to three classes: higher,
equal or lower. Sometimes the paraphrasing verbs
are accompanied by a preposition. In this paper we
will simply refer to all verbs and verb-prepositions
as verbs.
The distribution of the verbs in the training data
provides us already with valuable information. We
incorporate basic features describing this distribu-
tion to train our classifier. We also need addi-
tional semantic features that provide us with in-
sight into the relation between the NC and the
verb, therefore we use features constructed from
WordNet and the Google N-gram Corpus to train
our Memory-based paraphrase interpretation scor-
ing tool (Mephisto).
</bodyText>
<sectionHeader confidence="0.988405" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.9998015">
The system consists of three components: the fea-
ture extraction component, the classification com-
ponent and the ranking component. We will de-
scribe all three components.
</bodyText>
<subsectionHeader confidence="0.984105">
2.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999967043478261">
For each verb describing an NC we try to extract
those features that describe the probability that this
verb is a good interpretation of the NC. We assume
that given a NC N1N2 and a verb V , the NC inter-
pretation should be N2V N1. The phrase “Butter
made from peanuts” adequately describes peanut
butter.
The training data provides us with a total of
17,727 instances of NC verb pairs scored by hu-
man judges. This can be broken down into 4,360
unique verb phrases describing 250 NCs. This
distribution already gives us a good clue when we
are generating new rankings. The following are
the features we used:
Weighted mean in training data For each NC
that has to be ranked we find the most similar NC
in the training data by measuring the overlap in
verb phrases between the two NCs. We do this by
calculating the Jaccard coefficient over the sets of
verbs associated with the NCs. We adapt the high-
est ranking NC as most similar to our candidate
NC (the NC with most matching verbs). For each
verb V we then calculate the score as follows:
</bodyText>
<subsectionHeader confidence="0.596765">
Score= J * Ssim + (1 − J) * M
</subsectionHeader>
<bodyText confidence="0.995241833333333">
where J is the Jaccard score, Ssim is the
assigned score of the verb in the most similar
set and M is the mean score for the verb in the
training data.
Rank in training data For this feature we
directly compare the two verbs V1 and V2. We just
</bodyText>
<page confidence="0.936388">
260
</page>
<note confidence="0.635299">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 260–263,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.995142777777778">
feature values info gain gain ratio
verb1 4,093 0.24 0.02
verb2 4,093 0.24 0.02
verb1-verb2 768,543 1.06 0.06
verb1-verb2-LCS 986,031 1.29 0.07
n-gram score1 7 0.07 0.02
n-gram score2 7 0.01 0.08
weighted mean 7 0.29 0.12
rank 3 0.68 0.43
</table>
<tableCaption confidence="0.999759">
Table 1: Features used in our system
</tableCaption>
<bodyText confidence="0.997121923076923">
count the number of times that V1 is ranked higher
than V2 and vice versa for every NC where both
verbs occur. We end up with a positive, equal or
negative class.
WordNet Least Common Subsumer In order
to distinguish between different kinds of NCs we
use WordNet (Fellbaum, 1998) to determine the
kind of relation between the nouns. This idea is
supported by work by Levi (1978), Warren (1978)
and Nastase &amp; Szpakowicz (2003). Our intuition
is that the ranking of verb phrases is very depen-
dent on this relation between the nouns. To deter-
mine this we use the WordNet::QueryData (Ren-
nie, 2000) module. In the WordNet graph we look
for the Least Common Subsumer (LCS) of the
two nouns. The LCS is the lowest parent node of
both nouns. We combine the LCS with both verb
phrases into one feature.
Google N-gram features We use the Google N-
gram corpus to count co-occurence frequencies of
certain n-grams. An NC occurring often together
with a certain verb should indicate that that verb
is a good paraphrase for the NC. Using web text
for various NLP-tasks has been proven to be use-
ful (Lapata and Keller, 2005), also for NC inter-
pretation (Nakov and Hearst, 2005). Because of
data sparseness and the unlikelihood of finding a
perfect match for a certain n-gram, we adopt dif-
ferent strategies for constructing features. First of
all, we try to relax the matching conditions by ap-
plying certain regular expression. Given the NC
“abortion problem” and the paraphrasing verb “be
related to” , it seems unlikely you will ever en-
counter the n-gram “problem be related to abor-
tion”, yet in the training data “be related to” is the
number three verb for “abortion problem”. There-
fore, we first apply some simple inflection. Instead
of “be” we match on “is/are/being”. and we do a
comparable inflection for other verbs transforming
</bodyText>
<table confidence="0.6847255">
+up+ -dwn- =eq=
+up+ 23,494 7,099 8,912
-dwn- 7,168 23,425 8,912
=eq= 22,118 22,084 22,408
</table>
<tableCaption confidence="0.760353">
Table 2: Confusion matrix of the classes, with hor-
</tableCaption>
<bodyText confidence="0.926173166666667">
izontally the output classes and vertically the tar-
get classes
a verb such as “involve” into “involves/involving”.
Additionally we also match on singular and plural
nouns. We then use two different techniques to
find the n-gram frequencies:
</bodyText>
<equation confidence="0.998592">
N − gram1 = f(N2V ) + f(V N1)
f(V )
f(N2V N1)
N − gram2 = f(V )
</equation>
<bodyText confidence="0.948734764705882">
where f stands for the occurrences of the given
sequences of nouns and verb. We do not divide by
noun occurrences because they are constant for
every pair of verbs we compare.
Pairwise comparison of features For each
verb pair in an NC set we compare all numeric
features and assign one of the following symbols
to characterize the relation of the two verbs:
+++: V1 score is more than 10 times V2 score
++: V1 score is between 2 and 10 times V2 score
+: V1 score is between 1 and 2 times verb2 score
=: scores are equal
-: V2 score is between 1 and 2 times V1 score
- -: V2 score is between 2 and 10 times V1 score
- - -: V2 score is more than 10 times V1 score
An overview of the features is displayed in Ta-
ble 1.
</bodyText>
<subsectionHeader confidence="0.989458">
2.2 Classification
</subsectionHeader>
<bodyText confidence="0.999946454545454">
Our system makes use of Memory-Based Learn-
ing (MBL) for classification. MBL stores feature
representations of training instances in memory
without abstraction and classifies unseen instances
by matching their feature representation to all in-
stances in memory, finding the most similar in-
stances. The class of these most similar instances
is then copied to the new instance The learning
algorithm our system uses is the IB1 classier as
implemented in TiMBL (version 6.1.5). IB1 is a
supervised decision-tree-based implementation of
</bodyText>
<page confidence="0.979017">
261
</page>
<table confidence="0.9989312">
Settings TiMBL F-score Spearman ρ Pearson r KullbackLeibler div.
k=3 all features 0.48 0.50 0.44 1.91
k=3 no external features 0.53 0.48 0.41 2.05
k=11 all features 0.51 0.50 0.42 1.97
k=11 no external features 0.20 - - -
</table>
<tableCaption confidence="0.998345">
Table 3: Results for different settings on the development set
</tableCaption>
<bodyText confidence="0.999831538461538">
the k-nearest neighbor algorithm for learning clas-
sification tasks (Aha et al., 1991). The TiMBL pa-
rameters we used in the Mephisto system for the
IB1 classifier are the overlap metric, weighting us-
ing GainRatio, and k=3, taking into account the
instances on the 3 most similar positions to extrap-
olate the class of the instance. More information
about these settings can be found in the TiMBL
reference guide (Daelemans et al., 2009). We train
our classifier on the provided training data to clas-
sify instances into one of three classes; +up+ if
V1 ranks higher than V2 , =eq= if both verbs rank
equally and -dwn- if V1 ranks lower than V2.
</bodyText>
<subsectionHeader confidence="0.995954">
2.3 Ranking
</subsectionHeader>
<bodyText confidence="0.999967444444444">
The final step is to combine all the classification
into one score per verb. This is done in a very
straight forward way: a verb receives one point
every time it is classified as +up+. This results in
scores for each verb paraphrasing an NC. We then
perform a simple post processing step: we reas-
sign classes to each verb based on the final scores
they have received and recalculate their scores. We
repeat this process until the scores converge.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999738947368421">
For development the original training set was di-
vided in a development training set of 15,966 lines
and a development test set of 1,761 lines, which
contains 23 NCs. The distribution and ranking fea-
tures were calculated using only the development
training set. Because we compare for each NC ev-
ery verb to every other verb the TiMBL training
instance-base contains 1,253,872 lines, and the de-
velopment test set 145,620. The results for differ-
ent settings are in Table 3. Although the TiMBL
F-score (macro-averaged) of using all features is
actually lower than using only semantic features at
k=3, the final correlations are in favor of using all
features. There does not seem to be an improve-
ment when extrapolating from 11 neighbouring in-
stances in the instance-base over 3. In fact, when
using no external features and k=11, the classifier
overgeneralizes and classifies every instance as
=eq= and consequently does not provide a ranking
</bodyText>
<table confidence="0.999719">
System Spearman ρ Pearson r Cosine
UvT-MEPHISTO 0.450 0.411 0.635
UCD-PN 0.441 0.361 0.669
UCD-GOGGLE-III 0.432 0.395 0.652
UCD-GOGGLE-II 0.418 0.375 0.660
UCD-GOGGLE-I 0.380 0.252 0.629
UCAM 0.267 0.219 0.374
NC-INTERP 0.186 0.070 0.466
Baseline 0.425 0.344 0.524
</table>
<tableCaption confidence="0.999049">
Table 4: Final results for SemEval-2 Task 9
</tableCaption>
<bodyText confidence="0.999760206896552">
at all. Additionally, classifying with k=11 takes
considerably longer than with k=3. The settings
we use for our final system are k=3 and we use all
features. Table 2 displays a confusion matrix of
the classification on the development test set. Not
surprisingly the classifier is very bad at recogniz-
ing the =eq= class. These mistakes are not as bad
as miss-classifying a +up+ instance as -dwn- and
vice versa, and fortunately these mistakes happen
less often.
The official test set contains 32,830 instances,
almost twice as many as the training set. This
breaks down into 2,837,226 cases to classify. In
Table 4 are the final results of the task with all
participating systems and their macro-averaged
Spearman, Pearson and Cosine correlation. Also
shown is the baseline, which involves scoring a
given verb paraphrase by its frequency in the train-
ing set. The final results are quite a bit lower than
the results on the development set. This could
be coincidence (the final test set is about twenty
times larger than our development test set), but it
could also be due to overfitting on the development
set. The ten best and worst scoring compounds are
shown in Table 5 with their Least Common Sub-
sumer as taken from WordNet. The best-scoring
NC “jute products” achieves a Spearman p of 0.75
while the worst-scoring compound, “electron mi-
croscope” only achieves 0.12.
</bodyText>
<sectionHeader confidence="0.997176" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999906666666667">
We have shown that a Memory-based pairwise
approach to ranking with features taken from
WordNet and the Google N-gram corpus achieves
</bodyText>
<page confidence="0.990951">
262
</page>
<table confidence="0.999821909090909">
Best scoring NCs LCS Spearman ρ
jute products physical entity 0.75
ceramics products artifact 0.75
steel frame physical entity 0.74
cattle population entity 0.74
metal body physical entity 0.74
winter blooming entity 0.73
warbler family entity 0.72
wool scarf artifact 0.71
fiber optics physical entity 0.70
petroleum products physical entity 0.70
Worst scoring NCs LCS Spearman ρ
electron microscope whole 0.12
light bulb physical entity 0.15
yesterday evening measure 0.16
student loan entity 0.16
theater orchestra entity 0.17
sunday restrictions abstraction 0.20
yesterday afternoon measure 0.20
relations agency abstraction 0.21
crime novelist entity 0.21
office buildings structure 0.21
</table>
<tableCaption confidence="0.758441666666667">
Table 5: Best and worst scoring noun compounds
with their Least Common Subsumer and Spear-
man ρ correlation
</tableCaption>
<bodyText confidence="0.999955205128205">
good results on the task of ranking verbs para-
phrasing noun compounds. We outperform the
strong baseline and also systems using an unsuper-
vised approach. If we analyse our results we see
that our system scores particularly well on noun
compounds describing materials: in Table 5 we
see that all top ten compounds are either “arti-
facts”, “physical entities” or “entities” according
to WordNet and the relation is quite direct: gen-
erally a made of relation seems appropriate. If we
look at the bottom ten on the other hand, we see re-
lations such as “abstraction” and “measure”: these
are harder to qualify. Also, an “electron micro-
scope” will generally not be perceived as a micro-
scope made of electrons. We can conclude that for
NCs where the relation between the nouns is more
obscure the verbs are harder to rank.
If we look at the Information Gain Ratio, of
all features the rank difference of the verbs in the
training data seems to be the strongest feature, and
of the external features the frequency difference of
the entire phrase containing the NC and the verb.
A lot more investigations could be made into the
viability of using large n-gram collections such as
the Google N-gram corpus for paraphrase tasks.
It might also be interesting to explore a some-
what more challenging variant of this task by not
providing the verbs to be ranked a priori. This
would probably be more interesting for real world
applications because often the task is not only
ranking but finding the verbs in the first place. Our
system should be able to handle this task with mi-
nor modifications: we simply regards all verbs in
the training-data candidates to be ranked. Then,
a pre-filtering step should take place to weed out
irrelevant verbs based on an indicator such as the
LCS of the nouns. In addition a threshold could be
implemented to only accept a (further) limited set
of verbs in the final ranking.
</bodyText>
<sectionHeader confidence="0.999187" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999825666666667">
David W. Aha, Dennis Kibler, and Marc K. Albert.
1991. Instance-based learning algorithms. Mach.
Learn.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-
armuid O´ S´eaghdha, Stan Szpakowicz, and Tony
Veale. 2010. Semeval-2 task 9: The interpreta-
tion of noun compounds using paraphrasing verbs
and prepositions. In Proceedings of the 5th SIGLEX
Workshop on Semantic Evaluation.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2009. Timbl: Tilburg
memory-based learner - version 6.2 - reference
guide.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM
Trans. Speech Lang. Process.
Judith N. Levi. 1978. The Syntax and Semantics of
Complex Nominals.
Preslav Nakov and Marti Hearst. 2005. Search engine
statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of the 9th
Conference on Computational Natural Language
Learning.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings
of the 5th International Workshop on Computational
Semantics.
Jason Rennie. 2000. Wordnet::querydata: a perl mod-
ule for accessing the wordnet database.
Beatrice Warren. 1978. Semantic Patterns of Noun-
Noun Compounds.
</reference>
<page confidence="0.998905">
263
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956529">
<title confidence="0.999795">UvT: Memory-based pairwise ranking of paraphrasing verbs</title>
<author confidence="0.99998">Sander Wubben</author>
<affiliation confidence="0.993785">Tilburg centre for Cognition and Communication Tilburg University</affiliation>
<address confidence="0.986997">The Netherlands</address>
<email confidence="0.997761">s.wubben@uvt.nl</email>
<abstract confidence="0.99853375">In this paper we describe Mephisto, our system for Task 9 of the SemEval-2 workshop. Our approach to this task is to develop a machine learning classifier which determines for each verb pair describing a noun compound which verb should be ranked higher. These classifications are then combined into one ranking. Our classifier uses features from the Google Ngram Corpus, WordNet and the provided training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David W Aha</author>
<author>Dennis Kibler</author>
<author>Marc K Albert</author>
</authors>
<title>Instance-based learning algorithms.</title>
<date>1991</date>
<location>Mach. Learn.</location>
<contexts>
<context position="7754" citStr="Aha et al., 1991" startWordPosition="1344" endWordPosition="1347">tances. The class of these most similar instances is then copied to the new instance The learning algorithm our system uses is the IB1 classier as implemented in TiMBL (version 6.1.5). IB1 is a supervised decision-tree-based implementation of 261 Settings TiMBL F-score Spearman ρ Pearson r KullbackLeibler div. k=3 all features 0.48 0.50 0.44 1.91 k=3 no external features 0.53 0.48 0.41 2.05 k=11 all features 0.51 0.50 0.42 1.97 k=11 no external features 0.20 - - - Table 3: Results for different settings on the development set the k-nearest neighbor algorithm for learning classification tasks (Aha et al., 1991). The TiMBL parameters we used in the Mephisto system for the IB1 classifier are the overlap metric, weighting using GainRatio, and k=3, taking into account the instances on the 3 most similar positions to extrapolate the class of the instance. More information about these settings can be found in the TiMBL reference guide (Daelemans et al., 2009). We train our classifier on the provided training data to classify instances into one of three classes; +up+ if V1 ranks higher than V2 , =eq= if both verbs rank equally and -dwn- if V1 ranks lower than V2. 2.3 Ranking The final step is to combine al</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>David W. Aha, Dennis Kibler, and Marc K. Albert. 1991. Instance-based learning algorithms. Mach. Learn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Butnariu</author>
<author>Su Nam Kim</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Stan Szpakowicz</author>
<author>Tony Veale</author>
</authors>
<title>Semeval-2 task 9: The interpretation of noun compounds using paraphrasing verbs and prepositions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th SIGLEX Workshop on Semantic Evaluation.</booktitle>
<marker>Butnariu, Kim, Nakov, S´eaghdha, Szpakowicz, Veale, 2010</marker>
<rawString>Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diarmuid O´ S´eaghdha, Stan Szpakowicz, and Tony Veale. 2010. Semeval-2 task 9: The interpretation of noun compounds using paraphrasing verbs and prepositions. In Proceedings of the 5th SIGLEX Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory-based learner - version 6.2 - reference guide.</title>
<date>2009</date>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2009</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2009. Timbl: Tilburg memory-based learner - version 6.2 - reference guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<contexts>
<context position="4126" citStr="Fellbaum, 1998" startWordPosition="708" endWordPosition="709">July 2010. c�2010 Association for Computational Linguistics feature values info gain gain ratio verb1 4,093 0.24 0.02 verb2 4,093 0.24 0.02 verb1-verb2 768,543 1.06 0.06 verb1-verb2-LCS 986,031 1.29 0.07 n-gram score1 7 0.07 0.02 n-gram score2 7 0.01 0.08 weighted mean 7 0.29 0.12 rank 3 0.68 0.43 Table 1: Features used in our system count the number of times that V1 is ranked higher than V2 and vice versa for every NC where both verbs occur. We end up with a positive, equal or negative class. WordNet Least Common Subsumer In order to distinguish between different kinds of NCs we use WordNet (Fellbaum, 1998) to determine the kind of relation between the nouns. This idea is supported by work by Levi (1978), Warren (1978) and Nastase &amp; Szpakowicz (2003). Our intuition is that the ranking of verb phrases is very dependent on this relation between the nouns. To determine this we use the WordNet::QueryData (Rennie, 2000) module. In the WordNet graph we look for the Least Common Subsumer (LCS) of the two nouns. The LCS is the lowest parent node of both nouns. We combine the LCS with both verb phrases into one feature. Google N-gram features We use the Google Ngram corpus to count co-occurence frequenci</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Trans. Speech Lang. Process.</journal>
<contexts>
<context position="4955" citStr="Lapata and Keller, 2005" startWordPosition="854" endWordPosition="857"> dependent on this relation between the nouns. To determine this we use the WordNet::QueryData (Rennie, 2000) module. In the WordNet graph we look for the Least Common Subsumer (LCS) of the two nouns. The LCS is the lowest parent node of both nouns. We combine the LCS with both verb phrases into one feature. Google N-gram features We use the Google Ngram corpus to count co-occurence frequencies of certain n-grams. An NC occurring often together with a certain verb should indicate that that verb is a good paraphrase for the NC. Using web text for various NLP-tasks has been proven to be useful (Lapata and Keller, 2005), also for NC interpretation (Nakov and Hearst, 2005). Because of data sparseness and the unlikelihood of finding a perfect match for a certain n-gram, we adopt different strategies for constructing features. First of all, we try to relax the matching conditions by applying certain regular expression. Given the NC “abortion problem” and the paraphrasing verb “be related to” , it seems unlikely you will ever encounter the n-gram “problem be related to abortion”, yet in the training data “be related to” is the number three verb for “abortion problem”. Therefore, we first apply some simple inflec</context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>Mirella Lapata and Frank Keller. 2005. Web-based models for natural language processing. ACM Trans. Speech Lang. Process.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith N Levi</author>
</authors>
<title>The Syntax and Semantics of Complex Nominals.</title>
<date>1978</date>
<contexts>
<context position="4225" citStr="Levi (1978)" startWordPosition="726" endWordPosition="727">,093 0.24 0.02 verb2 4,093 0.24 0.02 verb1-verb2 768,543 1.06 0.06 verb1-verb2-LCS 986,031 1.29 0.07 n-gram score1 7 0.07 0.02 n-gram score2 7 0.01 0.08 weighted mean 7 0.29 0.12 rank 3 0.68 0.43 Table 1: Features used in our system count the number of times that V1 is ranked higher than V2 and vice versa for every NC where both verbs occur. We end up with a positive, equal or negative class. WordNet Least Common Subsumer In order to distinguish between different kinds of NCs we use WordNet (Fellbaum, 1998) to determine the kind of relation between the nouns. This idea is supported by work by Levi (1978), Warren (1978) and Nastase &amp; Szpakowicz (2003). Our intuition is that the ranking of verb phrases is very dependent on this relation between the nouns. To determine this we use the WordNet::QueryData (Rennie, 2000) module. In the WordNet graph we look for the Least Common Subsumer (LCS) of the two nouns. The LCS is the lowest parent node of both nouns. We combine the LCS with both verb phrases into one feature. Google N-gram features We use the Google Ngram corpus to count co-occurence frequencies of certain n-grams. An NC occurring often together with a certain verb should indicate that that</context>
</contexts>
<marker>Levi, 1978</marker>
<rawString>Judith N. Levi. 1978. The Syntax and Semantics of Complex Nominals.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: Application to noun compound bracketing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="5008" citStr="Nakov and Hearst, 2005" startWordPosition="863" endWordPosition="866">rmine this we use the WordNet::QueryData (Rennie, 2000) module. In the WordNet graph we look for the Least Common Subsumer (LCS) of the two nouns. The LCS is the lowest parent node of both nouns. We combine the LCS with both verb phrases into one feature. Google N-gram features We use the Google Ngram corpus to count co-occurence frequencies of certain n-grams. An NC occurring often together with a certain verb should indicate that that verb is a good paraphrase for the NC. Using web text for various NLP-tasks has been proven to be useful (Lapata and Keller, 2005), also for NC interpretation (Nakov and Hearst, 2005). Because of data sparseness and the unlikelihood of finding a perfect match for a certain n-gram, we adopt different strategies for constructing features. First of all, we try to relax the matching conditions by applying certain regular expression. Given the NC “abortion problem” and the paraphrasing verb “be related to” , it seems unlikely you will ever encounter the n-gram “problem be related to abortion”, yet in the training data “be related to” is the number three verb for “abortion problem”. Therefore, we first apply some simple inflection. Instead of “be” we match on “is/are/being”. and</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compound bracketing. In Proceedings of the 9th Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Exploring noun-modifier semantic relations.</title>
<date>2003</date>
<booktitle>In Proceedings of the 5th International Workshop on Computational Semantics.</booktitle>
<contexts>
<context position="4272" citStr="Nastase &amp; Szpakowicz (2003)" startWordPosition="731" endWordPosition="734"> 0.02 verb1-verb2 768,543 1.06 0.06 verb1-verb2-LCS 986,031 1.29 0.07 n-gram score1 7 0.07 0.02 n-gram score2 7 0.01 0.08 weighted mean 7 0.29 0.12 rank 3 0.68 0.43 Table 1: Features used in our system count the number of times that V1 is ranked higher than V2 and vice versa for every NC where both verbs occur. We end up with a positive, equal or negative class. WordNet Least Common Subsumer In order to distinguish between different kinds of NCs we use WordNet (Fellbaum, 1998) to determine the kind of relation between the nouns. This idea is supported by work by Levi (1978), Warren (1978) and Nastase &amp; Szpakowicz (2003). Our intuition is that the ranking of verb phrases is very dependent on this relation between the nouns. To determine this we use the WordNet::QueryData (Rennie, 2000) module. In the WordNet graph we look for the Least Common Subsumer (LCS) of the two nouns. The LCS is the lowest parent node of both nouns. We combine the LCS with both verb phrases into one feature. Google N-gram features We use the Google Ngram corpus to count co-occurence frequencies of certain n-grams. An NC occurring often together with a certain verb should indicate that that verb is a good paraphrase for the NC. Using we</context>
</contexts>
<marker>Nastase, Szpakowicz, 2003</marker>
<rawString>Vivi Nastase and Stan Szpakowicz. 2003. Exploring noun-modifier semantic relations. In Proceedings of the 5th International Workshop on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Rennie</author>
</authors>
<title>Wordnet::querydata: a perl module for accessing the wordnet database.</title>
<date>2000</date>
<contexts>
<context position="4440" citStr="Rennie, 2000" startWordPosition="762" endWordPosition="764"> used in our system count the number of times that V1 is ranked higher than V2 and vice versa for every NC where both verbs occur. We end up with a positive, equal or negative class. WordNet Least Common Subsumer In order to distinguish between different kinds of NCs we use WordNet (Fellbaum, 1998) to determine the kind of relation between the nouns. This idea is supported by work by Levi (1978), Warren (1978) and Nastase &amp; Szpakowicz (2003). Our intuition is that the ranking of verb phrases is very dependent on this relation between the nouns. To determine this we use the WordNet::QueryData (Rennie, 2000) module. In the WordNet graph we look for the Least Common Subsumer (LCS) of the two nouns. The LCS is the lowest parent node of both nouns. We combine the LCS with both verb phrases into one feature. Google N-gram features We use the Google Ngram corpus to count co-occurence frequencies of certain n-grams. An NC occurring often together with a certain verb should indicate that that verb is a good paraphrase for the NC. Using web text for various NLP-tasks has been proven to be useful (Lapata and Keller, 2005), also for NC interpretation (Nakov and Hearst, 2005). Because of data sparseness and</context>
</contexts>
<marker>Rennie, 2000</marker>
<rawString>Jason Rennie. 2000. Wordnet::querydata: a perl module for accessing the wordnet database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Warren</author>
</authors>
<date>1978</date>
<journal>Semantic Patterns of NounNoun Compounds.</journal>
<contexts>
<context position="4240" citStr="Warren (1978)" startWordPosition="728" endWordPosition="729">2 verb2 4,093 0.24 0.02 verb1-verb2 768,543 1.06 0.06 verb1-verb2-LCS 986,031 1.29 0.07 n-gram score1 7 0.07 0.02 n-gram score2 7 0.01 0.08 weighted mean 7 0.29 0.12 rank 3 0.68 0.43 Table 1: Features used in our system count the number of times that V1 is ranked higher than V2 and vice versa for every NC where both verbs occur. We end up with a positive, equal or negative class. WordNet Least Common Subsumer In order to distinguish between different kinds of NCs we use WordNet (Fellbaum, 1998) to determine the kind of relation between the nouns. This idea is supported by work by Levi (1978), Warren (1978) and Nastase &amp; Szpakowicz (2003). Our intuition is that the ranking of verb phrases is very dependent on this relation between the nouns. To determine this we use the WordNet::QueryData (Rennie, 2000) module. In the WordNet graph we look for the Least Common Subsumer (LCS) of the two nouns. The LCS is the lowest parent node of both nouns. We combine the LCS with both verb phrases into one feature. Google N-gram features We use the Google Ngram corpus to count co-occurence frequencies of certain n-grams. An NC occurring often together with a certain verb should indicate that that verb is a good</context>
</contexts>
<marker>Warren, 1978</marker>
<rawString>Beatrice Warren. 1978. Semantic Patterns of NounNoun Compounds.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>