<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.963032">
The chunk as the period of the functions length and frequency
of words on the syntagmatic axis
</title>
<author confidence="0.903769">
Jacques Vergne
</author>
<affiliation confidence="0.797661">
GREYC - Universite de Caen - France
</affiliation>
<email confidence="0.977984">
Jacques.Vergne@info.unicaen.fr
</email>
<sectionHeader confidence="0.993315" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999505772727273">
Chunking is segmenting a text into chunks,
sub-sentential segments, that Abney ap-
proximately defined as stress groups. Chunk-
ing usually uses monolingual resources, most
often exhaustive, sometimes partial : function
words and punctuations, which often mark
beginnings and ends of chunks. But, to ex-
tend this method to other languages, mono-
lingual resources have to be multiplied. We
present a new method : endogenous chunk-
ing, which uses no other resource than the
text to be segmented itself. The idea of this
method comes from Zipf : to make the least
communication effort, speakers are driven to
shorten frequent words. A chunk then can be
characterized as the period of the periodic
correlated functions length and frequency of
words on the syntagmatic axis. This original
method takes its advantage to be applied to a
great number of languages of alphabetic
script, with the same algorithm, without any
resource.
</bodyText>
<sectionHeader confidence="0.961619" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.985986470588236">
Chunking is a frequent segmentation step in
many processing types : robust parsers, parsers
of linear complexity (Vergne, 2000), computing
stress groups and linking them in tts systems, to
compute macro-prosody (Vannier et al., 1999),
in automatic indexing, the chunk as another in-
dexed grain above the word in the grain hierar-
chy, and in sub-sentential alignment, the chunk
as an aligned grain.
The method we propose is based on the prop-
erties of the functions length and frequency of
words on the syntagmatic axis. These two func-
tions are correlated : integer, periodic, synchro-
nous, in phase opposition, and their period al-
lows to define the chunk. On a period, the length
function is non-decreasing, and the frequency
function is non-increasing. These concepts con-
tinue in Zipf&apos;s direction : minimizing the com-
munication effort drives the speaker to shorten
frequent words (Zipf, 1949). The length metrics
defined by Zipf is not the number of letters, but
the number of syllables or the number of pho-
nemes of the written form (Zipf, 1935); the met-
rics of our method is also the number of sylla-
bles, or more precisely the number of vowel nu-
clei, computable from the written form; this met-
rics takes its root into the oral origin of the
chunk. The word frequency is measured in the
segmented text.
This method of segmentation into chunks is
based on digital properties, and is valid on lan-
guages with alphabetic script. It is endogenous,
as it computes on the text to be segmented and
does not use any resource external to the parsed
text.
1 Structure model of the chunk accord-
ing to Abney and according to Dejean
The concept of chunk has been proposed by
Steve Abney (1991). It has been based on prop-
erties of speech : Abney defined the chunk as a
stress group. As speech is constrained by the vo-
cal system, we can see the chunk as a generic
concept on natural languages, a concept of lan-
guage. Herve Dejean (1998) has proposed a
structure model for the chunk : beginnings and
ends of chunk (words or morphemes) around a
kernel (Dejean, 1998, page 117); our method
uses this model.
For instance, the written form &amp;quot;Commission&amp;quot;
has been found in the following chunks in the
same text :
</bodyText>
<equation confidence="0.569122857142857">
[ Commission europeenne J
[ la Commission J
[ la Commission europeenne J
[ dans la Commission J
And here is the synthesis :
[ dans [ la [ Commission J europeenne J
[ beginnings [ kernel ] ends ]
</equation>
<page confidence="0.963768">
202
</page>
<bodyText confidence="0.966184137931035">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 202–205,
Paris, October 2009. c�2009 Association for Computational Linguistics
2 Local deductions and their generali-
zation at text level
Properties of the chunk are used locally at occur-
rence level : an occurrence of a written form is
locally a beginning or an end of a chunk. An im-
portant question is to decide how to articulate
local deductions at occurrence level and their
global merging at text level.
We know that occurrences of the same written
form may be occurrences of more than one word,
in different contexts. For instance, &amp;quot;on&amp;quot; in Eng-
lish is the beginning of a chunk in &amp;quot;on the con-
trary&amp;quot;, but it is the end of a chunk in &amp;quot;it is going
on&amp;quot;. These two occurrences correspond to two
different words, which have different positions
and different contexts, and their local deductions
cannot be merged. So, we can merge local de-
ductions for occurrences of the same word. In
practice, we merge local deductions for occur-
rences of a written form if there is no beginning -
end contradiction.
We tried full merging, as if all occurrences
were of the same word. This solution remains
valid for monofocused short texts (some thou-
sands words). But, to be able to chunk longer
texts, we have chosen now the solution of a par-
tial generalization (see below in 4).
</bodyText>
<sectionHeader confidence="0.812928" genericHeader="method">
3 Two properties of a chunk
</sectionHeader>
<bodyText confidence="0.956904111111111">
The algorithm exploits two properties of the
chunk.
3.1 Property 1 : the chunk is a constituent of
the virgulot
Herv6 D6jean (1998) has defined the &amp;quot;entre-
ponctuations&amp;quot; as a constituent delimited by two
punctuations. Nadine Lucas (Lucas, 2001) has
proposed the term &amp;quot;virgulot&amp;quot;, that we will use
now. We define the following constituent hierar-
chy : the text is constituted of virgulots, them-
selves constituted of chunks, themselves consti-
tuted of occurrences of written forms.
Property exploited by the algorithm :
- a written form attested at the beginning of a
virgulot is a beginning of a chunk,
- a written form attested at the end of a virgulot
is an end of a chunk.
Here are some instances of virgulots :
</bodyText>
<construct confidence="0.857649833333333">
, in denen Aale leben ,
, bis die BewirtschaftungsplŠne vorliegen .
. It also intends to explore measures ,
, before migrating upstream to spend most of
their lives .
, en las aguas centro-occidentales del Oceano
Atl‡ntico .
, donde transcurre la mayor parte de su vida .
. Lasciandosi trasportare dalla corrente e nuo-
tando ,
, dove si riproducono una sola volta e poi
muoiono .
</construct>
<bodyText confidence="0.99980775">
First written forms of virgulots are beginnings
of chunks (prepositions, pronouns, ...), and their
last written forms are ends of chunks (nouns,
verbs, adjectives, ...).
</bodyText>
<subsectionHeader confidence="0.502982">
3.2 Property 2 : the chunk is the period of the
</subsectionHeader>
<bodyText confidence="0.950457">
correlated functions length and fre-
quency of words on the syntagmatic axis
We define two integer functions of words on the
syntagmatic axis (inside a virgulot) : their length,
defined as their number of syllables, and their
frequency in the text to be segmented.
Here is an instance of a virgulot :
, would migrate from the rivers on their territories ,
length: 1 3 1 1 2 1 1 4
frequ.: 10 3 6 65 2 6 4 1
On the length function, we have the following
non-decreasing sequences : [1 3] [1 1 2] [1 1 4].
On the frequency function, we have the fol-
lowing non-increasing sequences : [10 3] [6] [65
2] [6 4 1].
For these two functions, a period corresponds
to a sequence; in other words, these sequences
give a way to segment; these 2 functions are syn-
chronous : sequences of both functions (nearly)
define the same periods; on a (synchronous) pe-
riod, both functions are in phase opposition : on a
period (which defines a chunk), the length func-
tion is non-decreasing, and the frequency func-
tion is non-increasing; the common properties of
these two functions allow us to call them corre-
lated; it is an other way to say that short words
are frequent and that long words are rare.
We notice, following Zipf (1949) in &amp;quot;Human
Behavior and the Principle of Least-Effort&amp;quot; that
writing and speech are an optimal compression;
it reminds the principles of file compression in
computer science : frequent data are short coded,
and rare data are long coded. Let us make an ob-
servation on the Zipf law, as it is known today :
this law makes a relation between frequency and
rows of words sorted by decreasing frequency; if
we knew only this law, we would forget length
of words; but Zipf proposed to consider length
and frequency together, in a correlated way, as
an optimization (the Least-Effort). As we use
length and frequency together, in a correlated
way, we go back to the origin of Zipf&apos;s concepts.
</bodyText>
<page confidence="0.994912">
203
</page>
<bodyText confidence="0.99997445">
To compute word length from the written
form, length is defined as the number of sylla-
bles, i.e. the number of vowel nuclei (a sequence
of contiguous vowels corresponds to a vowel
nucleus, and to a length equal to 1). This calcula-
tion needs as input the vowels of the alphabet
(Latin or Greek). There is a particular case : is
the y vowel or consonant. The y is vowel in &amp;quot;sys-
tem&amp;quot; (length 2) and consonant in &amp;quot;rayon&amp;quot; (length
2); y is consonant by default; y is vowel at the
beginning or the end of a word, or alone (usually,
by, y); y is vowel between 2 consonants; these
rules are enough to process all cases for the 20
natural languages of the corpus. Acronyms (se-
quences of uppercases) have a length equal to
twice their numbers of letters (tendency to be in
the end of chunk). A number (sequence of fig-
ures) has a length equal to 1, whatever its num-
ber of figures (tendency to be in the beginning of
chunk).
</bodyText>
<sectionHeader confidence="0.9839975" genericHeader="method">
4 An algorithm based on these proper-
ties
</sectionHeader>
<bodyText confidence="0.99650516">
The frequency and the length of every written
form are computed.
For the property 1, based on the virgulot, the
text is processed, and occurrences of written
forms at the beginning or end of virgulot are
noted as beginning or end of chunk.
For the property 2, based on monotonous se-
quences, the text is processed, while noting bor-
ders between 2 monotonous sequences, that
gives for each border an end and a beginning of
chunk. A Boolean function &amp;quot;in the same se-
quence&amp;quot; returns whether 2 contiguous words are
in the same monotonous sequence (i.e. in the
same chunk). Four solutions are experimented :
on length only, on frequency only, on length
AND frequency (then shorter chunks), or on
length OR frequency (then longer chunks). Re-
sults are very comparable, because both func-
tions are strongly correlated1. For example, this
function, in &amp;quot;length OR frequency&amp;quot; mode, on
words i and i+1, to express the fact that these two
words are in the same sequence, has the follow-
ing form :
words i and i+1 are not separators of virgulot
AND
</bodyText>
<equation confidence="0.993081">
( length(i+1) ≥ length(i) OR
frequency(i+1) ≤ frequency(i) )
</equation>
<footnote confidence="0.542628666666667">
1 Using length alone allows, not using frequency, to
get a method usable on a very short text, as a
search engine query.
</footnote>
<bodyText confidence="0.958490833333333">
The generalization of local deductions is done
the following way : for all occurrences of a writ-
ten form, a synthesis of local deductions is done.
There are 8 cases : 2 properties, 4 cases for each
(2 Booleans : beginning, end). If all local deduc-
tions are compatible, they are merged, i.e. occur-
rences without any local deduction take the tag
of occurrences with the same local deduction :
either beginning or end of chunk.
Here is the trace of the process on our instance
of virgulot :
virgul. sequ. general. result len. freq.
</bodyText>
<figure confidence="0.802632666666667">
b e b e b e b e
[1,0] [1,0] [0,0] [2,0] 1 10 would
[0,0] [0,1] [0,1] [0,2] 3 3 migrate
[0,0] [1,0] [1,0] [2,0] 1 6 from
[0,0] [0,0] [1,0] [1,0] 1 65 the
[0,0] [0,1] [0,1] [0,2] 2 2 rivers
[0,0] [1,0] [1,0] [2,0] 1 6 on
[0,0] [0,0] [1,0] [1,0] 1 4 their
[0,1] [0,1] [0,0] [0,2] 4 1 territories
</figure>
<bodyText confidence="0.975864761904762">
From the first property (the first column of
Booleans), would is the beginning, and territo-
ries is the end of the virgulot, therefore begin-
ning and end of a chunk ([ marks a beginning of
chunk, ] marks an end of chunk) :
, [ would migrate from the rivers on their territories ],
The second property (the second column of
Booleans) which exploits the monotonous se-
quences, here in &amp;quot;length OR frequency&amp;quot; mode,
gives the following chunking :
, [ would migrate ] [ from the rivers ]
[ on their territories ] ,
The generalization of local deductions (the
third column of Booleans) adds the fact that the
and their are beginnings of a chunk elsewhere in
this text.
Then these three sources of deduction are
merged, and we obtain the following segmenta-
tion (the forth column) :
, [ would migrate ] [ from [ the rivers ]
[ on [ their territories ] ,
</bodyText>
<sectionHeader confidence="0.9547915" genericHeader="method">
5 Some sentences segmented into
chunks
</sectionHeader>
<bodyText confidence="0.999914875">
The validation corpus of the method is composed
of 12 press releases (about 1000 words each for
one language), every release is written into 6 to
20 languages, and of the part 1 of the &amp;quot;Treaty
establishing a Constitution for Europe&amp;quot; in 11
languages (about 10 000 words for one lan-
guage), from the website of the European Union
(http://europa.eu/).
</bodyText>
<page confidence="0.99659">
204
</page>
<bodyText confidence="0.993413333333333">
The following sentences are extracted from
the release IP/05/1018 of 2005 (and processed in
&amp;quot;length OR frequency&amp;quot; mode) :
</bodyText>
<figure confidence="0.973711583333333">
[ Die LaichgrŸnde ] [ der Aale ] befinden ]
[ im Sargassosee ] [ im mittleren Westatlantik ] .
[ the west-
ern central Atlantic ] Ocean ] .
[ Las anguilas ] desovan ] [ en [ el Mar [ de [ los
Sargazos ] , [ en [ las aguas ] centro-occidentales ]
[ del Oceano Atl‡ntico ] .
[ La zone [ de frai ] [ de l’anguille ] [ se situe [ en
mer ] [ des Sargasses ] , [ dans [ la partie centre-
ouest ] [ de l’ocean Atlantique ] .
[ Le anguille ] [ si riproducono ] [ nel mar [ dei
Sargassi ] , [ nell’Atlantico centro-occidentale ] .
</figure>
<bodyText confidence="0.99240625">
The following sentences are extracted from
the part 1 of the &amp;quot;Treaty establishing a Constitu-
tion for Europe&amp;quot; (and processed in &amp;quot;length OR
frequency&amp;quot; mode) :
</bodyText>
<subsubsectionHeader confidence="0.406741">
[ Die Union ] steht allen europŠischen ] Staaten of-
</subsubsectionHeader>
<construct confidence="0.8648865">
[ ihre Werte ] achten ] [ und [ sich ver-
pflichten ] , [ sie gemeinsam ] [ zu fšrdern ] .
euro-
peens ] [ qui respectent ] [ ses valeurs ] [ et [ qui
s&apos;engagent ] [ d [ les promouvoir ] [ en commun ] .
[ L&apos;Unione [ e aperta ] [ a tutti ] [ gli Stati europei ]
[ che rispettano ] [ i suoi valori ] [ e [ si impegna-
no ] [ a promuoverli congiuntamente ] .
</construct>
<sectionHeader confidence="0.897412" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.9999585">
While characterizing the chunk in a purely digi-
tal way, from properties of length et frequency
functions of words on the syntagmatic axis, this
original method consists in calculations on the
text to segment; it has the advantage to be ap-
plied to a great number of languages, with the
same algorithm, without any monolingual re-
source : languages with alphabetic script, with a
written word which separates function words
from content words (it is not the case in Finnish),
and compatible with a structure model of the
chunk where function words generally are before
content words; the method is promising for the
22 languages of the European Community2.
</bodyText>
<footnote confidence="0.540352666666667">
2 See results on :
http://www.info.unicaen.fr/~jvergne/chunking_mu
ltilingue_endogene/
</footnote>
<bodyText confidence="0.999920764705882">
This method can be applied in automatic in-
dexing, for search-engines (as Exalead does, to
be able to output the most frequent terms associ-
ated to the documents of the answer), and in sub-
sentential alignment, to constraint the statistical
alignment (as in Similis, the alignment software
of Lingua et Machina, but this software uses
monolingual resources for every language). The
interesting feature of this method is not to need
any resource for a new language to process3.
As it is independent from specificities of each
language, this method is not &amp;quot;multilanguage&amp;quot;,
neither &amp;quot;multi-monolanguage&amp;quot;, but as it exploits
generic properties of natural languages, that is
properties of language, as an abstraction of natu-
ral languages, we could perhaps simply call it a
&amp;quot;linguistic&amp;quot; method.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.958501535714286">
Steven Abney. 1991. Parsing By Chunks. in
Principle-Based Parsing, 257-278, Kluwer
Academic Publishers.
Herve Dejean. 1998. Concepts et algorithmes
pour la decouverte des structures formelles
des langues. These de doctorat de l&apos;universite
de Caen, France.
Nadine Lucas. 2001. ƒtude et modelisation de
l&apos;explication dans les textes. Actes du Collo-
que &amp;quot;L&apos;explication: enjeux cognitifs et com-
municationnels&amp;quot;, Paris.
Gerald Vannier, Anne Lacheret-Dujour, Jacques
Vergne. 1999. Pauses location and duration
calculated with syntactic dependencies and
textual considerations for t.t.s. system. ICPhS
1999, San Francisco, USA, August 1999.
Jacques Vergne. 2000. Tutorial : Trends in Ro-
bust Parsing. Coling 2000.
George K. Zipf. 1935. The psychobiology of lan-
guage : An introduction to dynamic philology.
Boston, Mass., Houghton-Mifflin.
George K. Zipf. 1949. Human Behavior and the
Principle of Least-Effort. Addison-Wesley.
3 But a problem for this large scale multilingual
method is to evaluate the results on so many lan-
guages : we need a speaker for every language.
For the moment, it is done for German, English,
Spanish, French and Italian.
</reference>
<figure confidence="0.996668153846154">
[ The Union ] [ shall be open ] [ to [ all
[ European
States ] [ which respect ] [ its values ] [ and
[ are
committed ] [ to promoting ] them ] together ] .
[ La Uni—n ] [ est‡ abierta ] [ a todos ] [ los Esta-
dos ] europeos ] [ que respeten ] [ sus valores ] [ y
[ se comprometan ] [ a promoverlos ] [ en comœn ] .
[ sich
[ Eels spawn ] [ in [ the Sargasso Sea [ in
fen ] , [ die
[ L&apos;Union [ est ouverte ]
[ d [ tous [ les ƒtats ]
</figure>
<page confidence="0.976401">
205
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.535468">
<title confidence="0.958761">chunk as the period of the functions of words on the syntagmatic axis</title>
<author confidence="0.8950745">Jacques Vergne GREYC</author>
<email confidence="0.744739">Jacques.Vergne@info.unicaen.fr</email>
<abstract confidence="0.997248826086957">Chunking is segmenting a text into chunks, sub-sentential segments, that Abney approximately defined as stress groups. Chunking usually uses monolingual resources, most often exhaustive, sometimes partial : function words and punctuations, which often mark beginnings and ends of chunks. But, to extend this method to other languages, monolingual resources have to be multiplied. We present a new method : endogenous chunking, which uses no other resource than the text to be segmented itself. The idea of this method comes from Zipf : to make the least communication effort, speakers are driven to shorten frequent words. A chunk then can be characterized as the period of the periodic correlated functions length and frequency of words on the syntagmatic axis. This original method takes its advantage to be applied to a great number of languages of alphabetic script, with the same algorithm, without any resource.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Parsing By Chunks.</title>
<date>1991</date>
<booktitle>in Principle-Based Parsing,</booktitle>
<pages>257--278</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="2793" citStr="Abney (1991)" startWordPosition="465" endWordPosition="466">ethod is also the number of syllables, or more precisely the number of vowel nuclei, computable from the written form; this metrics takes its root into the oral origin of the chunk. The word frequency is measured in the segmented text. This method of segmentation into chunks is based on digital properties, and is valid on languages with alphabetic script. It is endogenous, as it computes on the text to be segmented and does not use any resource external to the parsed text. 1 Structure model of the chunk according to Abney and according to Dejean The concept of chunk has been proposed by Steve Abney (1991). It has been based on properties of speech : Abney defined the chunk as a stress group. As speech is constrained by the vocal system, we can see the chunk as a generic concept on natural languages, a concept of language. Herve Dejean (1998) has proposed a structure model for the chunk : beginnings and ends of chunk (words or morphemes) around a kernel (Dejean, 1998, page 117); our method uses this model. For instance, the written form &amp;quot;Commission&amp;quot; has been found in the following chunks in the same text : [ Commission europeenne J [ la Commission J [ la Commission europeenne J [ dans la Commis</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven Abney. 1991. Parsing By Chunks. in Principle-Based Parsing, 257-278, Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herve Dejean</author>
</authors>
<title>Concepts et algorithmes pour la decouverte des structures formelles des langues. These de doctorat de l&apos;universite de</title>
<date>1998</date>
<location>Caen, France.</location>
<contexts>
<context position="3034" citStr="Dejean (1998)" startWordPosition="512" endWordPosition="513"> method of segmentation into chunks is based on digital properties, and is valid on languages with alphabetic script. It is endogenous, as it computes on the text to be segmented and does not use any resource external to the parsed text. 1 Structure model of the chunk according to Abney and according to Dejean The concept of chunk has been proposed by Steve Abney (1991). It has been based on properties of speech : Abney defined the chunk as a stress group. As speech is constrained by the vocal system, we can see the chunk as a generic concept on natural languages, a concept of language. Herve Dejean (1998) has proposed a structure model for the chunk : beginnings and ends of chunk (words or morphemes) around a kernel (Dejean, 1998, page 117); our method uses this model. For instance, the written form &amp;quot;Commission&amp;quot; has been found in the following chunks in the same text : [ Commission europeenne J [ la Commission J [ la Commission europeenne J [ dans la Commission J And here is the synthesis : [ dans [ la [ Commission J europeenne J [ beginnings [ kernel ] ends ] 202 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 202–205, Paris, October 2009. c�2009 Associa</context>
</contexts>
<marker>Dejean, 1998</marker>
<rawString>Herve Dejean. 1998. Concepts et algorithmes pour la decouverte des structures formelles des langues. These de doctorat de l&apos;universite de Caen, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadine Lucas</author>
</authors>
<title>ƒtude et modelisation de l&apos;explication dans les textes. Actes du Colloque &amp;quot;L&apos;explication: enjeux cognitifs et communicationnels&amp;quot;,</title>
<date>2001</date>
<location>Paris.</location>
<contexts>
<context position="5121" citStr="Lucas, 2001" startWordPosition="879" endWordPosition="880"> occurrences of a written form if there is no beginning - end contradiction. We tried full merging, as if all occurrences were of the same word. This solution remains valid for monofocused short texts (some thousands words). But, to be able to chunk longer texts, we have chosen now the solution of a partial generalization (see below in 4). 3 Two properties of a chunk The algorithm exploits two properties of the chunk. 3.1 Property 1 : the chunk is a constituent of the virgulot Herv6 D6jean (1998) has defined the &amp;quot;entreponctuations&amp;quot; as a constituent delimited by two punctuations. Nadine Lucas (Lucas, 2001) has proposed the term &amp;quot;virgulot&amp;quot;, that we will use now. We define the following constituent hierarchy : the text is constituted of virgulots, themselves constituted of chunks, themselves constituted of occurrences of written forms. Property exploited by the algorithm : - a written form attested at the beginning of a virgulot is a beginning of a chunk, - a written form attested at the end of a virgulot is an end of a chunk. Here are some instances of virgulots : , in denen Aale leben , , bis die BewirtschaftungsplŠne vorliegen . . It also intends to explore measures , , before migrating upstre</context>
</contexts>
<marker>Lucas, 2001</marker>
<rawString>Nadine Lucas. 2001. ƒtude et modelisation de l&apos;explication dans les textes. Actes du Colloque &amp;quot;L&apos;explication: enjeux cognitifs et communicationnels&amp;quot;, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Vannier</author>
<author>Anne Lacheret-Dujour</author>
<author>Jacques Vergne</author>
</authors>
<title>Pauses location and duration calculated with syntactic dependencies and textual considerations for t.t.s. system. ICPhS 1999,</title>
<date>1999</date>
<location>San Francisco, USA,</location>
<contexts>
<context position="1350" citStr="Vannier et al., 1999" startWordPosition="208" endWordPosition="211"> the least communication effort, speakers are driven to shorten frequent words. A chunk then can be characterized as the period of the periodic correlated functions length and frequency of words on the syntagmatic axis. This original method takes its advantage to be applied to a great number of languages of alphabetic script, with the same algorithm, without any resource. Introduction Chunking is a frequent segmentation step in many processing types : robust parsers, parsers of linear complexity (Vergne, 2000), computing stress groups and linking them in tts systems, to compute macro-prosody (Vannier et al., 1999), in automatic indexing, the chunk as another indexed grain above the word in the grain hierarchy, and in sub-sentential alignment, the chunk as an aligned grain. The method we propose is based on the properties of the functions length and frequency of words on the syntagmatic axis. These two functions are correlated : integer, periodic, synchronous, in phase opposition, and their period allows to define the chunk. On a period, the length function is non-decreasing, and the frequency function is non-increasing. These concepts continue in Zipf&apos;s direction : minimizing the communication effort d</context>
</contexts>
<marker>Vannier, Lacheret-Dujour, Vergne, 1999</marker>
<rawString>Gerald Vannier, Anne Lacheret-Dujour, Jacques Vergne. 1999. Pauses location and duration calculated with syntactic dependencies and textual considerations for t.t.s. system. ICPhS 1999, San Francisco, USA, August 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Vergne</author>
</authors>
<title>Tutorial : Trends in Robust Parsing. Coling</title>
<date>2000</date>
<contexts>
<context position="1244" citStr="Vergne, 2000" startWordPosition="194" endWordPosition="195">r resource than the text to be segmented itself. The idea of this method comes from Zipf : to make the least communication effort, speakers are driven to shorten frequent words. A chunk then can be characterized as the period of the periodic correlated functions length and frequency of words on the syntagmatic axis. This original method takes its advantage to be applied to a great number of languages of alphabetic script, with the same algorithm, without any resource. Introduction Chunking is a frequent segmentation step in many processing types : robust parsers, parsers of linear complexity (Vergne, 2000), computing stress groups and linking them in tts systems, to compute macro-prosody (Vannier et al., 1999), in automatic indexing, the chunk as another indexed grain above the word in the grain hierarchy, and in sub-sentential alignment, the chunk as an aligned grain. The method we propose is based on the properties of the functions length and frequency of words on the syntagmatic axis. These two functions are correlated : integer, periodic, synchronous, in phase opposition, and their period allows to define the chunk. On a period, the length function is non-decreasing, and the frequency funct</context>
</contexts>
<marker>Vergne, 2000</marker>
<rawString>Jacques Vergne. 2000. Tutorial : Trends in Robust Parsing. Coling 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George K Zipf</author>
</authors>
<title>The psychobiology of language : An introduction to dynamic philology.</title>
<date>1935</date>
<location>Boston, Mass., Houghton-Mifflin.</location>
<contexts>
<context position="2159" citStr="Zipf, 1935" startWordPosition="348" endWordPosition="349">he properties of the functions length and frequency of words on the syntagmatic axis. These two functions are correlated : integer, periodic, synchronous, in phase opposition, and their period allows to define the chunk. On a period, the length function is non-decreasing, and the frequency function is non-increasing. These concepts continue in Zipf&apos;s direction : minimizing the communication effort drives the speaker to shorten frequent words (Zipf, 1949). The length metrics defined by Zipf is not the number of letters, but the number of syllables or the number of phonemes of the written form (Zipf, 1935); the metrics of our method is also the number of syllables, or more precisely the number of vowel nuclei, computable from the written form; this metrics takes its root into the oral origin of the chunk. The word frequency is measured in the segmented text. This method of segmentation into chunks is based on digital properties, and is valid on languages with alphabetic script. It is endogenous, as it computes on the text to be segmented and does not use any resource external to the parsed text. 1 Structure model of the chunk according to Abney and according to Dejean The concept of chunk has b</context>
</contexts>
<marker>Zipf, 1935</marker>
<rawString>George K. Zipf. 1935. The psychobiology of language : An introduction to dynamic philology. Boston, Mass., Houghton-Mifflin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George K Zipf</author>
</authors>
<title>Human Behavior and the Principle of Least-Effort.</title>
<date>1949</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="2006" citStr="Zipf, 1949" startWordPosition="319" endWordPosition="320"> indexed grain above the word in the grain hierarchy, and in sub-sentential alignment, the chunk as an aligned grain. The method we propose is based on the properties of the functions length and frequency of words on the syntagmatic axis. These two functions are correlated : integer, periodic, synchronous, in phase opposition, and their period allows to define the chunk. On a period, the length function is non-decreasing, and the frequency function is non-increasing. These concepts continue in Zipf&apos;s direction : minimizing the communication effort drives the speaker to shorten frequent words (Zipf, 1949). The length metrics defined by Zipf is not the number of letters, but the number of syllables or the number of phonemes of the written form (Zipf, 1935); the metrics of our method is also the number of syllables, or more precisely the number of vowel nuclei, computable from the written form; this metrics takes its root into the oral origin of the chunk. The word frequency is measured in the segmented text. This method of segmentation into chunks is based on digital properties, and is valid on languages with alphabetic script. It is endogenous, as it computes on the text to be segmented and do</context>
<context position="7389" citStr="Zipf (1949)" startWordPosition="1293" endWordPosition="1294"> [6 4 1]. For these two functions, a period corresponds to a sequence; in other words, these sequences give a way to segment; these 2 functions are synchronous : sequences of both functions (nearly) define the same periods; on a (synchronous) period, both functions are in phase opposition : on a period (which defines a chunk), the length function is non-decreasing, and the frequency function is non-increasing; the common properties of these two functions allow us to call them correlated; it is an other way to say that short words are frequent and that long words are rare. We notice, following Zipf (1949) in &amp;quot;Human Behavior and the Principle of Least-Effort&amp;quot; that writing and speech are an optimal compression; it reminds the principles of file compression in computer science : frequent data are short coded, and rare data are long coded. Let us make an observation on the Zipf law, as it is known today : this law makes a relation between frequency and rows of words sorted by decreasing frequency; if we knew only this law, we would forget length of words; but Zipf proposed to consider length and frequency together, in a correlated way, as an optimization (the Least-Effort). As we use length and fr</context>
</contexts>
<marker>Zipf, 1949</marker>
<rawString>George K. Zipf. 1949. Human Behavior and the Principle of Least-Effort. Addison-Wesley.</rawString>
</citation>
<citation valid="false">
<title>3 But a problem for this large scale multilingual method is to evaluate the results on so many languages : we need a speaker for every language. For the moment, it is done for German, English, Spanish, French and Italian.</title>
<marker></marker>
<rawString>3 But a problem for this large scale multilingual method is to evaluate the results on so many languages : we need a speaker for every language. For the moment, it is done for German, English, Spanish, French and Italian.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>