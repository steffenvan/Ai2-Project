<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000737">
<title confidence="0.633014">
TakeLab: Systems for Measuring Semantic Text Similarity
</title>
<author confidence="0.652143">
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan,
Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c
</author>
<affiliation confidence="0.9979815">
University of Zagreb
Faculty of Electrical Engineering and Computing
</affiliation>
<email confidence="0.989989">
{frane.saric, goran.glavas, mladen.karan, jan.snajder, bojana.dalbelo}@fer.hr
</email>
<sectionHeader confidence="0.996622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999255588235294">
This paper describes the two systems for
determining the semantic similarity of short
texts submitted to the SemEval 2012 Task 6.
Most of the research on semantic similarity
of textual content focuses on large documents.
However, a fair amount of information is con-
densed into short text snippets such as social
media posts, image captions, and scientific ab-
stracts. We predict the human ratings of sen-
tence similarity using a support vector regres-
sion model with multiple features measuring
word-overlap similarity and syntax similarity.
Out of 89 systems submitted, our two systems
ranked in the top 5, for the three overall eval-
uation metrics used (overall Pearson – 2nd
and 3rd, normalized Pearson – 1st and 3rd,
weighted mean – 2nd and 5th).
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999840068181818">
Natural language processing tasks such as text clas-
sification (Sebastiani, 2002), text summarization
(Lin and Hovy, 2003; Aliguliyev, 2009), informa-
tion retrieval (Park et al., 2005), and word sense dis-
ambiguation (Sch¨utze, 1998) rely on a measure of
semantic similarity of textual documents. Research
predominantly focused either on the document sim-
ilarity (Salton et al., 1975; Maguitman et al., 2005)
or the word similarity (Budanitsky and Hirst, 2006;
Agirre et al., 2009). Evaluating the similarity of
short texts such as sentences or paragraphs (Islam
and Inkpen, 2008; Mihalcea et al., 2006; Oliva et
al., 2011) received less attention from the research
community. The task of recognizing paraphrases
(Michel et al., 2011; Socher et al., 2011; Wan et
al., 2006) is sufficiently similar to reuse some of the
techniques.
This paper presents the two systems for auto-
mated measuring of semantic similarity of short
texts which we submitted to the SemEval-2012 Se-
mantic Text Similarity Task (Agirre et al., 2012). We
propose several sentence similarity measures built
upon knowledge-based and corpus-based similarity
of individual words as well as similarity of depen-
dency parses. Our two systems, simple and syn-
tax, use supervised machine learning, more specif-
ically the support vector regression (SVR), to com-
bine a large amount of features computed from pairs
of sentences. The two systems differ in the set of
features they employ.
Our systems placed in the top 5 (out of 89 sub-
mitted systems) for all three aggregate correlation
measures: 2nd (syntax) and 3rd (simple) for overall
Pearson, 1st (simple) and 3rd (syntax) for normal-
ized Pearson, and 2nd (simple) and 5th (syntax) for
weighted mean.
The rest of the paper is structured as follows. In
Section 2 we describe both knowledge-based and
corpus-based word similarity measures. In Section
3 we describe in detail the features used by our sys-
tems. In Section 4 we report the experimental results
cross-validated on the development set as well as the
official results on all test sets. Conclusions and ideas
for future work are given in Section 5.
</bodyText>
<sectionHeader confidence="0.9344" genericHeader="method">
2 Word Similarity Measures
</sectionHeader>
<bodyText confidence="0.958292">
Approaches to determining semantic similarity of
sentences commonly use measures of semantic sim-
</bodyText>
<page confidence="0.981102">
441
</page>
<note confidence="0.972741">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 441–448,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9993152">
ilarity between individual words. Our systems use
the knowledge-based and the corpus-based (i.e., dis-
tributional lexical semantics) approaches, both of
which are commonly used to measure the semantic
similarity of words.
</bodyText>
<subsectionHeader confidence="0.95912">
2.1 Knowledge-based Word Similarity
</subsectionHeader>
<bodyText confidence="0.999833055555556">
Knowledge-based word similarity approaches rely
on a semantic network of words, such as Word-
Net. Given two words, their similarity can be esti-
mated by considering their relative positions within
the knowledge base hierarchy.
All of our knowledge-based word similarity mea-
sures are based on WordNet. Some measures use
the concept of a lowest common subsumer (LCS)
of concepts c1 and c2, which represents the lowest
node in the WordNet hierarchy that is a hypernym
of both c1 and c2. We use the NLTK library (Bird,
2006) to compute the PathLen similarity (Leacock
and Chodorow, 1998) and Lin similarity (Lin, 1998)
measures. A single word often denotes several con-
cepts, depending on its context. In order to compute
the similarity score for a pair of words, we take the
maximum similarity score over all possible pairs of
concepts (i.e., WordNet synsets).
</bodyText>
<subsectionHeader confidence="0.99937">
2.2 Corpus-based Word Similarity
</subsectionHeader>
<bodyText confidence="0.999580952380952">
Distributional lexical semantics models determine
the meaning of a word through the set of all con-
texts in which the word appears. Consequently, we
can model the meaning of a word using its distribu-
tion over all contexts. In the distributional model,
deriving the semantic similarity between two words
corresponds to comparing these distributions. While
many different models of distributional semantics
exist, we employ latent semantic analysis (LSA)
(Turney and Pantel, 2010) over a large corpus to es-
timate the distributions.
For each word wi, we compute a vector xi using
the truncated singular value decomposition (SVD)
of a tf-idf weighted term-document matrix. The co-
sine similarity of vectors xi and xj estimates the
similarity of the corresponding words wi and wj.
Two different word-vector mappings were com-
puted by processing the New York Times Annotated
Corpus (NYT) (Sandhaus, 2008) and Wikipedia.
Aside from lowercasing the documents and remov-
ing punctuation, we perform no further preprocess-
</bodyText>
<tableCaption confidence="0.999837">
Table 1: Evaluation of word similarity measures
</tableCaption>
<table confidence="0.9997058">
Measure ws353 ws353-sim ws353-rel
PathLen 0.29 0.61 -0.05
Lin 0.33 0.64 -0.01
Dist (NYT) 0.50 0.50 0.51
Dist (Wikipedia) 0.62 0.66 0.55
</table>
<bodyText confidence="0.9849093">
ing (e.g., no stopwords removal or stemming). Upon
removing the words not occurring in at least two
documents, we compute the tf-idf. The word vec-
tors extracted from NYT corpus and Wikipedia have
a dimension of 200 and 500, respectively.
We compared the measures by computing the
Spearman correlation coefficient on the Word-
Sim3531 data set, as well as its similarity and re-
latedness subsets described in (Agirre et al., 2009).
Table 1 provides the results of the comparison.
</bodyText>
<sectionHeader confidence="0.957106" genericHeader="method">
3 Semantic Similarity of Sentences
</sectionHeader>
<bodyText confidence="0.999809">
Our systems use supervised regression with SVR as
a learning model, where each system exploits differ-
ent feature sets and SVR hyperparameters.
</bodyText>
<subsectionHeader confidence="0.998987">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.99961975">
We list all of the preprocessing steps our systems
perform. If a preprocessing step is executed by only
one of our systems, the system’s name is indicated
in parentheses.
</bodyText>
<listItem confidence="0.876781">
1. All hyphens and slashes are removed;
2. The angular brackets (&lt; and &gt;) that enclose the
tokens are stripped (simple);
3. The currency values are simplified, e.g.,
$US1234 to $1234 (simple);
4. Words are tokenized using the Penn Treebank
compatible tokenizer;
5. The tokens n’t and ’m are replaced with not and
am, respectively (simple);
6. The two consecutive words in one sentence that
appear as a compound in the other sentence are
replaced by the said compound. E.g., cater pil-
lar in one sentence is replaced with caterpil-
lar only if caterpillar appears in the other sen-
tence;
</listItem>
<footnote confidence="0.9936745">
1http://www.cs.technion.ac.il/˜gabr/
resources/data/wordsim353/wordsim353.html
</footnote>
<page confidence="0.994703">
442
</page>
<listItem confidence="0.9433002">
7. Words are POS-tagged using Penn Treebank
compatible POS-taggers: NLTK (Bird, 2006)
for simple, and OpenNLP2 for syntax;
8. Stopwords are removed using a list of 36 stop-
words (simple).
</listItem>
<bodyText confidence="0.999940666666667">
While we acknowledge that some of the prepro-
cessing steps we take may not be common, we did
not have the time to determine the influence of each
individual preprocessing step on the results to either
warrant their removal or justify their presence.
Since, for example, sub-par, sub par and subpar
are treated as equal after preprocessing, we believe it
makes our systems more robust to inputs containing
small orthographic differences.
</bodyText>
<subsectionHeader confidence="0.9983">
3.2 Ngram Overlap Features
</subsectionHeader>
<bodyText confidence="0.999970142857143">
We use many features previously seen in paraphrase
classification (Michel et al., 2011). Several features
are based on the unigram, bigram, and trigram over-
lap. Before computing the overlap scores, we re-
move punctuation and lowercase the words. We con-
tinue with a detailed description of each individual
feature.
</bodyText>
<subsectionHeader confidence="0.947364">
Ngram Overlap
</subsectionHeader>
<bodyText confidence="0.9995485">
Let S1 and S2 be the sets of consecutive ngrams
(e.g., bigrams) in the first and the second sentence,
respectively. The ngram overlap is defined as fol-
lows:
</bodyText>
<equation confidence="0.99571675">
( |S1 |�
|S2|
|S1 ∩ S2 |+ |S1 ∩ S2|
(1)
</equation>
<bodyText confidence="0.999600769230769">
The ngram overlap is the harmonic mean of the de-
gree to which the second sentence covers the first
and the degree to which the first sentence covers the
second. The overlap, defined by (1), is computed for
unigrams, bigrams, and trigrams.
Additionally we observe the content ngram over-
lap – the overlap of unigrams, bigrams, and tri-
grams exclusively on the content words. The con-
tent words are nouns, verbs, adjectives, and adverbs,
i.e., the lemmas having one of the following part-of-
speech tags: JJ, JJR, JJS, NN, NNP, NNS, NNPS,
RB, RBR, RBS, VB, VBD, VBG, VBN, VBP, and
VBZ. Intuitively, the function words (prepositions,
</bodyText>
<footnote confidence="0.747517">
2http://opennlp.apache.org/
</footnote>
<bodyText confidence="0.999855454545455">
conjunctions, articles) carry less semantics than con-
tent words and thus removing them might eliminate
the noise and provide a more accurate estimate of
semantic similarity.
In addition to the overlap of consecutive ngrams,
we also compute the skip bigram and trigram over-
lap. Skip-ngrams are ngrams that allow arbitrary
gaps, i.e., ngram words need not be consecutive in
the original sentence. By redefining S1 and S2 to
represent the sets of skip ngrams, we employ eq. (1)
to compute the skip-n gram overlap.
</bodyText>
<subsectionHeader confidence="0.999115">
3.3 WordNet-Augmented Word Overlap
</subsectionHeader>
<bodyText confidence="0.967372857142857">
One can expect a high unigram overlap between very
similar sentences only if exactly the same words (or
lemmas) appear in both sentences. To allow for
some lexical variation, we use WordNet to assign
partial scores to words that are not common to both
sentences. We define the WordNet augmented cov-
erage PWN(·, ·):
</bodyText>
<equation confidence="0.95397225">
1 �
PWN(S1, S2) = |S2|
1 if w ∈ S
sim(w, w&apos;) otherwise
</equation>
<bodyText confidence="0.99741525">
where sim(·, ·) represents the WordNet path length
similarity. The WordNet-augmented word over-
lap feature is defined as a harmonic mean of
PWN(S1, S2) and PWN(S2, S1).
</bodyText>
<subsectionHeader confidence="0.833455">
Weighted Word Overlap
</subsectionHeader>
<bodyText confidence="0.999717">
When measuring sentence similarities we give
more importance to words bearing more content, by
using the information content
</bodyText>
<equation confidence="0.985738">
ic(w) = ln Ew.ECqe ) (w
&apos;)
</equation>
<bodyText confidence="0.999959875">
where C is the set of words in the corpus and
freq(w) is the frequency of the word w in the cor-
pus. We use the Google Books Ngrams (Michel et
al., 2011) to obtain word frequencies because of its
excellent word coverage for English. Let S1 and S2
be the sets of words occurring in the first and second
sentence, respectively. The weighted word cover-
age of the second sentence by the first sentence is
</bodyText>
<equation confidence="0.996709285714286">
−1
ngo(S1, S2) = 2 ·
score(w1, S2)
w1ES1
score(w, S) = {
max
w&apos;ES
</equation>
<page confidence="0.952019">
443
</page>
<bodyText confidence="0.982049363636364">
given by:
The weighted word overlap between two sen-
tences is calculated as the harmonic mean of the
wwc(S1, S2) and wwc(S2, S1).
This measure proved to be very useful, but it
could be improved even further. Misspelled frequent
words are more frequent than some correctly spelled
but rarely used words. Hence dealing with mis-
spelled words would remove the inappropriate heavy
penalty for a mismatch between correctly and incor-
rectly spelled words.
</bodyText>
<subsectionHeader confidence="0.574423">
Greedy Lemma Aligning Overlap
</subsectionHeader>
<bodyText confidence="0.999952642857143">
This measure computes the similarity between
sentences using the semantic alignment of lem-
mas. First we compute the word similarity be-
tween all pairs of lemmas from the first and the
second sentence, using either the knowledge-based
or the corpus-based semantic similarity. We then
greedily search for a pair of most similar lemmas;
once the lemmas are paired, they are not considered
for further matching. Previous research by Lavie
and Denkowski (2009) proposed a similar alignment
strategy for machine translation evaluation. After
aligning the sentences, the similarity of each lemma
pair is weighted by the larger information content of
the two lemmas:
</bodyText>
<equation confidence="0.72759">
sim(l1, l2) = max(ic(l1), ic(l2)) · ssim(l1, l2) (2)
</equation>
<bodyText confidence="0.999851727272727">
where ssim(l1, l2) is the semantic similarity be-
tween lemmas l1 and l2.
The overall similarity between two sentences is
defined as the sum of similarities of paired lemmas
normalized by the length of the longer sentence:
where P is the set of lemma pairs obtained by greedy
alignment. We take advantage of greedy align over-
lap in two features: one computes glao(·, ·) by us-
ing the Lin similarity for ssim(·, ·) in (2), while the
other feature uses the distributional (LSA) similarity
to calculate ssim(·, ·).
</bodyText>
<subsectionHeader confidence="0.749119">
Vector Space Sentence Similarity
</subsectionHeader>
<bodyText confidence="0.998108461538462">
This measure is motivated by the idea of composi-
tionality of distributional vectors (Mitchell and La-
pata, 2008). We represent each sentence as a sin-
gle distributional vector u(·) by summing the dis-
tributional (i.e., LSA) vector of each word w in the
sentence S: u(S) = EwES xw, where xw is the
vector representation of the word w. Another sim-
ilar representation uW (·) uses the information con-
tent ic(w) to weigh the LSA vector of each word
before summation: uW(S) = EwES ic(w)xw.
The simple system uses |cos(u(S1),u(S2)) |and
|cos(uW(S1),uW(S2)) |for the vector space sen-
tence similarity features.
</bodyText>
<subsectionHeader confidence="0.940146">
3.4 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.9999645">
We use dependency parsing to identify the lemmas
with the corresponding syntactic roles in the two
sentences. We also compute the overlap of the de-
pendency relations of the two sentences.
</bodyText>
<subsectionHeader confidence="0.93098">
Syntactic Roles Similarity
</subsectionHeader>
<bodyText confidence="0.999925454545455">
The similarity of the words or phrases having the
same syntactic roles in the two sentences may be in-
dicative of their overall semantic similarity (Oliva et
al., 2011). For example, two sentences with very dif-
ferent main predicates (e.g., play and eat) probably
have a significant semantic difference.
Using Lin similarity ssim(·, ·), we obtain the sim-
ilarity between the matching lemmas in a sentence
pair for each syntactic role. Additionally, for each
role we compute the similarity of the chunks that
lemmas belong to:
</bodyText>
<equation confidence="0.999198">
�chunksim(C1, C2) = � ssim(l1, l2)
l1EC1 l2EC2
</equation>
<bodyText confidence="0.99811675">
where C1 and C2 are the sets of chunks of
the first and second sentence, respectively. The
final similarity score of two chunks is the
harmonic mean of chunksim(C1, C2)/|C1 |and
chunksim(C1, C2)/|C2|.
Syntactic roles that we consider are predicates (p),
subjects (s), direct (d), and indirect (i) (i.e., preposi-
tional) objects, where we use (o) to mean either (d)
or (i). The Stanford dependency parser (De Marn-
effe et al., 2006) produces the dependency parse of
the sentence. We infer (p), (s), and (d) from the syn-
tactic dependencies of type nsubj (nominal subject),
</bodyText>
<equation confidence="0.999786571428571">
wwc(S1, S2) = Ew�ES2 ( )
ic w�
E
wES1nS2 ic(w)
glao(S1,S2) = max(length(S1), length(S2))
E
(l1,l2)EP sim(l1, l2)
</equation>
<page confidence="0.987137">
444
</page>
<bodyText confidence="0.999958516129032">
nsubjpass (nominal subject passive), and dobj (di-
rect object). By combining the prep and pobj de-
pendencies (De Marneffe and Manning, 2008), we
identify (i). Since the (d) in one sentence often se-
mantically corresponds to (i) in the other sentence,
we pair all (o) of one sentence with all (o) of the
other sentence and define object similarity between
the two sentences as the maximum similarity among
all (o) pairs. Because the syntactic role might be
absent from both sentences (e.g., the object in sen-
tences “John sings” and “John is thinking”), we in-
troduce additional binary features indicating if the
comparison for the syntactic role in question exists.
Many sentences (especially longer ones) have two
or more (p). In such cases it is necessary to align
the corresponding predicate groups (i.e., the (p) with
its corresponding arguments) between the two sen-
tences, while also aggregating the (p), (s), and (o)
similarities of all aligned (p) pairs. The similarity
of two predicate groups is defined as the sum of (p),
(s), and (o) similarities. In each iteration, the greedy
algorithm pairs all predicate groups of the first sen-
tence with all predicate groups of the second sen-
tence and searches for a pair with the maximum sim-
ilarity. Once the predicate groups of two sentences
have been aligned, we compute the (p) similarity as
a weighted sum of (p) similarities for each predicate
pair group. The weight of each predicate group pair
equals the larger information content of two predi-
cates. The (s) and (o) similarities are computed in
the same manner.
</bodyText>
<subsectionHeader confidence="0.812715">
Syntactic Dependencies Overlap
</subsectionHeader>
<bodyText confidence="0.999633888888889">
Similar to the ngram overlap features, we measure
the overlap between sentences based on matching
dependency relations. A similar measure has been
proposed in (Wan et al., 2006). Two syntactic depen-
dencies are considered equal if they have the same
dependency type, governing lemma, and dependent
lemma. Let S1 and S2 be the set of all dependency
relations in the first and the second sentence, respec-
tively. Dependency overlap is the harmonic mean
between |S1 n S2|/|S1 |and |S1 n S2|/|S2|. Con-
tent dependency overlap computes the overlap in the
same way, but considers only dependency relations
between content lemmas.
Similarly to weighted word overlap, we com-
pute the weighted dependency relations overlap.
The weighted coverage of the second sentence de-
pendencies with the first sentence dependencies is
given by:
</bodyText>
<equation confidence="0.93114">
wdrc(S1, S2) = ErES2 max(ic(g(r)), ic(d(r)))
</equation>
<bodyText confidence="0.999365833333333">
where g(r) is the governing word of the dependency
relation r, d(r) is the dependent word of the depen-
dency relation r, and ic(l) is the information con-
tent of the lemma l. Finally, the weighted depen-
dency relations overlap is the harmonic mean be-
tween wdrc(S1, S2) and wdrc(S2, S1).
</bodyText>
<subsectionHeader confidence="0.82915">
3.5 Other Features
</subsectionHeader>
<bodyText confidence="0.9999785">
Although we primarily focused on developing the
ngram overlap and syntax-based features, some
other features significantly improve the performance
of our systems.
</bodyText>
<subsectionHeader confidence="0.872701">
Normalized Differences
</subsectionHeader>
<bodyText confidence="0.999850333333333">
Our systems take advantage of the following fea-
tures that measure normalized differences in a pair
of sentences: (A) sentence length, (B) the noun
chunk, verb chunk, and predicate counts, and (C)
the aggregate word information content (see Nor-
malized differences in Table 2).
</bodyText>
<subsectionHeader confidence="0.831354">
Numbers Overlap
</subsectionHeader>
<bodyText confidence="0.999930230769231">
The annotators gave low similarity scores to many
sentence pairs that contained different sets of num-
bers, even though their sentence structure was very
similar. Socher et al. (2011) improved the perfor-
mance of their paraphrase classifier by adding the
following features that compare the sets of num-
bers N1 and N2 in two sentences: N1 = N2,
N1 nN2 =� 0, and N1 C_ N2 VN2 C_ N1. We replace
the first two features with log (1 + |N1 |+ |N2|) and
2· |N1 n N2|/(|N1 |+ |N2|). Additionally, the num-
bers that differ only in the number of decimal places
are treated as equal (e.g., 65, 65.2, and 65.234 are
treated as equal, whereas 65.24 and 65.25 are not).
</bodyText>
<subsectionHeader confidence="0.849163">
Named Entity Features
</subsectionHeader>
<bodyText confidence="0.999572">
Shallow NE similarity treats capitalized words as
named entities if they are longer than one character.
If a token in all caps begins with a period, it is clas-
sified as a stock index symbol. The simple system
</bodyText>
<equation confidence="0.972167">
E
rES1nS2 max(ic(g(r)), ic(d(r)))
</equation>
<page confidence="0.999168">
445
</page>
<tableCaption confidence="0.999607">
Table 2: The usage of feature sets
</tableCaption>
<table confidence="0.978931933333334">
Feature set simple syntax
Ngram overlap + +
Content-ngram overlap - +
Skip-ngram overlap - +
WordNet-aug. overlap + -
Weighted word overlap + +
Greedy align. overlap - +
Vector space similarity + -
Syntactic roles similarity - +
Syntactic dep. overlap - +
Normalized differences* A,C A,B
Shallow NERC + -
Full NERC - +
Numbers overlap + +
* See Section 3.5
</table>
<bodyText confidence="0.979653842105263">
uses the following four features: the overlap of cap-
italized words, the overlap of stock index symbols,
and the two features indicating whether these named
entities were found in either of the two sentences.
In addition to the overlap of capitalized words, the
syntax system uses the OpenNLP named entity rec-
ognizer and classifier to compute the overlap of en-
tities for each entity class separately. We recognize
the following entity classes: persons, organizations,
locations, dates, and rudimentary temporal expres-
sions. The absence of an entity class from both sen-
tences is indicated by a separate binary feature (one
feature for each class).
Feature Usage in TakeLab Systems
Some of the features presented in the previous sec-
tions were used by both of our systems (simple and
syntax), while others were used by only one of the
systems. Table 2 indicates the feature sets used for
the two submitted systems.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.999931">
4.1 Model Training
</subsectionHeader>
<bodyText confidence="0.9986486">
For each of the provided training sets we trained a
separate Support Vector Regression (SVR) model
using LIBSVM (Chang and Lin, 2011). To ob-
tain the optimal SVR parameters C, g, and p, our
systems employ a grid search with nested cross-
</bodyText>
<tableCaption confidence="0.995024">
Table 3: Cross-validated results on train sets
</tableCaption>
<bodyText confidence="0.972771590909091">
MSRvid MSRpar SMTeuroparl
simple 0.8794 0.7566 0.7802
syntax 0.8698 0.7144 0.7308
validation. Table 3 presents the cross-validated per-
formance (in terms of Pearson correlation) on the
training sets. The models tested on the SMTnews
test set were trained on the SMTeuroparl train set.
For the OnWn test set, the syntax model was trained
on the MSRpar set, while the simple system’s model
was trained on the union of all train sets. The final
predictions were trimmed to a 0–5 range.
Our development results indicate that the
weighted word overlap, WordNet-augmented word
overlap, the greedy lemma alignment overlap, and
the vector space sentence similarity individually
obtain high correlations regardless of the devel-
opment set in use. Other features proved to be
useful on individual development sets (e.g., syntax
roles similarity on MSRvid and numbers overlap
on MSRpar). More research remains to be done in
thorough feature analysis and systematic feature
selection.
</bodyText>
<subsectionHeader confidence="0.999699">
4.2 Test Set Results
</subsectionHeader>
<bodyText confidence="0.999842157894737">
The organizers provided five different test sets to
evaluate the performance of the submitted systems.
Table 4 illustrates the performance of our systems
on individual test sets, accompanied by their rank.
Our systems outperformed most other systems on
MSRvid, MSRpar, and OnWN sets (Agirre et al.,
2012). However, they performed poorly on the
SMTeuroparl and SMTnews sets. While the corre-
lation scores on the MSRvid and MSRpar test sets
correspond to those obtained using cross-validation
on the corresponding train sets, the performance on
the SMT test sets is drastically lower than the cross-
validated performance on the corresponding train
set. The sentences in the SMT training set are signif-
icantly longer (30.4 tokens on average) than the sen-
tences in both SMT test sets (12.3 for SMTeuroparl
and 13.5 for SMTnews). Also there are several re-
peated pairs of extremely short and identical sen-
tences (e.g., “Tunisia” – “Tunisia” appears 17 times
</bodyText>
<page confidence="0.999325">
446
</page>
<tableCaption confidence="0.999647">
Table 4: Results on individual test sets
</tableCaption>
<table confidence="0.999373833333333">
simple syntax
MSRvid 0.8803 (1) 0.8620 (8)
MSRpar 0.7343 (1) 0.6985 (2)
SMTeuroparl 0.4771 (26) 0.3612 (63)
SMTnews 0.3989 (46) 0.4683 (18)
OnWN 0.6797 (9) 0.7049 (6)
</table>
<tableCaption confidence="0.997095">
Table 5: Aggregate performance on the test sets
</tableCaption>
<table confidence="0.722585333333333">
All ALLnrm Mean
simple 0.8133 (3) 0.8635 (1) 0.6753 (2)
syntax 0.8138 (2) 0.8569 (3) 0.6601 (5)
</table>
<bodyText confidence="0.999518916666667">
in the SMTeuroparl test set). The above measure-
ments indicate that the SMTeuroparl training set was
not representative of the SMTeuroparl test set for our
choice of features.
Table 5 outlines the aggregate performance of our
systems according to the three aggregate evaluation
measures proposed for the task (Agirre et al., 2012).
Both systems performed very favourably compared
to the other systems, achieving very high rankings
regardless of the aggregate evaluation measure.
The implementation of simple system is available
athttp://takelab.fer.hr/sts.
</bodyText>
<sectionHeader confidence="0.992871" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.994802666666667">
In this paper we described our submission to the
SemEval-2012 Semantic Textual Similarity Task.
We have identified some high performing features
for measuring semantic text similarity. Although
both of the submitted systems performed very well
on all but the two SMT test sets, there is still room
for improvement. The feature selection was ad-hoc
and more systematic feature selection is required
(e.g., wrapper feature selection). Introducing ad-
ditional features for deeper understanding (e.g., se-
mantic role labelling) might also improve perfor-
mance on this task.
</bodyText>
<sectionHeader confidence="0.998029" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995628">
This work was supported by the Ministry of Science,
Education and Sports, Republic of Croatia under the
Grant 036-1300646-1986. We would like to thank
the organizers for the tremendous effort they put into
formulating this challenging task.
</bodyText>
<sectionHeader confidence="0.982746" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998781666666667">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 19–27. As-
sociation for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012). ACL.
Ramiz M. Aliguliyev. 2009. A new sentence similarity
measure and sentence based extractive technique for
automatic text summarization. Expert Systems with
Applications, 36(4):7764–7772.
Steven Bird. 2006. NLTK: the natural language toolkit.
In Proceedings of the COLING/ACL on Interactive
presentation sessions, COLING-ACL ’06, pages 69–
72. Association for Computational Linguistics.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13–47.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1–
27:27. Software available at http://www.csie.
ntu.edu.tw/˜cjlin/libsvm.
Marie-Catherine De Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1–8. Association for Computational
Linguistics.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, volume 6, pages 449–454.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
</reference>
<page confidence="0.982515">
447
</page>
<reference confidence="0.999867623188406">
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine translation, 23(2):105–115.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 71–78. Association for
Computational Linguistics.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th inter-
national conference on Machine Learning, volume 1,
pages 296–304. San Francisco.
Ana G. Maguitman, Filippo Menczer, Heather Roinestad,
and Alessandro Vespignani. 2005. Algorithmic detec-
tion of semantic similarity. In Proceedings of the 14th
international conference on World Wide Web, pages
107–116. ACM.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,
Adrian Veres, Matthew K. Gray, et al. 2011. Quan-
titative analysis of culture using millions of digitized
books. Science, 331(6014):176.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
page 775. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. Proceedings ofACL-
08: HLT, pages 236–244.
Jes´us Oliva, J´ose Ignacio Serrano, Mar´ıa Dolores
Del Castillo, and ´Angel Iglesias. 2011. SyMSS: A
syntax-based measure for short-text semantic similar-
ity. Data &amp; Knowledge Engineering.
Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang. 2005.
Techniques for improving web retrieval effectiveness.
Information processing &amp; management, 41(5):1207–
1223.
Gerard Salton, Andrew Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613–620.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6(12):e26752.
Hinrich Sch¨utze. 1998. Automatic word sense discrimi-
nation. Computational linguistics, 24(1):97–123.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM computing surveys
(CSUR), 34(1):1–47.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. Advances in Neural Infor-
mation Processing Systems, 24.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal ofArtificial Intelligence Research, 37(1):141–
188.
Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris.
2006. Using dependency-based features to take the
“para-farce” out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop, volume
2006.
</reference>
<page confidence="0.997718">
448
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.395290">
<title confidence="0.999215">TakeLab: Systems for Measuring Semantic Text Similarity</title>
<author confidence="0.787812">Goran Glavaˇs</author>
<author confidence="0.787812">Mladen</author>
<author confidence="0.787812">Bojana Dalbelo</author>
<affiliation confidence="0.9749335">University of Faculty of Electrical Engineering and</affiliation>
<email confidence="0.826408">goran.glavas,mladen.karan,jan.snajder,</email>
<abstract confidence="0.988628388888889">This paper describes the two systems for determining the semantic similarity of short texts submitted to the SemEval 2012 Task 6. Most of the research on semantic similarity of textual content focuses on large documents. However, a fair amount of information is condensed into short text snippets such as social media posts, image captions, and scientific abstracts. We predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity. Out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evalmetrics used Pearson 2nd 3rd, Pearson 1st and 3rd, mean 2nd and 5th).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 Task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="2074" citStr="Agirre et al., 2012" startWordPosition="314" endWordPosition="317">l., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses. Our two systems, simple and syntax, use supervised machine learning, more specifically the support vector regression (SVR), to combine a large amount of features computed from pairs of sentences. The two systems differ in the set of features they employ. Our systems placed in the top 5 (out of 89 submitted systems) for all three aggregate correlation measures: 2nd (syntax) and 3rd (simple) for overall Pearson, 1st (simple) and </context>
<context position="21759" citStr="Agirre et al., 2012" startWordPosition="3541" endWordPosition="3544">high correlations regardless of the development set in use. Other features proved to be useful on individual development sets (e.g., syntax roles similarity on MSRvid and numbers overlap on MSRpar). More research remains to be done in thorough feature analysis and systematic feature selection. 4.2 Test Set Results The organizers provided five different test sets to evaluate the performance of the submitted systems. Table 4 illustrates the performance of our systems on individual test sets, accompanied by their rank. Our systems outperformed most other systems on MSRvid, MSRpar, and OnWN sets (Agirre et al., 2012). However, they performed poorly on the SMTeuroparl and SMTnews sets. While the correlation scores on the MSRvid and MSRpar test sets correspond to those obtained using cross-validation on the corresponding train sets, the performance on the SMT test sets is drastically lower than the crossvalidated performance on the corresponding train set. The sentences in the SMT training set are significantly longer (30.4 tokens on average) than the sentences in both SMT test sets (12.3 for SMTeuroparl and 13.5 for SMTnews). Also there are several repeated pairs of extremely short and identical sentences </context>
<context position="23090" citStr="Agirre et al., 2012" startWordPosition="3758" endWordPosition="3761"> 0.8803 (1) 0.8620 (8) MSRpar 0.7343 (1) 0.6985 (2) SMTeuroparl 0.4771 (26) 0.3612 (63) SMTnews 0.3989 (46) 0.4683 (18) OnWN 0.6797 (9) 0.7049 (6) Table 5: Aggregate performance on the test sets All ALLnrm Mean simple 0.8133 (3) 0.8635 (1) 0.6753 (2) syntax 0.8138 (2) 0.8569 (3) 0.6601 (5) in the SMTeuroparl test set). The above measurements indicate that the SMTeuroparl training set was not representative of the SMTeuroparl test set for our choice of features. Table 5 outlines the aggregate performance of our systems according to the three aggregate evaluation measures proposed for the task (Agirre et al., 2012). Both systems performed very favourably compared to the other systems, achieving very high rankings regardless of the aggregate evaluation measure. The implementation of simple system is available athttp://takelab.fer.hr/sts. 5 Conclusion and Future Work In this paper we described our submission to the SemEval-2012 Semantic Textual Similarity Task. We have identified some high performing features for measuring semantic text similarity. Although both of the submitted systems performed very well on all but the two SMT test sets, there is still room for improvement. The feature selection was ad-</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A pilot on semantic textual similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012). ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramiz M Aliguliyev</author>
</authors>
<title>A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Systems with Applications,</title>
<date>2009</date>
<pages>36--4</pages>
<contexts>
<context position="1198" citStr="Aliguliyev, 2009" startWordPosition="176" endWordPosition="177">ext snippets such as social media posts, image captions, and scientific abstracts. We predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity. Out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall Pearson – 2nd and 3rd, normalized Pearson – 1st and 3rd, weighted mean – 2nd and 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher e</context>
</contexts>
<marker>Aliguliyev, 2009</marker>
<rawString>Ramiz M. Aliguliyev. 2009. A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Systems with Applications, 36(4):7764–7772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
</authors>
<title>NLTK: the natural language toolkit.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive presentation sessions, COLING-ACL ’06,</booktitle>
<pages>69--72</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="4240" citStr="Bird, 2006" startWordPosition="657" endWordPosition="658">mmonly used to measure the semantic similarity of words. 2.1 Knowledge-based Word Similarity Knowledge-based word similarity approaches rely on a semantic network of words, such as WordNet. Given two words, their similarity can be estimated by considering their relative positions within the knowledge base hierarchy. All of our knowledge-based word similarity measures are based on WordNet. Some measures use the concept of a lowest common subsumer (LCS) of concepts c1 and c2, which represents the lowest node in the WordNet hierarchy that is a hypernym of both c1 and c2. We use the NLTK library (Bird, 2006) to compute the PathLen similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998) measures. A single word often denotes several concepts, depending on its context. In order to compute the similarity score for a pair of words, we take the maximum similarity score over all possible pairs of concepts (i.e., WordNet synsets). 2.2 Corpus-based Word Similarity Distributional lexical semantics models determine the meaning of a word through the set of all contexts in which the word appears. Consequently, we can model the meaning of a word using its distribution over all contexts. In the d</context>
<context position="7394" citStr="Bird, 2006" startWordPosition="1152" endWordPosition="1153">ues are simplified, e.g., $US1234 to $1234 (simple); 4. Words are tokenized using the Penn Treebank compatible tokenizer; 5. The tokens n’t and ’m are replaced with not and am, respectively (simple); 6. The two consecutive words in one sentence that appear as a compound in the other sentence are replaced by the said compound. E.g., cater pillar in one sentence is replaced with caterpillar only if caterpillar appears in the other sentence; 1http://www.cs.technion.ac.il/˜gabr/ resources/data/wordsim353/wordsim353.html 442 7. Words are POS-tagged using Penn Treebank compatible POS-taggers: NLTK (Bird, 2006) for simple, and OpenNLP2 for syntax; 8. Stopwords are removed using a list of 36 stopwords (simple). While we acknowledge that some of the preprocessing steps we take may not be common, we did not have the time to determine the influence of each individual preprocessing step on the results to either warrant their removal or justify their presence. Since, for example, sub-par, sub par and subpar are treated as equal after preprocessing, we believe it makes our systems more robust to inputs containing small orthographic differences. 3.2 Ngram Overlap Features We use many features previously see</context>
</contexts>
<marker>Bird, 2006</marker>
<rawString>Steven Bird. 2006. NLTK: the natural language toolkit. In Proceedings of the COLING/ACL on Interactive presentation sessions, COLING-ACL ’06, pages 69– 72. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating wordnet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="1514" citStr="Budanitsky and Hirst, 2006" startWordPosition="223" endWordPosition="226">in the top 5, for the three overall evaluation metrics used (overall Pearson – 2nd and 3rd, normalized Pearson – 1st and 3rd, weighted mean – 2nd and 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<pages>27--27</pages>
<note>Software available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="20314" citStr="Chang and Lin, 2011" startWordPosition="3313" endWordPosition="3316">rganizations, locations, dates, and rudimentary temporal expressions. The absence of an entity class from both sentences is indicated by a separate binary feature (one feature for each class). Feature Usage in TakeLab Systems Some of the features presented in the previous sections were used by both of our systems (simple and syntax), while others were used by only one of the systems. Table 2 indicates the feature sets used for the two submitted systems. 4 Results 4.1 Model Training For each of the provided training sets we trained a separate Support Vector Regression (SVR) model using LIBSVM (Chang and Lin, 2011). To obtain the optimal SVR parameters C, g, and p, our systems employ a grid search with nested crossTable 3: Cross-validated results on train sets MSRvid MSRpar SMTeuroparl simple 0.8794 0.7566 0.7802 syntax 0.8698 0.7144 0.7308 validation. Table 3 presents the cross-validated performance (in terms of Pearson correlation) on the training sets. The models tested on the SMTnews test set were trained on the SMTeuroparl train set. For the OnWn test set, the syntax model was trained on the MSRpar set, while the simple system’s model was trained on the union of all train sets. The final prediction</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1– 27:27. Software available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>De Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine De Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic text similarity using corpus-based word similarity and string similarity.</title>
<date>2008</date>
<journal>ACM Transactions on Knowledge Discovery from Data (TKDD),</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="1634" citStr="Islam and Inkpen, 2008" startWordPosition="242" endWordPosition="245">, weighted mean – 2nd and 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency</context>
</contexts>
<marker>Islam, Inkpen, 2008</marker>
<rawString>Aminul Islam and Diana Inkpen. 2008. Semantic text similarity using corpus-based word similarity and string similarity. ACM Transactions on Knowledge Discovery from Data (TKDD), 2(2):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael Denkowski</author>
</authors>
<title>The METEOR metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<booktitle>Machine translation,</booktitle>
<pages>23--2</pages>
<contexts>
<context position="11796" citStr="Lavie and Denkowski (2009)" startWordPosition="1894" endWordPosition="1897">ce dealing with misspelled words would remove the inappropriate heavy penalty for a mismatch between correctly and incorrectly spelled words. Greedy Lemma Aligning Overlap This measure computes the similarity between sentences using the semantic alignment of lemmas. First we compute the word similarity between all pairs of lemmas from the first and the second sentence, using either the knowledge-based or the corpus-based semantic similarity. We then greedily search for a pair of most similar lemmas; once the lemmas are paired, they are not considered for further matching. Previous research by Lavie and Denkowski (2009) proposed a similar alignment strategy for machine translation evaluation. After aligning the sentences, the similarity of each lemma pair is weighted by the larger information content of the two lemmas: sim(l1, l2) = max(ic(l1), ic(l2)) · ssim(l1, l2) (2) where ssim(l1, l2) is the semantic similarity between lemmas l1 and l2. The overall similarity between two sentences is defined as the sum of similarities of paired lemmas normalized by the length of the longer sentence: where P is the set of lemma pairs obtained by greedy alignment. We take advantage of greedy align overlap in two features:</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael Denkowski. 2009. The METEOR metric for automatic evaluation of machine translation. Machine translation, 23(2):105–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>49--2</pages>
<contexts>
<context position="4303" citStr="Leacock and Chodorow, 1998" startWordPosition="664" endWordPosition="667">of words. 2.1 Knowledge-based Word Similarity Knowledge-based word similarity approaches rely on a semantic network of words, such as WordNet. Given two words, their similarity can be estimated by considering their relative positions within the knowledge base hierarchy. All of our knowledge-based word similarity measures are based on WordNet. Some measures use the concept of a lowest common subsumer (LCS) of concepts c1 and c2, which represents the lowest node in the WordNet hierarchy that is a hypernym of both c1 and c2. We use the NLTK library (Bird, 2006) to compute the PathLen similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998) measures. A single word often denotes several concepts, depending on its context. In order to compute the similarity score for a pair of words, we take the maximum similarity score over all possible pairs of concepts (i.e., WordNet synsets). 2.2 Corpus-based Word Similarity Distributional lexical semantics models determine the meaning of a word through the set of all contexts in which the word appears. Consequently, we can model the meaning of a word using its distribution over all contexts. In the distributional model, deriving the semantic similarity between t</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>71--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1179" citStr="Lin and Hovy, 2003" startWordPosition="172" endWordPosition="175">ndensed into short text snippets such as social media posts, image captions, and scientific abstracts. We predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity. Out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall Pearson – 2nd and 3rd, normalized Pearson – 1st and 3rd, weighted mean – 2nd and 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 71–78. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th international conference on Machine Learning,</booktitle>
<volume>1</volume>
<pages>296--304</pages>
<location>San Francisco.</location>
<contexts>
<context position="4334" citStr="Lin, 1998" startWordPosition="671" endWordPosition="672">owledge-based word similarity approaches rely on a semantic network of words, such as WordNet. Given two words, their similarity can be estimated by considering their relative positions within the knowledge base hierarchy. All of our knowledge-based word similarity measures are based on WordNet. Some measures use the concept of a lowest common subsumer (LCS) of concepts c1 and c2, which represents the lowest node in the WordNet hierarchy that is a hypernym of both c1 and c2. We use the NLTK library (Bird, 2006) to compute the PathLen similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998) measures. A single word often denotes several concepts, depending on its context. In order to compute the similarity score for a pair of words, we take the maximum similarity score over all possible pairs of concepts (i.e., WordNet synsets). 2.2 Corpus-based Word Similarity Distributional lexical semantics models determine the meaning of a word through the set of all contexts in which the word appears. Consequently, we can model the meaning of a word using its distribution over all contexts. In the distributional model, deriving the semantic similarity between two words corresponds to compari</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th international conference on Machine Learning, volume 1, pages 296–304. San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana G Maguitman</author>
<author>Filippo Menczer</author>
<author>Heather Roinestad</author>
<author>Alessandro Vespignani</author>
</authors>
<title>Algorithmic detection of semantic similarity.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th international conference on World Wide Web,</booktitle>
<pages>107--116</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1463" citStr="Maguitman et al., 2005" startWordPosition="215" endWordPosition="218">of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall Pearson – 2nd and 3rd, normalized Pearson – 1st and 3rd, weighted mean – 2nd and 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et</context>
</contexts>
<marker>Maguitman, Menczer, Roinestad, Vespignani, 2005</marker>
<rawString>Ana G. Maguitman, Filippo Menczer, Heather Roinestad, and Alessandro Vespignani. 2005. Algorithmic detection of semantic similarity. In Proceedings of the 14th international conference on World Wide Web, pages 107–116. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Baptiste Michel</author>
<author>Yuan Kui Shen</author>
<author>Aviva P Aiden</author>
<author>Adrian Veres</author>
<author>Matthew K Gray</author>
</authors>
<title>Quantitative analysis of culture using millions of digitized books.</title>
<date>2011</date>
<journal>Science,</journal>
<volume>331</volume>
<issue>6014</issue>
<contexts>
<context position="1788" citStr="Michel et al., 2011" startWordPosition="266" endWordPosition="269">Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses. Our two systems, simple and syntax, use supervised machine learning, more specifically the support vector regression (SVR), to combine a large am</context>
<context position="8046" citStr="Michel et al., 2011" startWordPosition="1256" endWordPosition="1259">ntax; 8. Stopwords are removed using a list of 36 stopwords (simple). While we acknowledge that some of the preprocessing steps we take may not be common, we did not have the time to determine the influence of each individual preprocessing step on the results to either warrant their removal or justify their presence. Since, for example, sub-par, sub par and subpar are treated as equal after preprocessing, we believe it makes our systems more robust to inputs containing small orthographic differences. 3.2 Ngram Overlap Features We use many features previously seen in paraphrase classification (Michel et al., 2011). Several features are based on the unigram, bigram, and trigram overlap. Before computing the overlap scores, we remove punctuation and lowercase the words. We continue with a detailed description of each individual feature. Ngram Overlap Let S1 and S2 be the sets of consecutive ngrams (e.g., bigrams) in the first and the second sentence, respectively. The ngram overlap is defined as follows: ( |S1 |� |S2| |S1 ∩ S2 |+ |S1 ∩ S2| (1) The ngram overlap is the harmonic mean of the degree to which the second sentence covers the first and the degree to which the first sentence covers the second. Th</context>
<context position="10547" citStr="Michel et al., 2011" startWordPosition="1686" endWordPosition="1689">to both sentences. We define the WordNet augmented coverage PWN(·, ·): 1 � PWN(S1, S2) = |S2| 1 if w ∈ S sim(w, w&apos;) otherwise where sim(·, ·) represents the WordNet path length similarity. The WordNet-augmented word overlap feature is defined as a harmonic mean of PWN(S1, S2) and PWN(S2, S1). Weighted Word Overlap When measuring sentence similarities we give more importance to words bearing more content, by using the information content ic(w) = ln Ew.ECqe ) (w &apos;) where C is the set of words in the corpus and freq(w) is the frequency of the word w in the corpus. We use the Google Books Ngrams (Michel et al., 2011) to obtain word frequencies because of its excellent word coverage for English. Let S1 and S2 be the sets of words occurring in the first and second sentence, respectively. The weighted word coverage of the second sentence by the first sentence is −1 ngo(S1, S2) = 2 · score(w1, S2) w1ES1 score(w, S) = { max w&apos;ES 443 given by: The weighted word overlap between two sentences is calculated as the harmonic mean of the wwc(S1, S2) and wwc(S2, S1). This measure proved to be very useful, but it could be improved even further. Misspelled frequent words are more frequent than some correctly spelled but</context>
</contexts>
<marker>Michel, Shen, Aiden, Veres, Gray, 2011</marker>
<rawString>Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden, Adrian Veres, Matthew K. Gray, et al. 2011. Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the national conference on artificial intelligence,</booktitle>
<volume>21</volume>
<pages>775</pages>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="1657" citStr="Mihalcea et al., 2006" startWordPosition="246" endWordPosition="249">d 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses. Our two system</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the national conference on artificial intelligence, volume 21, page 775. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>Proceedings ofACL08: HLT,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="12706" citStr="Mitchell and Lapata, 2008" startWordPosition="2042" endWordPosition="2046">emantic similarity between lemmas l1 and l2. The overall similarity between two sentences is defined as the sum of similarities of paired lemmas normalized by the length of the longer sentence: where P is the set of lemma pairs obtained by greedy alignment. We take advantage of greedy align overlap in two features: one computes glao(·, ·) by using the Lin similarity for ssim(·, ·) in (2), while the other feature uses the distributional (LSA) similarity to calculate ssim(·, ·). Vector Space Sentence Similarity This measure is motivated by the idea of compositionality of distributional vectors (Mitchell and Lapata, 2008). We represent each sentence as a single distributional vector u(·) by summing the distributional (i.e., LSA) vector of each word w in the sentence S: u(S) = EwES xw, where xw is the vector representation of the word w. Another similar representation uW (·) uses the information content ic(w) to weigh the LSA vector of each word before summation: uW(S) = EwES ic(w)xw. The simple system uses |cos(u(S1),u(S2)) |and |cos(uW(S1),uW(S2)) |for the vector space sentence similarity features. 3.4 Syntactic Features We use dependency parsing to identify the lemmas with the corresponding syntactic roles i</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. Proceedings ofACL08: HLT, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Oliva</author>
<author>J´ose Ignacio Serrano</author>
<author>Mar´ıa Dolores Del Castillo</author>
<author>´Angel Iglesias</author>
</authors>
<title>SyMSS: A syntax-based measure for short-text semantic similarity.</title>
<date>2011</date>
<journal>Data &amp; Knowledge Engineering.</journal>
<contexts>
<context position="1678" citStr="Oliva et al., 2011" startWordPosition="250" endWordPosition="253">Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses. Our two systems, simple and syntax,</context>
<context position="13599" citStr="Oliva et al., 2011" startWordPosition="2191" endWordPosition="2194">tion content ic(w) to weigh the LSA vector of each word before summation: uW(S) = EwES ic(w)xw. The simple system uses |cos(u(S1),u(S2)) |and |cos(uW(S1),uW(S2)) |for the vector space sentence similarity features. 3.4 Syntactic Features We use dependency parsing to identify the lemmas with the corresponding syntactic roles in the two sentences. We also compute the overlap of the dependency relations of the two sentences. Syntactic Roles Similarity The similarity of the words or phrases having the same syntactic roles in the two sentences may be indicative of their overall semantic similarity (Oliva et al., 2011). For example, two sentences with very different main predicates (e.g., play and eat) probably have a significant semantic difference. Using Lin similarity ssim(·, ·), we obtain the similarity between the matching lemmas in a sentence pair for each syntactic role. Additionally, for each role we compute the similarity of the chunks that lemmas belong to: �chunksim(C1, C2) = � ssim(l1, l2) l1EC1 l2EC2 where C1 and C2 are the sets of chunks of the first and second sentence, respectively. The final similarity score of two chunks is the harmonic mean of chunksim(C1, C2)/|C1 |and chunksim(C1, C2)/|C</context>
</contexts>
<marker>Oliva, Serrano, Castillo, Iglesias, 2011</marker>
<rawString>Jes´us Oliva, J´ose Ignacio Serrano, Mar´ıa Dolores Del Castillo, and ´Angel Iglesias. 2011. SyMSS: A syntax-based measure for short-text semantic similarity. Data &amp; Knowledge Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eui-Kyu Park</author>
<author>Dong-Yul Ra</author>
<author>Myung-Gil Jang</author>
</authors>
<title>Techniques for improving web retrieval effectiveness.</title>
<date>2005</date>
<journal>Information processing &amp; management,</journal>
<volume>41</volume>
<issue>5</issue>
<pages>1223</pages>
<contexts>
<context position="1241" citStr="Park et al., 2005" startWordPosition="181" endWordPosition="184">mage captions, and scientific abstracts. We predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity. Out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall Pearson – 2nd and 3rd, normalized Pearson – 1st and 3rd, weighted mean – 2nd and 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficien</context>
</contexts>
<marker>Park, Ra, Jang, 2005</marker>
<rawString>Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang. 2005. Techniques for improving web retrieval effectiveness. Information processing &amp; management, 41(5):1207– 1223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Andrew Wong</author>
<author>Chung-Shu Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="1438" citStr="Salton et al., 1975" startWordPosition="211" endWordPosition="214">ntax similarity. Out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall Pearson – 2nd and 3rd, normalized Pearson – 1st and 3rd, weighted mean – 2nd and 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text S</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Gerard Salton, Andrew Wong, and Chung-Shu Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The New York Times annotated corpus. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Philadelphia, 6(12):e26752.</location>
<contexts>
<context position="5505" citStr="Sandhaus, 2008" startWordPosition="855" endWordPosition="856">ity between two words corresponds to comparing these distributions. While many different models of distributional semantics exist, we employ latent semantic analysis (LSA) (Turney and Pantel, 2010) over a large corpus to estimate the distributions. For each word wi, we compute a vector xi using the truncated singular value decomposition (SVD) of a tf-idf weighted term-document matrix. The cosine similarity of vectors xi and xj estimates the similarity of the corresponding words wi and wj. Two different word-vector mappings were computed by processing the New York Times Annotated Corpus (NYT) (Sandhaus, 2008) and Wikipedia. Aside from lowercasing the documents and removing punctuation, we perform no further preprocessTable 1: Evaluation of word similarity measures Measure ws353 ws353-sim ws353-rel PathLen 0.29 0.61 -0.05 Lin 0.33 0.64 -0.01 Dist (NYT) 0.50 0.50 0.51 Dist (Wikipedia) 0.62 0.66 0.55 ing (e.g., no stopwords removal or stemming). Upon removing the words not occurring in at least two documents, we compute the tf-idf. The word vectors extracted from NYT corpus and Wikipedia have a dimension of 200 and 500, respectively. We compared the measures by computing the Spearman correlation coef</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The New York Times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12):e26752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational linguistics,</journal>
<pages>24--1</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM computing surveys (CSUR),</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1139" citStr="Sebastiani, 2002" startWordPosition="168" endWordPosition="169">ver, a fair amount of information is condensed into short text snippets such as social media posts, image captions, and scientific abstracts. We predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity. Out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall Pearson – 2nd and 3rd, normalized Pearson – 1st and 3rd, weighted mean – 2nd and 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The tas</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>24</pages>
<contexts>
<context position="1809" citStr="Socher et al., 2011" startWordPosition="270" endWordPosition="273">v, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses. Our two systems, simple and syntax, use supervised machine learning, more specifically the support vector regression (SVR), to combine a large amount of features comp</context>
<context position="18110" citStr="Socher et al. (2011)" startWordPosition="2926" endWordPosition="2929">ing the ngram overlap and syntax-based features, some other features significantly improve the performance of our systems. Normalized Differences Our systems take advantage of the following features that measure normalized differences in a pair of sentences: (A) sentence length, (B) the noun chunk, verb chunk, and predicate counts, and (C) the aggregate word information content (see Normalized differences in Table 2). Numbers Overlap The annotators gave low similarity scores to many sentence pairs that contained different sets of numbers, even though their sentence structure was very similar. Socher et al. (2011) improved the performance of their paraphrase classifier by adding the following features that compare the sets of numbers N1 and N2 in two sentences: N1 = N2, N1 nN2 =� 0, and N1 C_ N2 VN2 C_ N1. We replace the first two features with log (1 + |N1 |+ |N2|) and 2· |N1 n N2|/(|N1 |+ |N2|). Additionally, the numbers that differ only in the number of decimal places are treated as equal (e.g., 65, 65.2, and 65.234 are treated as equal, whereas 65.24 and 65.25 are not). Named Entity Features Shallow NE similarity treats capitalized words as named entities if they are longer than one character. If a</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. Advances in Neural Information Processing Systems, 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>188</pages>
<contexts>
<context position="5087" citStr="Turney and Pantel, 2010" startWordPosition="785" endWordPosition="788"> pair of words, we take the maximum similarity score over all possible pairs of concepts (i.e., WordNet synsets). 2.2 Corpus-based Word Similarity Distributional lexical semantics models determine the meaning of a word through the set of all contexts in which the word appears. Consequently, we can model the meaning of a word using its distribution over all contexts. In the distributional model, deriving the semantic similarity between two words corresponds to comparing these distributions. While many different models of distributional semantics exist, we employ latent semantic analysis (LSA) (Turney and Pantel, 2010) over a large corpus to estimate the distributions. For each word wi, we compute a vector xi using the truncated singular value decomposition (SVD) of a tf-idf weighted term-document matrix. The cosine similarity of vectors xi and xj estimates the similarity of the corresponding words wi and wj. Two different word-vector mappings were computed by processing the New York Times Annotated Corpus (NYT) (Sandhaus, 2008) and Wikipedia. Aside from lowercasing the documents and removing punctuation, we perform no further preprocessTable 1: Evaluation of word similarity measures Measure ws353 ws353-sim</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal ofArtificial Intelligence Research, 37(1):141– 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>C´ecile Paris</author>
</authors>
<title>Using dependency-based features to take the “para-farce” out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<volume>volume</volume>
<contexts>
<context position="1828" citStr="Wan et al., 2006" startWordPosition="274" endWordPosition="277"> retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses. Our two systems, simple and syntax, use supervised machine learning, more specifically the support vector regression (SVR), to combine a large amount of features computed from pairs of </context>
<context position="16460" citStr="Wan et al., 2006" startWordPosition="2665" endWordPosition="2668">icate groups of the second sentence and searches for a pair with the maximum similarity. Once the predicate groups of two sentences have been aligned, we compute the (p) similarity as a weighted sum of (p) similarities for each predicate pair group. The weight of each predicate group pair equals the larger information content of two predicates. The (s) and (o) similarities are computed in the same manner. Syntactic Dependencies Overlap Similar to the ngram overlap features, we measure the overlap between sentences based on matching dependency relations. A similar measure has been proposed in (Wan et al., 2006). Two syntactic dependencies are considered equal if they have the same dependency type, governing lemma, and dependent lemma. Let S1 and S2 be the set of all dependency relations in the first and the second sentence, respectively. Dependency overlap is the harmonic mean between |S1 n S2|/|S1 |and |S1 n S2|/|S2|. Content dependency overlap computes the overlap in the same way, but considers only dependency relations between content lemmas. Similarly to weighted word overlap, we compute the weighted dependency relations overlap. The weighted coverage of the second sentence dependencies with the</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris. 2006. Using dependency-based features to take the “para-farce” out of paraphrase. In Proceedings of the Australasian Language Technology Workshop, volume 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>