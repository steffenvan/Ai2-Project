<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000850">
<title confidence="0.777514">
Sprinkling Topics for Weakly Supervised Text Classification
</title>
<author confidence="0.795341">
Swapnil Hingmire1,2 Sutanu Chakraborti2
</author>
<email confidence="0.683939">
swapnil.hingmire@tcs.com sutanuc@cse.iitm.ac.in
</email>
<affiliation confidence="0.985127">
1Systems Research Lab, Tata Research Development and Design Center, Pune, India
2Department of Computer Science and Engineering,
Indian Institute of Technology Madras, Chennai, India
</affiliation>
<sectionHeader confidence="0.980109" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999518125">
Supervised text classification algorithms
require a large number of documents la-
beled by humans, that involve a labor-
intensive and time consuming process.
In this paper, we propose a weakly su-
pervised algorithm in which supervision
comes in the form of labeling of Latent
Dirichlet Allocation (LDA) topics. We
then use this weak supervision to “sprin-
kle” artificial words to the training docu-
ments to identify topics in accordance with
the underlying class structure of the cor-
pus based on the higher order word asso-
ciations. We evaluate this approach to im-
prove performance of text classification on
three real world datasets.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999917193548388">
In supervised text classification learning algo-
rithms, the learner (a program) takes human la-
beled documents as input and learns a decision
function that can classify a previously unseen doc-
ument to one of the predefined classes. Usually a
large number of documents labeled by humans are
used by the learner to classify unseen documents
with adequate accuracy. Unfortunately, labeling
a large number of documents is a labor-intensive
and time consuming process.
In this paper, we propose a text classification
algorithm based on Latent Dirichlet Allocation
(LDA) (Blei et al., 2003) which does not need la-
beled documents. LDA is an unsupervised prob-
abilistic topic model and it is widely used to dis-
cover latent semantic structure of a document col-
lection by modeling words in the documents. Blei
et al. (Blei et al., 2003) used LDA topics as fea-
tures in text classification, but they use labeled
documents while learning a classifier. sLDA (Blei
and McAuliffe, 2007), DiscLDA (Lacoste-Julien
et al., 2008) and MedLDA (Zhu et al., 2009) are
few extensions of LDA which model both class
labels and words in the documents. These models
can be used for text classification, but they need
expensive labeled documents.
An approach that is less demanding in terms
of knowledge engineering is ClassifyLDA (Hing-
mire et al., 2013). In this approach, a topic model
on a given set of unlabeled training documents is
constructed using LDA, then an annotator assigns
a class label to some topics based on their most
probable words. These labeled topics are used
to create a new topic model such that in the new
model topics are better aligned to class labels. A
class label is assigned to a test document on the ba-
sis of its most prominent topics. We extend Clas-
sifyLDA algorithm by “sprinkling” topics to unla-
beled documents.
Sprinkling (Chakraborti et al., 2007) integrates
class labels of documents into Latent Semantic In-
dexing (LSI)(Deerwester et al., 1990). The ba-
sic idea involves encoding of class labels as ar-
tificial words which are “sprinkled” (appended)
to training documents. As LSI uses higher or-
der word associations (Kontostathis and Pottenger,
2006), sprinkling of artificial words gives better
and class-enriched latent semantic structure. How-
ever, Sprinkled LSI is a supervised technique and
hence it requires expensive labeled documents.
The paper revolves around the idea of labeling top-
ics (which are far fewer in number compared to
documents) as in ClassifyLDA, and using these la-
beled topic for sprinkling.
As in ClassifyLDA, we ask an annotator to as-
sign class labels to a set of topics inferred on the
unlabeled training documents. We use the labeled
topics to find probability distribution of each train-
ing document over the class labels. We create a
set of artificial words corresponding to a class la-
bel and add (or sprinkle) them to the document.
The number of such artificial terms is propor-
</bodyText>
<page confidence="0.982878">
55
</page>
<bodyText confidence="0.8704937">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 55–60,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
tional to the probability of generating the docu-
ment by the class label. We then infer a set of
topics on the sprinkled training documents. As
LDA uses higher order word associations (Lee et
al., 2010) while discovering topics, we hypothe-
size that sprinkling will improve text classification
performance of ClassifyLDA. We experimentally
verify this hypothesis on three real world datasets.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999982326086957">
Several researchers have proposed semi-
supervised text classification algorithms with
the aim of reducing the time, effort and cost
involved in labeling documents. These algorithms
can be broadly categorized into three categories
depending on how supervision is provided. In the
first category, a small set of labeled documents
and a large set of unlabeled documents is used
while learning a classifier. Semi-supervised text
classification algorithms proposed in (Nigam et
al., 2000), (Joachims, 1999), (Zhu and Ghahra-
mani, 2002) and (Blum and Mitchell, 1998) are a
few examples of this type. However, these algo-
rithms are sensitive to initial labeled documents
and hyper-parameters of the algorithm.
In the second category, supervision comes in the
form of labeled words (features). (Liu et al., 2004)
and (Druck et al., 2008) are a few examples of this
type. An important limitation of these algorithms
is coming up with a small set of words that should
be presented to the annotators for labeling. Also
a human annotator may discard or mislabel a pol-
ysemous word, which may affect the performance
of a text classifier.
The third type of semi-supervised text classifi-
cation algorithms is based on active learning. In
active learning, particular unlabeled documents or
features are selected and queried to an oracle (e.g.
human annotator).(Godbole et al., 2004), (Ragha-
van et al., 2006), (Druck et al., 2009) are a few ex-
amples of active learning based text classification
algorithms. However, these algorithms are sensi-
tive to the sampling strategy used to query docu-
ments or features.
In our approach, an annotator does not label
documents or words, rather she labels a small set
of interpretable topics which are inferred in an un-
supervised manner. These topics are very few,
when compared to the number of documents. As
the most probable words of topics are representa-
tive of the dataset, there is no need for the annota-
tor to search for the right set of features for each
class. As LDA topics are semantically more mean-
ingful than individual words and can be acquired
easily, our approach overcomes limitations of the
semi-supervised methods discussed above.
</bodyText>
<sectionHeader confidence="0.985065" genericHeader="method">
3 Background
</sectionHeader>
<subsectionHeader confidence="0.993063">
3.1 LDA
</subsectionHeader>
<bodyText confidence="0.99970525">
LDA is an unsupervised probabilistic generative
model for collections of discrete data such as text
documents. The generative process of LDA can be
described as follows:
</bodyText>
<listItem confidence="0.99667775">
1. for each topic t, draw a distribution over
words: Ot — Dirichlet(Nw)
2. for each document d E D
a. Draw a vector of topic proportions:
</listItem>
<equation confidence="0.590429">
Bd — Dirichlet(αt)
</equation>
<listItem confidence="0.7772405">
b. for each word w at position n in d
i. Draw a topic assignment:
</listItem>
<equation confidence="0.954957">
zd,n — Multinomial(Bd)
ii. Draw a word:
wd,n — Multinomial(zd,n)
</equation>
<bodyText confidence="0.992020588235294">
Where, T is the number of topics, φt is the word
probabilities for topic t, Bd is the topic probabil-
ity distribution, zd,n is topic assignment and wd,n
is word assignment for nth word position in docu-
ment d respectively. αt and Qw are topic and word
Dirichlet priors.
The key problem in LDA is posterior inference.
The posterior inference involves the inference of
the hidden topic structure given the observed doc-
uments. However, computing the exact posterior
inference is intractable. In this paper we estimate
approximate posterior inference using collapsed
Gibbs sampling (Griffiths and Steyvers, 2004).
The Gibbs sampling equation used to update the
assignment of a topic t to the word w E W at the
position n in document d, conditioned on αt, Qw
is:
</bodyText>
<equation confidence="0.998169333333333">
P(zd,n = t|zd,¬n, wd,n = w, αt, Nw) oC
E
v∈W ψv,t + Qv − 1
</equation>
<bodyText confidence="0.999975625">
where ψw,c is the count of the word w assigned
to the topic c, Ωc,d is the count of the topic c
assigned to words in the document d and W is
the vocabulary of the corpus. We use a subscript
d, -,n to denote the current token, zd,n is ignored
in the Gibbs sampling update. After performing
collapsed Gibbs sampling using equation 1, we
use word topic assignments to compute a point
</bodyText>
<equation confidence="0.997213">
ψw,t + Qw − 1
X (Ωt,d + αt − 1) (1)
</equation>
<page confidence="0.942844">
56
</page>
<bodyText confidence="0.999483333333333">
estimate of the distribution over words φw,c and
a point estimate of the posterior distribution over
topics for each document d (θd) is:
</bodyText>
<equation confidence="0.990301333333333">
φw,t =ψw,t + βw
&amp;quot;
LE N&apos;t + βv
v 1
EW
(2)
</equation>
<bodyText confidence="0.96222">
Let MD =&lt; Z, Φ, O &gt; be the hidden topic
structure, where Z is per word per document topic
assignment, Φ = 1φt} and O = 1θd}.
</bodyText>
<subsectionHeader confidence="0.999609">
3.2 Sprinkling
</subsectionHeader>
<bodyText confidence="0.999876714285714">
(Chakraborti et al., 2007) propose a simple ap-
proach called “sprinkling” to incorporate class la-
bels of documents into LSI. In sprinkling, a set of
artificial words are appended to a training docu-
ment which are specific to the class label of the
document. Consider a case of binary classification
with classes c1 and c2. If a document d belongs
to the class c1 then a set of artificial words which
represent the class c1 are appended into the doc-
ument d, otherwise a set of artificial words which
represent the class c2 are appended.
Singular Value Decomposition (SVD) is then
performed on the sprinkled training documents
and a lower rank approximation is constructed
by ignoring dimensions corresponding to lower
singular values. Then, the sprinkled terms are
removed from the lower rank approximation.
(Chakraborti et al., 2007) empirically show that
sprinkled words boost higher order word associ-
ations and projects documents with same class la-
bels close to each other in latent semantic space.
</bodyText>
<sectionHeader confidence="0.942188" genericHeader="method">
4 Topic Sprinkling in LDA
</sectionHeader>
<bodyText confidence="0.999972842105263">
In our text classification algorithm, we first infer a
set of topics on the given unlabeled document cor-
pus. We then ask a human annotator to assign one
or more class labels to the topics based on their
most probable words. We use these labeled topics
to create a new LDA model as follows. If the topic
assigned to the word w at the position n in docu-
ment d is t, then we replace it by the class label
assigned to the topic t. If more than one class la-
bels are assigned to the topic t, then we randomly
select one of the class labels assigned to the topic
t. If the annotator is unable to label a topic then
we randomly select a class label from the set of all
class labels. We then update the new LDA model
using collapsed Gibbs sampling.
We use this new model to infer the probability
distribution of each unlabeled training document
over the class labels. Let, θc,d be the probability of
generating document d by class c. We then sprin-
kle s artificial words of class label c to document
d, such that s = K * θc,d for some constant K.
We then infer a set of JCJ number of topics on
the sprinkled dataset using collapsed Gibbs sam-
pling, where C is the set of class labels of the
training documents. We modify collapsed Gibbs
sampling update in Equation 1 to carry class label
information while inferring topics. If a word in a
document is a sprinkled word then while sampling
a class label for it, we sample the class label asso-
ciated with the sprinkled word, otherwise we sam-
ple a class label for the word using Gibbs update
in Equation 1.
We name this model as Topic Sprinkled LDA
(TS-LDA). While classifying a test document, its
probability distribution over class labels is inferred
using TS-LDA model and it is classified to its most
probable class label. Algorithm for TS-LDA is
summarized in Table 1.
</bodyText>
<sectionHeader confidence="0.997626" genericHeader="evaluation">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999847363636364">
We determine the effectiveness of our algorithm
in relation to ClassifyLDA algorithm proposed in
(Hingmire et al., 2013). We evaluate and com-
pare our text classification algorithm by comput-
ing Macro averaged F1. As the inference of LDA
is approximate, we repeat all the experiments for
each dataset ten times and report average Macro-
F1. Similar to (Blei et al., 2003) we also learn
supervised SVM classifier (LDA-SVM) for each
dataset using topics as features and report average
Macro-F1.
</bodyText>
<subsectionHeader confidence="0.884122">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999772">
We use the following datasets in our experiments.
</bodyText>
<listItem confidence="0.761855">
1. 20 Newsgroups: This dataset contains
</listItem>
<bodyText confidence="0.757123166666667">
messages across twenty newsgroups. In our
experiments, we use bydate version of the
20Newsgroup dataset1. This version of the dataset
is divided into training (60%) and test (40%)
datasets. We construct classifiers on training
datasets and evaluate them on test datasets.
</bodyText>
<listItem confidence="0.947406">
2. SRAA: Simulated/Real/Aviation/Auto
UseNet data2: This dataset contains 73,218
</listItem>
<footnote confidence="0.962959">
1http://qwone.com/-jason/20Newsgroups/
2http://people.cs.umass.edu/-mccallum/
data.html
</footnote>
<equation confidence="0.9774176">
Ωt,d + αt
θt,d = r T
LΩi,d + αiJ
a-1
(3)
</equation>
<page confidence="0.984652">
57
</page>
<listItem confidence="0.906945103448276">
• Input: unlabeled document corpus-D, number of
topics-T and number of sprinkled terms-K
1. Infer T number of topics on D for LDA using col-
lapsed Gibbs sampling. Let MD be the hidden
topic structure of this model.
2. Ask an annotator to assign one or more class labels
ci E C to a topic based on its 30 most probable
words.
3. Initialization: For nth word in document d E D
if zd,n = t and the annotator has labeled topic t
with ci then, zd,n = ci
4. Update MD using collapsed Gibbs sampling up-
date in Equation 1.
5. Sprinkling: For each document d E D:
(a) Infer a probability distribution Bd over class
labels using MD using Equation 3.
(b) Let, Bc,d be probability of generating docu-
ment d by class c.
(c) Insert K *Bc,d distinct words associated with
the class c to the document d.
6. Infer |C |number of topics on the sprinkled docu-
ment corpus D using collapsed Gibbs sampling up-
date.
7. Let MD be the new hidden topic structure. Let us
call this hidden structure as TS-LDA.
8. Classification of an unlabled document d
(a) Infer Bd for document d using MD.
(b) k = argmaxi Bi,d
(c) yd = ck
</listItem>
<tableCaption confidence="0.8656155">
Table 1: Algorithm for sprinkling LDA topics for
text classification
</tableCaption>
<bodyText confidence="0.9982206">
UseNet articles from four discussion groups,
for simulated auto racing (sim auto), simulated
aviation (sim aviation), real autos (real auto), real
aviation (real aviation). Following are the three
classification tasks associated with this dataset.
</bodyText>
<listItem confidence="0.99813">
1. sim auto vs sim aviation vs real auto vs
real aviation
2. auto (sim auto + real auto) vs aviation
(sim aviation + real aviation)
3. simulated (sim auto + sim aviation) vs real
(real auto + real aviation)
</listItem>
<bodyText confidence="0.577318666666667">
We randomly split SRAA dataset such that 80%
is used as training data and remaining is used as
test data.
</bodyText>
<listItem confidence="0.9913655">
3. WebKB: The WebKB dataset3 contains 8145
web pages gathered from university computer
</listItem>
<footnote confidence="0.740387">
3http://www.cs.cmu.edu/˜webkb/
</footnote>
<bodyText confidence="0.9998419">
science departments. The task is to classify the
webpages as student, course, faculty or project.
We randomly split this dataset such that 80% is
used as training and 20% is used as test data.
We preprocess these datasets by removing
HTML tags and stop-words.
For various subsets of the 20Newsgroups and
WebKB datasets discussed above, we choose
number of topics as twice the number of classes.
For SRAA dataset we infer 8 topics on the train-
ing dataset and label these 8 topics for all the three
classification tasks. While labeling a topic, we
show its 30 most probable words to the human an-
notator.
Similar to (Griffiths and Steyvers, 2004), we set
symmetric Dirichlet word prior (βw) for each topic
to 0.01 and symmetric Dirichlet topic prior (αt)
for each document to 50/T, where T is number of
topics. We set K i.e. maximum number of words
sprinkled per class to 10.
</bodyText>
<subsectionHeader confidence="0.835635">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999934733333333">
Table 2 shows experimental results. We can ob-
serve that, TS-LDA performs better than Classi-
fyLDA in 5 of the total 9 subsets. For the comp-
religion-sci dataset TS-LDA and ClassifyLDA
have the same performance. However, Classi-
fyLDA performs better than TS-LDA for the three
classification tasks of SRAA dataset. We can also
observe that, performance of TS-LDA is close to
supervised LDA-SVM. We should note here that
in TS-LDA, the annotator only labels a few topics
and not a single document. Hence, our approach
exerts a low cognitive load on the annotator, at
the same time achieves text classification perfor-
mance close to LDA-SVM which needs labeled
documents.
</bodyText>
<subsectionHeader confidence="0.993164">
5.3 Example
</subsectionHeader>
<bodyText confidence="0.999632307692307">
Table 3 shows most prominent words of four
topics inferred on the med-space subset of the
20Newsgroup dataset. We can observe here that
most prominent words of the first topic do not rep-
resent a single class, while other topics represent
either med (medical) or space class. We can say
here that, these topics are not “coherent”.
We use these labeled topics and create a TS-
LDA model using the algorithm described in Table
1. Table 4 shows words corresponding to the top
two topics of the TS-LDA model. We can observe
here that these two topics are more coherent than
the topics in Table 3.
</bodyText>
<page confidence="0.997725">
58
</page>
<table confidence="0.999972466666667">
Dataset Text Classification (Macro-F1)
# Topics ClassifyLDA TS-LDA LDA-SVM
20Newsgroups
med-space 4 0.892 0.938 0.933
politics-religion 4 0.836 0.897 0.901
politics-sci 4 0.887 0.901 0.910
comp-religion-sci 6 0.853 0.853 0.872
politics-rec-religion-sci 8 0.842 0.858 0.862
SRAA
real auto-real aviation-sim auto- 8 0.766 0.741 0.820
sim aviation
auto-aviation 8 0.926 0.910 0.934
real-sim 8 0.918 0.902 0.923
WebKB
WebKB 8 0.627 0.672 0.730
</table>
<tableCaption confidence="0.959303">
Table 2: Experimental results of text classification on various datasets.
</tableCaption>
<table confidence="0.999818166666667">
ID Most prominent words in the Class (med
topic / space)
0 science scientific idea large theory med +
bit pat thought problem isn space
1 information health research medi- med
cal water cancer hiv aids children
institute newsletter
2 msg food doctor disease pain med
day treatment blood steve dyer
medicine symptoms
3 space nasa launch earth orbit space
moon shuttle data lunar satellite
</table>
<tableCaption confidence="0.967705">
Table 3: Topic labeling on the med-space subset of the
20Newsgroup dataset
</tableCaption>
<table confidence="0.999542875">
ID Most prominent words in the Class (med
topic / space)
0 msg medical health food disease med
years problem information doctor
pain cancer
1 space launch earth data orbit space
moon program shuttle lunar satel-
lite
</table>
<tableCaption confidence="0.996288333333333">
Table 4: Topics inferred on the med-space subset of the
20Newsgroup dataset after sprinkling labeled topics from Ta-
ble 3.
</tableCaption>
<bodyText confidence="0.999961461538462">
Hence, we can say here that, in addition to text
classification, sprinkling improves coherence of
topics.
We should note here that, in ClassifyLDA, the
annotator is able to assign a single class label to
a topic. If the annotator assigns a wrong class la-
bel to a topic representing multiple classes (e.g.
first topic in Table 3), then it may affect the perfor-
mance of the resulting classifier. However, in our
approach the annotator can assign multiple class
labels to a topic, hence our approach is more flexi-
ble for the annotator to encode her domain knowl-
edge efficiently.
</bodyText>
<sectionHeader confidence="0.99869" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999760925925926">
In this paper we propose a novel algorithm that
classifies documents based on class labels over
few topics. This reduces the need to label a large
collection of documents. We have used the idea
of sprinkling originally proposed in the context
of supervised Latent Semantic Analysis, but the
setting here is quite different. Unlike the work
in (Chakraborti et al., 2007), we do not assume
that we have class labels over the set of training
documents. Instead, to realize our goal of reduc-
ing knowledge acquisition overhead, we propose a
way of propagating knowledge of few topic labels
to the words and inducing a new topic distribu-
tion that has its topics more closely aligned to the
class labels. The results show that the approach
can yield performance comparable to entirely su-
pervised settings. In future work, we also envi-
sion the possibility of sprinkling knowledge from
background knowledge sources like Wikipedia
(Gabrilovich and Markovitch, 2007) to realize an
alignment of topics to Wikipedia concepts. We
would like to study effect of change in number of
topics on the text classification performance. We
will also explore techniques which will help an-
notators to encode their domain knowledge effi-
ciently when the topics are not well aligned to the
class labels.
</bodyText>
<sectionHeader confidence="0.998445" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9916986">
David M. Blei and Jon D. McAuliffe. 2007. Super-
vised Topic Models. In NIPS.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. The Journal of
Machine Learning Research, 3:993–1022, March.
</reference>
<page confidence="0.993742">
59
</page>
<reference confidence="0.996821485714286">
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of the eleventh annual conference on Com-
putational learning theory, pages 92–100.
Sutanu Chakraborti, Rahman Mukras, Robert Lothian,
Nirmalie Wiratunga, Stuart N. K. Watt, David J.
Harper. 2007. Supervised Latent Semantic Indexing
Using Adaptive Sprinkling. In IJCAI, pages 1582-
1587.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. JA-
SIS, 41(6):391–407.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2008. Learning from Labeled Features using
Generalized Expectation criteria. In SIGIR, pages
595–602.
Gregory Druck, Burr Settles, and Andrew McCallum.
2009. Active Learning by Labeling Features. In
EMNLP, pages 81–90.
Shantanu Godbole, Abhay Harpale, Sunita Sarawagi,
and Soumen Chakrabarti. 2004. Document Classifi-
cation through Interactive Supervision of Document
and Term Labels. In PKDD, pages 185–196.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
Scientific Topics. PNAS, 101(suppl. 1):5228–5235,
April.
Swapnil Hingmire, Sandeep Chougule, Girish K. Pal-
shikar, and Sutanu Chakraborti. 2013. Document
Classification by Topic Labeling. In SIGIR, pages
877–880.
Thorsten Joachims. 1999. Transductive Inference for
Text Classification using Support Vector Machines.
In ICML, pages 200–209.
April Kontostathis and William M. Pottenger. 2006. A
Framework for Understanding Latent Semantic In-
dexing (LSI) Performance. Inf. Process. Manage.,
42(1):56–73, January.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008. DiscLDA: Discriminative Learning for Di-
mensionality Reduction and Classification. In NIPS.
Sangno Lee, Jeff Baker, Jaeki Song, and James C.
Wetherbe. 2010. An Empirical Comparison of
Four Text Mining Methods. In Proceedings of the
2010 43rd Hawaii International Conference on Sys-
tem Sciences, pages 1–10.
Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu.
2004. Text Classification by Labeling Words. In
Proceedings of the 19th national conference on Ar-
tifical intelligence, pages 425–430.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text Classification
from Labeled and Unlabeled Documents using EM.
Machine Learning - Special issue on information re-
trieval, 39(2-3), May-June.
Hema Raghavan, Omid Madani, and Rosie Jones.
2006. Active Learning with Feedback on Features
and Instances. JMLR, 7:1655–1686, December.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from Labeled and Unlabeled Data with Label Prop-
agation. Technical report, Carnegie Mellon Univer-
sity.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009.
MedLDA: Maximum Margin Supervised Topic
Models for Regression and Classification. In ICML,
pages 1257–1264.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness Using Wikipedia-
based Explicit Semantic Analysis. In IJCAI, pages
1606–1611.
</reference>
<page confidence="0.998403">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.408095">
<title confidence="0.998974">Sprinkling Topics for Weakly Supervised Text Classification</title>
<author confidence="0.60036">swapnil hingmiretcs com sutanuccse iitm ac in</author>
<affiliation confidence="0.811521333333333">Research Lab, Tata Research Development and Design Center, Pune, of Computer Science and Indian Institute of Technology Madras, Chennai, India</affiliation>
<abstract confidence="0.999459352941177">Supervised text classification algorithms require a large number of documents labeled by humans, that involve a laborintensive and time consuming process. In this paper, we propose a weakly supervised algorithm in which supervision comes in the form of labeling of Latent Dirichlet Allocation (LDA) topics. We then use this weak supervision to “sprinkle” artificial words to the training documents to identify topics in accordance with the underlying class structure of the corpus based on the higher order word associations. We evaluate this approach to improve performance of text classification on three real world datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Jon D McAuliffe</author>
</authors>
<title>Supervised Topic Models.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1950" citStr="Blei and McAuliffe, 2007" startWordPosition="296" endWordPosition="299">adequate accuracy. Unfortunately, labeling a large number of documents is a labor-intensive and time consuming process. In this paper, we propose a text classification algorithm based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) which does not need labeled documents. LDA is an unsupervised probabilistic topic model and it is widely used to discover latent semantic structure of a document collection by modeling words in the documents. Blei et al. (Blei et al., 2003) used LDA topics as features in text classification, but they use labeled documents while learning a classifier. sLDA (Blei and McAuliffe, 2007), DiscLDA (Lacoste-Julien et al., 2008) and MedLDA (Zhu et al., 2009) are few extensions of LDA which model both class labels and words in the documents. These models can be used for text classification, but they need expensive labeled documents. An approach that is less demanding in terms of knowledge engineering is ClassifyLDA (Hingmire et al., 2013). In this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words. These labeled topics are used to create a new to</context>
</contexts>
<marker>Blei, McAuliffe, 2007</marker>
<rawString>David M. Blei and Jon D. McAuliffe. 2007. Supervised Topic Models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1565" citStr="Blei et al., 2003" startWordPosition="229" endWordPosition="232">real world datasets. 1 Introduction In supervised text classification learning algorithms, the learner (a program) takes human labeled documents as input and learns a decision function that can classify a previously unseen document to one of the predefined classes. Usually a large number of documents labeled by humans are used by the learner to classify unseen documents with adequate accuracy. Unfortunately, labeling a large number of documents is a labor-intensive and time consuming process. In this paper, we propose a text classification algorithm based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) which does not need labeled documents. LDA is an unsupervised probabilistic topic model and it is widely used to discover latent semantic structure of a document collection by modeling words in the documents. Blei et al. (Blei et al., 2003) used LDA topics as features in text classification, but they use labeled documents while learning a classifier. sLDA (Blei and McAuliffe, 2007), DiscLDA (Lacoste-Julien et al., 2008) and MedLDA (Zhu et al., 2009) are few extensions of LDA which model both class labels and words in the documents. These models can be used for text classification, but they ne</context>
<context position="11933" citStr="Blei et al., 2003" startWordPosition="2013" endWordPosition="2016">(TS-LDA). While classifying a test document, its probability distribution over class labels is inferred using TS-LDA model and it is classified to its most probable class label. Algorithm for TS-LDA is summarized in Table 1. 5 Experimental Evaluation We determine the effectiveness of our algorithm in relation to ClassifyLDA algorithm proposed in (Hingmire et al., 2013). We evaluate and compare our text classification algorithm by computing Macro averaged F1. As the inference of LDA is approximate, we repeat all the experiments for each dataset ten times and report average MacroF1. Similar to (Blei et al., 2003) we also learn supervised SVM classifier (LDA-SVM) for each dataset using topics as features and report average Macro-F1. 5.1 Datasets We use the following datasets in our experiments. 1. 20 Newsgroups: This dataset contains messages across twenty newsgroups. In our experiments, we use bydate version of the 20Newsgroup dataset1. This version of the dataset is divided into training (60%) and test (40%) datasets. We construct classifiers on training datasets and evaluate them on test datasets. 2. SRAA: Simulated/Real/Aviation/Auto UseNet data2: This dataset contains 73,218 1http://qwone.com/-jas</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. The Journal of Machine Learning Research, 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In Proceedings of the eleventh annual conference on Computational learning theory,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="5068" citStr="Blum and Mitchell, 1998" startWordPosition="794" endWordPosition="797">rify this hypothesis on three real world datasets. 2 Related Work Several researchers have proposed semisupervised text classification algorithms with the aim of reducing the time, effort and cost involved in labeling documents. These algorithms can be broadly categorized into three categories depending on how supervision is provided. In the first category, a small set of labeled documents and a large set of unlabeled documents is used while learning a classifier. Semi-supervised text classification algorithms proposed in (Nigam et al., 2000), (Joachims, 1999), (Zhu and Ghahramani, 2002) and (Blum and Mitchell, 1998) are a few examples of this type. However, these algorithms are sensitive to initial labeled documents and hyper-parameters of the algorithm. In the second category, supervision comes in the form of labeled words (features). (Liu et al., 2004) and (Druck et al., 2008) are a few examples of this type. An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling. Also a human annotator may discard or mislabel a polysemous word, which may affect the performance of a text classifier. The third type of semi-supervised tex</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining Labeled and Unlabeled Data with Co-Training. In Proceedings of the eleventh annual conference on Computational learning theory, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sutanu Chakraborti</author>
<author>Rahman Mukras</author>
<author>Robert Lothian</author>
<author>Nirmalie Wiratunga</author>
<author>Stuart N K Watt</author>
<author>David J Harper</author>
</authors>
<title>Supervised Latent Semantic Indexing Using Adaptive Sprinkling. In</title>
<date>2007</date>
<booktitle>IJCAI,</booktitle>
<pages>1582--1587</pages>
<contexts>
<context position="2834" citStr="Chakraborti et al., 2007" startWordPosition="447" endWordPosition="450">that is less demanding in terms of knowledge engineering is ClassifyLDA (Hingmire et al., 2013). In this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words. These labeled topics are used to create a new topic model such that in the new model topics are better aligned to class labels. A class label is assigned to a test document on the basis of its most prominent topics. We extend ClassifyLDA algorithm by “sprinkling” topics to unlabeled documents. Sprinkling (Chakraborti et al., 2007) integrates class labels of documents into Latent Semantic Indexing (LSI)(Deerwester et al., 1990). The basic idea involves encoding of class labels as artificial words which are “sprinkled” (appended) to training documents. As LSI uses higher order word associations (Kontostathis and Pottenger, 2006), sprinkling of artificial words gives better and class-enriched latent semantic structure. However, Sprinkled LSI is a supervised technique and hence it requires expensive labeled documents. The paper revolves around the idea of labeling topics (which are far fewer in number compared to documents</context>
<context position="8732" citStr="Chakraborti et al., 2007" startWordPosition="1446" endWordPosition="1449">s the vocabulary of the corpus. We use a subscript d, -,n to denote the current token, zd,n is ignored in the Gibbs sampling update. After performing collapsed Gibbs sampling using equation 1, we use word topic assignments to compute a point ψw,t + Qw − 1 X (Ωt,d + αt − 1) (1) 56 estimate of the distribution over words φw,c and a point estimate of the posterior distribution over topics for each document d (θd) is: φw,t =ψw,t + βw &amp;quot; LE N&apos;t + βv v 1 EW (2) Let MD =&lt; Z, Φ, O &gt; be the hidden topic structure, where Z is per word per document topic assignment, Φ = 1φt} and O = 1θd}. 3.2 Sprinkling (Chakraborti et al., 2007) propose a simple approach called “sprinkling” to incorporate class labels of documents into LSI. In sprinkling, a set of artificial words are appended to a training document which are specific to the class label of the document. Consider a case of binary classification with classes c1 and c2. If a document d belongs to the class c1 then a set of artificial words which represent the class c1 are appended into the document d, otherwise a set of artificial words which represent the class c2 are appended. Singular Value Decomposition (SVD) is then performed on the sprinkled training documents and</context>
<context position="18929" citStr="Chakraborti et al., 2007" startWordPosition="3186" endWordPosition="3189">he performance of the resulting classifier. However, in our approach the annotator can assign multiple class labels to a topic, hence our approach is more flexible for the annotator to encode her domain knowledge efficiently. 6 Conclusions and Future Work In this paper we propose a novel algorithm that classifies documents based on class labels over few topics. This reduces the need to label a large collection of documents. We have used the idea of sprinkling originally proposed in the context of supervised Latent Semantic Analysis, but the setting here is quite different. Unlike the work in (Chakraborti et al., 2007), we do not assume that we have class labels over the set of training documents. Instead, to realize our goal of reducing knowledge acquisition overhead, we propose a way of propagating knowledge of few topic labels to the words and inducing a new topic distribution that has its topics more closely aligned to the class labels. The results show that the approach can yield performance comparable to entirely supervised settings. In future work, we also envision the possibility of sprinkling knowledge from background knowledge sources like Wikipedia (Gabrilovich and Markovitch, 2007) to realize an</context>
</contexts>
<marker>Chakraborti, Mukras, Lothian, Wiratunga, Watt, Harper, 2007</marker>
<rawString>Sutanu Chakraborti, Rahman Mukras, Robert Lothian, Nirmalie Wiratunga, Stuart N. K. Watt, David J. Harper. 2007. Supervised Latent Semantic Indexing Using Adaptive Sprinkling. In IJCAI, pages 1582-1587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>JASIS,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="2932" citStr="Deerwester et al., 1990" startWordPosition="461" endWordPosition="464"> this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words. These labeled topics are used to create a new topic model such that in the new model topics are better aligned to class labels. A class label is assigned to a test document on the basis of its most prominent topics. We extend ClassifyLDA algorithm by “sprinkling” topics to unlabeled documents. Sprinkling (Chakraborti et al., 2007) integrates class labels of documents into Latent Semantic Indexing (LSI)(Deerwester et al., 1990). The basic idea involves encoding of class labels as artificial words which are “sprinkled” (appended) to training documents. As LSI uses higher order word associations (Kontostathis and Pottenger, 2006), sprinkling of artificial words gives better and class-enriched latent semantic structure. However, Sprinkled LSI is a supervised technique and hence it requires expensive labeled documents. The paper revolves around the idea of labeling topics (which are far fewer in number compared to documents) as in ClassifyLDA, and using these labeled topic for sprinkling. As in ClassifyLDA, we ask an an</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. JASIS, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Gideon Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning from Labeled Features using Generalized Expectation criteria.</title>
<date>2008</date>
<booktitle>In SIGIR,</booktitle>
<pages>595--602</pages>
<contexts>
<context position="5336" citStr="Druck et al., 2008" startWordPosition="838" endWordPosition="841">into three categories depending on how supervision is provided. In the first category, a small set of labeled documents and a large set of unlabeled documents is used while learning a classifier. Semi-supervised text classification algorithms proposed in (Nigam et al., 2000), (Joachims, 1999), (Zhu and Ghahramani, 2002) and (Blum and Mitchell, 1998) are a few examples of this type. However, these algorithms are sensitive to initial labeled documents and hyper-parameters of the algorithm. In the second category, supervision comes in the form of labeled words (features). (Liu et al., 2004) and (Druck et al., 2008) are a few examples of this type. An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling. Also a human annotator may discard or mislabel a polysemous word, which may affect the performance of a text classifier. The third type of semi-supervised text classification algorithms is based on active learning. In active learning, particular unlabeled documents or features are selected and queried to an oracle (e.g. human annotator).(Godbole et al., 2004), (Raghavan et al., 2006), (Druck et al., 2009) are a few example</context>
</contexts>
<marker>Druck, Mann, McCallum, 2008</marker>
<rawString>Gregory Druck, Gideon Mann, and Andrew McCallum. 2008. Learning from Labeled Features using Generalized Expectation criteria. In SIGIR, pages 595–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Burr Settles</author>
<author>Andrew McCallum</author>
</authors>
<title>Active Learning by Labeling Features.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>81--90</pages>
<contexts>
<context position="5918" citStr="Druck et al., 2009" startWordPosition="934" endWordPosition="937">t al., 2004) and (Druck et al., 2008) are a few examples of this type. An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling. Also a human annotator may discard or mislabel a polysemous word, which may affect the performance of a text classifier. The third type of semi-supervised text classification algorithms is based on active learning. In active learning, particular unlabeled documents or features are selected and queried to an oracle (e.g. human annotator).(Godbole et al., 2004), (Raghavan et al., 2006), (Druck et al., 2009) are a few examples of active learning based text classification algorithms. However, these algorithms are sensitive to the sampling strategy used to query documents or features. In our approach, an annotator does not label documents or words, rather she labels a small set of interpretable topics which are inferred in an unsupervised manner. These topics are very few, when compared to the number of documents. As the most probable words of topics are representative of the dataset, there is no need for the annotator to search for the right set of features for each class. As LDA topics are semant</context>
</contexts>
<marker>Druck, Settles, McCallum, 2009</marker>
<rawString>Gregory Druck, Burr Settles, and Andrew McCallum. 2009. Active Learning by Labeling Features. In EMNLP, pages 81–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shantanu Godbole</author>
<author>Abhay Harpale</author>
<author>Sunita Sarawagi</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Document Classification through Interactive Supervision of Document and Term Labels. In</title>
<date>2004</date>
<booktitle>PKDD,</booktitle>
<pages>185--196</pages>
<contexts>
<context position="5871" citStr="Godbole et al., 2004" startWordPosition="925" endWordPosition="928">s in the form of labeled words (features). (Liu et al., 2004) and (Druck et al., 2008) are a few examples of this type. An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling. Also a human annotator may discard or mislabel a polysemous word, which may affect the performance of a text classifier. The third type of semi-supervised text classification algorithms is based on active learning. In active learning, particular unlabeled documents or features are selected and queried to an oracle (e.g. human annotator).(Godbole et al., 2004), (Raghavan et al., 2006), (Druck et al., 2009) are a few examples of active learning based text classification algorithms. However, these algorithms are sensitive to the sampling strategy used to query documents or features. In our approach, an annotator does not label documents or words, rather she labels a small set of interpretable topics which are inferred in an unsupervised manner. These topics are very few, when compared to the number of documents. As the most probable words of topics are representative of the dataset, there is no need for the annotator to search for the right set of fe</context>
</contexts>
<marker>Godbole, Harpale, Sarawagi, Chakrabarti, 2004</marker>
<rawString>Shantanu Godbole, Abhay Harpale, Sunita Sarawagi, and Soumen Chakrabarti. 2004. Document Classification through Interactive Supervision of Document and Term Labels. In PKDD, pages 185–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding Scientific Topics.</title>
<date>2004</date>
<tech>PNAS, 101(suppl. 1):5228–5235,</tech>
<contexts>
<context position="7759" citStr="Griffiths and Steyvers, 2004" startWordPosition="1243" endWordPosition="1246">,n — Multinomial(zd,n) Where, T is the number of topics, φt is the word probabilities for topic t, Bd is the topic probability distribution, zd,n is topic assignment and wd,n is word assignment for nth word position in document d respectively. αt and Qw are topic and word Dirichlet priors. The key problem in LDA is posterior inference. The posterior inference involves the inference of the hidden topic structure given the observed documents. However, computing the exact posterior inference is intractable. In this paper we estimate approximate posterior inference using collapsed Gibbs sampling (Griffiths and Steyvers, 2004). The Gibbs sampling equation used to update the assignment of a topic t to the word w E W at the position n in document d, conditioned on αt, Qw is: P(zd,n = t|zd,¬n, wd,n = w, αt, Nw) oC E v∈W ψv,t + Qv − 1 where ψw,c is the count of the word w assigned to the topic c, Ωc,d is the count of the topic c assigned to words in the document d and W is the vocabulary of the corpus. We use a subscript d, -,n to denote the current token, zd,n is ignored in the Gibbs sampling update. After performing collapsed Gibbs sampling using equation 1, we use word topic assignments to compute a point ψw,t + Qw </context>
<context position="15129" citStr="Griffiths and Steyvers, 2004" startWordPosition="2553" endWordPosition="2556">ments. The task is to classify the webpages as student, course, faculty or project. We randomly split this dataset such that 80% is used as training and 20% is used as test data. We preprocess these datasets by removing HTML tags and stop-words. For various subsets of the 20Newsgroups and WebKB datasets discussed above, we choose number of topics as twice the number of classes. For SRAA dataset we infer 8 topics on the training dataset and label these 8 topics for all the three classification tasks. While labeling a topic, we show its 30 most probable words to the human annotator. Similar to (Griffiths and Steyvers, 2004), we set symmetric Dirichlet word prior (βw) for each topic to 0.01 and symmetric Dirichlet topic prior (αt) for each document to 50/T, where T is number of topics. We set K i.e. maximum number of words sprinkled per class to 10. 5.2 Results Table 2 shows experimental results. We can observe that, TS-LDA performs better than ClassifyLDA in 5 of the total 9 subsets. For the compreligion-sci dataset TS-LDA and ClassifyLDA have the same performance. However, ClassifyLDA performs better than TS-LDA for the three classification tasks of SRAA dataset. We can also observe that, performance of TS-LDA </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding Scientific Topics. PNAS, 101(suppl. 1):5228–5235, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapnil Hingmire</author>
<author>Sandeep Chougule</author>
<author>Girish K Palshikar</author>
<author>Sutanu Chakraborti</author>
</authors>
<title>Document Classification by Topic Labeling. In</title>
<date>2013</date>
<booktitle>SIGIR,</booktitle>
<pages>877--880</pages>
<contexts>
<context position="2304" citStr="Hingmire et al., 2013" startWordPosition="353" endWordPosition="357">cover latent semantic structure of a document collection by modeling words in the documents. Blei et al. (Blei et al., 2003) used LDA topics as features in text classification, but they use labeled documents while learning a classifier. sLDA (Blei and McAuliffe, 2007), DiscLDA (Lacoste-Julien et al., 2008) and MedLDA (Zhu et al., 2009) are few extensions of LDA which model both class labels and words in the documents. These models can be used for text classification, but they need expensive labeled documents. An approach that is less demanding in terms of knowledge engineering is ClassifyLDA (Hingmire et al., 2013). In this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words. These labeled topics are used to create a new topic model such that in the new model topics are better aligned to class labels. A class label is assigned to a test document on the basis of its most prominent topics. We extend ClassifyLDA algorithm by “sprinkling” topics to unlabeled documents. Sprinkling (Chakraborti et al., 2007) integrates class labels of documents into Latent Semantic Indexing (L</context>
<context position="11686" citStr="Hingmire et al., 2013" startWordPosition="1970" endWordPosition="1973">ent is a sprinkled word then while sampling a class label for it, we sample the class label associated with the sprinkled word, otherwise we sample a class label for the word using Gibbs update in Equation 1. We name this model as Topic Sprinkled LDA (TS-LDA). While classifying a test document, its probability distribution over class labels is inferred using TS-LDA model and it is classified to its most probable class label. Algorithm for TS-LDA is summarized in Table 1. 5 Experimental Evaluation We determine the effectiveness of our algorithm in relation to ClassifyLDA algorithm proposed in (Hingmire et al., 2013). We evaluate and compare our text classification algorithm by computing Macro averaged F1. As the inference of LDA is approximate, we repeat all the experiments for each dataset ten times and report average MacroF1. Similar to (Blei et al., 2003) we also learn supervised SVM classifier (LDA-SVM) for each dataset using topics as features and report average Macro-F1. 5.1 Datasets We use the following datasets in our experiments. 1. 20 Newsgroups: This dataset contains messages across twenty newsgroups. In our experiments, we use bydate version of the 20Newsgroup dataset1. This version of the da</context>
</contexts>
<marker>Hingmire, Chougule, Palshikar, Chakraborti, 2013</marker>
<rawString>Swapnil Hingmire, Sandeep Chougule, Girish K. Palshikar, and Sutanu Chakraborti. 2013. Document Classification by Topic Labeling. In SIGIR, pages 877–880.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Transductive Inference for Text Classification using Support Vector Machines. In</title>
<date>1999</date>
<booktitle>ICML,</booktitle>
<pages>200--209</pages>
<contexts>
<context position="5010" citStr="Joachims, 1999" startWordPosition="786" endWordPosition="787"> performance of ClassifyLDA. We experimentally verify this hypothesis on three real world datasets. 2 Related Work Several researchers have proposed semisupervised text classification algorithms with the aim of reducing the time, effort and cost involved in labeling documents. These algorithms can be broadly categorized into three categories depending on how supervision is provided. In the first category, a small set of labeled documents and a large set of unlabeled documents is used while learning a classifier. Semi-supervised text classification algorithms proposed in (Nigam et al., 2000), (Joachims, 1999), (Zhu and Ghahramani, 2002) and (Blum and Mitchell, 1998) are a few examples of this type. However, these algorithms are sensitive to initial labeled documents and hyper-parameters of the algorithm. In the second category, supervision comes in the form of labeled words (features). (Liu et al., 2004) and (Druck et al., 2008) are a few examples of this type. An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling. Also a human annotator may discard or mislabel a polysemous word, which may affect the performance o</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Transductive Inference for Text Classification using Support Vector Machines. In ICML, pages 200–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>April Kontostathis</author>
<author>William M Pottenger</author>
</authors>
<title>A Framework for Understanding Latent Semantic Indexing (LSI)</title>
<date>2006</date>
<journal>Performance. Inf. Process. Manage.,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="3136" citStr="Kontostathis and Pottenger, 2006" startWordPosition="493" endWordPosition="496">hese labeled topics are used to create a new topic model such that in the new model topics are better aligned to class labels. A class label is assigned to a test document on the basis of its most prominent topics. We extend ClassifyLDA algorithm by “sprinkling” topics to unlabeled documents. Sprinkling (Chakraborti et al., 2007) integrates class labels of documents into Latent Semantic Indexing (LSI)(Deerwester et al., 1990). The basic idea involves encoding of class labels as artificial words which are “sprinkled” (appended) to training documents. As LSI uses higher order word associations (Kontostathis and Pottenger, 2006), sprinkling of artificial words gives better and class-enriched latent semantic structure. However, Sprinkled LSI is a supervised technique and hence it requires expensive labeled documents. The paper revolves around the idea of labeling topics (which are far fewer in number compared to documents) as in ClassifyLDA, and using these labeled topic for sprinkling. As in ClassifyLDA, we ask an annotator to assign class labels to a set of topics inferred on the unlabeled training documents. We use the labeled topics to find probability distribution of each training document over the class labels. </context>
</contexts>
<marker>Kontostathis, Pottenger, 2006</marker>
<rawString>April Kontostathis and William M. Pottenger. 2006. A Framework for Understanding Latent Semantic Indexing (LSI) Performance. Inf. Process. Manage., 42(1):56–73, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Lacoste-Julien</author>
<author>Fei Sha</author>
<author>Michael I Jordan</author>
</authors>
<title>DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1989" citStr="Lacoste-Julien et al., 2008" startWordPosition="301" endWordPosition="304">beling a large number of documents is a labor-intensive and time consuming process. In this paper, we propose a text classification algorithm based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) which does not need labeled documents. LDA is an unsupervised probabilistic topic model and it is widely used to discover latent semantic structure of a document collection by modeling words in the documents. Blei et al. (Blei et al., 2003) used LDA topics as features in text classification, but they use labeled documents while learning a classifier. sLDA (Blei and McAuliffe, 2007), DiscLDA (Lacoste-Julien et al., 2008) and MedLDA (Zhu et al., 2009) are few extensions of LDA which model both class labels and words in the documents. These models can be used for text classification, but they need expensive labeled documents. An approach that is less demanding in terms of knowledge engineering is ClassifyLDA (Hingmire et al., 2013). In this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words. These labeled topics are used to create a new topic model such that in the new model to</context>
</contexts>
<marker>Lacoste-Julien, Sha, Jordan, 2008</marker>
<rawString>Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. 2008. DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangno Lee</author>
<author>Jeff Baker</author>
<author>Jaeki Song</author>
<author>James C Wetherbe</author>
</authors>
<title>An Empirical Comparison of Four Text Mining Methods.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 43rd Hawaii International Conference on System Sciences,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="4305" citStr="Lee et al., 2010" startWordPosition="683" endWordPosition="686">f each training document over the class labels. We create a set of artificial words corresponding to a class label and add (or sprinkle) them to the document. The number of such artificial terms is propor55 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 55–60, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tional to the probability of generating the document by the class label. We then infer a set of topics on the sprinkled training documents. As LDA uses higher order word associations (Lee et al., 2010) while discovering topics, we hypothesize that sprinkling will improve text classification performance of ClassifyLDA. We experimentally verify this hypothesis on three real world datasets. 2 Related Work Several researchers have proposed semisupervised text classification algorithms with the aim of reducing the time, effort and cost involved in labeling documents. These algorithms can be broadly categorized into three categories depending on how supervision is provided. In the first category, a small set of labeled documents and a large set of unlabeled documents is used while learning a clas</context>
</contexts>
<marker>Lee, Baker, Song, Wetherbe, 2010</marker>
<rawString>Sangno Lee, Jeff Baker, Jaeki Song, and James C. Wetherbe. 2010. An Empirical Comparison of Four Text Mining Methods. In Proceedings of the 2010 43rd Hawaii International Conference on System Sciences, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Xiaoli Li</author>
<author>Wee Sun Lee</author>
<author>Philip S Yu</author>
</authors>
<title>Text Classification by Labeling Words.</title>
<date>2004</date>
<booktitle>In Proceedings of the 19th national conference on Artifical intelligence,</booktitle>
<pages>425--430</pages>
<contexts>
<context position="5311" citStr="Liu et al., 2004" startWordPosition="833" endWordPosition="836">be broadly categorized into three categories depending on how supervision is provided. In the first category, a small set of labeled documents and a large set of unlabeled documents is used while learning a classifier. Semi-supervised text classification algorithms proposed in (Nigam et al., 2000), (Joachims, 1999), (Zhu and Ghahramani, 2002) and (Blum and Mitchell, 1998) are a few examples of this type. However, these algorithms are sensitive to initial labeled documents and hyper-parameters of the algorithm. In the second category, supervision comes in the form of labeled words (features). (Liu et al., 2004) and (Druck et al., 2008) are a few examples of this type. An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling. Also a human annotator may discard or mislabel a polysemous word, which may affect the performance of a text classifier. The third type of semi-supervised text classification algorithms is based on active learning. In active learning, particular unlabeled documents or features are selected and queried to an oracle (e.g. human annotator).(Godbole et al., 2004), (Raghavan et al., 2006), (Druck et al.</context>
</contexts>
<marker>Liu, Li, Lee, Yu, 2004</marker>
<rawString>Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu. 2004. Text Classification by Labeling Words. In Proceedings of the 19th national conference on Artifical intelligence, pages 425–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew Kachites McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text Classification from Labeled and Unlabeled Documents using EM. Machine Learning - Special issue on information retrieval,</title>
<date>2000</date>
<pages>39--2</pages>
<location>May-June.</location>
<contexts>
<context position="4992" citStr="Nigam et al., 2000" startWordPosition="782" endWordPosition="785">ve text classification performance of ClassifyLDA. We experimentally verify this hypothesis on three real world datasets. 2 Related Work Several researchers have proposed semisupervised text classification algorithms with the aim of reducing the time, effort and cost involved in labeling documents. These algorithms can be broadly categorized into three categories depending on how supervision is provided. In the first category, a small set of labeled documents and a large set of unlabeled documents is used while learning a classifier. Semi-supervised text classification algorithms proposed in (Nigam et al., 2000), (Joachims, 1999), (Zhu and Ghahramani, 2002) and (Blum and Mitchell, 1998) are a few examples of this type. However, these algorithms are sensitive to initial labeled documents and hyper-parameters of the algorithm. In the second category, supervision comes in the form of labeled words (features). (Liu et al., 2004) and (Druck et al., 2008) are a few examples of this type. An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling. Also a human annotator may discard or mislabel a polysemous word, which may affect</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. 2000. Text Classification from Labeled and Unlabeled Documents using EM. Machine Learning - Special issue on information retrieval, 39(2-3), May-June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hema Raghavan</author>
<author>Omid Madani</author>
<author>Rosie Jones</author>
</authors>
<date>2006</date>
<booktitle>Active Learning with Feedback on Features and Instances. JMLR,</booktitle>
<pages>7--1655</pages>
<contexts>
<context position="5896" citStr="Raghavan et al., 2006" startWordPosition="929" endWordPosition="933"> words (features). (Liu et al., 2004) and (Druck et al., 2008) are a few examples of this type. An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling. Also a human annotator may discard or mislabel a polysemous word, which may affect the performance of a text classifier. The third type of semi-supervised text classification algorithms is based on active learning. In active learning, particular unlabeled documents or features are selected and queried to an oracle (e.g. human annotator).(Godbole et al., 2004), (Raghavan et al., 2006), (Druck et al., 2009) are a few examples of active learning based text classification algorithms. However, these algorithms are sensitive to the sampling strategy used to query documents or features. In our approach, an annotator does not label documents or words, rather she labels a small set of interpretable topics which are inferred in an unsupervised manner. These topics are very few, when compared to the number of documents. As the most probable words of topics are representative of the dataset, there is no need for the annotator to search for the right set of features for each class. As</context>
</contexts>
<marker>Raghavan, Madani, Jones, 2006</marker>
<rawString>Hema Raghavan, Omid Madani, and Rosie Jones. 2006. Active Learning with Feedback on Features and Instances. JMLR, 7:1655–1686, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from Labeled and Unlabeled Data with Label Propagation.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="5038" citStr="Zhu and Ghahramani, 2002" startWordPosition="788" endWordPosition="792">assifyLDA. We experimentally verify this hypothesis on three real world datasets. 2 Related Work Several researchers have proposed semisupervised text classification algorithms with the aim of reducing the time, effort and cost involved in labeling documents. These algorithms can be broadly categorized into three categories depending on how supervision is provided. In the first category, a small set of labeled documents and a large set of unlabeled documents is used while learning a classifier. Semi-supervised text classification algorithms proposed in (Nigam et al., 2000), (Joachims, 1999), (Zhu and Ghahramani, 2002) and (Blum and Mitchell, 1998) are a few examples of this type. However, these algorithms are sensitive to initial labeled documents and hyper-parameters of the algorithm. In the second category, supervision comes in the form of labeled words (features). (Liu et al., 2004) and (Druck et al., 2008) are a few examples of this type. An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling. Also a human annotator may discard or mislabel a polysemous word, which may affect the performance of a text classifier. The thi</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from Labeled and Unlabeled Data with Label Propagation. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>MedLDA: Maximum Margin Supervised Topic Models for Regression and Classification. In</title>
<date>2009</date>
<booktitle>ICML,</booktitle>
<pages>1257--1264</pages>
<contexts>
<context position="2019" citStr="Zhu et al., 2009" startWordPosition="307" endWordPosition="310">abor-intensive and time consuming process. In this paper, we propose a text classification algorithm based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) which does not need labeled documents. LDA is an unsupervised probabilistic topic model and it is widely used to discover latent semantic structure of a document collection by modeling words in the documents. Blei et al. (Blei et al., 2003) used LDA topics as features in text classification, but they use labeled documents while learning a classifier. sLDA (Blei and McAuliffe, 2007), DiscLDA (Lacoste-Julien et al., 2008) and MedLDA (Zhu et al., 2009) are few extensions of LDA which model both class labels and words in the documents. These models can be used for text classification, but they need expensive labeled documents. An approach that is less demanding in terms of knowledge engineering is ClassifyLDA (Hingmire et al., 2013). In this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words. These labeled topics are used to create a new topic model such that in the new model topics are better aligned to cla</context>
</contexts>
<marker>Zhu, Ahmed, Xing, 2009</marker>
<rawString>Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. MedLDA: Maximum Margin Supervised Topic Models for Regression and Classification. In ICML, pages 1257–1264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing Semantic Relatedness Using Wikipediabased Explicit Semantic Analysis. In</title>
<date>2007</date>
<booktitle>IJCAI,</booktitle>
<pages>1606--1611</pages>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness Using Wikipediabased Explicit Semantic Analysis. In IJCAI, pages 1606–1611.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>