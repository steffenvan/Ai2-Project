<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003403">
<title confidence="0.937756">
USP-IBM-1 and USP-IBM-2: The ILP-based Systems for Lexical Sample
WSD in SemEval-2007
</title>
<author confidence="0.939907">
Lucia Specia, Maria das Grac¸as Volpe Nunes
</author>
<affiliation confidence="0.924563">
ICMC - University of S˜ao Paulo
</affiliation>
<address confidence="0.660673">
Trabalhador S˜ao-Carlense, 400, S˜ao Carlos, 13560-970, Brazil
</address>
<email confidence="0.981854">
{lspecia, gracan}@icmc.usp.br
</email>
<author confidence="0.997414">
Ashwin Srinivasan, Ganesh Ramakrishnan
</author>
<affiliation confidence="0.995352">
IBM India Research Laboratory
</affiliation>
<address confidence="0.749656">
Block 1, Indian Institute of Technology, New Delhi 110016, India
</address>
<email confidence="0.998959">
{ashwin.srinivasan, ganramkr}@in.ibm.com
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982357142857">
We describe two systems participating of the
English Lexical Sample task in SemEval-
2007. The systems make use of Inductive
Logic Programming for supervised learning
in two different ways: (a) to build Word
Sense Disambiguation (WSD) models from
a rich set of background knowledge sources;
and (b) to build interesting features from
the same knowledge sources, which are then
used by a standard model-builder for WSD,
namely, Support Vector Machines. Both sys-
tems achieved comparable accuracy (0.851
and 0.857), which outperforms considerably
the most frequent sense baseline (0.787).
</bodyText>
<sectionHeader confidence="0.998866" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99888466">
Word Sense Disambiguation (WSD) aims to iden-
tify the correct sense of ambiguous words in context.
Results from the last edition of the Senseval com-
petition (Mihalcea et al., 2004) have shown that, for
supervised learning, the best accuracies are obtained
with a combination of various types of features, to-
gether with traditional machine learning algorithms
based on feature-value vectors, such as Support Vec-
tor Machines (SVMs) and Naive Bayes. While the
features employed by these approaches are mostly
considered to be “shallow”, that is, extracted from
corpus or provided by shallow syntactic tools like
part-of-speech taggers, it is generally thought that
significant progress in automatic WSD would re-
quire a “deep” approach in which access to substan-
tial body of linguistic and world knowledge could
assist in resolving ambiguities. Although the ac-
cess to large amounts of knowledge is now possi-
ble due to the availability of lexicons like WordNet,
parsers, etc., the incorporation of such knowledge
has been hampered by the limitations of the mod-
elling techniques usually employed for WSD. Using
certain sources of information, mainly relational in-
formation, is beyond the capabilities of such tech-
niques, which are based on feature-value vectors.
Arguably, Inductive Logic Programming (ILP) sys-
tems provide an appropriate framework for dealing
with such data: they make explicit provisions for the
inclusion of background knowledge of any form; the
richer representation language used, based on first-
order logic, is powerful enough to capture contextual
relationships; and the modelling is not restricted to
being of a particular form (e.g., classification).
We describe the investigation of the use of ILP
for WSD in the Lexical Sample task of SemEval-
2007 in two different ways: (a) the construction of
models that can be used directly to disambiguate
words; and (b) the construction of interesting fea-
tures to be used by a standard feature-based algo-
rithm, namely, SVMs, to build disambiguation mod-
els. We call the systems resulting of the two differ-
ent approaches “USP-IBM-1” and “USP-IBM-2”,
respectively. The background knowledge is from 10
different sources of information extracted from cor-
pus, lexical resources and NLP tools.
In the rest of this paper we first present the spec-
ification of ILP implementations that construct ILP
models and features (Section 2) and then describe
the experimental evaluation on the SemEval-2007
Lexical Sample task data (Section 3).
</bodyText>
<page confidence="0.994416">
442
</page>
<bodyText confidence="0.749923">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 442–445,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.970586" genericHeader="method">
2 Inductive Logic Programming
</sectionHeader>
<bodyText confidence="0.999458083333333">
Inductive Logic Programming (ILP) (Muggleton,
1991) employs techniques from Machine Learning
and Logic Programming to build first-order theo-
ries or descriptions from examples and background
knowledge, which are also represented by first-order
clauses. Functionally, ILP can be characterised by
two classes of programs. The first, predictive ILP,
is concerned with constructing models (in this case,
sets of rules) for discriminating accurately amongst
positive and negative examples. The partial spec-
ifications provided by (Muggleton, 1994) form the
basis for deriving programs in this class:
</bodyText>
<listItem confidence="0.988759785714286">
• B is background knowledge consisting of a fi-
nite set of clauses = {C1, C2, ...I
• E is a finite set of examples = E+UE− where:
– Positive Examples. E+ = {e1, e2, ...I is
a non-empty set of definite clauses
– Negative Examples. E− = {f1, f2 ...I is
a set of Horn clauses (this may be empty)
• H, the output of the algorithm given B and E,
is acceptable if these conditions are met:
– Prior Satisfiability. B U E− K ❑
– Posterior Satisfiability. B U H U E− ❑
– Prior Necessity. B K E+
– Posterior Sufficiency. B U H �= e1 n e2 n
. . .
</listItem>
<bodyText confidence="0.999906166666667">
The second category of ILP programs, descriptive
ILP, is concerned with identifying relationships that
hold amongst the background knowledge and exam-
ples, without a view of discrimination. The partial
specifications for programs in this class are based
on the description in (Muggleton and Raedt, 1994):
</bodyText>
<listItem confidence="0.997543666666667">
• B is background knowledge
• E is a finite set of examples (this may be
empty)
• H, the output of the algorithm given B and E
is acceptable if the following condition is met:
– Posterior Sufficiency. B U H U E K ❑
</listItem>
<bodyText confidence="0.99998524137931">
The intuition behind the idea of exploiting a
feature-based model constructor that uses first-order
features is that certain sources of structured infor-
mation that cannot be represented by feature vectors
can, by a process of “propositionalization”, be iden-
tified and converted in a way that they can be accom-
modated in such vectors, allowing for traditional
learning techniques to be employed. Essentially, this
involve two steps: (1) a feature-construction step
that identifies all the features, that is, a set of clauses
H, that are consistent with the constraints provided
by the background knowledge B (descriptive ILP);
and (2) a feature-selection step that retains some of
the features based on their utility in classifying the
examples, for example, each clause must entail at
least one positive example (predictive ILP). In order
to be used by SVMs, each clause hi in H is con-
verted into a boolean feature fi that takes the value
1 (or 0) for any individual for which the body of
the clause is true (if the body is false). Thus, the
set of clauses H gives rise to a boolean vector for
each individual in the set of examples. The fea-
tures constructed may express conjunctions on dif-
ferent knowledge sources. For example, the follow-
ing boolean feature built from a clause for the verb
“ask” tests whether the sentence contains the expres-
sion “ask out” and the word “dinner”. More details
on the specifications of predictive and descriptive
ILP for WSD can be found in (Specia et al., 2007):
</bodyText>
<equation confidence="0.558563">
f1 (X) = f 1 expr(X, ’ask out’) n bag(X, dinner) = 1
</equation>
<bodyText confidence="0.812553">
Sl 0 otherwise
</bodyText>
<sectionHeader confidence="0.999586" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99887">
We investigate the performance of two kinds of ILP-
based models for WSD:
</bodyText>
<listItem confidence="0.954548111111111">
1. ILP models (USP-IBM-1 system): models con-
structed by an ILP system for predicting the
correct sense of a word.
2. ILP-assisted models (USP-IBM-2 system):
models constructed by SVMs for predicting the
correct sense of a word that, in addition to ex-
isting shallow features, use features built by an
ILP system according to the specification for
feature construction in Section 2.
</listItem>
<page confidence="0.997795">
443
</page>
<bodyText confidence="0.863298032258065">
The data for the English Lexical Sample task in
SemEval-2007 consists of 65 verbs and 35 nouns.
Examples containing those words were extracted
from the WSJ Penn Treebank II and Brown corpus.
The number of training / test examples varies from
19 / 2 to 2,536 / 541 (average = 222.8 / 48.5). The
senses of the examples were annotated according to
OntoNotes tags, which are groupings of WordNet
senses, and therefore are more coarse-grained. The
number of senses used in the training examples for
a given word varies from 1 to 13 (average = 3.6).
First-order clauses representing the following
background knowledge sources, which were au-
tomatically extracted from corpus and lexical re-
sources or provided by NLP tools, were used to de-
scribe the target words in both systems:
B1. Unigrams consisting of the 5 words to the
right and left of the target word.
B2. 5 content words to the right and left of the
target word.
B3. Part-of-speech tags of 5 words to the right and
left of the target word.
B4. Syntactic relations with respect to the target
word. If that word is a verb, subject and object syn-
tactic relations are represented. If it is a noun, the
representation includes the verb of which it is a sub-
ject or object, and the verb / noun it modifies.
B5. 12 collocations with respect to the target
word: the target word itself, 1st preposition to the
right, 1st and 2nd words to the left and right, 1st
noun, 1st adjective, and 1st verb to the left and right.
</bodyText>
<listItem confidence="0.951862888888889">
B6. A relative count of the overlapping words in
the sense inventory definitions of each of the pos-
sible senses of the target word and the words sur-
rounding that target word in the sentence, according
to the sense inventories provided.
B7. If the target word is a verb, its selectional
restrictions, defined in terms of the semantic fea-
tures of its arguments in the sentence, as given by
LDOCE. WordNet relations are used to make the
verification more generic and a hierarchy of feature
types is used to account for different levels of speci-
ficity in the restrictions.
B8. If the target word is a verb, the phrasal verbs
possibly occurring in a sentence, according to the
list of phrasal verbs given by dictionaries.
B9. Pairs of words in the sentence that occur fre-
quently in the corpus related by verb-subject/object
or subject/verb/object-modifier relations.
</listItem>
<bodyText confidence="0.990635565217391">
B10. Bigrams consisting of adjacent words in a
sentence occurring frequently in the corpus.
Of these 10 sources, B1–B6 correspond to the so
called “shallow features”, in the sense that they can
be straightforwardly represented by feature vectors.
A feature vector representation of these sources is
built to be used by the feature-based model construc-
tor. Clausal definitions for B1–B10 are directly used
by the ILP system.
We use the Aleph ILP system (Srinivasan, 1999)
to construct disambiguation models in USP-IBM-1
and to construct features to be used in USP-IBM-
2. Feature-based model construction in USP-IBM-
2 system is performed by a linear SVM (the SMO
implementation in WEKA).
In the USP-IBM-1 system, for each target word,
equipped with examples and background knowl-
edge definitions (B1–B10), Aleph constructs a set
of clauses in line with the specifications for predic-
tive ILP described in Section 2. Positive examples
are provided by the correct sense of the target word.
Negative examples are generated automatically us-
ing all the other senses. 3-fold cross-validation on
the training data was used to obtain unbiased esti-
mates of the predictive accuracy of the models for a
set of relevant parameters. The best average accura-
cies were obtained with the greedy induction strat-
egy, in conjunction with a minimal clause accuracy
of 2. The constructed clauses were used to predict
the senses in the test data following the order of their
production, in a decision-list like manner, with the
addition to the end of a default rule assigning the
majority sense for those cases which are not covered
by any other rule.
In the USP-IBM-2 system, for constructing the
“good” features for each target word from B1–
B10 (the “ILP-based features”), we first selected, in
Aleph, the clauses covering at least 1 positive exam-
ple. 3-fold cross-validation on the training data was
performed in order to obtain the best model possi-
ble using SVM with features in B1–B6 and the ILP-
based features. A feature selection method based
on information gain with various percentages of fea-
tures to be selected (1/64, ..., 1/2) was used, which
resulted in different numbers of features for each tar-
get word.
</bodyText>
<page confidence="0.99693">
444
</page>
<table confidence="0.99979275">
Baseline USP-IBM-1 USP-IBM-2
Nouns 0.809 0.882 0.882
Verbs 0.762 0.817 0.828
All 0.787 0.851 0.857
</table>
<tableCaption confidence="0.9679585">
Table 1: Average accuracies of the ILP-based mod-
els for different part-of-speeches
</tableCaption>
<bodyText confidence="0.9763159">
Table 1 shows the average accuracy of a base-
line classifier that simply votes for the most frequent
sense of each word in the training data against the
accuracy of our ILP-based systems, USP-IBM-1 and
USP-IBM-2, according to the part-of-speech of the
target word, and for all words. Clearly, the “ma-
jority class” classifier performs poorest, on average.
The difference between both ILP-based systems and
the baseline is statistically significant according to
a paired t-test with p &lt; 0.01. The two ILP-based
models appear to be comparable in their average ac-
curacy. Discarding ties, IBM-USP-2 outperforms
IBM-USP-1 for 31 of the words, but the advantage
is not statistically significant (cf. paired t-test).
The low accuracy of the ILP-based systems for
certain words may be consequence of some charac-
teristics of the data. In particular, the sense distri-
butions are very skewed in many cases, with differ-
ent distributions in the training and test data. For
example, in the case of “care” (accuracy = 0.428),
the majority sense in the training data is 1 (78.3%),
while in the test data the majority sense is 2 (71%).
In cases like this, many of the test examples remain
uncovered by the rules produced by the ILP system
and backing off to the majority sense also results in
a mistake, since the majority sense in the training
data does not apply for most of the test examples.
The same goes for the feature-based system: fea-
tures which are relevant for the test examples will
not be built or selected.
One relevant feature of ILP is its ability to pro-
duce expressive symbolic models. These models
can reproduce any kind of background knowledge
using sets of rules testing conjunctions of different
types of knowledge, which may include variables
(intensional clauses). This is valid both for the con-
struction of predictive models and for the construc-
tion of features (which are derived from the clauses).
Examples of rules induced for the verb “come” are
given in Figure 1. The first rule states that the sense
</bodyText>
<reference confidence="0.7521736">
sense(X, 3) :-
expr(X, ’come to’).
sense(X,1) :-
satisfy restrictions(X, [animate], nil);
(relation(X, subj, B), pos(X, B, nnp)).
</reference>
<figureCaption confidence="0.998544">
Figure 1: Examples of rules learned for “come”
</figureCaption>
<bodyText confidence="0.999712">
of the verb in a sentence X will be 3 (progress to a
state) if that sentence contains the expression “come
to”. The second rule states that the sense of the verb
will be 1 (move, travel, arrive) if its subject is “ani-
mate” and there is no object, or if it has has a subject
B that is a proper noun (nnp).
</bodyText>
<sectionHeader confidence="0.997144" genericHeader="method">
4 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999997333333333">
We have investigated the use of ILP as a mech-
anism for incorporating shallow and deep knowl-
edge sources into the construction of WSD mod-
els for the Semeval-2007 Lexical Sample Task data.
Results consistently outperform the most frequent
sense baseline. It is worth noticing that the knowl-
edge sources used here were initially designed for
the disambiguation of verbs (Specia et al., 2007)
and therefore we believe that further improvements
could be achieved with the identification and speci-
fication of other sources which are more appropriate
for the disambiguation of nouns.
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999831166666667">
R. Mihalcea, T. Chklovski, A. Kilgariff. 2004.
The SENSEVAL-3 English Lexical Sample Task.
SENSEVAL-3: 3rd Int. Workshop on the Evaluation of
Systems for Semantic Analysis of Text, 25–28.
S. Muggleton. 1991. Inductive Logic Program-ming.
New Generation Computing, 8(4):29-5-318.
S. Muggleton. 1994. Inductive Logic Programming:
derivations, successes and shortcomings. SIGART Bul-
letin, 5(1):5–11.
S. Muggleton and L. D. Raedt. 1994. Inductive logic
programming: Theory and methods. Journal of Logic
Programming, 19,20:629–679.
L. Specia, M.G.V. Nunes, A. Srinivasan, G. Ramakrish-
nan. 2007. Word Sense Disambiguation using Induc-
tive Logic Programming. Proceedings of the 16th In-
ternational Conference on ILP, Springer-Verlag.
A. Srinivasan. 1999. The Aleph Manual. Computing
Laboratory, Oxford University.
</reference>
<page confidence="0.99911">
445
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.180917">
<title confidence="0.495215">USP-IBM-1 and USP-IBM-2: The ILP-based Systems for Lexical Sample WSD in SemEval-2007 Specia, Maria das Volpe Nunes</title>
<affiliation confidence="0.995923">University of Paulo</affiliation>
<address confidence="0.99936">400, Carlos, 13560-970, Brazil</address>
<author confidence="0.994913">Ashwin Srinivasan</author>
<author confidence="0.994913">Ganesh Ramakrishnan</author>
<affiliation confidence="0.99999">IBM India Research Laboratory</affiliation>
<address confidence="0.957003">Block 1, Indian Institute of Technology, New Delhi 110016, India</address>
<abstract confidence="0.988179133333333">We describe two systems participating of the English Lexical Sample task in SemEval- 2007. The systems make use of Inductive Logic Programming for supervised learning in two different ways: (a) to build Word Sense Disambiguation (WSD) models from a rich set of background knowledge sources; and (b) to build interesting features from the same knowledge sources, which are then used by a standard model-builder for WSD, namely, Support Vector Machines. Both sysachieved comparable accuracy which outperforms considerably most frequent sense baseline</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<note>sense(X, 3) :- expr(X, ’come to’).</note>
<marker></marker>
<rawString>sense(X, 3) :- expr(X, ’come to’).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>