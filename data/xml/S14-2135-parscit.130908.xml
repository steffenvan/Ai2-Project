<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004153">
<title confidence="0.912269">
UNITOR: Aspect Based Sentiment Analysis with Structured Learning
</title>
<author confidence="0.890972">
Giuseppe Castellucci(†), Simone Filice(‡), Danilo Croce(*), Roberto Basili(*)
</author>
<affiliation confidence="0.74419375">
(†) Dept. of Electronic Engineering
(‡) Dept. of Civil Engineering and Computer Science Engineering
(*) Dept. of Enterprise Engineering
University of Roma, Tor Vergata, Italy
</affiliation>
<email confidence="0.990517">
{castellucci,filice}@ing.uniroma2.it; {croce,basili}@info.uniroma2.it
</email>
<sectionHeader confidence="0.993689" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999437764705882">
In this paper, the UNITOR system partici-
pating in the SemEval-2014 Aspect Based
Sentiment Analysis competition is pre-
sented. The task is tackled exploiting Ker-
nel Methods within the Support Vector
Machine framework. The Aspect Term
Extraction is modeled as a sequential tag-
ging task, tackled through SVMh&amp;quot;n&amp;quot;n. The
Aspect Term Polarity, Aspect Category
and Aspect Category Polarity detection are
tackled as a classification problem where
multiple kernels are linearly combined to
generalize several linguistic information.
In the challenge, UNITOR system achieves
good results, scoring in almost all rank-
ings between the 2nd and the 8th position
within about 30 competitors.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999751533333333">
In recent years, many websites started offering a
high level interaction with users, who are no more
a passive audience, but can actively produce new
contents. For instance, platforms like Amazon1 or
TripAdvisor2 allow people to express their opin-
ions on products, such as food, electronic items
or clothes. Obviously, companies are interested
in understanding what customers think about their
brands and products, in order to implement correc-
tive strategies on products themselves or on mar-
keting solutions. Performing an automatic analy-
sis of user opinions is then a very hot topic. The
automatic extraction of subjective information in
text materials is generally referred as Sentiment
Analysis or Opinion Mining and it is performed
</bodyText>
<footnote confidence="0.895676">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1http://www.amazon.com
2http://www.tripadvisor.com
</footnote>
<bodyText confidence="0.998691538461538">
via natural language processing, text analysis and
computational linguistics techniques. Task 4 in
SemEval 2014 edition3 (Pontiki et al., 2014) aims
at promoting research on Aspect Based Opinion
Mining (Liu, 2007), which is approached as a cas-
cade of 4 subtasks. For example, let us consider
the sentence:
The fried rice is amazing here. (1)
The Aspect Term Extraction (ATE) subtask aims
at finding words suggesting the presence of as-
pects on which an opinion is expressed, e.g.
fried rice in sentence 1. In the Aspect Term
Polarity (ATP) task the polarity evoked for each
aspect is recognized, i.e. a positive polarity is
expressed with respect to fried rice. In the
Aspect Category Detection (ACD) task the cate-
gory evoked in a sentence is identified, e.g. the
food category in sentence 1). In the Aspect Cat-
egory Polarity (ACP) task the polarity of each ex-
pressed category is recognized, e.g. a positive
category polarity is expressed in sentence 1.
Different strategies have been experimented in
recent years. Classical approaches are based on
machine learning techniques and rely on sim-
ple representation features, such as unigrams, bi-
grams, Part-Of-Speech (POS) tags (Pang et al.,
2002; Pang and Lee, 2008; Wiebe et al., 1999).
Other approaches adopt sentiment lexicons in or-
der to exploit some sort of prior knowledge about
the polar orientation of words. These resources are
usually semi-automatically compiled and provide
scores associating individual words to sentiments
or polarity orientation.
In this paper, the UNITOR system participat-
ing to the SemEval-2014 Aspect Based Sentiment
Analysis task (Pontiki et al., 2014) is presented.
The ATE task is modeled as a sequential labeling
problem. A sentence is considered as a sequence
of tokens: a Markovian algorithm is adopted in
</bodyText>
<footnote confidence="0.971572">
3http://alt.qcri.org/semeval2014/task4/
</footnote>
<page confidence="0.8879">
761
</page>
<note confidence="0.738509">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 761–767,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999904476190477">
order to decide what is an aspect term . All the
remaining tasks are modeled as multi-kernel clas-
sification problems based on Support Vector Ma-
chines (SVMs). Various representation have been
exploited using proper kernel functions (Shawe-
Taylor and Cristianini, 2004a). Tree Kernels
(Collins and Duffy, 2001; Moschitti et al., 2008;
Croce et al., 2011) are adopted in order to capture
structural sentence information derived from the
parse tree. Moreover, corpus-driven methods are
used in order to acquire meaning generalizations
in an unsupervised fashion (e.g. see (Pado and La-
pata, 2007)) through the analysis of distributions
of word occurrences in texts. It is obtained by the
construction of a Word Space (Sahlgren, 2006),
which provides a distributional model of lexical
semantics. Latent Semantic Kernel (Cristianini et
al., 2002) is thus applied within such space.
In the remaining, in Section 2 and 3 we will ex-
plain our approach in more depth. Section 4 dis-
cusses the results in the SemEval-2014 challenge.
</bodyText>
<sectionHeader confidence="0.529904" genericHeader="method">
2 Sequence Labeling for ATE
</sectionHeader>
<bodyText confidence="0.999472269230769">
The Aspect Term Extraction (ATE) has been mod-
eled as a sequential tagging process. We con-
sider each token representing the beginning (B),
the inside (I) or the outside (O) of an argu-
ment. Following this IOB notation, the resulting
ATE representation of a sentence like “The [fried
rice]ASPECTTERM is amazing here” can be expressed
by labeling each word according to its relative po-
sition, i.e.: [The]O [fried]B [rice]I [is]O [amaz-
ing]O [here]O.
The ATE task is thus a labeling process that
determines the individual (correct IOB) class for
each token. The labeling algorithm used is
SVMhmm (Altun et al., 2003)4: it combines
both a discriminative approach to estimate the
probabilities in the model and a generative ap-
proach to retrieve the most likely sequence of
tags that explains a sequence. Given an input
sequence x = (xi ... xl) E X of feature vec-
tors xi ... xl, the model predicts a tag sequence
y = (yi ... yl) E Y after learning a linear dis-
criminant function F : X x Y —* R over input-
output pairs. The labeling f(x) is thus defined as:
f(x) = arg maxy∈Y F (x, y; w) and it is obtained
by maximizing F over the response variable, y,
for a specific given input x. F is linear in some
</bodyText>
<page confidence="0.907601">
4
</page>
<footnote confidence="0.416312">
www.cs.cornell.edu/People/tj/svm light/svm hmm.html
</footnote>
<bodyText confidence="0.988956857142857">
combined feature representation of inputs and out-
puts Φ(x, y), i.e. F(x, y; w) = (w, Φ(x, y)).
In SVMhmm the observations xi ... xl can be
naturally expressed in terms of feature vectors. In
particular, we modeled each word through a set of
lexical and syntactic features, as described in the
following section.
</bodyText>
<subsectionHeader confidence="0.999206">
2.1 Modeling Features for ATE
</subsectionHeader>
<bodyText confidence="0.999589">
In the discriminative view of SV Mhmm, each
word is represented by a feature vector, describ-
ing its different observable properties. For in-
stance, the word rice in the example 1 is modeled
through the following features: Lexical features:
its lemma (rice) and POS tag (NN); Prefixes and
Suffixes: the first n and the last m characters of
the word (n = m = 3) (e.g. ric and ice); Con-
textual features: the left and right lexical contexts
represented by the 3 words before (BEGIN::BB
the::DT fried::JJ) and after (is::VBZ amazing::JJ
here::RB); the left and right syntactic contexts as
the POS bi-grams and tri-grams occurring before
(i.e. BB DT DT JJ BB DT JJ) and after (i.e.
VBZ JJ JJ RB VBZ JJ RB) the word; Gram-
matical features: features derived from the de-
pendency graph associated to the sentence, i.e.
boolean indicators that capture if the token is in-
volved in a Subj, Obj or Amod relation in the cor-
responding graph.
</bodyText>
<sectionHeader confidence="0.833891" genericHeader="method">
3 Multiple Kernel Approach for Polarity
and Category Detection
</sectionHeader>
<bodyText confidence="0.99992052631579">
We approached the remaining three subtasks of the
pipeline as classification problems with multiple
kernels, in line with (Castellucci et al., 2013). We
used Support Vector Machines (SVMs) (Joachims,
1999), a maximum-margin classifier that realizes
a linear discriminative model. The kernelized ver-
sion of SVM learns from instances xi exploiting
rich similarity measures (i.e.the kernel functions)
K(xi,xj) = (O(xi) · O(xj)). In this way projec-
tion functions O(·) can be implicitly used in order
to transform the initial feature space into a more
expressive one, where a hyperplane that separates
the data with the widest margin can be found.
Kernels can directly operate on variegate forms
of representation, such as feature vectors, trees,
sequences or graphs. Then, modeling instances
in different representations, specific kernels can
be defined in order to explore different linguis-
tic information. These variety of kernel functions
</bodyText>
<page confidence="0.979526">
762
</page>
<bodyText confidence="0.995008">
K1 ... Kn can be independently defined and the
combinations K1 + K2 + ... of multiple func-
tions can be integrated into SVM as they are still
kernels. The next section describes the represen-
tations as well as the kernel functions.
</bodyText>
<subsectionHeader confidence="0.999891">
3.1 Representing Lexical Information
</subsectionHeader>
<bodyText confidence="0.999982138888889">
The Bag of Word (BoW) is a simple repre-
sentation reflecting the lexical information of the
sentence. Each text is represented as a vector
whose dimensions correspond to different words,
i.e. they represent a boolean indicator of the pres-
ence or not of a word in the text. The resulting
kernel function is the cosine similarity (or linear
kernel) between vector pairs, i.e. linBoW. In line
with (Shawe-Taylor and Cristianini, 2004b) we in-
vestigated the contribution of the Polynomial Ker-
nel of degree 2, poly2BoW as it defines an implicit
space where also feature pairs, i.e. words pairs,
are considered.
In the polarity detection tasks, several polarity
lexicons have been exploited in order to have use-
ful hints of the intrinsic polarity of words. We
adopted MPQA Subjectivity Lexicon5 (Wilson et
al., 2005) and NRC Emotion Lexicon (Moham-
mad and Turney, 2013): they are large collection
of words provided with the underlying emotion
they generally evoke. While the former consid-
ers only positive and negative sentiments, the lat-
ter considers also eight primary emotions, orga-
nized in four opposing pairs, joy-sadness, anger-
fear, trust-disgust, and anticipation-surprise. We
define the Lexicon Based (LB) vectors as follows.
For each lexicon, let E = {e1, ..., e|E|} be the
emotion vocabulary defined in it. Let w ∈ s be
a word occurring in sentence s, with I(w, i) be-
ing the indicator function whose output is 1 if w
is associated to the emotion label ei, or 0 other-
wise. Then, given a sentence s, each ei, i.e. a di-
mension of the emotional vocabulary E, receives a
score si = Ew∈s I(w, i). Each sentence produces
a vector s~ ∈ ][B|E|, for each lexicon, on which a lin-
ear kernel linLB is applied.
</bodyText>
<subsectionHeader confidence="0.999868">
3.2 Generalizing Lexical Information
</subsectionHeader>
<bodyText confidence="0.99981">
Another representation is used to generalize the
lexical information of each text, without exploit-
ing any manually coded resource. Basic lexical
information is obtained by a co-occurrence Word
Space (WS) built accordingly to the methodology
</bodyText>
<sectionHeader confidence="0.492905" genericHeader="conclusions">
5 http://mpqa.cs.pitt.edu/lexicons/subj lexicon
</sectionHeader>
<bodyText confidence="0.998900403846154">
described in (Sahlgren, 2006) and (Croce and Pre-
vitali, 2010). A word-by-context matrix M is ob-
tained through a large scale corpus analysis. Then
the Latent Semantic Analysis (Landauer and Du-
mais, 1997) technique is applied as follows. The
matrix M is decomposed through Singular Value
Decomposition (SVD) (Golub and Kahan, 1965)
into the product of three new matrices: U, S, and
V so that S is diagonal and M = USVT . M
is then approximated by Mk = UkSkVkT, where
only the first k columns of U and V are used,
corresponding to the first k greatest singular val-
ues. This approximation supplies a way to project
a generic word wi into the k-dimensional space us-
ing W = UkS1/2
k , where each row corresponds to
the representation vector ~wi. The result is that ev-
ery word is projected in the reduced Word Space
and a sentence is represented by applying an addi-
tive model as an unbiased linear combination. We
adopted these vector representations using a linear
kernel, as in (Cristianini et al., 2002), i.e. linWS
and a Radial Basis Function Kernel rbfWS.
In Aspect Category Detection, and more gen-
erally in topic classification tasks, some specific
words can be an effective indicator of the under-
lying topic. For instance, in the restaurant do-
main, the word tasty may refer to the quality of
food. These kind of word-topic relationships can
be automatically captured by a Bag-of-Word ap-
proach, but with some limitations. As an exam-
ple, a BoW representation may not capture syn-
onyms or semantically related terms. This lack
of word generalization is partially compensated
by the already discussed Word Space. However,
this last representation aims at capturing the sense
of an overall sentence, and no particular rele-
vance is given to individual words, even if they
can be strong topic indicators. To apply a model-
ing more focused on topics, we manually selected
m seed words {σ1, ... , σm} that we consider re-
liable topic-indicators, for example spaghetti for
food. Notice that for every seed σi, as well as for
every word w the similarity function sim(σi, w)
can be derived from the Word Space represen-
tations ~σi and ~w, respectively. What we need
is a specific seed-based representation reflecting
the similarity between topic indicators and sen-
tences s. Given the words w occurring in s, the
Seed-Oriented (SO) representation of s is an m-
dimensional vector ~so(s) whose components are:
soi(s) = maxw∈s sim(σi, w). Alternatively, as
</bodyText>
<page confidence="0.98893">
763
</page>
<figure confidence="0.993427333333333">
ROOT
NSUBJa
AMODa
VBNa
COP
VBZ
be::v
JJ
amazing::j
ADVM
RB
here::r
NNa
pos
rice::n asp
</figure>
<bodyText confidence="0.992196555555556">
seeds σ refer to a set of evoked topics (i.e. as-
pect categories such as food) E1, ..., Et, we can
define a t-dimensional vector ~to(s) called Topic-
Oriented (TO) representation for s, whose fea-
tures are: toi(s) = maxw∈s,Uk∈Ei sim(σk, w).
The adopted word similarity function sim(·, ·)
over ~so(s) and ~to(s) depends on the experiments.
In the unconstrained setting, i.e. the Word Space
Topic Oriented WSTO system, sim(·, ·) consists
in the dot product over the Word Space represen-
tations ~σi and ~w. In the constrained case sim(·, ·)
corresponds to the Wu &amp; Palmer similarity based
on WordNet (Wu and Palmer, 1994), in the so
called WordNet Seed Oriented WNSO system.
The Radial Basis Function (RBF) kernel is then
applied onto the resulting feature vectors ~to(s) and
~so(s) in the rbfWSTO and rbfWNSO, respectively.
asp
</bodyText>
<subsectionHeader confidence="0.998649">
3.3 Generalizing Syntactic Information
</subsectionHeader>
<bodyText confidence="0.9999029">
In order to exploit the syntactic information, Tree
Kernel functions proposed in (Collins and Duffy,
2001) are adopted. Tree kernels exploit syntactic
similarity through the idea of convolutions among
syntactic tree substructures. Any tree kernel evalu-
ates the number of common substructures between
two trees T1 and T2 without explicitly considering
the whole fragment space. Many tree represen-
tations can be derived to represent the syntactic
information, according to different syntactic theo-
ries. For this experiment, dependency formalism
of parse trees is employed to capture sentences
syntactic information. As proposed in (Croce et
al., 2011), the kernel function is applied to ex-
amples modeled according the Grammatical Rela-
tion Centered Tree representation from the orig-
inal dependency parse structures, shown in Fig.
1: non-terminal nodes reflect syntactic relations,
such as NSUBJ, pre-terminals are the Part-Of-
Speech tags, such as nouns, and leaves are lex-
emes, such as rice::n and amazing::j6. In each ex-
ample, the aspect terms and the covering nodes are
enriched with a a suffix and all lexical nodes are
duplicated by the node asp in order to reduce data
sparseness. Moreover, prior information derived
by the lexicon can be injected in the tree, by du-
plicating all lexical nodes annotated in the MPQA
Subjectivity Lexicon, e.g. the adjective amazing,
with a node expressing the polarity (pos).
Given two tree structures T1 and T2, the
</bodyText>
<footnote confidence="0.985514">
6Each word is lemmatized to reduce data sparseness, but
they are enriched with POS tags.
</footnote>
<page confidence="0.99728">
764
</page>
<bodyText confidence="0.999974105263158">
(e.g., useful for verbs): 40,000 dimensional vec-
tors are thus derived for each target. The Singular
Value Decomposition is applied and the space di-
mensionality is reduced to k = 250. Two corpora
are used for generating two different Word Spaces,
one for the laptop and one for the restaurant do-
main. The Opinosis dataset (Ganesan et al., 2010)
is used to build the electronic domain Word Space,
while the restaurant domain corpus adopted is the
TripAdvisor dataset7. Both provided data and in-
domain data are first pre-processed through the
Stanford Parser (Klein and Manning, 2003) in or-
der to obtain POS tags or Dependency Trees.
A modified version of LibSVM has been
adopted to implement Tree Kernel. Parameters
such as the SVM regularization coefficient C, the
kernel parameters (for instance the degree of the
polynomial kernel) have been selected after a tun-
ing stage based on a 5-fold cross validation.
</bodyText>
<subsectionHeader confidence="0.995739">
4.1 Aspect Term Extraction
</subsectionHeader>
<bodyText confidence="0.999979931034483">
The Aspect Term Extraction task is modeled as a
sequential labeling problem. The feature represen-
tation described in Section 2.1, where each token
is associated to a specific target class according to
the IOB notation, is used in the SV Mhmm learn-
ing algorithm. In the constrained version of the
UNITOR system only the training data are used
to derive features. In the unconstrained case the
UNITOR system exploits lexical vectors derived
from a Word Space. Each token feature repre-
sentation is, in this sense, augmented through dis-
tributional vectors derived from the Word Spaces
described above. Obviously, the Opinosis Word
Space is used in the laptop subtask, while the Tri-
pAdvisor Word Space is used in the restaurant sub-
task. These allow the system to generalize the lex-
ical information, enabling a smoother match be-
tween words during training and test phases, hope-
fully capturing similarity phenomena such as the
relation between screen and monitor.
In Table 1 results in the laptop case are reported.
Our system performed quite well, and ranked in
6th and 10th position over 28 submitted systems.
In this case, the use of the Word Space is effec-
tive, as noticed by the 4 position gain in the final
ranking (almost 2 points in F1-measure). In Table
2 results in the restaurant case are reported. Here,
the use of Word Space does not give an improve-
ment in the final performance.
</bodyText>
<page confidence="0.667653">
7 http://sifaka.cs.uiuc.edu/˜wang296/Data/index.html
</page>
<tableCaption confidence="0.998924">
Table 1: Aspect Term Extraction Results - Laptop.
</tableCaption>
<table confidence="0.999944">
System (Rank) P R F1
UNITOR-C (10/28) .7741 .5764 .6608
UNITOR-U (6/28) .7575 .6162 .6795
Best-System-C (1/28) .8479 .6651 .7455
Best-System-U (2/28) .8251 .6712 .7403
</table>
<tableCaption confidence="0.940433">
Table 2: Aspect Term Extraction - Restaurants.
</tableCaption>
<table confidence="0.999682">
System (Rank) P R F1
UNITOR-C (5/29) .8244 .7786 .8009
UNITOR-U (6/29) .8131 .7865 .7996
Best-System-C (2/29) .8624 .8183 .8398
Best-System-U (1/29) .8535 .8271 .8401
</table>
<bodyText confidence="0.993479888888889">
In both cases, we observed that most of the
errors were associated to aspect terms composed
by multiple words. For example, in the sen-
tence The portions of the food that came out were
mediocre the gold aspect term is portions of
the food while our system was able only to re-
trieve food as aspect term. The system is mainly
able to recognize single word aspect terms and, in
most of the cases, double words aspect terms.
</bodyText>
<subsectionHeader confidence="0.998778">
4.2 Aspect Term Polarity
</subsectionHeader>
<bodyText confidence="0.999958740740741">
The Aspect Term Polarity subtask has been mod-
eled as a multi-class classification problem: for
a given set of aspect terms within a sentence, it
aims at determining whether the polarity of each
aspect term is positive, negative, neutral or con-
flict. It has been tackled using multi-kernel SVMs
in a One-vs-All Schema. In the constrained set-
ting, the linear combination of the following ker-
nel functions have been used: ptkGRCT, poly2BoW
that consider all the lemmatized terms in the sen-
tence, a poly2BoW that considers only the aspect
terms, poly2BoW of the terms around the aspect
terms in a window of size 5, linLB derived from
the Emolex lexicon. In the unconstrained setting
the sptkGRCT replaces the ptk counterpart and
the rbfWS is added by linearly combining Word
Space vectors for verbs, nouns adjective and ad-
verbs. Results in Table 3 show that the proposed
kernel combination allows to achieve the 8th posi-
tion with the unconstrained system in the restau-
rant domain. The differences with the constrained
setting demonstrate the contribution of the Word
Space acquired from the TripAdvisor corpus. Un-
fortunately, it is not true in the laptop domain, as
shown in Table 4. The use of the Opinosis corpus
lets to a performance drop of the unconstrained
setting. An error analysis shows that the main lim-
</bodyText>
<page confidence="0.995111">
765
</page>
<bodyText confidence="0.98649675">
itation of the proposed model is the inability to
capture deep semantic phenomena such as irony,
as in the negative sentence “the two waitress’s
looked like they had been sucking on lemons”.
</bodyText>
<tableCaption confidence="0.954834">
Table 3: Aspect Term Polarity Results - Restau-
rant.
</tableCaption>
<table confidence="0.999833">
System (Rank) Accuracy
UNITOR-C (12/36) .7248
UNITOR-U (8/36) .7495
Best-System-C (1/36) .8095
Best-System-U (5/36) .7768
</table>
<tableCaption confidence="0.985385">
Table 4: Aspect Term Polarity Results - Laptop.
</tableCaption>
<table confidence="0.9996006">
System (Rank) Accuracy
UNITOR-C (10/32) .6299
UNITOR-U (17/32) .5856
Best-System-C (1/32) .7048
Best-System-U (5/32) .6666
</table>
<subsectionHeader confidence="0.990924">
4.3 Aspect Category Detection
</subsectionHeader>
<bodyText confidence="0.9999305">
The Aspect Category Detection has been mod-
eled as a multi-label classification task where 5
categories (ambience, service, food, price, anec-
dotes/miscellaneous) must be recognized. In the
constrained version, each class has been tack-
led using a binary multi-kernel SVM equipped
with a linear combination of poly2BoW and
rbfWNSO. A category is assigned if the SVM
classifiers provides a positive prediction. The
anecdotes/miscellaneous acceptance threshold has
been set to 0.3 (it has been estimated over a de-
velopment set) due to its poor precision observed
during the tuning phase. Moreover, considering
each sentence is always associated to at least one
category, if no label has been assigned, then the
sentence is labelled with the category associated
to the highest prediction.
In the unconstrained case, each class has been
tackled using an ensemble of a two binary SVM-
based classifiers. The first classifier is a multi-
kernel SVM operating on a linear combination of
rbfWS and poly2BoW. The second classifier is a
SVM equipped with a rbfWSTO. A sentence is la-
belled with a category if at least one of the two cor-
responding classifiers predicts that label. The first
classifier assigns a label if the corresponding pre-
diction is positive. A more conservative strategy
is applied to the second classifier, and a category
is selected if its corresponding prediction is higher
than 0.3; again this threshold has been estimated
over a development set. As in the constrained ver-
sion, we observed a poor precision in the anec-
dotes/miscellaneous category, so we increased the
first classifier acceptance threshold to 0.3, while
the second classifier output is completely ignored.
Finally, if no label has been assigned, the sentence
is labelled with the category associated to the high-
est prediction of the first classifier.
</bodyText>
<tableCaption confidence="0.997037">
Table 5: Aspect Category Detection Results.
</tableCaption>
<table confidence="0.9996454">
System (Rank) P R F1
UNITOR-C (6/21) .8368 .7804 .8076
UNITOR-U (2/21) .8498 .8556 .8526
Best-System-C (1/21) .9104 .8624 .8857
Best-System-U (4/21) .8435 .7892 .8155
</table>
<bodyText confidence="0.965735166666667">
Table 5 reports the achieved results. Consider-
ing the simplicity of the proposed approach, the
results are impressive. The ensemble schema,
adopted in the unconstrained version, is very use-
ful in improving the recall and allows the system
to achieve the second position in the competition.
</bodyText>
<subsectionHeader confidence="0.991362">
4.4 Aspect Category Polarity
</subsectionHeader>
<bodyText confidence="0.999947416666667">
The Aspect Category Polarity subtask has been
modeled as a multi-class classification problem:
given a set of pre-identified aspect categories for a
sentence, it aims at determining the polarity (pos-
itive, negative, neutral or conflict) of each cate-
gory. It has been tackled using multi-kernel SVMs
in a One-vs-All Schema. In the constrained set-
ting, the linear combination of the following ker-
nel functions has been used: ptkGRCT, poly2BoW
that consider all the lemmatized terms in the sen-
tence, a poly2BoW that considers only verbs, nouns
adjective and adverbs in the sentence, linLB de-
rived from the MPQA sentiment lexicon. In the
unconstrained case the sptkGRCT replaces the ptk
counterpart and the rbfWS is added by linearly
combining Word Space vectors for verbs, nouns
adjective and adverbs.
Again, results shown in Table 6 suggest the pos-
itive contribution of the lexical generalization pro-
vided by the Word Space (in the sptkGRCT and
rbfWS) allows to achieve a good rank, i.e. the
4th position with the unconstrained system in the
restaurant domain. The error analysis underlines
that the proposed features do not capture irony.
</bodyText>
<tableCaption confidence="0.992014">
Table 6: Aspect Category Polarity Results.
</tableCaption>
<table confidence="0.9993922">
System (Rank) Accuracy
UNITOR-C (7/25) .7307
UNITOR-U (4/25) .7629
Best-System-C (1/25) .8292
Best-System-U (9/25) .7278
</table>
<page confidence="0.994887">
766
</page>
<sectionHeader confidence="0.990204" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99988365625">
Yasemin Altun, I. Tsochantaridis, and T. Hofmann.
2003. Hidden Markov support vector machines. In
Proceedings of the International Conference on Ma-
chine Learning.
Giuseppe Castellucci, Simone Filice, Danilo Croce,
and Roberto Basili. 2013. Unitor: Combining
syntactic and semantic kernels for twitter sentiment
analysis. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 369–
374, Atlanta, Georgia, USA, June. ACL.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neu-
ral Information Processing Systems (NIPS’2001),
pages 625–632.
Nello Cristianini, John Shawe-Taylor, and Huma
Lodhi. 2002. Latent semantic kernels. J. Intell.
Inf. Syst., 18(2-3):127–152.
Danilo Croce and Daniele Previtali. 2010. Mani-
fold learning for the semi-supervised induction of
framenet predicates: an empirical investigation. In
GEMS 2010, pages 7–16, Stroudsburg, PA, USA.
ACL.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of EMNLP, Edinburgh, Scotland, UK.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 340–348. ACL.
Gene Golub and W. Kahan. 1965. Calculating the sin-
gular values and pseudo-inverse of a matrix. Journal
of the Society for Industrial and Applied Mathemat-
ics: Series B, Numerical Analysis, 2(2):pp. 205–224.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. MIT Press, Cambridge, MA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
ACL’03, pages 423–430.
Tom Landauer and Sue Dumais. 1997. A solution to
plato’s problem: The latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104.
Bing Liu. 2007. Web data mining. Springer.
Saif Mohammad and Peter D. Turney. 2013. Crowd-
sourcing a word-emotion association lexicon. Com-
putational Intelligence, 29(3):436–465.
Alessandro Moschitti, Daniele Pighin, and Robert
Basili. 2008. Tree kernels for semantic role label-
ing. Computational Linguistics, 34.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML, Berlin, Germany, September.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics, 33(2).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1–135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In EMNLP, vol-
ume 10, pages 79–86, Stroudsburg, PA, USA. ACL.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).
Magnus Sahlgren. 2006. The Word-Space Model.
Ph.D. thesis, Stockholm University.
John Shawe-Taylor and Nello Cristianini. 2004a. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press, New York, NY, USA.
John Shawe-Taylor and Nello Cristianini. 2004b. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P.
O’Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In
Proceedings of the 37th annual meeting of the
ACL on Computational Linguistics, pages 246–253,
Stroudsburg, PA, USA. ACL.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technologies Conference/Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP 2005), Vancouver, CA.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the
32Nd Annual Meeting ofACL, ACL ’94, pages 133–
138, Stroudsburg, PA, USA. ACL.
</reference>
<page confidence="0.996745">
767
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.810279">
<title confidence="0.999274">UNITOR: Aspect Based Sentiment Analysis with Structured Learning</title>
<author confidence="0.999082">Simone Danilo Roberto</author>
<affiliation confidence="0.9655445">Dept. of Electronic Dept. of Civil Engineering and Computer Science Dept. of Enterprise University of Roma, Tor Vergata, Italy</affiliation>
<abstract confidence="0.994588277777778">this paper, the participating in the SemEval-2014 Aspect Based Sentiment Analysis competition is presented. The task is tackled exploiting Kernel Methods within the Support Vector Machine framework. The Aspect Term Extraction is modeled as a sequential tagtask, tackled through The Aspect Term Polarity, Aspect Category and Aspect Category Polarity detection are tackled as a classification problem where multiple kernels are linearly combined to generalize several linguistic information. the challenge, achieves good results, scoring in almost all rankbetween the and the position within about 30 competitors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasemin Altun</author>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
</authors>
<title>Hidden Markov support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5713" citStr="Altun et al., 2003" startWordPosition="860" endWordPosition="863"> for ATE The Aspect Term Extraction (ATE) has been modeled as a sequential tagging process. We consider each token representing the beginning (B), the inside (I) or the outside (O) of an argument. Following this IOB notation, the resulting ATE representation of a sentence like “The [fried rice]ASPECTTERM is amazing here” can be expressed by labeling each word according to its relative position, i.e.: [The]O [fried]B [rice]I [is]O [amazing]O [here]O. The ATE task is thus a labeling process that determines the individual (correct IOB) class for each token. The labeling algorithm used is SVMhmm (Altun et al., 2003)4: it combines both a discriminative approach to estimate the probabilities in the model and a generative approach to retrieve the most likely sequence of tags that explains a sequence. Given an input sequence x = (xi ... xl) E X of feature vectors xi ... xl, the model predicts a tag sequence y = (yi ... yl) E Y after learning a linear discriminant function F : X x Y —* R over inputoutput pairs. The labeling f(x) is thus defined as: f(x) = arg maxy∈Y F (x, y; w) and it is obtained by maximizing F over the response variable, y, for a specific given input x. F is linear in some 4 www.cs.cornell.</context>
</contexts>
<marker>Altun, Tsochantaridis, Hofmann, 2003</marker>
<rawString>Yasemin Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hidden Markov support vector machines. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Castellucci</author>
<author>Simone Filice</author>
<author>Danilo Croce</author>
<author>Roberto Basili</author>
</authors>
<title>Unitor: Combining syntactic and semantic kernels for twitter sentiment analysis.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>369--374</pages>
<publisher>ACL.</publisher>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="7827" citStr="Castellucci et al., 2013" startWordPosition="1234" endWordPosition="1237">(is::VBZ amazing::JJ here::RB); the left and right syntactic contexts as the POS bi-grams and tri-grams occurring before (i.e. BB DT DT JJ BB DT JJ) and after (i.e. VBZ JJ JJ RB VBZ JJ RB) the word; Grammatical features: features derived from the dependency graph associated to the sentence, i.e. boolean indicators that capture if the token is involved in a Subj, Obj or Amod relation in the corresponding graph. 3 Multiple Kernel Approach for Polarity and Category Detection We approached the remaining three subtasks of the pipeline as classification problems with multiple kernels, in line with (Castellucci et al., 2013). We used Support Vector Machines (SVMs) (Joachims, 1999), a maximum-margin classifier that realizes a linear discriminative model. The kernelized version of SVM learns from instances xi exploiting rich similarity measures (i.e.the kernel functions) K(xi,xj) = (O(xi) · O(xj)). In this way projection functions O(·) can be implicitly used in order to transform the initial feature space into a more expressive one, where a hyperplane that separates the data with the widest margin can be found. Kernels can directly operate on variegate forms of representation, such as feature vectors, trees, sequen</context>
</contexts>
<marker>Castellucci, Filice, Croce, Basili, 2013</marker>
<rawString>Giuseppe Castellucci, Simone Filice, Danilo Croce, and Roberto Basili. 2013. Unitor: Combining syntactic and semantic kernels for twitter sentiment analysis. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 369– 374, Atlanta, Georgia, USA, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems (NIPS’2001),</booktitle>
<pages>625--632</pages>
<contexts>
<context position="4363" citStr="Collins and Duffy, 2001" startWordPosition="640" endWordPosition="643">task is modeled as a sequential labeling problem. A sentence is considered as a sequence of tokens: a Markovian algorithm is adopted in 3http://alt.qcri.org/semeval2014/task4/ 761 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 761–767, Dublin, Ireland, August 23-24, 2014. order to decide what is an aspect term . All the remaining tasks are modeled as multi-kernel classification problems based on Support Vector Machines (SVMs). Various representation have been exploited using proper kernel functions (ShaweTaylor and Cristianini, 2004a). Tree Kernels (Collins and Duffy, 2001; Moschitti et al., 2008; Croce et al., 2011) are adopted in order to capture structural sentence information derived from the parse tree. Moreover, corpus-driven methods are used in order to acquire meaning generalizations in an unsupervised fashion (e.g. see (Pado and Lapata, 2007)) through the analysis of distributions of word occurrences in texts. It is obtained by the construction of a Word Space (Sahlgren, 2006), which provides a distributional model of lexical semantics. Latent Semantic Kernel (Cristianini et al., 2002) is thus applied within such space. In the remaining, in Section 2 a</context>
<context position="14405" citStr="Collins and Duffy, 2001" startWordPosition="2323" endWordPosition="2326">unconstrained setting, i.e. the Word Space Topic Oriented WSTO system, sim(·, ·) consists in the dot product over the Word Space representations ~σi and ~w. In the constrained case sim(·, ·) corresponds to the Wu &amp; Palmer similarity based on WordNet (Wu and Palmer, 1994), in the so called WordNet Seed Oriented WNSO system. The Radial Basis Function (RBF) kernel is then applied onto the resulting feature vectors ~to(s) and ~so(s) in the rbfWSTO and rbfWNSO, respectively. asp 3.3 Generalizing Syntactic Information In order to exploit the syntactic information, Tree Kernel functions proposed in (Collins and Duffy, 2001) are adopted. Tree kernels exploit syntactic similarity through the idea of convolutions among syntactic tree substructures. Any tree kernel evaluates the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. Many tree representations can be derived to represent the syntactic information, according to different syntactic theories. For this experiment, dependency formalism of parse trees is employed to capture sentences syntactic information. As proposed in (Croce et al., 2011), the kernel function is applied to examples modeled acco</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proceedings of Neural Information Processing Systems (NIPS’2001), pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
<author>Huma Lodhi</author>
</authors>
<title>Latent semantic kernels.</title>
<date>2002</date>
<journal>J. Intell. Inf. Syst.,</journal>
<pages>18--2</pages>
<contexts>
<context position="4895" citStr="Cristianini et al., 2002" startWordPosition="721" endWordPosition="724"> kernel functions (ShaweTaylor and Cristianini, 2004a). Tree Kernels (Collins and Duffy, 2001; Moschitti et al., 2008; Croce et al., 2011) are adopted in order to capture structural sentence information derived from the parse tree. Moreover, corpus-driven methods are used in order to acquire meaning generalizations in an unsupervised fashion (e.g. see (Pado and Lapata, 2007)) through the analysis of distributions of word occurrences in texts. It is obtained by the construction of a Word Space (Sahlgren, 2006), which provides a distributional model of lexical semantics. Latent Semantic Kernel (Cristianini et al., 2002) is thus applied within such space. In the remaining, in Section 2 and 3 we will explain our approach in more depth. Section 4 discusses the results in the SemEval-2014 challenge. 2 Sequence Labeling for ATE The Aspect Term Extraction (ATE) has been modeled as a sequential tagging process. We consider each token representing the beginning (B), the inside (I) or the outside (O) of an argument. Following this IOB notation, the resulting ATE representation of a sentence like “The [fried rice]ASPECTTERM is amazing here” can be expressed by labeling each word according to its relative position, i.e</context>
<context position="11911" citStr="Cristianini et al., 2002" startWordPosition="1910" endWordPosition="1913">, and V so that S is diagonal and M = USVT . M is then approximated by Mk = UkSkVkT, where only the first k columns of U and V are used, corresponding to the first k greatest singular values. This approximation supplies a way to project a generic word wi into the k-dimensional space using W = UkS1/2 k , where each row corresponds to the representation vector ~wi. The result is that every word is projected in the reduced Word Space and a sentence is represented by applying an additive model as an unbiased linear combination. We adopted these vector representations using a linear kernel, as in (Cristianini et al., 2002), i.e. linWS and a Radial Basis Function Kernel rbfWS. In Aspect Category Detection, and more generally in topic classification tasks, some specific words can be an effective indicator of the underlying topic. For instance, in the restaurant domain, the word tasty may refer to the quality of food. These kind of word-topic relationships can be automatically captured by a Bag-of-Word approach, but with some limitations. As an example, a BoW representation may not capture synonyms or semantically related terms. This lack of word generalization is partially compensated by the already discussed Wor</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, Lodhi, 2002</marker>
<rawString>Nello Cristianini, John Shawe-Taylor, and Huma Lodhi. 2002. Latent semantic kernels. J. Intell. Inf. Syst., 18(2-3):127–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Daniele Previtali</author>
</authors>
<title>Manifold learning for the semi-supervised induction of framenet predicates: an empirical investigation.</title>
<date>2010</date>
<booktitle>In GEMS 2010,</booktitle>
<pages>7--16</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10973" citStr="Croce and Previtali, 2010" startWordPosition="1741" endWordPosition="1745"> 0 otherwise. Then, given a sentence s, each ei, i.e. a dimension of the emotional vocabulary E, receives a score si = Ew∈s I(w, i). Each sentence produces a vector s~ ∈ ][B|E|, for each lexicon, on which a linear kernel linLB is applied. 3.2 Generalizing Lexical Information Another representation is used to generalize the lexical information of each text, without exploiting any manually coded resource. Basic lexical information is obtained by a co-occurrence Word Space (WS) built accordingly to the methodology 5 http://mpqa.cs.pitt.edu/lexicons/subj lexicon described in (Sahlgren, 2006) and (Croce and Previtali, 2010). A word-by-context matrix M is obtained through a large scale corpus analysis. Then the Latent Semantic Analysis (Landauer and Dumais, 1997) technique is applied as follows. The matrix M is decomposed through Singular Value Decomposition (SVD) (Golub and Kahan, 1965) into the product of three new matrices: U, S, and V so that S is diagonal and M = USVT . M is then approximated by Mk = UkSkVkT, where only the first k columns of U and V are used, corresponding to the first k greatest singular values. This approximation supplies a way to project a generic word wi into the k-dimensional space usi</context>
</contexts>
<marker>Croce, Previtali, 2010</marker>
<rawString>Danilo Croce and Daniele Previtali. 2010. Manifold learning for the semi-supervised induction of framenet predicates: an empirical investigation. In GEMS 2010, pages 7–16, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>Structured lexical similarity via convolution kernels on dependency trees.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="4408" citStr="Croce et al., 2011" startWordPosition="648" endWordPosition="651"> A sentence is considered as a sequence of tokens: a Markovian algorithm is adopted in 3http://alt.qcri.org/semeval2014/task4/ 761 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 761–767, Dublin, Ireland, August 23-24, 2014. order to decide what is an aspect term . All the remaining tasks are modeled as multi-kernel classification problems based on Support Vector Machines (SVMs). Various representation have been exploited using proper kernel functions (ShaweTaylor and Cristianini, 2004a). Tree Kernels (Collins and Duffy, 2001; Moschitti et al., 2008; Croce et al., 2011) are adopted in order to capture structural sentence information derived from the parse tree. Moreover, corpus-driven methods are used in order to acquire meaning generalizations in an unsupervised fashion (e.g. see (Pado and Lapata, 2007)) through the analysis of distributions of word occurrences in texts. It is obtained by the construction of a Word Space (Sahlgren, 2006), which provides a distributional model of lexical semantics. Latent Semantic Kernel (Cristianini et al., 2002) is thus applied within such space. In the remaining, in Section 2 and 3 we will explain our approach in more dep</context>
<context position="14948" citStr="Croce et al., 2011" startWordPosition="2402" endWordPosition="2405">tic information, Tree Kernel functions proposed in (Collins and Duffy, 2001) are adopted. Tree kernels exploit syntactic similarity through the idea of convolutions among syntactic tree substructures. Any tree kernel evaluates the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. Many tree representations can be derived to represent the syntactic information, according to different syntactic theories. For this experiment, dependency formalism of parse trees is employed to capture sentences syntactic information. As proposed in (Croce et al., 2011), the kernel function is applied to examples modeled according the Grammatical Relation Centered Tree representation from the original dependency parse structures, shown in Fig. 1: non-terminal nodes reflect syntactic relations, such as NSUBJ, pre-terminals are the Part-OfSpeech tags, such as nouns, and leaves are lexemes, such as rice::n and amazing::j6. In each example, the aspect terms and the covering nodes are enriched with a a suffix and all lexical nodes are duplicated by the node asp in order to reduce data sparseness. Moreover, prior information derived by the lexicon can be injected </context>
</contexts>
<marker>Croce, Moschitti, Basili, 2011</marker>
<rawString>Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured lexical similarity via convolution kernels on dependency trees. In Proceedings of EMNLP, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kavita Ganesan</author>
<author>ChengXiang Zhai</author>
<author>Jiawei Han</author>
</authors>
<title>Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>340--348</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="16186" citStr="Ganesan et al., 2010" startWordPosition="2609" endWordPosition="2612">uplicating all lexical nodes annotated in the MPQA Subjectivity Lexicon, e.g. the adjective amazing, with a node expressing the polarity (pos). Given two tree structures T1 and T2, the 6Each word is lemmatized to reduce data sparseness, but they are enriched with POS tags. 764 (e.g., useful for verbs): 40,000 dimensional vectors are thus derived for each target. The Singular Value Decomposition is applied and the space dimensionality is reduced to k = 250. Two corpora are used for generating two different Word Spaces, one for the laptop and one for the restaurant domain. The Opinosis dataset (Ganesan et al., 2010) is used to build the electronic domain Word Space, while the restaurant domain corpus adopted is the TripAdvisor dataset7. Both provided data and indomain data are first pre-processed through the Stanford Parser (Klein and Manning, 2003) in order to obtain POS tags or Dependency Trees. A modified version of LibSVM has been adopted to implement Tree Kernel. Parameters such as the SVM regularization coefficient C, the kernel parameters (for instance the degree of the polynomial kernel) have been selected after a tuning stage based on a 5-fold cross validation. 4.1 Aspect Term Extraction The Asp</context>
</contexts>
<marker>Ganesan, Zhai, Han, 2010</marker>
<rawString>Kavita Ganesan, ChengXiang Zhai, and Jiawei Han. 2010. Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 340–348. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene Golub</author>
<author>W Kahan</author>
</authors>
<title>Calculating the singular values and pseudo-inverse of a matrix.</title>
<date>1965</date>
<journal>Journal of the Society for Industrial and Applied Mathematics: Series B, Numerical Analysis,</journal>
<volume>2</volume>
<issue>2</issue>
<pages>205--224</pages>
<contexts>
<context position="11241" citStr="Golub and Kahan, 1965" startWordPosition="1784" endWordPosition="1787">ion Another representation is used to generalize the lexical information of each text, without exploiting any manually coded resource. Basic lexical information is obtained by a co-occurrence Word Space (WS) built accordingly to the methodology 5 http://mpqa.cs.pitt.edu/lexicons/subj lexicon described in (Sahlgren, 2006) and (Croce and Previtali, 2010). A word-by-context matrix M is obtained through a large scale corpus analysis. Then the Latent Semantic Analysis (Landauer and Dumais, 1997) technique is applied as follows. The matrix M is decomposed through Singular Value Decomposition (SVD) (Golub and Kahan, 1965) into the product of three new matrices: U, S, and V so that S is diagonal and M = USVT . M is then approximated by Mk = UkSkVkT, where only the first k columns of U and V are used, corresponding to the first k greatest singular values. This approximation supplies a way to project a generic word wi into the k-dimensional space using W = UkS1/2 k , where each row corresponds to the representation vector ~wi. The result is that every word is projected in the reduced Word Space and a sentence is represented by applying an additive model as an unbiased linear combination. We adopted these vector r</context>
</contexts>
<marker>Golub, Kahan, 1965</marker>
<rawString>Gene Golub and W. Kahan. 1965. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics: Series B, Numerical Analysis, 2(2):pp. 205–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-Scale SVM Learning Practical.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7884" citStr="Joachims, 1999" startWordPosition="1244" endWordPosition="1245">ts as the POS bi-grams and tri-grams occurring before (i.e. BB DT DT JJ BB DT JJ) and after (i.e. VBZ JJ JJ RB VBZ JJ RB) the word; Grammatical features: features derived from the dependency graph associated to the sentence, i.e. boolean indicators that capture if the token is involved in a Subj, Obj or Amod relation in the corresponding graph. 3 Multiple Kernel Approach for Polarity and Category Detection We approached the remaining three subtasks of the pipeline as classification problems with multiple kernels, in line with (Castellucci et al., 2013). We used Support Vector Machines (SVMs) (Joachims, 1999), a maximum-margin classifier that realizes a linear discriminative model. The kernelized version of SVM learns from instances xi exploiting rich similarity measures (i.e.the kernel functions) K(xi,xj) = (O(xi) · O(xj)). In this way projection functions O(·) can be implicitly used in order to transform the initial feature space into a more expressive one, where a hyperplane that separates the data with the widest margin can be found. Kernels can directly operate on variegate forms of representation, such as feature vectors, trees, sequences or graphs. Then, modeling instances in different repr</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-Scale SVM Learning Practical. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="16424" citStr="Klein and Manning, 2003" startWordPosition="2646" endWordPosition="2649">, but they are enriched with POS tags. 764 (e.g., useful for verbs): 40,000 dimensional vectors are thus derived for each target. The Singular Value Decomposition is applied and the space dimensionality is reduced to k = 250. Two corpora are used for generating two different Word Spaces, one for the laptop and one for the restaurant domain. The Opinosis dataset (Ganesan et al., 2010) is used to build the electronic domain Word Space, while the restaurant domain corpus adopted is the TripAdvisor dataset7. Both provided data and indomain data are first pre-processed through the Stanford Parser (Klein and Manning, 2003) in order to obtain POS tags or Dependency Trees. A modified version of LibSVM has been adopted to implement Tree Kernel. Parameters such as the SVM regularization coefficient C, the kernel parameters (for instance the degree of the polynomial kernel) have been selected after a tuning stage based on a 5-fold cross validation. 4.1 Aspect Term Extraction The Aspect Term Extraction task is modeled as a sequential labeling problem. The feature representation described in Section 2.1, where each token is associated to a specific target class according to the IOB notation, is used in the SV Mhmm lea</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL’03, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Landauer</author>
<author>Sue Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<contexts>
<context position="11114" citStr="Landauer and Dumais, 1997" startWordPosition="1764" endWordPosition="1768">tence produces a vector s~ ∈ ][B|E|, for each lexicon, on which a linear kernel linLB is applied. 3.2 Generalizing Lexical Information Another representation is used to generalize the lexical information of each text, without exploiting any manually coded resource. Basic lexical information is obtained by a co-occurrence Word Space (WS) built accordingly to the methodology 5 http://mpqa.cs.pitt.edu/lexicons/subj lexicon described in (Sahlgren, 2006) and (Croce and Previtali, 2010). A word-by-context matrix M is obtained through a large scale corpus analysis. Then the Latent Semantic Analysis (Landauer and Dumais, 1997) technique is applied as follows. The matrix M is decomposed through Singular Value Decomposition (SVD) (Golub and Kahan, 1965) into the product of three new matrices: U, S, and V so that S is diagonal and M = USVT . M is then approximated by Mk = UkSkVkT, where only the first k columns of U and V are used, corresponding to the first k greatest singular values. This approximation supplies a way to project a generic word wi into the k-dimensional space using W = UkS1/2 k , where each row corresponds to the representation vector ~wi. The result is that every word is projected in the reduced Word</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Tom Landauer and Sue Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Web data mining.</title>
<date>2007</date>
<publisher>Springer.</publisher>
<contexts>
<context position="2298" citStr="Liu, 2007" startWordPosition="319" endWordPosition="320">raction of subjective information in text materials is generally referred as Sentiment Analysis or Opinion Mining and it is performed This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://www.amazon.com 2http://www.tripadvisor.com via natural language processing, text analysis and computational linguistics techniques. Task 4 in SemEval 2014 edition3 (Pontiki et al., 2014) aims at promoting research on Aspect Based Opinion Mining (Liu, 2007), which is approached as a cascade of 4 subtasks. For example, let us consider the sentence: The fried rice is amazing here. (1) The Aspect Term Extraction (ATE) subtask aims at finding words suggesting the presence of aspects on which an opinion is expressed, e.g. fried rice in sentence 1. In the Aspect Term Polarity (ATP) task the polarity evoked for each aspect is recognized, i.e. a positive polarity is expressed with respect to fried rice. In the Aspect Category Detection (ACD) task the category evoked in a sentence is identified, e.g. the food category in sentence 1). In the Aspect Catego</context>
</contexts>
<marker>Liu, 2007</marker>
<rawString>Bing Liu. 2007. Web data mining. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Crowdsourcing a word-emotion association lexicon.</title>
<date>2013</date>
<journal>Computational Intelligence,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="9748" citStr="Mohammad and Turney, 2013" startWordPosition="1540" endWordPosition="1544">resence or not of a word in the text. The resulting kernel function is the cosine similarity (or linear kernel) between vector pairs, i.e. linBoW. In line with (Shawe-Taylor and Cristianini, 2004b) we investigated the contribution of the Polynomial Kernel of degree 2, poly2BoW as it defines an implicit space where also feature pairs, i.e. words pairs, are considered. In the polarity detection tasks, several polarity lexicons have been exploited in order to have useful hints of the intrinsic polarity of words. We adopted MPQA Subjectivity Lexicon5 (Wilson et al., 2005) and NRC Emotion Lexicon (Mohammad and Turney, 2013): they are large collection of words provided with the underlying emotion they generally evoke. While the former considers only positive and negative sentiments, the latter considers also eight primary emotions, organized in four opposing pairs, joy-sadness, angerfear, trust-disgust, and anticipation-surprise. We define the Lexicon Based (LB) vectors as follows. For each lexicon, let E = {e1, ..., e|E|} be the emotion vocabulary defined in it. Let w ∈ s be a word occurring in sentence s, with I(w, i) being the indicator function whose output is 1 if w is associated to the emotion label ei, or </context>
</contexts>
<marker>Mohammad, Turney, 2013</marker>
<rawString>Saif Mohammad and Peter D. Turney. 2013. Crowdsourcing a word-emotion association lexicon. Computational Intelligence, 29(3):436–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Robert Basili</author>
</authors>
<title>Tree kernels for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<contexts>
<context position="4387" citStr="Moschitti et al., 2008" startWordPosition="644" endWordPosition="647">ential labeling problem. A sentence is considered as a sequence of tokens: a Markovian algorithm is adopted in 3http://alt.qcri.org/semeval2014/task4/ 761 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 761–767, Dublin, Ireland, August 23-24, 2014. order to decide what is an aspect term . All the remaining tasks are modeled as multi-kernel classification problems based on Support Vector Machines (SVMs). Various representation have been exploited using proper kernel functions (ShaweTaylor and Cristianini, 2004a). Tree Kernels (Collins and Duffy, 2001; Moschitti et al., 2008; Croce et al., 2011) are adopted in order to capture structural sentence information derived from the parse tree. Moreover, corpus-driven methods are used in order to acquire meaning generalizations in an unsupervised fashion (e.g. see (Pado and Lapata, 2007)) through the analysis of distributions of word occurrences in texts. It is obtained by the construction of a Word Space (Sahlgren, 2006), which provides a distributional model of lexical semantics. Latent Semantic Kernel (Cristianini et al., 2002) is thus applied within such space. In the remaining, in Section 2 and 3 we will explain our</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Robert Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ECML,</booktitle>
<location>Berlin, Germany,</location>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML, Berlin, Germany, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4647" citStr="Pado and Lapata, 2007" startWordPosition="683" endWordPosition="687">n, Ireland, August 23-24, 2014. order to decide what is an aspect term . All the remaining tasks are modeled as multi-kernel classification problems based on Support Vector Machines (SVMs). Various representation have been exploited using proper kernel functions (ShaweTaylor and Cristianini, 2004a). Tree Kernels (Collins and Duffy, 2001; Moschitti et al., 2008; Croce et al., 2011) are adopted in order to capture structural sentence information derived from the parse tree. Moreover, corpus-driven methods are used in order to acquire meaning generalizations in an unsupervised fashion (e.g. see (Pado and Lapata, 2007)) through the analysis of distributions of word occurrences in texts. It is obtained by the construction of a Word Space (Sahlgren, 2006), which provides a distributional model of lexical semantics. Latent Semantic Kernel (Cristianini et al., 2002) is thus applied within such space. In the remaining, in Section 2 and 3 we will explain our approach in more depth. Section 4 discusses the results in the SemEval-2014 challenge. 2 Sequence Labeling for ATE The Aspect Term Extraction (ATE) has been modeled as a sequential tagging process. We consider each token representing the beginning (B), the in</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<journal>Found. Trends Inf. Retr.,</journal>
<pages>2--1</pages>
<contexts>
<context position="3296" citStr="Pang and Lee, 2008" startWordPosition="484" endWordPosition="487">, i.e. a positive polarity is expressed with respect to fried rice. In the Aspect Category Detection (ACD) task the category evoked in a sentence is identified, e.g. the food category in sentence 1). In the Aspect Category Polarity (ACP) task the polarity of each expressed category is recognized, e.g. a positive category polarity is expressed in sentence 1. Different strategies have been experimented in recent years. Classical approaches are based on machine learning techniques and rely on simple representation features, such as unigrams, bigrams, Part-Of-Speech (POS) tags (Pang et al., 2002; Pang and Lee, 2008; Wiebe et al., 1999). Other approaches adopt sentiment lexicons in order to exploit some sort of prior knowledge about the polar orientation of words. These resources are usually semi-automatically compiled and provide scores associating individual words to sentiments or polarity orientation. In this paper, the UNITOR system participating to the SemEval-2014 Aspect Based Sentiment Analysis task (Pontiki et al., 2014) is presented. The ATE task is modeled as a sequential labeling problem. A sentence is considered as a sequence of tokens: a Markovian algorithm is adopted in 3http://alt.qcri.org</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1–135, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<volume>10</volume>
<pages>79--86</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3276" citStr="Pang et al., 2002" startWordPosition="480" endWordPosition="483">spect is recognized, i.e. a positive polarity is expressed with respect to fried rice. In the Aspect Category Detection (ACD) task the category evoked in a sentence is identified, e.g. the food category in sentence 1). In the Aspect Category Polarity (ACP) task the polarity of each expressed category is recognized, e.g. a positive category polarity is expressed in sentence 1. Different strategies have been experimented in recent years. Classical approaches are based on machine learning techniques and rely on simple representation features, such as unigrams, bigrams, Part-Of-Speech (POS) tags (Pang et al., 2002; Pang and Lee, 2008; Wiebe et al., 1999). Other approaches adopt sentiment lexicons in order to exploit some sort of prior knowledge about the polar orientation of words. These resources are usually semi-automatically compiled and provide scores associating individual words to sentiments or polarity orientation. In this paper, the UNITOR system participating to the SemEval-2014 Aspect Based Sentiment Analysis task (Pontiki et al., 2014) is presented. The ATE task is modeled as a sequential labeling problem. A sentence is considered as a sequence of tokens: a Markovian algorithm is adopted in </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In EMNLP, volume 10, pages 79–86, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitrios Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Semeval-2014 task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval).</booktitle>
<contexts>
<context position="2228" citStr="Pontiki et al., 2014" startWordPosition="306" endWordPosition="309">n automatic analysis of user opinions is then a very hot topic. The automatic extraction of subjective information in text materials is generally referred as Sentiment Analysis or Opinion Mining and it is performed This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://www.amazon.com 2http://www.tripadvisor.com via natural language processing, text analysis and computational linguistics techniques. Task 4 in SemEval 2014 edition3 (Pontiki et al., 2014) aims at promoting research on Aspect Based Opinion Mining (Liu, 2007), which is approached as a cascade of 4 subtasks. For example, let us consider the sentence: The fried rice is amazing here. (1) The Aspect Term Extraction (ATE) subtask aims at finding words suggesting the presence of aspects on which an opinion is expressed, e.g. fried rice in sentence 1. In the Aspect Term Polarity (ATP) task the polarity evoked for each aspect is recognized, i.e. a positive polarity is expressed with respect to fried rice. In the Aspect Category Detection (ACD) task the category evoked in a sentence is i</context>
<context position="3717" citStr="Pontiki et al., 2014" startWordPosition="546" endWordPosition="549">s. Classical approaches are based on machine learning techniques and rely on simple representation features, such as unigrams, bigrams, Part-Of-Speech (POS) tags (Pang et al., 2002; Pang and Lee, 2008; Wiebe et al., 1999). Other approaches adopt sentiment lexicons in order to exploit some sort of prior knowledge about the polar orientation of words. These resources are usually semi-automatically compiled and provide scores associating individual words to sentiments or polarity orientation. In this paper, the UNITOR system participating to the SemEval-2014 Aspect Based Sentiment Analysis task (Pontiki et al., 2014) is presented. The ATE task is modeled as a sequential labeling problem. A sentence is considered as a sequence of tokens: a Markovian algorithm is adopted in 3http://alt.qcri.org/semeval2014/task4/ 761 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 761–767, Dublin, Ireland, August 23-24, 2014. order to decide what is an aspect term . All the remaining tasks are modeled as multi-kernel classification problems based on Support Vector Machines (SVMs). Various representation have been exploited using proper kernel functions (ShaweTaylor and Cristianini,</context>
</contexts>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the International Workshop on Semantic Evaluation (SemEval).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Stockholm University.</institution>
<contexts>
<context position="4784" citStr="Sahlgren, 2006" startWordPosition="708" endWordPosition="709">lems based on Support Vector Machines (SVMs). Various representation have been exploited using proper kernel functions (ShaweTaylor and Cristianini, 2004a). Tree Kernels (Collins and Duffy, 2001; Moschitti et al., 2008; Croce et al., 2011) are adopted in order to capture structural sentence information derived from the parse tree. Moreover, corpus-driven methods are used in order to acquire meaning generalizations in an unsupervised fashion (e.g. see (Pado and Lapata, 2007)) through the analysis of distributions of word occurrences in texts. It is obtained by the construction of a Word Space (Sahlgren, 2006), which provides a distributional model of lexical semantics. Latent Semantic Kernel (Cristianini et al., 2002) is thus applied within such space. In the remaining, in Section 2 and 3 we will explain our approach in more depth. Section 4 discusses the results in the SemEval-2014 challenge. 2 Sequence Labeling for ATE The Aspect Term Extraction (ATE) has been modeled as a sequential tagging process. We consider each token representing the beginning (B), the inside (I) or the outside (O) of an argument. Following this IOB notation, the resulting ATE representation of a sentence like “The [fried </context>
<context position="10941" citStr="Sahlgren, 2006" startWordPosition="1738" endWordPosition="1739"> emotion label ei, or 0 otherwise. Then, given a sentence s, each ei, i.e. a dimension of the emotional vocabulary E, receives a score si = Ew∈s I(w, i). Each sentence produces a vector s~ ∈ ][B|E|, for each lexicon, on which a linear kernel linLB is applied. 3.2 Generalizing Lexical Information Another representation is used to generalize the lexical information of each text, without exploiting any manually coded resource. Basic lexical information is obtained by a co-occurrence Word Space (WS) built accordingly to the methodology 5 http://mpqa.cs.pitt.edu/lexicons/subj lexicon described in (Sahlgren, 2006) and (Croce and Previtali, 2010). A word-by-context matrix M is obtained through a large scale corpus analysis. Then the Latent Semantic Analysis (Landauer and Dumais, 1997) technique is applied as follows. The matrix M is decomposed through Singular Value Decomposition (SVD) (Golub and Kahan, 1965) into the product of three new matrices: U, S, and V so that S is diagonal and M = USVT . M is then approximated by Mk = UkSkVkT, where only the first k columns of U and V are used, corresponding to the first k greatest singular values. This approximation supplies a way to project a generic word wi </context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model. Ph.D. thesis, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9317" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1470" endWordPosition="1473"> + K2 + ... of multiple functions can be integrated into SVM as they are still kernels. The next section describes the representations as well as the kernel functions. 3.1 Representing Lexical Information The Bag of Word (BoW) is a simple representation reflecting the lexical information of the sentence. Each text is represented as a vector whose dimensions correspond to different words, i.e. they represent a boolean indicator of the presence or not of a word in the text. The resulting kernel function is the cosine similarity (or linear kernel) between vector pairs, i.e. linBoW. In line with (Shawe-Taylor and Cristianini, 2004b) we investigated the contribution of the Polynomial Kernel of degree 2, poly2BoW as it defines an implicit space where also feature pairs, i.e. words pairs, are considered. In the polarity detection tasks, several polarity lexicons have been exploited in order to have useful hints of the intrinsic polarity of words. We adopted MPQA Subjectivity Lexicon5 (Wilson et al., 2005) and NRC Emotion Lexicon (Mohammad and Turney, 2013): they are large collection of words provided with the underlying emotion they generally evoke. While the former considers only positive and negative sentiments, the lat</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004a. Kernel Methods for Pattern Analysis. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9317" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1470" endWordPosition="1473"> + K2 + ... of multiple functions can be integrated into SVM as they are still kernels. The next section describes the representations as well as the kernel functions. 3.1 Representing Lexical Information The Bag of Word (BoW) is a simple representation reflecting the lexical information of the sentence. Each text is represented as a vector whose dimensions correspond to different words, i.e. they represent a boolean indicator of the presence or not of a word in the text. The resulting kernel function is the cosine similarity (or linear kernel) between vector pairs, i.e. linBoW. In line with (Shawe-Taylor and Cristianini, 2004b) we investigated the contribution of the Polynomial Kernel of degree 2, poly2BoW as it defines an implicit space where also feature pairs, i.e. words pairs, are considered. In the polarity detection tasks, several polarity lexicons have been exploited in order to have useful hints of the intrinsic polarity of words. We adopted MPQA Subjectivity Lexicon5 (Wilson et al., 2005) and NRC Emotion Lexicon (Mohammad and Turney, 2013): they are large collection of words provided with the underlying emotion they generally evoke. While the former considers only positive and negative sentiments, the lat</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004b. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
<author>Rebecca F Bruce</author>
<author>Thomas P O’Hara</author>
</authors>
<title>Development and use of a goldstandard data set for subjectivity classifications.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the ACL on Computational Linguistics,</booktitle>
<pages>246--253</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<marker>Wiebe, Bruce, O’Hara, 1999</marker>
<rawString>Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P. O’Hara. 1999. Development and use of a goldstandard data set for subjectivity classifications. In Proceedings of the 37th annual meeting of the ACL on Computational Linguistics, pages 246–253, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP</booktitle>
<location>Vancouver, CA.</location>
<contexts>
<context position="9696" citStr="Wilson et al., 2005" startWordPosition="1532" endWordPosition="1535">e. they represent a boolean indicator of the presence or not of a word in the text. The resulting kernel function is the cosine similarity (or linear kernel) between vector pairs, i.e. linBoW. In line with (Shawe-Taylor and Cristianini, 2004b) we investigated the contribution of the Polynomial Kernel of degree 2, poly2BoW as it defines an implicit space where also feature pairs, i.e. words pairs, are considered. In the polarity detection tasks, several polarity lexicons have been exploited in order to have useful hints of the intrinsic polarity of words. We adopted MPQA Subjectivity Lexicon5 (Wilson et al., 2005) and NRC Emotion Lexicon (Mohammad and Turney, 2013): they are large collection of words provided with the underlying emotion they generally evoke. While the former considers only positive and negative sentiments, the latter considers also eight primary emotions, organized in four opposing pairs, joy-sadness, angerfear, trust-disgust, and anticipation-surprise. We define the Lexicon Based (LB) vectors as follows. For each lexicon, let E = {e1, ..., e|E|} be the emotion vocabulary defined in it. Let w ∈ s be a word occurring in sentence s, with I(w, i) being the indicator function whose output </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), Vancouver, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32Nd Annual Meeting ofACL, ACL ’94,</booktitle>
<pages>133--138</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14052" citStr="Wu and Palmer, 1994" startWordPosition="2270" endWordPosition="2273">:n asp seeds σ refer to a set of evoked topics (i.e. aspect categories such as food) E1, ..., Et, we can define a t-dimensional vector ~to(s) called TopicOriented (TO) representation for s, whose features are: toi(s) = maxw∈s,Uk∈Ei sim(σk, w). The adopted word similarity function sim(·, ·) over ~so(s) and ~to(s) depends on the experiments. In the unconstrained setting, i.e. the Word Space Topic Oriented WSTO system, sim(·, ·) consists in the dot product over the Word Space representations ~σi and ~w. In the constrained case sim(·, ·) corresponds to the Wu &amp; Palmer similarity based on WordNet (Wu and Palmer, 1994), in the so called WordNet Seed Oriented WNSO system. The Radial Basis Function (RBF) kernel is then applied onto the resulting feature vectors ~to(s) and ~so(s) in the rbfWSTO and rbfWNSO, respectively. asp 3.3 Generalizing Syntactic Information In order to exploit the syntactic information, Tree Kernel functions proposed in (Collins and Duffy, 2001) are adopted. Tree kernels exploit syntactic similarity through the idea of convolutions among syntactic tree substructures. Any tree kernel evaluates the number of common substructures between two trees T1 and T2 without explicitly considering th</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32Nd Annual Meeting ofACL, ACL ’94, pages 133– 138, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>