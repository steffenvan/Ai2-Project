<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.947219">
Using Focus to Generate Complex and Simple Sentences
</title>
<author confidence="0.600059">
Marcia A. Derr
</author>
<affiliation confidence="0.463405">
AT&amp;T Bell Laboratories
</affiliation>
<address confidence="0.31224">
Murray Hill, NJ 07974 USA
</address>
<email confidence="0.256215">
and
</email>
<affiliation confidence="0.992032">
Department of Computer Science
Columbia University
</affiliation>
<sectionHeader confidence="0.897514" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879">
One problem for the generation of natural language
text is determining when to use a sequence of simple
sentences and when a single complex one is more
appropriate. In this paper, we show how focus of attention
is one factor that influences this decision and describe its
implementation in a system that generates explanations for
a student advisor expert system. The implementation uses
tests on functional information such as focus of attention
within the Prolog definite clause grammar formalism to
determine when to use complex sentences, resulting in an
efficient generator that has the same benefits as a
functional grammar system.
</bodyText>
<sectionHeader confidence="0.987147" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999873476190476">
Two problems in natural language generation are
deciding what to say and how to say it. This paper
addresses issues in the second of these tasks, that of
surface generation. Given a semantic representation of
what to say, a surface generator must construct an
appropriate surface structure taking into consideration a
wide variety of alternatives. When a generator is used to
produce text and not just single sentences, one decision it
must make is whether to use a sequence of simple
sentences or a single complex one. We show how focus of
attention can be used as the basis on which this decision
can be made.
A second goal of this paper is to introduce a
formalism for surface generation that uses aspects of Kay&apos;s
functional grammar (Kay, 1979) within a Prolog definite
clause grammar (Pereira and Warren, 1980). This
formalism was used to implement a surface generator that
makes choices about sentence complexity based on shifts in
focus of attention. The implementation was done as part
of an explanation facility for a student advisor expert
system being developed at Columbia University.
</bodyText>
<sectionHeader confidence="0.979729" genericHeader="method">
2. Language Generation Model
</sectionHeader>
<bodyText confidence="0.999715">
In our model of natural language generation, we
assume that the task of generating a response can be
divided into two stages: determining the semantic content
of the response and choosing a surface structure&apos;. One
</bodyText>
<author confidence="0.896853">
Kathleen R. McKeown
</author>
<affiliation confidence="0.998041">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.874577">
New York, NY 10027 USA
</address>
<bodyText confidence="0.999825615384616">
component makes decisions about which information to
include in the response and passes this information to a
surface generator. For example, an expert system
explanation facility may select part of the goal tree, a
particular goal and its antecedent subgoals, to explain a
behavior of the system. In the advisor system, the output
of this component consists of one or more logical
propositions where each proposition consists of a predicate
relating a group of arguments. The output includes
functional information, such as focus, and some syntactic
features, such as number and tense, for convenience.
Other information, such as the relationships between
propositions, is implicit in the organizational structure of
the output.
The output of the semantic component is passed on
to another component, the surface generator. The job of
generator is to use whatever syntactic and lexical
information is needed to translate the logical propositions
into English. The generator must be able to make choices
concerning various alternatives, such as whether to use
active or passive voice, or when to pronominalize. While
we have found the explanation facility for the advisor
system to be a valuable testbed for the surface generator,
the generator is an independent module that can be
transported to other domains by changing only the
vocabulary.
</bodyText>
<sectionHeader confidence="0.90285" genericHeader="method">
3. Choosing Surface Structure
</sectionHeader>
<bodyText confidence="0.999934818181818">
Given a set of propositions, one decision a surface
generator must make is whether to produce a simple
sentence for each proposition or whether to combine
propositions to form complex sentences. As an example,
consider propositions 1 and 2 below. These may be
expressed as two simple sentences (sequence 1) or as one
sentence containing a subordinate clause (sentence 2).
The sentences in I and 2 also show that a generation
system should be able to choose between definite and
indefinite reference and decide when to pronominalize.
Another decision is what syntactic structure to use, such as
</bodyText>
<footnote confidence="0.799671">
1. In order to concentrate on the task of surface generation, these two
stages are totally separate in our system, but we don&apos;t dispute the
value of interaction between the two (Appelt, 1983).
</footnote>
<page confidence="0.998565">
319
</page>
<bodyText confidence="0.859975142857143">
whether to use the active or the passive voice. Thus,
proposition 1 may be expressed as any of the sentences
shown in 3-5.
proposition 1: proposition 2:
predicate ==. give predicate need
protagonist .= John protagonist =. Mary
goal .= book goal book
</bodyText>
<listItem confidence="0.952662">
beneficiary =, Mary
1. John gave Mary a book.
Mary needed the book.
2. John gave Mary a book that she needed.
3. John gave Mary a book.
4. Mary was given a book by John.
5. A book was given to Mary by John.
</listItem>
<bodyText confidence="0.9997092">
Given that there are multiple ways to express the
same underlying message, how does a text generator
choose a surface structure that is appropriate? What are
some mechanisms for guiding the various choices?
Previous research has identified focus of attention as one
choice mechanism. McKeown (1982) demonstrated how
focus can be used to select sentence voice, and to
determine whether pronominalization is called for. In this
paper, we will show how focus can also be used as the
basis on which to combine propositions.
</bodyText>
<subsectionHeader confidence="0.999958">
3.1 Linguistic Background
</subsectionHeader>
<bodyText confidence="0.999075533333333">
Grosz (1977) distinguished between two types of
focus: global and immediate. Immediate focus refers to
how a speaker&apos;s center of attention shifts or remains
constant over two consecutive sentences, while global focus
describes the effect of a speaker&apos;s center of attention
throughout a sequence of discourse utterances on
succeeding utterances. In this paper, when we refer to
&amp;quot;focus of attention,&amp;quot; we are referring to immediate focus.
Phenomena reflecting immediate focus in text have
been studied by several linguists. Terminology and
definitions for these vary widely; some of the names that
have emerged include topic/comment, given/new, and
theme/rheme. These linguistic concepts describe
distinctions between functional roles elements play in a
sentence. In brief, they can be defined as follows:
</bodyText>
<listItem confidence="0.99571">
• Topic: Constituents in the sentence that
represent what the speaker is talking about.
Comment labels constituents that represent
what s/he has to say about that topic (see
Sgall, Hajicova, and Benesova, 1973; Lyons,
1968; Reinhart, 1981).
• Given: Information that is assumed by the
speaker to be derivable from context where
context may mean either the preceding
discourse or shared world knowledge. New
labels information that cannot be derived
(see Halliday, 1967; Prince, 1979; and
Chafe, 1976).
• Theme: The Prague School of linguists (see
Firbas, 1966; Firbas, 1974) define the theme
</listItem>
<bodyText confidence="0.99928511627907">
of a sentence as elements providing common
ground for the conversants. Rheme refers to
elements that function in conveying the
information to be imparted. In sentences
containing elements that are contextually
dependent, the contextually dependent
elements always function as theme. Thus,
the Prague School version is close to the
given/new distinction with the exception that
a sentence always contains a theme, while it
need not always contain given information2.
What is important here is that each of these
concepts, at one time or another, has been associated with
the selection of various syntactic structures. For example,
it has been suggested that new information and rheme
usually occur toward the end of a sentence (e.g., Halliday,
1967; Lyons, 1968; Sgall et al., 1973; Firbas, 1974). To
place this information in its proper position in the
sentence, structures other than the unmarked active
sentence may be required (for example, the passive).
Structures such as it-extraposition, there-insertion,
topicalization, and left-dislocation3 have been shown to
function in the introduction of new information into
discourse (Sidner, 1979; Prince, 1979), often with the
assumption that it will be talked about for a period of time
(Joshi and Weinstein, 1981). Pronominalization is another
linguistic device associated with these distinctions (see
Akmajian, 1973; Sidner, 1979).
One major difference between linguistic concepts
and immediate focus is that focusing describes an active
process on the part of speaker and listener. However, the
speaker&apos;s immediate focus influences the surfacing of each
of the linguistic concepts in the text. It influences topic
(and Halliday&apos;s theme) in that it specifies what the
speaker is focusing on (i.e., talking about) now. But it
also influences given information in that immediate focus
is linked to something that has been mentioned in the
previous utterance and thus, is already present in the
reader&apos;s consciousness. Since immediate focus is
intimately related to the linguistic definitions of functional
information, the influence of functional information on the
surface structure of the sentence can be extended to
immediate focus as well.
</bodyText>
<listItem confidence="0.997079090909091">
2. Halliday also discusses theme (Halliday, 1967), but he defines
theme as that which the speaker is talking about now, as opposed
to given, that which the speaker was talking about. Thus, his
notion of theme is closer to the concept of topic/comment
articulation. Furthermore, Halliday always ascribes the term
theme to the element occurring first in the sentence.
3. Some examples of these constructions are:
I. It was Sam who left the door open. (it-extraposition)
2. There are 3 blocks on the table. (there-insertion)
3. Sam, I like him. (left-dislocation)
4. Sam I like. (topicalization)
</listItem>
<page confidence="0.984232">
320
</page>
<subsectionHeader confidence="0.975722">
3.2 Focus and Complex Sentences
</subsectionHeader>
<bodyText confidence="0.999987153846154">
While previous research in both linguistics and
computer science has identified focus as a basis for
choosing sentence voice and for deciding when to
pronominalize, its influence on selecting complex sentence
structure over several simple sentences has for the most
part gone unnoticed. If a speaker wants to focus on a
single concept over a sequence of utterances, s/he may
need to present information about a second concept. In
such a case, a temporary digression must be made to the
second concept, but the speaker will immediately continue
to focus on the first. To signal that s/he is not shifting
focus, the speaker can use subordinate sentence structure
in describing the second concept.
Suppose that, in the previous example, focus is on
John in proposition 1 and book in proposition 2. If a third
proposition follows with focus returning to John, then the
surface generator can signal that the shift to book is only
temporary by combining the two propositions using
subordination as in sentence 2. A textual sequence
illustrating this possibility is shown in 6 below. On the
other hand, if the third proposition continues to focus on
book, then it is more appropriate to generate the first and
second propositions as two separate sentences as in
sentence 1 above. It may even be possible to combine the
second and third propositions using coordination as in the
textual sequence shown in 7 below.
</bodyText>
<listItem confidence="0.903644666666667">
6. John gave Mary a book that she needed.
He had seen it in the Columbia bookstore.
7. John gave Mary a book.
</listItem>
<bodyText confidence="0.9962914">
Mary needed the book and had been planning
on buying it herself.
Argument identity can also serve with focus as a
basis for combining propositions as shown in the example
below. In proposition 3, the values of predicate,
protagonist, and focus match the values of the
corresponding arguments in proposition 4. Thus, the two
propositions can be joined by deleting the protagonist and
predicate of the second proposition and using conjunction
to combine the two goals as in sentence 8. Note that if
the focus arguments were different, the propositions could
not be combined on this basis. Propositions 5 and 6, with
matching values for focus can also be combined by using
coordination and deleting the focused protagonist in the
second proposition (sentence 9).
</bodyText>
<equation confidence="0.4870739">
proposition 3: proposition 4:
predicate = buy predicate buy
protagonist = John protagonist = John
goal = book goal = cassette
focus = John focus = John
proposition 5: proposition 6:
predicate = read predicate = play
protagonist = Mary protagonist = Mary
goal = book goal = cassette
focus = Mary focus = Mary
</equation>
<listItem confidence="0.607117666666667">
8. John bought a book and a cassette.
9. Mary read the book and played the cassette.
4. A Formalism for Surface Generation
</listItem>
<bodyText confidence="0.999934222222222">
In this section we discuss the Prolog definite clause
grammar (DCG) formalism (Pereira and Warren, 1980)
and how it can be used for language generation, as well as
recognition. We then review the functional grammar
formalism (Kay, 1979) that has been used in other
generation systems (e.g., McKeown, 1982; Appelt, 1983).
Finally, we describe how aspects of a functional grammar
can be encoded in a DCG to produce a generator with the
best features of both formalisms.
</bodyText>
<subsectionHeader confidence="0.999146">
4.1 Definite Clause Grammars
</subsectionHeader>
<bodyText confidence="0.974425266666667">
The DCG formalism (Pereira and Warren, 1980) is
based on a method for for expressing grammar rules as
clauses of first-order predicate logic (Colmerauer, 1978;
Kowalski, 1980). As discussed by Pereira and Warren,
DCGs extend context-free grammars in several ways that
make them suitable for describing and analyzing natural
language. DCGs allow nonterminals to have arguments
that can be used to hold the string being analyzed, build
and pass structures that represent the parse tree, and carry
and test contextual information (such as number or
person). DCGs also allow extra conditions (enclosed within
brackets t and to be included in the rules, providing
another mechanism for encoding tests. A simple sentence
grammar is shown in Figure 1.
Viewed as a set of grammar rules, a DCG functions
as a declarative description of a language. Viewed as a set
of logic clauses, it functions as an executable program for
analyzing strings of the language. In particular, a DCG
can be executed by Prolog, a logic programming language
that implements an efficient resolution proof procedure
using a depth-first search strategy with backtracking and a
matching algorithm based on unification (Robinson, 1965).
To analyze a sentence, the sentence is encoded as an
argument to a Prolog goal. Prolog attempts to prove this
goal by matching it against the set of grammar clauses. If
the proof succeeds, the sentence is valid and a second
argument is instantiated to the parse tree structure. A
recognition goal and its resulting parse tree are shown in
Figure 1. More extensive examples can be found in
Pereira and Warren (1980).
</bodyText>
<equation confidence="0.666406863636364">
321.
sentence(s(NP,VP)) --&gt;
n_phrase(N P,N um),v_phrase(VP,N um).
n_phrase(np(Noun),Num) --&gt;
noun(Noun,Num).
noun(n(Root,Num),Num) --&gt;
[the], [Word], (is_noun(Root,Word,Num)1.
v_phrase(vp(Verb,NP),N um) --&gt;
verb(Verb,N um), n_phrase(NP,N2).
verb(v(Root,Num,Tense),Num) --&gt;
[Word], (is_verb(Root,Word,Num,Tense)].
is_noun(student,student,singular).
is_noun(student,students,plural).
is_noun(answer,answer,singular).
is_noun(answer,answers,plural).
is_verb(give,gives,singular,pres)
is_verb(give,give,plural,pres).
is_verb(give,gave,_,past).
Recognition Goal:
sentence(T,[the,student,gave,the,answer],[]).
Result:
T = s(np(n (student,singular)),
</equation>
<figure confidence="0.95362475">
vp(v(give,singular,past),
np(n(answer,singular))))
Generation Goal:
sentence (s (np(n (student,singular)),
vp(v(give,singular,past),
np(n(answer,singular)))),S,[1).
Result:
S [the,student,gave,the,answer]
</figure>
<figureCaption confidence="0.9902395">
Figure 1. Language recognition and generation using a
DCG
</figureCaption>
<bodyText confidence="0.834509714285714">
While Pereira and Warren concentrate on
describing the DCG formalism for language recognition
they also note its use for language generation, which is
similar to its use for language recognition. The main
difference is in the specification of input in the goal
arguments. In recognition, the input argument specifies a
surface string that is analyzed and returned as a parse tree
structure in another argument. In generation, the input
goal argument specifies a deep structure and a resulting
surface string is returned in the second argument. Though
not always practical, grammar rules can be designed to
work in both directions (as were the rules in Figure 1). A
generation goal and the sentence it produces are shown in
Figure I.
</bodyText>
<subsectionHeader confidence="0.993631">
4.2 Functional Grammars
</subsectionHeader>
<bodyText confidence="0.99370275">
Another formalism that has been used in previous
.generation systems (McKeown, 1982; Appelt, 1983) is the
functional grammar formalism (Kay, 1979)4. In a
functional grammar, functional information such as focus
and protagonist are treated in the same manner as
syntactic and grammatical information such as subject and
NP. By using functional information, input to the
generator is simplified as it need not completely specify all
the syntactic details. Instead, tests on functional
information, that select between alternative surface
structures, can be encoded in the grammar to arrive at the
complete syntactic structure from which the string is
generated. This formalism is consistent with the
assumption that is part of our generation model: that one
generation component produces a semantic specification
that feeds into another component for selecting the final
surface structure.
In the functional grammar formalism, both the
underlying message and the grammar are specified as
functional descriptions, lists of attribute-value pairs, that
are unified5 to produce a single complete surface structure
description. The text is then derived by linearizing the
complete surface structure description. As an example,
consider the proposition encoded as a functional
description below. When unified with a sentence grammar
that contains tests on focus to determine voice and order
constituents, sentence 12 is generated. If FOCUS were
&lt;GOAL&gt;, instead, sentence 13 would result.
</bodyText>
<equation confidence="0.998530142857143">
CAT = S
PRED = [LEX = give]
TENSE = PAST
PROT = [LEX = student]
GOAL = [LEX = answer]
BENEF = NONE
FOCUS = &lt;PROT&gt;
</equation>
<bodyText confidence="0.987835727272727">
12. The student gave the answer.
13. The answer was given by the student.
Previous implementations of functional grammars
have been concerned with the efficiency of the functional
grammar unification algorithm. Straightforward
implementations of the algorithm have proved too time-
consuming (McKeown, 1982) and efforts have been made
to alter the algorithm to improve efficiency (Appelt, 1983).
Efficiency continues to be a problem and a functional
grammar generator that can be used practically has as yet
to be developed.
</bodyText>
<subsectionHeader confidence="0.999749">
4.3 Combining the Formalisms
</subsectionHeader>
<bodyText confidence="0.999673">
We have implemented a surface generator based on
both the DCG formalism and the functional grammar
</bodyText>
<listItem confidence="0.835165142857143">
4. Functional grammar has also been referred to as unification
grammar (Appelt, 1983).
5. The functional grammar unification operation is similar to set
union. A description of the algorithm is given in Appelt (1983).
It is not to be confused with the unification process used in
resolution theorem proving, though a similarity has been noted by
Pereira and Warren (1983).
</listItem>
<page confidence="0.997321">
322
</page>
<bodyText confidence="0.964142022727273">
formalism. The result is a generator with the best features
of both grammars: simplification of input by using
functional information and efficiency of execution through
Prolog. Functional information, supplied as part of the
generation goal&apos;s input argument, is used by the grammar
rules to select an appropriate surface structure. The extra
conditions and context arguments allowed by the DCG
formalism provide the mechanism for testing functional
information.
Figure 2 shows a proposition encoded as the input
argument to a DCG goal. The proposition specifies, in
order, a predicate, protagonist, goal, beneficiary, and focus.
In this example, the focus argument is the same as the
protagonist. While the proposition also includes tense and
number information, less syntactic information is specified
compared to the input argument of the generation goal in
Figure 1. In particular, no information regarding
constituent order is specified. Also shown in Figure 2 are
some DCG rules for choosing syntactic structure based on
focus. The rules test for number agreement and tense, as
well. The sentence rule selects the focused argument as
the subject noun phrase. The vp rule determines that
focus is on the protagonist, selects active voice, and puts
the goal into a noun phrase followed by the beneficiary in
a to prepositional phrase. Thus, the order of constituents
in the generated sentence is not explicitly stated in the
input goal, but is determined during the generation
process. The sentence that results from the given
proposition is shown at the bottom of Figure 2.
Generation Goal:
sentence (prop (pred (give, past),
arg(student, singular),
arg(answer, singular),
arg(nil, ),
arg(student, singular)),S,ID.
Rules:
sentence(prop(Pred,Prot,Goal,Bene,Foc)) --&gt;
np(Foc),
vp(Pred,Prot,Goal,Bene,Foc).
vp(pred(Verb,Tense),Prot,Goal,Bene,Prot) --&gt;
Igetnum(Prot,Num)),
verb(Verb,Tense,Num,active),
np(Goal),
pp(to,Bene).
</bodyText>
<figure confidence="0.7124055">
Result:
S Ithe,student,gave,the,answer]
</figure>
<figureCaption confidence="0.928083">
Figure 2. DCG rules that use focus to select syntactic
structure
</figureCaption>
<sectionHeader confidence="0.515674" genericHeader="method">
5. Surface Generator Implementation
</sectionHeader>
<bodyText confidence="0.999993230769231">
A surface generator, with mechanisms for selecting
surface structure and, in particular, combining
propositions, was implemented as part of an explanation
facility for a student advisor expert system which is
implemented in Prolog. One component of the advisor
system, the planner, determines a student&apos;s schedule of
courses for a particular semester. An explanation of the
results of the planning process can be derived from a trace
of the Prolog goals that were invoked during planning
(Davis and Lenat, 1982). Each element of the trace is a
proposition that corresponds to a goal. The propositions
are organized hierarchically, with propositions toward the
top of the hierarchy corresponding to higher level goals.
Relationships between propositions are implicit in this
organization. For example, satisfying a higher level goal is
conditional on satisfying its subgoals. This provides a rich
testbed on which to experiment with techniques for
combining propositions. Because the expert system does
not yet automatically generate a trace of its execution, the
propositions that served as input to the surface generator
were hand-encoded from the results of several system
executions. In the current implementation, the grammar is
limited to handling input propositions structured as a list
of antecedents (subgoals) followed by a single consequence
(goal).
A grammar for automatically generating
explanations was implemented using the formalism
described in the previous section. The grammar encodes
several tests for combining propositions. Based on
temporary focus shift, it forms complex sentences using
subordination. Based on focus and argument identities it
uses coordination and identity deletion to combine
propositions. The grammar also includes tests on focus for
determining active/passive sentence voice, but does not
currently pronominalize on the basis of focus.
The generator determines that subordination is
necessary by checking whether focus shifts over a sequence
of three propositions. A simplified example of a DCG rule
foc_shift, that tests for this is shown in Figure 3. The
left-hand-side of this rule contains three input propositions
and an output proposition. Each proposition has five
arguments: verb, protagonist, goal, beneficiary, and focus.
If the first proposition focuses on Foci and mentions an
unfocused argument Goall, and if the second proposition
specifies Goall as its focus,6 but in the third proposition
the focus returns to Foci, then the first and second
propositions can be combined using subordination. The
combined propositions are returned as a single proposition
in the fourth argument; the third proposition is returned,
unchanged, in the third argument. Both can be tested for
further combination with other propositions. A sample
text produced using this rule is shown in 14 below.
</bodyText>
<footnote confidence="0.600765">
6. The right-hand-side of the rule contains a test to check that the
focus of the second proposition is different from the focus of the
first.
</footnote>
<page confidence="0.996714">
323
</page>
<table confidence="0.988489909090909">
fochift (
prop (Verb!, Protl, Goal!, Beni, Foc I ),
prop ( Verb2, Prot2, GoaI2, Ben2, Goall),
prop (Verb3, Prot3, GoaI3, Ben3, Foc 1 ),
prop (Verb!, Protl,
np(Goall, prop(Verb2, Prot2, GoaI2, Ben2, Goall)),
Beni, Foci))
--&gt;
(Goall \= Foc 1 ).
14. Assembly Language has a prerequisite that was taken.
Assembly Language does not conflict.
</table>
<figureCaption confidence="0.986405">
Figure 3. Combining propositions using subordination
</figureCaption>
<bodyText confidence="0.9997709375">
Other tests for combining propositions look for
identities among the arguments of propositions. Simplified
examples of these rules are id del and focilel in Figure 4.
According to id del, if the first and second proposition
differ only by the arguments Goall and Goal2, these
arguments are combined into one Goal and returned in the
third proposition. The result is a single sentence
containing a noun phrase conjunction as sentence 15
illustrates. The other rule, foc del, specifies that if two
propositions have the same focus, Foc, and in the second
proposition, the focus specifies the protagonist, then the
two propositions can form a coordinate sentence, deleting
the focused protagonist of the second proposition. Instead
of returning a proposition. foc_del in its right-hand-side,
invokes rules for generating a compound sentence. Sample
text generated by this rule is shown in 16.
</bodyText>
<table confidence="0.929500923076923">
id del (
prop (Verb, Prot, Goall, Ben, Foc),
prop (Verb, Prot, GoaI2, Ben, Foc),
prop (Verb, Prot, Goal, Ben, Foc))
--&gt;
(Goall Goa12, append (Goall, GoaI2, Goal)).
foc_del (
prop (Verb!, Protl, Goall, Beni, Foe),
prop (Verb2, Prot2, GoaI2, Ben2, Foc))
--&gt;
sentence (prop ( Verbl, Prot 1, Goall, Beni, Foe)),
(and),
verb_phrase (Verb2, Prot2, GoaI2, Ben2, Foc).
</table>
<listItem confidence="0.98370625">
15. Analysis of Algorithms requires Data Structures
and Discrete Math.
16. Introduction to Computer Programming does not
have prerequisites and does not conflict.
</listItem>
<figureCaption confidence="0.8244905">
Figure 4. Combining propositions using coordination and
identity deletion
</figureCaption>
<bodyText confidence="0.9927517">
The generator uses of the organization of the input
to show causal connectives. Recall that the input to the
generator is a set of propositions divided into a list of
antecedents and a single consequence that was derived by
the expert system. The generator can identify the
consequence for the reader by using a causal connective.
An explanation for why a particular course was not
scheduled is shown in 17. The antecedents are presented
in the first part of the explanation; the consequence,
introduced by therefore, follows.
</bodyText>
<listItem confidence="0.639639666666667">
17. Modeling and Analysis of Operating Systems requires
Fundamental Algorithms, Computability and Formal
Languages, and Probability.
</listItem>
<bodyText confidence="0.4388835">
Fundamental Algorithms and Computability and
Formal Languages were taken.
</bodyText>
<subsubsectionHeader confidence="0.397483">
Probability was not taken.
</subsubsectionHeader>
<bodyText confidence="0.884888">
Therefore, Modeling and Analysis of Operating
Systems was not added.
</bodyText>
<sectionHeader confidence="0.990663" genericHeader="method">
6. Related Work in Generation
</sectionHeader>
<bodyText confidence="0.979014285714286">
There are two basic classes of related work in
generation. The first class of systems makes use of
functional information in constructing the surface structure
of the text and has relatively little to say about how and
when to produce complex sentences. The second class of
work has addressed the problem of producing complex
sentences but does not incorporate functional information
as part of this decision making process.
Of the systems which make use of functional
information, three (Kay, 1979; McKeown, 1982; Appelt,
1983) have already been mentioned. Kay&apos;s work provides
the basis for McKeown&apos;s and Appelt&apos;s and emphasizes the
development of a formalism and grammar for generation
that allows for the use of functional information. Both
McKeown and Appelt make direct use of Kay&apos;s formalism,
with McKeown&apos;s emphasis being on the influence of focus
information on syntax and Appelt&apos;s emphasis being on the
development of a facility that allows interaction between
the grammar and an underlying planning component.
Nigel (Mann, 1983) is a fourth system that makes
use of functional information and is based on systemic
grammar (Hudson, 1974). A systemic grammar contains
choice points that query the environment to decide between
alternatives (the environment may include functional,
discourse, semantic, or contextual information). Mann&apos;s
emphasis, so far, has been on the development of the
system, on the development of a large linguistically
justified grammar, and on the influence of underlying
semantics on choices. The influence of functional
information on syntactic choice as well as the generation of
complex propositions are issues he has not yet addressed
within the systemic grammar framework.
Of previous systems that are able to combine simple
clauses to produce complex sentences, Davey&apos;s (1978) is
probably the most sophisticated. Davey&apos;s system is able to
recognize underlying semantic and rhetorical relations
between propositions to combine phrases using textual
connectives, also an important basis for combining
• propositions. His emphasis is on the identification of
contrastive relations that could be specified by connectives
such as although, but, or however. While Davey uses a
systemic grammar in his generator, he does not exploit the
</bodyText>
<page confidence="0.996736">
324
</page>
<bodyText confidence="0.999981333333333">
influence of functional information on generating complex
sentences.
Several other systems also touch on the generation
of complex sentences although it is not their main focus.
MUMBLE (McDonald, 1983) can produce complex
sentences if directed to do so. It is capable of ignoring
these directions when it is syntactically inappropriate to
produce complex sentences, but it can not decide when to
combine propositions. KDS (Mann, 1981) uses heuristics
that sometimes dictate that a complex sentence is
appropriate, but the heuristics are not based on general
linguistic principles. Ana (Kukich, 1983) can also
combine propositions, although, like Davey, the decision is
based on rhetorical relations rather than functional
information.
In sum, those systems that are capable of
generating complex sentences tend to rely on rhetorical,
semantic, or syntactic information to make their decisions.
Those systems that make use of functional information
have not investigated the general problem of choosing
between complex and simple sentences.
</bodyText>
<sectionHeader confidence="0.945417" genericHeader="method">
7. Future Directions
</sectionHeader>
<bodyText confidence="0.9967635">
The current implementation can be extended in a
variety of ways to produce better connected text.
Additional research is required to determine how and when
to use other textual connectives for combining propositions.
For example, the second and third sentences of 17 might
be better expressed as 18.
18. Although Fundamental Algorithms and
Computability and Formal Languages were taken,
Probability was not taken.
The question of how to organize propositions and
how to design the grammar to handle various organizations
deserves further attention. In the current implementation,
the grammar is limited to handling input propositions
structured as a list of antecedents and a single
consequence. If propositions were organized in trees rather
than lists, as in more complex explanations, the use of
additional connectives would be necessary.
The grammar can also be extended to include tests
for other kinds of surface choice such as definite/indefinite
reference, pronominalization, and lexical choice. As the
grammar grows larger and more complex, the task of
specifying rules becomes unwieldy. Further work is
needed to devise a method for automatically generating
DCG rules.
</bodyText>
<sectionHeader confidence="0.950808" genericHeader="conclusions">
8. Conclusions
</sectionHeader>
<bodyText confidence="0.999995692307692">
We have shown how focus of attention can be used
as the basis for a language generator to decide when to
combine propositions. By encoding tests on functional
information within the DCG formalism, we have
implemented an efficient generator that has the same
benefits as a functional grammar: input is simplified and
surface structure can be determined based on constituents&apos;
function within the sentence. In addition to producing
natural language explanations for the student advisor
application, this formalism provides a useful research tool
for experimenting with techniques for automatic text
generation. We plan to use it to investigate additional
criteria for determining surface choice.
</bodyText>
<sectionHeader confidence="0.990292" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999175">
We would like to thank Mitch Marcus for his
comments on an earlier version of this paper.
</bodyText>
<sectionHeader confidence="0.991496" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996444956521739">
Akmajian, A. (1973), &amp;quot;The role of focus in the
interpretation of anaphoric expressions,&amp;quot; In Anderson
and Kiparsky (Ed.), Festschriff for Morris Halle, Holt,
Rinehart, and Winston, New York, NY, 1973.
Appelt, Douglas E. (1983), &amp;quot;Telegram: a grammar
formalism for language planning,&amp;quot; Proceedings of the
21st Annual Meeting of the Association for
Computational Linguistics, 74-78, 1983.
Chafe, W. L. (1976), &amp;quot;Givenness, contrastiveness,
definiteness, subjects, topics, and points of view,&amp;quot; In Li,
C. N. (Ed.), Subject and Topic, Academic Press, New
York, 1976.
Colmerauer, A. (1978), &amp;quot;Metamorphosis grammars,&amp;quot; In
Bolc, L. (Ed.), Natural Language Communication with
Computers, Springer, Berlin, 1978.
Davey, Anthony. (1978), Discourse Production,
Edinburgh University Press, 1978.
Davis, Randall and Lenat, Douglas B. (1982),
Knowledge-Based Systems in Artificial Intelligence,
McGraw-Hill, New York, 1982.
Firbas, J. (1966), &amp;quot;On defining the theme in functional
sentence analysis,&amp;quot; Travaux Linguistiques de Prague 1,
University of Alabama Press, 1966.
Firbas, J. (1974), &amp;quot;Some aspects of the Czechoslovak
approach to problems of functional sentence
perspective,&amp;quot; Papers on Functional Sentence
Perspective, Academia, Prague, 1974.
Grosz, B. J. (1977), The representation and use of focus
in dialogue understanding. Technical note 151,
Stanford Research Institute, Menlo Park, CA, 1977.
Halliday, M. A. K. (1967), &amp;quot;Notes on transitivity and
theme in English,&amp;quot; Journal of Linguistics; 3, 1967.
Hudson, R.A. (1974), &amp;quot;Systemic generative grammar,&amp;quot;
Linguistics, 139, 5-42, 1974.
Joshi, A. and Weinstein, S. (1981), &amp;quot;Control of inference:
role of some aspects of discourse structure - centering,&amp;quot;
Proceedings of the 7th International Joint Conference
on Artificial Intelligence, 1981.
Kay, Martin. (1979), &amp;quot;Functional grammar,&amp;quot; Proceedings
of the 5th Annual Meeting of the Berkeley Linguistic
Society, 1979.
Kowalski, R. A. (1980), Logic for Problem Solving,
North Holland, New York, NY, 1980.
Kukich, Karen. (1983), &amp;quot;Design of a knowledge-based
report generator,&amp;quot; Proceedings of the 21st Annual
Meeting of the Association for Computational
</reference>
<page confidence="0.987351">
325
</page>
<reference confidence="0.999460761904762">
Linguistics, 145-150, 1983.
Lyons, J. (1968), Introduction to Theoretical Linguistics,
Cambridge University Press, London, 1968.
Mann, W.A. and Moore, J.A. (1981), &amp;quot;Computer
generation of multiparagraph English text,&amp;quot; American
Journal of Computational Linguistics, 7 (1), 17-29,
1981.
Mann, William C. (1983), &amp;quot;An overview of the Nigel text
generation grammar,&amp;quot; Proceedings of the 21st Annual
Meeting of the Association for Computational
Linguistics, 79-84, 1983.
McDonald, David D. (1983), &amp;quot;Description directed
control: its implications for natural language
generation,&amp;quot; Computers and Mathematics with
Applications, 9 (1), 111-129, 1983.
McKeown, Kathleen R. (1982), Generating Natural
Language Text in Response to Questions about
Database Structure, Ph.D. dissertation, University of
Pennsylvania, 1982.
Pereira, Fernando C. N. and Warren, David H. D.
(1980), &amp;quot;Definite clause grammars for language
analysis--a survey of the formalism and a comparison
with augmented transition networks,&amp;quot; Artificial
Intelligence, 13, 231-278, 1980.
Pereira, Fernando C. N. and Warren. David H. D.
(1983), &amp;quot;Parsing as deduction,&amp;quot; Proceedings of the
21 st Annual Meeting of the Association for
Computational Linguistics, 137-144, 1983.
Prince, E. (1979), &amp;quot;On the given/new distinction,&amp;quot; CLS,
15, 1979.
Reinhart, T. (1981), &amp;quot;Pragmatics and linguistics: an
analysis of sentence topics,&amp;quot; Philosophica, 1981.
Robinson, J. A. (1965), &amp;quot;A machine-oriented logic based
on the resolution principle,&amp;quot; Journal of the ACM, 12
(I), 23-41, 1965.
Sgall, P., Hajicova. E., and Benesova, E. (1973), Focus
and Generative Semantics, Scriptor Verlag, Democratic
Republic of Germany, 1973.
Sidner, C. L. (1979), Towards a Computation Theory of
Definite Anaphora Comprehension in English
Discourse, Ph.D. dissertation, MIT, Cambridge, MA,
1979.
</reference>
<page confidence="0.999113">
326
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.690859">
<title confidence="0.999991">Using Focus to Generate Complex and Simple Sentences</title>
<author confidence="0.999995">Marcia A Derr</author>
<affiliation confidence="0.999956">AT&amp;T Bell Laboratories</affiliation>
<address confidence="0.999731">Murray Hill, NJ 07974 USA</address>
<affiliation confidence="0.999896">Department of Computer Science Columbia University</affiliation>
<abstract confidence="0.999181307692308">One problem for the generation of natural language text is determining when to use a sequence of simple sentences and when a single complex one is more appropriate. In this paper, we show how focus of attention is one factor that influences this decision and describe its implementation in a system that generates explanations for a student advisor expert system. The implementation uses tests on functional information such as focus of attention within the Prolog definite clause grammar formalism to determine when to use complex sentences, resulting in an efficient generator that has the same benefits as a functional grammar system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Akmajian</author>
</authors>
<title>The role of focus in the interpretation of anaphoric expressions,&amp;quot;</title>
<date>1973</date>
<booktitle>In Anderson and Kiparsky (Ed.), Festschriff for</booktitle>
<location>Morris Halle, Holt, Rinehart, and Winston, New York, NY,</location>
<contexts>
<context position="8200" citStr="Akmajian, 1973" startWordPosition="1298" endWordPosition="1299">, 1968; Sgall et al., 1973; Firbas, 1974). To place this information in its proper position in the sentence, structures other than the unmarked active sentence may be required (for example, the passive). Structures such as it-extraposition, there-insertion, topicalization, and left-dislocation3 have been shown to function in the introduction of new information into discourse (Sidner, 1979; Prince, 1979), often with the assumption that it will be talked about for a period of time (Joshi and Weinstein, 1981). Pronominalization is another linguistic device associated with these distinctions (see Akmajian, 1973; Sidner, 1979). One major difference between linguistic concepts and immediate focus is that focusing describes an active process on the part of speaker and listener. However, the speaker&apos;s immediate focus influences the surfacing of each of the linguistic concepts in the text. It influences topic (and Halliday&apos;s theme) in that it specifies what the speaker is focusing on (i.e., talking about) now. But it also influences given information in that immediate focus is linked to something that has been mentioned in the previous utterance and thus, is already present in the reader&apos;s consciousness.</context>
</contexts>
<marker>Akmajian, 1973</marker>
<rawString>Akmajian, A. (1973), &amp;quot;The role of focus in the interpretation of anaphoric expressions,&amp;quot; In Anderson and Kiparsky (Ed.), Festschriff for Morris Halle, Holt, Rinehart, and Winston, New York, NY, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
</authors>
<title>Telegram: a grammar formalism for language planning,&amp;quot;</title>
<date>1983</date>
<booktitle>Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>74--78</pages>
<contexts>
<context position="4428" citStr="Appelt, 1983" startWordPosition="706" endWordPosition="707">orm complex sentences. As an example, consider propositions 1 and 2 below. These may be expressed as two simple sentences (sequence 1) or as one sentence containing a subordinate clause (sentence 2). The sentences in I and 2 also show that a generation system should be able to choose between definite and indefinite reference and decide when to pronominalize. Another decision is what syntactic structure to use, such as 1. In order to concentrate on the task of surface generation, these two stages are totally separate in our system, but we don&apos;t dispute the value of interaction between the two (Appelt, 1983). 319 whether to use the active or the passive voice. Thus, proposition 1 may be expressed as any of the sentences shown in 3-5. proposition 1: proposition 2: predicate ==. give predicate need protagonist .= John protagonist =. Mary goal .= book goal book beneficiary =, Mary 1. John gave Mary a book. Mary needed the book. 2. John gave Mary a book that she needed. 3. John gave Mary a book. 4. Mary was given a book by John. 5. A book was given to Mary by John. Given that there are multiple ways to express the same underlying message, how does a text generator choose a surface structure that is a</context>
<context position="12682" citStr="Appelt, 1983" startWordPosition="2036" endWordPosition="2037">n focus = John proposition 5: proposition 6: predicate = read predicate = play protagonist = Mary protagonist = Mary goal = book goal = cassette focus = Mary focus = Mary 8. John bought a book and a cassette. 9. Mary read the book and played the cassette. 4. A Formalism for Surface Generation In this section we discuss the Prolog definite clause grammar (DCG) formalism (Pereira and Warren, 1980) and how it can be used for language generation, as well as recognition. We then review the functional grammar formalism (Kay, 1979) that has been used in other generation systems (e.g., McKeown, 1982; Appelt, 1983). Finally, we describe how aspects of a functional grammar can be encoded in a DCG to produce a generator with the best features of both formalisms. 4.1 Definite Clause Grammars The DCG formalism (Pereira and Warren, 1980) is based on a method for for expressing grammar rules as clauses of first-order predicate logic (Colmerauer, 1978; Kowalski, 1980). As discussed by Pereira and Warren, DCGs extend context-free grammars in several ways that make them suitable for describing and analyzing natural language. DCGs allow nonterminals to have arguments that can be used to hold the string being anal</context>
<context position="16239" citStr="Appelt, 1983" startWordPosition="2527" endWordPosition="2528"> input in the goal arguments. In recognition, the input argument specifies a surface string that is analyzed and returned as a parse tree structure in another argument. In generation, the input goal argument specifies a deep structure and a resulting surface string is returned in the second argument. Though not always practical, grammar rules can be designed to work in both directions (as were the rules in Figure 1). A generation goal and the sentence it produces are shown in Figure I. 4.2 Functional Grammars Another formalism that has been used in previous .generation systems (McKeown, 1982; Appelt, 1983) is the functional grammar formalism (Kay, 1979)4. In a functional grammar, functional information such as focus and protagonist are treated in the same manner as syntactic and grammatical information such as subject and NP. By using functional information, input to the generator is simplified as it need not completely specify all the syntactic details. Instead, tests on functional information, that select between alternative surface structures, can be encoded in the grammar to arrive at the complete syntactic structure from which the string is generated. This formalism is consistent with the </context>
<context position="18127" citStr="Appelt, 1983" startWordPosition="2814" endWordPosition="2815">and order constituents, sentence 12 is generated. If FOCUS were &lt;GOAL&gt;, instead, sentence 13 would result. CAT = S PRED = [LEX = give] TENSE = PAST PROT = [LEX = student] GOAL = [LEX = answer] BENEF = NONE FOCUS = &lt;PROT&gt; 12. The student gave the answer. 13. The answer was given by the student. Previous implementations of functional grammars have been concerned with the efficiency of the functional grammar unification algorithm. Straightforward implementations of the algorithm have proved too timeconsuming (McKeown, 1982) and efforts have been made to alter the algorithm to improve efficiency (Appelt, 1983). Efficiency continues to be a problem and a functional grammar generator that can be used practically has as yet to be developed. 4.3 Combining the Formalisms We have implemented a surface generator based on both the DCG formalism and the functional grammar 4. Functional grammar has also been referred to as unification grammar (Appelt, 1983). 5. The functional grammar unification operation is similar to set union. A description of the algorithm is given in Appelt (1983). It is not to be confused with the unification process used in resolution theorem proving, though a similarity has been note</context>
<context position="27080" citStr="Appelt, 1983" startWordPosition="4167" endWordPosition="4168">eling and Analysis of Operating Systems was not added. 6. Related Work in Generation There are two basic classes of related work in generation. The first class of systems makes use of functional information in constructing the surface structure of the text and has relatively little to say about how and when to produce complex sentences. The second class of work has addressed the problem of producing complex sentences but does not incorporate functional information as part of this decision making process. Of the systems which make use of functional information, three (Kay, 1979; McKeown, 1982; Appelt, 1983) have already been mentioned. Kay&apos;s work provides the basis for McKeown&apos;s and Appelt&apos;s and emphasizes the development of a formalism and grammar for generation that allows for the use of functional information. Both McKeown and Appelt make direct use of Kay&apos;s formalism, with McKeown&apos;s emphasis being on the influence of focus information on syntax and Appelt&apos;s emphasis being on the development of a facility that allows interaction between the grammar and an underlying planning component. Nigel (Mann, 1983) is a fourth system that makes use of functional information and is based on systemic gram</context>
</contexts>
<marker>Appelt, 1983</marker>
<rawString>Appelt, Douglas E. (1983), &amp;quot;Telegram: a grammar formalism for language planning,&amp;quot; Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics, 74-78, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W L Chafe</author>
</authors>
<title>Givenness, contrastiveness, definiteness, subjects, topics, and points of view,&amp;quot; In</title>
<date>1976</date>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="6740" citStr="Chafe, 1976" startWordPosition="1078" endWordPosition="1079">be distinctions between functional roles elements play in a sentence. In brief, they can be defined as follows: • Topic: Constituents in the sentence that represent what the speaker is talking about. Comment labels constituents that represent what s/he has to say about that topic (see Sgall, Hajicova, and Benesova, 1973; Lyons, 1968; Reinhart, 1981). • Given: Information that is assumed by the speaker to be derivable from context where context may mean either the preceding discourse or shared world knowledge. New labels information that cannot be derived (see Halliday, 1967; Prince, 1979; and Chafe, 1976). • Theme: The Prague School of linguists (see Firbas, 1966; Firbas, 1974) define the theme of a sentence as elements providing common ground for the conversants. Rheme refers to elements that function in conveying the information to be imparted. In sentences containing elements that are contextually dependent, the contextually dependent elements always function as theme. Thus, the Prague School version is close to the given/new distinction with the exception that a sentence always contains a theme, while it need not always contain given information2. What is important here is that each of the</context>
</contexts>
<marker>Chafe, 1976</marker>
<rawString>Chafe, W. L. (1976), &amp;quot;Givenness, contrastiveness, definiteness, subjects, topics, and points of view,&amp;quot; In Li, C. N. (Ed.), Subject and Topic, Academic Press, New York, 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Colmerauer</author>
</authors>
<title>Metamorphosis grammars,&amp;quot;</title>
<date>1978</date>
<journal>In Bolc, L. (Ed.), Natural Language Communication with Computers,</journal>
<publisher>Springer,</publisher>
<location>Berlin,</location>
<contexts>
<context position="13018" citStr="Colmerauer, 1978" startWordPosition="2091" endWordPosition="2092">efinite clause grammar (DCG) formalism (Pereira and Warren, 1980) and how it can be used for language generation, as well as recognition. We then review the functional grammar formalism (Kay, 1979) that has been used in other generation systems (e.g., McKeown, 1982; Appelt, 1983). Finally, we describe how aspects of a functional grammar can be encoded in a DCG to produce a generator with the best features of both formalisms. 4.1 Definite Clause Grammars The DCG formalism (Pereira and Warren, 1980) is based on a method for for expressing grammar rules as clauses of first-order predicate logic (Colmerauer, 1978; Kowalski, 1980). As discussed by Pereira and Warren, DCGs extend context-free grammars in several ways that make them suitable for describing and analyzing natural language. DCGs allow nonterminals to have arguments that can be used to hold the string being analyzed, build and pass structures that represent the parse tree, and carry and test contextual information (such as number or person). DCGs also allow extra conditions (enclosed within brackets t and to be included in the rules, providing another mechanism for encoding tests. A simple sentence grammar is shown in Figure 1. Viewed as a s</context>
</contexts>
<marker>Colmerauer, 1978</marker>
<rawString>Colmerauer, A. (1978), &amp;quot;Metamorphosis grammars,&amp;quot; In Bolc, L. (Ed.), Natural Language Communication with Computers, Springer, Berlin, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Davey</author>
</authors>
<date>1978</date>
<booktitle>Discourse Production,</booktitle>
<publisher>University Press,</publisher>
<location>Edinburgh</location>
<marker>Davey, 1978</marker>
<rawString>Davey, Anthony. (1978), Discourse Production, Edinburgh University Press, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randall Davis</author>
<author>Douglas B Lenat</author>
</authors>
<date>1982</date>
<booktitle>Knowledge-Based Systems in Artificial Intelligence,</booktitle>
<location>McGraw-Hill, New York,</location>
<contexts>
<context position="21341" citStr="Davis and Lenat, 1982" startWordPosition="3286" endWordPosition="3289">e,answer] Figure 2. DCG rules that use focus to select syntactic structure 5. Surface Generator Implementation A surface generator, with mechanisms for selecting surface structure and, in particular, combining propositions, was implemented as part of an explanation facility for a student advisor expert system which is implemented in Prolog. One component of the advisor system, the planner, determines a student&apos;s schedule of courses for a particular semester. An explanation of the results of the planning process can be derived from a trace of the Prolog goals that were invoked during planning (Davis and Lenat, 1982). Each element of the trace is a proposition that corresponds to a goal. The propositions are organized hierarchically, with propositions toward the top of the hierarchy corresponding to higher level goals. Relationships between propositions are implicit in this organization. For example, satisfying a higher level goal is conditional on satisfying its subgoals. This provides a rich testbed on which to experiment with techniques for combining propositions. Because the expert system does not yet automatically generate a trace of its execution, the propositions that served as input to the surface</context>
</contexts>
<marker>Davis, Lenat, 1982</marker>
<rawString>Davis, Randall and Lenat, Douglas B. (1982), Knowledge-Based Systems in Artificial Intelligence, McGraw-Hill, New York, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Firbas</author>
</authors>
<title>On defining the theme in functional sentence analysis,&amp;quot; Travaux Linguistiques de Prague 1,</title>
<date>1966</date>
<publisher>Press,</publisher>
<institution>University of Alabama</institution>
<contexts>
<context position="6799" citStr="Firbas, 1966" startWordPosition="1088" endWordPosition="1089"> sentence. In brief, they can be defined as follows: • Topic: Constituents in the sentence that represent what the speaker is talking about. Comment labels constituents that represent what s/he has to say about that topic (see Sgall, Hajicova, and Benesova, 1973; Lyons, 1968; Reinhart, 1981). • Given: Information that is assumed by the speaker to be derivable from context where context may mean either the preceding discourse or shared world knowledge. New labels information that cannot be derived (see Halliday, 1967; Prince, 1979; and Chafe, 1976). • Theme: The Prague School of linguists (see Firbas, 1966; Firbas, 1974) define the theme of a sentence as elements providing common ground for the conversants. Rheme refers to elements that function in conveying the information to be imparted. In sentences containing elements that are contextually dependent, the contextually dependent elements always function as theme. Thus, the Prague School version is close to the given/new distinction with the exception that a sentence always contains a theme, while it need not always contain given information2. What is important here is that each of these concepts, at one time or another, has been associated wi</context>
</contexts>
<marker>Firbas, 1966</marker>
<rawString>Firbas, J. (1966), &amp;quot;On defining the theme in functional sentence analysis,&amp;quot; Travaux Linguistiques de Prague 1, University of Alabama Press, 1966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Firbas</author>
</authors>
<title>Some aspects of the Czechoslovak approach to problems of functional sentence perspective,&amp;quot; Papers on Functional Sentence Perspective,</title>
<date>1974</date>
<location>Academia, Prague,</location>
<contexts>
<context position="6814" citStr="Firbas, 1974" startWordPosition="1090" endWordPosition="1091">brief, they can be defined as follows: • Topic: Constituents in the sentence that represent what the speaker is talking about. Comment labels constituents that represent what s/he has to say about that topic (see Sgall, Hajicova, and Benesova, 1973; Lyons, 1968; Reinhart, 1981). • Given: Information that is assumed by the speaker to be derivable from context where context may mean either the preceding discourse or shared world knowledge. New labels information that cannot be derived (see Halliday, 1967; Prince, 1979; and Chafe, 1976). • Theme: The Prague School of linguists (see Firbas, 1966; Firbas, 1974) define the theme of a sentence as elements providing common ground for the conversants. Rheme refers to elements that function in conveying the information to be imparted. In sentences containing elements that are contextually dependent, the contextually dependent elements always function as theme. Thus, the Prague School version is close to the given/new distinction with the exception that a sentence always contains a theme, while it need not always contain given information2. What is important here is that each of these concepts, at one time or another, has been associated with the selectio</context>
</contexts>
<marker>Firbas, 1974</marker>
<rawString>Firbas, J. (1974), &amp;quot;Some aspects of the Czechoslovak approach to problems of functional sentence perspective,&amp;quot; Papers on Functional Sentence Perspective, Academia, Prague, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
</authors>
<title>The representation and use of focus in dialogue understanding.</title>
<date>1977</date>
<booktitle>Technical note 151, Stanford Research Institute,</booktitle>
<location>Menlo Park, CA,</location>
<contexts>
<context position="5449" citStr="Grosz (1977)" startWordPosition="884" endWordPosition="885"> a book by John. 5. A book was given to Mary by John. Given that there are multiple ways to express the same underlying message, how does a text generator choose a surface structure that is appropriate? What are some mechanisms for guiding the various choices? Previous research has identified focus of attention as one choice mechanism. McKeown (1982) demonstrated how focus can be used to select sentence voice, and to determine whether pronominalization is called for. In this paper, we will show how focus can also be used as the basis on which to combine propositions. 3.1 Linguistic Background Grosz (1977) distinguished between two types of focus: global and immediate. Immediate focus refers to how a speaker&apos;s center of attention shifts or remains constant over two consecutive sentences, while global focus describes the effect of a speaker&apos;s center of attention throughout a sequence of discourse utterances on succeeding utterances. In this paper, when we refer to &amp;quot;focus of attention,&amp;quot; we are referring to immediate focus. Phenomena reflecting immediate focus in text have been studied by several linguists. Terminology and definitions for these vary widely; some of the names that have emerged incl</context>
</contexts>
<marker>Grosz, 1977</marker>
<rawString>Grosz, B. J. (1977), The representation and use of focus in dialogue understanding. Technical note 151, Stanford Research Institute, Menlo Park, CA, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>Notes on transitivity and theme in English,&amp;quot;</title>
<date>1967</date>
<journal>Journal of Linguistics;</journal>
<volume>3</volume>
<contexts>
<context position="6708" citStr="Halliday, 1967" startWordPosition="1073" endWordPosition="1074">. These linguistic concepts describe distinctions between functional roles elements play in a sentence. In brief, they can be defined as follows: • Topic: Constituents in the sentence that represent what the speaker is talking about. Comment labels constituents that represent what s/he has to say about that topic (see Sgall, Hajicova, and Benesova, 1973; Lyons, 1968; Reinhart, 1981). • Given: Information that is assumed by the speaker to be derivable from context where context may mean either the preceding discourse or shared world knowledge. New labels information that cannot be derived (see Halliday, 1967; Prince, 1979; and Chafe, 1976). • Theme: The Prague School of linguists (see Firbas, 1966; Firbas, 1974) define the theme of a sentence as elements providing common ground for the conversants. Rheme refers to elements that function in conveying the information to be imparted. In sentences containing elements that are contextually dependent, the contextually dependent elements always function as theme. Thus, the Prague School version is close to the given/new distinction with the exception that a sentence always contains a theme, while it need not always contain given information2. What is im</context>
<context position="9076" citStr="Halliday, 1967" startWordPosition="1431" endWordPosition="1432">cepts in the text. It influences topic (and Halliday&apos;s theme) in that it specifies what the speaker is focusing on (i.e., talking about) now. But it also influences given information in that immediate focus is linked to something that has been mentioned in the previous utterance and thus, is already present in the reader&apos;s consciousness. Since immediate focus is intimately related to the linguistic definitions of functional information, the influence of functional information on the surface structure of the sentence can be extended to immediate focus as well. 2. Halliday also discusses theme (Halliday, 1967), but he defines theme as that which the speaker is talking about now, as opposed to given, that which the speaker was talking about. Thus, his notion of theme is closer to the concept of topic/comment articulation. Furthermore, Halliday always ascribes the term theme to the element occurring first in the sentence. 3. Some examples of these constructions are: I. It was Sam who left the door open. (it-extraposition) 2. There are 3 blocks on the table. (there-insertion) 3. Sam, I like him. (left-dislocation) 4. Sam I like. (topicalization) 320 3.2 Focus and Complex Sentences While previous resea</context>
</contexts>
<marker>Halliday, 1967</marker>
<rawString>Halliday, M. A. K. (1967), &amp;quot;Notes on transitivity and theme in English,&amp;quot; Journal of Linguistics; 3, 1967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Hudson</author>
</authors>
<title>Systemic generative grammar,&amp;quot;</title>
<date>1974</date>
<journal>Linguistics,</journal>
<volume>139</volume>
<pages>5--42</pages>
<contexts>
<context position="27698" citStr="Hudson, 1974" startWordPosition="4263" endWordPosition="4264"> already been mentioned. Kay&apos;s work provides the basis for McKeown&apos;s and Appelt&apos;s and emphasizes the development of a formalism and grammar for generation that allows for the use of functional information. Both McKeown and Appelt make direct use of Kay&apos;s formalism, with McKeown&apos;s emphasis being on the influence of focus information on syntax and Appelt&apos;s emphasis being on the development of a facility that allows interaction between the grammar and an underlying planning component. Nigel (Mann, 1983) is a fourth system that makes use of functional information and is based on systemic grammar (Hudson, 1974). A systemic grammar contains choice points that query the environment to decide between alternatives (the environment may include functional, discourse, semantic, or contextual information). Mann&apos;s emphasis, so far, has been on the development of the system, on the development of a large linguistically justified grammar, and on the influence of underlying semantics on choices. The influence of functional information on syntactic choice as well as the generation of complex propositions are issues he has not yet addressed within the systemic grammar framework. Of previous systems that are able </context>
</contexts>
<marker>Hudson, 1974</marker>
<rawString>Hudson, R.A. (1974), &amp;quot;Systemic generative grammar,&amp;quot; Linguistics, 139, 5-42, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Control of inference: role of some aspects of discourse structure - centering,&amp;quot;</title>
<date>1981</date>
<booktitle>Proceedings of the 7th International Joint Conference on Artificial Intelligence,</booktitle>
<contexts>
<context position="8097" citStr="Joshi and Weinstein, 1981" startWordPosition="1284" endWordPosition="1287">en suggested that new information and rheme usually occur toward the end of a sentence (e.g., Halliday, 1967; Lyons, 1968; Sgall et al., 1973; Firbas, 1974). To place this information in its proper position in the sentence, structures other than the unmarked active sentence may be required (for example, the passive). Structures such as it-extraposition, there-insertion, topicalization, and left-dislocation3 have been shown to function in the introduction of new information into discourse (Sidner, 1979; Prince, 1979), often with the assumption that it will be talked about for a period of time (Joshi and Weinstein, 1981). Pronominalization is another linguistic device associated with these distinctions (see Akmajian, 1973; Sidner, 1979). One major difference between linguistic concepts and immediate focus is that focusing describes an active process on the part of speaker and listener. However, the speaker&apos;s immediate focus influences the surfacing of each of the linguistic concepts in the text. It influences topic (and Halliday&apos;s theme) in that it specifies what the speaker is focusing on (i.e., talking about) now. But it also influences given information in that immediate focus is linked to something that h</context>
</contexts>
<marker>Joshi, Weinstein, 1981</marker>
<rawString>Joshi, A. and Weinstein, S. (1981), &amp;quot;Control of inference: role of some aspects of discourse structure - centering,&amp;quot; Proceedings of the 7th International Joint Conference on Artificial Intelligence, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Functional grammar,&amp;quot;</title>
<date>1979</date>
<booktitle>Proceedings of the 5th Annual Meeting of the Berkeley Linguistic Society,</booktitle>
<contexts>
<context position="1583" citStr="Kay, 1979" startWordPosition="257" endWordPosition="258"> tasks, that of surface generation. Given a semantic representation of what to say, a surface generator must construct an appropriate surface structure taking into consideration a wide variety of alternatives. When a generator is used to produce text and not just single sentences, one decision it must make is whether to use a sequence of simple sentences or a single complex one. We show how focus of attention can be used as the basis on which this decision can be made. A second goal of this paper is to introduce a formalism for surface generation that uses aspects of Kay&apos;s functional grammar (Kay, 1979) within a Prolog definite clause grammar (Pereira and Warren, 1980). This formalism was used to implement a surface generator that makes choices about sentence complexity based on shifts in focus of attention. The implementation was done as part of an explanation facility for a student advisor expert system being developed at Columbia University. 2. Language Generation Model In our model of natural language generation, we assume that the task of generating a response can be divided into two stages: determining the semantic content of the response and choosing a surface structure&apos;. One Kathleen</context>
<context position="12599" citStr="Kay, 1979" startWordPosition="2023" endWordPosition="2024">uy protagonist = John protagonist = John goal = book goal = cassette focus = John focus = John proposition 5: proposition 6: predicate = read predicate = play protagonist = Mary protagonist = Mary goal = book goal = cassette focus = Mary focus = Mary 8. John bought a book and a cassette. 9. Mary read the book and played the cassette. 4. A Formalism for Surface Generation In this section we discuss the Prolog definite clause grammar (DCG) formalism (Pereira and Warren, 1980) and how it can be used for language generation, as well as recognition. We then review the functional grammar formalism (Kay, 1979) that has been used in other generation systems (e.g., McKeown, 1982; Appelt, 1983). Finally, we describe how aspects of a functional grammar can be encoded in a DCG to produce a generator with the best features of both formalisms. 4.1 Definite Clause Grammars The DCG formalism (Pereira and Warren, 1980) is based on a method for for expressing grammar rules as clauses of first-order predicate logic (Colmerauer, 1978; Kowalski, 1980). As discussed by Pereira and Warren, DCGs extend context-free grammars in several ways that make them suitable for describing and analyzing natural language. DCGs </context>
<context position="16287" citStr="Kay, 1979" startWordPosition="2534" endWordPosition="2535">nput argument specifies a surface string that is analyzed and returned as a parse tree structure in another argument. In generation, the input goal argument specifies a deep structure and a resulting surface string is returned in the second argument. Though not always practical, grammar rules can be designed to work in both directions (as were the rules in Figure 1). A generation goal and the sentence it produces are shown in Figure I. 4.2 Functional Grammars Another formalism that has been used in previous .generation systems (McKeown, 1982; Appelt, 1983) is the functional grammar formalism (Kay, 1979)4. In a functional grammar, functional information such as focus and protagonist are treated in the same manner as syntactic and grammatical information such as subject and NP. By using functional information, input to the generator is simplified as it need not completely specify all the syntactic details. Instead, tests on functional information, that select between alternative surface structures, can be encoded in the grammar to arrive at the complete syntactic structure from which the string is generated. This formalism is consistent with the assumption that is part of our generation model:</context>
<context position="27050" citStr="Kay, 1979" startWordPosition="4163" endWordPosition="4164"> not taken. Therefore, Modeling and Analysis of Operating Systems was not added. 6. Related Work in Generation There are two basic classes of related work in generation. The first class of systems makes use of functional information in constructing the surface structure of the text and has relatively little to say about how and when to produce complex sentences. The second class of work has addressed the problem of producing complex sentences but does not incorporate functional information as part of this decision making process. Of the systems which make use of functional information, three (Kay, 1979; McKeown, 1982; Appelt, 1983) have already been mentioned. Kay&apos;s work provides the basis for McKeown&apos;s and Appelt&apos;s and emphasizes the development of a formalism and grammar for generation that allows for the use of functional information. Both McKeown and Appelt make direct use of Kay&apos;s formalism, with McKeown&apos;s emphasis being on the influence of focus information on syntax and Appelt&apos;s emphasis being on the development of a facility that allows interaction between the grammar and an underlying planning component. Nigel (Mann, 1983) is a fourth system that makes use of functional information</context>
</contexts>
<marker>Kay, 1979</marker>
<rawString>Kay, Martin. (1979), &amp;quot;Functional grammar,&amp;quot; Proceedings of the 5th Annual Meeting of the Berkeley Linguistic Society, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Kowalski</author>
</authors>
<title>Logic for Problem Solving,</title>
<date>1980</date>
<location>North Holland, New York, NY,</location>
<contexts>
<context position="13035" citStr="Kowalski, 1980" startWordPosition="2093" endWordPosition="2094">mmar (DCG) formalism (Pereira and Warren, 1980) and how it can be used for language generation, as well as recognition. We then review the functional grammar formalism (Kay, 1979) that has been used in other generation systems (e.g., McKeown, 1982; Appelt, 1983). Finally, we describe how aspects of a functional grammar can be encoded in a DCG to produce a generator with the best features of both formalisms. 4.1 Definite Clause Grammars The DCG formalism (Pereira and Warren, 1980) is based on a method for for expressing grammar rules as clauses of first-order predicate logic (Colmerauer, 1978; Kowalski, 1980). As discussed by Pereira and Warren, DCGs extend context-free grammars in several ways that make them suitable for describing and analyzing natural language. DCGs allow nonterminals to have arguments that can be used to hold the string being analyzed, build and pass structures that represent the parse tree, and carry and test contextual information (such as number or person). DCGs also allow extra conditions (enclosed within brackets t and to be included in the rules, providing another mechanism for encoding tests. A simple sentence grammar is shown in Figure 1. Viewed as a set of grammar rul</context>
</contexts>
<marker>Kowalski, 1980</marker>
<rawString>Kowalski, R. A. (1980), Logic for Problem Solving, North Holland, New York, NY, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Design of a knowledge-based report generator,&amp;quot;</title>
<date>1983</date>
<booktitle>Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>145--150</pages>
<contexts>
<context position="29425" citStr="Kukich, 1983" startWordPosition="4519" endWordPosition="4520">ploit the 324 influence of functional information on generating complex sentences. Several other systems also touch on the generation of complex sentences although it is not their main focus. MUMBLE (McDonald, 1983) can produce complex sentences if directed to do so. It is capable of ignoring these directions when it is syntactically inappropriate to produce complex sentences, but it can not decide when to combine propositions. KDS (Mann, 1981) uses heuristics that sometimes dictate that a complex sentence is appropriate, but the heuristics are not based on general linguistic principles. Ana (Kukich, 1983) can also combine propositions, although, like Davey, the decision is based on rhetorical relations rather than functional information. In sum, those systems that are capable of generating complex sentences tend to rely on rhetorical, semantic, or syntactic information to make their decisions. Those systems that make use of functional information have not investigated the general problem of choosing between complex and simple sentences. 7. Future Directions The current implementation can be extended in a variety of ways to produce better connected text. Additional research is required to deter</context>
</contexts>
<marker>Kukich, 1983</marker>
<rawString>Kukich, Karen. (1983), &amp;quot;Design of a knowledge-based report generator,&amp;quot; Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics, 145-150, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lyons</author>
</authors>
<title>Introduction to Theoretical Linguistics,</title>
<date>1968</date>
<publisher>Cambridge University Press,</publisher>
<location>London,</location>
<contexts>
<context position="6462" citStr="Lyons, 1968" startWordPosition="1035" endWordPosition="1036">g to immediate focus. Phenomena reflecting immediate focus in text have been studied by several linguists. Terminology and definitions for these vary widely; some of the names that have emerged include topic/comment, given/new, and theme/rheme. These linguistic concepts describe distinctions between functional roles elements play in a sentence. In brief, they can be defined as follows: • Topic: Constituents in the sentence that represent what the speaker is talking about. Comment labels constituents that represent what s/he has to say about that topic (see Sgall, Hajicova, and Benesova, 1973; Lyons, 1968; Reinhart, 1981). • Given: Information that is assumed by the speaker to be derivable from context where context may mean either the preceding discourse or shared world knowledge. New labels information that cannot be derived (see Halliday, 1967; Prince, 1979; and Chafe, 1976). • Theme: The Prague School of linguists (see Firbas, 1966; Firbas, 1974) define the theme of a sentence as elements providing common ground for the conversants. Rheme refers to elements that function in conveying the information to be imparted. In sentences containing elements that are contextually dependent, the conte</context>
</contexts>
<marker>Lyons, 1968</marker>
<rawString>Lyons, J. (1968), Introduction to Theoretical Linguistics, Cambridge University Press, London, 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Mann</author>
<author>J A Moore</author>
</authors>
<title>Computer generation of multiparagraph English text,&amp;quot;</title>
<date>1981</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>7</volume>
<issue>1</issue>
<pages>17--29</pages>
<marker>Mann, Moore, 1981</marker>
<rawString>Mann, W.A. and Moore, J.A. (1981), &amp;quot;Computer generation of multiparagraph English text,&amp;quot; American Journal of Computational Linguistics, 7 (1), 17-29, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
</authors>
<title>An overview of the Nigel text generation grammar,&amp;quot;</title>
<date>1983</date>
<booktitle>Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>79--84</pages>
<contexts>
<context position="27590" citStr="Mann, 1983" startWordPosition="4245" endWordPosition="4246"> the systems which make use of functional information, three (Kay, 1979; McKeown, 1982; Appelt, 1983) have already been mentioned. Kay&apos;s work provides the basis for McKeown&apos;s and Appelt&apos;s and emphasizes the development of a formalism and grammar for generation that allows for the use of functional information. Both McKeown and Appelt make direct use of Kay&apos;s formalism, with McKeown&apos;s emphasis being on the influence of focus information on syntax and Appelt&apos;s emphasis being on the development of a facility that allows interaction between the grammar and an underlying planning component. Nigel (Mann, 1983) is a fourth system that makes use of functional information and is based on systemic grammar (Hudson, 1974). A systemic grammar contains choice points that query the environment to decide between alternatives (the environment may include functional, discourse, semantic, or contextual information). Mann&apos;s emphasis, so far, has been on the development of the system, on the development of a large linguistically justified grammar, and on the influence of underlying semantics on choices. The influence of functional information on syntactic choice as well as the generation of complex propositions a</context>
</contexts>
<marker>Mann, 1983</marker>
<rawString>Mann, William C. (1983), &amp;quot;An overview of the Nigel text generation grammar,&amp;quot; Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics, 79-84, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D McDonald</author>
</authors>
<title>Description directed control: its implications for natural language generation,&amp;quot;</title>
<date>1983</date>
<journal>Computers and Mathematics with Applications,</journal>
<volume>9</volume>
<issue>1</issue>
<pages>111--129</pages>
<contexts>
<context position="29027" citStr="McDonald, 1983" startWordPosition="4458" endWordPosition="4459"> system is able to recognize underlying semantic and rhetorical relations between propositions to combine phrases using textual connectives, also an important basis for combining • propositions. His emphasis is on the identification of contrastive relations that could be specified by connectives such as although, but, or however. While Davey uses a systemic grammar in his generator, he does not exploit the 324 influence of functional information on generating complex sentences. Several other systems also touch on the generation of complex sentences although it is not their main focus. MUMBLE (McDonald, 1983) can produce complex sentences if directed to do so. It is capable of ignoring these directions when it is syntactically inappropriate to produce complex sentences, but it can not decide when to combine propositions. KDS (Mann, 1981) uses heuristics that sometimes dictate that a complex sentence is appropriate, but the heuristics are not based on general linguistic principles. Ana (Kukich, 1983) can also combine propositions, although, like Davey, the decision is based on rhetorical relations rather than functional information. In sum, those systems that are capable of generating complex sente</context>
</contexts>
<marker>McDonald, 1983</marker>
<rawString>McDonald, David D. (1983), &amp;quot;Description directed control: its implications for natural language generation,&amp;quot; Computers and Mathematics with Applications, 9 (1), 111-129, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Generating Natural Language Text in Response to Questions about Database Structure,</title>
<date>1982</date>
<institution>University of Pennsylvania,</institution>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="5189" citStr="McKeown (1982)" startWordPosition="841" endWordPosition="842">position 2: predicate ==. give predicate need protagonist .= John protagonist =. Mary goal .= book goal book beneficiary =, Mary 1. John gave Mary a book. Mary needed the book. 2. John gave Mary a book that she needed. 3. John gave Mary a book. 4. Mary was given a book by John. 5. A book was given to Mary by John. Given that there are multiple ways to express the same underlying message, how does a text generator choose a surface structure that is appropriate? What are some mechanisms for guiding the various choices? Previous research has identified focus of attention as one choice mechanism. McKeown (1982) demonstrated how focus can be used to select sentence voice, and to determine whether pronominalization is called for. In this paper, we will show how focus can also be used as the basis on which to combine propositions. 3.1 Linguistic Background Grosz (1977) distinguished between two types of focus: global and immediate. Immediate focus refers to how a speaker&apos;s center of attention shifts or remains constant over two consecutive sentences, while global focus describes the effect of a speaker&apos;s center of attention throughout a sequence of discourse utterances on succeeding utterances. In this</context>
<context position="12667" citStr="McKeown, 1982" startWordPosition="2034" endWordPosition="2035">tte focus = John focus = John proposition 5: proposition 6: predicate = read predicate = play protagonist = Mary protagonist = Mary goal = book goal = cassette focus = Mary focus = Mary 8. John bought a book and a cassette. 9. Mary read the book and played the cassette. 4. A Formalism for Surface Generation In this section we discuss the Prolog definite clause grammar (DCG) formalism (Pereira and Warren, 1980) and how it can be used for language generation, as well as recognition. We then review the functional grammar formalism (Kay, 1979) that has been used in other generation systems (e.g., McKeown, 1982; Appelt, 1983). Finally, we describe how aspects of a functional grammar can be encoded in a DCG to produce a generator with the best features of both formalisms. 4.1 Definite Clause Grammars The DCG formalism (Pereira and Warren, 1980) is based on a method for for expressing grammar rules as clauses of first-order predicate logic (Colmerauer, 1978; Kowalski, 1980). As discussed by Pereira and Warren, DCGs extend context-free grammars in several ways that make them suitable for describing and analyzing natural language. DCGs allow nonterminals to have arguments that can be used to hold the st</context>
<context position="16224" citStr="McKeown, 1982" startWordPosition="2525" endWordPosition="2526">pecification of input in the goal arguments. In recognition, the input argument specifies a surface string that is analyzed and returned as a parse tree structure in another argument. In generation, the input goal argument specifies a deep structure and a resulting surface string is returned in the second argument. Though not always practical, grammar rules can be designed to work in both directions (as were the rules in Figure 1). A generation goal and the sentence it produces are shown in Figure I. 4.2 Functional Grammars Another formalism that has been used in previous .generation systems (McKeown, 1982; Appelt, 1983) is the functional grammar formalism (Kay, 1979)4. In a functional grammar, functional information such as focus and protagonist are treated in the same manner as syntactic and grammatical information such as subject and NP. By using functional information, input to the generator is simplified as it need not completely specify all the syntactic details. Instead, tests on functional information, that select between alternative surface structures, can be encoded in the grammar to arrive at the complete syntactic structure from which the string is generated. This formalism is consi</context>
<context position="18040" citStr="McKeown, 1982" startWordPosition="2800" endWordPosition="2801">w. When unified with a sentence grammar that contains tests on focus to determine voice and order constituents, sentence 12 is generated. If FOCUS were &lt;GOAL&gt;, instead, sentence 13 would result. CAT = S PRED = [LEX = give] TENSE = PAST PROT = [LEX = student] GOAL = [LEX = answer] BENEF = NONE FOCUS = &lt;PROT&gt; 12. The student gave the answer. 13. The answer was given by the student. Previous implementations of functional grammars have been concerned with the efficiency of the functional grammar unification algorithm. Straightforward implementations of the algorithm have proved too timeconsuming (McKeown, 1982) and efforts have been made to alter the algorithm to improve efficiency (Appelt, 1983). Efficiency continues to be a problem and a functional grammar generator that can be used practically has as yet to be developed. 4.3 Combining the Formalisms We have implemented a surface generator based on both the DCG formalism and the functional grammar 4. Functional grammar has also been referred to as unification grammar (Appelt, 1983). 5. The functional grammar unification operation is similar to set union. A description of the algorithm is given in Appelt (1983). It is not to be confused with the un</context>
<context position="27065" citStr="McKeown, 1982" startWordPosition="4165" endWordPosition="4166"> Therefore, Modeling and Analysis of Operating Systems was not added. 6. Related Work in Generation There are two basic classes of related work in generation. The first class of systems makes use of functional information in constructing the surface structure of the text and has relatively little to say about how and when to produce complex sentences. The second class of work has addressed the problem of producing complex sentences but does not incorporate functional information as part of this decision making process. Of the systems which make use of functional information, three (Kay, 1979; McKeown, 1982; Appelt, 1983) have already been mentioned. Kay&apos;s work provides the basis for McKeown&apos;s and Appelt&apos;s and emphasizes the development of a formalism and grammar for generation that allows for the use of functional information. Both McKeown and Appelt make direct use of Kay&apos;s formalism, with McKeown&apos;s emphasis being on the influence of focus information on syntax and Appelt&apos;s emphasis being on the development of a facility that allows interaction between the grammar and an underlying planning component. Nigel (Mann, 1983) is a fourth system that makes use of functional information and is based o</context>
</contexts>
<marker>McKeown, 1982</marker>
<rawString>McKeown, Kathleen R. (1982), Generating Natural Language Text in Response to Questions about Database Structure, Ph.D. dissertation, University of Pennsylvania, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>David H D Warren</author>
</authors>
<title>Definite clause grammars for language analysis--a survey of the formalism and a comparison with augmented transition networks,&amp;quot;</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<pages>231--278</pages>
<contexts>
<context position="1650" citStr="Pereira and Warren, 1980" startWordPosition="265" endWordPosition="268"> representation of what to say, a surface generator must construct an appropriate surface structure taking into consideration a wide variety of alternatives. When a generator is used to produce text and not just single sentences, one decision it must make is whether to use a sequence of simple sentences or a single complex one. We show how focus of attention can be used as the basis on which this decision can be made. A second goal of this paper is to introduce a formalism for surface generation that uses aspects of Kay&apos;s functional grammar (Kay, 1979) within a Prolog definite clause grammar (Pereira and Warren, 1980). This formalism was used to implement a surface generator that makes choices about sentence complexity based on shifts in focus of attention. The implementation was done as part of an explanation facility for a student advisor expert system being developed at Columbia University. 2. Language Generation Model In our model of natural language generation, we assume that the task of generating a response can be divided into two stages: determining the semantic content of the response and choosing a surface structure&apos;. One Kathleen R. McKeown Department of Computer Science Columbia University New </context>
<context position="12467" citStr="Pereira and Warren, 1980" startWordPosition="1999" endWordPosition="2002">coordination and deleting the focused protagonist in the second proposition (sentence 9). proposition 3: proposition 4: predicate = buy predicate buy protagonist = John protagonist = John goal = book goal = cassette focus = John focus = John proposition 5: proposition 6: predicate = read predicate = play protagonist = Mary protagonist = Mary goal = book goal = cassette focus = Mary focus = Mary 8. John bought a book and a cassette. 9. Mary read the book and played the cassette. 4. A Formalism for Surface Generation In this section we discuss the Prolog definite clause grammar (DCG) formalism (Pereira and Warren, 1980) and how it can be used for language generation, as well as recognition. We then review the functional grammar formalism (Kay, 1979) that has been used in other generation systems (e.g., McKeown, 1982; Appelt, 1983). Finally, we describe how aspects of a functional grammar can be encoded in a DCG to produce a generator with the best features of both formalisms. 4.1 Definite Clause Grammars The DCG formalism (Pereira and Warren, 1980) is based on a method for for expressing grammar rules as clauses of first-order predicate logic (Colmerauer, 1978; Kowalski, 1980). As discussed by Pereira and Wa</context>
<context position="14475" citStr="Pereira and Warren (1980)" startWordPosition="2326" endWordPosition="2329">Prolog, a logic programming language that implements an efficient resolution proof procedure using a depth-first search strategy with backtracking and a matching algorithm based on unification (Robinson, 1965). To analyze a sentence, the sentence is encoded as an argument to a Prolog goal. Prolog attempts to prove this goal by matching it against the set of grammar clauses. If the proof succeeds, the sentence is valid and a second argument is instantiated to the parse tree structure. A recognition goal and its resulting parse tree are shown in Figure 1. More extensive examples can be found in Pereira and Warren (1980). 321. sentence(s(NP,VP)) --&gt; n_phrase(N P,N um),v_phrase(VP,N um). n_phrase(np(Noun),Num) --&gt; noun(Noun,Num). noun(n(Root,Num),Num) --&gt; [the], [Word], (is_noun(Root,Word,Num)1. v_phrase(vp(Verb,NP),N um) --&gt; verb(Verb,N um), n_phrase(NP,N2). verb(v(Root,Num,Tense),Num) --&gt; [Word], (is_verb(Root,Word,Num,Tense)]. is_noun(student,student,singular). is_noun(student,students,plural). is_noun(answer,answer,singular). is_noun(answer,answers,plural). is_verb(give,gives,singular,pres) is_verb(give,give,plural,pres). is_verb(give,gave,_,past). Recognition Goal: sentence(T,[the,student,gave,the,answer]</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, Fernando C. N. and Warren, David H. D. (1980), &amp;quot;Definite clause grammars for language analysis--a survey of the formalism and a comparison with augmented transition networks,&amp;quot; Artificial Intelligence, 13, 231-278, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H D David</author>
</authors>
<title>Parsing as deduction,&amp;quot;</title>
<date>1983</date>
<booktitle>Proceedings of the 21 st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>137--144</pages>
<marker>David, 1983</marker>
<rawString>Pereira, Fernando C. N. and Warren. David H. D. (1983), &amp;quot;Parsing as deduction,&amp;quot; Proceedings of the 21 st Annual Meeting of the Association for Computational Linguistics, 137-144, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Prince</author>
</authors>
<title>On the given/new distinction,&amp;quot;</title>
<date>1979</date>
<journal>CLS,</journal>
<volume>15</volume>
<contexts>
<context position="6722" citStr="Prince, 1979" startWordPosition="1075" endWordPosition="1076">ic concepts describe distinctions between functional roles elements play in a sentence. In brief, they can be defined as follows: • Topic: Constituents in the sentence that represent what the speaker is talking about. Comment labels constituents that represent what s/he has to say about that topic (see Sgall, Hajicova, and Benesova, 1973; Lyons, 1968; Reinhart, 1981). • Given: Information that is assumed by the speaker to be derivable from context where context may mean either the preceding discourse or shared world knowledge. New labels information that cannot be derived (see Halliday, 1967; Prince, 1979; and Chafe, 1976). • Theme: The Prague School of linguists (see Firbas, 1966; Firbas, 1974) define the theme of a sentence as elements providing common ground for the conversants. Rheme refers to elements that function in conveying the information to be imparted. In sentences containing elements that are contextually dependent, the contextually dependent elements always function as theme. Thus, the Prague School version is close to the given/new distinction with the exception that a sentence always contains a theme, while it need not always contain given information2. What is important here i</context>
<context position="7992" citStr="Prince, 1979" startWordPosition="1267" endWordPosition="1268">s been associated with the selection of various syntactic structures. For example, it has been suggested that new information and rheme usually occur toward the end of a sentence (e.g., Halliday, 1967; Lyons, 1968; Sgall et al., 1973; Firbas, 1974). To place this information in its proper position in the sentence, structures other than the unmarked active sentence may be required (for example, the passive). Structures such as it-extraposition, there-insertion, topicalization, and left-dislocation3 have been shown to function in the introduction of new information into discourse (Sidner, 1979; Prince, 1979), often with the assumption that it will be talked about for a period of time (Joshi and Weinstein, 1981). Pronominalization is another linguistic device associated with these distinctions (see Akmajian, 1973; Sidner, 1979). One major difference between linguistic concepts and immediate focus is that focusing describes an active process on the part of speaker and listener. However, the speaker&apos;s immediate focus influences the surfacing of each of the linguistic concepts in the text. It influences topic (and Halliday&apos;s theme) in that it specifies what the speaker is focusing on (i.e., talking a</context>
</contexts>
<marker>Prince, 1979</marker>
<rawString>Prince, E. (1979), &amp;quot;On the given/new distinction,&amp;quot; CLS, 15, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Reinhart</author>
</authors>
<title>Pragmatics and linguistics: an analysis of sentence topics,&amp;quot;</title>
<date>1981</date>
<tech>Philosophica,</tech>
<contexts>
<context position="6479" citStr="Reinhart, 1981" startWordPosition="1037" endWordPosition="1038">e focus. Phenomena reflecting immediate focus in text have been studied by several linguists. Terminology and definitions for these vary widely; some of the names that have emerged include topic/comment, given/new, and theme/rheme. These linguistic concepts describe distinctions between functional roles elements play in a sentence. In brief, they can be defined as follows: • Topic: Constituents in the sentence that represent what the speaker is talking about. Comment labels constituents that represent what s/he has to say about that topic (see Sgall, Hajicova, and Benesova, 1973; Lyons, 1968; Reinhart, 1981). • Given: Information that is assumed by the speaker to be derivable from context where context may mean either the preceding discourse or shared world knowledge. New labels information that cannot be derived (see Halliday, 1967; Prince, 1979; and Chafe, 1976). • Theme: The Prague School of linguists (see Firbas, 1966; Firbas, 1974) define the theme of a sentence as elements providing common ground for the conversants. Rheme refers to elements that function in conveying the information to be imparted. In sentences containing elements that are contextually dependent, the contextually dependent</context>
</contexts>
<marker>Reinhart, 1981</marker>
<rawString>Reinhart, T. (1981), &amp;quot;Pragmatics and linguistics: an analysis of sentence topics,&amp;quot; Philosophica, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Robinson</author>
</authors>
<title>A machine-oriented logic based on the resolution principle,&amp;quot;</title>
<date>1965</date>
<journal>Journal of the ACM,</journal>
<volume>12</volume>
<pages>23--41</pages>
<contexts>
<context position="14059" citStr="Robinson, 1965" startWordPosition="2255" endWordPosition="2256">ons (enclosed within brackets t and to be included in the rules, providing another mechanism for encoding tests. A simple sentence grammar is shown in Figure 1. Viewed as a set of grammar rules, a DCG functions as a declarative description of a language. Viewed as a set of logic clauses, it functions as an executable program for analyzing strings of the language. In particular, a DCG can be executed by Prolog, a logic programming language that implements an efficient resolution proof procedure using a depth-first search strategy with backtracking and a matching algorithm based on unification (Robinson, 1965). To analyze a sentence, the sentence is encoded as an argument to a Prolog goal. Prolog attempts to prove this goal by matching it against the set of grammar clauses. If the proof succeeds, the sentence is valid and a second argument is instantiated to the parse tree structure. A recognition goal and its resulting parse tree are shown in Figure 1. More extensive examples can be found in Pereira and Warren (1980). 321. sentence(s(NP,VP)) --&gt; n_phrase(N P,N um),v_phrase(VP,N um). n_phrase(np(Noun),Num) --&gt; noun(Noun,Num). noun(n(Root,Num),Num) --&gt; [the], [Word], (is_noun(Root,Word,Num)1. v_phra</context>
</contexts>
<marker>Robinson, 1965</marker>
<rawString>Robinson, J. A. (1965), &amp;quot;A machine-oriented logic based on the resolution principle,&amp;quot; Journal of the ACM, 12 (I), 23-41, 1965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E</author>
<author>E Benesova</author>
</authors>
<title>Focus and Generative Semantics, Scriptor Verlag, Democratic Republic of Germany,</title>
<date>1973</date>
<marker>E, Benesova, 1973</marker>
<rawString>Sgall, P., Hajicova. E., and Benesova, E. (1973), Focus and Generative Semantics, Scriptor Verlag, Democratic Republic of Germany, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
</authors>
<title>Towards a Computation Theory of Definite Anaphora Comprehension</title>
<date>1979</date>
<booktitle>in English Discourse, Ph.D. dissertation, MIT,</booktitle>
<location>Cambridge, MA,</location>
<contexts>
<context position="7977" citStr="Sidner, 1979" startWordPosition="1265" endWordPosition="1266">or another, has been associated with the selection of various syntactic structures. For example, it has been suggested that new information and rheme usually occur toward the end of a sentence (e.g., Halliday, 1967; Lyons, 1968; Sgall et al., 1973; Firbas, 1974). To place this information in its proper position in the sentence, structures other than the unmarked active sentence may be required (for example, the passive). Structures such as it-extraposition, there-insertion, topicalization, and left-dislocation3 have been shown to function in the introduction of new information into discourse (Sidner, 1979; Prince, 1979), often with the assumption that it will be talked about for a period of time (Joshi and Weinstein, 1981). Pronominalization is another linguistic device associated with these distinctions (see Akmajian, 1973; Sidner, 1979). One major difference between linguistic concepts and immediate focus is that focusing describes an active process on the part of speaker and listener. However, the speaker&apos;s immediate focus influences the surfacing of each of the linguistic concepts in the text. It influences topic (and Halliday&apos;s theme) in that it specifies what the speaker is focusing on (</context>
</contexts>
<marker>Sidner, 1979</marker>
<rawString>Sidner, C. L. (1979), Towards a Computation Theory of Definite Anaphora Comprehension in English Discourse, Ph.D. dissertation, MIT, Cambridge, MA, 1979.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>