<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.071816">
<title confidence="0.840343">
SemEval-2013 Task 4: Free Paraphrases of Noun Compounds
</title>
<author confidence="0.9622">
Iris Hendrickx
</author>
<affiliation confidence="0.8770965">
Radboud University Nijmegen &amp;
Universidade de Lisboa
</affiliation>
<email confidence="0.949868">
iris@clul.ul.pt
</email>
<author confidence="0.846922">
Preslav Nakov
</author>
<affiliation confidence="0.798195">
QCRI, Qatar Foundation
</affiliation>
<email confidence="0.98507">
pnakov@qf.org.qa
</email>
<author confidence="0.998571">
Stan Szpakowicz
</author>
<affiliation confidence="0.9666575">
University of Ottawa &amp;
Polish Academy of Sciences
</affiliation>
<email confidence="0.993311">
szpak@eecs.uottawa.ca
</email>
<sectionHeader confidence="0.995568" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999661304347826">
In this paper, we describe SemEval-2013 Task
4: the definition, the data, the evaluation and
the results. The task is to capture some of the
meaning of English noun compounds via para-
phrasing. Given a two-word noun compound,
the participating system is asked to produce
an explicitly ranked list of its free-form para-
phrases. The list is automatically compared
and evaluated against a similarly ranked list
of paraphrases proposed by human annota-
tors, recruited and managed through Ama-
zon’s Mechanical Turk. The comparison of
raw paraphrases is sensitive to syntactic and
morphological variation. The “gold” ranking
is based on the relative popularity of para-
phrases among annotators. To make the rank-
ing more reliable, highly similar paraphrases
are grouped, so as to downplay superficial dif-
ferences in syntax and morphology. Three
systems participated in the task. They all beat
a simple baseline on one of the two evalua-
tion measures, but not on both measures. This
shows that the task is difficult.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996454444444444">
A noun compound (NC) is a sequence of nouns
which act as a single noun (Downing, 1977), as in
these examples: colon cancer, suppressor protein,
tumor suppressor protein, colon cancer tumor sup-
pressor protein, etc. This type of compounding is
highly productive in English. NCs comprise 3.9%
and 2.6% of all tokens in the Reuters corpus and the
British National Corpus (BNC), respectively (Bald-
win and Tanaka, 2004).
</bodyText>
<author confidence="0.787542">
Zornitsa Kozareva
</author>
<affiliation confidence="0.963665">
University of Southern California
</affiliation>
<email confidence="0.962063">
kozareva@isi.edu
</email>
<author confidence="0.700343">
Diarmuid O´ S´eaghdha
</author>
<affiliation confidence="0.919989">
University of Cambridge
</affiliation>
<email confidence="0.974263">
do242@cam.ac.uk
</email>
<author confidence="0.988223">
Tony Veale
</author>
<affiliation confidence="0.953848">
University College Dublin
</affiliation>
<email confidence="0.961684">
tony.veale@ucd.ie
</email>
<bodyText confidence="0.999129147058823">
The frequency spectrum of compound types fol-
lows a Zipfian distribution ( O´ S´eaghdha, 2008), so
many NC tokens belong to a “long tail” of low-
frequency types. More than half of the two-noun
types in the BNC occur exactly once (Kim and Bald-
win, 2006). Their high frequency and high produc-
tivity make robust NC interpretation an important
goal for broad-coverage semantic processing of En-
glish texts. Systems which ignore NCs may give up
on salient information about the semantic relation-
ships implicit in a text. Compositional interpretation
is also the only way to achieve broad NC coverage,
because it is not feasible to list in a lexicon all com-
pounds which one is likely to encounter. Even for
relatively frequent NCs occurring 10 times or more
in the BNC, static English dictionaries provide only
27% coverage (Tanaka and Baldwin, 2003).
In many natural language processing applications
it is important to understand the syntax and seman-
tics of NCs. NCs often are structurally similar,
but have very different meaning. Consider caffeine
headache and ice-cream headache: a lack of caf-
feine causes the former, an excess of ice-cream – the
latter. Different interpretations can lead to different
inferences, query expansion, paraphrases, transla-
tions, and so on. A question answering system may
have to determine whether protein acting as a tumor
suppressor is an accurate paraphrase for tumor sup-
pressor protein. An information extraction system
might need to decide whether neck vein thrombosis
and neck thrombosis can co-refer in the same doc-
ument. A machine translation system might para-
phrase the unknown compound WTO Geneva head-
quarters as WTO headquarters located in Geneva.
</bodyText>
<page confidence="0.967096">
138
</page>
<bodyText confidence="0.997663243902439">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
Research on the automatic interpretation of NCs
has focused mainly on common two-word NCs. The
usual task is to classify the semantic relation under-
lying a compound with either one of a small number
of predefined relation labels or a paraphrase from an
open vocabulary. Examples of the former take on
classification include (Moldovan et al., 2004; Girju,
2007; O´ S´eaghdha and Copestake, 2008; Tratz and
Hovy, 2010). Examples of the latter include (Nakov,
2008b; Nakov, 2008a; Nakov and Hearst, 2008; But-
nariu and Veale, 2008) and a previous NC paraphras-
ing task at SemEval-2010 (Butnariu et al., 2010),
upon which the task described here builds.
The assumption of a small inventory of prede-
fined relations has some advantages – parsimony and
generalization – but at the same time there are lim-
itations on expressivity and coverage. For exam-
ple, the NCs headache pills and fertility pills would
be assigned the same semantic relation (PURPOSE)
in most inventories, but their relational semantics
are quite different (Downing, 1977). Furthermore,
the definitions given by human subjects can involve
rich and specific meanings. For example, Down-
ing (1977) reports that a subject defined the NC
oil bowl as “the bowl into which the oil in the en-
gine is drained during an oil change”, compared to
which a minimal interpretation bowl for oil seems
very reductive. In view of such arguments, linguists
such as Downing (1977), Ryder (1994) and Coulson
(2001) have argued for a fine-grained, essentially
open-ended space of interpretations.
The idea of working with fine-grained para-
phrases for NC semantics has recently grown in pop-
ularity among NLP researchers (Butnariu and Veale,
2008; Nakov and Hearst, 2008; Nakov, 2008a). Task
9 at SemEval-2010 (Butnariu et al., 2010) was de-
voted to this methodology. In that previous work,
the paraphrases provided by human subjects were
required to fit a restrictive template admitting only
verbs and prepositions occurring between the NC’s
constituent nouns. Annotators recruited through
Amazon Mechanical Turk were asked to provide
paraphrases for the dataset of NCs. The gold stan-
dard for each NC was the ranked list of paraphrases
given by the annotators; this reflects the idea that a
compound’s meaning can be described in different
ways, at different levels of granularity and capturing
different interpretations in the case of ambiguity.
For example, a plastic saw could be a saw made
of plastic or a saw for cutting plastic. Systems par-
ticipating in the task were given the set of attested
paraphrases for each NC, and evaluated according to
how well they could reproduce the humans’ ranking.
The design of this task, SemEval-2013 Task 4,
is informed by previous work on compound anno-
tation and interpretation. It is also influenced by
similar initiatives, such as the English Lexical Sub-
stitution task at SemEval-2007 (McCarthy and Nav-
igli, 2007), and by various evaluation exercises in
the fields of paraphrasing and machine translation.
We build on SemEval-2010 Task 9, extending the
task’s flexibility in a number of ways. The restric-
tions on the form of annotators’ paraphrases was re-
laxed, giving us a rich dataset of close-to-freeform
paraphrases (Section 3). Rather than ranking a set of
attested paraphrases, systems must now both gener-
ate and rank their paraphrases; the task they perform
is essentially the same as what the annotators were
asked to do. This new setup required us to innovate
in terms of evaluation measures (Section 4).
We anticipate that the dataset and task will be of
broad interest among those who study lexical se-
mantics. We believe that the overall progress in the
field will significantly benefit from a public-domain
set of free-style NC paraphrases. That is why our
primary objective is the challenging endeavour of
preparing and releasing such a dataset to the re-
search community. The common evaluation task
which we establish will also enable researchers to
compare their algorithms and their empirical results.
</bodyText>
<sectionHeader confidence="0.789101" genericHeader="method">
2 Task description
</sectionHeader>
<bodyText confidence="0.9998265">
This is an English NC interpretation task, which ex-
plores the idea of interpreting the semantics of NCs
via free paraphrases. Given a noun-noun compound
such as air filter, the participating systems are asked
to produce an explicitly ranked list of free para-
phrases, as in the following example:
</bodyText>
<listItem confidence="0.982303833333333">
1 filter for air
2 filter of air
3 filter that cleans the air
4 filter which makes air healthier
5 a filter that removes impurities from the air
. . .
</listItem>
<page confidence="0.996197">
139
</page>
<bodyText confidence="0.999956545454545">
Such a list is then automatically compared and
evaluated against a similarly ranked list of para-
phrases proposed by human annotators, recruited
and managed via Amazon’s Mechanical Turk. The
comparison of raw paraphrases is sensitive to syn-
tactic and morphological variation. The ranking
of paraphrases is based on their relative popular-
ity among different annotators. To make the rank-
ing more reliable, highly similar paraphrases are
grouped so as to downplay superficial differences in
syntax and morphology.
</bodyText>
<sectionHeader confidence="0.976012" genericHeader="method">
3 Data collection
</sectionHeader>
<bodyText confidence="0.999763258064516">
We used Amazon’s Mechanical Turk service to
collect diverse paraphrases for a range of “gold-
standard” NCs.1 We paid the workers a small fee
($0.10) per compound, for which they were asked to
provide five paraphrases. Each paraphrase should
contain the two nouns of the compound (in sin-
gular or plural inflectional forms, but not in an-
other derivational form), an intermediate non-empty
linking phrase and optional preceding or following
terms. The paraphrasing terms could have any part
of speech, so long as the resulting paraphrase was a
well-formed noun phrase headed by the NC’s head.
We gave the workers feedback during data col-
lection if they appeared to have misunderstood the
nature of the task. Once raw paraphrases had been
collected from all workers, we collated them into a
spreadsheet, and we merged identical paraphrases
in order to calculate their overall frequencies. Ill-
formed paraphrases – those violating the syntactic
restrictions described above – were manually re-
moved following a consensus decision-making pro-
cedure; every paraphrase was checked by at least
two task organizers. We did not require that the
paraphrases be semantically felicitous, but we per-
formed minor edits on the remaining paraphrases if
they contained obvious typos.
The remaining well-formed paraphrases were
sorted by frequency separately for each NC. The
most frequent paraphrases for a compound are as-
signed the highest rank 0, those with the next-
highest frequency are given a rank of 1, and so on.
</bodyText>
<footnote confidence="0.976078">
1Since the annotation on Mechanical Turk was going slowly,
we also recruited four other annotators to do the same work,
following exactly the same instructions.
</footnote>
<table confidence="0.9864254">
Total Min / Max / Avg
6,069 1 / 287 / 34.9
4,255 1 / 105 / 24.5
9,706 24 / 99 / 53.6
8,216 21 / 80 / 45.4
</table>
<tableCaption confidence="0.918205">
Table 1: Statistics of the trial and test datasets: the total
number of paraphrases with and without duplicates, and
the minimum / maximum / average per noun compound.
</tableCaption>
<bodyText confidence="0.998821894736842">
Paraphrases with a frequency of 1 – proposed for
a given NC by only one annotator – always occupy
the lowest rank on the list for that compound.
We used 174+181 noun-noun compounds from
the NC dataset of O´ S´eaghdha (2007). The trial
dataset, which we initially released to the partici-
pants, consisted of 4,255 human paraphrases for 174
noun-noun pairs; this dataset was also the training
dataset. The test dataset comprised paraphrases for
181 noun-noun pairs. The “gold standard” contained
9,706 paraphrases of which 8,216 were unique for
those 181 NCs. Further statistics on the datasets are
presented in Table 1.
Compared with the data collected for the
SemEval-2010 Task 9 on the interpretation of noun
compounds, the data collected for this new task have
a far greater range of variety and richness. For ex-
ample, the following (selected) paraphrases for work
area vary from parsimonious to expansive:
</bodyText>
<listItem confidence="0.999878333333334">
• area for work
• area of work
• area where work is done
• area where work is performed
• . . .
• an area cordoned off for persons responsible for
work
• an area where construction work is carried out
• an area where work is accomplished and done
• area where work is conducted
• office area assigned as a work space
• . . .
</listItem>
<figure confidence="0.969905166666667">
Trial/Train (174 NCs)
paraphrases
unique paraphrases
Test (181 NCs)
paraphrases
unique paraphrases
</figure>
<page confidence="0.965451">
140
</page>
<sectionHeader confidence="0.995158" genericHeader="method">
4 Scoring
</sectionHeader>
<bodyText confidence="0.999895521276596">
Noun compounding is a generative aspect of lan-
guage, but so too is the process of NC interpretation:
human speakers typically generate a range of possi-
ble interpretations for a given compound, each em-
phasizing a different aspect of the relationship be-
tween the nouns. Our evaluation framework reflects
the belief that there is rarely a single right answer
for a given noun-noun pairing. Participating systems
are thus expected to demonstrate some generativity
of their own, and are scored not just on the accu-
racy of individual interpretations, but on the overall
breadth of their output.
For evaluation, we provided a scorer imple-
mented, for good portability, as a Java class. For
each noun compound to be evaluated, the scorer
compares a list of system-suggested paraphrases
against a “gold-standard” reference list, compiled
and rank-ordered from the paraphrases suggested
by our human annotators. The score assigned to
each system is the mean of the system’s performance
across all test compounds. Note that the scorer re-
moves all determiners from both the reference and
the test paraphrases, so a system is neither punished
for not reproducing a determiner or rewarded for
producing the same determiners.
The scorer can match words identically or non-
identically. A match of two identical words Wgold
and Wtest earns a score of 1.0. There is a partial
score of (2 |P |/ (|PWgold |+ |PWtest|))2 for a
match of two words PWgold and PWtest that are
not identical but share a common prefix P, |P |&gt; 2,
e.g., wmatch(cutting, cuts) = (6/11)2 = 0.297.
Two n-grams Ngold = [GW1, ... , GWn] and
Ntest = [TW1, ... , TWn] can be matched if
wmatch(GWZ, TWZ) &gt; 0 for all i in 1..n. The
score assigned to the match of these two n-grams is
then EZ wmatch(GWZ, TWZ). For every n-gram
Ntest = [TW1, ... , TWn] in a system-generated
paraphrase, the scorer finds a matching n-gram
Ngold = [GW1, ... , GWn] in the reference para-
phrase Paragold which maximizes this sum.
The overall n-gram overlap score for a reference
paraphrase Paragold and a system-generated para-
phrase Paratest is the sum of the score calculated
for all n-grams in Paratest, where n ranges from 1
to the size of Paratest.
This overall score is then normalized by dividing
by the maximum value among the n-gram overlap
score for Paragold compared with itself and the n-
gram overlap score for Paratest compared with it-
self. This normalization step produces a paraphrase
match score in the range [0.0 – 1.0]. It punishes a
paraphrase Paratest for both over-generating (con-
taining more words than are found in Paragold)
and under-generating (containing fewer words than
are found in Paragold). In other words, Paratest
should ideally reproduce everything in Paragold,
and nothing more or less.
The reference paraphrases in the “gold standard”
are ordered by rank; the highest rank is assigned to
the paraphrases which human judges suggested most
often. The rank of a reference paraphrase matters
because a good participating system will aim to re-
produce the top-ranked “gold-standard” paraphrases
as produced by human judges. The scorer assigns
a multiplier of R/(R + n) to reference paraphrases
at rank n; this multiplier asymptotically approaches
0 for the higher values of n of ever lower-ranked
paraphrases. We choose a default setting of R = 8,
so that a reference paraphrase at rank 0 (the highest
rank) has a multiplier of 1, while a reference para-
phrase at rank 5 has a multiplier of 8/13 = 0.615.
When a system-generated paraphrase Paratest is
matched with a reference paraphrase Paragold, their
normalized n-gram overlap score is scaled by the
rank multiplier attaching to the rank of Paragold rel-
ative to the other reference paraphrases provided by
human judges. The scorer automatically chooses the
reference paraphrase Paragold for a test paraphrase
Paratest so as to maximize this product of normal-
ized n-gram overlap score and rank multiplier.
The overall score assigned to each system for
a specific compound is calculated in two differ-
ent ways: using isomorphic matching of suggested
paraphrases to the “gold-standard’s” reference para-
phrases (on a one-to-one basis); and using non-
isomorphic matching of system’s paraphrases to the
“gold-standard’s” reference paraphrases (in a poten-
tially many-to-one mapping).
Isomorphic matching rewards both precision and
recall. It rewards a system for accurately reproduc-
ing the paraphrases suggested by human judges, and
for reproducing as many of these as it can, and in
much the same order.
</bodyText>
<page confidence="0.99661">
141
</page>
<bodyText confidence="0.996009666666667">
In isomorphic mode, system’s paraphrases are
matched 1-to-1 with reference paraphrases on a first-
come first-matched basis, so ordering can be crucial.
Non-isomorphic matching rewards only preci-
sion. It rewards a system for accurately reproducing
the top-ranked human paraphrases in the “gold stan-
dard”. A system will achieve a higher score in a non-
isomorphic match if it reproduces the top-ranked hu-
man paraphrases as opposed to lower-ranked human
paraphrases. The ordering of system’s paraphrases
is thus not important in non-isomorphic matching.
Each system is evaluated using the scorer in both
modes, isomorphic and non-isomorphic. Systems
which aim only for precision should score highly
on non-isomorphic match mode, but poorly in iso-
morphic match mode. Systems which aim for pre-
cision and recall will face a more substantial chal-
lenge, likely reflected in their scores.
A naive baseline
We decided to allow preposition-only paraphrases,
which are abundant in the paraphrases suggested
by human judges in the crowdsourcing Mechanical
Turk collection process. This abundance means that
the top-ranked paraphrase for a given compound is
often a preposition-only phrase, or one of a small
number of very popular paraphrases such as used for
or used in. It is thus straightforward to build a naive
baseline generator which we can expect to score
reasonably on this task, at least in non-isomorphic
matching mode. For each test compound M H,
the baseline system generates the following para-
phrases, in this precise order: H of M, H in M, H
for M, H with M, H on M, H about M, H has M, H to
M, H used for M, H used in M.
This naive baseline is truly unsophisticated. No
attempt is made to order paraphrases by their corpus
frequencies or by their frequencies in the training
data. The same sequence of paraphrases is generated
for each and every test compound.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999625833333333">
Three teams participated in the challenge, and all
their systems were supervised. The MELODI sys-
tem relied on semantic vector space model built
from the UKWAC corpus (window-based, 5 words).
It used only the features of the right-hand head noun
to train a maximum entropy classifier.
</bodyText>
<table confidence="0.999248666666667">
Team isomorphic non-isomorphic
SFS 23.1 17.9
IIITH 23.1 25.8
MELODI-Primary 13.0 54.8
MELODI-Contrast 13.6 53.6
Naive Baseline 13.8 40.6
</table>
<tableCaption confidence="0.985225">
Table 2: Results for the participating systems; the base-
line outputs the same paraphrases for all compounds.
</tableCaption>
<bodyText confidence="0.99997292">
The IIITH system used the probabilities of the
preposition co-occurring with a relation to identify
the class of the noun compound. To collect statis-
tics, it used Google n-grams, BNC and ANC.
The SFS system extracted templates and fillers
from the training data, which it then combined with
a four-gram language model and a MaxEnt reranker.
To find similar compounds, they used Lin’s Word-
Net similarity. They further used statistics from the
English Gigaword and the Google n-grams.
Table 2 shows the performance of the partici-
pating systems, SFS, IIITH and MELODI, and the
naive baseline. The baseline shows that it is rela-
tively easy to achieve a moderately good score in
non-isomorphic match mode by generating a fixed
set of paraphrases which are both common and
generic: two of the three participating systems,
SFS and IIITH, under-perform the naive baseline
in non-isomorphic match mode, but outperform it
in isomorphic mode. The only system to surpass
this baseline in non-isomorphic match mode is the
MELODI system; yet, it under-performs against the
same baseline in isomorphic match mode. No par-
ticipating team submitted a system which would out-
perform the naive baseline in both modes.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999950875">
The conclusions we draw from the experience of or-
ganizing the task are mixed. Participation was rea-
sonable but not large, suggesting that NC paraphras-
ing remains a niche interest – though we believe it
deserves more attention among the broader lexical
semantics community and hope that the availabil-
ity of our freeform paraphrase dataset will attract a
wider audience in the future.
</bodyText>
<page confidence="0.993794">
142
</page>
<bodyText confidence="0.999988444444444">
We also observed a varied response from our an-
notators in terms of embracing their freedom to gen-
erate complex and rich paraphrases; there are many
possible reasons for this including laziness, time
pressure and the fact that short paraphrases are often
very appropriate paraphrases. The results obtained
by our participants were also modest, demonstrating
that compound paraphrasing is both a difficult task
and a novel one that has not yet been “solved”.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999947">
This work has partially supported by a small but ef-
fective grant from Amazon; the credit allowed us
to hire sufficiently many Turkers – thanks! And a
thank-you to our additional annotators Dave Carter,
Chris Fournier and Colette Joubarne for their com-
plete sets of paraphrases of the noun compounds in
the test data.
</bodyText>
<sectionHeader confidence="0.997393" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99869905952381">
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of complex nominals: Getting it right.
Proc. ACL04 Workshop on Multiword Expressions: In-
tegrating Processing, Barcelona, Spain, 24-31.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
Proc. 22nd International Conference on Computa-
tional Linguistics (COLING-08), Manchester, UK, 81-
88.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O´ S´eaghdha, Stan Szpakowicz, and Tony Veale.
2010. SemEval-2010 Task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. Proc. 5th International ACL Workshop on Se-
mantic Evaluation, Uppsala, Sweden, 39-44.
Seana Coulson. 2001. Semantic Leaps: Frame-Shifting
and Conceptual Blending in Meaning Construction.
Cambridge University Press, Cambridge, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4): 810-842.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. Proc.
45th Annual Meeting of the Association of Computa-
tional Linguistics, Prague, Czech Republic, 568-575.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting
semantic relations in noun compounds via verb seman-
tics. Proc. ACL-06 Main Conference Poster Session,
Sydney, Australia, 491-498.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. Proc.
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), Prague, Czech Republic, 48-53.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. Dan Moldovan
and Roxana Girju, eds., HLT-NAACL 2004: Workshop
on Computational Lexical Semantics, Boston, MA,
USA, 60-67.
Preslav Nakov and Marti Hearst. 2008. Solving rela-
tional similarity problems using the Web as a corpus.
Proc. 46th Annual Meeting of the Association for Com-
putational Linguistics ACL-08, Columbus, OH, USA,
452-460.
Preslav Nakov. 2008a. Improved statistical machine
translation using monolingual paraphrases. Proc. 18th
European Conference on Artificial Intelligence ECAI-
08, Patras, Greece, 338-342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. Proc.
13th International Conference on Artificial Intelli-
gence: Methodology, Systems, Applications AIMSA-
08, Varna, Bulgaria, Lecture Notes in Computer Sci-
ence 5253, Springer, 103-117.
Diarmuid O´ S´eaghdha. 2007. Designing and Evaluating
a Semantic Annotation Scheme for Compound Nouns.
In Proceedings of the 4th Corpus Linguistics Confer-
ence, Birmingham, UK.
Diarmuid O´ S´eaghdha. 2008. Learning compound
noun semantics. Ph.D. thesis, Computer Laboratory,
University of Cambridge. Published as University
of Cambridge Computer Laboratory Technical Report
735.
Diarmuid O´ S´eaghdha and Ann Copestake. 2009. Using
lexical and relational similarity to classify semantic re-
lations. Proc. 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
EACL-09, Athens, Greece, 621-629.
Diarmuid O´ S´eaghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. 22nd International Conference on Computa-
tional Linguistics (COLING-08), Manchester, UK.
Mary Ellen Ryder. 1994. Ordered Chaos: The Interpre-
tation of English Noun-Noun Compounds. University
of California Press, Berkeley, CA, USA.
Takaaki Tanaka and Tim Baldwin. 2003. Noun-noun
compound machine translation: A feasibility study
on shallow processing. Proc. ACL-2003 Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan, 17-24.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. Proc. 48th Annual Meeting of the As-
sociation for Computational Linguistics ACL-10, Up-
psala, Sweden, 678-687.
</reference>
<page confidence="0.999131">
143
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.240428">
<title confidence="0.989585">SemEval-2013 Task 4: Free Paraphrases of Noun Compounds</title>
<author confidence="0.998614">Iris Hendrickx</author>
<affiliation confidence="0.9956705">Radboud University Nijmegen &amp; Universidade de Lisboa</affiliation>
<email confidence="0.93027">iris@clul.ul.pt</email>
<author confidence="0.423143">Preslav Nakov</author>
<affiliation confidence="0.430751">QCRI, Qatar Foundation</affiliation>
<email confidence="0.543983">pnakov@qf.org.qa</email>
<author confidence="0.999316">Stan Szpakowicz</author>
<affiliation confidence="0.9297225">University of Ottawa &amp; Polish Academy of Sciences</affiliation>
<email confidence="0.958978">szpak@eecs.uottawa.ca</email>
<abstract confidence="0.998570375">In this paper, we describe SemEval-2013 Task 4: the definition, the data, the evaluation and the results. The task is to capture some of the meaning of English noun compounds via paraphrasing. Given a two-word noun compound, the participating system is asked to produce an explicitly ranked list of its free-form paraphrases. The list is automatically compared and evaluated against a similarly ranked list of paraphrases proposed by human annotators, recruited and managed through Amazon’s Mechanical Turk. The comparison of raw paraphrases is sensitive to syntactic and morphological variation. The “gold” ranking is based on the relative popularity of paraphrases among annotators. To make the ranking more reliable, highly similar paraphrases are grouped, so as to downplay superficial differences in syntax and morphology. Three systems participated in the task. They all beat a simple baseline on one of the two evaluation measures, but not on both measures. This shows that the task is difficult.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Takaaki Tanaka</author>
</authors>
<title>Translation by machine of complex nominals: Getting it right.</title>
<date>2004</date>
<booktitle>Proc. ACL04 Workshop on Multiword Expressions: Integrating Processing,</booktitle>
<pages>24--31</pages>
<location>Barcelona,</location>
<contexts>
<context position="1723" citStr="Baldwin and Tanaka, 2004" startWordPosition="263" endWordPosition="267">ntax and morphology. Three systems participated in the task. They all beat a simple baseline on one of the two evaluation measures, but not on both measures. This shows that the task is difficult. 1 Introduction A noun compound (NC) is a sequence of nouns which act as a single noun (Downing, 1977), as in these examples: colon cancer, suppressor protein, tumor suppressor protein, colon cancer tumor suppressor protein, etc. This type of compounding is highly productive in English. NCs comprise 3.9% and 2.6% of all tokens in the Reuters corpus and the British National Corpus (BNC), respectively (Baldwin and Tanaka, 2004). Zornitsa Kozareva University of Southern California kozareva@isi.edu Diarmuid O´ S´eaghdha University of Cambridge do242@cam.ac.uk Tony Veale University College Dublin tony.veale@ucd.ie The frequency spectrum of compound types follows a Zipfian distribution ( O´ S´eaghdha, 2008), so many NC tokens belong to a “long tail” of lowfrequency types. More than half of the two-noun types in the BNC occur exactly once (Kim and Baldwin, 2006). Their high frequency and high productivity make robust NC interpretation an important goal for broad-coverage semantic processing of English texts. Systems whic</context>
</contexts>
<marker>Baldwin, Tanaka, 2004</marker>
<rawString>Timothy Baldwin and Takaaki Tanaka. 2004. Translation by machine of complex nominals: Getting it right. Proc. ACL04 Workshop on Multiword Expressions: Integrating Processing, Barcelona, Spain, 24-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Butnariu</author>
<author>Tony Veale</author>
</authors>
<title>A conceptcentered approach to noun-compound interpretation.</title>
<date>2008</date>
<booktitle>Proc. 22nd International Conference on Computational Linguistics (COLING-08),</booktitle>
<pages>81--88</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="4377" citStr="Butnariu and Veale, 2008" startWordPosition="673" endWordPosition="677">ges 138–143, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, 2007; O´ S´eaghdha and Copestake, 2008; Tratz and Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve rich and specific meanings. </context>
</contexts>
<marker>Butnariu, Veale, 2008</marker>
<rawString>Cristina Butnariu and Tony Veale. 2008. A conceptcentered approach to noun-compound interpretation. Proc. 22nd International Conference on Computational Linguistics (COLING-08), Manchester, UK, 81-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Butnariu</author>
<author>Su Nam Kim</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Stan Szpakowicz</author>
<author>Tony Veale</author>
</authors>
<title>SemEval-2010 Task 9: The interpretation of noun compounds using paraphrasing verbs and prepositions.</title>
<date>2010</date>
<booktitle>Proc. 5th International ACL Workshop on Semantic Evaluation,</booktitle>
<pages>39--44</pages>
<location>Uppsala,</location>
<marker>Butnariu, Kim, Nakov, S´eaghdha, Szpakowicz, Veale, 2010</marker>
<rawString>Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diarmuid O´ S´eaghdha, Stan Szpakowicz, and Tony Veale. 2010. SemEval-2010 Task 9: The interpretation of noun compounds using paraphrasing verbs and prepositions. Proc. 5th International ACL Workshop on Semantic Evaluation, Uppsala, Sweden, 39-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seana Coulson</author>
</authors>
<title>Semantic Leaps: Frame-Shifting and Conceptual Blending in Meaning Construction.</title>
<date>2001</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="5302" citStr="Coulson (2001)" startWordPosition="828" endWordPosition="829">example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve rich and specific meanings. For example, Downing (1977) reports that a subject defined the NC oil bowl as “the bowl into which the oil in the engine is drained during an oil change”, compared to which a minimal interpretation bowl for oil seems very reductive. In view of such arguments, linguists such as Downing (1977), Ryder (1994) and Coulson (2001) have argued for a fine-grained, essentially open-ended space of interpretations. The idea of working with fine-grained paraphrases for NC semantics has recently grown in popularity among NLP researchers (Butnariu and Veale, 2008; Nakov and Hearst, 2008; Nakov, 2008a). Task 9 at SemEval-2010 (Butnariu et al., 2010) was devoted to this methodology. In that previous work, the paraphrases provided by human subjects were required to fit a restrictive template admitting only verbs and prepositions occurring between the NC’s constituent nouns. Annotators recruited through Amazon Mechanical Turk were</context>
</contexts>
<marker>Coulson, 2001</marker>
<rawString>Seana Coulson. 2001. Semantic Leaps: Frame-Shifting and Conceptual Blending in Meaning Construction. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela Downing</author>
</authors>
<title>On the creation and use of English compound nouns.</title>
<date>1977</date>
<journal>Language,</journal>
<volume>53</volume>
<issue>4</issue>
<pages>810--842</pages>
<contexts>
<context position="1396" citStr="Downing, 1977" startWordPosition="215" endWordPosition="216">Mechanical Turk. The comparison of raw paraphrases is sensitive to syntactic and morphological variation. The “gold” ranking is based on the relative popularity of paraphrases among annotators. To make the ranking more reliable, highly similar paraphrases are grouped, so as to downplay superficial differences in syntax and morphology. Three systems participated in the task. They all beat a simple baseline on one of the two evaluation measures, but not on both measures. This shows that the task is difficult. 1 Introduction A noun compound (NC) is a sequence of nouns which act as a single noun (Downing, 1977), as in these examples: colon cancer, suppressor protein, tumor suppressor protein, colon cancer tumor suppressor protein, etc. This type of compounding is highly productive in English. NCs comprise 3.9% and 2.6% of all tokens in the Reuters corpus and the British National Corpus (BNC), respectively (Baldwin and Tanaka, 2004). Zornitsa Kozareva University of Southern California kozareva@isi.edu Diarmuid O´ S´eaghdha University of Cambridge do242@cam.ac.uk Tony Veale University College Dublin tony.veale@ucd.ie The frequency spectrum of compound types follows a Zipfian distribution ( O´ S´eaghdh</context>
<context position="4882" citStr="Downing, 1977" startWordPosition="757" endWordPosition="758">Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve rich and specific meanings. For example, Downing (1977) reports that a subject defined the NC oil bowl as “the bowl into which the oil in the engine is drained during an oil change”, compared to which a minimal interpretation bowl for oil seems very reductive. In view of such arguments, linguists such as Downing (1977), Ryder (1994) and Coulson (2001) have argued for a fine-grained, essentially open-ended space of interpretations. The idea of working with fine-grained paraphrases for NC semantics has recently grown in popularit</context>
</contexts>
<marker>Downing, 1977</marker>
<rawString>Pamela Downing. 1977. On the creation and use of English compound nouns. Language, 53(4): 810-842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Improving the interpretation of noun phrases with cross-linguistic information.</title>
<date>2007</date>
<booktitle>Proc. 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>568--575</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4209" citStr="Girju, 2007" startWordPosition="649" endWordPosition="650"> Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, 2007; O´ S´eaghdha and Copestake, 2008; Tratz and Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most invent</context>
</contexts>
<marker>Girju, 2007</marker>
<rawString>Roxana Girju. 2007. Improving the interpretation of noun phrases with cross-linguistic information. Proc. 45th Annual Meeting of the Association of Computational Linguistics, Prague, Czech Republic, 568-575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
</authors>
<title>Interpreting semantic relations in noun compounds via verb semantics.</title>
<date>2006</date>
<booktitle>Proc. ACL-06 Main Conference Poster Session,</booktitle>
<pages>491--498</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2161" citStr="Kim and Baldwin, 2006" startWordPosition="329" endWordPosition="333">mpounding is highly productive in English. NCs comprise 3.9% and 2.6% of all tokens in the Reuters corpus and the British National Corpus (BNC), respectively (Baldwin and Tanaka, 2004). Zornitsa Kozareva University of Southern California kozareva@isi.edu Diarmuid O´ S´eaghdha University of Cambridge do242@cam.ac.uk Tony Veale University College Dublin tony.veale@ucd.ie The frequency spectrum of compound types follows a Zipfian distribution ( O´ S´eaghdha, 2008), so many NC tokens belong to a “long tail” of lowfrequency types. More than half of the two-noun types in the BNC occur exactly once (Kim and Baldwin, 2006). Their high frequency and high productivity make robust NC interpretation an important goal for broad-coverage semantic processing of English texts. Systems which ignore NCs may give up on salient information about the semantic relationships implicit in a text. Compositional interpretation is also the only way to achieve broad NC coverage, because it is not feasible to list in a lexicon all compounds which one is likely to encounter. Even for relatively frequent NCs occurring 10 times or more in the BNC, static English dictionaries provide only 27% coverage (Tanaka and Baldwin, 2003). In many</context>
</contexts>
<marker>Kim, Baldwin, 2006</marker>
<rawString>Su Nam Kim and Timothy Baldwin. 2006. Interpreting semantic relations in noun compounds via verb semantics. Proc. ACL-06 Main Conference Poster Session, Sydney, Australia, 491-498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>Proc. Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6742" citStr="McCarthy and Navigli, 2007" startWordPosition="1055" endWordPosition="1059">different ways, at different levels of granularity and capturing different interpretations in the case of ambiguity. For example, a plastic saw could be a saw made of plastic or a saw for cutting plastic. Systems participating in the task were given the set of attested paraphrases for each NC, and evaluated according to how well they could reproduce the humans’ ranking. The design of this task, SemEval-2013 Task 4, is informed by previous work on compound annotation and interpretation. It is also influenced by similar initiatives, such as the English Lexical Substitution task at SemEval-2007 (McCarthy and Navigli, 2007), and by various evaluation exercises in the fields of paraphrasing and machine translation. We build on SemEval-2010 Task 9, extending the task’s flexibility in a number of ways. The restrictions on the form of annotators’ paraphrases was relaxed, giving us a rich dataset of close-to-freeform paraphrases (Section 3). Rather than ranking a set of attested paraphrases, systems must now both generate and rank their paraphrases; the task they perform is essentially the same as what the annotators were asked to do. This new setup required us to innovate in terms of evaluation measures (Section 4).</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. Semeval2007 task 10: English lexical substitution task. Proc. Fourth International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, 48-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Adriana Badulescu</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
<author>Roxana Girju</author>
</authors>
<title>Models for the semantic classification of noun phrases.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004: Workshop on Computational Lexical Semantics,</booktitle>
<pages>60--67</pages>
<editor>Dan Moldovan and Roxana Girju, eds.,</editor>
<location>Boston, MA, USA,</location>
<contexts>
<context position="4196" citStr="Moldovan et al., 2004" startWordPosition="645" endWordPosition="648"> located in Geneva. 138 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, 2007; O´ S´eaghdha and Copestake, 2008; Tratz and Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) i</context>
</contexts>
<marker>Moldovan, Badulescu, Tatu, Antohe, Girju, 2004</marker>
<rawString>Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel Antohe, and Roxana Girju. 2004. Models for the semantic classification of noun phrases. Dan Moldovan and Roxana Girju, eds., HLT-NAACL 2004: Workshop on Computational Lexical Semantics, Boston, MA, USA, 60-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Solving relational similarity problems using the Web as a corpus.</title>
<date>2008</date>
<booktitle>Proc. 46th Annual Meeting of the Association for Computational Linguistics ACL-08,</booktitle>
<pages>452--460</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="4350" citStr="Nakov and Hearst, 2008" startWordPosition="669" endWordPosition="672">ation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, 2007; O´ S´eaghdha and Copestake, 2008; Tratz and Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve r</context>
</contexts>
<marker>Nakov, Hearst, 2008</marker>
<rawString>Preslav Nakov and Marti Hearst. 2008. Solving relational similarity problems using the Web as a corpus. Proc. 46th Annual Meeting of the Association for Computational Linguistics ACL-08, Columbus, OH, USA, 452-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Improved statistical machine translation using monolingual paraphrases.</title>
<date>2008</date>
<booktitle>Proc. 18th European Conference on Artificial Intelligence ECAI08,</booktitle>
<pages>338--342</pages>
<location>Patras, Greece,</location>
<contexts>
<context position="4311" citStr="Nakov, 2008" startWordPosition="665" endWordPosition="666">l Workshop on Semantic Evaluation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, 2007; O´ S´eaghdha and Copestake, 2008; Tratz and Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definition</context>
<context position="5568" citStr="Nakov, 2008" startWordPosition="868" endWordPosition="869"> specific meanings. For example, Downing (1977) reports that a subject defined the NC oil bowl as “the bowl into which the oil in the engine is drained during an oil change”, compared to which a minimal interpretation bowl for oil seems very reductive. In view of such arguments, linguists such as Downing (1977), Ryder (1994) and Coulson (2001) have argued for a fine-grained, essentially open-ended space of interpretations. The idea of working with fine-grained paraphrases for NC semantics has recently grown in popularity among NLP researchers (Butnariu and Veale, 2008; Nakov and Hearst, 2008; Nakov, 2008a). Task 9 at SemEval-2010 (Butnariu et al., 2010) was devoted to this methodology. In that previous work, the paraphrases provided by human subjects were required to fit a restrictive template admitting only verbs and prepositions occurring between the NC’s constituent nouns. Annotators recruited through Amazon Mechanical Turk were asked to provide paraphrases for the dataset of NCs. The gold standard for each NC was the ranked list of paraphrases given by the annotators; this reflects the idea that a compound’s meaning can be described in different ways, at different levels of granularity an</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008a. Improved statistical machine translation using monolingual paraphrases. Proc. 18th European Conference on Artificial Intelligence ECAI08, Patras, Greece, 338-342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Noun compound interpretation using paraphrasing verbs: Feasibility study.</title>
<date>2008</date>
<booktitle>Proc. 13th International Conference on Artificial Intelligence: Methodology, Systems, Applications AIMSA08, Varna, Bulgaria, Lecture Notes in Computer Science 5253, Springer,</booktitle>
<pages>103--117</pages>
<contexts>
<context position="4311" citStr="Nakov, 2008" startWordPosition="665" endWordPosition="666">l Workshop on Semantic Evaluation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, 2007; O´ S´eaghdha and Copestake, 2008; Tratz and Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definition</context>
<context position="5568" citStr="Nakov, 2008" startWordPosition="868" endWordPosition="869"> specific meanings. For example, Downing (1977) reports that a subject defined the NC oil bowl as “the bowl into which the oil in the engine is drained during an oil change”, compared to which a minimal interpretation bowl for oil seems very reductive. In view of such arguments, linguists such as Downing (1977), Ryder (1994) and Coulson (2001) have argued for a fine-grained, essentially open-ended space of interpretations. The idea of working with fine-grained paraphrases for NC semantics has recently grown in popularity among NLP researchers (Butnariu and Veale, 2008; Nakov and Hearst, 2008; Nakov, 2008a). Task 9 at SemEval-2010 (Butnariu et al., 2010) was devoted to this methodology. In that previous work, the paraphrases provided by human subjects were required to fit a restrictive template admitting only verbs and prepositions occurring between the NC’s constituent nouns. Annotators recruited through Amazon Mechanical Turk were asked to provide paraphrases for the dataset of NCs. The gold standard for each NC was the ranked list of paraphrases given by the annotators; this reflects the idea that a compound’s meaning can be described in different ways, at different levels of granularity an</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008b. Noun compound interpretation using paraphrasing verbs: Feasibility study. Proc. 13th International Conference on Artificial Intelligence: Methodology, Systems, Applications AIMSA08, Varna, Bulgaria, Lecture Notes in Computer Science 5253, Springer, 103-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Designing and Evaluating a Semantic Annotation Scheme for Compound Nouns.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th Corpus Linguistics Conference,</booktitle>
<location>Birmingham, UK.</location>
<marker>S´eaghdha, 2007</marker>
<rawString>Diarmuid O´ S´eaghdha. 2007. Designing and Evaluating a Semantic Annotation Scheme for Compound Nouns. In Proceedings of the 4th Corpus Linguistics Conference, Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Learning compound noun semantics.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Laboratory, University of Cambridge. Published as University of Cambridge Computer Laboratory</institution>
<marker>S´eaghdha, 2008</marker>
<rawString>Diarmuid O´ S´eaghdha. 2008. Learning compound noun semantics. Ph.D. thesis, Computer Laboratory, University of Cambridge. Published as University of Cambridge Computer Laboratory Technical Report 735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
<author>Ann Copestake</author>
</authors>
<title>Using lexical and relational similarity to classify semantic relations.</title>
<date>2009</date>
<booktitle>Proc. 12th Conference of the European Chapter of the Association for Computational Linguistics EACL-09,</booktitle>
<pages>621--629</pages>
<location>Athens, Greece,</location>
<marker>S´eaghdha, Copestake, 2009</marker>
<rawString>Diarmuid O´ S´eaghdha and Ann Copestake. 2009. Using lexical and relational similarity to classify semantic relations. Proc. 12th Conference of the European Chapter of the Association for Computational Linguistics EACL-09, Athens, Greece, 621-629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
<author>Ann Copestake</author>
</authors>
<title>Semantic classification with distributional kernels.</title>
<date>2008</date>
<booktitle>In Proc. 22nd International Conference on Computational Linguistics (COLING-08),</booktitle>
<location>Manchester, UK.</location>
<marker>S´eaghdha, Copestake, 2008</marker>
<rawString>Diarmuid O´ S´eaghdha and Ann Copestake. 2008. Semantic classification with distributional kernels. In Proc. 22nd International Conference on Computational Linguistics (COLING-08), Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Ellen Ryder</author>
</authors>
<title>Ordered Chaos: The Interpretation of English Noun-Noun Compounds.</title>
<date>1994</date>
<publisher>University of California Press,</publisher>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="5283" citStr="Ryder (1994)" startWordPosition="825" endWordPosition="826">nd coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve rich and specific meanings. For example, Downing (1977) reports that a subject defined the NC oil bowl as “the bowl into which the oil in the engine is drained during an oil change”, compared to which a minimal interpretation bowl for oil seems very reductive. In view of such arguments, linguists such as Downing (1977), Ryder (1994) and Coulson (2001) have argued for a fine-grained, essentially open-ended space of interpretations. The idea of working with fine-grained paraphrases for NC semantics has recently grown in popularity among NLP researchers (Butnariu and Veale, 2008; Nakov and Hearst, 2008; Nakov, 2008a). Task 9 at SemEval-2010 (Butnariu et al., 2010) was devoted to this methodology. In that previous work, the paraphrases provided by human subjects were required to fit a restrictive template admitting only verbs and prepositions occurring between the NC’s constituent nouns. Annotators recruited through Amazon M</context>
</contexts>
<marker>Ryder, 1994</marker>
<rawString>Mary Ellen Ryder. 1994. Ordered Chaos: The Interpretation of English Noun-Noun Compounds. University of California Press, Berkeley, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Tanaka</author>
<author>Tim Baldwin</author>
</authors>
<title>Noun-noun compound machine translation: A feasibility study on shallow processing.</title>
<date>2003</date>
<booktitle>Proc. ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<pages>17--24</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="2752" citStr="Tanaka and Baldwin, 2003" startWordPosition="426" endWordPosition="429">actly once (Kim and Baldwin, 2006). Their high frequency and high productivity make robust NC interpretation an important goal for broad-coverage semantic processing of English texts. Systems which ignore NCs may give up on salient information about the semantic relationships implicit in a text. Compositional interpretation is also the only way to achieve broad NC coverage, because it is not feasible to list in a lexicon all compounds which one is likely to encounter. Even for relatively frequent NCs occurring 10 times or more in the BNC, static English dictionaries provide only 27% coverage (Tanaka and Baldwin, 2003). In many natural language processing applications it is important to understand the syntax and semantics of NCs. NCs often are structurally similar, but have very different meaning. Consider caffeine headache and ice-cream headache: a lack of caffeine causes the former, an excess of ice-cream – the latter. Different interpretations can lead to different inferences, query expansion, paraphrases, translations, and so on. A question answering system may have to determine whether protein acting as a tumor suppressor is an accurate paraphrase for tumor suppressor protein. An information extraction</context>
</contexts>
<marker>Tanaka, Baldwin, 2003</marker>
<rawString>Takaaki Tanaka and Tim Baldwin. 2003. Noun-noun compound machine translation: A feasibility study on shallow processing. Proc. ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, Sapporo, Japan, 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A taxonomy, dataset, and classifier for automatic noun compound interpretation.</title>
<date>2010</date>
<booktitle>Proc. 48th Annual Meeting of the Association for Computational Linguistics ACL-10,</booktitle>
<pages>678--687</pages>
<location>Uppsala,</location>
<contexts>
<context position="4266" citStr="Tratz and Hovy, 2010" startWordPosition="656" endWordPosition="659">tional Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, 2007; O´ S´eaghdha and Copestake, 2008; Tratz and Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different</context>
</contexts>
<marker>Tratz, Hovy, 2010</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2010. A taxonomy, dataset, and classifier for automatic noun compound interpretation. Proc. 48th Annual Meeting of the Association for Computational Linguistics ACL-10, Uppsala, Sweden, 678-687.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>