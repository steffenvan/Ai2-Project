<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029830">
<title confidence="0.98736">
Painless Labeling with Application to Text Mining
</title>
<author confidence="0.991139">
Sajib Dasgupta
</author>
<affiliation confidence="0.8077">
Chittagong Indepedent University
Chittagong, Bangladesh
</affiliation>
<email confidence="0.996769">
sdgnew@gmail.com
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999753785714286">
Labeled data is not readily available for
many natural language domains, and it
typically requires expensive human effort
with considerable domain knowledge to
produce a set of labeled data. In this paper,
we propose a simple unsupervised system
that helps us create a labeled resource for
categorical data (e.g., a document set) us-
ing only fifteen minutes of human input.
We utilize the labeled resources to dis-
cover important insights about the data.
The entire process is domain independent,
and demands no prior annotation samples,
or rules specific to an annotation.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999548389830508">
Consider the following two scenarios:
Scenario 1: We start processing a new language
and we want to get an initial idea of the language
before embarking on the expensive process of cre-
ating hand annotated resources. For instance, we
may want to know how people express opinion in
a language of interest, what characterizes the sub-
jective content of the language and how expres-
sions of opinion differ along opinion types. The
question is how to acquire such first-hand insights
of an unknown language in quick time and with
minimal human effort?
Scenario 2: We have a set of blog articles and
we are interested in learning how blogging differs
across gender. In particular, we seek to learn the
writing styles or other indicative patterns – topics
of interest, word choices etc. – that can potentially
distinguish writings across gender. A traditional
NLP approach would be to collect a set of articles
that are tagged with gender information, which we
can then input to a learning system to learn pat-
terns that can differentiate gender. What if no such
annotation is available, as the bloggers don’t re-
veal their gender information? Could we arrange
a human annotation task to annotate the articles
along gender? Often the articles contain explicit
patterns (e.g., “my boyfriend”, “as a woman” etc.)
which help the annotators to annotate the articles.
Often there are no indicative patterns in the writ-
ten text, and it becomes impossible to annotate the
articles reliably.
The above scenarios depict the cases when we
are resource constrained and creating a new re-
source is nontrivial and time consuming. Given
such difficulties, it would be helpful if we could
design a system that requires less human input to
create a labeled resource. In this paper, we present
a simple unsupervised system that helps us cre-
ate a labeled resource with minimal human effort.
The key to our method is that instead of label-
ing the entire set of unlabeled instances the sys-
tem labels a subset of data instances for which it
is confident to achieve high level of accuracy. We
experiment with several document labeling tasks
and show that a high-quality labeled resource can
be produced by a clustering-based labeling system
that requires a mere fifteen minutes of human in-
put. It achieves 85% and 78% accuracy for the task
of sentiment and gender classification, showing its
effectiveness on two nontrivial labeling tasks with
distinct characteristics (see Section 3).
We also utilize the labeled resources created by
our system to learn discriminative patterns that
help us gain insights into a dataset. For instance,
we learn how users generally express opinion in a
language of interest, and how writing varies across
gender. The next section describes the details of
our main algorithm. We present experimental re-
sults in Section 3 and 4.
</bodyText>
<page confidence="0.970896">
402
</page>
<bodyText confidence="0.932880625">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 402–407,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
The sound from my system did seem to be a little better
(the CD’s were not skipping as much).
But the bottom line is it didn’t fix the problem as
the CDs are still skipping noticeably,
although not as bad as before...
</bodyText>
<tableCaption confidence="0.9282335">
Table 1: Snippet of an ambiguous CD Player re-
view.
</tableCaption>
<sectionHeader confidence="0.887203" genericHeader="method">
2 Problem Formulation
</sectionHeader>
<bodyText confidence="0.99992325">
We consider a general classification framework.
Let X = {x1, ... , xn} represents a categorical
dataset with n data points where xi ∈ ℜd. Let
cx ∈ {1,-1} is the true label of x1. Our goal is
label a subset of the data, X′ = {C1, C2} ⊆ X,
where C1 and C2 comprise data points of positive
and negative class respectively. Note that, X′ rep-
resents the subset of datapoints that are confidently
labeled by the system.
To illustrate, we show a snippet of a CD player
review taken from Amazon in Table 1. As you can
see this review is highly ambiguous, as it describes
both the positive and negative aspects of the prod-
uct: while the phrases a little better, not skipping,
and not as bad conveys a positive sentiment, the
phrases didn’t fir and skipping noticeably are neg-
ative sentiment-bearing. Any automated system
would find it hard to correctly label this review,
as the review is highly ambiguous. Our goal is to
remove such ambiguous data points from the data
space and label the remaining unambiguous data
points. The fact that unambiguous data instances
are easier to label allows us to use an automated
system to label them quickly with minimal human
effort (see the next section).
Now how could we set apart unambiguous data
points from the ambiguous ones from a set of unla-
beled data points? Note that we desire the system
to be unsupervised. We also desire the system to
be generic i.e., applicable to any application do-
main. Next we show how we extend spectral clus-
tering to achieve this goal.
</bodyText>
<subsectionHeader confidence="0.9984945">
2.1 Ambiguity Resolution with Iterative
Spectral Clustering
</subsectionHeader>
<bodyText confidence="0.999206">
In spectral clustering, a set of n data points is rep-
resented as an undirected graph, where each node
corresponds to a data point and the edge weight
between two nodes is their similarity as defined
by S. The goal is to induce a clustering, or equiv-
alently, a partitioning function f, which is typi-
cally represented as a vector of length n such that
</bodyText>
<footnote confidence="0.8960705">
1We present our system for binary classification task. It
can be extended fairly easily to multi-way classification tasks.
</footnote>
<bodyText confidence="0.9630818">
f(i) ∈ {1, −1} indicates which of the two clusters
data point i should be assigned to.
In spectral clustering, the normalized cut parti-
tion of a similarity graph, S is derived from the
solution of the following constrained optimization
</bodyText>
<equation confidence="0.991227">
problem: argmin j 7
Si (f (i) − f (j) )2
fE t �i Vdi √dj
</equation>
<bodyText confidence="0.994175586206896">
subject to fT Df = 1 and Df ⊥ 1, where
D is a diagonal matrix with Di,i = E j Si,j and
di = Di,i. The closed form solution to this opti-
mization problem is the eigenvector corresponding
to the second smallest eigenvalue of the Laplacian
matrix, L = D−1/2(D − S)D−1/2 (Shi and Ma-
lik (2000)). Clustering using the second eigenvec-
tor, is trivial: since we have a linearization of the
points, all we need to do is to determine a thresh-
old for partitioning the data points.
Second eigenvector reveals useful information
regarding the ambiguity of the individual data
points. In the computation of eigenvectors each
data point factors out orthogonal projections of
each of the neighboring data points. Ambigu-
ous data points factor out orthogonal projections
from both the positive and negative data instances,
and hence they have near zero values in the pivot
eigenvectors. We exploit this important informa-
tion. The basic idea is that the data points with
near zero values in the second eigenvector are
more ambiguous than those with large absolute
values. Hence, to cluster only the unambiguous
datapoints, we can therefore sort the data points
according to second eigenvector, and keep only the
top and bottom m(m &lt; n) datapoints. Finally, in-
stead of removing (n − m) datapoints at once, we
remove them in iteration.
Here is our final algorithm:
</bodyText>
<listItem confidence="0.999096833333333">
1. Let s : X × X → ℜ be a similarity function
defined over data X. Construct a similarity
matrix S such that Sij = s(xi, xj).
2. Construct the Laplacian matrix L =
D−1/2(D − S)D−1/2, where D is a diago-
nal matrix with Di,i = Ej Si,j.
3. Find eigenvector e2 corresponding to second
smallest eigenvalue of L.
4. Sort X according to e2 and remove α points
indexed from (|X|/2−α/2+1) to (|X|/2+
α/2).
5. If |X |= m, goto Step 6; else goto Step 1.
</listItem>
<page confidence="0.99584">
403
</page>
<table confidence="0.9998932">
Dataset System m= 5n m= 5 n m= 3�n m= 5n m= n Fully Supervised
Gender Kmeans++ 52.3% 51.6% 52.3% 51.7% 51.2% -
TSVM 53.1% 53.6% 52.7% 52.6% 52.0% 80.4%
OUR 78.5% 73.7% 69.3% 66.8% 64.4% -
Spam Kmeans++ 67.6% 58.6% 54.9% 53.8% 52.4% -
TSVM 87.8% 85.0% 82.7% 80.7% 78.9% 96.9%
OUR 83.8% 82.9% 80.4% 79.8% 78.4% -
Sentiment Kmeans++ 64.5% 61.4% 60.5% 57.8% 56.5% -
TSVM 70.2% 65.1% 61.5% 61.8% 60.4% 86.4%
OUR 90.3% 85.4% 79.9% 74.9% 71.2% -
</table>
<tableCaption confidence="0.9585045">
Table 2: Accuracy of automatically labeled data for each dataset. We also report 5-fold supervised
classification result for each dataset.
</tableCaption>
<listItem confidence="0.393305">
6. Sort X according to e2 and put top m2 data
</listItem>
<bodyText confidence="0.997685619047619">
points in cluster C1 and bottom m2 data points
in cluster C2.
In the algorithm stated above, we start with an
initial clustering of all of the data points, and then
iteratively remove the α most ambiguous points
from the data space. We iterate the process of re-
moving ambiguous data points and re-clustering
until we have m data points remaining. It should
not be difficult to see the advantage of removing
the data points in an iterative fashion (as opposed
to removing them in a single iteration): the clus-
ters produced in a given iteration are supposed
to be better than those in the previous iterations,
as subsequent clusterings are generated from less
ambiguous points. In all our experiments, we set
α to 100. Finally, we label the clusters by inspect-
ing 10 randomly sampled points from each cluster.
We use the cluster labels to assign labels to the m
unambiguous data points. Note that labeling the
clusters is the only form of human input we re-
quire in our system.
</bodyText>
<sectionHeader confidence="0.999755" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.997948620689655">
We use three text classification tasks for evalua-
tion:
Gender Classification: Here we classify blog
articles according to whether an article is written
by a male or female. We employ the blog dataset
as introduced by Schler et al. (2006) for this task.
The dataset contains 19320 blog articles, out of
which we randomly selected 5000 blog articles as
our dataset.
Spam Classification: Here the goal is to deter-
mine whether an email is Spam or Ham (i.e., not
spam). We use the Enron spam dataset as intro-
duced by Metris et al. (Metsis et al. (2006)). We
join together the BG section of Spam emails and
kaminski section of Ham emails, and randomly se-
lected 5000 emails as our dataset.
Sentiment Classification: Here the goal is to
determine whether the sentiment expressed in a
product review is positive or negative. We use
Pang et al.’s movie review dataset for this task
(Pang et al. (2002)). The dataset contains 2000
reviews annotated with the positive and negative
sentiment label.
To preprocess a document, we first tokenize and
downcase it, remove stop words, and represent it
as a vector of unigrams, using frequency as pres-
ence. For spectral clustering, we use dot product
as a measure of similarity between two documents
vectors.
</bodyText>
<table confidence="0.99908975">
Dataset Data points Features Pos:Neg
Gender 5000 75188 2751:2249
Spam 5000 23760 2492:2508
Sentiment 2000 24531 1000:1000
</table>
<tableCaption confidence="0.999692">
Table 3: Description of the datasets.
</tableCaption>
<subsectionHeader confidence="0.999206">
3.1 Accuracy of Automatically Labeled Data
</subsectionHeader>
<bodyText confidence="0.999989285714286">
For each dataset, given n unlabeled data points,
we apply our system to label m(m &lt;= n) least
ambiguous data points. We check the quality of
labeled data by comparing the assigned (cluster)
labels of m datapoints against their true labels,
and show the accuracy. Table 2 shows the accu-
racy of automatically labeled data for five different
values of m for each dataset. For example, when
m = n/5, our system labels 1000 out of available
5000 data points with 78.5% accuracy for the gen-
der dataset. These 1000 data points are the most
unambiguous out of the 5000 data points, as se-
lected by the algorithm. For m = n the system
labels the entire dataset.
As you can see, for all three datasets, the ac-
curacy of labeling unambiguous data instances is
much higher than the accuracy of labeling the en-
tire dataset. For instance, the accuracy of top n/5
unambiguous labeled instances of the sentiment
dataset is 90.3%, whereas the accuracy of labeling
the entire dataset is 71.2%. The more unambigu-
</bodyText>
<page confidence="0.99594">
404
</page>
<bodyText confidence="0.986462125">
ous the data instances are the higher is the qual-
ity of labeled data (as shown by the fact that the
accuracy of labeled instances increases as we in-
crease m). Notice that our system labels 60% of
the data points of the spam dataset with 80.4% ac-
curacy; 40% of the data points of the sentiment
dataset with 85.4% accuracy; and 20% of the data
points of the gender dataset with 78.5% accuracy.
We also report 5-fold supervised classification
result for each dataset. We used linear SVM for
classification with all parameters set to their de-
fault values. As you can see, when m = n/5 our
system achieves near supervised labeling perfor-
mance for the gender and sentiment dataset. One
of the reviewers asked how SVM performed when
trained with unambiguous data instances alone.
We refer to Dasgupta and Ng (2009) where the au-
thors report that training SVM with unambiguous
data alone produces rather inferior result. They,
however, work on a small data sample. It would
be interesting to know whether large number of
unambiguous (or, semi-ambiguous) data instances
could offset the need for ambiguous data in a gen-
eral classification setting. Given that unlabeled
data are abundantly available in many NLP tasks,
one can employ our method to create decent size
labeled data quickly from unlabeled data, and uti-
lize them later in the process to build an indepen-
dent classifier or augment the performance of an
existing classifier (Fuxman et al. (2009)).
We employed two baseline algorithms, i.e.,
kmeans++ and a semi-supervised learning system,
Transductive SVM. For kmeans++ we used the
following as a measure of ambiguity for each data
point: 1− (,x−µc )a a , where x is a data vector and
Ei (X−µc)
µi, i = 1 :k are k mean vectors. It ranges from
0 to 1. Ambiguity score near 0.5 suggests that
the data point is ambiguous. Following common
practice in document clustering, we reduced the
dimensions of the data to 100 using SVD before
we apply kmeans++. For transductive SVM, we
randomly selected 20 labeled data points as seeds.
Table 2 shows the result for each baseline.
Notice that our system beat the baselines (one
of them is a semisupervised system) by a big mar-
gin for the Gender and Sentiment dataset, whereas
Transductive SVM performs the best for the Spam
dataset. Interesting to point that our method of re-
moving ambiguous data instances to get a qualita-
tively stronger clustering contrasts with the max-
margin methods which use the ambiguous data
instances to acquire the margin. Also impor-
tant to mention that spectral clustering is a graph-
based clustering algorithm, where similarity mea-
sure employed to construct the graph plays a cru-
cial role in performance (Maier et al. (2013)). In
fact, “right” construction of the feature space and a
right similarity measure can considerably change
the performance of a graph-based clustering algo-
rithm. We have not tried different similarity mea-
sures in this initial study, but it provides us room
for improvement for a dataset like Spam.
Implementation Details: On a machine with
3GHz of Intel Quad Core Processor and 4GB of
RAM, the iterative spectral clustering algorithm
takes less than 2 minutes in Matlab for a dataset
comprising 5000 data points and 75188 features.
This along with the fact that human labelers take
on average 12 minutes to label the clusters sug-
gests that the entire labeling process requires less
than 15 minutes to complete.
</bodyText>
<sectionHeader confidence="0.8572" genericHeader="method">
4 Mining Patterns and Insights
</sectionHeader>
<bodyText confidence="0.999426882352941">
In this section, we show that we can utilize the
labeled resources created by our system to learn
discriminative patterns that help us gain insights
into a dataset (Don et al. (2007), Larsen and Aone
(1999), Cheng et al. (2007), Maiya et al. (2013)).
We utilize the top n/5 unambiguous labeled in-
stances for this task, where n is size of the dataset.
Note that the quality of unambiguous labeled in-
stances is much higher than the entire set of la-
beled instances (see Section 3.1), so the statis-
tics we collect from the unambiguous labeled in-
stances to identify discriminative patterns are sup-
posedly more reliable.
We learn our first category of discriminative
patterns the following way: for each cluster,
we rank all unigrams in the vocabulary by their
weighted log-likelihood ratio:
</bodyText>
<equation confidence="0.9991205">
P (wt  |cj)
P(wt  |cj) · log P(wt  |¬cj)
</equation>
<bodyText confidence="0.999965555555556">
where wt and cj denote the t-th word in the vocab-
ulary and the j-th cluster, respectively, and each
conditional probability is add one smoothed. In-
formally, a unigram w will have a high rank with
respect to a cluster c if it appears frequently in
c and infrequently in ¬c. The higher the score
the more discriminative the pattern is. We also
learn the discriminative bigrams similarly: for
each cluster, we rank all bigrams by their weighted
</bodyText>
<page confidence="0.997016">
405
</page>
<table confidence="0.999597285714286">
Dataset Class Top Discriminative Unigrams
Gender Female haha, wanna, sooo, lol, ppl, omg, hahaha, ur, yay, soo, cuz, bye, soooo, hehe, ate, hurts, sucks.
Male provide, reported, policies, administration, companies, development, policy, services, nations.
Spam Spam vicodin, goodbye, utf, rolex, watches, loading, promotion, reproductions, nepel, fw, fwd, click.
Ham risk, securities, statements, exchange, terms, third, events, act, investing, objectives, assumptions.
Sentiment Positive relationship, husband, effective, mother, strong, perfect, tale, novel, fascinating, outstanding.
Negative stupid, worst, jokes, bunch, sequel, lame, guess, dumb, boring, maybe, guys, video, flick, oh.
</table>
<tableCaption confidence="0.987717">
Table 4: Top discriminative unigram patterns identified by our system.
</tableCaption>
<table confidence="0.999681142857143">
Dataset Class Top Discriminative Bigrams
Gender Female wanna go, im so, im gonna, at like, don’t wanna, was sooo, was gonna, soo much, so yeah.
Male to provide, york times, the issue, understanding of, the political, bush admin, the democratic.
Spam Spam promotional material, adobe photoshop, name it, choose from, you name, stop getting, office xp.
Ham investment advice, this report, respect to, current price, risks and, information provided.
Sentiment Positive story of, her husband, relationship with, begins to, love and, life of, the central, the perfect.
Negative the worst, bad movie, bunch of, got to, too bad, action sequences, waste of, than this, the bad.
</table>
<tableCaption confidence="0.999788">
Table 5: Top discriminative bigram patterns identified by our system.
</tableCaption>
<bodyText confidence="0.999503428571428">
log-likelihood ratio score and select the top scor-
ing bigrams as the most discriminative bigrams.
Table 4 and 5 show the most discriminative un-
igrams and bigrams learned by our system. No-
tice that the learned patterns are quite informa-
tive. For instance, in the case of blog dataset we
learn that certain word usages (e.g., sooo, cuz etc.)
are more common in women’s writings, whereas
men’s writings often contain discussion of poli-
tics, news and technology. For sentiment data, the
patterns correspond well to the generic sentiment
lexicon manually created by the sentiment experts.
The ability of our system to learn top sentiment
features could be handy for a resource-scarce lan-
guage, which may not have a general purpose sen-
timent lexicon. Note that the system is not lim-
ited to unigram and bigram patterns only. The la-
beled instances can be utilized similarly to gather
statistics for other form of usage patterns includ-
ing syntactic and semantic patterns for document
collections.
</bodyText>
<sectionHeader confidence="0.999876" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9998312">
Automatic extraction of labeled data has gained
momentum in recent years (Durme and Pasca
(2008), Nakov and Hearst (2005), Fuxman et
al. (2009)). Traditionally, researchers use task-
specific heuristics to generate labeled data, e.g.,
searching for a specific pattern in the web to col-
lect data instances of a particular category (Hearst
(1992), Go et al. (2009), Hu et al. (2013)). An-
other line of research follows semi-supervised in-
formation extraction task, where given a list of
seed instances of a particular category, a bootstrap-
ping algorithm is applied to mine new instances
from large corpora (Riloff and Jones (1999), Et-
zioni et al. (2005), Durme and Pasca (2008)).
There has also been a surge of interests in unsu-
pervised approaches which primarily rely on clus-
tering to induce psuedo labels from large amount
of text (Clark (2000), Slonim and Tishby (2000),
Sahoo et al. (2006), Christodoulopoulos et al.
(2010)). We differ from existing unsupervised
clustering algorithms in a way that we uncompli-
cate spectral clustering by forcing it to cluster un-
ambiguous data points only, which ensures that the
system makes less mistakes during clustering and
the clustered data are qualitatively strong.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999991588235294">
We have presented a system that helps us create
a labeled resource for a given dataset with mini-
mal human effort. We also utilize the labeled
resources to discover important insights about the
data. The ability of our system to learn and vi-
sualize top discriminative patterns facilitates ex-
ploratory data analysis for a dataset that might be
unknown to us. Even if we have some knowledge
of the data, the system may unveil additional char-
acterisitcs that are unknown to us. The top fea-
tures induced for each classification task can also
be interpreted as our system’s ability to discover
new feature spaces, which can be utilized inde-
pendently or along with a simpler feature space
(e.g., bag of words) to learn a better classification
model. Additional research is needed to further
explore this idea.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999105">
We acknowledge three anonymous reviewers and
Vincent Ng for their valuable feedback on an ear-
lier draft of the paper.
</bodyText>
<page confidence="0.998665">
406
</page>
<sectionHeader confidence="0.990154" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99929359375">
H. Cheng, X. Yan, J. Han, and C. Hsu. 2007. Discrim-
inative frequent pattern analysis for effective classi-
fication. In International Conference on Data Engi-
neering (ICDE).
C. Christodoulopoulos, S. Goldwater, and M. Steed-
man. 2010. Two decades of unsupervised pos
induction: How far have we come? In Em-
pirical Methods in Natural Language Processing
(EMNLP).
Alexander Clark. 2000. Inducing syntactic categories
by context distributional clustering. In the Confer-
ence on Natural Language Learning (CoNLL).
S. Dasgupta and V. Ng. 2009. Mine the easy, clas-
sify the hard: A semi-supervised approach to au-
tomatic sentiment classification. In ACL-IJCNLP
2009: Proceedings of the Main Conference.
A. Don, E. Zheleva, M. Gregory, S. Tarkan, L. Auvil,
T. Clement, B. Shneiderman, and C. Plaisant. 2007.
Discovering interesting usage patterns in text col-
lections: integrating text mining with visualization.
In Proceedings of the ACM International Confer-
ence on Information and Knowledge Management
(CIKM).
Benjamin Van Durme and Marius Pasca. 2008. Find-
ing cars, goddesses and enzymes: Parametrizable
acquisition of labeled instances for open-domain in-
formation extraction. In the AAAI Conference on Ar-
tificial Intelligence (AAAI).
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from
the web: an experimental study. In Artificial Intelli-
gence.
A. Fuxman, A. Kannan, A. Goldberg, R. Agrawal,
P. Tsaparas, and J. Shafer. 2009. Improving classifi-
cation accuracy using automatically extracted train-
ing data. In 15th ACM Conference on Knowledge
Discovery and Data Mining (SIGKDD).
A Go, R Bhayani, and L Huang. 2009. Twitter sen-
timent classification using distant supervision. In
Project Report, Stanford University.
M. A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In the Inter-
national Conference on Computational Linguistics
(COLING).
X. Hu, J. Tang, H. Gao, and H. Liu. 2013. Unsuper-
vised sentiment analysis with emotional signals. In
In the Proceedings of the International World Wide
Web Conference (WWW).
B. Larsen and C. Aone. 1999. Fast and effective text
mining using linear-time document clustering. In
the Conference on Knowledge Discovery and Data
Mining (SIGKDD).
M. Maier, U. von Luxburg, and M. Hein. 2013. How
the result of graph clustering methods depends on
the construction of the graph. In ESAIM: Probabil-
ity and Statistics, vol. 17.
A. S. Maiya, J. P. Thompson, F. Loaiza-Lemos, and
R. M. Rolfe. 2013. Exploratory analysis of highly
heterogeneous document collections. In the Con-
ference on Knowledge Discovery and Data Mining
(SIGKDD).
V. Metsis, I. Androutsopoulos, and G. Paliouras. 2006.
Spam filtering with naive bayes - which naive bayes?
In 3rd Conference on Email and Anti-Spam (CEAS).
Preslav Nakov and Marti Hearst. 2005. Using the web
as an implicit training set: Application to structural
ambiguity resolution. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification us-
ing machine learning techniques. In Proceedings of
Empirical Methods in Natural Language Processing
(EMNLP).
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the ACM International Confer-
ence on Information and Knowledge Management
(CIKM).
N. Sahoo, J. Callan, R. Krishnan, G. Duncan, and
R. Padman. 2006. Incremental hierarchical cluster-
ing of text documents. In the International Confer-
ence on Information and Knowledge Management
(CIKM).
J. Schler, M. Koppel, S. Argamon, and J. Pennebaker.
2006. Effects of age and gender in blogging. In
AAAI Symposium on Computational Approaches for
Analyzing Weblogs.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence.
Noam Slonim and Naftali Tishby. 2000. Document
clustering using word clusters via the information
bottleneck method. In Proceedings of the ACM SI-
GIR Conference on Research and Development in
Information Retrieval.
</reference>
<page confidence="0.9983">
407
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.179060">
<title confidence="0.999151">Painless Labeling with Application to Text Mining</title>
<author confidence="0.255355">Sajib</author>
<affiliation confidence="0.342582">Chittagong Indepedent Chittagong,</affiliation>
<email confidence="0.999311">sdgnew@gmail.com</email>
<abstract confidence="0.9996718">Labeled data is not readily available for many natural language domains, and it typically requires expensive human effort with considerable domain knowledge to produce a set of labeled data. In this paper, we propose a simple unsupervised system that helps us create a labeled resource for categorical data (e.g., a document set) using only fifteen minutes of human input. We utilize the labeled resources to discover important insights about the data. The entire process is domain independent, and demands no prior annotation samples, or rules specific to an annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Cheng</author>
<author>X Yan</author>
<author>J Han</author>
<author>C Hsu</author>
</authors>
<title>Discriminative frequent pattern analysis for effective classification.</title>
<date>2007</date>
<booktitle>In International Conference on Data Engineering (ICDE).</booktitle>
<contexts>
<context position="15957" citStr="Cheng et al. (2007)" startWordPosition="2728" endWordPosition="2731">tel Quad Core Processor and 4GB of RAM, the iterative spectral clustering algorithm takes less than 2 minutes in Matlab for a dataset comprising 5000 data points and 75188 features. This along with the fact that human labelers take on average 12 minutes to label the clusters suggests that the entire labeling process requires less than 15 minutes to complete. 4 Mining Patterns and Insights In this section, we show that we can utilize the labeled resources created by our system to learn discriminative patterns that help us gain insights into a dataset (Don et al. (2007), Larsen and Aone (1999), Cheng et al. (2007), Maiya et al. (2013)). We utilize the top n/5 unambiguous labeled instances for this task, where n is size of the dataset. Note that the quality of unambiguous labeled instances is much higher than the entire set of labeled instances (see Section 3.1), so the statistics we collect from the unambiguous labeled instances to identify discriminative patterns are supposedly more reliable. We learn our first category of discriminative patterns the following way: for each cluster, we rank all unigrams in the vocabulary by their weighted log-likelihood ratio: P (wt |cj) P(wt |cj) · log P(wt |¬cj) whe</context>
</contexts>
<marker>Cheng, Yan, Han, Hsu, 2007</marker>
<rawString>H. Cheng, X. Yan, J. Han, and C. Hsu. 2007. Discriminative frequent pattern analysis for effective classification. In International Conference on Data Engineering (ICDE).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Christodoulopoulos</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Two decades of unsupervised pos induction: How far have we come?</title>
<date>2010</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="20425" citStr="Christodoulopoulos et al. (2010)" startWordPosition="3438" endWordPosition="3441">ata instances of a particular category (Hearst (1992), Go et al. (2009), Hu et al. (2013)). Another line of research follows semi-supervised information extraction task, where given a list of seed instances of a particular category, a bootstrapping algorithm is applied to mine new instances from large corpora (Riloff and Jones (1999), Etzioni et al. (2005), Durme and Pasca (2008)). There has also been a surge of interests in unsupervised approaches which primarily rely on clustering to induce psuedo labels from large amount of text (Clark (2000), Slonim and Tishby (2000), Sahoo et al. (2006), Christodoulopoulos et al. (2010)). We differ from existing unsupervised clustering algorithms in a way that we uncomplicate spectral clustering by forcing it to cluster unambiguous data points only, which ensures that the system makes less mistakes during clustering and the clustered data are qualitatively strong. 6 Conclusion We have presented a system that helps us create a labeled resource for a given dataset with minimal human effort. We also utilize the labeled resources to discover important insights about the data. The ability of our system to learn and visualize top discriminative patterns facilitates exploratory dat</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>C. Christodoulopoulos, S. Goldwater, and M. Steedman. 2010. Two decades of unsupervised pos induction: How far have we come? In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Inducing syntactic categories by context distributional clustering.</title>
<date>2000</date>
<booktitle>In the Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="20344" citStr="Clark (2000)" startWordPosition="3428" endWordPosition="3429">.g., searching for a specific pattern in the web to collect data instances of a particular category (Hearst (1992), Go et al. (2009), Hu et al. (2013)). Another line of research follows semi-supervised information extraction task, where given a list of seed instances of a particular category, a bootstrapping algorithm is applied to mine new instances from large corpora (Riloff and Jones (1999), Etzioni et al. (2005), Durme and Pasca (2008)). There has also been a surge of interests in unsupervised approaches which primarily rely on clustering to induce psuedo labels from large amount of text (Clark (2000), Slonim and Tishby (2000), Sahoo et al. (2006), Christodoulopoulos et al. (2010)). We differ from existing unsupervised clustering algorithms in a way that we uncomplicate spectral clustering by forcing it to cluster unambiguous data points only, which ensures that the system makes less mistakes during clustering and the clustered data are qualitatively strong. 6 Conclusion We have presented a system that helps us create a labeled resource for a given dataset with minimal human effort. We also utilize the labeled resources to discover important insights about the data. The ability of our syst</context>
</contexts>
<marker>Clark, 2000</marker>
<rawString>Alexander Clark. 2000. Inducing syntactic categories by context distributional clustering. In the Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dasgupta</author>
<author>V Ng</author>
</authors>
<title>Mine the easy, classify the hard: A semi-supervised approach to automatic sentiment classification.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP 2009: Proceedings of the Main Conference.</booktitle>
<contexts>
<context position="13103" citStr="Dasgupta and Ng (2009)" startWordPosition="2248" endWordPosition="2251">els 60% of the data points of the spam dataset with 80.4% accuracy; 40% of the data points of the sentiment dataset with 85.4% accuracy; and 20% of the data points of the gender dataset with 78.5% accuracy. We also report 5-fold supervised classification result for each dataset. We used linear SVM for classification with all parameters set to their default values. As you can see, when m = n/5 our system achieves near supervised labeling performance for the gender and sentiment dataset. One of the reviewers asked how SVM performed when trained with unambiguous data instances alone. We refer to Dasgupta and Ng (2009) where the authors report that training SVM with unambiguous data alone produces rather inferior result. They, however, work on a small data sample. It would be interesting to know whether large number of unambiguous (or, semi-ambiguous) data instances could offset the need for ambiguous data in a general classification setting. Given that unlabeled data are abundantly available in many NLP tasks, one can employ our method to create decent size labeled data quickly from unlabeled data, and utilize them later in the process to build an independent classifier or augment the performance of an exi</context>
</contexts>
<marker>Dasgupta, Ng, 2009</marker>
<rawString>S. Dasgupta and V. Ng. 2009. Mine the easy, classify the hard: A semi-supervised approach to automatic sentiment classification. In ACL-IJCNLP 2009: Proceedings of the Main Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Don</author>
<author>E Zheleva</author>
<author>M Gregory</author>
<author>S Tarkan</author>
<author>L Auvil</author>
<author>T Clement</author>
<author>B Shneiderman</author>
<author>C Plaisant</author>
</authors>
<title>Discovering interesting usage patterns in text collections: integrating text mining with visualization.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM).</booktitle>
<contexts>
<context position="15912" citStr="Don et al. (2007)" startWordPosition="2720" endWordPosition="2723">ation Details: On a machine with 3GHz of Intel Quad Core Processor and 4GB of RAM, the iterative spectral clustering algorithm takes less than 2 minutes in Matlab for a dataset comprising 5000 data points and 75188 features. This along with the fact that human labelers take on average 12 minutes to label the clusters suggests that the entire labeling process requires less than 15 minutes to complete. 4 Mining Patterns and Insights In this section, we show that we can utilize the labeled resources created by our system to learn discriminative patterns that help us gain insights into a dataset (Don et al. (2007), Larsen and Aone (1999), Cheng et al. (2007), Maiya et al. (2013)). We utilize the top n/5 unambiguous labeled instances for this task, where n is size of the dataset. Note that the quality of unambiguous labeled instances is much higher than the entire set of labeled instances (see Section 3.1), so the statistics we collect from the unambiguous labeled instances to identify discriminative patterns are supposedly more reliable. We learn our first category of discriminative patterns the following way: for each cluster, we rank all unigrams in the vocabulary by their weighted log-likelihood rat</context>
</contexts>
<marker>Don, Zheleva, Gregory, Tarkan, Auvil, Clement, Shneiderman, Plaisant, 2007</marker>
<rawString>A. Don, E. Zheleva, M. Gregory, S. Tarkan, L. Auvil, T. Clement, B. Shneiderman, and C. Plaisant. 2007. Discovering interesting usage patterns in text collections: integrating text mining with visualization. In Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Marius Pasca</author>
</authors>
<title>Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction.</title>
<date>2008</date>
<booktitle>In the AAAI Conference on Artificial Intelligence (AAAI).</booktitle>
<marker>Van Durme, Pasca, 2008</marker>
<rawString>Benjamin Van Durme and Marius Pasca. 2008. Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. In the AAAI Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>In Artificial Intelligence.</journal>
<contexts>
<context position="20151" citStr="Etzioni et al. (2005)" startWordPosition="3392" endWordPosition="3396">beled data has gained momentum in recent years (Durme and Pasca (2008), Nakov and Hearst (2005), Fuxman et al. (2009)). Traditionally, researchers use taskspecific heuristics to generate labeled data, e.g., searching for a specific pattern in the web to collect data instances of a particular category (Hearst (1992), Go et al. (2009), Hu et al. (2013)). Another line of research follows semi-supervised information extraction task, where given a list of seed instances of a particular category, a bootstrapping algorithm is applied to mine new instances from large corpora (Riloff and Jones (1999), Etzioni et al. (2005), Durme and Pasca (2008)). There has also been a surge of interests in unsupervised approaches which primarily rely on clustering to induce psuedo labels from large amount of text (Clark (2000), Slonim and Tishby (2000), Sahoo et al. (2006), Christodoulopoulos et al. (2010)). We differ from existing unsupervised clustering algorithms in a way that we uncomplicate spectral clustering by forcing it to cluster unambiguous data points only, which ensures that the system makes less mistakes during clustering and the clustered data are qualitatively strong. 6 Conclusion We have presented a system th</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. In Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fuxman</author>
<author>A Kannan</author>
<author>A Goldberg</author>
<author>R Agrawal</author>
<author>P Tsaparas</author>
<author>J Shafer</author>
</authors>
<title>Improving classification accuracy using automatically extracted training data.</title>
<date>2009</date>
<booktitle>In 15th ACM Conference on Knowledge Discovery and Data Mining (SIGKDD).</booktitle>
<contexts>
<context position="13741" citStr="Fuxman et al. (2009)" startWordPosition="2352" endWordPosition="2355"> report that training SVM with unambiguous data alone produces rather inferior result. They, however, work on a small data sample. It would be interesting to know whether large number of unambiguous (or, semi-ambiguous) data instances could offset the need for ambiguous data in a general classification setting. Given that unlabeled data are abundantly available in many NLP tasks, one can employ our method to create decent size labeled data quickly from unlabeled data, and utilize them later in the process to build an independent classifier or augment the performance of an existing classifier (Fuxman et al. (2009)). We employed two baseline algorithms, i.e., kmeans++ and a semi-supervised learning system, Transductive SVM. For kmeans++ we used the following as a measure of ambiguity for each data point: 1− (,x−µc )a a , where x is a data vector and Ei (X−µc) µi, i = 1 :k are k mean vectors. It ranges from 0 to 1. Ambiguity score near 0.5 suggests that the data point is ambiguous. Following common practice in document clustering, we reduced the dimensions of the data to 100 using SVD before we apply kmeans++. For transductive SVM, we randomly selected 20 labeled data points as seeds. Table 2 shows the r</context>
<context position="19647" citStr="Fuxman et al. (2009)" startWordPosition="3311" endWordPosition="3314">timent lexicon manually created by the sentiment experts. The ability of our system to learn top sentiment features could be handy for a resource-scarce language, which may not have a general purpose sentiment lexicon. Note that the system is not limited to unigram and bigram patterns only. The labeled instances can be utilized similarly to gather statistics for other form of usage patterns including syntactic and semantic patterns for document collections. 5 Related Work Automatic extraction of labeled data has gained momentum in recent years (Durme and Pasca (2008), Nakov and Hearst (2005), Fuxman et al. (2009)). Traditionally, researchers use taskspecific heuristics to generate labeled data, e.g., searching for a specific pattern in the web to collect data instances of a particular category (Hearst (1992), Go et al. (2009), Hu et al. (2013)). Another line of research follows semi-supervised information extraction task, where given a list of seed instances of a particular category, a bootstrapping algorithm is applied to mine new instances from large corpora (Riloff and Jones (1999), Etzioni et al. (2005), Durme and Pasca (2008)). There has also been a surge of interests in unsupervised approaches w</context>
</contexts>
<marker>Fuxman, Kannan, Goldberg, Agrawal, Tsaparas, Shafer, 2009</marker>
<rawString>A. Fuxman, A. Kannan, A. Goldberg, R. Agrawal, P. Tsaparas, and J. Shafer. 2009. Improving classification accuracy using automatically extracted training data. In 15th ACM Conference on Knowledge Discovery and Data Mining (SIGKDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Go</author>
<author>R Bhayani</author>
<author>L Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. In Project Report,</title>
<date>2009</date>
<institution>Stanford University.</institution>
<contexts>
<context position="19864" citStr="Go et al. (2009)" startWordPosition="3345" endWordPosition="3348">te that the system is not limited to unigram and bigram patterns only. The labeled instances can be utilized similarly to gather statistics for other form of usage patterns including syntactic and semantic patterns for document collections. 5 Related Work Automatic extraction of labeled data has gained momentum in recent years (Durme and Pasca (2008), Nakov and Hearst (2005), Fuxman et al. (2009)). Traditionally, researchers use taskspecific heuristics to generate labeled data, e.g., searching for a specific pattern in the web to collect data instances of a particular category (Hearst (1992), Go et al. (2009), Hu et al. (2013)). Another line of research follows semi-supervised information extraction task, where given a list of seed instances of a particular category, a bootstrapping algorithm is applied to mine new instances from large corpora (Riloff and Jones (1999), Etzioni et al. (2005), Durme and Pasca (2008)). There has also been a surge of interests in unsupervised approaches which primarily rely on clustering to induce psuedo labels from large amount of text (Clark (2000), Slonim and Tishby (2000), Sahoo et al. (2006), Christodoulopoulos et al. (2010)). We differ from existing unsupervised</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>A Go, R Bhayani, and L Huang. 2009. Twitter sentiment classification using distant supervision. In Project Report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="19846" citStr="Hearst (1992)" startWordPosition="3343" endWordPosition="3344">ent lexicon. Note that the system is not limited to unigram and bigram patterns only. The labeled instances can be utilized similarly to gather statistics for other form of usage patterns including syntactic and semantic patterns for document collections. 5 Related Work Automatic extraction of labeled data has gained momentum in recent years (Durme and Pasca (2008), Nakov and Hearst (2005), Fuxman et al. (2009)). Traditionally, researchers use taskspecific heuristics to generate labeled data, e.g., searching for a specific pattern in the web to collect data instances of a particular category (Hearst (1992), Go et al. (2009), Hu et al. (2013)). Another line of research follows semi-supervised information extraction task, where given a list of seed instances of a particular category, a bootstrapping algorithm is applied to mine new instances from large corpora (Riloff and Jones (1999), Etzioni et al. (2005), Durme and Pasca (2008)). There has also been a surge of interests in unsupervised approaches which primarily rely on clustering to induce psuedo labels from large amount of text (Clark (2000), Slonim and Tishby (2000), Sahoo et al. (2006), Christodoulopoulos et al. (2010)). We differ from exi</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Hu</author>
<author>J Tang</author>
<author>H Gao</author>
<author>H Liu</author>
</authors>
<title>Unsupervised sentiment analysis with emotional signals.</title>
<date>2013</date>
<booktitle>In In the Proceedings of the International World Wide Web Conference (WWW).</booktitle>
<contexts>
<context position="19882" citStr="Hu et al. (2013)" startWordPosition="3349" endWordPosition="3352"> is not limited to unigram and bigram patterns only. The labeled instances can be utilized similarly to gather statistics for other form of usage patterns including syntactic and semantic patterns for document collections. 5 Related Work Automatic extraction of labeled data has gained momentum in recent years (Durme and Pasca (2008), Nakov and Hearst (2005), Fuxman et al. (2009)). Traditionally, researchers use taskspecific heuristics to generate labeled data, e.g., searching for a specific pattern in the web to collect data instances of a particular category (Hearst (1992), Go et al. (2009), Hu et al. (2013)). Another line of research follows semi-supervised information extraction task, where given a list of seed instances of a particular category, a bootstrapping algorithm is applied to mine new instances from large corpora (Riloff and Jones (1999), Etzioni et al. (2005), Durme and Pasca (2008)). There has also been a surge of interests in unsupervised approaches which primarily rely on clustering to induce psuedo labels from large amount of text (Clark (2000), Slonim and Tishby (2000), Sahoo et al. (2006), Christodoulopoulos et al. (2010)). We differ from existing unsupervised clustering algori</context>
</contexts>
<marker>Hu, Tang, Gao, Liu, 2013</marker>
<rawString>X. Hu, J. Tang, H. Gao, and H. Liu. 2013. Unsupervised sentiment analysis with emotional signals. In In the Proceedings of the International World Wide Web Conference (WWW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Larsen</author>
<author>C Aone</author>
</authors>
<title>Fast and effective text mining using linear-time document clustering.</title>
<date>1999</date>
<booktitle>In the Conference on Knowledge Discovery and Data Mining (SIGKDD).</booktitle>
<contexts>
<context position="15936" citStr="Larsen and Aone (1999)" startWordPosition="2724" endWordPosition="2727"> machine with 3GHz of Intel Quad Core Processor and 4GB of RAM, the iterative spectral clustering algorithm takes less than 2 minutes in Matlab for a dataset comprising 5000 data points and 75188 features. This along with the fact that human labelers take on average 12 minutes to label the clusters suggests that the entire labeling process requires less than 15 minutes to complete. 4 Mining Patterns and Insights In this section, we show that we can utilize the labeled resources created by our system to learn discriminative patterns that help us gain insights into a dataset (Don et al. (2007), Larsen and Aone (1999), Cheng et al. (2007), Maiya et al. (2013)). We utilize the top n/5 unambiguous labeled instances for this task, where n is size of the dataset. Note that the quality of unambiguous labeled instances is much higher than the entire set of labeled instances (see Section 3.1), so the statistics we collect from the unambiguous labeled instances to identify discriminative patterns are supposedly more reliable. We learn our first category of discriminative patterns the following way: for each cluster, we rank all unigrams in the vocabulary by their weighted log-likelihood ratio: P (wt |cj) P(wt |cj)</context>
</contexts>
<marker>Larsen, Aone, 1999</marker>
<rawString>B. Larsen and C. Aone. 1999. Fast and effective text mining using linear-time document clustering. In the Conference on Knowledge Discovery and Data Mining (SIGKDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maier</author>
<author>U von Luxburg</author>
<author>M Hein</author>
</authors>
<title>How the result of graph clustering methods depends on the construction of the graph.</title>
<date>2013</date>
<booktitle>In ESAIM: Probability and Statistics,</booktitle>
<volume>17</volume>
<marker>Maier, von Luxburg, Hein, 2013</marker>
<rawString>M. Maier, U. von Luxburg, and M. Hein. 2013. How the result of graph clustering methods depends on the construction of the graph. In ESAIM: Probability and Statistics, vol. 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Maiya</author>
<author>J P Thompson</author>
<author>F Loaiza-Lemos</author>
<author>R M Rolfe</author>
</authors>
<title>Exploratory analysis of highly heterogeneous document collections.</title>
<date>2013</date>
<booktitle>In the Conference on Knowledge Discovery and Data Mining (SIGKDD).</booktitle>
<contexts>
<context position="15978" citStr="Maiya et al. (2013)" startWordPosition="2732" endWordPosition="2735">or and 4GB of RAM, the iterative spectral clustering algorithm takes less than 2 minutes in Matlab for a dataset comprising 5000 data points and 75188 features. This along with the fact that human labelers take on average 12 minutes to label the clusters suggests that the entire labeling process requires less than 15 minutes to complete. 4 Mining Patterns and Insights In this section, we show that we can utilize the labeled resources created by our system to learn discriminative patterns that help us gain insights into a dataset (Don et al. (2007), Larsen and Aone (1999), Cheng et al. (2007), Maiya et al. (2013)). We utilize the top n/5 unambiguous labeled instances for this task, where n is size of the dataset. Note that the quality of unambiguous labeled instances is much higher than the entire set of labeled instances (see Section 3.1), so the statistics we collect from the unambiguous labeled instances to identify discriminative patterns are supposedly more reliable. We learn our first category of discriminative patterns the following way: for each cluster, we rank all unigrams in the vocabulary by their weighted log-likelihood ratio: P (wt |cj) P(wt |cj) · log P(wt |¬cj) where wt and cj denote t</context>
</contexts>
<marker>Maiya, Thompson, Loaiza-Lemos, Rolfe, 2013</marker>
<rawString>A. S. Maiya, J. P. Thompson, F. Loaiza-Lemos, and R. M. Rolfe. 2013. Exploratory analysis of highly heterogeneous document collections. In the Conference on Knowledge Discovery and Data Mining (SIGKDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Metsis</author>
<author>I Androutsopoulos</author>
<author>G Paliouras</author>
</authors>
<title>Spam filtering with naive bayes - which naive bayes?</title>
<date>2006</date>
<booktitle>In 3rd Conference on Email and Anti-Spam (CEAS).</booktitle>
<contexts>
<context position="10406" citStr="Metsis et al. (2006)" startWordPosition="1786" endWordPosition="1789"> is the only form of human input we require in our system. 3 Experiments We use three text classification tasks for evaluation: Gender Classification: Here we classify blog articles according to whether an article is written by a male or female. We employ the blog dataset as introduced by Schler et al. (2006) for this task. The dataset contains 19320 blog articles, out of which we randomly selected 5000 blog articles as our dataset. Spam Classification: Here the goal is to determine whether an email is Spam or Ham (i.e., not spam). We use the Enron spam dataset as introduced by Metris et al. (Metsis et al. (2006)). We join together the BG section of Spam emails and kaminski section of Ham emails, and randomly selected 5000 emails as our dataset. Sentiment Classification: Here the goal is to determine whether the sentiment expressed in a product review is positive or negative. We use Pang et al.’s movie review dataset for this task (Pang et al. (2002)). The dataset contains 2000 reviews annotated with the positive and negative sentiment label. To preprocess a document, we first tokenize and downcase it, remove stop words, and represent it as a vector of unigrams, using frequency as presence. For spectr</context>
</contexts>
<marker>Metsis, Androutsopoulos, Paliouras, 2006</marker>
<rawString>V. Metsis, I. Androutsopoulos, and G. Paliouras. 2006. Spam filtering with naive bayes - which naive bayes? In 3rd Conference on Email and Anti-Spam (CEAS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Using the web as an implicit training set: Application to structural ambiguity resolution.</title>
<date>2005</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="19625" citStr="Nakov and Hearst (2005)" startWordPosition="3307" endWordPosition="3310">d well to the generic sentiment lexicon manually created by the sentiment experts. The ability of our system to learn top sentiment features could be handy for a resource-scarce language, which may not have a general purpose sentiment lexicon. Note that the system is not limited to unigram and bigram patterns only. The labeled instances can be utilized similarly to gather statistics for other form of usage patterns including syntactic and semantic patterns for document collections. 5 Related Work Automatic extraction of labeled data has gained momentum in recent years (Durme and Pasca (2008), Nakov and Hearst (2005), Fuxman et al. (2009)). Traditionally, researchers use taskspecific heuristics to generate labeled data, e.g., searching for a specific pattern in the web to collect data instances of a particular category (Hearst (1992), Go et al. (2009), Hu et al. (2013)). Another line of research follows semi-supervised information extraction task, where given a list of seed instances of a particular category, a bootstrapping algorithm is applied to mine new instances from large corpora (Riloff and Jones (1999), Etzioni et al. (2005), Durme and Pasca (2008)). There has also been a surge of interests in uns</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Using the web as an implicit training set: Application to structural ambiguity resolution. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="10750" citStr="Pang et al. (2002)" startWordPosition="1845" endWordPosition="1848"> 19320 blog articles, out of which we randomly selected 5000 blog articles as our dataset. Spam Classification: Here the goal is to determine whether an email is Spam or Ham (i.e., not spam). We use the Enron spam dataset as introduced by Metris et al. (Metsis et al. (2006)). We join together the BG section of Spam emails and kaminski section of Ham emails, and randomly selected 5000 emails as our dataset. Sentiment Classification: Here the goal is to determine whether the sentiment expressed in a product review is positive or negative. We use Pang et al.’s movie review dataset for this task (Pang et al. (2002)). The dataset contains 2000 reviews annotated with the positive and negative sentiment label. To preprocess a document, we first tokenize and downcase it, remove stop words, and represent it as a vector of unigrams, using frequency as presence. For spectral clustering, we use dot product as a measure of similarity between two documents vectors. Dataset Data points Features Pos:Neg Gender 5000 75188 2751:2249 Spam 5000 23760 2492:2508 Sentiment 2000 24531 1000:1000 Table 3: Description of the datasets. 3.1 Accuracy of Automatically Labeled Data For each dataset, given n unlabeled data points, </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM).</booktitle>
<contexts>
<context position="20128" citStr="Riloff and Jones (1999)" startWordPosition="3388" endWordPosition="3391">utomatic extraction of labeled data has gained momentum in recent years (Durme and Pasca (2008), Nakov and Hearst (2005), Fuxman et al. (2009)). Traditionally, researchers use taskspecific heuristics to generate labeled data, e.g., searching for a specific pattern in the web to collect data instances of a particular category (Hearst (1992), Go et al. (2009), Hu et al. (2013)). Another line of research follows semi-supervised information extraction task, where given a list of seed instances of a particular category, a bootstrapping algorithm is applied to mine new instances from large corpora (Riloff and Jones (1999), Etzioni et al. (2005), Durme and Pasca (2008)). There has also been a surge of interests in unsupervised approaches which primarily rely on clustering to induce psuedo labels from large amount of text (Clark (2000), Slonim and Tishby (2000), Sahoo et al. (2006), Christodoulopoulos et al. (2010)). We differ from existing unsupervised clustering algorithms in a way that we uncomplicate spectral clustering by forcing it to cluster unambiguous data points only, which ensures that the system makes less mistakes during clustering and the clustered data are qualitatively strong. 6 Conclusion We hav</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sahoo</author>
<author>J Callan</author>
<author>R Krishnan</author>
<author>G Duncan</author>
<author>R Padman</author>
</authors>
<title>Incremental hierarchical clustering of text documents.</title>
<date>2006</date>
<booktitle>In the International Conference on Information and Knowledge Management (CIKM).</booktitle>
<contexts>
<context position="20391" citStr="Sahoo et al. (2006)" startWordPosition="3434" endWordPosition="3437"> the web to collect data instances of a particular category (Hearst (1992), Go et al. (2009), Hu et al. (2013)). Another line of research follows semi-supervised information extraction task, where given a list of seed instances of a particular category, a bootstrapping algorithm is applied to mine new instances from large corpora (Riloff and Jones (1999), Etzioni et al. (2005), Durme and Pasca (2008)). There has also been a surge of interests in unsupervised approaches which primarily rely on clustering to induce psuedo labels from large amount of text (Clark (2000), Slonim and Tishby (2000), Sahoo et al. (2006), Christodoulopoulos et al. (2010)). We differ from existing unsupervised clustering algorithms in a way that we uncomplicate spectral clustering by forcing it to cluster unambiguous data points only, which ensures that the system makes less mistakes during clustering and the clustered data are qualitatively strong. 6 Conclusion We have presented a system that helps us create a labeled resource for a given dataset with minimal human effort. We also utilize the labeled resources to discover important insights about the data. The ability of our system to learn and visualize top discriminative pa</context>
</contexts>
<marker>Sahoo, Callan, Krishnan, Duncan, Padman, 2006</marker>
<rawString>N. Sahoo, J. Callan, R. Krishnan, G. Duncan, and R. Padman. 2006. Incremental hierarchical clustering of text documents. In the International Conference on Information and Knowledge Management (CIKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schler</author>
<author>M Koppel</author>
<author>S Argamon</author>
<author>J Pennebaker</author>
</authors>
<title>Effects of age and gender in blogging.</title>
<date>2006</date>
<booktitle>In AAAI Symposium on Computational Approaches for Analyzing Weblogs.</booktitle>
<contexts>
<context position="10096" citStr="Schler et al. (2006)" startWordPosition="1729" endWordPosition="1732">, as subsequent clusterings are generated from less ambiguous points. In all our experiments, we set α to 100. Finally, we label the clusters by inspecting 10 randomly sampled points from each cluster. We use the cluster labels to assign labels to the m unambiguous data points. Note that labeling the clusters is the only form of human input we require in our system. 3 Experiments We use three text classification tasks for evaluation: Gender Classification: Here we classify blog articles according to whether an article is written by a male or female. We employ the blog dataset as introduced by Schler et al. (2006) for this task. The dataset contains 19320 blog articles, out of which we randomly selected 5000 blog articles as our dataset. Spam Classification: Here the goal is to determine whether an email is Spam or Ham (i.e., not spam). We use the Enron spam dataset as introduced by Metris et al. (Metsis et al. (2006)). We join together the BG section of Spam emails and kaminski section of Ham emails, and randomly selected 5000 emails as our dataset. Sentiment Classification: Here the goal is to determine whether the sentiment expressed in a product review is positive or negative. We use Pang et al.’s </context>
</contexts>
<marker>Schler, Koppel, Argamon, Pennebaker, 2006</marker>
<rawString>J. Schler, M. Koppel, S. Argamon, and J. Pennebaker. 2006. Effects of age and gender in blogging. In AAAI Symposium on Computational Approaches for Analyzing Weblogs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianbo Shi</author>
<author>Jitendra Malik</author>
</authors>
<title>Normalized cuts and image segmentation.</title>
<date>2000</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence.</journal>
<contexts>
<context position="6745" citStr="Shi and Malik (2000)" startWordPosition="1140" endWordPosition="1144">sily to multi-way classification tasks. f(i) ∈ {1, −1} indicates which of the two clusters data point i should be assigned to. In spectral clustering, the normalized cut partition of a similarity graph, S is derived from the solution of the following constrained optimization problem: argmin j 7 Si (f (i) − f (j) )2 fE t �i Vdi √dj subject to fT Df = 1 and Df ⊥ 1, where D is a diagonal matrix with Di,i = E j Si,j and di = Di,i. The closed form solution to this optimization problem is the eigenvector corresponding to the second smallest eigenvalue of the Laplacian matrix, L = D−1/2(D − S)D−1/2 (Shi and Malik (2000)). Clustering using the second eigenvector, is trivial: since we have a linearization of the points, all we need to do is to determine a threshold for partitioning the data points. Second eigenvector reveals useful information regarding the ambiguity of the individual data points. In the computation of eigenvectors each data point factors out orthogonal projections of each of the neighboring data points. Ambiguous data points factor out orthogonal projections from both the positive and negative data instances, and hence they have near zero values in the pivot eigenvectors. We exploit this impo</context>
</contexts>
<marker>Shi, Malik, 2000</marker>
<rawString>Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Slonim</author>
<author>Naftali Tishby</author>
</authors>
<title>Document clustering using word clusters via the information bottleneck method.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="20370" citStr="Slonim and Tishby (2000)" startWordPosition="3430" endWordPosition="3433"> for a specific pattern in the web to collect data instances of a particular category (Hearst (1992), Go et al. (2009), Hu et al. (2013)). Another line of research follows semi-supervised information extraction task, where given a list of seed instances of a particular category, a bootstrapping algorithm is applied to mine new instances from large corpora (Riloff and Jones (1999), Etzioni et al. (2005), Durme and Pasca (2008)). There has also been a surge of interests in unsupervised approaches which primarily rely on clustering to induce psuedo labels from large amount of text (Clark (2000), Slonim and Tishby (2000), Sahoo et al. (2006), Christodoulopoulos et al. (2010)). We differ from existing unsupervised clustering algorithms in a way that we uncomplicate spectral clustering by forcing it to cluster unambiguous data points only, which ensures that the system makes less mistakes during clustering and the clustered data are qualitatively strong. 6 Conclusion We have presented a system that helps us create a labeled resource for a given dataset with minimal human effort. We also utilize the labeled resources to discover important insights about the data. The ability of our system to learn and visualize </context>
</contexts>
<marker>Slonim, Tishby, 2000</marker>
<rawString>Noam Slonim and Naftali Tishby. 2000. Document clustering using word clusters via the information bottleneck method. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>