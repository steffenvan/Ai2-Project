<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.96288">
A Hierarchical Neural Autoencoder for Paragraphs and Documents
</title>
<author confidence="0.997774">
Jiwei Li, Minh-Thang Luong and Dan Jurafsky
</author>
<affiliation confidence="0.995557">
Computer Science Department, Stanford University, Stanford, CA 94305, USA
</affiliation>
<email confidence="0.95608">
jiweil, lmthang, jurafsky@stanford.edu
</email>
<sectionHeader confidence="0.997257" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999932739130435">
Natural language generation of coherent
long texts like paragraphs or longer doc-
uments is a challenging problem for re-
current networks models. In this paper,
we explore an important step toward this
generation task: training an LSTM (Long-
short term memory) auto-encoder to pre-
serve and reconstruct multi-sentence para-
graphs. We introduce an LSTM model that
hierarchically builds an embedding for a
paragraph from embeddings for sentences
and words, then decodes this embedding
to reconstruct the original paragraph. We
evaluate the reconstructed paragraph us-
ing standard metrics like ROUGE and En-
tity Grid, showing that neural models are
able to encode texts in a way that preserve
syntactic, semantic, and discourse coher-
ence. While only a first step toward gener-
ating coherent text units from neural mod-
els, our work has the potential to signifi-
cantly impact natural language generation
and summarization1.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999354916666667">
Generating coherent text is a central task in natural
language processing. A wide variety of theories
exist for representing relationships between text
units, such as Rhetorical Structure Theory (Mann
and Thompson, 1988) or Discourse Representa-
tion Theory (Lascarides and Asher, 1991), for ex-
tracting these relations from text units (Marcu,
2000; LeThanh et al., 2004; Hernault et al., 2010;
Feng and Hirst, 2012, inter alia), and for extract-
ing other coherence properties characterizing the
role each text unit plays with others in a discourse
(Barzilay and Lapata, 2008; Barzilay and Lee,
</bodyText>
<footnote confidence="0.9275665">
1Code for models described in this paper are available at
www.stanford.edu/˜jiweil/.
</footnote>
<bodyText confidence="0.998958707317073">
2004; Elsner and Charniak, 2008; Li and Hovy,
2014, inter alia). However, applying these to text
generation remains difficult. To understand how
discourse units are connected, one has to under-
stand the communicative function of each unit,
and the role it plays within the context that en-
capsulates it, recursively all the way up for the
entire text. Identifying increasingly sophisticated
human-developed features may be insufficient for
capturing these patterns. But developing neural-
based alternatives has also been difficult. Al-
though neural representations for sentences can
capture aspects of coherent sentence structure (Ji
and Eisenstein, 2014; Li et al., 2014; Li and Hovy,
2014), it’s not clear how they could help in gener-
ating more broadly coherent text.
Recent LSTM models (Hochreiter and Schmid-
huber, 1997) have shown powerful results on gen-
erating meaningful and grammatical sentences in
sequence generation tasks like machine translation
(Sutskever et al., 2014; Bahdanau et al., 2014; Lu-
ong et al., 2015) or parsing (Vinyals et al., 2014).
This performance is at least partially attributable
to the ability of these systems to capture local
compositionally: the way neighboring words are
combined semantically and syntactically to form
meanings that they wish to express.
Could these models be extended to deal with
generation of larger structures like paragraphs or
even entire documents? In standard sequence-
to-sequence generation tasks, an input sequence
is mapped to a vector embedding that represents
the sequence, and then to an output string of
words. Multi-text generation tasks like summa-
rization could work in a similar way: the sys-
tem reads a collection of input sentences, and
is then asked to generate meaningful texts with
certain properties (such as—for summarization—
being succinct and conclusive). Just as the local
semantic and syntactic compositionally of words
can be captured by LSTM models, can the com-
</bodyText>
<page confidence="0.946009">
1106
</page>
<note confidence="0.976562">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1106–1115,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999960571428571">
positionally of discourse releations of higher-level
text units (e.g., clauses, sentences, paragraphs, and
documents) be captured in a similar way, with
clues about how text units connect with each an-
other stored in the neural compositional matrices?
In this paper we explore a first step toward this
task of neural natural language generation. We fo-
cus on the component task of training a paragraph
(document)-to-paragraph (document) autoencoder
to reconstruct the input text sequence from a com-
pressed vector representation from a deep learn-
ing model. We develop hierarchical LSTM mod-
els that arranges tokens, sentences and paragraphs
in a hierarchical structure, with different levels of
LSTMs capturing compositionality at the token-
token and sentence-to-sentence levels.
We offer in the following section to a brief de-
scription of sequence-to-sequence LSTM models.
The proposed hierarchical LSTM models are then
described in Section 3, followed by experimental
results in Section 4, and then a brief conclusion.
</bodyText>
<sectionHeader confidence="0.926335" genericHeader="method">
2 Long-Short Term Memory (LSTM)
</sectionHeader>
<bodyText confidence="0.999918538461539">
In this section we give a quick overview of LSTM
models. LSTM models (Hochreiter and Schmid-
huber, 1997) are defined as follows: given a
sequence of inputs X = {x1, x2, ..., xnX }, an
LSTM associates each timestep with an input,
memory and output gate, respectively denoted as
it, ft and ot. For notations, we disambiguate e and
h where et denote the vector for individual text
unite (e.g., word or sentence) at time step t while
ht denotes the vector computed by LSTM model
at time t by combining et and ht−1. Q denotes the
sigmoid function. The vector representation ht for
each time-step t is given by:
</bodyText>
<equation confidence="0.999729142857143">
Q � � �
Q ht−1
W · (1)
Q et
tanh
ct = ft · ct−1 + it · lt (2)
hst = ot · ct (3)
</equation>
<bodyText confidence="0.890872">
where W E R4Kx2K In sequence-to-sequence
generation tasks, each input X is paired with
a sequence of outputs to predict: Y =
{y1, y2, ..., ynY }. An LSTM defines a distribution
over outputs and sequentially predicts tokens us-
</bodyText>
<equation confidence="0.907832714285714">
ing a softmax function:
P(Y |X)
H= p(yt|x1, x2, ..., xt, y1, y2, ..., yt−1)
tE[1,ny]
exp(f(ht−1, eyt))
Ey, exp(f(ht−1, ey,))
(4)
</equation>
<bodyText confidence="0.998640166666667">
f(ht−1, eyt) denotes the activation function be-
tween eh−1 and eyt, where ht−1 is the representa-
tion outputted from the LSTM at time t − 1. Note
that each sentence ends up with a special end-of-
sentence symbol &lt;end&gt;. Commonly, the input
and output use two different LSTMs with differ-
ent sets of convolutional parameters for capturing
different compositional patterns.
In the decoding procedure, the algorithm termi-
nates when an &lt;end&gt; token is predicted. At each
timestep, either a greedy approach or beam search
can be adopted for word prediction. Greedy search
selects the token with the largest conditional prob-
ability, the embedding of which is then combined
with preceding output for next step token predic-
tion. For beam search, (Sutskever et al., 2014) dis-
covered that a beam size of 2 suffices to provide
most of benefits of beam search.
</bodyText>
<sectionHeader confidence="0.95009" genericHeader="method">
3 Paragraph Autoencoder
</sectionHeader>
<bodyText confidence="0.9987775">
In this section, we introduce our proposed hierar-
chical LSTM model for the autoencoder.
</bodyText>
<subsectionHeader confidence="0.994965">
3.1 Notation
</subsectionHeader>
<bodyText confidence="0.998952526315789">
Let D denote a paragraph or a document, which
is comprised of a sequence of ND sentences,
D = {s1, s2, ..., sND, endD}. An additional
”endD” token is appended to each document.
Each sentence s is comprised of a sequence of
tokens s = {w1, w2,..., wNs} where Ns denotes
the length of the sentence, each sentence end-
ing with an “ends” token. The word w is as-
sociated with a K-dimensional embedding ew,
ew = {e1w, e2w, ..., eKw }. Let V denote vocabu-
lary size. Each sentence s is associated with a K-
dimensional representation es.
An autoencoder is a neural model where output
units are directly connected with or identical to in-
put units. Typically, inputs are compressed into
a representation using neural models (encoding),
which is then used to reconstruct it back (decod-
ing). For a paragraph autoencoder, both the input
X and output Y are the same document D. The
</bodyText>
<equation confidence="0.705400333333333">
� it 1=
ft
ot
lt
H=
tE[1,ny]
</equation>
<page confidence="0.955015">
1107
</page>
<bodyText confidence="0.99393925">
autoencoder first compresses D into a vector rep-
resentation eD and then reconstructs D based on
eD.
For simplicity, we define LSTM(ht−1, et) to
be the LSTM operation on vectors ht−1 and et to
achieve ht as in Equ.1 and 2. For clarification,
we first describe the following notations used in
encoder and decoder:
</bodyText>
<listItem confidence="0.8759343">
• hwt and hst denote hidden vectors from LSTM
models, the subscripts of which indicate
timestep t, the superscripts of which indi-
cate operations at word level (w) or sequence
level (s). hst(enc) specifies encoding stage
and hst(dec) specifies decoding stage.
• ewt and est denotes word-level and sentence-
level embedding for word and sentence at po-
sition t in terms of its residing sentence or
document.
</listItem>
<subsectionHeader confidence="0.997651">
3.2 Model 1: Standard LSTM
</subsectionHeader>
<bodyText confidence="0.999990363636364">
The whole input and output are treated as one
sequence of tokens. Following Sutskever et al.
(2014) and Bahdanau et al. (2014), we trained
an autoencoder that first maps input documents
into vector representations from a LSTMencode
and then reconstructs inputs by predicting to-
kens within the document sequentially from a
LSTMdecode. Two separate LSTMs are imple-
mented for encoding and decoding with no sen-
tence structures considered. Illustration is shown
in Figure 1.
</bodyText>
<subsectionHeader confidence="0.998801">
3.3 Model 2: Hierarchical LSTM
</subsectionHeader>
<bodyText confidence="0.992559875">
The hierarchical model draws on the intuition that
just as the juxtaposition of words creates a joint
meaning of a sentence, the juxtaposition of sen-
tences also creates a joint meaning of a paragraph
or a document.
Encoder We first obtain representation vectors
at the sentence level by putting one layer of LSTM
(denoted as LSTMword
</bodyText>
<equation confidence="0.76768725">
encode) on top of its containing
words:
hwt (enc) = LSTMword
encode(ew t , hwt−1(enc)) (5)
</equation>
<bodyText confidence="0.9991695">
The vector output at the ending time-step is used
to represent the entire sentence as
</bodyText>
<equation confidence="0.5511495">
es = hw
ends
</equation>
<bodyText confidence="0.9607765">
To build representation eD for the current doc-
ument/paragraph D, another layer of LSTM (de-
noted as LSTMsentence
encode ) is placed on top of all sen-
tences, computing representations sequentially for
each timestep:
</bodyText>
<equation confidence="0.7031615">
hs t(enc) = LSTMsentence
encode (es t, hs t−1(enc)) (6)
</equation>
<bodyText confidence="0.919822">
Representation esendD computed at the final time
step is used to represent the entire document:
</bodyText>
<equation confidence="0.988932">
s
eD = endD.
</equation>
<bodyText confidence="0.998449666666667">
Thus one LSTM operates at the token level,
leading to the acquisition of sentence-level rep-
resentations that are then used as inputs into the
second LSTM that acquires document-level repre-
sentations, in a hierarchical structure.
Decoder As with encoding, the decoding algo-
rithm operates on a hierarchical structure with two
layers of LSTMs. LSTM outputs at sentence level
for time step t are obtained by:
</bodyText>
<equation confidence="0.877252">
hst(dec) = LSTMsentence
decode (es t, hs t−1(dec)) (7)
</equation>
<bodyText confidence="0.931679433333333">
The initial time step hs0(d) = eD, the end-to-end
output from the encoding procedure. hst(d) is used
as the original input into LSTMword
decode for subse-
LSTMquently predicting tokens within sentence t + 1.
word
decode predicts tokens at each position se-
quentially, the embedding of which is then com-
bined with earlier hidden vectors for the next time-
step prediction until the ends token is predicted.
The procedure can be summarized as follows:
hwt (dec) = LSTMsentence
decode (ew t , hw t−1(dec)) (8)
p(w|·) = softmax(ew,hwt−1(dec)) (9)
During decoding, LSTMword generates each
decode
word token w sequentially and combines it with
earlier LSTM-outputted hidden vectors. The
LSTM hidden vector computed at the final time
step is used to represent the current sentence.
This is passed to LSTMsentence
decode ,combined
with hst for the acquisition of ht+1, and outputted
to the next time step in sentence decoding.
For each timestep t, LSTMsentence
decode has to first
decide whether decoding should proceed or come
to a full stop: we add an additional token endD to
the vocabulary. Decoding terminates when token
endD is predicted. Details are shown in Figure 2.
</bodyText>
<page confidence="0.99403">
1108
</page>
<figureCaption confidence="0.999879666666667">
Figure 1: Standard Sequence to Sequence Model.
Figure 2: Hierarchical Sequence to Sequence Model.
Figure 3: Hierarchical Sequence to Sequence Model with Attention.
</figureCaption>
<subsectionHeader confidence="0.920334">
3.4 Model 3: Hierarchical LSTM with
Attention
</subsectionHeader>
<bodyText confidence="0.999730111111111">
Attention models adopt a look-back strategy by
linking the current decoding stage with input sen-
tences in an attempt to consider which part of the
input is most responsible for the current decoding
state. This attention version of hierarchical model
is inspired by similar work in image caption gen-
eration and machine translation (Xu et al., 2015;
Bahdanau et al., 2014).
Let H = {hi(e), h2(e), ..., h-,(e)} be the
</bodyText>
<page confidence="0.987145">
1109
</page>
<bodyText confidence="0.985832352941177">
collection of sentence-level hidden vectors for
each sentence from the inputs, outputted from
LSTMSentence
encode . Each element in H contains in-
formation about input sequences with a strong fo-
cus on the parts surrounding each specific sentence
(time-step). During decoding, suppose that est de-
notes the sentence-level embedding at current step
and that hst_1(dec) denotes the hidden vector out-
putted from LSTMsentence
decode at previous time step
t−1. Attention models would first link the current-
step decoding information, i.e., hst_1(dec) which
is outputted from LSTMsentence with each of the
dec
input sentences i E [1, N], characterized by a
strength indicator vi:
</bodyText>
<equation confidence="0.991499666666667">
vi = UT f(W1 · hst_1(dec) + W2 · hsi (enc)) (10)
W1, W2 E RKxK, U E RKx1. vi is then normal-
ized:
ai = P (11)
exp(vi)
i, exp(v� i)
</equation>
<bodyText confidence="0.5639025">
The attention vector is then created by averaging
weights over all input sentences:
</bodyText>
<equation confidence="0.9145185">
Xmt = aihsi(enc) (12)
iE[1,ND]
LSTM hidden vectors for current step is then
achieved by combining ct, est and hst_1(dec):
ct = ft · ct_1 + it · lt (14)
hst = ot · ct (15)
</equation>
<bodyText confidence="0.997927333333333">
where W E R4Kx3K. ht is then used for word
predicting as in the vanilla version of the hierar-
chical model.
</bodyText>
<subsectionHeader confidence="0.938471">
3.5 Training and Testing
</subsectionHeader>
<bodyText confidence="0.885146428571428">
Parameters are estimated by maximizing likeli-
hood of outputs given inputs, similar to standard
sequence-to-sequence models. A softmax func-
tion is adopted for predicting each token within
output documents, the error of which is first back-
propagated through LSTMword
decode to sentences,
then through LSTMsentence
decode to document repre-
sentation eD, and last through LSTMsentence
encode and
LSTMword
encode to inputs. Stochastic gradient de-
scent with minibatches is adopted.
</bodyText>
<table confidence="0.680408666666667">
dataset S per D W per D W per S
Hotel-Review 8.8 124.8 14.1
Wikipedia 8.4 132.9 14.8
</table>
<tableCaption confidence="0.740821">
Table 1: Statistics for the Datasets. W, S and D re-
</tableCaption>
<bodyText confidence="0.9721032">
spectively represent number of words, number of
sentences, and number of documents/paragraphs.
For example, “S per D” denotes average number
of sentences per document.
For testing, we adopt a greedy strategy with
no beam search. For a given document D, eD
is first obtained given already learned LSTMencode
parameters and word embeddings. Then in decod-
ing, LSTMsentence
decode computes embeddings at each
sentence-level time-step, which is first fed into the
binary classifier to decide whether sentence de-
coding terminates and then into LSTMword
decode for
word decoding.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.828328">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999785">
We implement the proposed autoencoder on two
datasets, a highly domain specific dataset consist-
ing of hotel reviews and a general dataset extracted
from Wkipedia.
Hotel Reviews We use a subset of hotel reviews
crawled from TripAdvisor. We consider only re-
views consisting sentences ranging from 50 to 250
words; the model has problems dealing with ex-
tremely long sentences, as we will discuss later.
We keep a vocabulary set consisting of the 25,000
most frequent words. A special “&lt;unk&gt;” token
is used to denote all the remaining less frequent
tokens. Reviews that consist of more than 2 per-
cent of unknown words are discarded. Our train-
ing dataset is comprised of roughly 340,000 re-
views; the testing set is comprised of 40,000 re-
views. Dataset details are shown in Table 1.
Wikipedia We extracted paragraphs from
Wikipedia corpus that meet the aforementioned
length requirements. We keep a top frequent
vocabulary list of 120,000 words. Paragraphs
with larger than 4 percent of unknown words are
discarded. The training dataset is comprised of
roughly 500,000 paragraphs and testing contains
roughly 50,000.
</bodyText>
<figure confidence="0.617044846153846">
σ # &amp;quot;hs#
t_1(dec)
σ W · es (13)
t
σ
mt
tanh t
# &amp;quot;
=
&amp;quot; it
ft
ot
lt
</figure>
<page confidence="0.916445">
1110
</page>
<subsectionHeader confidence="0.997253">
4.2 Training Details and Implementation
</subsectionHeader>
<bodyText confidence="0.999931818181818">
Previous research has shown that deep LSTMs
work better than shallow ones for sequence-to-
sequence tasks (Vinyals et al., 2014; Sutskever et
al., 2014). We adopt a LSTM structure with four
layer for encoding and four layer for decoding,
each of which is comprised of a different set of pa-
rameters. Each LSTM layer consists of 1,000 hid-
den neurons and the dimensionality of word em-
beddings is set to 1,000. Other training details are
given below, some of which follow Sutskever et al.
(2014).
</bodyText>
<listItem confidence="0.998940714285714">
• Input documents are reversed.
• LSTM parameters and word embeddings are
initialized from a uniform distribution be-
tween [-0.08, 0.08].
• Stochastic gradient decent is implemented
without momentum using a fixed learning
rate of 0.1. We stated halving the learning
rate every half epoch after 5 epochs. We
trained our models for a total of 7 epochs.
• Batch size is set to 32 (32 documents).
• Decoding algorithm allows generating at
most 1.5 times the number of words in inputs.
• 0.2 dropout rate.
• Gradient clipping is adopted by scaling gra-
</listItem>
<bodyText confidence="0.924880833333333">
dients when the norm exceeded a threshold
of 5.
Our implementation on a single GPU2 processes a
speed of approximately 600-1,200 tokens per sec-
ond. We trained our models for a total of 7 itera-
tions.
</bodyText>
<subsectionHeader confidence="0.991562">
4.3 Evaluations
</subsectionHeader>
<bodyText confidence="0.999327272727273">
We need to measure the closeness of the output
(candidate) to the input (reference). We first adopt
two standard evaluation metrics, ROUGE (Lin,
2004; Lin and Hovy, 2003) and BLEU (Papineni
et al., 2002).
ROUGE is a recall-oriented measure widely
used in the summarization literature. It measures
the n-gram recall between the candidate text and
the reference text(s). In this work, we only have
one reference document (the input document) and
ROUGE score is therefore given by:
</bodyText>
<equation confidence="0.9409028">
ROUGEn = Egramn∈input countmatch(gramn)
E
gramn∈input count(gramn)
(16)
2Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores.
</equation>
<bodyText confidence="0.999514666666667">
where countmatch denotes the number of n-grams
co-occurring in the input and output. We report
ROUGE-1, 2 and W (based on weighted longest
common subsequence).
BLEU Purely measuring recall will inappropri-
ately reward long outputs. BLEU is designed to
address such an issue by emphasizing precision.
n-gram precision scores for our situation are given
by:
</bodyText>
<equation confidence="0.9956704">
E
gramn∈output countmatch(gramn)
precisionn = E
gramn∈output count(gramn)
(17)
</equation>
<bodyText confidence="0.895017923076923">
BLEU then combines the average logarithm of
precision scores with exceeded length penaliza-
tion. For details, see Papineni et al. (2002).
Coherence Evaluation Neither BLEU nor
ROUGE attempts to evaluate true coherence.
There is no generally accepted and readily avail-
able coherence evaluation metric.3 Because of
the difficulty of developing a universal coherence
evaluation metric, we proposed here only a
tailored metric specific to our case. Based on the
assumption that human-generated texts (i.e., input
documents in our tasks) are coherent (Barzilay
and Lapata, 2008), we compare generated outputs
with input documents in terms of how much
original text order is preserved.
We develop a grid evaluation metric similar to
the entity transition algorithms in (Barzilay and
Lee, 2004; Lapata and Barzilay, 2005). The key
idea of Barzilay and Lapata’s models is to first
identify grammatical roles (i.e., object and sub-
ject) that entities play and then model the transi-
tion probability over entities and roles across sen-
tences. We represent each sentence as a feature-
vector consisting of verbs and nouns in the sen-
tence. Next we align sentences from output doc-
uments to input sentences based on sentence-to-
sentence F1 scores (precision and recall are com-
puted similarly to ROUGE and BLEU but at sen-
tence level) using feature vectors. Note that multi-
ple output sentences can be matched to one input
3Wolf and Gibson (2005) and Lin et al. (2011) proposed
metrics based on discourse relations, but these are hard to ap-
ply widely since identifying discourse relations is a difficult
problem. Indeed sophisticated coherence evaluation metrics
are seldom adopted in real-world applications, and summa-
rization researchers tend to use simple approximations like
number of overlapped tokens or topic distribution similarity
(e.g., (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and
Hakkani-T¨ur, 2011)).
</bodyText>
<page confidence="0.915755">
1111
</page>
<bodyText confidence="0.99796265909091">
Input-Wiki washington was unanimously elected President by the electors in both the 1788 – 1789 and
1792 elections . he oversaw the creation of a strong, well-financed national government that
maintained neutrality in the french revolutionary wars , suppressed the whiskey rebellion , and
won acceptance among Americans of all types. washington established many forms in govern-
ment still used today , such as the cabinet system and inaugural address . his retirement after
two terms and the peaceful transition from his presidency to that of john adams established a
tradition that continued up until franklin d . roosevelt was elected to a third term . washington
has been widely hailed as the ” father of his country ” even during his lifetime.
Output-Wiki washington was elected as president in 1792 and voters &lt;unk&gt; of these two elections until
1789 . he continued suppression &lt;unk&gt; whiskey rebellion of the french revolution war gov-
ernment , strong, national well are involved in the establishment of the fin advanced operations
, won acceptance . as in the government , such as the establishment of various forms of inau-
guration speech washington , and are still in use . &lt;unk&gt; continued after the two terms of his
quiet transition to retirement of &lt;unk&gt; &lt;unk&gt; of tradition to have been elected to the third
paragraph . but , ” the united nations of the father ” and in washington in his life , has been
widely praised.
Input-Wiki apple inc . is an american multinational corporation headquartered in cupertino , california ,
that designs , develops , and sells consumer electronics , computer software , online services ,
and personal com - puters . its bestknown hardware products are the mac line of computers , the
ipod media player , the iphone smartphone , and the ipad tablet computer . its online services
include icloud , the itunes store , and the app store . apple’s consumer software includes the os
x and ios operating systems , the itunes media browser, the safari web browser, and the ilife
and iwork creativity and productivity suites .
Output-Wiki apple is a us company in california , &lt;unk&gt; , to develop electronics , softwares , and pc , sells
. hardware include the mac series of computers , ipod , iphone . its online services , including
icloud , itunes store and in app store . softwares , including os x and ios operating system ,
itunes , web browser, &lt; unk&gt; , including a productivity suite.
Input-Wiki paris is the capital and most populous city of france . situated on the seine river, in the north of
the country , it is in the centre of the le-de-france region . the city of paris has a population of
2273305 inhabitants . this makes it the fifth largest city in the european union measured by the
population within the city limits .
Output-Wiki paris is the capital and most populated city in france . located in the &lt;unk&gt; , in the north of the
country , it is the center of &lt;unk&gt; . paris , the city has a population of &lt;num&gt; inhabitants .
this makes the eu ’ s population within the city limits of the fifth largest city in the measurement
.
Input-Review on every visit to nyc , the hotel beacon is the place we love to stay . so conveniently located
to central park , lincoln center and great local restaurants . the rooms are lovely . beds so
comfortable , a great little kitchen and new wizz bang coffee maker. the staff are so accommo-
dating and just love walking across the street to the fairway supermarket with every imaginable
goodies to eat.
Output-Review every time in new york , lighthouse hotel is our favorite place to stay. very convenient, central
park , lincoln center , and great restaurants . the room is wonderful , very comfortable bed , a
kitchenette and a large explosion of coffee maker. the staff is so inclusive, just across the street
to walk to the supermarket channel love with all kinds of what to eat.
</bodyText>
<tableCaption confidence="0.980707">
Table 2: A few examples produced by the hierarchical LSTM alongside the inputs.
</tableCaption>
<bodyText confidence="0.926426">
sentence. Assume that sentence sioutput is aligned
with sentence si, , where i and i&apos; denote position
input
index for a output sentence and its aligned input.
The penalization score L is then given by:
</bodyText>
<equation confidence="0.9461295">
|(j − i) − (j&apos; − i&apos;)|
(18)
</equation>
<bodyText confidence="0.998899571428572">
Equ. 18 can be interpreted as follows: (j − i)
denotes the distance in terms of position index be-
tween two outputted sentences indexed by j and i,
and (j&apos; − i&apos;) denotes the distance between their
mirrors in inputs. As we wish to penalize the
degree of permutation in terms of text order, we
penalize the absolute difference between the two
computed distances. This metric is also relevant
to the overall performance of prediction and re-
call: an irrelevant output will be aligned to a ran-
dom input, thus being heavily penalized. The de-
ficiency of the proposed metric is that it concerns
itself only with a semantic perspective on coher-
ence, barely considering syntactical issues.
</bodyText>
<subsectionHeader confidence="0.90068">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.99998425">
A summary of our experimental results is given
in Table 3. We observe better performances for
the hotel-review dataset than the open domain
Wikipedia dataset, for the intuitive reason that
</bodyText>
<equation confidence="0.9680232">
�X �
iE[1,Noutput−1] jE[i+1,Noutput]
2
L=
Noutput - (Noutput − 1)
</equation>
<page confidence="0.926537">
1112
</page>
<table confidence="0.999940285714286">
Model Dataset BLEU ROUGE-1 ROUGE-2 Coherence(L)
Standard Hotel Review 0.241 0.571 0.302 1.92
Hierarchical Hotel Review 0.267 0.590 0.330 1.71
Hierarchical+Attention Hotel Review 0.285 0.624 0.355 1.57
Standard Wikipedia 0.178 0.502 0.228 2.75
Hierarchical Wikipedia 0.202 0.529 0.250 2.30
Hierarchical+Attention Wikipedia 0.220 0.544 0.291 2.04
</table>
<tableCaption confidence="0.9873455">
Table 3: Results for three models on two datasets. As with coherence score L, smaller values signifies
better performances.
</tableCaption>
<bodyText confidence="0.984946875">
documents and sentences are written in a more
fixed format and easy to predict for hotel reviews.
The hierarchical model that considers sentence-
level structure outperforms standard sequence-
to-sequence models. Attention models at the
sentence level introduce performance boost over
vanilla hierarchical models.
With respect to the coherence evaluation, the
original sentence order is mostly preserved: the hi-
erarchical model with attention achieves L = 1.57
on the hotel-review dataset, equivalent to the fact
that the relative position of two input sentences
are permuted by an average degree of 1.57. Even
for the Wikipedia dataset where more poor-quality
sentences are observed, the original text order can
still be adequately maintained with L = 2.04.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="evaluation">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999982846153847">
In this paper, we extended recent sequence-to-
sequence LSTM models to the task of multi-
sentence generation. We trained an autoencoder
to see how well LSTM models can reconstruct in-
put documents of many sentences. We find that
the proposed hierarchical LSTM models can par-
tially preserve the semantic and syntactic integrity
of multi-text units and generate meaningful and
grammatical sentences in coherent order. Our
model performs better than standard sequence-to-
sequence models which do not consider the intrin-
sic hierarchical discourse structure of texts.
While our work on auto-encoding for larger
texts is only a preliminary effort toward allowing
neural models to deal with discourse, it nonethe-
less suggests that neural models are capable of en-
coding complex clues about how coherent texts are
connected .
The performance on this autoencoder task could
certainly also benefit from more sophisticated neu-
ral models. For example one extension might align
the sentence currently being generated with the
original input sentence (similar to sequence-to-
sequence translation in (Bahdanau et al., 2014)),
and later transform the original task to sentence-
to-sentence generation. However our long-term
goal here is not on perfecting this basic multi-text
generation scenario of reconstructing input docu-
ments, but rather on extending it to more important
applications.
That is, the autoencoder described in this work,
where input sequence X is identical to output Y , is
only the most basic instance of the family of doc-
ument (paragraph)-to-document (paragraph) gen-
eration tasks. We hope the ideas proposed in
this paper can play some role in enabling such
more sophisticated generation tasks like summa-
rization, where the inputs are original documents
and outputs are summaries or question answering,
where inputs are questions and outputs are the ac-
tual wording of answers. Sophisticated genera-
tion tasks like summarization or dialogue systems
could extend this paradigm, and could themselves
benefit from task-specific adaptations. In sum-
marization, sentences to generate at each timestep
might be pre-pointed to or pre-aligned to specific
aspects, topics, or pieces of texts to be summa-
rized. Dialogue systems could incorporate infor-
mation about the user or the time course of the
dialogue. In any case, we look forward to more
sophi4d applications of neural models to the im-
portant task of natural language generation.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="conclusions">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.99990875">
The authors want to thank Gabor Angeli, Sam
Bowman, Percy Liang and other members of the
Stanford NLP group for insightful comments and
suggestion. We also thank the three anonymous
ACL reviewers for helpful comments. This work
is supported by Enlight Foundation Graduate Fel-
lowship, and a gift from Bloomberg L.P, which we
gratefully acknowledge.
</bodyText>
<page confidence="0.987579">
1113
</page>
<sectionHeader confidence="0.994992" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995557257142857">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1–34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. arXiv preprint
cs/0405039.
Asli Celikyilmaz and Dilek Hakkani-T¨ur. 2011. Dis-
covery of topically coherent sentences for extractive
summarization. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 491–499. Association for Computational Lin-
guistics.
Micha Elsner and Eugene Charniak. 2008.
Coreference-inspired coherence modeling. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics on Hu-
man Language Technologies: Short Papers, pages
41–44. Association for Computational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-
level discourse parsing with rich linguistic fea-
tures. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 60–68. Association
for Computational Linguistics.
Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka,
et al. 2010. Hilda: a discourse parser using sup-
port vector machine classification. Dialogue &amp; Dis-
course, 1(3).
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Yangfeng Ji and Jacob Eisenstein. 2014. Represen-
tation learning for text-level discourse parsing. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics, volume 1,
pages 13–24.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and represen-
tations. In IJCAI, volume 5, pages 1085–1090.
Alex Lascarides and Nicholas Asher. 1991. Discourse
relations and defeasible knowledge. In Proceedings
of the 29th annual meeting on Association for Com-
putational Linguistics, pages 55–62. Association for
Computational Linguistics.
Huong LeThanh, Geetha Abeysinghe, and Christian
Huyck. 2004. Generating discourse structures for
written texts. In Proceedings of the 20th inter-
national conference on Computational Linguistics,
page 329. Association for Computational Linguis-
tics.
Jiwei Li and Eduard Hovy. 2014. A model of coher-
ence based on distributed sentence representation.
Jiwei Li, Rumeng Li, and Eduard Hovy. 2014. Recur-
sive deep models for discourse parsing. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
2061–2069.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 71–78.
Association for Computational Linguistics.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 997–1006. Association for Computational
Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81.
Thang Luong, Ilya Sutskever, Quoc V Le, Oriol
Vinyals, and Wojciech Zaremba. 2015. Addressing
the rare word problem in neural machine translation.
ACL.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Daniel Marcu. 2000. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional linguistics, 26(3):395–448.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112.
Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2014.
Grammar as a foreign language. arXiv preprint
arXiv:1412.7449.
</reference>
<page confidence="0.917291">
1114
</page>
<reference confidence="0.999506409090909">
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Com-
putational Linguistics, 31(2):249–287.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville,
Ruslan Salakhutdinov, Richard Zemel, and Yoshua
Bengio. 2015. Show, attend and tell: Neural im-
age caption generation with visual attention. arXiv
preprint arXiv:1502.03044.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li, and Yan Zhang. 2011a. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 433–443. Association for Computational Lin-
guistics.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimiza-
tion framework via iterative substitution. In Pro-
ceedings of the 34th international ACM SIGIR con-
ference on Research and development in Information
Retrieval, pages 745–754. ACM.
</reference>
<page confidence="0.99503">
1115
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.621978">
<title confidence="0.999762">A Hierarchical Neural Autoencoder for Paragraphs and Documents</title>
<author confidence="0.999384">Jiwei Li</author>
<author confidence="0.999384">Minh-Thang Luong</author>
<author confidence="0.999384">Dan Jurafsky</author>
<affiliation confidence="0.705118">Computer Science Department, Stanford University, Stanford, CA 94305,</affiliation>
<email confidence="0.998361">jiweil,lmthang,jurafsky@stanford.edu</email>
<abstract confidence="0.999419318181818">Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Longshort term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to signifi-</abstract>
<intro confidence="0.895411">cantly impact natural language generation</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</title>
<date>2014</date>
<contexts>
<context position="2828" citStr="Bahdanau et al., 2014" startWordPosition="422" endWordPosition="425">histicated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that they wish to express. Could these models be extended to deal with generation of larger structures like paragraphs or even entire documents? In standard sequenceto-sequence generation tasks, an input sequence is mapped to a vector embedding that represents the sequence, and then to an output string of words. Multi-text generation ta</context>
<context position="8859" citStr="Bahdanau et al. (2014)" startWordPosition="1435" endWordPosition="1438">rst describe the following notations used in encoder and decoder: • hwt and hst denote hidden vectors from LSTM models, the subscripts of which indicate timestep t, the superscripts of which indicate operations at word level (w) or sequence level (s). hst(enc) specifies encoding stage and hst(dec) specifies decoding stage. • ewt and est denotes word-level and sentencelevel embedding for word and sentence at position t in terms of its residing sentence or document. 3.2 Model 1: Standard LSTM The whole input and output are treated as one sequence of tokens. Following Sutskever et al. (2014) and Bahdanau et al. (2014), we trained an autoencoder that first maps input documents into vector representations from a LSTMencode and then reconstructs inputs by predicting tokens within the document sequentially from a LSTMdecode. Two separate LSTMs are implemented for encoding and decoding with no sentence structures considered. Illustration is shown in Figure 1. 3.3 Model 2: Hierarchical LSTM The hierarchical model draws on the intuition that just as the juxtaposition of words creates a joint meaning of a sentence, the juxtaposition of sentences also creates a joint meaning of a paragraph or a document. Encoder We</context>
<context position="12345" citStr="Bahdanau et al., 2014" startWordPosition="2002" endWordPosition="2005">cted. Details are shown in Figure 2. 1108 Figure 1: Standard Sequence to Sequence Model. Figure 2: Hierarchical Sequence to Sequence Model. Figure 3: Hierarchical Sequence to Sequence Model with Attention. 3.4 Model 3: Hierarchical LSTM with Attention Attention models adopt a look-back strategy by linking the current decoding stage with input sentences in an attempt to consider which part of the input is most responsible for the current decoding state. This attention version of hierarchical model is inspired by similar work in image caption generation and machine translation (Xu et al., 2015; Bahdanau et al., 2014). Let H = {hi(e), h2(e), ..., h-,(e)} be the 1109 collection of sentence-level hidden vectors for each sentence from the inputs, outputted from LSTMSentence encode . Each element in H contains information about input sequences with a strong focus on the parts surrounding each specific sentence (time-step). During decoding, suppose that est denotes the sentence-level embedding at current step and that hst_1(dec) denotes the hidden vector outputted from LSTMsentence decode at previous time step t−1. Attention models would first link the currentstep decoding information, i.e., hst_1(dec) which is</context>
<context position="27685" citStr="Bahdanau et al., 2014" startWordPosition="4548" endWordPosition="4551">which do not consider the intrinsic hierarchical discourse structure of texts. While our work on auto-encoding for larger texts is only a preliminary effort toward allowing neural models to deal with discourse, it nonetheless suggests that neural models are capable of encoding complex clues about how coherent texts are connected . The performance on this autoencoder task could certainly also benefit from more sophisticated neural models. For example one extension might align the sentence currently being generated with the original input sentence (similar to sequence-tosequence translation in (Bahdanau et al., 2014)), and later transform the original task to sentenceto-sentence generation. However our long-term goal here is not on perfecting this basic multi-text generation scenario of reconstructing input documents, but rather on extending it to more important applications. That is, the autoencoder described in this work, where input sequence X is identical to output Y , is only the most basic instance of the family of document (paragraph)-to-document (paragraph) generation tasks. We hope the ideas proposed in this paper can play some role in enabling such more sophisticated generation tasks like summar</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1724" citStr="Barzilay and Lapata, 2008" startWordPosition="256" endWordPosition="259">anguage generation and summarization1. 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alte</context>
<context position="18869" citStr="Barzilay and Lapata, 2008" startWordPosition="3077" endWordPosition="3080">(gramn) precisionn = E gramn∈output count(gramn) (17) BLEU then combines the average logarithm of precision scores with exceeded length penalization. For details, see Papineni et al. (2002). Coherence Evaluation Neither BLEU nor ROUGE attempts to evaluate true coherence. There is no generally accepted and readily available coherence evaluation metric.3 Because of the difficulty of developing a universal coherence evaluation metric, we proposed here only a tailored metric specific to our case. Based on the assumption that human-generated texts (i.e., input documents in our tasks) are coherent (Barzilay and Lapata, 2008), we compare generated outputs with input documents in terms of how much original text order is preserved. We develop a grid evaluation metric similar to the entity transition algorithms in (Barzilay and Lee, 2004; Lapata and Barzilay, 2005). The key idea of Barzilay and Lapata’s models is to first identify grammatical roles (i.e., object and subject) that entities play and then model the transition probability over entities and roles across sentences. We represent each sentence as a featurevector consisting of verbs and nouns in the sentence. Next we align sentences from output documents to i</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization. arXiv preprint cs/0405039.</title>
<date>2004</date>
<contexts>
<context position="19082" citStr="Barzilay and Lee, 2004" startWordPosition="3111" endWordPosition="3114">er BLEU nor ROUGE attempts to evaluate true coherence. There is no generally accepted and readily available coherence evaluation metric.3 Because of the difficulty of developing a universal coherence evaluation metric, we proposed here only a tailored metric specific to our case. Based on the assumption that human-generated texts (i.e., input documents in our tasks) are coherent (Barzilay and Lapata, 2008), we compare generated outputs with input documents in terms of how much original text order is preserved. We develop a grid evaluation metric similar to the entity transition algorithms in (Barzilay and Lee, 2004; Lapata and Barzilay, 2005). The key idea of Barzilay and Lapata’s models is to first identify grammatical roles (i.e., object and subject) that entities play and then model the transition probability over entities and roles across sentences. We represent each sentence as a featurevector consisting of verbs and nouns in the sentence. Next we align sentences from output documents to input sentences based on sentence-tosentence F1 scores (precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. arXiv preprint cs/0405039.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Discovery of topically coherent sentences for extractive summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>491--499</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Celikyilmaz, Hakkani-T¨ur, 2011</marker>
<rawString>Asli Celikyilmaz and Dilek Hakkani-T¨ur. 2011. Discovery of topically coherent sentences for extractive summarization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 491–499. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>Coreference-inspired coherence modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers,</booktitle>
<pages>41--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1860" citStr="Elsner and Charniak, 2008" startWordPosition="275" endWordPosition="278">variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji a</context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008. Coreference-inspired coherence modeling. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 41–44. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Textlevel discourse parsing with rich linguistic features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>60--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1565" citStr="Feng and Hirst, 2012" startWordPosition="231" endWordPosition="234"> coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1. 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the</context>
</contexts>
<marker>Feng, Hirst, 2012</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2012. Textlevel discourse parsing with rich linguistic features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 60–68. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Helmut Prendinger</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Hilda: a discourse parser using support vector machine classification.</title>
<date>2010</date>
<journal>Dialogue &amp; Discourse,</journal>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="1543" citStr="Hernault et al., 2010" startWordPosition="227" endWordPosition="230">semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1. 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively </context>
</contexts>
<marker>Hernault, Prendinger, Ishizuka, 2010</marker>
<rawString>Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka, et al. 2010. Hilda: a discourse parser using support vector machine classification. Dialogue &amp; Discourse, 1(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="2648" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="395" endWordPosition="399"> understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that they wish to express. Could these models be extended to deal with generation of larger structures like paragraphs or even entire documents? In standard s</context>
<context position="5189" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="780" endWordPosition="784">ntation from a deep learning model. We develop hierarchical LSTM models that arranges tokens, sentences and paragraphs in a hierarchical structure, with different levels of LSTMs capturing compositionality at the tokentoken and sentence-to-sentence levels. We offer in the following section to a brief description of sequence-to-sequence LSTM models. The proposed hierarchical LSTM models are then described in Section 3, followed by experimental results in Section 4, and then a brief conclusion. 2 Long-Short Term Memory (LSTM) In this section we give a quick overview of LSTM models. LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2, ..., xnX }, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as it, ft and ot. For notations, we disambiguate e and h where et denote the vector for individual text unite (e.g., word or sentence) at time step t while ht denotes the vector computed by LSTM model at time t by combining et and ht−1. Q denotes the sigmoid function. The vector representation ht for each time-step t is given by: Q � � � Q ht−1 W · (1) Q et tanh ct = ft · ct−1 + it · lt (2) hst = ot · ct (3) where W E R4Kx2K I</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangfeng Ji</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Representation learning for text-level discourse parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>13--24</pages>
<contexts>
<context position="2479" citStr="Ji and Eisenstein, 2014" startWordPosition="366" endWordPosition="369">2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to for</context>
</contexts>
<marker>Ji, Eisenstein, 2014</marker>
<rawString>Yangfeng Ji and Jacob Eisenstein. 2014. Representation learning for text-level discourse parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 13–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic evaluation of text coherence: Models and representations.</title>
<date>2005</date>
<booktitle>In IJCAI,</booktitle>
<volume>5</volume>
<pages>1085--1090</pages>
<contexts>
<context position="19110" citStr="Lapata and Barzilay, 2005" startWordPosition="3115" endWordPosition="3118">ts to evaluate true coherence. There is no generally accepted and readily available coherence evaluation metric.3 Because of the difficulty of developing a universal coherence evaluation metric, we proposed here only a tailored metric specific to our case. Based on the assumption that human-generated texts (i.e., input documents in our tasks) are coherent (Barzilay and Lapata, 2008), we compare generated outputs with input documents in terms of how much original text order is preserved. We develop a grid evaluation metric similar to the entity transition algorithms in (Barzilay and Lee, 2004; Lapata and Barzilay, 2005). The key idea of Barzilay and Lapata’s models is to first identify grammatical roles (i.e., object and subject) that entities play and then model the transition probability over entities and roles across sentences. We represent each sentence as a featurevector consisting of verbs and nouns in the sentence. Next we align sentences from output documents to input sentences based on sentence-tosentence F1 scores (precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched to one input 3Wolf and Gibs</context>
</contexts>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>Mirella Lapata and Regina Barzilay. 2005. Automatic evaluation of text coherence: Models and representations. In IJCAI, volume 5, pages 1085–1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Nicholas Asher</author>
</authors>
<title>Discourse relations and defeasible knowledge.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>55--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1437" citStr="Lascarides and Asher, 1991" startWordPosition="209" endWordPosition="212">ke ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1. 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the commun</context>
</contexts>
<marker>Lascarides, Asher, 1991</marker>
<rawString>Alex Lascarides and Nicholas Asher. 1991. Discourse relations and defeasible knowledge. In Proceedings of the 29th annual meeting on Association for Computational Linguistics, pages 55–62. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huong LeThanh</author>
<author>Geetha Abeysinghe</author>
<author>Christian Huyck</author>
</authors>
<title>Generating discourse structures for written texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>329</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1520" citStr="LeThanh et al., 2004" startWordPosition="223" endWordPosition="226">t preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1. 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encaps</context>
</contexts>
<marker>LeThanh, Abeysinghe, Huyck, 2004</marker>
<rawString>Huong LeThanh, Geetha Abeysinghe, and Christian Huyck. 2004. Generating discourse structures for written texts. In Proceedings of the 20th international conference on Computational Linguistics, page 329. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Eduard Hovy</author>
</authors>
<title>A model of coherence based on distributed sentence representation.</title>
<date>2014</date>
<contexts>
<context position="1879" citStr="Li and Hovy, 2014" startWordPosition="279" endWordPosition="282">or representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014</context>
</contexts>
<marker>Li, Hovy, 2014</marker>
<rawString>Jiwei Li and Eduard Hovy. 2014. A model of coherence based on distributed sentence representation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Rumeng Li</author>
<author>Eduard Hovy</author>
</authors>
<title>Recursive deep models for discourse parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>2061--2069</pages>
<contexts>
<context position="2496" citStr="Li et al., 2014" startWordPosition="370" endWordPosition="373">inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that t</context>
</contexts>
<marker>Li, Li, Hovy, 2014</marker>
<rawString>Jiwei Li, Rumeng Li, and Eduard Hovy. 2014. Recursive deep models for discourse parsing. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061–2069.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram cooccurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>71--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17435" citStr="Lin and Hovy, 2003" startWordPosition="2863" endWordPosition="2866">ed our models for a total of 7 epochs. • Batch size is set to 32 (32 documents). • Decoding algorithm allows generating at most 1.5 times the number of words in inputs. • 0.2 dropout rate. • Gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 5. Our implementation on a single GPU2 processes a speed of approximately 600-1,200 tokens per second. We trained our models for a total of 7 iterations. 4.3 Evaluations We need to measure the closeness of the output (candidate) to the input (reference). We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002). ROUGE is a recall-oriented measure widely used in the summarization literature. It measures the n-gram recall between the candidate text and the reference text(s). In this work, we only have one reference document (the input document) and ROUGE score is therefore given by: ROUGEn = Egramn∈input countmatch(gramn) E gramn∈input count(gramn) (16) 2Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores. where countmatch denotes the number of n-grams co-occurring in the input and output. We report ROUGE-1, 2 and W (based on weighted longest common subsequence). BLEU Purely </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 71–78. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>Automatically evaluating text coherence using discourse relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>997--1006</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19741" citStr="Lin et al. (2011)" startWordPosition="3225" endWordPosition="3228"> of Barzilay and Lapata’s models is to first identify grammatical roles (i.e., object and subject) that entities play and then model the transition probability over entities and roles across sentences. We represent each sentence as a featurevector consisting of verbs and nouns in the sentence. Next we align sentences from output documents to input sentences based on sentence-tosentence F1 scores (precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched to one input 3Wolf and Gibson (2005) and Lin et al. (2011) proposed metrics based on discourse relations, but these are hard to apply widely since identifying discourse relations is a difficult problem. Indeed sophisticated coherence evaluation metrics are seldom adopted in real-world applications, and summarization researchers tend to use simple approximations like number of overlapped tokens or topic distribution similarity (e.g., (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-T¨ur, 2011)). 1111 Input-Wiki washington was unanimously elected President by the electors in both the 1788 – 1789 and 1792 elections . he oversaw the creatio</context>
</contexts>
<marker>Lin, Ng, Kan, 2011</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 997–1006. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="17414" citStr="Lin, 2004" startWordPosition="2861" endWordPosition="2862">s. We trained our models for a total of 7 epochs. • Batch size is set to 32 (32 documents). • Decoding algorithm allows generating at most 1.5 times the number of words in inputs. • 0.2 dropout rate. • Gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 5. Our implementation on a single GPU2 processes a speed of approximately 600-1,200 tokens per second. We trained our models for a total of 7 iterations. 4.3 Evaluations We need to measure the closeness of the output (candidate) to the input (reference). We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002). ROUGE is a recall-oriented measure widely used in the summarization literature. It measures the n-gram recall between the candidate text and the reference text(s). In this work, we only have one reference document (the input document) and ROUGE score is therefore given by: ROUGEn = Egramn∈input countmatch(gramn) E gramn∈input count(gramn) (16) 2Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores. where countmatch denotes the number of n-grams co-occurring in the input and output. We report ROUGE-1, 2 and W (based on weighted longest common subse</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thang Luong</author>
<author>Ilya Sutskever</author>
<author>Quoc V Le</author>
<author>Oriol Vinyals</author>
<author>Wojciech Zaremba</author>
</authors>
<title>Addressing the rare word problem in neural machine translation.</title>
<date>2015</date>
<publisher>ACL.</publisher>
<contexts>
<context position="2849" citStr="Luong et al., 2015" startWordPosition="426" endWordPosition="430">ped features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that they wish to express. Could these models be extended to deal with generation of larger structures like paragraphs or even entire documents? In standard sequenceto-sequence generation tasks, an input sequence is mapped to a vector embedding that represents the sequence, and then to an output string of words. Multi-text generation tasks like summarizatio</context>
</contexts>
<marker>Luong, Sutskever, Le, Vinyals, Zaremba, 2015</marker>
<rawString>Thang Luong, Ilya Sutskever, Quoc V Le, Oriol Vinyals, and Wojciech Zaremba. 2015. Addressing the rare word problem in neural machine translation. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="1373" citStr="Mann and Thompson, 1988" startWordPosition="200" endWordPosition="203">valuate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1. 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The rhetorical parsing of unrestricted texts: A surface-based approach.</title>
<date>2000</date>
<booktitle>Computational linguistics,</booktitle>
<pages>26--3</pages>
<contexts>
<context position="1498" citStr="Marcu, 2000" startWordPosition="221" endWordPosition="222"> in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1. 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within t</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The rhetorical parsing of unrestricted texts: A surface-based approach. Computational linguistics, 26(3):395–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17468" citStr="Papineni et al., 2002" startWordPosition="2869" endWordPosition="2872"> epochs. • Batch size is set to 32 (32 documents). • Decoding algorithm allows generating at most 1.5 times the number of words in inputs. • 0.2 dropout rate. • Gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 5. Our implementation on a single GPU2 processes a speed of approximately 600-1,200 tokens per second. We trained our models for a total of 7 iterations. 4.3 Evaluations We need to measure the closeness of the output (candidate) to the input (reference). We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002). ROUGE is a recall-oriented measure widely used in the summarization literature. It measures the n-gram recall between the candidate text and the reference text(s). In this work, we only have one reference document (the input document) and ROUGE score is therefore given by: ROUGEn = Egramn∈input countmatch(gramn) E gramn∈input count(gramn) (16) 2Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores. where countmatch denotes the number of n-grams co-occurring in the input and output. We report ROUGE-1, 2 and W (based on weighted longest common subsequence). BLEU Purely measuring recall will inappropria</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="2805" citStr="Sutskever et al., 2014" startWordPosition="418" endWordPosition="421">tifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that they wish to express. Could these models be extended to deal with generation of larger structures like paragraphs or even entire documents? In standard sequenceto-sequence generation tasks, an input sequence is mapped to a vector embedding that represents the sequence, and then to an output string of words. M</context>
<context position="6880" citStr="Sutskever et al., 2014" startWordPosition="1086" endWordPosition="1089">t − 1. Note that each sentence ends up with a special end-ofsentence symbol &lt;end&gt;. Commonly, the input and output use two different LSTMs with different sets of convolutional parameters for capturing different compositional patterns. In the decoding procedure, the algorithm terminates when an &lt;end&gt; token is predicted. At each timestep, either a greedy approach or beam search can be adopted for word prediction. Greedy search selects the token with the largest conditional probability, the embedding of which is then combined with preceding output for next step token prediction. For beam search, (Sutskever et al., 2014) discovered that a beam size of 2 suffices to provide most of benefits of beam search. 3 Paragraph Autoencoder In this section, we introduce our proposed hierarchical LSTM model for the autoencoder. 3.1 Notation Let D denote a paragraph or a document, which is comprised of a sequence of ND sentences, D = {s1, s2, ..., sND, endD}. An additional ”endD” token is appended to each document. Each sentence s is comprised of a sequence of tokens s = {w1, w2,..., wNs} where Ns denotes the length of the sentence, each sentence ending with an “ends” token. The word w is associated with a K-dimensional em</context>
<context position="8832" citStr="Sutskever et al. (2014)" startWordPosition="1430" endWordPosition="1433"> 2. For clarification, we first describe the following notations used in encoder and decoder: • hwt and hst denote hidden vectors from LSTM models, the subscripts of which indicate timestep t, the superscripts of which indicate operations at word level (w) or sequence level (s). hst(enc) specifies encoding stage and hst(dec) specifies decoding stage. • ewt and est denotes word-level and sentencelevel embedding for word and sentence at position t in terms of its residing sentence or document. 3.2 Model 1: Standard LSTM The whole input and output are treated as one sequence of tokens. Following Sutskever et al. (2014) and Bahdanau et al. (2014), we trained an autoencoder that first maps input documents into vector representations from a LSTMencode and then reconstructs inputs by predicting tokens within the document sequentially from a LSTMdecode. Two separate LSTMs are implemented for encoding and decoding with no sentence structures considered. Illustration is shown in Figure 1. 3.3 Model 2: Hierarchical LSTM The hierarchical model draws on the intuition that just as the juxtaposition of words creates a joint meaning of a sentence, the juxtaposition of sentences also creates a joint meaning of a paragrap</context>
<context position="16164" citStr="Sutskever et al., 2014" startWordPosition="2642" endWordPosition="2645">tails are shown in Table 1. Wikipedia We extracted paragraphs from Wikipedia corpus that meet the aforementioned length requirements. We keep a top frequent vocabulary list of 120,000 words. Paragraphs with larger than 4 percent of unknown words are discarded. The training dataset is comprised of roughly 500,000 paragraphs and testing contains roughly 50,000. σ # &amp;quot;hs# t_1(dec) σ W · es (13) t σ mt tanh t # &amp;quot; = &amp;quot; it ft ot lt 1110 4.2 Training Details and Implementation Previous research has shown that deep LSTMs work better than shallow ones for sequence-tosequence tasks (Vinyals et al., 2014; Sutskever et al., 2014). We adopt a LSTM structure with four layer for encoding and four layer for decoding, each of which is comprised of a different set of parameters. Each LSTM layer consists of 1,000 hidden neurons and the dimensionality of word embeddings is set to 1,000. Other training details are given below, some of which follow Sutskever et al. (2014). • Input documents are reversed. • LSTM parameters and word embeddings are initialized from a uniform distribution between [-0.08, 0.08]. • Stochastic gradient decent is implemented without momentum using a fixed learning rate of 0.1. We stated halving the lea</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oriol Vinyals</author>
<author>Lukasz Kaiser</author>
<author>Terry Koo</author>
<author>Slav Petrov</author>
<author>Ilya Sutskever</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Grammar as a foreign language. arXiv preprint arXiv:1412.7449.</title>
<date>2014</date>
<contexts>
<context position="2883" citStr="Vinyals et al., 2014" startWordPosition="433" endWordPosition="436"> for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that they wish to express. Could these models be extended to deal with generation of larger structures like paragraphs or even entire documents? In standard sequenceto-sequence generation tasks, an input sequence is mapped to a vector embedding that represents the sequence, and then to an output string of words. Multi-text generation tasks like summarization could work in a similar way: the</context>
<context position="16139" citStr="Vinyals et al., 2014" startWordPosition="2638" endWordPosition="2641">00 reviews. Dataset details are shown in Table 1. Wikipedia We extracted paragraphs from Wikipedia corpus that meet the aforementioned length requirements. We keep a top frequent vocabulary list of 120,000 words. Paragraphs with larger than 4 percent of unknown words are discarded. The training dataset is comprised of roughly 500,000 paragraphs and testing contains roughly 50,000. σ # &amp;quot;hs# t_1(dec) σ W · es (13) t σ mt tanh t # &amp;quot; = &amp;quot; it ft ot lt 1110 4.2 Training Details and Implementation Previous research has shown that deep LSTMs work better than shallow ones for sequence-tosequence tasks (Vinyals et al., 2014; Sutskever et al., 2014). We adopt a LSTM structure with four layer for encoding and four layer for decoding, each of which is comprised of a different set of parameters. Each LSTM layer consists of 1,000 hidden neurons and the dimensionality of word embeddings is set to 1,000. Other training details are given below, some of which follow Sutskever et al. (2014). • Input documents are reversed. • LSTM parameters and word embeddings are initialized from a uniform distribution between [-0.08, 0.08]. • Stochastic gradient decent is implemented without momentum using a fixed learning rate of 0.1. </context>
</contexts>
<marker>Vinyals, Kaiser, Koo, Petrov, Sutskever, Hinton, 2014</marker>
<rawString>Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2014. Grammar as a foreign language. arXiv preprint arXiv:1412.7449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Wolf</author>
<author>Edward Gibson</author>
</authors>
<title>Representing discourse coherence: A corpus-based study.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="19719" citStr="Wolf and Gibson (2005)" startWordPosition="3220" endWordPosition="3223">rzilay, 2005). The key idea of Barzilay and Lapata’s models is to first identify grammatical roles (i.e., object and subject) that entities play and then model the transition probability over entities and roles across sentences. We represent each sentence as a featurevector consisting of verbs and nouns in the sentence. Next we align sentences from output documents to input sentences based on sentence-tosentence F1 scores (precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched to one input 3Wolf and Gibson (2005) and Lin et al. (2011) proposed metrics based on discourse relations, but these are hard to apply widely since identifying discourse relations is a difficult problem. Indeed sophisticated coherence evaluation metrics are seldom adopted in real-world applications, and summarization researchers tend to use simple approximations like number of overlapped tokens or topic distribution similarity (e.g., (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-T¨ur, 2011)). 1111 Input-Wiki washington was unanimously elected President by the electors in both the 1788 – 1789 and 1792 elections . </context>
</contexts>
<marker>Wolf, Gibson, 2005</marker>
<rawString>Florian Wolf and Edward Gibson. 2005. Representing discourse coherence: A corpus-based study. Computational Linguistics, 31(2):249–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kelvin Xu</author>
<author>Jimmy Ba</author>
<author>Ryan Kiros</author>
<author>Aaron Courville</author>
<author>Ruslan Salakhutdinov</author>
<author>Richard Zemel</author>
<author>Yoshua Bengio</author>
</authors>
<title>Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044.</title>
<date>2015</date>
<contexts>
<context position="12321" citStr="Xu et al., 2015" startWordPosition="1998" endWordPosition="2001">ken endD is predicted. Details are shown in Figure 2. 1108 Figure 1: Standard Sequence to Sequence Model. Figure 2: Hierarchical Sequence to Sequence Model. Figure 3: Hierarchical Sequence to Sequence Model with Attention. 3.4 Model 3: Hierarchical LSTM with Attention Attention models adopt a look-back strategy by linking the current decoding stage with input sentences in an attempt to consider which part of the input is most responsible for the current decoding state. This attention version of hierarchical model is inspired by similar work in image caption generation and machine translation (Xu et al., 2015; Bahdanau et al., 2014). Let H = {hi(e), h2(e), ..., h-,(e)} be the 1109 collection of sentence-level hidden vectors for each sentence from the inputs, outputted from LSTMSentence encode . Each element in H contains information about input sequences with a strong focus on the parts surrounding each specific sentence (time-step). During decoding, suppose that est denotes the sentence-level embedding at current step and that hst_1(dec) denotes the hidden vector outputted from LSTMsentence decode at previous time step t−1. Attention models would first link the currentstep decoding information, i</context>
</contexts>
<marker>Xu, Ba, Kiros, Courville, Salakhutdinov, Zemel, Bengio, 2015</marker>
<rawString>Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Liang Kong</author>
<author>Congrui Huang</author>
<author>Xiaojun Wan</author>
<author>Xiaoming Li</author>
<author>Yan Zhang</author>
</authors>
<title>Timeline generation through evolutionary trans-temporal summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>433--443</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20137" citStr="Yan et al., 2011" startWordPosition="3281" endWordPosition="3284">s (precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched to one input 3Wolf and Gibson (2005) and Lin et al. (2011) proposed metrics based on discourse relations, but these are hard to apply widely since identifying discourse relations is a difficult problem. Indeed sophisticated coherence evaluation metrics are seldom adopted in real-world applications, and summarization researchers tend to use simple approximations like number of overlapped tokens or topic distribution similarity (e.g., (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-T¨ur, 2011)). 1111 Input-Wiki washington was unanimously elected President by the electors in both the 1788 – 1789 and 1792 elections . he oversaw the creation of a strong, well-financed national government that maintained neutrality in the french revolutionary wars , suppressed the whiskey rebellion , and won acceptance among Americans of all types. washington established many forms in government still used today , such as the cabinet system and inaugural address . his retirement after two terms and the peaceful transition from his presidency to t</context>
</contexts>
<marker>Yan, Kong, Huang, Wan, Li, Zhang, 2011</marker>
<rawString>Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xiaoming Li, and Yan Zhang. 2011a. Timeline generation through evolutionary trans-temporal summarization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 433–443. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Xiaojun Wan</author>
<author>Jahna Otterbacher</author>
<author>Liang Kong</author>
<author>Xiaoming Li</author>
<author>Yan Zhang</author>
</authors>
<title>Evolutionary timeline summarization: a balanced optimization framework via iterative substitution.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,</booktitle>
<pages>745--754</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="20137" citStr="Yan et al., 2011" startWordPosition="3281" endWordPosition="3284">s (precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched to one input 3Wolf and Gibson (2005) and Lin et al. (2011) proposed metrics based on discourse relations, but these are hard to apply widely since identifying discourse relations is a difficult problem. Indeed sophisticated coherence evaluation metrics are seldom adopted in real-world applications, and summarization researchers tend to use simple approximations like number of overlapped tokens or topic distribution similarity (e.g., (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-T¨ur, 2011)). 1111 Input-Wiki washington was unanimously elected President by the electors in both the 1788 – 1789 and 1792 elections . he oversaw the creation of a strong, well-financed national government that maintained neutrality in the french revolutionary wars , suppressed the whiskey rebellion , and won acceptance among Americans of all types. washington established many forms in government still used today , such as the cabinet system and inaugural address . his retirement after two terms and the peaceful transition from his presidency to t</context>
</contexts>
<marker>Yan, Wan, Otterbacher, Kong, Li, Zhang, 2011</marker>
<rawString>Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong, Xiaoming Li, and Yan Zhang. 2011b. Evolutionary timeline summarization: a balanced optimization framework via iterative substitution. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 745–754. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>