<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<note confidence="0.793256">
DISCOURSE STRUCTURE ANALYSIS FOR NEWS VIDEO
</note>
<author confidence="0.7113215">
Yasuhiko Watanabe† Yoshihiro Okada† Sadao Kurohashi‡ Eiichi Iwanari†
† Dept. of Electronics and Informatics, Ryukoku University, Seta, Otsu, Shiga, Japan
</author>
<affiliation confidence="0.880306">
‡Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo, Kyoto, Japan
</affiliation>
<email confidence="0.994477">
watanabe@rins.ryukoku.ac.jp
</email>
<sectionHeader confidence="0.955782" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999719428571429">
Various kinds of video recordings have discourse
structures. Therefore, it is important to determine
how video segments are combined and what kind
of coherence relations they are connected with. In
this paper, we propose a method for estimating the
discourse structure of video news reports by ana-
lyzing the discourse structure of their transcripts.
</bodyText>
<sectionHeader confidence="0.993239" genericHeader="introduction">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999979708333333">
A large number of studies have been made on video
analysis, especially segmentation, feature extrac-
tion, indexing, and classification. On the other
hand, little attention has been given to the dis-
course structure (DS) of video data.
Various kinds of video recordings, such as dra-
mas, documentaries, news reports, and sports cast-
ings, have discourse structures. In other words,
each video segment of these video recordings is re-
lated to previous ones by some kind of relation
(coherence relation) which determines the role of
the video segments in discourse. For this reason,
it is important to determine how video segments
are combined and what kind of coherence relations
they are connected with. In addition, Nagao et.al
proposed a method for multimedia data summa-
rization using GDA tags [Nagao 00]. However, the
cost of making GDA tagged data is great. Our
method will be helpful in reducing the annotation
cost.
In this paper, we propose a method for estimat-
ing the discourse structure of video news reports.
Coherence relations between video segments are es-
timated in the following way:
</bodyText>
<listItem confidence="0.99525275">
1. a video news article is segmented into shots
by using DCT components,
2. consecutive shots are merged by using speech
information, and
</listItem>
<figureCaption confidence="0.9450405">
Figure 1: Procedure of discourse structure analysis
for news video
</figureCaption>
<listItem confidence="0.9997501">
3. coherence relations are estimated by using
three kinds of clues in the transcript of the
news video:
• clue expressions indicating a certain re-
lation,
• occurrence of identical/synonymous words/
phrases in topic chaining or topic-dominant
chaining relation, and
• similarity between two sentences in list
or contrast relation.
</listItem>
<bodyText confidence="0.998437">
Figure 1 shows the procedure of discourse structure
analysis for news video. We applied our method to
NHK1 News 2. This method is aimed to make the
process of retrieval, summarization, and informa-
tion extraction more efficient.
</bodyText>
<footnote confidence="0.9959315">
1Nippon Hoso Kyokai (Japan Broadcasting Corporation)
2NHK news reports do not have closed captions. Instead
of closed captions, we used scripts which were read out by
newscasters as transcripts.
</footnote>
<figure confidence="0.963546066666667">
merge shots
( using speech information)
segmentation
( using image information)
discourse structure analysis
( using transcripts )
news video
discourse
unit
discourse
structure
shots
shot (a) shot (b) shot (c)
(I) The US dollar inched up against the yen as the stock market (II) The US currency traded at
continued the selling trend in Tokyo today. 145.63–65 yen at 5 p.m. Tokyo.
</figure>
<figureCaption confidence="0.990433">
Figure 2: An example of shots and their transcript in a news video (NHK evening news, August/3/1998)
</figureCaption>
<sectionHeader confidence="0.7744035" genericHeader="method">
2. DISCOURSE STRUCTURE AND
VIDEO
</sectionHeader>
<bodyText confidence="0.999672365384615">
Little attention has been given to discourse struc-
ture of video data in image processing. This is be-
cause it is difficult to determine it only by analyzing
image data. In contrast to this, discourse structure
is the subject of a large number of studies in nat-
ural language processing. So several methods for
estimating the discourse structure of a text have
been explored[Sumita 92] [Kurohashi 94]. There-
fore, these methods can be applied to language data
of video data in order to determine discourse struc-
ture in video data.
In addition, some researchers in natural lan-
guage processing showed that the information of
discourse structure is useful for extracting signifi-
cant sentences and summarizing a text [Miike 94]
[Marcu 97]. It suggests that information of video
discourse structure is utilized for extracting signifi-
cant video segments and skimming. It may be use-
ful to look at video skimming and extraction of
significant segments before we discuss some points
about discourse structure analysis because they are
closely related to the discourse structure estima-
tion.
One of the simple ways to skim a video is by
using the pair of the first frame/image of the first
shot and the first sentence in the transcript. How-
ever, this representative pair of image and language
is often a poor topic explanation. To solve this
problem, Zhang, et.al, proposed a method for key-
frame selection by using several image features such
as colors, textures, and temporal features includ-
ing camera operations [Zhang 95]. Also, Smith and
Kanade proposed video skimming by selecting video
segments based on TFIDF, camera motion, human
face, captions on video, and so on [Smith 97]. These
techniques are broadly applicable, however, still
have problems. One is the semantic classification
of each segment. To solve this problem, Naka-
mura and Kanade proposed the spotting by asso-
ciation method which detects relevant video seg-
ments by associating image data and language data
[Nakamura 97]. Also, Watanabe, et.al, proposed
a method for analyzing telops (captions) in video
news reports by using layout and language informa-
tion [Watanabe 96]. However, these studies did not
deal with coherence relations between video seg-
ments.
In contrast to this, several works on discourse
structure have been made by researchers in natural
language processing. Pursuing these studies, we are
confronted with two points of discourse structure
analysis:
</bodyText>
<listItem confidence="0.98495725">
• available knowledge for estimating discourse
structure, and
• definition for discourse units and coherence
relations.
</listItem>
<bodyText confidence="0.994774301886793">
First, we shall discuss the available knowledge
for estimating discourse structure. Most studies
on discourse structure have focused on such ques-
tions as what kind of knowledge should be em-
ployed, and how inference may be performed based
on such knowledge (e.g., [Grosz 86], [Hobbs 85],
[Zadrozny 91]). In contrast to this, Kurohashi and
Nagao pointed out that a detailed knowledge base
with broad coverage is unlikely to be constructed
in the near future, and that we should analyze dis-
courses using presently available knowledge. For
these reasons, they proposed a method for estimat-
ing discourse structure by using surface information
in sentences [Kurohashi 94]. In video analysis, the
same problems occurred. Therefore, we propose
here a method for estimating the discourse struc-
ture in a news report by using surface information
in the transcript.
Next, we shall discuss the definition for dis-
course unit and coherence relation. As mentioned,
discourses are composed of segments (discourse units),
and these are connected to previous ones by co-
herence relations. However, there has been a vari-
ety of definitions for discourse unit and coherence
relation. For example, a discourse unit can be a
frame, a shot, or a group of several consecutive
shots. In this study, we consider as a discourse
unit, one or more shots which are associated with
one part of announcer’s speech. For example, shot
(a), (b), and (c) in Figure 2 represent consecutive
shots in the news video “Both yen and stock were
dropped”(Aug/3/1998), while sentence (I) and (II)
are parts of the transcript. Sentence (I) was spoken
in shot (a) and (b), and correspondingly, sentence
(II) was spoken in shot (c). As a result, shot (a)
and (b) were merged and the result was considered
as one discourse unit. On the other hand, shot (c)
alone constituted one discourse unit. We will ex-
plain how to extract the discourse units in Section
3.1.
In contrast to this, coherence relations strongly
depend on the genre of video data: dramas, docu-
mentaries, news reports, sports castings, and so on.
From the number of coherence relations suggested
so far, we selected the following relations for our
target, news reports:
List: Si and S; involve the same or similar events
or states, or the same or similar important
constituents
Contrast: Si and S; have distinct events or states,
or contrasting important constituents
Topic chaining: Si and S; have distinct predica-
tions about the same topic
</bodyText>
<listItem confidence="0.735822625">
Topic-dominant chaining: A dominant constituent
apart from a given topic in Si becomes a topic
in S;
Elaboration: S; gives details about a constituent
introduced in Si
Reason: S; is the reason for Si
Cause: S; occurs as a result of Si
Example: S; is an example of Si
</listItem>
<bodyText confidence="0.9959615">
where Si denotes the former segment and S; the
latter.
</bodyText>
<sectionHeader confidence="0.9614765" genericHeader="method">
3. ESTIMATION OF DISCOURSE
STRUCTURE
</sectionHeader>
<bodyText confidence="0.991345333333333">
Our determination of how video segments are com-
bined and what kind of coherence relations are in-
volved is made in the next way:
</bodyText>
<listItem confidence="0.9800804">
1. extract discourse units from a news report,
2. extract three kinds of clue information from
transcripts, and then, transform them into
reliable scores for some relations, and
3. choose the connected sentence and the rela-
</listItem>
<bodyText confidence="0.932827">
tion having the maximum reliable score. If
two or more connected sentences have the
same maximum score, the chronological near-
est segment is selected.
</bodyText>
<subsectionHeader confidence="0.99612">
3.1. Extraction of Discourse Units
</subsectionHeader>
<bodyText confidence="0.998673533333333">
A shot is generally regarded as a basic unit in video
analysis. In this study, however, not only a shot
but also more consecutive ones are considered a
basic unit (discourse unit). This is because there
are some cases where several consecutive shots cor-
respond with one sentences in a transcript. In this
case, these consecutive shots should be regarded
as a discourse unit. In contrast to this, one shot
should be regarded as a discourse unit when it cor-
respond with one or more sentences in a transcript.
In both cases, the start/end point of a discourse
unit often lies in the pause because the announcer
needs to take breath at the end of a sentence. As
a result, discourse units are extracted in the next
way:
</bodyText>
<listItem confidence="0.9508862">
1. detect scene cuts in a video by using DCT
components [Iwanari 94],
2. detect speech pauses in the video, and
3. extract the start/end points of discourse units
by detecting the cuts in the pause.
</listItem>
<bodyText confidence="0.999519545454545">
For evaluating this method, we used 105 news re-
ports of NHK News. The recall and precision of
discourse unit detection were 71% and 97%, respec-
tively, while those of scene change detection were
80% and 90%. We modified the extracted discourse
units by hand and used them in the discourse struc-
ture analysis described in Section 3.2. In addition,
each discourse unit was associated with the corre-
sponding sentences in a transcript by hands. This
is because NHK news reports do not have closed
captions.
</bodyText>
<subsectionHeader confidence="0.888866">
3.2. Detection of Coherence Relations
</subsectionHeader>
<bodyText confidence="0.9686866">
Rule-1
In order to extract discourse structure, we use three
kinds of clue information in transcripts:
Then they are transformed into reliable scores for
some relations. In other words, as a new sentence
(NS) comes in, reliable scores for all possible con-
nected sentences and relations are calculated by us-
ing above three types of clues. As a final result, we
choose the connected sentence (CS) and the rela-
tion having the maximum reliable score.
</bodyText>
<subsubsectionHeader confidence="0.727235">
3.2.1. Detection of Clue Expressions
</subsubsectionHeader>
<bodyText confidence="0.999033857142857">
In this study, we use 41 heuristic rules for finding
clue expressions by pattern matching and relating
them to proper relations with reliable scores. A
rule consists of two parts: (1) conditions for rule
application and (2) corresponding relation and re-
liable score. Conditions for rule application consist
of four parts:
</bodyText>
<listItem confidence="0.999551">
• rule applicable range,
• relation of CS to its previous DS,
• dependency structure pattern for CS, and
• dependency structure pattern for NS.
</listItem>
<bodyText confidence="0.998469384615385">
Pattern for CS and NS are matched not for word
sequences but for dependency structures of both
sentences. We apply each rule for the pairs of a CS
and NS. If the condition of the rule is satisfied, the
specified reliable score is given to the corresponding
relation between the CS and the NS.
For example, Rule-1 in Figure 3 gives a score (20
points) to the reason relation between two adjoin-
ing sentences if the NS starts with the expression
“nazenara (because)”. Rule-2 in Figure 3 is applied
not only for the neighboring CS but also for farther
CSs, by specifying the occurrence of identical words
“X” in the condition.
</bodyText>
<subsubsectionHeader confidence="0.800396">
3.2.2. Detection of Word/Phase Chain
</subsubsectionHeader>
<bodyText confidence="0.929337666666667">
In general, a sentence can be divided into two parts:
a topic part and a non-topic part. When two sen-
tences are in a topic chaining relation, the same
</bodyText>
<equation confidence="0.714705863636363">
range : 1
relation of CS : *
CS : * NS : nazenara
( because )
relation : reason
score : 20
Rule-2
range : *
relation of CS : *
*
*
relation : exemplification-present
score : 30
CS :
*
X no
( of )
X *
rei
NS : *
*
( example )
</equation>
<figureCaption confidence="0.76157">
Figure 3: Examples of heuristic rules for clue ex-
pressions
topic is maintained through them. Therefore, the
occurrence of identical/synonymous word/phrase
(the word/phrase chain) in topic parts of two sen-
tences supports this relation. On the other hand,
in the case of topic-dominant chaining relation, a
dominant constituent introduced in a non-topic part
of a prior sentence becomes a topic in a succeeding
sentence. As shown, the word/phrase chain from a
non-topic part of a prior sentence to a topic part
of a succeeding sentence supports this relation.
</figureCaption>
<bodyText confidence="0.97177615">
For these reasons, we detect word/phrase chains
and calculate reliable scores in the next way:
1. give scores to words/phrases in topic and non-
topic parts according to the degree of their
importance in sentences,
2. give scores to the matching of identical/synonymous
words/phrases according to the degree of their
agreement, and
3. give these relations the sum of the scores of
two chained words/phrases and the score of
their matching.
For example, by Rule-a and Rule-b in Figure 4,
words in a phrase whose head word is followed by
a topic marking postposition “wa” are given some
scores as topic parts. Also, a word in a non-topic
part in the sentential style, “ga aru (there is ...)” is
given a large score (11 points) by Rule-c in Figure 4
because this word is an important new information
in this sentence and topic-dominant chaining rela-
tion involving it often occur. Matching of phrases
</bodyText>
<listItem confidence="0.82999125">
• clue expressions indicating some relations,
• occurrence of identical/synonymous words/phrases
in topic chaining or topic-dominant chaining
relation, and
</listItem>
<figure confidence="0.8432546">
• similarity between two sentences in list or
contrast relation.
Matching
Rule-d
Non-topic part
</figure>
<figureCaption confidence="0.975820571428571">
Figure 4: Examples of rules for topic/non-topic
parts
like “A of B” is given a larger score (8 points) by
Rule-e than that of word like “A” alone by Rule-d
(5 points) in Figure 4.
3.2.3. Calculation of Similarity between Sen-
tences in a Transcript
</figureCaption>
<bodyText confidence="0.999889066666667">
When two sentences have list or contrast relation,
they have a certain similarity. As a result, we mea-
sure such a similarity for finding list or contrast
relation in the next way. First, the similarity value
between two words are calculated according to ex-
act matching, matching of their parts of speech,
and their closeness in a thesaurus dictionary. Sec-
ond, the similarity value between two word-strings
is calculated roughly by combining the similarity
values between words in the two word-strings with
the dynamic programming method for analyzing
conjunctive structures [Kurohashi 94]. Then, we
give the normalized similarity score between a CS
and an NS to their list and contrast relations as a
reliable score.
</bodyText>
<sectionHeader confidence="0.998524" genericHeader="evaluation">
4. EXPERIMENTS AND DISCUSSION
</sectionHeader>
<bodyText confidence="0.9996905">
For evaluating this method, we used 22 news re-
ports of NHK News. Each report was a few min-
utes in length. The experimental results are shown
in Table 1. As mentioned, news reports of NHK
News do not have closed captions. For this reason,
each video segment (discourse unit) was associated
with the corresponding sentences in a transcript by
hands.
</bodyText>
<tableCaption confidence="0.542397">
Table 1: Analysis results Failure
</tableCaption>
<table confidence="0.9982164">
Relation Success
List 2 1
Contrast 1 0
Topic chaining 38 11
Topic-dominant chaining 20 2
Elaboration 0 3
Reason 0 0
Cause 4 0
Example 0 0
Total 65 17
</table>
<bodyText confidence="0.999671815789474">
Figure 5 shows the video news report we used
in our experiment. As shown, shot (b) and (c)
were merged together because there was no pause
at the cut point between them. Sentence (I), (II),
(III), (IV), and (V) were associated with shot (a),
(b)(c), (d), (e), and (f), respectively. Coherence re-
lations between video segments were estimated in
the following way: a topic-dominant chaining re-
lation was estimated between shot (a) and (b)(c)
because “Prime Minister Obuchi” was found in the
topic part of sentence (II) and in the non-topic part
of sentence (I). The same relation was also esti-
mated between shot (b)(c) and (d) because “the
Fiscal Structural Reform Law” was found in the
topic part of sentence (III) and in the non-topic
part of sentence (II). On the contrary, topic chain-
ing relation was estimated between shot (a) and
(e) because “the Ministry of Finance” was found
in the topic parts of sentence (I) and (IV). In this
case, the system detected also another relation: a
topic-dominant chaining relation between shot (b)
and (e). However, the system selected the former
one because the former exceeded the latter in score.
The system also determined a topic chaining rela-
tion between shot (e) and (f). In this case, the
system additionally detected two other relations:
topic chaining relation between shot (a) and (f),
and topic-dominant chaining relation between shot
(b) and (f). But the relation between (a) and (f)
was chosen, because its reliable score was greater
than the score between (b) and (f) and equal to the
score between (e) and (f), but there the distance
between the shots was greater. Figure 6 shows the
result of this analysis.
As shown in Table 1, 11 topic chaining and
2 topic-dominant chaining relations could not be
extracted. The reasons were (1) the topic words
of the following sentences were omitted 3 and (2)
</bodyText>
<footnote confidence="0.889988">
3There are many ellipses in Japanese sentences.
</footnote>
<figure confidence="0.974952212121212">
pattern : * wa
score : 10
Rule-b
*
pattern :
*
* wa
score : 8
Topic part
Rule-a
Rule-c
pattern : * ga
score : 11 aru
( there is )
y *
Y *
score : 8
Rule-f
pattern :
x
Rule-e
pattern :
X *
no
niyoru
X Y * ( of  |by ) y *
score : 6
X * x *
pattern :
score : 5
x
*
shot transcript
</figure>
<bodyText confidence="0.986542111111111">
(I) In accordance with instructions of Prime Minister
Obuchi, the Ministry of Finance will decide about new
guidelines for budget requests which is free from the re-
strictions of the Fiscal Structural Reform Law.
(II) Prime Minister Obuchi called Vice Minister Tanami,
the Ministry of Finance, into the Official Residence, and
instructed him to make new budget request guidelines in
line with the freeze policy of the Fiscal Structural Reform
Law.
</bodyText>
<figure confidence="0.270857333333333">
(c)
(III) The Fiscal Structural Reform Law sets upper limits
(d)
</figure>
<bodyText confidence="0.940622555555556">
(IV) In accordance with prime minister’s instruction, the
Ministry of Finance establishes new guidelines in which
key government expenditures, for example, public works
projects, are permitted to go beyond the limits of the Law.
(V) the Ministry of Finance will decide about the new
guidelines by the middle of the next week, and government
ministries and agencies will submit their budget requests in
accordance with this guideline by the end of the month.
for expenditures in all categories but social security.
</bodyText>
<figureCaption confidence="0.8138555">
Figure 5: An example of news video (“New guidelines for budget requests”, NHK evening news, Au-
gust/3/1998)
</figureCaption>
<figure confidence="0.653333">
References
</figure>
<figureCaption confidence="0.8692225">
Figure 6: The result of discourse structural analysis
for the news video shown in Figure 5
</figureCaption>
<bodyText confidence="0.999685567567568">
the topic word was changed (e.g., driver —&gt; man
who drove the car) or abbreviated. Also, 3 elabo-
ration relations could not be extracted. This was
because there were no clue expressions for the elab-
oration relation in the sentences. However, the sys-
tem could mostly detect clue expressions and oc-
currence of identical/synonymous words/ phrases.
In some cases (e.g., a compound sentence), there
were many clues for an NS supporting various re-
lations to several CSs. The system could detect
them, however, extracted only one CS and relation.
In this study, we introduce a reliable score for de-
termining the most plausible CS and relation. As
shown in Table 1, this method is useful, however,
we should investigate a method for extracting more
CSs and relations than one when several CSs and
relations exist.
In this study, we assumed that image and lan-
guage data correspond to the same portion of a
news report. For this reason, it is likely that the re-
lation between images slightly di↵ers from the anal-
ysis result when image and language are taken form
di↵erent portions (correspondence problem between
image and language).
At the end of this section, we discuss video sum-
marization using discourse structure information.
First, we consider summarization of the news video
shown in Figure 5 with the summarization topic
concerning the Ministry of Finance. The summa-
rization system traces topic chaining relations and
generates video summarization which consists of
shots (a), (e), and (f). Next, we consider summa-
rization of the same news video with the summa-
rization topic concerning the Prime Minister. The
system traces a topic-dominant chaining relation
and generates video summarization which consists
of shot (a), (b), and (c).
</bodyText>
<reference confidence="0.99327062">
[Grosz 86] Grosz and Sidner: Attention, Intentions,
and the Structures of Discourse, Computational
Linguistics, 12-3, (1986).
[Hobbs 85] Hobbs: On the Coherence and Structure
of Discourse, Technical Report No. CSLI-85-37,
(1985).
[Iwanari 94] Iwanari and Ariki: Scene Clustering and
Cut Detection in Moving Images by DCT compo-
nents, (in Japanese), technical report of IEICE,
PRU-93-119, (1994).
[Kurohashi 92] Kurohashi and Nagao: Dynamic Pro-
gramming Method for Analyzing Conjunctive
Structures in Japanese, COLING-92, (1992).
[Kurohashi 94] Kurohashi and Nagao: Automatic De-
tection of Discourse Structure by Checking Surface
Information in Sentences, COLING-94, (1994).
[Marcu 97] Marcu: From Discourse Structures to Text
Summaries, ACL workshop on Intelligent Scalable
Text Summarization, (1997).
[Miike 94] Miike, Itoh, Ono, and Sumita: A Full-Text
Retrieval System with a Dynamic Abstract Gen-
eration Function, SIGIR-94, (1994).
[Nagao 00] Nagao, Shirai, and Hashida: Multimedia
Data Summarization Based on the Global Doc-
ument Annotation, (in Japanese), 6th Annual
Meeting of The Association for Natural Language
Processing, (2000).
[Nakamura 97] Nakamura and Kanade: Semantic
Analysis for Video Contents Extraction – Spotting
by Association in News Video, ACM Multimedia
97, (1997).
[Smith 97] Smith and Kanade: Video Skimming and
Characterization through the Combination of Im-
age and Language Understanding Techniques,
IEEE CVPR, (1997).
[Sumita 92] Sumita, Ono, Chino, Ukita, and Amano:
A Discourse Structure Analyzer for Japanese
Text, International Conference of Fifth Generation
Computer Systems, (1992).
[Watanabe 96] Watanabe, Okada, and Nagao: Se-
mantic Analysis of Telops in TV Newscasts, (in
Japanese), technical report of Information Pro-
cessing Society of Japan, NL-116–16, (1996).
[Zadrozny 91] Zadrozny and Jensen: Semantics of
Paragraphs, Computational Linguistics, 17-2,
(1991).
[Zhang 95] Zhang, Low, Smoliar, and Wu: Video Pars-
ing, Retrieval and Browsing: An Integrated and
Content-Based Solution, ACM Multimedia 95,
(1995).
</reference>
<figure confidence="0.99731825">
topic−dominant
chaining relation
( Prime Minister Obuchi )
a
b, c
e
d
f
topic chaining relation
( the Ministry of Finance )
topic−dominant
chaining relation
( Fiscal Structural
Reform Law )
topic chaining relation
( the Ministry of Finance )
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.785985">
<title confidence="0.904656">DISCOURSE STRUCTURE ANALYSIS FOR NEWS VIDEO</title>
<author confidence="0.897245">of Electronics</author>
<author confidence="0.897245">Ryukoku University Informatics</author>
<author confidence="0.897245">Otsu Seta</author>
<author confidence="0.897245">Japan Shiga</author>
<affiliation confidence="0.939856">School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo, Kyoto, Japan</affiliation>
<email confidence="0.940181">watanabe@rins.ryukoku.ac.jp</email>
<abstract confidence="0.9991315">Various kinds of video recordings have discourse structures. Therefore, it is important to determine how video segments are combined and what kind of coherence relations they are connected with. In this paper, we propose a method for estimating the discourse structure of video news reports by analyzing the discourse structure of their transcripts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Grosz</author>
</authors>
<title>Sidner: Attention, Intentions, and the Structures of Discourse,</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--3</pages>
<marker>[Grosz 86]</marker>
<rawString>Grosz and Sidner: Attention, Intentions, and the Structures of Discourse, Computational Linguistics, 12-3, (1986).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hobbs</author>
</authors>
<title>On the Coherence and Structure of Discourse,</title>
<date>1985</date>
<tech>Technical Report No. CSLI-85-37,</tech>
<marker>[Hobbs 85]</marker>
<rawString>Hobbs: On the Coherence and Structure of Discourse, Technical Report No. CSLI-85-37, (1985).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iwanari</author>
</authors>
<title>Ariki: Scene Clustering and Cut Detection in Moving Images by DCT components, (in Japanese), technical report of IEICE,</title>
<date>1994</date>
<pages>93--119</pages>
<marker>[Iwanari 94]</marker>
<rawString>Iwanari and Ariki: Scene Clustering and Cut Detection in Moving Images by DCT components, (in Japanese), technical report of IEICE, PRU-93-119, (1994).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurohashi</author>
</authors>
<title>Nagao: Dynamic Programming Method for Analyzing Conjunctive Structures</title>
<date>1992</date>
<booktitle>in Japanese, COLING-92,</booktitle>
<marker>[Kurohashi 92]</marker>
<rawString>Kurohashi and Nagao: Dynamic Programming Method for Analyzing Conjunctive Structures in Japanese, COLING-92, (1992).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurohashi</author>
</authors>
<title>Nagao: Automatic Detection of Discourse Structure by Checking Surface Information</title>
<date>1994</date>
<booktitle>in Sentences, COLING-94,</booktitle>
<marker>[Kurohashi 94]</marker>
<rawString>Kurohashi and Nagao: Automatic Detection of Discourse Structure by Checking Surface Information in Sentences, COLING-94, (1994).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcu</author>
</authors>
<title>From Discourse Structures to Text Summaries,</title>
<date>1997</date>
<booktitle>ACL workshop on Intelligent Scalable Text Summarization,</booktitle>
<marker>[Marcu 97]</marker>
<rawString>Marcu: From Discourse Structures to Text Summaries, ACL workshop on Intelligent Scalable Text Summarization, (1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Itoh Miike</author>
<author>Ono</author>
</authors>
<title>Sumita: A Full-Text Retrieval System with a Dynamic Abstract Generation Function,</title>
<date>1994</date>
<marker>[Miike 94]</marker>
<rawString>Miike, Itoh, Ono, and Sumita: A Full-Text Retrieval System with a Dynamic Abstract Generation Function, SIGIR-94, (1994).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shirai Nagao</author>
</authors>
<title>and Hashida: Multimedia Data Summarization Based on the Global Document Annotation,</title>
<date>2000</date>
<booktitle>(in Japanese), 6th Annual Meeting of The Association for Natural Language Processing,</booktitle>
<marker>[Nagao 00]</marker>
<rawString>Nagao, Shirai, and Hashida: Multimedia Data Summarization Based on the Global Document Annotation, (in Japanese), 6th Annual Meeting of The Association for Natural Language Processing, (2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nakamura</author>
</authors>
<title>Kanade: Semantic Analysis for Video Contents Extraction – Spotting by Association in News Video,</title>
<date>1997</date>
<journal>ACM Multimedia</journal>
<volume>97</volume>
<marker>[Nakamura 97]</marker>
<rawString>Nakamura and Kanade: Semantic Analysis for Video Contents Extraction – Spotting by Association in News Video, ACM Multimedia 97, (1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Smith</author>
</authors>
<title>Kanade: Video Skimming and Characterization through the Combination of Image and Language Understanding Techniques,</title>
<date>1997</date>
<journal>IEEE CVPR,</journal>
<marker>[Smith 97]</marker>
<rawString>Smith and Kanade: Video Skimming and Characterization through the Combination of Image and Language Understanding Techniques, IEEE CVPR, (1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ono Sumita</author>
<author>Ukita Chino</author>
<author>Amano</author>
</authors>
<date>1992</date>
<booktitle>A Discourse Structure Analyzer for Japanese Text, International Conference of Fifth Generation Computer Systems,</booktitle>
<marker>[Sumita 92]</marker>
<rawString>Sumita, Ono, Chino, Ukita, and Amano: A Discourse Structure Analyzer for Japanese Text, International Conference of Fifth Generation Computer Systems, (1992).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okada Watanabe</author>
</authors>
<title>and Nagao: Semantic Analysis of Telops in TV Newscasts,</title>
<date>1996</date>
<booktitle>(in Japanese), technical report of Information Processing Society of Japan, NL-116–16,</booktitle>
<marker>[Watanabe 96]</marker>
<rawString>Watanabe, Okada, and Nagao: Semantic Analysis of Telops in TV Newscasts, (in Japanese), technical report of Information Processing Society of Japan, NL-116–16, (1996).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zadrozny</author>
</authors>
<title>Jensen: Semantics of Paragraphs,</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--2</pages>
<marker>[Zadrozny 91]</marker>
<rawString>Zadrozny and Jensen: Semantics of Paragraphs, Computational Linguistics, 17-2, (1991).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Low Zhang</author>
</authors>
<title>Smoliar, and Wu: Video Parsing, Retrieval and Browsing: An Integrated and Content-Based Solution,</title>
<date>1995</date>
<journal>ACM Multimedia 95,</journal>
<marker>[Zhang 95]</marker>
<rawString>Zhang, Low, Smoliar, and Wu: Video Parsing, Retrieval and Browsing: An Integrated and Content-Based Solution, ACM Multimedia 95, (1995).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>