<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000144">
<title confidence="0.997706">
Evaluating Machine Translation Systems with Second Language
Proficiency Tests
</title>
<author confidence="0.931597">
Takuya Matsuzaki†‡ Akira Fujita‡ Naoya Todo‡ Noriko H. Arai‡†Dept. of Electrical Engineering and Computer Science, Nagoya University
</author>
<email confidence="0.945813">
matuzaki@nuee.nagoya-u.ac.jp
</email>
<note confidence="0.758603">
‡Information and Society Research Division, National Institute of Informatics
</note>
<email confidence="0.98896">
{a-fujita, ntodo, arai}@nii.ac.jp
</email>
<sectionHeader confidence="0.993772" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967736842105">
A lightweight, human-in-the-loop evalua-
tion scheme for machine translation (MT)
systems is proposed. It extrinsically eval-
uates MT systems using human subjects’
scores on second language ability test
problems that are machine-translated to
the subjects’ native language. A large-
scale experiment involving 320 subjects
revealed that the context-unawareness of
the current MT systems severely damages
human performance when solving the test
problems, while one of the evaluated MT
systems performed as good as a human
translation produced in a context-unaware
condition. An analysis of the experimental
results showed that the extrinsic evaluation
captured a different dimension of transla-
tion quality than that captured by manual
and automatic intrinsic evaluation.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999870983050848">
Automatic evaluation metrics, such as the BLEU
score (Papineni et al., 2002), were crucial ingredi-
ents for the advances of machine translation tech-
nology in the last decade. Meanwhile, the short-
comings of BLEU and similar n-gram proximity-
based metrics have been pointed out by many au-
thors including Callison-Burch et al. (2006). The
main criticisms include: 1) unreliability in evalu-
ating short translations, 2) non-interpretability of
the scores beyond numerical comparison, and 3)
bias towards statistical MT systems.
Manual evaluation of translation quality is more
reliable in many regards, but it is costly. Further-
more, it is not necessarily easy to analyze the char-
acteristics of MT systems based solely on the eval-
uation results such as a 5-point scale evaluation of
adequacy/fluency and a ranking of the outputs of
different systems.
A remedy for some of the above-raised issues is
task-based evaluation of MT systems (Jones et al.,
2005; Voss and Tate, 2006; Laoudi et al., 2006;
Jones et al., 2007; Schneider et al., 2010; Berka
et al., 2011), which measures the human perfor-
mance in a task such as information extraction
from a machine-translated text. The main bur-
den of conducting task-based evaluation is also its
cost; the development of a sizable amount of test
materials and the gathering of appropriate human
subjects is time consuming and expensive.
This paper proposes to utilize second-language
proficiency tests (SLPTs), such as TOEIC, as the
source of the specimens for extrinsic evaluation
of MT systems. For evaluating, e.g., English-to-
Japanese MT systems, a set of English test prob-
lems is translated by the systems and the trans-
lation qualities are evaluated by the test scores
achieved by native Japanese speakers on the trans-
lated problems.
In many languages, a large collection of SLPT
problems is available. More than 130 standard-
ized tests for 32 languages are listed in the English
Wikipedia page for ‘List of language proficiency
tests’ as of April 30, 2015. They are carefully
designed to evaluate various aspects of language
ability with objective criteria. We can thus obtain
an easy-to-use test set that focuses on a certain as-
pect of MT system performance by appropriately
choosing the problem types and levels. Moreover,
SLPTs are primarily designed to assess the test-
takers’ language ability but not their general intel-
ligence. Hence, as evidenced later in the paper, the
proposed scheme is expected to be robust against
the heterogeneity of the subjects, as long as they
are native speakers of the target language. This is
a desirable property for conducting a large-scale
experiment, possibly with crowdsourcing.
In the current paper, we utilize a typical for-
mat of multiple-choice dialogue completion prob-
lems (Figure 1). The subjects are given amachine-
</bodyText>
<page confidence="0.896577">
145
</page>
<note confidence="0.295606333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 145–149,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.572902" genericHeader="introduction">
INSTRUCTION
</sectionHeader>
<bodyText confidence="0.7341665">
Choose the most suitable utterance for the blank in the
following dialogue from choices 1, 2, 3, and 4.
</bodyText>
<figure confidence="0.4295462">
DIALOGUE
A: Hello. Can I help you?
B: Yes. [BLANK]
A: I’m sorry, I can’t find that name on the reservation list.
B: Oh, really? Then give me a new reservation, please.
OPTIONS
1. I’d like to make a reservation for Flight 502.
2. I have a reservation under the name Hashimoto.
3. I’m sure you can find my name on the list.
4. I wonder if you could tell me how to make a reservation.
</figure>
<figureCaption confidence="0.957278">
Figure 1: Example of multiple-choice dialogue
completion problem
</figureCaption>
<bodyText confidence="0.997055263157895">
translated conversation and asked to choose an ap-
propriate utterance from several options, which
are also machine-translated, to fill in a blank in
the conversation.
We evaluated four translation methods in the ex-
periment including both machine-translation and
manual-translation. The extrinsic evaluation re-
vealed that one of the MT systems is comparable
to the human translation produced by randomly
presenting the individual sentences to the trans-
lator without any context, but the translation pro-
duced by the best MT system is still far worse than
that produced by a human translator working on
the entire dialogue at once. Furthermore, we ex-
amined the relations between the extrinsic metric
based on the subjects’ scores and various intrin-
sic metrics including automatic scores such as the
BLEU score and manual evaluation. The test ma-
terial is available on request for research purposes.
</bodyText>
<sectionHeader confidence="0.987149" genericHeader="method">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.999396">
2.1 Overview of Experiment
</subsectionHeader>
<bodyText confidence="0.9999975">
We extrinsically evaluated four different transla-
tions of the same material, namely multiple-choice
dialogue completion problems taken from second
language ability tests. The original problems were
in English, and we translated them into Japanese.
Two of the translations were produced by MT sys-
tems, and the other two were produced by a hu-
man translator with and without considering the
contexts of the individual sentences in the dia-
logues. The human subjects solved the translated
problems without knowing whether a machine or
a human produced them. Finally, the translation
quality was evaluated based on the rate of correct
answers given by the human subjects.
</bodyText>
<subsectionHeader confidence="0.993757">
2.2 Participants
</subsectionHeader>
<bodyText confidence="0.999987363636364">
The subjects of the experiment included 320
Japanese junior high school students (12-15 years
old) from two schools (schools A and B). The
participants from school A consisted of 80 first-
year students, 80 second-year students, and 78
third-year students. All the students from school
B (82 students) were first-year students. Thus,
the participants had varying levels of English and
scholastic abilities. We will examine the effect of
these factors on the experimental results later in
the paper.
</bodyText>
<subsectionHeader confidence="0.992964">
2.3 Materials
</subsectionHeader>
<bodyText confidence="0.999991285714286">
All the problems used in the experiment consisted
of a short conversation between two people, where
part of an utterance is hidden. The subject was
presented with four options and asked to complete
the dialogue with the most appropriate one.
We randomly extracted 40 English dialogue
completion problems from mock National Center
Test for University Admissions conducted by one
of the largest preparatory schools in Japan. In the
extracted problems, the number of utterances in
one dialogue ranged from two to four, with each
utterance consisting of one to three sentences, and
an option including one or two sentences. All 40
problems contained 327 sentences.
</bodyText>
<subsectionHeader confidence="0.99786">
2.4 Translation Systems
</subsectionHeader>
<bodyText confidence="0.999226">
The English dialogue completion problems were
translated by four methods: 1
</bodyText>
<listItem confidence="0.637311">
G: Automatic translation by Google Translate2
Y : Automatic translation by Yahoo Translate3
</listItem>
<bodyText confidence="0.997031384615385">
HS: Human translation produced by providing in-
dividual sentences from the problems to a
translator in random order
HO: Human translation produced by a translator
working on the entire dialogue at once
The subscripts of HS and HO stand for the
translations of the sentences in “shuffled order”
and “original order”, respectively. The translations
by HS were created by first preparing a file con-
taining all the sentences from the 40 problems in
a randomized order and then asking a translator
to translate the file sentence-by-sentence, without
assuming any specific context. HS thus provides
</bodyText>
<footnote confidence="0.999977333333333">
1The two MT results were produced on June 11th, 2014.
2https://translate.google.co.jp/?hl=ja
3http://honyaku.yahoo.co.jp/
</footnote>
<page confidence="0.996697">
146
</page>
<bodyText confidence="0.999663416666667">
an estimate of the performance upper-bound of the
current MT systems since most current systems
translate each sentence individually.
We asked three native Japanese speakers who
are fluent in English to first produce the sentence-
by-sentence translations by method HS and then
translate all the dialogue problems in the normal
way (i.e., by HO). We randomly chose one of the
translators and used his translations as the test ma-
terial that the subjects solved. The other human
translations were used as the reference translations
for the automatic evaluation.
</bodyText>
<subsectionHeader confidence="0.961128">
2.5 Procedure
</subsectionHeader>
<bodyText confidence="0.9999754">
Each subject was given 12 different problems that
consisted of an equal number (3) of translated
problems produced by the four translation meth-
ods. Although the sets of problems were different
among the subjects, they were designed such that
the number of subjects who solve each translated
problem was roughly the same. Each subject was
given 12 sheets of paper, each of which showed
a problem and its answer choices, and was given
one minute to complete each problem.
</bodyText>
<subsectionHeader confidence="0.973921">
2.6 Extrinsic Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.999883">
The translation systems were evaluated by the av-
erage of the rate of correct answers made on the
translated problems. Let P = {pz} be the set of
original problems and M(p) be the translation of
problem p produced by method M. The correct
answer rate (CAR) on M(p) is defined as:
</bodyText>
<table confidence="0.35665375">
CARM(p) = # of subjects that correctly answered M(p)
.
# of subjects who solved M(p)
The extrinsic evaluation score of translation
</table>
<equation confidence="0.7731325">
method M is the average of CAR over P:
Avg-CARM =
</equation>
<subsectionHeader confidence="0.790297">
2.7 Intrinsic Evaluations
</subsectionHeader>
<bodyText confidence="0.999053">
Automatic Evaluation Metrics We also evalu-
ated the translation quality using BLEU, BLEU+1
(Lin and Och, 2004), RIBES (Isozaki et al., 2010),
and TER (Snover et al., 2006). We prepared two
sorts of reference translations: RefS and RefO.
RefS consisted of two manual translations of the
40 problems produced by method HS. RefO con-
sisted of three manual translations produced in the
normal way, i.e., by HO.
</bodyText>
<figure confidence="0.437105">
Correct Answer Rate
</figure>
<figureCaption confidence="0.935355">
Figure 2: Boxplots of Correct Answer Rates for
40 Problems
</figureCaption>
<bodyText confidence="0.999789142857143">
Human Evaluation Five native Japanese speak-
ers ranked the translations by the four systems for
each of the 40 problems. They were shown the
translations of a problem by the four methods with
its source problem in English and asked to give a
relative ranking among them, such as “G &lt; Y &lt;
HS = HO.” This method was adapted from the
manual evaluation conducted in the recent WMT
workshops (Callison-Burch et al., 2010). The rel-
ative ranking was broken down into six (= 4C2)
binary relations. For each relation “A &gt; B” found
in the broken-down relations, one point was added
to system A. The final ranking among the systems
for a problem was determined by the total points.
</bodyText>
<sectionHeader confidence="0.999887" genericHeader="evaluation">
3 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.9034365">
3.1 Preliminary Analysis: Robustness against
the Heterogeneity of the Human Subjects
</subsectionHeader>
<bodyText confidence="0.999980384615385">
We divided the participants from school A into
three groups according to grade level, and then ex-
amined the differences in the rate of correct an-
swers for each problem among each group. We
also compared the correct answer rates between
the participants in the 1st grade at schools A and
B. The two-way analysis of variance (ANOVA) re-
vealed that the grades and schools had no signifi-
cant effect on the correct answer rate for 38 out of
the 40 problems (p &gt; 0.05). The results showed
that the participants’ grade levels and scholastic
abilities (including English ability) did not affect
the test results.
</bodyText>
<subsectionHeader confidence="0.995396">
3.2 System-level Evaluation
</subsectionHeader>
<bodyText confidence="0.999947142857143">
We first present the system-level evaluation results
for the four translation methods. Figure 2 shows
the min/max and the quartiles of the correct an-
swer rates (CARs) for the 40 problems translated
by each system. The averages of the correct an-
swer rates are 0.524, 0.696, 0.693, and 0.875 for
each translation system G, Y , HS, and HO, re-
</bodyText>
<figure confidence="0.6894735">
0.0 0.2 0.4 0.6 0.8 1.0
Ho Hs Y G
Systems
1 ∑ CARM(p).
P �
PEP
</figure>
<page confidence="0.759595">
147
</page>
<table confidence="0.9979046">
Reference Metrics G Y Hs Ho
RefO BLEU 22.04 20.33 40.30 47.43
BLEU+1 22.08 20.37 40.33 47.46
RIBES 67.80 69.43 78.16 82.42
TER 41.72 43.66 27.47 24.14
RefS BLEU 27.53 23.63 41.24 30.69
BLEU+1 27.56 23.67 41.27 30.73
RIBES 73.61 73.63 80.18 70.59
TER 36.51 39.52 27.60 31.51
Avg-CAR 0.524 0.696 0.693 0.875
</table>
<tableCaption confidence="0.948831">
Table 1: Automatic Evaluation Scores and Aver-
age Correct Answer Rate
</tableCaption>
<bodyText confidence="0.99971762962963">
spectively. We conducted a pairwise t-test on each
adjacent set (G-Y , Y -HS, and HS-Hp) for the
CARs and found a statistically significant differ-
ence (p &lt; 0.05) between G and Y and HS and
Hp but not between Y and HS (p = 0.954).
Table 1 lists the five automatic evaluation scores
for each translation method measured against the
two reference translation sets. The averages of the
CARs over the 40 problems are also listed in the
bottom row of the table. There are several notice-
able facts. First, despite the significantly better av-
erage CAR for Y over G, BLEU, BLEU+1, and
TER prefer G to Y . Second, while the average
CARs for Y and HS are almost equal, there are
large differences between their automatic evalua-
tion scores across all metrics. Third, a comparison
of the corresponding automatic evaluation scores
using RefS and Refp reveals that G, Y , and HS
are more similar to the manual translations that
were produced without referring to the contexts of
the individual sentences than those produced tak-
ing the contexts into consideration. This is not
surprising. However, the large difference in the
correct answer rates for HS and Hp suggests that
ignorance of the context in the current MT sys-
tems severely degrades the comprehensibility of
the translations of texts like daily conversations.
</bodyText>
<subsectionHeader confidence="0.999321">
3.3 Agreement between Intrinsic and
Extrinsic Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9999617">
We examined how often an intrinsic metric cor-
rectly predicts the difference of the subjects’ test
performance on a problem. Specifically, for two
translation methods A and B, we say an intrinsic
metric M agrees with the CAR by the subjects on
problem pi iff metric M scores A’s translation of
pi (= A(pi)) better than B’s translation (= B(pi))
and the CAR is higher on A(pi) than on B(pi).
The rate of agreements is the fraction of the prob-
lems on which M and CAR agree. The agreement
</bodyText>
<figure confidence="0.9913441">
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
BLEU BLEU+1 RIBES TER Human
</figure>
<figureCaption confidence="0.8985506">
Figure 3: Agreement Rates between Intrinsic
Evaluation Metrics and Correct Answer Rate
BLEU BLEU+1 RIBES TER
Figure 4: Agreement Rates between Automatic
Evaluation Metrics and Human Evaluation
</figureCaption>
<bodyText confidence="0.989681583333333">
between two intrinsic metrics is defined similarly.
Figure 3 shows the rates of agreements between
the automatic metrics and CARs and between the
human evaluation and CARs. As Figure 3 shows,
all the agreement rates between the automatic met-
rics and CARs were less than 0.65. When consid-
ering a random baseline of 0.5, we may conclude
that the automatic metrics are not very good pre-
dictors of the CARs. This is unfortunate since the
CARs directly indicate the comprehensibility of
the translated dialogues. The disagreements can-
not be attributed only to the unreliability of au-
tomatic metrics on short translations. Figure 4
shows the rate of agreements between the auto-
matic metrics and the human evaluation. As Fig-
ure 4 shows, BLEU, BLEU+1, and TER agree
with human evaluation on nearly 90% of the prob-
lems when comparing Y and HS.
The human evaluation agrees with the CAR
slightly better than the automatic metrics. How-
ever, the agreement rates are still less than 0.7 for
all pairs of compared systems. These findings sug-
gest that there is an inherent discrepancy between
the assessment of the overall translation quality of
</bodyText>
<figure confidence="0.998634933333333">
1
0.9
G-Y Y-Hs Hs-Ho
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
G-Y Y-Hs Hs-Ho
</figure>
<page confidence="0.984364">
148
</page>
<bodyText confidence="0.99988275">
a problem and the CAR. It is presumably because
the CAR can be critically damaged by a subtle
translation mistake that spoils a coherent under-
standing of a dialogue.
</bodyText>
<sectionHeader confidence="0.987641" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999995736842105">
We have presented the results of an experiment, in
which machine- and human-translated second lan-
guage proficiency test (SLPT) problems were used
for extrinsic evaluation of the translation quality.
Comparison of four translation methods revealed,
most notably, the crucial importance of consider-
ing contexts of individual sentences in translating
dialogues. The analysis on the experimental re-
sults suggests that the extrinsic evaluation based
on SLPT problems captures a different dimension
of translation quality than the manual/automatic
intrinsic metrics. The robustness against the het-
erogeneity of human subjects and the abundance
of existing SLPT problems enable easy adaption
of the proposed evaluation scheme in addition to
the traditional intrinsic evaluations. Our future
work includes experiments with other types of
SLPT problems that focus on different aspects of
translation quality and language understanding.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999962714285714">
The authors are grateful to all the participants
in the experiments for their time and patience
and also grateful to Yoyogi Seminar for their
allowance to use their problems in the experi-
ments. This study is conducted as a part of the
Todai Robot Project (http://21robot.org/
?lang=english).
</bodyText>
<sectionHeader confidence="0.998584" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998792144927536">
Jan Berka, Martin ˇCern`y, and Ondˇrej Bojar. 2011.
Quiz-based evaluation of machine translation. The
Prague Bulletin ofMathematical Linguistics, 95:77–
86.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluation the role of bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2006), pages 249–256.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint 5th Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 17–53.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2010), pages 944–952.
Douglas Jones, Wade Shen, Neil Granoien, Martha
Herzog, and Clifford Weinstein. 2005. Measuring
translation quality by testing english speakers with
a new defense language proficiency test for arabic.
In Proceedings ofthe 2005 International Conference
on Intelligence Analysis.
Douglas Jones, Martha Herzog, Hussny Ibrahim,
Arvind Jairam, Wade Shen, Edward Gibson, and
Michael Emonts. 2007. Ilr-based mt compre-
hension test with multi-level questions. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL 2007);
Companion Volume, Short Papers, pages 77–80.
Jamal Laoudi, Calandra R. Tate, and Clare R.
Voss. 2006. Task-based mt evaluation: From
who/when/where extraction to event understanding.
In Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC-
2006), pages 2048–2053.
Chin-Yew Lin and Franz Josef Och. 2004. Orange:
A method for evaluating automatic evaluation met-
rics for machine translation. In Proceedings of
the 20th International Conference on Computational
Linguistics (COLING-2004), pages 501–507.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2002), pages 311–318.
Anne H. Schneider, Ielka van der Sluis, and Saturnino
Luz. 2010. Comparing intrinsic and extrinsic evalu-
ation of mt output in a dialogue system. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Translation (IWSLT-2010), pages 329–336.
Matthew Snover, Bonnie Dorr, Richard Schwaltz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAssociation for Machine Transla-
tion in the Americas (AMTA-2006), pages 223–231.
Clare R. Voss and Calandra R. Tate. 2006. Task-
based evaluation of machine translation (mt) en-
gines: Measuring how well people extract who,
when, where-type elements in mt output. In In Pro-
ceedings of 11th Annual Conference ofthe European
Association for Machine Translation (EAMT-2006),
pages 203–212.
</reference>
<page confidence="0.998958">
149
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.382125">
<title confidence="0.988901">Evaluating Machine Translation Systems with Second Proficiency Tests</title>
<affiliation confidence="0.5724525">H. of Electrical Engineering and Computer Science, Nagoya and Society Research Division, National Institute of</affiliation>
<email confidence="0.66942">ntodo,</email>
<abstract confidence="0.99941375">A lightweight, human-in-the-loop evaluation scheme for machine translation (MT) systems is proposed. It extrinsically evaluates MT systems using human subjects’ scores on second language ability test problems that are machine-translated to the subjects’ native language. A largescale experiment involving 320 subjects revealed that the context-unawareness of the current MT systems severely damages human performance when solving the test problems, while one of the evaluated MT systems performed as good as a human translation produced in a context-unaware condition. An analysis of the experimental results showed that the extrinsic evaluation captured a different dimension of translation quality than that captured by manual and automatic intrinsic evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jan Berka</author>
<author>Martin ˇCern`y</author>
<author>Ondˇrej Bojar</author>
</authors>
<title>Quiz-based evaluation of machine translation. The Prague Bulletin ofMathematical Linguistics,</title>
<date>2011</date>
<pages>95--77</pages>
<marker>Berka, ˇCern`y, Bojar, 2011</marker>
<rawString>Jan Berka, Martin ˇCern`y, and Ondˇrej Bojar. 2011. Quiz-based evaluation of machine translation. The Prague Bulletin ofMathematical Linguistics, 95:77– 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluation the role of bleu in machine translation research.</title>
<date>2006</date>
<booktitle>In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006),</booktitle>
<pages>249--256</pages>
<contexts>
<context position="1469" citStr="Callison-Burch et al. (2006)" startWordPosition="200" endWordPosition="203">ystems performed as good as a human translation produced in a context-unaware condition. An analysis of the experimental results showed that the extrinsic evaluation captured a different dimension of translation quality than that captured by manual and automatic intrinsic evaluation. 1 Introduction Automatic evaluation metrics, such as the BLEU score (Papineni et al., 2002), were crucial ingredients for the advances of machine translation technology in the last decade. Meanwhile, the shortcomings of BLEU and similar n-gram proximitybased metrics have been pointed out by many authors including Callison-Burch et al. (2006). The main criticisms include: 1) unreliability in evaluating short translations, 2) non-interpretability of the scores beyond numerical comparison, and 3) bias towards statistical MT systems. Manual evaluation of translation quality is more reliable in many regards, but it is costly. Furthermore, it is not necessarily easy to analyze the characteristics of MT systems based solely on the evaluation results such as a 5-point scale evaluation of adequacy/fluency and a ranking of the outputs of different systems. A remedy for some of the above-raised issues is task-based evaluation of MT systems </context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluation the role of bleu in machine translation research. In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006), pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar F Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>17--53</pages>
<contexts>
<context position="10937" citStr="Callison-Burch et al., 2010" startWordPosition="1724" endWordPosition="1727">ations of the 40 problems produced by method HS. RefO consisted of three manual translations produced in the normal way, i.e., by HO. Correct Answer Rate Figure 2: Boxplots of Correct Answer Rates for 40 Problems Human Evaluation Five native Japanese speakers ranked the translations by the four systems for each of the 40 problems. They were shown the translations of a problem by the four methods with its source problem in English and asked to give a relative ranking among them, such as “G &lt; Y &lt; HS = HO.” This method was adapted from the manual evaluation conducted in the recent WMT workshops (Callison-Burch et al., 2010). The relative ranking was broken down into six (= 4C2) binary relations. For each relation “A &gt; B” found in the broken-down relations, one point was added to system A. The final ranking among the systems for a problem was determined by the total points. 3 Results and Discussion 3.1 Preliminary Analysis: Robustness against the Heterogeneity of the Human Subjects We divided the participants from school A into three groups according to grade level, and then examined the differences in the rate of correct answers for each problem among each group. We also compared the correct answer rates between</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar F. Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Automatic evaluation of translation quality for distant language pairs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-2010),</booktitle>
<pages>944--952</pages>
<contexts>
<context position="10177" citStr="Isozaki et al., 2010" startWordPosition="1592" endWordPosition="1595">translation systems were evaluated by the average of the rate of correct answers made on the translated problems. Let P = {pz} be the set of original problems and M(p) be the translation of problem p produced by method M. The correct answer rate (CAR) on M(p) is defined as: CARM(p) = # of subjects that correctly answered M(p) . # of subjects who solved M(p) The extrinsic evaluation score of translation method M is the average of CAR over P: Avg-CARM = 2.7 Intrinsic Evaluations Automatic Evaluation Metrics We also evaluated the translation quality using BLEU, BLEU+1 (Lin and Och, 2004), RIBES (Isozaki et al., 2010), and TER (Snover et al., 2006). We prepared two sorts of reference translations: RefS and RefO. RefS consisted of two manual translations of the 40 problems produced by method HS. RefO consisted of three manual translations produced in the normal way, i.e., by HO. Correct Answer Rate Figure 2: Boxplots of Correct Answer Rates for 40 Problems Human Evaluation Five native Japanese speakers ranked the translations by the four systems for each of the 40 problems. They were shown the translations of a problem by the four methods with its source problem in English and asked to give a relative ranki</context>
</contexts>
<marker>Isozaki, Hirao, Duh, Sudoh, Tsukada, 2010</marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Automatic evaluation of translation quality for distant language pairs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-2010), pages 944–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Jones</author>
<author>Wade Shen</author>
<author>Neil Granoien</author>
<author>Martha Herzog</author>
<author>Clifford Weinstein</author>
</authors>
<title>Measuring translation quality by testing english speakers with a new defense language proficiency test for arabic.</title>
<date>2005</date>
<booktitle>In Proceedings ofthe 2005 International Conference on Intelligence Analysis.</booktitle>
<contexts>
<context position="2088" citStr="Jones et al., 2005" startWordPosition="298" endWordPosition="301"> The main criticisms include: 1) unreliability in evaluating short translations, 2) non-interpretability of the scores beyond numerical comparison, and 3) bias towards statistical MT systems. Manual evaluation of translation quality is more reliable in many regards, but it is costly. Furthermore, it is not necessarily easy to analyze the characteristics of MT systems based solely on the evaluation results such as a 5-point scale evaluation of adequacy/fluency and a ranking of the outputs of different systems. A remedy for some of the above-raised issues is task-based evaluation of MT systems (Jones et al., 2005; Voss and Tate, 2006; Laoudi et al., 2006; Jones et al., 2007; Schneider et al., 2010; Berka et al., 2011), which measures the human performance in a task such as information extraction from a machine-translated text. The main burden of conducting task-based evaluation is also its cost; the development of a sizable amount of test materials and the gathering of appropriate human subjects is time consuming and expensive. This paper proposes to utilize second-language proficiency tests (SLPTs), such as TOEIC, as the source of the specimens for extrinsic evaluation of MT systems. For evaluating, </context>
</contexts>
<marker>Jones, Shen, Granoien, Herzog, Weinstein, 2005</marker>
<rawString>Douglas Jones, Wade Shen, Neil Granoien, Martha Herzog, and Clifford Weinstein. 2005. Measuring translation quality by testing english speakers with a new defense language proficiency test for arabic. In Proceedings ofthe 2005 International Conference on Intelligence Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Jones</author>
<author>Martha Herzog</author>
<author>Hussny Ibrahim</author>
<author>Arvind Jairam</author>
<author>Wade Shen</author>
<author>Edward Gibson</author>
<author>Michael Emonts</author>
</authors>
<title>Ilr-based mt comprehension test with multi-level questions.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<pages>77--80</pages>
<contexts>
<context position="2150" citStr="Jones et al., 2007" startWordPosition="310" endWordPosition="313">hort translations, 2) non-interpretability of the scores beyond numerical comparison, and 3) bias towards statistical MT systems. Manual evaluation of translation quality is more reliable in many regards, but it is costly. Furthermore, it is not necessarily easy to analyze the characteristics of MT systems based solely on the evaluation results such as a 5-point scale evaluation of adequacy/fluency and a ranking of the outputs of different systems. A remedy for some of the above-raised issues is task-based evaluation of MT systems (Jones et al., 2005; Voss and Tate, 2006; Laoudi et al., 2006; Jones et al., 2007; Schneider et al., 2010; Berka et al., 2011), which measures the human performance in a task such as information extraction from a machine-translated text. The main burden of conducting task-based evaluation is also its cost; the development of a sizable amount of test materials and the gathering of appropriate human subjects is time consuming and expensive. This paper proposes to utilize second-language proficiency tests (SLPTs), such as TOEIC, as the source of the specimens for extrinsic evaluation of MT systems. For evaluating, e.g., English-toJapanese MT systems, a set of English test pro</context>
</contexts>
<marker>Jones, Herzog, Ibrahim, Jairam, Shen, Gibson, Emonts, 2007</marker>
<rawString>Douglas Jones, Martha Herzog, Hussny Ibrahim, Arvind Jairam, Wade Shen, Edward Gibson, and Michael Emonts. 2007. Ilr-based mt comprehension test with multi-level questions. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2007); Companion Volume, Short Papers, pages 77–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamal Laoudi</author>
<author>Calandra R Tate</author>
<author>Clare R Voss</author>
</authors>
<title>Task-based mt evaluation: From who/when/where extraction to event understanding.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC2006),</booktitle>
<pages>2048--2053</pages>
<contexts>
<context position="2130" citStr="Laoudi et al., 2006" startWordPosition="306" endWordPosition="309">ility in evaluating short translations, 2) non-interpretability of the scores beyond numerical comparison, and 3) bias towards statistical MT systems. Manual evaluation of translation quality is more reliable in many regards, but it is costly. Furthermore, it is not necessarily easy to analyze the characteristics of MT systems based solely on the evaluation results such as a 5-point scale evaluation of adequacy/fluency and a ranking of the outputs of different systems. A remedy for some of the above-raised issues is task-based evaluation of MT systems (Jones et al., 2005; Voss and Tate, 2006; Laoudi et al., 2006; Jones et al., 2007; Schneider et al., 2010; Berka et al., 2011), which measures the human performance in a task such as information extraction from a machine-translated text. The main burden of conducting task-based evaluation is also its cost; the development of a sizable amount of test materials and the gathering of appropriate human subjects is time consuming and expensive. This paper proposes to utilize second-language proficiency tests (SLPTs), such as TOEIC, as the source of the specimens for extrinsic evaluation of MT systems. For evaluating, e.g., English-toJapanese MT systems, a set</context>
</contexts>
<marker>Laoudi, Tate, Voss, 2006</marker>
<rawString>Jamal Laoudi, Calandra R. Tate, and Clare R. Voss. 2006. Task-based mt evaluation: From who/when/where extraction to event understanding. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC2006), pages 2048–2053.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Orange: A method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING-2004),</booktitle>
<pages>501--507</pages>
<contexts>
<context position="10147" citStr="Lin and Och, 2004" startWordPosition="1587" endWordPosition="1590">nsic Evaluation Metric The translation systems were evaluated by the average of the rate of correct answers made on the translated problems. Let P = {pz} be the set of original problems and M(p) be the translation of problem p produced by method M. The correct answer rate (CAR) on M(p) is defined as: CARM(p) = # of subjects that correctly answered M(p) . # of subjects who solved M(p) The extrinsic evaluation score of translation method M is the average of CAR over P: Avg-CARM = 2.7 Intrinsic Evaluations Automatic Evaluation Metrics We also evaluated the translation quality using BLEU, BLEU+1 (Lin and Och, 2004), RIBES (Isozaki et al., 2010), and TER (Snover et al., 2006). We prepared two sorts of reference translations: RefS and RefO. RefS consisted of two manual translations of the 40 problems produced by method HS. RefO consisted of three manual translations produced in the normal way, i.e., by HO. Correct Answer Rate Figure 2: Boxplots of Correct Answer Rates for 40 Problems Human Evaluation Five native Japanese speakers ranked the translations by the four systems for each of the 40 problems. They were shown the translations of a problem by the four methods with its source problem in English and </context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Orange: A method for evaluating automatic evaluation metrics for machine translation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING-2004), pages 501–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="1217" citStr="Papineni et al., 2002" startWordPosition="159" endWordPosition="162">ed to the subjects’ native language. A largescale experiment involving 320 subjects revealed that the context-unawareness of the current MT systems severely damages human performance when solving the test problems, while one of the evaluated MT systems performed as good as a human translation produced in a context-unaware condition. An analysis of the experimental results showed that the extrinsic evaluation captured a different dimension of translation quality than that captured by manual and automatic intrinsic evaluation. 1 Introduction Automatic evaluation metrics, such as the BLEU score (Papineni et al., 2002), were crucial ingredients for the advances of machine translation technology in the last decade. Meanwhile, the shortcomings of BLEU and similar n-gram proximitybased metrics have been pointed out by many authors including Callison-Burch et al. (2006). The main criticisms include: 1) unreliability in evaluating short translations, 2) non-interpretability of the scores beyond numerical comparison, and 3) bias towards statistical MT systems. Manual evaluation of translation quality is more reliable in many regards, but it is costly. Furthermore, it is not necessarily easy to analyze the charact</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne H Schneider</author>
<author>Ielka van der Sluis</author>
<author>Saturnino Luz</author>
</authors>
<title>Comparing intrinsic and extrinsic evaluation of mt output in a dialogue system.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT-2010),</booktitle>
<pages>329--336</pages>
<marker>Schneider, van der Sluis, Luz, 2010</marker>
<rawString>Anne H. Schneider, Ielka van der Sluis, and Saturnino Luz. 2010. Comparing intrinsic and extrinsic evaluation of mt output in a dialogue system. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT-2010), pages 329–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwaltz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Machine Translation in the Americas (AMTA-2006),</booktitle>
<pages>223--231</pages>
<contexts>
<context position="10208" citStr="Snover et al., 2006" startWordPosition="1598" endWordPosition="1601">ed by the average of the rate of correct answers made on the translated problems. Let P = {pz} be the set of original problems and M(p) be the translation of problem p produced by method M. The correct answer rate (CAR) on M(p) is defined as: CARM(p) = # of subjects that correctly answered M(p) . # of subjects who solved M(p) The extrinsic evaluation score of translation method M is the average of CAR over P: Avg-CARM = 2.7 Intrinsic Evaluations Automatic Evaluation Metrics We also evaluated the translation quality using BLEU, BLEU+1 (Lin and Och, 2004), RIBES (Isozaki et al., 2010), and TER (Snover et al., 2006). We prepared two sorts of reference translations: RefS and RefO. RefS consisted of two manual translations of the 40 problems produced by method HS. RefO consisted of three manual translations produced in the normal way, i.e., by HO. Correct Answer Rate Figure 2: Boxplots of Correct Answer Rates for 40 Problems Human Evaluation Five native Japanese speakers ranked the translations by the four systems for each of the 40 problems. They were shown the translations of a problem by the four methods with its source problem in English and asked to give a relative ranking among them, such as “G &lt; Y &lt;</context>
</contexts>
<marker>Snover, Dorr, Schwaltz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwaltz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation for Machine Translation in the Americas (AMTA-2006), pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clare R Voss</author>
<author>Calandra R Tate</author>
</authors>
<title>Taskbased evaluation of machine translation (mt) engines: Measuring how well people extract who, when, where-type elements in mt output. In</title>
<date>2006</date>
<booktitle>In Proceedings of 11th Annual Conference ofthe European Association for Machine Translation (EAMT-2006),</booktitle>
<pages>203--212</pages>
<contexts>
<context position="2109" citStr="Voss and Tate, 2006" startWordPosition="302" endWordPosition="305"> include: 1) unreliability in evaluating short translations, 2) non-interpretability of the scores beyond numerical comparison, and 3) bias towards statistical MT systems. Manual evaluation of translation quality is more reliable in many regards, but it is costly. Furthermore, it is not necessarily easy to analyze the characteristics of MT systems based solely on the evaluation results such as a 5-point scale evaluation of adequacy/fluency and a ranking of the outputs of different systems. A remedy for some of the above-raised issues is task-based evaluation of MT systems (Jones et al., 2005; Voss and Tate, 2006; Laoudi et al., 2006; Jones et al., 2007; Schneider et al., 2010; Berka et al., 2011), which measures the human performance in a task such as information extraction from a machine-translated text. The main burden of conducting task-based evaluation is also its cost; the development of a sizable amount of test materials and the gathering of appropriate human subjects is time consuming and expensive. This paper proposes to utilize second-language proficiency tests (SLPTs), such as TOEIC, as the source of the specimens for extrinsic evaluation of MT systems. For evaluating, e.g., English-toJapan</context>
</contexts>
<marker>Voss, Tate, 2006</marker>
<rawString>Clare R. Voss and Calandra R. Tate. 2006. Taskbased evaluation of machine translation (mt) engines: Measuring how well people extract who, when, where-type elements in mt output. In In Proceedings of 11th Annual Conference ofthe European Association for Machine Translation (EAMT-2006), pages 203–212.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>