<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.9989515">
Reducing Parsing Complexity by Intra—Sentence Segmentation
based on Maximum Entropy Model
</title>
<author confidence="0.999716">
Sung Dong Kim, Byoung-Tak Zhang, Yung Taek Kim
</author>
<affiliation confidence="0.9709025">
School of Computer Science and Engineering,
Seoul National University, Korea
</affiliation>
<email confidence="0.999008">
{sdkim,btzhang}@scai.snu.ac.kr, ytkim@cse.snu.ac.kr
</email>
<sectionHeader confidence="0.997395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999015">
Long sentence analysis has been a critical
problem because of high complexity. This pa-
per addresses the reduction of parsing com-
plexity by intra-sentence segmentation, and
presents maximum entropy model for deter-
mining segmentation positions. The model
features lexical contexts of segmentation posi-
tions, giving a probability to each potential
position. Segmentation coverage and accu-
racy of the proposed method are 96% and
88% respectively. The parsing efficiency is im-
proved by 77% in time and 71% in space.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999849845070423">
Long sentence analysis has been a critical
problem in machine translation because of
high complexity. In EBMT (example-based
machine translation), the longer a sentence
is, the less possible it is that the sentence
has an exact match in the translation archive,
and the less flexible an EBMT system will be
(Cranias et al., 1994). In idiom-based ma-
chine translation (Lee, 1993), long sentence
parsing is difficult because more resources are
spent during idiom recognition phase as sen-
tence length increases. A parser is often un-
able to analyze long sentences owing to their
complexity, though they have no grammatical
errors (Nasukawa, 1995).
In English-Korean machine translation,
idiom-based approach is adopted to overcome
the structural differences between two lan-
guages and to get more accurate translation.
The parser is a chart parser with a capabil-
ity of idiom recognition and translation, which
is adapted to English-Korean machine trans-
lation. Idioms are recognized prior to syn-
tactic analysis and the part of a sentence for
an idiom takes an edge in a chart (Winograd,
1983). When parsing long sentences, an am-
biguity of an idiom&apos;s range may cause more
edges than the number of words included in
the idiom (Yoon, 1994), which increases pars-
ing complexity much. A parser of practical
machine translation system should be able to
analyze long sentences in a reasonable time.
Most context-free parsing algorithms have
0(n3) passing complexities in terms of time
and space, where n is the length of a sen-
tence (Tomita, 1986). Our work is moti-
vated by the fact that parsing becomes more
efficient, if n becomes shorter. This paper
deals with the problem of parsing complex-
ity by way of reducing the length of sentence
to be analyzed. This reduction is achieved
by intra-sentence segmentation, which is
distinguished from inter—sentence segmen-
tation that is used for text categorization
(Beeferman et al., 1997) or sentence boundary
identification (Palmer and Hearst, 1997) (Rey-
nar and Ratnaparkhi, 1997). Intra-sentence
segmentation plays a role as a preliminary
step to a chart-based, context-free parser in
English-Korean machine translation.
There have been several methods for
reducing parsing complexities by intra-
sentence segmentation. In (Lyon and Prank,
1995)(Lyon and Dickerson, 1997), they took
advantage of the fact that the declarative
sentences almost always consist of three seg-
ments: [pre-subject : subject : predicate].
The complexity could be reduced by decom-
posing a sentence into three sections. Pattern
rules (Li et al., 1990) and sentence patterns
(Kim and Kim, 1995) were used to segment
long English sentences. They showed low seg-
mentation coverage, which means that many
of long sentences are not segmented by the
pattern rules or sentence patterns. And they
require much human efforts to construct pat-
tern rules or collect sentence patterns. These
factors may prevent them being applicable to
practical machine translation systems.
This paper presents a trainable model for
identifying potential segmentation positions
</bodyText>
<page confidence="0.99735">
164
</page>
<bodyText confidence="0.999928884615385">
in a sentence and determining appropriate
segmentation positions. Given a corpus anno-
tated with segmentation positions, our model
automatically learns the contextual evidences
about segmentation positions, which relieves
human of burden to construct pattern rules or
sentence patterns. These evidences are com-
bined under the maximum entropy framework
(Jaynes, 1957) to estimate the probability for
each position. By intra-sentence segmenta-
tion based on the proposed model, we achieve
more improved parsing efficiency by 77% in
time and 71% in space.
In Section 2 we introduce the maximum en-
tropy model. Section 3 describes features in-
corporated into the model and the process of
identifying potential segmentation positions.
The determination schemes of segmentation
positions are described in Section 4. Segmen-
tation performance of the model is presented
with the degree of contribution to efficient
parsing by the segmentation in Section 5. We
also compare our approach with other intra-
sentence segmentation approaches. Section 6
draws conclusions and presents some further
works.
</bodyText>
<sectionHeader confidence="0.976138" genericHeader="method">
2 Maximum Entropy Modeling
</sectionHeader>
<bodyText confidence="0.996385903225806">
Sentence patterns or pattern ruels specify the
sub-structures of the sentences. That is, seg-
mentation positions are determined in view of
the global sentence structure. If there is no
matched rules or patterns with a given sen-
tence, the sentence could not be segmented.
We assume that whether a word is a segmenta-
tion position depends on its surrounding con-
text. We try to find factors that affect the de-
termination of segmentation positions. Maxi-
mum entropy is a technique for automatically
acquiring knowledge from incomplete infor-
mation, without making any unsubstantiated
assumptions. It masters subtle effects so that
we may accurately model subtle dependencies.
It does not make any unwarranted assump-
tions, which means that maximum entropy
learns exactly what the data says. Therefore
it can perform well on unseen data.
The idea is to construct a model that as-
signs a probability to each potential segmen-
tation position in a sentence. We build a prob-
ability distribution p(y1x), where y E {0, 1}
is a random variable specifying the potential
segmentation position in a context x. A fea-
ture of a context is a binary-valued indicator
function f expressing the information about a
specific context.
Given a training sample of size N,
(xi, yi), - • - , (xN, YN), an empirical proba-
bility distribution can be defined as
</bodyText>
<equation confidence="0.99485">
#(x,Y)
13(s, Y) = N ,
</equation>
<bodyText confidence="0.9997455">
where #(x, y) is the number of occurrences of
(x,y). The expected value of feature ft with
respect to the empirical distribution 23(x, y) is
expressed as
</bodyText>
<equation confidence="0.9124885">
j3(f)
x,y
</equation>
<bodyText confidence="0.993207">
and the expected value of fi with respect to
the probability distribution p(y1x) is
</bodyText>
<equation confidence="0.989828">
p(f2) = E13(X)P(Y1X)fi(X1 0,
x,y
</equation>
<bodyText confidence="0.993188176470588">
where 13(x) is the empirical distribution of x
in the corpus. We want to build probability
distribution p(y1x) that is required to accord
to the feature fi useful in selecting segmenta-
tion positions: p(ft) = 13(f2) for all fi, E .F,
where ..T. is the set of candidate features. This
makes the probability distribution be built on
only training data.
Given a feature set .F, let C be the subset
of all distributions P that satisfies the require-
ment p(f) =
C a fp E P I p(fi) = Afi), for all fi EF}.
We choose a probability distribution consis-
tent with all the facts, but otherwise as uni-
form as possible. The uniformity of the prob-
ability distribution p(y1x) is measured by the
conditional entropy:
</bodyText>
<equation confidence="0.986557">
H(p) = -p(x, y) log p(y1x)
x,y
a: - E 23(x)p(ylx) log p(ylx).
x,y
</equation>
<bodyText confidence="0.9996015">
Thus, the probability distribution with maxi-
mum entropy is the most uniform distribution.
In building a model, we consider the linear
exponential family Q given as
</bodyText>
<equation confidence="0.760648875">
1
Q(f ) = IP(Yir) = ZA(x) exP(E Aifi(x&apos;&amp;quot;&apos;
165
log flp(ylx)73(&apos;&apos;Y) = E73(x,y)logp(ylx).
x,y x,y
That is, the model we want to build is
pi, = arg max H(p) = arg max LI-5(q).
PEC YEQ
</equation>
<bodyText confidence="0.999227916666667">
where Ai are real-valued parameters and
Z,, (x) is a normalizing constant:
An intersection of the class 2 of exponential
models with the class of desired distribution
(1) is nonempty, and the intersection contains
the maximum entropy distribution and fur-
thermore it is unique (Ratnaparkhi, 1994).
Finding p,„ E C that maximizes H(p) is a
problem in constrained optimization, which
cannot be explicitly written in general. There-
fore, we take advantage of the fact that the
models in 2 that satisfy p(f2) = 73(12) can
be explained under the maximum likelihood
framework (Ratnaparkhi, 1994). Maximum
likelihood principle also gives the unique dis-
tribution AK, the intersection of the class
with C.
We assume each occurrence of (x, y) is
sampled independently. Thus, log-likelihood
L 33(p) of the empirical distribution 73 as pre-
dicted by a model p can be defined as
The parameters Ai of exponential model (2)
are obtained by the Generalized Iterative Scal-
ing algorithm (Darroch and Ratcliff, 1972).
</bodyText>
<sectionHeader confidence="0.772132" genericHeader="method">
3 Construction of Features
</sectionHeader>
<bodyText confidence="0.999991166666667">
This section describes the features. From a
corpus, contextual evidences of segmentation
positions are collected and combined, result-
ing in features. The features are used in iden-
tifying potential segmentation positions and
included in the model.
</bodyText>
<subsectionHeader confidence="0.998829">
3.1 Segmentable Positions and Safe
Segmentation
</subsectionHeader>
<bodyText confidence="0.999825142857143">
A sentence is constructed by the combina-
tion of words, phrases, and clauses under the
well-defined grammar A sentence can be seg-
mented into shorter segments that correspond
to the constituents of the sentence. That is,
segments correspond to the nonterminal sym-
bols of the context-free grammarl The posi-
</bodyText>
<footnote confidence="0.923613">
1Nonterminal symbols include the ones for phrases,
such as NP (noun phrase) and VP (verb phrase),
</footnote>
<bodyText confidence="0.998191772727273">
tion of a word is called segmentable posi-
tion that can be a starting position of a spe-
cific segment.
Though the analysis complexity can be re-
duced by segmenting a sentence, there is
a mis-segmentation risk that causes pars-
ing failures. A segmentation can be called
safe segmentation that results in a coherent
blocks of words. In English-Korean transla-
tion, safe segmentation is defined as the one
which generates safe segments. A segment is
safe, when there is a syntactic category sym-
bol NP dominating the segment and the seg-
ment can be combined with adjacent segments
under a given grammar In Figure 1, (a) is an
unsafe segmentation because the second seg-
ment cannot be analyzed into one syntactic
category, resulting in parsing failure. By the
safe segmentation (b), the first segment cor-
responds to a noun phrase and the second to
a verb phrase, so that we can get a correct
analysis result.
</bodyText>
<figureCaption confidence="0.9956565">
Figure 1: Examples of unsafe/safe segmenta-
tion in English-Korean translation.
</figureCaption>
<subsectionHeader confidence="0.999691">
3.2 Lexical Contextual Constraints
</subsectionHeader>
<bodyText confidence="0.999965375">
A lexical context of a word includes seven-
word window: three to the left of a word and
three to the right of a word and a word itself.
It also includes the part-of-speeches of these
words, subcategorization information for two
words to the left, and position value. The
position value posi_v of the ith word wi is cal-
culated as
</bodyText>
<equation confidence="0.839452">
posi_v = r_
</equation>
<bodyText confidence="0.8959118">
where n is the number of words and R2 repre-
sents the number of regions in the sentence.
Region is the sequentially ordered block of
and the ones for clauses like RLCL (relative clause),
SUBCL (subordinate clause).
21t is a heuristically set value, and we set R as 4.
The students
who study hard will pass the exam
The students who study hard
will pass the exam
</bodyText>
<page confidence="0.946111">
166
</page>
<bodyText confidence="0.999105666666667">
words in a sentence, and posi_v represents the
region in which a word lies. It is included to
reflect the influence of the position of a word
on being a segmentation position. Thus, the
lexical context of a word is represented by 17
attributes as shown in Figure 2.
</bodyText>
<table confidence="0.661039166666667">
s_position?
wordi
wi-37 • - • 7 Wi+3
Pi-37 • • • ,Pi-I-3
a_cati_2, s_cati-1
posi_v
</table>
<figureCaption confidence="0.999336">
Figure 2: The structure of lexical context.
</figureCaption>
<bodyText confidence="0.985091157894737">
An example of a training data and a re-
sulting lexical context is shown in Figure 3.
A symbol &apos;It&apos; represents a segmentation posi-
tion marked by human annotators. Therefore,
the lexical context of word when includes the
value 1 for attribute s_position? and follow-
ings: three words to the left of when (became,
terribly, and worried) and part-of-speeches
of each word (VERB ADV ADJ), three words
to the right (they, saw, and what) and part-
of-speeches (PRON VERB PRON), subcat-
egorization information for two words to the
left (0 1), and position value (2).
Of course his parents became terribly worried
#when they saw what was happening
to Atzel.
( 1 when became terribly worried they saw
what VERB ADV ADJ PRON VERB
PRON 0 1 2)
</bodyText>
<figureCaption confidence="0.9797495">
Figure 3: An example of a training data and
a lexical context.
</figureCaption>
<bodyText confidence="0.9996272">
To get reliable statistics, much training
data is required. To alleviate this prob-
lem, we generate lexical contextual con-
straints by combining lexical contexts and
collect statistics for them. To generate lex-
ical contextual constraints and to identify
segmentable positions, we define two oper-
ations join (s) and consistency (.7--..). Let
(al, ... , an) and (b1, ... , bn) be lexical con-
texts and (C1,... , Cn) be lexical contextual
</bodyText>
<equation confidence="0.83281975">
1=Ci constraint. The operation join is defined as
(al, ... , an) e (bi, 7-- 7 bn) = (C17 • • • , Cn),
14,&apos; if ai 0 bi
ai if ai = bi &apos;
</equation>
<bodyText confidence="0.99653275">
where Y is don&apos;t-care term accepting any
value. A lexical contextual constraint is gen-
erated as a result of join operation. The
consistency is defined as
</bodyText>
<equation confidence="0.937758666666667">
k={((ai,...,a,) _=_ (C1, ... , Cn)) =k,
1 if (Ci = ai or Ci =&apos; *&apos;) for all 1 &lt; i &lt; n
0 otherwise
</equation>
<bodyText confidence="0.8777265">
The algorithm for generating lexical contex-
tual constraints is shown in Figure 4.
</bodyText>
<listItem confidence="0.966691384615385">
• Input: a set of active lexical contexts
Lau, = {/ci ... /cn} for word w,
where /ci = (a1,... ,a,).
• Output: a set of lexical contextual
constraints LCC,„ = Peel . • • lcck} ,
where /cci = (C1, • • - , Cn)-
1. Initialize LCC,,, = 0
2. Do the followings for each /ci E LCu,
(a) For all /ci(j 0 i), Count(1c3) = # of
matched attributes with /ci
(b) max_cnt= ax Count(lc3)
(c) For all Ici, warhgermeCi ouciEcnLt(lwci) = max_cnt,
lcc = 14 ED lei, LCCli, 4-- LCC,,, U {/cc}
</listItem>
<figureCaption confidence="0.992834">
Figure 4: Algorithm for generating lexical
contextual constraints.
</figureCaption>
<bodyText confidence="0.9898005">
A /cc plays the role of a feature. Following
is an example of a feature.
</bodyText>
<equation confidence="0.893513666666667">
{ 1 if xword = &amp;quot;that&amp;quot; and
f (x, y) = xi_i = &amp;quot;say&amp;quot; and y =1
0 otherwise
</equation>
<bodyText confidence="0.99940875">
We collect the statistics for each /cc. The fre-
quency of each /cc is counted as the number
of lexical contexts that satisfy the consistency
operation with the /cc.
</bodyText>
<equation confidence="0.93987425">
n
CICC) = E (ici =-
i.i.
cc). .
</equation>
<page confidence="0.97653">
167
</page>
<bodyText confidence="0.999949166666667">
Identifying segmentable positions is per-
formed with the consistency operation with
the lexical context of word w and /cc E LCC,,,.
The word whose lexical context is consistent
with /cc is identified as a segmentable posi-
tion.
</bodyText>
<sectionHeader confidence="0.940259" genericHeader="method">
4 Determination Schemes of
Segmentation Posit ions
</sectionHeader>
<bodyText confidence="0.999759625">
Segmentation positions are determined
through two steps: identifying segmentable
positions and selecting the most appropriate
position among them. Segmentable positions
are identified using the consistency operation.
Maximum entropy model in Section 2 gives a
probability to each position.
Segmentation performance is measured in
terms of coverage and accuracy. Coverage is
the ratio of the number of actually segmented
sentences to the number of segmentation tar-
get sentences that are longer than a words,
where a is a fixed constant distinguishing long
sentences from short ones. Accuracy is evalu-
ated in terms of the safe segmentation ratio.
They are defined as follows:
</bodyText>
<listItem confidence="0.812887666666667">
# of actually segmented Sent.
coverage =
# of Sent. to be segmented
accuracy =
# of actually segmented Sent.
(4)
</listItem>
<subsectionHeader confidence="0.969243">
4.1 Baseline Scheme
</subsectionHeader>
<bodyText confidence="0.958403375">
No contextual information is used in identify-
ing segmentable positions. They are empiri-
cally identified. A word that is tagged as a
segmentation position more than 5 times is
identified as a segmentable position. A set of
segmentable positions, D, is as follows.
segmentation position w,„ is selected as the one
that has highest p(wi) value:
</bodyText>
<equation confidence="0.9884315">
w,„ = arg max p(wi).
wiED
</equation>
<bodyText confidence="0.9998585">
This scheme serves as a baseline for comparing
the segmentation performance of the models.
</bodyText>
<subsectionHeader confidence="0.9954775">
4.2 A Scheme using Lexical
Contextual Constraints
</subsectionHeader>
<bodyText confidence="0.998246428571428">
Lexical contextual constraints are used in
identifying segmentable positions. Compared
with the baseline scheme, this scheme con-
siders contextual information of a word. All
consistent words with the defined lexical con-
textual constraints form a set of segmentable
positions D.
</bodyText>
<equation confidence="0.5533">
{tvi I (dowi /cc) = 1}.
</equation>
<bodyText confidence="0.991006333333333">
The maximum likelihood principle gives a
probability distribution for p(y I lccwi), where
y E 10, 11. Segmentation appropriateness is
evaluated by p(1 I /cc,). A position with the
highest p(1 I /ce,i) becomes a segmentation
position:
</bodyText>
<equation confidence="0.8945015">
w =argmax p(1 I /cow, ).
wi ED
</equation>
<subsectionHeader confidence="0.998105">
4.3 A Scheme using Lexical
Contextual Constraints with
Word Sets
</subsectionHeader>
<bodyText confidence="0.9999314">
Due to insufficient training samples for con-
structing lexical contextual constraints, some
segmentable positions may not be identified.
To alleviate this problem we introduce word
sets whose elements have linguistically similar
features. We define four word sets: coordinate
conjunction set, subordinate conjunction set,
interogative set, auxiliary verb set. The cate-
gories of word sets and the examples of their
members are shown in Table 1.
</bodyText>
<listItem confidence="0.522204">
(3)
# of Sent. with safe segmentation
</listItem>
<bodyText confidence="0.969991">
= {wi I wi is tagged as segmentation position
and #(tagged wi) 5}
In order to select the most appropriate po-
sition, the segmentation appropriateness of
each position is evaluated by the probability
of word wi:
# of tagged wi
</bodyText>
<equation confidence="0.850897">
P(tv.) = # of wi in the corpus
</equation>
<bodyText confidence="0.520888">
p(w2) represents the tendency that word to-
will be used as a segmentation position. A
</bodyText>
<tableCaption confidence="0.999826">
Table 1: The word sets and examples.
</tableCaption>
<table confidence="0.9909144">
Word Set Examples
Coordinate Conjunctions and, or, but
Subordinate Conjunctions if, when, ...
Interogatives how, what, ...
Auxiliary Verbs can, should, ...
</table>
<bodyText confidence="0.379964666666667">
Coordinate conjunctions have only 3 mem-
bers, but they frequently apprear in long sen-
tences. Subordinate conjunctions have 25
</bodyText>
<page confidence="0.997209">
168
</page>
<bodyText confidence="0.999404625">
members, interogatives 5 members, and aux-
iliary verbs have 12 members now. The words
belonging to each word set are treated equally.
Lexical contextual constraints are constructed
for words and word sets, so the statistics is
collected for both of them. The set of seg-
mentable positions V is defined somewhat dif-
ferently as:
</bodyText>
<equation confidence="0.8950985">
V = {wi, wsi I (kw;F-.-_- dcc,i) = 1
or (ictusi a /cciusj) = 11,
</equation>
<bodyText confidence="0.999769">
where wsi denotes a word set to which the jth
word in a sentence belongs.
In this scheme, p(1 1 /cc) or p(1 I /cc„)
expresses the segmentation appropriateness of
the position. Therefore, a segmentation posi-
tion is determined by
</bodyText>
<equation confidence="0.8826">
w. = arg max {p(1 I /ccuu), p(1 I ice;)}.
{10„wsi}ED
</equation>
<sectionHeader confidence="0.998642" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.942769">
5.1 Corpus and Construction of the
Maximum Entropy Model
</subsectionHeader>
<bodyText confidence="0.99996236">
We construct the corpus from two different
domains, where the sentences longer than 15
-words are extracted3. The training portion is
used to generate lexical contextual constraints
and to collect statistics for maximum entropy
model construction. From high school English
texts, 1500 sentences are tagged with segmen-
tation positions by human. Two people who
have some knowledge about English syntactic
structures read sentences, and marked words
as segmentation positions where they paused.
After generating lexical contextual con-
straints, we constructed the maximum en-
tropy model p(y(x), where x is a lexical con-
textual constraint and y E {0, 1}. The model
incorporates features that occur more than 5
times in the training data. 3626 candidate fea-
tures were generated without word sets and
3878 features with word sets. In Table 2,
training time and the number of active fea-
tures of the model are shown.
Segmentation performance is evaluated us-
ing test portion that consists of 1800 sentences
from two domains: high school English texts
and the Byte Magazine.
</bodyText>
<footnote confidence="0.9881312">
3The sentences with commas are excluded because
comma is an explicit segmentation position. Segments
resulting from a segmentation at commas may be the
manageable-sized ones. Our work is to segment long
sentences without explicit segmentation positions.
</footnote>
<tableCaption confidence="0.982532">
Table 2: Construction of models.
</tableCaption>
<table confidence="0.997952333333333">
Training # of
Time Active Features
Without 10 min 2720
Word Sets
With 12 min 2910
Word Sets
</table>
<subsectionHeader confidence="0.999656">
5.2 Segmentation Performance
</subsectionHeader>
<bodyText confidence="0.981921444444445">
In addition to coverage and accuracy, SC
value is also defined to express the degree of
contribution to efficient parsing by segmenta-
tion. It is the ratio of the sentences that can
benefit from intra-sentence segmentation. If a
long sentence is not segmented or is segmented
at unsafe segmentation positions, the sentence
is called a segmentation error sentence.
SC value is calculated as
</bodyText>
<listItem confidence="0.740050333333333">
= # of segmentation error sentences
SC 1
# of segmentation target sentences &apos;
</listItem>
<bodyText confidence="0.9961734">
A sentence longer than a words is con-
sidered as the segmentation target sentence,
where a is set to 12. Table 3 compares seg-
mentation performance for each determina-
tion scheme.
</bodyText>
<tableCaption confidence="0.997815">
Table 3: Segmentation performance of the de-
termination schemes of segmentation position.
</tableCaption>
<table confidence="0.995605333333333">
Determination Coverage/ SC
Schemes Accuracy (%)
Baseline 100/77.6 0.776
LCC 90.7/89 0.808
LCC with 95.8/87.9 0.865
Word Sets
</table>
<bodyText confidence="0.99455125">
By the comparison of the baseline scheme
with others, the accuracy is observed to de-
pend on the context information. Word sets
are helpful for increasing coverage with less
degradation of accuracy. Each scheme has su-
periority in terms of the different measures.
But in terms of applicability to practical sys-
tems, the third scheme is best for our purpose.
Table 4 shows the segmentation performance
of the scheme using LCC with word sets.
SC value for the sentences from the same
domain as training data is about 0.88, and
</bodyText>
<page confidence="0.999312">
169
</page>
<tableCaption confidence="0.9546255">
Table 4: Segmentation performance of LCC Table 5: Comparison of parsing efficiency
with word sets. with/without segmentation.
</tableCaption>
<table confidence="0.999691615384615">
Domain Sent. Coverage/ SC
Length Accuracy(%)
High-School 15,19 99.0/95.9 0.95
English Text
20,24 100/94.0 0.94
25,-29 96.0/81.3 0.78
30,- 100/67.5 0.68
Byte 15,49 94.0/92.6 0.87
Magazine
20,24 91.0/91.2 0.83
25,29 92.5/94.6 0.88
30-, 93.5/86.1 0.81
Total 1800 95.8/87.9 0.87
</table>
<bodyText confidence="0.999355">
about 0.85 for the sentenes from the Byte
Magazine. Though they slightly differ be-
tween test domains, about 87% of long sen-
tences can be parsed with less complexity and
without causing passing failures. It suggests
that the intra-sentence segmentation method
can be utilized for efficient parsing of the long
sentences.
</bodyText>
<subsectionHeader confidence="0.999742">
5.3 Parsing Efficiency
</subsectionHeader>
<bodyText confidence="0.999923333333333">
Parsing efficiency is generally measured by
the required time and memory for parsing.
In most cases, parsing sentences longer than
30 words could not complete without intra-
sentence segmentation. Therefore, the parsing
is performed for the sentences longer than 15
and less than 30 words. Ultra-Sparc 30 ma-
chine is used for experiments. The efficiency
improvement was measured by
</bodyText>
<equation confidence="0.924858166666667">
tunseg t$eg
Ertane X 100,
tunseg
memory rflunseg Mseg
X 100,
Munseg
</equation>
<bodyText confidence="0.999867">
where tunseg and Munseg are time and memory
during parsing without segmentation and tseg,
mseg are for the parsing with segmentation.
Table 5 summarizes the results.
By segmenting long sentences into several
manageable-sized segments, we can parse long
sentences with much less time and space.
</bodyText>
<subsectionHeader confidence="0.997947">
5.4 Comparison with Related Works
</subsectionHeader>
<bodyText confidence="0.999186666666667">
The intra-sentence segmentation method
based on the maximum entropy model is com-
pared with other approaches in terms of the
</bodyText>
<table confidence="0.996552454545454">
High-School Byte
English Text Magazine
With 4.6 sec 5.4 sec
Segmentation
0.9 MB 1.1 MB
Without 19.6 sec 25.1 sec
Segmentation
3.4 MB 3.7 MB
Improvement 7 6.5%
78.5%
73.5% 70.3%
</table>
<bodyText confidence="0.999577441176471">
segmentation coverage and the improvement
of parsing efficiency.
In (Lyon and Frank, 1995)(Lyon and Dick-
erson, 1997), a sentence is segmented into
three segments. Though parsing efficiency can
be improved by segmenting a sentence, this
method may be applied to only simple sen-
tences4. Long sentences are generally coordi-
nate sentences5 or complex sentences6. They
have more than two subjects, so applying this
method to such sentences seems to be inap-
propriate.
In (Kim and Kim, 1995), sentence patterns
are used to segment long sentences. This
method improve parsing efficiency by 30% in
time and 58% in space. However collecting
sentence patterns requires much human efforts
and segmentation coverage is only about 36%.
Li&apos;s method (Li et al., 1990) for sentence
segmentation also depends upon manual-
intensive pattern rules. Segmentation cover-
age seems to be unsatisfactory for practical
machine translation system.
The proposed method can be applied to co-
ordinate and complex sentences as well as sim-
ple sentences. It shows segmentation coverage
of about 96%. In addition, it needs no other
human efforts except for constructing training
data. Human annotators have only to read
sentences and mark segmentation positions,
which is more simple than collecting pattern
rules or sentence patterns. We can also get
much improved parsing efficiency: about 77%
in time and about 71% in space.
</bodyText>
<footnote confidence="0.978995285714286">
4A simple sentence has one subject and one predi-
cate.
5A coordinate sentence results from the combina-
tion of several simple sentences by the coordinate con-
junctions. - -
6A complex sentence consists of a main clause and
several subordinate clauses.
</footnote>
<page confidence="0.995869">
170
</page>
<sectionHeader confidence="0.996382" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999817172413793">
Practical machine translation systems should
be able to accommodate long sentences. Thus
intra-sentence segmentation is required as a
means for reducing parsing complexity. This
paper presents a method for intra-sentence
segmentation based on the maximum entropy
model. The method builds statistical models
automatically from a text corpus to provide
the segmentation appropriateness for safe seg-
mentation.
In the experiments with 1800 test sentences,
about 87% of them were benefited from seg-
mentation. The statistical intra-sentence seg-
mentation method can also relieve human of
the burden of constructing information, such
as segmentation rules or sentence patterns.
Experiments suggest that the proposed maxi-
mum entropy models can be incorporated into
the parser for practical machine translation
systems.
Further works can be done in two direc-
tions. First, studies on recovery mecha-
nisms for unsafe segmentation before parsing
seem necessary since unsafe segmentation may
cause parsing failures. Second, parsing control
mechanisms should be studied that exploit the
-characteristics of segmentation positions and
the parallelism among segments. This will en-
hance parsing efficiency further.
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999529777777778">
D. Beeferman, A. Berger, and J. Lafferty. 1997.
Text Segmentation using Exponential Models.
In Second Conference on Empirical Methods in
Natural Language Processing. Providence, RI.
Lambros Cranias, Harris Papageorgiou, and Ste-
lios Piperdis. 1994. A Matching Technique in
Example-Based Machine Translati on. In Pro-
ceedings of 1994 COLING, pages 100-104.
J.N. Darroch and D. Ratcliff. 1972. Generalized
Iterative Scaling for Log-linear Models. The
Annals of Mathematical Statistics, 43(5):1470-
1480.
E.T. Jaynes. 1957. Information Theory and Sta-
tistical Mechanics. Physical Review, 106:620-
630.
Sung Dong Kim and Yung Taek Kim. 1995.
Sentence Analysis using Pattern Matching in
English-Korean Machine Translation. In Pro-
ceedings of the 1995 ICCPOL, Oct. 25-28.
Ho Suk Lee. 1993. Automatic Construction of
Transfer Dictionary based on the C orpus for
English-Korean Machine Translation. Ph.D.
thesis, Seoul National University. In Korean.
Wei-Chuan Li, Tzusheng Pei, Bing-Huang Lee,
and Chuei-Feng Chiou. 1990. Parsing Long En-
glish Sentences with Pattern Rules. In Proceed-
ings of 25th Conference of COLING, pages 410-
412.
Caroline Lyon and Bob Dickerson. 1997. Reduc-
ing the Complexity of Parsing by a Method of
Decomposition. In International Workshop on
Parsing Technology, September.
Caroline Lyon and Ray Frank. 1995. Neural Net-
work Design for a Natural Language Parser.
In International Conference on Artificial Neural
Networks.
Tetsura Nasukawa. 1995. Robust Parsing Based
on Discourse Information. In 33rd Annual
Meeting of the ACL, pages 33-46.
David D. Palmer and Marti A. Hearst. 1997.
Adaptive Multilingual Sentence Boundary
Disambiguation. Computational Linguistics,
23(2):241-265.
A. Ra.tnaparkhi. 1994. A Simple Introduction
to Maximum Entropy Models for Natural Lan-
guage Processing. Technical report, Institute
for Research in Cognitive Science, University of
Pennsylvania 3401 Walnut Street, Suite 400A
Philadelphia, PA 19104-6228, May. IRCS Re-
port 97-08.
J.C. Reynar and A. Ratnaparkhi. 1997. A Maxi-
mum Entropy Approach to Identifying Sentence
Boundaries. In Proceedings of the Fifth Confer-
ence on Applied Natural Language Processing,
pages 16-19. Washington D.C.
Maseru Tomita. 1986. Efficient Parsing for Nat-
ural Language. Kluwer Academic Publishers.
T. Winograd. 1983. Language as a Cognitive Pro-
cess: Syntax, volume 1. Addison-Wesley.
Sung Hee Yoon. 1994. Efficient Parser to Find
Bilingual Idiomatic Expressions for English-
Korean Machine Translation. In Proceedings of
the 1994 ICCPOL, pages 455-460.
</reference>
<page confidence="0.998165">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.607972">
<title confidence="0.98444">Reducing Parsing Complexity by Intra—Sentence Segmentation based on Maximum Entropy Model</title>
<author confidence="0.998294">Sung Dong Kim</author>
<author confidence="0.998294">Byoung-Tak Zhang</author>
<author confidence="0.998294">Yung Taek</author>
<affiliation confidence="0.999353">School of Computer Science and Seoul National University,</affiliation>
<email confidence="0.977284">sdkim@scai.snu.ac.kr,ytkim@cse.snu.ac.kr</email>
<email confidence="0.977284">btzhang@scai.snu.ac.kr,ytkim@cse.snu.ac.kr</email>
<abstract confidence="0.962570692307692">Long sentence analysis has been a critical problem because of high complexity. This paper addresses the reduction of parsing complexity by intra-sentence segmentation, and presents maximum entropy model for determining segmentation positions. The model features lexical contexts of segmentation positions, giving a probability to each potential position. Segmentation coverage and accuracy of the proposed method are 96% and 88% respectively. The parsing efficiency is improved by 77% in time and 71% in space.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Text Segmentation using Exponential Models.</title>
<date>1997</date>
<booktitle>In Second Conference on Empirical Methods in Natural Language Processing.</booktitle>
<location>Providence, RI.</location>
<contexts>
<context position="2717" citStr="Beeferman et al., 1997" startWordPosition="419" endWordPosition="422"> machine translation system should be able to analyze long sentences in a reasonable time. Most context-free parsing algorithms have 0(n3) passing complexities in terms of time and space, where n is the length of a sentence (Tomita, 1986). Our work is motivated by the fact that parsing becomes more efficient, if n becomes shorter. This paper deals with the problem of parsing complexity by way of reducing the length of sentence to be analyzed. This reduction is achieved by intra-sentence segmentation, which is distinguished from inter—sentence segmentation that is used for text categorization (Beeferman et al., 1997) or sentence boundary identification (Palmer and Hearst, 1997) (Reynar and Ratnaparkhi, 1997). Intra-sentence segmentation plays a role as a preliminary step to a chart-based, context-free parser in English-Korean machine translation. There have been several methods for reducing parsing complexities by intrasentence segmentation. In (Lyon and Prank, 1995)(Lyon and Dickerson, 1997), they took advantage of the fact that the declarative sentences almost always consist of three segments: [pre-subject : subject : predicate]. The complexity could be reduced by decomposing a sentence into three secti</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1997</marker>
<rawString>D. Beeferman, A. Berger, and J. Lafferty. 1997. Text Segmentation using Exponential Models. In Second Conference on Empirical Methods in Natural Language Processing. Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lambros Cranias</author>
<author>Harris Papageorgiou</author>
<author>Stelios Piperdis</author>
</authors>
<title>A Matching Technique in Example-Based Machine Translati on.</title>
<date>1994</date>
<booktitle>In Proceedings of 1994 COLING,</booktitle>
<pages>100--104</pages>
<contexts>
<context position="1130" citStr="Cranias et al., 1994" startWordPosition="163" endWordPosition="166">sitions. The model features lexical contexts of segmentation positions, giving a probability to each potential position. Segmentation coverage and accuracy of the proposed method are 96% and 88% respectively. The parsing efficiency is improved by 77% in time and 71% in space. 1 Introduction Long sentence analysis has been a critical problem in machine translation because of high complexity. In EBMT (example-based machine translation), the longer a sentence is, the less possible it is that the sentence has an exact match in the translation archive, and the less flexible an EBMT system will be (Cranias et al., 1994). In idiom-based machine translation (Lee, 1993), long sentence parsing is difficult because more resources are spent during idiom recognition phase as sentence length increases. A parser is often unable to analyze long sentences owing to their complexity, though they have no grammatical errors (Nasukawa, 1995). In English-Korean machine translation, idiom-based approach is adopted to overcome the structural differences between two languages and to get more accurate translation. The parser is a chart parser with a capability of idiom recognition and translation, which is adapted to English-Kor</context>
</contexts>
<marker>Cranias, Papageorgiou, Piperdis, 1994</marker>
<rawString>Lambros Cranias, Harris Papageorgiou, and Stelios Piperdis. 1994. A Matching Technique in Example-Based Machine Translati on. In Proceedings of 1994 COLING, pages 100-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized Iterative Scaling for Log-linear Models. The Annals of Mathematical Statistics,</title>
<date>1972</date>
<pages>43--5</pages>
<contexts>
<context position="8693" citStr="Darroch and Ratcliff, 1972" startWordPosition="1390" endWordPosition="1393">zation, which cannot be explicitly written in general. Therefore, we take advantage of the fact that the models in 2 that satisfy p(f2) = 73(12) can be explained under the maximum likelihood framework (Ratnaparkhi, 1994). Maximum likelihood principle also gives the unique distribution AK, the intersection of the class with C. We assume each occurrence of (x, y) is sampled independently. Thus, log-likelihood L 33(p) of the empirical distribution 73 as predicted by a model p can be defined as The parameters Ai of exponential model (2) are obtained by the Generalized Iterative Scaling algorithm (Darroch and Ratcliff, 1972). 3 Construction of Features This section describes the features. From a corpus, contextual evidences of segmentation positions are collected and combined, resulting in features. The features are used in identifying potential segmentation positions and included in the model. 3.1 Segmentable Positions and Safe Segmentation A sentence is constructed by the combination of words, phrases, and clauses under the well-defined grammar A sentence can be segmented into shorter segments that correspond to the constituents of the sentence. That is, segments correspond to the nonterminal symbols of the con</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J.N. Darroch and D. Ratcliff. 1972. Generalized Iterative Scaling for Log-linear Models. The Annals of Mathematical Statistics, 43(5):1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Jaynes</author>
</authors>
<title>Information Theory and Statistical Mechanics. Physical Review,</title>
<date>1957</date>
<pages>106--620</pages>
<contexts>
<context position="4220" citStr="Jaynes, 1957" startWordPosition="640" endWordPosition="641">n efforts to construct pattern rules or collect sentence patterns. These factors may prevent them being applicable to practical machine translation systems. This paper presents a trainable model for identifying potential segmentation positions 164 in a sentence and determining appropriate segmentation positions. Given a corpus annotated with segmentation positions, our model automatically learns the contextual evidences about segmentation positions, which relieves human of burden to construct pattern rules or sentence patterns. These evidences are combined under the maximum entropy framework (Jaynes, 1957) to estimate the probability for each position. By intra-sentence segmentation based on the proposed model, we achieve more improved parsing efficiency by 77% in time and 71% in space. In Section 2 we introduce the maximum entropy model. Section 3 describes features incorporated into the model and the process of identifying potential segmentation positions. The determination schemes of segmentation positions are described in Section 4. Segmentation performance of the model is presented with the degree of contribution to efficient parsing by the segmentation in Section 5. We also compare our ap</context>
</contexts>
<marker>Jaynes, 1957</marker>
<rawString>E.T. Jaynes. 1957. Information Theory and Statistical Mechanics. Physical Review, 106:620-630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung Dong Kim</author>
<author>Yung Taek Kim</author>
</authors>
<title>Sentence Analysis using Pattern Matching in English-Korean Machine Translation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 1995 ICCPOL,</booktitle>
<pages>25--28</pages>
<contexts>
<context position="3395" citStr="Kim and Kim, 1995" startWordPosition="519" endWordPosition="522">7) (Reynar and Ratnaparkhi, 1997). Intra-sentence segmentation plays a role as a preliminary step to a chart-based, context-free parser in English-Korean machine translation. There have been several methods for reducing parsing complexities by intrasentence segmentation. In (Lyon and Prank, 1995)(Lyon and Dickerson, 1997), they took advantage of the fact that the declarative sentences almost always consist of three segments: [pre-subject : subject : predicate]. The complexity could be reduced by decomposing a sentence into three sections. Pattern rules (Li et al., 1990) and sentence patterns (Kim and Kim, 1995) were used to segment long English sentences. They showed low segmentation coverage, which means that many of long sentences are not segmented by the pattern rules or sentence patterns. And they require much human efforts to construct pattern rules or collect sentence patterns. These factors may prevent them being applicable to practical machine translation systems. This paper presents a trainable model for identifying potential segmentation positions 164 in a sentence and determining appropriate segmentation positions. Given a corpus annotated with segmentation positions, our model automatica</context>
<context position="23367" citStr="Kim and Kim, 1995" startWordPosition="3849" endWordPosition="3852"> 4.6 sec 5.4 sec Segmentation 0.9 MB 1.1 MB Without 19.6 sec 25.1 sec Segmentation 3.4 MB 3.7 MB Improvement 7 6.5% 78.5% 73.5% 70.3% segmentation coverage and the improvement of parsing efficiency. In (Lyon and Frank, 1995)(Lyon and Dickerson, 1997), a sentence is segmented into three segments. Though parsing efficiency can be improved by segmenting a sentence, this method may be applied to only simple sentences4. Long sentences are generally coordinate sentences5 or complex sentences6. They have more than two subjects, so applying this method to such sentences seems to be inappropriate. In (Kim and Kim, 1995), sentence patterns are used to segment long sentences. This method improve parsing efficiency by 30% in time and 58% in space. However collecting sentence patterns requires much human efforts and segmentation coverage is only about 36%. Li&apos;s method (Li et al., 1990) for sentence segmentation also depends upon manualintensive pattern rules. Segmentation coverage seems to be unsatisfactory for practical machine translation system. The proposed method can be applied to coordinate and complex sentences as well as simple sentences. It shows segmentation coverage of about 96%. In addition, it needs</context>
</contexts>
<marker>Kim, Kim, 1995</marker>
<rawString>Sung Dong Kim and Yung Taek Kim. 1995. Sentence Analysis using Pattern Matching in English-Korean Machine Translation. In Proceedings of the 1995 ICCPOL, Oct. 25-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ho Suk Lee</author>
</authors>
<title>Automatic Construction of Transfer Dictionary based on the C orpus for English-Korean Machine Translation.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Seoul National University. In Korean.</institution>
<contexts>
<context position="1178" citStr="Lee, 1993" startWordPosition="172" endWordPosition="173">n positions, giving a probability to each potential position. Segmentation coverage and accuracy of the proposed method are 96% and 88% respectively. The parsing efficiency is improved by 77% in time and 71% in space. 1 Introduction Long sentence analysis has been a critical problem in machine translation because of high complexity. In EBMT (example-based machine translation), the longer a sentence is, the less possible it is that the sentence has an exact match in the translation archive, and the less flexible an EBMT system will be (Cranias et al., 1994). In idiom-based machine translation (Lee, 1993), long sentence parsing is difficult because more resources are spent during idiom recognition phase as sentence length increases. A parser is often unable to analyze long sentences owing to their complexity, though they have no grammatical errors (Nasukawa, 1995). In English-Korean machine translation, idiom-based approach is adopted to overcome the structural differences between two languages and to get more accurate translation. The parser is a chart parser with a capability of idiom recognition and translation, which is adapted to English-Korean machine translation. Idioms are recognized p</context>
</contexts>
<marker>Lee, 1993</marker>
<rawString>Ho Suk Lee. 1993. Automatic Construction of Transfer Dictionary based on the C orpus for English-Korean Machine Translation. Ph.D. thesis, Seoul National University. In Korean.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Chuan Li</author>
<author>Tzusheng Pei</author>
<author>Bing-Huang Lee</author>
<author>Chuei-Feng Chiou</author>
</authors>
<title>Parsing Long English Sentences with Pattern Rules.</title>
<date>1990</date>
<booktitle>In Proceedings of 25th Conference of COLING,</booktitle>
<pages>410--412</pages>
<contexts>
<context position="3353" citStr="Li et al., 1990" startWordPosition="512" endWordPosition="515">y identification (Palmer and Hearst, 1997) (Reynar and Ratnaparkhi, 1997). Intra-sentence segmentation plays a role as a preliminary step to a chart-based, context-free parser in English-Korean machine translation. There have been several methods for reducing parsing complexities by intrasentence segmentation. In (Lyon and Prank, 1995)(Lyon and Dickerson, 1997), they took advantage of the fact that the declarative sentences almost always consist of three segments: [pre-subject : subject : predicate]. The complexity could be reduced by decomposing a sentence into three sections. Pattern rules (Li et al., 1990) and sentence patterns (Kim and Kim, 1995) were used to segment long English sentences. They showed low segmentation coverage, which means that many of long sentences are not segmented by the pattern rules or sentence patterns. And they require much human efforts to construct pattern rules or collect sentence patterns. These factors may prevent them being applicable to practical machine translation systems. This paper presents a trainable model for identifying potential segmentation positions 164 in a sentence and determining appropriate segmentation positions. Given a corpus annotated with se</context>
<context position="23634" citStr="Li et al., 1990" startWordPosition="3891" endWordPosition="3894">mented into three segments. Though parsing efficiency can be improved by segmenting a sentence, this method may be applied to only simple sentences4. Long sentences are generally coordinate sentences5 or complex sentences6. They have more than two subjects, so applying this method to such sentences seems to be inappropriate. In (Kim and Kim, 1995), sentence patterns are used to segment long sentences. This method improve parsing efficiency by 30% in time and 58% in space. However collecting sentence patterns requires much human efforts and segmentation coverage is only about 36%. Li&apos;s method (Li et al., 1990) for sentence segmentation also depends upon manualintensive pattern rules. Segmentation coverage seems to be unsatisfactory for practical machine translation system. The proposed method can be applied to coordinate and complex sentences as well as simple sentences. It shows segmentation coverage of about 96%. In addition, it needs no other human efforts except for constructing training data. Human annotators have only to read sentences and mark segmentation positions, which is more simple than collecting pattern rules or sentence patterns. We can also get much improved parsing efficiency: abo</context>
</contexts>
<marker>Li, Pei, Lee, Chiou, 1990</marker>
<rawString>Wei-Chuan Li, Tzusheng Pei, Bing-Huang Lee, and Chuei-Feng Chiou. 1990. Parsing Long English Sentences with Pattern Rules. In Proceedings of 25th Conference of COLING, pages 410-412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Lyon</author>
<author>Bob Dickerson</author>
</authors>
<title>Reducing the Complexity of Parsing by a Method of Decomposition.</title>
<date>1997</date>
<booktitle>In International Workshop on Parsing Technology,</booktitle>
<contexts>
<context position="3100" citStr="Lyon and Dickerson, 1997" startWordPosition="471" endWordPosition="474">plexity by way of reducing the length of sentence to be analyzed. This reduction is achieved by intra-sentence segmentation, which is distinguished from inter—sentence segmentation that is used for text categorization (Beeferman et al., 1997) or sentence boundary identification (Palmer and Hearst, 1997) (Reynar and Ratnaparkhi, 1997). Intra-sentence segmentation plays a role as a preliminary step to a chart-based, context-free parser in English-Korean machine translation. There have been several methods for reducing parsing complexities by intrasentence segmentation. In (Lyon and Prank, 1995)(Lyon and Dickerson, 1997), they took advantage of the fact that the declarative sentences almost always consist of three segments: [pre-subject : subject : predicate]. The complexity could be reduced by decomposing a sentence into three sections. Pattern rules (Li et al., 1990) and sentence patterns (Kim and Kim, 1995) were used to segment long English sentences. They showed low segmentation coverage, which means that many of long sentences are not segmented by the pattern rules or sentence patterns. And they require much human efforts to construct pattern rules or collect sentence patterns. These factors may prevent </context>
<context position="22999" citStr="Lyon and Dickerson, 1997" startWordPosition="3788" endWordPosition="3792">segmentation. Table 5 summarizes the results. By segmenting long sentences into several manageable-sized segments, we can parse long sentences with much less time and space. 5.4 Comparison with Related Works The intra-sentence segmentation method based on the maximum entropy model is compared with other approaches in terms of the High-School Byte English Text Magazine With 4.6 sec 5.4 sec Segmentation 0.9 MB 1.1 MB Without 19.6 sec 25.1 sec Segmentation 3.4 MB 3.7 MB Improvement 7 6.5% 78.5% 73.5% 70.3% segmentation coverage and the improvement of parsing efficiency. In (Lyon and Frank, 1995)(Lyon and Dickerson, 1997), a sentence is segmented into three segments. Though parsing efficiency can be improved by segmenting a sentence, this method may be applied to only simple sentences4. Long sentences are generally coordinate sentences5 or complex sentences6. They have more than two subjects, so applying this method to such sentences seems to be inappropriate. In (Kim and Kim, 1995), sentence patterns are used to segment long sentences. This method improve parsing efficiency by 30% in time and 58% in space. However collecting sentence patterns requires much human efforts and segmentation coverage is only about</context>
</contexts>
<marker>Lyon, Dickerson, 1997</marker>
<rawString>Caroline Lyon and Bob Dickerson. 1997. Reducing the Complexity of Parsing by a Method of Decomposition. In International Workshop on Parsing Technology, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Lyon</author>
<author>Ray Frank</author>
</authors>
<title>Neural Network Design for a Natural Language Parser.</title>
<date>1995</date>
<booktitle>In International Conference on Artificial Neural Networks.</booktitle>
<contexts>
<context position="22973" citStr="Lyon and Frank, 1995" startWordPosition="3785" endWordPosition="3788"> for the parsing with segmentation. Table 5 summarizes the results. By segmenting long sentences into several manageable-sized segments, we can parse long sentences with much less time and space. 5.4 Comparison with Related Works The intra-sentence segmentation method based on the maximum entropy model is compared with other approaches in terms of the High-School Byte English Text Magazine With 4.6 sec 5.4 sec Segmentation 0.9 MB 1.1 MB Without 19.6 sec 25.1 sec Segmentation 3.4 MB 3.7 MB Improvement 7 6.5% 78.5% 73.5% 70.3% segmentation coverage and the improvement of parsing efficiency. In (Lyon and Frank, 1995)(Lyon and Dickerson, 1997), a sentence is segmented into three segments. Though parsing efficiency can be improved by segmenting a sentence, this method may be applied to only simple sentences4. Long sentences are generally coordinate sentences5 or complex sentences6. They have more than two subjects, so applying this method to such sentences seems to be inappropriate. In (Kim and Kim, 1995), sentence patterns are used to segment long sentences. This method improve parsing efficiency by 30% in time and 58% in space. However collecting sentence patterns requires much human efforts and segmentat</context>
</contexts>
<marker>Lyon, Frank, 1995</marker>
<rawString>Caroline Lyon and Ray Frank. 1995. Neural Network Design for a Natural Language Parser. In International Conference on Artificial Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsura Nasukawa</author>
</authors>
<title>Robust Parsing Based on Discourse Information.</title>
<date>1995</date>
<booktitle>In 33rd Annual Meeting of the ACL,</booktitle>
<pages>33--46</pages>
<contexts>
<context position="1442" citStr="Nasukawa, 1995" startWordPosition="213" endWordPosition="214"> been a critical problem in machine translation because of high complexity. In EBMT (example-based machine translation), the longer a sentence is, the less possible it is that the sentence has an exact match in the translation archive, and the less flexible an EBMT system will be (Cranias et al., 1994). In idiom-based machine translation (Lee, 1993), long sentence parsing is difficult because more resources are spent during idiom recognition phase as sentence length increases. A parser is often unable to analyze long sentences owing to their complexity, though they have no grammatical errors (Nasukawa, 1995). In English-Korean machine translation, idiom-based approach is adopted to overcome the structural differences between two languages and to get more accurate translation. The parser is a chart parser with a capability of idiom recognition and translation, which is adapted to English-Korean machine translation. Idioms are recognized prior to syntactic analysis and the part of a sentence for an idiom takes an edge in a chart (Winograd, 1983). When parsing long sentences, an ambiguity of an idiom&apos;s range may cause more edges than the number of words included in the idiom (Yoon, 1994), which incr</context>
</contexts>
<marker>Nasukawa, 1995</marker>
<rawString>Tetsura Nasukawa. 1995. Robust Parsing Based on Discourse Information. In 33rd Annual Meeting of the ACL, pages 33-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
<author>Marti A Hearst</author>
</authors>
<date>1997</date>
<booktitle>Adaptive Multilingual Sentence Boundary Disambiguation. Computational Linguistics,</booktitle>
<pages>23--2</pages>
<contexts>
<context position="2779" citStr="Palmer and Hearst, 1997" startWordPosition="427" endWordPosition="430">ntences in a reasonable time. Most context-free parsing algorithms have 0(n3) passing complexities in terms of time and space, where n is the length of a sentence (Tomita, 1986). Our work is motivated by the fact that parsing becomes more efficient, if n becomes shorter. This paper deals with the problem of parsing complexity by way of reducing the length of sentence to be analyzed. This reduction is achieved by intra-sentence segmentation, which is distinguished from inter—sentence segmentation that is used for text categorization (Beeferman et al., 1997) or sentence boundary identification (Palmer and Hearst, 1997) (Reynar and Ratnaparkhi, 1997). Intra-sentence segmentation plays a role as a preliminary step to a chart-based, context-free parser in English-Korean machine translation. There have been several methods for reducing parsing complexities by intrasentence segmentation. In (Lyon and Prank, 1995)(Lyon and Dickerson, 1997), they took advantage of the fact that the declarative sentences almost always consist of three segments: [pre-subject : subject : predicate]. The complexity could be reduced by decomposing a sentence into three sections. Pattern rules (Li et al., 1990) and sentence patterns (Ki</context>
</contexts>
<marker>Palmer, Hearst, 1997</marker>
<rawString>David D. Palmer and Marti A. Hearst. 1997. Adaptive Multilingual Sentence Boundary Disambiguation. Computational Linguistics, 23(2):241-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ra tnaparkhi</author>
</authors>
<title>A Simple Introduction to Maximum Entropy Models for Natural Language Processing.</title>
<date>1994</date>
<tech>Technical report,</tech>
<pages>3401</pages>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania</institution>
<location>Walnut Street, Suite 400A Philadelphia, PA</location>
<contexts>
<context position="7994" citStr="tnaparkhi, 1994" startWordPosition="1277" endWordPosition="1278">istribution with maximum entropy is the most uniform distribution. In building a model, we consider the linear exponential family Q given as 1 Q(f ) = IP(Yir) = ZA(x) exP(E Aifi(x&apos;&amp;quot;&apos; 165 log flp(ylx)73(&apos;&apos;Y) = E73(x,y)logp(ylx). x,y x,y That is, the model we want to build is pi, = arg max H(p) = arg max LI-5(q). PEC YEQ where Ai are real-valued parameters and Z,, (x) is a normalizing constant: An intersection of the class 2 of exponential models with the class of desired distribution (1) is nonempty, and the intersection contains the maximum entropy distribution and furthermore it is unique (Ratnaparkhi, 1994). Finding p,„ E C that maximizes H(p) is a problem in constrained optimization, which cannot be explicitly written in general. Therefore, we take advantage of the fact that the models in 2 that satisfy p(f2) = 73(12) can be explained under the maximum likelihood framework (Ratnaparkhi, 1994). Maximum likelihood principle also gives the unique distribution AK, the intersection of the class with C. We assume each occurrence of (x, y) is sampled independently. Thus, log-likelihood L 33(p) of the empirical distribution 73 as predicted by a model p can be defined as The parameters Ai of exponential</context>
</contexts>
<marker>tnaparkhi, 1994</marker>
<rawString>A. Ra.tnaparkhi. 1994. A Simple Introduction to Maximum Entropy Models for Natural Language Processing. Technical report, Institute for Research in Cognitive Science, University of Pennsylvania 3401 Walnut Street, Suite 400A Philadelphia, PA 19104-6228, May. IRCS Report 97-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Approach to Identifying Sentence Boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>16--19</pages>
<location>Washington D.C.</location>
<contexts>
<context position="2810" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="431" endWordPosition="435">me. Most context-free parsing algorithms have 0(n3) passing complexities in terms of time and space, where n is the length of a sentence (Tomita, 1986). Our work is motivated by the fact that parsing becomes more efficient, if n becomes shorter. This paper deals with the problem of parsing complexity by way of reducing the length of sentence to be analyzed. This reduction is achieved by intra-sentence segmentation, which is distinguished from inter—sentence segmentation that is used for text categorization (Beeferman et al., 1997) or sentence boundary identification (Palmer and Hearst, 1997) (Reynar and Ratnaparkhi, 1997). Intra-sentence segmentation plays a role as a preliminary step to a chart-based, context-free parser in English-Korean machine translation. There have been several methods for reducing parsing complexities by intrasentence segmentation. In (Lyon and Prank, 1995)(Lyon and Dickerson, 1997), they took advantage of the fact that the declarative sentences almost always consist of three segments: [pre-subject : subject : predicate]. The complexity could be reduced by decomposing a sentence into three sections. Pattern rules (Li et al., 1990) and sentence patterns (Kim and Kim, 1995) were used to s</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>J.C. Reynar and A. Ratnaparkhi. 1997. A Maximum Entropy Approach to Identifying Sentence Boundaries. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 16-19. Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maseru Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language.</title>
<date>1986</date>
<volume>1</volume>
<publisher>Kluwer Academic</publisher>
<contexts>
<context position="2332" citStr="Tomita, 1986" startWordPosition="359" endWordPosition="360">English-Korean machine translation. Idioms are recognized prior to syntactic analysis and the part of a sentence for an idiom takes an edge in a chart (Winograd, 1983). When parsing long sentences, an ambiguity of an idiom&apos;s range may cause more edges than the number of words included in the idiom (Yoon, 1994), which increases parsing complexity much. A parser of practical machine translation system should be able to analyze long sentences in a reasonable time. Most context-free parsing algorithms have 0(n3) passing complexities in terms of time and space, where n is the length of a sentence (Tomita, 1986). Our work is motivated by the fact that parsing becomes more efficient, if n becomes shorter. This paper deals with the problem of parsing complexity by way of reducing the length of sentence to be analyzed. This reduction is achieved by intra-sentence segmentation, which is distinguished from inter—sentence segmentation that is used for text categorization (Beeferman et al., 1997) or sentence boundary identification (Palmer and Hearst, 1997) (Reynar and Ratnaparkhi, 1997). Intra-sentence segmentation plays a role as a preliminary step to a chart-based, context-free parser in English-Korean m</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>Maseru Tomita. 1986. Efficient Parsing for Natural Language. Kluwer Academic Publishers. T. Winograd. 1983. Language as a Cognitive Process: Syntax, volume 1. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung Hee Yoon</author>
</authors>
<title>Efficient Parser to Find Bilingual Idiomatic Expressions for EnglishKorean Machine Translation.</title>
<date>1994</date>
<booktitle>In Proceedings of the 1994 ICCPOL,</booktitle>
<pages>455--460</pages>
<contexts>
<context position="2030" citStr="Yoon, 1994" startWordPosition="310" endWordPosition="311">errors (Nasukawa, 1995). In English-Korean machine translation, idiom-based approach is adopted to overcome the structural differences between two languages and to get more accurate translation. The parser is a chart parser with a capability of idiom recognition and translation, which is adapted to English-Korean machine translation. Idioms are recognized prior to syntactic analysis and the part of a sentence for an idiom takes an edge in a chart (Winograd, 1983). When parsing long sentences, an ambiguity of an idiom&apos;s range may cause more edges than the number of words included in the idiom (Yoon, 1994), which increases parsing complexity much. A parser of practical machine translation system should be able to analyze long sentences in a reasonable time. Most context-free parsing algorithms have 0(n3) passing complexities in terms of time and space, where n is the length of a sentence (Tomita, 1986). Our work is motivated by the fact that parsing becomes more efficient, if n becomes shorter. This paper deals with the problem of parsing complexity by way of reducing the length of sentence to be analyzed. This reduction is achieved by intra-sentence segmentation, which is distinguished from in</context>
</contexts>
<marker>Yoon, 1994</marker>
<rawString>Sung Hee Yoon. 1994. Efficient Parser to Find Bilingual Idiomatic Expressions for EnglishKorean Machine Translation. In Proceedings of the 1994 ICCPOL, pages 455-460.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>