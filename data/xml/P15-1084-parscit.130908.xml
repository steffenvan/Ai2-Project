<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000218">
<title confidence="0.977608">
WikiKreator: Improving Wikipedia Stubs Automatically
</title>
<author confidence="0.985801">
Siddhartha Banerjee
</author>
<affiliation confidence="0.931433">
The Pennsylvania State University
Information Sciences and Technology
University Park, PA, USA
</affiliation>
<email confidence="0.998181">
sub253@ist.psu.edu
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947375">
Stubs on Wikipedia often lack comprehen-
sive information. The huge cost of edit-
ing Wikipedia and the presence of only a
limited number of active contributors curb
the consistent growth of Wikipedia. In this
work, we present WikiKreator, a system
that is capable of generating content au-
tomatically to improve existing stubs on
Wikipedia. The system has two compo-
nents. First, a text classifier built using
topic distribution vectors is used to as-
sign content from the web to various sec-
tions on a Wikipedia article. Second, we
propose a novel abstractive summariza-
tion technique based on an optimization
framework that generates section-specific
summaries for Wikipedia stubs. Experi-
ments show that WikiKreator is capable of
generating well-formed informative con-
tent. Further, automatically generated con-
tent from our system have been appended
to Wikipedia stubs and the content has
been retained successfully proving the ef-
fectiveness of our approach.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999860363636364">
Wikipedia provides comprehensive information
on various topics. However, a significant percent-
age of the articles are stubs1 that require exten-
sive effort in terms of adding and editing content
to transform them into complete articles. Ideally,
we would like to create an automatic Wikipedia
content generator, which can generate a compre-
hensive overview on any topic using available in-
formation from the web and append the gener-
ated content to the stubs. Addition of automati-
cally generated content can provide a useful start-
</bodyText>
<footnote confidence="0.9505485">
1https://en.wikipedia.org/wiki/
Wikipedia:Stub
</footnote>
<author confidence="0.861951">
Prasenjit Mitra
</author>
<affiliation confidence="0.922359666666667">
Qatar Computing Research Institute
Hamad Bin Khalifa University
Doha, Qatar
</affiliation>
<email confidence="0.973758">
pmitra@qf.org.qa
</email>
<bodyText confidence="0.999658972222222">
ing point for contributors on Wikipedia, which can
be improved upon later.
Several approaches to automatically generate
Wikipedia articles have been explored (Sauper
and Barzilay, 2009; Banerjee et al., 2014; Yao
et al., 2011). To the best of our knowledge, all
the above mentioned methods identify informa-
tion sources from the web using keywords and
directly use the most relevant excerpts in the fi-
nal article. Information from the web cannot
be directly copied into Wikipedia due to copy-
right violation issues (Banerjee et al., 2014).
Further, keyword search does not always sat-
isfy information requirements (Baeza-Yates et al.,
1999). To address the above-mentioned issues,
we present WikiKreator – a system that can au-
tomatically generate content for Wikipedia stubs.
First, WikiKreator does not operate using keyword
search. Instead, we use a classifier trained using
topic distribution features to identify relevant con-
tent for the stub. Topic-distribution features are
more effective than keyword search as they can
identify relevant content based on word distribu-
tions (Song et al., 2010). Second, we propose a
novel abstractive summarization (Dalal and Malik,
2013) technique to summarize content from mul-
tiple snippets of relevant information.2
Figure 1 shows a stub that we attempt to im-
prove using WikiKreator. Generally, in stubs, only
the introductory content is available; other sec-
tions (s1,..., sr) are absent. The stub also belongs
to several categories (C1,C2, etc. in Figure) on
Wikipedia. In this work, we address the following
research question: Given the introductory content,
the title of the stub and information on the cate-
gories - how can we transform the stub into a com-
</bodyText>
<footnote confidence="0.9991545">
2An example of our system’s output can be found
here – https://en.wikipedia.org/wiki/2014_
Enterovirus_D68_outbreak – content was added on
5th Jan, 2015. The sections on Epidemiology, Causes and
Prevention have been added using content automatically gen-
erated by our method.
</footnote>
<page confidence="0.892462">
867
</page>
<note confidence="0.980724">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 867–877,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.996254347826087">
WG,
WG,
1: Enterovirus D68 caused outbreak is the largest ever reported in North America.
2: Enterovirus D68 caused outbreak in the United States were reported in August.
3: The outbreak is the largest ever reported in August.
51 The outbreak is the largest ever reported in North America.
52 Enterovirus D68 caused outbreak of respiratory disease.
53 Clusters of the outbreak in the United States were reported in August.
Epidemiology
Categories:C , C , C
Section Title Is)
Section Title Is)
Refe,ences
Wikipedia Article Title
Wikipedia Page Structure
Content of section sl
Content of section s,
Introductory Content
List of References
•
•
•
•
</figure>
<figureCaption confidence="0.999972">
Figure 1: Overview of our word-graph based generation (left) to populate Wikipedia template (right)
</figureCaption>
<bodyText confidence="0.994985475409836">
prehensive Wikipedia article?
Our proposed approach consists of two stages.
First, a text classifier assigns content retrieved
from the web into specific sections of the
Wikipedia article. We train the classifier using
a set of articles within the same category. Cur-
rently, we limit the system to learn and assign
content into the 10 most frequent sections in any
given category. The training set includes con-
tent from the most frequent sections as instances
and their corresponding section titles as the class
labels. We extract topic distribution vectors us-
ing Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) and use the features to train a Random For-
est (RF) Classifier (Liaw and Wiener, 2002). To
gather web content relevant to the stub, we for-
mulate queries and retrieve top 20 search results
(pages) from Google. We use boilerplate detec-
tion (Kohlsch¨utter et al., 2010) to retain the im-
portant excerpts (text elements) from the pages.
The RF classifier classifies the excerpts into one of
the most frequent classes (section titles). Second,
we develop a novel Integer Linear Programming
(ILP) based abstractive summarization technique
to generate text from the classified content. Previ-
ous work only included the most informative ex-
cerpt in the article (Sauper and Barzilay, 2009); in
contrast, our abstractive summarization approach
minimizes loss of information that should ideally
be in an Wikipedia article by fusing content from
several sentences. As shown in Figure 1, we con-
struct a word-graph (Filippova, 2010) using all the
sentences (WG1) assigned to a specific class (Epi-
demiology) by the classifier. Multiple paths (sen-
tences) between the start and end nodes in the
graph are generated (WG2). We represent the gen-
erated paths as variables in the ILP problem. The
coefficients of each variable in the objective func-
tion of the ILP problem is obtained by combin-
ing the information score and the linguistic quality
score of the path. We introduce several constraints
into our ILP model. We limit the summary for
each section to a maximum of 5 sentences. Fur-
ther, we avoid redundant sentences in the summary
that carry similar information. The solution to the
optimization problem decides the paths that are se-
lected in the final section summary. For example,
in Figure 1, the final paths determined by the ILP
solution, – 1 and 2 in WG2, are assigned to a sec-
tion (sr), where (sr) is the section title Epidemiol-
ogy.
To the best of our knowledge, this work is
the first to address the issue of generating con-
tent automatically to transform Wikipedia stubs
into comprehensive articles. Further, we address
the issue of abstractive text summarization for
Wikipedia content generation. We evaluate our
approach by generating articles in three differ-
ent categories: Diseases and Disorders3, Amer-
ican Mathematicians4 and Software companies
of the United States5. Our LDA-based classi-
</bodyText>
<footnote confidence="0.9951645">
3https://en.wikipedia.org/wiki/Category:
Diseases_and_disorders
4https://en.wikipedia.org/wiki/Category:
American_mathematicians
5https://en.wikipedia.org/wiki/Category:
Software_companies_of_the_United_States
</footnote>
<page confidence="0.996512">
868
</page>
<bodyText confidence="0.99998">
fier outperforms a TFIDF-based classifier in all
the categories. We use ROUGE (Lin, 2004) to
compare content generated by WikiKreator and
the corresponding Wikipedia articles. The re-
sults of our evaluation confirm the benefits of us-
ing abstractive summarization for content genera-
tion over approaches that do not use summariza-
tion. WikiKreator outperforms other comparable
approaches significantly in terms of content selec-
tion. On ROUGE-1 scores, WikiKreator outper-
forms the perceptron-based baseline (Sauper and
Barzilay, 2009) by ∼20%. We also analyze re-
viewer reactions, by appending content into sev-
eral stubs on Wikipedia, most of which (∼77%)
have been retained by reviewers.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999969228571429">
Wikipedia has been used to compute semantic re-
latedness (Gabrilovich and Markovitch, 2007), in-
dex topics (Medelyan et al., 2008), etc. How-
ever, the problem of enhancing the content of
a Wikipedia article has not been addressed ad-
equately. Learning structures of templates from
the Wikipedia articles have been attempted in the
past (Sauper and Barzilay, 2009; Yao et al., 2011).
Both these efforts use queries to extract excerpts
from the web and the excerpts ranked as the most
relevant are added into the article. However, as al-
ready pointed out, current standards of Wikipedia
requires rewriting of web content to avoid copy-
right violation issues.
To address the issue of copyright violation,
multi-document abstractive summarization is re-
quired. Various abstractive approaches have been
proposed till date (Nenkova et al., 2011). How-
ever, these methods suffer from severe deficien-
cies. Template-based summarization methods
work well, but, it assumes prior domain knowl-
edge (Li et al., 2013). Writing style across ar-
ticles vary widely; hence learning templates au-
tomatically is difficult. In addition, such tech-
niques require handcrafted rules for sentence re-
alization (Gerani et al., 2014). Alternatively, we
can use text-to-text generation (T2T) (Ganitke-
vitch et al., 2011) techniques. WikiKreator con-
structs a word-graph structure similar to (Filip-
pova, 2010) using all the sentences that are as-
signed to a particular section by a text classifier.
Multiple paths (sentences) from the graph are gen-
erated. WikiKreator selects few sentences from
this set of paths using an optimization problem
formulation that jointly maximizes the informa-
</bodyText>
<figureCaption confidence="0.954064">
Figure 2: WikiKreator System Architecture: Con-
tent Retrieval and Content Summarization
</figureCaption>
<bodyText confidence="0.970608333333333">
tiveness and readability of section-specific snip-
pets and generates output that is informative, well-
formed and readable.
</bodyText>
<sectionHeader confidence="0.987923" genericHeader="method">
3 Proposed Approach
</sectionHeader>
<bodyText confidence="0.999698947368421">
Figure 2 shows the system architecture of
WikiKreator. We are required to generate content
to populate sections of the stubs (51, 52, etc.) that
belong to category C1. Categories on Wikipedia
group together pages on similar subjects. Hence,
categories characterize Wikipedia articles surpris-
ingly well (Zesch and Gurevych, 2007). Naturally,
we leverage knowledge existing in the categories
to build our text classifier. To learn category spe-
cific templates, the system should learn from ar-
ticles contained within the same or similar cate-
gories. WikiKreator learns category-specific tem-
plates using all the articles that can be reached us-
ing a top-down approach from the particular cate-
gory. For example, in addition to C1, WikiKreator
also learns templates from articles in C2 and C3
(the subcategories of C1). As shown in the Fig-
ure 2, we deploy a two stage process to generate
content for a stub:
</bodyText>
<listItem confidence="0.7712945">
[i] Content Retrieval and
[ii] Content Summarization.
</listItem>
<bodyText confidence="0.999832555555556">
In the first stage, our focus is to retrieve content
that is relevant to the stub, say, 51 that belongs
to C1. We extract all the articles that belong to C1
and the subcategories, namely, C2 and C3. A train-
ing set is created with the contents in the sections
of the articles as instances and the section titles as
the corresponding classes. Topic distribution vec-
tors for each section content are generated using
LDA (Blei et al., 2003). We train a Random Forest
</bodyText>
<figure confidence="0.99946596875">
CA.Cares(.p, Arid®JA) &amp;Stub.(SI
A
C C
A
C
A
A
S
S
S
[i] Content Retrieval
Extract Wikipedia Articles
Classification Model (LDA – RFI
Predicting Sections
Query Generati�� E—pt9
nob Search
B�ilerpl�te Rem�v�l ��������
Excerpts
Introductory Content
s, : gest paths of Section s,
sr: gest paths of Section s,
s� : gest paths of Section s,
Wikipedia Page
[ii] Content Summarization
For
Each
Section
Path Selection – ILP model
Final Summary for section
Word-graph construction
Excerpts 4 sentences
Path Generation
</figure>
<page confidence="0.992493">
869
</page>
<bodyText confidence="0.99997865">
(RF) classifier using the topic distribution vectors.
As mentioned earlier, only the top 10 most fre-
quent sections are considered for the multi-class
classification task. We retrieve relevant excerpts
from the web by formulating queries. The topic
model infers the topic distribution features of each
excerpt and the RF classifier predicts the section
(s1, s2, etc.) of the excerpt. All web automation
tasks are performed using HTMLUnit6. In the sec-
ond stage, our ILP based summarization approach
synthesizes information from multiple excerpts as-
signed to a section and presents the most informa-
tive and linguistically well-formed summary as the
corresponding content for each section. A word-
graph is constructed that generates several sen-
tences; only a few of the sentences are retained
based on the ILP solution. The predicted section
is entered in the stub article along with the final
sentences selected by the ILP solution as the cor-
responding section-specific content on Wikipedia.
</bodyText>
<subsectionHeader confidence="0.999613">
3.1 Content Retrieval
</subsectionHeader>
<bodyText confidence="0.99968715">
Article Extraction: Wikipedia provides an API7
to download articles in the XML format. Given a
category, the API is capable of extracting all the
articles under it. We recursively extract articles
by identifying all the categories in the hierarchy
that can be reached by the crawler using top-down
traversal. We use a simple python script8 to ex-
tract the section titles and the corresponding text
content from the XML dump.
Classification model: WikiKreator uses Latent
Dirichlet Allocation (LDA) to represent each doc-
ument as a vector of topic distributions. Each
topic is further represented as a vector of proba-
bilities of word distributions. Our intuition is that
the topic distribution vectors of the same sections
across different articles would be similar. Our ob-
jective is to learn these topic representations, such
that we can accurately classify any web excerpt by
inferring the topics in the text. Say C, a category
on Wikipedia, has k Wikipedia articles (W).
</bodyText>
<equation confidence="0.811045">
(C) = {W1, W2, W3, W4, ..., Wk}
</equation>
<bodyText confidence="0.9999615">
Each article Wj has several sections denoted as
sjicji where sji and cji refer to the section title and
content of the ith section in the jth article, respec-
tively. We concentrate on the 10 most frequent
</bodyText>
<footnote confidence="0.8794542">
6http://htmlunit.sourceforge.net/
7https://en.wikipedia.org/wiki/
Special:Export
8http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
</footnote>
<bodyText confidence="0.999707818181818">
sections in any category. Training using content
from sections that are not frequent might result in
sub-optimal classification models. In our experi-
ments, each frequent section had enough instances
to optimally train a classifier. Let us denote the 10
most frequent sections in any category as S. If
any sji from Wj exists in S, the content (cji) is
included in the training set along with the section
title (sji) as the corresponding class label. These
steps are repeated for all the articles in the cate-
gory. Each instance is then represented as:
</bodyText>
<equation confidence="0.7331">
cji = {pji(t1), pji(t2), pji(t3), ..., pji(tm)}
</equation>
<bodyText confidence="0.930592947368421">
where m is the number of topics. sji is the cor-
responding label for this training instance. The
set of topics are t1, t2, t3,..., tm while pji(tm)
refers to the probability of topic m of content cji.
Contents from the most frequent sections are each
considered as a document and LDA is applied to
generate document-topic distributions. We exper-
iment with several values of m and use the value
that generates the best classification model in each
category. The topic vectors and the correspond-
ing labels are used to train a Random Forest (RF)
classifier. As the classes might be unbalanced, we
apply resampling on the training set.
Predicting sections: In this step, we search the
web for relevant content on the stub and assign
them to their respective sections. We formulate
search queries to retrieve web pages using a search
engine. We extract multiple excerpts from the
pages and then the RF classifier predicts the class
(section label) for each excerpt.
(i) Query Generation: To search the web, we
formulate queries by combining the stub title and
keyphrases extracted from the first sentence of the
introductory content of the stub. The first sen-
tence generally contains the most important key-
words that represent the article. Focused queries
increases relevance of extraction as well as helps
in disambiguation of content. We use the topia
term extractor (Chatti et al., 2014) to extract
keyphrases. For example, the query generated for
a stub on Hereditary hyperbilirubinemia is Hered-
itary hyperbilirubinemia bilirubin metabolic dis-
order where bilirubin metabolic disorder are the
keyphrases generated from the first sentence of the
stub from Wikipedia. The query is used to identify
the top 20 URLs (search results) from Google9.
(ii) Boilerplate removal: Web content from the
search results obtained in the previous step re-
</bodyText>
<footnote confidence="0.990358">
9http://www.google.com
</footnote>
<page confidence="0.993882">
870
</page>
<bodyText confidence="0.998745818181818">
quires cleaning to retain only the relevant informa-
tion. Removal of irrelevant content is done using
boilerplate detection (Kohlsch¨utter et al., 2010).
The web pages contain several excerpts (text el-
ements) in between the HTML tags. Only the ex-
cerpts that are classified as relevant by the boiler-
plate detection technique are retained.
(iii) Classification and assignment of excerpts:
The LDA model generated earlier infers topic dis-
tribution of each excerpt based on word distribu-
tions. The RF classifier predicts the class (section
title) for each excerpt based on the topic distribu-
tion. However, predictions that do not have a high
level of confidence might lead to excerpts being
appended to inappropriate sections. Therefore, we
set the minimum confidence level at 0.5. If the
prediction confidence of the RF classifier for a par-
ticular excerpt is above the minimum confidence
level, the excerpt is assigned to the class; other-
wise, the excerpt is discarded.
In the next step, we apply summarization on the
excerpts assigned to each section.
</bodyText>
<subsectionHeader confidence="0.999155">
3.2 Content Summarization
</subsectionHeader>
<bodyText confidence="0.98616">
To summarize content for Wikipedia effectively,
we formulate an ILP problem to generate abstrac-
tive summaries for each section with the objective
of maximizing linguistic quality and information
content.
Word-graph: A word-graph is constructed using
all the sentences included in the excerpts assigned
to a particular section. We used the same tech-
nique to construct the word-graph as (Filippova,
2010) where the nodes represent the words (along
with parts-of-speech (POS)) and directed edges
between the nodes are added if the words are adja-
cent in the input sentences. Each sentence is con-
nected to dummy start and end nodes to mark the
beginning and ending of the sentences. The sen-
tences from the excerpts are added to the graph
in an iterative fashion. Once the first sentence
is added, words from the following sentences are
mapped onto a node in the graph provided that
they have the exact same word form and the same
POS tag. Inclusion of POS information prevents
ungrammatical mappings. The words are added to
the graph in the following order:
</bodyText>
<listItem confidence="0.998634333333333">
• Content words are added for which there are
no candidates in the existing graph;
• Content words for which multiple mappings
are possible or such words that occur more
than once in the sentence;
• Stopwords.
</listItem>
<bodyText confidence="0.999846351351351">
If multiple mappings are possible, the context of
the word is checked using word overlaps to the left
and right within a window of two words. Even-
tually, the word is mapped to that node that has
the highest context. We also changed Filippova’s
method by adding punctuations as nodes to the
graph. Figure 1 shows a simple example of the
word-graph generation technique. We do not show
POS and punctuations in the figure for the sake of
clarity. The Figure also shows that several pos-
sible paths (sentences) exist between the dummy
start and end nodes in the graph. Ideally, excerpts
for any section would contain multiple common
words as they belong to the same topic and have
been assigned the same section. The presence of
common words ensure that new sentences can be
generated from the graph by fusing original set of
sentences in the graph. Figure 1 shows an illus-
tration of our approach where the set of sentences
assigned to a particular section (WG1) are used to
create the word-graph. The word-graph generates
several possible paths between the dummy nodes;
we show only three such paths (WG2). To obtain
abstractive summaries, we remove generated paths
from the graph that are same or very similar to any
of the original sentences. If the cosine similarity
of a generated path to any of the original sentences
is greater than 0.8, we do not retain the path. We
compute cosine similarity after applying stopword
removal. However, we do not apply stemming as
our graph construction is based on words existing
in the same form in multiple sentences. Similar to
Filippova’s work, we set the minimum path length
(in words) to eight to avoid incomplete sentences.
Paths without verbs are discarded. The final set of
generated paths after discarding the ineligible ones
are used in the next step of summary generation.
</bodyText>
<subsectionHeader confidence="0.623419">
3.2.1 ILP based Path Selection
</subsectionHeader>
<bodyText confidence="0.999957">
Our goal is to select paths that maximize the in-
formativeness and linguistic quality of the gener-
ated summaries. To select the best multiple pos-
sible sentences, we apply an overgenerate and se-
lect (Walker et al., 2001) strategy. We formulate
an optimization problem that ‘selects’ a few of
the many generated paths in between the dummy
nodes from the word-graph. Let pi denote each
path obtained from the word-graph. We introduce
three different factors to judge the relevance of
</bodyText>
<page confidence="0.990689">
871
</page>
<bodyText confidence="0.9985203">
a path – Local informativeness (Iloc(pi)), Global
informativeness (Iglob(pi)) and Linguistic quality
(LQ(pi)). Any sentence path should be relevant
to the central topic of the article; this relevance
is tackled using Iglob(pi). Iloc(pi) models the
importance of a sentence among several possible
sentences that are generated from the word-graph.
Linguistic quality (LQ(pi)) is computed using a
trigram language model (Song and Croft, 1999)
that assigns a logarithmic score of probabilities of
occurrences of three word sequences in the sen-
tences.
Local Informativeness: In principle, we can use
any existing method that computes sentence im-
portance to account for Local Informativeness. In
our model, we use TextRank scores (Mihalcea and
Tarau, 2004) to generate an importance value of
each path. TextRank creates a graph of words from
the sentences. The score of each node in the graph
is calculated as shown in Equation (1):
</bodyText>
<equation confidence="0.992123333333333">
S(Vi) = (1 − d) + d x EV .d&apos; Vi wji w. S(Vi)
jc7( ) �Vk ∈adj(Vi) 7k
(1)
</equation>
<bodyText confidence="0.999918666666667">
where Vi represents the words and adj(Vi) denotes
the adjacent nodes of Vi. Setting d to 0.80 in our
experiments provided the best content selection re-
sults. The computation convergences to return fi-
nal word importance scores. The informativeness
score of a path Iloc(pi) is obtained by adding the
importance scores of the individual words in the
path.
Global Informativeness: To compute global
informativeness, we compute the relevance of a
sentence with respect to the query to assign higher
weights to sentences that explicitly mention the
main title or mention certain keywords that are
relevant to the article. We compute the cosine
similarity using TFIDF features between each
sentence and the original query that was formu-
lated during the web search stage. We define
global informativeness as follows:
</bodyText>
<equation confidence="0.995213">
Iglob(pi) = CosineSimilarity(Q, pi) (2)
</equation>
<bodyText confidence="0.995767285714286">
where Q denotes the formulated query.
Linguistic Quality: In order to compute Linguis-
tic quality, we use a language model that assigns
probabilities to sequence of words to compute lin-
guistic quality. Suppose a path contains a se-
quence of q words {w1, w2, ..., wq1. The score
LQ(pi) assigned to each path is defined as fol-
</bodyText>
<equation confidence="0.931637">
lows:
LQ(pi) = 1−LL(w1,w2,...,wq), (3)
1
</equation>
<bodyText confidence="0.973602176470588">
where LL(w1, w2, ..., wq) is defined as:
flq
LL(w1, ... , wq) = L 1 -log2 t=3 P(wt wt−1wt−2).
As can be seen from Equation (4), we com-
bine the conditional probability of different sets
of 3-grams (trigrams) in the sentence and aver-
aged the value by L – the number of conditional
probabilities computed. The LL(w1, w2, ... , wq)
scores are negative; with higher magnitude imply-
ing lower importance. Therefore, in Equation (3),
we take the reciprocal of the logarithmic value
with smoothing to compute LQ(pi). In our exper-
iments, we used a 3-gram model10 that is trained
on the English Gigaword corpus. Trigram models
have been successfully used in several text-to-text
generation tasks (Clarke and Lapata, 2006; Filip-
pova and Strube, 2008) earlier.
</bodyText>
<listItem confidence="0.7591646">
ILP Formulation: To select the best paths, we
combine all the above mentioned factors Iloc(pi),
Iglob(pi) and linguistic quality LQ(pi) in an opti-
mization framework. We maximize the following
objective function:
</listItem>
<equation confidence="0.6799135">
F (p1, ... ,pK) = EKi=1 T(pi) - Iloc(pi) - Iglob(pi) - LQ(pi) - pi
1
</equation>
<bodyText confidence="0.928199166666667">
where K represents the total number of generated
paths. Each pi represents a binary variable, that
can be either 0 or 1, depending on whether the path
is selected in the final summary or not. In addition,
T (pi) – the number of tokens in a path, is included
in the objective function. The term 1
</bodyText>
<equation confidence="0.844517">
T (pi) normal-
</equation>
<bodyText confidence="0.98191425">
izes the Textrank scores by the length of the sen-
tences. First, we ensure that a maximum of Smax
sentences are selected in the summary using Equa-
tion (6).
</bodyText>
<equation confidence="0.999632333333333">
K
pi :5 Smax (6)
i=1
</equation>
<bodyText confidence="0.999828166666667">
In our experiments, we set Smax to 5 to generate
short concise summaries in each section. Using a
length constraint enables us to only populate the
sections using the most informative content. We
introduce Equation (7) to prevent similar informa-
tion (cosine similarity &gt; 0.5) from being conveyed
</bodyText>
<footnote confidence="0.963785">
10The model is available here: http://www.keithv.
com/software/giga/. We used the VP 20K vocab ver-
sion.
</footnote>
<page confidence="0.991908">
872
</page>
<table confidence="0.80241525">
Category Most Frequent Sections
American Mathematicians Awards, Awards and honors, Biography, Books, Career, Education, Life, Publications, Selected publications, Work
Diseases and Disorders Causes, Diagnosis, Early life, Epidemiology, History, Pathophysiology, Prognosis, Signs and symptoms, Symptoms, Treatment
US Software companies Awards, Criticism, Features, Games, History, Overview, Products, Reception, Services, Technology
</table>
<tableCaption confidence="0.992297">
Table 1: Data characteristics of three domains on Wikipedia
</tableCaption>
<table confidence="0.9954385">
Category #Articles #Instances
American Mathematicians — 2100 1493
Diseases and Disorders — 7000 9098
US Software companies — 3600 2478
</table>
<tableCaption confidence="0.999602">
Table 2: Dataset used for classification
</tableCaption>
<bodyText confidence="0.999846461538462">
by different sentences. This constraint reduces re-
dundancy. If two sentences have a high degree of
similarity, only one out of the two can be selected
in the summary.
The ILP problem is solved using the Gurobi op-
timizer (2015). The solution to the problem de-
cides the paths that should be included in the final
summary. We populate the sections on Wikipedia
using the final summaries generated for each sec-
tion along with the section title. All the refer-
ences that have been used to generate the sen-
tences are appended along with the content gen-
erated on Wikipedia.
</bodyText>
<sectionHeader confidence="0.998246" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.879632272727273">
To evaluate the effectiveness of our proposed tech-
nique, we conduct several experiments. First, we
evaluate our content generation approach by gen-
erating content for comprehensive articles that al-
ready exist on Wikipedia. Second, we analyze re-
viewer reactions on our system generated articles
by adding content to several stubs on Wikipedia.
Our experiments were designed to answer the fol-
lowing questions:
(i)What are the optimal number of topic distribu-
tion features for each category? What are the clas-
sification accuracies in each domain?
(ii)To what extent can our technique generate the
content for articles automatically?
(iii)What are the general reviewer reactions on
Wikipedia and what percentage of automatically
generated content on Wikipedia is retained?
Dataset Construction: As mentioned earlier
in Section 3.1, we crawl Wikipedia articles by
traversing the category graph. Articles that contain
at least three sections were included in the training
set; other articles having lesser number of sections
</bodyText>
<figureCaption confidence="0.981574">
Figure 3: Performance of Classifier in the three
categories based on the number of topics.
</figureCaption>
<bodyText confidence="0.9973138125">
are generally labeled as stubs and hence not used
for training. Table 1 shows the most frequent sec-
tions in each category. Further, Table 2 shows the
total number of articles retrieved from Wikipedia
in each category. The total number of instances are
also shown. The number of instances denotes the
total number of the most frequent sections in each
category. As can be seen from the table, the num-
ber of instances is higher than the number of arti-
cles only in case of the category on diseases. This
implies that there are generally more common sec-
tions in the diseases category than the other cate-
gories.
In each category, the content from only the most
frequent sections were used to generate a topic
model. The topic model is further used to in-
fer topic distribution vectors from the training in-
stances. We used the MALLET toolkit (McCal-
lum, 2002) for generating topic distribution vec-
tors and the WEKA package (Hall et al., 2009) for
the classification tasks.
Optimal number of topics: The LDA model re-
quires a pre-defined number of topics. We exper-
iment with several values of the number of top-
ics ranging from 10 to 100. The topic distribution
features of the content of the instances are used
to train a Random Forest Classifier with the cor-
responding section titles as the class labels. As
can be seen in the Figure 3, the classification per-
formance varies across domains as well as on the
number of topics. The optimal number of top-
ics based on the dataset are marked in blue cir-
</bodyText>
<equation confidence="0.998146666666667">
(7)
pi + pi, ≤ 1 if sim(pi,pi,) ≥ 0.5.
∀i, i&apos; E [1, K], i =� i&apos;,
</equation>
<page confidence="0.997767">
873
</page>
<table confidence="0.9992405">
Category LDA-RF SVM-WV
American Mathematicians 0.778 0.478
Diseases and Disorders 0.886 0.801
US Software companies 0.880 0.537
</table>
<tableCaption confidence="0.999206">
Table 3: Classification: Weighted F-Scores
</tableCaption>
<bodyText confidence="0.997381891304348">
cles (40, 50 and 20 topics for Diseases, Software
Companies in US and American mathematicians,
respectively) in the Figure. We classify web ex-
cerpts using the best performing classifiers trained
using the optimal number of topic features in each
category.
Classification performance: We use 10-fold
cross validation to evaluate the accuracy of our
classifier. According to the F-Scores, our classifier
(LDA-RF) performs similarly in the categories on
Diseases and US Software companies. However,
the accuracy is lower in the American Mathemati-
cians category. We also experimented with a base-
line classifier, that is trained on TFIDF features
(upto trigrams). A Support vector machine (Cortes
and Vapnik, 1995) classifier obtained the best per-
formance using the TFIDF features. The base-
line system is referred to as SVM-WV. We exper-
imented with several other combinations of classi-
fiers; however, we show only the best performing
systems using the LDA and TFIDF features. As
can be seen from the Table 3, our classifier (LDA-
RF) outperforms SVM-WV significantly in all the
domains. SVM-WV performs better in the cate-
gory on diseases than the other two categories and
the performance is comparable to (LDA-RF). The
diseases category has more uniformity in terms of
the section titles, hence specific words or phrases
characterize the sections well. In contrast, word
distributions (LDA) work significantly better than
TFIDF features in the other two categories.
Error Analysis: We performed error analysis to
understand the reason for misclassifications. As
can be seen from the Table 1, all the categories
have several overlapping sections. For example,
Awards and honors and Awards contain similar
content. Authors use various section names for
similar content in the articles within the same cat-
egory. We analyzed the confusion matrices, and
found that multiple instances in Awards were clas-
sified into the class of Awards and honors. Simi-
lar observations are made on the Books and Pub-
lications classes – which are related sections in
the context of academic biographies. In future,
we plan to use semantic measures to relate similar
classes automatically and group them in the same
</bodyText>
<table confidence="0.9999386">
Category System ROUGE-1 ROUGE-2
WikiKreator 0.522 0.311
American Mathematicians Perceptron 0.431 0.193
Extractive 0.471 0.254
WikiKreator 0.537 0.323
Diseases and Disorders Perceptron 0.411 0.197
Extractive 0.473 0.232
WikiKreator 0.521 0.321
US Software companies Perceptron 0.421 0.228
Extractive 0.484 0.257
</table>
<tableCaption confidence="0.9453225">
Table 4: ROUGE-1 and 2 Recall values – Com-
paring system generated articles to model articles
</tableCaption>
<bodyText confidence="0.99435755">
class during classification.
Content Selection Evaluation: To evaluate the
effectiveness of our content generation process,
we generated the content of 500 randomly se-
lected articles that already exist on Wikipedia in
each of the categories. We compare WikiKreator’s
output against the current content of those ar-
ticles on Wikipedia using ROUGE (Lin, 2004).
ROUGE matches N-gram sequences that exist
in both the system generated articles and the
original Wikipedia articles (gold standard). We
also compare WikiKreator’s output with an ex-
isting Wikipedia generation system [Perceptron]
of Sauper and Barzilay (2009)11 that employs a
perceptron learning framework to learn topic spe-
cific extractors. Queries devised using the con-
junction of the document title and the section ti-
tle were used to obtain excerpts from the web
using a search engine, which were used in the
perceptron model. In Perceptron, the most im-
portant sections in the category was determined
using a bisectioning algorithm to identify clus-
ters of similar sections. To understand the ef-
fectiveness of our abstractive summarizer, we de-
sign a system (Extractive) that uses an extrac-
tive summarization module. In Extractive, we use
LexRank (Erkan and Radev, 2004) as the summa-
rizer instead of our ILP based abstractive summa-
rization model. We restrict the extractive sum-
maries to 5 sentences for accurate comparison of
both the systems. The same content was received
as input from the classifier by the Extractive as
well as our ILP-based system.
As can be seen from the Table 4, the ROUGE
scores obtained by WikiKreator is higher than that
of the other comparable systems in all the cat-
egories. The higher ROUGE scores imply that
WikiKreator is generally able to retrieve useful
information from the web, synthesize them and
present the important information in the article.
</bodyText>
<footnote confidence="0.974748">
11The system is available here: https://github.
com/csauper/wikipedia
</footnote>
<page confidence="0.995447">
874
</page>
<figure confidence="0.8544705">
Statistics Count
Number of stubs edited 40
Number of stubs retained without any changes 21
Number of stubs that required minor editing 6
Number of stubs where edits were modified by reviewers 4
Number of stubs in which content was removed 9
Average change in size of stubs 515 bytes
Average number of edits made post content-addition ∼3
</figure>
<tableCaption confidence="0.983926">
Table 5: Statistics of Wikipedia generation
</tableCaption>
<bodyText confidence="0.998351507042254">
However, it may also be noted that the Extractive
system outperforms the Perceptron framework.
Summarization from multiple sources generates
more informative summaries and is more effective
than ‘selection’ of the most informative excerpt,
which is often inadequate due to potential loss of
information. WikiKreator performs better than the
extractive system on all the categories. Our ILP-
based abstractive summarization system fuses and
selects content from multiple sentences, thereby
aggregating information successfully from multi-
ple sources. In contrast, LexRank ‘extracts’ the
top 5 sentences that results in some information
loss.
Analysis of Wikipedia Reviews: To compare our
method with the other techniques, it is necessary
to generate content and append to Wikipedia stubs
using all the techniques. However, recent work on
article generation (Banerjee et al., 2014) has al-
ready shown that content directly copied from web
sources cannot be used on Wikipedia. Further,
bots using copyrighted content might be banned
and real-users would have to read sub-standard ar-
ticles due to the internal tests we perform. Due to
the above mentioned reasons, we appended con-
tent generated only using our abstractive summa-
rization technique.
We published content generated by WikiKreator
on Wikipedia and appended the content to 40 ran-
domly selected stubs. As can be seen from the
Table 5, the content generated using our system
was generally accepted by the reviewers. Half of
the articles did not require any further changes;
while in 6 cases (15%) the reviewers asked us to
fix grammatical issues. In 9 stubs, the reliability of
the cited references was questioned. Information
sources on Wikipedia need to satisfy a minimum
reliability standard, which our algorithm currently
cannot determine. On an average, 3 edits were
made to the Wikipedia articles that we generated.
In general, there is an average increase in the con-
tent size of the stubs that we edited showing that
our method is capable of producing content that
generally satisfy Wikipedia criterion.
Analysis of section assignment: We manually in-
spected generated content of 20 articles in each
category. Generated summaries are both informa-
tive and precise. However, in certain cases, the
generated section title is not the same as the sec-
tion title in the original Wikipedia article. For
example, we generated content for the section
“Causes” for the article on Middle East Respira-
tory Syndrome (MERS)12:
Milk or meat may play a role in the transmission of the virus
. People should avoid drinking raw camel milk or meat that
has not been properly cooked. There is growing evidence
that contact with live camels or meat is causing MERS.
The corresponding content on the Wikipedia is in
a section labeled as “Transmission”. Section ti-
tles at the topmost level in a category might not be
relevant to all the articles. Instead of using a top-
down approach of traversing the category-graph,
we can also use a bottom-up approach where we
learn from all the categories that an article be-
longs to. For example, the article on MERS be-
longs to two categories: Viral respiratory tract in-
fection and Zoonoses. Training using all the cat-
egories will allow context-driven section identifi-
cation. Most frequent sections at a higher level in
the category graph might not always be relevant to
all the articles within a category.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99996065">
In this work, we presented WikiKreator that
can generate content automatically to improve
Wikipedia stubs. Our technique employes a topic-
model based text classifier that assigns web ex-
cerpts into various sections on an article. The
excerpts are summarized using a novel abstrac-
tive summarization technique that maximizes in-
formativeness and linguistic quality of the gen-
erated summary. Our experiments reveal that
WikiKreator is capable of generating well-formed
informative content. The summarization step en-
sures that we avoid any copyright violation issues.
The ILP based sentence generation strategy en-
sures that we generate novel content by synthesiz-
ing information from multiple sources and thereby
improve content selection. In future, we plan to
cluster related sections using semantic relatedness
measures. We also plan to estimate reliabilities of
sources to retrieve information only from reliable
sources.
</bodyText>
<footnote confidence="0.924761">
12https://en.wikipedia.org/wiki/Middle_
East_respiratory_syndrome
</footnote>
<page confidence="0.997759">
875
</page>
<sectionHeader confidence="0.996202" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999704452830188">
Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al.
1999. Modern information retrieval, volume 463.
ACM press New York.
Siddhartha Banerjee, Cornelia Caragea, and Prasenjit
Mitra. 2014. Playscript classification and auto-
matic wikipedia play articles generation. In Pattern
Recognition (ICPR), 2014 22nd International Con-
ference on, pages 3630–3635, Aug.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.
Mohamed Amine Chatti, Darko Dugoija, Hendrik
Thus, and Ulrik Schroeder. 2014. Learner mod-
eling in academic networks. In Advanced Learn-
ing Technologies (ICALT), 2014 IEEE 14th Interna-
tional Conference on, pages 117–121. IEEE.
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression an integer program-
ming approach. In Proceedings of the COL-
ING/ACL on Main conference poster sessions, pages
144–151. Association for Computational Linguis-
tics.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273–297.
Vipul Dalal and Latesh Malik. 2013. A survey of
extractive and abstractive text summarization tech-
niques. In Emerging Trends in Engineering and
Technology (ICETET), 2013 6th International Con-
ference on, pages 109–110. IEEE.
G¨unes Erkan and Dragomir R Radev. 2004.
Lexrank: Graph-based lexical centrality as salience
in text summarization. J. Artif. Intell. Res.(JAIR),
22(1):457–479.
Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 177–185. As-
sociation for Computational Linguistics.
Katja Filippova. 2010. Multi-sentence compression:
Finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 322–330. Association
for Computational Linguistics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In IJCAI, vol-
ume 7, pages 1606–1611.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel
corpora for text-to-text generation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1168–1179. Asso-
ciation for Computational Linguistics.
Shima Gerani, Yashar Mehdad, Giuseppe Carenini,
T. Raymond Ng, and Bita Nejat. 2014. Abstractive
summarization of product reviews using discourse
structure. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1602–1613. Association for Com-
putational Linguistics.
Inc. Gurobi Optimization. 2015. Gurobi optimizer ref-
erence manual.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10–
18.
Christian Kohlsch¨utter, Peter Fankhauser, and Wolf-
gang Nejdl. 2010. Boilerplate detection using shal-
low text features. In Proceedings of the third ACM
international conference on Web search and data
mining, pages 441–450. ACM.
Peng Li, Yinglin Wang, and Jing Jiang. 2013. Au-
tomatically building templates for entity summary
construction. Information Processing &amp; Manage-
ment, 49(1):330–340.
Andy Liaw and Matthew Wiener. 2002. Classification
and regression by randomforest. R news, 2(3):18–
22.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81.
Andrew K McCallum. 2002. {MALLET: A Machine
Learning for Language Toolkit}.
Olena Medelyan, Ian H Witten, and David Milne.
2008. Topic indexing with wikipedia. In Proceed-
ings of the AAAI WikiAI workshop, pages 19–24.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. Association for Compu-
tational Linguistics.
Ani Nenkova, Sameer Maskey, and Yang Liu. 2011.
Automatic summarization. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Tutorial Abstracts of ACL
2011, page 3. Association for Computational Lin-
guistics.
Christina Sauper and Regina Barzilay. 2009. Auto-
matically generating wikipedia articles: A structure-
aware approach. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume
1, pages 208–216. Association for Computational
Linguistics.
</reference>
<page confidence="0.987752">
876
</page>
<reference confidence="0.995965538461538">
Fei Song and W Bruce Croft. 1999. A general lan-
guage model for information retrieval. In Proceed-
ings of the eighth international conference on In-
formation and knowledge management, pages 316–
321. ACM.
Wei Song, Yu Zhang, Ting Liu, and Sheng Li. 2010.
Bridging topic modeling and personalized search.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 1167–
1175. Association for Computational Linguistics.
Marilyn A Walker, Owen Rambow, and Monica Ro-
gati. 2001. Spot: A trainable sentence planner.
In Proceedings of the second meeting of the North
American Chapter of the Association for Computa-
tional Linguistics on Language technologies, pages
1–8. Association for Computational Linguistics.
Conglei Yao, Xu Jia, Sicong Shou, Shicong Feng, Feng
Zhou, and HongYan Liu. 2011. Autopedia: auto-
matic domain-independent wikipedia article genera-
tion. In Proceedings of the 20th international con-
ference companion on World wide web, pages 161–
162. ACM.
Torsten Zesch and Iryna Gurevych. 2007. Analy-
sis of the wikipedia category graph for nlp applica-
tions. In Proceedings of the TextGraphs-2 Workshop
(NAACL-HLT 2007), pages 1–8.
</reference>
<page confidence="0.998018">
877
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.144850">
<title confidence="0.995031">WikiKreator: Improving Wikipedia Stubs Automatically</title>
<author confidence="0.619721">Siddhartha The Pennsylvania State</author>
<affiliation confidence="0.951362">Information Sciences and</affiliation>
<address confidence="0.414339">University Park, PA,</address>
<email confidence="0.991237">sub253@ist.psu.edu</email>
<abstract confidence="0.9984424">Stubs on Wikipedia often lack comprehensive information. The huge cost of editing Wikipedia and the presence of only a limited number of active contributors curb the consistent growth of Wikipedia. In this work, we present WikiKreator, a system that is capable of generating content automatically to improve existing stubs on Wikipedia. The system has two components. First, a text classifier built using topic distribution vectors is used to assign content from the web to various sections on a Wikipedia article. Second, we propose a novel abstractive summarization technique based on an optimization framework that generates section-specific summaries for Wikipedia stubs. Experiments show that WikiKreator is capable of generating well-formed informative content. Further, automatically generated content from our system have been appended to Wikipedia stubs and the content has been retained successfully proving the effectiveness of our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<title>Modern information retrieval, volume 463.</title>
<date>1999</date>
<publisher>ACM press</publisher>
<location>New York.</location>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al. 1999. Modern information retrieval, volume 463. ACM press New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddhartha Banerjee</author>
<author>Cornelia Caragea</author>
<author>Prasenjit Mitra</author>
</authors>
<title>Playscript classification and automatic wikipedia play articles generation.</title>
<date>2014</date>
<booktitle>In Pattern Recognition (ICPR), 2014 22nd International Conference on,</booktitle>
<pages>3630--3635</pages>
<contexts>
<context position="2053" citStr="Banerjee et al., 2014" startWordPosition="295" endWordPosition="298"> an automatic Wikipedia content generator, which can generate a comprehensive overview on any topic using available information from the web and append the generated content to the stubs. Addition of automatically generated content can provide a useful start1https://en.wikipedia.org/wiki/ Wikipedia:Stub Prasenjit Mitra Qatar Computing Research Institute Hamad Bin Khalifa University Doha, Qatar pmitra@qf.org.qa ing point for contributors on Wikipedia, which can be improved upon later. Several approaches to automatically generate Wikipedia articles have been explored (Sauper and Barzilay, 2009; Banerjee et al., 2014; Yao et al., 2011). To the best of our knowledge, all the above mentioned methods identify information sources from the web using keywords and directly use the most relevant excerpts in the final article. Information from the web cannot be directly copied into Wikipedia due to copyright violation issues (Banerjee et al., 2014). Further, keyword search does not always satisfy information requirements (Baeza-Yates et al., 1999). To address the above-mentioned issues, we present WikiKreator – a system that can automatically generate content for Wikipedia stubs. First, WikiKreator does not operat</context>
<context position="35910" citStr="Banerjee et al., 2014" startWordPosition="5696" endWordPosition="5699"> inadequate due to potential loss of information. WikiKreator performs better than the extractive system on all the categories. Our ILPbased abstractive summarization system fuses and selects content from multiple sentences, thereby aggregating information successfully from multiple sources. In contrast, LexRank ‘extracts’ the top 5 sentences that results in some information loss. Analysis of Wikipedia Reviews: To compare our method with the other techniques, it is necessary to generate content and append to Wikipedia stubs using all the techniques. However, recent work on article generation (Banerjee et al., 2014) has already shown that content directly copied from web sources cannot be used on Wikipedia. Further, bots using copyrighted content might be banned and real-users would have to read sub-standard articles due to the internal tests we perform. Due to the above mentioned reasons, we appended content generated only using our abstractive summarization technique. We published content generated by WikiKreator on Wikipedia and appended the content to 40 randomly selected stubs. As can be seen from the Table 5, the content generated using our system was generally accepted by the reviewers. Half of th</context>
</contexts>
<marker>Banerjee, Caragea, Mitra, 2014</marker>
<rawString>Siddhartha Banerjee, Cornelia Caragea, and Prasenjit Mitra. 2014. Playscript classification and automatic wikipedia play articles generation. In Pattern Recognition (ICPR), 2014 22nd International Conference on, pages 3630–3635, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="5446" citStr="Blei et al., 2003" startWordPosition="824" endWordPosition="827">) prehensive Wikipedia article? Our proposed approach consists of two stages. First, a text classifier assigns content retrieved from the web into specific sections of the Wikipedia article. We train the classifier using a set of articles within the same category. Currently, we limit the system to learn and assign content into the 10 most frequent sections in any given category. The training set includes content from the most frequent sections as instances and their corresponding section titles as the class labels. We extract topic distribution vectors using Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and use the features to train a Random Forest (RF) Classifier (Liaw and Wiener, 2002). To gather web content relevant to the stub, we formulate queries and retrieve top 20 search results (pages) from Google. We use boilerplate detection (Kohlsch¨utter et al., 2010) to retain the important excerpts (text elements) from the pages. The RF classifier classifies the excerpts into one of the most frequent classes (section titles). Second, we develop a novel Integer Linear Programming (ILP) based abstractive summarization technique to generate text from the classified content. Previous work only inc</context>
<context position="11902" citStr="Blei et al., 2003" startWordPosition="1824" endWordPosition="1827">in C2 and C3 (the subcategories of C1). As shown in the Figure 2, we deploy a two stage process to generate content for a stub: [i] Content Retrieval and [ii] Content Summarization. In the first stage, our focus is to retrieve content that is relevant to the stub, say, 51 that belongs to C1. We extract all the articles that belong to C1 and the subcategories, namely, C2 and C3. A training set is created with the contents in the sections of the articles as instances and the section titles as the corresponding classes. Topic distribution vectors for each section content are generated using LDA (Blei et al., 2003). We train a Random Forest CA.Cares(.p, Arid®JA) &amp;Stub.(SI A C C A C A A S S S [i] Content Retrieval Extract Wikipedia Articles Classification Model (LDA – RFI Predicting Sections Query Generati�� E—pt9 nob Search B�ilerpl�te Rem�v�l �������� Excerpts Introductory Content s, : gest paths of Section s, sr: gest paths of Section s, s� : gest paths of Section s, Wikipedia Page [ii] Content Summarization For Each Section Path Selection – ILP model Final Summary for section Word-graph construction Excerpts 4 sentences Path Generation 869 (RF) classifier using the topic distribution vectors. As ment</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Amine Chatti</author>
<author>Darko Dugoija</author>
<author>Hendrik Thus</author>
<author>Ulrik Schroeder</author>
</authors>
<title>Learner modeling in academic networks.</title>
<date>2014</date>
<booktitle>In Advanced Learning Technologies (ICALT), 2014 IEEE 14th International Conference on,</booktitle>
<pages>117--121</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="16776" citStr="Chatti et al., 2014" startWordPosition="2606" endWordPosition="2609">e formulate search queries to retrieve web pages using a search engine. We extract multiple excerpts from the pages and then the RF classifier predicts the class (section label) for each excerpt. (i) Query Generation: To search the web, we formulate queries by combining the stub title and keyphrases extracted from the first sentence of the introductory content of the stub. The first sentence generally contains the most important keywords that represent the article. Focused queries increases relevance of extraction as well as helps in disambiguation of content. We use the topia term extractor (Chatti et al., 2014) to extract keyphrases. For example, the query generated for a stub on Hereditary hyperbilirubinemia is Hereditary hyperbilirubinemia bilirubin metabolic disorder where bilirubin metabolic disorder are the keyphrases generated from the first sentence of the stub from Wikipedia. The query is used to identify the top 20 URLs (search results) from Google9. (ii) Boilerplate removal: Web content from the search results obtained in the previous step re9http://www.google.com 870 quires cleaning to retain only the relevant information. Removal of irrelevant content is done using boilerplate detection </context>
</contexts>
<marker>Chatti, Dugoija, Thus, Schroeder, 2014</marker>
<rawString>Mohamed Amine Chatti, Darko Dugoija, Hendrik Thus, and Ulrik Schroeder. 2014. Learner modeling in academic networks. In Advanced Learning Technologies (ICALT), 2014 IEEE 14th International Conference on, pages 117–121. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Constraintbased sentence compression an integer programming approach.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>144--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24847" citStr="Clarke and Lapata, 2006" startWordPosition="3935" endWordPosition="3938">1wt−2). As can be seen from Equation (4), we combine the conditional probability of different sets of 3-grams (trigrams) in the sentence and averaged the value by L – the number of conditional probabilities computed. The LL(w1, w2, ... , wq) scores are negative; with higher magnitude implying lower importance. Therefore, in Equation (3), we take the reciprocal of the logarithmic value with smoothing to compute LQ(pi). In our experiments, we used a 3-gram model10 that is trained on the English Gigaword corpus. Trigram models have been successfully used in several text-to-text generation tasks (Clarke and Lapata, 2006; Filippova and Strube, 2008) earlier. ILP Formulation: To select the best paths, we combine all the above mentioned factors Iloc(pi), Iglob(pi) and linguistic quality LQ(pi) in an optimization framework. We maximize the following objective function: F (p1, ... ,pK) = EKi=1 T(pi) - Iloc(pi) - Iglob(pi) - LQ(pi) - pi 1 where K represents the total number of generated paths. Each pi represents a binary variable, that can be either 0 or 1, depending on whether the path is selected in the final summary or not. In addition, T (pi) – the number of tokens in a path, is included in the objective funct</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>James Clarke and Mirella Lapata. 2006. Constraintbased sentence compression an integer programming approach. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 144–151. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="30860" citStr="Cortes and Vapnik, 1995" startWordPosition="4913" endWordPosition="4916">aticians, respectively) in the Figure. We classify web excerpts using the best performing classifiers trained using the optimal number of topic features in each category. Classification performance: We use 10-fold cross validation to evaluate the accuracy of our classifier. According to the F-Scores, our classifier (LDA-RF) performs similarly in the categories on Diseases and US Software companies. However, the accuracy is lower in the American Mathematicians category. We also experimented with a baseline classifier, that is trained on TFIDF features (upto trigrams). A Support vector machine (Cortes and Vapnik, 1995) classifier obtained the best performance using the TFIDF features. The baseline system is referred to as SVM-WV. We experimented with several other combinations of classifiers; however, we show only the best performing systems using the LDA and TFIDF features. As can be seen from the Table 3, our classifier (LDARF) outperforms SVM-WV significantly in all the domains. SVM-WV performs better in the category on diseases than the other two categories and the performance is comparable to (LDA-RF). The diseases category has more uniformity in terms of the section titles, hence specific words or phr</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vipul Dalal</author>
<author>Latesh Malik</author>
</authors>
<title>A survey of extractive and abstractive text summarization techniques.</title>
<date>2013</date>
<booktitle>In Emerging Trends in Engineering and Technology (ICETET), 2013 6th International Conference on,</booktitle>
<pages>109--110</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3021" citStr="Dalal and Malik, 2013" startWordPosition="445" endWordPosition="448">d search does not always satisfy information requirements (Baeza-Yates et al., 1999). To address the above-mentioned issues, we present WikiKreator – a system that can automatically generate content for Wikipedia stubs. First, WikiKreator does not operate using keyword search. Instead, we use a classifier trained using topic distribution features to identify relevant content for the stub. Topic-distribution features are more effective than keyword search as they can identify relevant content based on word distributions (Song et al., 2010). Second, we propose a novel abstractive summarization (Dalal and Malik, 2013) technique to summarize content from multiple snippets of relevant information.2 Figure 1 shows a stub that we attempt to improve using WikiKreator. Generally, in stubs, only the introductory content is available; other sections (s1,..., sr) are absent. The stub also belongs to several categories (C1,C2, etc. in Figure) on Wikipedia. In this work, we address the following research question: Given the introductory content, the title of the stub and information on the categories - how can we transform the stub into a com2An example of our system’s output can be found here – https://en.wikipedia.</context>
</contexts>
<marker>Dalal, Malik, 2013</marker>
<rawString>Vipul Dalal and Latesh Malik. 2013. A survey of extractive and abstractive text summarization techniques. In Emerging Trends in Engineering and Technology (ICETET), 2013 6th International Conference on, pages 109–110. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Intell. Res.(JAIR),</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="33966" citStr="Erkan and Radev, 2004" startWordPosition="5395" endWordPosition="5398">ay (2009)11 that employs a perceptron learning framework to learn topic specific extractors. Queries devised using the conjunction of the document title and the section title were used to obtain excerpts from the web using a search engine, which were used in the perceptron model. In Perceptron, the most important sections in the category was determined using a bisectioning algorithm to identify clusters of similar sections. To understand the effectiveness of our abstractive summarizer, we design a system (Extractive) that uses an extractive summarization module. In Extractive, we use LexRank (Erkan and Radev, 2004) as the summarizer instead of our ILP based abstractive summarization model. We restrict the extractive summaries to 5 sentences for accurate comparison of both the systems. The same content was received as input from the classifier by the Extractive as well as our ILP-based system. As can be seen from the Table 4, the ROUGE scores obtained by WikiKreator is higher than that of the other comparable systems in all the categories. The higher ROUGE scores imply that WikiKreator is generally able to retrieve useful information from the web, synthesize them and present the important information in </context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Intell. Res.(JAIR), 22(1):457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Sentence fusion via dependency graph compression.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>177--185</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24876" citStr="Filippova and Strube, 2008" startWordPosition="3939" endWordPosition="3943">om Equation (4), we combine the conditional probability of different sets of 3-grams (trigrams) in the sentence and averaged the value by L – the number of conditional probabilities computed. The LL(w1, w2, ... , wq) scores are negative; with higher magnitude implying lower importance. Therefore, in Equation (3), we take the reciprocal of the logarithmic value with smoothing to compute LQ(pi). In our experiments, we used a 3-gram model10 that is trained on the English Gigaword corpus. Trigram models have been successfully used in several text-to-text generation tasks (Clarke and Lapata, 2006; Filippova and Strube, 2008) earlier. ILP Formulation: To select the best paths, we combine all the above mentioned factors Iloc(pi), Iglob(pi) and linguistic quality LQ(pi) in an optimization framework. We maximize the following objective function: F (p1, ... ,pK) = EKi=1 T(pi) - Iloc(pi) - Iglob(pi) - LQ(pi) - pi 1 where K represents the total number of generated paths. Each pi represents a binary variable, that can be either 0 or 1, depending on whether the path is selected in the final summary or not. In addition, T (pi) – the number of tokens in a path, is included in the objective function. The term 1 T (pi) normal</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Sentence fusion via dependency graph compression. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 177–185. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
</authors>
<title>Multi-sentence compression: Finding shortest paths in word graphs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>322--330</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6361" citStr="Filippova, 2010" startWordPosition="971" endWordPosition="972"> (text elements) from the pages. The RF classifier classifies the excerpts into one of the most frequent classes (section titles). Second, we develop a novel Integer Linear Programming (ILP) based abstractive summarization technique to generate text from the classified content. Previous work only included the most informative excerpt in the article (Sauper and Barzilay, 2009); in contrast, our abstractive summarization approach minimizes loss of information that should ideally be in an Wikipedia article by fusing content from several sentences. As shown in Figure 1, we construct a word-graph (Filippova, 2010) using all the sentences (WG1) assigned to a specific class (Epidemiology) by the classifier. Multiple paths (sentences) between the start and end nodes in the graph are generated (WG2). We represent the generated paths as variables in the ILP problem. The coefficients of each variable in the objective function of the ILP problem is obtained by combining the information score and the linguistic quality score of the path. We introduce several constraints into our ILP model. We limit the summary for each section to a maximum of 5 sentences. Further, we avoid redundant sentences in the summary th</context>
<context position="10006" citStr="Filippova, 2010" startWordPosition="1519" endWordPosition="1521"> Various abstractive approaches have been proposed till date (Nenkova et al., 2011). However, these methods suffer from severe deficiencies. Template-based summarization methods work well, but, it assumes prior domain knowledge (Li et al., 2013). Writing style across articles vary widely; hence learning templates automatically is difficult. In addition, such techniques require handcrafted rules for sentence realization (Gerani et al., 2014). Alternatively, we can use text-to-text generation (T2T) (Ganitkevitch et al., 2011) techniques. WikiKreator constructs a word-graph structure similar to (Filippova, 2010) using all the sentences that are assigned to a particular section by a text classifier. Multiple paths (sentences) from the graph are generated. WikiKreator selects few sentences from this set of paths using an optimization problem formulation that jointly maximizes the informaFigure 2: WikiKreator System Architecture: Content Retrieval and Content Summarization tiveness and readability of section-specific snippets and generates output that is informative, wellformed and readable. 3 Proposed Approach Figure 2 shows the system architecture of WikiKreator. We are required to generate content to</context>
<context position="18730" citStr="Filippova, 2010" startWordPosition="2909" endWordPosition="2910">the minimum confidence level, the excerpt is assigned to the class; otherwise, the excerpt is discarded. In the next step, we apply summarization on the excerpts assigned to each section. 3.2 Content Summarization To summarize content for Wikipedia effectively, we formulate an ILP problem to generate abstractive summaries for each section with the objective of maximizing linguistic quality and information content. Word-graph: A word-graph is constructed using all the sentences included in the excerpts assigned to a particular section. We used the same technique to construct the word-graph as (Filippova, 2010) where the nodes represent the words (along with parts-of-speech (POS)) and directed edges between the nodes are added if the words are adjacent in the input sentences. Each sentence is connected to dummy start and end nodes to mark the beginning and ending of the sentences. The sentences from the excerpts are added to the graph in an iterative fashion. Once the first sentence is added, words from the following sentences are mapped onto a node in the graph provided that they have the exact same word form and the same POS tag. Inclusion of POS information prevents ungrammatical mappings. The wo</context>
</contexts>
<marker>Filippova, 2010</marker>
<rawString>Katja Filippova. 2010. Multi-sentence compression: Finding shortest paths in word graphs. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 322–330. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In IJCAI,</booktitle>
<volume>7</volume>
<pages>1606--1611</pages>
<contexts>
<context position="8731" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1321" endWordPosition="1324"> Wikipedia articles. The results of our evaluation confirm the benefits of using abstractive summarization for content generation over approaches that do not use summarization. WikiKreator outperforms other comparable approaches significantly in terms of content selection. On ROUGE-1 scores, WikiKreator outperforms the perceptron-based baseline (Sauper and Barzilay, 2009) by ∼20%. We also analyze reviewer reactions, by appending content into several stubs on Wikipedia, most of which (∼77%) have been retained by reviewers. 2 Related Work Wikipedia has been used to compute semantic relatedness (Gabrilovich and Markovitch, 2007), index topics (Medelyan et al., 2008), etc. However, the problem of enhancing the content of a Wikipedia article has not been addressed adequately. Learning structures of templates from the Wikipedia articles have been attempted in the past (Sauper and Barzilay, 2009; Yao et al., 2011). Both these efforts use queries to extract excerpts from the web and the excerpts ranked as the most relevant are added into the article. However, as already pointed out, current standards of Wikipedia requires rewriting of web content to avoid copyright violation issues. To address the issue of copyright viola</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipediabased explicit semantic analysis. In IJCAI, volume 7, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Chris Callison-Burch</author>
<author>Courtney Napoles</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1168--1179</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Ganitkevitch, Callison-Burch, Napoles, Van Durme, 2011</marker>
<rawString>Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme. 2011. Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1168–1179. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shima Gerani</author>
<author>Yashar Mehdad</author>
<author>Giuseppe Carenini</author>
<author>T Raymond Ng</author>
<author>Bita Nejat</author>
</authors>
<title>Abstractive summarization of product reviews using discourse structure.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1602--1613</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9834" citStr="Gerani et al., 2014" startWordPosition="1494" endWordPosition="1497">ipedia requires rewriting of web content to avoid copyright violation issues. To address the issue of copyright violation, multi-document abstractive summarization is required. Various abstractive approaches have been proposed till date (Nenkova et al., 2011). However, these methods suffer from severe deficiencies. Template-based summarization methods work well, but, it assumes prior domain knowledge (Li et al., 2013). Writing style across articles vary widely; hence learning templates automatically is difficult. In addition, such techniques require handcrafted rules for sentence realization (Gerani et al., 2014). Alternatively, we can use text-to-text generation (T2T) (Ganitkevitch et al., 2011) techniques. WikiKreator constructs a word-graph structure similar to (Filippova, 2010) using all the sentences that are assigned to a particular section by a text classifier. Multiple paths (sentences) from the graph are generated. WikiKreator selects few sentences from this set of paths using an optimization problem formulation that jointly maximizes the informaFigure 2: WikiKreator System Architecture: Content Retrieval and Content Summarization tiveness and readability of section-specific snippets and gene</context>
</contexts>
<marker>Gerani, Mehdad, Carenini, Ng, Nejat, 2014</marker>
<rawString>Shima Gerani, Yashar Mehdad, Giuseppe Carenini, T. Raymond Ng, and Bita Nejat. 2014. Abstractive summarization of product reviews using discourse structure. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1602–1613. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gurobi Optimization</author>
</authors>
<date>2015</date>
<note>Gurobi optimizer reference manual.</note>
<marker>Optimization, 2015</marker>
<rawString>Inc. Gurobi Optimization. 2015. Gurobi optimizer reference manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<pages>18</pages>
<contexts>
<context position="29356" citStr="Hall et al., 2009" startWordPosition="4664" endWordPosition="4667">the most frequent sections in each category. As can be seen from the table, the number of instances is higher than the number of articles only in case of the category on diseases. This implies that there are generally more common sections in the diseases category than the other categories. In each category, the content from only the most frequent sections were used to generate a topic model. The topic model is further used to infer topic distribution vectors from the training instances. We used the MALLET toolkit (McCallum, 2002) for generating topic distribution vectors and the WEKA package (Hall et al., 2009) for the classification tasks. Optimal number of topics: The LDA model requires a pre-defined number of topics. We experiment with several values of the number of topics ranging from 10 to 100. The topic distribution features of the content of the instances are used to train a Random Forest Classifier with the corresponding section titles as the class labels. As can be seen in the Figure 3, the classification performance varies across domains as well as on the number of topics. The optimal number of topics based on the dataset are marked in blue cir(7) pi + pi, ≤ 1 if sim(pi,pi,) ≥ 0.5. ∀i, i&apos;</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The weka data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Kohlsch¨utter</author>
<author>Peter Fankhauser</author>
<author>Wolfgang Nejdl</author>
</authors>
<title>Boilerplate detection using shallow text features.</title>
<date>2010</date>
<booktitle>In Proceedings of the third ACM international conference on Web search and data mining,</booktitle>
<pages>441--450</pages>
<publisher>ACM.</publisher>
<marker>Kohlsch¨utter, Fankhauser, Nejdl, 2010</marker>
<rawString>Christian Kohlsch¨utter, Peter Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate detection using shallow text features. In Proceedings of the third ACM international conference on Web search and data mining, pages 441–450. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Yinglin Wang</author>
<author>Jing Jiang</author>
</authors>
<title>Automatically building templates for entity summary construction.</title>
<date>2013</date>
<journal>Information Processing &amp; Management,</journal>
<volume>49</volume>
<issue>1</issue>
<contexts>
<context position="9635" citStr="Li et al., 2013" startWordPosition="1464" endWordPosition="1467"> Both these efforts use queries to extract excerpts from the web and the excerpts ranked as the most relevant are added into the article. However, as already pointed out, current standards of Wikipedia requires rewriting of web content to avoid copyright violation issues. To address the issue of copyright violation, multi-document abstractive summarization is required. Various abstractive approaches have been proposed till date (Nenkova et al., 2011). However, these methods suffer from severe deficiencies. Template-based summarization methods work well, but, it assumes prior domain knowledge (Li et al., 2013). Writing style across articles vary widely; hence learning templates automatically is difficult. In addition, such techniques require handcrafted rules for sentence realization (Gerani et al., 2014). Alternatively, we can use text-to-text generation (T2T) (Ganitkevitch et al., 2011) techniques. WikiKreator constructs a word-graph structure similar to (Filippova, 2010) using all the sentences that are assigned to a particular section by a text classifier. Multiple paths (sentences) from the graph are generated. WikiKreator selects few sentences from this set of paths using an optimization prob</context>
</contexts>
<marker>Li, Wang, Jiang, 2013</marker>
<rawString>Peng Li, Yinglin Wang, and Jing Jiang. 2013. Automatically building templates for entity summary construction. Information Processing &amp; Management, 49(1):330–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andy Liaw</author>
<author>Matthew Wiener</author>
</authors>
<title>Classification and regression by randomforest.</title>
<date>2002</date>
<journal>R news,</journal>
<volume>2</volume>
<issue>3</issue>
<pages>22</pages>
<contexts>
<context position="5532" citStr="Liaw and Wiener, 2002" startWordPosition="840" endWordPosition="843">t, a text classifier assigns content retrieved from the web into specific sections of the Wikipedia article. We train the classifier using a set of articles within the same category. Currently, we limit the system to learn and assign content into the 10 most frequent sections in any given category. The training set includes content from the most frequent sections as instances and their corresponding section titles as the class labels. We extract topic distribution vectors using Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and use the features to train a Random Forest (RF) Classifier (Liaw and Wiener, 2002). To gather web content relevant to the stub, we formulate queries and retrieve top 20 search results (pages) from Google. We use boilerplate detection (Kohlsch¨utter et al., 2010) to retain the important excerpts (text elements) from the pages. The RF classifier classifies the excerpts into one of the most frequent classes (section titles). Second, we develop a novel Integer Linear Programming (ILP) based abstractive summarization technique to generate text from the classified content. Previous work only included the most informative excerpt in the article (Sauper and Barzilay, 2009); in cont</context>
</contexts>
<marker>Liaw, Wiener, 2002</marker>
<rawString>Andy Liaw and Matthew Wiener. 2002. Classification and regression by randomforest. R news, 2(3):18– 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="8032" citStr="Lin, 2004" startWordPosition="1219" endWordPosition="1220">ive articles. Further, we address the issue of abstractive text summarization for Wikipedia content generation. We evaluate our approach by generating articles in three different categories: Diseases and Disorders3, American Mathematicians4 and Software companies of the United States5. Our LDA-based classi3https://en.wikipedia.org/wiki/Category: Diseases_and_disorders 4https://en.wikipedia.org/wiki/Category: American_mathematicians 5https://en.wikipedia.org/wiki/Category: Software_companies_of_the_United_States 868 fier outperforms a TFIDF-based classifier in all the categories. We use ROUGE (Lin, 2004) to compare content generated by WikiKreator and the corresponding Wikipedia articles. The results of our evaluation confirm the benefits of using abstractive summarization for content generation over approaches that do not use summarization. WikiKreator outperforms other comparable approaches significantly in terms of content selection. On ROUGE-1 scores, WikiKreator outperforms the perceptron-based baseline (Sauper and Barzilay, 2009) by ∼20%. We also analyze reviewer reactions, by appending content into several stubs on Wikipedia, most of which (∼77%) have been retained by reviewers. 2 Rela</context>
<context position="33094" citStr="Lin, 2004" startWordPosition="5260" endWordPosition="5261">Diseases and Disorders Perceptron 0.411 0.197 Extractive 0.473 0.232 WikiKreator 0.521 0.321 US Software companies Perceptron 0.421 0.228 Extractive 0.484 0.257 Table 4: ROUGE-1 and 2 Recall values – Comparing system generated articles to model articles class during classification. Content Selection Evaluation: To evaluate the effectiveness of our content generation process, we generated the content of 500 randomly selected articles that already exist on Wikipedia in each of the categories. We compare WikiKreator’s output against the current content of those articles on Wikipedia using ROUGE (Lin, 2004). ROUGE matches N-gram sequences that exist in both the system generated articles and the original Wikipedia articles (gold standard). We also compare WikiKreator’s output with an existing Wikipedia generation system [Perceptron] of Sauper and Barzilay (2009)11 that employs a perceptron learning framework to learn topic specific extractors. Queries devised using the conjunction of the document title and the section title were used to obtain excerpts from the web using a search engine, which were used in the perceptron model. In Perceptron, the most important sections in the category was determ</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
</authors>
<title>{MALLET: A Machine Learning for Language Toolkit}.</title>
<date>2002</date>
<contexts>
<context position="29273" citStr="McCallum, 2002" startWordPosition="4651" endWordPosition="4653">f instances are also shown. The number of instances denotes the total number of the most frequent sections in each category. As can be seen from the table, the number of instances is higher than the number of articles only in case of the category on diseases. This implies that there are generally more common sections in the diseases category than the other categories. In each category, the content from only the most frequent sections were used to generate a topic model. The topic model is further used to infer topic distribution vectors from the training instances. We used the MALLET toolkit (McCallum, 2002) for generating topic distribution vectors and the WEKA package (Hall et al., 2009) for the classification tasks. Optimal number of topics: The LDA model requires a pre-defined number of topics. We experiment with several values of the number of topics ranging from 10 to 100. The topic distribution features of the content of the instances are used to train a Random Forest Classifier with the corresponding section titles as the class labels. As can be seen in the Figure 3, the classification performance varies across domains as well as on the number of topics. The optimal number of topics based</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew K McCallum. 2002. {MALLET: A Machine Learning for Language Toolkit}.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Medelyan</author>
<author>Ian H Witten</author>
<author>David Milne</author>
</authors>
<title>Topic indexing with wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the AAAI WikiAI workshop,</booktitle>
<pages>pages</pages>
<contexts>
<context position="8769" citStr="Medelyan et al., 2008" startWordPosition="1328" endWordPosition="1331">n confirm the benefits of using abstractive summarization for content generation over approaches that do not use summarization. WikiKreator outperforms other comparable approaches significantly in terms of content selection. On ROUGE-1 scores, WikiKreator outperforms the perceptron-based baseline (Sauper and Barzilay, 2009) by ∼20%. We also analyze reviewer reactions, by appending content into several stubs on Wikipedia, most of which (∼77%) have been retained by reviewers. 2 Related Work Wikipedia has been used to compute semantic relatedness (Gabrilovich and Markovitch, 2007), index topics (Medelyan et al., 2008), etc. However, the problem of enhancing the content of a Wikipedia article has not been addressed adequately. Learning structures of templates from the Wikipedia articles have been attempted in the past (Sauper and Barzilay, 2009; Yao et al., 2011). Both these efforts use queries to extract excerpts from the web and the excerpts ranked as the most relevant are added into the article. However, as already pointed out, current standards of Wikipedia requires rewriting of web content to avoid copyright violation issues. To address the issue of copyright violation, multi-document abstractive summa</context>
</contexts>
<marker>Medelyan, Witten, Milne, 2008</marker>
<rawString>Olena Medelyan, Ian H Witten, and David Milne. 2008. Topic indexing with wikipedia. In Proceedings of the AAAI WikiAI workshop, pages 19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts. Association for Computational Linguistics.</title>
<date>2004</date>
<contexts>
<context position="22677" citStr="Mihalcea and Tarau, 2004" startWordPosition="3566" endWordPosition="3569">ath should be relevant to the central topic of the article; this relevance is tackled using Iglob(pi). Iloc(pi) models the importance of a sentence among several possible sentences that are generated from the word-graph. Linguistic quality (LQ(pi)) is computed using a trigram language model (Song and Croft, 1999) that assigns a logarithmic score of probabilities of occurrences of three word sequences in the sentences. Local Informativeness: In principle, we can use any existing method that computes sentence importance to account for Local Informativeness. In our model, we use TextRank scores (Mihalcea and Tarau, 2004) to generate an importance value of each path. TextRank creates a graph of words from the sentences. The score of each node in the graph is calculated as shown in Equation (1): S(Vi) = (1 − d) + d x EV .d&apos; Vi wji w. S(Vi) jc7( ) �Vk ∈adj(Vi) 7k (1) where Vi represents the words and adj(Vi) denotes the adjacent nodes of Vi. Setting d to 0.80 in our experiments provided the best content selection results. The computation convergences to return final word importance scores. The informativeness score of a path Iloc(pi) is obtained by adding the importance scores of the individual words in the path</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Sameer Maskey</author>
<author>Yang Liu</author>
</authors>
<title>Automatic summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts of ACL 2011,</booktitle>
<pages>page</pages>
<contexts>
<context position="9473" citStr="Nenkova et al., 2011" startWordPosition="1439" endWordPosition="1442"> been addressed adequately. Learning structures of templates from the Wikipedia articles have been attempted in the past (Sauper and Barzilay, 2009; Yao et al., 2011). Both these efforts use queries to extract excerpts from the web and the excerpts ranked as the most relevant are added into the article. However, as already pointed out, current standards of Wikipedia requires rewriting of web content to avoid copyright violation issues. To address the issue of copyright violation, multi-document abstractive summarization is required. Various abstractive approaches have been proposed till date (Nenkova et al., 2011). However, these methods suffer from severe deficiencies. Template-based summarization methods work well, but, it assumes prior domain knowledge (Li et al., 2013). Writing style across articles vary widely; hence learning templates automatically is difficult. In addition, such techniques require handcrafted rules for sentence realization (Gerani et al., 2014). Alternatively, we can use text-to-text generation (T2T) (Ganitkevitch et al., 2011) techniques. WikiKreator constructs a word-graph structure similar to (Filippova, 2010) using all the sentences that are assigned to a particular section </context>
</contexts>
<marker>Nenkova, Maskey, Liu, 2011</marker>
<rawString>Ani Nenkova, Sameer Maskey, and Yang Liu. 2011. Automatic summarization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts of ACL 2011, page 3. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Sauper</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatically generating wikipedia articles: A structureaware approach.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>208--216</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2030" citStr="Sauper and Barzilay, 2009" startWordPosition="291" endWordPosition="294">ly, we would like to create an automatic Wikipedia content generator, which can generate a comprehensive overview on any topic using available information from the web and append the generated content to the stubs. Addition of automatically generated content can provide a useful start1https://en.wikipedia.org/wiki/ Wikipedia:Stub Prasenjit Mitra Qatar Computing Research Institute Hamad Bin Khalifa University Doha, Qatar pmitra@qf.org.qa ing point for contributors on Wikipedia, which can be improved upon later. Several approaches to automatically generate Wikipedia articles have been explored (Sauper and Barzilay, 2009; Banerjee et al., 2014; Yao et al., 2011). To the best of our knowledge, all the above mentioned methods identify information sources from the web using keywords and directly use the most relevant excerpts in the final article. Information from the web cannot be directly copied into Wikipedia due to copyright violation issues (Banerjee et al., 2014). Further, keyword search does not always satisfy information requirements (Baeza-Yates et al., 1999). To address the above-mentioned issues, we present WikiKreator – a system that can automatically generate content for Wikipedia stubs. First, Wiki</context>
<context position="6123" citStr="Sauper and Barzilay, 2009" startWordPosition="933" endWordPosition="936"> Classifier (Liaw and Wiener, 2002). To gather web content relevant to the stub, we formulate queries and retrieve top 20 search results (pages) from Google. We use boilerplate detection (Kohlsch¨utter et al., 2010) to retain the important excerpts (text elements) from the pages. The RF classifier classifies the excerpts into one of the most frequent classes (section titles). Second, we develop a novel Integer Linear Programming (ILP) based abstractive summarization technique to generate text from the classified content. Previous work only included the most informative excerpt in the article (Sauper and Barzilay, 2009); in contrast, our abstractive summarization approach minimizes loss of information that should ideally be in an Wikipedia article by fusing content from several sentences. As shown in Figure 1, we construct a word-graph (Filippova, 2010) using all the sentences (WG1) assigned to a specific class (Epidemiology) by the classifier. Multiple paths (sentences) between the start and end nodes in the graph are generated (WG2). We represent the generated paths as variables in the ILP problem. The coefficients of each variable in the objective function of the ILP problem is obtained by combining the i</context>
<context position="8472" citStr="Sauper and Barzilay, 2009" startWordPosition="1279" endWordPosition="1282">hematicians 5https://en.wikipedia.org/wiki/Category: Software_companies_of_the_United_States 868 fier outperforms a TFIDF-based classifier in all the categories. We use ROUGE (Lin, 2004) to compare content generated by WikiKreator and the corresponding Wikipedia articles. The results of our evaluation confirm the benefits of using abstractive summarization for content generation over approaches that do not use summarization. WikiKreator outperforms other comparable approaches significantly in terms of content selection. On ROUGE-1 scores, WikiKreator outperforms the perceptron-based baseline (Sauper and Barzilay, 2009) by ∼20%. We also analyze reviewer reactions, by appending content into several stubs on Wikipedia, most of which (∼77%) have been retained by reviewers. 2 Related Work Wikipedia has been used to compute semantic relatedness (Gabrilovich and Markovitch, 2007), index topics (Medelyan et al., 2008), etc. However, the problem of enhancing the content of a Wikipedia article has not been addressed adequately. Learning structures of templates from the Wikipedia articles have been attempted in the past (Sauper and Barzilay, 2009; Yao et al., 2011). Both these efforts use queries to extract excerpts f</context>
<context position="33353" citStr="Sauper and Barzilay (2009)" startWordPosition="5295" endWordPosition="5298">l articles class during classification. Content Selection Evaluation: To evaluate the effectiveness of our content generation process, we generated the content of 500 randomly selected articles that already exist on Wikipedia in each of the categories. We compare WikiKreator’s output against the current content of those articles on Wikipedia using ROUGE (Lin, 2004). ROUGE matches N-gram sequences that exist in both the system generated articles and the original Wikipedia articles (gold standard). We also compare WikiKreator’s output with an existing Wikipedia generation system [Perceptron] of Sauper and Barzilay (2009)11 that employs a perceptron learning framework to learn topic specific extractors. Queries devised using the conjunction of the document title and the section title were used to obtain excerpts from the web using a search engine, which were used in the perceptron model. In Perceptron, the most important sections in the category was determined using a bisectioning algorithm to identify clusters of similar sections. To understand the effectiveness of our abstractive summarizer, we design a system (Extractive) that uses an extractive summarization module. In Extractive, we use LexRank (Erkan and</context>
</contexts>
<marker>Sauper, Barzilay, 2009</marker>
<rawString>Christina Sauper and Regina Barzilay. 2009. Automatically generating wikipedia articles: A structureaware approach. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 208–216. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Song</author>
<author>W Bruce Croft</author>
</authors>
<title>A general language model for information retrieval.</title>
<date>1999</date>
<booktitle>In Proceedings of the eighth international conference on Information and knowledge management,</booktitle>
<pages>316--321</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="22366" citStr="Song and Croft, 1999" startWordPosition="3518" endWordPosition="3521">erated paths in between the dummy nodes from the word-graph. Let pi denote each path obtained from the word-graph. We introduce three different factors to judge the relevance of 871 a path – Local informativeness (Iloc(pi)), Global informativeness (Iglob(pi)) and Linguistic quality (LQ(pi)). Any sentence path should be relevant to the central topic of the article; this relevance is tackled using Iglob(pi). Iloc(pi) models the importance of a sentence among several possible sentences that are generated from the word-graph. Linguistic quality (LQ(pi)) is computed using a trigram language model (Song and Croft, 1999) that assigns a logarithmic score of probabilities of occurrences of three word sequences in the sentences. Local Informativeness: In principle, we can use any existing method that computes sentence importance to account for Local Informativeness. In our model, we use TextRank scores (Mihalcea and Tarau, 2004) to generate an importance value of each path. TextRank creates a graph of words from the sentences. The score of each node in the graph is calculated as shown in Equation (1): S(Vi) = (1 − d) + d x EV .d&apos; Vi wji w. S(Vi) jc7( ) �Vk ∈adj(Vi) 7k (1) where Vi represents the words and adj(Vi</context>
</contexts>
<marker>Song, Croft, 1999</marker>
<rawString>Fei Song and W Bruce Croft. 1999. A general language model for information retrieval. In Proceedings of the eighth international conference on Information and knowledge management, pages 316– 321. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Song</author>
<author>Yu Zhang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Bridging topic modeling and personalized search.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>1167--1175</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="2943" citStr="Song et al., 2010" startWordPosition="434" endWordPosition="437">due to copyright violation issues (Banerjee et al., 2014). Further, keyword search does not always satisfy information requirements (Baeza-Yates et al., 1999). To address the above-mentioned issues, we present WikiKreator – a system that can automatically generate content for Wikipedia stubs. First, WikiKreator does not operate using keyword search. Instead, we use a classifier trained using topic distribution features to identify relevant content for the stub. Topic-distribution features are more effective than keyword search as they can identify relevant content based on word distributions (Song et al., 2010). Second, we propose a novel abstractive summarization (Dalal and Malik, 2013) technique to summarize content from multiple snippets of relevant information.2 Figure 1 shows a stub that we attempt to improve using WikiKreator. Generally, in stubs, only the introductory content is available; other sections (s1,..., sr) are absent. The stub also belongs to several categories (C1,C2, etc. in Figure) on Wikipedia. In this work, we address the following research question: Given the introductory content, the title of the stub and information on the categories - how can we transform the stub into a c</context>
</contexts>
<marker>Song, Zhang, Liu, Li, 2010</marker>
<rawString>Wei Song, Yu Zhang, Ting Liu, and Sheng Li. 2010. Bridging topic modeling and personalized search. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 1167– 1175. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Owen Rambow</author>
<author>Monica Rogati</author>
</authors>
<title>Spot: A trainable sentence planner.</title>
<date>2001</date>
<booktitle>In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21661" citStr="Walker et al., 2001" startWordPosition="3412" endWordPosition="3415">ply stemming as our graph construction is based on words existing in the same form in multiple sentences. Similar to Filippova’s work, we set the minimum path length (in words) to eight to avoid incomplete sentences. Paths without verbs are discarded. The final set of generated paths after discarding the ineligible ones are used in the next step of summary generation. 3.2.1 ILP based Path Selection Our goal is to select paths that maximize the informativeness and linguistic quality of the generated summaries. To select the best multiple possible sentences, we apply an overgenerate and select (Walker et al., 2001) strategy. We formulate an optimization problem that ‘selects’ a few of the many generated paths in between the dummy nodes from the word-graph. Let pi denote each path obtained from the word-graph. We introduce three different factors to judge the relevance of 871 a path – Local informativeness (Iloc(pi)), Global informativeness (Iglob(pi)) and Linguistic quality (LQ(pi)). Any sentence path should be relevant to the central topic of the article; this relevance is tackled using Iglob(pi). Iloc(pi) models the importance of a sentence among several possible sentences that are generated from the </context>
</contexts>
<marker>Walker, Rambow, Rogati, 2001</marker>
<rawString>Marilyn A Walker, Owen Rambow, and Monica Rogati. 2001. Spot: A trainable sentence planner. In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Conglei Yao</author>
<author>Xu Jia</author>
<author>Sicong Shou</author>
<author>Shicong Feng</author>
<author>Feng Zhou</author>
<author>HongYan Liu</author>
</authors>
<title>Autopedia: automatic domain-independent wikipedia article generation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference companion on World wide web,</booktitle>
<pages>161--162</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2072" citStr="Yao et al., 2011" startWordPosition="299" endWordPosition="302"> content generator, which can generate a comprehensive overview on any topic using available information from the web and append the generated content to the stubs. Addition of automatically generated content can provide a useful start1https://en.wikipedia.org/wiki/ Wikipedia:Stub Prasenjit Mitra Qatar Computing Research Institute Hamad Bin Khalifa University Doha, Qatar pmitra@qf.org.qa ing point for contributors on Wikipedia, which can be improved upon later. Several approaches to automatically generate Wikipedia articles have been explored (Sauper and Barzilay, 2009; Banerjee et al., 2014; Yao et al., 2011). To the best of our knowledge, all the above mentioned methods identify information sources from the web using keywords and directly use the most relevant excerpts in the final article. Information from the web cannot be directly copied into Wikipedia due to copyright violation issues (Banerjee et al., 2014). Further, keyword search does not always satisfy information requirements (Baeza-Yates et al., 1999). To address the above-mentioned issues, we present WikiKreator – a system that can automatically generate content for Wikipedia stubs. First, WikiKreator does not operate using keyword sea</context>
<context position="9018" citStr="Yao et al., 2011" startWordPosition="1369" endWordPosition="1372">r outperforms the perceptron-based baseline (Sauper and Barzilay, 2009) by ∼20%. We also analyze reviewer reactions, by appending content into several stubs on Wikipedia, most of which (∼77%) have been retained by reviewers. 2 Related Work Wikipedia has been used to compute semantic relatedness (Gabrilovich and Markovitch, 2007), index topics (Medelyan et al., 2008), etc. However, the problem of enhancing the content of a Wikipedia article has not been addressed adequately. Learning structures of templates from the Wikipedia articles have been attempted in the past (Sauper and Barzilay, 2009; Yao et al., 2011). Both these efforts use queries to extract excerpts from the web and the excerpts ranked as the most relevant are added into the article. However, as already pointed out, current standards of Wikipedia requires rewriting of web content to avoid copyright violation issues. To address the issue of copyright violation, multi-document abstractive summarization is required. Various abstractive approaches have been proposed till date (Nenkova et al., 2011). However, these methods suffer from severe deficiencies. Template-based summarization methods work well, but, it assumes prior domain knowledge </context>
</contexts>
<marker>Yao, Jia, Shou, Feng, Zhou, Liu, 2011</marker>
<rawString>Conglei Yao, Xu Jia, Sicong Shou, Shicong Feng, Feng Zhou, and HongYan Liu. 2011. Autopedia: automatic domain-independent wikipedia article generation. In Proceedings of the 20th international conference companion on World wide web, pages 161– 162. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Analysis of the wikipedia category graph for nlp applications.</title>
<date>2007</date>
<booktitle>In Proceedings of the TextGraphs-2 Workshop (NAACL-HLT</booktitle>
<pages>1--8</pages>
<contexts>
<context position="10841" citStr="Zesch and Gurevych, 2007" startWordPosition="1642" endWordPosition="1645">n optimization problem formulation that jointly maximizes the informaFigure 2: WikiKreator System Architecture: Content Retrieval and Content Summarization tiveness and readability of section-specific snippets and generates output that is informative, wellformed and readable. 3 Proposed Approach Figure 2 shows the system architecture of WikiKreator. We are required to generate content to populate sections of the stubs (51, 52, etc.) that belong to category C1. Categories on Wikipedia group together pages on similar subjects. Hence, categories characterize Wikipedia articles surprisingly well (Zesch and Gurevych, 2007). Naturally, we leverage knowledge existing in the categories to build our text classifier. To learn category specific templates, the system should learn from articles contained within the same or similar categories. WikiKreator learns category-specific templates using all the articles that can be reached using a top-down approach from the particular category. For example, in addition to C1, WikiKreator also learns templates from articles in C2 and C3 (the subcategories of C1). As shown in the Figure 2, we deploy a two stage process to generate content for a stub: [i] Content Retrieval and [ii</context>
</contexts>
<marker>Zesch, Gurevych, 2007</marker>
<rawString>Torsten Zesch and Iryna Gurevych. 2007. Analysis of the wikipedia category graph for nlp applications. In Proceedings of the TextGraphs-2 Workshop (NAACL-HLT 2007), pages 1–8.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>