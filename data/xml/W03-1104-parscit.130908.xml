<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9971">
A Differential LSI Method for Document Classification
</title>
<author confidence="0.998493">
Liang Chen
</author>
<affiliation confidence="0.997472">
Computer Science Department
University of Northern British Columbia
</affiliation>
<address confidence="0.707955">
Prince George, BC, Canada V2N 4Z9
</address>
<email confidence="0.99324">
chenl@unbc.ca
</email>
<author confidence="0.561847">
Naoyuki Tokuda
</author>
<affiliation confidence="0.386908">
R &amp; D Center, Sunflare Company
</affiliation>
<address confidence="0.744955">
Shinjuku-Hirose Bldg., 4-7 Yotsuya
Sinjuku-ku, Tokyo, Japan 160-0004
</address>
<email confidence="0.990342">
tokuda n@sunflare.co.jp
</email>
<author confidence="0.987095">
Akira Nagai
</author>
<affiliation confidence="0.9963815">
Advanced Media Network Center
Utsunomiya University
</affiliation>
<address confidence="0.528334">
Utsunomiya, Tochigi, Japan 321-8585
</address>
<email confidence="0.995319">
anagai@cc.utsunomiya-u.ac.jp
</email>
<sectionHeader confidence="0.995595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960818181818">
We have developed an effective prob-
abilistic classifier for document classi-
fication by introducing the concept of
the differential document vectors and
DLSI (differential latent semantics index)
spaces. A simple posteriori calculation
using the intra- and extra-document statis-
tics demonstrates the advantage of the
DLSI space-based probabilistic classifier
over the popularly used LSI space-based
classifier in classification performance.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958545454546">
This paper introduces a new efficient supervised
document classification procedure, whereby given a
number of labeled documents preclassified into a fi-
nite number of appropriate clusters in the database,
the classifier developed will select and classify any
of new documents introduced into an appropriate
cluster within the learning stage.
The vector space model is widely used in docu-
ment classification, where each document is repre-
sented as a vector of terms. To represent a doc-
ument by a document vector, we assign weights
to its components usually evaluating the frequency
of occurrences of the corresponding terms. Then
the standard pattern recognition and machine learn-
ing methods are employed for document classifica-
tion(Li et al., 1991; Farkas, 1994; Svingen, 1997;
Hyotyniemi, 1996; Merkl, 1998; Benkhalifa et al.,
1999; Iwayama and Tokunaga, 1995; Lam and Low,
1997; Nigam et al., 2000).
In view of the inherent flexibility imbedded within
any natural language, a staggering number of dimen-
sions seem required to represent the featuring space
of any practical document comprising the huge num-
ber of terms used. If a speedy classification algo-
rithm can be developed (Sch¨utze and Silverstein,
1997), the first problem to be resolved is the dimen-
sionality reduction scheme enabling the documents’
term projection onto a smaller subspace.
Like an eigen-decomposition method extensively
used in image processing and image recognition
(Sirovich and Kirby, 1987; Turk and Pentland,
1991), the Latent Semantic Indexing (LSI) method
has proved to be a most efficient method for the di-
mensionality reduction scheme in document analy-
sis and extraction, providing a powerful tool for the
classifier (Sch¨utze and Silverstein, 1997) when in-
troduced into document retrieval with a good per-
formance confirmed by empirical studies (Deer-
wester et al., 1990; Berry et al., 1999; Berry et
al., 1995).The LSI method has also demonstrated its
efficiency for automated cross-language document
retrieval in which no query translation is required
(Littman et al., 1998).
In this paper, we will show that exploiting both
of the distances to, and the projections onto, the
LSI space improves the performance as well as the
robustness of the document classifier. To do this,
we introduce, as the major vector space, the differ-
ential LSI (or DLSI) space which is formed from
the differences between normalized intra- and extra-
document vectors and normalized centroid vectors
of clusters where the intra- and extra-document
refers to the documents included within or outside of
the given cluster respectively. The new classifier sets
up a Baysian posteriori probability function for the
differential document vectors based on their projec-
tions on DLSI space and their distances to the DLSI
space, the document category with a highest proba-
bility is then selected. A similar approach is taken
by Moghaddam and Pentland for image recognition
(Moghaddam and Pentland, 1997; Moghaddam et
al., 1998).
We may summarize the specific features intro-
duced into the new document classification scheme
based on the concept of the differential document
vector and the DLSI vectors:
</bodyText>
<listItem confidence="0.984640380952381">
1. Exploiting the characteristic distance of the dif-
ferential document vector to the DLSI space
and the projection of the differential document
onto the DLSI space, which we believe to de-
note the differences in word usage between the
document and a cluster’s centroid vector, the
differential document vector is capable of cap-
turing the relation between the particular docu-
ment and the cluster.
2. A major problem of context sensitive seman-
tic grammar of natural language related to syn-
onymy and polysemy can be dampened by the
major space projection method endowed in the
LSIs used.
3. A maximum for the posteriori likelihood func-
tion making use of the projection of differen-
tial document vector onto the DLSI space and
the distance to the DLSI space provides a con-
sistent computational scheme in evaluating the
degree of reliability of the document belonging
to the cluster.
</listItem>
<bodyText confidence="0.9928562">
The rest of the paper is arranged as follows: Sec-
tion 2 will describe the main algorithm for setting up
the DLSI-based classifier. A simple example is com-
puted for comparison with the results by the stan-
dard LSI based classifier in Section 3. The conclu-
sion is given in Section 4.
.
For each document in the collection, we assign
each of the terms with a real vector
, with , where is the local
weighting of the term in the document indicating
the significance of the term in the document, while
is a global weight of all the documents, which is
a parameter indicating the importance of the term
in representing the documents. Local weights could
be either raw occurrence counts, boolean, or loga-
rithms of occurrence counts. Global ones could be
no weighting (uniform), domain specific, or entropy
weighting. Both of the local and global weights are
thoroughly studied in the literatures (Raghavan and
Wong, 1986; Luhn, 1958; van Rijsbergen, 1979;
Salton, 1983; Salton, 1988; Lee et al., 1997), and
will not be discussed further in this paper. An exam-
ple will be given below:
where ,is the total number of times that
term appears in the collection, the number of
times the term appears in the document , and
the number of documents in the collection. The doc-
ument vector can be normalized
as
</bodyText>
<equation confidence="0.800773">
(1)
</equation>
<bodyText confidence="0.9214836">
The normalized centroid vector
of a cluster can be calcu-
lated in terms of the normalized vector as
, where
and
</bodyText>
<sectionHeader confidence="0.342703" genericHeader="method">
2 Main Algorithm
</sectionHeader>
<subsectionHeader confidence="0.583226">
2.1 Basic Concepts
</subsectionHeader>
<bodyText confidence="0.996262538461538">
A term is defined as a word or a phrase that appears
at least in two documents. We exclude the so-called
stop words such as “a”, “the” , ”of” and so forth.
Suppose we select and list the terms that appear in
the documents as
by the following formula:
is a mean vector of the member documents in the
cluster which are normalized as ; i.e.,
. We can always
take itself as a normalized vector of the cluster.
A differential document vector is defined as
where and are normalized document vec-
tors satisfying some criteria as given above.
A differential intra-document vector is the dif-
ferential document vector defined as , where
and are two normalized document vectors of
same cluster.
A differential extra-document vector is the
differential document vector defined as ,
where and are two normalized document vec-
tors of different clusters.
The differential term by intra- and extra-
document matrices and are respectively de-
fined as a matrix, each column of which comprise
a differential intra- and extra- document vector re-
spectively.
</bodyText>
<subsectionHeader confidence="0.99595">
2.2 The Posteriori Model
</subsectionHeader>
<bodyText confidence="0.982561621621621">
Any differential term by document -by- matrix
of , say, of rank , whether it
is a differential term by intra-document matrix
or a differential term by extra-document matrix
can be decomposed by SVD into a product of three
matrices: , such that (left singular
matrix) and (right singular matrix) are an -by-
and-by- unitary matrices respectively with the
firstcolumns of U and V being the eigenvectors of
and respectively. Here is called sin-
gular matrix expressed by diag ),
where are nonnegtive square roots of eigen values
of
) of , deleting
other terms; we similarly obtain the two new matri-
ces and by keeping the left most columns
of and respectively. The product of ,and
provide a reduced matrix of which ap-
proximately equals to .
How we choose an appropriate value of , a re-
duced degree of dimension from the original matrix,
depends on the type of applications. Generally we
choose for , and the cor-
responding is normally smaller for the differential
term by intra-document matrix than that for the dif-
ferential term by extra- document matrix, because
the differential term by extra-document matrix nor-
mally has more columns than the differential term
by intra-document matrix has.
Each of differential document vectorcould find
a projection on the dimensional fact space spanned
by the columns of . The projection can easily
be obtained by .
Noting that the mean of the differential intra-
(extra-) document vectors are approximately, we
may assume that the differential vectors formed fol-
lows a high-dimensional Gaussian distribution so
that the likelihood of any differential vector will
be given by
where , and is the covariance of
the distribution computed from the training set ex-
pressed .
Since constitutes the eigenvalues of , we
have , and thus we have
,
where .
Because is a diagonal matrix, can be repre-
sented by a simpler form as: .
It is most convenient to estimate it as
where . In practice, ( )
could be estimated by fitting a function (say, )
to the available ( ), or we could let
since we only need to compare the rela-
tive probability. Because the columns of are or-
thogonal vectors, could be estimated by
. Thus, the likelihood function
could be estimated by
(2)
where ,,
, and is the rank of matrix . In
, for and for .
The diagonal elements of are sorted in the
decreasing order of magnitude. To obtain a new
reduced matrix , we simply keep the k-by-k
leftmost-upper corner matrix (
practice, may be chosen as , and may be
substituted for. Note that in equation (2), the term
describes the projection of onto the DLSI
space, while approximates the distance from
to DLSI space.
When both and are computed,
the Baysian posteriori function can be computed as:
where is set to where is the number
of clusters in the database 1 while is set to
</bodyText>
<figure confidence="0.840196">
.
2.3 Algorithm
2.3.1 Setting up the DLSI Space-Based
Classifier
1. By preprocessing documents, identify terms ei-
ther of the word and noun phrase from stop
words.
</figure>
<listItem confidence="0.778726166666667">
2. Construct the system terms by setting up the
term list as well as the global weights.
3. Normalize the document vectors of all the col-
lected documents, as well as the centroid vec-
tors of each cluster.
4. Construct the differential term by intra-
</listItem>
<bodyText confidence="0.82610525">
document matrix , such that each of its
column is an differential intra-document vec-
tor2.
,
followed by the composition of
giving an approximate in terms
of an appropriate , then evaluate the likeli-
hood function:
</bodyText>
<footnote confidence="0.916363857142857">
(3)
1 can also be set to be an average number of recalls
divided by the number of clusters in the data base if we do not
require that the clusters are non-overlapped
2For a cluster with elements, we may include at most
differential intra-document vectors in to avoid the linear
dependency among columns
</footnote>
<bodyText confidence="0.8388204">
where ,,
, and is the rank of
matrix . In practice, may be set to ,
and to if bothand are suffi-
ciently large.
</bodyText>
<figure confidence="0.784367304347826">
6. Construct the term by extra- document matrix
, such that each of its column is an
extra- differential document vector.
, by exploiting the SVD al-
gorithm, into (
diag , then with a proper , de-
fine the
. We now define the likelihood function as,
7. Decompose
to approximate
, ,
,is the rank of
matrix . In practice, may be set to ,
and to if both and are suf-
ficiently large.
8. Define the posteriori function:
(5)
is set to where is the number
of clusters in the database and is set to
.
2.3.2 Automatic Classification by DLSI
Space-Based Classifier
1. A document vector is set up by generating the
</figure>
<bodyText confidence="0.991272166666667">
terms as well as their frequencies of occurrence
in the document, so that a normalized docu-
ment vector is obtained for the document
from equation (1).
For each of the clusters of the data base, repeat
the procedure of item 2-4 below.
</bodyText>
<listItem confidence="0.718241666666667">
2. Using the document to be classified, construct a
differential document vector , where
5. Decompose , by an SVD algorithm, into
</listItem>
<equation confidence="0.359754">
( diag
(4)
</equation>
<bodyText confidence="0.985309">
where
is the normalized vector giving the center or
centroid of the cluster.
</bodyText>
<listItem confidence="0.647228">
3. Calculate the intra-document likelihood func-
</listItem>
<bodyText confidence="0.94721">
tion , and calculate the extra- docu-
ment likelihood function for the doc-
ument.
</bodyText>
<listItem confidence="0.63587775">
4. Calculate the Bayesian posteriori probability
function .
5. Select the cluster having a largest as
the recall candidate.
</listItem>
<sectionHeader confidence="0.980768" genericHeader="method">
3 Examples and Comparison
</sectionHeader>
<subsectionHeader confidence="0.988778">
3.1 Problem Description
</subsectionHeader>
<bodyText confidence="0.937255">
We demonstrate our algorithm by means of numeri-
cal examples below. Suppose we have the following
</bodyText>
<listItem confidence="0.971242">
8 documents in the database:
: Algebra and Geometry Education System.
: The Software of Computing Machinery.
: Analysis and Elements of Geometry.
: Introduction to Modern Algebra and Geometry.
: Theoretical Analysis in Physics.
: Introduction to Elements of Dynamics.
: Modern Alumina.
</listItem>
<bodyText confidence="0.983892454545454">
: The Foundation of Chemical Science.
And we know in advance that they belong to
4 clusters, namely, ,,
and where belongs
to Computer related field, to Mathematics, to
Physics, and to Chemical Science. We will show,
as an example, below how we will set up the classi-
fier to classify the following new document:
: “The Elements of Computing Science.”
We should note that a conventional matching
method of “common” words does not work in this
example, because the words “compute” and, “sci-
ence” in the new document appear in and
separately, while the word “elements” occur in both
and simultaneously, giving no indication on
the appropriate candidate of classification simply by
counting the “common” words among documents.
We will now set up the DLSI-based classifier and
LSI-based classifier for this example.
First, we can easily set up the document vectors of
the database giving the term by document matrix by
simply counting the frequency of occurrences; then
</bodyText>
<subsectionHeader confidence="0.998993">
3.2 DLSI Space-Based Classifier
</subsectionHeader>
<bodyText confidence="0.999605181818182">
The normalized form of the centroid of each cluster
is shown in Table 2.
Following the procedure of the previous section,
it is easy to construct both the differential term by
intra-document matrix and the differential term by
extra-document matrix. Let us denote the differ-
ential term by intra-document matrix by
and the differ-
ential term by extra-document matrix by
respectively.
Note that the’s and ’s can be found in the ma-
trices shown in tables 1 and 2.
Now that we know and ,we can de-
compose them into and
by using SVD algorithm, where
0.25081 0.0449575 -0.157836 -0.428217
0.130941 0.172564 0.143423 0.0844264
-0.240236 0.162075 -0.043428 0.257507
-0.25811 -0.340158 -0.282715 -0.166421
-0.237435 -0.125328 0.439997 -0.15309
0.300435 -0.391284 0.104845 0.193711
0.0851724 0.0449575 -0.157836 0.0549164
0.184643 -0.391284 0.104845 0.531455
-0.25811 -0.340158 -0.282715 -0.166421
0.135018 0.0449575 -0.157836 -0.0904727
0.466072 -0.391284 0.104845 -0.289423
-0.237435 -0.125328 0.439997 -0.15309
0.296578 0.172564 0.143423 -0.398707
-0.124444 0.162075 -0.043428 -0.0802377
-0.25811 -0.340158 -0.282715 -0.166421
-0.237435 -0.125328 0.439997 -0.15309
0.0851724 0.0449575 -0.157836 0.0549164
-0.124444 0.162075 -0.043428 -0.0802377
</bodyText>
<tableCaption confidence="0.791008">
we could further obtain the normalized form as in
Table 1.
</tableCaption>
<table confidence="0.99605264516129">
The document vector for the new document
is given by:
, and in normalized form by
.
diag
0.465291 0.234959 -0.824889 0.218762
-0.425481 -2.12675E-9 1.6628E-9 0.904967
-0.588751 0.733563 -0.196558 -0.276808
0.505809 0.637715 0.530022 0.237812
0.00466227 -0.162108 0.441095 0.0337051
-0.214681 0.13568 0.0608733 -0.387353
0.0265475 -0.210534 -0.168537 -0.529866
-0.383378 0.047418 -0.195619 0.0771912
0.216445 0.397068 0.108622 0.00918756
0.317607 -0.147782 -0.27922 0.0964353
0.12743 0.0388027 0.150228 -0.240946
0.27444 -0.367204 -0.238827 -0.0825893
-0.383378 0.047418 -0.195619 0.0771912
-0.0385053 -0.38153 0.481487 -0.145319
0.19484 -0.348692 0.0116464 0.371087
0.216445 0.397068 0.108622 0.00918756
-0.337448 -0.0652302 0.351739 -0.112702
0.069715 0.00888817 -0.208929 -0.350841
-0.383378 0.047418 -0.195619 0.0771912
0.216445 0.397068 0.108622 0.00918756
0.12743 0.0388027 0.150228 -0.240946
0.069715 0.00888817 -0.208929 -0.350841
0.200663 0.901144 -0.163851 0.347601
-0.285473 -0.0321555 0.746577 0.600078
0.717772 -0.400787 -0.177605 0.540952
-0.60253 -0.162097 -0.619865 0.475868
</table>
<bodyText confidence="0.996162357142857">
We now choose the number in such a way that
remains sufficiently large. Let us choose
and to test the
classifier. Now using equations (3), (4) and (5),
we can calculate the ,
ble 3. The having a largest is
chosen as the cluster to which the new document
belongs. Because both, are actually quite
small, we may here set ,
and . The last row of Ta-
ble 3 clearly shows that Cluster , that is, “Math-
ematics” is the best possibility regardless of the pa-
rameters or chosen,
showing the robustness of the computation.
</bodyText>
<subsectionHeader confidence="0.999803">
3.3 LSI Space-Based Classifier
</subsectionHeader>
<bodyText confidence="0.999480444444445">
As we have already explained in Introduction, the
LSI based-classifier works as follows: First, employ
an SVD algorithm on the term by document matrix
to set up an LSI space, then the classification is com-
pleted within the LSI space.
Using the LSI-based classifier, our experiment
show that, it will return , namely “Physics”, as
the most likely cluster to which the document be-
longs. This is obviously a wrong result.
</bodyText>
<subsectionHeader confidence="0.997586">
3.4 Conclusion of the Example
</subsectionHeader>
<bodyText confidence="0.99997275">
For this simple example, the DLSI space-based ap-
proach finds the most reasonable cluster for the doc-
ument “The elements of computing science”, while
the LSI approach fails to do so.
</bodyText>
<sectionHeader confidence="0.977435" genericHeader="conclusions">
4 Conclusion and Remarks
</sectionHeader>
<bodyText confidence="0.999979884615385">
We have made use of the differential vectors of two
normalized vectors rather than the mere scalar co-
sine of the angle of the two vectors in document
classification procedure, providing a more effective
means of document classifier. Obviously the con-
cept of differential intra- and extra-document vec-
tors imbeds a richer meaning than the mere scalar
measure of cosine, focussing the characteristics of
each document wheere the new classifier demon-
strates an improved and robust performance in doc-
ument classification than the LSI-based cosine ap-
proach. Our model considers both of the projec-
tions and the distances of the differential vectors to
the DLSI spaces, improving the adaptability of the
conventional LSI-based method to the unique char-
acteristics of the individual documents which is a
common weakness of the global projection schemes
including the LSI. The simple experiment demon-
strates convincingly that the performance of our
model outperforms the standard LSI space-based ap-
proach. Just as the cross-language ability of LSI,
DLSI method should also be able to be used for doc-
ument classification of docuements in multiple lan-
guages. We have tested our method using larger col-
lection of texts, we will give details of the results
elsewhere. .
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997806090909091">
M. Benkhalifa, A. Bensaid, and A Mouradi. 1999.
Text categorization using the semi-supervised fuzzy c-
means algorithm. In 18th International Conference of
the North American Fuzzy Information Processing So-
ciety, pages 561–565.
Michael W. Berry, Susan T. Dumais, and G. W. O’Brien.
1995. Using linear algebra for intelligent information
retrieval. SIAM Rev., 37:573–595.
Michael W. Berry, Zlatko Drmac, and Elizabeth R. Jes-
sup. 1999. Matrices, vector spaces, and information
retrieval. SIAM Rev., 41(2):335–362.
</reference>
<bodyText confidence="0.61019275">
diag
and fi-
nally for each differential document vec-
tor ( ) as shown in Ta-
</bodyText>
<tableCaption confidence="0.999336">
Table 1: The normalized document vectors
</tableCaption>
<table confidence="0.999930888888889">
Algebra 0.5 0 0 0.5 0 0 0 0
Alumina 0 0 0 0 0 0 0.707106781 0
Analysis 0 0 0.577350269 0 0.577350269 0 0 0
Chemical 0 0 0 0 0 0 0 0.577350269
Compute 0 0.577350269 0 0 0 0 0 0
Dynamics 0 0 0 0 0 0.577350269 0 0
Education 0.5 0 0 0 0 0 0 0
Element 0 0 0.577350269 0 0 0.577350269 0 0
Foundation 0 0 0 0 0 0 0 0.577350269
Geometry 0.5 0 0.577350269 0.5 0 0 0 0
Introduction 0 0 0 0.5 0 0.577350269 0 0
Machine 0 0.577350269 0 0 0 0 0 0
Modern 0 0 0 0.5 0 0 0.707106781 0
Physics 0 0 0 0 0.577350269 0 0 0
Science 0 0 0 0 0 0 0 0.577350269
Software 0 0.577350269 0 0 0 0 0 0
System 0.5 0 0 0 0 0 0 0
Theory 0 0 0 0 0.577350269 0 0 0
</table>
<tableCaption confidence="0.990773">
Table 2: The normalized cluster centers
</tableCaption>
<table confidence="0.999786">
Algebra 0.353553391 0.311446376 0 0
Alumina 0 0 0 0.5
Analysis 0 0.359627298 0.40824829 0
Chemical 0 0 0 0.40824829
Compute 0.40824829 0 0 0
Dynamics 0 0 0.40824829 0
Education 0.353553391 0 0 0
Element 0 0.359627298 0.40824829 0
Foundation 0 0 0 0.40824829
Geometry 0.353553391 0.671073675 0 0
Introduction 0 0.311446376 0.40824829 0
Machine 0.40824829 0 0 0
Modern 0 0.311446376 0 0.5
Physics 0 0 0.40824829 0
Science 0 0 0 0.40824829
Software 0.40824829 0 0 0
System 0.353553391 0 0 0
Theory 0 0 0.40824829 0
</table>
<tableCaption confidence="0.99805">
Table 3: Classification with DLSI space-based classifier
</tableCaption>
<table confidence="0.8904332">
:
-0.085338834 -0.565752063 -0.368120678 -0.077139955 -0.085338834 -0.556196907 -0.368120678 -0.077139955
-0.404741071 -0.403958563 -0.213933843 -0.250613624
-0.164331163 0.249931018 0.076118938 0.35416984
0.000413135 0.000430473 0.00046034 0.000412671 3.79629E-5 7.03221E-5 3.83428E-5 3.75847E-5
-0.281162007 0.022628465 -0.326936108 0.807673935 -0.281162007 -0.01964297 -0.326936108 0.807673935
-0.276920807 0.6527666 0.475906836 -0.048681069
-0.753558043 -0.619983845 0.258017361 -0.154837357
0.002310807 0.002065451 0.002345484 0.003140447 0.003283825 0.001838634 0.001627501 0.002118787
0.056242843 0.064959115 0.061404975 0.041963635 0.003838728 0.012588493 0.007791905 0.005878172
</table>
<reference confidence="0.999762294871795">
Scott Deerwester, Susan T. Dumais, Grorge W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Societyfor Information Science, 41(6):391–
407.
Jennifer Farkas. 1994. Generating document clusters us-
ing thesauri and neural networks. In Canadian Con-
ference on Electrical and Computer Engineering, vol-
ume 2, pages 710–713.
H. Hyotyniemi. 1996. Text document classification
with self-organizing maps. In STeP ’96 - Genes, Nets
and Symbols. Finnish Artificial Intelligence Confer-
ence, pages 64–72.
M. Iwayama and T. Tokunaga. 1995. Hierarchical
bayesian clustering for automatic text classification.
In Proceedings of the Fourteenth International Joint
Conference on Artificial Intelligence, volume 2, pages
1322–1327.
Wai Lam and Kon-Fan Low. 1997. Automatic document
classification based on probabilistic reasoning: Model
and performance analysis. In Proceedings of the IEEE
International Conference on Systems, Man and Cyber-
netics, volume 3, pages 2719–2723.
D. L. Lee, Huei Chuang, and K. Seamons. 1997. Docu-
ment ranking and the vector-space model. IEEE Soft-
ware, 14(2):67–75.
Wei Li, Bob Lee, Franl Krausz, and Kenan Sahin. 1991.
Text classification by a neural network. In Proceed-
ings of the Twenty-Third Annual Summer Computer
Simulation Conference, pages 313–318.
M. L. Littman, Fan Jiang, and Greg A. Keim. 1998.
Learning a language-independent representation for
terms from a partially aligned corpus. In Proceedings
of the Fifteenth International Conference on Machine
Learning, pages 314–322.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBMJournal of Research and Development,
2(2):159–165, April.
D. Merkl. 1998. Text classification with self-organizing
maps: Some lessons learned. Neurocomputing, 21(1-
3):61–77.
B. Moghaddam and A. Pentland. 1997. Probabilistic vi-
sual learning for object representation. IEEE Trans.
Pattern Analysis and Machine Intelligence, 19(7):696–
710.
B. Moghaddam, W. Wahid, and A. Pentland. 1998.
Beyond eigenfaces: Probabilistic matching for face
recognition. In The 3rd IEEE Int’l Conference on
Automatic Face &amp; Gesture Recognition, Nara, Japan,
April.
Kamal Nigam, Andrew Kachites MaCcallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using em. Ma-
chine Learning, 39(2/3):103–134, May.
V. V. Raghavan and S. K. M. Wong. 1986. A criti-
cal analysis of vector space model for information re-
trieval. Journal of the American Society for Informa-
tion Science, 37(5):279–87.
Gerard Salton. 1983. Introduction to Modern Informa-
tion Retrieval. McGraw-Hill.
Gerard Salton. 1988. Term-weighting approaches in
automatic text retrieval. Information Processing and
Management, 24(5):513–524.
Hinrich Sch¨utze and Craig Silverstein. 1997. Projections
for efficient document clustering. In Proceedings of
SIGIR’97, pages 74–81.
L. Sirovich and M. Kirby. 1987. Low-dimensional pro-
cedure for the characterization of human faces. Jour-
nal of the Optical Society ofAmerica A, 4(3):519–524.
Borge Svingen. 1997. Using genetic programming for
document classification. In John R. Koza, editor, Late
Breaking Papers at the 1997 Genetic Programming
Conference, pages 240–245, Stanford University, CA,
USA, 13–16 July. Stanford Bookstore.
M. Turk and A. Pentland. 1991. Eigenfaces for recogni-
tion. Journal of Cognitive Neuroscience, 3(1):71–86.
C. J. van Rijsbergen. 1979. Information retrieval. But-
terworths.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.047344">
<title confidence="0.999798">A Differential LSI Method for Document Classification</title>
<author confidence="0.959899">Liang</author>
<affiliation confidence="0.999694">Computer Science University of Northern British</affiliation>
<address confidence="0.846116">Prince George, BC, Canada V2N</address>
<email confidence="0.979579">chenl@unbc.ca</email>
<author confidence="0.914959">Naoyuki Tokuda</author>
<affiliation confidence="0.971386">R &amp; D Center, Sunflare Company</affiliation>
<address confidence="0.810597">Shinjuku-Hirose Bldg., 4-7 Yotsuya Sinjuku-ku, Tokyo, Japan 160-0004</address>
<email confidence="0.43048">tokudan@sunflare.co.jp</email>
<affiliation confidence="0.481234">Akira Advanced Media Network Utsunomiya</affiliation>
<address confidence="0.988725">Utsunomiya, Tochigi, Japan</address>
<email confidence="0.971057">anagai@cc.utsunomiya-u.ac.jp</email>
<abstract confidence="0.996761666666667">We have developed an effective probabilistic classifier for document classification by introducing the concept of the differential document vectors and DLSI (differential latent semantics index) spaces. A simple posteriori calculation using the intraand extra-document statistics demonstrates the advantage of the DLSI space-based probabilistic classifier over the popularly used LSI space-based classifier in classification performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Benkhalifa</author>
<author>A Bensaid</author>
<author>A Mouradi</author>
</authors>
<title>Text categorization using the semi-supervised fuzzy cmeans algorithm.</title>
<date>1999</date>
<booktitle>In 18th International Conference of the North American Fuzzy Information Processing Society,</booktitle>
<pages>561--565</pages>
<contexts>
<context position="1743" citStr="Benkhalifa et al., 1999" startWordPosition="239" endWordPosition="242"> classifier developed will select and classify any of new documents introduced into an appropriate cluster within the learning stage. The vector space model is widely used in document classification, where each document is represented as a vector of terms. To represent a document by a document vector, we assign weights to its components usually evaluating the frequency of occurrences of the corresponding terms. Then the standard pattern recognition and machine learning methods are employed for document classification(Li et al., 1991; Farkas, 1994; Svingen, 1997; Hyotyniemi, 1996; Merkl, 1998; Benkhalifa et al., 1999; Iwayama and Tokunaga, 1995; Lam and Low, 1997; Nigam et al., 2000). In view of the inherent flexibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used. If a speedy classification algorithm can be developed (Sch¨utze and Silverstein, 1997), the first problem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace. Like an eigen-decomposition method extensively used in image processing and image </context>
</contexts>
<marker>Benkhalifa, Bensaid, Mouradi, 1999</marker>
<rawString>M. Benkhalifa, A. Bensaid, and A Mouradi. 1999. Text categorization using the semi-supervised fuzzy cmeans algorithm. In 18th International Conference of the North American Fuzzy Information Processing Society, pages 561–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
<author>Susan T Dumais</author>
<author>G W O’Brien</author>
</authors>
<title>Using linear algebra for intelligent information retrieval.</title>
<date>1995</date>
<journal>SIAM Rev.,</journal>
<pages>37--573</pages>
<marker>Berry, Dumais, O’Brien, 1995</marker>
<rawString>Michael W. Berry, Susan T. Dumais, and G. W. O’Brien. 1995. Using linear algebra for intelligent information retrieval. SIAM Rev., 37:573–595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
<author>Zlatko Drmac</author>
<author>Elizabeth R Jessup</author>
</authors>
<title>Matrices, vector spaces, and information retrieval.</title>
<date>1999</date>
<journal>SIAM Rev.,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="2785" citStr="Berry et al., 1999" startWordPosition="402" endWordPosition="405">ensionality reduction scheme enabling the documents’ term projection onto a smaller subspace. Like an eigen-decomposition method extensively used in image processing and image recognition (Sirovich and Kirby, 1987; Turk and Pentland, 1991), the Latent Semantic Indexing (LSI) method has proved to be a most efficient method for the dimensionality reduction scheme in document analysis and extraction, providing a powerful tool for the classifier (Sch¨utze and Silverstein, 1997) when introduced into document retrieval with a good performance confirmed by empirical studies (Deerwester et al., 1990; Berry et al., 1999; Berry et al., 1995).The LSI method has also demonstrated its efficiency for automated cross-language document retrieval in which no query translation is required (Littman et al., 1998). In this paper, we will show that exploiting both of the distances to, and the projections onto, the LSI space improves the performance as well as the robustness of the document classifier. To do this, we introduce, as the major vector space, the differential LSI (or DLSI) space which is formed from the differences between normalized intra- and extradocument vectors and normalized centroid vectors of clusters </context>
</contexts>
<marker>Berry, Drmac, Jessup, 1999</marker>
<rawString>Michael W. Berry, Zlatko Drmac, and Elizabeth R. Jessup. 1999. Matrices, vector spaces, and information retrieval. SIAM Rev., 41(2):335–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>Grorge W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Societyfor Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>407</pages>
<contexts>
<context position="2765" citStr="Deerwester et al., 1990" startWordPosition="397" endWordPosition="401">to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace. Like an eigen-decomposition method extensively used in image processing and image recognition (Sirovich and Kirby, 1987; Turk and Pentland, 1991), the Latent Semantic Indexing (LSI) method has proved to be a most efficient method for the dimensionality reduction scheme in document analysis and extraction, providing a powerful tool for the classifier (Sch¨utze and Silverstein, 1997) when introduced into document retrieval with a good performance confirmed by empirical studies (Deerwester et al., 1990; Berry et al., 1999; Berry et al., 1995).The LSI method has also demonstrated its efficiency for automated cross-language document retrieval in which no query translation is required (Littman et al., 1998). In this paper, we will show that exploiting both of the distances to, and the projections onto, the LSI space improves the performance as well as the robustness of the document classifier. To do this, we introduce, as the major vector space, the differential LSI (or DLSI) space which is formed from the differences between normalized intra- and extradocument vectors and normalized centroid </context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, Grorge W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Societyfor Information Science, 41(6):391– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Farkas</author>
</authors>
<title>Generating document clusters using thesauri and neural networks.</title>
<date>1994</date>
<booktitle>In Canadian Conference on Electrical and Computer Engineering,</booktitle>
<volume>2</volume>
<pages>710--713</pages>
<contexts>
<context position="1672" citStr="Farkas, 1994" startWordPosition="231" endWordPosition="232">a finite number of appropriate clusters in the database, the classifier developed will select and classify any of new documents introduced into an appropriate cluster within the learning stage. The vector space model is widely used in document classification, where each document is represented as a vector of terms. To represent a document by a document vector, we assign weights to its components usually evaluating the frequency of occurrences of the corresponding terms. Then the standard pattern recognition and machine learning methods are employed for document classification(Li et al., 1991; Farkas, 1994; Svingen, 1997; Hyotyniemi, 1996; Merkl, 1998; Benkhalifa et al., 1999; Iwayama and Tokunaga, 1995; Lam and Low, 1997; Nigam et al., 2000). In view of the inherent flexibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used. If a speedy classification algorithm can be developed (Sch¨utze and Silverstein, 1997), the first problem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace. Like an eig</context>
</contexts>
<marker>Farkas, 1994</marker>
<rawString>Jennifer Farkas. 1994. Generating document clusters using thesauri and neural networks. In Canadian Conference on Electrical and Computer Engineering, volume 2, pages 710–713.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hyotyniemi</author>
</authors>
<title>Text document classification with self-organizing maps.</title>
<date>1996</date>
<booktitle>In STeP ’96 - Genes, Nets and Symbols. Finnish Artificial Intelligence Conference,</booktitle>
<pages>64--72</pages>
<contexts>
<context position="1705" citStr="Hyotyniemi, 1996" startWordPosition="235" endWordPosition="236">e clusters in the database, the classifier developed will select and classify any of new documents introduced into an appropriate cluster within the learning stage. The vector space model is widely used in document classification, where each document is represented as a vector of terms. To represent a document by a document vector, we assign weights to its components usually evaluating the frequency of occurrences of the corresponding terms. Then the standard pattern recognition and machine learning methods are employed for document classification(Li et al., 1991; Farkas, 1994; Svingen, 1997; Hyotyniemi, 1996; Merkl, 1998; Benkhalifa et al., 1999; Iwayama and Tokunaga, 1995; Lam and Low, 1997; Nigam et al., 2000). In view of the inherent flexibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used. If a speedy classification algorithm can be developed (Sch¨utze and Silverstein, 1997), the first problem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace. Like an eigen-decomposition method extensive</context>
</contexts>
<marker>Hyotyniemi, 1996</marker>
<rawString>H. Hyotyniemi. 1996. Text document classification with self-organizing maps. In STeP ’96 - Genes, Nets and Symbols. Finnish Artificial Intelligence Conference, pages 64–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Iwayama</author>
<author>T Tokunaga</author>
</authors>
<title>Hierarchical bayesian clustering for automatic text classification.</title>
<date>1995</date>
<booktitle>In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,</booktitle>
<volume>2</volume>
<pages>1322--1327</pages>
<contexts>
<context position="1771" citStr="Iwayama and Tokunaga, 1995" startWordPosition="243" endWordPosition="246">l select and classify any of new documents introduced into an appropriate cluster within the learning stage. The vector space model is widely used in document classification, where each document is represented as a vector of terms. To represent a document by a document vector, we assign weights to its components usually evaluating the frequency of occurrences of the corresponding terms. Then the standard pattern recognition and machine learning methods are employed for document classification(Li et al., 1991; Farkas, 1994; Svingen, 1997; Hyotyniemi, 1996; Merkl, 1998; Benkhalifa et al., 1999; Iwayama and Tokunaga, 1995; Lam and Low, 1997; Nigam et al., 2000). In view of the inherent flexibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used. If a speedy classification algorithm can be developed (Sch¨utze and Silverstein, 1997), the first problem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace. Like an eigen-decomposition method extensively used in image processing and image recognition (Sirovich and Ki</context>
</contexts>
<marker>Iwayama, Tokunaga, 1995</marker>
<rawString>M. Iwayama and T. Tokunaga. 1995. Hierarchical bayesian clustering for automatic text classification. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, volume 2, pages 1322–1327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wai Lam</author>
<author>Kon-Fan Low</author>
</authors>
<title>Automatic document classification based on probabilistic reasoning: Model and performance analysis.</title>
<date>1997</date>
<booktitle>In Proceedings of the IEEE International Conference on Systems, Man and Cybernetics,</booktitle>
<volume>3</volume>
<pages>2719--2723</pages>
<contexts>
<context position="1790" citStr="Lam and Low, 1997" startWordPosition="247" endWordPosition="250"> new documents introduced into an appropriate cluster within the learning stage. The vector space model is widely used in document classification, where each document is represented as a vector of terms. To represent a document by a document vector, we assign weights to its components usually evaluating the frequency of occurrences of the corresponding terms. Then the standard pattern recognition and machine learning methods are employed for document classification(Li et al., 1991; Farkas, 1994; Svingen, 1997; Hyotyniemi, 1996; Merkl, 1998; Benkhalifa et al., 1999; Iwayama and Tokunaga, 1995; Lam and Low, 1997; Nigam et al., 2000). In view of the inherent flexibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used. If a speedy classification algorithm can be developed (Sch¨utze and Silverstein, 1997), the first problem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace. Like an eigen-decomposition method extensively used in image processing and image recognition (Sirovich and Kirby, 1987; Turk and</context>
</contexts>
<marker>Lam, Low, 1997</marker>
<rawString>Wai Lam and Kon-Fan Low. 1997. Automatic document classification based on probabilistic reasoning: Model and performance analysis. In Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, volume 3, pages 2719–2723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Lee</author>
<author>Huei Chuang</author>
<author>K Seamons</author>
</authors>
<title>Document ranking and the vector-space model.</title>
<date>1997</date>
<journal>IEEE Software,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="5936" citStr="Lee et al., 1997" startWordPosition="921" endWordPosition="924">where is the local weighting of the term in the document indicating the significance of the term in the document, while is a global weight of all the documents, which is a parameter indicating the importance of the term in representing the documents. Local weights could be either raw occurrence counts, boolean, or logarithms of occurrence counts. Global ones could be no weighting (uniform), domain specific, or entropy weighting. Both of the local and global weights are thoroughly studied in the literatures (Raghavan and Wong, 1986; Luhn, 1958; van Rijsbergen, 1979; Salton, 1983; Salton, 1988; Lee et al., 1997), and will not be discussed further in this paper. An example will be given below: where ,is the total number of times that term appears in the collection, the number of times the term appears in the document , and the number of documents in the collection. The document vector can be normalized as (1) The normalized centroid vector of a cluster can be calculated in terms of the normalized vector as , where and 2 Main Algorithm 2.1 Basic Concepts A term is defined as a word or a phrase that appears at least in two documents. We exclude the so-called stop words such as “a”, “the” , ”of” and so f</context>
</contexts>
<marker>Lee, Chuang, Seamons, 1997</marker>
<rawString>D. L. Lee, Huei Chuang, and K. Seamons. 1997. Document ranking and the vector-space model. IEEE Software, 14(2):67–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Bob Lee</author>
<author>Franl Krausz</author>
<author>Kenan Sahin</author>
</authors>
<title>Text classification by a neural network.</title>
<date>1991</date>
<booktitle>In Proceedings of the Twenty-Third Annual Summer Computer Simulation Conference,</booktitle>
<pages>313--318</pages>
<contexts>
<context position="1658" citStr="Li et al., 1991" startWordPosition="226" endWordPosition="230">eclassified into a finite number of appropriate clusters in the database, the classifier developed will select and classify any of new documents introduced into an appropriate cluster within the learning stage. The vector space model is widely used in document classification, where each document is represented as a vector of terms. To represent a document by a document vector, we assign weights to its components usually evaluating the frequency of occurrences of the corresponding terms. Then the standard pattern recognition and machine learning methods are employed for document classification(Li et al., 1991; Farkas, 1994; Svingen, 1997; Hyotyniemi, 1996; Merkl, 1998; Benkhalifa et al., 1999; Iwayama and Tokunaga, 1995; Lam and Low, 1997; Nigam et al., 2000). In view of the inherent flexibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used. If a speedy classification algorithm can be developed (Sch¨utze and Silverstein, 1997), the first problem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspac</context>
</contexts>
<marker>Li, Lee, Krausz, Sahin, 1991</marker>
<rawString>Wei Li, Bob Lee, Franl Krausz, and Kenan Sahin. 1991. Text classification by a neural network. In Proceedings of the Twenty-Third Annual Summer Computer Simulation Conference, pages 313–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Littman</author>
<author>Fan Jiang</author>
<author>Greg A Keim</author>
</authors>
<title>Learning a language-independent representation for terms from a partially aligned corpus.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning,</booktitle>
<pages>314--322</pages>
<contexts>
<context position="2971" citStr="Littman et al., 1998" startWordPosition="429" endWordPosition="432">tion (Sirovich and Kirby, 1987; Turk and Pentland, 1991), the Latent Semantic Indexing (LSI) method has proved to be a most efficient method for the dimensionality reduction scheme in document analysis and extraction, providing a powerful tool for the classifier (Sch¨utze and Silverstein, 1997) when introduced into document retrieval with a good performance confirmed by empirical studies (Deerwester et al., 1990; Berry et al., 1999; Berry et al., 1995).The LSI method has also demonstrated its efficiency for automated cross-language document retrieval in which no query translation is required (Littman et al., 1998). In this paper, we will show that exploiting both of the distances to, and the projections onto, the LSI space improves the performance as well as the robustness of the document classifier. To do this, we introduce, as the major vector space, the differential LSI (or DLSI) space which is formed from the differences between normalized intra- and extradocument vectors and normalized centroid vectors of clusters where the intra- and extra-document refers to the documents included within or outside of the given cluster respectively. The new classifier sets up a Baysian posteriori probability func</context>
</contexts>
<marker>Littman, Jiang, Keim, 1998</marker>
<rawString>M. L. Littman, Fan Jiang, and Greg A. Keim. 1998. Learning a language-independent representation for terms from a partially aligned corpus. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 314–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBMJournal of Research and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="5867" citStr="Luhn, 1958" startWordPosition="912" endWordPosition="913">tion, we assign each of the terms with a real vector , with , where is the local weighting of the term in the document indicating the significance of the term in the document, while is a global weight of all the documents, which is a parameter indicating the importance of the term in representing the documents. Local weights could be either raw occurrence counts, boolean, or logarithms of occurrence counts. Global ones could be no weighting (uniform), domain specific, or entropy weighting. Both of the local and global weights are thoroughly studied in the literatures (Raghavan and Wong, 1986; Luhn, 1958; van Rijsbergen, 1979; Salton, 1983; Salton, 1988; Lee et al., 1997), and will not be discussed further in this paper. An example will be given below: where ,is the total number of times that term appears in the collection, the number of times the term appears in the document , and the number of documents in the collection. The document vector can be normalized as (1) The normalized centroid vector of a cluster can be calculated in terms of the normalized vector as , where and 2 Main Algorithm 2.1 Basic Concepts A term is defined as a word or a phrase that appears at least in two documents. W</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H. P. Luhn. 1958. The automatic creation of literature abstracts. IBMJournal of Research and Development, 2(2):159–165, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Merkl</author>
</authors>
<title>Text classification with self-organizing maps: Some lessons learned.</title>
<date>1998</date>
<journal>Neurocomputing,</journal>
<pages>21--1</pages>
<contexts>
<context position="1718" citStr="Merkl, 1998" startWordPosition="237" endWordPosition="238">database, the classifier developed will select and classify any of new documents introduced into an appropriate cluster within the learning stage. The vector space model is widely used in document classification, where each document is represented as a vector of terms. To represent a document by a document vector, we assign weights to its components usually evaluating the frequency of occurrences of the corresponding terms. Then the standard pattern recognition and machine learning methods are employed for document classification(Li et al., 1991; Farkas, 1994; Svingen, 1997; Hyotyniemi, 1996; Merkl, 1998; Benkhalifa et al., 1999; Iwayama and Tokunaga, 1995; Lam and Low, 1997; Nigam et al., 2000). In view of the inherent flexibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used. If a speedy classification algorithm can be developed (Sch¨utze and Silverstein, 1997), the first problem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace. Like an eigen-decomposition method extensively used in im</context>
</contexts>
<marker>Merkl, 1998</marker>
<rawString>D. Merkl. 1998. Text classification with self-organizing maps: Some lessons learned. Neurocomputing, 21(1-3):61–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Moghaddam</author>
<author>A Pentland</author>
</authors>
<title>Probabilistic visual learning for object representation.</title>
<date>1997</date>
<journal>IEEE Trans. Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>7</issue>
<pages>710</pages>
<contexts>
<context position="3866" citStr="Moghaddam and Pentland, 1997" startWordPosition="572" endWordPosition="575">r DLSI) space which is formed from the differences between normalized intra- and extradocument vectors and normalized centroid vectors of clusters where the intra- and extra-document refers to the documents included within or outside of the given cluster respectively. The new classifier sets up a Baysian posteriori probability function for the differential document vectors based on their projections on DLSI space and their distances to the DLSI space, the document category with a highest probability is then selected. A similar approach is taken by Moghaddam and Pentland for image recognition (Moghaddam and Pentland, 1997; Moghaddam et al., 1998). We may summarize the specific features introduced into the new document classification scheme based on the concept of the differential document vector and the DLSI vectors: 1. Exploiting the characteristic distance of the differential document vector to the DLSI space and the projection of the differential document onto the DLSI space, which we believe to denote the differences in word usage between the document and a cluster’s centroid vector, the differential document vector is capable of capturing the relation between the particular document and the cluster. 2. A </context>
</contexts>
<marker>Moghaddam, Pentland, 1997</marker>
<rawString>B. Moghaddam and A. Pentland. 1997. Probabilistic visual learning for object representation. IEEE Trans. Pattern Analysis and Machine Intelligence, 19(7):696– 710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Moghaddam</author>
<author>W Wahid</author>
<author>A Pentland</author>
</authors>
<title>Beyond eigenfaces: Probabilistic matching for face recognition.</title>
<date>1998</date>
<booktitle>In The 3rd IEEE Int’l Conference on Automatic Face &amp; Gesture Recognition,</booktitle>
<location>Nara, Japan,</location>
<contexts>
<context position="3891" citStr="Moghaddam et al., 1998" startWordPosition="576" endWordPosition="579">from the differences between normalized intra- and extradocument vectors and normalized centroid vectors of clusters where the intra- and extra-document refers to the documents included within or outside of the given cluster respectively. The new classifier sets up a Baysian posteriori probability function for the differential document vectors based on their projections on DLSI space and their distances to the DLSI space, the document category with a highest probability is then selected. A similar approach is taken by Moghaddam and Pentland for image recognition (Moghaddam and Pentland, 1997; Moghaddam et al., 1998). We may summarize the specific features introduced into the new document classification scheme based on the concept of the differential document vector and the DLSI vectors: 1. Exploiting the characteristic distance of the differential document vector to the DLSI space and the projection of the differential document onto the DLSI space, which we believe to denote the differences in word usage between the document and a cluster’s centroid vector, the differential document vector is capable of capturing the relation between the particular document and the cluster. 2. A major problem of context </context>
</contexts>
<marker>Moghaddam, Wahid, Pentland, 1998</marker>
<rawString>B. Moghaddam, W. Wahid, and A. Pentland. 1998. Beyond eigenfaces: Probabilistic matching for face recognition. In The 3rd IEEE Int’l Conference on Automatic Face &amp; Gesture Recognition, Nara, Japan, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew Kachites MaCcallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using em.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="1811" citStr="Nigam et al., 2000" startWordPosition="251" endWordPosition="254">oduced into an appropriate cluster within the learning stage. The vector space model is widely used in document classification, where each document is represented as a vector of terms. To represent a document by a document vector, we assign weights to its components usually evaluating the frequency of occurrences of the corresponding terms. Then the standard pattern recognition and machine learning methods are employed for document classification(Li et al., 1991; Farkas, 1994; Svingen, 1997; Hyotyniemi, 1996; Merkl, 1998; Benkhalifa et al., 1999; Iwayama and Tokunaga, 1995; Lam and Low, 1997; Nigam et al., 2000). In view of the inherent flexibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used. If a speedy classification algorithm can be developed (Sch¨utze and Silverstein, 1997), the first problem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace. Like an eigen-decomposition method extensively used in image processing and image recognition (Sirovich and Kirby, 1987; Turk and Pentland, 1991), the</context>
</contexts>
<marker>Nigam, MaCcallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew Kachites MaCcallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classification from labeled and unlabeled documents using em. Machine Learning, 39(2/3):103–134, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V V Raghavan</author>
<author>S K M Wong</author>
</authors>
<title>A critical analysis of vector space model for information retrieval.</title>
<date>1986</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>37</volume>
<issue>5</issue>
<contexts>
<context position="5855" citStr="Raghavan and Wong, 1986" startWordPosition="908" endWordPosition="911">ch document in the collection, we assign each of the terms with a real vector , with , where is the local weighting of the term in the document indicating the significance of the term in the document, while is a global weight of all the documents, which is a parameter indicating the importance of the term in representing the documents. Local weights could be either raw occurrence counts, boolean, or logarithms of occurrence counts. Global ones could be no weighting (uniform), domain specific, or entropy weighting. Both of the local and global weights are thoroughly studied in the literatures (Raghavan and Wong, 1986; Luhn, 1958; van Rijsbergen, 1979; Salton, 1983; Salton, 1988; Lee et al., 1997), and will not be discussed further in this paper. An example will be given below: where ,is the total number of times that term appears in the collection, the number of times the term appears in the document , and the number of documents in the collection. The document vector can be normalized as (1) The normalized centroid vector of a cluster can be calculated in terms of the normalized vector as , where and 2 Main Algorithm 2.1 Basic Concepts A term is defined as a word or a phrase that appears at least in two </context>
</contexts>
<marker>Raghavan, Wong, 1986</marker>
<rawString>V. V. Raghavan and S. K. M. Wong. 1986. A critical analysis of vector space model for information retrieval. Journal of the American Society for Information Science, 37(5):279–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="5903" citStr="Salton, 1983" startWordPosition="917" endWordPosition="918">with a real vector , with , where is the local weighting of the term in the document indicating the significance of the term in the document, while is a global weight of all the documents, which is a parameter indicating the importance of the term in representing the documents. Local weights could be either raw occurrence counts, boolean, or logarithms of occurrence counts. Global ones could be no weighting (uniform), domain specific, or entropy weighting. Both of the local and global weights are thoroughly studied in the literatures (Raghavan and Wong, 1986; Luhn, 1958; van Rijsbergen, 1979; Salton, 1983; Salton, 1988; Lee et al., 1997), and will not be discussed further in this paper. An example will be given below: where ,is the total number of times that term appears in the collection, the number of times the term appears in the document , and the number of documents in the collection. The document vector can be normalized as (1) The normalized centroid vector of a cluster can be calculated in terms of the normalized vector as , where and 2 Main Algorithm 2.1 Basic Concepts A term is defined as a word or a phrase that appears at least in two documents. We exclude the so-called stop words s</context>
</contexts>
<marker>Salton, 1983</marker>
<rawString>Gerard Salton. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>24--5</pages>
<contexts>
<context position="5917" citStr="Salton, 1988" startWordPosition="919" endWordPosition="920">ctor , with , where is the local weighting of the term in the document indicating the significance of the term in the document, while is a global weight of all the documents, which is a parameter indicating the importance of the term in representing the documents. Local weights could be either raw occurrence counts, boolean, or logarithms of occurrence counts. Global ones could be no weighting (uniform), domain specific, or entropy weighting. Both of the local and global weights are thoroughly studied in the literatures (Raghavan and Wong, 1986; Luhn, 1958; van Rijsbergen, 1979; Salton, 1983; Salton, 1988; Lee et al., 1997), and will not be discussed further in this paper. An example will be given below: where ,is the total number of times that term appears in the collection, the number of times the term appears in the document , and the number of documents in the collection. The document vector can be normalized as (1) The normalized centroid vector of a cluster can be calculated in terms of the normalized vector as , where and 2 Main Algorithm 2.1 Basic Concepts A term is defined as a word or a phrase that appears at least in two documents. We exclude the so-called stop words such as “a”, “t</context>
</contexts>
<marker>Salton, 1988</marker>
<rawString>Gerard Salton. 1988. Term-weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513–524.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
<author>Craig Silverstein</author>
</authors>
<title>Projections for efficient document clustering.</title>
<date>1997</date>
<booktitle>In Proceedings of SIGIR’97,</booktitle>
<pages>74--81</pages>
<marker>Sch¨utze, Silverstein, 1997</marker>
<rawString>Hinrich Sch¨utze and Craig Silverstein. 1997. Projections for efficient document clustering. In Proceedings of SIGIR’97, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Sirovich</author>
<author>M Kirby</author>
</authors>
<title>Low-dimensional procedure for the characterization of human faces.</title>
<date>1987</date>
<journal>Journal of the Optical Society ofAmerica A,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="2380" citStr="Sirovich and Kirby, 1987" startWordPosition="337" endWordPosition="340"> Tokunaga, 1995; Lam and Low, 1997; Nigam et al., 2000). In view of the inherent flexibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used. If a speedy classification algorithm can be developed (Sch¨utze and Silverstein, 1997), the first problem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace. Like an eigen-decomposition method extensively used in image processing and image recognition (Sirovich and Kirby, 1987; Turk and Pentland, 1991), the Latent Semantic Indexing (LSI) method has proved to be a most efficient method for the dimensionality reduction scheme in document analysis and extraction, providing a powerful tool for the classifier (Sch¨utze and Silverstein, 1997) when introduced into document retrieval with a good performance confirmed by empirical studies (Deerwester et al., 1990; Berry et al., 1999; Berry et al., 1995).The LSI method has also demonstrated its efficiency for automated cross-language document retrieval in which no query translation is required (Littman et al., 1998). In this</context>
</contexts>
<marker>Sirovich, Kirby, 1987</marker>
<rawString>L. Sirovich and M. Kirby. 1987. Low-dimensional procedure for the characterization of human faces. Journal of the Optical Society ofAmerica A, 4(3):519–524.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Borge Svingen</author>
</authors>
<title>Using genetic programming for document classification. In</title>
<date>1997</date>
<booktitle>Late Breaking Papers at the 1997 Genetic Programming Conference,</booktitle>
<pages>240--245</pages>
<editor>John R. Koza, editor,</editor>
<location>Stanford University, CA, USA,</location>
<contexts>
<context position="1687" citStr="Svingen, 1997" startWordPosition="233" endWordPosition="234">r of appropriate clusters in the database, the classifier developed will select and classify any of new documents introduced into an appropriate cluster within the learning stage. The vector space model is widely used in document classification, where each document is represented as a vector of terms. To represent a document by a document vector, we assign weights to its components usually evaluating the frequency of occurrences of the corresponding terms. Then the standard pattern recognition and machine learning methods are employed for document classification(Li et al., 1991; Farkas, 1994; Svingen, 1997; Hyotyniemi, 1996; Merkl, 1998; Benkhalifa et al., 1999; Iwayama and Tokunaga, 1995; Lam and Low, 1997; Nigam et al., 2000). In view of the inherent flexibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used. If a speedy classification algorithm can be developed (Sch¨utze and Silverstein, 1997), the first problem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace. Like an eigen-decompositio</context>
</contexts>
<marker>Svingen, 1997</marker>
<rawString>Borge Svingen. 1997. Using genetic programming for document classification. In John R. Koza, editor, Late Breaking Papers at the 1997 Genetic Programming Conference, pages 240–245, Stanford University, CA, USA, 13–16 July. Stanford Bookstore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Turk</author>
<author>A Pentland</author>
</authors>
<title>Eigenfaces for recognition.</title>
<date>1991</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="2406" citStr="Turk and Pentland, 1991" startWordPosition="341" endWordPosition="344">ow, 1997; Nigam et al., 2000). In view of the inherent flexibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used. If a speedy classification algorithm can be developed (Sch¨utze and Silverstein, 1997), the first problem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace. Like an eigen-decomposition method extensively used in image processing and image recognition (Sirovich and Kirby, 1987; Turk and Pentland, 1991), the Latent Semantic Indexing (LSI) method has proved to be a most efficient method for the dimensionality reduction scheme in document analysis and extraction, providing a powerful tool for the classifier (Sch¨utze and Silverstein, 1997) when introduced into document retrieval with a good performance confirmed by empirical studies (Deerwester et al., 1990; Berry et al., 1999; Berry et al., 1995).The LSI method has also demonstrated its efficiency for automated cross-language document retrieval in which no query translation is required (Littman et al., 1998). In this paper, we will show that </context>
</contexts>
<marker>Turk, Pentland, 1991</marker>
<rawString>M. Turk and A. Pentland. 1991. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1):71–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<date>1979</date>
<journal>Information retrieval. Butterworths.</journal>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information retrieval. Butterworths.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>